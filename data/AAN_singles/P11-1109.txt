Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1087?1097,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsExtracting Paraphrases from Definition Sentences on the WebChikara Hashimoto?
Kentaro Torisawa?
Stijn De Saeger?Jun?ichi Kazama?
Sadao Kurohashi??
?
?
?
National Institute of Information and Communications TechnologyKyoto, 619-0237, JAPAN?
?Graduate School of Informatics, Kyoto UniversityKyoto, 606-8501, JAPAN{?
ch,?
torisawa, ?
stijn,?
kazama}@nict.go.jp?kuro@i.kyoto-u.ac.jpAbstractWe propose an automatic method of extractingparaphrases from definition sentences, whichare also automatically acquired from the Web.We observe that a huge number of conceptsare defined in Web documents, and that thesentences that define the same concept tendto convey mostly the same information usingdifferent expressions and thus contain manyparaphrases.
We show that a large numberof paraphrases can be automatically extractedwith high precision by regarding the sentencesthat define the same concept as parallel cor-pora.
Experimental results indicated that withour method it was possible to extract about300,000 paraphrases from 6?
108 Web docu-ments with a precision rate of about 94%.1 IntroductionNatural language allows us to express the same in-formation in many ways, which makes natural lan-guage processing (NLP) a challenging area.
Ac-cordingly, many researchers have recognized thatautomatic paraphrasing is an indispensable compo-nent of intelligent NLP systems (Iordanskaja et al,1991; McKeown et al, 2002; Lin and Pantel, 2001;Ravichandran and Hovy, 2002; Kauchak and Barzi-lay, 2006; Callison-Burch et al, 2006) and have triedto acquire a large amount of paraphrase knowledge,which is a key to achieving robust automatic para-phrasing, from corpora (Lin and Pantel, 2001; Barzi-lay and McKeown, 2001; Shinyama et al, 2002;Barzilay and Lee, 2003).We propose a method to extract phrasal para-phrases from pairs of sentences that define the sameconcept.
The method is based on our observationthat two sentences defining the same concept canbe regarded as a parallel corpus since they largelyconvey the same information using different expres-sions.
Such definition sentences abound on the Web.This suggests that we may be able to extract a largeamount of phrasal paraphrase knowledge from thedefinition sentences on the Web.For instance, the following two sentences, both ofwhich define the same concept ?osteoporosis?, in-clude two pairs of phrasal paraphrases, which areindicated by underlines 1?
and 2?, respectively.
(1) a. Osteoporosis is a disease that 1?
decreases thequantity of bone and 2?
makes bones fragile.b.
Osteoporosis is a disease that 1?
reduces bonemass and 2?
increases the risk of bone fracture.We define paraphrase as a pair of expressions be-tween which entailment relations of both directionshold.
(Androutsopoulos and Malakasiotis, 2010).Our objective is to extract phrasal paraphrasesfrom pairs of sentences that define the same con-cept.
We propose a supervised method that exploitsvarious kinds of lexical similarity features and con-textual features.
Sentences defining certain conceptsare acquired automatically on a large scale from theWeb by applying a quite simple supervised method.Previous methods most relevant to our workused parallel corpora such as multiple translationsof the same source text (Barzilay and McKeown,2001) or automatically acquired parallel news texts(Shinyama et al, 2002; Barzilay and Lee, 2003;Dolan et al, 2004).
The former requires a largeamount of manual labor to translate the same texts1087in several ways.
The latter would suffer from thefact that it is not easy to automatically retrieve largebodies of parallel news text with high accuracy.
Onthe contrary, recognizing definition sentences forthe same concept is quite an easy task at least forJapanese, as we will show, and we were able to finda huge amount of definition sentence pairs from nor-mal Web texts.
In our experiments, about 30 milliondefinition sentence pairs were extracted from 6?108Web documents, and the estimated number of para-phrases recognized in the definition sentences usingour method was about 300,000, for a precision rateof about 94%.
Also, our experimental results showthat our method is superior to well-known compet-ing methods (Barzilay and McKeown, 2001; Koehnet al, 2007) for extracting paraphrases from defini-tion sentence pairs.Our evaluation is based on bidirectional check-ing of entailment relations between paraphrases thatconsiders the context dependence of a paraphrase.Note that using definition sentences is only thebeginning of our research on paraphrase extraction.We have a more general hypothesis that sentencesfulfilling the same pragmatic function (e.g.
defini-tion) for the same topic (e.g.
osteoporosis) conveymostly the same information using different expres-sions.
Such functions other than definition may in-clude the usage of the same Linux command, therecipe for the same cuisine, or the description of re-lated work on the same research issue.Section 2 describes related works.
Section 3presents our proposed method.
Section 4 reports onevaluation results.
Section 5 concludes the paper.2 Related WorkThe existing work for paraphrase extraction is cat-egorized into two groups.
The first involves a dis-tributional similarity approach pioneered by Lin andPantel (2001).
Basically, this approach assumes thattwo expressions that have a large distributional simi-larity are paraphrases.
There are also variants of thisapproach that address entailment acquisition (Geffetand Dagan, 2005; Bhagat et al, 2007; Szpektor andDagan, 2008; Hashimoto et al, 2009).
These meth-ods can be applied to a normal monolingual corpus,and it has been shown that a large number of para-phrases or entailment rules could be extracted.
How-ever, the precision of these methods has been rela-tively low.
This is due to the fact that the evidence,i.e., distributional similarity, is just indirect evidenceof paraphrase/entailment.
Accordingly, these meth-ods occasionally mistake antonymous pairs for para-phrases/entailment pairs, since an expression and itsantonymous counterpart are also likely to have alarge distributional similarity.
Another limitation ofthese methods is that they can find only paraphrasesconsisting of frequently observed expressions sincethey must have reliable distributional similarity val-ues for expressions that constitute paraphrases.The second category is a parallel corpus approach(Barzilay and McKeown, 2001; Shinyama et al,2002; Barzilay and Lee, 2003; Dolan et al, 2004).Our method belongs to this category.
This approachaligns expressions between two sentences in par-allel corpora, based on, for example, the overlapof words/contexts.
The aligned expressions are as-sumed to be paraphrases.
In this approach, the ex-pressions do not need to appear frequently in thecorpora.
Furthermore, the approach rarely mistakesantonymous pairs for paraphrases/entailment pairs.However, its limitation is the difficulty in preparinga large amount of parallel corpora, as noted before.We avoid this by using definition sentences, whichcan be easily acquired on a large scale from theWeb,as parallel corpora.Murata et al (2004) used definition sentences intwo manually compiled dictionaries, which are con-siderably fewer in the number of definition sen-tences than those on the Web.
Thus, the coverage oftheir method should be quite limited.
Furthermore,the precision of their method is much poorer thanours as we report in Section 4.For a more extensive survey on paraphrasingmethods, see Androutsopoulos and Malakasiotis(2010) and Madnani and Dorr (2010).3 Proposed methodOur method, targeting the Japanese language, con-sists of two steps: definition sentence acquisitionand paraphrase extraction.
We describe them below.3.1 Definition sentence acquisitionWe acquire sentences that define a concept (defini-tion sentences) as in Example (2), which defines ??1088????
(osteoporosis), from the 6?108 Web pages(Akamine et al, 2010) and the Japanese Wikipedia.
(2) ??????????????????????
(Osteoporosis is a disease that makes bones fragile.
)Fujii and Ishikawa (2002) developed an unsuper-vised method to find definition sentences from theWeb using 18 sentential templates and a languagemodel constructed from an encyclopedia.
On theother hand, we developed a supervised method toachieve a higher precision.We use one sentential template and an SVM clas-sifier.
Specifically, we first collect definition sen-tence candidates by a template ??NP??.
*?, where?
is the beginning of sentence and NP is the nounphrase expressing the concept to be defined followedby a particle sequence, ???
(comitative) and ???
(topic) (and optionally followed by comma), as ex-emplified in (2).
As a result, we collected 3,027,101sentences.
Although the particle sequence tendsto mark the topic of the definition sentence, it canalso appear in interrogative sentences and normal as-sertive sentences in which a topic is strongly empha-sized.
To remove such non-definition sentences, weclassify the candidate sentences using an SVM clas-sifier with a polynominal kernel (d = 2).1 SinceJapanese is a head-final language and we can judgewhether a sentence is interrogative or not from thelast words in the sentence, we included morphemeN -grams and bag-of-words (with the window sizeof N ) at the end of sentences in the feature set.
Thefeatures are also useful for confirming that the headverb is in the present tense, which definition sen-tences should be.
Also, we added the morphemeN -grams and bag-of-words right after the particlesequence in the feature set since we observe thatnon-definition sentences tend to have interrogativerelated words like ???
(what) or ????
((what) onearth) right after the particle sequence.
We chose 5as N from our preliminary experiments.Our training data was constructed from 2,911 sen-tences randomly sampled from all of the collectedsentences.
61.1% of them were labeled as positive.In the 10-fold cross validation, the classifier?s ac-curacy, precision, recall, and F1 were 89.4, 90.7,1We use SVMlight available at http://svmlight.joachims.org/.92.2, and 91.4, respectively.
Using the classifier,we acquired 1,925,052 positive sentences from allof the collected sentences.
After adding definitionsentences from Wikipedia articles, which are typi-cally the first sentence of the body of each article(Kazama and Torisawa, 2007), we obtained a totalof 2,141,878 definition sentence candidates, whichcovered 867,321 concepts ranging from weapons torules of baseball.
Then, we coupled two definitionsentences whose defined concepts were the sameand obtained 29,661,812 definition sentence pairs.Obviously, our method is tailored to Japanese.
Fora language-independent method of definition acqui-sition, see Navigli and Velardi (2010) as an example.3.2 Paraphrase extractionParaphrase extraction proceeds as follows.
First,each sentence in a pair is parsed by the depen-dency parser KNP2 and dependency tree frag-ments that constitute linguistically well-formed con-stituents are extracted.
The extracted dependencytree fragments are called candidate phrases here-after.
We restricted candidate phrases to predicatephrases that consist of at least one dependency re-lation, do not contain demonstratives, and in whichall the leaf nodes are nominal and all of the con-stituents are consecutive in the sentence.
KNP indi-cates whether each candidate phrase is a predicatebased on the POS of the head morpheme.
Then,we check all the pairs of candidate phrases betweentwo definition sentences to find paraphrase pairs.3In (1), repeated in (3), candidate phrase pairs to bechecked include ( 1?
decreases the quantity of bone,1?
reduces bone mass), ( 1?
decreases the quantityof bone, 2?
increases the risk of bone fracture), ( 2?makes bones fragile, 1?
reduces bone mass), and ( 2?makes bones fragile, 2?
increases the risk of bonefracture).
(3) a. Osteoporosis is a disease that 1?
decreases thequantity of bone and 2?
makes bones fragile.b.
Osteoporosis is a disease that 1?
reduces bonemass and 2?
increases the risk of bone fracture.2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html.3Our method discards candidate phrase pairs in which onesubsumes the other in terms of their character string, or the dif-ference is only one proper noun like ?toner cartridges that Ap-ple Inc. made?
and ?toner cartridges that Xerox made.?
Propernouns are recognized by KNP.1089f1 The ratio of the number of morphemes shared between two candidate phrases to the number of all of the morphemes in the two phrases.f2 The ratio of the number of a candidate phrase?s morphemes, for which there is a morpheme with small edit distance (1 in our experiment) inanother candidate phrase, to the number of all of the morphemes in the two phrases.
Note that Japanese has many orthographical variationsand edit distance is useful for identifying them.f3 The ratio of the number of a candidate phrase?s morphemes, for which there is a morpheme with the same pronunciation in another candidatephrase, to the number of all of the morphemes in the two phrases.
Pronunciation is also useful for identifying orthographic variations.Pronunciation is given by KNP.f4 The ratio of the number of morphemes of a shorter candidate phrase to that of a longer one.f5 The identity of the inflected form of the head morpheme between two candidate phrases: 1 if they are identical, 0 otherwise.f6 The identity of the POS of the head morpheme between two candidate phrases: 1 or 0.f7 The identity of the inflection (conjugation) of the head morpheme between two candidate phrases: 1 or 0.f8 The ratio of the number of morphemes that appear in a candidate phrase segment of a definition sentence s1 and in a segment that is NOT apart of the candidate phrase of another definition sentence s2 to the number of all of the morphemes of s1?s candidate phrase, i.e.
how manyextra morphemes are incorporated into s1?s candidate phrase.f9 The reversed (s1 ?
s2) version of f8.f10 The ratio of the number of parent dependency tree fragments that are shared by two candidate phrases to the number of all of the parent de-pendency tree fragments of the two phrases.
Dependency tree fragments are represented by the pronunciation of their component morphemes.f11 A variation of f10; tree fragments are represented by the base form of their component morphemes.f12 A variation of f10; tree fragments are represented by the POS of their component morphemes.f13 The ratio of the number of unigrams (morphemes) that appear in the child context of both candidate phrases to the number of all of the childcontext morphemes of both candidate phrases.
Unigrams are represented by the pronunciation of the morpheme.f14 A variation of f13; unigrams are represented by the base form of the morpheme.f15 A variation of f14; the numerator is the number of child context unigrams that are adjacent to both candidate phrases.f16 The ratio of the number of trigrams that appear in the child context of both candidate phrases to the number of all of the child contextmorphemes of both candidate phrases.
Trigrams are represented by the pronunciation of the morpheme.f17 Cosine similarity between two definition sentences from which a candidate phrase pair is extracted.Table 1: Features used by paraphrase classifier.The paraphrase checking of candidate phrasepairs is performed by an SVM classifier with a linearkernel that classifies each pair of candidate phrasesinto a paraphrase or a non-paraphrase.4 Candidatephrase pairs are ranked by their distance from theSVM?s hyperplane.
Features for the classifier arebased on our observation that two candidate phrasestend to be paraphrases if the candidate phrases them-selves are sufficiently similar and/or their surround-ing contexts are sufficiently similar.
Table 1 lists thefeatures used by the classifier.5 Basically, they rep-resent either the similarity of candidate phrases (f1-9) or that of their contexts (f10-17).
We think thatthey have various degrees of discriminative power,and thus we use the SVM to adjust their weights.Figure 1 illustrates features f8-12, for which youmay need supplemental remarks.
English is used forease of explanation.
In the figure, f8 has a positivevalue since the candidate phrase of s1 contains mor-phemes ?of bone?, which do not appear in the can-4We use SVMperf available at http://svmlight.joachims.org/svm perf.html.5In the table, the parent context of a candidate phrase con-sists of expressions that appear in ancestor nodes of the candi-date phrase in terms of the dependency structure of the sentence.Child contexts are defined similarly.Figure 1: Illustration of features f8-12.didate phrase of s2 but do appear in the other partof s2, i.e.
they are extra morphemes for s1?s candi-date phrase.
On the other hand, f9 is zero since thereis no such extra morpheme in s2?s candidate phrase.Also, features f10-12 have positive values since thetwo candidate phrases share two parent dependencytree fragments, (that increases) and (of fracture).We have also tried the following features, whichwe do not detail due to space limitation: the sim-ilarity of candidate phrases based on semanticallysimilar nouns (Kazama and Torisawa, 2008), entail-ing/entailed verbs (Hashimoto et al, 2009), and theidentity of the pronunciation and base form of thehead morpheme; N -grams (N=1,2,3) of child andparent contexts represented by either the inflectedform, base form, pronunciation, or POS of mor-1090Original definition sentence pair (s1, s2) Paraphrased definition sentence pair (s?1, s?2)s1: Osteoporosis is a disease that reduces bonemass and makes bonesfragile.s?1: Osteoporosis is a disease that decreases the quantity of bone andmakes bones fragile.s2: Osteoporosis is a disease that decreases the quantity of bone andincreases the risk of bone fracture.s?2: Osteoporosis is a disease that reduces bone mass and increasesthe risk of bone fracture.Figure 2: Bidirectional checking of entailment relation (?)
of p1 ?
p2 and p2 ?
p1.
p1 is ?reduces bone mass?in s1 and p2 is ?decreases the quantity of bone?
in s2.
p1 and p2 are exchanged between s1 and s2 to generatecorresponding paraphrased sentences s?1 and s?2.
p1 ?
p2 (p2 ?
p1) is verified if s1 ?
s?1 (s2 ?
s?2) holds.
In thiscase, both of them hold.
English is used for ease of explanation.pheme; parent/child dependency tree fragments rep-resented by either the inflected form, base form, pro-nunciation, or POS; adjacent versions (cf.
f15) ofN -gram features and parent/child dependency treefeatures.
These amount to 78 features, but we even-tually settled on the 17 features in Table 1 throughablation tests to evaluate the discriminative powerof each feature.The ablation tests were conducted using trainingdata that we prepared.
In preparing the training data,we faced the problem that the completely randomsampling of candidate paraphrase pairs provided uswith only a small number of positive examples.Thus, we automatically collected candidate para-phrase pairs that were expected to have a high like-lihood of being positive as examples to be labeled.The likelihood was calculated by simply summingall of the 78 feature values that we have tried, sincethey indicate the likelihood of a given candidateparaphrase pair?s being a paraphrase.
Note that val-ues of the features f8 and f9 are weighted with ?1,since they indicate the unlikelihood.
Specifically,we first randomly sampled 30,000 definition sen-tence pairs from the 29,661,812 pairs, and collected3,000 candidate phrase pairs that had the highestlikelihood from them.
The manual labeling of eachcandidate phrase pair (p1, p2) was based on bidirec-tional checking of entailment relation, p1 ?
p2 andp2 ?
p1, with p1 and p2 embedded in contexts.This scheme is similar to the one proposed bySzpektor et al (2007).
We adopt this scheme sinceparaphrase judgment might be unstable between an-notators unless they are given a particular contextbased on which they make a judgment.
As de-scribed below, we use definition sentences as con-texts.
We admit that annotators might be biased bythis in some unexpected way, but we believe thatthis is a more stable method than that without con-texts.
The labeling process is as follows.
First, fromeach candidate phrase pair (p1, p2) and its sourcedefinition sentence pair (s1, s2), we create two para-phrase sentence pairs (s?1, s?2) by exchanging p1 andp2 between s1 and s2.
Then, annotators check if s1entails s?1 and s2 entails s?2 so that entailment rela-tions of both directions p1 ?
p2 and p2 ?
p1 arechecked.
Figure 2 shows an example of bidirectionalchecking.
In this example, both entailment relations,s1 ?
s?1 and s2 ?
s?2, hold, and thus the candidatephrase pair (p1, p2) is judged as positive.
We used(p1, p2), for which entailment relations of both di-rections held, as positive examples (1,092 pairs) andthe others as negative ones (1,872 pairs).6We built the paraphrase classifier from the train-ing data.
As mentioned, candidate phrase pairs wereranked by the distance from the SVM?s hyperplane.4 ExperimentIn this paper, our claims are twofold.I.
Definition sentences on the Web are a treasuretrove of paraphrase knowledge (Section 4.2).II.
Our method of paraphrase acquisition fromdefinition sentences is more accurate than well-known competing methods (Section 4.1).We first verify claim II by comparing our methodwith that of Barzilay and McKeown (2001) (BMmethod), Moses7 (Koehn et al, 2007) (SMTmethod), and that of Murata et al (2004) (Mrtmethod).
The first two methods are well known foraccurately extracting semantically equivalent phrasepairs from parallel corpora.8 Then, we verify claim6The remaining 36 pairs were discarded as they containedgarbled characters of Japanese.7http://www.statmt.org/moses/8As anonymous reviewers pointed out, they are unsuper-vised methods and thus unable to be adapted to definition sen-1091I by comparing definition sentence pairs with sen-tence pairs that are acquired from the Web using Ya-hoo!JAPANAPI9 as a paraphrase knowledge source.In the latter data set, two sentences of each pairare expected to be semantically similar regardless ofwhether they are definition sentences.
Both sets con-tain 100,000 pairs.Three annotators (not the authors) checked evalu-ation samples.
Fleiss?
kappa (Fleiss, 1971) was 0.69(substantial agreement (Landis and Koch, 1977)).4.1 Our method vs. competing methodsIn this experiment, paraphrase pairs are extractedfrom 100,000 definition sentence pairs that are ran-domly sampled from the 29,661,812 pairs.
Beforereporting the experimental results, we briefly de-scribe the BM, SMT, and Mrt methods.BM method Given parallel sentences like multi-ple translations of the same source text, the BMmethod works iteratively as follows.
First, it collectsfrom the parallel sentences identical word pairs andtheir contexts (POS N -grams with indices indicat-ing corresponding words between paired contexts)as positive examples and those of different wordpairs as negative ones.
Then, each context is rankedbased on the frequency with which it appears in pos-itive (negative) examples.
The most likely K posi-tive (negative) contexts are used to extract positive(negative) paraphrases from the parallel sentences.Extracted positive (negative) paraphrases and theirmorpho-syntactic patterns are used to collect addi-tional positive (negative) contexts.
All the positive(negative) contexts are ranked, and additional para-phrases and their morpho-syntactic patterns are ex-tracted again.
This iterative process finishes if nofurther paraphrase is extracted or the number of iter-ations reaches a predefined threshold T .
In this ex-periment, following Barzilay and McKeown (2001),K is 10 and N is 1 to 3.
The value of T is not givenin their paper.
We chose 3 as its value based on ourpreliminary experiments.
Note that paraphrases ex-tracted by this method are not ranked.tences.
Nevertheless, we believe that comparing these methodswith ours is very informative, since they are known to be accu-rate and have been influential.9http://developer.yahoo.co.jp/webapi/SMT method Our SMT method uses Moses(Koehn et al, 2007) and extracts a phrase table, aset of two phrases that are translations of each other,given a set of two sentences that are translations ofeach other.
If you give Moses monolingual parallelsentence pairs, it should extract a set of two phrasesthat are paraphrases of each other.
In this experi-ment, default values were used for all parameters.To rank extracted phrase pairs, we assigned each ofthem the product of two phrase translation probabil-ities of both directions that were given by Moses.For other SMT methods, see Quirk et al (2004) andBannard and Callison-Burch (2005) among others.Mrt method Murata et al (2004) proposed amethod to extract paraphrases from two manuallycompiled dictionaries.
It simply regards a differencebetween two definition sentences of the same wordas a paraphrase candidate.
Paraphrase candidates areranked according to an unsupervised scoring schemethat implements their assumption.
They assume thata paraphrase candidate tends to be a valid paraphraseif it is surrounded by infrequent strings and/or if itappears multiple times in the data.In this experiment, we evaluated the unsupervisedversion of our method in addition to the supervisedone described in Section 3.2, in order to compareit fairly with the other methods.
The unsupervisedmethod works in the same way as the supervisedone, except that it ranks candidate phrase pairs bythe sum of all 17 feature values, instead of the dis-tance from the SVM?s hyperplane.
In other words,no supervised learning is used.
All the feature val-ues are weighted with 1, except for f8 and f9, whichare weighted with ?1 since they indicate the unlike-lihood of a candidate phrase pair being paraphrases.BM, SMT, Mrt, and the two versions of our methodwere used to extract paraphrase pairs from the same100,000 definition sentence pairs.Evaluation scheme Evaluation of each para-phrase pair (p1, p2) was based on bidirectionalchecking of entailment relations p1 ?
p2 and p2 ?p1 in a way similar to the labeling of the trainingdata.
The difference is that contexts for evaluationare two sentences that are retrieved from the Weband contain p1 and p2, instead of definition sen-tences from which p1 and p2 are extracted.
This1092is intended to check whether extracted paraphrasesare also valid for contexts other than those fromwhich they are extracted.
The evaluation proceedsas follows.
For the top m paraphrase pairs of eachmethod (in the case of the BM method, randomlysampled m pairs were used, since the method doesnot rank paraphrase pairs), we retrieved a sentencepair (s1, s2) for each paraphrase pair (p1, p2) fromthe Web, such that s1 contains p1 and s2 contains p2.In doing so, we make sure that neither s1 nor s2 arethe definition sentences from which p1 and p2 areextracted.
For each method, we randomly samplen samples from all of the paraphrase pairs (p1, p2)for which both s1 and s2 are retrieved.
Then, fromeach (p1, p2) and (s1, s2), we create two paraphrasesentence pairs (s?1, s?2) by exchanging p1 and p2 be-tween s1 and s2.
All samples, each consisting of(p1, p2), (s1, s2), and (s?1, s?2), are checked by threehuman annotators to determine whether s1 entailss?1 and s2 entails s?2 so that entailment relations ofboth directions are verified.
In advance of evaluationannotation, all the evaluation samples are shuffledso that the annotators cannot find out which sampleis given by which method for fairness.
We regardeach paraphrase pair as correct if at least two annota-tors judge that entailment relations of both directionshold for it.
You may wonder whether only one pairof sentences (s1, s2) is enough for evaluation since acorrect (wrong) paraphrase pair might be judged aswrong (correct) accidentally.
Nevertheless, we sup-pose that the final evaluation results are reliable ifthe number of evaluation samples is sufficient.
Inthis experiment, m is 5,000 and n is 200.
We useYahoo!JAPAN API to retrieve sentences.Graph (a) in Figure 3 shows a precision curvefor each method.
Sup and Uns respectively indi-cate the supervised and unsupervised versions of ourmethod.
The figure indicates that Sup outperformsall the others and shows a high precision rate ofabout 94% at the top 1,000.
Remember that thisis the result of using 100,000 definition sentencepairs.
Thus, we estimate that Sup can extract about300,000 paraphrase pairs with a precision rate ofabout 94%, if we use all 29,661,812 definition sen-tence pairs that we acquired.Furthermore, we measured precision after trivialparaphrase pairs were discarded from the evaluationsamples of each method.
A candidate phrase pairDefinition sentence pairs Sup Uns BM SMT Mrtwith trivial 1,381,424 24,049 9,562 18,184without trivial 1,377,573 23,490 7,256 18,139Web sentence pairs Sup Uns BM SMT Mrtwith trivial 277,172 5,101 4,586 4,978without trivial 274,720 4,399 2,342 4,958Table 2: Number of extracted paraphrases.
(p1, p2) is regarded as trivial if the pronunciation isthe same between p1 and p2,10 or all of the con-tent words contained in p1 are the same as thoseof p2.
Graph (b) gives a precision curve for eachmethod.
Again, Sup outperforms the others too, andmaintains a precision rate of about 90% until the top1,000.
These results support our claim II.The upper half of Table 2 shows the number ofextracted paraphrases with/without trivial pairs foreach method.11 Sup and Uns extracted many moreparaphrases.
It is noteworthy that Sup performed thebest in terms of both precision rate and the numberof extracted paraphrases.Table 3 shows examples of correct and incorrectoutputs of Sup.
As the examples indicate, many ofthe extracted paraphrases are not specific to defini-tion sentences and seem very reusable.
However,there are few paraphrases involving metaphors or id-ioms in the outputs due to the nature of definitionsentences.
In this regard, we do not claim that ourmethod is almighty.
We agree with Sekine (2005)who claims that several different methods are re-quired to discover a wider variety of paraphrases.In graphs (a) and (b), the precision of the SMTmethod goes up as rank goes down.
This strange be-havior is due to the scoring by Moses that workedpoorly for the data; it gave 1.0 to 82.5% of all thesamples, 38.8% of which were incorrect.
We suspectSMTmethods are poor at monolingual alignment forparaphrasing or entailment tasks since, in the tasks,data is much noisier than that used for SMT.
SeeMacCartney et al (2008) for similar discussion.4.2 Definition pairs vs.
Web sentence pairsTo collect Web sentence pairs, first, we randomlysampled 1.8 million sentences from the Web corpus.10There are many kinds of orthographic variants in Japanese,which can be identified by their pronunciation.11We set no threshold for candidate phrase pairs of eachmethod, and counted all the candidate phrase pairs in Table 2.109300.20.40.60.810  1000  2000  3000  4000  5000PrecisionTop-N?Sup_def??Uns_def??SMT_def??BM_def?
?Mrt_def?00.20.40.60.810  1000  2000  3000  4000  5000PrecisionTop-N?Sup_def_n??Uns_def_n??SMT_def_n??BM_def_n??Mrt_def_n?
(a) Definition sentence pairs with trivial paraphrases (b) Definition sentence pairs without trivial paraphrases00.20.40.60.810  1000  2000  3000  4000  5000PrecisionTop-N?Sup_www??Uns_www??SMT_www??BM_www?
?Mrt_www?00.20.40.60.810  1000  2000  3000  4000  5000PrecisionTop-N?Sup_www_n??Uns_www_n??SMT_www_n??BM_www_n??Mrt_www_n?
(c) Web sentence pairs with trivial paraphrases (d) Web sentence pairs without trivial paraphrasesFigure 3: Precision curves of paraphrase extraction.Rank Paraphrase pairCorrect13 ??????????????
(send a message to the e-mail address)?????????????????
(sendan e-mail message to the e-mail address)19 ?????????
(requested by a customer)??????????
(commissioned by a customer)70 ??????????
(describe the fiscal condition of company) ???????????
(indicate the fiscal stateof company)112 ????????????
(get information)????????
(get news)656 ????????
(it is a convention)?????????
(it is a rule)841 ???????????????
(represent the energy scale of earthquake)?????????
(represent the scaleof earthquake)929 ????????
(cause the oxidation of cells)?????????
(cause cellular aging)1,553 ???????
(remove dead skin cells)???????
(peel off dead skin cells)2,243 ?????????
(required for the development of fetus)???????????????
(indispensable for thegrowth and development of fetus)2,855 ???????
(correct eyesight)????????
(perform eyesight correction)2,931 ?????????
(call it even)??????????
(call it quits)3,667 ??????????????
(accumulated on a hard disk)??????????????????
(stored on ahard disk drive)4,870 ?????????
(excrete harmful substance)??????????
(discharge harmful toxin)5,501 ?????????????????????????
(mount two processor cores on one CPU)?????????????????????????
(build two processor cores into one package)10,675 ???????
(trade foreign currencies)????????
(exchange one currency for another)112,819 ???????????
(become a regular staff member of the company where (s)he has worked as a temp) ????????????
(employed by the company where (s)he has worked as a temp)193,553 ?????????????
(access Web sites)???????????
(visit WWW sites)Incorrect903 ??????????
(send to a Web browser)???????????
(send to a PC)2,530 ??????
(intend to balance)??????????
(intend to refresh)3,008 ????????????
(unable to digest with digestive enzymes)????????????
(hard to digest withdigestive enzymes)Table 3: Examples of correct and incorrect paraphrases extracted by our supervised method with their rank.1094We call them sampled sentences.
Then, using Ya-hoo!JAPANAPI, we retrieved up to 20 snippets rele-vant to each sampled sentence using all of the nounsin each sentence as a query.
After that, each snippetwas split into sentences, which we call snippet sen-tences.
We paired a sampled sentence and a snippetsentence that was the most similar to the sampledsentence.
Similarity is the number of nouns sharedby the two sentences.
Finally, we randomly sampled100,000 pairs from all the pairs.Paraphrase pairs were extracted from the Websentence pairs by using BM, SMT, Mrt and the su-pervised and unsupervised versions of our method.The features used with our methods were selectedfrom all of the 78 features mentioned in Section 3.2so that they performed well for Web sentence pairs.Specifically, the features were selected by ablationtests using training data that was tailored to Websentence pairs.
The training data consisted of 2,741sentence pairs that were collected in the same way asthe Web sentence pairs and was labeled in the sameway as described in Section 3.2.Graph (c) of Figure 3 shows precision curves.
Wealso measured precision without trivial pairs in thesame way as the previous experiment.
Graph (d)shows the results.
The lower half of Table 2 showsthe number of extracted paraphrases with/withouttrivial pairs for each method.Note that precision figures of our methods ingraphs (c) and (d) are lower than those of our meth-ods in graphs (a) and (b).
Additionally, none of themethods achieved a precision rate of 90% using Websentence pairs.12 We think that a precision rate ofat least 90% would be necessary if you apply auto-matically extracted paraphrases to NLP tasks with-out manual annotation.
Only the combination of Supand definition sentence pairs achieved that precision.Also note that, for all of the methods, the numbersof extracted paraphrases from Web sentence pairsare fewer than those from definition sentence pairs.From all of these results, we conclude that ourclaim I is verified.12Precision of SMT is unexpectedly good.
We found someWeb sentence pairs consisting of two mostly identical sentenceson rare occasions.
The method worked relatively well for them.5 ConclusionWe proposed a method of extracting paraphrasesfrom definition sentences on the Web.
From the ex-perimental results, we conclude that the followingtwo claims of this paper are verified.1.
Definition sentences on the Web are a treasuretrove of paraphrase knowledge.2.
Our method extracts many paraphrases fromthe definition sentences on the Web accurately;it can extract about 300,000 paraphrases from6 ?
108 Web documents with a precision rateof about 94%.Our future work is threefold.
First, we will releaseextracted paraphrases from all of the 29,661,812definition sentence pairs that we acquired, after hu-man annotators check their validity.
The result willbe available through the ALAGIN forum.13Second, we plan to induce paraphrase rulesfrom paraphrase instances.
Though our methodcan extract a variety of paraphrase instances ona large scale, their coverage might be insufficientfor real NLP applications since some paraphrasephenomena are highly productive.
Therefore, weneed paraphrase rules in addition to paraphrase in-stances.
Barzilay and McKeown (2001) inducedsimple POS-based paraphrase rules from paraphraseinstances, which can be a good starting point.Finally, as mentioned in Section 1, the work inthis paper is only the beginning of our research onparaphrase extraction.
We are trying to extract farmore paraphrases from a set of sentences fulfillingthe same pragmatic function (e.g.
definition) for thesame topic (e.g.
osteoporosis) on the Web.
Suchfunctions other than definition may include the us-age of the same Linux command, the recipe for thesame cuisine, or the description of related work onthe same research issue.AcknowledgmentsWe would like to thank Atsushi Fujita, FrancisBond, and all of the members of the InformationAnalysis Laboratory, Universal Communication Re-search Institute at NICT.13http://alagin.jp/1095ReferencesSusumu Akamine, Daisuke Kawahara, Yoshikiyo Kato,Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, TakuyaKawada, Kentaro Inui, Sadao Kurohashi, and YutakaKidawara.
2010.
Organizing information on the webto support user judgments on information credibil-ity.
In Proceedings of 2010 4th International Uni-versal Communication Symposium Proceedings (IUCS2010), pages 122?129.Ion Androutsopoulos and Prodromos Malakasiotis.2010.
A survey of paraphrasing and textual entailmentmethods.
Journal of Artificial Intelligence Research,38:135?187.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of the 43rd Annual Meeting of the Association forComputational Linguistics (ACL-2005), pages 597?604.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proceedings of HLT-NAACL2003, pages 16?23.Regina Barzilay and Kathleen R. McKeown.
2001.
Ex-tracting paraphrases from a parallel corpus.
In Pro-ceedings of the 39th Annual Meeting of the ACL jointwith the 10th Meeting of the European Chapter of theACL (ACL/EACL 2001), pages 50?57.Rahul Bhagat, Patrick Pantel, and Eduard Hovy.
2007.Ledir: An unsupervised algorithm for learning direc-tionality of inference rules.
In Proceedings of Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP2007), pages 161?170.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine translationusing paraphrases.
In Proceedings of the 2006 HumanLanguage Technology Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics (HLT-NAACL 2006), pages 17?24.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.
Un-supervised construction of large paraphrase corpora:exploiting massively parallel news sources.
In Pro-ceedings of the 20th international conference on Com-putational Linguistics (COLING 2004), pages 350?356.Joseph L. Fleiss.
1971.
Measuring nominal scale agree-ment among many raters.
Psychological Bulletin,76(5):378?382.Atsushi Fujii and Tetsuya Ishikawa.
2002.
Extractionand organization of encyclopedic knowledge informa-tion using the World Wide Web (written in Japanese).Institute of Electronics, Information, and Communica-tion Engineers, J85-D-II(2):300?307.Maayan Geffet and Ido Dagan.
2005.
The distributionalinclusion hypotheses and lexical entailment.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL 2005), pages107?114.Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,Stijn De Saeger, Masaki Murata, and Jun?ichi Kazama.2009.
Large-scale verb entailment acquisition fromthe web.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing(EMNLP 2009), pages 1172?1181.Lidija Iordanskaja, Richard Kittredge, and AlainPolgue`re.
1991.
Lexical selection and paraphrase ina meaning-text generation model.
In Ce?cile L. Paris,William R. Swartout, and William C. Mann, editors,Natural language generation in artificial intelligenceand computational linguistics, pages 293?312.
KluwerAcademic Press.David Kauchak and Regina Barzilay.
2006.
Para-phrasing for automatic evaluation.
In Proceedings ofthe 2006 Human Language Technology Conference ofthe North American Chapter of the Association forComputational Linguistics (HLT-NAACL 2006), pages455?462.Jun?ichi Kazama and Kentaro Torisawa.
2007.
Exploit-ing Wikipedia as external knowledge for named entityrecognition.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL 2007), pages 698?707, June.Jun?ichi Kazama and Kentaro Torisawa.
2008.
Inducinggazetteers for named entity recognition by large-scaleclustering of dependency relations.
In Proceedings ofthe 46th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies (ACL-08: HLT), pages 407?415.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Pro-ceedings of the 45th Annual Meeting of the Associa-tion for Computational Linguistics (ACL 2007), pages177?180.J.
Richard Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics, 33(1):159?174.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules for question answering.
Natural LanguageEngineering, 7(4):343?360.Bill MacCartney, Michel Galley, and Christopher D.Manning.
2008.
A phrase-based alignment model fornatural language inference.
In Proceedings of the 20081096Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP-2008), pages 802?811.Nitin Madnani and Bonnie Dorr.
2010.
Generatingphrasal and sentential paraphrases: A survey of data-driven methods.
Computational Linguistics, 36(3).Kathleen R. McKeown, Regina Barzilay, David Evans,Vasileios Hatzivassiloglou, Judith L. Klavans, AniNenkova, Carl Sable, Barry Schiffman, and SergeySigelman.
2002.
Tracking and summarizing newson a daily basis with columbia?s newsblaster.
In Pro-ceedings of the 2nd international conference on Hu-man Language Technology Research, pages 280?285.Masaki Murata, Toshiyuki Kanemaru, and Hitoshi Isa-hara.
2004.
Automatic paraphrase acquisition basedon matching of definition sentences in plural dictionar-ies (written in Japanese).
Journal of Natural LanguageProcessing, 11(5):135?149.Roberto Navigli and Paola Velardi.
2010.
Learningword-class lattices for definition and hypernym extrac-tion.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics (ACL2010), pages 1318?1327.Chris Quirk, Chris Brockett, and William Dolan.
2004.Monolingual machine translation for paraphrase gen-eration.
In Proceedings of the 2004 Conference onEmpirical Methods in Natural Language Processing(EMNLP-2004), pages 142?149.Deepak Ravichandran and Eduard H. Hovy.
2002.Learning surface text patterns for a question answer-ing system.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics(ACL 2002), pages 41?47.Satoshi Sekine.
2005.
Automatic paraphrase discoverybased on context and keywords between ne pairs.
InProceedings of the Third International Workshop onParaphrasing (IWP-2005), pages 80?87.Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.2002.
Automatic paraphrase acquisition from news ar-ticles.
In Proceedings of the 2nd international Con-ference on Human Language Technology Research(HLT2002), pages 313?318.Idan Szpektor and Ido Dagan.
2008.
Learning entail-ment rules for unary template.
In Proceedings of the22nd International Conference on Computational Lin-guistics (COLING2008), pages 849?856.Idan Szpektor, Eyal Shnarch, and Ido Dagan.
2007.Instance-based evaluation of entailment rule acquisi-tion.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics (ACL 2007),pages 456?463.1097
