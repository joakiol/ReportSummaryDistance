Proceedings of the 43rd Annual Meeting of the ACL, pages 403?410,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsDomain Kernels for Word Sense DisambiguationAlfio Gliozzo and Claudio Giuliano and Carlo StrapparavaITC-irst, Istituto per la Ricerca Scientica e TecnologicaI-38050, Trento, ITALY{gliozzo,giuliano,strappa}@itc.itAbstractIn this paper we present a supervisedWord Sense Disambiguation methodol-ogy, that exploits kernel methods to modelsense distinctions.
In particular a combi-nation of kernel functions is adopted toestimate independently both syntagmaticand domain similarity.
We defined a ker-nel function, namely the Domain Kernel,that allowed us to plug ?external knowl-edge?
into the supervised learning pro-cess.
External knowledge is acquired fromunlabeled data in a totally unsupervisedway, and it is represented by means of Do-main Models.
We evaluated our method-ology on several lexical sample tasks indifferent languages, outperforming sig-nificantly the state-of-the-art for each ofthem, while reducing the amount of la-beled training data required for learning.1 IntroductionThe main limitation of many supervised approachesfor Natural Language Processing (NLP) is the lackof available annotated training data.
This problem isknown as the Knowledge Acquisition Bottleneck.To reach high accuracy, state-of-the-art systemsfor Word Sense Disambiguation (WSD) are de-signed according to a supervised learning frame-work, in which the disambiguation of each wordin the lexicon is performed by constructing a dif-ferent classifier.
A large set of sense tagged exam-ples is then required to train each classifier.
Thismethodology is called word expert approach (Small,1980; Yarowsky and Florian, 2002).
However thisis clearly unfeasible for all-words WSD tasks, inwhich all the words of an open text should be dis-ambiguated.On the other hand, the word expert approachworks very well for lexical sample WSD tasks (i.e.tasks in which it is required to disambiguate onlythose words for which enough training data is pro-vided).
As the original rationale of the lexical sam-ple tasks was to define a clear experimental settingsto enhance the comprehension of WSD, they shouldbe considered as preceding exercises to all-wordstasks.
However this is not the actual case.
Algo-rithms designed for lexical sample WSD are oftenbased on pure supervision and hence ?data hungry?.We think that lexical sample WSD should regainits original explorative role and possibly use a min-imal amount of training data, exploiting instead ex-ternal knowledge acquired in an unsupervised wayto reach the actual state-of-the-art performance.By the way, minimal supervision is the basisof state-of-the-art systems for all-words tasks (e.g.
(Mihalcea and Faruque, 2004; Decadt et al, 2004)),that are trained on small sense tagged corpora (e.g.SemCor), in which few examples for a subset of theambiguous words in the lexicon can be found.
Thusimproving the performance of WSD systems withfew learning examples is a fundamental step towardsthe direction of designing a WSD system that workswell on real texts.In addition, it is a common opinion that the per-formance of state-of-the-art WSD systems is not sat-isfactory from an applicative point of view yet.403To achieve these goals we identified two promis-ing research directions:1.
Modeling independently domain and syntag-matic aspects of sense distinction, to improvethe feature representation of sense tagged ex-amples (Gliozzo et al, 2004).2.
Leveraging external knowledge acquired fromunlabeled corpora.The first direction is motivated by the linguisticassumption that syntagmatic and domain (associa-tive) relations are both crucial to represent sensedistictions, while they are basically originated byvery different phenomena.
Syntagmatic relationshold among words that are typically located closeto each other in the same sentence in a given tempo-ral order, while domain relations hold among wordsthat are typically used in the same semantic domain(i.e.
in texts having similar topics (Gliozzo et al,2004)).
Their different nature suggests to adopt dif-ferent learning strategies to detect them.Regarding the second direction, external knowl-edge would be required to help WSD algorithms tobetter generalize over the data available for train-ing.
On the other hand, most of the state-of-the-artsupervised approaches to WSD are still completelybased on ?internal?
information only (i.e.
the onlyinformation available to the training algorithm is theset of manually annotated examples).
For exam-ple, in the Senseval-3 evaluation exercise (Mihal-cea and Edmonds, 2004) many lexical sample taskswere provided, beyond the usual labeled trainingdata, with a large set of unlabeled data.
However,at our knowledge, none of the participants exploitedthis unlabeled material.
Exploring this direction isthe main focus of this paper.
In particular we ac-quire a Domain Model (DM) for the lexicon (i.e.a lexical resource representing domain associationsamong terms), and we exploit this information in-side our supervised WSD algorithm.
DMs can beautomatically induced from unlabeled corpora, al-lowing the portability of the methodology amonglanguages.We identified kernel methods as a viable frame-work in which to implement the assumptions above(Strapparava et al, 2004).Exploiting the properties of kernels, we have de-fined independently a set of domain and syntagmatickernels and we combined them in order to define acomplete kernel for WSD.
The domain kernels esti-mate the (domain) similarity (Magnini et al, 2002)among contexts, while the syntagmatic kernels eval-uate the similarity among collocations.We will demonstrate that using DMs inducedfrom unlabeled corpora is a feasible strategy to in-crease the generalization capability of the WSD al-gorithm.
Our system far outperforms the state-of-the-art systems in all the tasks in which it has beentested.
Moreover, a comparative analysis of thelearning curves shows that the use of DMs allowsus to remarkably reduce the amount of sense-taggedexamples, opening new scenarios to develop sys-tems for all-words tasks with minimal supervision.The paper is structured as follows.
Section 2 in-troduces the notion of Domain Model.
In particularan automatic acquisition technique based on LatentSemantic Analysis (LSA) is described.
In Section 3we present a WSD system based on a combinationof kernels.
In particular we define a Domain Ker-nel (see Section 3.1) and a Syntagmatic Kernel (seeSection 3.2), to model separately syntagmatic anddomain aspects.
In Section 4 our WSD system isevaluated in the Senseval-3 English, Italian, Spanishand Catalan lexical sample tasks.2 Domain ModelsThe simplest methodology to estimate the similar-ity among the topics of two texts is to representthem by means of vectors in the Vector Space Model(VSM), and to exploit the cosine similarity.
Moreformally, let C = {t1, t2, .
.
.
, tn} be a corpus, letV = {w1, w2, .
.
.
, wk} be its vocabulary, let T bethe k ?
n term-by-document matrix representing C ,such that ti,j is the frequency of word wi into the texttj .
The VSM is a k-dimensional space Rk, in whichthe text tj ?
C is represented by means of the vec-tor ~tj such that the ith component of ~tj is ti,j.
Thesimilarity among two texts in the VSM is estimatedby computing the cosine among them.However this approach does not deal well withlexical variability and ambiguity.
For example thetwo sentences ?he is affected by AIDS?
and ?HIV isa virus?
do not have any words in common.
In the404VSM their similarity is zero because they have or-thogonal vectors, even if the concepts they expressare very closely related.
On the other hand, the sim-ilarity between the two sentences ?the laptop hasbeen infected by a virus?
and ?HIV is a virus?
wouldturn out very high, due to the ambiguity of the wordvirus.To overcome this problem we introduce the notionof Domain Model (DM), and we show how to use itin order to define a domain VSM in which texts andterms are represented in a uniform way.A DM is composed by soft clusters of terms.
Eachcluster represents a semantic domain, i.e.
a set ofterms that often co-occur in texts having similar top-ics.
A DM is represented by a k?k?
rectangular ma-trix D, containing the degree of association amongterms and domains, as illustrated in Table 1.MEDICINE COMPUTER SCIENCEHIV 1 0AIDS 1 0virus 0.5 0.5laptop 0 1Table 1: Example of Domain MatrixDMs can be used to describe lexical ambiguityand variability.
Lexical ambiguity is representedby associating one term to more than one domain,while variability is represented by associating dif-ferent terms to the same domain.
For example theterm virus is associated to both the domain COM-PUTER SCIENCE and the domain MEDICINE (ambi-guity) while the domain MEDICINE is associated toboth the terms AIDS and HIV (variability).More formally, let D = {D1, D2, ..., Dk?}
be aset of domains, such that k?
k. A DM is fullydefined by a k?k?
domain matrix D representing ineach cell di,z the domain relevance of term wi withrespect to the domain Dz .
The domain matrix D isused to define a function D : Rk ?
Rk?
, that mapsthe vectors ~tj expressed into the classical VSM, intothe vectors ~t?j in the domain VSM.
D is defined by1D(~tj) = ~tj(IIDFD) = ~t?j (1)1In (Wong et al, 1985) the formula 1 is used to define aGeneralized Vector Space Model, of which the Domain VSM isa particular instance.where IIDF is a k ?
k diagonal matrix such thatiIDFi,i = IDF (wi), ~tj is represented as a row vector,and IDF (wi) is the Inverse Document Frequency ofwi.Vectors in the domain VSM are called DomainVectors (DVs).
DVs for texts are estimated by ex-ploiting the formula 1, while the DV ~w?i, correspond-ing to the word wi ?
V is the ith row of the domainmatrix D. To be a valid domain matrix such vectorsshould be normalized (i,e.
?
~w?i, ~w?i?
= 1).In the Domain VSM the similarity among DVs isestimated by taking into account second order rela-tions among terms.
For example the similarity of thetwo sentences ?He is affected by AIDS?
and ?HIVis a virus?
is very high, because the terms AIDS,HIV and virus are highly associated to the domainMEDICINE.A DM can be estimated from hand made lexicalresources such as WORDNET DOMAINS (Magniniand Cavaglia`, 2000), or by performing a term clus-tering process on a large corpus.
We think that thesecond methodology is more attractive, because itallows us to automatically acquire DMs for differentlanguages.In this work we propose the use of Latent Seman-tic Analysis (LSA) to induce DMs from corpora.LSA is an unsupervised technique for estimating thesimilarity among texts and terms in a corpus.
LSAis performed by means of a Singular Value Decom-position (SVD) of the term-by-document matrix Tdescribing the corpus.
The SVD algorithm can beexploited to acquire a domain matrix D from a largecorpus C in a totally unsupervised way.
SVD de-composes the term-by-document matrix T into threematrixes T ' V?k?UT where ?k?
is the diagonalk ?
k matrix containing the highest k ?
k eigen-values of T, and all the remaining elements set to0.
The parameter k?
is the dimensionality of the Do-main VSM and can be fixed in advance2 .
Under thissetting we define the domain matrix DLSA asDLSA = INV??k?
(2)where IN is a diagonal matrix such that iNi,i =1q?
~w?i, ~w?i?, ~w?i is the ith row of the matrix V??k?
.32It is not clear how to choose the right dimensionality.
Inour experiments we used 50 dimensions.3When DLSA is substituted in Equation 1 the Domain VSM4053 Kernel Methods for WSDIn the introduction we discussed two promising di-rections for improving the performance of a super-vised disambiguation system.
In this section weshow how these requirements can be efficiently im-plemented in a natural and elegant way by using ker-nel methods.The basic idea behind kernel methods is to embedthe data into a suitable feature space F via a map-ping function ?
: X ?
F , and then use a linear al-gorithm for discovering nonlinear patterns.
Insteadof using the explicit mapping ?, we can use a kernelfunction K : X ?
X ?
R, that corresponds to theinner product in a feature space which is, in general,different from the input space.Kernel methods allow us to build a modular sys-tem, as the kernel function acts as an interface be-tween the data and the learning algorithm.
Thusthe kernel function becomes the only domain spe-cific module of the system, while the learning algo-rithm is a general purpose component.
Potentiallyany kernel function can work with any kernel-basedalgorithm.
In our system we use Support Vector Ma-chines (Cristianini and Shawe-Taylor, 2000).Exploiting the properties of the kernel func-tions, it is possible to define the kernel combinationschema asKC(xi, xj) =n?l=1Kl(xi, xj)?Kl(xj, xj)Kl(xi, xi)(3)Our WSD system is then defined as combinationof n basic kernels.
Each kernel adds some addi-tional dimensions to the feature space.
In particular,we have defined two families of kernels: Domainand Syntagmatic kernels.
The former is composedby both the Domain Kernel (KD) and the Bag-of-Words kernel (KBoW ), that captures domain aspects(see Section 3.1).
The latter captures the syntag-matic aspects of sense distinction and it is composedby two kernels: the collocation kernel (KColl) andis equivalent to a Latent Semantic Space (Deerwester et al,1990).
The only difference in our formulation is that the vectorsrepresenting the terms in the Domain VSM are normalized bythe matrix IN, and then rescaled, according to their IDF value,by matrix IIDF.
Note the analogy with the tf idf term weightingschema (Salton and McGill, 1983), widely adopted in Informa-tion Retrieval.the Part of Speech kernel (KPoS) (see Section 3.2).The WSD kernels (K ?WSD and KWSD) are then de-fined by combining them (see Section 3.3).3.1 Domain KernelsIn (Magnini et al, 2002), it has been claimed thatknowing the domain of the text in which the wordis located is a crucial information for WSD.
Forexample the (domain) polysemy among the COM-PUTER SCIENCE and the MEDICINE senses of theword virus can be solved by simply consideringthe domain of the context in which it is located.This assumption can be modeled by defining akernel that estimates the domain similarity amongthe contexts of the words to be disambiguated,namely the Domain Kernel.
The Domain Kernel es-timates the similarity among the topics (domains) oftwo texts, so to capture domain aspects of sense dis-tinction.
It is a variation of the Latent Semantic Ker-nel (Shawe-Taylor and Cristianini, 2004), in which aDM (see Section 2) is exploited to define an explicitmapping D : Rk ?
Rk?
from the classical VSM intothe Domain VSM.
The Domain Kernel is defined byKD(ti, tj) =?D(ti),D(tj)???D(ti),D(tj)??D(ti),D(tj)?
(4)where D is the Domain Mapping defined in equa-tion 1.
Thus the Domain Kernel requires a DomainMatrix D. For our experiments we acquire the ma-trix DLSA, described in equation 2, from a genericcollection of unlabeled documents, as explained inSection 2.A more traditional approach to detect topic (do-main) similarity is to extract Bag-of-Words (BoW)features from a large window of text around theword to be disambiguated.
The BoW kernel, de-noted by KBoW , is a particular case of the DomainKernel, in which D = I, and I is the identity ma-trix.
The BoW kernel does not require a DM, then itcan be applied to the ?strictly?
supervised settings,in which an external knowledge source is not pro-vided.3.2 Syntagmatic kernelsKernel functions are not restricted to operate on vec-torial objects ~x ?
Rk.
In principle kernels can bedefined for any kind of object representation, as for406example sequences and trees.
As stated in Section 1,syntagmatic relations hold among words collocatedin a particular temporal order, thus they can be mod-eled by analyzing sequences of words.We identified the string kernel (or word se-quence kernel) (Shawe-Taylor and Cristianini, 2004)as a valid instrument to model our assumptions.The string kernel counts how many times a (non-contiguous) subsequence of symbols u of lengthn occurs in the input string s, and penalizes non-contiguous occurrences according to the number ofgaps they contain (gap-weighted subsequence ker-nel).Formally, let V be the vocabulary, the featurespace associated with the gap-weighted subsequencekernel of length n is indexed by a set I of subse-quences over V of length n. The (explicit) mappingfunction is defined by?nu(s) =?i:u=s(i)?l(i), u ?
V n (5)where u = s(i) is a subsequence of s in the posi-tions given by the tuple i, l(i) is the length spannedby u, and ?
?
]0, 1] is the decay factor used to penal-ize non-contiguous subsequences.The associate gap-weighted subsequence kernel isdefined bykn(si, sj) = ?
?n(si), ?n(sj)?
=Xu?V n?n(si)?n(sj) (6)We modified the generic definition of the stringkernel in order to make it able to recognize collo-cations in a local window of the word to be disam-biguated.
In particular we defined two Syntagmatickernels: the n-gram Collocation Kernel and the n-gram PoS Kernel.
The n-gram Collocation ker-nel KnColl is defined as a gap-weighted subsequencekernel applied to sequences of lemmata around theword l0 to be disambiguated (i.e.
l?3, l?2, l?1, l0,l+1, l+2, l+3).
This formulation allows us to esti-mate the number of common (sparse) subsequencesof lemmata (i.e.
collocations) between two exam-ples, in order to capture syntagmatic similarity.
Inanalogy we defined the PoS kernel KnPoS , by settings to the sequence of PoSs p?3, p?2, p?1, p0, p+1,p+2, p+3, where p0 is the PoS of the word to be dis-ambiguated.The definition of the gap-weighted subsequencekernel, provided by equation 6, depends on the pa-rameter n, that represents the length of the sub-sequences analyzed when estimating the similarityamong sequences.
For example, K2Coll allows us torepresent the bigrams around the word to be disam-biguated in a more flexible way (i.e.
bigrams can besparse).
In WSD, typical features are bigrams andtrigrams of lemmata and PoSs around the word tobe disambiguated, then we defined the CollocationKernel and the PoS Kernel respectively by equations7 and 84.KColl(si, sj) =p?l=1K lColl(si, sj) (7)KPoS(si, sj) =p?l=1K lPoS(si, sj) (8)3.3 WSD kernelsIn order to show the impact of using Domain Modelsin the supervised learning process, we defined twoWSD kernels, by applying the kernel combinationschema described by equation 3.
Thus the followingWSD kernels are fully specified by the list of thekernels that compose them.Kwsd composed by KColl, KPoS and KBoWK?wsd composed by KColl, KPoS , KBoW and KDThe only difference between the two systems isthat K ?wsd uses Domain Kernel KD.
K ?wsd exploitsexternal knowledge, in contrast to Kwsd, whose onlyavailable information is the labeled training data.4 Evaluation and DiscussionIn this section we present the performance of ourkernel-based algorithms for WSD.
The objectives ofthese experiments are:?
to study the combination of different kernels,?
to understand the benefits of plugging externalinformation using domain models,?
to verify the portability of our methodologyamong different languages.4The parameters p and ?
are optimized by cross-validation.The best results are obtained setting p = 2, ?
= 0.5 for KColland ?
?
0 for KPoS .4074.1 WSD tasksWe conducted the experiments on four lexical sam-ple tasks (English, Catalan, Italian and Spanish)of the Senseval-3 competition (Mihalcea and Ed-monds, 2004).
Table 2 describes the tasks by re-porting the number of words to be disambiguated,the mean polysemy, and the dimension of training,test and unlabeled corpora.
Note that the organiz-ers of the English task did not provide any unlabeledmaterial.
So for English we used a domain modelbuilt from a portion of BNC corpus, while for Span-ish, Italian and Catalan we acquired DMs from theunlabeled corpora made available by the organizers.#w pol # train # test # unlabCatalan 27 3.11 4469 2253 23935English 57 6.47 7860 3944 -Italian 45 6.30 5145 2439 74788Spanish 46 3.30 8430 4195 61252Table 2: Dataset descriptions4.2 Kernel CombinationIn this section we present an experiment to em-pirically study the kernel combination.
The basickernels (i.e.
KBoW , KD , KColl and KPoS) havebeen compared to the combined ones (i.e.
Kwsd andK ?wsd) on the English lexical sample task.The results are reported in Table 3.
The resultsshow that combining kernels significantly improvesthe performance of the system.KD KBoW KPoS KColl Kwsd K?wsdF1 65.5 63.7 62.9 66.7 69.7 73.3Table 3: The performance (F1) of each basic ker-nel and their combination for English lexical sampletask.4.3 Portability and PerformanceWe evaluated the performance of K ?wsd and Kwsd onthe lexical sample tasks described above.
The resultsare showed in Table 4 and indicate that using DMsallowed K ?wsd to significantly outperform Kwsd.In addition, K ?wsd turns out the best systems forall the tested Senseval-3 tasks.Finally, the performance of K ?wsd are higher thanthe human agreement for the English and Spanishtasks5.Note that, in order to guarantee an uniform appli-cation to any language, we do not use any syntacticinformation provided by a parser.4.4 Learning CurvesThe Figures 1, 2, 3 and 4 show the learning curvesevaluated on K ?wsd and Kwsd for all the lexical sam-ple tasks.The learning curves indicate that K ?wsd is far su-perior to Kwsd for all the tasks, even with few ex-amples.
The result is extremely promising, for itdemonstrates that DMs allow to drastically reducethe amount of sense tagged data required for learn-ing.
It is worth noting, as reported in Table 5, thatK ?wsd achieves the same performance of Kwsd usingabout half of the training data.% of trainingEnglish 54Catalan 46Italian 51Spanish 50Table 5: Percentage of sense tagged examples re-quired by K ?wsd to achieve the same performance ofKwsd with full training.5 Conclusion and Future WorksIn this paper we presented a supervised algorithmfor WSD, based on a combination of kernel func-tions.
In particular we modeled domain and syn-tagmatic aspects of sense distinctions by definingrespectively domain and syntagmatic kernels.
TheDomain kernel exploits Domain Models, acquiredfrom ?external?
untagged corpora, to estimate thesimilarity among the contexts of the words to be dis-ambiguated.
The syntagmatic kernels evaluate thesimilarity between collocations.We evaluated our algorithm on several Senseval-3 lexical sample tasks (i.e.
English, Spanish, Ital-ian and Catalan) significantly improving the state-ot-the-art for all of them.
In addition, the performance5It is not clear if the inter-annotator-agreement can be con-siderated the upper bound for a WSD system.408MF Agreement BEST Kwsd K ?wsd DM+English 55.2 67.3 72.9 69.7 73.3 3.6Catalan 66.3 93.1 85.2 85.2 89.0 3.8Italian 18.0 89.0 53.1 53.1 61.3 8.2Spanish 67.7 85.3 84.2 84.2 88.2 4.0Table 4: Comparative evaluation on the lexical sample tasks.
Columns report: the Most Frequent baseline,the inter annotator agreement, the F1 of the best system at Senseval-3, the F1 of Kwsd, the F1 of K ?wsd,DM+ (the improvement due to DM, i.e.
K ?wsd ?Kwsd).0.50.550.60.650.70.750 0.2 0.4 0.6 0.8 1F1Percentage of training setK'wsdK wsdFigure 1: Learning curves for English lexical sampletask.0.650.70.750.80.850.90 0.2 0.4 0.6 0.8 1F1Percentage of training setK'wsdK wsdFigure 2: Learning curves for Catalan lexical sampletask.of our system outperforms the inter annotator agree-ment in both English and Spanish, achieving the up-per bound performance.We demonstrated that using external knowledge0.250.30.350.40.450.50.550.60.650 0.2 0.4 0.6 0.8 1F1Percentage of training setK'wsdK wsdFigure 3: Learning curves for Italian lexical sampletask.0.60.650.70.750.80.850.90 0.2 0.4 0.6 0.8 1F1Percentage of training setK'wsdK wsdFigure 4: Learning curves for Spanish lexical sam-ple task.inside a supervised framework is a viable method-ology to reduce the amount of training data requiredfor learning.
In our approach the external knowledgeis represented by means of Domain Models automat-409ically acquired from corpora in a totally unsuper-vised way.
Experimental results show that the useof Domain Models allows us to reduce the amountof training data, opening an interesting research di-rection for all those NLP tasks for which the Knowl-edge Acquisition Bottleneck is a crucial problem.
Inparticular we plan to apply the same methodology toText Categorization, by exploiting the Domain Ker-nel to estimate the similarity among texts.
In this im-plementation, our WSD system does not exploit syn-tactic information produced by a parser.
For the fu-ture we plan to integrate such information by addinga tree kernel (i.e.
a kernel function that evaluates thesimilarity among parse trees) to the kernel combi-nation schema presented in this paper.
Last but notleast, we are going to apply our approach to developsupervised systems for all-words tasks, where thequantity of data available to train each word expertclassifier is very low.AcknowledgmentsAlfio Gliozzo and Carlo Strapparava were partiallysupported by the EU project Meaning (IST-2001-34460).
Claudio Giuliano was supported by the EUproject Dot.Kom (IST-2001-34038).
We would liketo thank Oier Lopez de Lacalle for useful comments.ReferencesN.
Cristianini and J. Shawe-Taylor.
2000.
An introduc-tion to Support Vector Machines.
Cambridge Univer-sity Press.B.
Decadt, V. Hoste, W. Daelemens, and A. van denBosh.
2004.
Gambl, genetic algorithm optimiza-tion of memory-based wsd.
In Proc.
of Senseval-3,Barcelona, July.S.
Deerwester, S. Dumais, G. Furnas, T. Landauer, andR.
Harshman.
1990.
Indexing by latent semantic anal-ysis.
Journal of the American Society of InformationScience.A.
Gliozzo, C. Strapparava, and I. Dagan.
2004.
Unsu-pervised and supervised exploitation of semantic do-mains in lexical disambiguation.
Computer Speechand Language, 18(3):275?299.B.
Magnini and G. Cavaglia`.
2000.
Integrating subjectfield codes into WordNet.
In Proceedings of LREC-2000, pages 1413?1418, Athens, Greece, June.B.
Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.2002.
The role of domain information in wordsense disambiguation.
Natural Language Engineer-ing, 8(4):359?373.R.
Mihalcea and P. Edmonds, editors.
2004.
Proceedingsof SENSEVAL-3, Barcelona, Spain, July.R.
Mihalcea and E. Faruque.
2004.
Senselearner: Min-imally supervised WSD for all words in open text.
InProceedings of SENSEVAL-3, Barcelona, Spain, July.G.
Salton and M.H.
McGill.
1983.
Introduction to mod-ern information retrieval.
McGraw-Hill, New York.J.
Shawe-Taylor and N. Cristianini.
2004.
Kernel Meth-ods for Pattern Analysis.
Cambridge University Press.S.
Small.
1980.
Word Expert Parsing: A Theory of Dis-tributed Word-based Natural Language Understand-ing.
Ph.D. Thesis, Department of Computer Science,University of Maryland.C.
Strapparava, A. Gliozzo, and C. Giuliano.
2004.
Pat-tern abstraction and term similarity for word sensedisambiguation: Irst at senseval-3.
In Proc.
ofSENSEVAL-3 Third International Workshop on Eval-uation of Systems for the Semantic Analysis of Text,pages 229?234, Barcelona, Spain, July.S.K.M.
Wong, W. Ziarko, and P.C.N.
Wong.
1985.
Gen-eralized vector space model in information retrieval.In Proceedings of the 8th ACM SIGIR Conference.D.
Yarowsky and R. Florian.
2002.
Evaluating sense dis-ambiguation across diverse parameter space.
NaturalLanguage Engineering, 8(4):293?310.410
