An Ontology-Based Approach to Disambiguation of Semantic RelationsTine Lassen and Thomas Vestskov TerneyDepartment of Computer Science, Roskilde University, Denmarktlassen@ruc.dk, tvt@ruc.dkAbstractThis paper describes experiments in usingmachine learning for relation disambiguation.There have been succesfuld experiments incombining machine learning and ontologies,or light-weight ontologies such as WordNet,for word sense disambiguation.
However,what we are trying to do, is to disambiguatecomplex concepts consisting of two simplerconcepts and the relation that holds betweenthem.
The motivation behind the approach isto expand existing methods for content basedinformation retrieval.
The experiments havebeen performed using an annotated extract ofa corpus, consisting of prepositions surroun-ded by noun phrases, where the prepositionsdenote the relation we are trying disambigu-ate.
The results show an unexploited opportu-nity of including prepositions and the relationsthey denote, e.g.
in content based informationretrieval.1 IntroductionWhat we describe in this paper, which we refer to as re-lation disambiguation, is in some sense similar to wordsense disambiguation.
In traditional word sense disam-biguation the objective is to associate a distinguishablesense with a given word (Ide and Ve?ronis, 1998).
Itis not a novel idea to use machine learning in con-nection with traditional word sense disambiguation,and as such it is not a novel idea to include some kindof generalization of the concept that a word expres-ses in the learning task either (Yarowsky, 1992).
Ot-her projects have used light-weight ontologies such asWordNet in this kind of learning task (Voorhees, 1993;Agirre and Martinez, 2001).
What we believe is ourcontribution with this work is the fact that we attempt tolearn complex concepts that consist of two simpler con-cepts, and the relation that holds between them.
Thus,we start out with the knowledge that some relationholds between two concepts, which we could expressas REL(concept1,concept2), and what we aim at beingable to do is to fill in a more specific relation type thanthe generic REL, and get e.g.
POF(concept1,concept2)in the case where a preposition expresses a partitive re-lation.
This makes it e.g.
possible to determine fromthe sentence ?France is in Europe?
that France is a partof Europe.
As in word sense disambiguation we herepresuppose a finite and minimal set of relations, whichis described in greater detail in section 2.The ability to identify these complex structures in text,can facilitate a more content based information retri-eval as opposed to more traditional search engines,where the information retrieval relies more or lessexclusively on keyword recognition.
In the OntoQueryproject1, pertinent text segments are retrieved based onthe conceptual content of the search phrase as well asthe text segments (Andreasen et al, 2002; Andreasenet al, 2004).
Concepts are here identified through theircorresponding surface form (noun phrases), and map-ped into the ontology.
As a result, we come from a flatstructure in a text to a graph structure, which describesthe concepts that are referred to in a given text segment,in relation to each other.However, at the moment the ontology is strictly asubsumption-based hierarchy and, further, only relati-vely simple noun phrases are recognized and mappedinto the ontology.
The work presented here expandsthis scope by including other semantic relations be-tween noun phrases.
Our first experiments in this di-rection have been an analysis of prepositions with sur-rounding noun phrases (NPs).
Our aim is to show thatthere is an affinity between the ontological types of theNP-heads and the relation that the preposition denotes,which can be used to represent the text as a complexsemantic structure, as opposed to simply running text.The approach to showing this has been to annotate acorpus and use standard machine learning methods onthis corpus.2 Semantic relationsThe following account is based on the work of (Jensenand Nilsson, 2006): Relations exist between entities re-ferred to in discourse.
They can exist at different synta-ctic levels; across sentence boundaries as in example 1,or within a sentence, a phrase or a word.
The relations1http://www.ontoquery.dk72can be denoted by different parts of speech, such as averb, a preposition or an adjective, or they can be impli-citly present in compounds and genitive constructionsas in example 2.Semantic relations are n-ary: In example 1 below theverb form ?owns?
denotes a binary relation between Pe-ter and a dog, and in example 3, the verb form ?gave?denotes a ternary relation between Peter, the dog and abone.
In example 4 the preposition ?in?
denotes a bi-nary relation between the dog and the yard.
(1) Peter owns a dog.
It is a German shepherd.
(2) Peter?s dog.
(3) Peter gave the dog a bone.
(4) The dog in the yard.In the framework of this machine learning project, wewill only consider binary relations denoted by prepo-sitions.
A preposition, however, can be ambiguous inregard to which relation it denotes.
As an example, letus consider the Danish preposition i (Eng: in): The sur-face form i in ?A i B?
can denote at least five differentrelations between A and B:1.
A patient relation PNT; a relation where one of thearguments?
case role is patient, e.g.
?
?ndringer istofskiftet?
(changes in the metabolism).2.
A locational relation LOC; a relation that denotesthe location/position of one of the arguments com-pared to the other argument, e.g.
?skader i hjer-temuskulaturen?
(injuries in the heart muscle).3.
A temporal relation TMP; a relation that denotesthe placement in time of one of the argumentscompared to the other, e.g.
?mikrobiologien i1800-tallet?
(microbiology in the 19th century).4.
A property ascription relation CHR; a relation thatdenotes a characterization relation between one ofthe arguments and a property, e.g.
?antioxidanteri renfremstillet form?
(antioxidants in a pure form)5.
A ?with respect to?
relation WRT; an underspeci-fied relation that denotes an ?aboutness?
relationbetween the arguments, e.g.
?forskelle i saltindta-gelsen?
(differences in the salt intake) .As presented above, the idea is to perform supervisedmachine learning, that will take into account the sur-face form of the preposition and the ontological typeof the heads of the surrounding noun phrases, and onthis basis be able to determine the relation that holdsbetween noun phrases surrounding a preposition in un-seen text.3 The corpusIn order to establish a training set, a small corpus of ap-proximately 18,500 running words has been compiledfrom texts from the domain of nutrition and afterwardsannotated with the ontological type of the head of thenoun phrases, and the semantic relation denoted by thepreposition 2.All the text samples in this corpus derive from ?TheDanish National Encyclopedia?
(Gyldendal, 2004), andare thus not only limited domain-wise, but also of avery specific text type which can be classified as expert-to-non-expert.
Thus, we cannot be certain that our re-sults can be directly transferred to a larger or more ge-neral domain, or to a different text type.
This aspectwould have to be empirically determined.3.1 AnnotationFor the purpose of learning relations, 952 excerpts ofthe form:NP ?
P ?NP (5)have been extracted from the corpus and annotated withinformation about part of speech, ontological type andrelation type for NP heads and prepositions, respecti-vely.
An example of the analyzed text excerpts are gi-ven in table 1 on the following page, where each rowindicates a level of the analysis.The POS-tagging and head extraction have been doneautomatically, the ontological type assignation partlyautomatically (ontology look-up) and partly manually(for words that do not exist as instantiations of conceptsin the ontology).
The relation annotation has been donemanually.The tags used in the annotation on the three levels are:POS-tags.
Our tagger uses a subset of the PAROLEtag set, consisting of 43 tags, see (Hansen, 2000),which means that it is a low level POS taggingwith little morphosyntactic information.
We onlyuse the tags in order to extract NPs and preposi-tions, and thus do not need a more fine-grainedinformation level.SIMPLE-tags.
The tags used for the ontological typeannotation consist of abbreviations of the types inthe SIMPLE top ontology.
The tag set consists of151 tags.Relation-tags.
The tags used for the relation anno-tation derive from a minimal set of relations thathave been used in earlier OntoQuery related work.The set can be seen in table 22Extraction, POS-tagging and initial ontological and re-lation type annotation was done by Dorte Haltrup Hansen,CST, University of Copenhagen73surface form blodprop (thrombosis) i (in) hjertet (the heart)syntactic structure head of first NP preposition head of second NPrelation and ontological type disease location body partTable 1: Example of the text excerpts analyzed in our experiments.
Each row indicate a level of analysisThe manual relation annotation has been done by oneannotator for this initial project.
The ideal situationwould be to have several annotators annotate the cor-pus.
If two or more people annotate the same corpus,they are almost certain to disagree on some occasions.This disagreement can have two sources: first it can bedue to cognitive differences.
Two people subjected tothe same utterance are not guaranteed to perceive thesame content, or to perceive the content intended bythe producer of the utterance.
Many factors are at playhere; cultural background, knowledge, memory, etc.Secondly, it can be due to conceptual, lexical or syn-tactic ambiguity in the utterance.
We cannot removethese sources of disagreement, but we can introducetools that make the annotation more consistent.
Byusing a finite and minimal realtion tag set and, further,by introducing paraphrase tests, we hope to minimizethe risk of inter-annotator disagreement in a future an-notation on a larger scale.3.1.1 The ontological type annotationAs noted above, the ontological types used in the ex-periments derive from the SIMPLE top ontology (Pe-dersen, 1999; Lenci et al, 2000).
The heads of thephrases have been annotated with the lowest possiblenode, i.e.
ontological type, of the top ontology.
In thecase of blodprop the annotation of ontological type is?disease?, since ?disease?
is the lowest node in the topontology in the path from thrombosis to the top.
This isillustrated in figure 1, which shows the path from blod-prop (thrombosis) to the top level of SIMPLE.Thus, for the purpose of this project, we only consi-der one node for each concept: the lowest possiblenode in the top ontology.
Another approach wouldbe to consider the the full path to the top node, andalso including the path from the leaf node to thelowest node in the top ontology.
In the example depi-cted in figure 1, the full path from trombosis to thetop node would be trombosis?cardiovascular disease?disease?phenomenon?event?entity?top or trombosis?cardiovascular disease?disease?agentive?top.3.1.2 The set of relationsFor the purpose of the manual relation annotation, weneeded to decide on a finite set of possible relations thatcan be denoted by prepositions.
This is a non-trivialtask, as it is almost impossible to foresee which rela-tions prepositions can denote generally, and in the texttype at hand specifically, by introspection alone.
Themethod that we decided to use was the following: AntopentityeventagentivephenomenondiseasethrombosisyI:666cardiovascular disease6Top ontologyDomain ontology.............................................................Figure 1: An illustration of the path from blodprop(thrombosis) to the top level of the SIMPLE ontology.initial set of relations that have all been used in priorOntoQuery-related work (Nilsson, 2001; Madsen et al,2001; Madsen et al, 2000), were chosen as a point ofdeparture.
The final set was found by annotating thetext segments using this set as the possible relation ty-pes, and the relations that are actually manifested inthe data then form the final subset that was used as in-put for a machine learning algorithm.
The final subsetis shown in table 2.Role DescriptionAGT Agent of act or processBMO By means of, instrument, viaCBY Caused byCHR Characteristic (property ascription)CMP Comprising, has partDST Destination of moving processLOC Location, positionPNT Patient of act or processSRC Source of act or processTMP Temporal aspectsWRT With respect toTable 2: The set of relations used in the annotation,which is a subset of the set proposed in Nilsson, 2001.743.2 Paraphrase testsIn order to ensure a consistent relation annotation, itis necessary to develop a set of paraphrase tests thatcan help the annotator determine which relation a givenpreposition denotes in a given context.
Some relationsare particularly difficult to intuitively keep apart fromclosely related relations.
One of these problematic re-lation pairs is treated in some detail below.For example locative and partitive relations can be diffi-cult to keep apart, probably because they to some extentare overlapping semantically.
From a philosophical po-int of view, an important question is ?when does an en-tity become part of the entity it is located in?
?, but froma practical point of view, we are interested in answe-ring the question ?how can we decide if a given relationa locative or partitive relation?
?.In this paper we will only treat the latter question.
Atool that is useful for this purpose is the paraphrase test:If we can paraphrase the text segment in question intothe phrasing the test prescribes, while preserving thesemantic content, we can conclude that the relation is apossible relation for the given phrase.3.2.1 Attribute Transportation TestThe two relations LOC and POF can be difficult to dif-ferentiate, even when using paraphrase tests.
There-fore, an additional test that could be considered, isRuus?
attribute transportation test (Ruus, 1995)3.
Inthe example ?The pages in the book?, the book getse.g.
the attribute ?binding: {hardback | paperback}?from cover, and the attribute ?paper grade:{bond | book| bristol | newsprint}?
from pages.Figure 2: A graphical representation of the relation be-tween book and pagesWe cannot observe an attribute transport, neither fromthe bird to the roof, nor the other way.
This suggeststhat it is possible to use the atrribute transportation testin order to determine whether a given relation is a POFor a LOC relation.
Thus, we can now formulate thefollowing paraphrase test for POF:POF: A consists e.g.
of B andA has the attribute X, from B.3We will here ignore the question of direction of transport4 ExperimentsThe annotation process generates af a feature space ofsix dimensions, namely the lemmatized form of the twoheads of the noun phrases, the ontological types of theheads, the preposition and the relation.
In the corpusthere is a total of only 952 text segments.
In generalthe distribution of the data is highly skewed and spar-seness is a serious problem.
More than half of the in-stances are of the relation type WRT or PNT, and therest of the instances are distributed among the remai-ning 10 relations with only 14 instances scattered overthe tree smallest classes.
This is illustrated in figure 3.There are 332 different combinations of ontological ty-pes where 197 are unique.
There are 681 different he-ads and 403 of them are unique, with all of them beinglemmatized.Figure 3: An illustration of the distribution of the 12possible relations.Our assumption is that there is consistency in whichrelations prepositions usually denote in particular con-texts, and hence the learning algorithms should be ableto generalize well.
We also assume that the additionof the ontological types of the head of the NP, is themost vital information in classifying the relation type,at least in this case where data is sparse.We have run the experiments with a Support VectorMachine algorithm SMO (Keerthi et al, 2001) andthe prepositional rule learning algorithm JRip (Cohen,1995).
The former in order to get high precision, thelatter in order to get easily interpretable rules for lateranalysis (see section 4.1).
The experiments were runusing 10-fold-cross-validation, with a further partitionof the training set at each fold into a tuning and a trai-ning set.
The tuning set was used to optimize the pa-rameter4 settings for each algorithm .
The implemen-tation of the algorithms that we used, was the WEKAsoftware package (Frank et al, 2005).4For SMO the parameters where complexity, kernel usedand gamma for the RBF kernel.
For JRip it was number offolds used for growing and pruning, minimum number of in-stances covered and number of optimization runs75The experiments were run on seven different combi-nations of the feature space, ranging from using onlythe heads to using both heads, preposition and ontolo-gical types of the heads.
This was done in order to getinsight into the importance of using ontological typesin the learning.
The results of these experiments areshown in table 3.
The last column shows the precisionfor a projected classifier (PC) in the cases where it out-performs the trivial rejector.
The projected classifier,in this case, assigns the relation that is most commonfor the corresponding input pair; e.g if the ontologicaltypes are DIS/HUM, then the most common relation isPNT.
The trivial rejector, which assigns the most com-mon relation, in this case WRT, to all the instances,achieves a precision of 37.8%.Feature space JRip SVM PC1 Preposition 68.4 68.5 67.62 Ontological types 74.4 77.0 61.83 Lemma 66.8 73.3 ?4 Lemma and Preposi-tion72.3 83.4 ?5 Ontological types andLemma74,7 81.7 ?6 Ontological types andPreposition82.6 86.6 ?7 Ontological types,Preposition andLemma84,0 88.3 ?Table 3: The precision of SVM, JRip and a projectedclassifier on the seven different combinations of inputfeatures.
?Lemma?
here is short for lemmatized NPhead.The following conclusions can be drawn from table 3.The support vector machine algorithm produces a re-sult which in all cases is better than the baseline, i.e.
weare able to produce a model that generalizes well overthe training instances compared to the projected clas-sifier or the trivial rejector.
This difference is not sta-tistically significant at a confidence level of 0.95 whenonly training on the surface form of prepositions.A comparison of line 1?3 shows that training on onto-logical types seems to be superior to using lemmatizedNP heads or prepositions, though the superiority is notstatistically significant when comparing to the lemma-tized NP heads.
When comparing line 4?7 the diffe-rence between the results are not statistically signifi-cant.
This fact may owe to the data sparseness.
Howe-ver, comparing line 1 to line 6 or 7, shows that the im-provement of adding the preposition and the lemma-tized NP heads to the ontological types is statisticallysignificant.In general, the results reveal an unexplored opportu-nity to include ontological types and the relations thatprepositions denote in information retrieval.
In the nextsection, we will look more into the rules created by theJRip algorithm from a linguistic point of view.4.1 Analyzing the rulesIn this section we will take a deeper look into the rulesproduced by JRip on the data set with only ontologicaltypes, since they are the most interesting in this context.The JRip algorithm produced on average 21 rules.
Themost general rule covering almost half of the instan-ces is the default rule, that assigns all instances to theWRT relation if no other rules apply.
At the other endof the spectrum, there are ten rules covering no morethan 34 instances, but with a precision of 100%.
It isfutile to analyse these rules, since they cover the mostinfrequent relations and hence may be overfitting thedata set.
However, this seems not be the case with arule like ?if the ontotype of the first head is DISEASEand and the ontotype of the second head is HUMANthen the relation is PATIENT?
covering an instance ase.g.
?iron deficiency in females?.The rule with the second highest coverage, and a fairlylow precision of around 66%, is the rule: ?if the on-totype of the second head is BODY PART then therelation type is LOCATIVE?.
The rule covers instan-ces as e.g.
?.
.
.
thrombosis in the heart?
but also incor-rectly classifies all instances as LOCATIVE where therelation type should be SOURCE.
E.g.
the sentence?.
.
.
iron absorbtion from the intestine?, which is in facta SOURCE relation, but is classified as LOCATIVE bythe rule.One of the least surprising and most precise rules is:?if the ontotype of the second head is TIME then therelation type is TEMPORAL?
covering an instance ase.g.
?.
.
.
diet for many months?.
We would expect asimilar rule to be produced, if we had performed thelearning task on a general language corpus.5 Conclusion and future workEven though the experiments are in an early phase, theresults indicate that it is possible to analyse the seman-tic relation a preposition denotes between two nounphrases, by using machine learning and an annotatedcorpus ?
at least within the domain covered by the on-tology.
Future work will therefore include annotationand investigation of a general language corpus.
Also, amore thorough examination of the corpus, more specifi-cally an investigation of which relations or prepositionsthat are most difficult to analyse.
Also, we will experi-ment with the amount of information that we train on,not as we have already done by in- or excluding typesof information, but rather the extension of the infor-mation: Could we predict the ontological type of oneof the arguments by looking at the other?
Finally, anexplicit inclusion of the whole ontology in the learningprocess is on the agenda, as proposed in section 3.1.1on page 3, in the anticipation that the learner will pro-duce an even better model.766 AcknowledgementsWe would like to thank Troels Andreasen, Per AnkerJensen and two anonymous reviewers for fruitful com-ments.
The latter especially for comments on the expe-rimental part and inter-annotator agreement.References[Agirre and Martinez2001] E. Agirre and D. Martinez.2001.
Learning class-to-class selectional preferences.
[Andreasen et al2002] Troels Andreasen, Per AnkerJensen, J?rgen Fischer Nilsson, Patrizia Paggio, Bo-lette Sandford Pedersen, and Hanne Erdman Thomsen.2002.
Ontological extraction of content for text que-rying.
In Lecture Notes in Computer Science, volume2553, pages 123 ?
136.
Springer-Verlag.
[Andreasen et al2004] Troels Andreasen, Per AnkerJensen, J&#248;rgen Fischer Nilsson, Patrizia Paggio,Bolette Sandford Pedersen, and Hanne Erdman Thom-sen. 2004.
Content-based text querying with onto-logical descriptors.
Data & Knowledge Engineering,48(2):199?219.
[Cohen1995] William W. Cohen.
1995.
Fast effectiverule induction.
In Armand Prieditis and Stuart Russell,editors, Proceedings of the 12th International Confe-rence on Machine Learning, pages 115?123, TahoeCity, CA.
Morgan Kaufmann.
[Frank et al2005] Eibe Frank, Mark Hall, and LenTrigg.
2005.
Weka.
Publicly available, November.
[Gyldendal2004] Gyldendal.
2004.
The danish natio-nal encyclopedia.
ISBN: 8702031051.
[Hansen2000] Dorte Haltrup Hansen.
2000.
Tr?ningog brug af brill-taggeren pa?
danske tekster.
Technicalreport, CST.
[Ide and Ve?ronis1998] Nancy Ide and Jean Ve?ronis.1998.
Special issue on word sense disambiguation: In-troduction to the special issue on word sense disambi-guation: the state of the art.
Computational Lingui-stics, 24.
[Jensen and Nilsson2006] Per Anker Jensen andJ?rgen Fischer Nilsson, 2006.
Syntax and Semantics ofPrepositions, volume 29 of Text, Speech and LanguageTechnology, chapter Ontology-Based Semantics forPrepositions.
Springer.
[Keerthi et al2001] S. Sathiya Keerthi, Shirish Krish-naj Shevade, Chiranjib Bhattacharyya, and K. R. K.Murthy.
2001.
Improvements to platt?s smo algo-rithm for svm classifier design.
Neural Computation,13(3):637?649.
[Lenci et al2000] Alessandro Lenci, Nuria Bel, Fede-rica Busa, Nicoletta Calzolari1, Elisabetta Gola, Mo-nica Monachini, Antoine Ogonowski, Ivonne Peters,Wim Peters, Nilda Ruimy, Marta Villegas, and Anto-nio Zampolli.
2000.
Simple: A general framework forthe development of multilingual lexicons.
Internatio-nal Journal of Lexicography, 13(4):249?263.
[Madsen et al2000] Bodil Nistrup Madsen, Bo-lette Sandford Pedersen, and Hanne Erdman Thomsen.2000.
Semantic relations in content-based queryingsystems: a research presentation from the ontoqueryproject.
In K Simov and A Kiryakov, editors, Ontolo-gyes and Lexical Knowledge Bases.
Proceedings of the1st International Workshop, OntoLex 2000.
Universityof Southern Denmark, Kolding.
[Madsen et al2001] Bodil Nistrup Madsen, Bo-lette Sandford Pedersen, and Hanne Erdman Thomsen.2001.
Defining semantic relations for ontoquery.
InPer Anker Jensen and P Skadhauge, editors, Procee-dings of the First International OntoQuery WorkshopOntology-based interpretation of NP?s.
University ofSouthern Denmark, Kolding.
[Nilsson2001] J?rgen Fischer Nilsson.
2001.
A logico-algebraic framework for ontologies, ontolog.
In Jensenand Skadhauge, editors, Proceedings of the First In-ternational OntoQuery Workshop Ontology-based in-terpretation of NP?s.
University of Southern Denmark,Kolding.
[Pedersen1999] Bolette Sandford Pedersen.
1999.
Dendanske simple-ordbog.
en semantisk, ontologibaseretordbog.
In C. Poulsen, editor, DALF 99, Datalingvi-stisk Forenings a?rsm?de 1999.
Center for sprogtekno-logi.
[Ruus1995] Hanne Ruus.
1995.
Danske kerneord.Centrale dele af den danske leksikalske norm 1-2.
Mu-seum Tusculanums Forlag.
[Voorhees1993] EllenM.
Voorhees.
1993.
Using word-net to disambiguate word senses for text retrieval.
InRobert Korfhage, Edie M. Rasmussen, and Peter Wil-lett, editors, Proceedings of the 16th Annual Interna-tional ACM-SIGIR Conference on Research and Deve-lopment in Information Retrieval.
Pittsburgh, PA, USA,June 27 - July 1, 1993, pages 171?180.
ACM.
[Yarowsky1992] David Yarowsky.
1992.
Word-sensedisambiguation using statistical models of Roget?s ca-tegories trained on large corpora.
In Proceedings ofCOLING-92, pages 454?460, Nantes, France, July.77
