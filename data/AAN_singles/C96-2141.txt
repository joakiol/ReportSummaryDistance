HMM-Based Word Alignment in Statistical TranslationStephan Voge l  Hermann Ney  Chr i s toph  T i l lmannLehrs tuh l  ffir In fo rmat ik  V, RWTH AachenD-52056 Aachen,  Germany{vogel, ney ,  t illmann}@inf ormat ik.
rwth-aachen, deAbstractIn this paper, we describe a new modelfor word alignment in statistical trans-lation and present experimental results.The idea of the model is to make thealignment probabilities dependent on thedifferences in the alignment positionsrather than on the absolute positions.To achieve this goal, the approach us-es a first-order Hidden Markov model(HMM) for the word alignment problemas they are used successfully in speechrecognition for the time alignment prob-lem.
The difference to the time align-ment HMM is that there is no monotonyconstraint for the possible word order-ings.
We describe the details of the mod-el and test the model on several bilingualcorpora.1 IntroductionIn this paper, we address the problem of wordalignments for a bilingual corpus.
In the recentyears, there have been a number of papers con-sidering this or similar problems: (Brown et al,1990), (Dagan et al, 1993), (Kay et al, 1993),(Fung et al, 1993).In our approach, we use a first-order HiddenMarkov model (HMM) (aelinek, 1976), which issimilar, but not identical to those used in speechrecognition.
The key component of this approachis to make the alignment probabilities dependentnot on the absolute position of the word align-ment, but on its relative position; i.e.
we considerthe differences in the index of the word positionsrather than the index itself.The organization of the paper is as follows.After reviewing the statistical approach to ma-chine translation, we first describe the convention-al model (mixture model).
We then present ourfirst-order HMM approach in lull detail.
Finallywe present some experimental results and compareour model with the conventional model.2 Review: Translation ModelThe goal is the translation of a text given in somelanguage F into a target language E. For conve-nience, we choose for the following exposition aslanguage pair French and English, i.e.
we are giv-en a French string f~ = fx ...fj...fJ, which is to betranslated into an English string e / = el...ei...cl.Among all possible English strings, we will choosethe one with the highest probability which is givenby Bayes' decision rule:a{ = argmax{P,.
(c{lAa)} q= argmax {Pr(ejt) .
l ' r ( f  le\[)}el ~Pr(e{) is the language model of the target lan-guage, whereas Pr(fJle{) is the string translationmodel.
The argmax operation denotes the searchproblem.
In this paper, we address the problemof introducing structures into the probabilistic de-pendencies in order to model the string translationprobability Pr(f~ le{).3 Al ignment ModelsA key issne in modeling the string translationprobability Pr(J'~le I) is the question of how wedefine the correspondence b tween the words ofthe English sentence and the words of the Frenchsentence.
In typical cases, we can assume a sort ofpairwise dependence by considering all word pairs(fj, ei) for a given sentence pair I.-/1\[~'J', elqlj' We fur-ther constrain this model by assigning each Frenchword to exactly one English word.
Models describ-ing these types of dependencies are referred to asalignment models.In this section, we describe two models for wordalignrnent in detail:,.
a mixture-based alignment model, which wasintroduced in (Brown et al, 1990);?
an HMM-based alignment model.In this paper, we address the question of how todefine specific models for the alignment probabil-ities.
The notational convention will be as fol-lows.
We use the symbol Pr(.)
to denote general836probability distributions with (nearly) no Sl)eeiticasSUml)tions.
In contrast, for modcl-t)ased prol)--ability distributions, we use the generic symbolv(.
).3.1  Al ignment  w i th  M ix ture  D is t r i |mt ionHere, we describe the mixture-based alignmentmodel in a fornmlation which is different fronlthe original formulation ill (Brown el, a\[., 1990).We will ,is(: this model as reference tbr the IIMM-based alignments to lie 1)resented later.The model is based on a decomposition of thejoint probability \[br ,l'~ into a product over theprobabilities for each word J):aj=lwheFe~ fo\[' norll-la\]iz;i, t on 17(~/SOllS~ the 8elltC\]\[celength probability p(J\] l) has been included.
Thenext step now is to assutne a sort O\['l,airwise inter-act, ion between tim French word f j  an(l each, F,n-glish word ci, i = 1, ... l .
These dep('ndencies arecaptured in the lbrm of a rnixtnre distritmtion:1p(J)le{) = ~_.p(i, fjlc I)i=1I= ~_~p(ilj, l).p(fjle~)i=1Putting everything together, we have the followingmixture-based ntodel:J lr,'(fi!l~I) = p(JIO ' H ~_~ \[~,(ilJ, l).
~,(j)led\] (1)j= l  i=twith the following ingredients:?
sentence length prob~d)ility: P(J l l);?
mixture alignment probability: p( i l j  , I);?
translation probM)ility: p(f\[e).Assuming a tmifornl ~flignment prol)ability1.p(ilj, 1) = 7we arrive at the lh'st model proposed t)y (Brownet al, 1990).
This model will be referred to asIB M 1 model.To train the translation probabilities p(J'fc), weuse a bilingual (;orpus consisting of sentence pairs\[:/ ';4"1 : ', .
, s  Using the ,,laxin,ul , like-lihood criterion, we ol)tain the following iterative L aequation (Brown et al, 1990):/ ) ( f ie)  = ~ -  will,$'A(f,e) = ~ 2 ~5(f,J).~) }~ a(e,e~.~)For unilbrm alignment probabilities, it can beshown (Brown et al, 1990), that there is only oneoptinnnn and therefore the I,',M algorithm (Baum,1!
)72) always tinds the global optimum.For mixture alignment model with nonunilbrmalignment probabilities (subsequently referred toas IBM2 model), there ~tre to() many alignrnentparameters Pill j ,  I) to be estimated for smMl co lpora.
Therefore, a specific model tbr tile Mign-ment in:obabilities i used:r ( i - j~- )  (~) p( i l j  , 1) = l .
IE i ' : l  "( it --" J J-)This model assumes that the position distance rel-ative to the diagonal ine of the (j, i) plane is thedominating factor (see Fig.
1).
'lb train this mod-el, we use the ,naximutn likelihood criterion in theso-called ulaximmn al)proximation, i.e.
the likeli-hood criterion covers only tile most lik(-.ly align:inch, rather than the set of all alignm(,nts:dP,'(f(I,:I) ~ I I  ~"IU HO, ~)v(J} I,:~)\] (a)j=lIn training, this criterion amounts to a sequenceof iterations, each of which consists of two steps:* posi l ion al ignmcnl:  (riven the model parame-ters, deLerlniim the mosL likely position align-\]lient.?
paramc, lcr cst imal ion:  Given the positionalignment, i.e.
goiug along the alignmentpaths for all sentence pairs, perform maxi-tnulu likelihood estimation of the model pa-rameters; for model-De(' distributions, theseestimates result in relative frequencies.l)ue to the natnre of tile nfixture tnod(:l, thereis no interaction between d jacent  word positions.Theretbre, the optimal position i for each posi-tion j can be determined in(lependently of theneighbouring positions.
Thus l.he resulting train-ing procedure is straightforward.a.2  Al ignment  w i th  HMMWe now propose all HMM-based alignment model.
'\['he motivation is that typicMly we have a stronglocalization effect in aligning the words in paralleltexts (for language pairs fi:om \]ndoeuropean lan-guages): the words are not distrilmted arbitrarilyover the senteuce \])ositions, but tend to form clus-ters.
Fig.
1 illustrates this effect for the languagepair German-  15'nglish.Each word of the German sentence is assignedto a word of the English sentence.
The alignmentshave a strong tendency to preserve the local neigh-borhood when going from the one langnage to theother language.
In mm,y cases, although not al~ways, there is an even stronger restriction: thediffereuce in the position index is smMler than 3.837DAYSBOTHONEIGHTATITMAKECANWEIFTHINKIWELL+ + + + + + + + +j~ + ++ + + + + + + ~J  ~+ ++++++++/+?+.
- .+ + + + + + +/+ + + + ++ + + + + ~x~ + + + + ++ + + + +/+ D + + + + +++ + + ~ + + + + + + ++ + + _~ + + + + + + ++ + +~ + + ++ +++++ +jg  + + + + + + + + ++~ +++ + + + + + + +g + + ++ + ++ + + + +zaaFigure 1: Word alignment for a German- Englishsentence pair.To describe these word-by-word aligmnents, weintroduce the mapping j ---+ aj, which assigns aword f j  in position j to a word el in position{ = aj.
The concept of these alignments i similarto the ones introduced by (Brown et al, 1990),but we wilt use another type of dependence in theprobability distributions.
Looking at such align-ments produced by a hmnan expert, it is evidentthat the mathematical model should try to cap-ture the strong dependence of aj on the previousaligmnent.
Therefore the probability of alignmentaj for position j should have a dependence on theprevious alignment aj _ 1 :p(a j ia j_ l , i )  ,where we have inchided the conditioning on thetotal length \[ of the English sentence for normal-ization reasons.
A sinfilar approach as been cho-sen by (Da.gan et al, 1993).
Thus the problemformulation is similar to that of the time align-ment problem in speech recognition, where theso-called IIidden Markov models have been suc-cessfully used for a long time (Jelinek, 1976).
Us-ing the same basic principles, we can rewrite theprobability by introducing the 'hidden' alignmentsaf := al.
.
.aj .
.
.aa for a sentence pair If,a; e{\]:Pr(f~al es) = ~_,Vr(fal, aT\[ eI't,a7,1= ~ 1-IP"(k,"stfT-',"{ -*,e/)a I j=lSo far there has been no basic restriction of theapproach.
We now assume a first-order depen-dence on the alignments aj only:Vr(fj,aslf{ -~, J-* a I , e l )where, in addition, we have assmned that tiletranslation probability del)ends only oil aj and notoil aj-:l. Putting everything together, we have theibllowing llMM-based model:aPr(f:i'le{) = ~ I-I \[p(ajlaj - ' ,  l).p(Y)lea,)\] (4)af J=,with the following ingredients:?
IlMM alignment probability: p(i\]i', I) orp(a j  la j _ l ,  I ) ;?
translation probabflity: p(f\]e).In addition, we assume that the t{MM align-ment probabilities p(i\[i', \[) depend only on thejump width (i - i').
Using a set of non-negativeparameters {s ( i -  i')}, we can write the IIMMalignment probabilities in the form:4 i -  i') (5)p(ili', i )  = E '  s(1 - i')1=1This form ensures that for each word positioni', i' = 1, ..., I, the ItMM alignment probabilitiessatisfy the normMization constraint.Note the similarity between Equations (2) and(5).
The mixtm;e model can be interpreted as azeroth-order model in contrast to the first-ordertlMM model.As with the IBM2 model, we use again the max-imum approximation:JPr(fiSle~) "~ max\]--\[ \[p(asl<*j-1, z)p(fj l<~,)\] (6)a '  / .ll.
j,,,j= lIn  th is  case, the task  o f  f ind ing  the  opt ima lalignment is more involved than in the case of themixture model (lBM2).
Thereibre, we have to re-sort to dynainic programming for which we havethe following typical reeursion formula:Q(i, j )  = p(f j  lel) ,nvax \[p(ili', 1) .
Q(i', j - 1)\]i =l , .
, , IHere, Q(i, j )  is a sort of partial probability asin time alignment for speech recognition (Jelinek,197@.4 Exper imenta l  Resu l t s4.1 The  Task and the  CorpusThe models were tested on several tasks:?
the Avalanche Bulletins published by theSwiss Federal Institute for Snow andAvalanche Research (SHSAR) in Davos,Switzerland and made awtilable by the Eu-p "q  I ropean Corpus Initiative (I,CI/MCI, 1994);?
the Verbmobil Corpus consisting of sponta-neously spoken dialogs in the domain of ap-pointment scheduling (Wahlster, 1993);838,, the EuTrans C, orpus which contains tyl)icalphrases from the tourists and t.ravel docnain.
(EuTrans, 1996).'
l 'able \] gives the details on the size of tit<; cor-pora a, ud t;\]t<'it' vocal>ulary.
It shottld I>e notedthat in a.ll thes(; three ca.ses the ratio el' vocal)t,-\]ary size a.ml numl)er of running words is not veryfaw)rable.Tall)le, I: (,orpol :L(,o~pt s l,angua.ge Words Voc.
SizeAvalancJte\] A\[ \ [ ra i l sVerlmlobilFrolt ch(~('~ l lal lSpanishI,;nglish( le  11 anEnglish62849,\]4805--1:77@-1588815027925,\] 27199322652008t 63(}dO 172`\]/13For several years 1)et;weeu 83 and !
)2, theAvalanche Bulletins are awdlabte for I>oth Get-ntan and I!'ren(;\]l.
The following is a tyl)ical sen--t<;nce t>air fS;onl the <;or:IreS:Bei zu('.rst recht holnm, Sl)~i.tev tM'eren 'l'em-l)eraJ, uren sind vou Samsta.g his 1)ienstag tno fgett auf <l<'~t; All>ennor(ls<'.ite un</ am All>en-.ha.uptkanml oberhalb 2000 m 60 his 80 cmNeuschnee gel'aJlen.l)ar des temp&'atures d' abord dlevdes, puisplus basses, 60 h 8(1 cm de neige sent tombsde samedi h. mardi  matin sur le versant herdel; la eft're des Alpes au-dessus de 2000 l\[1.An exa,nq)le fi'om the Vet%mobil corpus is givenin Figure 1.4.2 Tra in ing  and  ILesul tsl,;ach of the three COrlJora.
were ttsed to train 1)othal ignnmnt models, the mixture-I>ased al ignmentmodel in Eq.
(1) and the llMM-base<l a.lignntentmod('l in Eq.(d).
ltere, we will consider the ex-p<'.rimenta.l tesl;s on tit<'.
Avalanche corpus in moredetail.
The traii, ing procedure consiste(l of thefollowing steps:?
, Init ial ization training: IBMI model trahtedfor t0 iterations of the i';M algorithm.,, l{,efinement traiuiug: The translation pcoba-1)ilities Dotn the initialization training wet'(;use+d to initialize both the IBM2 model andthe I I M M-based nligntnent mo<t<'+lIBM2 Model: 5 iteratious using Lit(" max-i lnum a.I)proximatiolt (Eq+(3))I IMM Model: 5 iterations usiug l le  max-.imum al)l)roximation (Fq.
(6))'l'h(, resulting perl>h:'~xity (inverse g<~olu(;l.ric av-era,ge of the likelihoods) for the dilferent lno(lelsave given iu tim Tal>\[es 2 and 3 for the Awdanehe<:<)rims.
In adclitiou t;o the total i>erl>lexity, whi<'.his the' globa.l opt imizat ion criterion, the tables al-so show the perplexities of the translation prob-abilities and of the al ignment probabil it ies.
Thelast line in Table 2 gives the perplexity measureswh(m a.lJplying the rtlaxilnun| approximat ion andCOml>uting the perph'~xity in t;\]lis approximation.These values are equal to the ones after initializingthe IBM2 and HMM models, as they should be.From Ta,ble 3, we can see.
that the mixture align-ment gives slightly better perplexity values for thetranslation l)roba.1)ilities, whereas the I IMM mod-el produces a smaller perplexity for the al ignmentl>rohal)ilities.
In the calculatiot, of the, perplexi-ties, th<' seld;en(;e length probal)ility was not in=eluded.Tahle 2: IBM I: Translation, a, l igmnent and totalpert)h'~xil.y as a. fimction of' the iteration.Iteration Tra,nslatiotl.
Alignrnent Total01291099.363.722.67t.871.8620.0720.0720.0720.0720.071994.007/1.5753.6237.5537.36Max.
3.88 20.07 77.!
)5'l'able 3: '1 rans\] ~+tion, aligmn en t and totaJ perplex-ity as a function of the itcra.tion for the IBM2 (A)and the I IMM model (13)Iter.
Tratmlat;i(mA 0l23,\]51~ 01345A ligniN.elJ t3.88- 20.073.17 10.823.25 10.153.22 10.103.20 \] 0.063.18 10.053.88 20.073.37 7.993.46 6.17;{./17 5.90"Ld6 5.853.`\]5 5.8,\]' l 'otal77.9534.2733.0332.4832.1832.0077.9526.982t.3620.4820.2/120.18Anoth<2r inl;crc:sting question is whether theIIMM alignntent model helps in finding good andsharply fo('usscd word+to-word (-orres\]Jondences.As an (;xamf,1o, Table 4 gives a COmlm+rison ofthe translatioJ~ probabil it ies p(f l  e) bctweett themixture and the IIMM alignnw+nt model For the(,e, u +l word Alpensiidhang.
The counts of thewords a.re given in brackets.
The, re is virLually no,:lilfc~rc~nce between the translation l.al>les for thetwo nn)dels (1BM2 and I IMM).
But+ itt general,the tl M M model seems to giw'.
slightly better re-suits in the cases of (;, ttna t COml+olmd words likeAlpcus'iidha'n,(I vcrsant sud des Alpes which re-quire \['u,tction words in the trattslation.839Table 4: Alpens/idhang.IBM1 Alpes (684) 0.171des (1968) 0.035le (1419) 0.039sud (416) 0.427sur (769) 0.040versant (431) 0.284IBM2 Alpes (684) 0.276sud (41.6) 0.371versant (431) 0.356HMM Alpes (684) 0.284des (1968) 0.028sud (416) 0.354versant (431) 0.333This is a result of the smoother position align-ments produced by the HMM model.
A pro-nounced example is given in Figure 2.
'She prob-lem of the absolute position alignment can hedemonstrated at the positions (a) and (c): bothSchneebretlgefahr und Schneeverfrachtungen havea high probability on neige.
The IBM2 modelschooses the position near the diagonal, as thisis the one with the higher probability.
Again,Schneebrettgefahr generates de which explains thewrong alignment near the diagonal in (c).However, this strength of the HMM model canalso be a weakness as in the case of est developpeist ... entstanden (see (b) in Figure 2.
Therequired two large jumps are correctly found bythe mixture model, but not by the HMM mod-el.
These cases suggest an extention to the HMMmodel.
In general, there are only a small numberof big jumps in the position alignments in a givensentence pair.
Therefore a model could be usefulthat distinguishes between local and big jumps.The models have also been tested on the Verb-mobil Translation Corpus as well as on a smallCorpus used in the EuTrans project.
The sen-tences in the EuTrans corpus are in generalshort phrases with simple grammatical structures.However, the training corpus is very small and theproduced alignments are generally of poor quality.There is no marked difference for the two align-ment models.Table 5: Perplexity results(b) Verbmobil Corpus.for (a) EuTrans andModel Iter.
Transl.
Align.
TotalIBM1 10 2.610 6.233 16.267IBM2 5 2.443 4.003 9.781HMM 5 2.461 3.934 9.686IBM1 10 4.373 10.674 46.672IBM2 5 4.696 6.538 30.706ItMM 5 4.859 5.452 26.495The Verbmobil Corpus consists of spontaneous-ly spoken dialogs in the domain of appointmentscheduling.
The assumption that every word inthe source language is aligned to a word in thetarget language breaks down for many sentencepairs, resulting in poor alignment.
This in turnaffects the quality of the translation probabilities.Several extensions to the current IIMM basedmodel could be used to tackle these problems:* The results presented here did not use theconcept of the empty word.
For the HMM-based model this, however, requires a second-order rather than a first-order model.. We could allow for multi-word phrases inboth languages.?
In addition to the absolute or relative align-ment positions, the alignment probabilitiescan be assumed to depend on part of speechtags or on the words themselves.
(confermodel 4 in (Brown et al, 1990)).5 ConclusionIn this paper, we have presented an itMM-basedapproach for rnodelling word aligmnents in par-allel texts.
The characteristic feature of this ap-proach is to make the alignment probabilities ex-plicitly dependent on the alignment position of theprevious word.
We have tested the model suc-cessfully on real data.
The HMM-based approachproduces translation probabilities comparable tothe mixture alignment model.
When looking atthe position alignments those generated by theItMM model are in general much smoother.
Thiscould be especially helpful for languages uch asGerman, where compound words are matched toseveral words in the source language.
On the oth-er hand, large jumps due to different word order-ings in the two languages are successfully modeled.We are presently studying and testing a nmltilevelHMM model that allows only a small number oflarge jumps.
The ultimate test of the differentalignment and translation models can only be car-ried out in the framework of a fully operationaltranslation system.6 AcknowledgementThis research was partly supported by the (\]er-man Federal Ministery of Education, Science, t{e-search and Technology under the Contract Num-ber 01 IV 601 A (Verbmobil) and under the EspritResearch Project 20268 'EuTrans).ReferencesL.
E. Baum.
11972.
An inequality and associat-ed maximization technique in statistical esti-mation of probabilistic functions of a Markovprocess.
Inequalities, 3:1-8.840ENTSTANDENSCHNEEBRETTGEFAHI~LOKALEERHEBLICHEEINEM2O00OBERHALBSCHNEEVERFRACHTUNGENDURCHISTGOTTHARDGEBIETIMUNDWALLISIMENTSTANDENSCHNEEBRETTGEFAHRLOKALEERHEBLICHEEINEM2000OBERHALBSCHNEEVERFRACHTUNGENDURCHISTGOTTHARDGEBIETIMUNDWALL IS+++ + ++ ++++ ++ + + + + + + + ++ ++ + + + + + ++ ++ ++ + + + + + + + + + ++ + + +++ + + + +  +++ + + + + + + + + ++ + + ++ + ++ +~ ++ + + + + + + + +/~ ++ + + + + + + + +t44 + ++++++++;/;/#+++++++j ,+?+ + -I- ~ + + ++ + ~ + + + + + + ++ +~ + + + + + + + +.
~ +  ++ + + + + + +O + + + +  + + + + +  +++ ++ ++ ++ + ++ + + + + +  ++I+ +++++ ++ + ++ + - + + Mixture+ j + + + + + + + + ++ , + + + + + + + + ++ + + + + + + + -t- + + + ++ + + + +(b)+ + + + + + + + + + ++ + ++ ++ + + + + +  + +++ + + + ++ + ++ + + + + + + +  ++ ++ + ++ ++ + ++ + + + + + + +  ++ ++ + + +++ +++ + + + + +  + + + + + + +  ++ ++ + + + + + 4 -+ + + + + + + + + + ++ + + + ++ + + + + +~+ + + + + + + + + + + + + + ~ + ~ + + + + + + + + ~+ + + + + + + + + + + + + +/ l~-Q-g  + + + ~ + + + + + + + +~+++++++++1/+++++++++++1/++++_12/ , ,+++ + + + + + + + + / + + + + + + + + + + + + y + + + / - - ' + + + H M M+ + + + + + + + I + + + + + + + + + + + + O- - I~ ' -g -g  + + + + ++ + + + ~ + + + + + + + + + + + + + + + + + + + ++ + + ~- t~t~-~ + + + + + + + + + + + + + + + + + + + + + ++ + ~/~ + + + + + + + + + + + + + ?
?
+ + ?
?
+ + + + + + ++ ~ J + + + + + + + + + + + + + + + + + + + + + + + + + + ++ ~ + + + + + + + + + + + + + + + + + + + + + + + + + + + ++ + + +  +++ + + + + +  +++ + + + + + + + +  ++ + ++ +?
oAFigure 2: Alignments generated by the IBM2 and the HMM model.Peter F. Brown, Vincent J. Della Piet, ra,Stephen A. I)ella Pietra, and Robert 1, Mercer.\]993.
The Mat, hema.tics of Statistical Machine'lS:unslalfion: Parameter Estimatiom (\]omputa-tional Linguistics, 19(2):26")--31 1.hlo Dagan, Ken Ctmreh, and William A. Gale.1993.
l{,obust 13ilingual Word Alignment, forMachine Aided 'l'rm~sl~ttion l'rocecdings of theWorkshop on Very Largc Corpora, C, oluml)us,Ohio, 1-8ECI/MC\[: The European Corpus Initiative Mul-.tilingual Corpus 1.
\[!)94.
Association for Com-pul;ational binguistics.EuTrans.
'l'he I)etinidon of a M'I' '\['ask.
'l)eh-nieal Report, I,~f\]'rans Project 1996(I,'orth-conuni,g), l)epto, de Sistemas Informaticos yComputacion (DSIC), Universidad Politecnicade Valencia.Pascale I"ung, and Kenneth Ward Church.
11994.K-vet: A flew N)proach \[br aligning parMleltexts.
Proceedings of COLING 94, 1096-ll02,Kyoto, Japan.Frederik Jelinek.
1976.
Speech Recognition byStatistical Met;hods.
Proceedings of the \[l~l?1'\],Vol.
64,532-556, April 11976.Martin Kay, and Martin RSscheisen.
1993.
Text-'l}anslation Alignment.
Computational Lin-guistics, 19(1):121-142Wolfgang Wahlster.
t993.
Verbmobil: 'l?ransla-tion of Face-to-Face Dialogs.
Proceedings of theMT' Summit IV, \]27-135, Kobe, Japan.841
