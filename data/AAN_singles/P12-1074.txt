Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 703?711,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsA Computational Approach to the Automation of Creative NamingGo?zde O?zbalFBK-Irst / Trento, Italygozbalde@gmail.comCarlo StrapparavaFBK-Irst / Trento, Italystrappa@fbk.euAbstractIn this paper, we propose a computational ap-proach to generate neologisms consisting ofhomophonic puns and metaphors based on thecategory of the service to be named and theproperties to be underlined.
We describe allthe linguistic resources and natural languageprocessing techniques that we have exploitedfor this task.
Then, we analyze the perfor-mance of the system that we have developed.The empirical results show that our approachis generally effective and it constitutes a solidstarting point for the automation of the namingprocess.1 IntroductionA catchy, memorable and creative name is an im-portant key to a successful business since the nameprovides the first image and defines the identity ofthe service to be promoted.
A good name is able tostate the area of competition and communicate thepromise given to customers by evoking semantic as-sociations.
However, finding such a name is a chal-lenging and time consuming activity, as only fewwords (in most cases only one or two) can be used tofulfill all these objectives at once.
Besides, this taskrequires a good understanding of the service to bepromoted, creativity and high linguistic skills to beable to play with words.
Furthermore, since manynew products and companies emerge every year, thenaming style is continuously changing and creativ-ity standards need to be adapted to rapidly changingrequirements.The creation of a name is both an art and a science(Keller, 2003).
Naming has a precise methodologyand effective names do not come out of the blue.
Al-though it might not be easy to perceive all the effortbehind the naming process just based on the finaloutput, both a training phase and a long process con-sisting of many iterations are certainly required forcoming up with a good name.From a practical point of view, naming agenciesand branding firms, together with automatic namegenerators, can be considered as two alternative ser-vices that facilitate the naming process.
However,while the first type is generally expensive and pro-cessing can take rather long, the current automaticgenerators are rather na?
?ve in the sense that they arebased on straightforward combinations of randomwords.
Furthermore, they do not take semantic rea-soning into account.To overcome the shortcomings of these two alter-native ways (i.e.
naming agencies and na?
?ve gener-ators) that can be used for obtaining name sugges-tions, we propose a system which combines severallinguistic resources and natural language processing(NLP) techniques to generate creative names, morespecifically neologisms based on homophonic punsand metaphors.
In this system, similarly to the pre-viously mentioned generators, users are able to de-termine the category of the service to be promotedtogether with the features to be emphasized.
Ourimprovement lies in the fact that instead of randomgeneration, we take semantic, phonetic, lexical andmorphological knowledge into consideration to au-tomatize the naming process.Although various resources provide distinct tipsfor inventing creative names, no attempt has beenmade to combine all means of creativity that can beused during the naming process.
Furthermore, inaddition to the devices stated by copywriters, there703might be other latent methods that these experts un-consciously use.
Therefore, we consider the taskof discovering and accumulating all crucial featuresof creativity to be essential before attempting to au-tomatize the naming process.
Accordingly, we cre-ate a gold standard of creative names and the corre-sponding creative devices that we collect from var-ious sources.
This resource is the starting point ofour research in linguistic creativity for naming.The rest of the paper is structured as follows.First, we review the state-of-the-art relevant to thenaming task.
Then, we give brief information aboutthe annotation task that we have conducted.
Lateron, we describe the model that we have designedfor the automatization of the naming process.
Af-terwards, we summarize the annotation task that wehave carried out and analyze the performance ofthe system with concrete examples by discussing itsvirtues and limitations.
Finally, we draw conclu-sions and outline ideas for possible future work.2 Related WorkIn this section, we will analyze the state of the artconcerning the naming task from three different as-pects: i) linguistic ii) computational iii) commercial.2.1 LinguisticLittle research has been carried out to investigatethe linguistic aspects of the naming mechanism.B.
V. Bergh (1987) built a four-fold linguistic topol-ogy consisting of phonetic, orthographic, morpho-logical and semantic categories to evaluate the fre-quency of linguistic devices in brand names.
Baoet al (2008) investigated the effects of relevance,connotation, and pronunciation of brand names onpreferences of consumers.
Klink (2000) basedhis research on the area of sound symbolism (i.e.
?the direct linkage between sound and meaning?
(Leanne Hinton, 2006)) by investigating whether thesound of a brand name conveys an inherent mean-ing and the findings showed that both vowels andconsonants of brand names communicate informa-tion related to products when no marketing com-munications are available.
Kohli et al (2005) ana-lyzed consumer evaluations of meaningful and non-meaningful brand names and the results suggestedthat non-meaningful brand names are evaluated lessfavorably than meaningful ones even after repeatedexposure.
Lastly, cog (2011) focused on the seman-tics of branding and based on the analysis of severalinternational brand names, it was shown that cogni-tive operations such as domain reduction/expansion,mitigation, and strengthening might be used uncon-sciously while creating a new brand name.2.2 ComputationalTo the best of our knowledge, there is only one com-putational study in the literature that can be appliedto the automatization of name generation.
Stock andStrapparava (2006) introduce an acronym ironic re-analyzer and generator called HAHAcronym.
Thissystem both makes fun of existing acronyms, andproduces funny acronyms that are constrained to bewords of the given language by starting from con-cepts provided by users.
HAHAcronym is mainlybased on lexical substitution via semantic field op-position, rhyme, rhythm and semantic relations suchas antonyms retrieved from WordNet (Stark andRiesenfeld, 1998) for adjectives.As more na?
?ve solutions, automatic name gener-ators can be used as a source of inspiration in thebrainstorming phase to get ideas for good names.As an example, www.business-name-generators.com randomly combines abbreviations, syllables andgeneric short words from different domains to ob-tain creative combinations.
The domain genera-tor on www.namestation.com randomly generatesname ideas and available domains based on allit-erations, compound words and custom word lists.Users can determine the prefix and suffix of thenames to be generated.
The brand name generatoron www.netsubstance.com takes keywords as in-puts and here users can configure the percentage ofthe shifting of keyword letters.
Lastly, the mecha-nism of www.naming.net is based on name combi-nations among common words, Greek and Latin pre-fixes, suffixes and roots, beginning and ending wordparts and rhymes.
A shortcoming of these kinds ofautomatic generators is that random generation canoutput so many bad suggestions and users have to bepatient to find the name that they are looking for.
Inaddition, these generations are based on straightfor-ward combinations of words and they do not includea mechanism to also take semantics into account.2.3 CommercialMany naming agencies and branding firms1 provideprofessional service to aid with the naming of new1e.g.
www.eatmywords.com, www.designbridge.com, www.ahundredmonkeys.com704products, domains, companies and brands.
Such ser-vices generally require customers to provide briefinformation about the business to be named, fill inquestionnaires to learn about their markets, competi-tors, and expectations.
In the end, they present a listof name candidates to be chosen from.
Although theresulting names can be successful and satisfactory,these services are very expensive and the processingtime is rather long.3 Dataset and AnnotationIn order to create a gold standard for linguistic cre-ativity in naming, collect the common creativity de-vices used in the naming process and determine thesuitable ones for automation, we conducted an an-notation task on a dataset of 1000 brand and com-pany names from various domains (O?zbal et al,2012).
These names were compiled from a bookdedicated to brand naming strategies (Botton andCegarra, 1990) and various web resources relatedto creative naming such as adslogans.co.uk andbrandsandtags.com.Our list contains names which were invented viavarious creativity methods.
While the creativity insome of these names is independent of the contextand the names themselves are sufficient to realize themethods used (e.g.
alliteration in Peak Performance,modification of one letter in Vimeo), for some ofthem the context information such as the descriptionof the product or the area of the company is alsonecessary to fully understand the methods used.
Forinstance, Thanks a Latte is a coffee bar name wherethe phonetic similarity between ?lot?
and ?latte?
(acoffee type meaning ?milk?
in Italian) is exploited.The name Caterpillar, which is an earth-movingequipment company, is used as a metaphor.
There-fore, we need extra information regarding the do-main description in addition to the names.
Accord-ingly, while building our dataset, we conducted twoseparate branches of annotation.
The first branch re-quired the annotators to fill in the domain descrip-tion of the names in question together with their et-ymologies if required, while the second asked themto determine the devices of creativity used in eachname.In order to obtain the list of creativity devices, wecollected a total of 31 attributes used in the namingprocess from various resources including academicpapers, naming agents, branding and advertisementexperts.
To facilitate the task for the annotators,we subsumed the most similar attributes when re-quired.
Adopting the four-fold linguistic topologysuggested by Bergh et al (B. V. Bergh, 1987), wemapped these attributes into phonetic, orthographic,morphological and semantic categories.
The pho-netic category includes attributes such as rhyme (i.e.repetition of similar sounds in two or more words- e.g.
Etch-a-sketch) and reduplication (i.e.
repeat-ing the root or stem of a word or part of it exactlyor with a slight change - e.g.
Teenie Weenie), whilethe orthographic category consists of devices such asacronyms (e.g.
BMW) and palindromes (i.e.
words,phrases, numbers that can be read the same way ineither direction e.g.
Honda ?Civic?).
The third cat-egory is the morphology which contains affixation(i.e.
forming different words by adding morphemesat the beginning, middle or end of words - e.g.Nutella) and blending (i.e.
forming a word by blend-ing sounds from two or more distinct words andcombining their meanings - e.g.
Wikipedia by blend-ing ?Wiki?
and ?encyclopedia?).
Finally, the seman-tic category includes attributes such as metaphors(i.e.
Expressing an idea through the image of anotherobject - e.g.
Virgin) and punning (i.e.
using a wordin different senses or words with sound similarity toachieve specific effect such as humor - e.g.
Thai MeUp for a Thai restaurant).4 System DescriptionThe resource that we have obtained after the anno-tation task provides us with a starting point to studyand try to replicate the linguistic and cognitive pro-cesses behind the creation of a successful name.
Ac-cordingly, we have made a systematic attempt toreplicate these processes, and implemented a systemwhich combines methods and resources used in var-ious areas of Natural Language Processing (NLP) tocreate neologisms based on homophonic puns andmetaphors.
While the variety of creativity devicesis actually much bigger, our work can be consid-ered as a starting point to investigate which kinds oftechnologies can successfully be exploited in whichway to support the naming process.
The task that wedeal with requires: 1) reasoning of relations betweenentities and concepts; 2) understanding the desiredproperties of entities determined by users; 3) identi-fying semantically related terms which are also con-sistent with the objectives of the advertisement; 4)finding terms which are suitable metaphors for theproperties that need to be emphasized; 5) reasoning705about phonetic properties of words; 6) combiningall this information to create natural sounding neol-ogisms.In this section, we will describe in detail the workflow of the system that we have designed and imple-mented to fulfill these requirements.4.1 Specifying the category and propertiesOur design allows users to determine the categoryof the product/brand/company to be advertised (e.g.shampoo, car, chocolate) optionally together withthe properties (e.g.
softening, comfortable, addic-tive) that they want to emphasize.
In the currentimplementation, categories are required to be nounswhile properties are required to be adjectives.
Theseinputs that are specified by users constitute the mainingredients of the naming process.
After the de-termination of these ingredients, several techniquesand resources are utilized to enlarge the ingredientlist, and thereby to increase the variety of new andcreative names.4.2 Adding common sense knowledgeAfter the word defining the category is determinedby the user, we need to automatically retrieve moreinformation about this word.
For instance, if the cat-egory has been determined as ?shampoo?, we needto learn that ?it is used for washing hair?
or ?itcan be found in the bathroom?, so that all this ex-tra information can be included in the naming pro-cess.
To achieve that, we use ConceptNet (Liu andSingh, 2004), which is a semantic network contain-ing common sense, cultural and scientific knowl-edge.
This resource consists of nodes representingconcepts which are in the form of words or shortphrases of natural language, and labeled relationsbetween them.ConceptNet has a closed class of relations ex-pressing connections between concepts.
After theanalysis of these relations according to the require-ments of the task, we have decided to use the oneslisted in Table 1 together with their description inthe second column.
The third column states whetherthe category word should be the first or second ar-gument of the relation in order for us to considerthe new word that we discover with that relation.Since, for instance, the relations MadeOf(milk, *)and MadeOf(*, milk) can be used for different goals(the former to obtain the ingredients of milk, andthe latter to obtain products containing milk), weRelation Description # POSHasA What does it possess?
1 nPartOf What is it part of?
2 nUsedFor What do you use it for?
1 n,vAtLocation Where would you find it?
2 nMadeOf What is it made of 1 nCreatedBy How do you bring it into existence?
1 nHasSubevent What do you do to accomplish it?
2 vCauses What does it make happen?
1 n,vDesires What does it want?
1 n,vCausesDesire What does it make you want to do?
1 n,vHasProperty What properties does it have?
1 aReceivesAction What can you do to it?
1 vTable 1: ConceptNet relations.need to make this differentiation.
Via ConceptNet 5,the latest version of ConceptNet, we obtain a list ofrelations such as AtLocation(shampoo, bathroom),UsedFor(shampoo, clean) and MadeOf(shampoo,perfume) with the query word ?shampoo?.
We addall the words appearing in relations with the categoryword to our ingredient list.
Among these new words,multiwords are filtered out since most of them arenoisy and for our task a high precision is more im-portant than a high recall.Since sense information is not provided, one ofthe major problems in utilizing ConceptNet is thedifficulty in disambiguating the concepts.
In ourcurrent design, we only consider the most commonsenses of words.
As another problem, the part-of-speech (POS) information is not available in Con-ceptNet.
To handle this problem, we have deter-mined the required POS tags of the new words thatcan be obtained from the relations with an additionalgoal of filtering out the noise.
These tags are statedin the fourth column of Table 1.4.3 Adding semantically related wordsTo further increase the size of the ingredient list,we utilize another resource called WordNet (Miller,1995), which is a large lexical database for English.In WordNet, nouns, verbs, adjectives and adverbsare grouped into sets of cognitive synonyms calledsynsets.
Each synset in WordNet expresses a dif-ferent concept and they are connected to each otherwith lexical, semantic and conceptual relations.We use the direct hypernym relation of WordNetto retrieve the superordinates of the category word(e.g.
cleansing agent, cleanser and cleaner for thecategory word shampoo).
We prefer to use this re-lation of WordNet instead of the relation ?IsA?
in706ConceptNet to avoid getting too general words.
Al-though we can obtain only the direct hypernyms inWordNet, no such mechanism exists in ConceptNet.In addition, while WordNet has been built by lin-guists, ConceptNet is built from the contributions ofmany thousands of people across the Web and natu-rally it also contains a lot of noise.In addition to the direct hypernyms of the cate-gory word, we increase the size of the ingredient listby adding synonyms of the category word, the newwords coming from the relations and the propertiesdetermined by the user.It should be noted that we do not consider anyother statistical or knowledge based techniques forsemantic relatedness.
Although they would allow usto discover more concepts, it is difficult to under-stand if and how these concepts pertain to the con-text.
In WordNet we can decide what relations toexplore, with the result of a more precise processwith possibly less recall.4.4 Retrieving metaphorsA metaphor is a figure of speech in which an impliedcomparison is made to indicate how two things thatare not alike in most ways are similar in one impor-tant way.
Metaphors are common devices for evo-cation, which has been found to be a very importanttechnique used in naming according to the analysisof our dataset.In order to generate metaphors, we start with theset of properties determined by the user and adopta similar technique to the one proposed by (Veale,2011).
In this work, to metaphorically ascribe aproperty to a term, stereotypes for which the prop-erty is culturally salient are intersected with stereo-types to which the term is pragmatically compara-ble.
The stereotypes for a property are found byquerying on the web with the simile pattern ?as?property?
as *?.
Unlike the proposed approach,we do not apply any intersection with comparablestereotypes since the naming task should favor fur-ther terms to the category word in order to exagger-ate, to evoke and thereby to be more effective.The first constituent of our approach uses thepattern ?as ?property?
as *?
with the addition of??property?
like *?, which is another importantblock for building similes.
Given a property, thesepatterns are harnessed to make queries through theweb api of Google Suggest.
This service performsauto-completion of search queries based on popu-lar searches.
Although top 10 (or fewer) sugges-tions are provided for any query term by GoogleSuggest, we expand these sets by adding each let-ter of the alphabet at the end of the provided phrase.Thereby, we obtain 10 more suggestions for each ofthese queries.
Among the metaphor candidates thatwe obtain, we filter out multiwords to avoid noise asmuch as possible.
Afterwards, we conduct a lemma-tization process on the rest of the candidates.
Fromthe list of lemmas, we only consider the ones whichappear in WordNet as a noun.
Although the listthat we obtain in the end has many potentially valu-able metaphors (e.g.
sun, diamond, star, neon forthe property bright), it also contains a lot of uncom-mon and unrelated words (e.g.
downlaod, myspace,house).
Therefore, we need a filtering mechanism toremove the noise and keep only the best metaphors.To achieve that, the second constituent of themetaphor retrieval mechanism makes a query inConceptNet with the given property.
Then, all thenouns coming from the relations in the form ofHasProperty(*, property) are collected to find wordshaving that property.
The POS check to obtain onlynouns is conducted with a look-up in WordNet asbefore.
It should be noted that this technique wouldnot be enough to retrieve metaphors alone since itcan also return noise (e.g.
blouse, idea, color, home-schooler for the property bright).After we obtain two different lists of metaphorcandidates with the two mechanisms mentionedabove, we take the intersection of these lists andconsider only the words appearing in both lists asmetaphors.
In this manner, we aim to remove thenoise coming from each list and obtain more reli-able metaphors.
To illustrate, for the same exampleproperty bright, the metaphors obtained at the endof the process are sun, light and day.4.5 Generating neologismsAfter the ingredient list is complete, the phoneticmodule analyzes all ingredient pairs to generate ne-ologisms with possibly homophonic puns based onphonetic similarity.To retrieve the pronunciation of the ingredients,we utilize the CMU Pronouncing Dictionary (Lenzo,2007).
This resource is a machine-readable pro-nunciation dictionary of English which is suitablefor uses in speech technology, and it contains over125,000 words together with their transcriptions.
Ithas mappings from words to their pronunciations707Input Successful output Unsuccessful outputCategory Properties Word Ingredients Word Ingredientsbarirish lively wooden traditionalwarm hospitable friendlybeertender bartender, beer barkplace workplace, barbarty party, bar barl girl, barginess guinness, gin bark work, barperfumeattractive strong intoxicatingunforgettable feminine mysticsexy audacious provocativemysticious mysterious, mystic provocadeepe provocative, deepbussling buss, puzzlingmysteelious mysterious, steelsunglassescool elite though authenticcheap sportyspectacools spectacles, cool spocleang sporting, cleanelectacles spectacles, electpolarice polarize, icerestaurantwarm elegant friendly originalitalian tasty cozy moderneatalian italian, eat dusta pasta, dustpastarant restaurant, pasta hometess hostess, homepeatza pizza, eatshampoosmooth bright soft volumizinghydrating qualityfragrinse fragrance, rinse furl girl, furcleansun cleanser, sun sasun satin, sunTable 2: A selection of succesful and unsuccessful neologisms generated by the model.and the current phoneme set contains 39 phonemesbased on the ARPAbet symbol set, which has beendeveloped for speech recognition uses.
We con-ducted a mapping from the ARPAbet phonemes tothe international phonetic alphabet (IPA) phonemesand we grouped the IPA phonemes based on thephoneme classification documented in IPA.
Morespecifically, we grouped the ones which appear inthe same category such as p-b, t-d and s-z for theconsonants; i-y and e-?
for the vowels.After having the pronunciation of each word inthe ingredient list, shorter pronunciation strings arecompared against the substrings of longer ones.Among the different possible distance metrics thatcan be applied for calculating the phonetic similaritybetween two pronunciation strings, we have chosenthe Levenshtein distance (Levenshtein, 1966).
Thisdistance is a metric for measuring the amount of dif-ference between two sequences, defined as the min-imum number of edits required for the transforma-tion of one sequence into the other.
The allowableedit operations for this transformation are insertion,deletion, or substitution of a single character.
For ex-ample, the Levenshtein distance between the strings?kitten?
and ?sitting?
is 3, since the following threeedits change one into the other, and there is no wayto do it with fewer than three edits: kitten?
sitten(substitution of ?k?
with ?s?
), sitten?
sittin (substi-tution of ?e?
with ?i?
), sittin ?
sitting (insertion of?g?
at the end).
For the distance calculation, we em-ploy relaxation by giving a smaller penalty for thephonemes appearing in the same phoneme groupsmentioned previously.
We normalize each distanceby the length of the pronunciation string consideredfor the distance calculation and we only allow thecombination of word pairs that have a normalizeddistance score less than 0.5, which was set empiri-cally.Since there is no one-to-one relationship betweenletters and phonemes and no information aboutwhich phoneme is related to which letter(s) is avail-able, it is not straightforward to combine two wordsafter determining the pairs via Levenshtein distancecalculation.
To solve this issue, we use the Berke-ley word aligner2 for the alignment of letters andphonemes.
The Berkeley Word Aligner is a sta-tistical machine translation tool that automaticallyaligns words in a sentence-aligned parallel corpus.To adapt this tool according to our needs, we splitall the words in our dictionary into letters and theirmapped pronunciation to their phonemes, so that thealigner could learn a mapping from phonemes tocharacters.
The resulting alignment provides the in-formation about from which index to which indexthe replacement of the substring of a word shouldoccur.
Accordingly, the substring of the word whichhas a high phonetic similarity with a specific wordis replaced with that word.
As an example, if thefirst ingredient is bright and the second ingredient islight, the name blight can be obtained at the end of2http://code.google.com/p/berkeleyaligner/708this process.4.6 Checking phonetic likelihoodTo check the likelihood and well-formedness of thenew string after the replacement, we learn a 3-gramlanguage model with absolute smoothing.
For learn-ing the language model, we only consider the wordsin the CMU pronunciation dictionary which also ex-ist in WordNet.
This filtering is required in orderto eliminate a large number of non-English trigramswhich would otherwise cause too high probabilitiesto be assigned to very unlikely sequences of charac-ters.
We remove the words containing at least onetrigram which is very unlikely according to the lan-guage model.
The threshold to determine the un-likely words is set to the probability of the least fre-quent trigram observed in the training data.5 EvaluationWe evaluated the performance of our system witha manual annotation in which 5 annotators judgeda set of neologisms along 4 dimensions: 1) appro-priateness, i.e.
the number of ingredients (0, 1 or2) used to generate the neologism which are appro-priate for the input; 2) pleasantness, i.e.
a binary de-cision concerning the conformance of the neologismto the sound patterns of English; 3) humor/wittiness,i.e.
a binary decision concerning the wittiness of theneologism; 4) success, i.e.
an assessment of the fit-ness of the neologism as a name for the target cate-gory/properties (unsuccessful, neutral, successful).To create the dataset, we first compiled a listof 50 categories by selecting 50 hyponyms of thesynset consumer goods in WordNet.
To determinethe properties to be underlined, we asked two anno-tators to state the properties that they would expectto have in a product or company belonging to eachcategory in our category list.
Then, we merged theanswers coming from the two annotators to createthe final set of properties for each category.Although our system is actually able to producea limitless number of results for a given input, welimited the number of outputs for each input toreduce the effort required for the annotation task.Therefore, we implemented a ranking mechanismwhich used a hybrid scoring method by giving equalweights to the language model and the normalizedphonetic similarity.
Among the ranked neologismsfor each input, we only selected the top 20 to buildthe dataset.
It should be noted that for some inputDimensionAPP PLE HUM SUX2 9.54 0 0 27.043 33.3 25.34 32.77 49.524 41.68 38.6 34.57 18.775 15.48 36.06 32.66 4.673+ 90.46 100 100 72.96Table 3: Inter-annotator agreement (in terms of majorityclass, MC) on the four annotation dimensions.combinations the system produced less than 20 neol-ogisms.
Accordingly, our dataset consists of a totalnumber of 50 inputs and 943 neologisms.To have a concrete idea about the agreement be-tween annotators, we calculated the majority classfor each dimension.
With 5 annotators, a majorityclass greater than or equal to 3 means that the abso-lute majority of the annotators agreed on the samedecision.
Table 3 shows the distribution of majorityclasses along the four dimensions of the annotation.For pleasantness (PLE) and humor (HUM), the ab-solute majority of the annotators (i.e.
3/5) agreed onthe same decision in 100% of the cases, while for ap-propriateness (APP) the figure is only slightly lower.Concerning success, arguably the most subjective ofthe four dimensions, in 27% of the cases it is notpossible to take a majority decision.
Nevertheless,in almost 73% of the cases the absolute majority ofthe annotators agreed on the annotation of this di-mension.Table 4 shows the micro and macro-average ofthe percentage of cases in which at least 3 anno-tators have labeled the ingredients as appropriate(APP), and the neologisms as pleasant (PLE), hu-morous (HUM) or successful (SUX).
The system se-lects appropriate ingredients in approximately 60%of the cases, and outputs pleasant, English-soundingnames in ?87% of the cases.
Almost one name outof four is labeled as successful by the majority of theannotators, which we regard as a very positive resultconsidering the difficulty of the task.
Even thoughwe do not explicitly try to inject humor in the neol-ogisms, more than 15% of the generated names turnout to be witty or amusing.
The system managed togenerate at least one successful name for all 50 inputcategories and at least one witty name for 42.
As ex-pected, we found out that there is a very high corre-lation (91.56%) between the appropriateness of the709DimensionAccuracy APP PLE HUM SUXmicro 59.60 87.49 16.33 23.86macro 60.76 87.01 15.86 24.18Table 4: Accuracy of the generation process along thefour dimensions.ingredients and the success of the name.
A success-ful name is also humorous in 42.67% of the cases,while 62.34% of the humorous names are labeled assuccessful.
This finding confirms our intuition thatamusing names have the potential to be very appeal-ing to the customers.
In more than 76% of the cases,a humorous name is the product of the combinationof appropriate ingredients.In Table 2, we show a selection of successfuland unsuccessful outputs generated for the categoryand the set of properties listed under the block ofcolumns labeled as Input according to the majorityof annotators (i.e.
3 or more).
As an example of pos-itive outcomes, we can focus on the columns underSuccessful output for the input target word restau-rant.
The model correctly selects the ingredientseat (a restaurant is UsedFor eating), pizza and pasta(which are found AtLocation restaurant) to generatean appropriate name.
The three ?palatable?
neolo-gisms generated are eatalian (from the combinationof eat and Italian), pastarant (pasta + restaurant)and peatza (pizza + eat).
These three suggestions areamusing and have a nice ring to them.
As a matterof fact, it turns out that the name Eatalian is actuallyused by at least one real Italian restaurant located inLos Angeles, CA3.For the same set of stimuli, the model also se-lects some ingredients which are not really relatedto the use-case, e.g., dust and hostess (both of whichcan be found AtLocation restaurant) and home (asynonym for plate, which can be found AtLocationrestaurant, in the baseball jargon).
With these in-gredients, the model produces the suggestion dustawhich sounds nice but has a negative connotation,and hometess which can hardly be associated to theinput category.A rather common class of unsuccessful outputsinclude words that, by pure chance, happen to bealready existing in English.
In these cases, no actualneologism is generated.
Sometimes, the generated3http://www.eataliancafe.com/words have rather unpleasant or irrelevant meanings,as in the case of bark for bar.
Luckily enough, thesekinds of outputs can easily be eliminated by filteringout all the output words which can already be foundin an English dictionary or which are found to havea negative valence with state-of-the-art techniques(e.g.
SentiWordNet (Esuli and Sebastiani, 2006)).Another class of negative results includes neolo-gisms generated from ingredients that the modelcannot combine in a good English-sounding neol-ogism (e.g.
spocleang from sporting and clean forsunglasses or sasun from satin and sun for sham-poo).6 ConclusionIn this paper, we have focused on the task of automa-tizing the naming process and described a computa-tional approach to generate neologisms with homo-phonic puns based on phonetic similarity.
This studyis our first step towards the systematic emulation ofthe various creative devices involved in the namingprocess by means of computational methods.Due to the complexity of the problem, a unifiedmodel to handle all the creative devices at the sametime seems outside the reach of the current state-of-the-art NLP techniques.
Nevertheless, the resourcethat we collected, together with the initial imple-mentation of this model should provide a good start-ing point for other researchers in the area.
We be-lieve that our contribution will motivate other re-search teams to invest more effort in trying to tacklethe related research problems.As future work, we plan to improve the quality ofthe output by considering word sense disambigua-tion techniques to reduce the effect of inappropriateingredients.
We also want to extend the model to in-clude multiword ingredients and to generate not onlywords but also short phrases.
Then, we would liketo focus on other classes of creative devices, suchas affixation or rhyming.
Lastly, we plan to makethe system that we have developed publicly avail-able and collect user feedback for further develop-ment and improvement.AcknowledgmentsThe authors were partially supported by a GoogleResearch Award.710ReferencesL.
Oliver B. V. Bergh, K. Adler.
1987.
Linguistic distinc-tion among top brand names.
Journal of AdvertisingResearch, pages 39?44.Yeqing Bao, Alan T Shao, and Drew Rivers.
2008.
Cre-ating new brand names: Effects of relevance, conno-tation, and pronunciation.
Journal of Advertising Re-search, 48(1):148.Marcel Botton and Jean-Jack Cegarra, editors.
1990.
Lenom de marque.
Paris McGraw Hill.2011.
Cognitive tools for successful branding.
AppliedLinguistics, 32:369?388.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sentiword-net: A publicly available lexical resource for opinionmining.
pages 417?422.Kevin Lane Keller.
2003.
Strategic brand management:building, measuring and managing brand equity.
NewJersey: Prentice Hall.Richard R. Klink.
2000.
Creating brand names withmeaning: The use of sound symbolism.
MarketingLetters, 11(1):5?20.C Kohli, K Harich, and Lance Leuthesser.
2005.
Creat-ing brand identity: a study of evaluation of new brandnames.
Journal of Business Research, 58(11):1506?1515.John J. Ohala Leanne Hinton, Johanna Nichols.
2006.Sound Symbolism.
Cambridge University Press.Kevin Lenzo.
2007.
The cmu pronouncing dictionary.http://www.speech.cs.cmu.edu/cgi-bin/cmudict.V.
Levenshtein.
1966.
Binary codes capable of correct-ing deletions, insertions, and reversals.
Soviet PhysicsDoklady, 10:707?710.H.
Liu and P. Singh.
2004.
Conceptnet ?
a practi-cal commonsense reasoning tool-kit.
BT TechnologyJournal, 22(4):211?226.George A. Miller.
1995.
Wordnet: A lexical database forenglish.
Communications of the ACM, 38:39?41.Go?zde O?zbal, Carlo Strapparava, and Marco Guerini.2012.
Brand Pitt: A corpus to explore the art of nam-ing.
In Proceedings of the eighth international confer-ence on Language Resources and Evaluation (LREC-2012), Istanbul, Turkey, May.Michael M. Stark and Richard F. Riesenfeld.
1998.Wordnet: An electronic lexical database.
In Proceed-ings of 11th Eurographics Workshop on Rendering.MIT Press.Oliviero Stock and Carlo Strapparava.
2006.
Laughingwith HAHAcronym, a computational humor system.In proceedings of the 21st national conference on Arti-ficial intelligence - Volume 2, pages 1675?1678.
AAAIPress.Tony Veale.
2011.
Creative language retrieval: A robusthybrid of information retrieval and linguistic creativ-ity.
In Proceedings of ACL 2011, Portland, Oregon,USA, June.711
