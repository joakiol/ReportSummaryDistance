Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 394?397,Prague, June 2007. c?2007 Association for Computational LinguisticsUMND2 : SenseClusters Applied to theSense Induction Task of SENSEVAL-4Ted PedersenDepartment of Computer ScienceUniversity of MinnesotaDuluth, MN 55812tpederse@d.umn.eduhttp://senseclusters.sourceforge.netAbstractSenseClusters is a freely?available open?source system that served as the Univer-sity of Minnesota, Duluth entry in theSENSEVAL-4 sense induction task.
For thistask SenseClusters was configured to con-struct representations of the instances to beclustered using the centroid of word co-occurrence vectors that replace the wordsin an instance.
These instances are thenclustered using k?means where the numberof clusters is discovered automatically usingthe Adapted Gap Statistic.
In these experi-ments SenseClusters did not use any infor-mation outside of the raw untagged text thatwas to be clustered, and no tuning of the sys-tem was performed using external corpora.1 IntroductionThe object of the sense induction task ofSENSEVAL-4 (Agirre and Soroa, 2007) was tocluster 27,132 instances of 100 different words(35 nouns and 65 verbs) into senses or classes.The task data consisted of the combination of thetest and training data (minus the sense tags) fromthe English lexical sample task.
Each instance isa context of several sentences which contains anoccurrence of a given word that serves as the targetof sense induction.SenseClusters is based on the presumption thatwords that occur in similar contexts will have similarmeanings.
This intuition has been presented as boththe Distributional Hypothesis (Harris, 1968) and theStrong Contextual Hypothesis (Miller and Charles,1991).SenseClusters has been in active development atthe University of Minnesota, Duluth since 2002.
It isan open?source project that is freely?available fromsourceforge, and has been been described in detailin numerous publications (e.g., (Purandare and Ped-ersen, 2004), (Pedersen et al, 2005), (Pedersen andKulkarni, 2007)).SenseClusters supports a variety of techniques forselecting lexical features, representing contexts tobe clustered, determining the appropriate number ofcluster automatically, clustering, labeling of clus-ters, and evaluating cluster quality.
The configu-ration used in SENSEVAL-4 was just one possiblecombination of these techniques.2 Methodology in Sense Induction TaskFor this task, SenseClusters represents the instancesto be clustered using second order co?occurrencevectors.
These are constructed by first identifyingword co?occurrences, and then replacing each wordin an instance to be clustered with its co-occurrencevector.
Then all the vectors that make up an instanceare averaged together to represent that instance.A co?occurrence matrix is constructed by identi-fying bigrams that occur in the contexts to be clus-tered two or more times and have a Pointwise Mu-tual Information (PMI) score greater than five.
If thevalue of PMI is near 1.0, this means that the words inthe bigram occur together approximately the num-ber of times expected by chance, and they are notstrongly associated.
If this value is greater than 1,then the words in the bigram are occurring more of-394ten than expected by chance, and they are thereforeassociated.The rows of the co?occurrence matrix representthe first word in the selected bigrams, and thecolumns represent the second word.
A window sizeof 12 is allowed, which means that up to 10 inter-vening words can be observed between the pair ofwords in the bigram.
This rather large window sizewas employed since the sample sizes for each wordwere relatively small, often no more than a few hun-dred instances.A stop list was used to eliminate bigrams whereeither word is a high?frequency low?content word.The particular list used is distributed with the NgramStatistics Package and is loosely based on theSMART stop list.
It consists of 295 words; in addi-tion, all punctuation, single letter words, and num-bers (with the exception of years) were eliminated.Each of the contexts that contain a particular tar-get word is represented by a single vector that is theaverage (or the centroid) of all the co-occurrencevectors found for the words that make up the con-text.
This results in a context by feature matrix,where the features are the words that occur withthe words in the contexts (i.e., second order co?occurrences).
The k?means algorithm is used forclustering the contexts, where the number of clus-ters is automatically discovered using the AdaptedGap Statistic (Pedersen and Kulkarni, 2006).
Thepremise of this method is to create a randomizedsample of data with the same characteristics of theobserved data (i.e., the contexts to be clustered).This is done by fixing the marginal totals of the con-text by feature matrix and then generating random-ized values that are consistent with those marginaltotals.
This creates a matrix that is can be viewedas being from the same population as the observeddata, except that the data is essentially noise (be-cause it is randomly generated).The randomized data is clustered for successivevalues of k from 1 to some upper limit (the num-ber of contexts or the point at which the criterionfunctions have plateaued).
For each value of k thecriterion function measures the quality of the clus-tering solution.
The same is done for that observeddata, and the difference between the criterion func-tion for the observed data and the randomized datais determined, and the value of k where that differ-ence is largest is selected as the best solution for k,since that is when the clustered data least resemblesnoise, and is therefore the most organized or bestsolution.
In these experiments the criterion functionwas intra-cluster similarity.3 Results and DiscussionThere was an unsupervised and a supervised eval-uation performed in the sense induction task.
Of-ficial scores were reported for 6 participating sys-tems, plus the most frequent sense (MFS) baseline,so rankings (when available) are provided from 1(HIGH) to 7 (LOW).
We also conducted an evalu-ation using the SenseClusters method.3.1 Unsupervised EvaluationThe unsupervised evaluation was based on the tradi-tional clustering measures of F-score, entropy, andpurity.
While the participating systems clustered thefull 27,132 instances, only the 4,581 instance subsetthat corresponds to the English lexical sample eval-uation data was scored in the evaluation.
Table 1shows the averaged F-scores over all 100 words, all35 nouns, and all 65 verbs.In this table the SenseClusters system (UMND2)is compared to the MFS baseline, which is attainedby assigning all the instances of a word to a sin-gle cluster.
We also include several random base-lines, where randomX indicates that one of X pos-sible clusters was randomly assigned to each in-stance of a word.
Thus, approximately 100 ?
Xdistinct clusters are created across the 100 words.The random results are not ranked as they were nota part of the official evaluation.
We also present thehighest (HIGH, rank 1) and lowest (LOW, rank 7)scores from participating systems, to provide pointsof comparison.The randomX baseline is useful in determiningthe sensitivity of the evaluation technique to thenumber of clusters discovered.
The average num-ber of classes in the gold standard test data is 2.9, sorandom3 approximates a system that randomly as-signs the correct number of clusters.
It attains anF-score of 50.0.
Note that random2 performs some-what better (59.7), suggesting that all other thingsbeing equal, the F-score is biased towards methodsthat find a smaller than expected number of clusters.395Table 1: Unsupervised F-Score (test)All Nouns Verbs RankMFS/HIGH 78.9 80.7 76.8 1UMND2 66.1 67.1 65.0 4random2 59.7 60.9 58.4LOW 56.1 65.8 45.1 7random3 50.0 49.9 50.1random4 44.9 44.2 45.7random10 29.7 28.0 31.7random50 17.9 14.9 21.1As the number of random clusters increases the F-score declines sharply, showing that it is highly sen-sitive to the number of clusters discovered, and sig-nificantly penalizes systems that find more clustersthan indicated in the gold standard data.We observed for UMND2 that purity (81.7) isquite a bit higher than the F-score (66.1), and thatit discovered a smaller number of clusters on aver-age (1.4) than exists in the gold standard data (2.9).This shows that while SenseClusters was able to findrelatively pure clusters, it errored in finding too fewclusters, and was therefore penalized to some degreeby the F-score.3.2 Supervised EvaluationA supervised evaluation was also carried out onthe same clustering of the 27,132 instances as wasused in the unsupervised evaluation, following themethod defined in (Agirre et al, 2006).
Here thetrain portion (22,281 instances) is used to learn a ta-ble of probabilities that is used to map discoveredclusters in the test data to gold standard classes.
Thecluster assigned to each instance in the test portion(4,851 instances) is mapped (assigned) to the mostprobable class associated with that cluster as definedby this table.After this transformation is performed, the newlymapped test results are scored using the scorer2 pro-gram, which is the official evaluation program ofthe English lexical sample task and reports the F-measure, which in these experiments is simply ac-curacy since precision and recall are the same.In Table 2 we show the results of the super-vised evaluation, which includes the highest andlowest score from participating systems, as well asTable 2: Supervised Accuracy (test)All Nouns Verbs RankHIGH 81.6 86.8 75.7 1UMND2 80.6 84.5 76.2 2random2 78.9 81.6 75.8MFS 78.7 80.9 76.2 4LOW 78.5 81.4 75.2 7random4 78.4 81.1 75.5random3 78.3 80.5 75.9random10 77.9 79.8 75.8random50 75.6 78.5 72.4UMND2, MFS, and the same randomX baselines asincluded in the unsupervised evaluation.We observed that the difference between the scoreof the best performing system (HIGH) and the ran-dom50 baseline is six points (81.6 - 75.6).
In theunsupervised evaluation of this same data this dif-ference is 61 points (78.9 - 17.9) according to theF-score.The smaller range of values for the supervisedmeasure can be understood by noting that the map-ping operation alters the number and distribution ofclusters as discovered in the test data.
For exam-ple, random3 results in an average of 2.9 clusters perword in the test data, but after mapping the averagenumber of clusters is 1.1.
The average number ofclusters discovered by UMND2 is 1.4, but after map-ping this average is reduced to 1.1.
For random50,the average number of clusters per word is 24.1, butafter mapping is 2.0.
This shows that the super-vised evaluation has a tendency to converge uponthe MFS, which corresponds to assigning 1 clusterper word.When looking at the randomX results in the su-pervised evaluation, it appears that this method doesnot penalize systems for getting the number of clus-ters incorrect (as the F-score does).
This is shown bythe very similar results for the randomX baselines,where the only difference in their results is the num-ber of clusters.
This lack of a penalty is due to thefact that the mapping operation takes a potentiallylarge number of clusters and maps them to relativelyfew classes (e.g., random50) and then performs theevaluation.3963.3 SenseClusters Evaluation (F-Measure)An evaluation was carried out on the full 27,132instance train+test data set using the SenseClustersevaluation methodology, which was first defined in(Pedersen and Bruce, 1997).
This corresponds toan unsupervised version of the F-measure, whichin these experiments can be viewed as an accuracymeasure since precision and recall are the same (asis the case for the supervised measure).It aligns discovered clusters with classes such thattheir agreement is maximized.
The clusters andclasses must be aligned one to one, so a large penaltycan result if the number of discovered clusters dif-fers from the number of gold standard classes.1For UMND2, there were 145 discovered clustersand 368 gold standard classes.
Due to the one toone alignment that is required, the 145 discoveredclusters were aligned with 145 gold standard classessuch that there was agreement for 15,291 of 27,132instances, leading to an F-measure (accuracy) of56.36 percent.
Note that this is significantly lowerthan the F-score of UMND2 for the train+test data,which was 63.1.
This illustrates that the SenseClus-ters F-measure and the F-score are not equivalent.4 ConclusionsOne of the strengths of SenseClusters (UMND2) isthat it is able to automatically identify the number ofclusters without any manual intervention or settingof parameters.
In these experiments the AdaptedGap statistic was quite conservative, only discover-ing on average 1.4 classs per word, where the ac-tual number of classes in the gold standard data was2.9.
However, this is a reasonable result, since formany words there were just a few hundred instances.Also, the gold standard class distinctions were heav-ily skewed, with the majority sense occurring 80%of the time on average.
Under such conditions,there may not be sufficient information available foran unsupervised clustering algorithm to make finegrained distinctions, and so discovering one clusterfor a word may be a better course of action that mak-ing divisions that are not well supported by the data.1An implementation of this measure is available in theSenseClusters system, or by contacting the author.5 AcknowledgmentsThese experiments were conducted with version0.95 of the SenseClusters system.
Many thanks toAmruta Purandare and Anagha Kulkarni for theirinvaluable work on this and previous versions ofSenseClusters.This research was partially supported by the Na-tional Science Foundation Faculty Early Career De-velopment (CAREER) Program (#0092784).ReferencesE.
Agirre and A. Soroa.
2007.
Semeval-2007 task 2:Evaluating word sense induction and discriminationsystems.
In Proceedings of SemEval-2007: 4th Inter-national Workshop on Semantic Evaluations, June.E.
Agirre, D.
Mart?
?nez, O.
Lo?pez de Lacalle, andA.
Soroa.
2006.
Two graph-based algorithms forstate-of-the-art wsd.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 585?593, Sydney, Australia, July.Z.
Harris.
1968.
Mathematical Structures of Language.Wiley, New York.G.A.
Miller and W.G.
Charles.
1991.
Contextual corre-lates of semantic similarity.
Language and CognitiveProcesses, 6(1):1?28.T.
Pedersen and R. Bruce.
1997.
Distinguishing wordsenses in untagged text.
In Proceedings of the Sec-ond Conference on Empirical Methods in Natural Lan-guage Processing, pages 197?207, Providence, RI,August.T.
Pedersen and A. Kulkarni.
2006.
Automatic clusterstopping with criterion functions and the Gap Statistic.In Proceedings of the Demo Session of HLT/NAACL,pages 276?279, New York City, June.T.
Pedersen and A. Kulkarni.
2007.
Unsupervised dis-crimination of person names in web contexts.
In Pro-ceedings of the Eighth International Conference on In-telligent Text Processing and Computational Linguis-tics, pages 299?310, Mexico City, February.T.
Pedersen, A. Purandare, and A. Kulkarni.
2005.
Namediscrimination by clustering similar contexts.
In Pro-ceedings of the Sixth International Conference on In-telligent Text Processing and Computational Linguis-tics, pages 220?231, Mexico City, February.A.
Purandare and T. Pedersen.
2004.
Word sensediscrimination by clustering contexts in vector andsimilarity spaces.
In Proceedings of the Conferenceon Computational Natural Language Learning, pages41?48, Boston, MA.397
