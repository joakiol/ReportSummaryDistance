Proceedings of the Workshop on BioNLP, pages 144?152,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsExploring Two Biomedical Text Genres for Disease RecognitionAur?lie N?v?ol, Won Kim, W. John Wilbur, Zhiyong Lu*National Center for Biotechnology InformationU.S.
National Library of MedicineBethesda, MD 20894, USA{neveola,wonkim,wilbur,luzh}@ncbi.nlm.nih.govAbstractIn the framework of contextual informationretrieval in the biomedical domain, this paperreports on the automatic detection of diseaseconcepts in two genres of biomedical text:sentences from the literature and PubMed userqueries.
A statistical model and a NaturalLanguage Processing algorithm for diseaserecognition were applied on both corpora.While both methods show good performance(F=77% vs. F=76%) on the sentence corpus,results on the query corpus indicate that thestatistical model is more robust (F=74% vs.F=70%).1 IntroductionContextual Information Retrieval (IR) is makinguse of additional information or assumptions aboutthe users?
needs beyond the obvious intent of thequery.
IR systems need to go beyond the task ofproviding generally relevant information by assist-ing users in finding information that is relevant tothem and their specific needs at the time of thesearch.
A practical example of a Google contextualIR feature is when the search engine returns a mapshowing restaurant locations to a user entering aquery such as ?Paris restaurants.
?The contextual aspects of a user?s search weredefined for example by Saracevic (1997) who dis-cussed integrating the cognitive, affective, and sit-uational levels of human computer interaction inIR systems.
Other research efforts studied users?search behavior based on their level of domainknowledge (Zhang et al, 2005) or aimed at  mod-eling users?
interests and search habits (Rose andLevinson, 2004; Teevan et al, 2005).Information about the search context may besought explicitly from the user through profiling orrelevance feedback (Shen et al, 2005).
Recentwork also exploited query log analysis and basiccomputer environment information (Wen et al2004), which involve no explicit interaction withthe user.
In adaptive information retrieval, contextinformation is inferred based on query analysis andcollection characteristics (Bai and Nie 2008).In the biomedical domain, a need for contextualinformation retrieval was identified in particularfor clinical queries submitted to PubMed (Pratt andWasserman, 2000).
Building on the idea that a spe-cific type of document is required for searches witha ?clinical?
context, the PubMed Clinical Queriesportal was developed (Haynes and Wilczynski,2004).
A perhaps more prominent contextual fea-ture of PubMed is the ?citation sensor?, whichidentifies queries classified by Rose and Levinsonas reflecting a ?Navigational?
or ?Obtain resource?goal.
For example, the citation sensor will identifyand retrieve a specific citation if the user enters thearticle title as the query.
The analysis of Entrezlogs shows that MEDLINE is the most populardatabase among the 30 or so databases maintainedby the National Center for Biotechnology Informa-tion (NCBI) as it receives most of Entrez traffic.This suggests that there is a need to complementthe information retrieved from MEDLINE by giv-ing contextual access to other NCBI resources re-144levant to users?
queries, such as Entrez Gene, Clin-ical Q&A or BookShelf.
In addition, the NLM es-timated that about 1/3 of PubMed users are notbiomedical professionals.
In this light, providingan access point to consumer information such asthe Genetics Home Reference might also be useful.To achieve this, the sensor project was recentlylaunched with the goal of recognizing a variety ofbiomedical concepts (e.g.
gene, protein and drugnames) in PubMed queries.
These high-level con-cepts will help characterize users?
search context inorder to provide them with information related totheir need beyond PubMed.
For instance, if a userquery contains the drug name ?Lipitor?, it will berecognized by the drug sensor and additional in-formation on this drug from Clinical Q&A will beshown in the side bar in addition to defaultPubMed results.
Since disease names are commonin PubMed queries, the goal of this work is to in-vestigate and benchmark computational techniquesfor automatic disease name recognition as an aid toimplementing PubMed search contexts.2 Related WorkDespite a significant body of literature in biomedi-cal named entity recognition, most work has beenfocused on gene, protein, drug and chemical namesthrough challenges such as BioCreAtIvE1 or theTREC Genomics/Chemical tracks (Park and Kim,2006).
Other work addressed the identification of?medical problems?
in clinical text (Aronson et al2007; Meystre and Haug, 2005).
This task was thetopic of a Medical NLP challenge2, which releaseda corpus of anonymized radiography reports anno-tated with ICD9 codes.
Although there is someinterest in the biomedical community in the identi-fication of disease names and more specifically theidentification of relationships between diseases andgenes or proteins (Rindflesh and Fizman, 2003),there are very few resources available to train orevaluate automatic disease recognition systems.
Tothe best of our knowledge, the only publicly avail-able corpus for disease identification in the litera-ture was developed by Jimeno et al (2008).
Theauthors annotated 551 MEDLINE sentences withUMLS concepts and used this dataset to bench-mark three different automatic methods for diseasename recognition.
A MEDLINE corpus annotated1 http://biocreative.sourceforge.net/2 http://www.computationalmedicine.org/challenge/index.phpwith ?malignancy?
mentions and part-of-speechtags is also available (Jin et al 2006).
This corpusis targeted to a very restricted type of diseases.
Theannotations are also domain specific, so that ?can-cer of the lung?
is not considered a malignancymention but a mention of malignancy and a men-tion of malignancy location.As in previous studies, we aim to investigate thecomplexity of automatic disease recognition usingstate-of-the-art computational techniques.
Thiswork is novel in at least three aspects: first, in ad-dition to using the MEDLINE sentence corpus(Jimeno et al2008), we developed a new corpuscomprising disease annotations on 500 randomlyselected PubMed queries.
This allowed us to inves-tigate the influence of local context3 through thecomparison of system performance between twodifferent genres of biomedical text.
Second, byusing a knowledge based tool previously ben-chmarked on the same MEDLINE corpus (Jimenoet al 2008), we show that significant performancedifferences can be observed when parameters areadjusted.
Finally, a state-of-the-art statistical ap-proach was adapted for disease name recognitionand evaluated on both corpora.3 Two Biomedical Corpora with diseaseannotationsThe first issue in the development of such a corpusis to define the very concept of disease.
Among thenumerous terminological resources available, suchas Medical Subject Headings (MeSH?, 4,354 dis-ease concepts) or the International Classification ofDiseases (ICD9, ~18,000 disease concepts), theUMLS Metathesaurus?
is the most comprehensive:the 2008AB release includes 252,284 concepts inthe disorder Semantic Group defined by McCrayet al (2001).
The UMLS Metathesaurus is part ofthe Semantic Network, which also includes a set ofbroad subject categories, or Semantic Types, thatprovide a consistent categorization of all conceptsrepresented in the Metathesaurus.
The SemanticGroups aim at providing an even broader categori-zation for UMLS concepts.
For example, the dis-order Semantic Group comprises 12 SemanticTypes including Disease or Syndrome, Cell or Mo-lecular Dysfunction and Congenital Abnormalities.3 Here, by context, we mean the information surrounding adisease mention available in the corpora.
This is different fromthe ?search context?
previously discussed.145Furthermore, like the gene mention (Morgan etal.
2008) and gene normalization (Smith et al2008) tasks in BioCreative II, the task of diseasename recognition can also be performed at twodifferent levels:1. disease mention: the detection of a snippetof text that refers to a disease concept (e.g.?alzheimer?
in the sample query shown inTable 2)2. disease concept: the recognition of a con-trolled vocabulary disease concept (e.g.
?C0002395-alzheimer?s disease?
in our Ta-ble 2 example) in text.In this work, we evaluate and report system per-formance at the concept level.3.1 Biomedical literature corpusSentence Kniest dysplasia is a moderatelysevere chondrodysplasia pheno-type that results from mutationsin the gene for type ii collagencol2a1.Annotations C0265279-Kniest dysplasiaC0343284-Chondrodysplasia,unspecifiedTable 1: Excerpt of literature corpus (PMID: 7874117)The corpus made available by Jimeno et al con-sists of 551 MEDLINE sentences annotated withUMLS concepts or concept clusters: concepts thatwere found to be linked to the same term.
For ex-ample, the concepts ?Pancreatic carcinoma?
(C0235974) and ?Malignant neoplasm of pan-creas?
(C0346647) share the same synonym ?Pan-creas Cancer?, thus they were clustered.
Thesentences were selected from a set of articles cu-rated for Online Mendelian Inheritance in Man(OMIM) and contain an average of 27(+/- 11) to-kens, where tokens are defined as sequences ofcharacters separated by white space.
A set ofUMLS concepts (or clusters) is associated witheach sentence in the corpus.
However, no boun-dary information linking a phrase in a sentence toan annotation was available.
Table 1 shows a sam-ple sentence and its annotations.3.2 Biomedical query corpusA total of 500 PubMed queries were randomly se-lected and divided into two batches of 300 and 200queries, respectively.
Queries were on average3.45(+/- 2.64) tokens long in the 300 query batchand 3.58(+/- 4.63) for the 200 query batch, whichis consistent with the average length of PubMedqueries (3 tokens) reported by Herskovic et al(2007).The queries in the first set were annotated usingKnowtator (Ogren, 2006) by three annotators withdifferent backgrounds (one biologist, one informa-tion scientist, one computational linguist).
Twoannotators annotated the queries using UMLS con-cepts from the disorder group, while the other an-notator simply annotated diseases withoutreference to UMLS concepts.
Table 2 shows asample query and its annotations.
A consensus setwas obtained after a meeting between the annota-tors where diverging annotations were discussedand annotators agreed on a final, unique, version ofall annotations.
The consensus set contains 89 dis-ease concepts (76 unique).Query alzheimer csf amyloidAnnotations  Ann.
1: ?alzheimer?
; 0-8;Ann.
2, 3: ?alzheimer?
; 0-8;C0002395-alzheimer?s diseaseTable 2: Excerpt of annotated 300-query corpus.
Boun-dary information is given as the character interval of theannotated string in the query (here, 0-8).The queries in the second set were annotatedwith UMLS concepts from the disorder group byone of the annotators who also worked on the pre-vious set.
In this set, 53 disease concepts were an-notated (51 unique).4 Automatic disease recognitionWith the perspective of a contextual IR applica-tion where the disease concepts found in querieswill be used to refer users to disease-specific in-formation in databases other than MEDLINE, weare concerned with high precision performance.For this reason, we decided to experiment withmethods that showed the highest precision whencompared to others.
In addition, given the size ofthe corpora available and the type of the annota-146tions, machine learning methods such as CRFs orSVM did not seem applicable.Table 3 shows a description of the training andtest sets for each corpus.Table 3: Description of the training and test sets4.1 Natural Language ProcessingDisease recognition was performed using the Natu-ral Language Processing algorithm implemented inMetaMap (Aronson, 2001)4.
The tool was re-stricted to retrieve concepts from the disordergroup, using the UMLS 2008AB release and?longest match?
feature.In practice, MetaMap parses the input text intonoun phrases, generates variants of these phrasesusing knowledge sources such as the SPECIALISTlexicon, and maps the phrases to UMLS concepts.4.2 Priority ModelThe priority model was first introduced in (Tanabeand Wilbur, 2006) and is adapted here to detectdisease mentions in free text.
Because our evalua-tion is performed at the concept level, the mentionsextracted by the model are then mapped to UMLSusing MetaMap.The priority model approach is based on two setsof phrases: one names of diseases, D, and onenames of non-diseases, N. One trains the model toassign two numbers, p and q, to each token t thatappears in a phrase in either D or N. Roughly, p isthe probability that a phrase from D or N that hasthe token t in it is actually from D and q is the rela-tive weight that should be assigned to t for thispurpose and represents a quality estimate.
Given aphrase4 Additional information is also available athttp://metamap.nlm.nih.gov/1 2 kph t t t?
(1)and for each it  the corresponding numbers ip  andiq  we estimate the probability that ph D  by1 22 11 1k kkj i i jij j iprob p q q p q(2)The training procedure for the model actuallychooses the values of all the p and q quantities tooptimize theprobvalues over all of D and N.For this work we have extended the approach toinclude a quantity21 1 22 11 1k kkj i i jij j iqual q p q q p q prob(3)which represents a weighted average of all thequality numbers iq .
We apply this formula to ob-tainqualas long as0.5.probIf0.5probwereplace all numbersip  by 1 ip  in (2) and (3) toobtainqual.For this application we obtained the sets D andN from the SEMCAT data (Tanabe, Thom et al2006) supplemented with the latest UMLS data.We removed any term from D and N that containedless than five characters in order to decrease theoccurrence of ambiguous terms.
Also the 1,000most frequent terms from D were examined ma-nually and the ambiguous ones were removed.
Theend result is a set of 332,984 phrases in D and4,253,758 phrases in N. We trained the prioritymodel on D and N and applied the resulting train-ing to compute for each phrase in D and N a vectorof values,prob qual.
In this way D and N areconverted toDV  and NV .
We then constructed aMahalanobis classifier (Duda, Hart and Stork,2001) for two dimensional vectors as the differ-ence in the Mahalanobis distance of any such vec-tor to Gaussian approximations toDV  and NV .
Werefer to the number produced by this classifier asthe Mahalanobis score.
By randomly dividing bothD and N into three equal size pieces and trainingon two from each and testing on the third, in athree-fold cross validation we found the Mahala-nobis classifier to perform at 98.4% average preci-sion and 93.9% precision-recall breakeven point.In a final step we applied a simple regression me-thod to estimate the probability that a given Maha-Data Lit.
Corpus Query CorpusTraining 276 sentences(487 disease con-cepts, 185 unique)300 queries (89disease concepts,76 unique)Testing 275 sentences(437 disease con-cepts, 185 unique)200 queries (53disease concepts,51 unique)All 551 sentences(924 disease con-cepts, 280 unique)500 queries (142disease concepts,120 unique)147lanobis score was produced by a phrase belongingto D and not N. Given a phrase phr we will denotethis final probability produced as PMA(phr).The second important ingredient of our statistic-al process is how we produce phrases from a pieceof text.
Given a string of text TX we apply tokeni-zation to TX to produce an ordered set of tokens1 2, , , nt t t?.
Among the tokens produced will bepunctuation marks and stop words and we denotethe set of all such tokens by Z .
We call a tokensegment, ,j kt t?maximal if it contains no ele-ment of Z  and if either 1j  or1jt Zandlikewise if k n  or1kt Z.
Given text TX wewill denote the set of all maximal token segmentsproduced in this way bymax ( ).S TXNow given amaximal token segment mts=, ,j kt t?we definetwo different methods of finding phrases in mts.The first assumes we are given an arbitrary set ofphrases PH.
We recursively define a set of phrases,I mts PHbeginning with this set empty andwith the parameteru j.
Each iteration consistsof asking for the largest v k  for which, ,u vt t PH?.
If there is such a v  we add, ,u vt t?to,I mts PHand set1u v.Otherwise we set1u u.
We repeat this processas long as u k .
The second approach assumeswe are given an arbitrary set of two token phrasesP2.
Again we recursively define a set of phrases, 2J mts Pbeginning with this set empty andwith the parameteru j.
Each iteration consistsof asking for the largest v k  for which given any,  i u i v,1, 2i it t P. If there is such a vwe add, ,u vt t?to, 2J mts Pand set1u v. Otherwise we set1u u.
We repeatthis process as long as u k .In order to apply our phrase extraction proce-dures we need good sets of phrases.
In addition toD and N already defined above, we use another setof phrases defined as follows.
Let R denote the setof all token strings with two or more tokens whichdo not contain tokens from Z and for which thereare at least three MEDLINE records (title and ab-stract text only) in which the token string is re-peated at least twice.We then defineR R D N. We makeuse of R  in addition to D and N. For the set 2Pwe take the set of all two token phrases inMEDLINE documents for which the two tokensco-occur as this phrase much more than expected,i.e., with a2 10,000(based on the two-by-twocontingency table).#Initialization: Given a text TX, setmaxS S TXand .X#Processing: While(S ){I. select mts SII.
If( ,I mts D ) ,K I mts Delse if( ,I mts R ) ,K I mts Relse if( ,I mts N ) Kelseif( , 2J mts P ) , 2K J mts Pelse KIII.
X X KIV.
S S mts}#Return: All pairs , ,  phr PMA phr phr XFigure 1: Phrase finding algorithmWith these preliminaries, our phrase finding al-gorithm in pseudo-code is shown in Figure 1.The output of this algorithm may then be filteredby setting a threshold on the PMA values to accept.5 Results5.1 Assessing the difficulty of the taskTo assess the difficulty of disease recognition, wecomputed the inter-annotator agreement (IAA) onthe 300-query corpus.
Agreement was computed atthe disease mention level for all three annotatorsand at the disease concept level for the two annota-tors who produced UMLS annotations.Inter-annotator agreement measures for NLPapplications have been recently discussed byArtstein and Poesio (2008) who advocate for theuse of chance corrected measures.
However, in ourcase, agreement was partly computed on a verylarge set of categories (UMLS concepts) so wedecided to use Knowtator?s built-in feature, whichcomputes IAA as the percentage of agreement and148allows partial string matches.
For example, in thequery ?dog model transient ischemic attacks?, an-notator 1 selected ?ischemic attacks?
as a disorderwhile annotator 2 and 3 selected ?transient ischem-ic attacks?
as UMLS concept C0007787: Attacks,Transient Ischemic.
In this case, at the subclasslevel (?disorder?)
we have a match for this annota-tion.
But at the exact span or exact category level,there is no match.
Table 4 shows details of IAA atthe disease mention level when partial matches aretaken into account.
For exact span matches, theIAA is lower, at 64.87% on average.Disorder IAA Ann.
1 Ann.
2 Ann.
3Ann.
1 100% 71.77% 75.86%Ann.
2  100% 71.68%Ann.
3   100%Table 4: Agreement on disease mention annotations(partial match allowed) ?
average is 73.10%At the concept level, the agreement (when par-tial matches were allowed) varied significantlydepending on the semantic types.
It ranged be-tween 33% for Findings and 83% for Mental orBehavioral Dysfunction.
However, agreement onthe most frequent category, Disease or Syndrome,was 72%, which is close to the annotators?
overallagreement at the mention level.
One major causeof disagreement was ambiguity caused by conceptsthat were clustered by Jimeno et al For example,in query ?osteoporosis and ?fracture pattern?, an-notator 2 marked ?osteoporosis?
with both?C0029456-osteoporosis?
(a Disease or Syndromeconcept) and ?C1962963-osteoporosis adverseevent?
(a Finding concept) while annotator 3 onlyused ?C0029456-osteoporosis?.5.2 Results on Literature corpusAs shown in Table 3, the corpus was randomlysplit into a training set (276 sentences) and a testset (275 sentences).
The training set was used todetermine the optimal probability threshold for thePriority Model and parameter selection for Meta-Map, respectively.Priority Model parameter adjustments: the firstresult observed from applying the Priority Modelwas that D yielded about 90% of the output of thealgorithm.
Also results coming from R  and 2Pwere not well mapped to UMLS concepts by Me-taMap.
As a result, in this work we ignored diseasecandidates retrieved based on R  and 2P .
The bestF-measure was obtained for a threshold of 0.3,which was consequently used on the test set.Since the Priority Model algorithm does not per-form any mapping to a controlled vocabularysource, the mapping was performed by applyingMetaMap to the snippets of text returned with aprobability value above the threshold.Threshold P R F0 64 73 67.1 67 73 70.2 67 73 70.3 68 73 71.4 68 73 70.5 68 72 69.6 68 72 69.7 68 72 69.8 68 68 68.9 65 60 62Table 5: Precision (P), Recall (R) and F-measure of thePriority Model on the training set for different values ofthe probability threshold.The results presented in Table 5 were obtainedbefore any MetaMap adjustments were made.MetaMap parameter adjustments: an error anal-ysis was performed to adjust MetaMap settings.Errors fell into the following categories:A more specific disease should have beenrecognized (e.g.
?deficiency?
vs. ?C2 defi-ciency?
)The definition of a cluster was lacking(e.g.
?G6PD deficiency?
comprisedC0237987- Glucose-6-phosphate dehydro-genase deficiency anemia and C0017758-Glucosphosphate Dehydrogenase Defi-ciency but not C0017920- Deficiency ofglucose-6-phosphatase)MetaMap mapping was erroneous (e.g.
?hereditary breast?
was mapped toC0729233-Dissecting aneurysm of thethoracic aorta instead of C0346153-Hereditary Breast Cancer)The results of inter-annotator agreement and fur-ther study of MetaMap mappings indicated thatconcepts with the semantic type Findings seemed149to be frequently retrieved erroneously.
For this rea-son, we also experimented not taking Findings intoaccount as an additional adjustment for MetaMap.Table 6 shows the results of applying the MetaMapadjustments yielded from the error analysis on thetraining corpus.Threshold Findings P R F.3 Yes 80 78 79.3 No 85 78 81Table 6: performance of the Priority Model on the train-ing set for threshold .3 depending on whether mappingsto Findings are used in the ?adjustments?MetaMap disorder detection was also performeddirectly on the training corpus.
An error analysissimilar to what was presented above was carriedout to determine the best parameters.
Table 7 be-low shows the results obtained when all conceptsfrom the 12 Semantic Types (STs) in the disordergroup are taken into account with no adjustments(?raw?).
Then, results including the adjustmentsfrom the error analysis are shown when all 12 STsare taken into account, when Findings are excluded(11STs) and when only the most frequent 6STs inthe training set are taken into account.Processing P R FRaw (12 STs) 50 77 61Adjusted (12 STs) 52 75 61Adjusted (11 STs) 57 73 64Adjusted (6 STs) 77 72 74Table 7: Performance of MetaMap on the training setFinally, Table 8 shows the performance of bothmethods on the test set, using the optimal settingsdetermined on the training set:Method P R FPriority Model 80 74 77MetaMap 75 78 76Table 8: Precision (P), Recall (R) and F-measure of thePriority Model and MetaMap on the test set5.3 Results on Query CorpusThe 300-query corpus was used as a training setand the 200-query corpus was used as a test set.For consistency with work on the literature corpus,we assessed the disease recognition on a gold stan-dard set including ?clusters?
of UMLS conceptswere appropriate.
As previously with the Literaturecorpus, we used the training set to determine thebest settings for each method.
The performance ofthe Priority Model at different values of the proba-bility threshold, based on the use of D and N as thesets of sample phrases is similar to that obtainedwith the literature corpus; 0.3 stands out as one ofthe three values for which the best F-measure isobtained (tied with .5 and .8).Because of the brevity of queries vs. sentences,the MetaMap error analysis was very succinct andresulted in:Removal of C0011860-Diabetes mellitustype 2  as mapping for ?diabetes?Removal of all occurrences of C0600688-Toxicity and C0424653-Weight symptom(finding)Adjustment on the number of STs taken in-to accountThe difference in performance obtained on thetraining set for the different MetaMap adjustmentsconsidered is shown in Table 9 when MetaMapwas applied to Priority Model output and in Table10 when it was applied directly on the queries.Threshold Findings P R F.3 Yes 60 72 65.3 No 73 70 71Table 9: performance of the Priority Model on the train-ing set for threshold .3 depending on whether mappingsto Findings are used in the ?adjustments?Processing P R FRaw (12 STs) 41 82 55Adjusted (12 STs) 44 82 57Adjusted (11 STs) 58 81 68Adjusted (6 STs) 64 75 69Table 10: performance of MetaMap on the training setFinally, Table 11 shows the performance of bothmethods on the test set, using the optimal settingsdetermined on the training set:Method P R FPriority Model 76 72 74MetaMap 66 74 70Table 11: Precision (P), Recall (R) and F-measure ofthe Priority Model and MetaMap on the test set1506 DiscussionComparing the Two Methods.
The performanceof both methods on the query corpus is comparableto inter-annotator agreement (F=70-74 vs. IAA=72on Disease and Syndromes).
On both corpora, thePriority Model achieves higher precision and F-measure, while MetaMap achieves better recall.Comparing the results obtained with MetaMapwith those reported by Jimeno et al, precision islower, but recall is much higher.
This is likely tobe due to the different MetaMap settings, and theuse of different UMLS versions - Jimeno et al didnot provide any of this information, but based onthe publication date of their paper, it is likely thatthey used one of the 2006 UMLS releases.
Meystreand Haug (2006) also found that significant per-formance differences could be obtained with Me-taMap by adjusting the content of the knowledgesources used.On both text genres, 0.3 was found to be the op-timal probability threshold for the Priority Model.Based on the performance at different values of thethreshold, it seems that the model is quite efficientat ruling out highly unlikely diseases.
However, forvalues above .3 the performance does not varygreatly.Comparing Text Genres.
For both methods,disease recognition seems more efficient on sen-tences.
This is to be expected: sentences providemore context (e.g.
more tokens surrounding thedisease mention are available) and allow for moreefficient disambiguation, for example on acro-nyms.
Although acronyms are frequent both inqueries and sentences, more undefined acronymsare found in queries.
However, the difference inperformance between the two methods seemshigher on the query corpus.
This indicates that thePriority Model could be more robust to sparse con-text.It should be noted that there were diseases in allsentences in the literature corpus vs. about 1/3 to1/2 of the queries.
In addition, the query corpusincluded many author names, which could createconfusion with disease names (in particular for thePriority Model).
This difficulty was not found inthe sentence corpus.
However, sentences some-times contain negated mention of diseases, whichnever occurred in the query corpus where little tono syntax is used.We also noticed that while Findings seemed tobe generally problematic concepts in both corpora,other concepts such as Injury and Poisoning weremuch more prevalent in the query corpus.
For thisreason, for the general task of disease recognition,a drastic restriction to as little as 6 STs is probablynot advisable.Limitations of the study.
One limitation of ourstudy is the relatively small number of diseaseconcepts in the query corpus.
Although the queryand sentence corpus contain about 500 que-ries/sentences each, there are significantly less dis-ease concepts found in queries compared tosentences.
As a result, there is also less repetitionin the disease concept found.
This is partly due tothe brevity of queries compared to sentences butmainly to the fact that while all the sentences in theliterature corpus had at least one disease concept,this was not the case for the query corpus.
We arecurrently addressing this issue with the ongoingdevelopment of a large scale query corpus anno-tated for diseases and other relevant biomedicalentities.7 ConclusionsWe found that of the two steps of disease recogni-tion, disease mention gets the higher inter-annotator agreement (vs. concept mapping).
Wehave applied a statistical and an NLP method forthe automatic recognition of disease concepts intwo genres of biomedical text.
While both methodsshow good performance (F=77% vs. F=76%) onthe sentence corpus, results indicate that the statis-tical model is more robust on the query corpuswhere very little disease context information isavailable (F=74% vs. F=70%).
As a result, thepriority model will be used for disease detection inPubMed queries in order to characterize users?search contexts for contextual IR.AcknowledgmentsThis research was supported by the Intramural Re-search Program of the NIH, National Library ofMedicine.
The authors would like to thank S.Shooshan and T. Tao for their contribution to theannotation of the query corpus; colleagues in theNCBI engineering branch for their valuable feed-back at every step of the project.151ReferencesAlan R. Aronson, Olivier Bodenreider, Dina Demner-Fushman, Kin Wah Fung, Vivan E. Lee, James G.Mork et al 2007.
From Indexing the Biomedical Li-terature to Coding Clinical Text: Experience withMTI and Machine Learning Approaches.
ACLWorkshop BioNLP.Alan Aronson.
2001.
Effective mapping of biomedicaltext to the UMLS Metathesaurus: the MetaMap pro-gram.
Proceedings of AMIA Symp:17-21.Ron Artstein and Massimo Poesio.
2008.
Inter-CoderAgreement for Computational Linguistics.
Compu-tational Linguistics 34(4): 555-596Jing Bai, and Jian-Yun Nie.
2008.
Adapting informationretrieval to query contexts.
Information Processing &Management.
44(6):1902-22Robert O. Duda, Peter.
E. Hart and David G. Stork.2001.
Pattern Classification.
New York: John Wiley& Sons, Inc.R.
Brian Haynes and Nancy L. Wilczynski.
2004.
Op-timal search strategies for retrieving scientificallystrong studies of diagnosis from Medline: analyticalsurvey.
BMJ.
328(7447):1040.Jorge R. Herskovic, Len Y. Tanaka, William Hersh andElmer V. Bernstam.
2007.
A day in the life ofPubMed: analysis of a typical day's query log.
Jour-nal of the American Medical Informatics Association.14(2):212-20.Antonio Jimeno, Ernesto Jimenez-Ruiz, Vivian Lee,Sylvain Gaudan, Rafael Berlanga and DietrichRebholz-Schuhmann.
2008.
Assessment of diseasenamed entity recognition on a corpus of annotatedsentences.
BMC Bioinformatics.
11;9 Suppl 3:S3.Yang Jin, Ryan T McDonald, Kevin Lerman, Mark AMandel, Steven Carroll, Mark Y Liberman et al2006.
Automated recognition of malignancy men-tions in biomedical literature.
BMC Bioinformatics.7:492.Alexa T. McCray, Anita Burgun and Olivier Bodenreid-er.
2001.
Aggregating UMLS semantic types forreducing conceptual complexity.
Proceedings ofMedinfo 10(Pt 1):216-20.St?phane Meystre and Peter J. Haug.
2006.
Natural lan-guage processing to extract medical problems fromelectronic clinical documents: performance evalua-tion.
J Biomed Inform.
39(6):589-99.Alexander A. Morgan, Zhiyong Lu, Xinglong Wang,Aaron M. Cohen, Juliane Fluck, Patrick Ruch et al2008.
Overview of BioCreative II gene normaliza-tion.
Genome Biol.
9 Suppl 2:S3.Phillip V. Ogren.
2006.
Knowtator: A plug-in for creat-ing training and evaluation data sets for BiomedicalNatural Language systems.
9th Intl.
Prot?g?
Confe-renceJong C. Park and Jung-Jae Kim.
2006.
Named EntityRecognition.
In S. Ananiadou and J. McNaught(Eds), Text Mining for Biology and Biomedicine (pp.121-42).
Boston|London:Artech House Inc.Wanda Pratt and Henry Wasserman.
2000.
QueryCat:automatic categorization of MEDLINE queries.
Pro-ceedings of AMIA Symp:655-9.Tom C. Rindflesh and Marcelo Fiszman.
2003.
Theinteraction of domain knowledge and linguistic struc-ture in natural language processing: interpretinghypernymic propositions in biomedical text.
J Bio-med Inform.
36(6):462-77Daniel E. Rose and Danny Levinson.
2004.
Understand-ing user goals in web search.
In Proceedings of the13th international Conference on World WideWeb:13-9Tefko Saracevic.
1997.
The Stratified Model of Infor-mation Retrieval Interaction: Extension and Applica-tion.
Proceedings of the 60th meeting of the.American Society for Information Science:313-27Xuehua Shen, Bin Tan and ChengXiang Zhai.
2005Context-sensitive information retrieval using impli-cit feedback, In Proceedings of the 28th annual in-ternational conference ACM SIGIR conference onResearch and development in information retrieval:43-50.Larry Smith, Laurraine K. Tanabe, Rie J. Ando, Cheng-Ju Kuo, I-Fang Chung , Chun-Nan Hsu et al 2008.Overview of BioCreative II gene mention recogni-tion.
Genome Biol.
9 Suppl 2:S2.Laurraine K. Tanabe, Lynn.
H. Thom, Wayne Matten,Donald C. Comeau and W. John Wilbur.
2006.SemCat: semantically categorized entities for ge-nomics.
Proceedings of AMIA Symp: 754-8.Laurraine K. Tanabe and W. John Wilbur.
2006.
APriority Model for Named Entities.
Proceedings ofHLT-NAACL BioNLP Workshop:33-40Jaime Teevan, Susan T. Dumais and Eric Horvitz.
2005.Personalizing search via automated analysis of in-terests and activities.
In Proceeding of ACM-SIGIR?05:449?56.Ji-Rong Wen, Ni Lao, Wei-Ying Ma.
2004.
Probabilis-tic model for contextual retrieval.
Proceedings ofACM-SIGIR?04:57?63Xiangmin Zhang, Hermina G.B.
Anghelescu and Xiao-jun Yuan.
2005.
Domain knowledge, search beha-vior, and search effectiveness of engineering andscience students: An exploratory study, InformationResearch 10(2): 217.152
