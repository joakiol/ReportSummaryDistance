Information Fusion in the Context  of Mu l t i -DocumentSummarizat ionReg ina  BarzUay  and Kath leen  R .
McKeownDept.
of Computer  ScienceColumbia UniversityNew York, NY 10027, USAMichae l  E lhadadDept.
of Computer  ScienceBen-Gurion UniversityBeer-Sheva, IsraelAbst rac tWe present a method to automatically generatea concise summary by identifying and synthe-sizing similar elements across related text froma set of multiple documents.
Our approach isunique in its usage of language generation toreformulate the wording of the summary.1 In t roduct ionInformation overload has created an acute needfor summarization.
Typically, the same infor-mation is described by many different onlinedocuments.
Hence, summaries that synthesizecommon information across documents and em-phasize the differences would significantly helpreaders.
Such a summary would be beneficial,for example, to a user who follows a single eventthrough several newswires.
In this paper, wepresent research on the automatic fusion of simi-lar information across multiple documents usinglanguage generation to produce a concise sum-mary.We propose a method for summarizing a spe-cific type of input: news articles presenting dif-ferent descriptions of the same event.
Hundredsof news stories on the same event are produceddaily by news agencies.
Repeated informationabout the event is a good indicator of its impor-tancy to the event, and can be used for summarygeneration.Most research on single document summa-rization, particularly for domain independenttasks, uses sentence xtraction to produce asummary (Lin and Hovy, 1997; Marcu, 1997;Salton et al, 1991).
In the case of multi-document summarization of articles about thesame event, the original articles can includeboth similar and contradictory information.Extracting all similar sentences would producea verbose and repetitive summary, while ex-tracting some similar sentences could producea summary biased towards ome sources.Instead, we move beyond sentence xtraction,using a comparison of extracted similar sen-tences to select the phrases that should be in-cluded in the summary and sentence generationto reformulate them as new text.
Our workis part of a full summarization system (McK-eown et al, 1999), which extracts ets of simi-lax sentences, themes (Eskin et al, 1999), in thefirst stage for input to the components describedhere.Our model for multi-document summariza-tion represents a number of departures fromtraditional language generation.
Typically, lan-guage generation systems have access to a fullsemantic representation f the domain.
A con-tent planner selects and orders propositionsfrom an underlying knowledge base to form textcontent.
A sentence planner determines how tocombine propositions into a single sentence, anda sentence generator ealizes each set of com-bined propositions as a sentence, mapping fromconcepts to words and building syntactic struc-ture.
Our approach differs in the following ways:Content  p lann ing  operates  over fullsentences, producing sentence  frag-ments .
Thus, content planning straddlesthe border between interpretation and gen-eration.
We preprocess the similar sen-tences using an existing shallow parser(Collins, 1996) and a mapping to predicate-argument structure.
The content plannerfinds an intersection of phrases by com-paring the predicate-argument structures;through this process it selects the phrasesthat can adequately convey the commoninformation of the theme.
It also ordersselected phrases and augments them with550On 3th of September 1995, 120 hostages were releasedby Bosnian Serbs.
Serbs were holding over 250 U.N. per-sonnel.
Bosnian serb leader Radovan Karadjic said he ex-pected "a sign of goodwill" from the international com-munity.
U.S. F-16 fighter jet was shot down by Bosnian !Serbs.
Electronic beacon signals, which might have beeni transmitted by a downed U.S. fighter pilot in Bosnia,were no longer being received.
After six days, O'Grady,downed pilot, was rescued by Marine force.
The missionwas carried out by CH-53 helicopters with an escort ofmissile- and rocket-armed Cobra helicopters.Figure 1: Summary produced by our system us-ing 12 news articles as input.information needed for clarification (en-tity descriptions, temporal references, andnewswire source references).Sentence generation begins withphrases.
Our task is to produce fluent sen-tences that combine these phrases, arrang-ing them in novel contexts.
In this process,new grammatical constraints may be im-posed and paraphrasing may be required.We developed techniques to map predicate-argument structure produced by thecontent-planner to the functional represen-tation expected by FUF/SURGE(Elhadad,1993; Robin, 1994) and to integrate newconstraints on realization choice, using sur-face features in place of semantic or prag-matic ones typically used in sentence gen-eration.An example summary automatically gener-ated by the system from our corpus of themesis shown in Figure 1.
We collected a corpusof themes, that was divided into a training por-tion and a testing portion.
We used the trainingdata for identification of paraphrasing rules onwhich our comparison algorithm is built.
Thesystem we describe has been fully implementedand tested on a variety of input articles; thereare, of course, many open research issues thatwe are continuing to explore.In the following sections, we provide anoverview of existing multi-document summa-rization systems, then we will detail our sen-tence comparison technique, and describe thesentence generation component.
We provide ex-amples of generated summaries and concludewith a discussion of evaluation.2 Re la ted  WorkAutomatic summarizers typically identify andextract the most important sentences from aninput article.
A variety of approaches exist fordetermining the salient sentences in the text:statistical techniques based on word distribu-tion (Salton et al, 1991), symbolic techniquesbased on discourse structure (Marcu, 1997),and semantic relations between words (Barzi-lay and Elhadad, 1997).
Extraction techniquescan work only if summary sentences already ap-pear in the article.
Extraction cannot handlethe task we address, because summarization ofmultiple documents requires information aboutsimilarities and differences across articles.While most of the summarization work hasfocused on single articles, a few initial projectshave started to study multi-document summa-rization documents.
In constrained omains,e.g., terrorism, a coherent summary of sev-eral articles can be generated, when a detailedsemantic representation of the source text isavailable.
For example, information extractionsystems can be used to interpret the sourcetext.
In this framework, (Raclev and McKe-own, 1998) use generation techniques to high-light changes over time across input articlesabout the same event.
In an arbitrary domain,statistical techniques are used to identify simi-larities and differences across documents.
Someapproaches directly exploit word distribution inthe text (Salton et al, 1991; Carbonell andGoldstein, 1998).
Recent work (Mani and Bloe-dorn, 1997) exploits semantic relations betweentext units for content representation, such assynonymy and co-reference.
A spreading acti-vation algorithm and graph matching is used toidentify similarities and differences across doc-uments.
The output is presented as a set ofparagraphs with similar and unique words high-lighted.
However, if the same information ismentioned several times in different documents,much of the summary will be redundant.
Whilesome researchers address this problem by select-ing a subset of the repetitions (Carbonell andGoldstein, 1998), this approach is not alwayssatisfactory.
As we will see in the next section~we can both eliminate redundancy from the out-put and retain balance through the selection ofcommon information.551On Friday, a U.S. F-16 fighter jet was shot down byBosnian Serb missile while policing the no-fly zone overthe region.A Bosnian Serb missile shot down a U.S. F-16 overnorthern Bosnia on Friday.On the eve of the meeting, a U.S. F-16 fighter was shotdown while on a routine patrol over northern Bosnia.O'Grady's F-16 fighter jet, based in Aviano, Italy, wasshot down by a Bosnian Serb SA-6 anti-aircraft missilelast Friday and hopes had diminished for finding himalive despite intermittent electronic signals from the areawhich later turned out to be a navigational beacon.Figure 2: A collection of similar sentences - -theme.3 Content  Se lect ion :  ThemeIn tersect ionTo avoid redundant statements in a summary,we could select one sentence from the set of sim-ilar sentences that meets some criteria (e.g., athreshold number of common content words).Unfortunately, any representative sentence usu-ally includes embedded phrases containing in-formation that is not common to other similarsentences.
Therefore, we need to intersect hetheme sentences to identify the common phrasesand then generate a new sentence.
Phrases pro-duced by theme intersection will form the con-tent of the generated summary.Given the theme shown in Figure 2, how canwe determine which phrases hould be selectedto form the summary content?
For our exampletheme, the problem is to determine that onlythe phrase "On Friday, U.S. F-16 fighter jetwas shot down by a Bosnian Serb missile" iscommon across all sentences.The first sentence includes the clause; how-ever, in other sentences, it appears in differ-ent paraphrased forms, such as "A BosnianSerb missile shot down a U.S. F-16 on Fri-day.".
Hence, we need to identify similari-ties between phrases that are not identical inwording, but do report the same fact.
If para-phrasing rules are known, we can compare thepredicate-argument structure of the sentencesand find common parts.
Finally, having selectedthe common parts, we must decide how to com-bine phrases, whether additional information isneeded for clarification, and how to order theresulting sentences to form the summary.shootclass: verb voice :passivetense: past polarity: +f ighter  missileclass: noun class: noundefinite: yesU.S.class: nounFigure 3: DSYNT of the sentence "U.S. fighterwas shot by missile.
"3.1 An A lgor i thm for ThemeIntersect ionIn order to identify theme intersections, en-tences must be compared.
To do this, weneed a sentence representation that emphasizessentence features that are relevant for com-parison such as dependencies between sentenceconstituents, while ignoring irrelevant featuressuch as constituent ordering.
Since predicate-argument structure is a natural way to repre-sent constituent dependencies, we chose a de-pendency based representation called DSYNT(Kittredge and Mel'~uk, 1983).
An example ofa sentence and its DSYNT tree is shown in Fig-ure 3.
Each non-auxiliary word in the sentencehas a node in the DSYNT tree, and this node isconnected to its direct dependents.
Grammat-ical features of each word are also kept in thenode.
In order to facilitate comparison, wordsare kept in canonical form.In order to construct a DSYNT we first runour sentences through Collin's robust, statisti-cal parser (Collins, 1996).
We developed a rule-based component that transforms the phrase-structure output of the parser to a DSYNT rep-resentation.
Functional words (determiners andauxiliaries) are eliminated from the tree and thecorresponding syntactic features are updated.The comparison algorithm starts with all sen-tence trees rooted at verbs from the inputDSYNT, and traverses them recursively: if twonodes are identical, they are added to the out-put tree, and their children are compared.
Oncea full phrase (a verb with at least two con-stituents) has been found, it is added to theintersection.
If nodes are not identical, thealgorithm tries to apply an appropriate para-phrasing rule from a set of rules described inthe next section.
For example, if the phrases552"group of students" and "students" are com-pared, then the omit empty head rule is appli-cable, since "group" is an empty noun and canbe dropped from the comparison, leaving twoidentical words, "students".
If there is no ap-plicable paraphrasing rule, then the comparisonis finished and the intersection result is empty.All the sentences in the theme are comparedin pairs.
Then, these intersections are sortedaccording to their frequencies and all intersec-tions above a given threshold result in themeintersection.For the theme in Figure 2, the intersectionresult is "On Friday, a U.S. F-16 fighter jet wasshot down by Bosnian Serb missile."
13.2 Paraphrasing Rules Derived fromCorpus Analys isIdentification of theme intersection requires col-lecting paraphrasing patterns which occur inour corpus.
Paraphrasing is defined as alter-native ways a human speaker can choose to"say the same thing" by using linguistic knowl-edge (as opposed to world knowledge) (Iordan-skaja et al, 1991).
Paraphrasing has beenwidely investigated in the generation commu-nity (Iordanskaja et al, 1991; Robin, 1994).
(Dras, 1997) considered sets of paraphrases re-quired for text transformation i  order to meetexternal constraints uch as length or read-ability.
(Jacquemin et al, 1997) investigatedmorphology-based paraphrasing in the contextof a term recognition task.
However, there is nogeneral algorithm capable of identifying a sen-tence as a paraphrase of another.In our case, such a comparison is less difficultsince theme sentences are a priori close semanti-cally, which significantly constrains the kinds ofparaphrasing we need to check.
In order to ver-ify this assumption, we analyzed paraphrasingpatterns through themes of our training corpusderived from the Topic Detection and Trackingcorpus (Allan et al, 1998).
Overall, 200 pairs ofsentences conveying the same information wereanalyzed.
We found that 85% of the paraphras-ing is achieved by syntactic and lexical transfor-mations.
Examples of paraphrasing that requireworld knowledge are presented below:1.
"The Bosnian Serbs freed 121 U.N. soldiers1To be exact, the result of the algorithm is a DSYNTthat linearizes as this sentence.last week at Zvornik" and "Bosnian Serbleaders freed about one-third of the U.N.personnel"2.
"Sheinbein showed no visible reaction to theruling."
and "Samuel Sheinbein showed noreaction when Chief Justice Aharon Barakread the 3-2 decision"Since "surface" level paraphrasing comprisesthe vast majority of paraphrases in our corpusand is easier to identify than those requiringworld-knowledge, we studied paraphrasing pat-terns in the corpus.
We found the followingmost frequent paraphrasing categories:1. ordering of sentence components: "Tuesdaythey met..." and "They met ... tuesday";2. main clause vs. a relative clause: "...abuilding was devastated by the bomb" and"...a building, devastated by the bomb";3. realization in different syntactic ategories,e.g., classifier vs. apposition: "Palestinianleader Ararat" and "Ararat, palestinianleader", "Pentagon speaker" and "speakerfrom the Pentagon";4. change in grammatical features: ac-tive/passive, time, number.
"...a buildingwas devastated by the bomb" and "...thebomb devastated a building";5. head omission: "group of students" and"students";6. transformation from one part of speechto another: "building devastation" and"... building was devastated";7. using semantically related words suchas synonyms: "return" and "alight","regime" and "government".The patterns presented above cover 82% ofthe syntactic and lexical paraphrases (which is,in turn, 70~0 of all variants).
These categoriesform the basis for paraphrasing rules used byour intersection algorithm.The majority of these categories can be iden-tified in an automatic way.
However, some ofthe rules can only be approximated to a certaindegree.
For example, identification of similar-ity based on semantic relations between wordsdepends on the coverage of the thesaurus.
We553identify word similarity using synonym relationsfrom WordNet.
Currently, paraphrasing usingpart of speech transformations is not supportedby the system.
All other paraphrase classes weidentified are implemented in our algorithm fortheme intersection.3.3 Tempora l  Order ingA property that is unique to multi-documentsummarization is the effect of time perspective(Radev and McKeown, 1998).
When reading anoriginal text, it is possible to retrieve the cor-rect temporal sequence of events which is usu-ally available xplicitly.
However, when we putpieces of text from different sources together,we must provide the correct ime perspective tothe reader, including the order of events, thetemporal distance between events and correcttemporal references.In single-document summarization, one of thepossible orderings of the extracted informationis provided by the input document i self.
How-ever, in the case of multiple-document summa-rization, some events may not be described inthe same article.
Furthermore, the order be-tween phrases can change significantly from onearticle to another.
For example, in a set of ar-ticles about the Oklahoma bombing from ourtraining set, information about the "bombing"itself, "the death toll" and "the suspects" appearin three different orders in the articles.
Thisphenomenon can be explained by the fact thatthe order of the sentences i highly influencedby the focus of the article.One possible discourse strategy for sum-maries is to base ordering of sentences onchronological order of events.
To find the timean event occurred, we use the publication dateof the phrase referring to the event.
This givesus the best approximation to the order of eventswithout carrying out a detailed interpretationof temporal references to events in the article,which are not always present.
Typically, anevent is first referred to on the day it occurred.Thus, for each phrase, we must find the earliestpublication date in the theme, create a "timestamp", and order phrases in the summary ac-cording to this time stamp.Temporal distance between events is an essen-tim part of the summary.
For example, in thesummary in Figure 1 about a "U.S. pilot doumedin Bosnia", the lengthy duration between "thehelicopter was shot down" and "the pilot wasrescued" is the main point of the story.
Wewant to identify significant ime gaps betweenevents, and include them in the summary.
To doso, we compare the time stamps of the themes,and when the difference between two subse-quent time stamps exceeds a certain threshold(currently two days), the gap is recorded.
Atime marker will be added to the output sum-mary for each gap, for example "According to aReuters report on the 10/21"Another time-related issue that we addressis normalization of temporal references in thesummary.
If the word "today" is used twicein the summary, and each time it refers to adifferent date, then the resulting summary canbe misleading.
Time references uch as "to-day" and "Monday" are clear in the context ofa source article, but can be ambiguous when ex-tracted from the article.
This ambiguity can becorrected by substitution of this temporal ref-erence with the full t ime/date reference, suchas "10//21 '' .
By corpus analysis, we collecteda set of patterns for identification of ambigu-ous dates.
However, we currently don't handletemporal references requiring inference to re-solve (e.g., "the day before the plane crashed,""around Christmas").4 Sentence  Generat ionThe input to the sentence generator is a set ofphrases that are to be combined and realizedas a sentence.
Input features for each phraseare determined by the information recovered byshallow analysis during content planning.
Be-cause this input structure and the requirementson the generator are quite different from typicallanguage generators, we had to address the de-sign of the input language specification and itsinteraction with existing features in a new way,instead of using the existing SURGE syntacticrealization in a "black box" manner.As an example, consider the case of tempo-ral modifiers.
The DSYNT for an input phrasewill simply note that it contains a prepositionalphrase.
FUF/SURGE, our language generator,requires that the input contain a semantic role,circumstantial which in turn contains a tempo-ral feature.The labelling of the circumstantial as timeallows SURGE to make the following decisions554given a sentence such as: "After they madean emergency landing, the pilots were reportedmissing."?
The selection of the position of the timecircumstantial in front of the clause?
The selection of the mood of the embeddedclause as "finite".The semantic input also provides a solid ba-sis to authorize sophisticated revisions to a baseinput.
If the sentence planner decides to ad-join a source to the clause, SURGE can decideto move the time circumstantial to the end ofthe clause, leading to: "According to Reuters onThursday night, the pilots were reported miss-ing after making an emergency landing."
With-out such paraphrasing ability, which might bedecided based on the semantic roles, time andsources, the system would have to generate anawkward sentence with both circumstantials ap-pearing one after another at the front of thesentence.While in the typical generation scenarioabove, the generator can make choices based onsemantic information, in our situation, the gen-erator has only a low-level syntactic structure,represented asa DSYNT.
It would seem at firstglance that realizing such an input should beeasier for the syntactic realization component.The generator in that case is left with little lessto do than just linearizing the input specifica-tion.
The task we had to solve, however, is moredifficult for two reasons:1.
The input specification we define must al-low the sentence planner to perform revi-sions; that is, to attach new constituents(such as source) to a base input specifica-tion without taking into account all possi-ble syntactic interactions between the newconstituent and existing ones;2.
SURGE relies on semantic information tomake decisions and verify that these deci-sions are compatible with the rest of thesentence structure.
When the semantic in-formation is not available, it is more diffi-cult to predict hat the decisions are com-patible with the input provided in syntacticform.We modified the input specification languagefor FUF/SURGE to account for these problems.We added features that indicate the ordering ofcircumstantials in the output.
Ordering of cir-cumstantials can easily be derived from theirordering in the input.
Thus, we label circum-stantials with the features front-i (i-th circum-stantial at the front of the sentence) and end-i(i-th circumstantial t the end), where i indi-cates the relative ordering of the circumstantialwithin the clause.In addition, if possible, when mapping inputphrases to a SURGE syntactic input, the sen-tence planner tries to determine the semantictype of circumstantial by looking up the prepo-sition (for example: "after" indicates a "time"circumstantial).
This allows FUF/SURGE tomap the syntactic ategory of the circumstan-tial to the semantic and syntactic features ex-pected by SURGE.
However, in cases where thepreposition is ambiguous (e.g., "in" can indi-cate "time" or "location") the generator mustrely solely on ordering circumstantials based onordering found in the input.We have modified SURGE to accept his typeof input: in all places SURGE checks the se-mantic type of the circumstantial before makingchoices, we verified that the absence of the cor-responding input feature would not lead to aninappropriate default being selected.
In sum-mary, this new application for syntactic realiza-tion highlights the need for supporting hybridinputs of variable abstraction levels.
The imple-mentation benefited from the bidirectional na-ture of FUF unification in the handling of hy-brid constraints and required little change tothe existing SURGE grammar.
While we usedcircumstantials to illustrate the issues, we alsohandled revision for a variety of other categoriesin the same manner.5 EvaluationEvaluation of multi-document summarization isdifficult.
First, we have not yet found an exist-ing collection of human written summaries ofmultiple documents which could serve as a goldstandard.
We have begun a joint project withthe Columbia Journalism School which will pro-vide such data in the future.
Second, methodsused for evaluation of extraction-based systemsare not applicable for a system which involvestext regeneration.
Finally, the manual effortneeded to develop test beds and to judge sys-555tem output is far more extensive than for singledocument summarization; consider that a hu-man judge would have to read many input ar-ticles (our largest test set contained 27 inputarticles) to rate the validity of a summary.Consequently, the evaluation that we per-formed to date is limited.
We performed a quan-titative evaluation of our content-selection com-ponent.
In order to prevent noisy input fromthe theme construction component from skew-ing the evaluation, we manually constructed26 themes, each containing 4 sentences on aver-age.
Far more training data is needed to tunethe generation portion.
While we have tunedthe system to perform with minor errors on themanual set of themes we have created (the miss-ing article in the fourth sentence of the sum-mary in Figure 1 is an example), we need morerobust input data from the theme constructioncomponent, which is still under development, totrain the generator before beginning large scaletesting.
One problem in improving output isdetermining how to recover from errors in toolsused in early stages of the process, such as thetagger and the parser.5.1 In tersect ion  ComponentThe evaluation task for the content selectionstage is to measure how well we identify com-mon phrases throughout multiple sentences.Our algorithm was compared against intersec-tions extracted by human judges from eachtheme, producing 39 sentence-level predicate-argument structures.
Our intersection algo-rithm identified 29 (74%) predicate-argumentstructures and was able to identify correctly69% of the subjects, 74% of the main verbs,and 65% of the other constituents in our listof model predicate-argument structures.
Wepresent system accuracy separately for each cat-egory, since identifying a verb or a subject is,in most cases, more important han identifyingother sentence constituents.6 Conc lus ions  and  Future  WorkIn this paper, we presented an implementedalgorithm for multi-document summarizationwhich moves beyond the sentence xtractionparadigm.
Assuming a set of similar sentencesas input extracted from multiple documents onthe same event (McKeown et al, 1999; Eskin etal., 1999), our system identifies common phrasesacross sentences and uses language generationto reformulate them as a coherent summary.The use of generation to merge similar infor-mation is a new approach that significantly im-proves the quality of the resulting summaries,reducing repetition and increasing fluency.The system we have developed serves as apoint of departure for research in a variety ofdirections.
First is the need to use learning tech-niques to identify paraphrasing patterns in cor-pus data.
As a first pass, we found paraphrasingrules manually.
This initial set might allow us toautomatically identify more rules and increasethe performance of our comparison algorithm.From the generation side, our main goal is tomake the generated summary more concise, pri-marily by combining clauses together.
We willbe investigating what factors influence the com-bination process and how they can be computedfrom input articles.
Part of combination will in-volve increasing coherence of the generated textthrough the use of connectives, anaphora or lex-ical relations (Jing, 1999).One interesting problem for future work is thequestion of how much context to include froma sentence from which an intersected phrase isdrawn.
Currently, we include no context, butin some cases context is crucial even though itis not a part of the intersection.
This is thecase, for example, when the context negates, ordenies, the embedded sub-clause which matchesa sub-clause in another negating context.
Insuch cases, the resulting summary is actuallyfalse.
This occurs just once in our test cases, butit is a serious error.
Our work will characterizethe types of contextual information that shouldbe retained and will develop algorithms for thecase of negation, among others.AcknowledgmentsWe would like to thank Yael Dahan-Netzer forher help with SURGE.
This material is basedupon work supported by the National ScienceFoundation under grant No.
IRI-96-1879.
Anyopinions, findings, and conclusions or recom-mendations expressed in this material are thoseof the authors and do not necessarily reflect theviews of the National Science Foundation.556ReferencesJames Allan, Jaime Carbonell, George Dod-dington, Jon Yamron, and Y. Yang.
1998.Topic detection and tracking pilot study:Final report.
In Proceedings of the Broad-cast News Understanding and TranscriptionWorkshop, pages 194-218.Regina Barzilay and Michael Elhadad.
1997.Using lexical chains for text summarization.In Proceedings of the A CL Workshop on In-telligent Scalable Text Summarization, pages10-17, Madrid, Spain, August.
ACL.Jaime Carbonell and Jade Goldstein.
1998.The use of mmr, diversity-based rerankingfor reordering documents and producing sum-maries.
In Proceedings of the 21st Annual In-ternational ACM SIGIR Conference on Re-search and Development in Information Re-trieval, Melbourne, Australia, August.Michael Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
InProceedings of the 35th Annual Meeting ofthe Association for Computational Linguis-tics, Santa Cruz, California.Mark Dras.
1997.
Reluctant paraphrase: Tex-tual restructuring under an optimisationmodel.
In Proceedings of PA CLING97, pages98-104, Ohme, Japan.Michael Elhadad.
1993.
Using Argumentationto Control Lexical Choice: A Functional Uni-fication Implementation.
Ph.D. thesis, De-partment of Computer Science, ColumbiaUniversity, New York.Eleazar Eskin, Judith Klavans, and VasileiosHatzivassiloglou.
1999.
Detecting similarityby apllying learning over indicators, submit-ted.Lidija Iordanskaja, Richard Kittredge, andAlain Polguere, 1991.
Natural anguage Gen-eration in Artificial Intelligence and Compu-tational Linguistics, chapter 11.
Kluwer Aca-demic Publishers.Cristian Jacquemin, Judith L. Klavans, andEvelyne Tzoukermann.
1997.
Expansion ofmulti-word terms for indexing and retrievalusing morphology and syntax.
In proceedingsof the 35th Annual Meeting of the A CL, pages24-31, Madrid, Spain, July.
ACL.Hongyan Jing.
1999.
Summary generationthrough intelligent cutting and pasting of theinput document.
PhD thesis proposal.Richard Kittredge and Igor A. Mel'Suk.
1983.Towards a computable model of meaning-textrelations within a natural sublanguage.
InProceedings of the Eighth International JointConference on Artificial Intelligence (IJCAI-83), pages 657-659, Karlsruhe, West Ger-many, August.Chin-Yew Lin and Eduard Hovy.
1997.
Iden-tifying topics by position.
In Proceedings ofthe 5th A CL Conference on Applied NaturalLanguage Processing, pages 283-290, Wash-ington, D.C., April.Inderjeet Mani and Eric Bloedorn.
1997.
Multi-document summarization by graph searchand matching.
In Proceedings of the Fif-teenth National Conference on Artificial In-telligence (AAAI-97), pages 622-628, Provi-dence, Rhode Island.
AAAI.Daniel Marcu.
1997.
From discourse structuresto text summaries.
In Proceedings of the A CLWorkshop on Intelligent Scalable Text Sum-marization, pages 82-88, Madrid, Spain, Au-gust.
ACL.Kathleen R McKeown, Judith Klavans,Vasileios Hatzivassiloglou, Regina Barzilay,and Eleazar Eskin.
1999.
Towards multi-document summarization by reformulation:Progress and prospects, ubmitted.Dragomir R. Radev and Kathleen R. McKeown.1998.
Generating natural language sum-maries from multiple on-line sources.
Compu-tational Linguistics, 24(3):469-500, Septem-ber.Jacques Robin.
1994.
Revision-Based Gener-ation of Natural Language Summaries Pro-riding Historical Background: Corpus-BasedAnalysis, Design, Implementation, and Eval-uation.
Ph.D. thesis, Department of Com-puter Science, Columbia University, NY.Gerald Salton, James Allan, Chris Buckley,and Amit Singhal.
1991.
Automatic analy-sis, theme generation, and summarization fmachine-readable texts.
Science, 264:1421-1426, June.557
