Coling 2010: Poster Volume, pages 81?89,Beijing, August 2010Improved Unsupervised Sentence Alignment for Symmetrical andAsymmetrical Parallel CorporaFabienne Braune Alexander FraserInstitute for Natural Language ProcessingUniversita?t Stuttgart{braunefe,fraser}@ims.uni-stuttgart.deAbstractWe address the problem of unsupervisedand language-pair independent alignmentof symmetrical and asymmetrical parallelcorpora.
Asymmetrical parallel corporacontain a large proportion of 1-to-0/0-to-1and 1-to-many/many-to-1 sentence corre-spondences.
We have developed a novelapproach which is fast and allows us toachieve high accuracy in terms of F1 forthe alignment of both asymmetrical andsymmetrical parallel corpora.
The sourcecode of our aligner and the test sets arefreely available.1 IntroductionSentence alignment is the problem of, given a par-allel text, finding a bipartite graph matching min-imal groups of sentences in one language to theirtranslated counterparts.
Because sentences do notalways align 1-to-1, the sentence alignment task isnon-trivial.The achievement of high accuracy with mini-mal consumption of computational resources is acommon requirement for sentence alignment ap-proaches.
However, in order to be applicable toparallel corpora in any language without requir-ing a separate training set, a method for sentence-alignment should also work in an unsupervisedfashion and be language pair independent.
By?unsupervised?, we denote methods that infer thealignment model directly from the data set to bealigned.
Language pair independence refers to ap-proaches that require no specific knowledge aboutthe languages of the parallel texts to align.We have developed an approach to unsuper-vised and language-pair independent sentencealignment which allows us to achieve high accu-racy in terms of F1 for the alignment of both sym-metrical and asymmetrical parallel corpora.
Dueto the incorporation of a novel two-pass searchprocedure with pruning, our approach is accept-ably fast.
Compared with Moore?s bilingual sen-tence aligner (Moore, 2002), we obtain an averageF1 of 98.38 on symmetrical parallel documents,while Moore?s aligner achieves 94.06.
On asym-metrical documents, our approach achieves 97.67F1 while Moore?s aligner obtains 88.70.
On av-erage, our sentence aligner is only about 4 timesslower than Moore?s aligner.This paper is organized as follows: previouswork is described in section 2.
In section 3, wepresent our approach.
Finally, in section 4, weconduct an extensive evaluation, including a briefinsight into the impact of our aligner on the over-all performance of an MT system.2 Related WorkAmong approaches that are unsupervised and lan-guage independent, (Brown et al, 1991) and (Galeand Church, 1993) use sentence-length statisticsin order to model the relationship between groupsof sentences that are translations of each other.
Asshown in (Chen, 1993) the accuracy of sentence-length based methods decreases drastically whenaligning texts containing small deletions or freetranslations.
In contrast, our approach augments asentence-length based model with lexical statisticsand hence constantly provides high quality align-ments.
(Moore, 2002) proposes a multi-pass search81procedure where sentence-length based statisticsare used in order to extract the training data forthe IBM Model-1 translation tables.
The ac-quired lexical statistics are then combined withthe sentence-length based model in order to ex-tract 1-to-1 correspondences with high accuracy1.Moore?s approach constantly achieves high preci-sion, is robust to sequences of inserted and deletedtext, and is fast.
However, the obtained recall isat most equal to the proportion of 1-to-1 corre-spondences contained in the parallel text to align.This point is especially problematic when align-ing asymmetrical parallel corpora.
In contrast,our approach allows to extract 1-to-many/many-to-1 correspondences.
Hence, we achieve highaccuracy in terms of precision and recall on bothsymmetrical and asymmetrical documents.
More-over, because we use, in the last pass of our multi-pass method, a novel two-stage search procedure,our aligner also requires acceptably low computa-tional resources.
(Deng et al, 2006) have developed a multi-pass method similar to (Moore, 2002) but wherethe last pass is composed of two alignment pro-cedures: a standard dynamic programming (DP)search that allows one to find many-to-manyalignments containing a large amount of sentencesin each language and a divisive clustering al-gorithm that optimally refines those alignmentsthrough iterative binary splitting.
This alignmentmethod allows one to find, in addition to 1-to-1 correspondences, high quality 1-to-many/many-to-1 alignments.
However, 1-to-0 and 0-to-1 cor-respondences are not modeled in this approach2.This leads to poor performance on parallel textscontaining that type of correspondence.
Further-more performing an exhaustive DP search in or-der to find large size many-to-many alignmentsinvolves high computational costs.
In comparisonto (Deng et al, 2006), our approach works in theopposite way.
Our two-step search procedure first1The used search heuristic is a forward-backward compu-tation with a pruned dynamic programming procedure as theforward pass.2In (Deng et al, 2006), p. 5, the p(ak) = p(x, y) whichdetermines the prior probability of having an alignment con-taining x source and y target sentences, is equal to 0 if x < 1or y < 1.
As p(ak) is a multiplicative factor of the model,the probability of having an insertion or a deletion is alwaysequal to 0.finds a model-optimal alignment composed of thesmallest possible correspondences, namely 1-to-0/0-to-1 and 1-to-1, and then merges those cor-respondences into larger alignments.
This allowsthe finding of 1-to-0/0-to-1 alignments as wellas high quality 1-to-many/many-to-1 alignments,leading to high accuracy on parallel texts but alsoon corpora containing large blocs of inserted ordeleted text.
Furthermore, our approach keeps thecomputational costs of the alignment procedurelow: our aligner is, on average, about 550 timesfaster than our implementation3 of (Deng et al,2006).Many other approaches to sentence-alignmentare either supervised or language dependent.
Theapproaches by (Chen, 1993), (Ceausu et al, 2006)or (Fattah et al, 2007) need manually alignedpairs of sentences in order to train the used align-ment models.
The approaches by (Wu, 1994),(Haruno and Yamazaki, 1996), (Ma, 2006) and(Gautam and Sinha, 2007) require an externallysupplied bilingual lexicon.
Similarly, the ap-proaches by (Simard and Plamondon, 1998) or(Melamed, 2000) are language pair dependent in-sofar as they are based on cognates.3 Two-Step Clustering ApproachWe present here our two-step clustering approachto sentence alignment4 which is the main contri-bution of this paper.
We begin by giving the mainideas of our approach using an introductory exam-ple (section 3.1).
Then we show to which extentcomputational costs are reduced in comparison toa standard DP search (section 3.2) before present-ing the theoretical background of our approach(section 3.3).
We further discuss a novel prun-ing strategy used within our approach (section3.4).
This pruning technique is another importantcontribution of this paper.
Next, we present thealignment model (section 3.5) which is a slightlymodified version of the alignment model used in(Moore, 2002).
Finally, we describe the overall3In order to provide a precise comparison between ouraligner and (Deng et al, 2006), we have implemented theirmodel into our optimized framework.4Note that our approach does not aim to find many-to-many alignments.
None of the unsupervised sentence align-ment approaches discussed in section 2 are able to correctlyfind that type of correspondence.82procedure required to align a parallel text with ourmethod (section 3.6).3.1 Sketch of ApproachConsider a parallel text composed of six sourcelanguage sentences Fi and four target languagesentences Ej .
Further assume that the correctalignment between the given texts is composed offour correspondences: three 1-to-1 alignments be-tween F1, E1; F2, E2 and F6, E4 as well as a 3-to-1 alignment between F3, F4, F5 and E3.
Figure 1illustrates this alignment.F1 E1F2 E2F3F4F5F6 E4E3Figure 1: Correct Alignment between Fi and EjIn the perspective of a statistical approach tosentence alignment, the alignment in figure 1 isfound by computing the model-optimal alignmentA?
for the bitext considered:A?
= argmaxA?ak?ASCORE(ak) (1)where SCORE(ak) denotes the score attributedby the alignment model5 to a minimal alignmentak composing A?.
The optimization given inequation 1 relies on two commonly made assump-tions: (c1) a model-optimal alignment A?
canbe decomposed into k minimal and independentalignments ak; (c2) each alignment ak dependsonly on local portions of text in both languages.The search for A?
is generally performed us-ing a dynamic programming (DP) procedure overthe space formed by the l source and m targetsentences.
The computation of A?
using a DPsearch relies on the assumption (c3) that sentencealignment is a monotonic and continuous process.The DP procedure recursively computes the opti-mal score D(l,m)?
for a sequence of alignmentscovering the whole parallel corpus.
The optimalscore D(l,m)?
is given by the following recur-5The alignment model will be presented in section 3.5.sion:D(l,m)?
= min0?x,y?R , x=1?y=1D(l ?
x,m?
y)??
logSCORE(ak)(2)where x denotes the number of sentences on thesource language side of ak and y the number ofsentences on the target language side of ak.The constant R constitutes an upper bound tothe number of sentences that are allowed on eachside of a minimal alignment ak.
This constant hasan important impact on the computational costsof the DP procedure insofar as it determines thenumber of minimal alignments that have to becompared and scored at each step of the recursiongiven in equation 2.
As will be shown in section3.2, the number of comparisons increases depend-ing on R.The solution we propose to the combinatorialgrowth of the number of performed operationsconsists of dividing the search for A?
into twosteps.
First, a model-optimal alignment A?1, inwhich the value of R is fixed to 1, is found.
Sec-ond, the alignments a?k composing A?1 are mergedinto clusters mr containing up to R sentences oneither the source or target language side.
Thealignment composed of these clusters is A?R.The search for the first alignment A?1 is per-formed using a standard DP procedure as given inequation 2 but withR = 1.
This first alignment is,hence, only composed of 0-to-1, 1-to-0 and 1-to-1correspondences.
Using our example, we show, infigure 2, the alignment A?1 found in the first stepof our approach.
The neighbors of F4, that is F3and F5, are aligned as 1-to-0 correspondences.F1 E1F2 E2F3F4F5F6 E4E3Figure 2: A?1 in our Approach (first step)The search for A?R is performed using a DPsearch over the alignments a?k composingA?1.
Thescore D(AR)?
obtained when all alignments a?k ?A?1 have been optimally clustered can be written83recursively as:D(AR)?
= min0?r?RD(AR ?
r)??
logSCORE(mr)(3)whereD(AR?r)?
denotes the best score obtainedfor the prefix covering all minimal alignments inA?1 except the last r minimal alignments consid-ered for composing the last cluster mr.The application of the second step of our ap-proach is illustrated in figure 3.
The first align-ment, between F1 and E1, cannot be merged to bepart of a 1-to-many or many-to-1 cluster becausethe following alignment in A?1 is also 1-to-1.
Soit must be retained as given in A?1.
The five lastalignments are, however, candidates for compos-ing clusters.
For instance, the alignment F2-E2and F3-, where  denotes the empty string, couldbe merged in order to compose the 2-to-1 clusterF2,F3-E2.
However, in our example, the align-ment model chooses to merge the alignments F3-, F4-E3 and F5- in order to compose the 3-to-1cluster F3,F4,F5-E3.F1 E1F2 E2F3F4F5F6 E4E3Figure 3: A?R in our Approach (second step)3.2 Computational GainsThe aim of this section is to give an idea aboutwhy our method is faster than the standard DPapproach.
Let C denote the number of compar-isons performed at each step of the recursion ofthe standard DP procedure, as given in equation2.
This amount is equivalent to the number ofpossible combinations of x source sentences withy target sentences.
Hence, for an approach find-ing all types of correspondences except many-to-many, we have:C = 2R+ 1 (4)In terms of lookups in the word-correspondencetables of a model including lexical statistics, thenumber of operations Cl performed at each stepof the recursion is given by:Cl = R?
?
w2 (5)where R?
denotes the number of scored sen-tences6.
w denotes the average length of eachsentence in terms of words.
The total number oflookups performed in order to align a parallel textcontaining l source and m target sentences usinga standard DP procedure is hence given by:L = R?
?
w2 ?
l ?m (6)In the perspective of our two-step search proce-dure, the computational costs of the search for theinitial alignment A?1 is given by:L?1 = w2 ?
l ?m (7)For the second step of our approach, because A?Ris a cluster of A?1, the dynamic programming pro-cedure used to find this alignment is no longerover the l ?
m space formed by the source andtarget sentences but instead over the space formedby the minimal alignments a?k in A?1.
The aver-age number of those alignments is approximatelyl+m2 .7 The number of lookups performed at eachstep of our DP procedure is given by:L?2 = R?
?
w2 ?l +m2 (8)where R?
and w are defined as in equation 6.The total number of lookups for our clustering ap-proach is hence given by:L?1+2 = (w2 ?
l ?m) + (R?
?
w2 ?l +m2 ) (9)In order to compare the costs of our approach anda standard DP search over the l ?m space formedby the source and target sentences, we re-writeequation 6 as:L = (w2 ?
l ?m) + ((R??
1) ?w2 ?
l ?m) (10)The comparison of equation 9 with equation 10shows that the computational gains obtained usingour two-step approach reside in the reduction ofthe search space from l ?m to l+m2 .86In a framework where no caching of scores is performed,we have R?
= R2 +R+1 compared sentences while score-caching allows one to reduce R?
to R.7Note that this amount tends to l +m when A?1 containsa large number of 0-to-1/1-to-0 correspondences.8It should be noted that through efficient pruning, thesearch space of the standard (DP) procedure can be furtherreduced, see section 3.4.843.3 Theoretical BackgroundWe now present the theoretical foundation of ourapproach.
First, we rewrite equation 1 in a moredetailed fashion as:A?R = argmaxA?ak(xk,yk)?ARP (ak(xk, yk), sqi , trj)(11)with 0 ?
xk, yk ?
R, where R denotes the max-imal amounts x and y of source and target lan-guage sentences composing a minimal alignmentak(xk, yk).
The distribution P (ak(xk, yk), sqi , trj)specifies the alignment model presented in section3.5.As seen in section 3.1, the formulation of thealignment problem as given in equation 11 and theuse of a DP search in order to solve this equationrely on the assumptions (c1) to (c3).
Followingthese assumptions, a model-optimal alignmentA?1can be defined as an ordered set of minimal align-ments a?k(xk, yk), with 0 ?
xk, yk ?
1, where thealigned portions of text are sequential.
In otherwords, if the k ?
th alignment a?k(xk, yk) con-tains the sequences sqi and trj of source and tar-get language sentences, then the next alignmenta?k+1(xk+1, yk+1) is composed of the sequencessuq+1 and tvr+1.
Hence, each alignment composingAR, with R > 1, can be obtained through sequen-tial merging of a series of alignments a?k(xk, yk) ?A?1.9 Accordingly, the sequences of sentences su1and tv1 are obtained by merging sq1 and tr1 withsuq+1 and tvr+1.
It can then be assumed that (c4) theordered set of minimal alignments composing A?Runder equation 11 is equivalent to the set of clus-ters obtained by sequentially merging the minimalalignments composing A?1.
Following assump-tion (c4), the optimization over ak(xk, yk) ?
ARis equivalent to an optimization over the mergedalignmentsmr(xr, yr) ?
AR.
Hence, equation 11is equivalent to:A?R = argmaxAR?mr(xr,yr)?ARP (mr(xr, yr), sui , tvj )(12)where each mr(xr, yr) is obtained by merging rminimal alignments a?k(xk, yk) ?
A?1.9Alignments of type 1-to-0/0-to-1 and 1-to-1 are assumedto be clusters where a minimal alignment a?k(xk, yk) ?
A?1has been merged with the empty alignment e0(0, 0)(, ).The computation of A?R is done in twosteps.
First, a model-optimal alignment A?1 isfound using a standard DP procedure as de-fined in equation 2 but with R = 1 and whereSCORE(ak) is given by the alignment model?
logP (ak, sll?x+1, tmm?y+1).
In the second step,the search procedure used to find the optimalclusters is defined as in equation 3 but whereSCORE(mr) is given by the alignment model?
logP (mr, sui , tvj ).3.4 Search Space PruningIn order to further reduce the costs of finding A?1,we initially pruned the search space in the samefashion as (Moore, 2002).
We explored a nar-row band around the main diagonal of the bi-text to align.
Each time the approximated align-ment came close to the boundaries of the band,the search was reiterated with a larger band size.However, the computational costs for alignmentsthat were not along the diagonal quickly increasedwith this pruning strategy.
A high loss of effi-ciency was hence observed when aligning asym-metrical documents with this technique.
Inciden-tally, Moore reports, in his experiments, that forthe alignment of a parallel text containing 300deleted sentences, the computational costs of hispruned DP procedure is 40 times higher than for acorpus containing no deletions.In order to overcome this problem, we devel-oped a pruning strategy that allows us to avoid theloss of efficiency occurring when aligning asym-metrical documents.
Instead of exploring a nar-row band around the main diagonal of the text toalign, we use sentence-length statistics in order tocompute an approximate path through the consid-ered bitext.
Our search procedure then exploresthe groups of sentences that are around this path.If the approximated alignment comes close to theboundaries of the band, the search is re-iterated.The path initially provided using a sentence-length model10 and then iteratively refined iscloser to the correct alignment than the main di-agonal of the bitext to align.
Hence, the approxi-mated alignment does not come close to the band10The used model is the sentence-length based componentof (Moore, 2002), which is able to find 1-to-0/0-to-1 corre-spondences.85as often as when searching around the main di-agonal.
This results in relatively high computa-tional gains, especially for asymmetrical paralleltexts (see section 4).3.5 Moore?s Alignment ModelThe model we use is basically the same as in(Moore, 2002) but minor modifications have beenmade in order to integrate this model in our two-step clustering approach.
The three componentdistributions of the model are given by11:P (ak, sqi , trj) = P (ak)P (sqi |ak)P (trj |ak, sqi )(13)The first component, P (ak), specifies the gen-eration of a minimal alignment ak.
The secondcomponent, P (sqi |ak), specifies the generation ofa sequence sqi of source language sentences ina minimal alignment ak.
The last component,i.e.
P (trj |ak, sqi ), specifies the generation of a se-quence of target language sentences depending ona sequence of generated source sentences.Our first modification to Moore?s model con-cerns the component distribution P (ak).
In thesecond pass of our two-step approach, which isthe computation of the model-optimal clusteredalignment A?R, we estimate P (ak) by computingthe relative frequency of sequences of alignmentsa?k in the initial alignment A?1 that are candidatesfor composing a cluster mr of specific size.12 Asecond minimal modification to Moore?s modelconcerns the lexical constituent of P (trj |ak, sqi ),which we denote here by P (fb|en, ak).
In contrastwith Moore, we use the best alignment (Viterbialignment) of each target word fb with all sourcewords en, according to IBM Model-1:P (fb|en, ak) =argmaxlen=1 Pt(fb|en)le + 1(14)where le denotes the number of words in thesource sentence(s) of ak.
Our experimental resultshave shown that this variant performed slightlybetter than Moore?s summing over all alignments.11In order to simplify the presentation of the model, weuse the short notation ak for denoting ak(xk, yk)12For the computation ofA?1, the distribution P (ak) is de-fined as in Moore?s work.3.6 Alignment ProcedureIn order to align a parallel text (sl1, tm1 ) we usea multi-pass procedure similar to (Moore, 2002)but where the last pass is replaced by our two-step clustering approach.
In the first pass, an ap-proximate alignment is computed using sentence-length based statistics and the one-to-one corre-spondences with likelihood higher than a giventhreshold are selected for the training of the IBMModel-1 translation tables13.
Furthermore, eachfound alignment is cached in order to be used asthe initial diagonal determining the search spacefor the next pass.
In the second pass, the corpus isre-aligned according to our two-step approach: (i)a model-optimal14 alignment containing at mostone sentence on each side of the minimal align-ments ak(xk, yk) is found; (ii) those alignmentsare model-optimally merged in order to obtain analignment containing up to R sentences on eachside of the clusters mr(xr, yr).
In our experi-ments, a maximum number of 4 sentences is al-lowed on each side of a cluster.4 ExperimentsWe evaluate our approach (CA) using three base-lines against which we compare alignment qual-ity and computational costs.15 The first (Mo) isthe method by (Moore, 2002).
As a second base-line (Std), we have implemented an aligner thatfinds the same type of correspondences as our ap-proach but performs a standard DP search insteadof our two-pass clustering procedure and imple-ments Moore?s pruning strategy.
Our third base-line (Std P.) is similar to (Std) but integrates ourpruning technique.16 We also evaluate the impact13Words with frequency < 3 in the corpus have beendropped.14This is optimal according to the alignment model whichwill be presented in section 3.5.15We do not evaluate sentence-length based methods inour experiments because these methods obtain an F1 whichis generally about 10% lower than for our approach onsymmetrical documents.
For asymmetrical documents theperformance is even worse.
For example, when usingGale&Church F1 sinks to 13.8 on documents which are notaligned at paragraph level and contain small deletions.16We do not include (Deng et al, 2006) in our exper-iments because our implementation of this aligner is 550times slower than our proposed method and the inability tofind 1-to-0/0-to-1 correspondences makes it inappropriate forasymmetrical documents.86S 1-1 1-N/N-1 0-1/1-0 Oth.
Tot.1 88.2% 10.9 % 0.005% 0.85% 3,8772 91.9% 7.5% 0.007% 0.53% 2,6463 91.6% 2.7% 4.3% 1.4% 23,7154 44.8% 6.2% 49% 0.01% 2,606Table 1: Test Set for Evaluation with 2 ?
N ?
4of our aligner on the overall performance of anMT system.Evaluation.
We evaluate the alignment accu-racy of our approach using four test sets annotatedat sentence-level.
The two first are composedof hand aligned documents from the Europarlcorpus for the language-pairs German-to-Englishand French-to-English.
The third is composedof an asymmetric document from the German-to-English part of the Europarl corpus.
Our fourthtest set is a version of the BAF corpus (Simard,1998), where we corrected the tokenization.
BAFis an interesting heterogeneous French-to-Englishtest set composed of 11 texts belonging to fourdifferent genres.
The types of correspondencescomposing our test sets are given in table 1.
Themetrics used are precision, recall and F117.
Onlyalignments that correspond exactly to referencealignments count as correct.
The computationalcosts required for each approach are measured inseconds.
The time required to train IBM Model-1is not included in our calculations18.Summary of Results.
Regarding alignment ac-curacy, the results in table 2 show that (CA) ob-tains, on average, an F1 that is 4.30 better thanfor (Mo) on symmetrical documents.
The resultsin table 3 show that, on asymmetrical texts, (CA)achieves an F1 which is 8.97 better than (Mo).The accuracy obtained using (CA), (Std) and (StdP.)
is approximately the same.
We have furthercompared the accuracy of (CA) with (Std) forfinding 1-to-many/many-to-1 alignments.
The ob-tained results show that (CA) achieves an F1 thatis 5.0 better than (Std).Regarding computational costs, the time re-quired by (CA) is on average 4 times larger than17We measure precision, recall and F1 on the 1-to-N/N-to-1 alignments,N >= 1, which means that we view insertionsand deletions as ?negative?
decisions, like Moore.18The reason for this decision is that our optimized frame-work trains the Model-1 translation tables far faster thanMoore?s bilingual sentence aligner.for (Mo) when aligning symmetrical documents.On asymmetrical documents, (Mo) is, however,only 1.5 times faster than (CA).
Compared to(Std), (CA) is approximately 6 times faster onsymmetrical and 80 times faster on asymmetricaldocuments.
The time of (Std P.) is 3 times higherthan for (CA) on symmetrical documents and 22times higher on asymmetrical documents.
Thisshows that, first, our pruning technique is moreefficient than Moore?s and, second, that the mainincrease in speed is due to the two step clusteringapproach.Discussion.
On the two first test sets, (Mo)achieves high precision while the obtained recallis limited by the number of correspondences thatare not 1-to-1 (see table 1).
Regarding (Std), (StdP.)
and (CA), all aligners achieve high precisionas well as high recall, leading to an F1 which isover 98% for both documents.
The computationalcosts of (CA) for the alignment of symmetricaldocuments are, on average, 4 times higher than(Mo), 6 times lower than (Std) and 3.5 timeslower than (Std P.).
On our third test set (Mo)achieves, with an F1 of 88.70, relatively poorrecall while the other aligners reach precisionand recall values that are over 98%.
Regardingthe computational costs, (CA) is only 1.5 timesslower than (Mo) on asymmetrical documentswhile it is 80 times faster than (Std) and about 22times faster than (Std P.).
On our fourth test setall evaluated aligners perform approximately thesame than on Europarl.
While (Mo) obtains, with94.46, an F1 which is the same as for Europarl,(CA) performs, with an F1 of 97.67, about1% worse than on Europarl.
A slightly largerdecrease of 1.6% is observed for (Std) whichobtains 96.81 F1.
Note, however, that (CA), (Std)and (Std P.) still perform about 3% better than(Mo).
Regarding computational costs, (CA) is4 times slower than (Mo) and 40 times fasterthan (Std).
The high difference in speed betweenour approach and (Std) is due to the fact that theBAF corpus contains texts of variable symmetrywhile (Std) shows a great speed decrease whenaligning asymmetrical documents.
Finally, wehave compared the accuracy of (Std) and (CA) forthe finding of 1-to-many/many-to-1 alignmentscontaining at least 3 sentences on the ?many?87Appr.
Lang.
Prec.
Rec.
F1 SpeedMo D-E 98.75 87.88 92.99 935sMo F-E 98.97 91.56 95.12 1,661sStd D-E 98.42 98.57 98.49 24,152sStd F-E 98.45 98.83 98.64 35,041sStd P. D-E 98.37 98.49 98.43 13,387sStd P. F-E 98.41 98.78 98.60 21,848sCA D-E 98.25 98.70 98.47 3,461sCA F-E 98.00 98.60 98.30 6,978sTable 2: Performance on EuroparlAppr.
Prec.
Rec.
F1 SpeedMo 97.90 81.08 88.70 552sStd 97.66 97.74 97.70 71,475sStd P. 97.74 97.81 97.77 17,502sCA 97.38 97.97 97.67 800sTable 3: Performance on asym.
documentsAppr.
Prec.
Rec.
F1 SpeedMo 96.58 92.43 94.46 563sStd 96.82 96.80 96.81 84,988sCA 97.05 97.63 97.34 2,137sTable 4: Performance on BAFside.
This experiment has shown that (Std)finds a larger amount of those alignments whilemaking numerous wrong conjectures.
On theother hand, (CA) finds less 1-to-many/many-to-1correspondences but makes only few incorrecthypotheses.
Hence, F1 is about 5% better for(CA).MT evaluation We also measured the impactof 1-to-N/N-to-1 alignments (which are not ex-tracted by Moore) on MT.
We used standard set-tings of the Moses toolkit, and the Europarl de-vtest2006 set as our test set.
We ran MERT sep-arately for each system.
System (s1) was trainedjust on the 1-to-1 alignments extracted from theEuroparl v3 corpus by our system while system(s2) was trained with all correspondences found.
(s1) obtains a BLEU score of 0.2670 while (s2)obtains a BLEU score of 0.2703.
Application ofthe pairwise bootstrap test (Koehn, 2004) showsthat (s2) is significantly better than (s1).5 ConclusionWe have addressed the problem of unsupervisedand language-pair independent alignment of sym-metrical and asymmetrical parallel corpora.
Wehave developed a novel approach which is fastand allows us to achieve high accuracy in termsof F1 for the alignment of bilingual corpora.Our method achieved high accuracy on symmet-rical and asymmetrical parallel corpora, and wehave shown that the 1-to-N/N-to-1 alignments ex-tracted by our approach are useful.
The sourcecode of the aligner and the test sets are availableat http://sourceforge.net/projects/gargantua .6 AcknowledgementsThe first author was partially supported by theHasler Stiftung19.
Support for both authors wasprovided by Deutsche Forschungsgemeinschaftgrants Models of Morphosyntax for StatisticalMachine Translation and SFB 732.ReferencesBrown, Peter F., Jennifer C. Lai, and Robert L. Mercer.1991.
Aligning sentences in parallel corpora.
In InProceedings of 29th Annual Meeting of the Associa-tion for Computational Linguistics, pages 169?176.Ceausu, Alexandru, Dan Stefanescu, and Dan Tufis.2006.
Acquis communautaire sentence alignmentusing support vector machines.
In LREC 2006:Fifth International Conference on Language Re-sources and Evaluation.Chen, Stanley F. 1993.
Aligning sentences in bilingualcorpora using lexical information.
In Proceedingsof the 31st Annual Meeting of the Association forComputational Linguistics, pages 9?16.Deng, Yoggang, Shankar Kumar, and William Byrne.2006.
Segmentation and alignment of parallel textfor statistical machine translation.
Natural Lan-guage Engineering, 12:1?26.Fattah, Mohamed Abdel, David B. Bracewell, FujiRen, and Shingo Kuroiwa.
2007.
Sentence align-ment using p-nnt and gmm.
Computer Speech andLanguage, (21):594?608.Gale, William A. and Kenneth Ward Church.
1993.
Aprogram for aligning sentences in bilingual corpora.Computational Linguistics, 19(1):75?102.Gautam, Mrityunjay and R. M. K. Sinha.
2007.
Aprogram for aligning sentences in bilingual cor-pora.
Proceedings of the International Conference19http://www.haslerstiftung.ch/.88on Computing: Theory and Applications, ICCTA?07, (1):480?484.Haruno, M. and T. Yamazaki.
1996.
High-performance bilingual text alignment using statisti-cal and dictionary information.
In Proceedings ofACL ?96, pages 131?138.Koehn, Philipp.
2004.
Statistical significance tests formachine translation evaluation.
In Lin, Dekang andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.Ma, Xiaoyi.
2006.
Champollion: A robust paral-lel text sentence aligner.
In LREC 2006: Fifth In-ternational Conference on Language Resources andEvaluation.Melamed, I. Dan.
2000.
Models of translationalequivalence among words.
Computational Linguis-tics, 26:221?249.Moore, Robert.
2002.
Fast and accurate sentencealignment of bilingual corpora.
In In Proceedingsof 5th Conference of the Association for MachineTranslation in the Americas, pages 135?244.Simard, Michel and Pierre Plamondon.
1998.
Bilin-gual sentence alignment: Balancing robustness andaccuracy.
Machine Translation, 13(1):59?80.Simard, Michel.
1998.
The baf: A corpus of english-french bitext.
In Proceedings of LREC 98, Granada,Spain.Wu, Dekai.
1994.
Aligning a parallel English-Chinesecorpus statistically with lexical criteria.
In In Pro-ceedings of the 32nd Annual Conference of theAssociation for Computational Linguistics, 80?87,Las, pages 80?87.89
