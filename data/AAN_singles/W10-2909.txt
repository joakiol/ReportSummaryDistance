Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 57?66,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsImproved Unsupervised POS Induction Using Intrinsic Clustering Qualityand a Zipfian ConstraintRoi ReichartICNCThe Hebrew Universityroiri@cs.huji.ac.ilRaanan FattalInstitute of computer scienceThe Hebrew Universityraananf@cs.huji.ac.ilAri RappoportInstitute of computer scienceThe Hebrew Universityarir@cs.huji.ac.ilAbstractModern unsupervised POS taggers usuallyapply an optimization procedure to a non-convex function, and tend to converge tolocal maxima that are sensitive to start-ing conditions.
The quality of the tag-ging induced by such algorithms is thushighly variable, and researchers report av-erage results over several random initial-izations.
Consequently, applications arenot guaranteed to use an induced taggingof the quality reported for the algorithm.In this paper we address this issue usingan unsupervised test for intrinsic cluster-ing quality.
We run a base tagger withdifferent random initializations, and selectthe best tagging using the quality test.
Asa base tagger, we modify a leading un-supervised POS tagger (Clark, 2003) toconstrain the distributions of word typesacross clusters to be Zipfian, allowing usto utilize a perplexity-based quality test.We show that the correlation between ourquality test and gold standard-based tag-ging quality measures is high.
Our re-sults are better in most evaluation mea-sures than all results reported in the liter-ature for this task, and are always betterthan the Clark average results.1 IntroductionUnsupervised part-of-speech (POS) induction isof major theoretical and practical importance.
Itcounters the arbitrary nature of manually designedtag sets, and avoids manual corpus annotationcosts.
The task enjoys considerable current inter-est in the research community (see Section 3).Most unsupervised POS tagging algorithms ap-ply an optimization procedure to a non-convexfunction, and tend to converge to local maximathat strongly depend on the algorithm?s (usuallyrandom) initialization.
The quality of the tag-gings produced by different initializations variessubstantially.
Figure 1 demonstrates this phe-nomenon for a leading POS induction algorithm(Clark, 2003).
The absolute variability of the in-duced tagging quality is 10-15%, which is around20% of the mean.
Strong variability has also beenreported by other authors (Section 3).The common practice in the literature is to re-port mean results over several random initializa-tions of the algorithm (e.g.
(Clark, 2003; Smithand Eisner, 2005; Goldwater and Griffiths, 2007;Johnson, 2007)).
This means that applications us-ing the induced tagging are not guaranteed to usea tagging of the reported quality.In this paper we address this issue using anunsupervised test for intrinsic clustering quality.We present a quality-based algorithmic family Q.Each of its concrete member algorithms Q(B) runsa base tagger B with different random initializa-tions, and selects the best tagging according thequality test.
If the test is highly positively corre-lated with external tagging quality measures (e.g.,those based on gold standard tagging), Q(B) willproduce better results than B with high probability.We experiment with two base taggers, Clark?soriginal tagger (CT) and Zipf Constrained Clark(ZCC).
ZCC is a novel algorithm of interest in itsown right, which is especially suitable as a basetagger in the family Q. ZCC is a modification ofClark?s algorithm in which the distribution of thenumber of word types in a cluster (cluster typesize) is constrained to be Zipfian.
This propertyholds for natural languages, hence we can expecta higher correlation between ZCC and an acceptedunsupervised quality measure, perplexity.We show that for both base taggers, the corre-lation between our unsupervised quality test andgold standard based tagging quality measures ishigh.
For the English WSJ corpus, the Q(ZCC)570.45 0.5 0.5502040V0.7 0.8 0.901020NVI0.4 0.5 0.602040Many to 10.4 0.5020401 to 1Figure 1: Distribution of the quality of the tag-gings produced in 100 runs of the Clark POS in-duction algorithm (with different random initial-izations) for sections 2-21 of the WSJ corpus.
Allgraphs are 10-bin histograms presenting the num-ber of runs (y-axis) with the corresponding qual-ity (x-axis).
Quality is evaluated with 4 clusteringevaluation measures: V, NVI, greedy m-1 map-ping and greedy 1-1 mapping.
The quality of theinduced tagging varies considerably.algorithm gives better results than CT with proba-bility 82-100% (depending on the external qualitymeasure used).
Q(CT) is shown to be better thanthe original CT algorithm as well.
Our results arebetter in most evaluation measures than all previ-ous results reported in the literature for this task,and are always better than Clark?s average results.Section 2 describes the ZCC algorithm and ourquality measure.
Section 3 discusses previouswork.
Section 4 presents the experimental setupand Section 5 reports our results.2 The Q(ZCC) AlgorithmGiven an N word corpus M consisting of plaintext, with word types W = {w1, .
.
.
, wm}, theunsupervised POS induction task is to find a classmembership function g from W into a set of classlabels {c1, .
.
.
, cn}.
In the version tackled in thispaper, the number of classes n is an input of the al-gorithm.
The membership function g can be usedto tag a corpus if it is deterministic (as the func-tion learned in this work) or if a rule for selectinga single tag for every word is provided.Most modern unsupervised POS taggers pro-duce taggings of variable quality that strongly de-pend on their initialization.
Our approach towardsgenerating a single high quality tagging is to use afamily of algorithms Q.
Each member Q(B) of Qutilizes a base tagger B, which is run using severalrandom initializations.
The final output is selectedaccording to an unsupervised quality test.
We fo-cus here on Clark?s tagger (Clark, 2003) (CT),probably the leading POS induction algorithm (seeTable 3).We start with a description of the original CT.We then detail ZCC, a modification of CT thatconstrains the clustering space by adding a Zipf-based constraint.
Our perplexity-based unsuper-vised tagging quality test is discussed next.
Fi-nally, we provide an unsupervised technique forselecting the parameter of the Zipfian constraint.2.1 The Original Clark Tagger (CT)The tagger?s statistical model combines dis-tributional and morphological information withthe likelihood function of the Brown algorithm(Brown et al, 1992; Ney et al, 1994; Martin etal., 1998).
In the Brown algorithm a class assign-ment function g is selected such that the class bi-gram likelihood of the corpus, p(M |g), is max-imized.
Morphological and distributional infor-mation is introduced to the Clark model througha prior p(g).
The prior prefers morphologicallyuniform clusters and skewed cluster sizes.The probability function the algorithm tries tomaximize is:(1) p(M, g) = p(M |g) ?
p(g)(2) p(M |g) = ?i=Ni=2 p(g(wi)|g(wi?1))(3) p(g) = ?nj=1 ?j?g(w)=j qj(w)Where qj(wi) is the probability of assigningwi ?
W by cluster cj according to the morpho-logical model and ?j is the coefficient of clusterj, which equals to the number of word types as-signed to that cluster divided by the total numberof word types in the vocabulary W .
The objectiveof the algorithm is formally specified by:g?
= argmaxgp(M, g)To find the cluster assignment g?
an iterativealgorithm is applied.
As initialization, the wordsin W are randomly assigned to clusters (clustersare thus of similar sizes).
Then, for each word(words are ordered by their frequency in the cor-pus) the algorithm computes the effect that mov-ing it from its current cluster to each of the otherclusters would have on the probability function.The word is moved to the cluster having the high-est positive effect (if there is no such cluster, theword is not moved).
The last step is performed it-eratively until no improvement to the probabilityfunction is possible through a single operation.58The probability function has many local max-ima and the one to which the algorithm conver-gences strongly depends on the initial assignmentof words to clusters.
The quality of the clusters in-duced in different runs of the algorithm is highlyvariable (Figure 1).2.2 The Cluster Type Size Zipf ConstraintThe motivation behind using a Zipfian constraint isthe following observation: when a certain statisticis known to affect the quality of the induced clus-tering and it is not explicitly manipulated by the al-gorithm, strong fluctuations in its values are likelyto imply that there are uncontrolled fluctuations inthe quality of the induced clusterings.
Thus, in-troducing a constraint that we believe holds in realdata increases the correlation between clusteringquality and a well accepted unsupervised qualitymeasure (perplexity).Our ZCC algorithm searches for a class assign-ment function g that maximizes the probabilityfunction (1) under a constraint on the clusteringspace, namely constraining the cluster type sizedistribution induced by g to be Zipfian.
This con-straint holds in many languages (Mitzenmacher,2004) and is demonstrated in Figure 3 for the En-glish corpus with which we experiment in this pa-per.Zipf?s law predicts that the fraction of elementsin class k is given by:f(k; s;n) = 1/ks?ni=1(1/is)where s is a parameter of the distribution and n thenumber of clusters.Denote the cluster type size distribution derivedfrom the algorithm?s cluster assignment function gby T (g).
The objective of the algorithm isg??
= argmaxgp(M, g) s.t.
T (g) ?
Zipf(s)To impose the Zipfian distribution on the in-duced clusters size, we make two modifications tothe original CT algorithm.
First, at initialization,words are randomly assigned to clusters in a waythat cluster sizes are distributed according to theZipfian distribution (with a parameter s).
Specifi-cally, we randomly select words to be assigned tothe first cluster until the fraction of word types inthe cluster equals to the prediction given by Zipf?slaw.
We then randomly assign words to the secondcluster and so on.Second, we change the basic operation of the al-gorithm from moving a word to a cluster to swap-ping two words between two different clusters.For each word wi (again, words are ordered bytheir frequency in the corpus as in CT), the algo-rithm computes the effect on the probability func-tion of moving it from its current cluster ccurr toeach of the other clusters.
We denote the clustershowing the best effect by cbest.
Then, we searchthe words of cbest for the word wj whose transitionto ccurr has the best effect on the probability func-tion.
If the sum of the effects of moving wi fromccurr to cbest and moving wj from cbest to ccurris positive, the swapping is performed.
If swap-ping is not performed, we repeat the process forwi, this time searching for cbest among all otherclusters except of former cbest candidates1.2.3 Unsupervised Identification of HighQuality RunsPerplexity is a standard measure for languagemodel evaluation.
A language model defines thetransition probabilities for every word wi given thewords that precede it.
The perplexity of a languagemodel for a given corpus having N words is de-fined to beN???
?N?i=11p(wi|w1 .
.
.
wi?1)An important property of perplexity that makesit attractive as a measure for language model per-formance is that in some sense the best model forany corpus has the lowest perplexity for that cor-pus (Goodman, 2001).
Thus, the lower the per-plexity of the language model, the better it is.Clark (2003) proposed a perplexity based testfor the quality of his POS induction algorithm.
Inthat test, a bigram class-based language model istrained on a training corpus (using the tagging ofthe unsupervised tagger) and applied to anothertest corpus.
In such a model the transition prob-ability from a word wj to a word wi is givenby p(C(wi)|C(wj)) where C(wk) is the class as-signed by the POS induction algorithm to wk.
Inthe training phase the bigram transition probabili-ties are computed using the training corpus, and in1To make the algorithm more time efficient, for each wordwi we perform only three iterations of the searching for cbest,and for each cbest candidate we compute for at most 500words the effect on the probability function of the removalto ccurr .592 4 6 8 10880882884886888890KAveragePerplexity2 4 6 8 100.40.50.60.70.80.9KRankCorrelationFigure 2: Left: average perplexity vs. the param-eter K (tightness of the entropy outliers filter; seetext for a full explanation).
Right: Spearman?srank correlation between perplexity and an exter-nal (many-to-one) quality of the clustering as afunction of K. The three curves are for ZCC,using different exponents (triangles: 0.9, circles:1.3, solid: 1.1).
A model whose quality improves(decreased perplexity) with K (left) demonstratesbetter correlation between perplexity and externalquality (right).
In all three graphs the x axis is inunits of 5K (e.g., a graph x value of 2 means that10 clusterings were removed from the top of thelist and 10 from its bottom).the test phase the perplexity of the learned modelis evaluated on the test corpus.
Better POS induc-tion algorithms yield lower perplexity languagemodels.
However, Clark did not study the correla-tion between the perplexity measure and the goldstandard tagging.In this paper, we use Clark?s perplexity basedtest as the unsupervised quality test used by thefamily Q.
To provide a high quality prediction, thistest should highly correlate with external cluster-ing quality.
To the best of our knowledge, such acorrelation has not been explored so far.2.4 Unsupervised Parameter SelectionThe base ZCC algorithm has one input parame-ter, the exponent s of the Zipfian distribution.
Vir-tually all unsupervised algorithms utilize param-eters whose values affect their results.
While itis methodologically valid to simply determine avalue based on reasonable considerations or a de-velopment set, to keep the fully unsupervised na-ture of our work we now present a method foridentifying the best parameter assignment.
Themethod also casts some additional interesting lighton the nature of the problem.Like cluster type size, the distribution of clusterinstance size in natural languages is also Zipfian(see Figure 3).
A naive application of this con-straint into the ZCC algorithm would be to allowswapping words between clusters only if they an-notate the same number of word instances in thecorpus.
However, this constraint, either by itselfor in combination with the cluster type size con-straint, is too restrictive.We utilize it for parameter selection as follows.Recall that our family of algorithms Q(B) runs abase tagger B several times.
Each specific runyields a clustering Ci.
The final result is selectedfrom the set of clusterings C = {Ci}.
We donot explicitly address the number of instances con-tained in a cluster, but we can prune from C thoseclusterings for which this distribution is very dif-ferent.
Again, imposing a constraint that is knownto hold reduces quality fluctuations between dif-ferent runs.To measure the similarity between the clusterinstance size distribution of two clusterings in-duced by two runs of the algorithm, we treat theclusters induced by a given run as samples froma random variable.
The events of this variable arethe induced clusters and the probability assignedto each event is equal to the number of word in-stances contained in the corresponding cluster, di-vided by the total number of word instances in thetagged corpus.
The entropy of this random vari-able is used as a statistic for the word instancedistribution.
Clusterings having similar cluster in-stance size distributions also have similar valuesof this statistic.We apply an entropy outliers filter to the set ofclusterings C. In this filter, we sort the membersof C (these are clusterings obtained in differentruns of the base tagger) according to their clus-ter instance size entropy, and prune K runs fromthe beginning and K runs from the end of the list.The perplexity-based quality test described aboveis applied only to members of C that were notpruned in this step.Figure 2 (left) shows the average perplexity ofa set of clusterings as a function of the parame-ter K of the entropy-based filter.
Results are pre-sented for 100 runs of ZCC2 with three differentexponent values (0.9, 1.1, 1.3).
These assignmentsyield considerably different Zipfian distributions.While all three models have similar average per-plexity over all 100 runs, only the solid line (cor-responding to an exponent value of 1.1) consis-2See Section 4 for the experimental setup.60tently decreases (improves) with K. The circledline (corresponding to an exponent value of 1.3)monotonically decreases with K until a certain Kvalue, while the line with triangles (correspond-ing to an exponent value of 0.9) remains relativelyconstant.Figure 2 (right) shows that models for whichthe entropy-based filter improves perplexity moredrastically, exhibit better correlation between per-plexity and external clustering quality3.Our unsupervised parameter selection method isthus based on finding a value which exhibits a con-sistent decrease in perplexity as a function of K,the number of clusterings pruned from the begin-ning and end of the entropy-sorted list.
In the restof this paper we show results where the exponentvalue is 1.1.3 Previous WorkUnsupervised POS induction/tagging is a fruitfularea of research.
A major direction is HiddenMarkov Models (HMM) (Merialdo, 1994; Bankoand Moore, 2004; Wang and Schuurmans, 2005).Several recent works have tried to improve thismodel using Bayesian estimation (Goldwater andGriffiths, 2007; Johnson, 2007; Gao and Johnson,2008), sophisticated initialization (Goldberg et al,2008), induction of an initial clustering used totrain an HMM (Freitag, 2004; Biemann, 2006),infinite HMM models (Van Gael et al, 2009), in-tegration of integer linear programming into theparameter estimation process (Ravi and Knight,2009), and biasing the model such that the num-ber of possible tags that each word can get is small(Grac?a et al, 2009).The Bayesian works integrated into the modelinformation about the distribution of words to POStags.
For example, Johnson (2007) integrated tothe EM-HMM model a prior that prefers cluster-ings where the distributions of hidden states towords is skewed.Other approaches include transformation basedlearning (Brill, 1995), contrastive estimation forconditional random fields (Smith and Eisner,2005), Markov random fields (Haghighi andKlein, 2006), a multilingual approach (Snyder etal., 2008; Snyder et al, 2008) and expanding a3The figure is for greedy many-to-one mapping andSpearman?s rank correlation coefficient, explained in furtherSections.
Other external measures and rank correlation scoresdemonstrate the same pattern.partial dictionary and use it to learn disambigua-tion rules (Zhao and Marcus, 2009).These works, except (Haghighi and Klein,2006; Johnson, 2007; Gao and Johnson, 2008)and one experiment in (Goldwater and Griffiths,2007), used a dictionary listing the allowable tagsfor each word in the text.
This dictionary is usu-ally extracted from the manual tagging of the text,contradicting the unsupervised nature of the task.Clearly, the availability of such a dictionary is notalways a reasonable assumption (see e.g.
(Gold-water and Griffiths, 2007)).In a different algorithmic direction, (Schuetze,1995) applied latent semantic analysis with SVDbased dimensionality reduction, and (Schuetze,1995; Clark, 2003; Dasgupta and NG, 2007) useddistributional and morphological statistics to findmeaningful word types clusters.
Clark (2003) isthe only such work to have evaluated its algorithmas a POS tagger for large corpora, like we do inthis paper.A Zipfian constraint was utilized in (Goldwaterand et al, 2006) for language modeling and mor-phological disambiguation.The problem of convergence to local maximahas been discussed in (Smith and Eisner, 2005;Haghighi and Klein, 2006; Goldwater and Grif-fiths, 2007; Johnson, 2007; Gao and Johnson,2008) with a detailed demonstration in (Johnson,2007).
All these authors (except Smith and Eisner(2005), see below), however, reported average re-sults over several runs and did not try to identifythe runs that produce high quality tagging.Smith and Eisner (2005) initialized with allweights equal to zero (uninformed, deterministicinitialization) and performed unsupervised modelselection across smoothing parameters by evaluat-ing the training criterion on unseen, unlabeled de-velopment data.
In this paper we show that for thetagger of (Clark, 2003) such a method providesmediocre results (Table 2) even when the train-ing criterion (likelihood or data probability for thistagger) is evaluated on the test set.
Moreover, weshow that our algorithm outperforms existing POStaggers for most evaluation measures (Table 3).Identifying good solutions among many runs ofa randomly-initialized algorithm is a well knownproblem.
We discuss here the work of (Smith andEisner, 2004) that addressed the problem in the un-supervised POS tagging context.
In this work, de-terministic annealing (Rose et al, 1990) was ap-61plied to an HMM model for unsupervised POStagging with a dictionary.
This method is not sen-sitive to its initialization, and while it is not the-oretically guaranteed to converge to a better so-lution than the traditional EM-HMM, it was ex-perimentally shown to achieve better results.
Theproblem has, of course, been addressed in othercontexts as well (see, e.g., (Wang et al, 2002)).4 Experimental Setup and EvaluationSetup.
We used the English WSJ PennTreebankcorpus in our experiments.
We induced POS tagsfor sections 2-21 (43K word types, 950K word in-stances of which 832K (87.6%) are not punctua-tion marks), using Q(ZCC), Q(CT), and CT. Forthe unsupervised quality test, we trained the bi-gram class-based language model on sections 2-21with the induced clusters, and computed its per-plexity on section 23.In Q(ZCC) and Q(CT), the base taggers wererun a 100 times each, using different random ini-tializations.
In each run we induce 13 clusters,since this is the number of unique POS tags re-quired to cover 98% of the word types in WSJ(Figure 3)4.
Some previous work (e.g., (Smith andEisner, 2005)) also induced 13 non-punctuationtags.We compare the results of our algorithm tothose of the original Clark algorithm5.
The in-duced clusters are evaluated against two POS tagsets: one is the full set of WSJ POS tags, and theother consists of the non-punctuation tags of thefirst set.Punctuation marks constitute a sizeable volumeof corpus tokens and are easy to cluster correctly.Hence, evaluting against the full tag set that in-cludes punctuation artificially increases the qual-ity of the reported results, which is why we reportresults for the non-punctuation tag set.
However,to be able to directly compare with previous work,we also report results for the full WSJ POS tagset.
We do so by assigning a singleton cluster toeach punctuation mark (in addition to the 13 clus-ters).
This simple heuristic yields very high per-formance on punctuation, scoring (when all otherterminals are assumed perfect tagging) 99.6% in1-to-1 accuracy.4Some words can get more than one POS tag.
In the fig-ure, for these words we increased the counters of all theirpossible tags.5Downloaded from www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html.In addition to comparing the different algo-rithms, we compare the correlation between ourtagging quality test and external clustering qualityfor both the original CT algorithm and our ZCCalgorithm.Clustering Quality Evaluation.
The inducedPOS tags have arbitrary names.
To evaluate themagainst a manually annotated corpus, a propercorrespondence with the gold standard POS tagsshould be established.
Many evaluation measuresfor unsupervised clustering against gold standardexist.
Here we use measures from two well ac-cepted families: mapping based and informationtheoretic (IT) based.
For a recent discussion onthis subject see (Reichart and Rappoport, 2009).The mapping based measures are accuracy withgreedy many-to-1 (M-1) and with greedy 1-to-1(1-1) mappings of the induced to the gold labels.In the former mapping, two induced clusters canbe mapped to the same gold standard cluster, whilein the latter mapping each and every induced clus-ter is assigned a unique gold cluster.After each induced label is mapped to a goldlabel, tagging accuracy is computed.
Accuracy isdefined to be the number of correctly tagged wordsin the corpus divided by the total number of wordsin the corpus.The IT based measures we use are V (Rosen-berg and Hirschberg, 2007) and NVI (Reichart andRappoport, 2009).
The latter is a normalization ofthe VI measure (Meila, 2007).
VI and NVI inducethe same order over clusterings but NVI values forgood clusterings lie in [0, 1].
For V, the higherthe score, the better the clustering.
For NVI lowerscores imply improved clustering quality.
We usee as the base of the logarithm.Evaluation of the Quality Test.
To mea-sure the correlation between the score producedby the tagging quality test and the external qual-ity of a tagging, we use two well accepted mea-sures: Spearman?s rank correlation coefficientand Kendall Tau (Kendall and Dickinson, 1990).These measure the correlation between two sortedlists.
For the computation of these measures, werank the clusterings once according to the identifi-cation criterion and once according to the externalquality measure.The measures are given by the equations:(6) kendall ?
tau = 2(nc?nd)r(r?1)(7) Spearsman = 1 ?
6?ri=1 d2ir(r2?1)620 10 20 30 4000.20.40.60.81Number of POS TagsFractionofItemsFigure 3: The fraction of word types (solid curve)and word instances (dashed curve) labeled withthe k (X axis) most frequent POS tags (in typesand tokens respectively) in sections 2-21 of theWSJ corpus.where r is the number of runs (100 in our case),nc and nd are the numbers of concordant and dis-cordant pairs respectively6 and di is the absolutevalue of the difference between the ranks of itemi.The two measures have the properties that aperfect agreement between rankings results in ascore of 1, a perfect disagreement results in a scoreof ?1, completely independent rankings have thevalue of 0 on the average, the range of values isbetween ?1 and 1, and increasing values implyincreasing agreement between the rankings.
For adiscussion see (Lapata, 2006).5 ResultsTable 1 presents the results of the Q(ZCC) andQ(CT) algorithms, which are both better thanthose of the original Clark tagger CT.
The Q al-gorithms provide a tagging that is better than thatproduced by CT in 82-100% (Q(ZCC)) and 75-100% (Q(CT)) of the cases.The Q(ZCC) algorithm is superior when eval-uated with the mapping based measures.
TheQ(CT) algorithm is superior when evaluated withthe IT measures.Table 3 presents reported results for all recentalgorithms we are aware of that tackled the taskof unsupervised POS induction from plain text 7.The settings of the various experiments vary interms of the exact gold annotation scheme usedfor evaluation (the full WSJ set was used by allauthors except Goldwater and Griffiths (2007) and6A pair r, t in two lists X and Y is concordant ifsign(Xt ?
Xr) = sign(Yt ?
Yr), where Xr is the indexof r in the list X .7VG and GG used 2 as the base of the logarithm in ITmeasures, which affects VI.
We converted the VI numbersreported in their papers to base e.the GGTP-17 model which used the set of 17coarse grained tags proposed by (Smith and Eis-ner, 2005)) and the size of the test set.
The num-bers reported for the algorithms of other works arethe average performance over multiple runs, sinceno method for identification of high quality tag-gings was used.The results of our algorithms are superior, ex-cept for the M-1 performance of some of the mod-els of (Johnson, 2007) and of the GGTP-17 andGGTP-45 models of (Grac?a et al, 2009).
Notethat the models of (Johnson, 2007) and the GGTP-45 model induce 40-50 clusters compared to our34 (13 non-punctuation plus the additional 21 sin-gleton punctuation tags).
Increasing the numberof clusters is known to improve the M-1 mea-sure (Reichart and Rappoport, 2009).
GGTP-17gives the best M-1 results, but its 1-1 results aremuch worse than those of Q(ZCC), Q(CT), andCT, and the information theoretic measures V andNVI were not reported for it.Recall that the Q algorithms tag punctuationmarks according to the scheme which assigns eachof them a unique cluster (Section 4), while previ-ous work does not distinguish punctuation marksfrom other tokens.
To quantify the effect vari-ous punctuation schemes have on the results re-ported in Table 3, we evaluated the ?iHMM: PY-fixed?
model (Van Gael et al, 2009) and the Q al-gorithms when punctuation is excluded and whenboth PY-fixed and Q algorithms use the punctua-tion scheme described in Section 4.For the PY-fixed, which induces 91 clusters,results are (punctuation is excluded, heuristic isused): V(0.530, 0.608), NVI (0.999, 0.823), 1-1(0.484, 0.543), M-1 (0.591, 0.639).
The resultsfor the Q algorithms are given in Table 1 (topline: excluding punctuation, bottom line: usingthe heuristic).
The Q algorithms are better for theV, NVI and 1-1 measures.
For M-1 evaluation,PY-fixed, which induces substantially more clus-ters (91 compared to our 34) is better.In what follows, we provide an analysis of thecomponents of our algorithms.
To explore thequality of our tagging component, ZCC, table 4compares the mean, mode and standard deviationof a 100 runs of ZCC with 100 runs of the originalCT algorithm8.
The performance of the tagging8In mode calculation we treat the 100 runs as samples ofa continuous random variable.
We divide the results rangeto 10 bins of the same size.
The mode is the center of thebin having the largest number of runs.
If there is more than63Alg.
V NVI 1-1 M-1Q(ZCC)no punct.
0.538 (85, 2.6) 0.849 (82, 3.2) 0.521 (100, 4.3) 0.533 (84, 1.7)with punct.
0.637 (85, 1.8) 0.678 (82, 2.6) 0.58 (100, 3) 0.591 (84, 1.18)Q(CT)no punct.
0.545 (92, 3.3) 0.837 (88, 4.4) 0.492 (99,1.4) 0.526 (75, 1)with punct.
0.644 (92, 2.5) 0.662 (88, 4.2) 0.555 (99, 0.5) 0.585 (75, 0.58)Table 1: Quality of the tagging produced by Q(ZCC) and Q(CT).
The top (bottom) line for each algorithmpresents the results when punctuation is not included (is included) in the evaluation (Section 4).
The leftnumber in the parentheses is the fraction of Clark?s (CT) results that scored worse than our models (%from 100 runs).
The right number in the parentheses is 100 times the difference between the score of ourmodel and the mean score of 100 runs of Clark?s (CT).
Q(ZCC) is better than Q(CT) in the mappingsmeasures, while Q(CT) is better in the IT measures.
Both are better than the original Clark tagger CT.Data Probability Likelihood PerplexityV m-to-1 V m-to-1 V m-to-1Alg.
SRC KT SRC KT SRC KT SRC KT SRC KT SRC KTCT 0.2 0.143 0.071 0.045 0.338 0.23 0.22 0.148 0.568 0.397 0.476 0.33ZCC 0.134 0.094 0.118 0.078 0.517 0.352 0.453 0.321 0.82 0.62 0.659 0.484Table 2: Correlation of unsupervised quality measures (columns) with clustering quality of two basetaggers (CT and ZCC, rows).
Correlation is measured by Spearman (SRC) and Kendall Tau (KT) rankcorrelation coefficients.
The quality measures are data probability (left part), likelihood (middle side)and perplexity (right part), and correlation is between these and two of the external evaluation measures,m-to-1 mapping and V (results for the other two clustering evaluation measures, 1-1 mapping and NVI,are very similar).
Results for the perplexity quality test used by family Q are superior; data probabilityand likelihood provide only a mediocre indication for the quality of induced clustering.
Note that thecorrelation values are much higher for ZCC than for CT.components are quite similar, with a small advan-tage to CT in mean and to ZCC in mode.Our quality test is based on the perplexity of aclass bigram language model trained with the in-duced tagging.
To emphasize its strength we com-pare it to two natural quality tests: the likelihoodand value of the probability function to which thetagging algorithm converges (equations (2) and (1)in Section 2.1).
The results are shown in Table2 First, we see that our perplexity quality test ismuch better correlated with the quality of the tag-ging induced by both ZCC and CT. Second, thecorrelation is indeed much higher for ZCC thanfor CT.The power of Q(ZCC) lies in the combinationbetween the perplexity-based quality test and thetagging component ZCC.
The performance of thetagging component ZCC does not provide a def-inite improvement over the original Clark tagger.ZCC compromises mean tagging results for an im-proved correlation between Q?s quality measureone such bin, we average their centers.
We use this techniquesince it is rare to see two different runs of either algorithmwith the exact same quality.and gold standard-based tagging evaluation.6 ConclusionIn this paper we addressed unsupervised POS tag-ging as a task where the quality of a single tag-ging is to be reported, rather than the average per-formance of a tagging algorithm over many runs.We introduced a family of algorithms Q(B) basedon an unsupervised test for tagging quality that isused to select a high quality tagging from the out-put of multiple runs of a POS tagger B.We introduced the ZCC tagger which modifiesthe original Clark tagger by constraining the clus-tering space using a cluster type size Zipfian con-straint, conforming with a known property of nat-ural languages.We showed that the tagging produced by ourQ(ZCC) algorithm is better than that of the Clarkalgorithm with a probability of 82-100%, depend-ing on the measure used.
Moreover, our taggingoutperforms in most evaluation measures the re-sults reported in all recent works that addressedthe task.In future work, we intend to try to improve64Alg.
V VI M-1 1-1Q(ZCC) 0.637 2.06 0.591 0.58Q(CT) 0.644 2.01 0.585 0.555CT 0.619 2.14 0.576 0.543HK ?
?
?
0.413J ?
4.23 -5.740.43 -0.620.37 ?0.47GG ?
2.8 ?
?G-J ?
4.03 ?4.47?
0.4 ?0.499VG 0.54 -0.592.49 ?2.91?
?GGTP-45 ?
?
0.654 0.445GGTP-17 ?
?
0.702 0.495Table 3: Comparison of our algorithms with therecent fully unsupervised POS taggers for whichresults are reported.
HK: (Haghighi and Klein,2006), trained and evaluated with a corpus of193K tokens and 45 induced tags.
GG: (Goldwa-ter and Griffiths, 2007), trained and evaluated witha corpus of 24K tokens and 17 induced tags.
J :(Johnson, 2007) inducing 25-50 tags (the resultsthat are higher than Q in the M-1 measure are for40-50 tags).
GJ: (Gao and Johnson, 2008), induc-ing 50 tags.
VG: (Van Gael et al, 2009), inducing47-192 tags.
GGTP-45: (Grac?a et al, 2009), in-ducing 45 tags.
GGTP-17: (Grac?a et al, 2009),inducing 17 tags.
All five were trained and evalu-ated with the full WSJ PTB (1.17M words).
LowerVI values indicates better clustering.Statistic V NVI M-1 1-1CTMean 0.512 0.881 0.516 0.478Mode 0.502 0.886 0.514 0.465Std 0.022 0.035 0.018 0.028ZCCMean 0.503 0.908 0.512 0.478Mode 0.509 0.907 0.518 0.47Std 0.021 0.036 0.018 0.0295Table 4: Average performance of ZCC comparedwith CT (results presented without punctuation).Presented are mean, mode (see text for its calcu-lation), and standard deviation (std).
CT mean re-sults are slightly better, and both algorithms haveabout the same standard deviation.
ZCC sacrificesa small amount of mean quality for a good corre-lation with our quality test, which allows Q(ZCC)to be much better than the mean of CT and mostof its runs.our quality measure, experiment with additionallanguages, and apply the ?family of algorithms?paradigm to additional relevant NLP tasks.ReferencesMichele Banko and Robert C. Moore, 2003.
Part ofSpeech Tagging in Context.
COLING ?04.Chris Biemann, 2006.
Unsupervised Part-of-Speech Tagging Employing Efficient Graph Cluster-ing.
COLING-ACL ?06 Student Research Work-shop.Thorsten Brants, 1997.
The NEGRA Export Format.CLAUS Report, Saarland University.Eric Brill, 1995.
Unsupervised Learning if Disam-biguation Rules for Part of Speech Tagging.
3rdWorkshop on Very Large Corpora.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouze, Jenifer C. Lai and Robert Mercer, 1992.Class-Based N-Gram Models of Natural Language.Computational Linguistics, 18:467-479.Alexander Clark, 2003.
Combining Distributional andMorphological Information for Part of Speech In-duction.
EACL ?03.Sajib Dasgupta and Vincent Ng, 2007.
Unsu-pervised Part-of-Speech Acquisition for Resource-Scarce Languages.
EMNLP ?07.Steven Finch, Nick Chater and Martin Redington,1995.
Acquiring syntactic information from distri-butional statistics.
Connectionist models of memoryand language.
UCL Press, London.Dayne Freitag, 2004.
Toward Unsupervised Whole-Corpus Tagging.
COLING ?04.Jianfeng Gao and Mark Johnson, 2008.
A compari-son of Bayesian estimators for unsupervised HiddenMarkov Model POS taggers.
EMNLP ?08.Yoav Goldberg, Meni Adler and Michael Elhadad,2008.
EM Can Find Pretty Good HMM POS-Taggers (When Given a Good Start).
ACL ?08Sharon Goldwater, Tom Griffiths, and Mark Johnson,2006.
Interpolating between types and tokens by es-timating power-law generators.
NIPS ?06.Sharon Goldwater and Tom Griffiths, 2007.
A fullyBayesian approach to unsupervised part-of-speechtagging.
ACL ?07.Joshua Goodman, 2001.
A Bit of Progress in Lan-guage Modeling, Extended Version.
Microsoft Re-search Technical Report MSR-TR-2001-72.Joa?o Grac?a, Kuzman Ganchev, Ben Taskar and Fre-nando Pereira, 2009.
Posterior vs. Parameter Spar-sity in Latent Variable Models.
NIPS ?09.65Maurice Kendall and Jean Dickinson, 1990.
RankCorrelation methods.
Oxford University Press, NewYork.Aria Haghighi and Dan Klein, 2006.
Prototype-drivenLearning for Sequence Labeling.
HLT-NAACL ?06.Mark Johnson, 2007.
Why Doesnt EM Find GoodHMM POS-Taggers?
EMNLP-CoNLL ?07.Harold W. Kuhn, 1955.
The Hungarian method forthe assignment problem.
Naval Research LogisticsQuarterly, 2:83-97.Mirella Lapata, 2006.
Automatic Evaluation of In-formation Ordering: Kendall?s Tau.
ComputationalLinguistics, 4:471-484.Sven Martin, Jorg Liermann, and Hermann Ney, 1998.Algorithms for bigram and trigram word clustering.Speech Communication, 24:19-37.Marina Meila, 2007.
Comparing Clustering - an In-formation Based Distance.
Journal of MultivariateAnalysis, 98:873-895.Bernard Merialdo, 1994.
Tagging English Text witha Probabilistic Model.
Computational Linguistics,20(2):155-172.Michael Mitzenmacher , 2004.
A Brief History ofGenerative Models for Power Law and LognormalDistributions.
Internet Mathematics, 1(2):226-251.James Munkres, 1957.
Algorithms for the Assignmentand Transportation Problems.
Journal of the SIAM,5(1):32-38.Hermann Ney, Ute Essen, and Reinhard Kneser,1994.
On structuring probabilistic dependencies instochastic language modelling.
Computer Speechand Language, 8:1-38.Sujith Ravi and Kevin Knight, 2009.
Minimized Mod-els for Unsupervised Part-of-Speech Tagging.
ACL?09.Roi Reichart and Ari Rappoport, 2009.
The NVI Clus-tering Evaluation Measure.
CoNLL ?09.Kenneth Rose, Eitan Gurewitz, and Geoffrey C. Fox,1990.
Statistical Mechanics and Phase Transitionsin Clustering.
Physical Review Letters, 65(8):945-948.Andrew Rosenberg and Julia Hirschberg, 2007.
V-Measure: A Conditional Entropy-Based ExternalCluster Evaluation Measure.
EMNLP ?07.Hinrich Schuetze, 1995.
Distributional part-of-speechtagging.
EACL ?95.Noah A. Smith and Jason Eisner, 2004.
AnnealingTechniques for Unsupervised Statistical LanguageLearning.
ACL ?04.Noah A. Smith and Jason Eisner, 2005.
ContrastiveEstimation: Training Log-Linear Models on Unla-beled Data.
ACL ?05.Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,and Regina Barzilay, 2009.
Adding More Lan-guages Improves Unsupervised Multilingual Part-of-Speech Tagging: A Bayesian Non-ParametricApproach.
NAACL ?09.Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,and Regina Barzilay, 2008.
Unsupervised Multi-lingual Learning for POS Tagging.
EMNLP ?08.Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahra-mani, 2009.
The Infinite HMM for UnsupervisedPOS Tagging.
EMNLP ?09.Qin Iris Wang and Dale Schuurmans, 2005.
Im-proved Estimation for Unsupervised Part-of-SpeechTagging.
IEEE NLP-KE ?05.Shaojun Wang, Dale Schuurmans and Yunxin Zhao,2002.
The Latent Maximum Entropy Principle.ISIT ?02.Qiuye Zhao and Mitch Marcus, 2009.
A Simple Un-supervised Learner for POS Disambiguation RulesGiven Only a Minimal Lexicon.
EMNLP ?09.66
