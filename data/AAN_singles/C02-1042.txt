Using Knowledge to Facilitate Factoid Answer PinpointingEduard Hovy, Ulf Hermjakob, Chin-Yew Lin, Deepak RavichandranInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292-6695USA{hovy,ulf,cyl,ravichan}@isi.eduAbstractIn order to answer factoid questions, theWebclopedia QA system employs arange of knowledge resources.
Theseinclude a QA Typology with answerpatterns, WordNet, information abouttypical numerical answer ranges, andsemantic relations identified by a robustparser, to filter out likely-looking butwrong candidate answers.
This paperdescribes the knowledge resources andtheir impact on system performance.1.
IntroductionThe TREC evaluations of QA systems(Voorhees, 1999) require answers to be drawnfrom a given source corpus.
Early QA systemsused a simple filtering technique, question worddensity within a fixed n-word window, topinpoint answers.
Robust though this may be,the window method is not accurate enough.
Inresponse, factoid question answering systemshave evolved into two types:?
Use-Knowledge: extract query words fromthe input question, perform IR against thesource corpus, possibly segment resultingdocuments, identify a set of segmentscontaining likely answers, apply a set ofheuristics that each consults a differentsource of knowledge to score eachcandidate, rank them, and select the best(Harabagiu et al, 2001; Hovy et al, 2001;Srihari and Li, 2000; Abney et al, 2000).?
Use-the-Web: extract query words from thequestion, perform IR against the web,extract likely answer-bearing sentences,canonicalize the results, and select the mostfrequent answer(s).
Then, for justification,locate examples of the answers in the sourcecorpus (Brill et al, 2001; Buchholz, 2001).Of course, these techniques can be combined:the popularity ratings from Use-the-Web canalso be applied as a filtering criterion (Clarke etal., 2001), or the knowledge resource heuristicscan filter the web results.
However, simplygoing to the web without using furtherknowledge (Brill et al, 2001) may return theweb?s majority opinions on astrology, the killersof JFK, the cancerous effects of microwaveovens, etc.
?fun but not altogether trustworthy.In this paper we describe the range offiltering techniques our system Webclopediaapplies, from simplest to most sophisticated, andindicate their impact on the system.2.
Webclopedia ArchitectureAs shown in Figure 1, Webclopedia adopts theUse-Knowledge architecture.
Its modules aredescribed in more detail in (Hovy et al, 2001;Hovy et al, 1999):?
Question parsing: Using BBN?sIdentiFinder (Bikel et al, 1999), theCONTEX parser (Hermjakob, 1997)produces a syntactic-semantic analysis ofthe question and determines the QA type.?
Query formation: Single- and multi-wordunits (content words) are extracted from theanalysis, and WordNet synsets (Fellbaum,1998) are used for query expansion.
Aseries of Boolean queries of decreasingspecificity is formed.?
IR: The publicly available IR engine MG(Witten et al, 1994) returns the top-rankedN documents.?
Selecting and ranking sentences: For eachdocument, the most promising K sentencesare located and scored using a formula thatrewards word and phrase overlap with thequestion and its expanded query words.
Resultsare ranked.?
Parsing candidates: CONTEX parses thetop-ranked 300 sentences.?
Pinpointing: As described in Section 3, anumber of knowledge resources are used toperform filtering/pinpointing operations.?
Ranking of answers: The candidateanswers?
scores are compared and thewinner(s) are output.3.
Knowledge Used for Pinpointing3.1   Type 1: Question Word MatchingUnlike (Prager et al, 1999), we do not firstannotate the source corpus, but perform IRdirectly on the source text, using MG (Witten etal., 1994).
To determine goodness, we assign aninitial base score to each retrieved sentence.
Wethen compare the sentence to the question andadapt this score as follows:?
exact matches of proper names double thebase score.?
matching an upper-cased term adds a 60%bonus of the base score for multi-wordsterms and 30% for single words (matching?United States?
is better than just ?United?).?
matching a WordNet synonym of a termdiscounts by 10% (lower case) and 50%(upper case).
(When ?Cage?
matches?cage?, the former may be the last name of aperson and the latter an object; the casemismatch signals less reliability.)?
lower-case term matches after Porterstemming are discounted 30%; upper-casematches 70% (Porter stemming is moreaggressive than WordNet stemming).?
Porter stemmer matches of both questionand sentence words with lower case arediscounted 60%; with upper case, 80%.?
if CONTEX indicates a term as beingqsubsumed (see Section 3.9) the term isdiscouned 90% (in ?Which countrymanufactures weapons of massdestruction?
?, ?country?
will be marked asqsubsumed).The top-scoring 300 sentences are passed on forfurther filtering.3.2  Type 2: Qtargets, the QA Typology,and the Semantic OntologyWe classify desired answers by their semantictype, which have been taxonomized in theWebclopedia QA Typology (Hovy et al, 2002),Candidate answer parsing?
Steps: parse sentences?
Engines: CONTEXMatching?
Steps: match general constraint patterns against parse treesmatch desired semantic type against parse tree elementsassign score to words in sliding window?
Engine: MatcherRanking and answer extraction?
Steps: rank candidate answersextract and format them?
Engine: Answer ranker/formatterQA typology?
QA types, categorized in taxonomyConstraint patterns?
Identify likely answers in relation toother parts of the sentenceCreate queryRetrieve documentsSelect & rank sentencesParse top sentencesParse questionInput questionPerform additional inferenceRank and prepare answersOutput answersQuestion parsing?
Steps: parse questionfind desired semantic type?
Engines:  IdentiFinder  (BBN)CONTEXMatch sentences against answersQuery creation?
Steps: extract, combine important wordsexpand query words using WordNetcreate queries, order by specificity?
Engines: Query creatorIR?
Steps: retrieve top 1000 documents?
Engines: MG (RMIT Melbourne)Sentence selection and ranking?
Steps: score each sentence in each documentrank sentences and pass top 300 along?
Engines:RankerFigure 1.
Webclopedia architecture.http://www.isi.edu/natural-language/projects/webclopedia/Taxonomy/taxonomy_toplevel.html).The currently approx.
180 classes,  which wecall qtargets, were developed after an analysis ofover 17,000 questions (downloaded in 1999from answers.com) and later enhancements toWebclopedia.
They are of several types:?
common semantic classes such as PROPER-PERSON, EMAIL-ADDRESS, LOCATION,PROPER-ORGANIZATION;?
classes particular to QA such as YES:NO,ABBREVIATION-EXPANSION, and WHY-FAMOUS;?
syntactic classes such as NP and NOUN,when no semnatic type can be determined(e.g., ?What does Peugeot manufacture??);?
roles and slots, such as REASON and TITLE-P respectively, to indicate a desired relationwith an anchoring concept.Given a question, the CONTEX parser uses aset of 276 hand-built rules to identify its mostlikely qtarget(s), and records them in a backoffscheme (allowing more general qtarget nodes toapply when more specific ones fail to find amatch).
The generalizations are captured in atypical concept ontology, a 10,000-node extractof WordNet.The recursive part of pattern matching isdriven mostly by interrogative phrases.
Forexample, the rule that determines theapplicability of the qtarget WHY-FAMOUSrequires the question word ?who?, followed bythe copula, followed by a proper name.
Whenthere is no match at the current level, the systemexamines any interrogative constituent, or wordsin special relations to it.
For example, theqtarget TEMPERATURE-QUANTITY (as in?What is the melting point of X??
requires assyntactic object something that in the ontology issubordinate to TEMP-QUANTIFIABLE-ABS-TRACT with, as well, the word ?how?
pairedwith ?warm?, ?cold?, ?hot?, etc., or the phrase?how many degrees?
and a TEMPERATURE-UNIT (as defined in the ontology).3.3 Type 3: Surface Pattern MatchingOften qtarget answers are expressed using ratherstereotypical words or phrases.
For example, theyear of birth of a person is typically expressedusing one of these phrases:<name> was born in <birthyear><name> (<birthyear>?<deathyear>)We have developed a method to learn suchpatterns automatically from text on the web(Ravichandran and Hovy, 2002).
We haveadded into the QA Typology the patterns forappropriate qtargets (qtargets with closed-listanswers, such as PLANETS, require no patterns).Where some QA systems use such patternsexclusively (Soubbotin and Soubbotin, 2001) orpartially (Wang et al, 2001; Lee et al, 2001),we employ them as an additional source ofevidence for the answer.
Preliminary results onfor a range of qtargets, using the TREC-10questions and the TREC corpus, are:Question type(qtarget)Number ofquestionsMRR onTREC docsBIRTHYEAR 8 0.47875INVENTORS 6 0.16667DISCOVERERS 4 0.1250DEFINITIONS 102 0.3445WHY-FAMOUS 3 0.6666LOCATIONS 16 0.753.4  Type 4: Expected Numerical RangesQuantity-targeting questions are oftenunderspecified and rely on culturally sharedcooperativeness rules and/or world knowledge:Q: How many people live in Chile?S1: ?From our correspondent comes goodnews about the nine people living in  Chile?
?A1: nineWhile certainly nine people do live in Chile,we know what the questioner intends.
We havehand-implemented a rule that provides defaultrange assumptions for POPULATION questionsand biases quantity questions accordingly.3.5 Type 5: Abbreviation ExpansionAbbreviations often follow a pattern:Q: What does NAFTA stand for?S1: ?This range of topics includes the NorthAmerican Free Trade Agreement, NAFTA,and the world trade agreement GATT.
?S2: ?The interview now changed to the subjectof trade and pending economic issues, such asthe issue of opening the rice market, NAFTA,and the issue of Russia repaying economiccooperation funds.
?After Webclopedia identifies the qtarget asABBREVIATION-EXPANSION, it extractspossible answer candidates, including ?NorthAmerican Free Trade Agreement?
from S1 and?the rice market?
from S2.
Rules for acronymmatching easily prefer the former.3.6 Type 6: Semantic Type MatchingPhone numbers, zip codes, email addresses,URLs, and different types of quantities obeylexicographic patterns that can be exploited formatching, as inQ: What is the zip code for Fremont, CA?S1: ?
?from Everex Systems Inc., 48431Milmont Drive, Fremont, CA 94538.?andQ: How hot is the core of the earth?S1.
?The temperature of Earth?s inner coremay be as high as 9,000 degrees Fahrenheit(5,000 degrees Celsius).
?Webclopedia identifies the qtargets respectivelyas ZIP-CODE and TEMPERATURE-QUANTITY.Approx.
30 heuristics (cascaded) apply to theinput before parsing to mark up numbers andother orthographically recognizable units of allkinds, including (likely) zip codes, quotations,year ranges, phone numbers, dates, times,scores, cardinal and ordinal numbers, etc.Similar work is reported in (Kwok et al, 2001).3.7 Type 7: Definitions from WordNetWe have found a 10% increase in accuracy inanswering definition questions by using externalglosses obtained from WordNet.
ForQ: What is the Milky Way?Webclopedia identified two leading answercandidates:A1: outer regionsA2: the galaxy that contains the EarthComparing these with the WordNet gloss:WordNet: ?Milky Way?the galaxy containingthe solar system?allows Webclopedia to straightforwardly matchthe candidate with the greater word overlap.Curiously, the system also needs to useWordNet to answer questions involvingcommon knowledge, as in:Q: What is the capital of the United States?because authors of the TREC collection do notfind it necessary to explain what Washington is:Ex: ?Later in the day, the president returned toWashington, the capital of the United States.
?While WordNet?s definitionWordnet: ?Washington?the capital of theUnited States?directly provides the answer to the matcher, italso allows the IR module to focus its search onpassages containing ?Washington?, ?capital?,and ?United States?, and the matcher to pick agood motivating passage in the source corpus.Clearly, this capability can be extended toinclude (definitional and other) informationprovided by other sources, includingencyclopedias and the web (Lin 2002).3.8 Type 8: Semantic Relation MatchingSo far, we have considered individual words andgroups of words.
But often this is insufficient toaccurately score an answer.
As also noted in(Buchholz, 2001), pinpointing can be improvedsignificantly by matching semantic relationsamong constituents:Q: Who killed Lee Harvey Oswald?Qtargets: PROPER-PERSON & PROPER-NAME,PROPER-ORGANIZATIONS1: ?Belli?s clients have included Jack Ruby,who killed John F. Kennedy assassin LeeHarvey Oswald, and Jim and Tammy Bakker.
?S2: ?On Nov. 22, 1963, the building gainednational notoriety when Lee Harvey Oswaldallegedly shot and killed President John F.Kennedy from a sixth floor window as thepresidential motorcade passed.
?The CONTEX parser (Hermjakob, 1997;2001) provides the semantic relations.
Theparser uses machine learning techniques to builda robust grammar that produces semanticallyannotated syntax parses of English (and Koreanand Chinese) sentences at approx.
90% accuracy(Hermjakob, 1999).The matcher compares the parse trees of S1and S2 to that of the question.
Both S1 and S2receive credit for matching question words ?LeeHarvey Oswald?
and ?kill?
(underlined), as wellas for finding an answer (bold) of the properqtarget type (PROPER-PERSON).
However, isthe answer ?Jack Ruby?
or ?President John F.Kennedy??
The only way to determine this is toconsider the semantic relationship between thesecandidates and the verb ?kill?
(parse treessimplified, and only portions shown here):[1] Who killed Lee Harvey Oswald?
[S-SNT](SUBJ) [2] Who  [S-INTERR-NP](PRED) [3] Who  [S-INTERR-PRON](PRED) [4] killed  [S-TR-VERB](OBJ) [5] Lee Harvey Oswald  [S-NP](PRED) [6] Lee?Oswald  [S-PROPER-NAME](MOD) [7] Lee  [S-PROPER-NAME](MOD) [8] Harvey  [S-PROPER-NAME](PRED) [9] Oswald  [S-PROPER-NAME](DUMMY) [10] ?
[D-QUESTION-MARK][1] Jack Ruby, who killed John F. Kennedy assassinLee Harvey Oswald  [S-NP](PRED) [2] <Jack Ruby>1  [S-NP](DUMMY) [6] ,  [D-COMMA](MOD) [7] who killed John F. Kennedy assassinLee Harvey Oswald  [S-REL-CLAUSE](SUBJ) [8] who<1>  [S-INTERR-NP](PRED) [10] killed  [S-TR-VERB](OBJ) [11] JFK assassin?Oswald  [S-NP](PRED) [12] JFK?Oswald [S-PROP-NAME](MOD) [13] JFK  [S-PROPER-NAME](MOD) [19] assassin  [S-NOUN](PRED) [20] ?Oswald [S-PROPER-NAME]Although the PREDs of both S1 and S2match that of the question ?killed?, only S1matches ?Lee Harvey Oswald?
as the head ofthe logical OBJect.
Thus for S1, the matcherawards additional credit to node [2] (Jack Ruby)for being the logical SUBJect of the killing(using anaphora resolution).
In S2, the parse treecorrectly records that node [13] (?John F.Kennedy?)
is not the object of the killing.
Thusdespite its being closer to ?killed?, the candidatein S2 receives no extra credit from semanticrelation matching.It is important to note that the matcherawards extra credit for each matching semanticrelationship between two constituents, not onlywhen everything matches.
This granularityimproves robustness in the case of partialmatches.Semantic relation matching applies not onlyto logical subjects and objects, but also to allother roles such as location, time, reason, etc.
(for additional examples see http://www.isi.edu/natural-language/projects/webclopedia/sem-rel-examples.html).
It also applies at not only thesentential level, but at all levels, such as post-modifying prepositional and pre-modifyingdeterminer phrasesAdditionally, Webclopedia uses 10 lists ofword variations with a total of 4029 entries forsemantically related concepts such as ?toinvent?, ?invention?
and ?inventor?, and rulesfor handling them.
For example, via coercing?invention?
to ?invent?, the system can give?Johan Vaaler?
extra credit for being a likelylogical subject of ?invention?
:Q: Who invented the paper clip?Qtargets: PROPER-PERSON & PROPER-NAME,PROPER-ORGANIZATIONS1: ?The paper clip, weighing a desk-crushing1,320 pounds, is a faithful copy of NorwegianJohan Vaaler?s 1899 invention, said PerLangaker of the Norwegian School ofManagement.
?while ?David?
actually loses points for beingoutside of the clausal scope of the inventing:S2: ?
?Like the guy who invented the safety pin,or the guy who invented the paper clip,?
Davidadded.
?3.9 Type 9: Word Window ScoringWebclopedia also includes a typical window-based scoring module that moves a window overthe text and assigns a score to each windowposition depending on a variety of criteria (Hovyet al, 1999).
Unlike (Clarke et al, 2001; Lee etal., 2001; Chen et al, 2001), we have notdeveloped a very sophisticated scoring function,preferring to focus on the modules that employinformation deeper than the word level.This method is applied only when no othermethod provides a sufficiently high-scoringanswer.
The window scoring function isS  = (500/(500+w))*(1/r) * ?
[(?I1.5*q*e*b*u)1.5]Factors:w: window width (modulated by gaps ofvarious lengths: ?white house?
?
?white car andhouse?
),r: rank of qtarget in list returned byCONTEX,I: window word information content (inverselog frequency score of each word), summed,q: # different question words matched, plusspecific rewards (bonus q=3.0),e: penalty if word matches one of questionword?s WordNet synset items (e=0.8),b: bonus for matching main verb, propernames, certain target words (b=2.0),u: (value 0 or 1) indicates whether a word hasbeen qsubsumed (?subsumed?
by the qtarget)and should not contribute (again) to the score.For example, ?In what year did Columbusdiscover America??
the qsubsumed words are?what?
and ?year?.4.
Performance EvaluationIn TREC-10?s QA track, Webclopedia receivedan overall Mean Reciprocal Rank (MRR) scoreof 0.435, which put it among the top 4performers of the 68 entrants (the average MRRscore for the main QA task was about 0.234).The pinpointing heuristics are fairly accurate:when Webclopedia finds answers, it usuallyranks them in the first place (1st place: 35.5%;2nd: 8.94%; 3rd: 5.69%; 4th: 3.05%; 5th: 5.28%;not found: 41.87%).We determined the impact of eachknowledge source on system performance, usingthe TREC-10 test corpus using the standardMRR scoring.
We applied the system to thequestions of each knowledge type separately,with and without its specific knowledgesource/algorithm.
Results are shown in Table 1,columns A (without) and B (with).
To indicateoverall effect, we also show (in columns C andD) the percentage of questions in TREC-10 and-9 respecively of each knowledge type.5.
ConclusionsIt is tempting to search for a single techniquethat will solve the whole problem (for example,Ittycheriah et al (2001) focus on the subset offactoid questions answerable by NPs, and train astatistical model to perform NP-oriented answerpinpointing).
Our experience, however, is thateven factoid QA is varied enough to requirevarious special-purpose techniques andknowledge.
The theoretical limits of the varioustechniques are not known, though Light et al?s(2001) interesting work begins to study this.Column A: % questions of the knowledge typeanswered correctly without using knowlegeColumn B: % questions, now using knowledgeColumn C: % questions of type in TREC-10Column D: % questions of type in TREC-9A B C DAbbreviation exp.
20.0 70.0  1.0 2.3Number ranges 50.0 50.0  1.2 1.8WordNet (def Qs) 48.3 67.5 20.9 5.1Semantic types- locator types N/A N/A  0.0 0.4- quantity types 22.5 48.7 10.8 5.5- date/year types 45.0 57.3  9.2 10.2Patterns- definitions ?
34.4 20.9 5.1- why-famous  ?
66.7 0.6 ?- locations ?
75.0 3.2 ?- birthyear ?
47.9 1.6 ?Semantic relations 39.4 46.5 72.2 85.7Table 1.
Performance of knowledge sources.Semantic relation scores measured only onquestions in which they could logically apply.We conclude that factoid QA performancecan be significantly improved by the use ofknowledge attuned to specific question typesand specific information characteristics.
Most ofthe techniques for exploiting this knowledgerequire learning to ensure robustness.
Toimprove performance beyond this, we believe acombination of going to the web and turning todeeper world knowledge and automatedinference (Harabagiu et al, 2001) to be theanswer.
It remains an open question how muchwork these techniques would require, and whattheir payoff limits are.ReferencesAbney, S., M. Collins, and A. Singhal.
2000.
AnswerExtraction.
Proceedings of the Applied NaturalLanguage Processing Conference (ANLP-NAACL-00), Seattle, WA, 296?301.Bikel, D., R. Schwartz, and R. Weischedel.
1999.An Algorithm that Learns What?s in a Name.Machine Learning?Special Issue on NLLearning, 34, 1?3.Brill, E., J. Lin, M. Banko, S. Dumais, and A. Ng.2001.
Data-Intensive Question Answering.Proceedings of the TREC-10 Conference.
NIST,Gaithersburg, MD, 183?189.Buchholz, S. 2001.
Using Grammatical Relations,Answer Frequencies and the World Wide Web forTREC Question Answering.
Proceedings of theTREC-10 Conference.
NIST, 496?503.Chen, J., A.R.
Diekema, M.D.
Taffet, N. McCracken,N.
Ercan Ozgencil, O. Yilmazel, and E.D.
Liddy.2001.
CNLP at TREC-10 QA Track.
Proceedingsof the TREC-10 Conference.
NIST, 480?490.Clarke, C.L.A., G.V.
Cormack, T.R.
Lynam, C.M.
Li,and G.L.
McLearn.
2001.
Web ReinforcedQuestion Answering.
Proceedings of the TREC-10 Conference.
NIST, 620?626.Clarke, C.L.A., G.V.
Cormack, and T.R.
Lynam.2001.
Exploiting Redundancy in QuestionAnswering.
Proceedings of the SIGIRConference.
New Orleans, LA, 358?365.Fellbaum, Ch.
(ed).
1998.
WordNet: An ElectronicLexical Database.
Cambridge: MIT Press.Harabagiu, S., D. Moldovan, M. Pasca, R. Mihalcea,M.
Surdeanu, R. Buneascu, R. G?rju, V. Rus andP.
Morarescu.
2001.
FALCON: BoostingKnowledge for Answer Engines.
Proceedings ofthe 9th Text Retrieval Conference (TREC-9),NIST, 479?488.Hermjakob, U.
1997.
Learning Parse andTranslation Decisions from Examples with RichContext.
Ph.D. dissertation, University of TexasAustin.
file://ftp.cs.utexas.edu/pub/mooney/papers/hermjakob-dissertation 97.ps.gz.Hermjakob, U.
2001.
Parsing and QuestionClassification for Question Answering.Proceedings of the Workshop on QuestionAnswering at ACL-2001.
Toulouse, France.Hovy, E.H., L. Gerber, U. Hermjakob, M. Junk, andC.-Y.
Lin.
1999.
Question Answering inWebclopedia.
Proceedings of the TREC-9Conference.
NIST.
Gaithersburg, MD, 655?673.Hovy, E.H., U. Hermjakob, and D. Ravichandran.2002.
A Question/Answer Typology with SurfaceText Patterns.
Poster in Proceedings of theDARPA Human Language TechnologyConference (HLT).
San Diego, CA, 234?238.Hovy, E.H., U. Hermjakob, and C.-Y.
Lin.
2001.
TheUse of External Knowledge in Factoid QA.Proceedings of the TREC-10 Conference.
NIST,Gaithersburg, MD, 166?174.Ittycheriah, A., M. Franz, and S. Roukos.
2001.IBM?s Statistical Question Answering System.Proceedings of the TREC-10 Conference.
NIST,Gaithersburg, MD, 317?323.Kwok, K.L., L. Grunfeld, N. Dinstl, and M. Chan.2001.
TREC2001 Question-Answer, Web andCross Language experiments using PIRCS.Proceedings of the TREC-10 Conference.
NIST,Gaithersburg, MD, 447?451.Lee, G.G., J. Seo, S. Lee, H. Jung, B-H. Cho, C. Lee,B-K. Kwak, J, Cha, D. Kim, J-H. An, H. Kim,and K. Kim.
2001.
SiteQ: Engineering HighPerformance QA System Using Lexico=SemanticPattern Matching and Shallow NLP.
Proceedingsof the TREC-10 Conference.
NIST, Gaithersburg,MD, 437?446.Light, M., G.S.
Mann, E. Riloff, and E. Breck.
2001.Analyses for Elucidating Current QuestionAnswering Technology.
Natural LanguageEngineering, 7:4, 325?342.Lin, C.-Y.
2002.
The Effectiveness of Dictionary andWeb-Based Answer Reranking.
Proceedings ofthe 19th International Conference onComputational Linguistics (COLING 2002),Taipei, Taiwan.Oh, JH., KS.
Lee, DS.
Chang, CW.
Seo, and KS.Choi.
2001.
TREC-10 Experiments at KAIST:Batch Filtering and Question Answering.Proceedings of the TREC-10 Conference.
NIST,Gaithersburg, MD, 354?361.Prager, J., E. Brown, D.R.
Radev, and K. Czuba.1999.
One Search Engine or Two for QuestionAnswering.
Proceedings of the TREC-9Conference.
NIST, Gaithersburg, MD, 235?240.Ravichandran, D. and E.H. Hovy.
2002.
LearningSurface Text Patterns for a Question AnsweringSystem.
Proceedings of the ACL conference.Philadelphia, PA.Soubbotin, M.M.
and S.M.
Soubbotin.
2001.
Patternsof Potential Answer Expressions as Clues to theRight Answer.
Proceedings of the TREC-10Conference.
NIST, Gaithersburg, MD, 175?182.Srihari, R. and W. Li.
2000.
A Question AnsweringSystem Supported by Information Extraction.Proceedings of the 1st Meeting of the NorthAmerican Chapter of the Association forComputational Linguistics (ANLP-NAACL-00),Seattle, WA, 166?172.Voorhees, E. 1999.
Overview of the QuestionAnswering Track.
Proceedings of the TREC-9Conference.
NIST, Gaithersburg, MD, 71?81.Wang, B., H. Xu, Z. Yang, Y. Liu, X. Cheng, D. Bu,and S. Bai.
2001.
TREC-10 Experiments at CAS-ICT: Filtering, Web, and QA.
Proceedings of theTREC-10 Conference.
NIST, 229?241.Witten, I.H., A. Moffat, and T.C.
Bell.
1994.Managing Gigabytes: Compressing and IndexingDocuments and Images.
New York: VanNostrand Reinhold.
