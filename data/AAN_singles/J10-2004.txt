Re-structuring, Re-labeling, and Re-aligningfor Syntax-Based Machine TranslationWei Wang?Language Weaver, Inc.Jonathan May?
?USC/Information Sciences InstituteKevin Knight?USC/Information Sciences InstituteDaniel Marcu?Language Weaver, Inc.This article shows that the structure of bilingual material from standard parsing and alignmenttools is not optimal for training syntax-based statistical machine translation (SMT) systems.We present three modifications to the MT training data to improve the accuracy of a state-of-the-art syntax MT system: re-structuring changes the syntactic structure of training parse trees toenable reuse of substructures; re-labeling alters bracket labels to enrich rule application context;and re-aligning unifies word alignment across sentences to remove bad word alignments andrefine good ones.
Better structures, labels, and word alignments are learned by the EM algorithm.We show that each individual technique leads to improvement as measured by BLEU, and wealso show that the greatest improvement is achieved by combining them.
We report an overall1.48 BLEU improvement on the NIST08 evaluation set over a strong baseline in Chinese/Englishtranslation.1.
BackgroundSyntactic methods have recently proven useful in statistical machine translation (SMT).In this article, we explore different ways of exploiting the structure of bilingual materialfor syntax-based SMT.
In particular, we ask what kinds of tree structures, tree labels,and word alignments are best suited for improving end-to-end translation accuracy.We begin with structures from standard parsing and alignment tools, then use the EMalgorithm to revise these structures in light of the translation task.
We report an overall+1.48 BLEU improvement on a standard Chinese-to-English test.?
6060 Center Drive, Suite 150, Los Angeles, CA, 90045, USA.
E-mail: wwang@languageweaver.com.??
4676 Admiralty Way, Marina del Rey, CA, 90292, USA.
E-mail: jonmay@isi.edu.?
4676 Admiralty Way, Marina del Rey, CA, 90292, USA.
E-mail: knight@isi.edu.?
6060 Center Drive, Suite 150, Los Angeles, CA, 90045, USA.
E-mail: dmarcu@languageweaver.com.Submission received: 6 November 2008; revised submission received: 10 September 2009; accepted forpublication: 1 January 2010.?
2010 Association for Computational LinguisticsComputational Linguistics Volume 36, Number 2We carry out our experiments in the context of a string-to-tree translation system.This system accepts a Chinese string as input, and it searches through a multiplicity ofEnglish tree outputs, seeking the one with the highest score.
The string-to-tree frame-work is motivated by a desire to improve target-language grammaticality.
For example,it is common for string-basedMT systems to output sentences with no verb.
By contrast,the string-to-tree framework forces the output to respect syntactic requirements?forexample, if the output is a syntactic tree whose root is S (sentence), then the S will gen-erally have a child of type VP (verb phrase), which will in turn contain a verb.
Anothermotivation is better treatment of function words.
Often, these words are not literallytranslated (either by themselves or as part of a phrase), but rather they control whathappens in the translation, as with case-marking particles or passive-voice particles.Finally, much of the re-ordering we find in translation is syntactically motivated, andthis can be captured explicitly with syntax-based translation rules.
Tree-to-tree systemsare also promising, but in this work we concentrate only on target-language syntax.
Thetarget-language generation problem presents a difficult challenge, whereas the sourcesentence is fixed and usually already grammatical.To prepare training data for such a system, we begin with a bilingual text that hasbeen automatically processed into segment pairs.
We require that the segments be singlesentences on the English side, whereas the corresponding Chinese segments may besentences, sentence fragments, or multiple sentences.
We then parse the English side ofthe bilingual text using a re-implementation of the Collins (1997) parsing model, whichwe train on the Penn English Treebank (Marcus, Santorini, and Marcinkiewicz 1993).Finally, we word-align the segment pairs according to IBMModel 4 (Brown et al 1993).Figure 1 shows a sample (tree, string, alignment) triple.We build two generative statistical models from this data.
First, we construct asmoothed n-gram language model (Kneser and Ney 1995; Stolcke 2002) out of theEnglish side of the bilingual data.
This model assigns a probability P(e) to any candidatetranslation, rewarding translations whose subsequences have been observed frequentlyin the training data.Second, we build a syntax-based translation model that we can use to producecandidate English trees fromChinese strings.
Following previouswork in noisy-channelFigure 1A sample learning case for the syntax-based machine translation system described in this article.248Wang et al Re-structuring, Re-labeling, and Re-aligningSMT (Brown et al 1993), our model operates in the English-to-Chinese direction?we envision a generative top?down process by which an English tree is graduallytransformed (by probabilistic rules) into an observed Chinese string.
We represent acollection of such rules as a tree transducer (Knight and Graehl 2005).
In order toconstruct this transducer from parsed and word-aligned data, we use the GHKM ruleextraction algorithm of Galley et al (2004).
This algorithm computes the unique set ofminimal rules needed to explain any sentence pair in the data.
Figure 2 shows all theminimal rules extracted from the example (tree, string, alignment) triple in Figure 1.Note that rules specify rotation (e.g., R1, R5), direct translation (R3, R10), insertion anddeletion (R2, R4), and tree traversal (R9, R7).
The extracted rules for a given exampleform a derivation tree.We collect all rules over the entire bilingual corpus, and we normalize rule countsin this way: P(rule) = count(rule)count(LHS-root(rule)) .
When we apply these probabilities to derive anEnglish sentence e and a corresponding Chinese sentence c, we wind up computing thejoint probability P(e, c).
We smooth the rule counts with Good?Turing smoothing (Good1953).This extraction method assigns each unaligned Chinese word to a default rule inthe derivation tree.
We next follow Galley et al (2006) in allowing unaligned Chinesewords to participate in multiple translation rules.
In this case, we obtain a derivationforest of minimal rules.
Galley et al show how to use EM to count rules over deriva-tion forests and obtain Viterbi derivation trees of minimal rules.
We also follow Galleyet al in collecting composed rules, namely, compositions of minimal rules.
These largerrules have been shown to substantially improve translation accuracy (Galley et al 2006;DeNeefe et al 2007).
Figure 3 shows some of the additional rules.With these models, we can decode a new Chinese sentence by enumerating andscoring all of the English trees that can be derived from it by rule.
The score is aweighted product of P(e) and P(e, c).
To search efficiently, we employ the CKY dynamic-programming parsing algorithm (Yamada and Knight 2002; Galley et al 2006).
Thisalgorithm builds English trees on top of Chinese spans.
In each cell of the CKY matrix,we store the non-terminal symbol at the root of the English tree being built up.
We alsoFigure 2Minimal rules extracted from the learning case in Figure 1 using the GHKM procedure.249Computational Linguistics Volume 36, Number 2Figure 3Additional rules extracted from the learning case in Figure 1.store English words that appear at the left and right corners of the tree, as these areneeded for computing the P(e) score when cells are combined.
For CKY to work, alltransducer rules must be broken down, or binarized, into rules that contain at most twovariables?more efficient search can be gained if this binarization produces rules thatcan be incrementally scored by the language model (Melamed, Satta, and Wellington2004; Zhang et al 2006).
Finally, we employ cube pruning (Chiang 2007) for furtherefficiency in the search.When scoring translation candidates, we add several smaller models.
One modelrewards longer translation candidates, off-setting the language model?s desire for shortoutput.
Other models punish rules that drop Chinese content words or introducespurious English content words.
We also include lexical smoothing models (Gale andSampson 1996; Good 1953) to help distinguish good low-count rules from bad low-count rules.
The final score of a translation candidate is a weighted linear combinationof log P(e), log P(e, c), and the scores from these additional smaller models.
We obtainweights through minimum error-rate training (Och 2003).The system thus constructed performs fairly well at Chinese-to-English translation,as reflected in the NIST06 common evaluation of machine translation quality.1However, it would be surprising if the parse structures and word alignments in ourbilingual data were somehow perfectly suited to syntax-based SMT?we have so farused out-of-the-box tools like IBM Model 4 and a Treebank-trained parser.
Huang andKnight (2006) already investigated whether different syntactic labels would be moreappropriate for SMT, though their study was carried out on a weak baseline translationsystem.
In this article, we take a broad view and investigate how changes to syntacticstructures, syntactic labels, and word alignments can lead to substantial improvementsin translation quality on top of a strong baseline.
We design our methods aroundproblems that arise in MT data whose parses and alignments use some Penn Treebank-style annotations.
We believe that some of the techniques will apply to other annotationschemes, but conclusions here are limited to Penn Treebank-style trees.The rest of this article is structured as follows.
Section 2 describes the corpora andmodel configurations used in our experiments.
In each of the next three sections wepresent a technique for modifying the training data to improve syntax MT accuracy:tree re-structuring in Section 3, tree re-labeling in Section 4, and re-aligning in Section 5.In each of these three sections, we also present experiment results to show the impactof each individual technique on end-to-end MT accuracy.
Section 6 shows the improve-ment made by combining all three techniques.
We conclude in Section 7.1 http://nist.gov/speech/tests/mt/2006/doc/mt06eval official results.html.250Wang et al Re-structuring, Re-labeling, and Re-aligning2.
Corpora for ExperimentsFor our experiments, we use a 245 million word Chinese/English bitext, available fromLDC.
A re-implementation of the Collins (1997) parser runs on the English half of thebitext to produce parse trees, and GIZA runs on the entire bitext to produce M4 wordalignments.
We extract a subset of 36 million words from the entire bitext, by selectingonly sentences in the mainland news domain.
We extract translation rules from theseselected 36 million words.
Experiments show that our Chinese/English syntax MTsystems built from this selected bitext give as high BLEU scores as from the entire bitext.Our development set consists of 1,453 lines and is extracted from the NIST02?NIST05 evaluation sets, for tuning of feature weights.
The development set is from thenewswire domain, andwe chose it to represent awide period of time rather than a singleyear.
We use the NIST08 evaluation set as our test set.
Because the NIST08 evaluation setis a mix of newswire text andWeb text, we also report the BLEU scores on the newswireportion.We use two 5-gram languagemodels.
One is trained on the English half of the bitext.The other is trained on one billion words of monolingual data.
Kneser?Ney smoothing(Kneser and Ney 1995) is applied to both language models.
Language models arerepresented using randomized data structures similar to those of Talbot and Osborne(2007) in decoding for efficient RAM usage.To test the significance of improvements over the baseline, we compute paired boot-strap p-values (Koehn 2004) for BLEU between the baseline system and each improvedsystem.3.
Re-structuring Trees for TrainingOur translation system is trained on Chinese/English data, where the English sidehas been automatically parsed into Penn Treebank-style trees.
One striking fact aboutthese trees is that they contain many flat structures.
For example, base noun phrasesfrequently have five or more direct children.
It is well known in monolingual parsingresearch that these flat structures cause problems.
Although thousands of rewrite rulescan be learned from the Penn Treebank, these rules still do not cover the new rewritesobserved in held-out test data.
For this reason, and to extract more general knowl-edge, many monolingual parsing models aremarkovized so that they can produce flatstructures incrementally and horizontally (Collins 1997; Charniak 2000).
Other parsingsystems binarize the training trees in a pre-processing step, then learn to model thebinarized corpus (Petrov et al 2006); after parsing, their results are flattened backin a post-processing step.
In addition, Johnson (1998b) shows that different types oftree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank IIrepresentation) can have a large effect on the parsing performance of a PCFG estimatedfrom these trees.We find that flat structures are also problematic for syntax-based machine transla-tion.
The rules we learn from tree/string/alignment triples often lack sufficient gener-alization power.
For example, consider the training samples in Figure 4.
We should beable to learn enough from these two samples to translate the new phrase?.W-#L??
Z ?
{ 3?VIKTOR CHERNOMYRDIN AND HIS COLLEAGUEinto its English equivalent victor chernomyrdin and his colleagues.251Computational Linguistics Volume 36, Number 2Figure 4Learning translation rules from flat English structures.However, the learned rules R12 and R13 do not fit together nicely.
R12 can translate?.W-#L??
into an English base noun phrase (NPB) that includes viktorchernomyrdin, but only if it is preceded by words that translate into an English JJ andan English NNP.
Likewise, R13 can translateZ ?
{ 3?into an NPB that includesand his colleagues, but only if preceded by two NNPs.
Both rules want to create an NPB,and neither can supply the other with what it needs.If we re-structure the training trees as shown in Figure 5, we get much betterbehavior.
Now rule R14 translates?.W-#L??
into a free-standing NPB.
Thisgives rule R15 the ability to translateZ ?
{ 3?, because it finds the necessaryNPB to its left.Here, we are re-structuring the trees in our MT training data by binarizing them.This allows us to extract better translation rules, though of course an extracted rule mayhave more than two variables.
Whether the rules themselves should be binarized is aseparate question, addressed in Melamed, Satta, andWellington (2004) and Zhang et al(2006).
One can decide to re-structure training data trees, binarize translation rules, ordo both, or do neither.
Here we focus on English tree re-structuring.In this section, we explore the generalization ability of simple re-structuring meth-ods like left-, right-, and head-binarization, and also their combinations.
Simple bina-rization methods binarize syntax trees in a consistent fashion (left-, right-, or head-)and thus cannot guarantee that all the substructures can be factored out.
For example,consistent right binarization of the training examples in Figure 4 makes available R14,but misses R15.
We therefore also introduce a parallel re-structuring method in whichwe binarize both to the left and right at the same time, resulting in a binarizationforest.
We employ the EM algorithm (Dempster, Laird, and Rubin 1977) to learn thebinarization bias for each tree node in the corpus from the parallel alternatives.252Wang et al Re-structuring, Re-labeling, and Re-aligningFigure 5Learning translation rules from binarized English structures.3.1 Some ConceptsWe now explain some concepts to facilitate the descriptions of the re-structuring meth-ods.
We train our translation model on alignment graphs (Galley et al 2004).
Analignment graph is a tuple of a source-language sentence f, a target-language parse treethat yields e and translates from f, and the word alignments a between e and f. Thegraphs in Figures 1, 4, and 5 are examples of alignment graphs.In the alignment graph, a node in the parse tree is called admissible if rules can beextracted from it.We can extract rules from a node if and only if the yield of the tree nodeis consistent with the word alignments?the f string covered by the node is contiguousbut not empty, and the f string does not align to any e string that is not covered bythe node.
An admissible tree node is one where rules overlap.
Figure 6 shows differentbinarizations of the left tree in Figure 4.
In this figure, the NPB node in tree (1) is notadmissible because the f string, V-C, that the node covers also aligns to NNP3, which isnot covered by the NPB.
Node NPB in tree (2), on the other hand, is admissible.A set of sibling tree nodes is called factorizable if we can form an admissiblenew node dominating them.
In Figure 6, sibling nodes NNP1, NNP2, and NNP3 arefactorizable because we can factorize them out and form a new node NPB, resultingin tree (2).
Sibling tree nodes JJ, NNP1, and NNP2 are not factorizable.
Not all siblingnodes are factorizable, so not all sub-phrases can be acquired and syntactified.
Ourmainpurpose is to re-structure parse trees by factorization such that syntactified sub-phrasescan be employed in translation.With these concepts defined, we now present the re-structuring methods.253Computational Linguistics Volume 36, Number 2Figure 6Left, right, and head binarizations on the left tree in Figure 4.
Tree leaves of nodes JJ and NNP1are omitted for convenience.
Heads are marked with ?.
New nonterminals introduced bybinarization are denoted by X-bars.3.2 Binarizing Syntax TreesWe re-structure parse trees by binarizing the trees.
We are going to binarize a tree noden that dominates r children n1, ..., nr.
Binarization is performed by introducing new treenodes to dominate a subset of the children nodes.
We allow ourselves to form only onenew node at a time to avoid over-generalization.
Because labeling is not the concern ofthis section, we re-label the newly formed nodes as n.3.2.1 Simple Binarization Methods.
The left binarization of node n (e.g., the NPB in tree(1) of Figure 6) factorizes the leftmost r ?
1 children by forming a new node n (i.e., theNPB in tree (1)) to dominate them, leaving the last child nr untouched; and then makesthe new node n the left child of n. The method then recursively left-binarizes the newlyformed node n until two leaves are reached.
We left-binarize the left tree in Figure 4 intoFigure 6 (1).The right binarization of node n factorizes the rightmost r ?
1 children by forminga new node n (i.e., the NPB in tree (2)) to dominate them, leaving the first child n1untouched; and then makes the new node n the right child of n. The method thenrecursively right-binarizes the newly formed node n. For instance, we right-binarizethe left tree in Figure 4 into Figure 6 (2) and then into Figure 6 (6).The head binarization of node n left-binarizes n if the head is the first child;otherwise, it right-binarizes n. We prefer right-binarization to left-binarization whenboth are applicable under the head restriction because our initial motivation was togeneralize the NPB-rooted translation rules.
As we show in experiments, binarizationof other types of phrases contributes to translation accuracy as well.Any of these simple binarization methods is easy to implement, but each in itselfis incapable of giving us all the factorizable sub-phrases.
Binarizing all the way to theleft, for example, from unbinarized tree to tree (1) and to tree (3) in Figure 6, does notenable us to acquire a substructure that yields NNP2, NNP3, and their translationalequivalences.
To obtain more factorizable sub-phrases, we need to parallel-binarize inboth directions.254Wang et al Re-structuring, Re-labeling, and Re-aligningFigure 7Packed forest obtained by packing trees (3) and (6) in Figure 6.3.2.2 Parallel Binarization.
Simple binarizations transform a parse tree into another singleparse tree.
Parallel binarization transforms a parse tree into a binarization forest, packedto enable dynamic programming when we extract translation rules from it.Borrowing terms from parsing semirings (Goodman 1999), a packed forest is com-posed of additive forest nodes (?-nodes) and multiplicative forest nodes (?-nodes).
Inthe binarization forest, a ?-node corresponds to a tree node in the unbinarized tree or anew tree node introduced during tree binarization; and this ?-node composes several?-nodes, forming a one-level substructure that is observed in the unbinarized tree orin one of its binarized tree.
A ?-node corresponds to alternative ways of binarizing thesame tree node and it contains one or more ?-nodes.
The same ?-node can appear inmore than one place in the packed forest, enabling sharing.
Figure 7 shows a packedforest obtained by packing trees (3) and (6) in Figure 6 via the following tree parallelbinarization algorithm.We use a memoization procedure to recursively parallel-binarize a parse tree.To parallel-binarize a tree node n that has children n1, ...,nr, we employ the followingsteps: If r ?
2, parallel-binarize tree nodes n1, ..., nr, producing binarization?-nodes ?
(n1), ..., ?
(nr), respectively.
Construct node ?
(n) as the parentof ?
(n1), ...,?(nr).
Construct an additive node ?
(n) as the parent of ?
(n).Otherwise, execute the following steps. Right-binarize n, if any contiguous2 subset of children n2, ...,nr isfactorizable, by introducing an intermediate tree node labeled as n. Werecursively parallel-binarize n to generate a binarization forest node ?
(n).We also recursively parallel-binarize n1, forming a binarization forestnode ?(n1).
We form a multiplicative forest node ?R as the parent of?
(n1) and ?
(n). Left-binarize n if any contiguous subset of n1, ...,nr?1 is factorizable andif this subset contains n1.
Similar to the previous right-binarization,we introduce an intermediate tree node labeled as n, recursivelyparallel-binarize n to generate a binarization forest node ?
(n), recursively2 For practical purposes we factorize only subsets that cover contiguous spans to avoid introducingdiscontiguous constituents.
In principle, the algorithm works fine without this condition.255Computational Linguistics Volume 36, Number 2parallel-binarize nr to generate a binarization forest node ?
(nr), and thenform a multiplicative forest node ?L as the parent of ?
(n) and ?
(nr). Form an additive node ?
(n) as the parent of the two already formedmultiplicative nodes ?L and ?R.The (left and right) binarization conditions consider any subset to enable the fac-torization of small constituents.
For example, in the left tree of Figure 4, although theJJ, NNP1, and NNP2 children of the NPB are not factorizable, the subset JJ NNP1 isfactorizable.
The binarization from this tree to the tree in Figure 6 (1) serves as a relayingstep for us to factorize JJ and NNP1 in the tree in Figure 6 (3).
The left-binarizationcondition is stricter than the right-binarization condition to avoid spurious binarization,that is, to avoid the same subconstituent being reached via both binarizations.In parallel binarization, nodes are not always binarizable in both directions.
Forexample, we do not need to right-binarize tree (2) because NNP2 and NNP3 are notfactorizable, and thus cannot be used to form sub-phrases.
It is still possible to right-binarize tree (2) without affecting the correctness of the parallel binarization algorithm,but that will spuriously increase the branching factor of the search for the rule extrac-tion, because we will have to expand more tree nodes.A special version of parallel binarization is the parallel head binarization, whereboth the left and the right binarization must respect the head propagation propertyat the same time.
Parallel head binarization guarantees that new nodes introduced bybinarization always contain the head constituent, which will become convenient whenhead-driven syntax-based language models are integrated into a bottom?up decodingsearch by intersecting with the trees inferred from the translation model.Our re-structuring of MT training trees is realized by tree binarization, but this doesnot mean that our re-structuring method can factor out phrases covered only by two(binary) constituents.
In fact, a nice property of parallel binarization is that for anyfactorizable substructure in the unbinarized tree, we can always find a correspondingadmissible ?-node in the parallel-binarized packed forest, and thus we can alwaysextract that phrase.
A leftmost substructure like the lowest NPB-subtree in tree (3) ofFigure 6 can be made factorizable by several successive left binarizations, resulting inthe ?5(NPB)-node in the packed forest in Figure 7.
A substructure in the middle can befactorized by the composition of several left- and right-binarizations.
Therefore, after atree is parallel-binarized, to make the sub-phrases available to the MT system, all weneed to do is to extract rules from the admissible nodes in the packed forest.
Rules thatcan be extracted from the original unrestructured tree can be extracted from the packedforest as well.Parallel binarization results in parse forests.
Thus translation rules need to be ex-tracted from training data consisting of (e-forest, f, a)-tuples.3.3 Extracting Translation Rules from (e-forest, f, a)-tuplesOur algorithm to extract rules from (e-forest, f, a)-tuples is a natural generalizationof the (e-parse, f, a)-based rule extraction algorithm in Galley et al (2006).
A similarproblem is also elegantly addressed in Mi and Huang (2008) in detail.
The forest-basedrule extraction algorithm takes as input a (e-forest, f, a)-triple, and outputs a derivationforest (Galley et al 2006), which consists of overlapping translation rules.
The algorithmrecursively traverses the e-forest top?down, extracts rules only at admissible e-forest256Wang et al Re-structuring, Re-labeling, and Re-aligningnodes, and transforms e-forest nodes into synchronous derivation-forest nodes via thefollowing two procedures, depending on which condition is met. Condition 1: If we reach an additive e-forest node, for each of its children,which are multiplicative e-forest nodes, we go to condition 2 to recursivelyextract rules from it to obtain a set of multiplicative derivation-forestnodes, respectively.
We form an additive derivation-forest node, and takethese newly produced multiplicative derivation-forest nodes (by going tocondition 2) as children.
After this, we return the additivederivation-forest node.For instance, at node ?1(NPB) in Figure 7, for each of its children, e-forestnodes ?2(NPB) and ?11(NPB), we go to condition 2 to extract rules on it,to form multiplicative derivation forest nodes, ?
(R16) and ?
(R17) inFigure 8. Condition 2: If we reach a multiplicative e-forest node, we extract a set ofrules rooted at it using the procedure in Galley et al (2006); and for eachrule, we form a multiplicative derivation-forest node, and go to condition 1to form the additive derivation-forest nodes for the additive frontiere-forest nodes of the newly extracted rule, and then make these additivederivation-forest nodes the children of the multiplicative derivation-forestnode.
After this, we return a set of multiplicative derivation-forest nodes,each corresponding to one rule extracted from the multiplicative e-forestnode we just reached.Figure 8A synchronous derivation forest built from a (e-forest, f, a) triple.
The e-forest is shown inFigure 7.257Computational Linguistics Volume 36, Number 2For example, at node ?11(NPB) in Figure 7, we extract a rule from it andform derivation-forest node ?
(R17) in Figure 8.
We then go to condition 1to obtain, for each of the additive frontier e-forest nodes (in Figure 7) ofthis rule, a derivation-forest node, namely, ?
(NNP), ?
(NNP), and ?
(NPB)in Figure 8.
We make these derivation-forest ?-nodes the children ofderivation-forest node ?
(R17).This procedure transforms the packed e-forest in Figure 7 into a packed synchro-nous derivation in Figure 8.
This algorithm is an extension of the extraction algorithmin Galley et al (2006), in the sense that we have an extra condition (1) to relay ruleextraction on additive e-forest nodes.The forest-based rule extraction algorithm produces much larger grammars thanthe tree-based one, making it difficult to scale to very large training data.
From a50M-word Chinese-to-English parallel corpus, we can extract more than 300 milliontranslation rules, while the tree-based rule extraction algorithm gives approximately100 million.
However, the restructured trees from the simple binarization methods arenot guaranteed to give the best trees for syntax-based machine translation.
What wedesire is a binarization method that still produces single parse trees, but is able to mixleft binarization and right binarization in the same tree.
In the following, we use the EMalgorithm to learn the desirable binarization on the forest of binarization alternativesproposed by the parallel binarization algorithm.3.4 Learning How to Binarize Via the EM AlgorithmThe basic idea of applying the EM algorithm to choose a re-structuring is as follows.
Weperform a set {?}
of binarization operations on a parse tree ?.
Each binarization ?
isthe sequence of binarizations on the necessary (i.e., factorizable) nodes in ?
in pre-order.Each binarization ?
results in a restructured tree ??.
We extract rules from (?
?, f, a),generating a translation model consisting of parameters (i.e., rule probabilities) ?.
Ouraim is to first obtain the rule probabilities that are the maximum likelihood estimateof the training tuples, and then produce the Viterbi binarization tree for each trainingtuple.The probability P(?
?, f, a) of a (?
?, f, a)-tuple is what the basic syntax-based trans-lation model is concerned with.
It can be further computed by aggregating the ruleprobabilities P(r) in each derivation?
in the set of all derivations ?
(Galley et al 2004).That is,P(?
?, f, a) =?????r?
?P(r) (1)The rule probabilities are estimated by the inside?outside algorithm (Lari andYoung 1990; Knight, Graehl, and May 2008), which needs to run on derivation forests.Our previous sections have already presented algorithms to transform a parse tree intoa binarization forest, and then transform the (e-forest, f, a)-tuples into derivation forests(e.g., Figure 8), on which the inside?outside algorithm can then be applied.In the derivation forests, an additive node labeled as A dominates several mul-tiplicative nodes, each corresponding to a translation rule resulting from either leftbinarization or right binarization of the original structure.
We use rule r to either referto a rule or to a multiplicative node in the derivation forest.
We use root(r) to representthe root label of the rule, and parent(r) to refer to the additive node that is the parent258Wang et al Re-structuring, Re-labeling, and Re-aligningof the node corresponding to the r. Each rule node (or multiplicative node) dom-inates several other additive children nodes, and we present the ith child node aschildi(r), among the total number of n children.
For example, in Figure 8, for the rule rcorresponding to the left child of the forest root (labeled as NPB), parent(r) is NPB, andchild1(NPB) = r. Based on these notations, we can compute the inside probability ?
(A)of an additive node labeled as A and the outside probability ?
(B) of an additive forestnode labeled as B as follows.?
(A) =?r?{child(A)}P(r)??i=1...n?
(childi(r)) (2)?
(B) =?r:B?{child(r)}P(r)?
?(parent(r))??C?{child(r)}?{B}?
(C) (3)In the expectation step, the contribution of each occurrence of a rule in a derivation-forest to the total expected count of that rule is computed as?(parent(r))?
P(r)??i=1...n?
(childi(r)) (4)In the maximization step, we use the expected counts of rules, #r, to update the proba-bilities of the rules.P(r) = #r?rule q:root(q)=root(r) #q(5)Because it is well known that applying EM with tree fragments of different sizescauses overfitting (Johnson 1998a), and because it is also known that syntax MT modelswith larger composed rules in the mix significantly outperform rules that minimallyexplain the training data (minimal rules) in translation accuracy (Galley et al 2006), weuse minimal rules to construct the derivation forests from (e-binarization-forest, f, a)-tuples during running of the EM algorithm, but, after the EM re-structuring is finished,we build the final translation model using composed rules for MT evaluation.Figure 9 is the actual pipeline that we use for EM binarization.
We first generate apacked e-forest via parallel binarization.
We then extract minimal translation rules fromFigure 9Using the EM algorithm to choose re-structuring.259Computational Linguistics Volume 36, Number 2the (e-forest, f, a)-tuples, producing synchronous derivation forests.
We run the inside?outside algorithm on the derivation forests until convergence.
We obtain the Viterbiderivations and project the English parses from the derivations.
Finally, we extractcomposed rules using Galley et al (2006)?s (e-tree, f, a)-based rule extraction algorithm.When extracting composed rules from (e-parse, f, a)-tuples, we use an ?ignoring-X-node?
trick to the rule extraction method in Galley et al (2006) to avoid breaking thelocal dependencies captured in complex rules.
The trick is that new nodes introducedby binarization are not counted when computing the rule size limit unless they appearas the rule roots.
The motivation is that newly introduced nodes break the local depen-dencies, deepening the parses.
In Galley et al, a composed rule is extracted only if thenumber of internal nodes it contains does not exceed a limit, similar to the phrase lengthlimit in phrase-based systems.
This means that rules extracted from the restructuredtrees will be smaller than those from the unrestructured trees, if the X nodes are deletedfrom the rules.
As shown in Galley et al, smaller rules lose context, and thus givelower translation accuracy.
Ignoring X nodes when computing the rule sizes preservesthe unrestructured rules in the resulting translation model and adds substructures asbonuses.3.5 Experimental ResultsWe carried out experiments to evaluate different tree binarization methods in termsof translation accuracy for Chinese-to-English translation.
The baseline syntax MT sys-tem was trained on the original, non-restructured trees.
We also built one MT systemby training on left-binarizations of training trees, and another by training on EM-binarizations of training trees.Table 1 shows the results on end-to-endMT.
The bootstrap p-values were computedfor the pairwise BLEU comparison between the EM binarization and the baseline.
Theresults show that tree binarization improves MT system accuracy, and that EM binariza-tion outperforms left binarization.
The results also show that the EM re-structuring sig-nificantly outperforms (p <0.05) the no re-structuring baseline on the NIST08 eval set.The MT improvement by tree re-structuring is also validated by our previous work(Wang, Knight, and Marcu 2007), in which we reported a 1 BLEU point gain from EMbinarization under other training/testing conditions; other simple binarizationmethodswere examined in that work aswell, showing that simple binarizations also improveMTaccuracy, and that EM binarization consistently outperforms the simple binarizationmethods.Table 1Translation accuracy versus binarization algorithms.
In this and all other tables reporting BLEUperformance, statistically significant improvements over the baseline are highlighted.
p = thepaired bootstrap p-value computed between each system and the baseline, showing the level atwhich the two systems are significantly different.EXPERIMENT NIST08 NIST08-NWBLEU p BLEU pno binarization (baseline) 29.12 ?
35.33 ?left binarization 29.35 0.184 35.46 0.360EM binarization 29.74 0.010 36.12 0.016260Wang et al Re-structuring, Re-labeling, and Re-aligningTable 2# admissible nodes, # rules versus re-structuring methods.RE-STRUCTURING METHOD # ADMISSIBLE NODES (M) # RULES (M)no binarization 13 76.0left binarization 17.2 153.4EM binarization 17.4 154.8We think that these improvements are explained by the fact that tree re-structuringintroduces more admissible trees nodes in the training trees and enables the formingof additional rules.
As a result, re-structuring produces more rules.
Table 2 shows thenumber of admissible nodes made available by each re-structuring method, as well asby the baseline.
Table 2 also shows the sizes of the resulting grammars.The EM binarization is able to introduce more admissible nodes because it mixesboth left and right binarizations in the same tree.
We computed the binarization biaseslearned by the EM algorithm for each nonterminal from the binarization forest ofparallel head binarizations of the training trees (Table 3).
Of course, the binarizationbias chosen by left-/right-binarization methods would be 100% deterministic.
Onenoticeable message from Table 3 is that most of the categories are actually biased towardleft-binarization.
The reason might be that the head sub-constituents of most Englishcategories tend to be on the left.Johnson (1998b) argues that the more nodes there are in a treebank, the strongerthe independence assumptions implicit in the PCFG model are, and the less accuratethe estimated PCFG will usually be?more nodes break more local dependencies.
Ourexperiments, on the other hand, show MT accuracy improvement by introducing moreadmissible nodes.
This initial contradiction actually makes sense.
The key is that we usecomposed rules to build our final MT system and that we introduce the ?ignoring-X-node?
trick to preserve the local dependencies.
More nodes in training trees weaken theaccuracy of a translation model of minimal rules, but boost the accuracy of a translationmodel of composed rules.Table 3Binarization bias learned by the EM re-structuring method on the model 4 word alignments.nonterminal left-binarization (%) right-binarization (%)NP 98 2NPB 1 99VP 95 5PP 86 14ADJP 67 33ADVP 76 24S 94 6S-C 17 83SBAR 93 7QP 89 11WHNP 98 2SINV 94 6CONJP 69 31261Computational Linguistics Volume 36, Number 24.
Re-Labeling Trees for TrainingThe syntax translation model explains (e-parse, f, a)-tuples by a series of applicationsof translation rules.
At each derivation step, which rule to apply next depends onlyon the nonterminal label of the frontier node being expanded.
In the Penn Treebankannotation, the nonterminal labels are too coarse to encode enough context informationto accurately predict the next translation rule to apply.
As a result, using the PennTreebank annotation can license ill-formed subtrees (Figure 10).
This subtree containsan error that induces a VP as an SG-C when the head of the VP is the finite verb dis-cussed.
The translation error leads to the ungrammatical ?...
confirmed discussed ... ?.
Thistranslation error occurs due to the fact that there is no distinction between finite VPsand non-finite VPs in Penn Treebank annotation.
Monolingual parsing suffers similarly,but to a lesser degree.Re-structuring of training trees enables the reuse of sub-constituent structures, butfurther introduces new nonterminals and actually reduces the context for rules, thusmaking this ?coarse nonterminal?
problem more severe.
In Figure 11, R23 may beextracted from a construct like S(S CC S) via tree binarization, and R24 may be extractedfrom a construct like S(NP NP-C VP) via tree binarization.
Composing R23 and R24forms the structure in Figure 11(b), which, however, is ill-formed.
This wrong structurein Figure 11(b) yields ungrammatical translations like he likes reading she does not likereading.
Tree binarization enables the reuse of substructures, but causes over-generationof trees at the same time.We solve the coarse-nonterminal problem by refining/re-labeling the training treelabels.
Re-labeling is done by enriching the nonterminal label of each tree node basedon its context information.Re-labeling has already been used in monolingual parsing research to improveparsing accuracy of PCFGs.
We are interested in two types of re-labeling methods:Linguistically motivated re-labeling (Klein and Manning 2003; Johnson 1998b) enrichesthe labels of parser training trees using parent labels, head word tag labels, and/orsibling labels.
Automatic category splitting (Petrov et al 2006) refines a nonterminalFigure 10MT output errors due to coarse Penn Treebank annotations.
Oval nodes in (b) are ruleoverlapping nodes.
Subtree (b) is formed by composing the LHSs of R20, R21, and R22.262Wang et al Re-structuring, Re-labeling, and Re-aligningFigure 11Tree binarization over-generalizes the parse tree.
Translation rules R23 and R24 are acquiredfrom binarized training trees, aiming for reuse of substructures.
Composing R23 and R24,however, results in an ill-formed tree.
The new nonterminal S introduced in tree binarizationneeds to be refined into different sub-categories to prevent R23 and R24 from being composed.Automatic category splitting can be employed for refining the S.by classifying the nonterminal into a fine-grained sub-category, and this sub-classingis learned via the EM algorithm.
Category splitting is realized by several splitting-and-merging cycles.
In each cycle, the nonterminals in the PCFG rules are split by splittingeach nonterminal into two.
The EM algorithm is employed to estimate the split PCFG onthe Penn Treebank training trees.
After that, 50% of the new nonterminals are mergedbased on some loss function, to avoid overfitting.4.1 Linguistic Re-labelingIn the linguistically motivated approach, we employ the following set of rules to re-labeltree nodes.
In our MT training data: SPLIT-VP: annotates each VP nodes with its head tag, and then merges allfinite VP forms to a single VPF. SPLIT-IN: annotates each IN node with the combination of IN and itsparent node label.
IN is frequently overloaded in the Penn Treebank.
Forinstance, its parent can be PP or SBAR.These two operations re-label the tree in Figure 12(a1) to Figure 12(b1).
Examplerules extracted from these two trees are shown in Figure 12(a2) and Figure 12(b2),respectively.
We apply this re-labeling on the MT training tree nodes, and then acquirerules from these re-labeled trees.
We chose to split only these two categories becauseour syntax MT system tends to frequently make parse errors in these two categories,and because, as shown by Klein and Manning (2003), further refining the VP and INcategories is very effective in improving monolingual parsing accuracy.This type of re-labeling fixes the parse error in Figure 10.
SPLIT-VP transforms theR18 root to VPF, and the R17 frontier node to VP.VBN.
Thus R17 and R18 can never becomposed, preventing the wrong tree being formed by the translation model.4.2 Statistical Re-labelingOur second re-labeling approach is to learn the split categories for the node labels ofthe training trees via the EM algorithm, as in Petrov et al (2006).
Rather than using263Computational Linguistics Volume 36, Number 2Figure 12Re-labeling of parse trees.264Wang et al Re-structuring, Re-labeling, and Re-aligningtheir parser to directly produce category-split parse trees for the MT training data, weseparate the parsing step and the re-labeling step.
The re-labeling method is as follows.1.
Run a parser to produce the MT training trees.2.
Binarize the MT training trees via the EM binarization algorithm.3.
Learn an n-way split PCFG from the binarized trees via the algorithmdescribed in Petrov et al (2006).4.
Produce the Viterbi split annotations on the binarized training trees withthe learned category-split PCFG.As we mentioned earlier, tree binarization sometimes makes the decoder over-generalize the trees in the MT outputs, but we still binarize the training trees beforeperforming category splitting, for two reasons.
The first reason is that the improve-ment on MT accuracy we achieved by tree re-structuring indicates that the benefitwe obtained from structure reuse triumphs the problem of tree over-generalization.The second is that carrying out category splitting on unbinarized training trees blowsup the grammar?splitting a CFG rule of rank 10 results in 211 split rules.
This re-labeling procedure tries to achieve further improvement by trying to fix the tree over-generalization problem of re-structuring while preserving the gain we have alreadyobtained from tree re-structuring.Figure 12(c1) shows a category-split tree, and Figure 12(c2) shows the minimal xRsrules extracted from the split tree.
In Figure 12(c2), the two VPs (VP-0 and VP-2) nowbelong to two different categories and cannot be used in the same context.In this re-labeling procedure, we separate the re-labeling step from the parsing step,rather than using a parser like the one in Petrov et al (2006) to directly produce category-split parse trees on the English corpus.
We think that we benefit from this separation inthe following ways: First, this gives us the freedom to choose the parser to produce theinitial trees.
Second, this enables us to train the re-labeler on the domains where the MTsystem is trained, instead of on the Penn Treebank.
Third, this enables us to choose ourown tree binarization methods.Tree re-labeling fragments the translation rules.
Each refined rule now fits in fewercontexts than its corresponding coarse rule.
Re-labeling, however, does not explodethe grammar size, nor does re-labeling deteriorate the reuse of substructures.
Thisis because the re-labeling (whether linguistic or automatic) results in very consistentannotations.
Table 4 shows the sizes of the translation grammars from different re-labelings of the training trees, as well as that from the unrelabeled ones.Table 4Grammar size vs. re-labeling methods.
Re-labeling does not explode the grammar size.RE-LABELING METHOD # RULES (M) NONTERMINAL SET SIZENo re-labeling 154.80 144Linguistically motivated re-labeling 154.97 2104-way splitting (90% merging) 158.89 1788-way splitting (90% merging) 160.62 1954-way splitting (50% merging) 164.15 326265Computational Linguistics Volume 36, Number 2It would be very interesting to perform automatic category splitting with synchro-nous translation rules and run the EM algorithm on the synchronous derivation forests.Synchronous category splitting is computationally much more expensive, so we do notstudy it here.4.3 Experimental ResultsWe ran end-to-end MT experiments by re-labeling the MT training trees.
Our two base-line systems were a syntax MT system with neither re-structuring nor re-labeling, anda syntax MT system with re-structuring but no re-labeling.
The linguistically motivatedre-labeling method was applied directly on the original (unrestructured) training trees,so that it could be compared to the first baseline.
The automatic category splitting re-labeling method was applied to binarized trees so as to avoid the explosion of the splitgrammar, so it is compared to the second baseline.
The experiment results are shown inTable 5.Both re-labeling methods help MT accuracy.
Putting both re-structuring and re-labeling together results in 0.93 BLEU points improvement on NIST08 set, and 1 BLEUpoint improvement on the newswire subset.
All p-values are computed between there-labeling systems and Baseline1.
The improvement made by the linguistically moti-vated re-labeling method is significant at the 0.05 level.
Because the automatic categorysplitting is carried out on the top of EM re-structuring and because, as we have alreadyshown, EM re-structuring significantly improves Baseline1, putting them together re-sults in better translations with more confidence.If we compare these results to those in Table 1, we notice that re-structuring tendsto help MT accuracy more than re-labeling.
We mentioned earlier that re-structuringovergeneralizes structures, but enables reuse of substructures.
Results in Table 5 andTable 1 show substructure reuse mitigates structure over-generalization in our tree re-structuring method.5.
Re-aligning (Tree, String) Pairs for TrainingSo far, we have improved the English structures in our parsed, aligned training corpus.We now turn to improving the word alignments.Some MT systems use the same model for alignment and translation?examplesinclude Brown et al (1993), Wu (1997), Alshawi, Bangalore, and Douglas (1998), Yamadaand Knight (2001, 2002), and Cohn and Blunsom (2009).
Other systems use Brown et alfor alignment, then collect counts for a completely different model, such as Och andNeyTable 5Impact of re-labeling methods on MT accuracy as measured by BLEU.
Four-way splitting wascarried out on EM-binarized trees; thus, it already benefits from tree re-structuring.
All p-valuesare computed against Baseline1.EXPERIMENT NIST08 NIST08-NWBLEU p BLEU pBaseline1 (no re-structuring and no re-labeling) 29.12 ?
35.33 ?Linguistically motivated re-labeling 29.57 0.029 35.85 0.050Baseline2 (EM re-structuring but no re-labeling) 29.74 ?
36.12 ?4-way splitting (w/ 90% merging) 30.05 0.001 36.42 0.003266Wang et al Re-structuring, Re-labeling, and Re-aligning(2004) or Chiang (2007).
Our basic syntax-based system falls into this second category,as we learn our syntactic translation model from a corpus aligned with word-basedtechniques.
We would like to inject more syntactic reasoning into the alignment process.We start by contrasting two generative translation models.5.1 The Traditional IBM Alignment ModelIBMModel 4 (Brown et al 1993) learns a set of four probability tables to compute P( f |e)given a foreign sentence f and its target translation e via the following (simplified)generative story:1.
A fertility y for each word ei in e is chosen with probability Pfert(y|ei).2.
A null word is inserted next to each fertility-expanded word withprobability Pnull.3.
Each token ei in the fertility-expanded word and null string is translatedinto some foreign word fi in f with probability Ptrans( fi|ei).4.
The position of each foreign word fi that was translated from ei is changedby?
(which may be positive, negative, or zero) with probabilityPdistortion(?|A(ei),B( fi)), where A and B are functions over the source andtarget vocabularies, respectively.Brown et al (1993) describe an EM algorithm for estimating values for the fourtables in the generative story.
With those values in hand, we can calculate the highest-probability (Viterbi) alignment for any given string pair.Two scale problems arise in this algorithm.
The first is the time complexity of enu-merating alignments for fractional count collection.
This is solved by considering onlya subset of alignments, and by bootstrapping the Ptrans table with a simpler model thatadmits fast count collection via dynamic programming, such as IBM Model 1 (Brownet al 1993) or Aachen HMM (Vogel, Ney, and Tillmann 1996).
The second problem isone of space.
In theory, the initial Ptrans table contains a cell for every English wordpaired with every Chinese word?this would be infeasible.
Fortunately, in practice, thetable can be initialized with only those word pairs observed co-occurring in the paralleltraining text.5.2 A Syntax Re-alignment ModelOur syntax translation model learns a single probability table to compute P(etree, f )given a foreign sentence f and a parsed target translation etree.
In the following gen-erative story we assume a starting variable with syntactic type v.1.
Choose a rule r to replace v, with probability Prule(r|v).2.
For each variable with syntactic type vi in the partially completed (tree,string) pair, continue to choose rules ri with probability Prule(ri|vi) toreplace these variables until there are no variables remaining.We can use this model to explain unaligned (tree, string) pairs from our trainingdata.
With a large enough rule set, any given (tree, string) pair will admit many deriva-tions.
Consider again the example from Figure 1.
The particular alignment associated267Computational Linguistics Volume 36, Number 2with that (tree, string) pair yields the minimal rules of Figure 2.
A different alignmentyields different rules.
Figure 13 shows two other alignments and their correspondingminimal rules.
As noted before, a set of minimal rules in proper sequence formsa derivation tree of rules that explains the (tree, string) pair.
Because rules explainvariable-size fragments (e.g., R35 vs. R6), the possible derivation trees of rules thatexplain a sentence pair have varying sizes.
The smallest such derivation tree has a singlelarge rule (which does not appear in Figure 13).
When our model chooses a particularderivation tree of minimal rules to explain a given (tree, string) pair it implicitly choosesthe alignment that produced these rules as well.3 Our model can choose a derivation byusing any of the rules in Figures 13 and 2.
We would prefer it select the derivation thatyields the good alignment in Figure 1.We can also develop an EM learning approach for this model.
As in the IBMapproach, we have both time and space issues.
Time complexity, as we will see sub-sequently, is O(mn3), where m is the number of nodes in the English training tree andn is the length of the corresponding Chinese string.
Space is more of a problem.
Wewould like to initialize EM with all the rules that might conceivably be used to explainthe training data.
However, this set is too large to practically enumerate.To reduce the model space we first create a bootstrap alignment using a simplerword-based model.
Then we acquire a set of minimal translation rules from the (tree,string, alignment) triples.
Armed with these rules, we can discard the word-basedalignments and re-alignwith the syntax translation model.We summarize the approach described in this section as:1.
Obtain bootstrap alignments for a training corpus using word-basedalignment.2.
Extract minimal rules from the corpus and alignments using GHKM,noting the partial alignment that is used to extract each rule.3.
Construct derivation forests for each (tree, string) pair, ignoring thealignments, and run EM to obtain Viterbi derivation trees, then use theannotated partial alignments to obtain Viterbi alignments.4.
Use the new alignments to re-train the full MT system, this time collectingcomposed rules as well as minimal rules.5.3 EM Training for the Syntax Translation ModelConsider the example of Figure 13 again.
The top alignment was the bootstrap align-ment, and thus prior to any experiment we obtained the corresponding indicatedminimal rule set and derivation.
This derivation is reasonable but there are some poorlymotivated rules, from a linguistic standpoint.
The Chinese word?
?roughly meansthe two shores in this context, but the rule R35 learned from the alignment incorrectlyincludes between.
However, other sentences in the training corpus have the correct3 Strictly speaking there is actually a one-to-many mapping between a derivation tree of minimal rules andthe alignment that yields these rules, due to the handling of unaligned words.
However, the choice of onepartial alignment over another does not affect results and in practice we impose a one-to-one mappingbetween minimal rules and the partial alignments that imply them by selecting the most frequentlyobserved partial alignment for a given minimal rule.268Wang et al Re-structuring, Re-labeling, and Re-aligningFigure 13The minimal rules extracted from two different alignments of the sentence in Figure 1.alignment, which yields rules in Figure 2, such as R8.
Figure 2 also contains rules R4and R6, learned from yet other sentences in the training corpus, which handle the?...
?structure (which roughly translates to in between), thus allowing a derivation whichcontains the minimal rule set of Figure 2 and implies the alignment in Figure 1.EM distributes rule probabilities in such a way as to maximize the probability of thetraining corpus.
It thus prefers to use one rule many times instead of several different269Computational Linguistics Volume 36, Number 2rules for the same situation over several sentences, if possible.
R35 is a possible rulein 46 of the 329,031 sentence pairs in the training corpus, and R8 is a possible rule in100 sentence pairs.
Well-formed rules are more usable than ill-formed rules and thepartial alignments behind these rules, generally also well-formed, become favored aswell.
The top row of Figure 14 contains an example of an alignment learned by thebootstrap alignment model that includes an incorrect link.
Rule R25, which is extractedfrom this alignment, is a poor rule.
A set of commonly seen rules learned from othertraining sentences provide a more likely explanation of the data, and the consequentalignment omits the spurious link.Figure 14The impact of a bad alignment on rule extraction.
Including the alignment link indicated by thedotted line in the example leads to the rule set in the second row.
The re-alignment proceduredescribed in Section 5.2 learns to prefer the rule set at bottom, which omits the bad link.270Wang et al Re-structuring, Re-labeling, and Re-aligningTable 6Translation performance, grammar size versus the re-alignment algorithm proposed inSection 5.2, and re-alignment as modified in Section 5.4.EXPERIMENT NIST08 NIST08-NW # RULES (M)BLEU p BLEU pno re-alignment (baseline) 29.12 ?
35.33 ?
76.0EM re-alignment 29.18 0.411 35.52 0.296 75.1EM re-alignment with size prior 29.37 0.165 35.96 0.050 110.4Now we need an EM algorithm for learning the parameters of the rule set thatmaximize?corpusP(tree, string).
Knight, Graehl, andMay (2008) present a generic such algo-rithm for tree-to-string transducers that runs in O(mn3) time, as mentioned earlier.
Thealgorithm consists of two components: DERIV, which is a procedure for constructinga packed forest of derivation trees of rules that explain a (tree, string) bitext corpusgiven that corpus and a rule set, and TRAIN, which is an iterative parameter-settingprocedure.We initially attempted to use the top-down DERIV algorithm of Knight, Graehl, andMay (2008), but as the constraints of the derivation forests are largely lexical, too muchtime was spent on exploring dead-ends.
Instead we build derivation forests using thefollowing sequence of operations:1.
Binarize rules using the synchronous binarization algorithm fortree-to-string transducers described in Zhang et al (2006).2.
Construct a parse chart with a CKY parser simultaneously constrained onthe foreign string and English tree, similar to the bilingual parsing of Wu(1997).43.
Recover all reachable edges by traversing the chart, starting from thetopmost entry.Because the chart is constructed bottom-up, leaf lexical constraints are encounteredimmediately, resulting in a narrower search space and faster running time than thetop-down DERIV algorithm for this application.
The Viterbi derivation tree tells uswhich English words produce which Chinese words, so we can extract a word-to-wordalignment from it.Although in principle the re-alignment model and translation model learn parame-ter weights over the same rule space, in practice we limit the rules used for re-alignmentto the set of minimal rules.5.4 Adding a Rule Size PriorAn initial re-alignment experiment shows a small rise in BLEU scores from the baseline(Table 6), but closer inspection of the rules favored by EM implies we can do even better.4 In the cases where a rule is not synchronous-binarizable, standard left?right binarization is performedand proper permutation of the disjoint English tree spans must be verified when building the part of thechart that uses this rule.271Computational Linguistics Volume 36, Number 2EM has a tendency to favor a few large rules over many small rules, even when thesmall rules are more useful.
Referring to the rules in Figures 2 and 13, note that possiblederivations for (taiwan?s,?l)5 are R33, R2?R3, and R38?R40.
Clearly the third deriva-tion is not desirable, and we do not discuss it further.
Between the first two derivations,R2?R3 is preferred over R33, as the conditioning for possessive insertion is not related tothe specific Chinese word being inserted.
Of the 1,902 sentences in the training corpuswhere this pair is seen, the bootstrap alignments yield the R33 derivation 1,649 timesand the R2?R3 derivation 0 times.
Re-alignment does not change the result much; thenew alignments yield the R33 derivation 1,613 times and again never choose R2?R3.
Therules in the second derivation themselves are not rarely seen?R2 is in 13,311 forestsother than those where R33 is seen, and R3 is in 2,500 additional forests.
EM gives R2 aprobability of e?7.72?better than 98.7% of rules, and R3 a probability of e?2.96.
But R33receives a probability of e?6.32 and is preferred over the R2?R3 derivation, which has acombined probability of e?10.68.The preference for shorter derivations containing large rules over longer derivationscontaining small rules is due to a general tendency for EM to prefer derivations withfew atoms.
Marcu and Wong (2002) note this preference but consider the phenomenona feature, rather than a bug.
Zollmann and Sima?an (2005) combat the overfitting aspectfor parsing by using a held-out corpus and a straight maximum likelihood estimate,rather than EM.
DeNero, Bouchard-Co?te?, and Klein (2008) encourage small rules with amodeling approach; they put a Dirichlet process prior of rule size over their model andlearn the parameters of the geometric distribution of that prior with Gibbs sampling.Weuse a simpler modeling approach to accomplish the same goals as DeNero, Bouchard-Co?te?, and Klein which, although less elegant, is more scalable and does not require aseparate Bayesian inference procedure.As the probability of a derivation is determined by the product of its atom probabil-ities, longer derivations with more probabilities to multiply have an inherent disadvan-tage against shorter derivations, all else being equal.
EM is an iterative procedure andthus such a bias can lead the procedure to converge with artificially raised probabilitiesfor short derivations and the large rules that constitute them.
The relatively rare ap-plicability of large rules (and thus lower observed partial counts) does not overcome theinherent advantage of large coverage.
To combat this, we introduce size terms into ourgenerative story, ensuring that all competing derivations for the same sentence containthe same number of atoms:1.
Choose a rule size s with cost csize(s)s?1.2.
Choose a rule r (of size s) to replace the start symbol with probabilityPrule(r|s, v).3.
For each variable in the partially completed (tree, string) pair, continue tochoose sizes followed by rules, recursively to replace these variables untilthere are no variables remaining.This generative story changes the derivation comparison from R33 vs. R2?R3 to S2?R33 vs. R2?R3, where S2 is the atom that represents the choice of size 2 (the size of a rule5 The Chinese gloss is simply ?taiwan?.272Wang et al Re-structuring, Re-labeling, and Re-aligningin this context is the number of non-leaf and non-root nodes in its tree fragment).
Notethat the variable number of inclusions implied by the exponent in the generative storyabove ensures that all derivations have the same size.
For example, a derivation withone size-3 rule, a derivation with one size-2 and one size-1 rule, and a derivation withthree size-1 rules would each have three atoms.
With this revised model that allows forfair comparison of derivations, the R2?R3 derivation is chosen 1,636 times, and S2?R33is not chosen.
R33 does, however, appear in the translation model, as the expanded ruleextraction described in Section 1 creates R33 by joining R2 and R3.The probability of size atoms, like that of rule atoms, is decided by EM.
The revisedgenerative story tends to encourage smaller sizes by virtue of the exponent.
This doesnot, however, simply ensure the largest number of rules per derivation is used in allcases.
Ill-fitting and poorly motivated rules such as R42, R43, and R44 in Figure 13 arenot preferred over R8, even though they are smaller.
However, R6 and R8 are preferredover R35, as the former are useful rules.
Although the modified model does not sum to1, it can nevertheless lead to an improvement in BLEU score.5.5 Experimental ResultsThe end-to-end MT experiment used as baseline and described in Sections 3.5 and 4.3was trained on IBMModel 4 word alignments, obtained by running GIZA, as describedin Section 2.
We compared this baseline to an MT system that used alignments obtainedby re-aligning the GIZA alignments using the method of Section 5.2 with the 36 millionword subset of the training corpus used for re-alignment learning.
We next comparedthe baseline to an MT system that used re-alignments obtained by also incorporatingthe size prior described in Section 5.4.
As can be seen by the results in Table 6, the sizeprior method is needed to obtain reasonable improvement in BLEU.
These results areconsistent with those reported in May and Knight (2007), where gains in Chinese andArabicMT systemswere observed, though over aweaker baseline andwith less trainingdata than is used in this work.6.
Combining TechniquesWe have thus far seen gains in BLEU score by independent improvements in trainingdata tree structure, syntax labeling, and alignment.
This naturally raises the questionof whether the techniques can be combined, that is, if improvement in one aspect oftraining data aids in improvement of another.
As reported in Section 4.3 and Table 5,we were able to improve re-labeling efforts and take advantage of the split-and-mergetechnique of Petrov et al (2006) by first re-structuring via the method described in Sec-tion 3.4.
It is unlikely that such re-structuring or re-labeling would aid in a subsequentre-alignment procedure like that of Section 5.2, for re-structuring changes trees basedon a given alignment, and re-alignment can only change links when multiple instancesof a (subtree, substring) tuple are found in the data with different partial alignments.Re-structuring beforehand changes the trees over different alignments differently.
It isunlikely that many (subtree, substring) tuples with more than one partial alignmentwould remain after a re-structuring.However, re-structuring may benefit from a prior re-alignment.
We do not want re-structuring decisions to be made over bad alignments, so unifying alignments basedon common syntax should lead EM to make a more confident binarization decision.273Computational Linguistics Volume 36, Number 2Table 7Summary of experiments in this article, including a combined experiment with re-alignment,re-structuring, and re-labeling.EXPERIMENT NIST08 NIST08-NW # RULES (M)BLEU p BLEU pBaseline (no binarization, no re-labeling, 29.12 ?
35.33 ?
76.0Model 4 alignments)left binarization 29.35 0.184 35.46 0.360 153.4EM binarization 29.74 0.010 36.12 0.016 154.8Linguistic re-labeling 29.57 0.029 35.85 0.050 154.97EM binarization + EM re-labeling 30.05 0.001 36.42 0.003 158.89(4-way splitting w/ 90% merging)EM re-alignment 29.18 0.411 35.52 0.296 75.1Size prior EM re-alignment 29.37 0.165 35.96 0.050 110.4Size prior EM re-alignment + 30.6 0.001 36.73 0.002 222.0EM binarization + EM re-labelingBetter re-structuring should in turn lead to better re-labeling, and this should increasethe performance of the overall MT pipeline.To test this hypothesis we pre-processed alignments using the modified re-alignment procedure described in Section 5.4.
We next used those alignments to obtainnew binarizations of trees following the EM binarization method described in Sec-tion 3.4.
Finally, re-labeling was done on these binarized trees using 4-way splitting with90% merging, as described in Section 4.
The final trees, along with the alignments usedto get these trees and of course the parallel Chinese sentences, were then used as thetraining data of our MT pipeline.
The results of this combined experiment are shownin Table 7 along with the other experiments from this article, for ease of comparison.As can be seen from this table, the progressive improvement of training data leadsto an overall improvement in MT system performance.
As noted previously, theretends to be a correspondence between the number of unique rules extracted and MTperformance.
The final combined experiment has the greatest number of unique rules.The improvements made to syntax and alignment described in this article unify thesetwo independently determined annotations over the bitext, and this thus leads to moreadmissible nodes and a greater ability to extract rules.
Such a unification can lead toover-generalization, as rules lacking sufficient context may be extracted and used to thesystem?s detriment.
This is why a re-labeling technique is also needed, to ensure thatsufficient rule specificity is maintained.7.
ConclusionThis article considered three modifications to MT training data that encourage im-proved performance in a state-of-the-art syntactic MT system.
The improvementschanged syntactic structure, altered bracket labels, and unified alignment across sen-tences, and when combined led to an improvement of 1.48 BLEU points over a strongbaseline in Chinese?English translation.
The techniques herein described require only274Wang et al Re-structuring, Re-labeling, and Re-aligningthe training data used in the original MT task and are thus applicable to a string-to-treeMT system for any language pair.AcknowledgmentsSome of the results in this article appearin Wang, Knight, and Marcu (2007) andMay and Knight (2007).
The linguisticallymotivated re-labeling method is due toSteve DeNeefe, Kevin Knight, and DavidChiang.
The authors also wish to thankSlav Petrov for his help with the Berkeleyparser, and the anonymous reviewersfor their helpful comments.
This researchwas supported under DARPA ContractNo.
HR0011-06-C-0022, BBN subcontract9500008412.ReferencesAlshawi, Hiyan, Srinivas Bangalore,and Shona Douglas.
1998.
Automaticacquisition of hierarchical transductionmodels for machine translation.
InProceedings of the 36th Annual Meeting ofthe Association for Computational Linguistics(ACL) and 17th International Conference onComputational Linguistics (COLING) 1998,pages 41?47, Montre?al.Brown, Peter F., Stephen A. Della Pietra,Vincent J. Della Pietra, and Robert L.Mercer.
1993.
The mathematics ofstatistical machine translation: Parameterestimation.
Computational Linguistics,19(2):263?312.Charniak, Eugene.
2000.
A maximum-entropy-inspired parser.
In Proceedings ofthe 1st North American Chapter of theAssociation for Computational LinguisticsConference (NAACL), pages 132?139,Seattle, WA.Chiang, David.
2007.
Hierarchicalphrase-based translation.
ComputationalLinguistics, 33(2):201?228.Cohn, Trevor and Phil Blunsom.
2009.A Bayesian model of syntax-directedtree to string grammar induction.
InProceedings of the 2009 Conference onEmpirical Methods in Natural LanguageProcessing, pages 352?361, Singapore.Collins, Michael.
1997.
Three generative,lexicalized models for statistical parsing.In Proceedings of the 35th Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 16?23, Madrid.Dempster, Arthur P., Nan M. Laird, andDonald B. Rubin.
1977.
Maximumlikelihood from incomplete data via theEM algorithm.
Journal of the RoyalStatistical Society, 39(1):1?38.DeNeefe, Steve, Kevin Knight, Wei Wang,and Daniel Marcu.
2007.
What cansyntax-based MT learn from phrase-basedMT?
In Proceedings of EMNLP?CoNLL-2007,pages 755?763, Prague.DeNero, John, Alexandre Bouchard-Co?te?,and Dan Klein.
2008.
Sampling alignmentstructure under a Bayesian translationmodel.
In Proceedings of the 2008 Conferenceon Empirical Methods in Natural LanguageProcessing (EMNLP), pages 314?323,Honolulu, HI.Gale, William A. and Geoffrey Sampson.1996.
Good-Turing frequency estimationwithout tears.
Journal of QuantitativeLinguistics, 2(3):217?237.Galley, Michel, Jonathan Graehl, KevinKnight, Daniel Marcu, Steve DeNeefe,Wei Wang, and Ignacio Thayer.
2006.Scalable inference and training ofcontext-rich syntactic translation models.In Proceedings of the 21st InternationalConference on Computational Linguistics(COLING) and 44th Annual Meeting of theAssociation for Computational Linguistics(ACL), pages 961?968, Sydney.Galley, Michel, Mark Hopkins, Kevin Knight,and Daniel Marcu.
2004.
What?s in atranslation rule?
In Proceedings of theHuman Language Technology Conference andthe North American Association forComputational Linguistics (HLT-NAACL),pages 273?280, Boston, MA.Good, Irving J.
1953.
The populationfrequencies of species and the estimationof population parameters.
Biometrika,40(3):237?264.Goodman, Joshua.
1999.
Semiring parsing.Computational Linguistics, 25(4):573?605.Huang, Bryant and Kevin Knight.
2006.Relabeling syntax trees to improvesyntax-based machine translationaccuracy.
In Proceedings of the mainconference on Human Language TechnologyConference of the North American Chapterof the Association of ComputationalLinguistics (NAACL-HLT), pages 240?247,New York, NY.Johnson, Mark.
1998a.
The DOP estimationmethod is biased and inconsistent.Computational Linguistics, 28(1):71?76.Johnson, Mark.
1998b.
PCFG models oflinguistic tree representations.Computational Linguistics, 24(4):613?632.Klein, Dan and Chris Manning.
2003.Accurate unlexicalized parsing.
In275Computational Linguistics Volume 36, Number 2Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 423?430, Sapporo.Kneser, Reinhard and Hermann Ney.1995.
Improved backing-off form-gram language modeling.
InProceedings of the International Conferenceon Acoustics, Speech, and SignalProcessing (ICASSP) 1995, pages 181?184,Detroit, MI.Knight, Kevin and Jonathan Graehl.
2005.An overview of probabilistic treetransducers for natural languageprocessing.
In Proceedings of the SixthInternational Conference on IntelligentText Processing and ComputationalLinguistics (CICLing), pages 1?25,Mexico City.Knight, Kevin, Jonathan Graehl, andJonathan May.
2008.
Training treetransducers.
Computational Linguistics,34(3):391?427.Koehn, Philipp.
2004.
Statistical significancetests for machine translation evaluation.In Proceedings of the 2004 Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP), pages 388?395,Barcelona.Lari, Karim and Steve Young.
1990.
Theestimation of stochastic context-freegrammars using the inside-outsidealgorithm.
Computer Speech and Language,4:35?56.Marcu, Daniel and William Wong.
2002.A phrase-based, joint probability modelfor statistical machine translation.In Proceedings of the 2002 Conferenceon Empirical Methods in Natural LanguageProcessing (EMNLP), pages 133?139,Philadelphia, PA.Marcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building alarge annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313?330.May, Jonathan and Kevin Knight.
2007.Syntactic re-alignment models formachine translation.
In Proceedingsof the 2007 Joint Conference on EmpiricalMethods in Natural Language Processingand Computational Natural LanguageLearning (EMNLP?CoNLL), pages 360?368,Prague.Melamed, I. Dan, Giorgio Satta, andBenjamin Wellington.
2004.
Generalizedmultitext grammars.
In Proceedings of the42nd Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 662?669, Barcelona.Mi, Haitao and Liang Huang.
2008.Forest-based translation rule extraction.In Proceedings of the 2008 Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP), pages 206?214,Honolulu, HI.Och, Franz and Hermann Ney.
2004.
Thealignment template approach to statisticalmachine translation.
ComputationalLinguistics, 30(4):417?449.Och, Franz Josef.
2003.
Minimum error ratetraining for machine translation.
InProceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 160?167, Sapporo.Petrov, Slav, Leon Barrett, RomainThibaux, and Dan Klein.
2006.
Learningaccurate, compact, and interpretabletree annotation.
In Proceedings of the21st International Conference onComputational Linguistics (COLING) and44th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 433?440, Sydney.Stolcke, Andreas.
2002.
SRILM?anextensible language modeling toolkit.In Proceedings of the 7th InternationalConference on Spoken LanguageProcessing (ICSLP) 2002, pages 901?904,Denver, CO.Talbot, David and Miles Osborne.
2007.Randomised language modelling forstatistical machine translation.
InProceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics(ACL), pages 512?519, Prague.Vogel, Stephan, Hermann Ney, andChristoph Tillmann.
1996.
HMM-basedword alignment in statistical translation.
InProceedings of the International Conference onComputational Linguistics (COLING) 1996,pages 836?841, Copenhagen.Wang, Wei, Kevin Knight, and DanielMarcu.
2007.
Binarizing syntax treesto improve syntax-based machinetranslation accuracy.
In Proceedingsof the 2007 Joint Conference on EmpiricalMethods in Natural Language Processingand Computational Natural LanguageLearning (EMNLP?CoNLL), pages 746?754,Prague.Wu, Dekai.
1997.
Stochastic inversiontransduction grammars and bilingualparsing of parallel corpora.
ComputationalLinguistics, 23(3):377?404.Yamada, Kenji and Kevin Knight.
2001.A syntax-based statistical translationmodel.
In Proceedings of the 39thAnnual Meeting of the Association for276Wang et al Re-structuring, Re-labeling, and Re-aligningComputational Linguistics (ACL),pages 523?530, Toulouse.Yamada, Kenji and Kevin Knight.
2002.A decoder for syntax-based statisticalMT.
In Proceedings of the 40th AnnualMeeting of the Association for ComputationalLinguistics (ACL), pages 303?310,Philadelphia, PA.Zhang, Hao, Liang Huang, Daniel Gildea,and Kevin Knight.
2006.
Synchronousbinarization for machine translation.In Proceedings of the main conferenceon Human Language TechnologyConference of the North American Chapterof the Association of ComputationalLinguistics (HLT-NAACL), pages 256?263,New York, NY.Zollmann, Andreas and Khalil Sima?an.2005.
A consistent and efficient estimatorfor data-oriented parsing.
Journal ofAutomata, Languages and Combinatorics,10(2/3):367?388.277
