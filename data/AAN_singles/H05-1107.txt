Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 851?858, Vancouver, October 2005. c?2005 Association for Computational LinguisticsA Backoff Model for Bootstrapping Resourcesfor Non-English Languages?Chenhai Xi and Rebecca HwaDepartment of Computer ScienceUniversity of Pittsburgh{chenhai,hwa}@cs.pitt.eduAbstractThe lack of annotated data is an ob-stacle to the development of manynatural language processing applica-tions; the problem is especially severewhen the data is non-English.
Pre-vious studies suggested the possibilityof acquiring resources for non-Englishlanguages by bootstrapping from highquality English NLP tools and paral-lel corpora; however, the success ofthese approaches seems limited for dis-similar language pairs.
In this paper,we propose a novel approach of com-bining a bootstrapped resource with asmall amount of manually annotateddata.
We compare the proposed ap-proach with other bootstrapping meth-ods in the context of training a ChinesePart-of-Speech tagger.
Experimentalresults show that our proposed ap-proach achieves a significant improve-ment over EM and self-training andsystems that are only trained on man-ual annotations.1 IntroductionNatural language applications that use super-vised learning methods require annotated train-ing data, but annotated data is scarce for many?We thank Stephen Clark, Roger Levy, Carol Nichols,and the three anonymous reviewers for their helpful com-ments.non-English languages.
It has been suggestedthat annotated data for these languages mightbe automatically created by leveraging paral-lel corpora and high-accuracy English systems(Yarowsky and Ngai, 2001; Diab and Resnik,2002).
The studies are centered around theassumption that linguistic analyses for English(e.g., Part-of-Speech tags, Word sense disam-biguation, grammatical dependency relation-ships) are also valid analyses in the translationof the English.
For example, in the Englishnoun phrase the red apples, red modifies ap-ples; the same modifier relationship also exists inits French translations les pommes rouges, eventhough the word orders differ.
To the extentthat the assumption is true, annotated data inthe non-English language can be created by pro-jecting English analyses across a word alignedparallel corpus.
The resulting projected datacan then serve as (albeit noisy) training exam-ples to develop applications in the non-Englishlanguage.The projection approach faces both a theo-retical and a practical challenge.
Theoretically,it is well-known that two languages often donot express the same meaning in the same way(Dorr, 1994).
Practically, the projection frame-work is sensitive to component errors.
In partic-ular, poor word alignments significantly degradethe accuracy of the projected annotations.
Pre-vious research on resource projection attemptsto address these problems by redistributing theparameter values (Yarowsky and Ngai, 2001) orby applying transformation rules (Hwa et al,8512002).
Their experimental results suggest thatwhile these techniques can overcome some er-rors, they are not sufficient for projected datathat are very noisy.In this work, we tackle the same problems byrelaxing the zero manual annotation constraint.The main question we address is: how can wemake the most out of a small set of manually la-beled data (on the non-English side).
Followingthe work of Yarowsky and Ngai (2001) we focuson the task of training a Part-of-Speech (POS)tagger, but we conduct our experiments withthe more dissimilar language pair of English-Chinese instead of English-French.
Throughempirical studies, we show that when the wordalignment quality is sufficiently poor, the er-ror correction techniques proposed by Yarowskyand Ngai are unable to remove enough mistakesin the projected data.
We propose an alternativeapproach that is inspired by backoff languagemodeling techniques in which the parameters oftwo tagging models (one trained on manually la-beled data; the other trained on projected data)are combined to achieve a more accurate finalmodel.2 BackgroundThe idea of trying to squeeze more out of an-notated training examples has been explored ina number of ways in the past.
Most popularis the family of bootstrapping algorithms, inwhich a model is seeded with a small amount oflabeled data and iteratively improved as moreunlabeled data are folded into the training set,typically, through unsupervised learning.
An-other approach is active learning (Cohn et al,1996), in which the model is also iteratively im-proved but the training examples are chosen bythe learning model, and the learning process issupervised.
Finally, the work that is the closestto ours in spirit is the idea of joint estimation(Smith and Smith, 2004).Of the bootstrapping methods, perhaps themost well-known is the Expectation Maximiza-tion (EM) algorithm.
This approach has beenexplored in the context of many NLP applica-tions; one example is text classification (Nigamet al, 1999).
Another bootstrapping approachreminiscent of EM is self-training.
Yarowsky(1995) used this method for word sense disam-biguation.
In self-training, annotated examplesare used as seeds to train an initial classifierwith any supervised learning method.
This ini-tial classifier is then used to automatically an-notate data from a large pool of unlabeled ex-amples.
Of these newly labeled data, the oneslabeled with the highest confidence are used asexamples to train a new classifier.
Yarowskyshowed that repeated application of this pro-cess resulted in a series of word sense classi-fiers with improved accuracy and coverage.
Alsorelated is the co-training algorithm (Blum andMitchell, 1998) in which the bootstrapping pro-cess requires multiple learners that have differ-ent views of the problem.
The key to co-trainingis that the views should be conditionally inde-pendent given the label.
The strong indepen-dence requirement on the views is difficult tosatisfy.
For practical applications, different fea-tures sets or models (that are not conditionallyindependent) have been used as an approxima-tion for different views.
Co-training has been ap-plied to a number of NLP applications, includ-ing POS-tagging (Clark et al, 2003), parsing(Sarkar, 2001), word sense disambiguation (Mi-halcea, 2004), and base noun phrase detection(Pierce and Cardie, 2001).
Due to the relaxationof the view independence assumption, most em-pirical studies suggest a marginal improvement.The common thread between EM, self-training,and co-training is that they all bootstrap offof unannotated data.
In this work, we explorean alternative to ?pure?
unannotated data; ourdata have been automatically annotated withprojected labels from English.
Although theprojected labels are error-prone, they provide uswith more information than automatically pre-dicted labels used in bootstrapping methods.With a somewhat different goal in mind, ac-tive learning addresses the problem of choosingthe most informative data for annotators to la-bel so that the model would achieve the greatestimprovement.
Active learning also has been ap-plied to many NLP applications, including POStagging (Engelson and Dagan, 1996) and pars-852ing (Baldridge and Osborne, 2003).
The draw-back of an active learning approach is that itassumes that a staff of annotators is waiting oncall, ready to label the examples chosen by thesystem at every iteration.
In practice, it is morelikely that one could only afford to staff anno-tators for a limited period of time.
Althoughactive learning is not a focus in this paper, weowe some ideas to active learning in choosing asmall initial set of training examples; we discussthese ideas in section 3.2.More recently, Smith and Smith (2004) pro-posed to merge an English parser, a word align-ment model, and a Korean PCFG parser trainedfrom a small number of Korean parse trees un-der a unified log linear model.
Their results sug-gest that a joint model produces somewhat moreaccurate Korean parses than a PCFG Koreanparser trained on a small amount of annotatedKorean parse trees alone.
Their motivation issimilar to the starting point of our work: that aword aligned parallel corpus and a small amountof annotated data in the foreign language sideoffer information that might be exploited.
Ourapproach differs from theirs in that we do notoptimize the three models jointly.
One concernis that joint optimization might not result in op-timal parameter settings for the individual com-ponents.
Because our focus is primarily on ac-quiring non-English language resources, we onlyuse the parallel corpus as a means of projectingresources from English.3 Our ApproachThis work explores developing a Chinese POStagger without a large manually annotated cor-pus.
Our approach is to train two separatemodels from two different data sources: a largecorpus of automatically tagged data (projectedfrom English) and a small corpus of manuallytagged data; the two models are then combinedinto one via the Whitten-Bell backoff languagemodel.3.1 Projected DataOne method of acquiring a large corpus of au-tomatically POS tagged Chinese data is byprojection (Yarowsky and Ngai, 2001).
Thisapproach requires a sentence-aligned English-Chinese corpus, a high-quality English tagger,and a method of aligning English and Chinesewords that share the same meaning.
Given theparallel corpus, we tagged the English wordswith a publicly available maximum entropy tag-ger (Ratnaparkhi, 1996), and we used an im-plementation of the IBM translation model (Al-Onaizan et al, 1999) to align the words.
TheChinese words in the parallel corpus would thenreceive the same POS tags as the English wordsto which they are aligned.
Next, the basic pro-jection algorithm is modified to accommodatetwo complicating factors.
First, word align-ments are not always one-to-one.
To compen-sate, we assign a default tag to unaligned Chi-nese words; in the case of one-Chinese-to-many-English, the Chinese word would receive the tagof the final English word.
Second, English andChinese do not share the same tag set.
Fol-lowing Yarowsky and Ngai (2001), we define 12equivalence classes over the 47 Penn-English-Treebank POS tags.
We refer to them as CoreTags.
With the help of 15 hand-coded rules anda Naive Bayes model trained on a small amountof manually annotated data, the Core Tags canbe expanded to the granularity of the 33 Penn-Chinese-Treebank POS tags (which we refer toas Full Tags).3.2 Manually Annotated DataSince the amount of manual annotation is lim-ited, we must decide what type of data to anno-tate.
In the spirit of active learning, we aim toselect sentences that may bring about the great-est improvements in the accuracy of our model.Because it is well known that handling unknownwords is a serious problem for POS taggers, ourstrategy for selecting sentences for manual anno-tation is to maximize the word coverage of thein ital model.
That is, we wish to find a smallset of sentences that would lead to the greatestreduction of currently unknown words Findingthese sentences is a NP-hard problem becausethe 0/1 knapsack problem could be reduced tothis problem in polynomial-time (Gurari, 1989).Thus, we developed an approximation algorithmfor finding sentences with the maximum word853M : number of tokens will be annotated.S={s1, s2, .
.
.
, sn}: the unannotated corpus.Ssel : set of selected sentences in S.Sunsel : set of unselected sentences in S.|Ssel| : number of tokens in Ssel.TY PE(Ssel) : number of types in Ssel.MWC:randomly choose Ssel ?
Ssuch that|Ssel| ?
M .For each sentence si in Sselfind a sentence rj in Sunselwhich maximizes swap score(si, rj).if swap score(si, rj) > 0{Ssel = (Ssel ?
si) ?
rj ;Sunsel = (Sunsel ?
rj) ?
si;}swap score(si, rj){Ssel new = (Ssel ?
si) ?
rj ;if ( |Ssel new| > M ) return -1;else return TY PE(Ssel new)?
TY PE(Ssel);}Figure 1: The pseudo-code for MWC algorithm.The input is M and S and the output is Sselcoverage of unknown words (MWC).
This algo-rithm is described in Figure 1,3.3 Basic POS Tagging ModelIt is well known that a POS tagger can betrained with an HMM (Weischedel et al, 1993).Given a trained model, the most likely tag se-quence T?
= {t1, t2, .
.
.
tn} is computed for theinput word sentence: W?
= {w1, w2, .
.
.
wn}:T?
= arg maxTP (T |W ) = arg maxTP (T |W )P (T )The transition probability P (T ) is approxi-mated by a trigram model:P (T ) ?
p(t1)p(t2|t1)n?i=3p(ti|ti?1, ti?2),and the observation probability P (W |T ) is com-puted byP (W |T ) ?n?i=1p(wi|ti).3.4 Combined ModelsFrom the two data sources, two separate trigramtaggers have been trained (Tanno from manuallyannotated data and Tproj from projected data).This section considers ways of combining theminto a single tagger.
The key insight that drivesour approach is based on reducing the effect ofunknown words.
We see the two data sources ascomplementary in that the large projected datasource has better word coverage while the man-ually labeled one is good at providing tag-to-tagtransitions.
Based on this principle, one way ofmerging these two taggers into a single HMM(denoted as Tinterp) is to use interpolation:pinterp(w|t) = ??
panno(w|t)+(1?
?)?
pproj(w|t)pinterp(ti|ti?1, ti?2) = panno(ti|ti?1, ti?2)where ?
is a tunable weighting parameter1 ofthe merged tagger.
This approach may be prob-lematic because it forces the model to alwaysinclude some fraction of poor parameter values.Therefore, we propose to estimate the observa-tion probabilities using backoff.
The parametersof Tback are estimated as follows:pback(w|t) ={?(t)?
panno(w|t) if panno(w|t) > 0?
(t) ?
pproj(w|t) if panno(w|t) = 0pback(ti|ti?1, ti?2) = panno(ti|ti?1, ti?2)where ?
(t) is a discounting coefficient and ?is set to satisfy that?all words P (w|t) = 1.The discounting coefficient is computed usingthe Witten-Bell discounting method:?
(t) =Canno(t)Canno(t) + Sanno(t),where Canno(t) is the count of tokens whosetag is t in the manually annotated corpus and1In our experiments, the value of ?
is set to 0.8 basedon held-out data.854Sanno(t) is the seen types of words with tag t.In other words, we trust the parameter estimatesfrom the model trained on manual annotation bydefault unless it is based on unreliable statistics.4 ExperimentsWe conducted a suite of experiments to inves-tigate the effect of allowing a small amount ofmanually annotated data in conjunction withusing annotations projected from English.
Wefirst establish a baseline of training on projecteddata alone in Section 4.1.
It is an adaptation ofthe approach described by Yarowsky and Ngai(2001).
Next, we consider the case of usingmanually annotated data alone in Section 4.2.We show that there is an increase in accuracywhen the MWC active learning strategy is used.In Section 4.3, we show that with an appro-priate merging strategy, a tagger trained fromboth data sources achieves higher accuracy.
Fi-nally, in Section 4.4, we evaluate our approachagainst other semi-supervised methods to ver-ify that the projected annotations, though noisy,contain useful information.We use an English-Chinese Federal BroadcastInformation Service (FBIS) corpus as the datasource for the projected annotation.
We sim-ulated the manual annotation process by usingthe POS tags provided by the Chinese Treebankversion 4 (CHTB).
We used about a thousandsentences from CHTB as held-out data.
The re-maining sentences are split into ten-fold crossvalidation sets.
Each test set contains 1400 sen-tences.
Training data are selected (using MWC)from the remaining 12600 sentences.
The re-ported results are the average of the ten trials.One tagger is considered to be better than an-other if, according to the paired t-test, we areat least 95% confident that their difference inaccuracy is non-zero.
Performance is measuredin terms of the percentage of correctly taggedtokens in the test data.
For comparability withTproj (which assumes no availability of manu-ally annotated data), most experimental resultsare reported with respect to the reduced CoreTag gold standard; evaluation against the full33 CHTB tag gold standard is reported in Sec-tion 4.4.4.1 Tagger Trained from ProjectedDataTo determine the quality of Tproj for Chinese,we replicate the POS-tagging experiment inYarowsky and Ngai (2001).
Trained on all pro-jected data, the tagger has an accuracy of 58.2%on test sentences.
The low accuracy rate sug-gests that the projected data is indeed verynoisy.
To reduce the noise in the projected data,Yarowsky and Ngai developed a re-estimationtechnique based on the observation that wordsin French, English and Czech have a strong ten-dency to exhibit only a single core POS tagand very rarely have more than two.
Apply-ing the same re-estimation technique that favorsthis bias to the projected Chinese data raisesthe final tagger accuracy to 59.1%.
That re-estimation did not help English-Chinese projec-tion suggests that the dissimilarity between thetwo languages is an important factor.
A relatedreason for the lower accuracy rate is due to poorword alignments in the English-Chinese corpus.As a further noise reduction step, we automat-ically filter out sentence pairs that were poorlyaligned (i.e., the sentence pairs had too manyunaligned words or too many one-to-many align-ments).
This results in a corpus of about 9000FBIS sentences.
A tagger trained on the filtereddata has an improved accuracy of 64.5%.
Wetake this to be Tproj used in later experiments.4.2 Taggers Trained from ManuallyLabeled DataThis experiment verifies that the MaximumWord Coverage (MWC) selection scheme pre-sented in Section 3.2 is helpful in selecting datafor training Tanno.
We compare it against ran-dom selection.
Figure 2 plots the taggers?
per-formances on test sentences as the number ofmanually annotated tokens increase from 100 to30,000.
We see that the taggers trained on dataselected by MWC outperform those trained onrandomly selected data.
Thus, in the main ex-periments, we always use MWC to select the setof manually tagged data for training Tanno.8550.35 0.40.45 0.50.55 0.60.65 0.70.75 0.80.85 0.9  100300050001000015000200002500030000accuracynumber of annotated tokensRandom MWCFigure 2: A comparison between MWC and ran-dom selection.4.3 Evaluation of the CombinedTaggers0.40.45 0.50.55 0.60.65 0.70.75 0.80.85 0.9  100300050001000015000200002500030000accuracynumber of annotated tokensproj anno concat Interp backFigure 3: A comparison of the proposed backoffapproach against alternative methods of com-bining Tproj and TannoTo investigate how Tanno and Tproj might bemerged to form a higher quality tagger, we con-duct an experiment to evaluate the differentalternatives described in section 3.4: Tinterp,and Tback.
They are compared against threebaselines: Tanno, Tproj , and Tconcat, a taggertrained from the concatenation of the two datasources.
To determine the effect of manual an-notation, we vary the size of the training set forTanno from 100 tokens (fewer than 10 sentences)to 30,000 tokens (about 1000 sentences).
Thelearning curves are plotted in Figure 3.
The re-sult suggests that Tback successfully incorporatesinformation from both the manually annotateddata and the projected data.
The improvementover training on manually annotated data alone(Tanno) is especially high when fewer than 10,000manually annotated tokens are available.
As ex-pected, Tinterp, and Tconcat perform worse thanTanno because they are not as effective at dis-counting the erroneous projected annotations.4.4 Comparisons with OtherSemi-Supervised ApproachesThis experiment evaluates the proposed back-off approach against two other semi-supervisedapproaches: self-training (denoted as Tself ) andEM (denoted as Tem).
Both start with a fully su-pervised model (Tanno) and iteratively improveit by seeing more unannotated data.2 As dis-cussed earlier, a major difference between ourproposed approach and the bootstrapping meth-ods is that our approach makes use of anno-tations projected from English while the boot-strapping methods rely on unannotated dataalone.
To investigate the effect of leveragingfrom English resources, we use the Chinese por-tion of the FBIS parallel corpus (the same 9000sentences as the training corpus of Tproj butwithout the projected tags) as the unannotateddata source for the bootstrapping methods.Figure 4 compares the four learning curves.We have evaluated them both in terms of theCore Tag gold standard and in terms of FullTag gold standard.
Although all three ap-proaches produce taggers with higher accuraciesthan that of Tanno, our backoff approach outper-forms both self-training and EM.
The differenceis especially prominent when manual annota-tion is severely limited.
When more manual an-notations are made available, the gap narrows;however, the differences are still statistically sig-nificant at 30,000 manually annotated tokens.These results suggest that projected data havemore useful information than unannotated data.2In our implementation of self-training, the top 10%of the unannoated sentences with the highest confidencescores is selected.
The confidence score is computed as:logP (T |W )length of the sentence .8560.40.45 0.50.55 0.60.65 0.70.75 0.80.85 0.9  100300050001000015000200002500030000accuracynumber of annotated tokensanno self em back0.40.45 0.50.55 0.60.65 0.70.75 0.80.85 0.9  100300050001000015000200002500030000accuracynumber of annotated tokensanno self em back(a) (b)Figure 4: A comparison of Backoff against self-training and EM.
(a) Evaluation against the CoreTag gold standard.
(b) Evaluation against the Full Tag gold standard.5 DiscussionWhile the experimental results support our intu-ition that Tback is effective in making use of bothdata sources, there are still two questions worthaddressing.
First, there may be other ways ofestimating the parameters of a merged HMMfrom the parameters of Tanno and Tproj .
For ex-ample, a natural way of merging the two taggersinto a single HMM (denoted as Tmerge) is to usethe values of the observation probabilities fromTproj and the values of the transition probabili-ties from Tanno:pmerge(w|t) = pproj(w|t),pmerge(ti|ti?1, ti?2) = panno(ti|ti?1, ti?2).Another is the reverse of Tmerge:prev merge(w|t) = panno(w|t)prev merge(ti|ti?1, ti?2) = pproj(ti|ti?1, ti?2)Tmerge is problematic because it ignores all man-ual word-tag annotations; however, Trev merge?slearning curve is nearly identical to that of Tanno(graph not shown).
Its models do not take ad-vantage of the broader word coverage of theprojected data, so it does not perform as wellas Tback.
Trev merge outperforms Tmerge whentrained from more than 2000 manually anno-tated tokens.
We make two observations fromthis finding.
One is that the differences betweenpproj(ti|ti?1, ti?2) and panno(ti|ti?1, ti?2) are notlarge.
Another is that the success of the mergedHMM tagger hinges on the goodness of the ob-servation probabilities, p(w|t).
This is in accordwith our motivation in improving the reliabilityof p(w|t) through backoff.Second, while our experimental results sug-gest that Tback outperforms self-training andEM, these approaches are not incompatible withone another.
Because Tback is partially esti-mated from the noisy corpus of projected an-notations, it might be further improved byapplying a bootstrapping algorithm over thenoisy corpus (with the projected tags removed).To test our hypothesis, we initialized the self-training algorithm with a backoff tagger thatused 3000 manually annotated tokens.
This ledto a slight but statistically significant improve-ment, from 74.3% to 74.9%.6 Conclusion and Future WorkIn summary, we have shown that backoff is an ef-fective technique for combining manually anno-tated data with a large but noisy set of automat-ically annotated data (from projection).
Our ap-857proach is the most useful when a small amountof annotated tokens is available.
In our exper-iments, the best results were achieved when weused 3000 manually annotated tokens (approxi-mately 100 sentences).The current study points us to several direc-tions for future work.
One is to explore ways ofapplying the proposed approach to other learn-ing models.
Another is to compare against othermethods of combining evidences from multiplelearners.
Finally, we will investigate whetherthe proposed approach can be adapted to morecomplex tasks in which the output is not a classlabel but a structure (e.g.
parsing).ReferencesYaser Al-Onaizan, Jan Curin, Michael Jahr, KevinKnight, John Lafferty, I. Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, andDavid Yarowsky.
1999.
Statistical machine transla-tion.
Technical report, JHU.
citeseer.nj.nec.com/al-onaizan99statistical.html.Jason Baldridge and Miles Osborne.
2003.
Active learn-ing for HPSG parse selection.
In Proceedings of the 7thConference on Natural Language Learning, Edmonton,Canada, June.Avrim Blum and Tom Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
In Proceedings ofthe 1998 Conference on Computational Learning The-ory, pages 92?100, Madison, WI.Stephen Clark, James Curran, and Miles Osborne.
2003.Bootstrapping pos-taggers using unlabelled data.
InProc.
of the Computational Natural Language Learn-ing Conference, pages 164?167, Edmonton, Canada,June.David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-dan.
1996.
Active learning with statistical models.Journal of Artificial Intelligence Research, 4:129?145.Mona Diab and Philip Resnik.
2002.
An unsupervisedmethod for word sense tagging using parallel corpora.In Proceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics, Philadelphia,PA.Bonnie J. Dorr.
1994.
Machine translation divergences:A formal description and proposed solution.
Compu-tational Linguistics, 20(4):597?635.Sean P. Engelson and Ido Dagan.
1996.
Minimizing man-ual annotation cost in supervised training from copora.In Proceedings of the 34th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 319?326,Santa Cruz, CA.Eitan Gurari.
1989.
An Introduction to the Theory ofComputation.
Ohio State University Computer Sci-ence Press.Rebecca Hwa, Philip S. Resnik, Amy Weinberg, andOkan Kolak.
2002.
Evaluating translational corre-spondence using annotation projection.
In Proceed-ings of the 40th Annual Meeting of the Association forComputational Linguistics.Rada Mihalcea.
2004.
Co-training and self-training forword sense disambiguation.
In Proceedings of the Con-ference on Computational Natural Language Learning(CoNLL-2004).Kamal Nigam, Andrew McCallum, Sebastian Thrun, andTom Mitchell.
1999.
Text Classification from Labeledand Unlabeled Documents using EM.
Machine Learn-ing, 1(34).David Pierce and Claire Cardie.
2001.
Limitations ofco-training for natural language learning from largedatasets.
In Proceedings of the 2001 Conference onEmpirical Methods in Natural Language Processing(EMNLP-01), pages 1?9, Pittsburgh, PA.Adwait Ratnaparkhi.
1996.
A maximum entropy modelfor part-of-speech tagging.
In Eric Brill and KennethChurch, editors, Proceedings of the Conference on Em-pirical Methods in Natural Language Processing, pages133?142.
Association for Computational Linguistics,Somerset, New Jersey.Anoop Sarkar.
2001.
Applying co-training methods tostatistical parsing.
In Proceedings of the Second Meet-ing of the North American Association for Compu-tational Linguistics, pages 175?182, Pittsburgh, PA,June.David A. Smith and Noah A. Smith.
2004.
Bilingualparsing with factored estimation: Using english toparse korean.
In Proceedings of the 2005 Conferenceon Empirical Methods in Natural Language Processing(EMNLP-05).Ralph Weischedel, Richard Schwartz, Jeff Palmucci,Marie Meteer, and Lance Ramshaw.
1993.
Copingwith ambiguity and unknown words through proba-bilistic models.
Comput.
Linguist., 19(2):361?382.David Yarowsky and Grace Ngai.
2001.
Inducing mul-tilingual POS taggers and NP bracketers via robustprojection across aligned corpora.
In Proceedings ofthe Second Meeting of the North American Associa-tion for Computational Linguistics, pages 200?207.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Proceed-ings of the 35th Annual Meeting of the Associationfor Computational Linguistics, pages 189?196, Cam-bridge, MA.858
