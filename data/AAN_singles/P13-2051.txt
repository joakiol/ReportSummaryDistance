Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 283?288,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsUsing Lexical Expansion to Learn Inference Rules from Sparse DataOren Melamud?, Ido Dagan?, Jacob Goldberger?, Idan Szpektor??
Computer Science Department, Bar-Ilan University?
Faculty of Engineering, Bar-Ilan University?
Yahoo!
Research Israel{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.ilidan@yahoo-inc.comAbstractAutomatic acquisition of inference rulesfor predicates is widely addressed by com-puting distributional similarity scores be-tween vectors of argument words.
Inthis scheme, prior work typically refrainedfrom learning rules for low frequencypredicates associated with very sparse ar-gument vectors due to expected low reli-ability.
To improve the learning of suchrules in an unsupervised way, we proposeto lexically expand sparse argument wordvectors with semantically similar words.Our evaluation shows that lexical expan-sion significantly improves performancein comparison to state-of-the-art baselines.1 IntroductionThe benefit of utilizing template-based inferencerules between predicates was demonstrated inNLP tasks such as Question Answering (QA)(Ravichandran and Hovy, 2002) and InformationExtraction (IE) (Shinyama and Sekine, 2006).
Forexample, the inference rule ?X treat Y ?
X relieveY?, between the templates ?X treat Y?
and ?X re-lieve Y?
may be useful to identify the answer to?Which drugs relieve stomach ache?
?.The predominant unsupervised approach forlearning inference rules between templates is viadistributional similarity (Lin and Pantel, 2001;Ravichandran and Hovy, 2002; Szpektor and Da-gan, 2008).
Specifically, each argument slot ina template is represented by an argument vector,containing the words (or terms) that instantiate thisslot in all of the occurrences of the template in alearning corpus.
Two templates are then deemedsemantically similar if the argument vectors oftheir corresponding slots are similar.Ideally, inference rules should be learned forall templates that occur in the learning corpus.However, many templates are rare and occur onlyfew times in the corpus.
This is a typical NLPphenomenon that can be associated with either asmall learning corpus, as in the cases of domainspecific corpora and resource-scarce languages, orwith templates with rare terms or long multi-wordexpressions such as ?X be also a risk factor to Y?or ?X finish second in Y?, which capture very spe-cific meanings.
Due to few occurrences, the slotsof rare templates are represented with very sparseargument vectors, which in turn lead to low relia-bility in distributional similarity scores.A common practice in prior work for learn-ing predicate inference rules is to simply disre-gard templates below a minimal frequency thresh-old (Lin and Pantel, 2001; Kotlerman et al, 2010;Dinu and Lapata, 2010; Ritter et al, 2010).
Yet,acquiring rules for rare templates may be benefi-cial both in terms of coverage, but also in termsof more accurate rule application, since rare tem-plates are less ambiguous than frequent ones.We propose to improve the learning of rules be-tween infrequent templates by expanding their ar-gument vectors.
This is done via a ?dual?
distribu-tional similarity approach, in which we considertwo words to be similar if they instantiate similarsets of templates.
We then use these similaritiesto expand the argument vector of each slot withwords that were identified as similar to the originalarguments in the vector.
Finally, similarities be-tween templates are computed using the expandedvectors, resulting in a ?smoothed?
version of theoriginal similarity measure.Evaluations on a rule application task showthat our lexical expansion approach significantlyimproves the performance of the state-of-the-artDIRT algorithm (Lin and Pantel, 2001).
In addi-tion, our approach outperforms a similarity mea-sure based on vectors of latent topics instead ofword vectors, a common way to avoid sparsenessissues by means of dimensionality reduction.2832 Technical BackgroundThe distributional similarity score for an inferencerule between two predicate templates, e.g.
?X re-sign Y?
X quit Y?, is typically computed by mea-suring the similarity between the argument vec-tors of the corresponding X slots and Y slots ofthe two templates.
To this end, first the argumentvectors should be constructed and then a similaritymeasure between two vectors should be provided.We note that we focus here on binary templateswith two slots each, but this approach can be ap-plied to any template.A common starting point is to compute aco-occurrence matrix M from a learning cor-pus.
M ?s rows correspond to the template slotsand the columns correspond to the various termsthat instantiate the slots.
Each entry Mi,j , e.g.Mx quit,John, contains a count of the number oftimes the term j instantiated the template slot i inthe corpus.
Thus, each row Mi,?
corresponds toan argument vector for slot i.
Next, some func-tion of the counts is used to assign weights to allMi,j entries.
In this paper we use pointwise mu-tual information (PMI), which is common in priorwork (Lin and Pantel, 2001; Szpektor and Dagan,2008).Finally, rules are assessed using some similar-ity measure between corresponding argument vec-tors.
The state-of-the-art DIRT algorithm (Lin andPantel, 2001) uses the highly cited Lin similaritymeasures (Lin, 1998) to score rules between bi-nary templates as follows:(1)Lin(v, v?)
=?w?v?v?
[v(w) + v?(w)]?w?v?v?
[v(w) + v?
(w)](2)DIRT (l ?
r)=?Lin(vl:x, vr:x) ?
Lin(vl:y, vr:y)where v and v?
are two argument vectors, l andr are the templates participating in the inferencerule and vl:x corresponds to the argument vectorof slot X of template l, etc.
While the originalDIRT algorithm utilizes the Lin measure, one canreplace it with any other vector similarity measure.A separate line of research for word simi-larity introduced directional similarity measuresthat have a bias for identifying generaliza-tion/specification relations, i.e.
relations be-tween predicates with narrow (or specific) seman-tic meanings to predicates with broader meaningsinferred by them (unlike the symmetric Lin).
Onesuch example is the Cover measure (Weeds andWeir, 2003):(3)Cover(v, v?)
=?w?v?v?
[v(w)]?w?v?v?
[v(w)]As can be seen, in the core of the Lin and Covermeasures, as well as in many other well knowndistributional similarity measures such as Jaccard,Dice and Cosine, stand the number of shared ar-guments vs. the total number of arguments in thetwo vectors.
Therefore, when the argument vec-tors are sparse, containing very few non-zero fea-tures, these scores become unreliable and volatile,changing greatly with every inclusion or exclusionof a single shared argument.3 Lexical Expansion SchemeWe wish to overcome the sparseness issues in rarefeature vectors, especially in cases where argu-ment vectors of semantically similar predicatescomprise similar but not exactly identical argu-ments.
To this end, we propose a three stepscheme.
First, we learn lexical expansion sets forargument words, such as the set {euros, money}for the word dollars.
Then we use these sets to ex-pand the argument word vectors of predicate tem-plates.
For example, given the template ?X canbe exchanged for Y?, with the following argumentwords instantiating slot X {dollars, gold}, andthe expansion set above, we would expand the ar-gument word vector to include all the followingwords {dollars, euros, money, gold}.
Finally, weuse the expanded argument word vectors to com-pute the scores for predicate inference rules with agiven similarity measure.When a template is instantiated with an ob-served word, we expect it to also be instantiatedwith semantically similar words such as the onesin the expansion set of the observed word.
We?blame?
the lack of such template occurrencesonly on the size of the corpus and the sparsenessphenomenon in natural languages.
Thus, we uti-lize our lexical expansion scheme to syntheticallyadd these expected but missing occurrences, ef-fectively smoothing or generalizing over the ex-plicitly observed argument occurrences.
Our ap-proach is inspired by query expansion (Voorhees,1994) in Information Retrieval (IR), as well as bythe recent lexical expansion framework proposedin (Biemann and Riedl, 2013), and the work by284Miller et al (2012) on word sense disambigua-tion.
Yet, to the best of our knowledge, this is thefirst work that applies lexical expansion to distri-butional similarity feature vectors.
We next de-scribe our scheme in detail.3.1 Learning Lexical ExpansionsWe start by constructing the co-occurrence matrixM (Section 2), where each entry Mt:s,w indicatesthe number of times that word w instantiates slots of template t in the learning corpus, denoted by?t:s?, where s can be either X or Y.In traditional distributional similarity, the rowsMt:s,?
serve as argument vectors of template slots.However, to learn expansion sets we take a ?dual?view and consider each matrix column M?
:?,w (de-noted vw) as a feature vector for the argumentword w. Under this view, templates (or morespecifically, template slots) are the features.
Forinstance, for the word dollars the respective fea-ture vector may include entries such as ?X can beexchanged for?, ?can be exchanged for Y?, ?pur-chase Y?
and ?sell Y?.We next learn an expansion set per each wordw by computing the distributional similarity be-tween the vectors of w and any other argumentword w?, sim(vw, vw?).
Then we take the N mostsimilar words as w?s expansion set with degreeN , denoted by LNw = {w?1, ..., w?N}.
Any simi-larity measure could be used, but as our experi-ments show, different measures generate sets withdifferent properties, and some may be fitter for ar-gument vector expansion than others.3.2 Expanding Argument VectorsGiven a row count vector Mt:s,?
for slot s of tem-plate t, we enrich it with expansion sets as fol-lows.
For each w in Mt:s,?, the original count invt:s(w) is redistributed equally between itself andall words in w?s expansion set, i.e.
all w?
?
LNw ,(possibly yielding fractional counts) where N is aglobal parameter of the model.
Specifically, thenew count that is assigned to each word w is itsremaining original count after it has been redis-tributed (or zero if no original count), plus all thecounts that were distributed to it from other words.Next, PMI weights are recomputed according tothe new counts, and the resulting expanded vectoris denoted by v+t:s. Similarity between templateslots is now computed over the expanded vectorsinstead of the original ones, e.g.
Lin(v+l:x, v+r:x).4 Experimental SettingsWe constructed a relatively small learning corpusfor investigating the sparseness issues of such cor-pora.
To this end, we used a random sample fromthe large scale web-based ReVerb corpus1 (Faderet al, 2011), comprising tuple extractions of pred-icate templates with their argument instantiations.We applied some clean-up preprocessing to theseextractions, discarding stop words, rare words andnon-alphabetical words that instantiated either theX or the Y argument slots.
In addition, we dis-carded templates that co-occur with less than 5unique argument words in either of their slots, as-suming that such few arguments cannot convey re-liable semantic information, even with expansion.Our final corpus consists of around 350,000 ex-tractions and 14,000 unique templates.
In this cor-pus around one third of the extractions refer totemplates that co-occur with at most 35 unique ar-guments in both their slots.We evaluated the quality of inferencerules using the dataset constructed by Zeich-ner et al (2012)2, which contains about 6,500manually annotated template rule applications,each labeled as correct or not.
For example,?The game develop eye-hand coordination9 Thegame launch eye-hand coordination?
is a ruleapplication in this dataset of the rule ?X developY ?
X launch Y?, labeled as incorrect, and?Captain Cook sail to Australia?
Captain Cookdepart for Australia?
is a rule application of therule ?X sail to Y ?
X depart for Y?, labeled ascorrect.
Specifically, we induced two datasetsfrom Zeichner et al?s dataset, denoted DS-5-35and DS-5-50, which consist of all rule applica-tions whose templates are present in our learningcorpus and co-occurred with at least 5 and atmost 35 and 50 unique argument words in boththeir slots, respectively.
DS-5-35 includes 311rule applications (104 correct and 207 incorrect)and DS-5-50 includes 502 rule applications (190correct and 312 incorrect).Our evaluation task is to rank all rule applica-tions in each test set based on the similarity scoresof the applied rules.
Optimal performance wouldrank all correct rule applications above the in-correct ones.
As a baseline for rule scoring we1http://reverb.cs.washington.edu/2http://www.cs.biu.ac.il/nlp/downloads/annotation-rule-application.htm285used the DIRT algorithm scheme, denoted DIRT-LE-None.
We then compared between the perfor-mance of this baseline and its expanded versions,testing two similarity measures for generating theexpansion sets of arguments: Lin and Cover.
Wedenote these expanded methods DIRT-LE-SIM-N,where SIM is the similarity measure used to gen-erate the expansion sets and N is the lexical expan-sion degree, e.g.
DIRT-LE-Lin-2.We remind the reader that our scheme utilizestwo similarity measures.
The first measure as-sesses the similarity between the argument vectorsof the two templates in the rule.
This measureis kept constant in our experiments and is iden-tical to DIRT?s similarity measure (Lin).
3 Thesecond measure assesses the similarity betweenwords and is used for the lexical expansion of ar-gument vectors.
Since this is the research goalof this paper, we experimented with two differentmeasures for lexical expansion: a symmetric mea-sure (Lin) and an asymmetric measure (Cover).To this end we evaluated their effect on DIRT?srule ranking performance and compared them to avanilla version of DIRT without lexical expansion.As another baseline, we follow Dinu and La-pata (2010) inducing LDA topic vectors for tem-plate slots and computing predicate template infer-ence rule scores based on similarity between thesevectors.
We use standard hyperparameters forlearning the LDA model (Griffiths and Steyvers,2004).
This method is denoted LDA-K, where K isthe number of topics in the model.5 ResultsWe evaluated the performance of each testedmethod by measuring Mean Average Precision(MAP) (Manning et al, 2008) of the rule applica-tion ranking computed by this method.
In orderto compute MAP values and corresponding sta-tistical significance, we randomly split each testset into 30 subsets.
For each method we com-puted Average Precision on every subset and thentook the average as the MAP value.
We variedthe degree of the lexical expansion in our modeland the number of topics in the topic model base-line to analyze their effect on the performance ofthese methods on our datasets.
We note that in ourmodel a greater degree of lexical expansion cor-3Experiments with Cosine as the template similarity mea-sure instead of Lin for both DIRT and its expanded versionsyielded similar results.
We omit those for brevity.responds to more aggressive smoothing (or gen-eralization) of the explicitly observed data, whilethe same goes for a lower number of topics in thetopic model.
The results on DS-5-35 and DS-5-50are illustrated in Figure 1.The most dramatic improvement over the base-lines is evident in DS-5-35, where DIRT-LE-Cover-2 achieves a MAP score of 0.577 in com-parison to 0.459 achieved by its DIRT-LE-Nonebaseline.
This is indeed the dataset where we ex-pected expansion to affect most due the extremesparseness of argument vectors.
Both DIRT-LE-Cover-N and DIRT-LE-Lin-N outperform DIRT-LE-None for all tested values of N , with statisti-cal significance via a paired t-test at p < 0.05 forDIRT-LE-Cover-N where 1 ?
N ?
5, and p <0.01 for DIRT-LE-Cover-2.
On DS-5-50, improve-ment over the DIRT-LE-None baseline is still sig-nificant with both DIRT-LE-Cover-N and DIRT-LE-Lin-N outperforming DIRT-LE-None.
DIRT-LE-Cover-N again performs best and achieves arelative improvement of over 10% with statisticalsignificance at p < 0.05 for 2 ?
N ?
3.The above shows that expansion is effective forimproving rule learning between infrequent tem-plates.
Furthermore, the fact that DIRT-LE-Cover-N outperforms DIRT-LE-Lin-N suggests that us-ing directional expansions, which are biased togeneralizations of the observed argument words,e.g.
vehicle as an expansion for car, is more ef-fective than using symmetrically related words,such as bicycle or automobile.
This conclusionappears also to be valid from a semantic reason-ing perspective, as given an observed predicate-argument occurrence, such as ?drive car?
we canmore likely infer that a presumed occurrence ofthe same predicate with a generalization of the ar-gument, such as ?drive vehicle?, is valid, i.e.
?drivecar ?
drive vehicle?.
On the other hand while?drive car ?
drive automobile?
is likely to bevalid, ?drive car ?
drive bicycle?
and ?drive ve-hicle?
drive bicycle?
are not.Figure 1 also depicts the performance of LDAas a vector smoothing approach.
LDA-K out-performs the DIRT-LE-None baseline under DS-5-35 but with no statistical significance.
UnderDS-5-50 LDA-K performs worst, slightly outper-forming DIRT-LE-None only for K=450.
Further-more, under both datasets, LDA-K is outperformedby DIRT-LE-Cover-N.
These results indicate thatLDA is less effective than our expansion approach.286Figure 1: MAP scores on DS-5-35 and DS-5-50 for the original DIRT scheme, denoted DIRT-LE-None,and for the compared smoothing methods as follows.
DIRT with varied degrees of lexical expansionis denoted as DIRT-LE-Lin-N and DIRT-LE-Cover-N.
The topic model with varied number of topics isdenoted as LDA-K. Data labels indicate the expansion degree (N) or the number of LDA topics (K),depending on the tested method.One reason may be that in our model, every expan-sion set may be viewed as a cluster around a spe-cific word, an outstanding difference in compari-son to topics, which provide a global partition overall words.
We note that performance improve-ment of singleton document clusters over globalpartitions was also shown in IR (Kurland and Lee,2009).In order to further illustrate our lexical expan-sion scheme we focus on the rule application?Captain Cook sail to Australia?
Captain Cookdepart for Australia?, which is labeled as correctin our test set and corresponds to the rule ?X sailto Y ?
X depart for Y?.
There are 30 words in-stantiating the X slot of the predicate ?sail to?in our learning corpus including {Columbus, em-peror, James, John, trader}.
On the other hand,there are 18 words instantiating the X slot of thepredicate ?depart for?
including {Amanda, Jerry,Michael, mother, queen}.
While semantic simi-larity between these two sets of words is evident,they share no words in common, and therefore theoriginal DIRT algorithm, DIRT-LE-None, wronglyassigns a zero score to the rule.The following are descriptions of some of theargument word expansions performed by DIRT-LE-Cover-2 (using the notation LNw defined in Sec-tion 3.1) for the X slot of ?sail to?
L2John = {mr.,dr.
}, L2trader = {people, man}, and for the X slotof ?depart for?, L2Michael = {John, mr.}, L2mother ={people, woman}.
Given these expansions the twoslots now share the following words {mr. ,people,John} and the rule score becomes positive.It is also interesting to compare the expansionsperformed by DIRT-LE-Lin-2 to the above.
Forinstance in this case L2mother = {father, sarah},which does not identify people as a shared argu-ment for the rule.6 ConclusionsWe propose to improve the learning of infer-ence rules between infrequent predicate templateswith sparse argument vectors by utilizing a novelscheme that lexically expands argument vectorswith semantically similar words.
Similarities be-tween argument words are discovered using a dualdistributional representation, in which templatesare the features.We tested the performance of our expansionapproach on rule application datasets that werebiased towards rare templates.
Our evaluationshowed that rule learning with expanded vectorsoutperformed the baseline learning with originalvectors.
It also outperformed an LDA-based simi-larity model that overcomes sparseness via dimen-sionality reduction.In future work we plan to investigate how ourscheme performs when integrated with manuallyconstructed resources for lexical expansion, suchas WordNet (Fellbaum, 1998).AcknowledgmentsThis work was partially supported by the IsraeliMinistry of Science and Technology grant 3-8705,the Israel Science Foundation grant 880/12, andthe European Community?s Seventh FrameworkProgramme (FP7/2007-2013) under grant agree-ment no.
287923 (EXCITEMENT).287ReferencesChris Biemann and Martin Riedl.
2013.
Text: Nowin 2d!
a framework for lexical expansion with con-textual similarity.
Journal of Language Modeling,1(1).Georgiana Dinu and Mirella Lapata.
2010.
Topic mod-els for meaning similarity in context.
In Proceedingsof COLING: Posters.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of EMNLP.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.Thomas L Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the Nationalacademy of Sciences of the United States of Amer-ica, 101(Suppl 1):5228?5235.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2010.
Directional distribu-tional similarity for lexical inference.
Natural Lan-guage Engineering, 16(4):359?389.Oren Kurland and Lillian Lee.
2009.
Clusters, lan-guage models, and ad hoc information retrieval.ACM Transactions on Information Systems (TOIS),27(3):13.Dekang Lin and Patrick Pantel.
2001.
DIRT ?
discov-ery of inference rules from text.
In Proceedings ofKDD.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of COLING-ACL.Christopher D Manning, Prabhakar Raghavan, andHinrich Schu?tze.
2008.
Introduction to informationretrieval, volume 1.
Cambridge University PressCambridge.Tristan Miller, Chris Biemann, Torsten Zesch, andIryna Gurevych.
2012.
Using distributional similar-ity for lexical expansion in knowledge-based wordsense disambiguation.
Proceedings of COLING,Mumbai, India.Deepak Ravichandran and Eduard Hovy.
2002.
Learn-ing surface text patterns for a question answeringsystem.
In Proceedings of ACL.Alan Ritter, Oren Etzioni, et al 2010.
A latent dirich-let alocation method for selectional preferences.
InProceedings of ACL.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted rela-tion discovery.
In Proceedings of NAACL.Idan Szpektor and Ido Dagan.
2008.
Learning entail-ment rules for unary templates.
In Proceedings ofCOLING.Ellen M Voorhees.
1994.
Query expansion usinglexical-semantic relations.
In Proceedings of SIGIR.Julie Weeds and David Weir.
2003.
A general frame-work for distributional similarity.
In Proceedings ofEMNLP.Naomi Zeichner, Jonathan Berant, and Ido Dagan.2012.
Crowdsourcing inference-rule evaluation.
InProceedings of ACL (short papers).288
