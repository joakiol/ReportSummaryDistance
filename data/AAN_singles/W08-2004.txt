Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 25?32Manchester, August 2008Encoding Tree Pair-based Graphs in Learning Algorithms:the Textual Entailment Recognition CaseAlessandro MoschittiDISI, University of TrentoVia Sommarive 1438100 POVO (TN) - Italymoschitti@dit.unitn.itFabio Massimo ZanzottoDISP, University of Rome ?Tor Vergata?Via del Politecnico 100133 Roma, Italyzanzotto@info.uniroma2.itAbstractIn this paper, we provide a statistical ma-chine learning representation of textual en-tailment via syntactic graphs constitutedby tree pairs.
We show that the natural wayof representing the syntactic relations be-tween text and hypothesis consists in thehuge feature space of all possible syntac-tic tree fragment pairs, which can only bemanaged using kernel methods.
Experi-ments with Support Vector Machines andour new kernels for paired trees show thevalidity of our interpretation.1 IntroductionRecently, a lot of valuable work on the recogni-tion of textual entailment (RTE) has been carriedout (Bar Haim et al, 2006).
The aim is to detectimplications between sentences like:T1?
H1T1?Wanadoo bought KStones?H1?Wanadoo owns KStones?where T1and H1stand for text and hypothesis, re-spectively.Several models, ranging from the simple lexi-cal similarity between T and H to advanced LogicForm Representations, have been proposed (Cor-ley and Mihalcea, 2005; Glickman and Dagan,2004; de Salvo Braz et al, 2005; Bos and Mark-ert, 2005).
However, since a linguistic theory ableto analytically show how to computationally solvethe RTE problem has not been developed yet, toc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.design accurate systems, we should rely upon theapplication of machine learning.
In this perspec-tive, TE training examples have to be representedin terms of statistical feature distributions.
Thesetypically consist in word sequences (along withtheir lexical similarity) and the syntactic structuresof both text and hypothesis (e.g.
their parse trees).The interesting aspect with respect to other naturallanguage problems is that, in TE, features usefulat describing an example are composed by pairs offeatures from Text and Hypothesis.For example, using a word representation, a textand hypothesis pair, ?T,H?, can be representedby the sequences of words of the two sentences,i.e.
?t1, .., tn?
and ?h1, .., hm?, respectively.
If wecarry out a blind and complete statistical correla-tion analysis of the two sequences, the entailmentproperty would be described by the set of subse-quence pairs from T and H , i.e.
the set R ={?st, sh?
: st= ?ti1, .., til?, sh= ?hj1, .., hjr?, l ?n, r ?
m}.
The relation set R constitutes anaive and complete representation of the example?T,H?
in the feature space {?v,w?
: v,w ?
V ?
},where V is the corpus vocabulary1 .Although the above representation is correct andcomplete from a statistically point of view, it suf-fers from two practical drawbacks: (a) it is expo-nential in V and (b) it is subject to high degree ofdata sparseness which may prevent to carry out ef-fective learning.
The traditional solution for thisproblem relates to consider the syntactic structureof word sequences which provides their general-ization.The use of syntactic trees poses the problemof representing structures in learning algorithms.1V?
is larger than the actual space, which is the one ofall possible subsequences with gaps, i.e.
it only contains allpossible concatenations of words respecting their order.25For this purpose, kernel methods, and in partic-ular tree kernels allow for representing trees interms of all possible subtrees (Collins and Duffy,2002).
Unfortunately, the representation in entail-ment recognition problems requires the definitionof kernels over graphs constituted by tree pairs,which are in general different from kernels appliedto single trees.
In (Zanzotto and Moschitti, 2006),this has been addressed by introducing semanticlinks (placeholders) between text and hypothesisparse trees and evaluating two distinct tree ker-nels for the trees of texts and for those of hypothe-ses.
In order to make such disjoint kernel combi-nation effective, all possible assignments betweenthe placeholders of the first and the second en-tailment pair were generated causing a remarkableslowdown.In this paper, we describe the feature space ofall possible tree fragment pairs and we show that itcan be evaluated with a much simpler kernel thanthe one used in previous work, both in terms ofdesign and computational complexity.
Moreover,the experiments on the RTE datasets show that ourproposed kernel provides higher accuracy than thesimple union of tree kernel spaces.2 Fragments of Tree Pair-based GraphsThe previous section has pointed out that RTE canbe seen as a relational problem between word se-quences of Text and Hypothesis.
The syntacticstructures embedded in such sequences can be gen-eralized by natural language grammars.
Such gen-eralization is very important since it is evident thatentailment cases depend on the syntactic structuresof Text and Hypothesis.
More specifically, the setR described in the previous section can be ex-tended and generalized by considering syntacticderivations2 that generate word sequences in thetraining examples.
This corresponds to the follow-ing set of tree fragment pairs:R?= {?
?t, ?h?
: ?t?
F(T ), ?h?
F(H)}, (1)where F(?)
indicates the set of tree fragments of aparse tree (i.e.
the one of the text T or of the hy-pothesis H).
R?
contains less sparse relations thanR.
For instance, given T1and H1of the previoussection, we would have the following relational de-scription:2By cutting derivation at different depth, different degreesof generalization can be obtained.R?={?NPNNP,NPNNP?
, ?SNP VP,SNP VP?
,?SNPNNPVPVBPboughtNPNNP,SNPNNPVPVBPownsNPNNP?
,?VPVBPboughtNPNNP,VPVBPownsNPNNP?
, ..}These features (relational pairs) generalize theentailment property, e.g.
the pair ?
[VP [VBP bought] [NP]],[VP [VBP own] [NP]]?
generalizes many word sequences,i.e.
those external to the verbal phrases and inter-nal to the NPs.We can improve this space by adding semanticlinks between the tree fragments.
Such linksor placeholders have been firstly proposed in(Zanzotto and Moschitti, 2006).
A placeholderassigned to a node of ?tand a node of ?hstatesthat such nodes dominate the same (or similar) in-formation.
In particular, placeholders are assignedto nodes whose words tiin T are equal, similar, orsemantically dependent on words hjin H .
Usingplaceholders, we obtain a richer fragment pairbased representation that we call R?p, exemplifiedhereafter:{?SNPNNP XVPVBPboughtNPNNP Y,SNPNNP XVPVBPownsNPNNP Y?, ?SNP VPVBPboughtNPNNP Y,SNP VPVBPownsNPNNP Y?, ?SNP VP,SNP VP?
, ...}The placeholders (or variables) indicated withX and Y specify that the NNPs labeled bythe same variables dominate similar or identicalwords.
Therefore, an automatic algorithm thatassigns placeholders to semantically similar con-stituents is needed.
Moreover, although R?p con-tains more semantic and less sparse features than26both R?
and R, its cardinality is still exponential inthe number of the words of T and H .
This meansthat standard machine learning algorithms cannotbe applied.
In contrast, tree kernels (Collins andDuffy, 2002) can be used to efficiently generatethe huge space of tree fragments but, to generatethe space of pairs of tree fragments, a new kernelfunction has to be defined.The next section provides a solution to bothproblems.
i.e.
an algorithm for placeholders as-signments and for the computation of paired treekernels which generates R?
and R?p representa-tions.F(VPVbookNPDaNflight)={VPV NPDaNflight,VPV NPD N,NPDaNflight,NPDaN ,NPD Nflight,NPD N,Nflight, .
.
.
}Figure 1: A syntactic parse tree.3 Kernels over Semantic Tree Pair-basedGraphsThe previous section has shown that placeholdersenrich a tree-based graph with relational informa-tion, which, in turn, can be captured by meansof word semantic similarities simw(wt, wh), e.g.
(Corley and Mihalcea, 2005; Glickman et al,2005).
More specifically, we use a two-step greedyalgorithm to anchor the content words (verbs,nouns, adjectives, and adverbs) in the hypothesisWHto words in the text WT.In the first step, each word whin WHis con-nected to all words wtin WTthat have the max-imum similarity simw(wt, wh) with it (more thanone wtcan have the maximum similarity with wh).As result, we have a set of anchors A ?
WT?WH.simw(wt, wh) is computed by means of three tech-niques:1.
Two words are maximally similar if they havethe same surface form wt= wh.2.
Otherwise, WordNet (Miller, 1995) similari-ties (as in (Corley and Mihalcea, 2005)) anddifferent relation between words such as verbentailment and derivational morphology areapplied.3.
The edit distance measure is finally used tocapture the similarity between words that aremissed by the previous analysis (for mis-spelling errors or for the lack of derivationalforms in WordNet).In the second step, we select the final anchor setA??
A, such that ?wt(or wh) ?!
?wt, wh?
?
A?.The selection is based on a simple greedy algo-rithm that given two pairs ?wt, wh?
and ?w?t, wh?to be selected and a pair ?st, sh?
already selected,considers word proximity (in terms of number ofwords) between wtand stand between w?tand st;the nearest word will be chosen.Once the graph has been enriched with seman-tic information we need to represent it in the learn-ing algorithm; for this purpose, an interesting ap-proach is based on kernel methods.
Since the con-sidered graphs are composed by only two trees, wecan carried out a simplified computation of a graphkernel based on tree kernel pairs.3.1 Tree KernelsTree Kernels (e.g.
see NLP applications in (Giu-glea and Moschitti, 2006; Zanzotto and Moschitti,2006; Moschitti et al, 2007; Moschitti et al,2006; Moschitti and Bejan, 2004)) represent treesin terms of their substructures (fragments) whichare mapped into feature vector spaces, e.g.
?n.The kernel function measures the similarity be-tween two trees by counting the number of theircommon fragments.
For example, Figure 1 showssome substructures for the parse tree of the sen-tence "book a flight".
The main advantage oftree kernels is that, to compute the substructuresshared by two trees ?1and ?2, the whole fragmentspace is not used.
In the following, we report theformal definition presented in (Collins and Duffy,2002).Given the set of fragments {f1, f2, ..} = F , theindicator function Ii(n) is equal 1 if the target fiisrooted at node n and 0 otherwise.
A tree kernel isthen defined as:TK(?1, ?2) =?n1?N?1?n2?N?2?
(n1, n2) (2)where N?1and N?2are the sets of the ?1?s and ?2?s27nodes, respectively and?
(n1, n2) =|F|?i=1Ii(n1)Ii(n2)The latter is equal to the number of common frag-ments rooted in the n1and n2nodes and ?
can beevaluated with the following algorithm:1. if the productions at n1and n2are differentthen ?
(n1, n2) = 0;2. if the productions at n1and n2are thesame, and n1and n2have only leaf children(i.e.
they are pre-terminals symbols) then?
(n1, n2) = 1;3. if the productions at n1and n2are the same,and n1and n2are not pre-terminals then?
(n1, n2) =nc(n1)?j=1(1 + ?
(cjn1, cjn2)) (3)where nc(n1) is the number of the children ofn1and cjnis the j-th child of the node n. Notethat since the productions are the same, nc(n1) =nc(n2).Additionally, we add the decay factor ?
by mod-ifying steps (2) and (3) as follows3:2.
?
(n1, n2) = ?,3.
?
(n1, n2) = ?nc(n1)?j=1(1 + ?
(cjn1, cjn2)).The computational complexity of Eq.
2 isO(|N?1| ?
|N?2|) although the average runningtime tends to be linear (Moschitti, 2006).3.2 Tree-based Graph KernelsThe above tree kernel function can be applied tothe parse trees of two texts or those of the two hy-potheses to measure their similarity in terms of theshared fragments.
If we sum the contributions ofthe two kernels (for texts and for hypotheses) asproposed in (Zanzotto and Moschitti, 2006), wejust obtain the feature space of the union of thefragments which is completely different from thespace of the tree fragments pairs, i.e.
R?
.
Notethat the union space is not useful to describe which3To have a similarity score between 0 and 1, we also ap-ply the normalization in the kernel space, i.e.
K?
(?1, ?2) =TK(?1,?2)?TK(?1,?1)?TK(?2,?2).grammatical and lexical property is at the sametime held by T and H to trig the implication.Therefore to generate the space of the frag-ment pairs we need to define the kernel betweentwo pairs of entailment examples ?T1,H1?
and?T2,H2?
asKp(?T1,H1?, ?T2,H2?)
==?n1?T1?n2?T2?n3?H1?n4?H2?
(n1, n2, n3, n4),where ?
evaluates the number of subtrees rootedin n1and n2combined with those rooted in n3andn4.
More specifically, each fragment rooted intothe nodes of the two texts?
trees is combined witheach fragment rooted in the two hypotheses?
trees.Now, since the number of subtrees rooted in thetexts is independent of the number of trees rootedin the hypotheses,?
(n1, n2, n3, n4) = ?
(n1, n2)?
(n3, n4).Therefore, we can rewrite Kpas:Kp(?T1,H1?, ?T2,H2?)
==?n1?T1?n2?T2?n3?H1?n4?H2?
(n1, n2)?
(n3, n4) ==?n1?T1?n2?T2?
(n1, n2)?n3?H1?n4?H2?
(n3, n4) == Kt(T1, T2)?Kt(H1,H2).
(4)This result shows that the natural kernel to rep-resent textual entailment sentences is the kernelproduct, which corresponds to the set of all possi-ble syntactic fragment pairs.
Note that, such kernelcan be also used to evaluate the space of fragmentpairs for trees enriched with relational information,i.e.
by placeholders.4 Approximated Graph KernelThe feature space described in the previous sec-tion correctly encodes the fragment pairs.
How-ever, such huge space may result inadequate alsofor algorithms such as SVMs, which are in generalrobust to many irrelevant features.
An approxima-tion of the fragment pair space is given by the ker-nel described in (Zanzotto and Moschitti, 2006).Hereafter we illustrate its main points.First, tree kernels applied to two texts or two hy-potheses match identical fragments.
When place-holders are added to trees, the labeled fragments28are matched only if the basic fragments and theassigned placeholders match.
This means thatwe should use the same placeholders for all textsand all hypotheses of the corpus.
Moreover, theyshould be assigned in a way that similar syntac-tic structures and similar relational information be-tween two entailment pairs can be matched, i.e.same placeholders should be assigned to the po-tentially similar fragments.Second, the above task cannot be carried out atpre-processing time, i.e.
when placeholders areassigned to trees.
At the running time, instead,we can look at the comparing trees and make amore consistent decision on the type and order ofplaceholders.
Although, there may be several ap-proaches to accomplish this task, we apply a basicheuristic which is very intuitive:Choose the placeholder assignment that maxi-mizes the tree kernel function over all possible cor-respondencesMore formally, let A and A?
be the placeholder setsof ?T,H?
and ?T ?,H ?
?, respectively, without lossof generality, we consider |A| ?
|A?| and we aligna subset of A to A?.
The best alignment is the onethat maximizes the syntactic and lexical overlap-ping of the two subtrees induced by the aligned setof anchors.
By calling C the set of all bijectivemappings from S ?
A, with |S| = |A?|, to A?,an element c ?
C is a substitution function.
Wedefine the best alignment cmaxthe one determinedbycmax= argmaxc?C(TK(t(T, c), t(T?, i))+TK(t(H, c), t(H?, i)),where (1) t(?, c) returns the syntactic tree enrichedwith placeholders replaced by means of the sub-stitution c, (2) i is the identity substitution and (3)TK(?1, ?2) is a tree kernel function (e.g.
the onespecified by Eq.
2) applied to the two trees ?1and?2.At the same time, the desired similarity valueto be used in the learning algorithm is givenby the kernel sum: TK(t(T, cmax), t(T?, i)) +TK(t(H, cmax), t(H?, i)), i.e.
by solving the fol-lowing optimization problem:Ks(?T,H?, ?T?,H??)
=maxc?C(TK(t(T, c), t(T?, i))+TK(t(H, c), t(H?, i)),(5)For example, let us compare the following twopairs (T1,H1) and (T2,H2) in Fig.
2.To assign the placeholders 1 , 2 and 3 of(T2,H2) to those of (T1,H1), i.e.
X and Y , weneed to maximize the similarity between the twotexts?
trees and between the two hypotheses?
trees.It is straightforward to derive that X=1 and Y=3 al-low more substructures (i.e.
large part of the trees)to be identical, e.g.
[S [NP 1 X VP]] , [VP [VBPNP 3 Y ]], [S [NP 1 X VP [VBP NP 3 Y ]]].Finally, it should be noted that, (a)Ks(?T,H?, ?T?,H??)
is a symmetric functionsince the set of derivation C are always computedwith respect to the pair that has the largest anchorset and (b) it is not a valid kernel as the maxfunction does not in general produce valid kernels.However, in (Haasdonk, 2005), it is shown thatwhen kernel functions are not positive semidef-inite like in this case, SVMs still solve a dataseparation problem in pseudo Euclidean spaces.The drawback is that the solution may be only alocal optimum.
Nevertheless, such solution canstill be valuable as the problem is modeled with avery rich feature space.Regarding the computational complexity, run-ning the above kernel on a large training set mayresult very expensive.
To overcome this drawback,in (Moschitti and Zanzotto, 2007), it has been de-signed an algorithm to factorize the evaluation oftree subparts with respect to the different substitu-tion.
The resulting speed-up makes the applicationof such kernel feasible for datasets of ten of thou-sands of instances.5 ExperimentsThe aim of the experiments is to show that thespace of tree fragment pairs is the most effectiveto represent Tree Pair-based Graphs for the designof Textual Entailment classifiers.5.1 Experimental SetupTo compare our model with previous work weimplemented the following kernels in SVM-light(Joachims, 1999):?
Ks(e1, e2) = Kt(T1, T2) + Kt(H1,H2),where e1= ?T1,H1?
and e2= ?T2,H2?are two text and hypothesis pairs and Ktisthe syntactic tree kernel (Collins and Duffy,2002) presented in the previous section.?
Kp(e1, e2) = Kt(T1, T2) ?
Kt(H1,H2),which (as shown in the previous sections) en-29T1?
H1SNP XNNP XWanadooVPVBPboughtNP YNNP YKStonesSNP XNNP XWanadooVPVBPownsNP YNNP YKStonesT2?
H2SNP 1NP 1DTtheNN 1presidentPP 2INofNP 2NNP 2MiramaxVPVBPboughtNP 3DTaNN 3castleSNP 1NP 1DTtheNN 1presidentPP 2INofNP 2NNP 2MiramaxVPVBPownNP 3DTaNN 3castleFigure 2: The problem of finding the correct mapping between placeholderscodes the tree fragment pairs with and with-out placeholders.?
Kmax(e1, e2) = maxc?C(Kt(?c(T1), ?c(T2))+Kt(?c(H1), ?c(H2))), where c is a possi-ble placeholder assignment which connectsnodes from the first pair with those of the sec-ond pair and ?c(?)
transforms trees accordingto c.?
Kpmx(e1, e2) = maxc?C(Kt(?c(T1), ?c(T2))?Kt(?c(H1), ?c(H2))).Note that Kmaxis the kernel proposed in (Zanzottoand Moschitti, 2006) and Kpmxis a hybrid kernelbased on the maximum Kp, which uses the spaceof tree fragment pairs.
For all the above kernels,we set the default cost factor and trade-off param-eters and we set ?
to 0.4.To experiment with entailment relations, weused the data sets made available by the first (Da-gan et al, 2005) and second (Bar Haim et al, 2006)Recognizing Textual Entailment Challenge.
Thesecorpora are divided in the development sets D1and D2 and the test sets T1 and T2.
D1 contains567 examples whereas T1, D2 and T2 all have thesame size, i.e.
800 instances.
Each example is anordered pair of texts for which the entailment rela-tion has to be decided.5.2 Evaluation and DiscussionTable 1 shows the results of the above kernelson the split used for the RTE competitions.
Thefirst column reports the kernel model.
The secondand third columns illustrate the model accuracy forRTE1 whereas column 4 and 5 show the accuracyfor RTE2.
Moreover, ?
P indicates the use of stan-dard syntactic trees and P the use of trees enrichedwith placeholders.
We note that:First, the space of tree fragment pairs, gener-ated by Kpimproves the one generated by Ks(i.e.the simple union of the fragments of texts and hy-potheses) of 4 (58.9% vs 54.9%) and 0.9 (53.5%vs 52.6%) points on RTE1 and RTE2, respectively.This suggests that the fragment pairs are more ef-fective for encoding the syntactic rules describingthe entailment concept.Second, on RTE1, the introduction of placehold-ers does not improve Kpor Kssuggesting that fortheir correct exploitation an extension of the spaceof tree fragment pairs should be modeled.Third, on RTE2, the impact of placeholdersseems more important but only Kmaxand Ksare able to fully exploit their semantic contribu-tion.
A possible explanation is that in order touse the set of all possible assignments (required byKmax), we needed to prune the ?too large?
syntac-tic trees as also suggested in (Zanzotto and Mos-chitti, 2006).
This may have negatively biased thestatistical distribution of tree fragment pairs.Finally, although we show that Kpis better30Kernels RTE1 RTE2?
P P ?
P PKs54.9 50.0 52.6 59.5Kp58.9 55.5 53.5 56.0Kmax- 58.25 - 61.0Kpmx- 50.0 - 56.8Table 1: Accuracy of different kernel models using(P) and not using (?
P) placeholder information onRTE1 and RTE2.suited for RTE than the other kernels, its accuracyis lower than the state-of-the-art in RTE.
This is be-cause the latter uses additional models like the lex-ical similarity between text and hypothesis, whichgreatly improve accuracy.6 ConclusionIn this paper, we have provided a statistical ma-chine learning representation of textual entailmentvia syntactic graphs constituted by tree pairs.
Wehave analytically shown that the natural way ofrepresenting the syntactic relations between textand hypothesis in learning algorithms consists inthe huge feature space of all possible syntactic treefragment pairs, which can only be managed usingkernel methods.Therefore, we used tree kernels, which allow forrepresenting trees in terms of all possible subtrees.More specifically, we defined a new model for theentailment recognition problems, which requiresthe definition of kernels over graphs constituted bytree pairs.
These are in general different from ker-nels applied to single trees.
We also studied an-other alternative solution which concerns the useof semantic links (placeholders) between text andhypothesis parse trees (to form relevant semanticfragment pairs) and the evaluation of two distincttree kernels for the trees of texts and for those ofhypotheses.
In order to make such disjoint kernelcombination effective, all possible assignments be-tween the placeholders of the first and the secondentailment pair have to be generated (causing a re-markable slowdown).Our experiments on the RTE datasets show thatour proposed kernel may provide higher accuracythan the simple union of tree kernel spaces with amuch simpler and faster algorithm.
Future workwill be devoted to make the tree fragment pairspace more effective, e.g.
by using smaller and ac-curate tree representation for text and hypothesis.AcknowledgmentsWe would like to thank the anonymous reviewersfor their professional and competent reviews andfor their invaluable suggestions.Alessandro Moschitti would like to thank the Eu-ropean Union project, LUNA (spoken LanguageUNderstanding in multilinguAl communicationsystems) contract n 33549 for supporting part ofhis research.ReferencesBar Haim, Roy, Ido Dagan, Bill Dolan, Lisa Ferro,Danilo Giampiccolo, Bernardo Magnini, and IdanSzpektor.
2006.
The II PASCAL RTE challenge.In PASCAL Challenges Workshop, Venice, Italy.Bos, Johan and Katja Markert.
2005.
Recognisingtextual entailment with logical inference.
In Pro-ceedings of Human Language Technology Confer-ence and Conference on Empirical Methods in Nat-ural Language Processing, pages 628?635, Vancou-ver, British Columbia, Canada, October.
Associationfor Computational Linguistics.Collins, Michael and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: Kernels overdiscrete structures, and the voted perceptron.
In Pro-ceedings of ACL02.Corley, Courtney and Rada Mihalcea.
2005.
Measur-ing the semantic similarity of texts.
In Proc.
of theACL Workshop on Empirical Modeling of SemanticEquivalence and Entailment, pages 13?18, Ann Ar-bor, Michigan, June.
Association for ComputationalLinguistics.Dagan, Ido, Oren Glickman, and Bernardo Magnini.2005.
The PASCAL RTE challenge.
In PASCALChallenges Workshop, Southampton, U.K.de Salvo Braz, R., R. Girju, V. Punyakanok, D. Roth,and M. Sammons.
2005.
An inference model for se-mantic entailment in natural language.
In Proceed-ings of AAAI, pages 1678?1679.Giuglea, Ana-Maria and Alessandro Moschitti.
2006.Semantic role labeling via framenet, verbnet andpropbank.
In Proceedings of Coling-ACL, Sydney,Australia.Glickman, Oren and Ido Dagan.
2004.
Probabilistictextual entailment: Generic applied modeling of lan-guage variability.
In Proceedings of the Workshop onLearning Methods for Text Understanding and Min-ing, Grenoble, France.Glickman, Oren, Ido Dagan, and Moshe Koppel.
2005.Web based probabilistic textual entailment.
In Pro-ceedings of the 1st Pascal Challenge Workshop,Southampton, UK.31Haasdonk, Bernard.
2005.
Feature space interpretationof SVMs with indefinite kernels.
IEEE Trans Pat-tern Anal Mach Intell, 27(4):482?92, Apr.Joachims, Thorsten.
1999.
Making large-scale svmlearning practical.
In Schlkopf, B., C. Burges, andA.
Smola, editors, Advances in Kernel Methods-Support Vector Learning.
MIT Press.Miller, George A.
1995.
WordNet: A lexicaldatabase for English.
Communications of the ACM,38(11):39?41, November.Moschitti, Alessandro and Cosmin Adrian Bejan.2004.
A semantic kernel for predicate argumentclassification.
In CoNLL-2004, USA.Moschitti, A. and F. Zanzotto.
2007.
Fast and effectivekernels for relational learning from texts.
In Ghahra-mani, Zoubin, editor, Proceedings of the 24th An-nual International Conference on Machine Learning(ICML 2007).Moschitti, Alessandro, Daniele Pighin, and RobertoBasili.
2006.
Semantic Role Labeling via Tree Ker-nel Joint Inference.
In Proceedings of CoNLL-X.Moschitti, Alessandro, Silvia Quarteroni, RobertoBasili, and Suresh Manandhar.
2007.
Exploitingsyntactic and shallow semantic kernels for questionanswer classification.
In Proceedings ACL, Prague,Czech Republic.Moschitti, Alessandro.
2006.
Efficient convolutionkernels for dependency and constituent syntactictrees.
In ECML?06.Zanzotto, Fabio Massimo and Alessandro Moschitti.2006.
Automatic learning of textual entailmentswith cross-pair similarities.
In Proceedings of the21st Coling and 44th ACL, pages 401?408, Sydney,Australia, July.32
