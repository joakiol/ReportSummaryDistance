Cluster-Based Query Expansion for Statistical Question AnsweringLucian Vlad Lita ?Siemens Medical Solutionslucian.lita@siemens.comJaime CarbonellCarnegie Mellon Universityjgc@cs.cmu.eduAbstractDocument retrieval is a critical componentof question answering (QA), yet little workhas been done towards statistical modelingof queries and towards automatic generationof high quality query content for QA.
Thispaper introduces a new, cluster-based queryexpansion method that learns queries knownto be successful when applied to similarquestions.
We show that cluster-based ex-pansion improves the retrieval performanceof a statistical question answering systemwhen used in addition to existing query ex-pansion methods.
This paper presents exper-iments with several feature selection meth-ods used individually and in combination.We show that documents retrieved using thecluster-based approach are inherently differ-ent than documents retrieved using existingmethods and provide a higher data diversityto answers extractors.1 IntroductionInformation retrieval has received sporadic exam-ination in the context of question answering (QA).Over the past several years, research efforts have in-vestigated retrieval quality in very controlled scenar-ios under the question answering task.
At a firstglance, document and passage retrieval is reason-able when considering the fact that its performanceis often above 80% for this stage in the questionanswering process.
However, most often, perfor-mance is measured in terms of the presence of at?
work done at Carnegie Mellonleast one relevant document in the retrieved docu-ment set, regardless of relevant document density ?where a document is relevant if it contains at leastone correct answer.
More specifically, the retrievalstage is considered successful even if there is a sin-gle document retrieved that mentions a correct an-swer, regardless of context.
This performance mea-sure is usually not realistic and revealing in questionanswering.In typical scenarios, information extraction is notalways able to identify correct answers in free text.When successfully found, correct answers are notalways assigned sufficiently high confidence scoresto ensure their high ranks in the final answer set.As a result, overall question answering scores arestill suffering and considerable effort is being di-rected towards improving answer extraction and an-swer merging, yet little attention is being directedtowards retrieval.A closer look at retrieval in QA shows that thetypes of documents retrieved are not always con-ducive to correct answers given existing extractionmethods.
It is not sufficient to retrieve a relevantdocument if the answer is difficult to extract from itscontext.
Moreover, the retrieval techniques are oftenvery simple, consisting of extracting keywords fromquestions, expanding them using conventional meth-ods such as synonym expansion and inflectional ex-pansion, and then running the queries through a re-trieval engine.In order to improve overall question answeringperformance, additional documents and better doc-uments need to be retrieved.
More explicitly, infor-mation retrieval needs to: a) generate query typesand query content that is designed to be successful(high precision) for individual questions and b) en-426sure that the documents retrieved by the new queriesare different than the documents retrieved using con-ventional methods.
By improving retrieval alongthese dimensions, we provide QA systems with ad-ditional new documents, increasing the diversity andthe likelihood of extracting correct answers.
In thispaper, we present a cluster-based method for ex-panding queries with new content learned from theprocess of answering similar questions.
The newqueries are very different from existing content sincethey are not based on the question being answered,but on content learned from other questions.1.1 Related WorkExperiments using the CMU Javelin (Collins-Thompson et al, 2004) and Waterloo?s MultiText(Clarke et al, 2002) question answering systemscorroborate the expected direct correlation betweenimproved document retrieval performance and QAaccuracy across systems.
Effectiveness of the re-trieval component was measured using question cov-erage ?
number of questions with at least one rele-vant document retrieved ?
and mean average preci-sion.
Results suggest that retrieval methods adaptedfor question answering which include question anal-ysis performed better than ad-hoc IR methods whichsupports previous findings (Monz, 2003).In question answering, queries are often ambigu-ous since they are directly derived from the ques-tion keywords.
Such query ambiguity has been ad-dressed in previous research (Raghavan and Allan,2002) by extracting part of speech patterns and con-structing clarification queries.
Patterns are mappedinto manually generated clarification questions andpresented to the user.
The results using the clarity(Croft et al, 2001) statistical measure suggest thatquery ambiguity is often reduced by using clarifica-tion queries which produce a focused set of docu-ments.Another research direction that tailors the IR com-ponent to question answering systems focuses onquery formulation and query expansion (Woods etal., 2001).
Taxonomic conceptual indexing systembased on morphological, syntactic, and semanticfeatures can be used to expand queries with inflectedforms, hypernyms, and semantically related terms.In subsequent research (Bilotti et al, 2004), stem-ming is compared to query expansion using inflec-tional variants.
On a particular question answeringcontrolled dataset, results show that expansion us-ing inflectional variants produces higher recall thanstemming.Recently (Riezler et al, 2007) used statistical ma-chine translation for query expansion and took a steptowards bridging the lexical gap between questionsand answers.
In (Terra et al, 2005) query expansionis studied using lexical affinities with different queryformulation strategies for passage retrieval.
Whenevaluated on TREC datasets, the affinity replace-ment method obtained significant improvements inprecision, but did not outperform other methods interms of recall.2 Cluster-Based Retrieval for QAIn order to explore retrieval under question answer-ing, we employ a statistical system (SQA) thatachieves good factoid performance on the TRECQA task: for ?
50% of the questions a correct an-swer is in the top highest confidence answer.
Ratherthan manually defining a complete answering strat-egy ?
the type of question, the queries to be run, theanswer extraction, and the answer merging meth-ods ?
for each type of question, SQA learns dif-ferent strategies for different types of similar ques-tions SQA takes advantage of similarity in trainingdata (questions and answers from past TREC evalua-tions), and performs question clustering.
Two meth-ods are employed constraint-based clustering andEM with similar performance.
The features usedby SQA clustering are surface-form n-grams as wellas part of speech n-grams extracted from questions.However, any clustering method can be employed inconjunction with the methods presented in this pa-per.The questions in each cluster are similar in somerespect (i.e.
surface form and syntax), SQA usesthem to learn a complete answering strategy.
Foreach cluster of training questions, SQA learns an an-swering strategy.
New questions may fall in morethan one cluster, so multiple answering strategies at-tempt simultaneously to answer it.In this paper we do not cover a particular ques-tion answering system such as SQA and we do notexamine the whole QA process.
We instead focuson improving retrieval performance using a set of427similar questions.
The methods presented here cangeneralize when similar training questions are avail-able.
Since in our experiments we employ a cluster-based QA system, we use individual clusters of simi-lar questions as local training data for learning betterqueries.2.1 Expansion Using Individual QuestionsMost existing question answering systems use IR ina simple, straight-forward fashion: query terms areextracted online from the test question and used toconstruct basic queries.
These queries are then ex-panded from the original keyword set using statisti-cal methods, semantic, and morphological process-ing.
Using these enhanced queries, documents (orpassages) are retrieved and the top K are furtherprocessed.
This approach describes the traditionalIR task and does not take advantage of specific con-straints, requirements, and rich context available inthe QA process.
Pseudo-relevance feedback is oftenused in question answering in order to improve thechances of retrieving relevant documents.
In web-based QA, often systems rely on retrieval enginesto perform the keyword expansion.
Some questionanswering systems associate additional predefinedstructure or content based on the question classifi-cation.
However, there this query enhancement pro-cess is static and does not use the training data andthe question answering context differently for indi-vidual questions.Typical question answering queries used in docu-ment or passage retrieval are constructed using mor-phological and semantic variations of the contentwords in the question.
However, these expandedqueries do not benefit from the underlying structureof the question, nor do they benefit from availabletraining data, which provides similar questions thatwe already know how to answer.2.2 Expansion Based on Similar QuestionsWe introduce cluster-based query expansion(CBQE), a new task-oriented method for query ex-pansion that is complementary to existing strategiesand that leads to different documents which containcorrect answers.
Our approach goes beyond singlequestion-based methods and takes advantage ofhigh-level correlations that appear in the retrievalprocess for similar questions.The central idea is to cluster available trainingquestions and their known correct answers in or-der to exploit the commonalities in the retrieval pro-cess.
From each cluster of similar questions welearn a different, shared query content that is usedin retrieving relevant documents - documents thatcontain correct answers.
This method leveragesthe fact that answers to similar questions tend toshare contextual features that can be used to enhancekeyword-based queries.
Experiments with questionanswering data show that our expanded queries in-clude a different type of content compared to andin addition to existing methods.
These queries havetraining question clusters as a source for expansionrather than an individual test question.
We show thatCBQE is conducive to the retrieval of relevant doc-uments, different than the documents that can be re-trieved using existing methods.We take advantage of the fact that for similartraining questions, good IR queries are likely toshare structure and content features.
Such featurescan be learned from training data and can then beapplied to new similar questions.
Note that some ofthese features cannot be generated through simplequery expansion, which does not takes advantage ofsuccessful queries for training questions.
Featuresthat generate the best performing queries across anentire cluster are then included in a cluster-specificfeature set, which we will refer to as the query con-tent model.While pseudo-relevance feedback is performedon-line for each test question, cluster-based rel-evance feedback is performed across all trainingquestions in each individual cluster.
Relevance feed-back is possible for training data, since correct an-swers are already known and therefore documentrelevance can be automatically and accurately as-sessed.Algorithm 1 shows how to learn a query contentmodel for each individual cluster, in particular: howto generate queries enhanced with cluster-specificcontent, how to select the best performing queries,and how to construct the query content model to beused on-line.Initially, simple keyword-based queries are for-mulated using words and phrases extracted directlyfrom the free question keywords that do not appearin the cluster definition.
The keyword queries are428Algorithm 1 Cluster-based relevance feedback algorithm forretrieval in question answering1: extract keywords from training questions in a cluster andbuild keyword-based queries; apply traditional query ex-pansion methods2: for all keyword-based query do3: retrieve an initial set of documents4: end for5: classify documents into relevant and non-relevant6: select top k most discriminative features (e.g.
n-grams,paraphrases) from retrieved documents (across all trainingquestions).7: use the top k selected features to enhance keyword-basedqueries ?
adding one feature at a time (k new queries)8: for all enhanced queries do9: retrieve a second set of documents10: end for11: classify documents into relevant and non-relevant based12: score enhanced queries according to relevant documentdensity13: include in the query content model the top h features whosecorresponding enhanced queries performed best across alltraining questions in the cluster ?
up to 20 queries in ourimplementationthen subjected to frequently used forms of query ex-pansion such as inflectional variant expansion andsemantic expansion (table ??).
Further process-ing depends on the available and desired process-ing tools and may generate variations of the origi-nal queries: morphological analysis, part of speechtagging, syntactic parsing.
Synonym and hypernymexpansion and corpus-based techniques can be em-ployed as part of the query expansion process, whichhas been extensively studied (Bilotti et al, 2004).The cluster-based query expansion has the advan-tage of being orthogonal to traditional query expan-sion and can be used in addition to pseudo-relevancefeedback.
CBQE is based on context shared by sim-ilar training questions in each cluster, rather than onindividual question keywords.
Since cluster-basedexpansion relies on different features compared totraditional expansion, it leads to new relevant doc-uments, different from the ones retrieved using theexisting expansion techniques.3 The Query Content ModelSimple queries are run through a retrieval engine inorder to produce a set of potentially relevant docu-ments.
While this step may produce relevant doc-uments, we would like to construct more focusedqueries, likely to retrieve documents with correct an-swers and appropriate contexts.
The goal is to addquery content that increases retrieval performanceon training questions.
Towards this end, we evaluatethe discriminative power of features (n-grams andparaphrases), and select the ones positively corre-lated with relevant documents and negatively corre-lated with non-relevant documents.
The goal of thisapproach is to retrieve documents containing simple,high precision answer extraction patterns.
FeaturesCluster: When did X start working for Y?Simple Queries Query Content ModelX, Y ?X joined Y in?X, Y, start, working ?X started working for Y?X, Y, ?start working?
?X was hired by Y?X, Y, working ?Y hired X?.
.
.
X, Y, ?job interview?.
.
.Table 1: Sample cluster-based expansion featuresthat best discriminate passages containing correctanswers from those that do not, are selected aspotential candidates for enhancing keyword-basedqueries.
For each question-answer pair, we gener-ate enhanced queries by individually adding selectedfeatures (e.g.
Table 1) to simple queries.
The result-ing queries are subsequently run through a retrievalengine and scored using the measure of choice (e.g.average precision).
The content features used toconstruct the top h features and corresponding en-hanced queries are included in the query contentmodel.The query content model is a collection of fea-tures used to enhance the content of queries whichare successful across a range of similar questions(Table 1).
The collection is cluster specific and notquestion specific - i.e.
features are derived fromtraining data and enhanced queries are scored us-ing training question answer pairs.
Building a querycontent model does not preclude traditional queryexpansion.
Through the query content model we al-low shared context to play a more significant role inquery generation.4 Experiments With Cluster-BasedRetrievalWe tested the performance of cluster-based con-tent enhanced queries and compared it to the per-429formance of simple keyword-based queries and tothe performance of queries expanded through syn-onyms and inflectional variants.
We also experimentwith several feature selection methods for identify-ing content features conducive to successful queries.These experiments were performed with a web-based QA system which uses the Google API fordocument retrieval and a constraint-based approachfor question clustering.
Using this system weretrieved ?300, 000 and built a document set of?10GB.
For each new question, we identify train-ing questions that share a minimum surface struc-ture (e.g.
a size 3 skip-ngram in common) whichwe consider to be the prototype of a loose cluster.Each cluster represents a different, implicit notion ofquestion similarity based on the set of training ques-tions it covers.
Therefore different clusters lead todifferent retrieval strategies.
These retrieval experi-ments are restricted to using only clusters of size 4 orhigher to ensure sufficient training data for learningqueries from individual clusters.
All experimentswere performed using leave-one-out cross valida-tion.For evaluating the entire statistical question an-swering system, we used all questions from TREC8-12.
One of the well-known problems in QA consistsof questions having several unknown correct an-swers with multiple answer forms ?
different waysof expressing the same answer.
Since we are lim-ited to a set of answer keys, we avoid the this prob-lem by using all temporal questions from this datasetfor evaluating individual stages in the QA process(i.e.
retrieval) and for comparing different expan-sion methods.
These questions have the advantageof having a more restrictive set of possible answersurface forms, which lead to a more accurate mea-sure of retrieval performance.
At the same time theycover both more difficult questions such as ?Whenwas General Manuel Noriega ousted as the leaderof Panama and turned over to U.S.
authorities?
?as well as simpler questions such as ?What yeardid Montana become a state??.
We employed thisdataset for an in-depth analysis of retrieval perfor-mance.We generated four sets of queries and we testedtheir performance.
We are interested in observ-ing to what extent different methods produce addi-tional relevant documents.
The initial set of queriesare constructed by simply using a bag-of-words ap-proach on the question keywords.
These queriesare run through the retrieval engine, each generating100 documents.
The second set of queries builds onthe first set, expanding them using synonyms.
Eachword and potential phrase is expanded using syn-onyms extracted from WordNet synsets.
For eachenhanced query generated, 100 documents are re-trieved.
To construct the third set of queries, we ex-pand the queries in the first two sets using inflec-tional variants of all the content words (e.g.
verbconjugations and noun pluralization (Bilotti et al,2004)).
For each of these queries we also retrieve100 documents.When text corpora are indexed without usingstemming, simple queries are expanded to includemorphological variations of keywords to improve re-trieval and extraction performance.
Inflectional vari-ants include different pluralizations for nouns (e.g.report, reports) and different conjugations for verbs(e.g.
imagine, imagines, imagined, imagining).
Un-der local corpus retrieval inflectional expansion by-passes the unrelated term conflation problem thatstemmers tend to have, but at the same time, recallmight be lowered if not all related words with thesame root are considered.
For a web-based questionanswering system, the type of retrieval depends onthe search-engine assumptions, permissible querystructure, query size limitation, and search enginebandwidth (allowable volume of queries per time).By using inflectional expansion with queries that tar-get web search engines, the redundancy for support-ing different word variants is higher, and has thepotential to increase answer extraction performance.Finally, in addition to the previous expansion meth-ods, we employ our cluster-based query expansionmethod.
These queries incorporate the top mostdiscriminative ngrams and paraphrases (section 4.1)learned from the training questions covered by thesame cluster.
Instead of further building an expan-sion using the original question keywords, we ex-pand using contextual features that co-occur withanswers in free text.
For all the training ques-tions in a cluster, we gather statistics about the co-occurrence of answers and potentially beneficial fea-tures.
These statistics are then used to select the bestfeatures and apply them to new questions whose an-swers are unknown.
Figure 1 shows that approx-430Figure 1: Cumulative effect of expansion methodsimately 90% of the questions consistently benefitfrom cluster-based query expansion when comparedto approximately 75% of the questions when em-ploying the other methods combined.
Each questioncan be found in multiple clusters of different reso-lution.
Since different clusters may lead to differ-ent selected features, questions benefit from multi-ple strategies and even though one cluster-specificstrategy cannot produce relevant documents, othercluster-specific strategies may be able to.The cluster-based expansion method can generatea large number of contextual features.
When com-paring feature selection methods, we only select thetop 10 features from each method and use them toenhance existing question-based queries.
Further-more, in order to retrieve, process, extract, and scorea manageable number of documents, we limited theretrieval to 10 documents for each query.
In Fig-ure 1 we observe that even as the other methodsretrieve more documents, ?
90% of the questionsstill benefit from the cluster-based method.
In otherwords, the cluster-based method generates queriesusing a different type of content and in turn, thesequeries retrieve a different set documents than theother methods.
This observation is true even if wecontinue to retrieve up to 100 documents for sim-ple queries, synonym-expanded queries, and inflec-tional variants-expanded queries.This result is very encouraging since it suggeststhat the answer extraction components of ques-tion answering systems are exposed to a differenttype of relevant documents, previously inaccessibleto them.
Through these new relevant documents,cluster-based query expansion has the potential toprovide answer extraction with richer and more var-ied sources of correct answers for 90% of the ques-tions.new relevant documentssimple 4.43 100%synonyms 1.48 33.4%inflect 2.37 53.43%cluster 1.05 23.65%all 9.33 210.45%all - synonyms 7.88 177.69%all - inflect 6.99 157.69%all - cluster 8.28 186.80%Table 2: Keyword-based (?simple?
), synonym, inflectionalvariant, and cluster-based expansion.
Average number of newrelevant documents across instances at 20 documents retrieved.Although expansion methods generate additionalrelevant documents that simpler methods cannot ob-tain, an important metric to consider is the den-sity of these new relevant documents.
We are in-terested in the number/percentage of new relevantdocuments that expansion methods contribute with.Table 2 shows at retrieval level of twenty docu-ments how different query generation methods per-form.
We consider keyword based methods to be thebaseline and add synonym expanded queries (?syn-onym?
), inflectional variants expanded queries (?in-flect?)
which build upon the previous two types ofqueries, and finally the cluster enhanced queries(?cluster?)
which contain features learned from train-ing data.
We see that inflectional variants havethe most impact on the number of new documentsadded, although synonym expansion and cluster-based expansion also contribute significantly.4.1 Feature Selection for CBQEContent features are learned from the training databased on observing their co-occurrences with cor-rect answers.
In order to find the most appropri-ate content features to enhance our cluster-specificqueries, we have experimented with several featureselection methods (Yang and Pederson, 1997): in-formation gain, chi-square, and scaled chi-square(phi).
Information gain (IG) measures the reductionin entropy for the pre presence/absence of an answerin relevant passages, given an n-gram feature.
Chi-square (?2) is a non-parametric measure of associa-431tion that quantifies the passage-level association be-tween n-gram features and correct answers.Given any of the above methods, individual n-gram scores are combined at the cluster level by av-eraging over individual questions in the cluster.
Infigure 2 we compare these feature selection meth-ods on our dataset.
The selected features are used toenhance queries and retrieve additional documents.We measure the fraction of question instances forwhich enhanced queries obtain at least one new rel-evant document.
The comparison is made with thedocument set generated by keyword-based queries,synonym expansion, and inflectional variant expan-sion.
We also include in our comparison the com-bination of all feature selection methods (?All?).
In0 20 40 60 80 1000.50.550.60.650.70.75Instances With Additional Relevant Documents#docs retrievedfractionofinstancesAllPrecIGainPhiChi2Figure 2: Selection methods for cluster-based expansionthis experiment, average precision on training dataproves to be the best predictor of additional relevantdocuments: ?71% of the test questions benefit fromqueries based on average precision feature selection.However, the other feature selection methods alsoobtain a high performance, benefiting ?68% of thetest question instances.Since these feature selection methods have differ-ent biases, we expect to observe a boost in perfor-mance (73%) from merging their feature sets (Fig-ure 2).
In this case there is a trade-off betweena 2% boost in performance and an almost doubleset of features and enhanced queries.
This trans-lates into more queries and more documents to beprocessed.
Although it is not the focus of this re-search, we note that a clever implementation couldincrementally add features from the next best selec-tion method only after the existing queries and doc-uments have been processed.
This approach lendsitself to be a good basis for utility-based modelsand planning (Hiyakumoto et al, 2005).
We in-0.3 0.4 0.5 0.6 0.70.40.50.60.7Cluster Enhanced Queriesfeature selection score (train)averageprecision(retrieval)Precision at   1Precision at   5Precision at 10Figure 3: Average precision of cluster enhanced queriesvestigate to what extent the scores of the selectedfeatures are meaningful and correlate with actual re-trieval performance on test data by measuring theaverage precision of these queries at different num-ber of documents retrieved.
Figure 3 shows preci-sion at one, five, and ten documents retrieved.
Weobserve that feature scores correlate well with ac-tual retrieval performance, a result confirmed by allthree retrieval levels, suggesting that useful featureslearned.
The average precision also increases withmore documents retrieved, which is a desirable qual-ity in question answering.4.2 Qualitative ResultsThe cluster-based relevance feedback process can beused to discover several artifacts useful in questionanswering.
For several of the clusters, we observethat the feature selection process consistently andwith high confidence selected features such as ?nounNP1 has one meaning?
where NP1 is the first nounphrase in the question.
The goal is to add such fea-tures to the keyword-based queries to retrieve highprecision documents.
Note that our example, NP1would be different for different test questions.The indirect reason for selecting such features isin fact the discovery of authorities: websites that fol-low a particular format and which have a particulartype of information, relevant to a cluster.
In the ex-ample above, the websites answers.com and word-net.princeton.edu consistently included answers toclusters relevant to a person?s biography.
Simi-larly, wikipedia.org often provides answers to def-initional questions (e.g.
?what is uzo??).
By includ-432ing non-intuitive phrases, the expansion ensures thatthe query will retrieve documents from a particularauthoritative source ?
during feature selection, theseauthorities supplied high precision documents for alltraining questions in a particular cluster, hence fea-tures specific to these sources were identified.Q: When did Bob Marley die?
[A: answers.com]The noun Bob Marley has one meaning:Jamaican singer who popularized reggae (1945-81)Born: 6 February 1945Birthplace: St. Ann?s Parish, JamaicaDied: 11 May 1981 (cancer)Songs: Get Up, Stand Up, Redemption Song .
.
.In this example, profiles for many entities men-tioned in a question cluster were found on severalauthority websites.
Due to unlikely expansions suchas ?noun Bob Marley has one meaning?
the entity?Bob Marley?, the answer to the question ?Whendid Bob Marley die??
can easily be found.
In fact,this observation has the potential to lead to a cluster-based authority discovery method, in which certainsources are given more credibility and are used morefrequently than others.
For example, by observingthat for most questions in a cluster, the wikipedia sitecovers at least one correct answer (ideally that canactually be extracted), then it should be considered(accessed) for test questions before other sources ofdocuments.
Through this process, given a set ofquestions processed using the IBQA approach, a setof authority answer sources can be identified.5 Conclusions & Future WorkWe presented a new, cluster-based query expansionmethod that learns query content which is success-fully used in answering other similar questions.
Tra-ditional QA query expansion is based only on theindividual keywords in a question.
In contrast, thecluster-based expansion learns features from contextshared by similar training questions from a cluster.Since the features of cluster-based expansion aredifferent from the features used in traditional queryexpansion, they lead to new relevant documents thatare different from documents retrieved using exist-ing expansion techniques.
Our experiments showthat more than 90% of the questions benefit fromour cluster-based method when used in addition totraditional expansion methods.Retrieval in local corpora offers more flexibilityin terms of query structure and expressivity.
Thecluster-based method can be extended to take advan-tage of structure in addition to content.
More specif-ically, different query structures could benefit differ-ent types of questions.
However, learning structuremight require more training questions for each clus-ter.
Further research can also be done to improvethe methods of combining learned content into morerobust and generalizable queries.
Finally we are in-terested modifying our cluster-based expansion forthe purpose of automatically identifying authoritysources for different types of questions.ReferencesM.
W. Bilotti, B. Katz, and J. Lin.
2004.
What worksbetter for question answering: Stemming or morpho-logical query expansion?
In IR4QA, SIGIR Workshop.C.
Clarke, G. Cormack, G. Kemkes, M. Laszlo, T. Ly-nam, E. Terra, and P. Tilker.
2002.
Statistical selectionof exact answers.K.
Collins-Thompson, E. Terra, J. Callan, and C. Clarke.2004.
The effect of document retrieval quality on fac-toid question-answering performance.W.B.
Croft, S. Cronen-Townsend, and V. Lavrenko.2001.
Relevance feedback and personalization: A lan-guage modeling perspective.
In DELOS-NSF Work-shop on Personalization and Recommender Systems inDigital Libraries.L.
Hiyakumoto, L.V.
Lita, and E. Nyberg.
2005.
Multi-strategy information extraction for question answer-ing.C.
Monz.
2003.
From document retrieval to questionanswering.
In Ph.
D. Dissertation, Universiteit VanAmsterdam.H.
Raghavan and J. Allan.
2002.
Using part-of-speechpatterns to reduce query ambiguity.S.
Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, andY.
Liu.
2007.
Statistical machine translation for queryexpansion in answer retrieval.
In ACL.E.
Terra, C.L., and A. Clarke.
2005.
Comparing queryformulation and lexical affinity replacements in pas-sage retrieval.
In ELECTRA, SIGIR Workshop.W.A.
Woods, S.J.
Green, P. Martin, and A. Houston.2001.
Aggressive morphology and lexical relations forquery expansion.Y.
Yang and J. Pederson.
1997.
Feature selection in sta-tistical learning of text categorizatio n.433
