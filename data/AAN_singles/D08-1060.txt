Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 572?581,Honolulu, October 2008. c?2008 Association for Computational LinguisticsGeneralizing Local and Non-Local Word-Reordering Patterns forSyntax-Based Machine TranslationBing ZhaoIBM T.J. Watson ResearchYorktown Heights, NY-10598zhaob@us.ibm.comYaser Al-onaizanIBM T.J. Watson ResearchYorktown Heights, NY-10598onaizan@us.ibm.comAbstractSyntactic word reordering is essential fortranslations across different grammar struc-tures between syntactically distant language-pairs.
In this paper, we propose to em-bed local and non-local word reordering de-cisions in a synchronous context free gram-mar, and leverages the grammar in a chart-based decoder.
Local word-reordering is ef-fectively encoded in Hiero-like rules; whereasnon-local word-reordering, which allows forlong-range movements of syntactic chunks,is represented in tree-based reordering rules,which contain variables correspond to source-side syntactic constituents.
We demonstratehow these rules are learned from parallel cor-pora.
Our proposed shallow Tree-to-Stringrules show significant improvements in trans-lation quality across different test sets.1 IntroductionOne of the main issues that a translator (human ormachine) must address during the translation pro-cess is how to match the different word orders be-tween the source language and the target language.Different language-pairs require different levels ofword reordering.
For example, when we translatebetween English and Spanish (or other Romancelanguages), most of the word reordering neededis local because of the shared syntactical features(e.g., Spanish noun modifier constructs are writtenin English as modifier noun).
However, for syn-tactically distant language-pairs such as Chinese-English, long-range reordering is required wherewhole phrases are moved across the sentence.The idea of ?syntactic cohesion?
(Fox, 2002) ischaracterized by its simplicity, which has attractedresearchers for years.
Previous works include sev-eral approaches of incorporating syntactic informa-tion to preprocess the source sentences to make themmore like the target language in structure.
Xia andMcCord (2004) (Niessen and Ney, 2004; Collins etal., 2005) described approaches applied to language-pairs such as French-English and German-English.Later, Wang et al (2007) presented specific rulesto pre-order long-range movements of words, andimproved the translations for Chinese-to-English.Overall, these works are similar, in that they designa few language-specific and linguistically motivatedreordering rules, which are generally simple.
Theeleven rules described in Wang et al (2007) are ap-pealing, as they have rather simple structure, mod-eling only NP, VP and LCP via one-level sub-treestructure with two children, in the source parse-tree(a special case of ITG (Wu, 1997)).
It effectively en-hances the quality of the phrase-based translation ofChinese-to-English.
One major weakness is that thereordering decisions were done in the preprocessingstep, therefore rendering the decoding process un-able to recover the reordering errors from the rules ifincorrectly applied to.
Also the reordering decisionsare made without the benefits of additional models(e.g., the language models) that are typically usedduring decoding.Another method to address the re-ordering prob-lem in translation is the Hiero model proposed byChiang (2005), in which a probabilistic synchronouscontext free grammar (PSCFG) was applied to guidethe decoding.
Hiero rules generalize phrase-pairs572by introducing a single generic nonterminal (i.e., avariable) [X].
The combination of variables and lex-icalized words in a Hiero rule nicely captures localword and phrase reordering (modeling an implicitreordering window of max-phrase length).
Theserules are then applied in a CYK-style decoder.
InHiero rules, any nested phrase-pair can be general-ized as variables [X].
This usually leads to too manyredundant translations, which worsens the spuriousambiguities (Chiang, 2005) problems for both de-coding and optimization (i.e., parameter tuning).
Wefound thatvariables (nonterminal [X]) in Hiero rulesoffer a generalization too coarse to improve the ef-fectiveness of hierarchical models?
performance.We propose to enrich the variables in Hiero ruleswith additional source syntactic reordering informa-tion, in the form of shallow Tree-to-String syntacticstructures.
The syntactic information is representedby flat one-level sub-tree structures, with Hiero-likenonterminal variables at the leaf nodes.
The syntac-tic rules, proposed in this paper, are composed of(possibly lexicalized) source treelets and target sur-face strings, with one or more variables that helpcapture local-reordering similar to the Hiero rules.Variables in a given rule are derived not only fromthe embedded aligned blocks (phrase-pairs), but alsofrom the aligned source syntactic constituents.
Thealigned constituents, as in our empirical observa-tions for Chinese-English, tend to move together intranslations.
The decoder is guided by these rules toreduce spurious derivations; the rules also constrainthe exploration of the search space toward bettertranslation quality and sometime improved speed bybreaking long sentences into pieces.
Overall, whatwe want is to enable the long-range reordering deci-sions to be local in a chart-based decoder.To be more specific, we think the simple shal-low syntactic structure is powerful enough for cap-turing the major structure-reordering patterns, suchas NP, VP and LCP structures.
We also use sim-ple frequency-based feature functions, similar to theblocks used in phrase-based decoder, to further im-prove the rules?
representation power.
Overall, thisenables us to avoid either a complex decoding pro-cess to generate the source parse tree, or difficultcombinatorial optimizations for the feature func-tions associated with rules.In Marton and Resnik (2008), hiero variableswere disambiguated with additional binary featurefunctions, with their weights optimized in standardMER training.
The combinatorial effects of theadded feature functions can make the feature se-lection and optimization of the weights rather dif-ficult.
Since the grammar is essentially the sameas the Hiero ones, a standard CYK decoder can besimply applied in their work.
Word reordering canalso be addressed via distortion models.
Work in(Al-Onaizan and Kishore, 2006; Xiong et al, 2006;Zens et al, 2004; Kumar and Byrne, 2005; Tillmannand Zhang, 2005) modeled the limited informationavailable at phrase-boundaries.
Syntax-based ap-proaches such as (Yamada and Knight, 2001; Graehland Knight, 2004; Liu et al, 2006) heavily rely onthe parse-tree to constrain the search space by as-suming a strong mapping of structures across distantlanguage-pairs.
Their algorithms are also subject toparsers?
performances to a larger extent, and havehigh complexity and less scalability in reality.
In Liuet al (2007), multi-level tree-structured rules weredesigned, which made the decoding process verycomplex, and auxiliary rules have to be designedand incorporated to shrink multiple source nonter-minals into one target nonterminal.
From our em-pirical observations, most of the time, however, themulti-level tree-structure is broken in the translationprocess, and POS tags are frequently distorted.
In-deed, strictly following the source parse tree is usu-ally not necessary, and maybe too expensive for thetranslation process.The remainder of this paper is structured as fol-lows: in section ?
2, we define the notations in oursynchronous context free grammar, in section ?
3,the rule extractions are illustrated in details, in sec-tion ?
4, the decoding process of applying these rulesis described.
Experiments in ?
5 were carried outusing GALE Dev07 datasets.
Improved translationqualities were obtained by applying the proposedTree-to-String rules.
Conclusions and discussionsare given in ?
6.2 Shallow Tree-to-String RulesOur proposed rules are in the form of probabilis-tic synchronous context free grammar (PSCFG).
Weadopt the notations used in (Chiang, 2005).
Let Nbe a set of nonterminals, a rule has the following573form:X ?< `; ?;?;?
; w?
>, (1)where X abstracts nonterminal symbols in N ; ?
?
[N,VS ]+ is a sequence of one or more source 1words (as in the vocabulary of VS) and nonterminalsymbols in N ; ?
?
[N,VT ]+ is a sequence of oneor more target words (in VT ) and nonterminals in N. ?
is the one-to-one alignment of the nonterminalsbetween ?
and ?
; w?
contains non-negative weightsassociated with each rule; ` is a label-symbol speci-fying the root node of the source span covering ?.
Inour grammar, ` is one of the labels (e.g., NP) definedin the source treebank tagset (in our case UPennChinese tagset) indicating that the source span ?
isrooted at `.
Additionally, a NULL tag ?
in ` denotesa flat structure of ?, in which no constituent structurewas found to cover the span, and we need to backoff to the normal Hiero-style rules.
Our nonterminalsymbols include the labels and the POS tags in thesource parse trees.In the following, we will illustrate the Tree-to-String rules we are proposing.
At the same time, wewill describe the extraction algorithm, with whichwe derive our rules from the word-aligned source-parsed parallel text.
Our nonterminal set N is a re-duced set of the treebank tagset (Xue et al, 2005).
Itconsists of 17 unique labels.The rules we extract belong to one of the follow-ing categories:?
?
contains only words, and ` is NULL; this cor-responds to the general blocks used in phrase-based decoder (Och and Ney, 2004);?
?
contains words and variables of [X,0] and[X,1], and ` is NULL; this corresponds to theHiero rules as in Chiang (2005);?
?
contains words and variables in the formof [X,TAG2], in which TAG is from the LDCtagset; this defines a well formed subtree, inwhich at least one child (constituent) is alignedto continuous target ngrams.
If ?
contains onlyvariables from LDC tag set, this indicates allthe constituents (children) in the subtree arealigned.
This is a superset of rules generalizing1we use end-user terminologies for source and target.2we index the tags for multiple occurrences in one rulethose in Wang et al (2007).
If ?
contains vari-ables from POS tags, this essentially producesa superset of the monolingual side POS-basedreordering rules explored in Tillmann (2008).We focus on the third category ?
a syntactic label` over the span of ?, indicating the covered sourcewords consist of a linguistically well-defined phrase.` together with ?
define a tree-like structure: the rootnode is `, and the aligned children are nonterminalsin ?.
The structure information is encoded in (`,?)
pair-wise connections, and the variables keep thegeneralizations over atomic translation-pairs similarto Hiero models.
When the rule is applied duringdecoding time, the labels, the tree-structure and thelexical items need to be all matched.3 Learning and Applying RulesA parser is assumed for the source language in theparallel data.
In our case, a Chinese parser is appliedfor training and test data.
A word alignment model isused to align the source words with the target words.3.1 ExtractionsOur rule extraction is a three-step process.
First, tra-ditional blocks (phrase-pairs) extraction is carriedout.
Secondly, Tree-to-String rules, are then ex-tracted from the aligned blocks, of which the sourceside is covered by a complete subtree, with differentpermutations of the embedded aligned constituents,or partially lexicalized constituents.
Otherwise, theHiero-like rules will be extracted when there is nosub-tree structure identified, in our final step.
Fre-quencies of extracted rules were counted to computefeature functions.Figure 1-(a) shows that a subtree (with root atVP) is aligned to the English string.
Considering thehuge quantity of all the permutations of the alignedconstituents under the tree, only part of the Tree-to-String rules extracted are shown in Figure 1-(c).
Thevariables incorporate linguistic information in theassigned tag by the parser.
When there is no alignedconstituent for further generalization, the variables,defined in our grammar, back off to the Hiero-likeones without any label-identity information.
Onesuch example is in the rule ??
[X,0]?
[X,VP] ?
[X,VP] before the [X,0]?, in which the Hiero-style574March      before   the    sunrise???
??
?VPPP VP???
?
before the sunrise??
March??
sunrise[X,PP] [X,VP]  [X,VP] [X,PP][X,PP]??
March [X,PP]???
?
[X,VP] [X,VP] before the sunrise???
???
March before the sunrise ?
[X,0]  ?
[X,VP] [X,VP ] before the [X,0](a) Parse-Tree Alignment (b) Blocks Alignment (c) Tree-to-String rules with root of VPFigure 1: Example rules extracted.
(a) the aligned source parse tree with target string; (b) general blocks alignment;(c) Tree-to-String rules, with root of VP.
The tree structure is aligned with target stringsThis The casecases triggered trigger an enormous tremendous a hugeshockshocked  shocksinIn the locallocally In the localThis case The case This case wasa great shock great shocks a huge shocklocally in the local local??
?
??
??
??
?
?inthis case the locals triggered enormous shocktriggered a huge?
?
?
??
??
??
?
?IPNP VPDP NPDT NNPPPVPNPNNVV NPADJPJJNPNNTranslations of ???????????
:triggered a huge shock in the locallocally triggered an enormous shockFigure 2: Subtree of ?VP(PP,VP)?
triggered a reordering pattern of swapping the order of the two children PP and VPin the source parse tree.
This will move the translation ?in the local?
after the translation of ?triggered a huge shock?,to form the preferred translation in the highlighted cell: ?triggered a huge shock in the local?.variable [X,0] and the label-based variable [X,VP]co-exist in our proposed rule.We illustrate several special cases of our extractedTree-to-String rules in the following.
We index thevariables with their positions to indicate the align-ment ?, and skip the feature function w?
to simplifythe notations.X ?< [X, IP ]; [X,NP0] [X,V P0]; (2)[X,NP0] is [X,V P0] > .The rule in Eqn.
2 shows that a source tree rootedat IP, with two children of NP and VP generalizedinto variables [X,NP] and [X,VP]; they are rewritteninto ?
[X,NP] is [X,VP]?, with the spontaneous wordis inserted.
Such rules are not allowed in Hiero-stylemodels, as there is no lexical item between the twovariables (Chiang, 2005) in the source side.
Thisrule will generate a spontaneous word ?is?
from thegiven subtree structure.
Usually, it is very hard toalign the spontaneous word correctly, and the ruleswe proposed indicate that spontaneous words aregenerated directly from the source sub-tree struc-ture, and they might not necessarily get algned tosome particular source words.A second example is shown in Eqn.
3, which issimilar to the Hiero rules:X ?< ?
; [X, 0] zhiyi; (3)one of the [X, 0] > .The rule in Eqn.
3 shows that when there isno linguistically-motivated root covering the span,([X,NULL] is then assigned), we simply backoff to the Hiero rules.
In this case, the sourcespan of [X, 0] zhiyi is rewritten into the target?one of the [X, 0]?, without considering the map-575ping of the root of the span.
In this way, the repre-sentation power is kept in the variables in our rules,even if the source subtree is aligned to a discontin-uous sequence on the target side.
This is importantfor Chinese-to-English, because the grammar struc-ture is so different that more than 40% of the subtreestructures were not kept during the translation in ourstudy on hand-aligned data.
Following strictly thesource side syntax will derail from these informativetranslation patterns.X ?< [X,NP ]; [X,NN1][X,NN2][X,NN3];[X,NN3][X,NN1][X,NN2] > .
(4)Eqn.
4. is a POS-based rule ?
a special case inour proposed rules.
This rule shows the reorder-ing patterns for three adjacent NN?s.
POS basedrules can be very informative for some language-pairs such as Arabic-to-English, where the ADJ isusually moved before NN during the translations.As also shown in Eqn.
4 for POS sequences, in theUPenn treebank-style parse trees, a root usually havemore than two variables.
Our rule set for subtree,therefore, contain more than two variables: ?X ?<[X, IP ]; [X,ADV P0][X,NP0][X,V P0]; [X,NP0][X,ADV P0][X,V P0] >?.
A CYK-style decoderhas to rely on binarization to preprocess thegrammar as did in (Zhang et al, 2006) to handlemulti-nonterminal rules.
We adopt the so-calleddotted-rule or dotted-production, similar to theEarly-style algorithm (Earley, 1970), to handle themulti-nonterminal rules in our chart-based decoder.3.2 Feature FunctionsAs used in most of the SMT decoders for a phrase-pair, a set of standard feature functions are appliedin our decoder, including IBM Model-1 like scoresin both directions, relative frequencies in both direc-tions.
In addition to these features, a counter is as-sociated to each rule to collect how many rules wereapplied so far to generate a hypothesis.
The stan-dard Minimum Error Rate training (Och, 2003) wasapplied to tune the weights for all feature types.The number of extracted rules from the GALEdata is generally large.
We pruned the rules accord-ing to their frequencies, and only keep at most thetop-50 frequent candidates for each source side.4 Chart-based DecoderGiven the source sentence, with constituent parse-trees, the decoder is to find the best derivation D?which yield the English string e?:e?
= argmaxD?{?(D)?(e)?
(f |e)}, (5)where ?
(D) is the cost for each of the derivationsthat lead to e from a given source-parsed f ; ?
(e)is for cost functions from the standard n-gram lan-guage models; ?
(f |e) is the cost for the standardtranslation models, including general blocks.
Weseparate the costs for normal blocks and the general-ized rules explicitly here, because the blocks containstronger lexical evidences observed directly fromdata, and we assign them with less cost penaltiesvia a different weight factor visible for optimization,and prefer the lexical match over the derived pathsduring the decoding.Our decoder is a chart-based parser with beam-search for each cell in a chart.
Because the tree-structure can have more than two children, there-fore, the Tree-to-String rules extracted usually con-tain more than two variables.
Slightly different fromthe decoder in (Chiang, 2005), we implementedthe dotted-rule in Early-style parser to handle rulescontaining more than two variables.
Our cube-expansion, implemented the cube-pruning in Chiang(2007), and integrated piece-wise cost computationsfor language models via LM states.
The intermedi-ate hypotheses were merged (recombined) accord-ing to their LM states and other cost model states.We use MER (Och, 2003) to tune the decoder?s pa-rameters using a development data set.Figure 2 shows an example of a tree-based rulefired at the subtree of VP covering the highlightedcell.
When a rule is applied at a certain cell in thechart, the covered source ngram should match notonly the lexical items in the rules, but also the tree-structures as well.
The two children under the sub-tree root VP are PP (?????
: in the local) and VP(????????
: triggered a huge shock ).
Thisrule triggered a swap of these children to generatethe correct word order in the translation: ?triggereda huge shock in the local?.5765 ExperimentsOur training data consists of two corpora: the GALEChinese-English parallel corpus and the LDC hand-aligned corpus1.
The Chinese side of these two cor-pora were parsed using a constituency parser (Luo,2003).
The average labeled F-measure of the parseris 81.4%.Parallel sentences were first word-aligned usinga MaxEnt aligner (Ittycheriah and Roukos, 2005).Then, phrase-pairs that overlap with our develop-ment and test set were extracted from the wordalignments (from both hand alignments and auto-matically aligned GALE corpora) based on the pro-jection principle (Tillmann, 2003).
Besides the regu-lar phrase-pairs, we also extracted the Tree-to-Stringrules from the two corpora.
The detailed statisticsare shown in Table 1.
Our re-implementation of Hi-ero system is the baseline.
We integrated the elevenreordering rules described in (Wang et al, 2007),in our chart-based decoder.
In addition, we reportthe results of using the Tree-to-String rules extractedfrom the hand-aligned training data and the automat-ically aligned training data.
We also report the resultof our translation quality in terms of both BLEU (Pa-pineni et al, 2002) and TER (Snover et al, 2006)against four human reference translations.5.1 The DataTable 1 shows the statistics of our training, develop-ment and test data.
As our word aligner (Ittycheriahand Roukos, 2005) can introduce errors in extractingTree-to-String rules, we use a small hand-aligneddata set ?CE16K?, which consists of 16K sentence-pairs, to get relatively clean rules, free from align-ment errors.
A much larger GALE data set, whichconsists of 10 million sentence-pairs, is used to in-vestigate the scalability of our proposed approach.Table 1: Training and Test DataTrain/test sentences src words tgt wordsCE16K 16379 380103 477801GALE 10.5M 274M 310MMT03 919 24099 -Dev07 2303 61881 -1LDC2006E93The NIST 2003 MT Evaluation (MT03) is usedas our development data set to tune the decoder?sparameters toward better BLEU score.
The text partof GALE 2007 Chinese-to-English Development set(GALE DEV07) is used as our test set.
MT03 con-sists of 919 sentences, whereas GALE DEV07 con-sists of 2303 sentences under two genres: NewsWireand WebLog.
Both have four human reference trans-lations.5.2 Details of Extracted RulesFrom the hand-aligned data, the rules we extractedfall into three categories: regular blocks (phrase-pairs), Hiero-like rules, and Tree-to-String rules.The statistics of the extracted rules are shown in Ta-ble 2Table 2: Rules extracted from hand-aligned dataTypes FrequencyBlock 846965Hiero 508999Tree-to-String 409767Total 1765731We focus on Tree-to-String rules.
Table 3 showsthe detailed statistics of the Tree-to-String rules ex-tracted from the Chinese-to-English hand-alignedtraining data.
The following section provides a de-tailed analysis of the most frequent subtrees ob-served in our training data.5.2.1 Frequent Subtrees: NP, VP, and DNPThe majority of Tree-to-String rules we extractedare rooted at the following labels: NP (46%),VP(22.8%), DNP (2.23%), and QP(2.94%).Wang et al (2007) covers only subtrees of NP,VP, and LCP, which are a subset of our proposedTree-to-String rules here.
They apply these rules asa pre-processing step to reorder the input sentenceswith hard decisions.
Our proposed Tree-to-Stringrules, on the contrary, are applied during the de-coding process which allows for considering manypossible competing reordering options for the givensentences, and the decoder will choose the best oneaccording to the cost functions.Table 4 shows the statistics of reordering rulesfor subtrees rooted at VP.
The statistics suggest that577Table 5: Hiero, Tree-Based (eleven rules in Wang et al (2007)), and Tree-to-String Rules with ?DE?Ruleset Root Src Tgt FrequencyHieroNULL [X,0] ?
[X,1] [X,0] ?s [X,1] 347NULL [X,0] ?
[X,1] [X,1] of [X,0] 306NULL [X,0] ?
[X,1] [X,0] of [X,1] 174Tree-BasedNP DNP(NP) NP NP DNP(NP) -NP DNP(PP) NP NP DNP(PP) -NP DNP(LCP) NP NP DNP(LCP) -Tree-to-String[X,DNP] [X,NP] [X,DEG] [X,NP] [X,DEG] 580[X,DNP] [X,NP] [X,DEG] [X,DEG] [X,NP] 2163[X,DNP] [X,NP] [X,DEG] [X,NP] , [X,DEG] 4Table 3: Distributions of the NP, VP, QP, LCP rulesRoot Frequency Percentage (%)NP 189616 46.2VP 93535 22.8IP 68341 16.6PP 18519 4.51DNP 9141 2.23QP 12064 2.94LCP 4127 1.00CP 2994 0.73PRN 2810 0.68DP 1415 0.34Others 6879 1.67Total 409767 -Table 4: Distribution of the reordering rules for subtreesrooted at VP: [X,VP]; [X,PP] [X,VP]; statistics are col-lected from GALE training dataRoot Target FrequencyVP[X,PP] [X,VP] 126310[X,VP] [X,PP] 22144[X,PP] , [X,VP] 1524[X,PP] that [X,VP] 1098[X,PP] and [X,VP] 831it is impossible to come up with a reordering rulethat is always applicable.
For instance, (Wang etal., 2007) will always swap the children of the sub-tree VP(PP,VP).
However, the statistics shown in Ta-ble 4 suggest that might not be best way.
In fact,due to parser?s performance and word alignment ac-curacies, the statistics we collected from the GALEdataset, containing 10 million sentence-pairs, showthat the children in the subtree VP(PP,VP) is trans-lated monotonically 126310 times, while reorderedof only 22144 times.
However, the hand-aligneddata support the swap for 1245 times, and monotoni-cally for only 168 times.
Part of this disagreement isdue to the word segmentation errors, incorrect wordalignments and unreliable parsing results.Another observations through our extracted Tree-to-String rules is on the controlled insertion of thetarget spontaneous2 (function) words.
Instead of hy-pothesizing spontaneous words based only on thelanguage model or only on observing in phrase-pairs, we make use of the Tree-to-String rules to getsuggestion on the insertion of spontaneous words.In this way, we can make sure that the spontaneouswords are generated from the structure information,as opposed to those from a pure hypothesis.
The ad-vantage of this method is shown in Table 4.
For in-stance, the word ?that?
and the punctuation ?,?
weregenerated in the target side of the rule.
This provesthat our model can provide a more principled way togenerate spontaneous words needed for fluent trans-lations.5.2.2 DEG and DECAn interesting linguistic phenomenon that we in-vestigated is the Chinese word DE ???.
???
is aninformative lexical clue that indicates the need forlong range phrasal movements.
Table 5 shows a few2Target spontaneous words are function words that do nothave specific lexical source informants and are needed to makethe target translation fluent.578high-frequent reordering rules that contain the Chi-nese word ?DE?.The three type of rules handle ?DE?
differently.
Amajor difference is the structure in the source side.Hiero rules do not consider any structure, and ap-ply the rule of ?
[X,0] ?
[X,1]?.
Tree-based rules,as described in Wang et al (2007) do not handle?
directly; they are often implicitly taken care ofwhen reordering DNPs instead.
Our proposed Tree-to-String rules model ?
directly in a subtree con-taining DEG/DEC, which triggers word reorderingwithin the structure.
Our rule set includes all theabove three rule-types with the associated frequen-cies, this enriched the reordering choices to be cho-sen by the chart-based decoder, guided by the statis-tics collected from the data and the language modelcosts.5.3 EvaluationWe tuned the decoding parameters using the MT03data set, and applied the updated parameters to theGALE evaluation set.
The eleven rules of VP, NP,and LCP (tree-based) improved the Hiero baseline3from 32.43 to 33.02 on BLEU.
The reason, the tree-reordering does not gain much over Hiero baseline,is probably that the reordering patterns covered bytree-reordering rules, are potentially handled in thestandard Hiero grammar.A small but noticeable further improvement overtree-based rules, from 33.02 to 33.26, was ob-tained on applying Tree-to-String rules extractedfrom hand-aligned dataset.
We think that the Tree-based rules covers major reordering patterns forChinese-English, and our hand-aligned dataset isalso too small to capture representative statistics andmore reordering patterns.
A close check at the ruleswe learned from the hand-aligned data shows thatthe tree-based rules are simply the subset of therules extracted.
The Tree-to-String grammar im-proved the Hiero baseline from 32.43 to 33.26 onBLEU; considering the effects from the tree-basedrules only, the additional information improved theBLEU scores from 33.02 to 33.26.
Similar picturesof improvements were observed for the two unseentests of newswire and weblog in GALE data.When applying the rules extracted from the much3Hiero results are from our own re-implementation.larger GALE training set with about ten millionsentence-pairs, we achieved significant improve-ments from both genres (newswire and web data).The improvements are significant in both BLEUand TER.
BLEU improved from 32.44 to 33.51 onnewswire, and from 25.88 to 27.91 on web data.Similar improvements were found in TER as shownin the table.
The gain came mostly from the richerextracted rule set, which not only presents robuststatistics for reordering patterns, but also offers moretarget spontaneous words generated from the syntac-tic structures.
Since the top-frequent rules extractedare NP, VP, and IP as shown in Table 3, our proposedrules will be able to win the correct word order withreliable statistics, as long as the parser shows accept-able performances on these structures.
This is espe-cially important for weblog data, where the parser?soverall accuracy potentially might not be very good.Table 7 shows the translations from differentgrammars for the same source sentence.
Both Tree-based and Tree-to-String methods get the correct re-ordering, while the latter can suggest insertions oftarget spontaneous words like ?a?
to allow the trans-lation to run more fluently.6 Conclusion and DiscussionsIn this paper, we proposed our approach to modelboth local and non-local word-reordering in oneprobabilistic synchronous CFG.
Our current modelincorporates source-side syntactic information, tomodel the observations that the source syntactic con-stituent tends to move together during translations.The proposed rule set generalizes over the variablesin Hiero-rules, and we also showed the special casesof the Tree-based rules and the POS-based rules.Since the proposed rules has at most one-level treestructure, they can be easily applied in a chart-baseddecoder.
We analyzed the statistics of our rules,qualitatively and quantitatively.
Next, we comparedour work with other research, especially with thework in Wang et al (2007).
Finally, we reportedour empirical results on Chinese-English transla-tions.
Our Tree-to-String rules showed significantimprovements over the Hiero baseline on the GALEDEV07 test set.Given the low accuracy of the parsers, and the po-tential errors from Chinese word-segmentations, and579Table 6: Hiero, Tree-Based (NP, VP, LCP), and Tree-to-String rules extracted from hand-aligned data (H) or fromGALE training data (G)Setup MT03 GALE07-NewsWire GALE07-WeblogBLEUr4n4 TER BLEUr4n4 TER BLEUr4n4 TERHiero 32.43 59.75 31.68 61.45 25.99 65.65Tree-based 33.02 59.84 32.22 61.46 25.67 65.64Tree-to-String (H) 33.26 61.04 32.44 61.36 25.88 65.54Tree-to-String (G) 35.51 57.28 33.51 59.71 27.91 62.88Table 7: Hiero, Tree-Based (NP, VP, LCP), Tree-to-String TranslationsSrc-Sent ???????????
?Hiero in this case local triggered shock .Tree-Based the case triggered uproar in the local.Tree-to-String the case triggered a huge uproar in the local .word-alignments, our rules learned are still noisy.Exploring better cost functions associate each rulemight lead to further improvement.
Because ofthe relative high accuracy of English parsers, manyworks such as Zollmann and Venugopal (2006) andShen et al (2008) emphasize on using syntax in tar-get languages, to directly influence the fluency as-pect of the translation output.
In future, we plan toincorporate features from target-side syntactic infor-mation, and connect them with the source informa-tion explored in this paper, to model long-distancereordering for better translation quality.AcknowledgmentsThe authors would like to thank the anonymousreviewers for their comments to improve this pa-per.
This work was supported by DARPA GALEprogram under the contract number HR0011-06-2-0001.ReferencesYaser Al-Onaizan and Papineni.
Kishore.
2006.
Distor-tion models for statistical machine translation.
In Pro-ceedings of ACL-COLING, pages 529?536.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL?05), pages 263?270, AnnArbor, Michigan, June.
Association for ComputationalLinguistics.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
In Computational Linguistics.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL.Jay Earley.
1970.
An efficient context-free parsing al-gorithm.
In Communications of the ACM., volume 13,pages 94?102.Heidi J.
Fox.
2002.
Phrasal cohesion and statisticalmachine translation.
In Proc.
of the Conference onEmpirical Methods in Natural Language Processing,pages 304?311, Philadelphia, PA, July 6-7.Jonathan Graehl and Kevin Knight.
2004.
Training treetransducers.
In Proc.
NAACL-HLT.Abraham Ittycheriah and Salim Roukos.
2005.
A maxi-mum entropy word aligner for arabic-english machinetranslation.
In HLT/EMNLP.Shankar Kumar and William Byrne.
2005.
Local phrasereordering models for statistical machine translation.In HLT/EMNLP 2005, Vancouver, B.C., Canada.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In ACL-Coling.Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007.Forest-to-string statistical translation rules.
In 45thAnnual Meeting of the Association for ComputationalLinguistics.Xiaoqiang Luo.
2003.
A maximum entropy chinesecharacter-based parser.
In Proc.
of ACL.Yuval Marton and Philip Resnik.
2008.
Soft syntacticconstraints for hierarchical phrased-based translation.In ACL.580Sonja Niessen and Hermann Ney.
2004.
Statisticalmachine translation with scarce resources using mor-phosyntactic information.
In Computational Linguis-tics.Franz J. Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.In Computational Linguistics, volume 30, pages 417?449.Franz J. Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proc.
of the 41stAnnual Meeting of the Association for ComputationalLinguistics, Japan, Sapporo, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalu-ation of machine translation.
In Proc.
of the 40th An-nual Conf.
of the Association for Computational Lin-guistics (ACL 02), pages 311?318, Philadelphia, PA,July.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of ACL.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In AMTA.Christoph Tillmann and Tong Zhang.
2005.
A localizedprediction model for statistical machine translation.
InProceedings of the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL?05), pages557?564, Ann Arbor, Michigan, June.
Association forComputational Linguistics.Christoph Tillmann.
2003.
A projection extension algo-rithm for statistical machine translation.
In Proc.
ofthe Conference on Empirical Methods in Natural Lan-guage Processing.Christoph Tillmann.
2008.
A rule-driven dynamic pro-gramming decoder for statistical mt.
In HLT SecondWorkshop on Syntax and Structure in Statistical Trans-lation.Chao Wang, Michael Collins, and Phillip Koehn.
2007.Chinese syntactic reordering for statistical machinetranslation.
In proceedings of EMNLP.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.
InComputational Linguistics, volume 23(3), pages 377?403.Fei Xia and Michael McCord.
2004.
Improving a sta-tistical mt system with automatically learned rewritepatterns.
In the 20th International Conference onComputational Linguistics (COLING 2004), Geneva,Switzerland, Aug 22-29.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum entropy based phrase reordering model for sta-tistical machine translation.
In ACL-Coling.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The penn chinese treebank: Phrasestructure annotation of a large corpus.
In Natural Lan-guage Engineering, volume 11, pages 207?238.K.
Yamada and Kevin.
Knight.
2001.
Syntax-based Sta-tistical Translation Model.
In Proceedings of the Con-ference of the Association for Computational Linguis-tics (ACL-2001).Richard Zens, E. Matusov, and Hermmann Ney.
2004.Improved word alignment using a symmetric lexiconmodel.
In Proceedings of the 20th International Con-ference on Computational Linguistics (CoLing 2004),pages 36?42, Geneva, Switzerland, Auguest.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for machinetranslation.
In Proceedings of the HLT-NAACL.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax augmented machine translation via chart parsing.In Proc.
of NAACL 2006 - Workshop on statistical ma-chine translation.581
