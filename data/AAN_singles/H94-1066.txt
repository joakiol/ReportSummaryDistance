SIGNAL PROCESSING FOR ROBUST SPEECH RECOGNITIONFu-Hua Liu, Pedro J. Moreno, Richard M. Stem, Alejandro AceroDepartment ofElectrical and Computer EngineeringSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213ABSTRACTThis paper describes a series of ceps~al-based compensation pro-cedures that render the SPHINX-II system more robust withrespect to acoustical environment.
The first algorithm, phone-dependent cepstral compensation, is similar in concept to the pre-viously-described MFCDCN method, except hat cepstral com-pensation vectors are selected according to the current phonetichypothesis, rather than on the basis of SNR or VQ codeword iden-tity.
We also describe two procedures toaccomplish adaptation ofthe VQ codebook for new environments, as well as the use ofreduced-bandwidth f~equency analysis to process telephone-band-width speech.
Use of the various compensation algorithms in con-sort produces a reduction of error ates for SPHINX-II by as muchas 40 percent relative to the rate achieved with eepstral mean nor-realization alone, in both development test sets and in the contextof the 1993 ARPA CSR evaluations.1.
INTRODUCTIONA continuing problem with current speech recognition technologyis that of lack of robustness with respect to environmental v riabil-ity.
For example, the use of microphones other than the ARPAstandard Sennheiser HM--414 "close-talking" headset (CLSTLK)severely degrades the performance of systems like the originalSPHINX system, even in a relatively quiet office environment\[e.g.
1,2\].
Applications such as speech recognition i automobiles,over telephones, on a factory floor, or outdoors demand an evengreater degree of environmental robustness.In this paper we describe and compare the performance of a seriesof cepstrum-based procedures that enable the CMU SPHINX-II\[8\] speech recognition system to maintain a high level of recogni-tion accuracy over a wide variety of acoustical environments.
Wealso discuss the aspects of these algorithms that appear to havecontributed most significantly to the success of the SPHINX-IIsystem in the 1993 ARPA CSR evaluations for microphone inde-pendence (Spoke 5) and calibrated noise sources (Spoke 8).In previous years we described the performance of cepstral map-ping procedures such as the CDCN algorithm, which is effectivebut fairly computationally costly \[2\].
More recently we discussedthe use of eepstral highpass-filtering al orithms \[such as the popu-lax RASTA and cepstral-mean-normalization lg rithms (CMN)\[6\].
These algorithms are very simple to implement but somewhatlimited in effectiveness, and CMN is now part of baseline process-ing for the CMU and many other systems.In this paper we describe several new procedures that when usedin consort can provide as much as an additional 40 percentimprovement over baseline processing with CMN.
These tech-niques include:?
Phone-dependent cepstral compensation?
Environmental interpolation f compensation vectors?
Codebook adaptation?
Reduced-band analysis for telephone-bandwidth speech.?
Silence codebook adaptationIn Sec.
2 we describe these compensation procedures in detail,and we examine their effect on recognition accuracy in Secs.
3and 4.2.
ENVIRONMENTAL COMPENSATIONALGORITHMSWe begin this section by reviewing the previously-describedMFCDCN algorithm, which is the basis for most of the new pro-cedures discussed.
We then discuss blind environment selectionand environmental interpolation asthey apply to MFCDCN.
Thecomplementary procedures of phone-dependent cepstral normal-ization and codebook adaptation are described.
We close this sec-tion with brief description of reduced-bandwidth analysis andsilence-codebook adaptation, which are very beneficial in pro-cessing telephone-bandwidth speech and speech recorded in thepresence of strong background noise, respectively.2.1.
Multiple Fixed Codeword-DependentCepstral Normalization (MFCDCN)Multiple fixed codeword-dependent cepstral normalization(MFCDCN) provides additive cepstral compensation vectors thatdepend on signal-to-noise ratio (SNR) and that also vary fromcodeword to codeword of the vector-quantized (VQ) representa-tion of the incoming speech at each SNR \[6\].
At low SNRs thesevectors primarily compensate for effects of additive noise.
Athigher SNRs, the algorithm compensates forlinear filtering, whileat intermediate SNRs, they compensate for both of these effects.Environmental independence is provided by computing compen-sation vectors for a number of different environments and select-ing the compensation e vironment that results in minimal residualVQ distortion.Compensation vectors for the chosen testing environment areapplied to normalize the utterance according to the expression330ftt = z t + r \ [k  t, l r e\] where kt, I t ,  t, and e are the VQ code-word index, instantaneous frame SNR, time frame index and theindex of the chosen environment, respectively, and ~t, zt, and r arethe compensated (transformed) ata, original data and compensa-tion vectors, respectively.2.2.
Blind Environment SelectionIn several of the compensation procedures used, includingMFCDCN, one of a set of environments must be selected as partof the compensation process.
We considered three procedures forenvironment selection in our experiments.The first procedure, referred to as selection by compensation,applies compensation vectors from each possible nvironmentsuccessively tothe incoming test utterance.
The environment e ischosen that minimizes the average residual VQ distortion over theentire utterance.
In the second approach, referred to as environ-merit-specific VQ, environment-specific ookbooks are generatedfrom the original uncompensated speech.
By vector quantizing thetest data using each environment-specific codebook in turn, theenvironment with the minimum VQ distortion ischosen.
The thirdprocedure, referred to as Gaassian environment classifier, modelseach environment with mixtures of Gaussian densities.
Environ-ment selection is accomplished sothat he test data has the highestprobability from the corresponding classifier.
~ latter approachis similar to one proposed previously by BBN \[7\].All three methods produce similar speech recognition accuracy.2.3.
Interpolated FCDCN (IFCDCN)In cases where the testing environment does not closely resembleany of the particular environment used to develop compensationparameters for MFCDCN, interpolating the compensation vectorsof several environments can be more helpful than using compensa-tion vectors from a single (incorrect) environment.
As inMFCDCN, compensation vectors used in the interpolated f ixedcodeword-dependent  cepst ra l  normal i za t ion  algorithm(IFCDCN) are precomputed for environments in the training data-base in the estimation phase.
Compensation vectors for new envi-ronments are obtained by linear interpolation of several of theMFCDCN compensation vectors:?\[k, l \]  = ~E~=lfe" rtk,  l, elwhere ?\[k, I\] , t\[k,l,e\], andre are the estimated compensationvectors, the environment-specific ompensation vector for the e thenvironment, and the weighting factor for the e th environment,respectively.The weighting factors for each environment are also based onresidual VQ distortion:p (el ~ exp {De~ (202) }f " -  -where O is the codebook standard deviation using speech from theCLSTLK microphone, ~ represents he testing utterance, and Dj331and D e are the residual VQ distortions of the j4h and e th environ-ments.
We have generally used a value of 3 for E.2.4.
Phone-Dependent Cepstral Normalization(PDCN)In this section we discuss an approach to environmental compen-sation in which additive cepstral compensation vectors are selectedaccording to the current phoneme hypothesis inthe search process,rather than according to physical parameters such as SNR or VQeodeword identity.
Since this phoneme-based approach relies oninformation from the acoustic-phonetic and language models todetermine the compensation vectors, it can be referred to as a"back-end" compensation procedures, while other approachessuch as MFCDCN which work independently of the decoder canbe regarded as "front-end" compensation schemes.Estimation of PDCN compensation Vectors.
In the currentimplementation f phone-dependent cepstral normalization(PDCN), we develop compensation vectors that are specific toindividual phonetical events, using a base phone set of 51 pho-nemes, including silence but excluding other types of non-lexicalevents.
This is accomplished by running the decoder in supervisedmode using CLSTLK data and correction transcriptions.
AllCLSTLK utterances are divided into phonetic segments.
For everyphonetic label, a difference vector is computed by accumulatingthe cepstral difference between the CLSTLK training data, xp andits noisy counterpart, z r Compensation vectors are computed byaveraging the corresponding difference vector as follows,e \[p\] =ZAu= S(ft-') IEt= I (xt-zt)lit__ is(/t-p)whereft is the phoneme for frame t, p the phoneme index and T ulength of the uth utterance out of A sentences.Compensation of PDCN in Recognition.
The SPHINX-H systemuses the senone \[4,8\], a generalized state-based probability densityfunction, as the basic unit to compute the likelihood from acousti-cal models.
The probability density function for senone s in framet for the cepstral vector z t of incoming speech can be expressed as=where m t stands for the index of the best B Gaussian mixtures ofand are the cor- senone s for cepstra vector zt, and J.l.rat, Omt , Wra tth ?
responding mean, standard eviation, and weight for the m t nux-ture of senone s. Multiple compensated cepslxal vectors are formedin PDCN by adding various compensation vectors to incomingcepstra, ftt.p(t ) , where Rt.
p = z t+c \ [p  \] on a frame-by-framebasis for the presumed phoneme index, p.The amount of computation needed for this procedure is reducedbecause in SPHINX-H, each senone corresponds toonly one dis-tinctive base phoneme.
A cepstral vector can be normalized with aproper PDCN compensation factor corresponding to the particularbase phonetical identity.
As a result, senone probabilities can becalculated by the presumed phonetic identity that corresponds to agiven senone.
Using this approach, the senone probability inPDCN is re-written asPr(tt.p\[ s) = znBt= 1Wnff(It, p;~n, On, )where n t is the index of the best B Gaussian mixtures for senones at frame t with respect to the PDCN-normalized ceprtral vector~t,p, for the corresponding phonetic labelp for senone s.Interpellated PDCN (IPDCN).
PDCN, like SDCN and FCDCN\[3,6\], assumes the existence of a database of utterances recordedin stereo) in the training and testing environments.
In situationswhere no data from any particular testing environment is availablefor estimation, IPDCN is desirable.
Based on an ensemble ofpre-computed PDCN compensation vectors, IPDCN applies to theincoming utterance an interpolation f compensation vectors fromseveral of the closest environments (IPDCN).
The interpolation isperformed in the same way that it was for IFCDCN.
In the currentimplementation, we use the 3 closest environments with the best 4Ganssian mixtures in interpolation.2,5.
Codebook  Adaptat ion  (DCCA andBWCA)A vector quantization (VQ) codebook, which is a set of mean vec-tors and/or co-variance matrices of cepstral representations, al oexhibits ome fundamental differences when mismatches areencountered between training and testing environments \[7\].
Thissuggests that when such mismatches xist, the codebook can be"tuned" to better characterize the cepstral space of testing data.
Inthis section, we propose two different implementations of uchcodebook adaptation.Dual-Channel Codebook Adaptation (DCCA).
Dual-ChannelCodebook Adaptation (DCCA) exploits the existence of speechthat is simultaneously recorded using the CLSTLK microphoneand a number of secondary microphones.
From the viewpoint offront-end compensation, the senone probability density functioncan be expressed as the Gaussian mixtureB BPst = k ~= lWkN(it;~k'?k) = k ~= 1WkN(Zt+Szt;~tk'?k)where k, z t, 8z t , Jt t, ~t k, o k are the mixture index among top Bmixtures, noisy observation vector, compensation vector, compen-sated vector, mean vector for the k th mixture, and variance, respec-tively.
The senone probability density function is re-written asPst = ZkB= l WkN(Zt;Ilk k, O k +80 k)= ZkB= l WkN<Z::where 8gt and 8o k are deviations from cepstral space of targetnoisy environment to that of the reference training environmentfor means and variance in the corresponding Gaussian mixtures.In implementing DCCA, VQ encoding is performed on speechfrom the CLSTLK microphone processed with CMN.
The outputVQ labels are shared by the CLSTLK data and the correspondingdata in the secondary (or target) environment.
For each subspace inthe CLSTLK training environment, we generate he correspondingmeans and variances for the target environment.
Thus, a one-to-one mapping between the means and variances of the cepstralspace of the CLSTLK training condition and that of the target con-dition is established.Recognition is accomplished by shifting the means of the Gauss-ian mixtures according to the relationshipsB B= wkN(',+s':.k, ok)=Pat k=l  k f lBaum-Welch Codebook Adaptation (BWCA).
There are manyapplications in which stereo data simuRaneously recorded in theCLSTLK and target environments are unavailable.
In these cir-cumstances, transformations can be developed between environ-ments using the contents of the adaptation utterances using theBaum-Welch algorithm.In Bantu-Welch codebook adaptation, mean vectors and covari-ance matrices, along with senones, are re-estimated and updatedusing the Bantu-Welch algorithm \[5\] during each iteration of train-ing process.
To compensate for the effect of changes in acousticalenvironments, he Baum-Welch approach is used to transform themeans and covariances toward the cepstral space of the target est-ing environments.
This is exactly like normal training, except thatonly a few adaptation utterances are available, and that number offree parameters to be estimated (i.e.
the means and variances of theVQ codewords) is very small.2 .6.
Reduced Bandwidth  Ana lys i s  fo r  Te le -phone  SpeechThe conventional SPHINX-II system uses signal processing thatextracts Mel-frequency epstral coefficients (MFCC) over an anal-ysis range of 130 to 6800 Hz.
This choice of analysis bandwidth isappropriate when the system processes speech recorded throughgoed-quality microphones such as the CLSTLK microphone.
Nev-ertheless, when speech is recorded from telephone lines, previousresearch at CMU \[9\] indicates that error rates are sharplydecreased when the analysis bandwidth is reduced.
This is accom-plished by performing the normal DFT analysis with the normal16,000-Hz sampling rate, but only retaining DFT coefficients afterthe triangular f equency smothing from center frequencies of 200to 3700 Hz.
Reduced-bandwidth MFCC coefficients are obtainedby performing the discrete-cosine transform only on these fre-quency-weighted DFr  coefficients.To determine whether or not speech from an unknown environ-ment is of telephone nature, we use the Gaussian environmentclassifier approach, as described in Sec.
2.2.
Two VQ codebooksare used, one for telephone speech using a wideband front-endanalysis and another for non-telephone speech.
The speech wasclassified to maximize nvironmental likelihood.2.7 .
S i lence  codebook  adaptat ionWhen dealing with speech-like noises such as a speech or music inthe background the compensation techniques described above pro-332vide only partial recovery.
Most of these techniques assume cer-tain statistical features for the noise (such as stationarity atthesentence l vel), that are not valid.
The SPHINX-l/recognitionsystem still produces a large number of insertion errors in difficultrecognition environments, such as those used in the 1993 CSRSpoke 8 evaluation, even when cepstral compensation is used.
Wehave found that the use of silence codebook adaptation (SCA)helps reduce insertion rates in these circumstances byprovidingbetter discrimination between speech and speech-like noises.In SCA processing, the HMM parameters (codebook means, vari-ances, and probabilities) are updated for the silence and noise seg-ments by exposure to lraining data from a corpus that more closelyapproximates the testing environment than speech from theCLSTLK microphone.
If not enough data are available, an updateof the cepstral means only is performed.
Further details on howthis procedure was implemented for the 1993 CSR Spoke 8 evalu-ation are provided in Sec.
4.23.
PERFORMANCE OF ALGORITHMS INDEVELOPMENTAL TESTINGIn this and the following section we describe the results of a seriesof experiments that compare the recognition accuracy of the vari-ous algorithms described in Sec.
2 using the ARPA CSR WallStreet Journal task.
The 7000 WSJ0 utterances recorded using theCLSTLK microphone were used for the training corpus, and inmost cases the system was tested using the 330 utterances fromsecondary microphones in the 1992 evaluation test set.
This testset has a closed vocabulary of5000 words.Two implementations of the SPHINX recognition system wereused in these evaluations.
For most of the development work andfor the official 1993 CSR evaluations for Spoke 5 and Spoke 8, asmaller and faster version of SPHINX-II was used than the imple-mentation used for the official ATIS and CSR Hub evaluations.We refer to the faster system as SPHINX-IIa in this paper.SPHINX-Ha differs from SPHINX-II in two ways: it uses a big-ram grammar ( ather than a trigram grammar) and it uses only onecodebook (rather than 27 phone-dependent codebooks).
Spoke 5and selected other test sets were subsequently re-evaluated using aversions of SPHINX-II that was very similar to the one used in theATIS and CSR Hub evaluations.3.1.
Comparison of MFCDCN, IFCDCN,PDCN, and IPDCNWe first consider the relative performance of the MFCDCN,IFCDCN, PDCN, and IPDCN algorithms, which were evaluatedusing the training and test sets described above.
Recognition accu-racy using these algorithms i compared to the baseline perfor-mance, which was obtained using conventional Mel-cepstrumbased signal processing in conjunction with cepstral man normal-ization (CMN).Table 1 compares word error ates obtained using various process-ing schemes along with the corresponding reduction of word errorrates with the respect to the baseline with CMN.
Compensationvectors used for these comparisons were developed from trainingdata that include the testing environments.
Table 2 summarizessimilar esults that were obtained when the actual testing environ-ment was excluded from the set of data used to develop the com-pensation vectors.COMPENSATION CLSTLK OTHERALGORITHM mle mlcsCMN (baseline) 7.6 21.4CMN+MFCDCN 7.6 14.5CMN+IFCDCN 7.8 15.1CMN+PDCN 7.9 16.9CMN+MFCDCN+PDCN 7.6 12.8Table 1: Word error ates obtained on the secondary-talc datafrom the 1992 WSJ evaluation test using CMN, MFCDCN, andPDCN with and without environment i erpolation.COMPENSATIONALGORITHMCMN (baseline)CMN+MFCDCNCMN+MFCDCN+PDCNCMN+IFCDCNCMN+IFCDCN+IPDCNCLSTLK OTHERmle mles7.6 21.47.6 16.17.6 14.87.6 15.67.6 13.5Table 2: Word error ates obtained using CMN, MFCDCN, andPDCN as in Table 1, but with the testing environments excludedfrom the corpus used to develop compensation vectors.The results of Table 1 indicate that PDCN when applied in isola-tion provides arecognition error rate that is not as good as thatobtained using MFCDCN.
Nevertheless, the effects of PDCN andMFCDCN are complementary in that he use of the two algorithmsin combination provides alower error ate than was observed witheither algorithm applied by itself.
The results in Table 2 demon-strate that he use of environment i erpolation is helpful when thetesting environment is not included in the set used to develop com-pensation vectors.
Environmental interpolation degrades perfor-mance slightly, however, when the actual testing environment wasobserved in developing the compensation vectors.3.2.
Performance of Codebook AdaptationTable 3 compares word error rates obtained with the DCCA andBWCA as described in Sec.
2.5 with error rates obtained withCMN and MFCDCN.
The Bantu-Welch codebook adaptation wasimplemented with four iterations of re-estimation, re-estimatingthe codebook means only.
(Means and variances were re-estimatedin a pilot experiment, but with no improvement i  performance.
)COMPENSATION CLSTLK OTHERALGORITHM mle micsCMN (baseline) 7.6 21.4CMN+MFCDCN 7.6 14.5CMN+IFCDCN 7.8 15.1CMN+DCCA 7.9 14.2CMN+MFCDCN+DCCA 7.6 12.3CMN+BWCA 7.9 16.7CMN+MFCDCN+BWCA 7.6 13.5Table 3: Comparison of error ates obtained using codebookadaptation with and without MFCDCN.333Table 4 provides imilar comparisons, but with the testing envi-ronments excluded from the corpus used to develop compensationvectors (as in Table 2).COMPENSATION CLSTLK OTHEALGORITHM mie RmlcsCMN (baseline) 7.6 21.4CMN+MFCDCN 7.6 16.1CMN+MFCDCN+DCCA I 7.6 15.8CMN+MFCDCN+BWCA 7.6 15.5CMN+IFCDCN 7.6 15.6CMN+IFCDCN+DCCA 7.6 15.0CMN+IFCDCN+BWCA \[ 7.6 14.6Table ,l: Word error ates as in Table 3, but with the testing envi-ronments excluded from the corpus used to develop the compen-The results of Tables 3 and 4 indicate that the effectiveness ofcodebook adaptation used in isolation to reduce rror ate is aboutequal to that of MFCDCN.
Once again, the use of environmentalinterpolation is helpful in cases in which the testing environmentwas not used to develop compensation vectors.3.3.
Reduced-bandwidth Processing for Tele-phone SpeechTable 5 compares error rates obtained using conventional signalprocessing versus reduced-bandwidth analysis for the telephone-microphone subset of the development set, and for the remainingmicrophones.PROCESSING NON-TELEPHONE TELEPHONEBANDWIDTH mlcs  mics  IFull bandwidth 11.2 39.0Reduced bandwidth I 22.3 15.5Table 5: Word error ates obtained using conventional andreduced-bandwidth analysis for the 1992 WSJ test set.It can be seen in Table 5 that the use of a reduced analysis band-width dramatically improves recognition error rate when the sys-tem is trained with high-quality speech and tested usingtelephone-bandwidth speech.In an unofficial evaluation we applied reduced-bandwidth analysisto the telephone speech data of Spoke 6 (known-microphone adap-tation) from the 1993 ARPA CSR evaluation.
Using a version ofSPHINX-II trained with only the 7000 sentences ofthe WSJ0 cor-pus, we observed an error rate for the test set of 13.4%.
Thisresults compares favorably with results reported by other sitesusing systems that were trained with both the WSJ0 snd WSJ1corpora.4.
PERFORMANCE USING THE 1993 CSREVALUATION DATAWe summarize in this section the results of experiments u ing the1993 ARPA CSR WSJ test set, including official results obtainedusing SPHINX-Ha and subsequent evaluations with SPHINX-II.4.1.
Spoke  5: M ic rophone IndependenceThe testing data for Spoke 5 were samples of speech from 10microphones that had never been used previously in ARPA evalua-tions.
One of the microphones is a telephone handset and anotheris a speakerphone.The evaluation system for Spoke 5 first performs acrude environ-mental classification using the Gaussian environment classifier,blindly separating incoming speech into utterances that areassumed to be recorded either from wideband microphones ortele-phone-bandwidth microphones and channels.
Speech that isassumed to be recorded from a full-bandwidth microphone is pro-cessed using a combination f IFCDCN and IPDCN, interpolatingover the closest three environments forIFCDCN and over the bestfour environments for IPDCN.
Speech that is believed to berecorded through atelephone channel is processed using a combi-nation of narrow-band processing as described in Sec.
2.6 andMFCDCN.
The systems were trained using the CLSTLK micro-phone.CONDI-TIONPOC1C2C3TESTING COMPEN- SPHX-IIa SPHX-HMIC SATION 11/93 12/93Other ON 15.6 10.4OFF !
21.3 16.8 OthercLSTLK ON 10.0 6.6CLSTLK OFF I 10.1 6.5Table 6: Comparison of word error ates for Spoke 5 using twodifferent recognition systems, SPHINX-Ha and SPHINX-If.Recognition error rates on the 5000-word $5 task are summarizedin Table 6.
The results for SPHINX-Ha are the official evaluationresults.
The test data were re-run in 12/93 using a version ofSPHINX-II that was very similar to that used for the Hub evalua-tion.
Although this evaluation was "unofficial", it was performedwithout any further algorithm development or exposure to the testset after the official evaluation.
We note that the baseline system(without "compensation") already includes CMN.We believe that one of the most meaningful figures of merit forenvironmental compensation is the ratio of errors for the P0 andC2 conditions (i.e.
the ratio of errors obtained with CLSTLKspeech and speech in the target environments with compensationenabled).
For this test set, switching from speech from theCLSTLK microphone tospeech from the secondary microphonescauses the error rates to increase by a factor of 1.3 for the 8 non-telephone nvironments, by a factor of 2.4 for the 2 telephoneenvironments, and by a factor of 1.5 fur the complete set of testingdata.
In fact, in 3 of the 10 secondary environments, he compen-sated error rate obtained using the secondary miss was within 25percent of the CLSTLK error rate.
Interestingly enough, the ratioof errors for the P0 and C2 conditions i unaffected by whetherSPHINX-II or SPHINX-Ha was used for recognition, confirmingthat in these conditions, the amount of error eduction provided by334environmental compensation does not depend on how powerful arecognition system is used.4.2.
Spoke 8: Calibrated Noise SourcesSpoke 8 considers the performance of speech recognition systemsin the presence of background interference consisting of speechfrom AM-radio talk shows or various types of music at 3 differentSNRs, 0, 10 and 20 dB.
The speech is simultaneously collectedusing two microphones, the CLSTLK microphone and a desktopAudio-Technica microphone with known acoustical characteris-tics.
The 0-dB and 10-dB conditions are more difficult than theacoustical environment of Spoke 5, because the background signalfor both the AM-radio and music conditions i  frequently speech-like, and because it is highly non-stationary.
SNRs are measured atthe input to the Audio-Technica microphoneThe evaluation system used a combination of two algorithms forenvironmental robustness, MFCDCN, and silence codebook adap-tation (SCA).
New silence codebooks were created using animplementation of Baum-Welch codebook adaptation, asdescribed in Sec.
2.5.
Two ceps~al codebooks were developed forSPHINX-IIa, with one codebook representing the noise andsilence HMMs, and the other codebook representing the otherphones.
The normal Baum-Welch re-estimation formulas wereused updating only the means for the noise and silence HMMs.NOISETYPEMusicAM radioSNR (dB)0102001020CONDITIONS(% error rate)P0 C1 C2 C358.7 77.9 17.7 16.919.5 32.2 14.8 12.014.8 15.0 14.3 11.575.5 86.7 29.1 25.925.5 36.9 15.4 12.115.8 16.6 15.4 12.4Table 7: Word error ates for Spoke 8 evaluation usingSPHINX-Ha.
The evaluation system included MFCDCN andSCA.
The Audio-Technica is used as the secondary microphone.The reduced-performance SPHINX-IIa system was used as inSpoke 5, except hat two cepstral codebooks were needed toimplement silence codebook adaptation, one to model the noiseand silence segments, and the other to model the remaining pho-nemes.
Four codebooks were developed to model silence seg-ments for different combinations of SNR and background noise,and the codebook used to provide silence compensation was cho-sen blindly on the basis of minimizing residual VQ distortion.System parameters were chosen that optimize performance for the10-dB SNR.
Results for Spoke 8 are summarized in Table 7.
Bycomparing the C2 and C3 results using the CLSTLK microphonewith the P0 and S1 results using the Audio-Technica mic, we notethat very little degradation is observed in the 20-dB condition butthat recognition accuracy is quite low for the 0-riB condition, evenwhen the signal is compensated.
The use of MFCDCN and SCAimproves recognition accuracy by 35.0 percent overall, and theratio of overaU error rates for the C2 and P0 conditions i  1.49, asin Spoke 5.
As expected, the AM-radio interference was more dif-ficult to cope with than the musical interference atall SNRs, pre-sumably because it is more speech-like.5.
SUMMARY AND CONCLUSIONSIn this paper we describe anumber of procedures that improve therecognition accuracy of the SPHINX-II system in unknown acous-tical environments.
We found that the use of MFCDFN and phone-dependent cepstral normalization reduces the error rate by 40 per-cent compared to that obtained with CMN alone.
The use ofBaum-Welch codebook adaptation with MFCDCN reduces theerror rate by 37 percent compared to that obtained with CMNalone.
The use of reduced-frequency processing reduces error atesfor telephone-bandwidth speech by 58 percent compared to therate observed for conventional signal processing.
The performanceof these systems for the 1993 CSR Spoke 5 and Spoke 8 evalua-tious is described.ACKNOWLEDGMENTSThis research was sponsored by the Department of the Navy,Naval Research Laboratory under Grant No.
N00014-93-1-2005.The views and conclusions contained in this document are those ofthe authors and should not be interpreted as representing the offi-cial pericles, either expressed or implied, of the U.S. Government.We thank Raj Reddy, Mei-Yuh Hwang, and the rest of the speechgroup for their contributions to this work, and Yoshiaki Ohshimain particular for seminal discussions on reduced-bandwidth fre-quency analysis.REFERENCES1.
Juang, B.-H., "Speech Recognition i  Adverse Environ-ments", Computer Speech and Language, 5:275-294, 1991.2.
Acero, A., Acoustical nd Etwironmental Robustness in Auto-matic SPeech Recognition, Kluwer Academic Publishers, Bos-ton, MA, 1993.3.
Lin, F.H., Acero, A., and Stern, R.M., "Effective Joint Com-pensation of Speech for the Effects of AddRive Noise and Lin-ear Filtering", ICASSP-92, pp.
865-868, March 1992.4.
Hwang, M.Y., Subphonetic A oustic Modeling for Speaker-Independent Continuous Speech Recognition, Ph.D. Thesis,Carnegie Mellon University, 1993.5.
Huang, X.D., Ariki, Y., and Jack, M., Hidden Marker Modelsfor Speech Recognition, Edinburgh University Press, Edin-burgh, U.IC, 1990.6.
Liu, F.H., Stern, R.M., Huang, X.D., and Acero A., "EfficientCepstral Normalization for Robust Speech Recognition," Pro-ceedings of ARPA Speech and Natural Language Workshop,pp.
69-74, Princeton, March 1993.7.
Schwartz, R., Anestesakos, T., Kubala, E, Makhoul, J.,Nguyen, L., and Zavaliagkos, G., "Comparative Experimentson Large Vocabulary Speech Recognition", Prec.
ARPAHuman Language Technology Workshop, March, 1993.8.
Huang, X., AUeva, F., Hen, H., Hwang, M., Lee, IC, andRosenfeld, R., "The SPHINX-II Speech Recognition System:An Overview", Computer Speech and Language, 2:137-148,1993.9.
Moreno, P. J., and Stern, R. M., "Sources of Degradation ofSpeech Recognition i  the Telephone Network", ICASSP-94,April, 1994.335
