Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 255?265, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsExploring Vector Space Models to Predict the Compositionality ofGerman Noun-Noun CompoundsSabine Schulte im Walde and Stefan Mu?ller and Stephen RollerInstitut fu?r Maschinelle SprachverarbeitungUniversita?t StuttgartPfaffenwaldring 5b, 70569 Stuttgart, Germany{schulte,muellesn,roller}@ims.uni-stuttgart.deAbstractThis paper explores two hypotheses regardingvector space models that predict the compo-sitionality of German noun-noun compounds:(1) Against our intuition, we demonstrate thatwindow-based rather than syntax-based distri-butional features perform better predictions,and that not adjectives or verbs but nouns rep-resent the most salient part-of-speech.
Ouroverall best result is state-of-the-art, reach-ing Spearman?s ?
= 0.65 with a word-space model of nominal features from a 20-word window of a 1.5 billion word web cor-pus.
(2) While there are no significant dif-ferences in predicting compound?modifier vs.compound?head ratings on compositionality,we show that the modifier (rather than thehead) properties predominantly influence thedegree of compositionality of the compound.1 IntroductionVector space models and distributional informationhave been a steadily increasing, integral part of lex-ical semantic research over the past 20 years.
Onthe one hand, vector space models (see Turney andPantel (2010) and Erk (2012) for two recent sur-veys) have been exploited in psycholinguistic (Lundand Burgess, 1996) and computational linguistic re-search (Schu?tze, 1992) to explore the notion of ?sim-ilarity?
between a set of target objects within a ge-ometric setting.
On the other hand, the distribu-tional hypothesis (Firth, 1957; Harris, 1968) hasbeen exploited to determine co-occurrence featuresfor vector space models that best describe the words,phrases, sentences, etc.
of interest.While the emergence of vector space models is in-creasingly pervasive within data-intensive lexical se-mantics, and even though useful features have beenidentified in general terms:1 when it comes to a spe-cific semantic phenomenon, we need to explore therelevant distributional features in order to investigatethe respective phenomenon.
Our research is inter-ested in the meaning of German compounds.
Morespecifically, we aim to predict the degrees of compo-sitionality of German noun-noun compounds (e.g.,Feuerwerk ?fire works?)
with regard to the mean-ings of their constituents (e.g., Feuer ?fire?
and Werk?opus?).
This prediction uses vector space models,and our goal is to identify salient features that de-termine the degree of compositionality of the com-pounds by relying on the distributional similaritiesbetween the compounds and their constituents.In this vein, we systematically explore window-based and syntax-based contextual clues.
Since thetargets in our vector space models are all nouns(i.e., the compound nouns, the modifier nouns, andthe head nouns), our hypothesis is that adjectivesand verbs are expected to provide salient distri-butional properties, as adjective/verb meaning andnoun meaning are in a strong interdependent rela-tionship.
Even more, we expect adjectives and verbsthat are syntactically bound to the nouns under con-sideration (syntax-based, i.e., attributive adjectivesand subcategorising verbs) to outperform those that?just?
appear in the window contexts of the nouns(window-based).
In order to investigate this first1See Agirre et al(2009) and Bullinaria and Levy (2007;2012), among others, for systematic comparisons of co-occurrence features on various semantic relatedness tasks.255hypothesis, we compare window-based and syntax-based distributional features across parts-of-speech.Concerning a more specific aspect of compoundmeaning, we are interested in the contributions ofthe modifier noun versus head noun properties withregard to the meaning of the noun-noun compounds.While there has been prior psycholinguistic researchon the constituent contributions (e.g., Gagne?
andSpalding (2009; 2011)), computational linguisticshas not yet paid much attention to this issue, asfar as we know.
Our hypothesis is that the dis-tributional properties of the head constituents aremore salient than the distributional properties ofthe modifier constituents in predicting the degreeof compositionality of the compounds.
In order toassess this second hypothesis, we compare the vec-tor space similarities between the compounds andtheir modifier constituents with those of the com-pounds and their head constituents, with regard tothe overall most successful features.The paper is organised as follows.
Section 2 in-troduces the compound data that is relevant for thispaper, i.e., the noun-noun compounds and the com-positionality ratings.
Section 3 performs and dis-cusses the vector space experiments to explore ourhypotheses, and Section 4 describes related work.2 Data2.1 German Noun-Noun CompoundsCompounds are combinations of two or more sim-plex words.
Traditionally, a number of criteria (suchas compounds being syntactically inseparable, andthat compounds have a specific stress pattern) havebeen proposed, in order to establish a border be-tween compounds and non-compounds.
However,Lieber and Stekauer (2009a) demonstrated that noneof these tests are universally reliable to distinguishcompounds from other types of derived words.Compounds have thus been a recurrent focusof attention within theoretical, cognitive, and inthe last decade also within computational linguis-tics.
Recent evidence of this strong interest are theHandbook of Compounding (Lieber and Stekauer,2009b) on theoretical perspectives, and a series ofworkshops2 and special journal issues with respectto multi-word expressions (including various types2www.multiword.sourceforge.netof compounds) and the computational perspective(Journal of Computer Speech and Language, 2005;Language Resources and Evaluation, 2010; ACMTransactions on Speech and Language Processing,to appear).Our focus of interest is on German noun-nouncompounds (see Fleischer and Barz (2012) for a de-tailed overview and Klos (2011) for a recent de-tailed exploration), such as Ahornblatt ?maple leaf?,Feuerwerk ?fireworks?, and Obstkuchen ?fruit cake?where both the grammatical head (in German, thisis the rightmost constituent) and the modifier arenouns.
More specifically, we are interested in thedegrees of compositionality of German noun-nouncompounds, i.e., the semantic relatedness betweenthe meaning of a compound (e.g., Feuerwerk) andthe meanings of its constituents (e.g., Feuer ?fire?and Werk ?opus?
).Our work is based on a selection of noun com-pounds by von der Heide and Borgwaldt (2009),who created a set of 450 concrete, depictable Ger-man noun compounds according to four compo-sitionality classes: compounds that are transpar-ent with regard to both constituents (e.g., Ahorn-blatt ?maple leaf?
); compounds that are opaquewith regard to both constituents (e.g., Lo?wenzahn?lion+tooth ?
dandelion?
); compounds that aretransparent with regard to the modifier but opaquewith regard to the head (e.g., Feuerzeug ?fire+stuff?
lighter?
); and compounds that are opaque withregard to the modifier but transparent with regard tothe head (e.g., Fliegenpilz ?fly+mushroom ?
toad-stool?
).From the compound set by von der Heide andBorgwaldt, we disregarded noun compounds withmore than two constituents (in some cases, the mod-ifier or the head was complex itself) as well as com-pounds where the modifiers were not nouns.
Ourfinal set comprises a subset of their compounds in-cluding 244 two-part noun-noun compounds.2.2 Compositionality Ratingsvon der Heide and Borgwaldt (2009) collected hu-man ratings on compositionality for all their 450compounds.
The compounds were distributed over5 lists, and 270 participants judged the degree ofcompositionality of the compounds with respect totheir first as well as their second constituent, on256Compounds Mean Ratings and Standard Deviationswhole literal meanings of constituents whole modifier head mean rangeAhornblatt ?maple leaf?
maple leaf 6.03 ?
1.49 5.64 ?
1.63 5.71 ?
1.70(1) high/highPostbote ?post man?
mail messenger 6.33 ?
0.96 5.87 ?
1.55 5.10 ?
1.99Seezunge ?sole?
sea tongue 1.85 ?
1.28 3.57 ?
2.42 3.27 ?
2.32(2) mid/midWindlicht ?storm lamp?
wind light 3.52 ?
2.08 3.07 ?
2.12 4.27 ?
2.36Lo?wenzahn ?dandelion?
lion tooth 1.66 ?
1.54 2.10 ?
1.84 2.23 ?
1.92(3) low/lowMaulwurf ?mole?
mouth throw 1.58 ?
1.43 2.21 ?
1.68 2.76 ?
2.10Fliegenpilz ?toadstool?
fly/bow tie mushroom 2.00 ?
1.20 1.93 ?
1.28 6.55 ?
0.63(4) low/highFlohmarkt ?flea market?
flea market 2.31 ?
1.65 1.50 ?
1.22 6.03 ?
1.50Feuerzeug ?lighter?
fire stuff 4.58 ?
1.75 5.87 ?
1.01 1.90 ?
1.03(5) high/lowFleischwolf ?meat chopper?
meat wolf 1.70 ?
1.05 6.00 ?
1.44 1.90 ?
1.42Table 1: Examples of compound ratings.a scale between 1 (definitely opaque) and 7 (defi-nitely transparent).
For each compound?constituentpair, they collected judgements from 30 participants,and calculated the rating mean and the standard de-viation.
We refer to this set as our compound?constituent ratings.A second experiment collected human ratings oncompositionality for our subset of 244 noun-nouncompounds.
In this case, we asked the participantsto provide a unique score for each compound asa whole, again on a scale between 1 and 7.
Thecollection was performed via Amazon MechanicalTurk (AMT)3.
We randomly distributed our subsetof 244 compounds over 21 batches, with 12 com-pounds each, in random order.
In order to control forspammers, we also included two German fake com-pound nouns into each of the batches, in random po-sitions of the lists.
If participants did not recognisethe fake words, all of their ratings were rejected.
Wecollected between 27 and 34 ratings per target com-pound.
For each of the compounds we calculated therating mean and the standard deviation.
We refer tothis second set as our compound whole ratings.Table 1 presents example mean ratings for thecompound?constituent ratings as well as for thecompound whole ratings, accompanied by the stan-dard deviations.
We selected two examples eachfor five categories of mean ratings: the compound?constituent ratings were (1) high or (2) mid or (3)low with regard to both constituents; the compound?constituent ratings were (4) low with regard to themodifier but high with regard to the head; (5) viceversa.
Roller et al(2013) performed a thorough3www.mturk.comFigure 1: Distribution of compound ratings.analysis of the two sets of ratings, and assessed theirreliability from several perspectives.Figure 1 shows how the mean ratings for the com-pounds as a whole, for the compound?modifier pairsas well as for the compound?head pairs are dis-tributed over the range [1, 7]: For each set, we in-dependently sorted the 244 values and plotted them.The purpose of the figure is to illustrate that the rat-ings for our 244 noun-noun compounds are not par-ticularly skewed to any area within the range.4Figure 2 again shows the mean ratings for thecompounds as a whole as well as for the compound?constituent pairs, but in this case only the compoundwhole ratings were sorted, and the compound?constituent ratings were plotted against the com-pound whole ratings.
According to the plot, thecompound?modifier ratings (red) seem to correlatebetter with the compound whole ratings than thecompound?head ratings (yellow) do.
This intuitionwill be confirmed in Section 3.1.4The illustration idea was taken from Reddy et al(2011b).257Figure 2: Compounds ratings sorted by whole ratings.3 Vector Space Models (VSMs)The goal of our vector space models is to identifydistributional features that are salient to predict thedegree of compositionality of the compounds, by re-lying on the similarities between the compound andconstituent properties.In all our vector space experiments, we used co-occurrence frequency counts as induced from Ger-man web corpora, and calculated local mutual in-formation (LMI)5 values (Evert, 2005), to instantiatethe empirical properties of our target nouns with re-gard to the various corpus-based features.
LMI is ameasure from information theory that compares theobserved frequencies O with expected frequenciesE, taking marginal frequencies into account:LMI = O ?
log OE ,with E representing the product of the marginal fre-quencies over the sample size.6 In comparison to(pointwise) mutual information (Church and Hanks,1990), LMI improves the problem of propagatinglow-frequent events, by multiplying mutual infor-mation by the observed frequency.Relying on the LMI vector space models, the co-sine determined the distributional similarity betweenthe compounds and their constituents, which was inturn used to predict the compositionality betweenthe compound and the constituents, assuming thatthe stronger the distributional similarity (i.e., the co-sine values), the larger the degree of compositional-ity.5Alternatively, we also used the raw frequencies in all ex-periments below.
The insights into the various features wereidentical to those based on LMI, but the predictions were worse.6See http://www.collocations.de/AM/ for amore detailed illustration of association measures (incl.
LMI).The vector space predictions were evaluatedagainst the human ratings on the degree of compo-sitionality, using the Spearman Rank-Order Correla-tion Coefficient ?
(Siegel and Castellan, 1988).
The?
correlation is a non-parametric statistical test thatmeasures the association between two variables thatare ranked in two ordered series.
In Section 3.3 wewill compare the overall effect of the various fea-ture types and correlate all 488 compound?modifierand compound?head predictions against the ratingsat the same time; in Section 3.4 we will comparethe different effects of the features for compound?modifier pairs vs. compound?head pairs and thuscorrelate 244 predictions in both cases.After introducing a baseline and an upper boundfor our vector space experiments in Section 3.1 aswell as our web corpora in Section 3.2, Section 3.3presents window-based in comparison to syntax-based vector space models (distinguishing variouspart-of-speech features).
In Section 3.4 we then fo-cus on the contribution of modifiers vs. heads in thevector space models, with regard to the overall mostsuccessful features.3.1 Baseline and Upper BoundTable 2 presents the baseline and the upper boundvalues for the vector space experiments.
Thebaseline in the first two lines follows a proce-dure performed by Reddy et al(2011b), and re-lies on a random assignment of rating values [1, 7]to the compound?modifier and the compound?head pairs.
The 244 random values for thecompound?constituent pairs were then each corre-lated against the compound whole ratings.
Therandom compound?modifier ratings show a base-line correlation of ?
= 0.0959 with the compoundwhole ratings, and the random compound?head rat-ings show a baseline correlation of ?
= 0.1019 withthe compound whole ratings.The upper bound in the first two lines shows thecorrelations between the human ratings from the twoexperiments, i.e., between the 244 compound wholeratings and the respective compound?modifier andcompound?head ratings.
The compound?modifierratings exhibit a strong correlation with the com-pound whole ratings (?
= 0.6002), while the cor-relation between the compound?head ratings andthe compound whole ratings is not even moderate258Function?Baseline Upper Boundmodifier only .0959 .6002head only .1019 .1385addition .1168 .7687multiplication .1079 .7829Table 2: Baseline/Upper bound ?
correlations.(?
= 0.1385).
Obviously, the semantics of the mod-ifiers had a much stronger impact on the semanticjudgements of the compounds, thus confirming ourintuition from Section 2.2.The lower part of the table shows the respec-tive baseline and upper bound values when thecompound?modifier ratings and the compound?head ratings were combined by standard arith-metic operations, cf.
Widdows (2008) andMitchell and Lapata (2010), among others: thecompound?modifier and compound?head ratingswere treated as vectors, and the vector fea-tures (i.e., the compound?constituent ratings) wereadded/multiplied to predict the compound whole rat-ings.
As in the related work, the arithmetic op-erations strengthen the predictions, and multiplica-tion reached an upper bound of ?
= 0.7829, thusoutperforming not only the head-only but also themodifier-only upper bound.3.2 German Web CorporaMost of our experiments rely on the sdeWaC corpus(Faa?
et al 2010), a cleaned version of the Germanweb corpus deWaC created by the WaCky group (Ba-roni et al 2009).
The corpus cleaning had focusedmainly on removing duplicates from the deWaC, andon disregarding sentences that were syntactically ill-formed (relying on a parsability index provided by astandard dependency parser (Schiehlen, 2003)).
ThesdeWaC contains approx.
880 million words and canbe downloaded from http://wacky.sslmit.unibo.it/.While the sdeWaC is an attractive corpus choicebecause it is a web corpus with a reasonable size,and yet has been cleaned and parsed (so that wecan induce syntax-based distributional features), ithas one serious drawback for a window-based ap-proach (and, in general, for corpus work going be-yond the sentence border): The sentences in the cor-pus have been sorted alphabetically, so going be-yond the sentence border is likely to entering a sen-tence that did not originally precede or follow thesentence of interest.
So window co-occurrence inthe sdeWaC actually refers to x words to the left andright BUT within the same sentence.
Thus, enlarg-ing the window size does not effectively change theco-occurrence information any more at some point.For this reason, we additionally use WebKo, a pre-decessor version of the sdeWaC, which comprisesmore data (approx.
1.5 billion words in compari-son to 880 million words) and is not alphabeticallysorted, but is less clean and had not been parsed (be-cause it was not clean enough).3.3 Window-based vs. Syntax-based VSMsWindow-based Co-Occurrence When applyingwindow-based co-occurrence features to our vec-tor space models, we specified a corpus, a part-of-speech and a window size, and then determined theco-occurrence strengths of our compound nouns andtheir constituents with regard to the respective con-text words.
For example, when restricting the part-of-speech to adjectives and the window size to 5, wecounted how often our targets appeared with any ad-jectives in a window of five words to the left andto the right.
We looked at lemmas, and deletedany kind of sentence punctuation.
In general, wechecked windows of sizes 1, 2, 5, 10, and 20.
In onecase we extended the window up to 100 words.The window-based models compared the effectof varying the parts-of-speech of the co-occurringwords, motivated by the hypothesis that adjectivesand verbs were expected to provide salient distribu-tional properties.
So we checked which parts-of-speech provided specific insight into the distribu-tional similarity between nominal compounds andnominal constituents: We used common nouns vs.adjectives vs. main verbs that co-occurred with thetarget nouns in the corpora.
Figure 3 illustrates thebehaviour of the Spearman Rank-Order CorrelationCoefficient values ?
over the window sizes 1, 2, 5,10, and 20 within sdeWaC (sentence-internal) andWebKo (beyond sentence borders), when restrictingand combining the co-occurring parts-of-speech.
Itis clear from the figure that relying on nouns wasthe best choice, even better than combining nounswith adjectives and verbs.
The differences for nounsvs.
adjectives or verbs in the 20-word windows were259Figure 3: Window-based sdeWaC and WebKo ?
correlations across part-of-speech features.significant.7 Furthermore, the larger WebKo dataoutperformed the cleaned sdeWaC data, reaching anoptimal prediction of ?
= 0.6497.8 The corpus dif-ferences for NN and NN+ADJ+VV were significant.As none of the window lines had reached an op-timal correlation with a window size of 20 yet (i.e.,the correlation values were still increasing), we en-larged the window size up to 100 words, in orderto check on the most successful window size.
Werestricted the experiment to nominal features (withnouns representing the overall most successful fea-tures).
The correlations did not increase with largerwindows: the optimal prediction was still performedat a window size of 20.Syntax-based Co-Occurrence When applyingsyntax-based co-occurrence features to our vectorspace models, we relied only on the sdeWaC cor-pus because WebKo was not parsed and thus didnot provide syntactic information.
We specified asyntax-based feature type and then determined theco-occurrence strengths of our compounds and con-stituents with regard to the respective context words.In order to test our hypothesis that syntax-basedinformation is more salient than window-based in-formation to predict the compositionality of ourcompound nouns, we compared a number of po-tentially salient syntactic features for noun similar-ity: the syntactic functions of nouns in verb subcat-egorisation (intransitive and transitive subjects; di-rect and PP objects), and those categories that fre-7All significance tests in this paper were performed byFisher r-to-z transformation.8For a fair corpus comparison, we repeated the experimentswith WebKo on sentence-internal data.
It still outperformed thesdeWaC corpus.quently modify nouns or are modified by nouns (ad-jectives and prepositions).
With regard to subcate-gorisation functions, verbs subcategorising our tar-get nouns represented the dimensions in the vectorspace models.
For example, we used all verbs asvector dimensions that took our targets as direct ob-jects, and vector values were based on these syntac-tic co-occurrences.
For a noun like Buch ?book?,the strongest verb dimensions were lesen ?read?,schreiben ?write?, and kaufen ?buy?.
With regardto modification, we considered the adjectives andprepositions that modified our target nouns, as wellas the prepositions that were modified by our targetnouns.
For the noun Buch, strong modifying adjec-tive dimensions were neu ?new?, erschienen ?pub-lished?, and heilig ?holy?
; strong modifying prepo-sition dimensions were in ?in?, mit ?with?, and zu?on?
; and strong modified preposition dimensionswere von ?by?, u?ber ?about?, and fu?r ?for?.Figure 4 demonstrates that the potentially salientsyntactic functions had different effects on predict-ing compositionality.
The top part of the figureshows the modification-based correlations, the mid-dle part shows the subcategorisation-based corre-lations, and at the bottom of the figure we repeatthe ?
correlation values for window-based adjec-tives and verbs (within a window of 20 words)from the sdeWaC.
The syntax-based predictions bymodification and subcategorisation were all signif-icantly worse than the predictions by the respec-tive window-based parts-of-speech.
Furthermore,the figure shows that there are strong differenceswith regard to the types of syntactic functions,when predicting compositionality: Relying on ourtarget nouns as transitive subjects of verbs is al-260Figure 4: Syntax-based correlations.most useless (?
= 0.1194); using the intransi-tive subject function improves the prediction (?
=0.2121); interestingly, when abstracting over subject(in)transitivity, i.e., when we use all verbs as vectorspace features that appeared with our target nouns assubjects ?independently whether this was an intran-sitive or a transitive subject?
was again more suc-cessful (?
= 0.2749).
Relying on our noun targets asdirect objects is again slightly better (?
= 0.2988);as pp objects it is again slightly worse (?
= 0.2485).None of these differences were significant, though.Last but not least, we concatenated all syntax-based features to a large syntactic VSM (and wealso considered variations of syntax-based featureset concatenations), but the results of any unifiedcombinations were clearly below the best individ-ual predictions.
So the best syntax-based predic-tors were adjectives that modified our compound andconstituent nouns, with ?
= 0.3455, which how-ever just (non-significantly) outperformed the bestadjective setting in our window-based vector space(?
= 0.3394).
Modification by prepositions didnot provide salient distributional information, with?
= 0.2044/0.1725 relying on modifying/modifiedprepositions.In sum, attributive adjectives and verbs that sub-categorised our target nouns as direct objects werethe most salient syntax-based distributional fea-tures but nevertheless predicted worse than ?just?window-based adjectives and verbs, respectively.3.4 Role of Modifiers vs. HeadsThis section tests our hypothesis that the distribu-tional properties of the head constituents are moresalient than the distributional properties of the mod-ifier constituents in predicting the degree of compo-sitionality of the compounds.
Our rating data enablesus to explore the modifier/head distinction with re-gard to two perspectives.Perspective (i): Salient Features for Compound?Modifier vs.
Compound?Head Pairs Instead ofcorrelating all 488 compound?constituent predic-tions against the ratings, we distinguished betweenthe 244 compound?modifier predictions and the 244compound?head predictions.
This perspective al-lowed us to distinguish between the salience of thevarious feature types with regard to the semanticrelatedness between compound?modifier pairs vs.compound?head pairs.Figure 5 presents the correlation values whenpredicting the degrees of compositionality ofcompound?modifier (M in the left panel) vs.compound?head (H in the right panel) pairs, asbased on the window features and the various parts-of-speech.
The prediction of the parts-of-speech isNN > NN+ADJ+VV > VV > ADJand ?with few exceptions?
the predictions are im-proving with increasing window sizes, as the over-all predictions in the previous section did.
Butwhile in smaller window sizes the predictions ofthe compound?head ratings are better than those ofthe compound?modifier ratings, this difference van-ishes with larger windows.
With regard to a win-dow size of 20 there is no significant difference be-tween predicting the semantic relatedness betweencompound?modifier vs. compound?head pairs.When using the syntactic features to predict thedegrees of compositionality of compound?modifiervs.
head?compound pairs, in all but one of thesyntactic feature types the verb subcategorisation aswell as the modification functions allowed a strongerprediction of compound?head ratings in compari-son to compound?modifier ratings.
The only syn-tactic feature that was significantly better to predictcompound?modifier ratings was relying on transi-tive subjects.
In sum, the predictions based on syn-tactic features in most but not all cases behaved inaccordance with our hypothesis.As in our original experiments in Section 3.3,the syntax-based features were significantly outper-formed by the window-based features.
The syn-tactic features reached an optimum of ?
= 0.2224and ?
= 0.3502 for predicting modifier?compound261Figure 5: Window-based correlations (modifiers vs. heads).vs.
head?compound degrees of compositionality (inboth cases relying on attributive adjectives), in com-parison to ?
= 0.5698 and ?
= 0.5745 when relyingon nouns in a window of 20 words.Perspective (ii): Contribution of Modifiers/Headsto Compound Meaning This final analysis ex-plores the contributions of the modifiers and of theheads with regard to the compound meaning, by cor-relating only one type of compound?constituent pre-dictions with the compound whole ratings.
I.e., wepredicted the compositionality of the compound bythe distributional similarity between the compoundand only one of its constituents, checking if themeaning of the compound is determined more by themeaning of the modifier or the head.
This analysis isin accordance with the upper bound in Section 3.1,where the compound?constituent ratings were cor-related with the compound whole ratings.Figure 6 presents the correlation values whendetermining the compound whole ratings byonly compound?modifier predictions, or onlycompound?head predictions, or by adding or multi-plying the modifier and head predictions.
The under-lying features rely on a 20-word window (adjectives,verbs, nouns, and across parts-of-speech).
It is strik-ing that in three out of four cases the predictions ofthe compound whole ratings were performed simi-larly well (i) by only the compound?modifier pre-dictions, and (ii) by multiplying the compound?modifier and the compound?head predictions.
So,as in the calculation of the upper bound, the dis-tributional semantics of the modifiers had a muchstronger impact on the semantics of the compoundthan the distributional semantics of the heads did.Figure 6: Predicting the compound whole ratings.3.5 DiscussionThe vector space models explored two hypothesesto predict the compositionality of German noun-noun compounds by distributional features.
Re-garding hypothesis 1, we demonstrated that ?againstour intuitions?
not adjectives or verbs whose mean-ings are strongly interdependent with the mean-ings of nouns provided the most salient distribu-tional information, but that relying on nouns wasthe best choice, in combination with a 20-word win-dow, reaching state-of-the-art ?
= 0.6497.
Thelarger but less clean web corpus WebKo outper-formed the smaller but cleaner successor sdeWaC.Furthermore, the syntax-based predictions by adjec-tive/preposition modification and by verb subcate-gorisation (as well as various concatenations of syn-tactic VSMs) were all worse than the predictions bythe respective window-based parts-of-speech.Regarding hypothesis 2, we distinguished thecontributions of modifiers vs. heads to the com-pound meaning from two perspectives.
(i) The pre-dictions of the compound?modifier vs. compound?262head ratings did not differ significantly when us-ing features from increasing window sizes, but withsmall window sizes the compound?head ratingswere predicted better than the compound?modifierratings.
This insight fits well to the stronger im-pact of syntax-based features on compound?headin comparison to compound?modifier predictionsbecause ?even though German is a language withcomparably free word order?
we can expect manysyntax-based features (especially attributive adjec-tives and prepositions) to appear in close vicinityto the nouns they depend on or subcategorise.
Weconclude that the features that are salient to predictsimilarities between the compound?modifier vs. thecompound?head pairs are different, and that basedon small windows the distributional similarity be-tween compounds and heads is stronger than be-tween compounds and modifiers, but based on largercontexts this difference vanishes.
(ii) With regardto the overall meaning of the compound, the influ-ence of the modifiers was not only much strongerin the human ratings (cf.
Section 2) and in the up-per bound (cf.
Section 3.1), but also in the vectorspace models (cf.
Figure 6).
While this insight con-tradicts our second hypothesis (that the head proper-ties are more salient than the modifier properties inpredicting the compositionality of the compound),it fits into a larger picture that has primarily beendiscussed in psycholinguistic research on compoundmeaning, where various factors such as the semanticrelation between the modifier and the head (Gagne?and Spalding, 2009) and the modifier properties, in-ferential processing and world knowledge (Gagne?and Spalding, 2011) were taken into account.
How-ever, also in psycholinguistic studies that explorethe semantic role of modifiers and heads in nouncompounds there is no agreement about which con-stituent properties are inherited by the compound.4 Related WorkMost computational approaches to model the mean-ing or compositionality of compounds have beenperformed for English, including work on parti-cle verbs (McCarthy et al 2003; Bannard, 2005;Cook and Stevenson, 2006); adjective-noun com-binations (Baroni and Zamparelli, 2010; Boleda etal., 2013); and noun-noun compounds (Reddy etal., 2011b; Reddy et al 2011a).
Most closely re-lated to our work is Reddy et al(2011b), whorelied on window-based distributional models topredict the compositionality of English noun-nouncompounds.
Their gold standard also comprisedcompound?constituent ratings as well as compoundwhole ratings, but the resources had been cleanedmore extensively, and they reached ?
= 0.714.Concerning vector space explorations and seman-tic relatedness in more general terms, Bullinaria andLevy (2007; 2012) also systematically assessed arange of factors in VSMs (corpus type and size,window size, association measures, and corpus pre-processing, among others) against four semantictasks, however not including compositionality rat-ings.
Similarly, Agirre et al(2009) compared andcombined a WordNet-based and various distribu-tional models to predict the pair similarity of the 65Rubenstein and Goodenough word pairs and the 353word pairs in WordSim353.
They varied windowsizes, dependency relations and raw words in themodels.
On WordSim353, they reached ?
= 0.66,which is slightly better than our best result, but atthe same time the dataset is smaller.Concerning computational models of Germancompounds, there is not much previous work.
Ourown work (Schulte im Walde, 2005; Ku?hner andSchulte im Walde, 2010) has addressed the degreesof compositionality of German particle verbs.
Zins-meister and Heid (2004) are most closely related toour current study.
They suggested a distributionalmodel to identify lexicalised German noun com-pounds by comparing the verbs that subcategorisethe noun compound with those that subcategorisethe head noun as direct objects.5 ConclusionThis paper presented experiments to predict thecompositionality of German noun-noun compounds.Our overall best result is state-of-the-art, reachingSpearman?s ?
= 0.65 with a word-space model ofnominal features from a 20-word window of a 1.5billion word web corpus.
Our experiments demon-strated that (1) window-based features outperformedsyntax-based features, and nouns outperformed ad-jectives and verbs; (2) the modifier properties pre-dominantly influenced the compositionality.263ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pasca, and Aitor Soroa.
2009.
AStudy on Similarity and Relatedness Using Distribu-tional and WordNet-based Approaches.
In Proceed-ings of the North American Chapter of the Associationfor Computational Linguistics and Human LanguageTechnologies Conference, pages 19?27, Boulder, Col-orado.Collin Bannard.
2005.
Learning about the Meaning ofVerb?Particle Constructions from Corpora.
ComputerSpeech and Language, 19:467?478.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare Vectors, Adjectives are Matrices: Represent-ing Adjective-Noun Constructions in Semantic Space.In Proceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages1183?1193, Cambridge, MA, October.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The WaCky Wide Web:A Collection of Very Large Linguistically ProcessedWeb-Crawled Corpora.
Language Resources andEvaluation, 43(3):209?226.Gemma Boleda, Marco Baroni, Nghia The Pham, andLouise McNally.
2013.
On Adjective-Noun Compo-sition in Distributional Semantics.
In Proceedings ofthe 10th International Conference on ComputationalSemantics, Potsdam, Germany.John A. Bullinaria and Joseph P. Levy.
2007.
ExtractingSemantic Representations from Word Co-OccurrenceStatistics: A Computational Study.
Behavior ResearchMethods, 39(3):510?526.John A. Bullinaria and Joseph P. Levy.
2012.
ExtractingSemantic Representations from Word Co-OccurrenceStatistics: Stop-Lists, Stemming, and SVD.
BehaviorResearch Methods, 44:890?907.Kenneth W. Church and Patrick Hanks.
1990.
Word As-sociation Norms, Mutual Information, and Lexicogra-phy.
Computational Linguistics, 16(1):22?29.Paul Cook and Suzanne Stevenson.
2006.
ClassifyingParticle Semantics in English Verb-Particle Construc-tions.
In Proceedings of the ACL/COLING Workshopon Multiword Expressions: Identifying and ExploitingUnderlying Properties, Sydney, Australia.Katrin Erk.
2012.
Vector Space Models of Word Mean-ing and Phrase Meaning: A Survey.
Language andLinguistics Compass, 6(10):635?653.Stefan Evert.
2005.
The Statistics of Word Co-Occurrences: Word Pairs and Collocations.
Ph.D.thesis, Institut fu?r Maschinelle Sprachverarbeitung,Universita?t Stuttgart.Gertrud Faa?
Ulrich Heid, and Helmut Schmid.
2010.Design and Application of a Gold Standard for Mor-phological Analysis: SMOR in Validation.
In Pro-ceedings of the 7th International Conference on Lan-guage Resources and Evaluation, pages 803?810, Val-letta, Malta.John R. Firth.
1957.
Papers in Linguistics 1934-51.Longmans, London, UK.Wolfgang Fleischer and Irmhild Barz.
2012.
Wortbil-dung der deutschen Gegenwartssprache.
de Gruyter.Christina L. Gagne?
and Thomas L. Spalding.
2009.Constituent Integration during the Processing of Com-pound Words: Does it involve the Use of RelationalStructures?
Journal of Memory and Language, 60:20?35.Christina L. Gagne?
and Thomas L. Spalding.
2011.
In-ferential Processing and Meta-Knowledge as the Basesfor Property Inclusion in Combined Concepts.
Journalof Memory and Language, 65:176?192.Zellig Harris.
1968.
Distributional Structure.
In Jerold J.Katz, editor, The Philosophy of Linguistics, OxfordReadings in Philosophy, pages 26?47.
Oxford Univer-sity Press.Verena Klos.
2011.
Komposition und Kompositionalita?t.Number 292 in Reihe Germanistische Linguistik.
Wal-ter de Gruyter, Berlin.Natalie Ku?hner and Sabine Schulte im Walde.
2010.
De-termining the Degree of Compositionality of GermanParticle Verbs by Clustering Approaches.
In Proceed-ings of the 10th Conference on Natural Language Pro-cessing, pages 47?56, Saarbru?cken, Germany.Rochelle Lieber and Pavol Stekauer.
2009a.
Intro-duction: Status and Definition of Compounding.
InThe Oxford Handbook on Compounding (Lieber andStekauer, 2009b), chapter 1, pages 3?18.Rochelle Lieber and Pavol Stekauer, editors.
2009b.
TheOxford Handbook of Compounding.
Oxford Univer-sity Press.Kevin Lund and Curt Burgess.
1996.
ProducingHigh-Dimensional Semantic Spaces from Lexical Co-Occurrence.
Behavior Research Methods, Instru-ments, and Computers, 28(2):203?208.Diana McCarthy, Bill Keller, and John Carroll.
2003.Detecting a Continuum of Compositionality in PhrasalVerbs.
In Proceedings of the ACL-SIGLEX Workshopon Multiword Expressions: Analysis, Acquisition andTreatment, Sapporo, Japan.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin Distributional Models of Semantics.
Cognitive Sci-ence, 34:1388?1429.Siva Reddy, Ioannis P. Klapaftis, Diana McCarthy, andSuresh Manandhar.
2011a.
Dynamic and Static Pro-totype Vectors for Semantic Composition.
In Pro-ceedings of the 5th International Joint Conference on264Natural Language Processing, pages 705?713, ChiangMai, Thailand.Siva Reddy, Diana McCarthy, and Suresh Manandhar.2011b.
An Empirical Study on Compositionality inCompound Nouns.
In Proceedings of the 5th Interna-tional Joint Conference on Natural Language Process-ing, pages 210?218, Chiang Mai, Thailand.Stephen Roller, Sabine Schulte im Walde, and SilkeScheible.
2013.
The (Un)expected Effects of Apply-ing Standard Cleansing Models to Human Ratings onCompositionality.
In Proceedings of the 9th Workshopon Multiword Expressions, Atlanta, GA.Michael Schiehlen.
2003.
A Cascaded Finite-StateParser for German.
In Proceedings of the 10th Con-ference of the European Chapter of the Association forComputational Linguistics, pages 163?166, Budapest,Hungary.Sabine Schulte im Walde.
2005.
Exploring Features toIdentify Semantic Nearest Neighbours: A Case Studyon German Particle Verbs.
In Proceedings of the In-ternational Conference on Recent Advances in NaturalLanguage Processing, pages 608?614, Borovets, Bul-garia.Hinrich Schu?tze.
1992.
Dimensions of Meaning.
In Pro-ceedings of Supercomputing, pages 787?796.Sidney Siegel and N. John Castellan.
1988.
Non-parametric Statistics for the Behavioral Sciences.McGraw-Hill, Boston, MA.Peter D. Turney and Patrick Pantel.
2010.
From Fre-quency to Meaning: Vector Space Models of Se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Claudia von der Heide and Susanne Borgwaldt.
2009.Assoziationen zu Unter-, Basis- und Oberbegriffen.Eine explorative Studie.
In Proceedings of the 9thNorddeutsches Linguistisches Kolloquium, pages 51?74.Dominic Widdows.
2008.
Semantic Vector Products:Some Initial Investigations.
In Proceedings of the 2ndConference on Quantum Interaction, Oxford, UK.Heike Zinsmeister and Ulrich Heid.
2004.
Collocationsof Complex Nouns: Evidence for Lexicalisation.
InProceedings of Konvens, Vienna, Austria.265
