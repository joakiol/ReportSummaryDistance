Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 154?159,Dublin, Ireland, August 23-24, 2014.BUAP: Polarity Classification of Short TextsDavid Pinto1, Darnes Vilarin?o1, Saul Leo?n1, Miguel Jasso1,2, Cupertino Lucero21 Beneme?rita Universidad Auto?noma de Puebla14 Sur y Av.
San Claudio, CU, 72570Puebla, Puebla, Me?xico{dpinto,darnes,saul.leon}@cs.buap.mx2 Universidad Tecnolo?gica de Izu?car de MatamorosProlongacio?n Reforma 168, Santiago Mihuacan, 74420Izu?car de Matamoros, Puebla, Me?xicomigueljhdz18@yahoo.com.mx, cuper lucero@hotmail.comAbstractWe report the results we obtained at the sub-task B (Message Polarity Classification) of Se-mEval 2014 Task 9.
The features used forrepresenting the messages were basically tri-grams of characters, trigrams of PoS and anumber of words selected by means of a graphmining tool.
Our approach performed slightlybelow the overall average, except when a cor-pus of tweets with sarcasm was evaluated,in which we performed quite well obtainingaround 6% above the overall average.1 IntroductionAnalyzing polarity in texts is an important task thatmay have various applications in real life.
There ex-ist plenty of tasks that may be benefited of computa-tional procedures that automatically allow to detectif the author intention has been to express himself asa positive, negative, neutral or objective manner.
Letus consider, for instance, when a public figure (suchas a politician, celebrity, or business leader) wouldlike to investigate its reputation in public media.
An-other example would be to calculate the reputationof a public or private institution.
In any case, theconstruction of methods for determining the polar-ity of messages at Internet would help to investigatetheir reputation.In this paper, we present the results we obtainedwhen we carried out experiments for the subtask BThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/of Semeval 2014 Task 9, which was named ?Mes-sage Polarity Classification?, and was defined as fol-low: ?Given a message, decide whether the mes-sage is of positive, negative, or neutral sentiment.For messages conveying both a positive and nega-tive sentiment, whichever is the stronger sentimentshould be chosen?.The remaining of this paper is structured as fol-lows.
In Section 2 we present some related workfound at the literature with respect to the identifica-tion of emotions in short texts such as twitter.
Sec-tion 3 presents the description of the features andclassification model used in our experiments.
Theresults obtained together with a discussion of theseresults are given in Section 4.
Finally, the conclu-sions are given in Section 5.2 Related WorkThere exist a number of works in literature associ-ated to the automatic identification of emotions inTwitter, mainly due to the massification of this so-cial network around the world and the easy mannerwe can access to the Tweets from API?s provided byTwitter itself.
Some of these works have focused onthe contribution of some particular features, such asPart of Speech (PoS) tags, emoticons, etc.
on theaforementioned task.
In Agarwal et al.
(2011), forexample, the a priori likelihood of each PoS is cal-culated.
They use up to 100 additional features thatinclude emoticons and a dictionary of positive andnegative words.
They have reported a 60% of ac-curacy in the task.
On the other hand, in Mukher-jee and Bhattacharyya (2012), a strategy based ondiscursive relations, such as conectiveness and con-154ditionals, with low number of lexical resources isproposed.
These relations are integrated in classi-cal models of representation like bag of words withthe aim of improving the accuracy values obtainedin the process of classification.
The influence of se-mantic operators such as modals and negations areanalyzed, in particular, the degree in which they af-fect the emotion present in a given paragraph or sen-tence.One of the major advances obtained in the taskof sentiment analysis has been done in the frame-work of the SemEval competition.
In 2013, severalteams have participated with different approachesBecker et al.
(2013); Han et al.
(2013); Chawla et al.
(2013); Balahur and Turchi (2013); Balage Filhoand Pardo (2013); Moreira et al.
(2013); Reckmanet al.
(2013); Tiantian et al.
(2013); Marchand et al.
(2013); Clark and Wicentwoski (2013); Hamdanet al.
(2013); Mart?
?nez-Ca?mara et al.
(2013); Lev-allois (2013).
Most of these works have contributedin the mentioned task by proposing methods, tech-niques for representing and classifying documentstowards the automatic classification of sentiment inTweets.3 Description of the Presented ApproachWe have employed a supervised approach based onmachine learning in which we construct a classifica-tion model using the following general features ob-tained from the training corpus.1.
Character trigrams2.
PoS tags trigrams3.
Significant Tweet words obtained by using agraph mining tool known as SubDueThe description of how we calculated each featurein order to construct a representation vector for eachmessage is given as follows.The probability of each character trigram giventhe polarity class, P (trigram|class), was cal-culated in the training corpus.
Thereafter, weassigned a normalized probability to each sen-tence polarity by combining the probability ofeach character trigram of the sentence, i.e.,?|message|i=1log [P (trigrami|class)].
Since wehave four classes (?positive?,?negative?,?neutral?and ?objective?
), we have obtained four features forthe final vectorial representation of the message.We then calculated other four features by per-forming a similar calculation than the previous one,but in this case, using the PoS tags of the message.For this purpose, we used the Twitter NLP and Part-of-Speech Tagging tool provided by the CarnegieMellon University (Owoputi et al., 2013).
Since thePoS tag given by this tool is basically a character,then the same procedure can be applied.We performed preliminary experiments by usingthese eight features on a trial corpus, and we ob-served that the results may be improved by select-ing significant words that may not be discoveredby the statistical techniques used until now.
So,we decided to make use of techniques based ongraph mining for attempting to find those signifi-cant words.
In order to find them, we constructed agraph representation for each message class (?pos-itive?,?negative?,?neutral?
and ?objective?
), usingthe training corpus.
The manner we constructedthose graphs is shown as follows.Formally, given a graph G = (V,E,L, f) with Vbeing the non-empty set of vertices, E ?
V ?V theedges, L the tag set, and f : E ?
L, a functionthat assigns a tag to a pair of associated vertices.This graph-based representation attempt to capturethe sequence among the sentence words, so as thesequence among their PoS tags with the aim of feed-ing a graph mining tool which may extract relevantfeatures that may be further used for representing thetexts.
Thus, the set V is constructed from the differ-ent words and PoS of the target document.In order to demonstrate the way we construct thegraph for each short text, consider the followingmessage: ?ooh i love you for posting this :-)?.
Theassociated graph representation to this message isshown in Figure 1.Once each paragraph is represented by means ofa graph, we apply a data mining algorithm in or-der to find subgraphs from which we will be ableto find the significant words which will be, in ourcase, basically, the nodes of these subgraphs.
Sub-due is a data mining tool widely used in structureddomains.
This tool has been used for discoveringstructured patterns in texts represented by means ofgraphs Olmos et al.
(2005).
Subdue uses an eval-uation model named ?Minimum encoding?, a tech-155Figure 1: Graph based message representation with words and their corresponding PoS tagsnique derived from the minimum description lengthprinciple Rissanen (1989), in which t he best graphsub-structures are chosen.
The best subgraphs arethose that minimize the number of bits that repre-sent the graph.
In this case, the number of bits iscalculated consi dering the size of the graph adjan-cency matrix.
Thus, the best substructure is the onethat minimizes I(S) + I(G|S), where I(S) is thenumber of bits required to describe the sub structureS, and I(G|S) is the number of bits required to de-scribe graph G after it has been compacted by thesubstructure S.By applying this procedure we obtained 597 sig-nicant negative words, 445 positive words, 616 ob-jective words and 925 positive words.
For the finalrepresentation vector we compiled the union of thesewords, obtaining 1915 significant words.
Therefore,the total number of features for each message was1,923.We have used the training corpus provided at thecompetition (Rosenthal et al., 2014), however, weremoved all those messsages tagged as the class?objective-OR-neutral?, because all these messagesintroduced noise to the classification process.
In to-tal, we constructed 5,217 vectors of message repre-sentation which fed a support vector machine classi-fier.
We have used the SVM implementation of theWEKA tool with default parameters for our exper-iments (Hall et al., 2009).
The obtained results areshown in the next section.4 Experimental ResultsThe test corpus was made up short texts (mes-sages) categorized as: ?LiveJournal2014?,?SMS2013?, ?Twitter2013?, ?Twitter2014?
and?Twitter2014Sarcasm?.
A complete description ofthe training and test datasets can be found at thetask description paper (Rosenthal et al., 2014).In Table 1 we can see the results obtained at thecompetition.
Our approach performed in almost allthe cases slightly below to the overall average, ex-cept when we processed the corpus of Twitter withSarcasm characteristics.
We consider that two mainproblems were the cause of this result: 1) The corpuswas very unbalanced and our approaches for allevi-ating this problem were not sufficient, and 2) Fromour particular point of view, there were a high differ-ence between the vocabulary of the training and thetest corpus, thus, leading the classification model tofail.156Table 1: Results obtained at the substask B of the Semeval 2014 Task 9System LiveJournal2014 SMS2013 Twitter2013 Twitter2014 Twitter2014Sarcasm AverageNRC-Canada-B 74.84 70.28 70.75 69.85 58.16 68.78CISUC KIS-B-late 74.46 65.90 67.56 67.95 55.49 66.27coooolll-B 72.90 67.68 70.40 70.14 46.66 65.56TeamX-B 69.44 57.36 72.12 70.96 56.50 65.28RTRGO-B 72.20 67.51 69.10 69.95 47.09 65.17AUEB-B 70.75 64.32 63.92 66.38 56.16 64.31SWISS-CHOCOLATE-B 73.25 66.43 64.81 67.54 49.46 64.30SentiKLUE-B 73.99 67.40 69.06 67.02 43.36 64.17TUGAS-B 69.79 62.77 65.64 69.00 52.87 64.01SAIL-B 69.34 56.98 66.80 67.77 57.26 63.63senti.ue-B 71.39 59.34 67.34 63.81 55.31 63.44Synalp-Empathic-B 71.75 62.54 63.65 67.43 51.06 63.29Lt 3-B 68.56 64.78 65.56 65.47 47.76 62.43UKPDIPF-B 71.92 60.56 60.65 63.77 54.59 62.30AMI ERIC-B 65.32 60.29 70.09 66.55 48.19 62.09ECNU-B 69.44 59.75 62.31 63.17 51.43 61.22LyS-B 69.79 60.45 66.92 64.92 42.40 60.90SU-FMI-B-late 68.24 61.67 60.96 63.62 48.34 60.57NILC USP-B-twitter 69.02 61.35 65.39 63.94 42.06 60.35CMU-Qatar-B-late 65.63 62.95 65.11 65.53 40.52 59.95columbia nlp-B 68.79 59.84 64.60 65.42 40.02 59.73CMUQ-Hybrid-B-late 65.14 61.75 63.22 62.71 40.95 58.75Citius-B 62.40 57.69 62.53 61.92 41.00 57.11KUNLPLab-B 63.77 55.89 58.12 61.72 44.60 56.82USP Biocom-B 67.80 53.57 58.05 59.21 43.56 56.44UPV-ELiRF-B 64.11 55.36 63.97 59.33 37.46 56.05Rapanakis-B 59.71 54.02 58.52 63.01 44.69 55.99DejaVu-B 64.69 55.57 57.43 57.02 42.46 55.43GPLSI-B 57.32 46.63 57.49 56.06 53.90 54.28Indian Inst of Tech-Patna-B 60.39 51.96 52.58 57.25 41.33 52.70BUAP-B 53.94 44.27 56.85 55.76 51.52 52.47SAP-RI-B 57.86 49.00 50.18 55.47 48.64 52.23UMCC DLSI Sem 53.12 50.01 51.96 55.40 42.76 50.65Alberta-B 52.38 49.05 53.85 52.06 40.40 49.55SINAI-B 58.33 57.34 50.59 49.50 31.15 49.38IBM EG-B 59.24 46.62 54.51 52.26 34.14 49.35SU-sentilab-B-tweet 55.11 49.60 50.17 49.52 31.49 47.18lsis lif-B 61.09 38.56 46.38 52.02 34.64 46.54IITPatna-B 54.68 40.56 50.32 48.22 36.73 46.10UMCC DLSI Graph-B 47.81 36.66 43.24 45.49 53.15 45.27University-of-Warwick-B 39.60 29.50 39.17 45.56 39.77 38.72DAEDALUS-B 40.83 40.86 36.57 33.03 28.96 36.05Overall average 63.81 55.82 59.72 60.30 45.43 57.025 ConclusionsWe have presented an approach for detecting mes-sage polarity using basically three kind of features:character trigrams, PoS tags trigrams and significantwords obtained by means of a graph mining tool.The obtained results show that these features werenot sufficient for detecting the correct polarity of agiven message with high precision.
We consider thatthe unbalanced characteristic and the fact the vocab-ulary changed significantly from the training to thetest corpus influenced the results we obtained at thecompetition.
However, a deep analysis we plan todo to the datasets evaluated will allow us in the fu-ture to find more accurate features for the messagepolarity detection task.ReferencesApoorv Agarwal, Boyi Xie, Ilia Vovsha, OwenRambow, and Rebecca Passonneau.
Sentimentanalysis of twitter data.
In Proceedings of theWorkshop on Language in Social Media (LSM2011), pages 30?38, Portland, Oregon, June 2011.Pedro Balage Filho and Thiago Pardo.
Nilc usp:A hybrid system for sentiment analysis in twittermessages.
In Second Joint Conference on Lexicaland Computational Semantics (*SEM), Volume 2:157Proceedings of the Seventh International Work-shop on Semantic Evaluation (SemEval 2013),pages 568?572, Atlanta, Georgia, USA, June2013.Alexandra Balahur and Marco Turchi.
Improvingsentiment analysis in twitter using multilingualmachine translated data.
In Proceedings of the In-ternational Conference Recent Advances in Natu-ral Language Processing RANLP 2013, pages 49?55, Hissar, Bulgaria, September 2013.
INCOMALtd.
Shoumen, BULGARIA.Lee Becker, George Erhart, David Skiba, and Valen-tine Matula.
Avaya: Sentiment analysis on twitterwith self-training and polarity lexicon expansion.In Second Joint Conference on Lexical and Com-putational Semantics (*SEM), Volume 2: Pro-ceedings of the Seventh International Workshopon Semantic Evaluation (SemEval 2013), pages333?340, Atlanta, Georgia, USA, June 2013.Karan Chawla, Ankit Ramteke, and Pushpak Bhat-tacharyya.
Iitb-sentiment-analysts: Participationin sentiment analysis in twitter semeval 2013 task.In Second Joint Conference on Lexical and Com-putational Semantics (*SEM), Volume 2: Pro-ceedings of the Seventh International Workshopon Semantic Evaluation (SemEval 2013), pages495?500, Atlanta, Georgia, USA, June 2013.Sam Clark and Rich Wicentwoski.
Swatcs: Combin-ing simple classifiers with estimated accuracy.
InSecond Joint Conference on Lexical and Compu-tational Semantics (*SEM), Volume 2: Proceed-ings of the Seventh International Workshop on Se-mantic Evaluation (SemEval 2013), pages 425?429, Atlanta, Georgia, USA, June 2013.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Wit-ten.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18, November2009.
ISSN 1931-0145.Hussam Hamdan, Frederic Be?chet, and Patrice Bel-lot.
Experiments with dbpedia, wordnet and sen-tiwordnet as resources for sentiment analysis inmicro-blogging.
In Second Joint Conference onLexical and Computational Semantics (*SEM),Volume 2: Proceedings of the Seventh Inter-national Workshop on Semantic Evaluation (Se-mEval 2013), pages 455?459, Atlanta, Georgia,USA, June 2013.Qi Han, Junfei Guo, and Hinrich Schuetze.
Codex:Combining an svm classifier and character n-gram language models for sentiment analysis ontwitter text.
In Second Joint Conference onLexical and Computational Semantics (*SEM),Volume 2: Proceedings of the Seventh Inter-national Workshop on Semantic Evaluation (Se-mEval 2013), pages 520?524, Atlanta, Georgia,USA, June 2013.Clement Levallois.
Umigon: sentiment analysis fortweets based on terms lists and heuristics.
In Sec-ond Joint Conference on Lexical and Computa-tional Semantics (*SEM), Volume 2: Proceedingsof the Seventh International Workshop on Seman-tic Evaluation (SemEval 2013), pages 414?417,Atlanta, Georgia, USA, June 2013.Morgane Marchand, Alexandru Ginsca, RomaricBesanc?on, and Olivier Mesnard.
[lvic-limsi]: Us-ing syntactic features and multi-polarity words forsentiment analysis in twitter.
In Second Joint Con-ference on Lexical and Computational Semantics(*SEM), Volume 2: Proceedings of the SeventhInternational Workshop on Semantic Evaluation(SemEval 2013), pages 418?424, Atlanta, Geor-gia, USA, June 2013.Eugenio Mart?
?nez-Ca?mara, Arturo Montejo-Ra?ez,M.
Teresa Mart?
?n-Valdivia, and L. Alfonso Uren?aLo?pez.
Sinai: Machine learning and emotion ofthe crowd for sentiment analysis in microblogs.
InSecond Joint Conference on Lexical and Compu-tational Semantics (*SEM), Volume 2: Proceed-ings of the Seventh International Workshop on Se-mantic Evaluation (SemEval 2013), pages 402?407, Atlanta, Georgia, USA, June 2013.Silvio Moreira, Joa?o Filgueiras, Bruno Martins,Francisco Couto, and Ma?rio J. Silva.
Reac-tion: A naive machine learning approach for sen-timent classification.
In Second Joint Confer-ence on Lexical and Computational Semantics(*SEM), Volume 2: Proceedings of the SeventhInternational Workshop on Semantic Evaluation(SemEval 2013), pages 490?494, Atlanta, Geor-gia, USA, June 2013.Subhabrata Mukherjee and Pushpak Bhattacharyya.158Sentiment analysis in Twitter with lightweightdiscourse analysis.
In Proceedings of COLING2012, pages 1847?1864, Mumbai, India, Decem-ber 2012.
The COLING 2012 Organizing Com-mittee.Ivan Olmos, Jesus A. Gonzalez, and Mauricio Os-orio.
Subgraph isomorphism detection using acode based representation.
In FLAIRS Confer-ence, pages 474?479, 2005.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah ASmith.
Improved part-of-speech tagging foronline conversational text with word clusters.In Proceedings of NAACL-HLT, pages 380?390,2013.Hilke Reckman, Cheyanne Baird, Jean Crawford,Richard Crowell, Linnea Micciulla, SaratenduSethi, and Fruzsina Veress.
teragram: Rule-baseddetection of sentiment phrases using sas senti-ment analysis.
In Second Joint Conference onLexical and Computational Semantics (*SEM),Volume 2: Proceedings of the Seventh Inter-national Workshop on Semantic Evaluation (Se-mEval 2013), pages 513?519, Atlanta, Georgia,USA, June 2013.Jorma Rissanen.
Stochastic Complexity in Statis-tical Inquiry Theory.
World Scientific Publish-ing Co., Inc., River Edge, NJ, USA, 1989.
ISBN981020311X.Sara Rosenthal, Preslav Nakov, Alan Ritter, andVeselin Stoyanov.
Semeval-2014 task 9: Senti-ment analysis in twitter.
In Proceedings of the 8thInternational Workshop on Semantic Evaluation(SemEval-2014), Dublin, Ireland, 2014.Zhu Tiantian, Zhang Fangxi, and Man Lan.
Ec-nucs: A surface information based system de-scription of sentiment analysis in twitter in thesemeval-2013 (task 2).
In Second Joint Con-ference on Lexical and Computational Semantics(*SEM), Volume 2: Proceedings of the SeventhInternational Workshop on Semantic Evaluation(SemEval 2013), pages 408?413, Atlanta, Geor-gia, USA, June 2013.159
