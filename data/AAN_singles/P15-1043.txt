Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 441?450,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsContent Models for Survey Generation: A Factoid-Based EvaluationRahul Jha?, Catherine Finegan-Dollak?, Reed Coke?, Ben King?, Dragomir Radev??
?Department of EECS, University of Michigan, USA?School of Information, University of Michigan, USA{rahuljha,cfdollak,reedcoke,benking,radev}@umich.eduAbstractWe present a new factoid-annotateddataset for evaluating content modelsfor scientific survey article generationcontaining 3,425 sentences from 7 topicsin natural language processing.
We alsointroduce a novel HITS-based contentmodel for automated survey article gen-eration called HITSUM that exploits thelexical network structure between sen-tences from citing and cited papers.
Usingthe factoid-annotated data, we conduct apyramid evaluation and compare HITSUMwith two previous state-of-the-art contentmodels: C-Lexrank, a network based con-tent model, and TOPICSUM, a Bayesiancontent model.
Our experiments show thatour new content model captures usefulsurvey-worthy information and outper-forms C-Lexrank by 4% and TOPICSUMby 7% in pyramid evaluation.1 IntroductionSurvey article generation is the task of automat-ically building informative surveys for scientifictopics.
Given the rapid growth of publications inscientific fields, the development of such systemsis crucial as human-written surveys exist for a lim-ited number of topics and get outdated quickly.In this paper, we investigate content models forextracting survey-worthy information from scien-tific papers.
Such models are an essential com-ponent of any system for automatic survey arti-cle generation.
Earlier work in the area of surveyarticle generation has investigated content mod-els based on lexical networks (Mohammad et al,2009; Qazvinian and Radev, 2008).
These mod-els take as input citing sentences that describeimportant papers on the topic and assign them asalience score based on centrality in a lexical net-work formed by the input citing sentences.
In thisFactoidWeightQuestion Answeringanswer extraction6question classification6definition of question answering5TREC QA track5information retrieval5Dependency Parsingnon-projective dependency structures /trees6projectivity / projective dependency trees6deterministic parsing approaches: Nivre?salgorithm5terminology: head - dependent4grammar driven approaches fordependency parsing4Table 1: Sample factoids from the topics of ques-tion answering and dependency parsing alongwith their factoid weights.paper, we propose a new content model based onnetwork structure previously unexplored for thistask that exploits the lexical relationship betweenciting sentences and the sentences from the origi-nal papers that they cite.
Our new formulation ofthe lexical network structure fits nicely with thehubs and authorities model for identifying impor-tant nodes in a network (Kleinberg, 1999), leadingto a new content model called HITSUM.
In addi-tion to this new content model, we also describehow Bayesian content models previously exploredin the news domain can be adapted for the contentmodeling task for survey generation.For the task of evaluating various content mod-els discussed in this paper, we have annotated atotal of 3,425 sentences across 7 topics in the fieldof natural language processing with factoids fromeach of the topics.
The factoids we use were ex-tracted from existing survey articles and tutorialson each topic (Jha et al, 2013), and thus repre-sent information that must be captured by a surveyarticle on the corresponding topic.
Each of the fac-toids is assigned a weight based on its frequency inthe surveys/tutorials, which allows us to do pyra-441Topic # Sentencesdependency parsing 487named entity recognition 383question answering 452semantic role labeling 466sentiment analysis 613summarization 507word sense disambiguation 425Table 2: List of seven NLP topics used in our ex-periments along with input size.mid evaluation of our content models.
Some sam-ple factoids are shown in Table 1.
Evaluation usingfactoids extracted from existing survey articles canhelp us understand the limits of automated surveyarticle generation and how well these systems canbe expected to perform.
For example, if certainkinds of factoids are missing consistently from ourinput sentences, improvements in content modelsare unlikely to get us closer to the goal of generat-ing survey articles that match those generated byhumans, and effort must be directed to extractingtext from other sources that will contain the miss-ing information.
On the other hand, if most of thefactoids exist in the input sentences but importantfactoids are not found by the content models, wecan think of strategies for improving these modelsby doing error analysis.The main contributions of this paper are:?
HITSUM, a new HITS-based content modelfor automatic survey generation for scientifictopics.?
A new dataset of 3,425 factoid-annotatedsentences for scientific articles in 7 topics.?
Experimental results for pyramid evalua-tion comparing three existing content models(Lexrank, C-Lexrank, TOPICSUM) with HIT-SUM.The rest of this paper is organized as follows.Section 2 describes the dataset used in our exper-iment and the factoid annotation process.
Sec-tion 3 describes each of the content models usedin our experiments including HITSUM.
Section 4describes our experiments and Section 5 summa-rizes the results.
We summarize the related workin Section 6 and conclude in Section 7.2 DataPrior research in automatic survey generation hasexplored using text from different parts of scien-tific papers.
Some of the recent work has treatedsurvey generation as a direct extension of sin-gle paper summarization (Qazvinian and Radev,2008) and used citing sentences to a set of relevantpapers as the input for the summarizer (Moham-mad et al, 2009; Qazvinian et al, 2013).
How-ever, in our prior work, we have observed that it?sdifficult to generate coherent and readable sum-maries using just citing sentences and have pro-posed the use of sentences from introductory textsof papers that cite a number of important paperson a topic (Jha et al, 2015).
The use of full textallows for the use of discourse structure of thesedocuments in framing coherent and readable sur-veys.
Since the content models we explore aremeant to be part of a larger system that should beable to generate coherent and readable survey ar-ticles, we use the introduction sentences for ourexperiments as well.The corpus we used for extracting our experi-mental data was the ACL Anthology Network, acomprehensive bibliographic dataset that containsfull text and citations for papers in most of theimportant venues in natural language processing(Radev et al, 2013).
An oracle method is used forselecting the initial set of papers for each topic.For each topic, the bibliographies of at least threehuman-written surveys were extracted, and anypapers that appeared in more than one survey wereadded to the target document set for the topic.The text for summarization is extracted from in-troductory sections of papers that cite papers in thetarget document set.
The intuition behind this isthat the introductory sections of papers that citethese target document summarize the research inpapers from the target document set as well as therelationships between these papers.
Thus, theseintroductions can be thought of as mini-surveysfor specific aspects of the topic; combining textfrom these introductory sections should allow usto generate good comprehensive survey articlesfor the topic1.
For our experiments, we sort the cit-ing papers based on the number of papers they cite1Other sections of papers might have such information,e.g.
related work.
Initial data analysis showed, however, thatnot all papers in our corpus had related work sections.
Thusfor consistency, we decided to use introduction sections.
Theperfect system for this task would be able to extract ?relatedwork style?
text segments from an entire paper.442Input sentence FactoidsAccording to [1] , the corpus based supervised machine learning methods arethe most successful approaches to WSD where contextual features have beenused mainly to distinguish ambiguous words in these methods.supervised wsd, corpus based wsdCompared with supervised methods, unsupervised methods do not requiretagged corpus, but the precision is usually lower than that of the supervisedmethods.supervised wsd, unsupervised wsdWord sense disambiguation (WSD) has been a hot topic in natural languageprocessing, which is to determine the sense of an ambiguous word in a specificcontext.definition of word sense disambiguationImprovement in the accuracy of identifying the correct word sense will result inbetter machine translation systems, information retrieval systems, etc.wsd for machine translation, wsd for in-formation retrievalThe SENSEVAL evaluation framework ( Kilgarriff 1998 ) was a DARPA-stylecompetition designed to bring some conformity to the field of WSD, althoughit has yet to achieve that aim completely.sensevalTable 3: Sample input sentences from the topic of word sense disambiguation annotated with factoids.in the target document set, pick the top 20 papers,and extract sentences from their introductions toform the input text for the summarizer.
The seventopics used in our experiments and input size foreach topic are shown in Table 2.Once the input text for each topic has been ex-tracted, we annotate the sentences in the inputtext with factoids for that topic.
Some annotatedsentences in the topic of word sense disambigua-tion are shown in Table 3.
Given this new an-notated data, we can compare how the factoidsare distributed across different citing sentences (asannotated by Jha et al (2013)) and introductionsentences that we have annotated.
For this, wedivide the factoids into five categories: defini-tions, venue, resources, methodology, and appli-cations.
The fractional distribution of factoids inthese categories is shown in Table 4.
We can seethat the distribution of factoids relating to venues,methodology and applications is similar for thetwo datasets.
However, factoids related to defini-tional sentences are almost completely missing inthe citing sentences data.
This lack of backgroundinformation in citing sentences is one of the moti-vations for using introduction sentences for surveyarticle generation as opposed to previous work.The complete set of factoids as wellas annotated sentences for all the top-ics is available for download at http://clair.si.umich.edu/corpora/Surveyor_CM_Data.tar.gz.3 Content ModelsWe now describe each of the content models usedin our experiments.Factoid category % Citing % Introdefinitions 0 4venue 6 6resources 18 2methodology 70 83applications 6 5Table 4: Fractional distribution of factoids acrossvarious categories in citing sentences vs introduc-tion sentences.3.1 LexrankLexrank is a network-based content selection al-gorithm that serves as a baseline for our experi-ments.
Given an input set of sentences, it first cre-ates a network using these sentences where eachnode represents a sentence and each edge repre-sents the tf-idf cosine similarity between the sen-tences.
Two methods for creating the network arepossible.
First, we can remove all edges that arelower than a certain threshold of similarity (gener-ally set to 0.1).
The Lexrank value for a node p(u)in this case is calculated as:1?
dN+ d?v?adj[u]p(v)deg(v)Where N is the total number of sentences, d isthe damping factor that controls the probability ofa random jump (usually set to 0.85), deg(v) is thedegree of the node v, and adj[u] is the set of nodesconnected to the node u.
A different way of creat-ing the network is to treat the sentence similaritiesas edge weights and use the adjacency matrix asa transition matrix after normalizing the rows; theformula then becomes:443A dictionary such as the LDOCE has broad coverage of word senses, useful for WSD .This paper describes a program that disambiguates English word senses in unrestricted text usingstatistical models of the major Roget?s Thesaurus categories.Our technique offers benefits both for online semantic processing and for the challenging task ofmapping word senses across multiple MRDs in creating a merged lexical database.The words in the sentences may be any of the 28,000 headwords in Longman?s Dictionary ofContemporary English (LDOCE) and are disambiguated relative to the senses given in LDOCE.This paper describes a heuristic approach to automatically identifying which senses of a machine-readable dictionary (MRD) headword are semantically related versus those which correspond tofundamentally different senses of the word.Figure 1: A sentence from Pcitingwith a high hub score (bolded) and some of sentences from Pcitedthat it links to (italicised).
The sentence from Pcitingobtain a high hub score by being connected to thesentences with high authority scores.1?
dN+ d?v?adj[u]cos(u, v)TotalCosvp(v)Where cos(u, v) gives the tf-idf cosine similar-ity between sentence u and v and TotalCosv=?z?adj[v]cos(z, v).
In our experiments, we em-ploy this second formulation.
The above equationcan be solved efficiently using the power method(Newman, 2010) to obtain p(u) for each node,which is then used as the score for ordering thesentences.
The final Lexrank values p(u) for anode represent the stationary distribution of theMarkov chain represented by the transition matrix.Lexrank has been shown to perform well in sum-marization experiments (Erkan and Radev, 2004).3.2 C-LexrankC-Lexrank is a clustering-based summarizationsystem that was proposed by Qazvinian and Radev(2008) to summarize different perspectives in cit-ing sentences that reference a paper or a topic.To create summaries, C-LexRank constructs afully connected network in which vertices are sen-tences, and edges are cosine similarities calculatedusing the tf-idf vectors of citation sentences.
Itthen employs a hierarchical agglomeration clus-tering algorithm proposed by Clauset et al (2004)to find communities of sentences that discuss thesame scientific contributions.
Once the graph isclustered and communities are formed, the methodextracts sentences from different clusters to builda summary.
It iterates through the clusters fromlargest to smallest, choosing the most salient sen-tence of each cluster, until the summary lengthlimit is reached.
The salience of a sentence in itscluster is defined as its Lexrank value in the lexicalnetwork formed by sentences in the cluster.3.3 HITSUMThe input set of sentences in our data come fromintroductory sections of papers that cite importantpapers on a topic.
We?ll refer to the set of cit-ing papers that provide the input text for the sum-marizer as Pcitingand the set of important papersthat represent the research we are trying to sum-marize as Pcited.
Both Lexrank and C-Lexrankwork by finding central sentences in a networkformed by the input sentences and thus, only usethe lexical information present in Pciting, while ig-noring additional lexical information from the pa-pers in Pcited.
We now present a formulation thatuses the network structure that exists between thesentences in the two sets of papers to incorporateadditional lexical information into the summariza-tion system.
This system is based on the hubs andauthorities or the HITS model (Kleinberg, 1999)and hence is called HITSUM.HITSUM, in addition to the sentences from theintroductory sections of papers in Pciting, alsouses sentences from the abstracts of Pcited.
It startsby computing the tf-idf cosine similarity betweenthe sentences of each paper pi?
Pcitingwith thesentences in the abstracts of each paper pj?
Pcitedthat is directly cited by pi.
A directed edge is cre-ated between every sentence siin piand sjin pjif sim(si, sj) > smin, where sminis a similaritythreshold (set to 0.1 for our experiments).
Oncethis process has been completed for all papers inPciting, we end up with a bipartite graph betweensentences from Pcitingand Pcited.In this bipartite graph, sentences in Pcitedthat444?B?C/QA?D/J07?1005?C/NER?D/I08?1071the 0.066 question 0.044 metathesaurus 0.00032 ne 0.028 wikipedia 0.0087of 0.040 questions 0.038 umls 0.00032 entity 0.022 pages 0.0053and 0.034 answer 0.028 biomedical 0.00024 named 0.022 million 0.0018a 0.029 answering 0.022 relevance 0.00024 entities 0.017 extracting 0.0018in 0.027 qa 0.021 citation 0.00024 ner 0.014 articles 0.0018to 0.027 answers 0.017 wykoff 0.00024 names 0.009 contributors 0.0018is 0.017 2001 0.016 bringing 0.00016 location 0.008 version 0.0009for 0.014 system 0.011 appropriately 0.00016 tagging 0.007 dakka 0.0009that 0.012 trec 0.008 organized 0.00016 recognition 0.007 service 0.0009we 0.011 factoid 0.008 foundation 0.00016 classes 0.007 academic 0.0009Figure 2: Top words from different word distributions learned by TOPICSUM on our input document setof 15 topics.
?Bis the background word distribution that captures stop words.
?C/QAand ?C/NERarethe word distributions for the topics of question answering and named entity recognition respectively.
?D/J07?1005is the document-specific word distribution for a single paper in question answering thatfocuses on clinical question answering.
?D/I08?1071is the document-specific word distribution for asingle paper in named entity recognition that focuses on named entity recognition in Wikipedia articles.have a lot of incoming edges represent sentencesthat presented important contributions in the field.Similarly, sentences in Pcitingthat have a lot ofoutgoing edges represent sentences that summa-rize a number of important contributions in thefield.
This suggests using the HITS algorithm,which, given a network, assigns hubs and author-ities scores to each node in the network in a mu-tually reinforcing way.
Thus, nodes with high au-thority scores are those that are pointed to by anumber of good hubs, and nodes with high hubscores are those that point to a number of goodauthorities.
This can be formalized with the fol-lowing equation for the hub score of a node:h(v) =?u?successors(v)a(u)Where h(v) is the hub score for node v,successors(v) is the set of all nodes that v has anedge to, and a(u) is the authority score for nodeu.
Similarly, the authority score for each node iscomputed as:a(v) =?u?predecessors(v)h(u)Where predecessors(v) is the set of all nodesthat have an edge to v. The hub and authority scorefor each node can be computed using the powermethod that starts with an initial value and itera-tively updates the scores for each node based onthe above equations until the hub and authorityscores for each node converge to within a toler-ance value (set to 1E-08 for our experiments).In our bipartite lexical network, we expect sen-tences in Pcitedreceiving high authority scores tobe the ones reporting important contributions andsentences in Pcitingthat receive high hub scoresto be sentences summarizing important contribu-tions.
Figure 1 shows an example of a sentencewith a high hub score from the topic of word sensedisambiguation, along with some of the sentencesthat it points to.
HITSUM computes the hub andauthority score for each sentence in the lexical net-work and then uses the hub scores for sentences inPcitingas their relevance score.
Sentences fromPcitedare part of the lexical network, but are notused in the output summary.3.4 TOPICSUMTOPICSUM is a probabilistic content model pre-sented in Haghighi and Vanderwende (2009)and is very similar to an earlier model calledBayesSum proposed by Daum?e and Marcu (2006).It is a hierarchical, LDA (Latent Dirichlet Alloca-tion) style model that is based on the followinggenerative story:2words in any sentence in thecorpus can come from one of three word distri-butions: a background word distribution ?Bthatflexibly models stop words, a content word dis-tribution ?Cfor each document set that modelscontent relevant to the entire document set, anda document-specific word distribution ?D.
Theword distributions are learned using Gibbs sam-pling.
Given n document sets each with k doc-2To avoid confusion in use of the term ?topic,?
in this pa-per we refer to topics in the LDA sense as ?word distribu-tions.?
?Topics?
in this paper refer to the natural languageprocessing topics such as question answering, word sensedisambiguation, etc.445Topic Lexrank C-Lexrank TOPICSUM HITSUMdependency parsing 0.47 0.76 0.62 1.00?named entity recognition 0.80 0.89 0.90?0.80question answering 0.65 0.67 0.65 0.76?sentiment analysis 0.64 0.62 0.75?0.63semantic role labeling 0.75?0.67 0.65 0.69summarization 0.52 0.75?0.57 0.68word sense disambiguation 0.78 0.66 0.67 0.79?Average 0.66 0.72 0.69 0.76?Table 5: Pyramid scores obtained by different content models for each topic along with average scoresfor each model across all topics.
For each topic as well as the average, the best performing method hasbeen highlighted with a?.uments, we get n content word distributions andn ?
k document-specific distributions leading to atotal of 1 + n+ n ?
k word distributions.To illustrate the kind of distributions TOPIC-SUM learns in our dataset, Figure 2 shows thetop words along with their probabilities from thebackground word distribution, two content distri-butions and two document-specific word distribu-tions.
We see that the model effectively capturesgeneral content words for each topic.
?C/QAis theword distribution for the topic of question answer-ing, while ?D/J07?1005is the document-specificword distribution for a specific paper in the docu-ment set for question answering3that focuses onclinical question answering.
The word distribu-tion ?D/J07?1005contains words that are relevantto the specific subtopic in the paper, while ?C/QAcontains content words relevant to the generaltopic of question answering.
Similar results canbe seen in the word distributions for named entityrecognition ?C/NERand the document-specificword distribution for a specific paper in the topic?D/I08?10714that focuses on comparable entitymining.These topics, learned using Gibbs sampling, canbe used to select sentences for a summary in thefollowing way.
To summarize a document set, wegreedily select sentences that minimize the KL-divergence of our summary to the document-set-specific topic.
Thus, the score for each sentence sisKL(?C||Ps) where Psis the sentence word dis-tribution with add-one smoothing applied to bothdistributions.
Using this objective, sentences that3Dina Demner-Fushman and Jimmy Lin.
2007.
Answer-ing Clinical Questions with Knowledge-Based and StatisticalTechniques.
Computational Linguistics.4Wisam Dakka and Silviu Cucerzan.
2008.
Augmentingwikipedia with named entity tags.
In Proceedings of IJCNLP.contain words from the content word distributionwith high probability are more likely to be selectedin the generated summary.We implemented TOPICSUM in Pythonusing Numpy and then optimized it usingScipy Weave.
This code is available for useat https://github.com/rahuljha/content-models.
The repository alsocontains Python code for HITSUM.4 ExperimentsFor evaluating our content models, we gener-ated 2,000-character-long summaries using eachof the systems (Lexrank, C-Lexrank, HITSUM,and TOPICSUM) for each of the topics.
The sum-maries are generated by ranking the input sen-tences using each content model and picking thetop sentences till the budget of 2,000 characters isreached.
Each of these summaries is then givena pyramid score (Nenkova and Passonneau, 2004)computed using the factoids assigned to each sen-tence.For the pyramid evaluation, the factoids are or-ganized in a pyramid of order n. The top tier inthis pyramid contains the highest weighted fac-toids, the next tier contains the second highestweighted factoids, and so on.
The score assignedto a summary is the ratio of the sum of the weightsof the factoids it contains to the sum of weightsof an optimal summary with the same number offactoids.
Pyramid evaluation allows us to capturehow each content model performs in terms of se-lecting sentences with the most highly weightedfactoids.
Since the factoids have been extractedfrom human-written surveys and tutorials on eachof the topics, the pyramid score gives us an idea ofthe survey-worthiness of the sentences selected by446Question classification is a crucial component of modern question answering system.A what-type question is defined as the one whose question word is ?what?, ?which?, ?name?
or ?list?.This metaclassifier beats all published numbers on standard question classification benchmarks[4.4].Due to its challenge, this paper focuses on what-type question classification.In this paper, we focus on fine-category classification.The promise of a machine learning approach is that the QA system builder can now focus on de-signing features and providing labeled data, rather than coding and maintaining complex heuristicrule bases.Figure 3: Part of the summary generated by HITSUM for the topic of question answering.each content model.5 Results and DiscussionThe results of pyramid evaluation are summarizedin Table 5.
It shows the pyramid score obtained byeach system on each of the topics as well as the av-erage score.
The highest performing system on av-erage is HITSUM with an average performance of76%.
HITSUM does especially well for the topicsof dependency parsing, question answering, andword sense disambiguation.
The second best per-forming system is C-Lexrank, which is not sur-prising because it was developed specifically forthe task of scientific paper summarization.
How-ever, HITSUM outperforms C-Lexrank on severaltopics and by 4% on average.Figure 3 shows part of the summary generatedby HITSUM for the topic of question answering.The summary contains mostly informative sen-tences about different aspects of question answer-ing.
One obvious drawback of this summary isthat it?s not very coherent and readable.
How-ever, previous work has shown how network basedcontent models can be combined with discoursemodels to generate informative yet readable sum-maries (Jha et al, 2015).
We looked at some of thenetwork statistics of the lexical networks used byHITSUM.
One of the things we noticed is that thelexical networks for topics where HITSUM per-forms well seem to have higher degree assorta-tivity compared to the topics for which it doesn?tperform well.
High degree assortativity in lexicalnetworks means sentences with high degree tendto be linked to other sentences with high degree.This suggests that HITS performs well for topicswhere a set of important factoids are mentioned inmany citing and source sentences.
A larger evalua-tion dataset is needed for a more thorough analysisof how the network properties of these lexical net-works correlate with the performance of variouscontent models.TOPICSUM does well on the topics of namedentity recognition and sentiment analysis, but doesnot do well on average.
This can be attributed tothe fact that it was developed as a content modelfor the domain of news summarization and doesnot translate well to our domain.
All systems out-perform Lexrank, which achieves the lowest aver-age score.
This result is also intuitive, because ev-ery other system in our evaluation uses additionalinformation not used by Lexrank: C-Lexrank ex-ploits the community structure in the input set ofsentences, HITSUM exploits the lexical informa-tion from cited sentences, and TOPICSUM exploitsinformation about global word distribution acrossall topics.The different systems we tried in our evaluationdepend on using different lexical information andseem to perform well for different topics.
Thissuggests that further gains can be made by com-bining these systems.
For example, C-Lexrankand HITSUM can be combined by utilizing boththe network formed by citing sentences and thenetwork between the citing sentences and the citedsentences into a larger lexical network.
TOPIC-SUM scores can be combined with these network-based system by using the TOPICSUM scores as aprior for each node, and then running either Pager-ank or HITS on top of it.
We leave exploration ofsuch hybrid systems to future work.6 Related WorkThe goal of content models in the context of sum-marization is to extract a representation from in-put text that can help in identifying important sen-tences that should be in the output summary.
Ourwork is related to two main classes of contentmodels: network-based methods and probabilis-447tic methods.
We summarize related work for eachof these classes of content models, followed by ashort summary of the related work in the domainof scientific summarization.Network-based content models: Network-based content models (Erkan and Radev, 2004;Mihalcea and Tarau, 2004) work by convertingthe input sentences into a network.
Each sentenceis represented by a node in the network, andthe edges between sentences are given weightbased on the similarities of sentences.
They thenrun Pagerank on this network, and sentences areselected based on their Pagerank score in thenetwork.
For computing Pagerank, the networkcan either be pruned by removing edges thathave weights less than a certain threshold, ora weighted version of Pagerank can be run onthe network.
The method can also be modifiedfor query-focused summarization (Otterbacheret al, 2009).
C-Lexrank (Qazvinian and Radev,2008) modifies Lexrank by first running a clus-tering algorithm on the network to partition thenetwork into different communities and thenselecting sentences from each community byrunning Lexrank on the sub-network within eachcommunity.
C-Lexrank was also used in the taskof automated survey generation with encouragingresults (Mohammad et al, 2009).Probabilistic content models: One of thefirst probabilistic content models seems to beBAYESSUM (Daum?e and Marcu, 2006), designedfor query-focused summarization.
BAYESSUMmodels a set of document collections using a hi-erarchical LDA style model.
Each word in a sen-tence can be generated using one of three languagemodels: 1) a general English language model thatcaptures English filler or background knowledge,2) a document-specific language model, and 3) aquery language model.
These language models areinferred using expectation propagation, and thensentences are ranked based on their likelihood ofbeing generated from the query language model.A similar model for general multidocument sum-marization called TOPICSUM was proposed byHaghighi and Vanderwende (2009), where thequery language model is replaced by a document-collection-specific language model; thus sentencesare selected based on how likely they are to con-tain information that summarizes the entire doc-ument collection instead of information pertain-ing to individual documents or background knowl-edge.Barzilay and Lee (2004) present a HiddenMarkov Model (HMM) based content modelwhere the hidden states of the HMM representthe topics in the text.
The transition probabili-ties are learned through Viterbi decoding.
Theyshow that the HMM model can be used for both re-ordering of sentences for coherence and discrimi-native scoring of sentences for extractive summa-rization.
Fung and Ngai (2006) present a simi-lar HMM-based model for multi-document sum-marization.
Jiang and Zhai (2005) proposed anHMM-based model for the problem of extract-ing coherent passages relevant to a query froma relevant document.
They learn an HMM withtwo background states (B1and B2) and a query-relevant state (R), each associated with a languagemodel.
The HMM starts in background state B1,switches to relevant state R and then switches tothe next background state B2.
The sentences thatthe HMM emits while in R constitute the query-relevant passage from the document.Scientific summarization: Early work in scien-tific summarization used abstracts of scientific ar-ticles to produce summaries of specific scientificpapers (Kupiec et al, 1995).
However, later work(Elkiss et al, 2008) showed that citation sentencesare as important in understanding the main contri-butions of a paper.Nanba and Okumura (1999) explored using ref-erence information to build a system for support-ing writing survey articles.
Their system extractsciting sentences that describe a referred paper andidentify the type of reference relationships.
Thetype of references can be one of the three: 1) typeB that base on other researcher?s theory, 2) typeC that compare with related works, or 3) type Orepresenting relationships other than B or C. Theyposit that type C sentences are the most importantfor survey generation and can help show the simi-larities and differences among cited papers.Teufel and Moens (2002) propose a method forsummarizing scientific articles based on rhetoricalstatus of sentences in scientific articles.
They an-notate sentences in a corpus of 80 scientific arti-cles with rhetorical status, where the rhetorical sta-tus can be one of aim (specific research goal), tex-tual (section structure), own (neutral description ofown work), background (generally accepted back-ground), contrast (comparison with other work),448basis (agreement with or continuation of otherwork), and other (neutral description of other?swork).
They describe classifiers for tagging therhetorical status of sentences automatically andpresent a method for using this to assign relevancescore to sentences.In other work, Kan et al (2002) use a corpus of2000 annotated bibliographies for scientific papersas a first step towards a supervised summariza-tion system.
They found that summaries in theircorpus were mostly single-document abstractivesummaries that were both indicative and informa-tive and were organized around a ?theme,?
makingthem ideal for query-based summarization.
Meiand Zhai (2008) presented an impact-based sum-marization method for single-paper summariza-tion that assigns relevance scores to sentences ina paper based on their similarity to the set of cit-ing sentences that reference the paper.More recently, Hoang and Kan (2010) presenta method for automated related work generation.Their system takes as input a set of keywords ar-ranged in a hierarchical fashion that describes atarget paper?s topic.
They hypothesize that sen-tences in a related work provide either backgroundinformation or specific contributions.
They usetwo different models to extract these two kindsof sentences using the input tree and combinesthem to create the final output summary.
Zhanget al (2013) explore methods for biomedical sum-marization by identifying cliques in a networkof semantic predications extracted from citations.These cliques are then clustered and labeled toidentify different points of view represented in thesummary.7 Conclusion and Future WorkWe have presented a new factoid-annotated datasetfor evaluating content models for scientific surveyarticle generation by annotating sentences fromseven topics in natural language processing.
Wealso introduce a new HITS-based content modelcalled HITSUM for survey article generation thatexploits the lexical information from cited papersalong with citing papers to rank input sentencesfor survey-worthiness.
We conduct pyramidevaluation using our factoid dataset to compareHITSUM with existing network-based methods(Lexrank, C-Lexrank) as well as methods basedon Bayesian content modeling (TOPICSUM).
Onaverage, HITSUM outperforms C-Lexrank by 4%and TOPICSUM by 7%.
Since the different con-tent models use different kinds of lexical informa-tion, further gains might be obtained by combiningsome of these models into a joint model.
We planto explore this in future work.ReferencesRegina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In Daniel MarcuSusan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 113?120,Boston, Massachusetts, USA, May 2 - May 7.
Asso-ciation for Computational Linguistics.Aaron Clauset, Mark E. J. Newman, and CristopherMoore.
2004.
Finding community structure in verylarge networks.
Phys.
Rev.
E, 70(6):066111, Dec.Hal Daum?e, III and Daniel Marcu.
2006.
Bayesianquery-focused summarization.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th Annual Meeting of the As-sociation for Computational Linguistics, ACL-44,pages 305?312, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Aaron Elkiss, Siwei Shen, Anthony Fader, G?unes?Erkan, David States, and Dragomir R. Radev.
2008.Blind men and elephants: What do citation sum-maries tell us about a research article?
Journal ofthe American Society for Information Science andTechnology, 59(1):51?62.G?unes?
Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based centrality as salience in text summa-rization.
Journal of Artificial Intelligence Research(JAIR).Pascale Fung and Grace Ngai.
2006.
One story, oneflow: Hidden markov story models for multilingualmultidocument summarization.
ACM Trans.
SpeechLang.
Process., 3(2):1?16, July.Aria Haghighi and Lucy Vanderwende.
2009.
Ex-ploring content models for multi-document summa-rization.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, NAACL ?09, pages 362?370,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Cong Duy Vu Hoang and Min-Yen Kan. 2010.
To-wards automated related work summarization.
InProceedings of the 23rd International Conference onComputational Linguistics: Posters, COLING ?10,pages 427?435, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Rahul Jha, Amjad Abu-Jbara, and Dragomir R. Radev.2013.
A system for summarizing scientific topics449starting from keywords.
In Proceedings of The As-sociation for Computational Linguistics (short pa-per).Rahul Jha, Reed Coke, and Dragomir R. Radev.
2015.Surveyor: A system for generating coherent surveyarticles for scientific topics.
In Proceedings of theTwenty-Ninth AAAI Conference.Jing Jiang and ChengXiang Zhai.
2005.
Accuratelyextracting coherent relevant passages using hiddenMarkov models.
pages 289?290.Min-Yen Kan, Judith L. Klavans, and Kathleen R.McKeown.
2002.
Using the Annotated Bibliog-raphy as a Resource for Indicative Summarization.In The International Conference on Language Re-sources and Evaluation (LREC), Las Palmas, Spain.Jon M. Kleinberg.
1999.
Authoritative sources ina hyperlinked environment.
J. ACM, 46:604?632,September.Julian Kupiec, Jan Pedersen, and Francine Chen.
1995.A trainable document summarizer.
In Proceedingsof the 18th Annual International ACM SIGIR Con-ference on Research and Development in Informa-tion Retrieval (SIGIR-95), pages 68?73.Qiaozhu Mei and ChengXiang Zhai.
2008.
Gener-ating impact-based summaries for scientific litera-ture.
In Proceedings of the 46th Annual Confer-ence of the Association for Computational Linguis-tics (ACL-08), pages 816?824.Rada Mihalcea and Paul Tarau.
2004.
TextRank:Bringing order into texts.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP-04), July.Saif Mohammad, Bonnie Dorr, Melissa Egan, AhmedHassan, Pradeep Muthukrishan, Vahed Qazvinian,Dragomir Radev, and David Zajic.
2009.
Using ci-tations to generate surveys of scientific paradigms.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, NAACL ?09, pages 584?592, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Hidetsugu Nanba and Manabu Okumura.
1999.
To-wards multi-paper summarization using referenceinformation.
In Proceedings of the 16th Interna-tional Joint Conference on Artificial Intelligence(IJCAI-99), pages 926?931.Ani Nenkova and Rebecca Passonneau.
2004.
Evalu-ating content selection in summarization: The pyra-mid method.
In Proceedings of the North Ameri-can Chapter of the Association for ComputationalLinguistics - Human Language Technologies (HLT-NAACL ?04).Mark E. J. Newman.
2010.
Networks: An Introduc-tion.
Oxford University Press.Jahna Otterbacher, Gunes Erkan, and Dragomir R.Radev.
2009.
Biased lexrank: Passage retrieval us-ing random walks with question-based priors.
Inf.Process.
Manage., 45(1):42?54, January.Vahed Qazvinian and Dragomir R. Radev.
2008.Scientific paper summarization using citation sum-mary networks.
In Proceedings of the 22nd Inter-national Conference on Computational Linguistics(COLING-08), Manchester, UK.Vahed Qazvinian, Dragomir R. Radev, Saif M. Mo-hammad, Bonnie Dorr, David Zajic, MichaelWhidby, and Taesun Moon.
2013.
Generating ex-tractive summaries of scientific paradigms.
J. Artif.Int.
Res., 46(1):165?201, January.Dragomir R. Radev, Pradeep Muthukrishnan, VahedQazvinian, and Amjad Abu-Jbara.
2013.
The aclanthology network corpus.
Language Resourcesand Evaluation, pages 1?26.Simone Teufel and Marc Moens.
2002.
Summariz-ing scientific articles: experiments with relevanceand rhetorical status.
Computational Linguistics,28(4):409?445.Han Zhang, Marcelo Fiszman, Dongwook Shin, Bart-lomiej Wilkowski, and Thomas C. Rindflesch.
2013.Clustering cliques for graph-based summarization ofthe biomedical research literature.
BMC Bioinfor-matics, 14:182.450
