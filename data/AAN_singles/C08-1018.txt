Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 137?144Manchester, August 2008Sentence Compression Beyond Word DeletionTrevor Cohn and Mirella LapataSchool of InformaticsUniversity of Edinburgh{tcohn,mlap}@inf.ed.ac.ukAbstractIn this paper we generalise the sen-tence compression task.
Rather than sim-ply shorten a sentence by deleting wordsor constituents, as in previous work, werewrite it using additional operations suchas substitution, reordering, and insertion.We present a new corpus that is suitedto our task and a discriminative tree-to-tree transduction model that can naturallyaccount for structural and lexical mis-matches.
The model incorporates a novelgrammar extraction method, uses a lan-guage model for coherent output, and canbe easily tuned to a wide range of compres-sion specific loss functions.1 IntroductionAutomatic sentence compression can be broadlydescribed as the task of creating a grammaticalsummary of a single sentence with minimal in-formation loss.
It has recently attracted much at-tention, in part because of its relevance to appli-cations.
Examples include the generation of sub-titles from spoken transcripts (Vandeghinste andPan, 2004), the display of text on small screenssuch as mobile phones or PDAs (Corston-Oliver,2001), and, notably, summarisation (Jing, 2000;Lin, 2003).Most prior work has focused on a specificinstantiation of sentence compression, namelyword deletion.
Given an input sentence ofwords, w1, w2.
.
.
wn, a compression is formedby dropping any subset of these words (Knightc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.and Marcu, 2002).
The simplification renders thetask computationally feasible, allowing efficientdecoding using a dynamic program (Knight andMarcu, 2002; Turner and Charniak, 2005; McDon-ald, 2006).
Furthermore, constraining the problemto word deletion affords substantial modeling flex-ibility.
Indeed, a variety of models have been suc-cessfully developed for this task ranging from in-stantiations of the noisy-channel model (Knightand Marcu, 2002; Galley and McKeown, 2007;Turner and Charniak, 2005), to large-margin learn-ing (McDonald, 2006; Cohn and Lapata, 2007),and Integer Linear Programming (Clarke, 2008).However, the simplification also renders the tasksomewhat artificial.
There are many rewrite opera-tions that could compress a sentence, besides dele-tion, including reordering, substitution, and inser-tion.
In fact, professional abstractors tend to usethese operations to transform selected sentencesfrom an article into the corresponding summarysentences (Jing, 2000).Therefore, in this paper we consider sentencecompression from a more general perspective andgenerate abstracts rather than extracts.
In thisframework, the goal is to find a summary of theoriginal sentence which is grammatical and con-veys the most important information without nec-essarily using the same words in the same or-der.
Our task is related to, but different from,paraphrase extraction (Barzilay, 2003).
We mustnot only have access to paraphrases (i.e., rewriterules), but also be able to combine them in order togenerate new text, while attempting to produce ashorter resulting string.
Quirk et al (2004) presentan end-to-end paraphrasing system inspired byphrase-based machine translation that can both ac-quire paraphrases and use them to generate newstrings.
However, their model is limited to lexicalsubstitution ?
no reordering takes place ?
and is137lacking the compression objective.Once we move away from extractive compres-sion we are faced with two problems.
First, wemust find an appropriate training set for our ab-stractive task.
Compression corpora are not natu-rally available and existing paraphrase corpora donot normally contain compressions.
Our secondproblem concerns the modeling task itself.
Ideally,our learning framework should handle structuralmismatches and complex rewriting operations.In what follows, we first present a new cor-pus for abstractive compression which we createdby having annotators compress sentences whilerewriting them.
Besides obtaining useful data formodeling purposes, we also demonstrate that ab-stractive compression is a meaningful task.
Wethen present a tree-to-tree transducer capable oftransforming an input parse tree into a compressedparse tree.
Our approach is based on synchronoustree substitution grammar (STSG, Eisner (2003)),a formalism that can account for structural mis-matches, and is trained discriminatively.
Specifi-cally, we generalise the model of Cohn and Lapata(2007) to our abstractive task.
We present a noveltree-to-tree grammar extraction method which ac-quires paraphrases from bilingual corpora and en-sure coherent output by including a ngram lan-guage model as a feature.
We also develop a num-ber of loss functions suited to the abstractive com-pression task.
We hope that some of the work de-scribed here might be of relevance to other gen-eration tasks such as machine translation (Eisner,2003), multi-document summarisation (Barzilay,2003), and text simplification (Carroll et al, 1999).2 Abstractive Compression CorpusA stumbling block to studying abstractive sentencecompression is the lack of widely available corporafor training and testing.
Previous work has beenconducted almost exclusively on Ziff-Davis, a cor-pus derived automatically from document abstractpairs (Knight and Marcu, 2002), or on human-authored corpora (Clarke, 2008).
Unfortunately,none of these data sources are suited to our prob-lem since they have been produced with a sin-gle rewriting operation, namely word deletion.
Al-though there is a greater supply of paraphrasingcorpora, such as the Multiple-Translation Chinese(MTC) corpus1and theMicrosoft Research (MSR)Paraphrase Corpus (Quirk et al, 2004), they arealso not ideal, since they have not been created1Available by the LDC, Catalog Number LDC2002T01,ISBN 1-58563-217-1.with compression in mind.
They contain amplerewriting operations, however they do not explic-itly target information loss.For the reasons just described, we created ourown corpus.
We collected 30 newspaper articles(575 sentences) from the British National Corpus(BNC) and the American News Text corpus, forwhich we obtained manual compressions.
In or-der to confirm that the task was feasible, five ofthese documents were initially compressed by twoannotators (not the authors).
The annotators weregiven instructions that explained the task and de-fined sentence compression with the aid of exam-ples.
They were asked to paraphrase while preserv-ing the most important information and ensuringthe compressed sentences remained grammatical.They were encouraged to use any rewriting opera-tions that seemed appropriate, e.g., to delete words,add new words, substitute them or reorder them.Assessing inter-annotator agreement is notori-ously difficult for paraphrasing tasks (Barzilay,2003) since there can be many valid outputs fora given input.
Also our task is doubly subjectivein deciding which information to remove from thesentence and how to rewrite it.
In default of anagreement measure that is well suited to the taskand takes both decisions into account, we assessedthem separately.
We first examined whether the an-notators compressed at a similar level.
The com-pression rate was 56% for one annotator and 54%for the other.2We also assessed whether theyagreed in their rewrites by measuring BLEU (Pap-ineni et al, 2002).
The inter-annotator BLEU scorewas 23.79%, compared with the source agreementBLEU of only 13.22%.
Both the compression rateand BLEU score indicate that the task is well-defined and the compressions valid.
The remain-ing 25 documents were compressed by a single an-notator to ensure consistency.
All our experimentsused the data from this annotator.3Table 1 illustrates some examples from our cor-pus.
As can be seen, some sentences contain a sin-gle rewrite operation.
For instance, a PP is para-phrased with a genitive (see (1)), a subordinateclause with a present participle (see (2)), a passivesentence with an active one (see (3)).
However, inmost cases many rewrite decisions take place allat once.
Consider sentence (4).
Here, the conjunc-tion high winds and snowfalls is abbreviated to2The term ?compression rate?
refers to the percentage ofwords retained in the compression.3Available from http://homepages.inf.ed.ac.uk/tcohn/paraphrase.1381a.
The future of the nation is in your hands.1b.
The nation?s future is in your hands.2a.
As he entered a polling booth in Katutura, he said.2b.
Entering a polling booth in Katutura, he said.3a.
Mr Usta was examined by Dr Raymond Crockett, aHarley Street physician specialising in kidney disease.3b.
Dr Raymond Crockett, a Harley Street physician, ex-amined Mr Usta.4a.
High winds and snowfalls have, however, groundedat a lower level the powerful US Navy Sea Stallionhelicopters used to transport the slabs.4b.
Bad weather, however, has grounded the helicopterstransporting the slabs.5a.
To experts in international law and relations, the USaction demonstrates a breach by a major power of in-ternational conventions.5b.
Experts say the US are in breach of international con-ventions.Table 1: Compression examples from our corpus; (a) sen-tences are the source, (b) sentences the target.bad weather and the infinitive clause to transportto the present participle transporting.
Note that theprenominal modifiers US Navy Sea Stallion andthe verb used have been removed.
In sentence (5),the verb say is added and the NP a breach by amajor power of international conventions is para-phrased by the sentence the US are in breach ofinternational conventions.3 Basic ModelOur work builds on the model developed by Cohnand Lapata (2007).
They formulate sentence com-pression as a tree-to-tree rewriting task.
A syn-chronous tree substitution grammar (STSG, Eisner(2003)) licenses the space of all possible rewrites.Each grammar rule is assigned a weight, andthese weights are learnt in discriminative training.For prediction, a specialised generation algorithmfinds the best scoring compression using the gram-mar rules.
Cohn and Lapata apply this model to ex-tractive compression with state-of-the-art results.This model is appealing for our task for severalreasons.
Firstly, the synchronous grammar pro-vides expressive power to model consistent syn-tactic effects such as reordering, changes in non-terminal categories and lexical substitution.
Sec-ondly, it is discriminatively trained, which allowsfor the incorporation of all manner of powerful fea-tures.
Thirdly, the learning framework can be tai-lored to the task by choosing an appropriate lossfunction.
In the following we describe their modelin more detail with emphasis on the synchronousgrammar, the model structure, and the predictionand training algorithms.
Section 4 presents our ex-tensions and modifications.Grammar The grammar defines a space oftree pairs over uncompressed and compressed sen-Grammar rules:?S, S?
?
?NP1VBD2NP3, NP1VBD2NP3?
?S, S?
?
?NP1VBD2NP3, NP3was VBN2by NP1?
?NP, NP?
?
?he, him?
?NP, NP?
?
?he, he?
?NP, NP?
?
?he, Peter?
?VBD, VBN?
?
?sang, sung?
?NP, NP?
?
?a song, a song?Input tree:[S [NP HeNP[VP sangVBD[NP aDTsongNN]]]Output trees:[S [NP He] [VP sang [NP a song]]][S [NP Him] [VP sang [NP a song]]][S [NP Peter] [VP sang [NP a song]]][S [NP A song] [VP was [VP sung [PP by he]]]][S [NP A song] [VP was [VP sung [PP by him]]]][S [NP A song] [VP was [VP sung [PP by Peter]]]]Figure 1: Example grammar and the output trees it licencesfor an input tree.
The numbered boxes in the rules denotelinked variables.
Pre-terminal categories are not shown for theoutput trees for the sake of brevity.tences, which we refer to henceforth as the sourceand target.
We use the grammar to find the set ofsister target sentences for a given source sentence.Figure 1 shows a toy grammar and the set of possi-ble target (output) trees for the given source (input)tree.
Each output tree is created by applying a se-ries of grammar rules, where each rule matches afragment of the source and creates a fragment ofthe target tree.
A rule in the grammar consists ofa pair of elementary trees and a mapping betweenthe variables (frontier non-terminals) in both trees.A derivation is a sequence of rules yielding a targettree with no remaining variables.Cohn and Lapata (2007) extract a STSG froma parsed, word-aligned corpus of source and tar-get sentences.
Specifically, they extract the mini-mal set of synchronous rules which can describeeach tree pair.
These rules are minimal in the sensethat they cannot be made smaller (e.g., by replac-ing a subtree with a variable) while still honouringthe word-alignment.Decoding The grammar allows us to searchfor all sister trees for a given tree.
The decodermaximises over this space:y?=argmaxy:S(y)=x?
(y) (1)where ?
(y) =?r?y??
(r, S(y)), ??
(2)Here x is the source (uncompressed) tree, yis a derivation which produces the source tree,S(y) = x, and a target tree, T (y),4and r is a gram-mar rule.
The ?
function scores the derivation and4Equation 1 optimises over derivations rather than targettrees to allow tractable inference.139is defined in (2) as a linear function over the rulesused.
Each rule?s score is an inner product betweenits feature vector, ?
(r,yS), and the model parame-ters, ?.
The feature functions are set by hand, whilethe model parameters are learned in training.The maximisation problem in (1) can be solvedefficiently using a dynamic program.
Derivationswill have common sub-structures whenever theytransduce the same source sub-tree into a targetsub-tree.
This is captured in a chart, leading toan efficient bottom-up algorithm.
The asymptotictime complexity of this search is O(SR) where Sis the number of source nodes andR is the numberof rules matching a given node.Training The model is trained usingSVMstruct, a large margin method for structuredoutput problems (Joachims, 2005; Tsochantaridiset al, 2005).
This training method allows the useof a configurable loss function, ?
(y?,y), whichmeasures the extent to which the model?s predic-tion, y, differs from the reference, y?.
Centralto training is the search for a derivation whichis both high scoring and has high loss comparedto the gold standard.5This requires finding themaximiser of H(y) in one of:Hs= (1?
??(y?)??
(y), ??)?
(y?,y)Hm= ?(y?,y)?
??(y?)??
(y), ??
(3)where the subscripts s and m denote slack andmargin rescaling, which are different formulationsof the training problem (see Tsochantaridis et al(2005) and Taskar et al (2003) for details).The search for the maximiser of H(y) in (3)requires the tracking of the loss value.
This canbe achieved by extending the decoding algorithmsuch that the chart cells also store the loss param-eters (e.g., for precision, the number of true andfalse positives (Joachims, 2005)).
Consequently,this extension leads to a considerably higher timeand space complexity compared to decoding.
Forexample, with precision loss the time complexityis O(S3R) as each step must consider O(S2) pos-sible loss parameter values.4 ExtensionsIn this section we present our extensions of Cohnand Lapata?s (2007) model.
The latter was de-signed with the simpler extractive compression inmind and cannot be readily applied to our task.5Spurious ambiguity in the grammar means that there areoften many derivations linking the source and target.
We fol-low Cohn and Lapata (2007) by choosing the derivation withthe most rules, which should provide good generalisation.Grammar It is relatively straightforward toextract a grammar from our corpus.
This grammarwill contain many rules encoding deletions andstructural transformations but there will be manyunobserved paraphrases, no matter how good theextraction method (recall that our corpus consistssolely of 565 sentences).
For this reason, we ex-tract a grammar from our abstractive corpus in themanner of Cohn and Lapata (2007) (see Section 5for details) and augment it with a larger gram-mar obtained from a parallel bilingual corpus.
Cru-cially, our second grammar will not contain com-pression rules, just paraphrasing ones.
We leave itto the model to learn which rules serve the com-pression objective.Our paraphrase grammar extraction methoduses bilingual pivoting to learn paraphrases oversyntax tree fragments, i.e., STSG rules.
Pivotingtreats the paraphrasing problem as a two-stagetranslation process.
Some English text is translatedto a foreign language, and then translated back intoEnglish (Bannard and Callison-Burch, 2005):p(e?|e) =?fp(e?|f)p(f |e) (4)where p(f |e) is the probability of translatingan English string e into a foreign string f andp(e?|f) the probability of translating the same for-eign string into some other English string e?.
Wethus obtain English-English translation probabili-ties p(e?|e) by marginalizing out the foreign text.Instead of using strings (Bannard and Callison-Burch, 2005), we use elementary trees on the En-glish side, resulting in a monolingual STSG.
Weobtain the elementary trees and foreign strings us-ing the GKHM algorithm (Galley et al, 2004).This takes as input a bilingual word-aligned corpuswith trees on one side, and finds the minimal setof tree fragments and their corresponding stringswhich is consistent with the word alignment.
Thisprocess is illustrated in Figure 2 where the alignedpair on the left gives rise to the rules shown onthe right.
Note that the English rules and for-eign strings shown include variable indices wherethey have been generalised.
We estimate p(f |e)and p(e?|f) from the set of tree-to-string rulesand then then pivot each tree fragment to produceSTSG rules.
Figure 3 illustrates the process for the[VP does not VP] fragment.Modeling and Decoding Our grammar ismuch larger and noisier than a grammar extractedsolely for deletion-based compression.
So, in or-der to encourage coherence and inform lexical se-140SNP VPVBZdoesRBgoHe notne pasIl vaPRP VPNPHeIlPRPgovaVPVPVBZdoesRBnotne    pasVPSNP VP12111 2Figure 2: Tree-to-string grammar extraction using the GHKMalgorithm, showing the aligned sentence pair and the resultingrules as tree fragments and their matching strings.
The boxednumbers denote variables.VPVBZdoesRBnotne     pasVPn 'nene peut...VPMDwillRBnotVBVPVBPdoRBnotVB1111111Figure 3: Pivoting the [VP does not VP] fragment.lection we incorporate a ngram language model(LM) as a feature.
This requires adapting the scor-ing function, ?, in (2) to allow features over targetngrams:?
(y) =?r?y??
(r, S(y)), ?
?+?m?T (y)??
(m,S(y)), ??
(5)where m are the ngrams and ?
is a new fea-ture function over these ngrams (we use only onengram feature: the trigram log-probability).
Sadly,the scoring function in (5) renders the chart-basedsearch used for training and decoding intractable.In order to provide sufficient context to the chart-based algorithm, we must also store in each chartcell the n ?
1 target tokens at the left and rightedges of its yield.
This is equivalent to using asour grammar the intersection between the originalgrammar and the ngram LM (Chiang, 2007), andincreases the decoding complexity to an infeasibleO(SRL2(n?1)V)whereL is the size of the lexicon.We adopt a popular approach in syntax-inspiredmachine translation to address this problem (Chi-ang, 2007).
The idea is to use a beam-search overthe intersection grammar coupled with the cube-pruning heuristic.
The beam limits the number ofitems in a given chart cell to a fixed constant, re-gardless of the number of possible LM contextsand non-terminal categories.
Cube-pruning furtherlimits the number of items considered for inclu-sion in the beam, reducing the time complexityto a more manageable O(SRBV ) where B is thebeam size.
We refer the interested reader to Chiang(2007) for details.Training The extensions to the model in (5)also necessitate changes in the training proce-dure.
Recall that training the basic model of Cohnand Lapata (2007) requires finding the maximiserof H(y) in (3).
Their model uses a chart-based al-gorithm for this purpose.
As in decoding we alsouse a beam search for training, thereby avoidingthe exponential time complexity of exact search.The beam search requires an estimate of the qual-ity for incomplete derivations.
We use the marginrescaling objective, Hmin (3), and approximatethe loss using the current (incomplete) loss param-eter values in each chart cell.
We use a wide beamof 200 unique items or 500 items in total to reducethe impact of the approximation.Our loss functions are tailored to the task anddraw inspiration from metrics developed for ex-tractive compression but also for summarisationand machine translation.
They are based on theHamming distance over unordered bags of items.This measures the number of predicted items thatdid not appear in the reference, along with apenalty for short output:?hamming(y?,y) = f+max (l ?
(t+ f), 0) (6)where t and f are the number of true and falsepositives, respectively, when comparing the pre-dicted target, y, with the reference, y?, and l isthe length of the reference.
The second term pe-nalises short output, as predicting very little ornothing would otherwise be unpenalised.
We havethree Hamming loss functions over: 1) tokens,2) ngrams (n ?
3), or 3) CFG productions.
Theselosses all operate on unordered bags and there-fore might reward erroneous predictions.
For ex-ample, a permutation of the reference tokens haszero token-loss.
The CFG and ngram losses haveoverlapping items which encode a partial order,and therefore are less affected.In addition, we developed a fourth loss func-tion to measure the edit distance between themodel?s prediction and the reference, both as bags-of-tokens.
This measures the number of insertionsand deletions.
In contrast to the previous loss func-tions, this requires the true positive counts to beclipped to the number of occurrences of each typein the reference.
The edit distance is given by:?edit(y?,y) = p+ q ?
2?imin(pi, qi) (7)where p and q denote the number of target tokensin the predicted and reference derivation, respec-tively, and piand qiare the counts for type i.141?ADJP,NP?
?
?subject [PP to NP1], part [PP of NP1]?
(T)?ADVP,RB?
?
?as well, also?
(T)?ADJP,JJ?
?
?too little, insufficient?
(P)?S,S?
?
?S1and S2, S2and S1?
(P)?NP,NP?
?
?DT1NN2, DT1NN2?
(S)?NP,NP?
?
?DT1NN2, NN2?
(S)Table 2: Sample grammar rules extracted from the trainingset (T), pivoted set (P) or generated from the source (S).5 Experimental DesignIn this section we present our experimental set-up for assessing the performance of our model.We give details on the corpora and grammars weused, model parameters and features,6the baselineused for comparison with our approach, and ex-plain how our system output was evaluated.Grammar Extraction Our grammar usedrules extracted directly from our compression cor-pus (the training partition, 480 sentences) and abilingual corpus (see Table 2 for examples).
Theformer corpus was word-aligned using the Berke-ley aligner (Liang et al, 2006) initialised witha lexicon of word identity mappings, and parsedwith Bikel?s (2002) parser.
From this we extractedgrammar rules following the technique describedin Cohn and Lapata (2007).
For the pivot grammarwe use the French-English Europarl v2 which con-tains approximately 688K sentences.
Again, thecorpus was aligned using the Berkeley aligner andthe English side was parsed with Bikel?s parser.
Weextracted tree-to-string rules using our implemen-tation of the GHKM method.
To ameliorate the ef-fects of poor alignments on the grammar, we re-moved singleton rules before pivoting.In addition to the two grammars described, wescanned the source trees in the compression cor-pus and included STSG rules to copy each CFGproduction or delete up to two of its children.
Thisis illustrated in Table 2 where the last two rules arederived from the CFG production NP?DT NN inthe source tree.
All trees are rooted with a distin-guished TOP non-terminal which allows the ex-plicit modelling of sentence spanning sub-trees.These grammars each had 44,199 (pivot), 7,813(train) and 22,555 (copy) rules.
We took theirunion, resulting in 58,281 unique rules and 13,619unique source elementary trees.Model Parameters Our model was trainedon 480 sentences, 36 sentences were used for de-velopment and 59 for testing.
We used a varietyof syntax-based, lexical and compression-specific6The software and corpus can be downloaded fromhttp://homepages.inf.ed.ac.uk/tcohn/paraphrase.For every rule:origin of rulefor each origin, o: log po(s, t), log po(s|t), log po(t|s)sR, tR, sR?
tRs, t, s ?
t, s = tboth s and t are pre-terminals and s = t or s 6= tnumber of terminals/variables/dropped variablesordering of variables as numbers/non-terminalsnon-terminal sequence of vars identical after reorderingpre-terminal or terminal sequences are identicalnumber/identity of common/inserted/dropped terminalssource is shorter/longer than targettarget is a compression of the source using deletesFor every ngram :log p(wi|wi?1i?
(n?1))Table 3: The feature set.
Rules were drawn from the trainingset, bilingual pivoting and directly from the source trees.
s andt are the source and target elementary trees in a rule, the sub-scriptRreferences the root non-terminal, w are the terminalsin the target tree.features (196,419 in total).
These are summarisedin Table 3.
We also use a trigram language modeltrained on the BNC (100 million words) using theSRI Language Modeling toolkit (Stolcke, 2002),with modified Kneser-Ney smoothing.An important parameter in our modeling frame-work is the choice of loss function.
We evaluatedthe loss functions presented in Section 4 on the de-velopment set.
We ran our system for each of thefour loss functions and asked two human judgesto rate the output on a scale of 1 to 5.
The Ham-ming loss over tokens performed best with a meanrating of 3.18, closely followed by the edit dis-tance (3.17).
We chose the former over the latteras it is less coarsely approximated during search.Baseline There are no existing models thatcan be readily trained on our abstractive com-pression data.
Instead, we use Cohn and Lapata?s(2007) extractive model as a baseline.
The latterwas trained on an extractive compression corpusdrawn from the BNC (Clarke, 2008) and tunedto provide a similar compression rate to our sys-tem.
Note that their model is a strong baseline:it performed significantly better than competitiveapproaches (McDonald, 2006) across a variety ofcompression corpora.Evaluation Methodology Sentence compres-sion output is commonly evaluated by elicitinghuman judgments.
Following Knight and Marcu(2002), we asked participants to rate the grammati-cality of the target compressions and howwell theypreserved the most important information fromthe source.
In both cases they used a five pointrating scale where a high number indicates bet-ter performance.
We randomly selected 30 sen-tences from the test portion of our corpus.
These142Models Grammaticality Importance CompRExtract 3.10?2.43?82.5Abstract 3.38?2.85??
79.2Gold 4.51 4.02 58.4Table 4: Mean ratings on compression output elicited by hu-mans;?
: significantly different from the gold standard;?
: sig-nificantly different from the baseline.sentences were compressed automatically by ourmodel and the baseline.
We also included goldstandard compressions.
Our materials thus con-sisted of 90 (30 ?
3) source-target sentences.
Wecollected ratings from 22 unpaid volunteers, allself reported native English speakers.
Both studieswere conducted over the Internet using a custombuilt web interface.6 ResultsOur results are summarised in Table 4, where weshow the mean ratings for our system (Abstract),the baseline (Extract), and the gold standard.
Wefirst performed an Analysis of Variance (ANOVA)to examine the effect of different system compres-sions.
The ANOVA revealed a reliable effect onboth grammaticality and importance (significantover both subjects and items (p < 0.01)).We next examined in more detail between-system differences.
Post-hoc Tukey tests revealedthat our abstractive model received significantlyhigher ratings than the baseline in terms of impor-tance (?
< 0.01).
We conjecture that this is dueto the synchronous grammar we employ whichis larger and more expressive than the baseline.In the extractive case, a word sequence is eitherdeleted or retained.
We may, however, want to re-tain the meaning of the sequence while renderingthe sentence shorter, and this is precisely what ourmodel can achieve, e.g., by allowing substitutions.As far as grammaticality is concerned, our abstrac-tive model is numerically better than the extrac-tive baseline but the difference is not statisticallysignificant.
Note that our model has to work a lotharder than the baseline to preserve grammatical-ity since we allow arbitrary rewrites which maylead to agreement or tense mismatches, and selec-tional preference violations.
The scope for errors isgreatly reduced when performing solely deletions.Finally, both the abstractive and extractive out-puts are perceived as significantly worse than thegold standard both in terms of grammaticalityand importance (?
< 0.01).
This is not surpris-ing: human-authored compressions are more fluentand tend to omit genuinely superfluous informa-tion.
This is also mirrored in the compression ratesshown in Table 4.
When compressing, humans em-O: Kurtz came from Missouri, and at the age of 14, hitch-hiked to Los Angeles seeking top diving coaches.E: Kurtz came from Missouri, and at 14, hitch-hiked to LosAngeles seeking top diving coaches.A: Kurtz hitch-hiked to Los Angeles seeking top divingcoaches.G: Kurtz came from Missouri, and at 14, hitch-hiked to LosAngeles seeking diving coaches.O: The scheme was intended for people of poor or moderatemeans.E: The scheme was intended for people of poor means.A: The scheme was planned for poor people.G: The scheme was intended for the poor.O: He died last Thursday at his home from complicationsfollowing a fall, said his wife author Margo Kurtz.E: He died last at his home from complications following afall, said wife, author Margo Kurtz.A: His wife author Margo Kurtz died from complicationsafter a decline.G: He died from complications following a fall.O: But a month ago, she returned to Britain, taking the chil-dren with her.E: She returned to Britain, taking the children.A: But she took the children with him.G: But she returned to Britain with the children.Table 5: Compression examples including human and systemoutput (O: original sentence, E: Extractive model, A: Abstrac-tive model, G: gold standard)ploy not only linguistic but also world knowledgewhich is not accessible to our model.
Although thesystem can be forced to match the human compres-sion rate, the grammaticality and information con-tent both suffer.
More sophisticated features couldallow the system to narrow this gap.We next examined the output of our system inmore detail by recording the number of substitu-tions, deletions and insertions it performed on thetest data.
Deletions accounted for 67% of rewriteoperations, substitutions for 27%, and insertionsfor 6%.
Interestingly, we observe a similar ratioin the human compressions.
Here, deletions arealso the most common rewrite operation (69%) fol-lowed by substitutions (24%), and insertions (7%).The ability to perform substitutions and insertionsincreases the compression potential of our system,but can also result in drastic meaning changes.
Inmost cases (63%) the compressions produced byour system did not distort the meaning of the orig-inal.
Humans are clearly better at this, 96.5% oftheir compressions were meaning preserving.We illustrate example output of our system inTable 5.
For comparison we also present the goldstandard compressions and baseline output.
In thefirst sentence the system rendered Kurtz the sub-ject of hitch-hiked.
At the same time it deleted theverb and its adjunct from the first conjunct (camefrom Missouri ) as well as the temporal modi-fier at the age of 14 from the second conjunct.The second sentence shows some paraphrasing:the verb intended is substituted with planned and143poor is now modifying people rather than means.In the third example, our system applies multi-ple rewrites.
It deletes last Thursday at his home,moves wife author Margo Kurtz to the subject po-sition, and substitutes fall with decline.
Unfortu-nately, the compressed sentence expresses a ratherdifferent meaning from the original.
It is not MargoKurtz who died but her husband.
Finally, our lastsentence illustrates a counter-intuitive substitution,the pronoun her is rewritten as him.
This is becausethey share the French translation lui and thus piv-oting learns to replace the less common word (inlegal corpora) her with him.
This problem couldbe addressed by pivoting over multiple bitexts withdifferent foreign languages.Possible extensions and improvements to thecurrent model are many and varied.
Firstly, ashinted at above, the model would benefit from ex-tensive feature engineering, including source con-ditioned features and ngram features besides theLM.
A richer grammar would also boost perfor-mance.
This could be found by pivoting over morebitexts in many foreign languages or making useof existing or paraphrase corpora.
Finally, we planto apply the model to other paraphrasing tasks in-cluding fully abstractive document summarisation(Daum?e III and Marcu, 2002).AcknowledgementsThe authors acknowledge the support of EPSRC(grants GR/T04540/01 and GR/T04557/01).Special thanks to Phil Blunsom, James Clarke andMiles Osborne for their insightful suggestions.ReferencesC.
Bannard, C. Callison-Burch.
2005.
Paraphrasingwith bilingual parallel corpora.
In Proceedings ofthe 43rd ACL, 255?262, Ann Arbor, MI.R.
Barzilay.
2003.
Information Fusion for Multi-Document Summarization: Praphrasing and Gener-ation.
Ph.D. thesis, Columbia University.D.
Bikel.
2002.
Design of a multi-lingual, parallel-processing statistical parsing engine.
In Proceedingsof the HLT, 24?27, San Diego, CA.J.
Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin,J.
Tait.
1999.
Simplifying text for language impairedreaders.
In Proceedings of the 9th EACL, 269?270,Bergen, Norway.D.
Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.J.
Clarke.
2008.
Global Inference for Sentence Com-pression: An Integer Linear Programming Approach.Ph.D.
thesis, University of Edinburgh.T.
Cohn, M. Lapata.
2007.
Large margin synchronousgeneration and its application to sentence compres-sion.
In Proceedings of the EMNLP/CoNLL, 73?82,Prague, Czech Republic.S.
Corston-Oliver.
2001.
Text Compaction for Dis-play on Very Small Screens.
In Proceedings of theNAACL Workshop on Automatic Summarization, 89?98, Pittsburgh, PA.H.
Daum?e III, D. Marcu.
2002.
A noisy-channel modelfor document compression.
In Proceedings of the40th ACL, 449?456, Philadelphia, PA.J.
Eisner.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In Proceedings ofthe ACL Interactive Poster/Demonstration Sessions,205?208, Sapporo, Japan.M.
Galley, K. McKeown.
2007.
Lexicalized Markovgrammars for sentence compression.
In Proceedingsof the NAACL/HLT, 180?187, Rochester, NY.M.
Galley, M. Hopkins, K. Knight, D. Marcu.
2004.What?s in a translation rule?
In Proceedings of theHLT/NAACL, 273?280, Boston, MA.H.
Jing.
2000.
Sentence reduction for automatic textsummarization.
In Proceedings of the ANLP, 310?315, Seattle, WA.T.
Joachims.
2005.
A support vector method for multi-variate performance measures.
In Proceedings of the22nd ICML, 377?384, Bonn, Germany.K.
Knight, D. Marcu.
2002.
Summarization be-yond sentence extraction: a probabilistic approachto sentence compression.
Artificial Intelligence,139(1):91?107.P.
Liang, B. Taskar, D. Klein.
2006.
Alignment byagreement.
In Proceedings of the HLT/NAACL, 104?111, New York, NY.C.-Y.
Lin.
2003.
Improving summarization perfor-mance by sentence compression ?
a pilot study.
InProceedings of the 6th International Workshop onInformation Retrieval with Asian Languages, 1?8,Sapporo, Japan.R.
McDonald.
2006.
Discriminative sentence com-pression with soft syntactic constraints.
In Proceed-ings of the 11th EACL, 297?304, Trento, Italy.K.
Papineni, S. Roukos, T. Ward, W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In Proceedings of the 40th ACL,311?318, Philadelphia, PA.C.
Quirk, C. Brockett, W. Dolan.
2004.
Monolin-gual machine translation for paraphrase generation.In Proceedings of the EMNLP, 142?149, Barcelona,Spain.A.
Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proceedings of the ICSLP, Den-ver, CO.B.
Taskar, C. Guestrin, D. Koller.
2003.
Max marginMarkov networks.
In Proceedings of NIPS 16.I.
Tsochantaridis, T. Joachims, T. Hofmann, Y. Altun.2005.
Large margin methods for structured and in-terdependent output variables.
Journal of MachineLearning Research, 6:1453?1484.J.
Turner, E. Charniak.
2005.
Supervised and unsu-pervised learning for sentence compression.
In Pro-ceedings of 43rd ACL, 290?297, Ann Arbor, MI.V.
Vandeghinste, Y. Pan.
2004.
Sentence compressionfor automated subtitling: A hybrid approach.
In Pro-ceedings of the ACL Workshop on Text Summariza-tion, 89?95, Barcelona, Spain.144
