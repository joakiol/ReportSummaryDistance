Decomposition Kernels for Natural Language ProcessingFabrizio Costa Sauro Menchetti Alessio CeroniDipartimento di Sistemi e Informatica,Universita` degli Studi di Firenze,via di S. Marta 3, 50139 Firenze, Italy{costa,menchett,passerini,aceroni,p-f} AT dsi.unifi.itAndrea Passerini Paolo FrasconiAbstractWe propose a simple solution to the se-quence labeling problem based on an ex-tension of weighted decomposition ker-nels.
We additionally introduce a multi-instance kernel approach for representinglexical word sense information.
Thesenew ideas have been preliminarily testedon named entity recognition and PP at-tachment disambiguation.
We finally sug-gest how these techniques could be poten-tially merged using a declarative formal-ism that may provide a basis for the inte-gration of multiple sources of informationwhen using kernel-based learning in NLP.1 IntroductionMany tasks related to the analysis of natural lan-guage are best solved today by machine learningand other data driven approaches.
In particular,several subproblems related to information extrac-tion can be formulated in the supervised learningframework, where statistical learning has rapidlybecome one of the preferred methods of choice.A common characteristic of many NLP problemsis the relational and structured nature of the rep-resentations that describe data and that are inter-nally used by various algorithms.
Hence, in or-der to develop effective learning algorithms, it isnecessary to cope with the inherent structure thatcharacterize linguistic entities.
Kernel methods(see e.g.
Shawe-Taylor and Cristianini, 2004) arewell suited to handle learning tasks in structureddomains as the statistical side of a learning algo-rithm can be naturally decoupled from any rep-resentational details that are handled by the ker-nel function.
As a matter of facts, kernel-basedstatistical learning has gained substantial impor-tance in the NLP field.
Applications are numerousand diverse and include for example refinementof statistical parsers (Collins and Duffy, 2002),tagging named entities (Cumby and Roth, 2003;Tsochantaridis et al, 2004), syntactic chunking(Daume?
III and Marcu, 2005), extraction of rela-tions between entities (Zelenko et al, 2003; Cu-lotta and Sorensen, 2004), semantic role label-ing (Moschitti, 2004).
The literature is rich withexamples of kernels on discrete data structuressuch as sequences (Lodhi et al, 2002; Leslie etal., 2002; Cortes et al, 2004), trees (Collins andDuffy, 2002; Kashima and Koyanagi, 2002), andannotated graphs (Ga?rtner, 2003; Smola and Kon-dor, 2003; Kashima et al, 2003; Horva?th et al,2004).
Kernels of this kind can be almost in-variably described as special cases of convolu-tion and other decomposition kernels (Haussler,1999).
Thanks to its generality, decompositionis an attractive and flexible approach for definingthe similarity between structured objects startingfrom the similarity between smaller parts.
How-ever, excessively large feature spaces may resultfrom the combinatorial growth of the number ofdistinct subparts with their size.
When too manydimensions in the feature space are irrelevant, theGram matrix will be nearly diagonal (Scho?lkopfet al, 2002), adversely affecting generalization inspite of using large margin classifiers (Ben-Davidet al, 2002).
Possible cures include extensive useof prior knowledge to guide the choice of rele-vant parts (Cumby and Roth, 2003; Frasconi et al,2004), the use of feature selection (Suzuki et al,2004), and soft matches (Saunders et al, 2002).
In(Menchetti et al, 2005) we have shown that bettergeneralization can indeed be achieved by avoid-ing hard comparisons between large parts.
In a17weighted decomposition kernel (WDK) only smallparts are matched, whereas the importance of thematch is determined by comparing the sufficientstatistics of elementary probabilistic models fit-ted on larger contextual substructures.
Here weintroduce a position-dependent version of WDKthat can solve sequence labeling problems withoutsearching the output space, as required by other re-cently proposed kernel-based solutions (Tsochan-taridis et al, 2004; Daume?
III and Marcu, 2005).The paper is organized as follows.
In the nexttwo sections we briefly review decomposition ker-nels and its weighted variant.
In Section 4 we in-troduce a version of WDK for solving supervisedsequence labeling tasks and report a preliminaryevaluation on a named entity recognition problem.In Section 5 we suggest a novel multi-instance ap-proach for representing WordNet information andpresent an application to the PP attachment am-biguity resolution problem.
In Section 6 we dis-cuss how these ideas could be merged using adeclarative formalism in order to integrate mul-tiple sources of information when using kernel-based learning in NLP.2 Decomposition KernelsAn R-decomposition structure (Haussler, 1999;Shawe-Taylor and Cristianini, 2004) on a set X isa triple R = ?
~X , R,~k?
where ~X = (X1, .
.
.
,XD)is a D?tuple of non?empty subsets of X , R isa finite relation on X1 ?
?
?
?
?
XD ?
X , and~k = (k1, .
.
.
, kD) is a D?tuple of positive defi-nite kernel functions kd : Xd ?
Xd 7?
IR.
R(~x, x)is true iff ~x is a tuple of ?parts?
for x ?
i.e.
~xis a decomposition of x.
Note that this defini-tion of ?parts?
is very general and does not re-quire the parthood relation to obey any specificmereological axioms, such as those that will beintroduced in Section 6.
For any x ?
X , letR?1(x) = {(x1, .
.
.
, xD) ?
~X : R(~x, x)} de-note the multiset of all possible decompositions1of x.
A decomposition kernel is then defined asthe multiset kernel between the decompositions:KR(x, x?)
=?~x ?
R?1(x)~x?
?
R?1(x?
)D?d=1?d(xd, x?d) (1)1Decomposition examples in the string domain includetaking all the contiguous fixed-length substrings or all thepossible ways of dividing a string into two contiguous sub-strings.where, as an alternative way of combining the ker-nels, we can use the product instead of a summa-tion: intuitively this increases the feature space di-mension and makes the similarity measure moreselective.
Since decomposition kernels form arather vast class, the relation R needs to be care-fully tuned to different applications in order tocharacterize a suitable kernel.
As discussed inthe Introduction, however, taking all possible sub-parts into account may lead to poor predictivity be-cause of the combinatorial explosion of the featurespace.3 Weighted Decomposition KernelsA weighted decomposition kernel (WDK) is char-acterized by the following decomposition struc-ture:R = ?
~X , R, (?, ?1, .
.
.
, ?D)?where ~X = (S,Z1, .
.
.
, ZD), R(s, z1, .
.
.
, zD, x)is true iff s ?
S is a subpart of x called the selectorand ~z = (z1, .
.
.
, zD) ?
Z1??
?
?
?ZD is a tuple ofsubparts of x called the contexts of s in x. Precisedefinitions of s and ~z are domain-dependent.
Forexample in (Menchetti et al, 2005) we present twoformulations, one for comparing whole sequences(where both the selector and the context are subse-quences), and one for comparing attributed graphs(where the selector is a single vertex and the con-text is the subgraph reachable from the selectorwithin a short path).
The definition is completedby introducing a kernel on selectors and a kernelon contexts.
The former can be chosen to be theexact matching kernel, ?, on S ?
S, defined as?
(s, s?)
= 1 if s = s?
and ?
(s, s?)
= 0 otherwise.The latter, ?d, is a kernel on Zd ?
Zd and pro-vides a soft similarity measure based on attributefrequencies.
Several options are available for con-text kernels, including the discrete version of prob-ability product kernels (PPK) (Jebara et al, 2004)and histogram intersection kernels (HIK) (Odoneet al, 2005).
Assuming there are n categoricalattributes, each taking on mi distinct values, thecontext kernel can be defined as:?d(z, z?)
=n?i=1ki(z, z?)
(2)where ki is a kernel on the i-th attribute.
Denote bypi(j) the observed frequency of value j in z. Then18ki can be defined as a HIK or a PPK respectively:ki(z, z?)
=mi?j=1min{pi(j), p?i(j)} (3)ki(z, z?)
=mi?j=1?pi(j) ?
p?i(j) (4)This setting results in the following general formof the kernel:K(x, x?)
=?
(s, ~z) ?
R?1(x)(s?, ~z?)
?
R?1(x?)?
(s, s?
)D?d=1?d(zd, z?d) (5)where we can replace the summation of kernelswith?Dd=1 1 + ?d(zd, z?d).Compared to kernels that simply count the num-ber of substructures, the above function weightsdifferent matches between selectors according tocontextual information.
The kernel can be after-wards normalized in [?1, 1] to prevent similarityto be boosted by the mere size of the structuresbeing compared.4 WDK for sequence labeling andapplications to NERIn a sequence labeling task we want to map inputsequences to output sequences, or, more precisely,we want to map each element of an input sequencethat takes label from a source alphabet to an ele-ment with label in a destination alphabet.Here we cast the sequence labeling task intoposition specific classification, where different se-quence positions give independent examples.
Thisis different from previous approaches in the lit-erature where the sequence labeling problem issolved by searching in the output space (Tsochan-taridis et al, 2004; Daume?
III and Marcu, 2005).Although the method lacks the potential for col-lectively labeling all positions simultaneously, itresults in a much more efficient algorithm.In the remainder of the section we introducea specialized version of the weighted decompo-sition kernel suitable for a sequence transductiontask originating in the natural language process-ing domain: the named entity recognition (NER)problem, where we map sentences to sequences ofa reduced number of named entities (see Sec.4.1).More formally, given a finite dictionary ?
ofwords and an input sentence x ?
?
?, our input ob-jects are pairs of sentences and indices r = (x, t)Figure 1: Sentence decomposition.where r ?
??
?
IN.
Given a sentence x, two in-tegers b ?
1 and b ?
e ?
|x|, let x[b] denote theword at position b and x[b..e] the sub-sequence ofx spanning positions from b to e. Finally we willdenote by ?
(x[b]) a word attribute such as a mor-phological trait (is a number or has capital initial,see 4.1) for the word in sentence x at position b.We introduce two versions of WDK: one withfour context types (D = 4) and one with in-creased contextual information (D = 6) (seeEq.
5).
The relation R depends on two integerst and i ?
{1, .
.
.
, |x|}, where t indicates the po-sition of the word we want to classify and i theposition of a generic word in the sentence.
Therelation for the first kernel version is defined as:R = {(s, zLL, zLR, zRL, zRR, r)} such that theselector s = x[i] is the word at position i, the zLL(LeftLeft) part is a sequence defined as x[1..i] ifi < t or the null sequence ?
otherwise and thezLR (LeftRight) part is the sequence x[i + 1..t] ifi < t or ?
otherwise.
Informally, zLL is the initialportion of the sentence up to word of position i,and zLR is the portion of the sentence from wordat position i + 1 up to t (see Fig.
1).
Note thatwhen we are dealing with a word that lies to theleft of the target word t, its zRL and zRR parts areempty.
Symmetrical definitions hold for zRL andzRR when i > t. We define the weighted decom-position kernel for sequences asK(r, r?)=|x|?t=1|x?|?t?=1??
(s, s?)?d?{LL,LR,RL,RR}?
(zd, z?d) (6)where ??
(s, s?)
= 1 if ?
(s) = ?(s?)
and 0 oth-erwise (that is ??
checks whether the two selectorwords have the same morphological trait) and ?is Eq.
2 with only one attribute which then boilsdown to Eq.
3 or Eq.
4, that is a kernel over the his-togram for word occurrences over a specific part.Intuitively, when applied to word sequences,this kernel considers separately words to the left19of the entry we want to transduce and those toits right.
The kernel computes the similarity foreach sub-sequence by matching the correspondingbag of enriched words: each word is matched onlywith words that have the same trait as extracted by?
and the match is then weighted proportionally tothe frequency count of identical words precedingand following it.The kernel version with D=6 adds two partscalled zLO (LeftOther) and zRO (RightOther) de-fined as x[t+1..|r|] and x[1..t] respectively; theserepresent the remaining sequence parts so that x =zLL ?
zLR ?
zLO and x = zRL ?
zRR ?
zRO.Note that the WDK transforms the sentencein a bag of enriched words computed in a pre-processing phase thus achieving a significant re-duction in computational complexity (compared tothe recursive procedure in (Lodhi et al, 2002)).4.1 Named Entity Recognition ExperimentalResultsNamed entities are phrases that contain the namesof persons, organizations, locations, times andquantities.
For example in the following sentence:[PER Wolff ] , currently a journalist in [LOCArgentina ] , played with [PER Del Bosque ] in thefinal years of the seventies in [ORG Real Madrid].we are interested in predicting that Wolff and DelBosque are people?s names, that Argentina is aname of a location and that Real Madrid is a nameof an organization.The chosen dataset is provided by the sharedtask of CoNLL?2002 (Saunders et al, 2002)which concerns language?independent named en-tity recognition.
There are four types of phrases:person names (PER), organizations (ORG), loca-tions (LOC) and miscellaneous names (MISC),combined with two tags, B to denote the first itemof a phrase and I for any non?initial word; all otherphrases are classified as (OTHER).
Of the twoavailable languages (Spanish and Dutch), we runexperiments only on the Spanish dataset which is acollection of news wire articles made available bythe Spanish EFE News Agency.
We select a sub-set of 300 sentences for training and we evaluatethe performance on test set.
For each category, weevaluate the F?=1 measure of 4 versions of WDK:word histograms are matched using HIK (Eq.
3)and the kernels on various parts (zLL, zLR,etc) arecombined with a summation SUMHIK or productPROHIK; alternatively the histograms are com-Table 1: NER experiment D=4CLASS SUMHIS PROHIS SUMPRO PROPROB-LOC 74.33 68.68 72.12 66.47I-LOC 58.18 52.76 59.24 52.62B-MISC 52.77 43.31 46.86 39.00I-MISC 79.98 80.15 77.85 79.65B-ORG 69.00 66.87 68.42 67.52I-ORG 76.25 75.30 75.12 74.76B-PER 60.11 56.60 59.33 54.80I-PER 65.71 63.39 65.67 60.98MICRO F?=1 69.28 66.33 68.03 65.30Table 2: NER experiment with D=6CLASS SUMHIS PROHIS SUMPRO PROPROB-LOC 74.81 73.30 73.65 73.69I-LOC 57.28 58.87 57.76 59.44B-MISC 56.54 64.11 57.72 62.11I-MISC 78.74 84.23 79.27 83.04B-ORG 70.80 73.02 70.48 73.10I-ORG 76.17 78.70 74.26 77.51B-PER 66.25 66.84 66.04 67.46I-PER 68.06 71.81 69.55 69.55MICRO F?=1 70.69 72.90 70.32 72.38bined with a PPK (Eq.
4) obtaining SUMPPK,PROPPK.The word attribute considered for the selectoris a word morphologic trait that classifies a wordin one of five possible categories: normal word,number, all capital letters, only capital initial andcontains non alphabetic characters, while the con-text histograms are computed counting the exactword frequencies.Results reported in Tab.
1 and Tab.
2 show thatperformance is mildly affected by the differentchoices on how to combine information on the var-ious contexts, though it seems clear that increasingcontextual information has a positive influence.Note that interesting preliminary results can beobtained even without the use of any refined lan-guage knowledge, such as part of speech taggingor shallow/deep parsing.5 Kernels for word semantic ambiguityParsing a natural language sentence often involvesthe choice between different syntax structures thatare equally admissible in the given grammar.
Oneof the most studied ambiguity arise when decidingbetween attaching a prepositional phrase either tothe noun phrase or to the verb phrase.
An examplecould be:1. eat salad with forks (attach to verb)2. eat salad with tomatoes (attach to noun)20The resolution of such ambiguities is usually per-formed by the human reader using its past expe-riences and the knowledge of the words mean-ing.
Machine learning can simulate human experi-ence by using corpora of disambiguated phrases tocompute a decision on new cases.
However, giventhe number of different words that are currentlyused in texts, there would never be a sufficientdataset from which to learn.
Adding semantic in-formation on the possible word meanings wouldpermit the learning of rules that apply to entire cat-egories and can be generalized to all the memberwords.5.1 Adding Semantic with WordNetWordNet (Fellbaum, 1998) is an electronic lexi-cal database of English words built and annotatedby linguistic researchers.
WordNet is an exten-sive and reliable source of semantic informationthat can be used to enrich the representation of aword.
Each word is represented in the database bya group of synonym sets (synset), with each synsetcorresponding to an individual linguistic concept.All the synsets contained inWordNet are linked byrelations of various types.
An important relationconnects a synset to its hypernyms, that are its im-mediately broader concepts.
The hypernym (andits opposite hyponym) relation defines a semantichierarchy of synsets that can be represented as adirected acyclic graph.
The different lexical cat-egories (verbs, nouns, adjectives and adverbs) arecontained in distinct hierarchies and each one isrooted by many synsets.Several metrics have been devised to computea similarity score between two words using Word-Net.
In the following we resort to a multiset ver-sion of the proximity measure used in (Siolas andd?Alche Buc, 2000), though more refined alterna-tives are also possible (for example using the con-ceptual density as in (Basili et al, 2005)).
Giventhe acyclic nature of the semantic hierarchies, eachsynset can be represented by a group of paths thatfollows the hypernym relations and finish in one ofthe top level concepts.
Two paths can then be com-pared by counting how many steps from the rootsthey have in common.
This number must then benormalized dividing by the square root of the prod-uct between the path lengths.
In this way one canaccounts for the unbalancing that arise from dif-ferent parts of the hierarchies being differently de-tailed.
Given two paths pi and pi?, let l and l?
betheir lengths and n be the size of their commonpart, the resulting kernel is:k(pi, pi?)
= n?l ?
l?
(7)The demonstration that k is positive definite arisefrom the fact that n can be computed as a posi-tive kernel k?
by summing the exact match ker-nels between the corresponding positions in pi andpi?
seen as sequences of synset identifiers.
Thelengths l and l?
can then be evaluated as k?
(pi, pi)and k?
(pi?, pi?)
and k is the resulting normalizedversion of k?.The kernel between two synsets ?
and ??
canthen be computed by the multi-set kernel (Ga?rtneret al, 2002a) between their corresponding paths.Synsets are organized into forty-five lexicogra-pher files based on syntactic category and logicalgroupings.
Additional information can be derivedby comparing the identifiers ?
and ??
of these fileassociated to ?
and ??.
The resulting synset kernelis:??
(?, ??)
= ?
(?, ??)
+?pi???pi???
?k(pi, pi?)
(8)where ?
is the set of paths originating from ?
andthe exact match kernel ?
(?, ??)
is 1 if ?
?
??
and0 otherwise.
Finally, the kernel ??
between twowords is itself a multi-set kernel between the cor-responding sets of synsets:??
(?, ??)
=????????????
(?, ??)
(9)where ?
are the synsets associated to the word ?.5.2 PP Attachment Experimental ResultsThe experiments have been performed using theWall-Street Journal dataset described in (Ratna-parkhi et al, 1994).
This dataset contains 20, 800training examples and 3, 097 testing examples.Each phrase x in the dataset is reduced to a verbxv, its object noun xn1 and prepositional phraseformed by a preposition xp and a noun xn2 .
Thetarget is either V or N whether the phrase is at-tached to the verb or the noun.
Data have been pre-processed by assigning to all the words their cor-responding synsets.
Additional meanings derivedfrom specific synsets have been attached to thewords as described in (Stetina and Nagao, 1997).The kernel between two phrases x and x?
is thencomputed by combining the kernels between sin-gle words using either the sum or the product.21Method Acc Pre RecS 84.6% ?
0.65% 90.8% 82.2%P 84.8% ?
0.65% 92.2% 81.0%SW 85.4% ?
0.64% 90.9% 83.6%SWL 85.3% ?
0.64% 91.1% 83.2%PW 85.9% ?
0.63% 92.2% 83.1%PWL 86.2% ?
0.62% 92.1% 83.7%Table 3: Summary of the experimental results onthe PP attachment problem for various kernel pa-rameters.Results of the experiments are reported in Tab.
3for various kernels parameters: S or P denote ifthe sum or product of the kernels between wordsare used, W denotes that WordNet semantic infor-mation is added (otherwise the kernel between twowords is just the exact match kernel) and L denotesthat lexicographer files identifiers are used.
An ad-ditional gaussian kernel is used on top ofKpp.
TheC and ?
parameters are selected using an inde-pendent validation set.
For each setting, accuracy,precision and recall values on the test data are re-ported, along with the standard deviation of the es-timated binomial distribution of errors.
The resultsdemonstrate that semantic information can help inresolving PP ambiguities.
A small difference ex-ists between taking the product instead of the sumof word kernels, and an additional increase in theamount of information available to the learner isgiven by the use of lexicographer files identifiers.6 Using declarative knowledge for NLPkernel integrationData objects in NLP often require complex repre-sentations; suffice it to say that a sentence is nat-urally represented as a variable length sequenceof word tokens and that shallow/deep parsers arereliably used to enrich these representations withlinks between words to form parse trees.
Finally,additional complexity can be introduced by in-cluding semantic information.
Various facets ofthis richness of representations have been exten-sively investigated, including the expressivenessof various grammar formalisms, the exploitationof lexical representation (e.g.
verb subcategoriza-tion, semantic tagging), and the use of machinereadable sources of generic or specialized knowl-edge (dictionaries, thesauri, domain specific on-tologies).
All these efforts are capable to addresslanguage specific sub-problems but their integra-tion into a coherent framework is a difficult feat.Recent ideas for constructing kernel functionsstarting from logical representations may offer anappealing solution.
Ga?rtner et al (2002) have pro-posed a framework for defining kernels on higher-order logic individuals.
Cumby and Roth (2003)used description logics to represent knowledgejointly with propositionalization for defining a ker-nel function.
Frasconi et al (2004) proposedkernels for handling supervised learning in a set-ting similar to that of inductive logic programmingwhere data is represented as a collection of factsand background knowledge by a declarative pro-gram in first-order logic.
In this section, we brieflyreview this approach and suggest a possible way ofexploiting it for the integration of different sourcesof knowledge that may be available in NLP.6.1 Declarative KernelsThe definition of decomposition kernels as re-ported in Section 2 is very general and covers al-most all kernels for discrete structured data de-veloped in the literature so far.
Different kernelsare designed by defining the relation decompos-ing an example into its ?parts?, and specifyingkernels for individual parts.
In (Frasconi et al,2004) we proposed a systematic approach to suchdesign, consisting in formally defining a relationby the set of axioms it must satisfy.
We reliedon mereotopology (Varzi, 1996) (i.e.
the theoryof parts and places) in order to give a formal def-inition of the intuitive concepts of parthood andconnection.
The formalization of mereotopolog-ical relations allows to automatically deduce in-stances of such relations on the data, by exploit-ing the background knowledge which is typicallyavailable on structured domains.
In (Frasconi etal., 2004) we introduced declarative kernels (DK)as a set of kernels working on mereotopologicalrelations, such as that of proper parthood (?P) ormore complex relations based on connected parts.A typed syntax for objects was introduced in orderto provide additional flexibility in defining kernelson instances of the given relation.
A basic kernelon parts KP was defined as follows:KP (x, x?
)=?s?P xs?
?P x?
?T (s, s?)(?
(s, s?
)+KP (s, s?
))(10)where ?T matches objects of the same type and ?is a kernel over object attributes.22Declarative kernels were tested in (Frasconi etal., 2004) on a number of domains with promisingresults, including a biomedical information extrac-tion task (Goadrich et al, 2004) aimed at detectingprotein-localization relationships within Medlineabstracts.
A DK on parts as the one defined inEq.
(10) outperformed state-of-the-art ILP-basedsystems Aleph and Gleaner (Goadrich et al, 2004)in such information extraction task, and requiredabout three orders of magnitude less training time.6.2 Weighted Decomposition DeclarativeKernelsDeclarative kernels can be combined with WDKin a rather straightforward way, thus taking the ad-vantages of both methods.
A simple approach isthat of using proper parthood in place of selec-tors, and topology to recover the context of eachproper part.
A weighted decomposition declara-tive kernel (WD2K) of this kind would be definedas in Eq.
(10) simply adding to the attribute ker-nel ?
a context kernel that compares the surround-ing of a pair of objects?as defined by the topol-ogy relation?using some aggregate kernel such asPPK or HIK (see Section 3).
Note that such defini-tion extends WDK by adding recursion to the con-cept of comparison by selector, and DK by addingcontexts to the kernel between parts.
Multiple con-texts can be easily introduced by employing differ-ent notions of topology, provided they are consis-tent with mereotopological axioms.
As an exam-ple, if objects are words in a textual document, wecan define l-connection as the relation for whichtwo words are l-connected if there are consequen-tial within the text with at most l words in be-tween, and obtain growingly large contexts by in-creasing l. Moreover, an extended representationof words, as the one employing WordNet semanticinformation, could be easily plugged in by includ-ing a kernel for synsets such as that in Section 5.1into the kernel ?
on word attributes.
Additionalrelations could be easily formalized in order to ex-ploit specific linguisitc knowledge: a causal rela-tion would allow to distinguish between precedingand following context so to take into considerationword order; an underlap relation, associating twoobjects being parts of the same super-object (i.e.pre-terminals dominated by the same non-terminalnode), would be able to express commanding no-tions.The promising results obtained with declarativekernels (where only very simple lexical informa-tion was used) together with the declarative easeto integrate arbitrary kernels on specific parts areall encouraging signs that boost our confidence inthis line of research.ReferencesRoberto Basili, Marco Cammisa, and Alessandro Mos-chitti.
2005.
Effective use of wordnet seman-tics via kernel-based learning.
In 9th Conferenceon Computational Natural Language Learning, AnnArbor(MI), USA.S.
Ben-David, N. Eiron, and H. U. Simon.
2002.
Lim-itations of learning via embeddings in euclidean halfspaces.
J. of Mach.
Learning Research, 3:441?461.M.
Collins and N. Duffy.
2002.
New ranking algo-rithms for parsing and tagging: Kernels over dis-crete structures, and the voted perceptron.
In Pro-ceedings of the Fortieth Annual Meeting on Associa-tion for Computational Linguistics, pages 263?270,Philadelphia, PA, USA.C.
Cortes, P. Haffner, and M. Mohri.
2004.
Ratio-nal kernels: Theory and algorithms.
J. of MachineLearning Research, 5:1035?1062.A.
Culotta and J. Sorensen.
2004.
Dependency treekernels for relation extraction.
In Proc.
of the 42ndAnnual Meeting of the Association for Computa-tional Linguistics, pages 423?429.C.
M. Cumby and D. Roth.
2003.
On kernel meth-ods for relational learning.
In Proc.
Int.
Conferenceon Machine Learning (ICML?03), pages 107?114,Washington, DC, USA.H.
Daume?
III and D. Marcu.
2005.
Learning as searchoptimization: Approximate large margin methodsfor structured prediction.
In International Confer-ence on Machine Learning (ICML), pages 169?176,Bonn, Germany.C.
Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
The MIT Press.P.
Frasconi, S. Muggleton, H. Lodhi, and A. Passerini.2004.
Declarative kernels.
Technical Report RT2/2004, Universita` di Firenze.T.
Ga?rtner, P. A. Flach, A. Kowalczyk, and A. J. Smola.2002a.
Multi-instance kernels.
In C. Sammut andA.
Hoffmann, editors, Proceedings of the 19th In-ternational Conference on Machine Learning, pages179?186.
Morgan Kaufmann.T.
Ga?rtner, J.W.
Lloyd, and P.A.
Flach.
2002b.
Ker-nels for structured data.
In S. Matwin and C. Sam-mut, editors, Proceedings of the 12th InternationalConference on Inductive Logic Programming, vol-ume 2583 of Lecture Notes in Artificial Intelligence,pages 66?83.
Springer-Verlag.23T.
Ga?rtner.
2003.
A survey of kernels for structureddata.
SIGKDD Explorations Newsletter, 5(1):49?58.M.
Goadrich, L. Oliphant, and J. W. Shavlik.
2004.Learning ensembles of first-order clauses for recall-precision curves: A case study in biomedical infor-mation extraction.
In Proc.
14th Int.
Conf.
on Induc-tive Logic Programming, ILP ?04, pages 98?115.D.
Haussler.
1999.
Convolution kernels on discretestructures.
Technical Report UCSC-CRL-99-10,University of California, Santa Cruz.T.
Horva?th, T. Ga?rtner, and S. Wrobel.
2004.
Cyclicpattern kernels for predictive graph mining.
In Pro-ceedings of the Tenth ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing, pages 158?167.
ACM Press.T.
Jebara, R. Kondor, and A. Howard.
2004.
Proba-bility product kernels.
J. Mach.
Learn.
Res., 5:819?844.H.
Kashima and T. Koyanagi.
2002.
Kernels forSemi?Structured Data.
In Proceedings of the Nine-teenth International Conference on Machine Learn-ing, pages 291?298.H.
Kashima, K. Tsuda, and A. Inokuchi.
2003.Marginalized kernels between labeled graphs.
InProceedings of the Twentieth International Confer-ence on Machine Learning, pages 321?328, Wash-ington, DC, USA.C.
S. Leslie, E. Eskin, and W. S. Noble.
2002.
Thespectrum kernel: A string kernel for SVM proteinclassification.
In Pacific Symposium on Biocomput-ing, pages 566?575.H.
Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristian-ini, and C. Watkins.
2002.
Text classification us-ing string kernels.
Journal of Machine Learning Re-search, 2:419?444.S.
Menchetti, F. Costa, and P. Frasconi.
2005.Weighted decomposition kernels.
In Proceedings ofthe Twenty-second International Conference on Ma-chine Learning, pages 585?592, Bonn, Germany.Alessandro Moschitti.
2004.
A study on convolutionkernels for shallow semantic parsing.
In 42-th Con-ference on Association for Computational Linguis-tic, Barcelona, Spain.F.
Odone, A. Barla, and A. Verri.
2005.
Building ker-nels from binary strings for image matching.
IEEETransactions on Image Processing, 14(2):169?180.A Ratnaparkhi, J. Reynar, and S. Roukos.
1994.
Amaximum entropy model for prepositional phraseattachment.
In Proceedings of the ARPA HumanLanguage Technology Workshop, pages 250?255,Plainsboro, NJ.C.
Saunders, H. Tschach, and J. Shawe-Taylor.
2002.Syllables and other string kernel extensions.
In Pro-ceedings of the Nineteenth International Conferenceon Machine Learning, pages 530?537.B.
Scho?lkopf, J. Weston, E. Eskin, C. S. Leslie, andW.
S. Noble.
2002.
A kernel approach for learn-ing from almost orthogonal patterns.
In Proc.
ofECML?02, pages 511?528.J.
Shawe-Taylor and N. Cristianini.
2004.
KernelMethods for Pattern Analysis.
Cambridge Univer-sity Press.G.
Siolas and F. d?Alche Buc.
2000.
Support vectormachines based on a semantic kernel for text cate-gorization.
In Proceedings of the IEEE-INNS-ENNSInternational Joint Conference on Neural Networks,volume 5, pages 205 ?
209.A.J.
Smola and R. Kondor.
2003.
Kernels and regular-ization on graphs.
In B. Scho?lkopf and M.K.
War-muth, editors, 16th Annual Conference on Compu-tational Learning Theory and 7th Kernel Workshop,COLT/Kernel 2003, volume 2777 of Lecture Notesin Computer Science, pages 144?158.
Springer.J Stetina and M Nagao.
1997.
Corpus based pp attach-ment ambiguity resolution with a semantic dictio-nary.
In Proceedings of the Fifth Workshop on VeryLarge Corpora, pages 66?80, Beijing, China.J.
Suzuki, H. Isozaki, and E. Maeda.
2004.
Convo-lution kernels with feature selection for natural lan-guage processing tasks.
In Proc.
of the 42nd AnnualMeeting of the Association for Computational Lin-guistics, pages 119?126.I.
Tsochantaridis, T. Hofmann, T. Joachims, and Y. Al-tun.
2004.
Support vector machine learning for in-terdependent and structured output spaces.
In Proc.21st Int.
Conf.
on Machine Learning, pages 823?830, Banff, Alberta, Canada.A.C.
Varzi.
1996.
Parts, wholes, and part-whole re-lations: the prospects of mereotopology.
Data andKnowledge Engineering, 20:259?286.D.
Zelenko, C. Aone, and A. Richardella.
2003.
Ker-nel methods for relation extraction.
Journal of Ma-chine Learning Research, 3:1083?1106.24
