Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 507?511,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLinguistically debatable or just plain wrong?Barbara Plank, Dirk Hovy and Anders S?gaardCenter for Language TechnologyUniversity of Copenhagen, DenmarkNjalsgade 140, DK-2300 Copenhagen Sbplank@cst.dk,dirk@cst.dk,soegaard@hum.ku.dkAbstractIn linguistic annotation projects, we typ-ically develop annotation guidelines tominimize disagreement.
However, in thisposition paper we question whether weshould actually limit the disagreementsbetween annotators, rather than embracingthem.
We present an empirical analysisof part-of-speech annotated data sets thatsuggests that disagreements are systematicacross domains and to a certain extend alsoacross languages.
This points to an un-derlying ambiguity rather than random er-rors.
Moreover, a quantitative analysis oftag confusions reveals that the majority ofdisagreements are due to linguistically de-batable cases rather than annotation errors.Specifically, we show that even in the ab-sence of annotation guidelines only 2% ofannotator choices are linguistically unmo-tivated.1 IntroductionIn NLP, we often model annotation as if it re-flected a single ground truth that was guided byan underlying linguistic theory.
If this was true,the specific theory should be learnable from theannotated data.
However, it is well known thatthere are linguistically hard cases (Zeman, 2010),where no theory provides a clear answer, so an-notation schemes commit to more or less arbi-trary decisions.
For example, in parsing auxil-iary verbs may head main verbs, or vice versa,and in part-of-speech (POS) tagging, possessivepronouns may belong to the category of deter-miners or the category of pronouns.
This posi-tion paper argues that annotation projects shouldembrace these hard cases rather than pretend theycan be unambiguously resolved.
Instead of usingoverly specific annotation guidelines, designed tominimize inter-annotator disagreement (Duffieldet al, 2007), and adjudicating between annotatorsof different opinions, we should embrace system-atic inter-annotator disagreements.
To motivatethis, we present an empirical analysis showing1.
that certain inter-annotator disagreements aresystematic, and2.
that actual errors are in fact so infrequent asto be negligible, even when linguists annotatewithout guidelines.The empirical analysis presented below relieson text corpora annotated with syntactic cate-gories or parts-of-speech (POS).
POS is part ofmost linguistic theories, but nevertheless, thereare still many linguistic constructions ?
even veryfrequent ones ?
whose POS analysis is widelydebated.
The following sentences exemplify someof these hard cases that annotators frequentlydisagree on.
Note that we do not claim that bothanalyses in each of these cases (1?3) are equallygood, but that there is some linguistic motivationfor either analysis in each case.
(1) Noam goes out tonightNOUN VERB ADP/PRT ADV/NOUN(2) Noam likes social mediaNOUN VERB ADJ/NOUN NOUN(3) Noam likes his carNOUN VERB DET/PRON NOUNTo substantiate our claims, we first comparethe distribution of inter-annotator disagreementsacross domains and languages, showing that mostdisagreements are systematic (Section 2).
Thissuggests that most annotation differences derivefrom hard cases, rather than random errors.We then collect a corpus of such disagreementsand have experts mark which ones are due to ac-tual annotation errors, and which ones reflect lin-guistically hard cases (Section 3).
The resultsshow that the majority of disagreements are due507to hard cases, and only about 20% of conflict-ing annotations are actual errors.
This suggeststhat inter-annotator agreement scores often hidethe fact that the vast majority of annotations areactually linguistically motivated.
In our case, lessthan 2% of the overall annotations are linguisti-cally unmotivated.Finally, in Section 4, we present an experimenttrying to learn a model to distinguish between hardcases and annotation errors.2 Annotator disagreements acrossdomains and languagesIn this study, we had between 2-10 individual an-notators with degrees in linguistics annotate dif-ferent kinds of English text with POS tags, e.g.,newswire text (PTB WSJ Section 00), transcriptsof spoken language (from a database containingtranscripts of conversations, Talkbank1), as wellas Twitter posts.
Annotators were specifically notpresented with guidelines that would help them re-solve hard cases.
Moreover, we compare profes-sional annotation to that of lay people.
We in-structed annotators to use the 12 universal POStags of Petrov et al (2012).
We did so in or-der to make comparison between existing datasets possible.
Moreover, this allows us to fo-cus on really hard cases, as any debatable case inthe coarse-grained tag set is necessarily also partof the finer-grained tag set.2For each domain,we collected exactly 500 doubly-annotated sen-tences/tweets.
Besides these English data sets, wealso obtained doubly-annotated POS data from theFrench Social Media Bank project (Seddah et al,2012).3All data sets, except the French one, arepublicly available at http://lowlands.ku.dk/.We present disagreements as Hinton diagramsin Figure 1a?c.
Note that the spoken language datadoes not include punctuation.
The correlationsbetween the disagreements are highly significant,with Spearman coefficients ranging from 0.6441http://talkbank.org/2Experiments with variation n-grams on WSJ (Dickinsonand Meurers, 2003) and the French data lead us to estimatethat the fine-to-coarse mapping of POS tags disregards about20% of observed tag-pair confusion types, most of which re-late to fine-grained verb and noun distinctions, e.g.
past par-ticiple versus past in ?[..]
criminal lawyers speculated/VBDvs.
VBN that [..]?.3We mapped POS tags into the universal POS tags usingthe mappings available here: https://code.google.com/p/universal-pos-tags/(spoken and WSJ) to 0.869 (spoken and Twit-ter).
Kendall?s ?
ranges from 0.498 (Twitter andWSJ) to 0.659 (spoken and Twitter).
All diagramshave a vaguely ?dagger?-like shape, with the bladegoing down the diagonal from top left to bot-tom right, and a slightly curved ?hilt?
across thecounter-diagonal, ending in the more pronouncedADP/PRT confusion cells.Disagreements are very similar across all threedomains.
In particular, adpositions (ADP) are con-fused with particles (PRT) (as in the case of ?getout?
); adjectives (ADJ) are confused with nouns(as in ?stone lion?
); pronouns (PRON) are con-fused with determiners (DET) (?my house?
); nu-merals are confused with adjectives, determiners,and nouns (?2nd time?
); and adjectives are con-fused with adverbs (ADV) (?see you later?).
InTwitter, the X category is often confused withpunctuations, e.g., when annotating punctuationacting as discourse continuation marker.Our analyses show that a) experts disagree onthe known hard cases when freely annotating text,and b) that these disagreements are the sameacross text types.
More surprisingly, though, wealso find that, as discussed next, c) roughly thesame disagreements are also observed when com-paring the linguistic intuitions of lay people.More specifically, we had lay annotators on thecrowdsourcing platform Crowdflower re-annotatethe training section of Gimpel et al (2011).
Theycollected five annotations per word.
Only annota-tors that had answered correctly on 4 gold items(randomly chosen from a set of 20 gold itemsprovided by the authors) were allowed to submitannotations.
In total, 177 individual annotatorssupplied answers.
We paid annotators a rewardof $0.05 for 10 items.
The full data set con-tains 14,619 items and is described in further de-tail in Hovy et al (2014).
Annotators were satis-fied with the task (4.5 on a scale from 1 to 5) andfelt that instructions were clear (4.4/5), and the payreasonable (4.1/5).
The crowdsourced annotationsaggregated using majority voting agree with theexpert annotations in 79.54% of the cases.
If wepre-filter the data via Wiktionary and use an item-response model (Hovy et al, 2013) rather than ma-jority voting, the agreement rises to 80.58%.Figure 2 presents the Hinton diagram of the dis-agreements of lay people.
Disagreements are verysimilar to the disagreements between expert an-notators, especially on Twitter data (Figure 1b).508a) b) c)Figure 1: Hinton diagrams of inter-annotator disagreement on (a) excerpt from WSJ (Marcus et al,1993), (b) random Twitter sample, and (c) pre-transcribed spoken language excerpts from talkbank.orgOne difference is that lay people do not confusenumerals very often, probably because they relymore on orthographic cues than on distributionalevidence.
The disagreements are still strongly cor-related with the ones observed with expert anno-tators, but at a slightly lower coefficient (with aSpearman?s ?
of 0.493 and Kendall?s ?
of 0.366for WSJ).Figure 2: Disagreement between lay annotatorsLastly, we compare the disagreements of anno-tators on a French social media data set (Seddah etal., 2012), which we mapped to the universal POStag set.
Again, we see the familiar dagger shape.The Spearman coefficient with English Twitter is0.288; Kendall?s ?
is 0.204.
While the correlationis weaker across languages than across domains, itremains statistically significant (p < 0.001).3 Hard cases and annotation errorsIn the previous section, we demonstrated thatsome disagreements are consistent across domainsand languages.
We noted earlier, though, that dis-agreements can arise both from hard cases andfrom annotation errors.
This can explain someFigure 3: Disagreement on French social mediaof the variation.
In this section, we investigatewhat happens if we weed out obvious errors bydetecting annotation inconsistencies across a cor-pus.
The disagreements that remain are the trulyhard cases.We use a modified version of the a priori algo-rithm introduced in Dickinson and Meurers (2003)to identify annotation inconsistencies.
It worksby collecting ?variation n-grams?, i.e.
the longestsequence of words (n-gram) in a corpus that hasbeen observed with a token being tagged differ-ently in another occurence of the same n-gram inthe same corpus.
The algorithm starts off by look-ing for unigrams and expands them until no longern-grams are found.For each variation n-gram that we found inWSJ-00, i.e, a word in various contexts and thepossible tags associated with it, we present anno-tators with the cross product of contexts and tags.Essentially, we ask for a binary decision: Is the tagplausible for the given context?We used 3 annotators with PhD degrees in lin-guistics.
In total, our data set contains 880 items,509i.e.
440 annotated confusion tag pairs.
The rawagreement was 86%.
Figure 4 shows how trulyhard cases are distributed over tag pairs (dark graybars), as well as the proportion of confusions withrespect to a given tag pair that are truly hard cases(light gray bars).
The figure shows, for instance,that the variation n-gram regarding ADP-ADV isthe second most frequent one (dark gray), andapproximately 70% of ADP-ADV disagreementsare linguistically hard cases (light gray).
NOUN-PRON disagreements are always linguistically de-batable cases, while they are less frequent.Figure 4: Relative frequency of hard casesA survey of hard cases.
To further test the ideaof there being truly hard cases that probably can-not be resolved by linguistic theory, we presentednine linguistics faculty members with 10 of theabove examples and asked them to pick their fa-vorite analyses.
In 8/10 cases, the faculty mem-bers disagreed on the right analysis.4 Learning to detect annotation errorsIn this section, we examine whether we can learna classifier to distinguish between hard cases andannotation errors.
In order to do so, we train a clas-sifier on the annotated data set containing 440 tag-confusion pairs by relying only on surface formfeatures.
If we balance the data set and perform 3-fold cross-validation, a L2-regularized logistic re-gression (L2-LR) model achieves an f1-score fordetecting errors at 70% (cf.
Table 1), which isabove average, but not very impressive.The two classes are apparently not easily sepa-rable using surface form features, as illustrated inf1HARD CASES ERRORSL2-LR 73%(71-77) 70%(65-75)NN 76%(76-77) 71%(68-72)Table 1: Classification resultsFigure 5: Hard cases and errors in 2d-PCAthe two-dimensional plot in Figure 5, obtained us-ing PCA.
The logistic regression decision bound-ary is plotted as a solid, black line.
This is prob-ably also why the nearest neighbor (NN) classi-fier does slightly better, but again, performance israther low.
While other features may reveal thatthe problem is in fact learnable, our initial experi-ments lead us to conclude that, given the low ratioof errors over truly hard cases, learning to detecterrors is often not worthwhile.5 Related workJuergens (2014) presents work on detecting lin-guistically hard cases in the context of wordsense annotations, e.g., cases where expert an-notators will disagree, as well as differentiat-ing between underspecified, overspecified andmetaphoric cases.
This work is similar to ours inspirit, but considers a very different task.
Whilewe also quantify the proportion of hard cases andpresent an analysis of these cases, we also showthat disagreements are systematic.Our work also relates to work on automaticallycorrecting expert annotations for inconsistencies(Dickinson and Meurers, 2003).
This work isvery different in spirit from our work, but sharesan interest in reconsidering expert annotations,and we made use of their mining algorithm here.There has also been recent work on adjudicat-510ing noisy crowdsourced annotations (Dawid andSkene, 1979; Smyth et al, 1995; Carpenter, 2008;Whitehill et al, 2009; Welinder et al, 2010; Yanet al, 2010; Raykar and Yu, 2012; Hovy et al,2013).
Again, their objective is orthogonal toours, namely to collapse multiple annotations intoa gold standard rather than embracing disagree-ments.Finally, Plank et al (2014) use small samples ofdoubly-annotated POS data to estimate annotatorreliability and show how those metrics can be im-plemented in the loss function when inducing POStaggers to reflect confidence we can put in annota-tions.
They show that not biasing the theory to-wards a single annotator but using a cost-sensitivelearning scheme makes POS taggers more robustand more applicable for downstream tasks.6 ConclusionIn this paper, we show that disagreements betweenprofessional or lay annotators are systematic andconsistent across domains and some of them aresystematic also across languages.
In addition, wepresent an empirical analysis of POS annotationsshowing that the vast majority of inter-annotatordisagreements are competing, but valid, linguis-tic interpretations.
We propose to embrace suchdisagreements rather than using annotation guide-lines to optimize inter-annotator agreement, whichwould bias our models in favor of some linguistictheory.AcknowledgementsWe would like to thank the anonymous reviewersfor their feedback, as well as Djam?e Seddah for theFrench data.
This research is funded by the ERCStarting Grant LOWLANDS No.
313695.ReferencesBob Carpenter.
2008.
Multilevel Bayesian models ofcategorical data annotation.
Technical report, Ling-Pipe.A.
Philip Dawid and Allan M. Skene.
1979.
Max-imum likelihood estimation of observer error-ratesusing the EM algorithm.
Applied Statistics, pages20?28.Markus Dickinson and Detmar Meurers.
2003.
Detect-ing errors in part-of-speech annotation.
In EACL.Cecily Duffield, Jena Hwang, Susan Brown, DmitriyDligach, Sarah Vieweg, Jenny Davis, and MarthaPalmer.
2007.
Criteria for the manual grouping ofverb senses.
In LAW.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-Speech Taggingfor Twitter: Annotation, Features, and Experiments.In ACL.Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,and Eduard Hovy.
2013.
Learning whom to trustwith MACE.
In NAACL.Dirk Hovy, Barbara Plank, and Anders S?gaard.
2014.Experiments with crowdsourced re-annotation of aPOS tagging data set.
In ACL.David Juergens.
2014.
An analysis of ambiguity inword sense annotations.
In LREC.Mitchell Marcus, Mary Marcinkiewicz, and BeatriceSantorini.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In LREC.Barbara Plank, Dirk Hovy, and Anders S?gaard.
2014.Learning part-of-speech taggers with inter-annotatoragreement loss.
In EACL.Vikas C. Raykar and Shipeng Yu.
2012.
Eliminat-ing Spammers and Ranking Annotators for Crowd-sourced Labeling Tasks.
Journal of Machine Learn-ing Research, 13:491?518.Djam?e Seddah, Benoit Sagot, Marie Candito, VirginieMouilleron, and Vanessa Combet.
2012.
TheFrench Social Media Bank: a treebank of noisy usergenerated content.
In COLING.Padhraic Smyth, Usama Fayyad, Mike Burl, Pietro Per-ona, and Pierre Baldi.
1995.
Inferring ground truthfrom subjective labelling of Venus images.
In NIPS.Peter Welinder, Steve Branson, Serge Belongie, andPietro Perona.
2010.
The multidimensional wisdomof crowds.
In NIPS.Jacob Whitehill, Paul Ruvolo, Tingfan Wu, JacobBergsma, and Javier Movellan.
2009.
Whose voteshould count more: Optimal integration of labelsfrom labelers of unknown expertise.
In NIPS.Yan Yan, R?omer Rosales, Glenn Fung, Mark Schmidt,Gerardo Hermosillo, Luca Bogoni, Linda Moy, andJennifer Dy.
2010.
Modeling annotator expertise:Learning when everybody knows a bit of something.In AIStats.Daniel Zeman.
2010.
Hard problems of tagset con-version.
In Proceedings of the Second InternationalConference on Global Interoperability for LanguageResources.511
