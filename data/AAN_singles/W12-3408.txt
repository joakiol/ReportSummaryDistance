Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 55?61,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsStatistical Parsing of Spanish and Data Driven LemmatizationJoseph Le Roux?
Beno?t Sagot?
Djam?
Seddah?,??
Laboratoire d?Informatique Paris Nord, Universit?
Paris Nord, UMR CNRS 7030?Alpage, INRIA & Universit?
Paris Diderot?
Universit?
Paris Sorbonneleroux@univ-paris13.fr, benoit.sagot@inria.fr, djame.seddah@paris-sorbonne.frAbstractAlthough parsing performances have greatlyimproved in the last years, grammar inferencefrom treebanks for morphologically rich lan-guages, especially from small treebanks, isstill a challenging task.
In this paper we in-vestigate how state-of-the-art parsing perfor-mances can be achieved on Spanish, a lan-guage with a rich verbal morphology, with anon-lexicalized parser trained on a treebankcontaining only around 2,800 trees.
We relyon accurate part-of-speech tagging and data-driven lemmatization to provide parsing mod-els able to cope lexical data sparseness.
Pro-viding state-of-the-art results on Spanish, ourmethodology is applicable to other languageswith high level of inflection.1 IntroductionGrammar inference from treebanks has become thestandard way to acquire rules and weights for pars-ing devices.
Although tremendous progress hasbeen achieved in this domain, exploiting small tree-banks is still a challenging task, especially for lan-guages with a rich morphology.
The main difficultyis to make good generalizations from small exam-ple sets exhibiting data sparseness.
This difficultyis even greater when the inference process relieson semi-supervised or unsupervised learning tech-niques which are known to require more training ex-amples, as these examples do not explicitly containall the information.In this paper we want to explore how we can copewith this difficulty and get state-of-the-art syntac-tic analyses with a non-lexicalized parser that usesmodern semisupervised inference techniques.
Werely on accurate data-driven lemmatization and part-of-speech tagging to reduce data sparseness and easethe burden on the parser.
We try to see how wecan improve parsing structure predictions solely bymodifying the terminals and/or the preterminals ofthe trees.
We keep the rest of the tagset as is.In order to validate our method, we perform ex-periments on the Cast3LB constituent treebank forSpanish (Castillan).
This corpus is quite small,around 3,500 trees, and Spanish is known to havea rich verbal morphology, making the tag set quitecomplex and difficult to predict.
Cowan and Collins(2005) and Chrupa?a (2008) already showed inter-esting results on this corpus that will provide us witha comparison for this work, especially on the lexicalaspects as they used lexicalized frameworks whilewe choose PCFG-LAs.This paper is structured as follows.
In Section 2we describe the Cast3LB corpus in details.
In Sec-tion 3 we present our experimental setup and resultswhich we discuss and compare in Section 4.
Finally,Section 5 concludes the presentation.2 Data SetThe Castillan 3LB treebank (Civit and Mart?, 2004)contains 3,509 constituent trees with functional an-notations.
It is divided in training (2,806 trees), de-velopment (365 trees) and test (338 trees).We applied the transformations of Chrupa?a(2008) to the corpus where CP and SBAR nodesare added to the subordinate and relative clauses butwe did not perform any other transformations, likethe coordination modification applied by Cowan andCollins (2005).The Cast3LB tag set is rich.
In particular part-of-speech (POS) tags are fine-grained and encode pre-cise morphological information while non-terminaltags describe subcategorization and function labels.55Without taking functions into account, there are 43non-terminal tags.
The total tag set thus comprises149 symbols which makes the labeling task chal-lenging.The rich morphology of Spanish can be observedin the treebank through word form variation.
Table 1shows some figures extracted from the corpus (train-ing, development and test).
In particular the wordform/lemma ratio is 1.54, which is similar to otherRomance language treebanks (French FTB and Ital-ian ITB).# of tokens 94 907# of unique word forms 17 979# of unique lemmas 11 642ratio word form/lemma 1.54Table 1: C3LB propertiesThus, we are confronted with a small treebankwith a rich tagset and a high word diversity.
Allthese conditions make the corpus a case in point forbuilding a parsing architecture for morphologically-rich languages.3 ExperimentsWe conducted experiments on the Cast3LB develop-ment set in order to test various treebank modifica-tions, that can be divided in two categories: (i) mod-ification of the preterminal symbols of the treebankby using simplified POS tagsets; (ii) modification ofthe terminal symbols of the treebank by replacingword tokens by lemmas.3.1 Experimental SetupIn this section we describe the parsing formalismand POS tagging settings used in our experiments.PCFG-LAs To test our hypothesis, we use thegrammatical formalism of Probabilistic Context-Free Grammars with Latent Annotations (PCFG-LAs) (Matsuzaki et al, 2005; Petrov et al, 2006).These grammars depart from the standard PCFGs byautomatically refining grammatical symbols duringthe training phase, using unsupervised techniques.They have been applied successfully to a wide rangeof languages, among which French (Candito andSeddah, 2010), German (Petrov and Klein, 2008),Chinese and Italian (Lavelli and Corazza, 2009).For our experiments, we used the LORG PCFG-LA parser implementing the CKY algorithm.
Thissoftware also implements the techniques from Attiaet al (2010) for handling out-of-vocabulary words,where interesting suffixes for part-of-speech taggingare collected on the training set, ranked accordingto their information gain with regards to the part-of-speech tagging task.
Hence, all the experimentsare presented in two settings.
In the first one, calledgeneric, unknown words are replaced with a dummytoken UNK, while in the second one, dubbed IG, weuse the collected suffixes and typographical infor-mation to type unknown words.1 We retained the 30best suffixes of length 1, 2 and 3.The grammar was trained using the algorithmof Petrov and Klein (2007) using 3 rounds ofsplit/merge/smooth2 .
For lexical rules, we appliedthe strategy dubbed simple lexicon in the Berkeleyparser.
Rare words ?
words occurring less than 3times in the training set ?
are replaced by a specialtoken, which depends on the OOV handling method(generic or IG), before collecting counts.POS tagging We performed parsing experimentswith three different settings regarding POS infor-mation provided as an input to the parser: (i) withno POS information, which constitutes our base-line; (ii) with gold POS information, which can beconsidered as a topline for a given parser setting;(iii) with POS information predicted using the MEltPOS-tagger (Denis and Sagot, 2009), using threedifferent tagsets that we describe below.MElt is a state-of-the-art sequence labeller thatis trained on both an annotated corpus and an ex-ternal lexicon.
The standard version of MElt relieson Maximum-Entropy Markov models (MEMMs).However, in this work, we have used a multiclassperceptron instead, as it allows for much faster train-ing with very small performance drops (see Table 2).For training purposes, we used the training sectionof the Cast3LB (76,931 tokens) and the Leffe lexi-con (Molinero et al, 2009), which contains almost800,000 distinct (form, category) pairs.3We performed experiments using three different1Names generic and IG originally come from Attia et al(2010).2We tried to perform 4 and 5 rounds but 3 rounds proved tobe optimal on this corpus.3Note that MElt does not use information from the exter-56TAGSET baseline reduced2 reduced3Nb.
of tags 106 42 57Multiclass PerceptronOverall Acc.
96.34 97.42 97.25Unk.
words Acc.
91.17 93.35 92.30Maximum-Entropy Markov model (MEMM)Overall Acc.
96.46 97.42 97.25Unk.
words Acc.
91.57 93.76 92.87Table 2: MElt POS tagging accuracy on the Cast3LBdevelopment set for each of the three tagsets.
We pro-vide results obtained with the standard MElt algorithm(MEMM) as well as with the multiclass perceptron, usedin this paper, for which training is two orders of magni-tude faster.
Unknown words represent as high as 13.5 %of all words.tagsets: (i) a baseline tagset which is identicalto the tagset used by Cowan and Collins (2005)and Chrupa?a (2008); with this tagset, the trainingcorpus contains 106 distinct tags;(ii) the reduced2 tagset, which is a simplificationof the baseline tagset: we only retain the first twocharacters of each tag from the baseline tagset; withthis tagset, the training corpus contains 42 distincttags;(iii) the reduced3 tagset, which is a variant ofthe reduced2 tagset: contrarily to the reduced2tagset, the reduced3 tagset has retained the moodinformation for verb forms, as it proved relevantfor improving parsing performances as shown by(Cowan and Collins, 2005); with this tagset, thetraining corpus contains 57 distinct tags.Melt POS tagging accuracy on the Cast3LB de-velopment set for these three tagsets is given in Ta-ble 2, with overall figures together with figures com-puted solely on unknown words (words not attestedin the training corpus, i.e., as high as 13.5 % of alltokens).3.2 BaselineThe first set of experiments was conducted with thebaseline POS tagset.
Results are summarized in Ta-ble 3.
This table presents parsing statistics on theCast3LB development set in the 3 POS settings in-nal lexicon as constraints, but as features.
Therefore, the set ofcategories in the external lexicon need not be identical to thetagset.
In this work, the Leffe categories we used include somemorphological information (84 distinct categories).troduced above (i) no POS provided, (ii) gold POSprovided and (iii) predicted POS provided.
For eachPOS tagging setting it shows labeled precision, la-beled recall, labeled F1-score, the percentage of ex-act match and the POS tagging accuracy.
The latterneeds not be the same as presented in Section 3.1 be-cause (i) punctuation is ignored and (ii) if the parsercannot use the information provided by the tagger,it is discarded and the parser performs POS-taggingon its own.MODEL LP LR F1 EXACT POSWord OnlyGeneric 81.42 81.04 81.23 14.47 90.89IG 80.15 79.60 79.87 14.19 85.01Gold POSGeneric 87.83 87.49 87.66 30.59 99.98IG 86.78 86.53 86.65 27.96 99.98Pred.
POSGeneric 84.47 84.39 84.43 22.44 95.82IG 83.60 83.66 83.63 21.78 95.82Table 3: Baseline PARSEVAL scores on Cast3LB dev.
set(?
40 words)As already mentioned above, this tagset contains106 distinct tags.
On the one hand it means that POStags contain useful information.
On the other hand italso means that the data is already sparse and addingmore sparseness with the IG suffixes and typograph-ical information is detrimental.
This is a major dif-ference between this POS tagset and the two follow-ing ones.3.3 Using simplified tagsetsWe now turn to the modified tagsets and measuretheir impact on the quality of the syntactic analyses.Results are summarized in Table 4 for the reduced2tagset and in Table 5 for reduced3.
In these two set-tings, we can make the following remarks.?
Parsing results are better with reduced3, whichindicates that verbal mood is an important fea-ture for correctly categorizing verbs at the syn-tactic level.?
When POS tags are not provided, using suffixesand typographical information improves OOVword categorization and leads to a better tag-ging accuracy and F1 parsing score (78.94 vs.81.81 for reduced2 and 79.69 vs. 82.44 for re-duced3).57?
When providing the parser with POS tags,whether gold or predicted, both settings showan interesting difference w.r.t.
to unknownwords handling.
When using reduced2, the IGsetting is better than the generic one, whereasthe situation is reversed in reduced3.
This indi-cates that reduced2 is too coarse to help finelycategorizing unknown words and that the re-finement brought by IG is beneficial, howeverthe added sparseness.
For reduced3 it is diffi-cult to say whether it is the added richness ofthe POS tagset or the induced OOV sparsenessthat explains why IG is detrimental.MODEL LP LR F1 EXACT POSWord OnlyGeneric 78.86 79.02 78.94 15.23 88.18IG 81.89 81.72 81.81 16.17 92.19Gold POSGeneric 86.56 85.90 86.23 26.64 100.00IG 86.90 86.63 86.77 29.28 100.00Pred.
POSGeneric 84.16 83.81 83.99 21.05 96.76IG 84.57 84.32 84.45 21.38 96.76Table 4: PARSEVAL scores on Cast3LB development setwith reduced2 tagset (?
40 words)MODEL LP LR F1 EXACT POSWord OnlyGeneric 79.61 79.78 79.69 14.90 87.29IG 82.57 82.31 82.44 14.24 91.63Gold POSGeneric 88.08 87.69 87.89 30.59 100.00IG 87.56 87.31 87.43 29.61 100.00Pred.
POSGeneric 85.56 85.38 85.47 23.03 96.56IG 85.32 85.24 85.28 23.36 96.56Table 5: PARSEVAL scores on Cast3LB development setwith reduced3 tagset (?
40 words)3.4 Lemmatization ImpactBeing a morphologically rich language, Spanish ex-hibits a high level of inflection similar to severalother Romance languages, for example French andItalian (gender, number, verbal mood).
Furthermore,Spanish belongs to the pro-drop family and cliticpronouns are often affixed to the verb and carryfunctional marks.
This makes any small treebankof this language an interesting play field for statis-tical parsing.
In this experiment, we want to uselemmatization as a form of morphological cluster-ing.
To cope with the loss of information, we pro-vide the parser with predicted POS.
Lemmatizationis carried out by the morphological analyzer MOR-FETTE, (Chrupa?a et al, 2008) while POS taggingis done by the MElt tagger.
Lemmatization perfor-mances are on a par with previously reported resultson Romance languages (see Table 6)TAGSET ALL SEEN UNK (13.84%)baseline 98.39 99.01 94.55reduced2 98.37 98.88 95.18reduced3 98.24 98.88 94.23Table 6: Lemmatization performance on the Cast3LB.To make the parser less sensitive to lemmatizationand tagging errors, we train both tools on a 20 jack-kniffed setup4.
Resulting lemmas and POS tags arethen reinjected into the train set.
The test corporais itself processed with tools trained on the unmod-ified treebank.
Results are presented Table 7.
Theyshow an overall small gain, compared to the previ-ous experiments but provide a clear improvement onthe richest tagset, which is the most difficult to parsegiven its size (106 tags).First, we remark that POS tagging accuracy withthe baseline tagset when no POS is provided is lowerthan previously observed.
This can be easily ex-plained: it is more difficult to predict POS with mor-phological information when morphological infor-mation is withdrawn from input.Second, and as witnessed before, reduction of thePOS tag sparseness using a simplified tagset and in-crease of the lexical sparseness by handling OOVwords using typographical information have adverseeffects.
This can be observed in the generic Pre-dicted POS section of Table 7 where the baselinetagset is the best option.
On the other hand, in IGPredicted POS, using the reduced3 is better thanbaseline and reduced2.
Again this tagset is a trade-off between rich information and data sparseness.4The training set is split in 20 chunks and each one is pro-cessed with a tool trained on the 19 other chunks.
This enablesthe parser to be less sensitive to lemmatization and/or pos tag-ging errors.58TAGSET LR LP F1 EX POSWord Only ?
Genericbaseline 79.70 80.51 80.1 15.23 74.04reduced2 79.19 79.78 79.48 15.56 89.25reduced3 79.92 80.03 79.97 13.16 87.67Word Only ?
IGbaseline 80.67 81.32 80.99 15.89 75.02reduced2 80.54 81.3 80.92 15.13 90.93reduced3 80.52 80.94 80.73 15.13 88.53Pred.
POS ?
Genericbaseline 85.03 85.57 85.30 23.68 95.68reduced2 83.98 84.73 84.35 23.36 96.78reduced3 84.93 85.19 85.06 21.05 96.60Pred.
POS ?
IGbaseline 84.60 85.06 84.83 23.68 95.68reduced2 84.29 84.82 84.55 21.71 96.78reduced3 84.86 85.39 85.12 22.70 96.60Table 7: Lemmmatization ExperimentsIn all cases reduced2 is below the other tagsetswrt.
to Parseval F1 although tagging accuracy is bet-ter.
We can conclude that it is too poor from an in-formational point of view.4 DiscussionThere is relatively few works actively pursued onstatistical constituency parsing for Spanish.
The ini-tial work of Cowan and Collins (2005) consistedin a thorough study of the impact of various mor-phological features on a lexicalized parsing model(the Collins Model 1) and on the performance gainbrought by the reranker of Collins and Koo (2005)used in conjunction with the feature set developedfor English.
Direct comparison is difficult as theyused a different test set (approximately, the concate-nation of our development and test sets).
They reportan F-score of 85.1 on sentences of length less than40.5However, we are directly comparable with Chru-pa?a (2008)6 who adapted the Collins Model 2 toSpanish.
As he was focusing on wide coverage LFGgrammar induction, he enriched the non terminal an-notation scheme with functional paths rather thantrying to obtain the optimal tagset with respect topure parsing performance.
Nevertheless, using the5See http://pauillac.inria.fr/~seddah/spmrl-spanish.html for details on comparison with thatwork.6We need to remove CP and SBAR nodes to be fairly com-parable.same split and providing gold POS, our system pro-vides better performance (around 2.3 points better,see Table 8).It is of course not surprising for a PCFG-LAmodel to outperform a Collins?
model based lexi-calized parser.
However, it is a fact that, on suchsmall treebank configurations, PCFG-LA are cru-cially lacking annotated data.
It is only by greatlyreducing the POS tagset and using either a state-of-the-art tagger or a lemmatizer (or both), that we canboost our system performance.The sensitivity of PCFG-LA models to lexical datasparseness was also shown on French by Seddahet al (2009).
In fact they showed that perfor-mance of state-of-the-art lexicalized parsers (Char-niak, Collins models, etc.)
were crossing thatof Berkeley parsers when the training set containsaround 2500?3000 sentences.
Here, with around2,800 sentences of training data, we are probablyin a setting where both parser types exhibit simi-lar performances, as we suspect French and Spanishto behave in the same way.
It is therefore encour-aging to notice that our approach, which relies onaccurate POS tagging and lemmatization, providesstate-of-the-art performance.
Let us add that a simi-lar method, involving only MORFETTE, was appliedwith success to Italian within a PCFG-LA frame-work and French with a lexicalized parser, both lead-ing to promising results (Seddah et al, 2011; Seddahet al, 2010).5 ConclusionWe presented several experiments reporting the im-pact of lexical sparseness reduction on non lexical-ized statistical parsing.
We showed that, by usingstate-of-the-art lemmatization and POS tagging ona reduced tagset, parsing performance can be on apar with lexicalized models that manage to extractmore information from a small corpus exhibiting arich lexical diversity.
It remains to be seen whetherapplying the same kind of simplifications to the restof the tagset, i.e.
on the internal nodes, can furtherimprove parse structure quality.
Finally, the methodswe presented in this paper are not language specificand can be applied to other languages if similar re-sources exist.59TAGSET MODE TOKENS ALL ?
70 ?
40reduced3 Gen. pred.
POS 83.92 84.27 85.08eval.
w/o CP/SBAR 84.02 84.37 85.24baseline IG pred.
lemma & POS 84.15 84.40 85.26eval.
w/o CP/SBAR 84.34 84.60 85.45reduced3 Gen. gold POS 86.21 86.63 87.84eval.
w/o CP/SBAR 86.35 86.77 88.01baseline gold POS 83.96 84.58 ?
(Chrupa?a, 2008)Table 8: PARSEVAL F-score results on the Cast3LB test setAcknowledgmentsThanks to Grzegorz Chrupa?a and Brooke Cowan foranswering our questions and making data availableto us.
This work is partly funded by the French Re-search Agency (EDyLex, ANR-09-COORD-008).ReferencesMohammed Attia, Jennifer Foster, Deirdre Hogan,Joseph Le Roux, Lamia Tounsi, and Josef van Gen-abith.
2010.
Handling unknown words in statisticallatent-variable parsing models for arabic, english andfrench.
In Proceedings of the NAACL/HLT Workshopon Statistical Parsing of Morphologically Rich Lan-guages (SPMRL 2010), Los Angeles, CA.Marie Candito and Djam?
Seddah.
2010.
Parsing wordclusters.
In Proceedings of the NAACL/HLT Workshopon Statistical Parsing of Morphologically Rich Lan-guages (SPMRL 2010), Los Angeles, CA.Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-abith.
2008.
Learning morphology with morfette.
InIn Proceedings of LREC 2008, Marrakech, Morocco.ELDA/ELRA.Grzegorz Chrupa?a.
2008.
Towards a machine-learningarchitecture for lexical functional grammar parsing.Ph.D.
thesis, Dublin City University.M.
Civit and M. A.
Mart?.
2004.
Building cast3lb: Aspanish treebank.
Research on Language and Compu-tation, 2(4):549 ?
574.Michael Collins and Terry Koo.
2005.
Discrimina-tive reranking for natural language parsing.
Compu-tational Linguistics, 31(1):25?69.B.
Cowan and M. Collins.
2005.
Morphology andreranking for the statistical parsing of spanish.
In Pro-ceedings of the conference on Human Language Tech-nology and Empirical Methods in Natural LanguageProcessing, pages 795?802.
Association for Computa-tional Linguistics.Pascal Denis and Beno?t Sagot.
2009.
Coupling an anno-tated corpus and a morphosyntactic lexicon for state-of-the-art pos tagging with less human effort.
In Pro-ceedings of PACLIC 2009, Hong-Kong, China.Alberto Lavelli and Anna Corazza.
2009.
The berkeleyparser at the evalita 2009 constituency parsing task.
InEVALITA 2009 Workshop on Evaluation of NLP Toolsfor Italian.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic cfg with latent annotations.
InProceedings of the 43rd Annual Meeting of the Associ-ation for Computational Linguistics (ACL), pages 75?82.Miguel A. Molinero, Beno?t Sagot, and Lionel Nicolas.2009.
A morphological and syntactic wide-coveragelexicon for spanish: The leffe.
In Proceedings of theInternational Conference RANLP-2009, pages 264?269, Borovets, Bulgaria, September.
Association forComputational Linguistics.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Human Language Tech-nologies 2007: The Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics; Proceedings of the Main Conference, pages404?411, Rochester, New York, April.
Association forComputational Linguistics.Slav Petrov and Dan Klein.
2008.
Parsing german withlatent variable grammars.
In Proceedings of the ACLWorkshop on Parsing German.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, Sydney, Australia, July.
Asso-ciation for Computational Linguistics.Djam?
Seddah, Marie Candito, and Benoit Crabb?.
2009.Cross parser evaluation and tagset variation: A FrenchTreebank study.
In Proceedings of the 11th Interna-tion Conference on Parsing Technologies (IWPT?09),pages 150?161, Paris, France, October.
Associationfor Computational Linguistics.Djam?
Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,Josef van Genabith, and Marie Candito.
2010.60Lemmatization and statistical lexicalized parsing ofmorphologically-rich languages.
In Proceedings of theNAACL/HLT Workshop on Statistical Parsing of Mor-phologically Rich Languages (SPMRL 2010), Los An-geles, CA.Djam?
Seddah, Joseph Le Roux, and Beno?t Sagot.
2011.Towards using data driven lemmatization for statisti-cal constituent parsing of italian.
In Working Notes ofEVALITA 2011, Rome, Italy, December.61
