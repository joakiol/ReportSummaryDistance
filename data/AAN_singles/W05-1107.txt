Scaling High-Order Character Language Models to GigabytesBob CarpenterAlias-i, Inc.181 North 11th St., #401, Brooklyn, NY 11211carp@colloquial.comAbstractWe describe the implementation steps re-quired to scale high-order character lan-guage models to gigabytes of trainingdata without pruning.
Our online modelsbuild character-level PAT trie structures onthe fly using heavily data-unfolded imple-mentations of an mutable daughter mapswith a long integer count interface.
Ter-minal nodes are shared.
Character 8-gramtraining runs at 200,000 characters persecond and allows online tuning of hy-perparameters.
Our compiled models pre-compute all probability estimates for ob-served n-grams and all interpolation pa-rameters, along with suffix pointers tospeedup context computations from pro-portional to n-gram length to a constant.The result is compiled models that arelarger than the training models, but exe-cute at 2 million characters per second ona desktop PC.
Cross-entropy on held-outdata shows these models to be state of theart in terms of performance.1 IntroductionCharacter n-gram language models have been ap-plied to just about every problem amenable to sta-tistical language modeling.
The implementationwe describe here has been integrated as the sourcemodel in a general noisy-channel decoder (with ap-plications to spelling correction, tokenization andcase normalization) and the class models for sta-tistical classification (with applications includingspam filtering, topic categorization, sentiment analy-sis and word-sense disambiguation).
In addition tothese human language tasks, n-grams are also popu-lar as estimators for entropy-based compression andsource models for cryptography.
(Teahan, 2000)and (Peng, 2003) contain excellent overviews ofcharacter-level models and their application from acompression and HMM perspective, respectively.Our hypothesis was that language-model smooth-ing would behave very much like the classifiers ex-plored in (Banko and Brill, 2001), in that more datatrumps better estimation technique.
We managedto show that the better of the interpolation mod-els used in (Chen and Goodman, 1996), namelyDirichlet smoothing with or without update exclu-sion, Witten-Bell smoothing with or without updateexclusion, and absolute discounting with update ex-clusion converged for 8-grams after 1 billion charac-ters to cross entropies of 1.43+/-0.01.
The absolutediscounting with update exclusion is what Chen andGoodman refer to as the Kneser-Ney method, andit was the clear winner in their evaluation.
Theyonly tested non-parametric Witten-Bell with a sub-optimal hyperparameter setting (1.0, just as in Wit-ten and Bell?s original implementation).
After a bil-lion characters, roughly 95 percent of the characterswere being estimated from their highest-order (7)context.
The two best models, parametric Witten-Bell and absolute discounting with update exclu-sion (aka Kneser-Ney), were even closer in cross-entropy, and depending on the precise sample (wekept rolling samples as described below), and after amillion or so characters, the differences even at thehigher variance 12-grams were typically in the +/-0.01 range.
With a roughly 2.0 bit/character devi-ation, a 10,000 character sample, which is the sizewe used, leads to a 2?
(95.45%) confidence intervalof +/-0.02, and the conclusion that the differencesbetween these systems was insignificant.Unlike in the token-based setting, we are not op-timistic about the possibility of improving these re-sults dramatically by clustering character contexts.The lower-order models are very well trained withexisting quantities of data and do a good job ofthis kind of smoothing.
We do believe that train-ing hyperparameters for different model orders in-dependently might improve cross-entropy fraction-ally; we found that training them hierarchically,as in (Samuelsson, 1996), actually increased cross-entropy.
We believe this is a direct correlate of theeffectiveness of update exclusion; the lower-ordermodels do not need to be the best possible modelsof those orders, but need to provide good estimateswhen heavily weighted, as in smoothing.
The globaloptimization allows a single setting to balance theseattributes, but optimizing each dimension individu-ally should do even better.
But with the number ofestimates taking place at the highest possible orders,we do not believe the amount of smoothing will havethat large an impact overall.These experiments had a practical goal ?
weneeded to choose a language modeling implemen-tation for LingPipe and we didn?t want to take thestandard Swiss Army Knife approach because mostof our users are not interested in running experi-ments on language modeling, but rather using lan-guage models in applications such as informationretrieval, classification, or clustering.
These appli-cations have actually been shown to perform betteron the basis of character language models than to-ken models ((Peng, 2003)).
In addition, character-level models require no decisions about tokeniza-tion, token normalization and subtoken modeling (asin (Klein et al, 2003)).We chose to include the Witten-Bell method inour language modeling API because it is derivedfrom full corpus counts, which we also use for col-location and relative frequency statistics within andacross corpora, and thus the overall implementationeffort was simpler.
For just language modeling, anupdate exclusion implementation of Kneser-Ney isno more complicated than Witten-Bell.In this paper, we describe the implementation de-tails behind storing the model counts, how we sam-ple the training character stream to provide low-cost,online leave-one-out style hyperparameter estima-tion, and how we compile the models and evaluatethem over text inputs to achieve linear performancethat is nearly independent of n-gram length.
We alsodescribe some of the design patterns used at the in-terface level for training and execution.
As far aswe know, the online leave-one-out analysis is novel,though there are epoch-based precursors in the com-pression literature.As far as we know, no one has built a charac-ter language model implementation that will comeclose to the one presented here in terms of scala-bility.
This is largely because they have not beendesigned for the task rather than any fundamentallimitation.
In fact, we take the main contributionof this paper to be a presentation of simple datasharing and data unfolding techniques that wouldalso apply to token-level language models.
Beforestarting our presentation, we?ll review some of thelimitations of existing systems.
For a start, noneof the systems of which we are aware can scale to64-bit values for counts, which is necessary for thesize models we are considering without pruning orcount scaling.
It?s simply easier to find 4 billion in-stances of a character than of a token.
In fact, thecompression models typically use 16 bits for storingcounts and then just scale downward when neces-sary, thus not even trying to store a full set of countsfor even modest corpora.
The standard implemen-tations of character models in the compression liter-ature represent ordinary trie nodes as arrays, whichis hugely wasteful for large sparse implementations;they represent PAT-trie nodes as pointers into theoriginal text plus counts, which works well for longn-gram lengths (32) over small data sets (1 MB) butdoes not scale well for reasonable n-gram lengths (8-12) over larger data sets (100MB-1GB).
The stan-dard token-level language models used to restrictattention to 64K tokens and thus require 16-bit to-ken representatives per node just as our character-based approach; with the advent of large vocabu-lary speech recognition, they now typically use 32-bits per node just to represent the token.
Arrays ofdaughter nodes and lack of sharing of low-count ter-minal nodes were the biggest space hogs in our ex-periments, and as far as we know, none of the stan-dard approaches take the immutable data unfoldingapproach we adopt to eliminate this overhead.
Thuswe would like to stress again that existing character-level compression and token-level language model-ing systems were simply not designed for handlinglarge character-level models.We would also like to point out that the stan-dard finite state machine implementations of lan-guage models do not save any space over the trie-based implementations, typically only approximatesmoothing using backoff rather than interpolation,and further suffer from a huge space explosion whendeterminized.
The main advantage of finite state ap-proaches is at the interface level in that they workwell with hand-written constraints and can interfaceon either side of a given modeling problem.
Forinstance, typical language models implemented astrivial finite state transducers interface neatly withtriphone acoustic models on the one side and withsyntactic grammars on the other.
When placed inthat context, the constraints from the grammar canoften create an overall win in space after composi-tion.2 Online Character Language ModelsFor generality, we use the 16-bit subset of unicode asprovided by Java 1.4.2 to represent characters.
Thispresents an additional scaling problem compared toASCII or Latin1, which fit in 7 and 8 bits.Formally, if Char is a set of characters, a languagemodel is defined to be a mapping P from the set Char?of character sequences into non-negative real num-bers.
A process language model is normalized oversequences of length n: ?X?Char?,|X |=n P(X) = 1.0.We also implement bounded language models whichnormalize over all sequences, but their implementa-tion is close enough to the process models that wedo not discuss them further here.
The basic inter-faces are provided in Figure 1 (with names short-ened to preserve space).
Note that the process andsequence distribution is represented through markerinterfaces, whereas the cross-cutting dynamic lan-guage models support training and compilation, aswell as the estimation inherited from the languageinterface LM {double log2Prob(char[] cs,int start, int end);}interface ProcessLM extends LM {}interface SequenceLM extends LM {}interface DynamicLM extends LM {double train(char[] cs,int start, int end);void compile(ObjectOutput out)throws IOException;}Figure 1: Language Model Interfacemodel interface.We now turn to the statistics behind character-level langauge models.
The chain rule factorsP(x0, .
.
.
,xk?1) = ?i<k P(xi|x0, .
.
.
,xi?1).
An n-gram language model estimates a character usingonly the last n ?
1 symbols, ?P(xk|x0, .
.
.
,xk?1) =?P(xk|xk?n+1, .
.
.
,xk?1); we follow convention in de-noting generic estimators by ?P.The maximum likelihood estimator for n-gramsis derived from frequency counts for sequence Xand symbol c, PML(c|X) = count(Xc)/extCount(X),where count(X) is the number of times the sequenceX was observed in the training data and extCount(X)is the number of single-symbol extensions of Xobserved: extCount(X) = ?c?Char count(Xc).
Whentraining over one or more short samples, the dis-parity between count(X) and extCount(X) can belarge: for abracadabra, count(a) = 5, count(bra) =2, extCount(a) = 4, and extCount(bra) = 1.We actually provide two implementations of lan-guage models as part of LingPipe.
For languagemodels as random processes, there is no padding.They correspond to normalizing over sequences of agiven length in that the sum of probabilities for char-acter sequences of length k will sum to 1.0.
Witha model that inserts begin-of-sequence and end-of-sequence characters and estimates only the end-of-sequence character, normalization is over all strings.Statistically, these are very different models.
Inpractice, they are only going to be distinguishableif the boundaries are very significant and the to-tal string length is relatively small.
For instance,they are not going to make much difference in esti-mating probabilities of abstracts of 1000 characters,even though the start and ends are significant (e.g.capitals versus punctuation being preferred at be-ginning and end of abstracts) because cross-entropywill be dominated by the other 1000 characters.
Onthe other hand, for modeling words, for instanceas a smoothing step for token-level part-of-speech,named-entity or language models, the begin/end ofa word will be significant, representing capitaliza-tion, prefixes and suffixes in a language.
In fact, thislatter motivation is why we provide padded models.It is straightforward to implement the padded mod-els on top of the process models, which is why wediscuss the process models here.
But note that wedo not pad all the way to maximum n-gram length,as that would bias the begin/end statistics for shortwords.We use linear interpolation to form a mixturemodel of all orders of maximum likelihood es-timates down to the uniform estimate PU(c) =1/|Char|.
The interpolation ratio ?
(dX) ranges be-tween 0 and 1 depending on the context dX .
?P(c|dX) = ?
(dX)PML(c|dX)+ (1??
(dX)) ?P(c|X)?P(c) = ?
()PML(c)+ (1??
())(1/|Char|)The Witten-Bell estimator computed the interpo-lation parameter ?
(X) using only overall trainingcounts.
The best performing model that we evalu-ated is parameterized Witten-Bell interpolation withhyperparameter K, for which the interpolation ratiois defined to be:?
(X) = extCount(X)extCount(X)+K ?
numExts(X)We take numExts(X) = |{c|count(Xc)> 0}| to be thenumber of different symbols observed following thesequence X in the training data.
The original Witten-Bell estimator set K = 1.
We optimize the hyperpa-rameter K online (see the next section).3 Online Models and HyperparameterEstimationA language model is online if it can be estimatedfrom symbols as they arrive.
An advantage of onlinemodels is that they are easy to use for adaptation to024681012141618100000  1e+006  1e+007  1e+008OptimalHyperparameterSettingAmount of Data (characters)2-gram4-gram8-gram12-gramFigure 2: Optimal Hyperparameter Settings forWitten-Belldocuments or styles, hence their inclusion in com-mercial dictation packages such as DragonDictateand ViaVoice.
Another advantage is that they areeasy to integrate into tag-a-little/learn-a-little sys-tems such as MITRE?s Alembic Workbench.With online models, we are able to estimate hy-perparameters using an online form of leave-one-outanalysis (Ney et al, 1995).
This can be performedin a number of ways as long as the model efficientlyestimates likelihoods given a set of hyperparametersettings.
We opted for the simplest technique wecould muster to find the right settings.
This wasmade easier because we only have a single hyperpa-rameter whose behavior is fairly flat around the op-timal setting and because the optimal setting didn?tchange quickly with increasing data.
The optimalsettings are shown in Figure 2.
Also note that the op-timal value is rarely at 1 except for very low-order n-grams.
To save the complexity of maintaining an in-terval around the best estimate do do true hill climb-ing, we simply kept rolling averages of values log-arithmically spaced from 1/4 to 32.
We also imple-mented a training method that kept track of the last10,000 character estimates (made before the charac-ters were used for training, of course).
We used a cir-cular queue for this data structure because its size isfixed and it allowed a constant time insert of the lastrecorded value.
We used one circular queue for eachhyperparameter setting, thus storing around 5MB orso worth of samples.
These samples can be usedto provide an estimate of the best hyperparameterat any given point in the algorithm?s execution.
Weused this explicit method rather than the much lesscostly rolling average method so that results wouldbe easier to report.
We actually believe just keep-ing a rolling average of measured cross-entropies ononline held-out samples is sufficient.We also sampled the character stream rather thanestimating each character before training.
With a gi-gabyte of characters, we only needed to sample 1in 100,000 characters to find enough data for esti-mates.
At this rate, online hyperparameter estimatedid not measurably affect training time, which wasdominated by simply constructing the trie.We only estimated a single hyperparameter ratherthan one for each order to avoid having to solve amultivariate estimation problem; although we cancollect the data online, we would either have to im-plement an EM-like solution or spend a lot time perestimate iterating to find optimal parameters.
Thismay be worthwhile for cases where less data is avail-able.
As the training data increased, the sensitivity totraining parameters decreased.
Counterintuitively,we found that recursively estimating each order fromlow to high, as implemented in (Samuelsson, 1996),actually increased entropy considerably.
Clearly theestimator is using the fact that lower-order estimatesshould not necessarily be optimal for use on theirown.
This is a running theme of the discountingmethods of smoothing such as absolute discountingor Kneser-Ney.Rather than computing each estimate for hyperpa-rameter and n-gram length separately, we first gatherthe counts for each suffix and each context and thenumber of outcomes for that context.
This is the ex-pensive step, as it require looking up counts in thetrie structure.
Extension counts require a loop overall the daughters of a context node in the trie be-cause we did not have enough space to store them onnodes.
With all of these counts, the n-gram etimatesfor each n and each hyperparameter setting can becomputed from shortest to longest, with the lowerorder estimates contributing the smoothed estimatefor the next higher order.4 Substring CountersOur n-gram language models derive estimates fromcounts of substrings of length n or less in the traininginterface Node {Node increment(char[] cs,int start, int end);long count(char[] cs,int start, int end);long extCount(char[] cs,int start, int end);int numExts(char[] cs,int start, int end);Node prune(long minCount);}Figure 3: Trie Node Interfacecorpus.
Our counter implementation was the trick-iest component to scale as it essentially holds thestatistics derived from the training data.
It containsstatistics sufficient to implement all of the estimatorsdefined above.
The only non-trivial case is Kneser-Ney, which is typically implemented using the tech-nique known in the compression literature as ?up-date exclusion?
(Moffat, 1990).
Under update ex-clusion, if a count ?abc?
is updated and the context?ab?
was known, then counts for ?a?
and ?ab?
areexcluded from the update process.
We actually com-pute these counts from the total counts by noting thatthe update exclusion count is equal to the number ofunique characters found following a shorter context.That is, the count for ?ab?
for smoothing is equal tothe number of characters ?x?
such that ?xab?
has anon-zero count, because these are the situations inwhich the count of ?ab?
is not excluded.
This is notan efficient way to implement update exclusion, butmerely an expedient so we could share implementa-tions for experimental purposes.
Straight update ex-clusion is actually more efficient to implement thanfull counts, but we wanted the full set of charactersubstring counts for other purposes, as well as lan-guage modeling.Our implementation relies heavily on a data un-folded object-oriented implementation of Patriciatries.
Unlike the standard suffix tree algorithms forconstructing this trie for all substrings as in (Clearyand Teahan, 1997), we limit the length and makecopies of characters rather than pointing back intothe original source.
This is more space efficient thanthe suffix-tree approach for our data set sizes and n-gram lengths.The basic node interface is as shown in Figure 3.Note that the interface is in terms of long integer val-ues.
This was necessary to avoid integer overflow inour root count when data size exceeded 2 GB andour 1-gram counts when data sizes exceeded 5 or6GB.
A widely used alternative used for compres-sion is to just scale all the counts by dividing bytwo (and typically pruning those that go to zero);this allows PPM to use 8-bit counters at the cost ofarithmetic precision ((Moffat, 1990)).
We eschewpruning because we also use the counts to find sig-nificant collocations.
Although most collocation andsignificance statistics are not affected by global scal-ing, cross-entropy suffers tremendously if scaling isdone globally rather than only on the nodes that needit.Next note that the interface is defined in termsof indexed character slices.
This obviates a hugeamount of otherwise unnecessary object creationand garbage collection.
It is simply not efficientenough, even with the newer generational garbagecollectors, to create strings or even lighter charactersequences where needed on the heap; slice indicescan be maintained in local variables.The increment method increments the count foreach prefix of the specified character slice.
Thecount method returns the count of a given charactersequence, extensionCount the count of all one-character extensions, numExtensions the numberof extensions.
The extensions method returnsall the observed extensions of a character sequence,which is useful for enumerating over all the nodes inthe trie.Global pruning is implemented, but was not nec-essary for our scalability experiments.
It is neces-sary for compilation; we could not compile modelsnearly as large as those kept online.
Just the sizeof the floating point numbers (two per node for es-timate and interpolation) lead to 8 bytes per node.In just about every study every undertaken, includ-ing our informal ones, unpruned models have out-performed pruned ones.
Unfortunately, applicationswill typically not have a gigabyte of memory avail-able for models.
The best performing models for agiven size are those trained on as much data avail-able and pruned to the specified size.
Our prun-ing is simply a minimum count approach, becausethe other methods have not been shown to improvemuch on this baseline.Finally, note that both the increment and pruneArrayDtrNodeB, S, I, LOneDtrNodeB, S, I, LNodeAbstractNodeDtrNodePatNodeTerminalNodeB, S, I, LTwoDtrNodeB, S, I, LThreeDtrNodeB, S, I, LPat1Node1, 2, 3, B, S, I, LPat2Node1, 2, 3, B, S, I, LPat3Node1, 2, 3, B, S, I, LPat4Node1, 2, 3, B, S, I, LPatArrayNode1, 2, 3, B, S, I, LFigure 4: Unfolded Trie Classesmethods return nodes themselves.
This is to sup-port the key implementation technique for scalabil-ity ?
replacing immutable objects during increments.Rather than having a fixed mutable node representa-tion, nodes can return results that are essentially re-placements for themselves.
For instance, there is animplementation of Node that provides a count as abyte (8 bits) and a single daughter.
If that class getsincremented above the byte range, it returns a nodewith a short-based counter (16 bits) and a daughterthat?s the result of incrementing the daughter.
If theclass gets incremented for a different daughter path,then it returns a two-daughter implementation.
Ofcourse, both of these can happen, with a new daugh-ter that pushes counts beyond the byte range.
Thisstrategy may be familiar to readers with experiencein Prolog (O?Keefe, 1990) or Lisp (Norvig, 1991),where many standard algorithms are implementedthis way.A diagram of the implementations of Node is pro-vided in Figure 4.
At the top of the diagram is theNode interface itself.
The other boxes all representabstract classes, with the top class, AbstractNode,forming an abstract adapter for most of the utilitymethods in Node (which were not listed in the inter-face).The abstract subclass DtrNode is used for nodeswith zero or more daughters.
It requires its exten-sions to return parallel arrays of daughters and char-acters and counts from which it implements all theupdate methods at a generic level.abstract class TwoDtrNodeextends DtrNode {final char mC1; final Node mDtr1final char mC2; final node mDtr2;TwoDtrNode(char c1, Node dtr1,char c2, Node dtr2,mC1 = c1; mDtr1 = dtr1;mC2 = c2; mDtr2 = dtr2;}Node getDtr(char c) {return c == mC1?
mDaughter1: ( c == mC2?
mDaughter2: null );}[] chars() {return new char[] { mC1, mC2 };}Node[] dtrs() {return new Node[] { mDaughter1,mDaughter2 };}int numDtrs() { return 2; }}Figure 5: Two Daughter Node ImplementationThe subclass TerminalNode is used for nodeswith no daughters.
Its implementation is particu-larly simple because the extension count, the num-ber of extensions and the count for any non-emptysequence starting at this node are zero.
The nodeswith non-empty daughters are not much more com-plex.
For instance, the two-daughter node abstractclass is shown in Figure 5.All daughter nodes come with four concrete im-plementations, based on the size of storage allocatedfor counts: byte (8 bits), short (16 bits), int (32bits), or long (64 bits).
The space savings from onlyallocating bytes or shorts is huge.
These concreteimplementations do nothing more than return theirown counts as long values.
For instance, the shortimplementation of three-daughter nodes is shown inFigure 6.
Note that because these nodes are not pub-lic, the factory can be guaranteed to only call theconstructor with a count that can be cast to a shortvalue and stored.Increments are performed by the superclassfinal class ThreeDtrNodeShortextends ThreeDtrNode {final short mCount;ThreeDtrNodeShort(char c1, Node dtr1,char c2, Node dtr2,char c3, Node dtr3,long count) {super(c1,dtr1,c2,dtr2,c3,dtr3);mCount = (short) count;}long count() { return mCount; }}Figure 6: Three Daughter Short Nodeand will call constructors of the appropriatesize.
The increment method as defined inAbstractDtrNode is given in Figure 7.
Thismethod increments all the suffixes of a string.The first line just increments the local node if thearray slice is empty; this involves taking its charac-ters, its daughters and calling the factory with oneplus its count to generate a new node.
This gener-ates a new immutable node.
If the first character inthe slice is an existing daughter, then the daughter isincremented and the result is used to increment theentire node.
Note the assignment to dtrs[k] afterthe increment; this is to deal with the immutability.The majority of the code is just dealing with the casewhere a new daughter needs to be inserted.
Of spe-cial note here is the factory instance called on theremaining slice; this will create a PAT node.
Thisappears prohibitively expensive, but we refactored tothis approach from a binary-tree based method withalmost no noticeable hit in speed; most of the arraysstabilize after very few characters and the resizingsof big arrays later on is quite rare.
We even replacedthe root node implementation which was formerlya map because it was not providing a measurablespeed boost.Once the daughter characters and daughters aremarshalled, the factory calls the appropriate con-structor based on the number of the character anddaughters.
The factory then just calls the appropri-ately sized constructor as shown in Figure 8.Unlike other nodes, low count terminal nodes arestored in an array and reused.
Thus if the result ofan increment is within the cache bound, the storedNode increment(char[] cs,int start, int end) {// empty slice; incr this nodeif (start == end)return NodeFactory.createNode(chars(),dtrs(),count()+1l);char[] dtrCs = chars();// search for dtrint k = Arrays.binarySearch(dtrCs,cs[start]);Node[] dtrs = dtrs();if (k >= 0) { // found dtrdtrs[k] = dtrs[k].increment(cs,start+1,end);return NodeFactory.createNode(dtrCs,dtrs,count()+1l);}// insert new dtrchar[] newCs = new char[dtrs.length+1];Node[] newDtrs = new Node[dtrs.length+1];int i = 0;for (; i < dtrs.length&& dtrCs[i] < cs[start];++i) {newCs[i] = dtrCs[i];newDtrs[i] = dtrs[i];}newCs[i] = cs[start];newDtrs[i] = NodeFactory.createNode(cs,start+1,end,1);for (; i < dtrCs.length; ++i) {newCs[i+1] = dtrCs[i];newDtrs[i+1] = dtrs[i];}return NodeFactory.createNode(newCs,newDtrs,count()+1l);}Figure 7: Increment in AbstractDtrNodeversion is returned.
Because terminal nodes are im-mutable, this does not cause problems with consis-tency.
In practice, terminal nodes are far and awaythe most common type of node, and the greatest sav-ing in space came from carefully coding terminalnodes.The abstract class PatNode implements a so-called ?Patricia?
trie node, which has a single chainof descendants each of which has the same count.There are four fixed-length implementations for theone, two, three and four daughter case.
For theseimplementations, the daughter characters are storedin member variables.
For the array implementa-tion, PatArrayNode, the daughter chain is storedstatic Node createNode(char c, Node dtr,long n) {if (n <= Byte.MAX_VALUE)return new OneDtrNodeByte(c,dtr,n);if (n <= Short.MAX_VALUE)return new OneDtrNodeShort(c,dtr,n);if (n <= Integer.MAX_VALUE)return new OneDtrNodeInt(c,dtr,n);return new OneDtrNodeLong(c,dtr,n);}Figure 8: One Daughter Factory Methodas an array.
Like the generic daughter nodes, PATnodes contain implementations for byte, short, intand long counters.
They also contain constant im-plementations for one, two and three counts.
Wefound in profiling that the majority of PAT nodeshad counts below four.
By providing constant imple-mentations, no memory at all is used for the counts(other than a single static component per class).
Patnodes themselves are actually more common thatregular daughter nodes in high-order character tries,because most long contexts are deterministic.
Asn-gram order increases, so does the proportion ofPAT nodes.
Implementing increments for PAT nodesis only done once in the abstract class PatNode.Each PAT node implementation supplied an array ina standardized interface to the implementations inPatNode.
That array is created as needed and onlylives long enough to carry out the required incrementor lookup.
Java?s new generational garbage collectoris fairly efficient at dealing with garbage collectionfor short-lived objects such as the trie nodes.5 CompilationOur online models are tuned primarily for scalabil-ity, and secondarily for speed of substring counts.Even the simplest model, Witten-Bell, requires foreach context length that exists, summing over exten-sion counts and doing arithmetic including severaldivisions and multiplications per order a logarithmat the end.
Thus straightforward estimation frommodels is unsuitable for static, high throughput ap-plications.
Instead, models may be compiled to aless compact but more efficient static representation.We number trie nodes breadth-first in unicode or-der beginning from the root and use this indexing forfour parallel arrays following (Whittaker and Raj,2001).
The main difference is that we have notchar int float float intIdx Ctx C Suf logP log(1-? )
Dtr0 n/a n/a n/a n/a -0.63 11 a 0 -2.60 -0.41 62 b 0 -3.89 -0.58 93 c 0 -4.84 -0.32 104 d 0 -4.84 -0.32 115 r 0 -3.89 -0.58 126 a b 2 -2.51 -0.58 137 a c 3 -3.49 -0.32 148 a d 4 -3.49 -0.32 159 b r 5 -1.40 -0.58 1610 c a 1 -1.59 -0.32 1711 d a 1 -1.59 -0.32 1812 r a 1 -1.17 -0.32 1913 ab r 9 -0.77 n/a n/a14 ac a 10 -1.10 n/a n/a15 ad a 11 -1.10 n/a n/a16 br a 12 -0.67 n/a n/a17 ca d 8 -1.88 n/a n/a18 da b 6 -1.55 n/a n/a19 ra c 7 -1.88 n/a n/aFigure 9: Compiled Representation of 3-grams for?abracadabra?coded to a fixed n-gram length, costing us a bit ofspace in general, and also that we included contextsuffix pointers, costing us more space but savinglookups for all suffixes during smoothing.The arrays are (1) the character leading to thenode, (2) the log estimate of the last character in thepath of characters leading to this node given the pre-vious characters in the path, (3) the log of one mi-nus the interpolation parameter for the context rep-resented by the full path of characters leading to thisnode, (4) the index of the first daughter of the node,and (5) index of the suffix of this node.
Note that thedaughters of a given node will be contiguous and inunicode order given the breadth-first nature of the in-dexing, ranging from the daughter index of the nodeto the daughter index of the next node.We show the full set of parallel arrays for trigramcounts for the string ?abracadabra?
in Figure 9.
Thefirst column is for the array index, and is not explic-itly represented.
The second column, labeled ?Ctx?,is the context, and this is also not explicitly repre-sented.
The remaining columns are explicitly repre-sented.
The third column is for the character.
Thefourth column is an integer backoff suffix pointer;for instance, in the row with index 13, the contextis ?ab?, and the character is ?r?, meaning it repre-sents ?abr?
in the trie.
The suffix index is 9, whichis for ?br?, the suffix of ?abr?.
The fifth and sixthcolumns are 32-bit floating point estimates, the fifthof log2 P(r|ab), and the sixth is empty because thereis no context for ?abr?, just an outcome.
The valueof log2(1 ?
?
(ab)) is found in the row indexed 6,and equal to -0.58.
The seventh and final columnis the integer index of the first daughter of a givennode.
The value of the daughter pointer for the fol-lowing node provides an upper bound.
For instance,in the row index 1 for the string ?a?, the daughter in-dex is 6, and the next row?s daughter index is 9, thusthe daughters of ?a?
fall between 6 and 8 inclusively?
these are ?ab?, ?ac?
and ?ad?
respectively.
Notethat the daughter characters are always in alphabeti-cal order, allowing for a binary search for daughters.For n-gram estimators, we need to computelogP(cn|c0 ?
?
?cn?1).
We start with the longest se-quence ck, .
.
.
,cn?1 that exists in the trie.
If binarysearch finds the outcome cn among the daughters ofthis node, we return its log probability estimate; thishappens in over 90 percent of estimates with rea-sonably sized training sets.
If the outcome characteris not found, we continue with shorter and shortercontexts, adding log interpolation values from thecontext nodes until we find the result or reach theuniform estimate at the root, at which point we addits estimate and return it.
For instance, the estimateof log2 P(r|ab) = ?0.77 can be read directly off therow indexed 13 in Figure 9.
But log2 P(a|ab) =?0.58+ log2 P(a|b)=?0.58+?0.58+ log2 P(a)=?0.58+?0.58+?2.60, requiring two interpolationsteps.For implementation purposes, it is significant thatwe keep track of where we backed off from.
Therow for ?a?, where the final estimate was made, willbe the starting point for lookup next time.
This isthe main property of the fast string algorithms ?
weknow that the context ?ba?
does not exist, so we donot need to go back to the root and start our searchall over again at the next character.
The result isa linear bound on lookup time because each back-off of n characters guarantees at least n steps to getback to the same context length, thus there can?t bemore backoff steps than characters input.
The mainbottleneck in run time is memory bandwidth due tocache misses.The log estimates can be compressed using asmuch precision as needed (Whittaker and Raj,2001), or even reduced to integral values and integerarithmetic used for computing log estimates.
We usefloats and full characters for simplicity and speed.6 Corpora and ParsersOur first corpus is 7 billion characters from the NewYork Times section of the Linguistic Data Consor-tium?s Gigaword corpus.
Only the body of docu-ments of type story were used.
Paragraphs indi-cated by XML markup were begun with a singletab character.
All newlines were converted to singlewhitepspaces, and all other data was left unmodi-fied.
The data is problematic in at least two ways.First, the document set includes repeats of earlierdocuments.
Language models provide a good wayof filtering these repeated documents out, but we didnot do so for our measurements because there werefew enough of them that it made little differenceand we wanted to simplify other comparative eval-uations.
Second, the document set includes numer-ical list data with formatting such as stock marketreports.
The Times data uses 87 ASCII characters.Our second corpus is the 5 billion charactersdrawn from abstracts in the United States?
NationalLibrary of Medicine?s 2004 MEDLINE baseline ci-tation set.
Abstract truncation markers were re-moved.
MEDLINE uses a larger character set of 161characters, primarily extending ASCII with diacrit-ics on names and Greek letters.By comparison, (Banko and Brill, 2001) used onebillion tokens for a disambiguation task, (Brown etal., 1991) used 583 million tokens for a languagemodel task, and (Chen and Goodman, 1996) cleverlysampled from 250 million tokens to evaluate higher-order models by only training on sequences used inthe held-out and test sets.Our implementation is based a generic text parserand text handler interface, much like a simplifiedversion of XML?s SAX content handler and XMLparser.
A text parser is implemented for the variousdata sets, including decompressing their zipped andgzipped forms and parsing their XML, SGML andtokenized form.
A handler is then implemented thatadds data to the online models and polls the modelfor results intermittently for generating graphs.7 ResultsWe used a 1.4GB Java heap (unfortunately, the max-imum allowable with Java on 32-bit Intel hardwarewithout taking drastic measures), which allowed usto train 6-grams on up to 7 billion characters withroom to spare.
Roughly, 8-grams ran out of mem-ory at 1 billion characters, 12 grams at 100 millioncharacters, and 32 grams at 10 million characters.We did not experiment with pruning for this paper,though our API supports both thresholded and pdi-visive scaling pruning.
Training the counters de-pends heavily on the length of n-gram, with 5-gramstraining at 431,000 characters per second, 8-gramsat 204,000 char/s, 12-grams at 88,000 char/s and 32-grams at 46,000 char/s, including online hyperpara-meter estimation (using a $2000 PC running Win-dows XP and Sun?s 1.4.2 JDK, with a 3.0GHz Pen-tium 4, 2GB of ECC memory at 800MHz, and two10K SATA drives in RAID 0).Our primary results are displayed in Figure 11and Figure 10, which plot sample cross-entropyrates against amount of text used to build the mod-els for various n-gram lengths.
Sample cross en-tropy is simply the average log (base 2) probabil-ity estimate per character.
All entropies are re-ported for the best hyperparameter settings throughonline leave-one-out estimation for parameterizedWitten-Bell smoothing.
Each data point in the plotuses the average entropy rate over a sample sizeof up to 10,000 for MEDLINE and 100,000 forthe Times, with the samples being drawn evenlyover the data arriving since the last plot point.For instance, the point plotted at 200,000 charac-ters for MEDLINE uses a sample of every 10thcharacter between character 100,000 and 200,000whereas the sample at 2,000,000,000 charactersuses every 100,000th character between characters1,000,000,000 and 2,000,000,000.Like the Tipster data used by (Chen and Good-man, 1996), the immediately noticeable feature ofthe plots is the jaggedness early on, including some0.811.21.41.61.822.22.42.62.8310000  100000  1e+006  1e+007  1e+008  1e+009  1e+010SampleCross-EntropyRate(bitsper character)Amount of Data (characters)Entropy=1.495Entropy=1.5704-gram6-gram8-gram12-gram16-gramFigure 10: NY Times Sample Cross-Entropy Rates11.522.533.54100  1000  10000  100000  1e+006  1e+007  1e+008  1e+009  1e+010SampleCrossEntropyRate(bitsper character)Amount of Data (characters)Entropy=1.36Entropy=1.4352-gram4-gram6-gram8-gram12-gramFigure 11: MEDLINE Sample Cross-Entropy Ratesridiculously low cross-entropy rates reported for theTimes data.
This is largely due to low trainingdata count, high n-gram models being very goodat matching repeated passages coupled with the factthat a 2000 word article repeated out of 10,000 sam-ple characters provides quite a cross-entropy reduc-tion.
For later data points, samples are sparser andthus less subject to variance.For applications other than cross-entropy bake-offs, 5-grams to 8-grams seem to provide the right2345678100  1000  10000  100000  1e+006  1e+007  1e+008  1e+009  1e+010SampleVariance(squaredbitsper character)Amount of Data (characters)2-gram4-gram8-gramFigure 12: MEDLINE Sample Variancescompromise between accuracy and efficiency.We were surprised that MEDLINE had lowern-gram entropy bounds than the Times, especiallygiven the occurrence of duplication within the Timesdata (MEDLINE does not contain duplicates in thebaseline).
The best MEDLINE operating point is in-dicated in the figure, with a sample cross-entropyrate of 1.36 for 12-grams trained on 100 millioncharacters of data; 8-gram entropy is 1.435 at nearly1 billion characters.
The best performance for theTimes corpus was also for 12-grams at 100 mil-lion characters, but the sample cross-entropy was1.49; with 8-gram sample cross-entropy as low as1.570 at 1 billion characters.
Although MEDLINEmay be full of jargon and mixed-case alphanumericacronyms, the way in which they are used is highlypredictable given enough training data.
Data in theTimes such as five and six digit stock reports, sportsscores, etc., seem to provide a challenge.The per-character sample variances for 2-grams,4-grams and 8-grams for MEDLINE are given inFigure 12.
We did not plot results for higher-ordern-grams, as their variance was almost identical tothat of 8-grams.
Standard error is the square root ofvariance, or about 2.0 in the range of interest.
With10,000 samples, variance should be 4/10,000, withstandard error the square root of this, or 0.02.
Thisis in line with measurement variances found at thetail end of the plots, but not at the beginnings.Most interestingly, it turned out that smoothingmethod did not matter once n-grams were large, thusbringing the results of (Banko and Brill, 2001) tobear on those of (Chen and Goodman, 1996).
Thecomparison for 12-grams and then for the tail ofmore data for 8-grams in Figures 13 and 14.
Fig-ure 14 shows the smoothing methods for 8-grams onan order of magnitude more data.ConclusionsWe have shown that it is possible to use object ori-ented techniques to scale language model counts tovery high levels without pruning on relatively mod-est hardware.
Even more space could be saved byunfolding characters to bytes (especially for tokenmodels).
Different smoothing models tend to con-verge to each other after gigabytes of data, makingsmoothing much less critical.Full source with unit tests, javadoc, and applica-tions is available from the LingPipe web site:http://www.alias-i.com/lingpipe1.41.61.822.22.4100000  1e+006  1e+007  1e+008SampleCrossEntropyRate(bitsper character)Amount of Data (characters)Absolute Discounting 12-gramDirichlet w. Update Exclusion 12-gramWitten-Bell w. Update Exclusion 12-gramDirichlet 12-gramAbsolute Discounting w. Update Exclusion 12-gramWitten-Bell 12-gramFigure 13: Comparison of Smoothing for 12-grams1.421.441.461.481.51.521e+009 2e+008SampleCrossEntropyRate(bitsper character)Amount of Data (characters)Absolute Discounting 8-gramDirichlet w. Update Exclusion 8-gramWitten-Bell w. Update Exclusion 8-gramDirichlet 8-gramAbsolute Discounting w. Update Exclusion 8-gramWitten-Bell 8-gramFigure 14: Comparison of Smoothing for 8-gramsReferencesMichele Banko and Eric Brill.
2001.
Scaling to very verylarge corpora for natural language disambiguation.
InProceedings of the 39th Meeting of the ACL.Eric Brill and Robert C. Moore.
2000.
An improvederror model for noisy channel spelling correction.
InProceedings of the 38th Annual Meeting of the ACL.Peter F. Brown, Stephen Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1991.
Word-sense dis-ambiguation using statistical methods.
pages 264?270.Stanley F. Chen and Joshua Goodman.
1996.
An empir-ical study of smoothing techniques for language mod-eling.
In Proceedings of the 34th Annual Meeting ofthe ACL, pages 310?318.John G. Cleary and William J. Teahan.
1997.
Un-bounded length contexts for PPM.
The ComputerJournal, 40(2/3):67??
?Thomas M. Cover and Joy A. Thomas.
1991.
Elementsof Information Theory.
John Wiley.Frederick Jelinek and Robert L. Mercer.
1980.
Inter-polated estimation of Markov source parameters fromsparse data.
In Proceedings of the Workshop on Pat-tern Recognition in Practice.
North-Holland.Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-pher D. Manning.
2003.
Named entity recognitionwith character-level models.
In Proceedings the 7thConNLL, pages 180?183.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking off for n-gram language modeling.
In Pro-ceedings of ICASSP, pages 181?184.Kevin Knight and Vasileios Hatzivassiloglou.
1995.Two-level, many-paths generation.
In Proceedings ofthe 33rd Annual Meeting of the ACL.Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, andNanda Kambhatla.
2003. tRuEcasIng.
In Proceed-ings of the 41st Annual Meeting of the ACL, pages152?159.David J. C. MacKay and Linda C. Peto.
1995.
A hier-archical Dirichlet language model.
Natural LanguageEngineering, 1(3):1?19.Alistair Moffat.
1990.
Implementing the PPM data com-pression scheme.
IEEE Transactions on Communica-tions, 38:1917?1921.Hermann Ney, U. Essen, and Reinhard Kneser.
1995.On the estimation of ?small?
probabilities by leaving-one-out.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 17:1202?1212.Peter Norvig.
1991.
Paradigms of Artificial IntelligenceProgramming: Case Studies in Common Lisp.
Mor-gan Kaufmann.Richard O?Keefe.
1990.
The Craft of Prolog.
MIT Press.Fuchun Peng.
2003.
Building Probabilistic Models forLanguage Independent Text Classification.
Ph.D. the-sis.Gerasimos Potamianos and Frederick Jelinek.
1998.
Astudy of n-gram and decision tree letter language mod-eling methods.
Speech Communication, 24(3):171?192.Christer Samuelsson.
1996.
Handling sparse data by suc-cessive abstraction.
In Proceedings of COLING-96,Copenhagen.William J. Teahan and John G. Cleary.
1996.
The en-tropy of english using PPM-based models.
In DataCompression Conference, pages 53?62.William J. Teahan.
2000.
Text classification and segmen-tation using minimum cross-entropy.
In Proceeding ofRIAO 2000.Edward Whittaker and Bhiksha Raj.
2001.
Quantization-based language model compression.
In Proceedings ofEurospeech 2001, pages 33?36.ChengXiang Zhai and John Lafferty.
2004.
A study ofsmoothing methods for language models applied to in-formation retrieval.
ACM Transactions on InformationSystems, 2(2):179?214.
