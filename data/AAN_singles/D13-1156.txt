Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1492?1502,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsFast Joint Compression and Summarization via Graph CutsXian Qian and Yang LiuThe University of Texas at Dallas800 W. Campbell Rd., Richardson, TX, USA{qx,yangl}@hlt.utdallas.eduAbstractExtractive summarization typically uses sen-tences as summarization units.
In contrast,joint compression and summarization can usesmaller units such as words and phrases, re-sulting in summaries containing more infor-mation.
The goal of compressive summariza-tion is to find a subset of words that max-imize the total score of concepts and cut-ting dependency arcs under the grammar con-straints and summary length constraint.
Wepropose an efficient decoding algorithm forfast compressive summarization using graphcuts.
Our approach first relaxes the length con-straint using Lagrangian relaxation.
Then wepropose to bound the relaxed objective func-tion by the supermodular binary quadratic pro-gramming problem, which can be solved ef-ficiently using graph max-flow/min-cut.
S-ince finding the tightest lower bound suffersfrom local optimality, we use convex relax-ation for initialization.
Experimental resultson TAC2008 dataset demonstrate our methodachieves competitive ROUGE score and hasgood readability, while is much faster than theinteger linear programming (ILP) method.1 IntroductionAutomatic multi-document summarization helpsreaders get the most important information fromlarge amounts of texts.
Summarization techniquescan be roughly divided into two categories: extrac-tive and abstractive.
Extractive summarization caststhe summarization task as a sentence selection prob-lem: identifying important summary sentences fromone or multiple documents.
Many methods havebeen developed in the past decades, including super-vised approaches that use classifiers to predict sum-mary sentences, graph based approaches to rank thesentences, and recent global optimization methodssuch as integer linear programming (Gillick et al2008) (ILP) and submodular maximization methods(Lin and Bilmes, 2011).
Though extractive summa-rization is popular because of its simplicity and highreadability, it has limitations in that it selects eachsentence as a whole, and thus may miss informativepartial sentences.To improve the informativeness, joint com-pression and summarization was proposed (Berg-Kirkpatrick et al 2011), which uses words as sum-marization units, unlike extractive summarizationwhere each sentence is a basic undecomposable u-nit.
To achieve better readability, manually definedgrammar constraints or automatically learned mod-els based on syntax trees are added during the sum-marization process.
Up to now, the state of the artcompressive systems are based on integer linear pro-gramming (ILP).
Because ILP suffers from expo-nential complexity, word-based compression sum-marization is an order of magnitude slower thansentence-based extraction.One common way to solve an ILP problem is touse its LP relaxation and round the results.
How-ever Berg-Kirkpatrick et al(2011) found that LPrelaxation gave poor results, finding unacceptablysuboptimal solutions.
For speedup, they proposed atwo stage method where they performed some sen-tence selection in the first step to reduce the numberof candidates.
Despite their empirical success, such1492a pruning approach has its inherent problem in thatit may eliminate correct sentences in the first step.Recently, Almeida and Martins (2013) proposed afast joint decoding algorithm based on dual decom-position.
For fast convergence, they added quadraticpenalty terms to alleviate the learning rate problem.In this paper, we propose an efficient decoding al-gorithm for fast ILP based compressive summariza-tion using graph cuts.
Our assumption is that all con-cepts are word n-grams and non-negatively scored.The rationale for the non-negativity assumption is s-traightforward: the score of a concept reflects its in-formativeness, hence should be non-negative.
Givena set of documents, each word is associated with abinary variable, indicating whether the word is se-lected in the summary.
Our idea is to approximatethe ILP as a binary quadratic programming problemwhere coefficients of all quadratic terms are non-negative.
It is well known that such binary quadrat-ic function is supermodular, and its maximum canbe solved efficiently using graph max-flow/min-cut.Hence the key is to find the coefficients of the super-modular binary quadratic function (SBQF) so thatits maximum is close to the optimal ILP objectivefunction.
Our solution consists of 3 steps.
First,we show that the subtree deletion model and gram-mar constraints can be eliminated by adding SBQF-s to the objective function.
Second, we relax thesummary length constraint using Lagrangian relax-ation.
Third, we propose a family of SBQFs thatare lower bounds of the ILP objective function.
S-ince finding the tightest lower bound suffers fromlocal optimality, we choose to use convex relaxationfor initialization.
To demonstrate our technique, weconduct experiments on Text Analysis Conference(TAC) datasets using the same train/test splits as pre-vious work (Berg-Kirkpatrick et al 2011).
We com-pare our approach with the state-of-the-art ILP basedapproach in terms of summary quality (ROUGE s-cores and sentence quality) and speed.
Experimen-tal results show that our proposed method achievescompetitive performance with ILP, while about 100times faster.2 Compressive Summarization2.1 Extractive SummarizationAs our method is an approximation of ILP basedmethod, we first briefly review the ILP based extrac-tive summarization and compressive summarization.Gillick and Favre (2009) introduced the concept-based ILP for summarization.
A concept is a basicsemantic unit.
They used word bigrams as such lan-guage concepts.
Their system achieved the highestROUGE score on the TAC 2009 evaluation.
Thisapproach selects sentences so that the total scoreof language concepts appearing in the summary ismaximized.
The association between the languageconcepts and sentences serves as the constraints, inaddition to the summary length constraint.Formally, given a set of sentences S = {sn}Nn=1,extractive summarization can be represented by a bi-nary vector y, where yn indicates whether sentencesn is selected.
Let C = {c1, .
.
.
cJ} denote the setof concepts in S, e.g., word bigrams (Gillick andFavre, 2009).
Each concept cj is associated with agiven score wj and a binary variable vj indicatingif cj is selected in the summary.
Let njk denote theindex of the sentence containing the kth occurrenceof concept cj , and ln denote the length of sentencesn.
The ILP based extractive summarization systemcan be formulated as below:maxy,vJ?j=1wjvjs.t.
vj =?kynjk 1 ?
j ?
J (1)N?i=1ynln ?
Lv,y are binaryThe first constraint is imposed by the relation be-tween concept selection and sentence selection: s-electing a sentence leads to the selection of all theconcepts it contains, and selecting a concept onlyhappens when it is present in at least one of the se-lected sentences.
The second constraint is the sum-mary length constraint.As solving an ILP problem is generally NP-hard,pre-pruning of candidate concepts and sentences isnecessary for efficient summarization.
For exam-1493ple, the ICSI system (Gillick et al 2008) removedthe sentences that are too short or have non-overlapwith the queries, and concepts with document fre-quency less than 3, resulting in 95.8 sentences andabout 80 concepts per topic on the TAC2009 dataset.Therefore the actual scale of ILP is rather small afterpruning (e.g., 176 variables and 372 constraints pertopic).
Empirical studies showed that such small s-cale ILP can be solved within a few seconds (Gillickand Favre, 2009).2.2 Compressive SummarizationThe quality of sentence-based extractive summariza-tion is limited by the informativeness of the orig-inal sentences and the summary length constraint.To remove the unimportant part from a long sen-tence, sentence compression is proposed to generatemore informative summaries (Liu and Liu, 2009; Liet al 2013a).
Recent studies show that joint sen-tence compression and extraction, namely compres-sive summarization, outperforms pipeline systemsthat run extractive summarization on the compressedsentences or compress selected summary sentences(Martins and Smith, 2009; Berg-Kirkpatrick et al2011; Chali and Hasan, 2012).
In Berg-Kirkpatricket al(2011), compressive summarization inte-grates the concept model for extractive summariza-tion (Gillick and Favre, 2009) and subtree deletionmodel for sentence compression.
The score of acompressive summary consists of two parts, scoresof selected concepts, and scores of the broken arcsin the dependency parse trees.
The selected word-s must satisfy the length constraint and grammarconstraints that include subtree constraint and somemanually defined hard constraints.Formally, let x = x1 .
.
.
xI denote the word se-quence of documents, where s1 = x1, .
.
.
xl1 corre-sponds to the first sentence, s2 = xl1+1, .
.
.
, xl1+l2corresponds to the second sentence, and so on.
Acompressive summary can be represented by a bi-nary vector z, where zi indicates whether word xiis selected in the summary.
Let ahm denote the arcxh ?
xm in the dependency parse tree of the cor-responding sentence containing words xh and xm,and A = {ahm} denote the set of dependency arcs.The subtree constraint ensures that word xm is se-lected only if its head xh is selected.
In order toguarantee the readability, grammar constraints areadded to prohibit the breaks of some specific arc-s. For example, Clarke and Lapata (2008) nev-er deleted an arc whose dependency label is SUB,OBJ, PMOD, SBAR or VC.
In this paper, we useB ?
A to denote the set of these arcs that mustnot be broken in summarization.
We use ojk to de-note the indices of words corresponding to the kthoccurrence of cj .
For example, suppose the jthconcept European Union appears twice in the doc-ument: x22x23 = x50x51 =European Union, thenoj1 = {22, 23}, oj2 = {50, 51}.The compressive summarization model can beformulated as an integer programming problemmaxz,vJ?j=1wj ?
vj +?ahm?Awahmzh(1?
zm)s.t.
vj =?k?i?ojkzi ?j?izi ?
Lzh ?
zm ?ahm ?
A (2)zh = zm ?ahm ?
Bz,v are binaryAccording to the subtree deletion model, the scoreof arc ahm is included if zh = 1 and zm = 0, whichcan be formulated as wahm ?
zh(1 ?
zm).
The firstconstraint is similar to that in extractive summariza-tion, that is, a concept is selected if and only if anyof its occurrence is selected.
The third and fourthconstraints are the subtree constraints and manual-ly defined grammar constraints respectively.
In therest of the paper, without loss of generality, we re-move the fourth constraint by directly substitutingone variable for the other.Finding the optimal summary is generally NP-hard.
Unlike extractive summarization where the s-cale of the problem (the number of sentences andconcepts) is small, the number of variables in com-pressive summarization is linear in the number ofwords, which is usually thousands on the TACdatasets.
Hence solving such a problem using ILPbased decoding algorithms is not efficient especiallywhen the document set is large.14943 Fast Decoding via Graph CutsIn this section, we introduce our fast decoding al-gorithm.
We assume that all the concepts are wordn-grams, and their scores are non-negative.
The non-negativity assumption can reduce the computationalcomplexity, but is also reasonable: the score of aconcept denotes its informativeness, hence shouldbe non-negative.
For example, Li et al(2013b)proposed to use the estimated normalized frequen-cies of concepts as scores, which are essentially non-negative.
The basic idea of our method is to approx-imate the above optimization problem (2) by the su-permodular binary quadratic programming (SBQP)problem:maxz?i?izi +?ij?ijzizjs.t.
z is binary (3)where ?ij ?
0.
It is known that such a binaryquadratic function is supermodular, and its maxi-mum can be solved efficiently using graph max-flow/min-cut (Billionnet and Minoux, 1985; Kol-mogorov and Zabih, 2004).
Now the problem is tofind the optimal ?, ?
for a good approximation.3.1 Formulate Grammar Constraints andSubtree Deletion Model by SBQFWe show that the subtree deletion model can be for-mulated equivalently using SBQF.
First, we can e-liminate the constraint zh ?
zm by adding a penaltyterm to the objective function.
That is,max f(z)s.t.
zh ?
zmz is binaryis equivalent tomax f(z)??(1?
zh)zms.t.
z is binaryWe can see that the penalty term??
(1?zh)zm ex-cludes zh = 0, zm = 1 from the feasible set, andfor zh ?
zm, both problems have the same objectivefunction value.
Hence the two problems are equiva-lent.
Notice that the coefficient of quadratic term in??
(1 ?
zh)zm is positive, hence the penalty termis supermodular.Now we eliminate the third constraint in problem(2) using the penalized objective function describedabove.
Note that the fourth constraint has been elim-inated by variable substitution, we havemaxz,vJ?j=1wj ?
vj +?ahm?Awahmzh(1?
zm)???ahm?A(1?
zh)zms.t.
vj =?k?i?ojkzi ?j (4)?izi ?
Lz,v are binaryWe can see that for each arc ahm, there must be apositive quadratic term +?zhzm in the objectivefunction, which guarantees the supermodularity ofthe objective function, no matter what wahm is.3.2 Eliminate Length Constraint UsingLagrangian RelaxationProblem (4) is NP-hard, because for any feasible v,it is a SBQP with a length constraint.
Since size con-strained minimum cut problem is generally NP-hard(Nagano et al 2011), Problem (4) can not be castas a SBQP as long as P ?= NP.
One popular way todeal with the size constrained optimization problemis Lagrangian relaxation.
We introduce Lagrangianmultiplier ?
to the length constraint in Problem (4),and getmin?maxz,vJ?j=1wj ?
vj +?ahm?Awahmzh(1?
zm)???ahm?A(1?
zh)zm+?(L??izi)s.t.
vj =?k?i?ojkzi ?j (5)?
?
0z,v are binaryWe solve the relaxed problem iteratively.
In eachiteration, we fix ?
and solve the inner maximiza-tion problem (details described below).
The score of1495each word is penalized by ?
?
with larger ?, few-er words are selected.
Hence the summary lengthcan be adjusted by ?.
The optimal ?
can be foundusing binary search.
We maintain an upper bound?max , and a lower bound ?min, which is initially 0.In each iteration, we choose ?
= 12(?max + ?min)and search the optimal z.
If the duality gap vanish-es, i.e., ?
(L ?
?i zi) = 0 and?i zi ?
L, then weget the global solution of Problem (4).
Otherwise,if?i zi > L, then the current ?
is too small, so weset ?min = ?
; otherwise, ?
> 0 and?i zi < L,we set ?max = ?.
The search process terminates if?max ?
?min is less than a predefined threshold.3.3 Eliminate v Using SupermodularRelaxationNow we consider the inner maximization Problem(5).
It is still not a SBQP, since the objective func-tion is not a linear function of zizj .
We propose toapproximate the objective function using SBQP.
Oursolution consists of two steps.
First we relax the firstconstraint of Problem (5) by bounding the objec-tive function with a family of supermodular pseudoboolean functions (Boros and Hammer, 2002).
Sec-ond we reformulate these pseudo boolean functionsequivalently as quadratic functions.Similar to the bounding strategy in (Qian and Liu,2013), we relax the logical disjunction by lineariza-tion.
Using the fact that for any binary vector a, wehave?ai = maxp??
?ipiaiwhere ?
denotes the probability simplex?
= {p|?kpk = 1, pk ?
0}We havevj =?k?i?ojkzi= maxpj??
?kpjk?i?ojkziPlug the equation above into the objective functionof Problem (5), we get the following optimizationproblemmaxz,pJ?j=1???kpjkwj?i?ojkzi??+?ahm?Awahmzh(1?
zm)???ahm?A(1?
zh)zm+?(L??izi)s.t.
z is binary (6)pj ?
?
?jLetQ(p, z) denote the objective function of Prob-lem (6).
Given p, we can see that Q is a supermod-ular pseudo boolean function because coefficientsof all non-linear terms are non-negative.
Using thefact that for any binary vector a = [a1, .
.
.
ar]T ,ai ?
{0, 1}, 1 ?
i ?
r,r?i=1ai = maxb?
{0,1}( r?i=1ai ?
r + 1)b(Freedman and Drineas, 2005), we get the followingequivalent optimization problem of Problem (6)maxz,p,qJ?j=1?kpjkwjqjk??
?i?ojkzi ?
|ojk|+ 1??+?ahm?Awahmzh(1?
zm)???ahm?A(1?
zh)zm+?(L??izi)s.t.
z,q are binary (7)pj ?
?
?jwhere |ojk| is the size of ojk.Let R(z,p,q) denote the objective function ofProblem (7), to search the optimal point, we al-ternatively update p and z,q.
First we initializep = p(0).
In each iteration, we first fix p. It is ob-vious that Problem (7) is a SBQP, hence the optimalz,q can be solved efficiently using max-flow/min-cut.
Then we fix z,q, and update p using projected1496subgradient.
That ispnewj = P?
(pj +?R?pj?
)(8)where ?
> 0 is the step size in line search, and func-tion P?
(q) denotes the projection of q onto the fea-sible set ?P?
(q) = minp???p?
q?2which can be solved efficiently by sorting (Duchi etal., 2008).3.4 Initialize p Using Convex RelaxationSince R is non-concave, searching its maximum us-ing subgradient method suffers from local optimali-ty.
Though one can use techniques such as branch-and-bound for exact inference (Qian and Liu, 2013;Gormley and Eisner, 2013), here for fast decoding,we use convex relaxation to choose a good seedp(0).
Recall that pjk denotes the percentage of thekth occurrence contributing to cj .
The larger pjk is,the more likely the kth occurrence is selected.
Toestimate such likelihood, we replace the binary con-straint in extractive summarization (Problem (1)) by0 ?
y,v ?
1, since solving a relaxed LP is muchfaster than ILP.
Suppose y?
is the optimal solutionfor such a relaxed LP problem, we initialize p bypjk =y?njk?k y?njk(9)If for all k, y?njk = 0, then we initialize pjk usinguniform distributionpjk =1|oj |where |oj | is the frequency of cj .3.5 SummaryFor clarity, we summarize our decoding algorithm inAlgorithm 1.
Initial ?max can be arbitrarily large.
Inour experiments, we set ?max =?j wj , which em-pirically guarantees the summary length?i zi ?
Lwhen ?
= ?max.
The choice of the step size forupdating p is similar to the projected subgradien-t method in dual decomposition (Koo et al 2010).Algorithm 1 Compressive Summarization viaGraph CutsRequire: Scores of concepts {wj} and arcs {wahm},max summary length L.Ensure: Compressive summarization z?, where zi indi-cates whether the ith word is selected.Solve the relaxed LP of Problem (1) (replace the binaryconstraint by 0 ?
y,v ?
1) to get y.Initialize p(0) using Eq (9).Initialize sufficient large ?max, and ?min = 0while ?max ?
?min > ?
doSet ?
= 12 (?min + ?max)Set p = p(0).repeatFix p, solve Problem (7) to get z using max-flow/min-cut.Update p using Eq (8).until convergenceif?i zi > L then?min = ?else if?i zi < L then?max = ?elsebreakend ifend while4 Features and Hard ConstraintsWe choose discriminative models to learn the scoresof concepts and arcs.
For concept cj , its score iswj = ?Tconceptfconcept(cj)where fconcept(cj) is the feature vector of cj , and?concept is the corresponding weight vector of fea-ture fconcept(cj).
Similarly, score wahm is definedaswahm = ?Tarcfarc(ahm)Though our algorithm can handle general word n-gram concepts, we restrict the concepts to word bi-grams, which have been widely used recently in thesentence-based ILP extractive summarization sys-tems.
For a concept cj , we define the followingfeatures, some of which have been used in previouswork (Brandow et al 1995; Aker and Gaizauskas,2009; Edmundson, 1969; Radev, 2001; Li et al2013b).
All of these features are non-negative.?
Term frequency: the frequency of cj in the giv-en topic.1497?
Stop word ratio: ratio of stop words in cj .
Thevalue can be {0, 0.5, 1}.?
Similarity with topic title: the number of com-mon words in these two strings, divided by thelength of the longer string.?
Document ratio: percentage of documents con-taining cj .?
Sentence ratio: percentage of sentences con-taining cj .?
Sentence-title similarity: word uni-gram/bigrams cosine similarity betweenthe sentence containing cj and the topic title.For concepts appearing in multiple sentences,we choose the maximal similarity.?
Sentence-query similarity: word uni-gram/bigram cosine similarity betweenthe sentence containing cj and the topic query(concatenation of topic title and description).For concepts appearing in multiple sentences,we choose the maximal similarity.?
Sentence position: position of the sentencecontaining cj in the document.
For conceptsappearing in multiple sentences, we choose theminimum.?
Sentence length: length of the sentence con-taining cj .
For concepts appearing in multiplesentences, we choose the maximum.?
Paragraph starter: binary feature indicatingwhether cj appears in the first sentence of aparagraph.For subtree deletion model, we define the follow-ing features for arc ahm.?
POS tags of head word xh and child word xmand their concatenations.?
Dependency label of arc ahm and its parent arc.?
Word xm if xm is a conjunction word or prepo-sition word.
Word xh if xm is a conjunctionword or preposition word.?
Binary feature indicating whether the modifierxm is a temporal word such as Friday.We also define some hard constraints for subtreedeletion to improve the readability of the generatedcompressed sentences.?
C0 Arc ahm can be cut only if one of the twoconditions holds: (1) there is a comma, colon,or semicolon between the head and the modifi-er; (2) the modifier word is a preposition (POStag is IN) or a wh-word, such as what, who,whose (corresponds to POS tag IN, WDT, WP,WP$, WRB).?
C1 Arcs with dependency labels SUB, OBJ,PRD, SBAR or VC can not be cut.?
C2 Arcs in set phrases like so far, more than,according to can not be cut.?
C3 All arcs in coordinate structures can not becut, such as cats and dogs.Note that compared with previous work, our com-pression is more conservative.
Constraint C0 al-lows only a small portion of arcs to be cut.
Thisis based on our observation of the sentence com-pression corpus: removing preposition phrases (PP)or sub-clauses can greatly reduce the length of sen-tence, while hurting the readability little.
Cuttingother arcs like NMOD usually removes only one ortwo words, and possibly affects the sentence?s read-ability.5 Experimental Results5.1 Experimental SetupDue to the lack of training data for compressivesummarization, we learn the subtree deletion mod-el and the concept model separately.
Specifically,the sentence compression dataset (Clarke and La-pata, 2008) (referred as CL08) is used for subtreedeletion model training (?arc).
A sentence pair inthe corpus is kept for training the subtree deletionmodel if the compressed sentence can be derived bydeleting subtrees from the parse tree of the origi-nal sentence.
There are 3, 178 out of 5, 739 suchpairs.
The concept model (?concept) is learned fromthe TAC2009 dataset.
We create the oracle extrac-tive summaries with the maximal bigram recall asthe reference summary.
TAC2010 data is used as1498Corpus Sent.
Words TopicsTrain TAC2009 4, 216 117, 304 44CL08 3, 178 52, 624 N/ADevelop TAC2010 2, 688 72, 609 46Test TAC2008 4, 518 123, 946 48Table 1: Corpus statistics.
Training data consist oftwo parts, TAC2009 for learning the concept mod-el, CL08 (Clarke and Lapata, 2008) for learning thesubtree deletion model.development set for various parameter tuning.
Table1 has the descriptions of all the data used.We choose averaged perceptron for fast training.The number of iterations is tuned on the develop-ment data.
Remind that our algorithm is based on theassumption that scores of concepts are non-negative,?j, wj ?
0.
We assume that feature vector fconceptis non-negative (e.g., term frequency, n-gram fea-tures), then ?concept ?
0 is required to guarantee thenon-negativity of wj .
Therefore, we project ?conceptonto the non-negative space after each iteration.
S-ince training is offline, we use ILP based exact in-ference for accurate learning.
1To control the contributions of the concept mod-el and the subtree deletion model, we introduce aparameter ?, and modify the original maximizationproblem (Problem 2) to:maxz,vJ?j=1wj ?
vj + ???ahm?Awahmzh(1?
zm)We tune ?
on TAC2010 dataset.
For max-flow/min-cut, in our experiments, we implemented the im-proved shortest augmented path (SAP) method (Ed-monds and Karp, 1972).For performance measure of the summaries, weuse both ROUGE and linguistic quality.
ROUGEhas been widely used for summarization perfor-mance and can measure the informativeness of thesummaries (content match between system and ref-erence summaries).
Since joint compression andsummarization tends to pick isolated words to max-imize the information coverage in the system gener-ated summaries, it may have poor readability.
There-fore we conduct human evaluation for the linguis-1we choose the GLPK as our ILP solver, which is used in(Berg-Kirkpatrick et al 2011)tic quality for various systems.
The linguistic qual-ity consists of two parts.
One evaluates the gram-mar quality within a sentence.
Annotators markedif a compressed sentence is grammatically correc-t.
Typical grammar errors include lack of verb orsubordinate clause.
The other evaluates the coher-ence between sentences, including the order of sen-tences and irrelevant sentences.
For compressivesummaries, we removed the sentences with gram-mar errors when evaluating coherence.
The overalllinguistic quality score is the combined score of thepercentage of grammatically correct sentences andthe correct ordering of the summary sentences.
Thescore is scaled and ranges from 1 (bad) to 10 (good).5.2 Results on the Development SetWe conducted a series of experiments on the de-velopment dataset to investigate the effect of thenon-negative score assumption, SBQP approxima-tion, and initialization.
First, we build a stan-dard ILP based compressive summarizer without thenon-negative score assumption.
We varied ?
over{2?4, 2?3, .
.
.
24} and selected the optimal ?
= 2?2according to both ROUGE-2 score and linguisticquality.
This interpolation weight is used in all theother experiments.To study the impact of the non-negative score as-sumption, we build another summarizer by replac-ing the concept model with the one trained under thenon-negative constraint.
We also compared three d-ifferent initialization strategies for p. The first oneis uniform initialization, i.e., pjk = 1|oj | .
The secondone is a greedy approach, where extractive summa-rization is obtained by greedy search (i.e., add thetop ranked sentence iteratively), then we use the cor-responding y and Eq (9) to initialize p. The last oneis our convex relaxation method described in Sec-tion 3.4.Table 2 shows the comparison results.
For com-parison, we also include the sentence-based ILP ex-tractive summarization results.
We can see that theimpact of initial p is substantial.
Using convex re-laxation helps our method to survive from local opti-mality.
The non-negativity assumption has very lit-tle effect on the standard compressive summariza-tion (comparing the first two rows).
This empir-ical result demonstrates the appropriateness of theassumption we use in our proposed method.1499System R-2 LQILP (?
= 2?2) 11.22 6.3ILP (Non Neg.)
11.18 6.4Graph Cut (uniform) 9.54 5.9Graph Cut (greedy) 10.13 6.2Graph Cut (LP) 11.06 6.1Sent Extractive 10.11 7.3Table 2: Experimental results on developmen-t dataset.
R-2 and LQ are short for ROUGE-2 scoremultiplied by 100, and linguistic quality respective-ly.5.3 Results on Test DatasetTable 3 shows the summarization results for varioussystems on the TAC2008 data set.
We show both thesummarization performance and the speed2 of thesystem.
The presented systems include our graph-cut based method, the ILP based compression andsummarization, and the sentence-based extractivesummarization.
ILP 2-step refers to the 2-step fastdecoding strategy proposed by (Berg-Kirkpatrick etal., 2011).We also list the performance of some state-of-the-art systems, including the two ICSI systems (Gillicket al 2008), the compressive summarization sys-tem of Berg-Kirkpatrick et al(2011) (GBK?11),the multi-aspect ILP system of Woodsend and Lapa-ta (2012)(WL?12) and the dual decomposition basedsystem (Almeida andMartins, 2013) (AM?13).
Notethat for these referred systems since the linguisticquality results are not comparable due to differentjudgment methods.
For our graph-cut based method,to study the tradeoff between the readability of thesummary and the ROUGE scores, we present twoversions for this method: one uses all the constraints(C0-C3), the other does not use C0.We can see that our proposed method balancedspeed and quality.
Compared with ILP, we achievedcompetitive ROUGE scores, but with about 100xspeedup.
Our method is also faster than the 2-stepILP system.
We also tried another state-of-the-artLP solver, Gurobi version 5.53, it achieves 0.17 sec-onds per topic, much faster than GLPK, but stil-2For fair comparison, we only recode the running time fordecoding.
Other time costs like feature extraction, IO opera-tions are excluded.3www.gurobi.comSystem R-2 R-SU4 LQ sec.Graph Cut 11.74 14.54 6.5 0.056Graph Cut w/o C0 12.05 14.71 5.4 0.053ILP 11.86 14.62 6.6 5.5ILP (Non Neg.)
11.82 14.60 6.6 5.2ILP (2-step) 11.72 14.49 6.5 1.1Sent Extractive 11.06 13.93 7.1 0.13ICSI-1 11.0 13.4 - 0.38?ICSI-2 11.1 14.3 - -BGK?11 11.70 14.38 6.5?
-WL?12 11.37 14.47 - -AM?13 12.30+ 15.18+ 4.2?
0.41?Table 3: Experimental results on TAC2008 dataset.Columns 2-5 are scores of ROUGE-2, ROUGE-SU4, linguistic quality, and speed (seconds per top-ic).
ROUGE-2 and ROUGE-SU4 scores are multi-plied by 100.
All the experiments are conducted onthe platform Intel Core i5-2500 CPU 3.30GHz.
?numbers are not directly comparable due to differ-ent annotations or platforms.
+ extra resources areused.l slower than ours.
Regarding the grammar con-straints used in our system, from the two result-s for our graph-cut based method, we can see thatadding constraint C0 significantly decreases the R-2score but improves the language quality.
This showsthat word-based joint compression and summariza-tion can improve ROUGE score; however, we needto keep in mind about linguistic quality and find atradeoff between the ROUGE score and the linguis-tic quality.
Almeida and Martins (2013) trained theirmodel on extra corpora using multi-task learning,and achieved better results than ours.
The resultsof our system and theirs showed that Lagrangian re-laxation based method combined with combinatorialoptimization algorithms such as dynamic program-ming or minimum cut can exploit the inner structureof problems and achieve significant speedup overILP.Four example summaries produced by our systemare shown below.
Words in gray are not selected inthe summary.1500India?s space agency is ready to send a man to space within sev-en years if the government gives the nod, while preparations havelready begun for the launch of an unmanned lunar mission, a topofficial said.
India will launch more missions to the moon if itsmaiden unmanned spacecraft Chandrayaan-1, slated to be launchedby 2008, is successful a top space fficial said Tuesday.
The Unit-ed States, the European Space Agency, China, Japan and India areall planning lunar missions during the ext decade.India is ?a stepahead?
of China in satellite technology and can surpass Beijingin space research by tapping the talent of its huge pool of youngscientists, India?s space research chief said Monday.
The spaceagencies of India and France signed an agreement on Friday to co-operate in launching a satellite in four years that will help makeclimate predictions more accurate.
The Indian Space Research Or-ganization (ISRO) has short-listed experiments from five nation-s including the United States, Britain and Germany, for a slot onIndia?s unmanned moon mission Chandrayaan-1 to be undertakenby 2006-2007, the Press Trust of India (PTI) reported Monday.
Athree-member Afghan delegation is in Bangalore seeking help toset up a high-tech telemedicine facility in 10 Afghan cities linkedvia Indian satellites, Indo-Asian News Service reported Saturday.A woman was killed in Mississippi when a tree crashed on her car,becoming the 11th fatality blamed on the powerful Hurricane Kat-rina that slammed the US Gulf coast after pounding Florida, localTV reportedMonday.
The bill for the Hurricane Katrina disaster ef-fort has so far reached 2.87 billion dollars, federal officials said onTuesday.
The official death toll from Hurricane Katrina has risento 118 people in and around the swamped city of New Orleans,officials said Thursday.
The Foreign Ministry on Friday reportedthe first confirmed death of a Guatemalan due to Hurricane Kat-rina in the United States.
The Ugandan government has pledged200,000 US dollars toward relief and rebuilding efforts in the after-math of Hurricane Katrina, local press reported on Friday.
SwissReinsurance Co., the world?s second largest reinsurance companyon Monday doubled to 40 billion US dollars its initial estimate ofthe global insured losses caused by Hurricane Katrina in the UnitedStates.The A380 ?superjumbo?, which will be presented to the world ina lavish ceremony in southern France on Tuesday, will be prof-itable from 2008, its maker Airbus told the French financial news-paper La Tribune.
The A380 will take over from the Boeing 747as the biggest jet in the skies.
An association of residents living n-ear Paris?s Charles-de-Gaulle airport on Wednesday denounced thenoise pollution generated by the giant Airbus A380, after the newairliner?s maiden flight.
One problem that Airbus is encounteringwith its new A380 is that the craft pushes the envelope on the max-imum size of a commercial airplane.
With a whisper more than aroar, the largest passenger airliner ever built, the Airbus 380, tookoff on its maiden flight Wednesday.
?When she came in, she was in good spirits,?
a prison staffer toldthe New York Daily News.
Martha Stewart, the American celebrityhomemaker who had her own cooking and home improvement TVshow, reported to a federal prison in Alderson, West Virginia, onFriday to serve a five-month sentence for lying about a stock sale.Home fashion guru Martha Stewart said on Friday that she has ad-justed to prison life and is keeping busy behind bars since reportinga week ago to a federal penal camp in West Virginia, where sheis serving a five-month sentence for lying about a stock sale.
Thelawyer said he did not know what she is writing, but Stewart hassuggested since her conviction that she might write a book abouther recent experience with the legal system.
Walter Dellinger, thelawyer leading the appeal, said on NBC?s ?Today?
that Stewart isexploring ?innovative ways to do microwave cooking?
The lawyersaid he did not know with her fellow inmates.
As Martha Stewartarrives at the red-brick federal prison in Alderson, W. Va., on Fri-day to begin a five-month sentence, the company she founded isfocused both on life without her and on life once she returns.In most cases, the removed phrases do not hurt thereadability of the summaries.
The errors are mainlycaused by the break of sub-clauses or main claus-es that are separated by commas, for example, thefourth sentence in the last summary, The lawyer saidhe did not know what she is writing.
The compressedsentence is grammatically correct, but semanticallyincomplete.
Other errors are due to the lack of verb,subject, or object, or incorrect removal of PP, suchas the last sentence of the last summary.6 ConclusionIn this paper, we propose a fast decoding algorith-m for compressive summarization using graph cuts.Our idea is to approximate the original ILP prob-lem using supermodular binary quadratic program-ming (SBQP) problem.
Under the assumption thatscores of concepts are non-negative, we eliminatesubtree constraints and grammar constraints, andrelax the length constraint and non-supermodularpart of the problem step by step.
Our experimen-tal results showed that the graph cut based methodachieved competitive performance compared to ILP,while about 100 times faster.There are several possibilities for further researchinvolving our graph cut algorithms.
One idea is toapply it to the language model based compressionmethod (Clarke and Lapata, 2008).
The other isto adapt it to social media text summarization task,where text is much more noisy.
As graph cut is ageneral method, applying it to other binary struc-tured learning tasks is also an interesting direction.AcknowledgmentsWe?d like to thank three anonymous reviewers fortheir valuable comments.
This work is partly sup-ported by NSF award IIS-0845484 and DARPA un-der Contract No.
FA8750-13-2-0041.
Any opinionsexpressed in this material are those of the authorsand do not necessarily reflect the views of the fund-ing agencies.ReferencesA.
Aker and R. Gaizauskas.
2009.
Summary generationfor toponym-referenced images using object type lan-guage models.
In Proceedings of RANLP.1501Miguel Almeida and Andre Martins.
2013.
Fast and ro-bust compressive summarization with dual decompo-sition and multi-task learning.
In Proceedings of ACL,pages 196?206, August.Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProceedings of ACL-HLT, pages 481?490, June.A.
Billionnet and M. Minoux.
1985.
Maximizing a su-permodular pseudoboolean function: A polynomial al-gorithm for supermodular cubic functions.
DiscreteApplied Mathematics, 12(1):1 ?
11.Endre Boros and Peter L. Hammer.
2002.
Pseudo-boolean optimization.
Discrete Applied Mathematics,123(1C3):155 ?
225.Ronald Brandow, Karl Mitze, and Lisa F. Rau.
1995.Automatic condensation of electronic publications bysentence selection.
Information Processing & Man-agement, 31(5):675 ?
685.Yllias Chali and Sadid A. Hasan.
2012.
On the effective-ness of using sentence compression models for query-focused multi-document summarization.
In COLING,pages 457?474.James Clarke and Mirella Lapata.
2008.
Global in-ference for sentence compression: An integer linearprogramming approach.
J. Artif.
Intell.
Res.
(JAIR),31:399?429.John Duchi, Shai Shalev-Shwartz, Yoram Singer, andTushar Chandra.
2008.
Efficient projections onto theL1-ball for learning in high dimensions.
In Proceed-ings of ICML, pages 272?279.Jack Edmonds and Richard M. Karp.
1972.
Theoret-ical improvements in algorithmic efficiency for net-work flow problems.
J. ACM, 19(2):248?264, April.H.
P. Edmundson.
1969.
New methods in automatic ex-tracting.
J. ACM, 16(2):264?285, April.Daniel Freedman and Petros Drineas.
2005.
Energy min-imization via graph cuts: Settling what is possible.
InCVPR (2), pages 939?946.Dan Gillick and Benoit Favre.
2009.
A scalable globalmodel for summarization.
In Proceedings of the Work-shop on Integer Linear Programming for Natural Lan-guage Processing, pages 10?18, June.Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur.
2008.The ICSI summarization system at tac 2008.
In Pro-ceedings of the Text Understanding Conference.Matthew R. Gormley and Jason Eisner.
2013.
Noncon-vex global optimization for latent-variable models.
InProceedings of ACL, pages 444?454, August.Vladimir Kolmogorov and Ramin Zabih.
2004.
What en-ergy functions can be minimized via graph cuts.
IEEETransactions on Pattern Analysis and Machine Intelli-gence, 26:65?81.Terry Koo, Alexander M. Rush, Michael Collins, TommiJaakkola, and David Sontag.
2010.
Dual decomposi-tion for parsing with non-projective head automata.
InProceedings of EMNLP 2010, pages 1288?1298, Oc-tober.Chen Li, Fei Liu, Fuliang Weng, and Yang Liu.
2013a.Document summarization via guided sentence com-pression.
In Proceedings of EMNLP (to appear), Oc-tober.Chen Li, Xian Qian, and Yang Liu.
2013b.
Using super-vised bigram-based ILP for extractive summarization.In Proceedings of ACL, pages 1004?1013, August.Hui Lin and Jeff Bilmes.
2011.
A class of submodularfunctions for document summarization.
In Proceed-ings of ACL, pages 510?520, June.Fei Liu and Yang Liu.
2009.
From extractive to abstrac-tive meeting summaries: Can it be done by sentencecompression?
In Proceedings of ACL-IJCNLP 2009,pages 261?264, August.Andre?
F. T. Martins and Noah A. Smith.
2009.
Summa-rization with a joint model for sentence extraction andcompression.
In Proceedings of the Workshop on In-teger Linear Programming for Natural Langauge Pro-cessing, ILP ?09, pages 1?9.Kiyohito Nagano, Yoshinobu Kawahara, and Kazuyuk-i Aihara.
2011.
Size-constrained submodular min-imization through minimum norm base.
In ICML,pages 977?984.Xian Qian and Yang Liu.
2013.
Branch and bound algo-rithm for dependency parsing with non-local features.TACL, 1:105?151.Dragomir R. Radev.
2001.
Experiments in single andmultidocument summarization using mead.
In In FirstDocument Understanding Conference.Kristian Woodsend and Mirella Lapata.
2012.
Mul-tiple aspect summarization using integer linear pro-gramming.
In Proceedings of EMNLP-CoNLL, pages233?243, July.1502
