Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 781?787,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsAspect-Level Cross-lingual Sentiment Classificationwith Constrained SMTPatrik LambertUniversitat Pompeu Fabra, Barcelona, Spainpatrik.lambert@upf.eduAbstractMost cross-lingual sentiment classifica-tion (CLSC) research so far has been per-formed at sentence or document level.Aspect-level CLSC, which is more appro-priate for many applications, presents theadditional difficulty that we consider sub-sentential opinionated units which have tobe mapped across languages.
In this pa-per, we extend the possible cross-lingualsentiment analysis settings to aspect-levelspecific use cases.
We propose a method,based on constrained SMT, to transferopinionated units across languages by pre-serving their boundaries.
We show thatcross-language sentiment classifiers builtwith this method achieve comparable re-sults to monolingual ones, and we com-pare different cross-lingual settings.1 IntroductionSentiment analysis (SA) is the task of analysingopinions, sentiments or emotions expressed to-wards entities such as products, services, organi-sations, issues, and the various attributes of theseentities (Liu, 2012).
The analysis may be per-formed at the level of a document (blog post, re-view) or sentence.
However, this is not appropriatefor many applications because the same documentor sentence can contain positive opinions towardsspecific aspects and negative ones towards otheraspects.
Thus a finer analysis can be conductedat the level of the aspects of the entities towardswhich opinions are expressed, identifying for eachopinionated unit elements such as its target, polar-ity and the polar words used to qualify the target.The two main SA approaches presented in theliterature are (i) a machine learning approach,mostly supervised learning with features such asopinion words, dependency information, opinionshifters and quantifiers and (ii) a lexicon-based ap-proach, based on rules involving opinion wordsand phrases, opinion shifters, contrary clauses(but), etc.
Thus in most SA systems we may dis-tinguish three types of resources and text:TRAIN Resources (collection of training exam-ples, lexicons) used to train the classifier.TEST Opinions to be analysed.OUT Outcome of the analysis.
It depends on thelevel of granularity.
At the document or sentencelevel, it is the polarity of each document or sen-tence.
At the aspect level, it may the set of opiniontargets with their polarity.The internet multilingualism and the globalisa-tion of products and services create situations inwhich these three types of resources are not allin the same language.
In these situations, a lan-guage transfer is needed at some point to performthe SA analysis or to understand its results, thuscalled cross-lingual sentiment analysis (CLSA).Sentences or documents are handy granularitylevels for CLSA because the labels are not relatedto specific tokens and thus are not affected by alanguage transfer.
At the aspect level, labels areattached to a specific opinionated unit formed bya sequence of tokens.
When transferring these an-notations into another language, the opinionatedunits in the two languages have thus to be mapped.This paper is one of the first ones to addressCLSA at aspect level (see Section 3).
It makesthe following specific contributions:(i) an extended definition of CLSA includinguse cases and settings specific to aspect-levelanalyses (Section 2);(ii) a method to perform the language transferpreserving the opinionated unit boundaries.This avoids the need of mapping source andtarget opinionated units after the languagetransfer via methods such as word alignment(Section 4);781The paper also reports (in Section 5) experimentscomparing different settings described in Sec-tion 2.2 Use Cases and SettingsWe can think of the following use cases for CLSA:Use case I.
There are opinions we want to ana-lyse, but we do not avail of a SA system to performthis analysis.
We thus want to predict the polarityof opinions expressed in a language LTESTus-ing a classifier in another language LTRAIN.
Wecan assume that the language LOUTof the analysisoutcome1is the same as the one of the opinions.
Inthis case, equation 1 applies, yielding CLSA set-tings a and b as follows (see also Figure 1).LTRAIN6= LTEST;LOUT= LTEST(1)(a) available training resources are transferredinto the test language to build a classifier in thetest language.
(b) we translate the test into the language of theclassifier, classify the opinions in the test, and thentransfer back the analysis outcome into the sourcelanguage by projecting the labels or/and opinion-ated units onto the test set.
(a)TRAIN TRAINL?TESTTEST OUTLTESTTSAL?TESTLearn(b)TEST OUTL?TESTTESTL?TRAINOUTLTRAINTSALTRAINTProjFigure 1: Use case I settings.
SA refers to Senti-ment Analisys, T to Translation, Proj to Projec-tion and Learn to Learning, and the prime sym-bol designs a language into which a set has beenautomatically translated.Use case II.
We may have training resources inthe language of the opinions, but we need the re-1As mentioned above, at the aspect level, the outcome ofthe analysis may be a set of opinion targets with their polar-ity.
It may also be more complex, such as a set of opinionexpressions with their respective target, polarity, holder andtime (Liu, 2012).
The outcome may need to be in another lan-guage as the opinions themselves.
For example, a companybased in China may survey the opinions of their Spanish-speaking customers, and then transfer the SA outcome intoChinese so that their marketing department can understand it.sult of the analysis in a different language.
Here,the inequality of Eq.
2 applies, yielding CLSA set-tings c and d as follows (see also Figure 2).LOUT6= LTEST(2)(c) LTRAIN= LTEST; the test opinions arefirst analysed in their language, then the analysisoutcome is transferred into the desired language.
(d) LTRAIN= LOUT; the test set is first trans-ferred into the desired outcome language, and theSA is performed in this language.
(c) TEST OUTLTESTOUTL?OUTSALTEST T(d) TEST TESTL?OUTOUTTSALOUTFigure 2: Use case II settings.Use case II only makes sense for aspect-levelanalysis,2and to our knowledge, it was not ad-dressed in the literature so far.Use case III.
We want to benefit from dataavailable in several languages, either to have moreexamples and improve the classifier accuracy, or tohave a broader view of the opinions under study.In this paper we focus on use cases I and II.3 Related WorkThe main CLSC approaches described in the liter-ature are via lexicon transfer, via corpus transfer,via test translation and via joint classification.In the lexicon transfer approach, a source senti-ment lexicon is transferred into the target languageand a lexicon-based classifier is build in the tar-get language.
Approaches to transfer lexica in-clude machine translation (MT) (Mihalcea et al.,2007), Wordnet (Banea et al., 2011; Hassan et al.,2011; Perez-Rosas et al., 2012), relations betweendictionaries represented in graphs (Scheible et al.,2010), or triangulation (Steinberger et al., 2012).The corpus transfer approach consists of trans-ferring a source training corpus into the target lan-guage and building a corpus-based classifier in thetarget language.
Banea et al.
(2008) follow thisapproach, translating an annotated corpus via MT.Balamurali et al.
(2012) use linked Wordnets to2For document and sentence-level classification, the out-come is a set of polarity labels independent on language.782replace words in training and test corpora by their(language-independent) synset identifiers.
Gui etal.
(2014) reduce negative transfer in the processof transfer learning.
Popat et al.
(2013) performCLSA with clusters as features, bridging targetand source language clusters with word alignment.In the test translation approach, test sentencesfrom the target language are translated into thesource language and they are classified using asource language classifier (Bautin et al., 2008).Work on joint classification includes train-ing a classifier with features from multilingualviews (Banea et al., 2010; Xiao and Guo, 2012),co-training (Wan, 2009; Demirtas and Pech-enizkiy, 2013), joint learning (Lu et al., 2011),structural correspondence learning (Wei and Pal,2010; Prettenhofer and Stein, 2010) or mixturemodels (Meng et al., 2012).
Gui et al.
(2013) com-pare several of these approaches.Brooke et al.
(2009) and Balamurali et al.
(2013) conclude that at document level, it ischeaper to annotate resources in the target lan-guage than building CLSA systems.
This maynot be true at aspect level, in which the annota-tion cost is much higher.
In any case, when theskills to build such annotated resources are lack-ing, CLSA may be the only option.
In languagepairs in which no high-quality MT systems areavailable, MT may not be an appropriate trans-fer method (Popat et al., 2013; Balamurali et al.,2012).
However, Balahur and Turchi (2014) con-clude that MT systems can be used to build senti-ment analysis systems that can obtain comparableperformances to the one obtained for English.All this work was performed at sentence or doc-ument level.
Zhou et al.
(2012) and Lin et al.
(2014) work at the aspect level, but they focus oncross-lingual aspect extraction.
Haas and Versley(2015) use CLSA for individual syntactic nodes,however they need to map target-language andsource-language nodes with word alignment.4 Language TransferIn aspect-level SA, there may be several opinion-ated segments in each sentence.
When perform-ing a language transfer, each segment in the targetlanguage has to be mapped to its correspondingsegment in the source language.
This may not bean obvious task at all.
For example, if a standardMT system is used for language translation, thesource opinionated segment may be reordered andsplit in several parts in the target language.
Thenthe different parts have to be mapped to the orig-inal segment with a method such as word align-ment, which may introduce errors and may leavesome parts without a corresponding segment inthe source language.
To avoid these problems, wecould translate only the opinionated segments, in-dependently of each other.
However, the contextof these segments, which may be useful for someapplications, would then be lost.
Furthermore, thetranslation quality would be worse than when thesegments are translated within the whole sentencecontext.To solve these problems, we translate the wholesentences but with reordering constraints ensur-ing that the opinionated segments are preservedduring translation.
That is, the text between therelevant segment boundaries is not reordered normixed with the text outside these boundaries.3Thus the text in the target language segment comesonly from the corresponding source language seg-ment.
We use the Moses statistical MT (SMT)toolkit (Koehn et al., 2007) to perform the trans-lation.
In Moses, these reordering constraints areimplemented with the zone and wall tags, as in-dicated in Figure 3.
Moses also allows mark-upto be directly passed to the translation, via the xtag.
We use this functionality to keep track, via thetags <ou[id][-label]> and </ou[id]>, ofthe segment boundaries (ou stands for Opinion-ated Unit), of the opinionated segment identifier([id]) and, for training and evaluation purposes,of the polarity label ([-label]).
In the exampleof Figure 3, the id is 1 and the label is P.5 CLSA experimentsIn order to compare CLSA settings a and b (of usecase I), we needed data with opinion annotations atthe aspect level, in two different languages and inthe same domain.
We used the OpeNER4opinioncorpus,5and more specifically the opinion expres-sion and polarity label annotations of the hotel re-view component, in Spanish and English.
We splitthe data in training (train) and evaluation (test) setsas indicated in Table 1.The SMT system was trained on freely avail-3However, reordering within the segment text is allowed.4http://www.opener-project.eu/5Described in deliverable D5.42 (page 6) at:http://www.opener-project.eu/project/publications.html.This corpus will be freely available from June 2016 on, anduntil then can be used for research purposes.783Source: On the other hand <zone> <x translation="ou1-P">x</x> <wall/> a big ad-vantage <wall/> <x translation="/ou1">x</x> </zone> of the hostel is its placementTranslation: por otra parte <ou1-P>una gran ventaja</ou1> del hostal es su colocaci?onFigure 3: Source text with reordering constraint mark-up as well as code to pass tags, and its translation.Lang Docs Words Op.
UnitsTrain EN 346 32149 3643ES 359 31511 3905Test EN 49 4256 496ES 50 3733 484Table 1: Number of documents (Docs), words andopinionated units (Op.
Units) in the OpeNER an-notated data for English (EN) and Spanish (ES).able data from the 2013 workshop on Statisti-cal Machine Translation6(WMT 2013).
We alsocrawled monolingual data in the hotel bookingdomain, from booking.com and TripAdvisor.com.From these in-domain data we extracted 100k and50k word corpora, respectively for data selec-tion and language model (LM) interpolation tun-ing.
We selected the data closest to the domain inthe English-Spanish parallel corpora via a cross-entropy-based method (Moore and Lewis, 2010),using the open source XenC tool (Rousseau,2013).
The size of available and selected corporaare indicated in the first 4 rows of Table 2.
The LMwas an interpolation of LMs trained with the targetpart of the parallel corpora and with the rest of theBooking and Trip Advisor data (last 2 rows of Ta-ble 2).
We used Moses Experiment ManagementSystem (Koehn, 2010) with all default options tobuild the SMT system.7Because the common crawl corpus containedEnglish sentences in the Spanish side, we appliedan LM-based filter to select only sentence pairs inwhich the Spanish side was better scored by theSpanish LM than with the English LM, and con-versely for the English side.We conducted supervised sentiment classifica-tion experiments for settings a and b of use caseI (see Section 2).
We trained and evaluated clas-sifiers on the annotated data (Table 1), using asfeatures the tokens (unigrams) within opinion ex-pressions, and SP (Strong Positive), P (Positive),N (Negative) and SN (Strong Negative) as la-6http://www.statmt.org/wmt13/translation-task.html7We kept selected parallel data of the common crawl cor-pus for tuning and test.
We obtained BLEU scores of 42 and45 in the English?Spanish and Spanish?English directions.Available SelectedCorpus EN ES EN ESCommon Crawl 46.7 49.5 6.7 7.0Europarl v7 54.6 57.1 1.7 1.7News Commentary 4.5 5.1 4.5 5.1UN 321.7 368.6 3.4 3.5Booking 1.7 2.6 1.7 2.6Trip Advisor 23.4 4.4 23.4 4.4Table 2: Size of the available and selected corpora(in million words) in English (EN) and Spanish(ES) used to train the SMT system.TESTENTESTES?TRAINENTRAINEN?TRAINES1 mono 1 CL aMT1 CL bFigure 4: Experiments corresponding to group ofrows 1 of Table 3.
?mono?
refers to monolingualand ?CL a?
and ?CL b?
refer to settings a and b ofuse case I (Sec.
2).bels.
We performed the experiments with the wekatoolkit (Hall et al., 2009), using a filter to con-vert strings into word vectors, and two learning al-gorithms: SVMs and bagging with Fast DecisionTree Learner as base algorithm.Figure 4 represents the experiments conductedwith the EN test set.
A monolingual classifier inEnglish is trained with the EN training set, andevaluated with the EN test set (1 mono).
The re-LM Filter No FilConfig Train Test Bag.
SVM SVM1 mono EN EN 77.2 83.4 83.41 CL a EN?EN 70.3 75.4 75.81 CL b ES ES?73.0 75.8 73.62 mono ES ES 76.8 81.1 81.12 CL a ES?ES 66.2 72.5 73.02 CL b EN EN?74.5 77.6 76.8Table 3: Accuracy (in %) achieved by the differentsystems.
LM Filter and No Fil(ter) refer to thepresence or not of the LM filter for the commoncrawl parallel corpus.
?Bag.?
refers to bagging.784sults are reported in the first row of Table 3.
Toevaluate cross-lingual setting a, the ES training setis translated into English (see Section 4), and anEnglish classifier is trained on the translated dataand evaluated on the EN test set (1 CL a).
To eval-uate setting b, the EN test set is translated intoSpanish, and this translated test is used to evalu-ate a classifier trained on the ES training set (1 CLb).
With this very simple classifier, we achieveup to 83.4% accuracy in the monolingual case.With cross-lingual settings, we loose from about4% to 8% accuracy, and with the higher qualitySMT system (LM filter), CL-b setting is slightlybetter than CL-a.The same three experiments were conducted forthe ES test set (last three rows of Table 3).
Weachieved an accuracy of 81.1% in the monolin-gual case.
Here the CL-b setting achieved a clearlybetter accuracy than the CL-a setting (at least 5%more), and only from 2.3% to 3.5% below themonolingual one.
Thus with the higher qualitySMT system, it is always better to translate the testdata (CL-b setting) than the training corpus.Comparing the SVM classification accuracy inthe ?LM Filter?
and ?No Fil?
columns, we can seethe effect of introducing noise in the MT system.We observe that the results were more affected bythe translation of the test (-2.2% and -0.8% accu-racy) than the training set (+0.5% accuracy in bothcases).
This agrees with the intuition than errors inthe test directly affect the results and thus may bemore harmful than in the training set, where theymay hardly affect the results if they represent in-frequent examples.Regarding use case II, setting c implies a trans-lation of the analysis outcome.
We can use ourmethod to translate the relevant opinionated unitswith their predicted label in their test sentencecontext, and extract the relevant information in theoutcome language.
In setting d, the test is trans-lated in the same way as in setting b.6 Conclusions and PerspectivesWe extended the possible CLSA settings to aspect-level specific use cases.
We proposed a method,based on constrained SMT, to transfer opinionatedunits across languages by preserving their bound-aries.
With this method, we built cross-languagesentiment classifiers achieving comparable resultsto monolingual ones (from about 4 to 8% and 2.3to 3.5% loss in accuracy depending on the lan-guage and machine learning algorithm).
We ob-served that improving the MT quality had moreimpact in settings using a translated test than atranslated training corpus.
With the higher MTquality system, we achieved better accuracy bytranslating the test than the training corpus.As future work, we plan to investigate the ex-act effect of the reordering constraints in terms ofpossible translation model phrase pairs and targetlanguage model n-grams which may not be useddepending on the constraint parameters, in orderto find the best configuration.AcknowledgementsThis work has received funding from the Sev-enth Framework Program of the European Com-mission through the Intra-European Fellowship(CrossLingMind-2011-300828) Marie Curie Ac-tions.
We also acknowledge partners of theOpeNER project, in particular Montse Cuadros,for providing us with the aspect-level annotateddata.ReferencesAlexandra Balahur and Marco Turchi.
2014.
Com-parative experiments using supervised learning andmachine translation for multilingual sentiment anal-ysis.
Computer Speech & Language, 28(1):56?75.A.R.
Balamurali, Aditya Joshi, and Pushpak Bhat-tacharyya.
2012.
Cross-lingual sentiment analysisfor Indian languages using linked wordnets.
In Proc.of the International Conference on ComputationalLinguistics (COLING), pages 73?82, Mumbai, In-dia.A.
R. Balamurali, Mitesh M Khapra, and Pushpak Bat-tacharyya.
2013.
Lost in Translation: Viability ofMachine Translation for Cross Language SentimentAnalysis.
In Proc.
of International Conference onIntelligent Text Processing and Computational Lin-guistics (CICLing), pages 38?49, Samos, Greece.Carmen Banea, Rada Mihalcea, and Janyce Wiebe.2008.
A bootstrapping method for building subjec-tivity lexicons for languages with scarce resources.In Proc.
of the International Conference on Linguis-tic Resources and Evaluation (LREC), pages 2764?2767, Marrakech, Morocco, May.Carmen Banea, Rada Mihalcea, and Janyce Wiebe.2010.
Multilingual subjectivity: Are more lan-guages better?
In Proc.
of the International Con-ference on Computational Linguistics (COLING),pages 28?36, Beijing, China.785Carmen Banea, Rada Mihalcea, and Janyce Wiebe.2011.
Multilingual sentiment and subjectivity anal-ysis.
In D. M. Bikel and I. Zitouni, editors, Multilin-gual Natural Language Applications: From Theoryto Practice.
Prentice-Hall.Mikhail Bautin, Lohit Vijayarenu, and Steven Skiena.2008.
International sentiment analysis for news andblogs.
In Proc.
of the International Conference onWeblogs and Social Media, pages 19?26, Seattle,U.S.A.Julian Brooke, Milan Tofiloski, and Maite Taboada.2009.
Cross-Linguistic Sentiment Analysis: FromEnglish to Spanish.
In Proc.
of the InternationalConference on Recent Advances in Natural Lan-guage Processing (RANLP), pages 50?54, Borovets,Bulgaria.Erkin Demirtas and Mykola Pechenizkiy.
2013.
Cross-lingual Polarity Detection with Machine Transla-tion.
In Proc.
of the International Workshop on Is-sues of Sentiment Discovery and Opinion Mining- WISDOM ?13, pages 9:1?9:8, Chicago, Illinois,USA.
ACM Press.Lin Gui, Ruifeng Xu, Jun Xu, Li Yuan, Yuanlin Yao,Jiyun Zhou, Qiaoyun Qiu, Shuwei Wang, Kam-faiWong, and Ricky Cheung.
2013.
A Mixed Modelfor Cross Lingual Opinion Analysis.
In Second CCFConference, Natural Language Processing and Chi-nese Computing, pages 93?104.Lin Gui, Ruifeng Xu, Qin Lu, Jun Xu, Jian Xu, Bin Liu,and Xiaolong Wang.
2014.
Cross-lingual opinionanalysis via negative transfer detection.
In Proc.
ofthe Annual Meeting of the Association for Computa-tional Linguistics, pages 860?865, Baltimore, Mary-land.Michael Haas and Yannick Versley.
2015.
Subsen-tential sentiment on a shoestring: A crosslingualanalysis of compositional classification.
In Proc.
ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 694?704, Denver, Colorado.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: An update.SIGKDD Explorations, 11(1).Ahmed Hassan, Amjad AbuJbara, Rahul Jha, andDragomir Radev.
2011.
Identifying the semanticorientation of foreign words.
In Proc.
of the An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages592?597, Portland, Oregon, USA, June.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proc.
of the 45th Annual Meeting of theAssociation for Computational Linguistics (Demoand Poster Sessions), pages 177?180, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Philipp Koehn.
2010.
An experimental managementsystem.
Prague Bulletin of Mathematical Linguis-tics (PBML), (94):87?96.Zheng Lin, Xiaolong Jin, Xueke Xu, Yuanzhuo Wang,Weiping Wang, and Xueqi Cheng.
2014.
A cross-lingual joint aspect/sentiment model for sentimentanalysis.
In Proc.
of the ACM International Confer-ence on Conference on Information and KnowledgeManagement, CIKM ?14, pages 1089?1098, Shang-hai, China.Bing Liu.
2012.
Sentiment Analysis and Opinion Min-ing.
Synthesis Lectures on Human Language Tech-nologies.
Morgan & Claypool Publishers.Bin Lu, Chenhao Tan, Claire Cardie, and BenjaminK.
Tsou.
2011.
Joint bilingual sentiment classifi-cation with unlabeled parallel corpora.
In Proc.
ofthe Annual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies,pages 320?330, Portland, Oregon, USA.Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou,Ge Xu, and Houfeng Wang.
2012.
Cross-lingualmixture model for sentiment classification.
In Proc.of the Annual Meeting of the Association for Com-putational Linguistics, pages 572?581, Jeju Island,Korea.Rada Mihalcea, Carmen Banea, and Janyce Wiebe.2007.
Learning multilingual subjective language viacross-lingual projections.
In Proc.
of the AnnualMeeting of the Association for Computational Lin-guistics, pages 976?983, Prague, Czech Republic,June.Robert C. Moore and William Lewis.
2010.
Intelligentselection of language model training data.
In Pro-ceedings of the ACL 2010 Conference Short Papers,pages 220?224, Uppsala, Sweden.Veronica Perez-Rosas, Carmen Banea, and Rada Mi-halcea.
2012.
Learning sentiment lexicons in span-ish.
In Proc.
of the International Conference onLinguistic Resources and Evaluation (LREC), pages3077?3081, Istanbul, Turkey, may.Kashyap Popat, Balamurali A.R, Pushpak Bhat-tacharyya, and Gholamreza Haffari.
2013.
Thehaves and the have-nots: Leveraging unlabelled cor-pora for sentiment analysis.
In Proc.
of the AnnualMeeting of the Association for Computational Lin-guistics, pages 412?422, Sofia, Bulgaria.Peter Prettenhofer and Benno Stein.
2010.
Cross-language text classification using structural corre-spondence learning.
In Proc.
of the Annual Meet-ing of the Association for Computational Linguis-tics, pages 1118?1127, Uppsala, Sweden.
Associa-tion for Computational Linguistics.786A Rousseau.
2013.
XenC: An Open-Source Toolfor Data Selection in Natural Language Process-ing.
Prague Bulletin of Mathematical Linguistics(PBML), (100):73?82.Christian Scheible, Florian Laws, Lukas Michelbacher,and Hinrich Sch?utze.
2010.
Sentiment translationthrough multi-edge graphs.
In Proc.
of the Inter-national Conference on Computational Linguistics(COLING), pages 1104?1112, Beijing, China, Au-gust.Josef Steinberger, Mohamed Ebrahim, Maud Ehrmann,Ali Hurriyetoglu, Mijail Kabadjov, Polina Lenkova,Ralf Steinberger, Hristo Tanev, Silvia V?azquez, andVanni Zavarella.
2012.
Creating sentiment dictio-naries via triangulation.
Decision Support Systems,53(4):689 ?
694.Xiaojun Wan.
2009.
Co-training for cross-lingual sen-timent classification.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP, pages 235?243, Suntec, Singapore.Bin Wei and Christopher Pal.
2010.
Cross lingualadaptation: An experiment on sentiment classifica-tions.
In Proc.
of the ACL 2010 Conference ShortPapers, pages 258?262, Uppsala, Sweden.
Proc.
ofthe Annual Meeting of the Association for Compu-tational Linguistics.Min Xiao and Yuhong Guo.
2012.
Multi-view ad-aboost for multilingual subjectivity analysis.
InProc.
of the International Conference on Compu-tational Linguistics (COLING), pages 2851?2866,Mumbai, India.Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao.
2012.Cross-Language Opinion Target Extraction in Re-view Texts.
In IEEE 12th International Conferenceon Data Mining, pages 1200?1205, Brussels, Bel-gium.787
