Fine-Grained Word Sense Disambiguation Based on Parallel Corpora,Word Alignment, Word Clustering and Aligned WordnetsDan TUFI?Institute for ArtificialIntelligence13, ?13 Septembrie?Bucharest, 050711Romaniatufis@racai.roRadu IONInstitute for ArtificialIntelligence13, ?13 Septembrie?Bucharest, 050711Romaniaradu@racai.roNancy IDEDepartment of Computer ScienceVassar CollegePoughkeepsie,NY 12604-0520USAide@cs.vassar.eduAbstractThe paper presents a method for word sensedisambiguation based on parallel corpora.
Themethod exploits recent advances in wordalignment and word clustering based onautomatic extraction of translation equivalentsand being supported by available alignedwordnets for the languages in the corpus.
Thewordnets are aligned to the PrincetonWordnet, according to the principlesestablished by EuroWordNet.
The evaluationof the WSD system, implementing themethod described herein showed veryencouraging results.
The same system used ina validation mode, can be used to check andspot alignment errors in multilinguallyaligned wordnets as BalkaNet andEuroWordNet.1 IntroductionWord Sense Disambiguation (WSD) is well-known as one of the more difficult problems inthe field of natural language processing, as notedin  (Gale et al 1992; Kilgarriff, 1997; Ide andV?ronis, 1998), and others.
The difficulties stemfrom several sources, including the lack of meansto formalize the properties of context thatcharacterize the use of an ambiguous word in agiven sense, lack of a standard (and possiblyexhaustive) sense inventory, and the subjectivityof the human evaluation of such algorithms.
Toaddress the last problem, (Gale et al 1992) arguefor upper and lower bounds of precision whencomparing automatically assigned sense labelswith those assigned by human judges.
The lowerbound should not drop below the baseline usageof the algorithm (in which every word that isdisambiguated is assigned the most frequentsense) whereas the upper bound should not be toorestrictive?
when the word in question is hard todisambiguate even for human judges (a measureof this difficulty is the computation of theagreement rates between human annotators).Identification and formalization of thedetermining contextual parameters for a wordused in a given sense is the focus of WSD workthat treats texts in a monolingual setting?that is,a setting where translations of the texts in otherlanguages either do not exist or are notconsidered.
This focus is based on theassumption that for a given word w and two of itscontexts C1 and C2, if C1 ?
C2 (are perfectlyequivalent), then w is used with the same sense inC1 and C2.
A formalized definition of context fora given sense would then enable a WSD systemto accurately assign sense labels to occurrencesof w in unseen texts.
Attempts to characterizecontext for a given sense of a word haveaddressed a variety of factors:?
Context length: what is the size of the windowof text that should be considered to determinecontext?
Should it consist of only a few words,or include much larger portions of text??
Context content: should all context words beconsidered, or only selected words (e.g., onlywords in a certain part of speech or a certaingrammatical relations to the target word)?
Shouldthey be weighted based on distance from thetarget or treated as a ?bag of words???
Context formalization: how can contextinformation be represented to enable definitionsof an inter-context equivalence function?
Is therea single representation appropriate for all words,or does it vary according to, for example, theword?s part of speech?The use of multi-lingual parallel textsprovides a very different approach to the problemof context identification and characterization.?Context?
now becomes the word(s) by whichthe target word (i.e., the word to bedisambiguated) is translated in one or more otherlanguages.
The assumption here is that differentsenses of a word are likely to be lexicalizeddifferently in different languages; therefore, thetranslation can be used to identify the correctsense of a word.
Effectively, the translationcaptures the context as the translator conceived it.The use of parallel translations for sensedisambiguation brings up a different set of issues,primarily because the assumption that differentsenses are lexicalized differently in differentlanguages is true only to an extent.
For instance,it is well known that many ambiguities arepreserved across languages (e.g.
the Frenchint?r?t and the English interest), especiallylanguages that are relatively closely related.
Thisraises new questions: how many languages, andof which types (e.g., closely related languages,languages from different language families),provide adequate information for this purpose?How do we measure the degree to whichdifferent lexicalizations provide evidence for adistinct sense?We have addressed these questions inexperiments involving sense clustering based ontranslation equivalents extracted from parallelcorpora (Ide, 199; Ide et al, 2002).
Tufi?
and Ion(2003) build on this work and further describe amethod to accomplish a ?neutral?
labelling forthe sense clusters in Romanian and English thatis not bound to any particular sense inventory.Our experiments confirm that the accuracy ofword sense clustering based on translationequivalents is heavily dependent on the numberand diversity of the languages in the parallelcorpus and the language register of the paralleltext.
For example, using six source languagesfrom three language families (Romance, Slavicand Finno-Ugric), sense clustering of Englishwords was approximately 74% accurate; whenfewer languages and/or languages from lessdiverse families are used accuracy dropsdramatically.
This drop is obviously a result ofthe decreased chances that two or more senses ofan ambiguous word in one language will belexicalized differently in another when fewerlanguages, and languages that are more closelyrelated, are considered.To enhance our results, we have explored theuse of additional resources, in particular, thealigned wordnets in BalkaNet (Tufi?
et al2004a).
BalkaNet  is a European project that isdeveloping monolingual wordnets for five Balkanlanguages (Bulgarian, Greek, Romanian Serbian,and Turkish) and improving the Czech wordnetdeveloped in the EuroWordNet project.
Thewordnets are aligned to the Princeton Wordnet(PWN2.0), taken as an interlingual index,following the principles established by theEuroWordNet consortium.
The underlyinghypothesis in this experiment exploits thecommon intuition that reciprocal translations inparallel texts should have the same (or closelyrelated) interlingual meanings (in terms ofBalkaNet, interlingual index (ILI) codes).However, this hypothesis is reasonable if themonolingual wordnets are reliable and correctlylinked to the interlingual index (ILI).
Qualityassurance of the wordnets is a primary concern inthe BalkaNet project, and to this end, theconsortium developed several methods and toolsfor validation, described in various papersauthored by BalkaNet consortium members (seeProceedings of the Global WordNet Conference,Brno, 2004).We previously implemented a language-independent disambiguation program, calledWSDtool, which has been extended to serve as amultilingual wordnet checker and specializededitor for error-correction.
In (Tufi?, et al, 2004)it was demonstrated that the tool detected severalinterlingual alignment errors that had escapedhuman analysis.
In this paper, we describe adisambiguation experiment that exploits the ILIinformation in the corrected wordnets2 Methodology and the algorithmOur methodology consists of the following steps:1. given a bitext TL1L2 in languages L1 and L2 forwhich there are aligned wordnets, extract all pairsof lexical items that are reciprocaltranslations:{<WiL1 WjL2>+}2. for each lexical alignment <WiL1 WjL2>, extractthe ILI codes for the synsets that contain WiL1 andWjL2 respectively to yield two lists of ILI codes,L1ILI(WiL1) and L2ILI(WjL2)3. identify one ILI code common to theintersection L1ILI(WiL1) ?
L2ILI(WjL2) or a pair ofILI codes ILI1?
L1ILI(WiL1)  and ILI2?
L2ILI(WjL2),so that ILI1 and ILI2 are the most similar ILIcodes (defined below) among the candidate pairs(L1ILI(WiL1)?L2ILI(WjL2) [?
= Cartesian product].The accuracy of step 1 is essential for thesuccess of the validation method.
A recent sharedtask evaluation) of different word aligners(www.cs.unt.edu/~rada/wpt, organized on theoccasion of the Conference of the NAACLshowed that step 1 may be solved quite reliably.Our system (Tufi?
et al 2003) produced lexiconsrelevant for wordnets evaluation, with anaggregated F-measure as high as 84.26%.Meanwhile, the word-aligner was furtherimproved so that current performance on thesame data is about 1% better on all scores inword alignment and about 2% better in wordnet-relevant dictionaries.
The word alignmentproblem includes cases of null alignment, wherewords in one part of the bitext are not translatedin the other part; and cases of expressionalignment, where multiple words in one part ofthe bitext are translated as one or more words inthe other part.
Word alignment algorithmstypically do not take into account the part ofspeech (POS) of the words comprising atranslation equivalence pair, since cross-POStranslations are rather frequent.
However, for thealigned wordnet-based word sensedisambiguation we discard both translation pairswhich do not preserve the POS and nullalignments.
Multiword expressions included in awordnet are dealt with by the underlyingtokenizer.
Therefore, we consider only one-to-one, POS-preserving alignments.Once the translation equivalents wereextracted, then, for any translation equivalencepair <WL1 WL2> and two aligned wordnets, thesteps 2 and 3 above should ideally identify oneILI concept lexicalized by WL1 in language L1and by WL2 in language L2.
However, due tovarious reasons, the wordnets alignment mightreveal not the same ILI concept, but two conceptswhich are semantically close enough to licensethe translation equivalence of WL1 and WL2.
Thiscan be easily generalized to more than twolanguages.
Our measure of interlingual conceptssemantic similarity is based on PWN2.0structure.
We compute semantic-similarity scoreby formula:ss(ILI1, ILI2) = 1/1+kwhere k is the number of links from ILI1 to ILI2or from both ILI1 and ILI2 to the nearest commonancestor.
The semantic similarity score is 1 whenthe two concepts are identical, 0.33 for two sisterconcepts, and 0.5 for mother/daughter,whole/part, or concepts related by a single link.Based on empirical studies, we decided to set thesignificance threshold of the semantic similarityscore to 0.33.
Other approaches to similaritymeasures are described in (Budanitsky and Hirst2001).In order to describe the algorithm for WSDbased on aligned wordnets let us assume we havea parallel corpus containing texts in k+1languages (T, L1, L2?Lk), where T is the targetlanguage and L1, L2?Lk are the source languagesand monolingual wordnets for each of the k+1languages interlinked via an ILI-like structure.For each source language and for all occurrencesof a specific word in the target language T, webuild a matrix of translation equivalents as shownin Table 1 (eqij represents the translationequivalent in the ith source language of the jthoccurrence of the word in the target language).Occ #1 Occ #2 ?
Occ #nL1 eq11 eq12 ?
eq1nL2 eq21 eq22 ?
eq2n?
?
?
?
?Lk eqk1 eqk2 ?
eqknTable 1.
The translation equivalents matrix(EQ matrix)If the target word is not translated in language Li,eqij is represented by the null string.The second step transforms the matrix inTable 1 to a VSA (Validation and SenseAssignment) matrix with the same dimensions(Table 2).Occ #1 Occ #2 ?
Occ #nL1 VSA11  VSA12 ?
VSA 1nL2 VSA21 VSA22  VSA22?
?
?
?
?Lk VSAk1 VSAk2 ?
VSAknTable 2.
The VSA matrixHere,  VSAij = LENILI(WEN) ?
LiILI(WjLi),, whereLENILI(WEN) represent the ILI codes of all synsetsin which the target word WEN occurs, andLiILI(WjLi) is the list of ILI-codes for all synsets inwhich the translation equivalent for the jthoccurrence of WEN occurs.If no translation equivalent is found inlanguage Li for the jth occurrence of WEN,VSA(i,j) is undefined; otherwise, it is a setcontaining 0, 1, or more ILI codes.
For undefinedVSAs, the algorithm cannot determine the sensenumber for the corresponding occurrence of thetarget word.
However, it is very unlikely that anentire column in Table 2 is undefined, i.e., thatthere is no translation equivalent for anoccurrence of the target word in any of the sourcelanguages.When VSA(i,j) contains a single ILI code, thetarget occurrence and its translation equivalentare assigned the same sense.When VSA(i,j) is empty?i.e., when none ofthe senses of the target word corresponds to anILI code to which a sense of the translationequivalent was linked--the algorithm selects thepair in LENILI(WEN) ?
LiILI(WjLi) with the highestsimilarity score.
If no pair in LENILI(WEN) ?LiILI(WjLi) has a  the semantic similarity scoreabove the significance threshold, neither theoccurrence of the target word nor its translationequivalent can be semantically disambiguated;but once again, it is extremely rare that there isno translation equivalent for an occurrence of thetarget word in any of the source languages.In case of ties, the pair corresponding to themost frequent sense of the target word in thecurrent bitext pair is selected.
If this heuristic inturn fails, the choice is made in favor of the paircorresponding to the lowest PWN2.0 sensenumber for the target word, since PWN sensesare ordered by frequency.When the VSA cell contains two or more ILI-codes, we have the case of cross-lingualambiguity, i.e., two or more senses are commonto the target word and the correspondingtranslation equivalent in the ith language.2.1 Agglomerative clusteringAs noted before, when VSA(i,j) is undefined, wemay get the information from a VSAcorresponding to the same occurrence of thetarget word in a different language.
However, thisdemands that aligned wordnets are available forall languages in the parallel corpus, and that thequality of the inter-lingual linking is high for alllanguages concerned.
In cases where we cannotfulfill these requirements, we rely on a ?back-off?
method involving sense clustering based ontranslation equivalents, as discussed in (Ide, etal., 2002).
We apply the clustering method afterthe wordnet-based method has been applied, andtherefore each cluster containing anundisambiguated occurrence of the target wordwill also typically contain several occurrencesthat have already been assigned a sense.
We cantherefore assign the most frequent senseassignment in the cluster to previously unlabeledoccurrences within the same cluster.
Thecombined approach has two main advantages:?
it eliminates reliance only on high-quality, k-1aligned wordnets.
Indeed, having k+1 languagesin our corpus, we need only apply the WSDmethod to the aligned wordnets for the targetlanguage (English in our case) and one sourcelanguage, say Li, and alignment lexicons from thetarget language to every other language in thecorpus.
The WSD procedure in the bilingualsetting would ensure the sense assignment formost of the non-null translation equivalence pairsand the clustering algorithm would classify thetarget words which were not translated (or forwhich the word alignment algorithm didn?t find acorrect translation) in Li based on theirequivalents in the other k-1 source languages.?
it can reinforce or modify the senseassignment decided by the tie heuristics in caseof cross-lingual ambiguity.To perform the clustering, we derive a set ofm binary vectors VECT(Lp, TWi) for each sourcelanguage Lp and each target word i occurring mtimes in the corpus.
To compute the vectors, wefirst construct a Dictionary Entry ListDEL(Lp,TWi)={Wj | <TWi, Wj> is a translationequivalence pair}, comprising the ordered list ofall the translation equivalents in the sourcelanguage pL of the target word TWi.
In this partof the experiment, the translation equivalents areautomatically extracted from the parallel corpususing a hypothesis testing algorithm described in(Tufi?
2002).
VECT(Lp,TWik)  specifies which ofthe possible translations of TWi was actuallyused as an equivalent for the kth occurrence ofTWi.
All positions in VECT(Lp,TWik)  are set to0 except the bit at position h, which is 1 if thetranslation equivalent (Lp,TWik)=DELh(Lp,TWi).The vector for each target word occurrence isobtained by concatenating the VECT(Lp,TWik)for all k souce languages  and its length is?=k1pip  |)TW,DEL(L| .We use a Hierarchical Clustering Algorithmbased on Stolcke?s Cluster2.9 to classify similarvectors into sense classes.
Stolcke?s algorithmgenerates a clustering tree, the root of whichcorresponds to a baseline clustering (all theoccurrences are clustered in one sense class) andthe leaves are single element classes,corresponding to each occurrence vector of thetarget word.
An interior cut in the clustering treewill produce a specific number (say X) of sub-trees, the roots of which stand for X classes eachcontaining the vectors of their leaves.
We call aninterior cut a pertinent cut if X is equal to thenumber of senses TWi has been used throughoutthe entire corpus.
One should note that in aclustering tree many pertinent cuts could bepossible.
The pertinent cut which corresponds tothe correct sense clustering of the m occurrencesof TWi is called a perfect cut.
However, if TWihas Y possible senses, it is possible that only asubset of the Y senses will be used in an arbitrarytext.
Therefore, a perfect cut in a clustering treecannot be deterministically computed.
Instead ofderiving the clustering tree and guessing at aperfect cut, we stop the clustering algorithmwhen Z clusters have been created, where Z is thenumber of senses in which the occurrences ofTWi have been used in the text in question.However, the value of Z is specific to each wordand depends on the type and size of the text; itcannot therefore be computed a priori.
In ourprevious work (Tufi?
and Ion, 2003), toapproximate Z we imposed an exit condition forthe clustering algorithm based on distanceheuristics.
In particular, the algorithm stops whenthe minimal distance between the existing classesincreases beyond a given threshold level:?>+?+)1()()1(kdistkdistkdist                                   (1)where dist(k) is the minimal distance betweentwo clusters at the k-th  iteration  step and ?
is  anempirical numerical threshold.
Experimentationrevealed that reasonable results are achieved witha value for ?
is 0.12.
However, although thethreshold is a parameter for the clusteringalgorithm irrespective of the target words, thenumber of classes the clustering algorithmgenerates (Z) is still dependent on the particulartarget word and the corpus in which it appears.By using sense information produced by theILI-similarity approach, the algorithm and its exitcondition have been modified as describedbelow:- the sense label of a cluster is given by themajority sense of its members as assigned by thewordnet-based sense labelling; a clustercontaining only non-disambiguated occurrenceshas an wild-card sense label;- two joinable clusters (that is the clusters withthe minimal distance and the exit condition (1)not satisfied) are joint only when their senselabels is the same or one of them has an wild-card sense label; in this case the wild-card senselabel is turned into the sense label of the sense-assigned cluster.
Otherwise the next distantclusters are tried.- the algorithm stops when no clusters can befurther joined.3 The ExperimentThe parallel corpus we used for our experimentsis based on Orwell?s novel ?Ninety Eighty Four?
(1984) which has been initially developed by theMultext-East consortium.
Besides Orwell?soriginal text, the corpus contained professionaltranslations in six languages (Bulgarian, Czech,Estonian, Hungarian, Romanian and Slovene).The Multext-East corpus (and other languageresources) is maintained by Toma?
Erjavec and anew release of it may be found athttp://nl.ijs.si/ME/V3.
Later, the parallel corpushas been extended with many other new languagetranslations.
The BalkaNet consortium addedthree new translations to the ?1984?
corpus:Greek, Serbian and Turkish.
Each language textis tokenized, tagged and sentence aligned to theEnglish original.
We extracted from the entireparallel corpus only the languages of concern inthe BalkaNet project (English, Bulgarian, Czech,Greek, Romaniann, Serbian and Turkish) andfurther retained only the 1-1 sentence alignmentsbetween English and all the other languages.
Thisway, we built a unique alignment for all thelanguages and, by exploiting the transitivity ofsentence alignment, we are able to makeexperiments with any combination of languages.The BalkaNet version of the ?1984?
corpus isencoded as a sequence of translation units (TU),each containing one sentences per language, sothat they are reciprocal translations.
In order toevaluate both the performance of the WSDtooland to assess the accuracy of the interlinguallinking of the BalkaNet wordnets we selected abag of English target words (nouns and verbs)occurring in the corpus.
The selection consideredonly polysemous words (at least two senses perpart of speech) implemented (and ILI linked) inall BalkaNet wordnets.
There resulted 211 wordswith 1644 occurrences in the English part of theparallel corpus.Three experts independently sense-tagged allthe occurrences of the target words and thedisagreements were negotiated until consensuswas obtained.
The commonly agreed annotationrepresented the Gold Standard (GS) againstwhich the WSD algorithm was evaluated.Additionally, a number of 13 students, enrolled ina Computational Linguistics Master program,were asked to manually sense-tag overlappingsubsets of the same word occurrences.
Theoverlapping ensured that each target wordoccurrence was seen by at least three students.Based on the students?
annotations, using amajority voting, we computed another set ofcomparison data which below is referred to asSMAJ (Students MAJority).Finally, the same targeted words wereautomatically disambiguated by the WSDtoolalgorithm (ALG) which was run both with andwithout the back-off clustering algorithm.
Forthe basic wordnet-based WSD we used thePrinceton Wordnet, the Romanian wordnet andthe English-Romanian translation equivalencedictionary.
For the back-off clustering weextracted a four1 language translation dictionary(EN-RO-CZ-BG) based on which we computedthe initial clustering vectors for all occurrences ofthe target words.1 Although we used only RO, CZ and BGtranslation texts, nothing prevents us from using anyother translations, irrespective of whether theirlanguages belong or not to the BalkaNet consortium.Out of the 211 set of targeted words, with1644 occurrences the system could not make adecision for 38 (18 %) words with 63occurrences (3.83%).
Most of these words werehappax legomena (21) for which neither thewordnet-based step not the clustering back-offcould do anything.
Others, were not translated bythe same part of speech, were wrongly translatedby the human translator or not translated at all(28).
Finally, four occurrences remaineduntagged due to the incompleteness of theRomanian synsets linked to the relevant concepts(that is the four translation equivalents had theirrelevant sense missing from the Romanianwordnet).
Applying the simple heuristics (SH)that says that any unlabelled target occurrencereceives its most frequent sense, 42 out of 63 ofthem got a correct sense-tag.
The table belowsummarizes the results.WSD annotation Precision Recall FAWN  74.88% 72.01% 73.41%AWN + C 75.26% 72.38% 73.79%AWN + C + SH 74.93% 74.93% 74.93%SMAJ 72.99% 72.99% 72.99%Table 4.
WSD precision recall and F-measure forthe algorithm based on aligned wordnets (AWN),for AWN with clustering (AWN+C) and forAWN+C and the simple heuristics(AWN+C+SH) and for the students?
majorityvoting (SMAJ)It is interesting to note that in this experimentthe students?
majority annotation is less accuratethan the one achieved by the automatic WSDannotation in all three variants.
This is a veryencouraging result since it shows that the tedioushand-made WSD in building word-sensedisambiguated corpora for supervised trainingcan be avoided.4 ConclusionConsidering the fine granularity of the PWN2.0sense inventory, our disambiguation results usingparallel resources are superior to the state of theart in monolingual WSD (with the same senseinventory).
This is not surprising since theparallel texts contain implicit knowledge aboutthe sense of an ambiguous word, which has beenprovided by human translators.
The drawback ofour approach is that it relies on the existence ofparallel data, which in the vast majority of casesis not available.
On the other hand, supervisedmonolingual WSD relies on the existence of largesamples of training data, and our method can beapplied to produce such data to bootstrapmonolingual applications.
Given that parallelresources are becoming increasingly available, inparticular on the World Wide Web (see forinstance http://www.balkantimes.com where thesame news is published in 10 languages), andaligned wordnets are being produced for moreand more languages, it should be possible toapply our and similar methods to large amountsof parallel data in the not-too-distant future.One of the greatest advantages of ourapproach is that it can be used to automaticallysense-tag corpora in several languages at once.That is, if we have a parallel corpus in multiplelanguages (such as the Orwell corpus),disambiguation performed on any one of thempropagates to the rest via the ILI linkage.
Also,given that the vast majority of words in any givenlanguage are monosemous (e.g., approximately82% of the words in PWN have only one sense),the use of parallel corpora in multiple languagesfor WSD offers the potential to significantlyimprove results and provide substantial sense-annotated corpora for training in a range oflanguages.AcknowledgementsThe work reported here was carried within theEuropean project BalkaNet, no.
IST-2000 29388and support from the Romanian Ministry ofEducation and Research.ReferencesAlex.
Budanitsky and Graeme Hirst 2001.Semantic distance in WordNet: Anexperimental, application-oriented evaluationof five measures.
Proceedings of the Workshopon WordNet and Other Lexical Resources,Second meeting of the NAACL, Pittsburgh,June.William Gale, Ken Church and Dan Yarowsky1992.
Estimating upper and lower bounds onthe performance of wordsense disambiguationprograms.
Proceedings of the 30th AnnualMeeting of ACL, 249-256.Adam Kilgarriff 1997.
I don't believe in wordsenses.
In Computers and the Humanities, 31(2): 91-113.Nancy Ide and Jean V?ronis 1998.
Word SenseDisambiguation: The State of the Art.Computational Linguistics,24(1): 1-40.Nancy Ide, N. 1999.
Parallel translations as sensediscriminators.
SIGLEX99: StandardizingLexical Resources, ACL99 Workshop, CollegePark, Maryland, 52-61.Nancy Ide, Toma?
Erjavec and Dan Tufi?
2002.Sense Discrimination with Parallel Corpora.
InProceedings of the SIGLEX Workshop on WordSense Disambiguation: Recent Successes andFuture Directions, 56-60, Philadelphia.Andreas Stolcke 1996. ftp.icsi.berkeley.edu/pub/ai/stolcke/software/cluster-2.9.tar.Z/Dan Tufi?.
2002.
A cheap and fast way to builduseful translation lexicons.
In Proceedings ofthe 19th International Conference onComputational Linguistics, 1030-1036, Taipei.Dan Tufi?
and Radu Ion.
2003.
Word senseclustering based on translation equivalence inparallel texts; a case study in Romanian.
InProceedings of the International Conference onSpeech and Dialog ?
SPED, 13-26, Bucharest.Dan Tufi?,  Ana-Maria Barbu and Radu Ion2003.
A word-alignment system with limitedlanguage resources.
In Proceedings of theNAACL 2003 Workshop on Building and UsingParallel Texts; Romanian-English SharedTask, 36-39, Edmonton.Dan Tufi?, Radu Ion and Nancy Ide 2004.
Wordsense disambiguation as a wordnets validationmethod in Balkanet.
In Proceedings of theLREC?2004, 741-744, LisbonDan Tufi?, Dan Cristea and Sofia Stamou 2004a.BalkaNet: Aims, Methods, Results andPerspectives.
A General Overview.
In D.
Tufi?
(ed): Special Issue on BalkaNet.
RomanianJournal on Science and Technology ofInformation, 7(3-4):9-44
