Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 62?71,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsAssigning Deep Lexical TypesUsing Structured Classifier Features for Grammatical DependenciesJoa?o SilvaUniversity of LisbonDept.
Informatics, Faculty of SciencesCampo Grande, Lisboa, Portugaljsilva@di.fc.ul.ptAnto?nio BrancoUniversity of LisbonDept.
Informatics, Faculty of SciencesCampo Grande, Lisboa, Portugalantonio.branco@di.fc.ul.ptAbstractDeep linguistic grammars are able to pro-vide rich and highly complex grammaticalrepresentations of sentences, capturing, forinstance, long-distance dependencies and re-turning a semantic representation.
Thesegrammars lack robustness in the sense thatthey do not gracefully handle words miss-ing from their lexicon.
Several approacheshave been explored to handle this problem,many of which consist in pre-annotating theinput to the grammar with shallow processingmachine-learning tools.
Most of these tools,however, use features based on a fixed win-dow of context, such as n-grams.
We investi-gate whether the use of features that encodediscrete structures, namely grammatical de-pendencies, can improve the performance ofa machine learning classifier that assigns deeplexical types.
In this paper we report on thedesign and evaluation of this classifier.1 IntroductionParsing is one of the fundamental tasks in Nat-ural Language Processing and a critical step inmany applications.
Many of the most com-monly used parsers rely on probabilistic approaches.These parsers are obtained through data-drivenapproaches, by inferring a probabilistic languagemodel over a dataset of annotated sentences.Though these parsers always produce some analy-sis of their input sentences, they do not go into deeplinguistic analysis.Deep grammars, also referred to as precisiongrammars, seek to make explicit information abouthighly detailed linguistic phenomena and producecomplex grammatical representations for their in-put sentences.
For instance, they are able to cap-ture long-distance dependencies and produce the se-mantic representation of a sentence.
Although thereis a great variety of parsing methods (see (Mitkov,2004) for an overview), all CKY-based algorithmsrequire a lexical look-up initialization step that, foreach word in the input, returns all its possible cate-gories.From this it follows that if any of the words ina sentence is not present in the lexicon?an out-of-vocabulary (OOV) word?a full parse of thatsentence is impossible to obtain.
Given that nov-elty is one of the defining characteristics of natu-ral languages, unknown words will eventually oc-cur.
Hence, being able to handle OOV words is ofparamount importance if one wishes to use a gram-mar to analyze unrestricted texts.Another important issue is that of lexical ambigu-ity.
That is, words that may bear more than one lexi-cal category.
The combinatorial explosion of lexicaland syntactic ambiguity may hinder parsing due toincreased requirements in terms of parsing time andmemory usage.
Thus, even if there were no OOVwords in the input, being able to assign syntactic cat-egories to words prior to parsing may be desirablefor efficiency reasons.For the shallower parsing approaches, such asplain constituency parsing, it suffices to determinethe part-of-speech of words, so pre-processing theinput with a POS tagger is a common and effectiveway to tackle either of these problems.
However, thelinguistic information contained in the lexicon of a62deep grammar is much more fine-grained, includ-ing, in particular, the subcategorization frame (SCF)of the word, which further constraints what can betaken as a well-formed sentence by imposing sev-eral restrictions on co-occurring expressions.Thus, what for a plain POS tagger corresponds toa single category is often expanded into hundreds ofdifferent distinctions, and hence tags, when at thelevel of detail required by a deep grammar.
For in-stance, the particular grammar we will be using forthe study reported in this paper?a grammar follow-ing the HPSG framework?has in its current ver-sion a lexicon with roughly 160 types for verbs andnearly 200 types for common nouns.While the deep grammar may proceed with theanalysis knowing only the base POS category of aword, it does so at the cost of vastly increased am-biguity1 which may even allow the grammar to ac-cept ungrammatical sentences as valid.
This has leadto research that specifically targets annotating wordswith a tagset suitable for deep grammars.Current approaches tend to use shallow featureswith limited context (e.g.
n-grams).
However, giventhat the SCF is one of the most relevant pieces ofinformation that is associated with a word in thelexicon of a deep grammar, one would expect thatfeatures describing the inter-word dependencies ina sentence would be highly discriminative and helpto accurately assign lexical types.
Accordingly,in this paper we investigate the use of structuredfeatures that encode grammatical dependencies ina machine-learning classifier and how it compareswith state-of-the-art approaches.Our study targets Portuguese, a Romance lan-guage with a rich morphology, in particular in whatconcerns verb inflection (see for instance, (Mateus etal., 2003) for a detailed account of Portuguese gram-mar and (Branco et al, 2008) for an assessment ofthe issues raised by verbal ambiguity).Paper outline: Section 2 provides an overview ofrelated work, with a focus on supertagging, and in-troduces tree kernels as a way of handling structuredclassifier features.
Section 3 introduces the particu-lar deep grammar that is used in this work and how itsupports the creation of the corpus that provides the1For instance, a common noun POS tag could be taken asbeing any of the nearly 200 common noun types existing in thelexicon of the grammar we use in this paper.data for training and evaluation of the classifier.
Theclassifier itself, and the features it uses, are describedin Section 4.
Section 5 covers empirical evaluationand comparison with other approaches.
Finally, Sec-tion 6 concludes with some final remarks.2 Background and Related WorkThe construction of a hand-crafted lexicon for a deepgrammar is a time-consuming task requiring trainedlinguists.
More importantly, such lexica are invari-ably incomplete since they often do not cover spe-cialized domains and are slow to incorporate newwords.Accordingly, much research in this area has beenfocused on automatic lexical acquisition (Brent,1991; Briscoe and Carroll, 1997; Baldwin, 2005).That is, approaches that try to discover all the lex-ical types a given unknown word may occur with,thus effectively creating a new lexical entry.
How-ever, at run-time, it is still up to the grammar usingthe newly acquired lexical entry to choose which ofthose lexical types is the correct one for each par-ticular occurrence of that word; and, ultimately, onecan only acquire the lexicon entries for those wordsthat are present in the corpus.
Thus, any system thatis constantly exposed to new text?e.g.
parsing textfrom the Web?will eventually come across someunknown word that has not yet been acquired.
More-over, such words must be dealt with on-the-fly, sinceit is unlikely that the system can afford to wait untilit has accumulated enough occurrences of the un-known word to be able to apply offline lexicon ac-quisition methods.In the work reported in the present paper we usea different approach, closer to what is known as su-pertagging, where we assign on-the-fly a single lex-ical type to a word.2.1 SupertaggingPOS tagging is a task that relies only on local infor-mation (e.g.
the word and a small window of con-text) to achieve a form of syntactic disambiguation.As such, POS tags are commonly assigned priorto parsing as a way of reducing parsing ambiguityby restricting words to a certain syntactic category.Less ambiguity leads to a greatly reduced searchspace and, as a consequence, faster parsing.63Supertagging, first introduced by Bangalore andJoshi (1994), can be seen as a natural extension ofthis idea to a richer tagset, in particular to one thatincludes information on subcategorization frames.In (Bangalore and Joshi, 1994) supertagging wasapplied to the Lexicalized Tree Adjoining Grammar(LTAG) formalism.
As the name indicates, this is alexicalized grammar, like HPSG, but in LTAG eachlexical item is associated with one or more trees,the elementary structures, which localize informa-tion on dependencies, even long-range ones, by re-quiring that all and only the dependents be presentin the structure.The supertagger in (Bangalore and Joshi, 1994)assigns an elementary structure to each word us-ing a simple trigram model.
The data for trainingwas obtained by taking the sentences of length un-der 15 words in the Wall Street Journal together withsome other minor corpora, and parsing them withXTAG, a wide-coverage grammar for English basedon LTAG.
In addition, and due to data-sparseness,POS tags were used in training instead of words.Evaluation was performed over 100 held-out sen-tences from the Wall Street Journal.
For a tagset of365 elementary trees, this supertagger achieved 68%accuracy, which is far too low to be useful for pars-ing.In a later experiment, the authors improvedthe supertagger by smoothing model parametersand adding additional training data (Bangalore andJoshi, 1999).
The larger dataset was obtained byextending the corpus from the previous experimentwith Penn Treebank parses that were automaticallyconverted to LTAG.
The conversion process reliedon several heuristics, and though it is not perfect,the authors found that the issues concerning conver-sion were far outweighed by the benefit of increasedtraining data.The improved supertagger increased accuracy to92% (Bangalore and Joshi, 1999).
The supertaggercan also assign the n-best tags, which increases thechances of it assigning the correct supertag at thecost of leaving more unresolved ambiguity.
With 3-best tagging, it achieved 97% accuracy.A supertagger was also used by Clark and Curran(2007), in their case for a Combinatory CategorialGrammar (CCG).
This formalism uses a set of log-ical combinators to manipulate linguistic construc-tion tough, for our purposes here, it matters onlythat lexical items receive complex tags that describethe constituents they require to create a well-formedconstruction.The set of 409 lexical categories to be assignedwas selected by taking those categories that occur atleast 10 times in sections 02?21 of a CCG automaticannotation of Penn Treebank (CCGBank).Evaluation was performed over section 00 ofCCGBank, and achieved 92% per word accuracy.As with the LTAG supertagger, assigning morethan one tag can greatly increase accuracy.
How-ever, instead of a fixed n-best number of tags?which might be to low, or too high, depending onthe case at hand?the CCG supertagger assigns alltags with a likelihood within a factor ?
of the besttag.
A value for ?
as small as 0.1, which results inan average of 1.4 tags per word, is enough to boostaccuracy up to 97%.Supertagging for HPSG: There has been somework on using supertagging together with the HPSGframework.
As with other works on supertag-ging, it is mostly concerned with restricting theparser search space in order to increase parsing ef-ficiency, and not specifically with the handling ofOOV words.Prins and van Noord (2003) present an HMM-based supertagger for the Dutch Alpino grammar.An interesting feature of their approach is that thesupertagger is trained over the output of the parseritself, thus avoiding the need for a hand-annotateddataset.The supertagger was trained over 2 million sen-tences of newspaper text parsed by Alpino.
A goldstandard was created by having Alpino choose thebest parse for a set of 600 sentences.
The supertag-ger, when assigning a single tag (from a tagset with2,392 tags), achieves a token accuracy close to 95%.It is not clear to what extent these results can beaffected by some sort of bias in the disambiguationmodule of Alpino, given that both the sequence oflexical types in the training dataset and in the goldstandard are taken from the best parse produced byAlpino.Matsuzaki et al (2007) use a supertagger withthe Enju grammar for English.
The novelty in theirwork comes from the use of a context-free gram-mar (CFG) to filter the tag sequences produced by64the supertagger before running the HPSG parser.
Inthis approach, a CFG approximation of the HPSGis created.
The key property of this approxima-tion is that the language it recognizes is a supersetof the parsable supertag sequences.
Hence, if theCFG is unable to parse a sequence, it can be safelydiscarded, thus further reducing the amount of se-quences the HPSG parser has to deal with.The provided evaluation is mostly concerned withshowing the improvement in parsing speed.
Nev-ertheless, the quality of the supertagging processcan be inferred from the accuracy of the parse re-sults, which achieved a labeled precision and recallfor predicate-argument relations of 90% and 86%,respectively, over 2,300 sentences with up to 100words in section 23 of the Penn Treebank.Dridan (2009) tests two supertaggers, one inducedusing the TnT tagger (Brants, 2000) and another us-ing the C&C supertagger (Clark and Curran, 2007),over different datasets.
For simplicity, we will onlyrefer to the results of TnT over a dataset of 814 sen-tences of tourism data.The author experiments with various tag granu-larities in order to find a balance between tag ex-pressiveness and tag predictability.
For instance, as-signing only POS?a tagset with only 13 tags?isthe easiest task, with 97% accuracy, while a highlygranular supertag formed by the lexical type con-catenated with any selectional restriction present inthe lexical entry increases the number of possibletags to 803, with accuracy dropping to 91%.2.2 Support-Vector Machines and Tree KernelsSupport-vector machines (SVM) are a well knownsupervised machine-learning algorithm for linearbinary classification.
They are part of the fam-ily of kernel-based methods where a general pur-pose learning algorithm is coupled with a problem-specific kernel function (Cristianini and Shawe-Taylor, 2000).For the work presented in this paper we wishto apply the learning algorithm over discrete tree-like structures that encode grammatical dependen-cies (see Figure 1 for an example).
A suitable ker-nel for such a task is the tree kernel introduced byCollins and Duffy (2002), which uses a represen-tation that implicitly tracks all subtrees seen in thetraining data.This representation starts by implicitly enumerat-ing all subtrees that are found in the training data.
Agiven tree, T , is then represented by a (huge) vectorwhere the n-th position counts the number of occur-rences of the n-th subtree in T .Under this representation, the inner product oftwo trees gives a measure of their similarity.
How-ever, explicitly calculating such an operation is pro-hibitively expensive due to the high dimensions ofthe feature space.
Fortunately, the inner product canbe replaced by a rather simple kernel function thatsums over the subtrees that are common to both trees(see (Collins and Duffy, 2002) for a proof).3 Grammar and Base DatasetThe deep linguistic grammar used in this studyis LXGram, a hand-built HPSG grammar for Por-tuguese (Branco and Costa, 2008; Branco and Costa,2010).We used this grammar to support the annota-tion of a corpus.
That is, the grammar is usedto provide the set of possible analyses for a sen-tence (the parse forest).
Human annotators thenperform manual disambiguation by picking the cor-rect analysis from among all those that form theparse forest.2 This grammar-supported approach tocorpus annotation ensures that the various linguis-tic annotation layers?morphological, syntactic andsemantic?are consistent.The corpus that was used is composed mostly by asubset of the sentences in CETEMPu?blico, a corpusof plain text excerpts from the Pu?blico newspaper.After running LXGram and manually disam-biguating the parse forests, we were left with adataset consisting of 5,422 sentences annotated withall the linguistic information provided by LXGram.4 Classifier and Feature ExtractionFor training and classification we use SVM-light-TK(Moschitti, 2006), an extension to the widely-usedSVM-light (Joachims, 1999) software for SVMs thatadds a function implementing the tree kernel intro-duced in Section 2.2.
With SVM-light-TK one can2In our setup, two annotators work in a double-blindscheme, where those cases where they disagree are adjudicatedby a third annotator.
Inter-annotator agreement is 0.86.65directly provide one or more tree structures as fea-tures (using the standard parenthesis representationof trees) together with the numeric feature vectorsthat are already accepted by SVM-light.Given that the task at stake is a multi-class clas-sification problem but an SVM is a binary classi-fier, the problem must first be binarized (Galar etal., 2011).
For this work we have chosen a one-vs-one binarization scheme, where multiple classi-fiers are created, each responsible for discriminat-ing between a pair of classes.
This divides a prob-lem with n classes into n(n ?
1)/2 separate binaryproblems (i.e.
one classifier for each possible classpairing).
Each classifier then performs a binary de-cision, voting for one of the two classes it is taskedwith discriminating, and the class with the overalllargest number of votes is chosen.The dataset, having been produced with the helpof a deep grammar, contains a great deal of linguisticinformation.
The first step is thus to extract fromeach sentence the relevant features in a format thatcan be used by SVM-light-TK.Since we are aiming at discriminating betweendeep lexical types, which, among other information,encode the SCF of a word, the dependency structureassociated with a word is expected to be a piece ofhighly relevant information.
We start by extractingthe dependency representation of a sentence fromthe output of LXGram.3 The dependency represen-tation that is obtained through this process consistsof a list of tuples, each relating a pair of words in thesentence through a grammatical relation.The example in Figure 1 shows the dependencyrepresentation of the sentence ?a o segundo dia deviagem encontra?mos os primeiros golfinhos?
(Eng.
:by the second day of travel we found the first dol-phins).4 Note that each word is also annotated withits lexical type, POS tag and lemma, though this isnot shown in the example for the sake of readability.For a one-vs-one classifier tasked with discrim-inating between types A and B we are concernedwith finding instances of type A to be taken as posi-tive examples and instances of type B to be taken as3The details of this process are outside the scope of the cur-rent paper and will be reported elsewhere.4Relations in the example: ADV (adverb), C (complement),DO (direct object), PRED (predicate), SP (specifier) and TMP(temporal modifier).negative examples.Take, for instance, the word ?encontra?mos?
fromthe example in Figure 1.
Its lexical type in this par-ticular occurrence is verb-dir trans-lex, the type as-signed to transitive verbs by LXGram.
A one-vs-oneclassifier tasked with recognizing this type (againstsome other type) will take this instance as a positiveexample.However, the full dependency representation ofthe sentence has too many irrelevant features forlearning how to classify this word.
Instead, we fo-cus more closely on the information that is relevantto determining the SCF of the word by looking onlyat its immediate neighbors in the dependency graph:its dependents and the word it depends on.This information is encoded in two trees, shownin Figure 2, which are the actual features given toSVM-light-TK.One tree, labeled with H as root, is used to repre-sent the word and its dependents.
The target word ismarked by being under an asterisk ?category?
whilethe dependents fall under a ?category?
correspond-ing to the relation between the target word and thedependent.
The words appears as the leafs of thetree, with their POS tags as the pre-terminal nodes.5The second feature tree, labeled with D as root,encodes the target word?again marked with anasterisk?and the word it is dependent on.
In theexample shown in Figure 2, since the target word isthe main verb of the sentence, the feature tree has noother nodes apart from that of the target word.5 EvaluationThe following evaluation results were obtained fol-lowing a standard 10-fold cross-validation approach,where the folds were taken from a random shuffle ofthe sentences in the corpus.We compare the performance of our tree kernel(TK) approach with two other automatic annotators,TnT (Brants, 2000) and SVMTool (Gime?nez andMa`rquez, 2004).TnT is a statistical POS tagger, well known forits efficiency?in terms of training and taggingspeed?and for achieving state-of-the-art re-sults despite having a quite simple underlying5POS tags in the example: V (verb), PREP (preposition) andCN (common noun).66C(de, viagem) SP(dia, o) C(a, dia)ADV(dia, de) PRD(golfinhos, primeiros) TMP(encontra?mos, a)PRD(dia, segundo) SP(golfinhos, os) DO(encontra?mos, golfinhos)Figure 1: Dependency representationHTMPPREPabyDOCNgolfinhosdolphins*Vencontra?moswe-foundD*Vencontra?moswe-foundFigure 2: Features for SVM-light-TKmodel.
It is based on a second-order hiddenMarkov model extended with linear smooth-ing of parameters to address data-sparseness is-sues and suffix analysis for handling unknownwords.
TnT was used as a supertagger in (Dri-dan, 2009), where it achieved the best resultsfor this task, and is thus a good representativefor this approach to supertagging.
We run itout-of-the-box using the default settings.SVMTool is another statistical sequential taggerwhich, as the name indicates, is based onsupport-vector machines.
It is extremely flexi-ble in allowing to define which features shouldbe used in the model (e.g.
size of word win-dow, number of POS bigrams, etc.)
and the tag-ging strategy (left to right, bidirectional, num-ber of passes, etc).
In fact, due to this flexibil-ity, it is described as being a tagger generator.It beat TnT in a POS tagging task (Gime?nezand Ma`rquez, 2004), so we use it in the currentpaper to evaluate whether that lead is kept ina supertagging task.
We used the simplest set-tings, ?M0 LR?, which uses Model 0 in a leftto right tagging direction.66See (Gime?nez and Ma`rquez, 2006) for an explanation ofthese settings.The type distribution in the dataset is highlyskewed.
For instance, from the number of com-mon noun types that occur in this corpus, the twomost frequent ones are enough to account for 57%of all the common noun tokens.
Such skewed cat-egory distributions are usually a problematic issuefor machine-learning approaches since the numberof instances of the more rare categories is too smallto properly estimate the parameters of the model.For many types there are not enough instances inthe dataset to train a classifier.
Hence, the evalua-tion that follows is done only for the most frequenttypes.
For instance, top-10 means picking the 10most frequent types in the corpus, training one-vs-one classifiers for those types, and evaluating onlyover tokens with one of those types.
In addition, weshow only the evaluation results of verb types, forwhich SCF information is more varied and relevant.Table 1 show the accuracy results for each toolover the top-10, top-20 and top-30 most frequentverb types.Comparing both sequential supertaggers, onefinds that SVMTool is consistently better than TnT,which is in accordance with the results for POS tag-ging reported in (Gime?nez and Ma`rquez, 2004).Our TK approach beats both supertaggers when67TnT SVMTool TKtop-10 92.98% 94.22% 94.71%top-20 91.53% 92.39% 90.21%top-30 91.42% 92.38% 88.70%Table 1: Accuracy over frequent verb typeslooking at the top-10 verb types, but falls behind assoon as the number of types under consideration in-creases.
This seems to point towards data-sparsenessissues, an hypothesis we test by automatically ex-tending the dataset, as discussed next.5.1 Experiments with an Extended DatasetThe extended datasets were created by taking ad-ditional sentences from the Pu?blico newspaper, aswell as sentences from the Portuguese Wikipediaand from the Folha de Sa?o Paulo newspaper, pre-processing them with a POS tagger, and runningthem through LXGram.Such an approach is only made possible becauseLXGram, like many other modern HPSG gram-mars, includes a stochastic disambiguation modulethat automatically chooses the most likely analysisamong all those returned in the parse forest, insteadof requiring a manual choice by a human annota-tor (Branco and Costa, 2010).
The authors do notprovide a complete evaluation of this disambigua-tion module.
Instead, they perform a manual evalu-ation of a sample of 50 sentences that indicates thatthis module picks the correct reading in 40% of thecases.If this ratio is kept, 60% of the sentences in the ex-tended datasets will have an analysis that is, in someway, the wrong analysis, though it is not clear howthis translates into errors in the lexical types that endup being assigned to the tokens.
For instance, whenfaced with the rather common case of PP-attachmentambiguity, the disambiguation module may choosethe wrong attachment, which will count as being awrong analysis though most lexical types assignedto the words in the sentence may be correct.To evaluate this, we tested the disambiguationmodule over the base dataset, where we know whatthe correct parses are, and found that the grammarpicks the correct parse in 44% of the cases.
If wejust look at whether the lexical types are correct, thedataset sentences tokens unique oovbase 5,422 51,483 8,815 10.0%+ Pu?blico 10,727 139,330 18,899 7.6%+ Wiki 15,108 205,585 24,063 6.6%+ Folha 21,217 288,875 30,204 6.0%Table 2: Cumulative size of datasetsgrammar picks a sentence with fully correct types in68% of the cases.LXGram displayed a coverage of roughly 30%,and allowed us to build progressively larger datasetsas more data was added.
The cumulative sizes of theresulting datasets are shown in Table 2.
The Tablealso shows the ratio of OOV words, which was de-termined by taking the average of the ratio for eachof the 10 folds (i.e.
words that occur in a fold but notin any of the other 9 folds).We can now evaluate the tools over the four pro-gressively larger datasets and plot their learningcurves.
In the following Figures, the errors bars rep-resent a 95% confidence interval.All learning curves in the following Figures tell asomewhat similar story.The lead that SVMTool has over TnT when look-ing only at the base corpus is kept in the extendedcorpora.
Both sequential supertaggers only start tobenefit from the increased dataset at the final stage,when sentences from Folha de Sa?o Paulo are added.Before that stage the added data seems to be slightlydetrimental to them, possibly due to them being sen-sitive to noise in the automatically generated data.The learning curves give credence to the hypoth-esis put forward earlier that our TK approach wasbeing adversely affected by data-sparseness issueswhen classifying a greater number of verb types, andthat it has much to gain by an increase in the amountof training data.For the top-10 verb types, for which there isenough data in the base dataset, TK starts aheadfrom the outset and significantly increases its mar-gin over the two supertaggers.For the top-20 and top-30 verb types, TK startsbehind but its accuracy raises quickly as more dataare added, ending slightly ahead of SVMTool whenrunning over the largest dataset.685000 10000 15000 200000.900.920.940.96Over top?10 verb typesDataset sizeAccuracyll lll TnTSVMToolSVM?TKFigure 3: Learning curves (over top-10 verb types)5000 10000 15000 200000.900.920.940.96Over top?20 verb typesDataset sizeAccuracyl llll TnTSVMToolSVM?TKFigure 4: Learning curves (over top-20 verb types)5000 10000 15000 200000.900.920.940.96Over top?30 verb typesDataset sizeAccuracyl llll TnTSVMToolSVM?TKFigure 5: Learning curves (over top-30 verb types)dataset accuracybase 87.24%+ Pu?blico 82.67%+ Wiki 82.30%+ Folha 83.92%Table 3: MaltParser labeled accuracy5.2 Running over Predicted DependenciesIn the previous section, we were concerned withevaluating the classifier itself.
Accordingly, the fea-tures used by the classifier were the gold dependen-cies in the corpus.
However, on a running system,the features used by the classifier will be automati-cally generated by a dependency parser.
To evaluatethis setup, we used MaltParser (Nivre et al, 2007).Like the other tools, the parser was run out-of-the-box.
The 10-fold average labeled accuracy scoresfor each dataset shown in Table 3 can thus be seenas a lower bound on the achievable accuracy.
De-spite this, the performance over the base dataset isextremely good, on par with the best scores achievedfor other languages (cf.
(Nivre et al, 2007)).
How-ever, performance drops sharply when automaticallyannotated data is used, only beginning to pick upagain when running over the largest dataset.As expected, the noisy features that result fromthe automatic process have a detrimental effect onthe accuracy of the classifier.
For the same set ofexperiments reported previously, the accuracy of theSVM-TK classifier when running over predicted de-pendencies tends to trail 2.0?2.5% points behindthat of the classifier that uses gold dependencies, asshown in Table 4.6 Concluding RemarksIn this paper we reported on an novel approach to as-signing deep lexical types.
It uses an SVM classifierwith a tree kernel that allows it to seamlessly workwith features encoding discrete structures represent-ing the grammatical dependencies between words.Evaluation over the top-10 most frequent verbtypes showed that the grammatical dependencies ofa word, which can be seen as information on its SCF,are very helpful in allowing the classifier to accu-rately assign lexical types.
Our classifier clearly im-69top-10 top-20 top-30dataset gold pred.
gold pred.
gold pred.base 94.71% 93.14% 90.21% 88.66% 88.70% 87.01%+ Pu?blico 96.02% 93.83% 92.34% 90.35% 91.32% 88.97%+ Wiki 96.48% 93.95% 93.54% 91.29% 92.80% 90.21%+ Folha 96.98% 94.55% 94.46% 92.26% 93.93% 91.50%Table 4: SVM-TK classifier accuracy over gold and predicted featuresproves over TnT, which had displayed the best su-pertagging performance in other studies.When running the classifier for a greater numberof verb types, data-sparseness issues led to a dropin performance, which motivated additional experi-ments where the dataset was extended with automat-ically annotated data.
This allowed us to plot learn-ing curves that show that our approach can maintaina lead in accuracy when given more training data.Running the classifier over predicted featuresshows an expected drop in performance.
However,we anticipate that using larger corpora will alsobe effective in raising these scores since additionaltraining data not only improve the classifier, but alsothe underlying parser that provides the dependenciesthat are used as features.ReferencesTimothy Baldwin.
2005.
Bootstrapping deep lexical re-sources: Resources for courses.
In Timothy Baldwin,Anna Korhonen, and Aline Villavicencio, editors, Pro-ceedings of the ACL-SIGLEX Workshop on Deep Lex-ical Acquisition, pages 67?76.Srinivas Bangalore and Aravind Joshi.
1994.
Disam-biguation of super parts of speech (or supertags): Al-most parsing.
In Proceedings of the 15th Conferenceon Computational Linguistics (COLING), pages 154?160.Srinivas Bangalore and Aravind Joshi.
1999.
Supertag-ging: An approach to almost parsing.
ComputationalLinguistics, 25(2):237?265.Anto?nio Branco and Francisco Costa.
2008.
A computa-tional grammar for deep linguistic processing of Por-tuguese: LX-Gram, version A.4.1.
Technical ReportDI-FCUL-TR-08-17, University of Lisbon.Anto?nio Branco and Francisco Costa.
2010.
A deep lin-guistic processing grammar for Portuguese.
In Pro-ceedings of the 9th Encontro para o ProcessamentoComputacional da L?
?ngua Portuguesa Escrita e Fal-ada (PROPOR), LNAI, pages 86?89.
Springer.Anto?nio Branco, Francisco Costa, and Filipe Nunes.2008.
The processing of verbal inflection ambiguity:Characterization of the problem space.
In Proceedingsof the 21st Encontro Anual da Associac?a?o Portuguesade Lingu?
?stica (APL), pages 2577?2583.Thorsten Brants.
2000.
TnT ?
a statistical part-of-speech tagger.
In Proceedings of the 6th Applied Natu-ral Language Processing Conference and the 1st NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 224?231.Michael Brent.
1991.
Automatic acquisition of subcat-egorization frames from untagged text.
In Proceed-ings of the 29th Annual Meeting of the Association forComputational Linguistics, pages 209?214.Ted Briscoe and John Carroll.
1997.
Automatic extrac-tion of subcategorization from corpora.
In Proceed-ings of the 5th Applied Natural Language ProcessingConference, pages 356?363.Stephen Clark and James Curran.
2007.
Wide-coverageefficient statistical parsing with CCG and log-linearmodels.
Computational Linguistics, 33:493?552.Michael Collins and Nigel Duffy.
2002.
New rankingalgorithms for parsing and tagging: Kernels over dis-crete structures, and the voted perceptron.
In Proceed-ings of the 40th Annual Meeting of the Association forComputational Linguistics, pages 263?270.Nello Cristianini and John Shawe-Taylor.
2000.
AnIntroduction to Support Vector Machines and OtherKernel-Based Learning Methods.
Cambridge Univer-sity Press.Rebecca Dridan.
2009.
Using Lexical Statistics to Im-prove HPSG Parsing.
Ph.D. thesis, University of Saar-land.Mikel Galar, Alberto Fernande?z, Edurne Barrenechea,Humberto Bustince, and Francisco Herrera.
2011.
Anoverview of ensemble methods for binary classifiersin multi-class problems: Experimental study in one-vs-one and one-vs-all schemes.
Pattern Recognition,44:1761?1776.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2004.
SVMTool: Ageneral POS tagger generator based on support vector70machines.
In Proceedings of the 4th Language Re-sources and Evaluation Conference (LREC).Jesu?s Gime?nez and Llu?
?s Ma`rquez, 2006.
SVMTool:Technical Manual v1.3.
TALP Research Center, LSIDepartment, Universitat Politecnica de Catalunya.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In B. Scho?lkopf, C. Burges, and A.Smola, editors, Advances in Kernel Methods ?
Sup-port Vector Learning, chapter 11, pages 169?184.
MITPress, Cambridge, MA.Maria Helena Mira Mateus, Ana Maria Brito, Ine?sDuarte, Isabel Hub Faria, So?nia Frota, Gabriela Matos,Fa?tima Oliveira, Marina Viga?rio, and Alina Villalva.2003.
Grama?tica da L?
?ngua Portuguesa.
Caminho,5th edition.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2007.
Efficient HPSG parsing with supertagging andCFG-filtering.
In Proceedings of the 20th Interna-tional Joint Conference on Artificial Intelligence (IJ-CAI), pages 1671?1676.Ruslan Mitkov, editor.
2004.
The Oxford Handbook ofComputational Linguistics.
Oxford University Press.Alessando Moschitti.
2006.
Making tree kernels prac-tical for natural language learning.
In Proceedingsof the 11th European Chapter of the Association forComputational Linguistics.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
MaltParser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(2):95?135.Robbert Prins and Gertjan van Noord.
2003.
Reinforc-ing parser preferences through tagging.
Traitment Au-tomatique des Langues, 44:121?139.71
