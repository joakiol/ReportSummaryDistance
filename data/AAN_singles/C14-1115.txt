Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1218?1227, Dublin, Ireland, August 23-29 2014.Integrating Language and Visionto Generate Natural Language Descriptions of Videos in the WildJesse Thomason?University of Texas at Austinjesse@cs.utexas.eduSubhashini Venugopalan?University of Texas at Austinvsub@cs.utexas.eduSergio GuadarramaUniversity of California Berkeleysguada@eecs.berkeley.eduKate SaenkoUniversity of Massachusetts Lowellsaenko@cs.uml.eduRaymond MooneyUniversity of Texas at Austinmooney@cs.utexas.eduAbstractThis paper integrates techniques in natural language processing and computer vision to improverecognition and description of entities and activities in real-world videos.
We propose a strategyfor generating textual descriptions of videos by using a factor graph to combine visual detectionswith language statistics.
We use state-of-the-art visual recognition systems to obtain confidenceson entities, activities, and scenes present in the video.
Our factor graph model combines thesedetection confidences with probabilistic knowledge mined from text corpora to estimate the mostlikely subject, verb, object, and place.
Results on YouTube videos show that our approach im-proves both the joint detection of these latent, diverse sentence components and the detection ofsome individual components when compared to using the vision system alone, as well as overa previous n-gram language-modeling approach.
The joint detection allows us to automaticallygenerate more accurate, richer sentential descriptions of videos with a wide array of possiblecontent.1 IntroductionIntegrating language and vision is a topic that is attracting increasing attention in computational lin-guistics (Berg and Hockenmaier, 2013).
Although there is a fair bit of research on generating natural-language descriptions of images (Feng and Lapata, 2013; Yang et al., 2011; Li et al., 2011; Ordonez etal., 2011), there is significantly less work on describing videos (Barbu et al., 2012; Guadarrama et al.,2013; Das et al., 2013; Rohrbach et al., 2013; Senina et al., 2014).
In particular, much of the researchon videos utilizes artificially constructed videos with prescribed sets of objects and actions (Barbu et al.,2012; Yu and Siskind, 2013).
Generating natural-language descriptions of videos in the wild, such asthose posted on YouTube, is a very challenging task.In this paper, we focus on selecting content for generating sentences to describe videos.
Due to thelarge numbers of video actions and objects and scarcity of training data, we introduce a graphical modelfor integrating statistical linguistic knowledge mined from large text corpora with noisy computer vi-sion detections.
This integration allows us to infer which vision detections to trust given prior linguisticknowledge.
Using a large, realistic collection of YouTube videos, we demonstrate that this model effec-tively exploits linguistic knowledge to improve visual interpretation, producing more accurate descrip-tions compared to relying solely on visual information.
For example, consider the frames of the videoin Figure 1.
Instead of generating the inaccurate description ?A person is playing on the keyboard in thekitchen?
using purely visual information, our system generates the more correct ?A person is playing thepiano in the house?
by using statistics mined from parsed corpora to improve the interpretation of theuncertain visual detections, such as the presence of both a computer keyboard and a piano in the video.2 Background and Related WorkSeveral recent projects have integrated linguistic and visual information to aid description of images andvideos.
The most related work on image description is Baby Talk (Kulkarni et al., 2011), which uses?Indicates equal contributionThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1218Figure 1: Frames which depict a person playing a piano in front of a keyboard from one of the videosin our dataset.
Purely visual information is more confident in the computer keyboard?s presence than thepiano?s, while our model can correctly determine that the person is more likely to be playing the pianothan the computer keyboard.a Conditional Random Field (CRF) to integrate visual detections with statistical linguistic knowledgemined from parsed image descriptions and Google queries, and the work of Yang et al.
(2011) whichuses corpus statistics to aid the description of objects and scenes.
We go beyond the scope of theseprevious works by also selecting verbs through the integration of activity recognition from video andstatistics from parsed corpora.With regard to video description, the work of Barbu et al.
(2012) uses a small, hand-coded grammar todescribe a sparse set of prescribed activities.
In contrast, we utilize corpus statistics to aid the descriptionof a wide range of naturally-occurring videos.
The most similar work is (Krishnamoorthy et al., 2013;Guadarrama et al., 2013) which uses an n-gram language model to help determine the best subject-verb-object for describing a video.
Krishnamoorthy et al.
(2013) used a limited set of videos containinga small set of 20 entities, and the work of Guadarrama et al.
(2013) showed an advantage of usinglinguistic knowledge only for the case of ?zero shot activity recognition,?
in which the appropriate verbfor describing the activity was never seen during training.
Compared to this prior work, we explore amuch larger set of entities and activities (see Section 3.2) and add scene recognition (see Section 3.3) tofurther enrich the descriptions.
Our experiments demonstrate that our graphical model produces a moreaccurate subject-verb-object-place description than these simpler n-gram language modeling approaches.Our Contributions:?
We present a new method, a Factor Graph Model (FGM), to perform content selection by integratingvisual and linguistic information to select the best subject-verb-object-place description of a video.?
Our model includes scene (location) information which has not been addressed by previous videodescription works (Barbu et al., 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013).?
We demonstrate the scalability of our model by evaluating it on a large dataset of naturally occurringvideos (1297 training, 670 testing), recognizing sentential subjects out of 45 candidate entities,objects out of 218 candidate objects, verbs out of 218 candidate activities, and places out of 12candidate scenes.3 ApproachOur overall approach uses a probabilistic graphical model to integrate the visual detection of entities,activities, and scenes with language statistics to determine the best subject, verb, object, and place todescribe a given video.
A descriptive English sentence is generated from the selected sentential compo-nents.3.1 Video DatasetWe use the video dataset collected by Chen and Dolan (2011).
The dataset contains 1,967 short YouTubevideo clips paired with multiple human-generated natural-language descriptions.
The video clips are 10to 25 seconds in duration and typically consist of a single activity.
Portions of this dataset have beenused in previous work on video description (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013;Guadarrama et al., 2013).
We use 1,297 randomly selected videos for training and evaluate predictionson the remaining 670 test videos.12193.2 Visual Recognition of Subject, Verb, and ObjectWe utilize the visual recognition techniques employed by Guadarrama et al.
(2013) to process the videosand produce probabilistic detections of grammatical subjects, verbs, and objects.
In our data-set there are45 candidate entities for the grammatical subject (such as animal, baby, cat, chef, and person) and 241for the grammatical object (such as flute, motorbike, shrimp, person, and tv).
There are 218 candidateactivities for the grammatical verb, including climb, cut, play, ride, and walk.Entity Related Features From each video two frames per second are extracted and passed to pre-trained visual object classifiers and detectors.
As in Guadarrama et al.
(2013), we compute represen-tations based on detected objects using ObjectBank (Li et al., 2010) and the 20 PASCAL (Everinghamet al., 2010) object classes for each frame.
We use the PASCAL scores and ObjectBank scores withmax pooling over the set of frames as the entity descriptors for the video clip.
Additionally, to be ableto recognize more objects, we use the LLC-10k proposed by Deng et al.
(2012) which was trained onImageNet 2011 object dataset with 10k categories.
LLC-10K uses a bank of linear SVM classifiers overpooled local vector-quantized features learned from the 7K bottom level synsets of the 10K ImageNetdatabase.
We aggregate the 10K classifier scores obtained for each frame by doing max pooling acrossframes.Activity Related Features We use the activity recognizers described in Guadarrama et al.
(2013) toproduce probabilistic verb detections.
They extract Dense Trajectories developed by Wang et al.
(2011)and compute HoG (Histogram of Gradients), HoF (Histograms of Optical Flow) and MBH (MotionBoundary Histogram) features over space time volumes around the trajectories.
We used the defaultparameters proposed in Wang et al.
(2011) (N = 32, n?= 2, nr= 3) and adopted a standard bag-of-features representation.
We construct a codebook for each descriptor (Trajectory, HoG, HoF, MBH)separately.
For each descriptor we randomly sampled 100K points and clustered them using K-meansinto a codebook of 4000 words.
Descriptors are assigned to their closest vocabulary word using Eu-clidean distance.
Each video is then represented as a histogram over these clusters.Multi-channel SVM To allow object and activity features inform one another, we combine all thefeatures extracted using a multi-channel approach inspired by Zhang et al.
(2007) to build three non-linearSVM (Chang and Lin, 2011) classifiers for the subject, verb, and object, as described in Guadarrama etal.
(2013).
Note that we do not employ the hierarchical semantic model of Guadarrama et al.
(2013) toaugment our object or activity recognition.
In addition, each SVM learns a Platt scaling (Platt, 1999) topredict the label and a visual confidence value, C(t) ?
[0, 1], for each entity or activity t. The outputof the SVMs constitute the visual confidences on subject, verb, and object in all the models describedhenceforth.3.3 Visual Scene RecognitionIn addition to the techniques employed by Guadarrama et al.
(2013) used to obtain probabilistic de-tections of grammatical subjects, verbs, and objects, we developed a novel scene detector based onstate-of-the-art computer vision methods.We examined the description of all the 1,967 videos in the YouTube dataset and extracted scene wordsfrom the dependency parses as described in Section 3.4.
With the help of WordNet1we grouped the listof scene words and their synonyms into distinct scene classes.
Based on the frequency of mentions andthe coverage of scenes in the dataset, we shortlisted a set of 12 final scenes (mountain, pool, beach, road,kitchen, field, snow, forest, house, stage, track, and sky).For the detection itself, we follow Xiao et al.
(2010) and select several state-of-the-art features that arepotentially useful for scene recognition.
We extract GIST, HOG2x2, SSIM (self-similarity) and DenseSIFT descriptors.
We also extract LBP (Local Binary Patterns), Sparse SIFT Histograms, Line features,Color Histograms, Texton Histograms, Tiny Images, Geometric Probability Map and Geometric specifichistograms.
The code for extracting the features and computing kernels for the features is taken from1http://wordnet.princeton.edu1220the original papers as described in Xiao et al.
(2010).
Using the features and kernels, we train one-vs-allSVMs (Chang and Lin, 2011) to classify images into scene categories.
As in Xiao et al.
(2010), this gaveus 51 different SVM classifiers with different feature and kernel choices.
We use the images from theUIUC 15 scene dataset (Lazebnik et al., 2006) and the SUN 397 scene dataset (Xiao et al., 2010) fortraining the scene classifiers for all scenes except kitchen.
The training images for kitchen were obtainedby selecting 100 frames from about 15 training videos, since the classifier trained on images from theexisting scene datasets performed extremely poorly on the videos.
We use all the classifiers to detectscenes for each frame.
We then average the scene detection scores over all the classifiers across all theframes of the video.
This gives us visual confidence values, C(t), over all scene categories t for thevideo.3.4 Language StatisticsA key aspect of our approach is the use of language statistics mined from English text corpora tobias visual interpretation.
Like Krishnamoorthy et al.
(2013), we use dependency-parsed text fromfour large ?out of domain?
corpora: English Gigaword, British National Corpus (BNC), ukWac andWaCkypedia EN.
We also use a small, specialized ?in domain?
corpus: dependency parsed sentencesfrom the human-generated, English descriptions for the YouTube training videos mentioned in Sec-tion 3.1.
We extract SVOP (subject, verb, object, place) tuples from the dependency parses.
The subject-verb relationships are identified using nsubj dependencies, the verb-object relationships using dobj andprep dependencies.
Object-place relationships are identified using the prep dependency, checking thatthe noun modified by the preposition is one of our recognizable places (or synonyms of the recognizablescenes as indicated by WordNet).
We then extract co-occuring SV, VO, and OP bigram statistics fromthe resulting SVOP tuples to inform our factor-graph model, which uses both the out-of-domain (po) andin-domain (pi) bigram probabilities.3.5 Content Selection Using Factor GraphsIn order to combine visual and linguistic evidence, we use the probabilistic factor-graph model shownin Figure 2.
This model integrates the uncertain visual detections described in Sections 3.2 and 3.3 withthe language statistics described in Section 3.4 to predict the best words for describing the subject (S),verb (V), object (O), and place (P) for each test video.
After instantiating the potential functions forthis model, we perform a maximum a posteriori (MAP) estimation (via the max-product algorithm) todetermine the most probable joint set of values for these latent variables.Figure 2: The factor graph model used for content selection (right), and sample frames from a video tobe described (left).
Visual confidence values are observed (gray potentials) and inform sentence com-ponents.
Language potentials (dashed) connect latent words between sentence components.
Samples ofthe vision confidence values used as observations for the verb and object are shown for the example testvideo.1221Observation Potentials.
The observations in our model take the form of confidence scores from thevisual detectors described in Sections 3.2 and 3.3.
That is, the potential for each sentence componentk ?
{S, V,O, P}, ?k(t) = Ck(t) is the detection confidence that the classifier for component k (Ck)gives to the word t.Language Potentials.
Language statistics were gathered as described in Section 3.4 and used to deter-mine the language potentials as follows:?k,l(t, s) := p(l = s|k = t) := ?po(l = s|k = t) + (1?
?
)pi(l = s|k = t)Where k and l are two contiguous components in the SVOP sequence and t and s are words that arepossible values for these two components, respectively.
We would expect?V,O(ride,motorbike) := p(O=motorbike|V=ride)to be relatively high, since motorbike is a likely object of the verb ride.
The potential between twosequential components k and l in the SVOP sequence is computed by linearly interpolating the bigramprobability observed in the out-of-domain corpus of general text (po) and the in-domain corpus of videodescriptions (pi).
The interpolation parameter ?
adjusts the importance of these two corpora in deter-mining the bigram probability.
We optimized performance by fixing ?
= 0.25 when cross-validating onthe training data.
This weighting effectively allows general text corpora to be used to smooth the prob-ability estimates for video descriptions.
We note that meaningful information would likely be capturedby non-contiguous language potentials such as ?V,P, but that the resulting factor graphs would containcycles, preventing us from performing exact inference tractably.3.6 Sentence GenerationFinally, we use the SVOP tuple chosen by our model to generate an English sentence using the followingtemplate: ?Determiner (A,The) - Subject - Verb (Present, Present Continuous) - Preposition (optional)- Determiner (A,The) - Object (optional) - Preposition - Determiner (A,The) - Place (optional)?
Themost probable prepositions are identified using preposition-object and preposition-place bigram statisticsmined from the dependency parsed corpora described in Section 3.4.
Given an SVOP tuple, our objectiveis to generate a rich sentence using the subject, verb, object, and place information.
However, it is notprudent to add the object and place to the description of all videos since some verbs may be intransitiveand the place information may be redundant.
In order to achieve the best set of components to include,we use the above template to first generate a set of candidate sentences based on the SVO triple, SVPtriple and the SVOP quadruple.
Then, each sentence type (SVO, SVP, and SVOP) is ranked using theBerkeleyLM language model (Pauls and Klein, 2011) trained on the GoogleNgram corpus.
Finally, weoutput the sentence with the highest average 5-gram probability in order to normalize for sentence length.4 Experimental ResultsWe compared using the vision system alone to our model, which augments that system with linguisticknowledge.
Specifically, we consider the Highest Vision Confidence (HVC) model, which takes foreach sentence component the word with the highest confidence from the state-of-the-art vision detectorsdescribed in Sections 3.2 and 3.3.
We compare the results of this model on the 670 test videos to thoseof our Factor Graph Model (FGM), as discussed in Section 3.5.4.1 N-gram BaselineAdditionally, we compare both models against the existing, baseline n-gram model of Krishnamoorthyet al.
(2013) by extending their best n-gram model to support places.
To be specific, we build a quadra-gram model, similar to the trigram model of Krishnamoorthy et al.
(2013).
We first extract SVOP tuplesfrom the dependency parses as described in Section 3.4.
We then train a backoff language model withKneyser-Ney smoothing (Chen and Goodman, 1996) for estimating the likelihood of the SVOP quadru-ple.
On quadruples that are not seen during training, this quadragram language model backs off to SVO1222Most S% V% O% [P]% SVO% SVO[P]%n-gram 76.57 11.04 11.19 18.30 2.39 1.86HVC 76.57+22.24 11.94 17.24+4.33+2.92FGM 76.42+21.34 12.39 19.89+5.67+3.71Anyn-gram 86.87 19.25 21.94 21.75 5.67 2.65HVC 86.57+38.66 22.09 21.22+10.15+4.24FGM 86.27+37.16+24.63 24.67+10.45+6.10Table 1: Average binary accuracy of predicting the most common word (top) and of predicting any givenword (bottom).
Bold entries are statistically significantly (p < 0.05) greater than the HVC model, while+entries are significantly greater than the n-gram model.
No model scored significantly higher thanFGM on any metric.
[P] indicates that the score ranges only over the subset of videos for which anyannotator provided a place.triple and subject-verb, verb-object, object-place bigrams to estimate the probability of the quadruple.As in the case of the factor graph model, we consider the effect of learning from a domain specific textcorpus.
We build quadragram language models for both out-of-domain and in-domain text-corpora de-scribed in Section 3.4.
The probability of a quadragram in the language model is computed by linearlyinterpolating the probabilities from the in-domain and out-of-domain corpus.
We experiment with dif-ferent number of top subjects, objects, verbs, and places to estimate the most likely SVOP quadruplefrom the quadragram language model.
We report the results for the best performing n-gram model thatconsiders the top 5 subjects, 5 objects, 10 verbs, and 3 places based on the vision confidences and an out-of-domain corpus weight of 1.
This model also incorporates verb expansion as described in the originalwork (Krishnamoorthy et al., 2013).4.2 Content EvaluationTable 1 shows the accuracy of the models when their prediction for each sentence component is consid-ered correct only if it is the word most commonly used by human annotators to describe the video, as wellas the accuracy of the models when the prediction is considered correct if used by any of the annotators todescribe the video.
We evaluate the accuracy of each component (S,V,O,P) individually, and for completeSVO and SVOP tuples, where all components must be correct in order for a complete tuple to be judgedcorrect.
Because only about half (56.3%) of test videos were described with a place by some annotator,accuracies involving places (?[P]?)
are averaged only over the subset of videos for which any annotatorprovided a place.
Significance was determined using a paired t-test which compared the distributions ofthe binary correctness of each model?s prediction on each video for the specified component(s).We also use the WUP metric from Wordnet::Similarity2to measure the quality of the predicted wordsto account for semantically similar words.
For example, where the binary metric would mark ?slice?
asan incorrect substitute for ?cut?, the WUP metric will provide ?partial credit?
for such predictions.
Theresults using WUP similarity metrics for the most common word and any valid word (maximum WUPsimilarity is chosen from among valid words) are presented in Table 2.
Since WUP provides scores are inthe range [0,1], we view the scores as ?percent relevance,?
and we obtain tuple scores for each sentenceby taking the product of the component WUP scores.5 DiscussionIt is clear from the results in Table 1 that both the HVC and the FGM outperform the n-gram languagemodel approach used in the most-similar previous work (Krishnamoorthy et al., 2013; Guadarrama etal., 2013).
Note that while Krishnamoorthy et al.
(2013) showed an improvement with an n-gram modelconsidering only the top few vision detections, the FGM considers vision confidences over the entire set2http://wn-similarity.sourceforge.net/1223Most S% V% O% [P]% SVO% SVO[P]%n-gram 89.00 41.56 44.01 57.62 17.53 10.83HVC 89.09+?48.85 43.99 56.00+20.82+12.95FGM 89.01+47.05+45.29+59.64+21.54+14.50Anyn-gram 96.60 55.08 65.52 61.98 35.70 22.84HVC 96.54+?65.61 65.32 60.67+42.53+27.75FGM 96.32+63.49+67.52+64.68+42.43+29.34Table 2: Average WUP score of the predicted word against the most common word (top) and the max-imum score against any given word (bottom).
Bold entries are statistically significantly (p < 0.05)greater than the HVC model;+entries are significantly greater than the n-gram model;?entries aresignificantly greater than the FGM.
[P] indicates that the score ranges only over the subset of videos forwhich any annotator provided a place.of grammatical objects.
Additionally, our models are evaluated on a much more diverse set of videoswhile Krishnamoorthy et al.
(2013) evaluate the n-gram model on 185 videos (a small subset of the1,967 videos containing the 20 grammatical objects that their system recognized).The performance differences between the vision system (HVC) and our integrated model (FGM) aremodest but significant in important places.
Specifically, the FGM makes improvements to SVO (Table 1,top) and SVOP (Table 2, top) tuple accuracies.
FGM also significantly improves both the O and [P] (Ta-ble 1, bottom, and Table 2) component accuracies, suggesting that it can help clean up some noise fromthe vision systems even at the component level by considering related bigram probabilities.
FGM causesno significant losses under the binary metric, but performs worse than the HVC model on predicting averb component semantically similar to the correct verb under the WUP metric (Table 2).
This loss onthe verb component is worth the gains in tuple accuracy, since tuple prediction is the more difficult andmost central part of the content selection task.
Additionally, experiments by the authors of Guadarramaet al.
(2013) on Amazon Mechanical Turk have shown that humans tend to heavily penalize tuples anddescriptions even if they have most of the components correct.Table 3 shows frames from some test videos and the sentence components chosen by the models todescribe them.
In the top four videos we see the FGM improving raw vision results.
For example, itdetermines that a person is more likely slicing an onion than an egg.
Some specific confidence valuesfor the HVC can be seen for this video in Figure 2.
In the bottom two videos of Table 3 we see the HVCperforming better without linguistic information.
For example, the FGM intuits that a person is morelikely to be driving a car than lifting it, and steers the prediction away from the correct verb.
This maybe part of a larger phenomenon in which YouTube videos often depict unusual actions, and consequentlygeneral language knowledge can sometimes hurt performance by selecting more common activities.6 Future WorkCompared to the human gold standard descriptions, there appears to be room for improvement in de-tecting activities, objects, and scenes with high precision.
Visual recognition of entities and activities indiverse real-world videos is extremely challenging, partially due to lack of training data.
As a result ourcurrent model is faced with large amounts of noise in the vision potentials, especially for objects.
Goingforward, we believe that improving visual recognition will allow the language statistics to be even moreuseful.
We are currently exploring deep image feature representations (Donahue et al., 2013) to improveobject and verb recognition, as well as model transfer from large labeled object ontologies (Deng et al.,2009).From the generation perspective, there is scope to move beyond the template based sentence gener-ation.
This becomes particularly relevant if we detect multiple grammatical objects such as adjectivesor adverbs.
We need to decide whether additional grammatical objects would enrich the sentence de-1224FGM improves over HVC?A person is slicing the onion in the kitchen?Gold: person, slice, onion, (none)HVC: person, slice, egg, kitchenFGM: person, slice, onion, kitchen?A person is running a race on the road?Gold: person, run, race, (none)HVC: person, ride, race, groundFGM: person, run, race, road?A person is playing the guitar on the stage?Gold: person, play, guitar, treeHVC: person, play, water, kitchenFGM: person, play, guitar, stage?A person is playing a guitar in the house?Gold: person, play, guitar, (none)HVC: person, pour, chili, kitchenFGM: person, play, guitar, houseHVC better alone?A person is lifting a car on the road?Gold: person, lift, car, groundHVC: person, lift, car, roadFGM: person, drive, car, road?A person is pouring the egg in the kitchen?Gold: person, pour, mushroom, kitchenHVC: person, pour, egg, kitchenFGM: person, play, egg, kitchenTable 3: Example videos and: (Gold) the most common SVOP provided by annotators; (HVC) thehighest vision confidence selections; (FGM) the selections from our factor graph model.
The top sectionshows videos where the FGM improved over HVC; the bottom shows videos where the HVC did betteralone.
For each video, the sentence generated from the components chosen from the more successfulsystem is shown.scription and identify when to add them appropriately.
With increasing applications for such systems inautomatic video surveillance and video retrieval, generating richer and more diverse sentences for longervideos is an area for future research.
In comparison to previous approaches (Krishnamoorthy et al., 2013;Yang et al., 2011) the factor graph model can be easily extended to support this.
Additional nodes can beattached suitably to the graph to enable the prediction of adjectives and adverbs to enrich the base SVOPtuple.7 ConclusionsThis work introduces a new framework to generate simple descriptions of short videos by integratingvisual detection confidences with language statistics obtained from large textual corpora.
Experimentalresults show that our approach achieves modest improvements over a pure vision system and signifi-cantly improves over previous methods in predicting the complete subject-verb-object and subject-verb-object-place tuples.
Our work has a broad coverage of objects and verbs and extends previous works bypredicting place information.1225There are instances where our model fails to predict the correct verb when compared to the HVCmodel.
This could partially be because the SVM classifiers that detect activity already leverage entityinformation during training, and adding external language does not appear to improve verb predictionsignificantly.
Further detracting from performance, our model occasionally propagates, rather than cor-recting, errors from the HVC.
For example, when the HVC predicts the correct verb and incorrect object,such as in ?person ride car?
when the video truly depicts a person riding a motorbike, our model selectsthe more likely verb pairing ?person drive car?, extending the error from the object to the verb as well.Despite these drawbacks, our approach predicts complete subject-verb-object-place tuples moreclosely related to the most commonly used human descriptions than vision alone (Table 2), and in generalimproves both object and place recognition accuracies (Tables 1, 2).AcknowledgementsThis work was funded by NSF grant IIS1016312, DARPA Minds Eye grant W911NF-10-9-0059, andNSF ONR ATL grant N00014-11-1-0105.
Some of our experiments were run on the Mastodon Cluster(NSF grant EIA-0303609).ReferencesAndrei Barbu, Alexander Bridge, Zachary Burchill, Dan Coroian, Sven Dickinson, Sanja Fidler, Aaron Michaux,Sam Mussman, Siddharth Narayanaswamy, Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jeffrey MarkSiskind, Jarrell Waggoner, Song Wang, Jinlian Wei, Yifan Yin, and Zhiqi Zhang.
2012.
Video in sentences out.In Association for Uncertainty in Artificial Intelligence (UAI).Tamara Berg and Julia Hockenmaier.
2013.
Workshop on vision and language.
In Proceedings of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL).NAACL.Chih-Chung Chang and Chih-Jen Lin.
2011.
Libsvm: a library for support vector machines.
ACM Transactionson Intelligent Systems and Technology (TIST), 2(3):27.David L. Chen and William B. Dolan.
2011.
Collecting highly parallel data for paraphrase evaluation.
InProceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human LanguageTechnologies-Volume 1, pages 190?200.
Association for Computational Linguistics.Stanley F. Chen and Joshua Goodman.
1996.
An empirical study of smoothing techniques for language modeling.In Proceedings of the 34th annual meeting on Association for Computational Linguistics (ACL), pages 310?318.Association for Computational Linguistics.Pradipto Das, Chenliang Xu, Richard F. Doell, and Jason J. Corso.
2013.
A thousand frames in just a fewwords: Lingual description of videos through latent topics and sparse object stitching.
In IEEE Conference onComputer Vision and Pattern Recognition (CVPR).Jia Deng, Kai Li, Minh Do, Hao Su, and Li Fei-Fei.
2009.
Construction and analysis of a large scale imageontology.
Vision Sciences Society.Jia Deng, Jonathan Krause, Alex Berg, and Li Fei-Fei.
2012.
Hedging Your Bets: Optimizing Accuracy-Specificity Trade-offs in Large Scale Visual Recognition.
In IEEE Conference on Computer Vision and PatternRecognition (CVPR).Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell.
2013.Decaf: A deep convolutional activation feature for generic visual recognition.
arXiv preprint arXiv:1310.1531.Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman.
2010.
Thepascal visual object classes (voc) challenge.
International Journal of Computer Vision (IJCV), 88(2):303?338,June.Yansong Feng and Mirella Lapata.
2013.
Automatic caption generation for news images.
IEEE Transactions onPattern Analysis and Machine Intelligence (PAMI), 35(4):797?812.1226Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkarnenkar, Subhashini Venugopalan, Raymond Mooney,Trevor Darrell, and Kate Saenko.
2013.
Youtube2text: Recognizing and describing arbitrary activities usingsemantic hierarchies and zero-shot recognition.
In IEEE International Conference on Computer Vision (ICCV),December.Niveda Krishnamoorthy, Girish Malkarnenkar, Raymond J. Mooney, Kate Saenko, and Sergio Guadarrama.
2013.Generating natural-language video descriptions using text-mined knowledge.
In Proceedings of the AAAI Con-ference on Artificial Intelligence (AAAI), pages 541?547.Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Alexander Berg, Yejin Choi, and Tamara Berg.
2011.Baby talk: Understanding and generating image descriptions.
In IEEE Conference on Computer Vision andPattern Recognition (CVPR).
IEEE.Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006.
Beyond bags of features: Spatial pyramid matchingfor recognizing natural scene categories.
In IEEE Conference on Computer Vision and Pattern Recognition(CVPR), volume 2, pages 2169?2178.
IEEE.Li-Jia Li, Hao Su, Eric Xing, and Li Fei-Fei.
2010.
Object bank: A high-level image representation for sceneclassication and semantic feature sparsification.
In Advances in Neural Information Processing Systems (NIPS).Siming Li, Girish Kulkarni, Tamara L. Berg, Alexander C. Berg, and Yejin Choi.
2011.
Composing simpleimage descriptions using web-scale n-grams.
In Proceedings of the Fifteenth Conference on ComputationalNatural Language Learning (CoNLL), pages 220?228, Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Tanvi S. Motwani and Raymond J. Mooney.
2012.
Improving video activity recognition using object recognitionand text mining.
In Proceedings of the European Conference on Artificial Intelligence (ECAI), pages 600?605.Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011.
Im2text: Describing images using 1 millioncaptioned photographs.
In Advances in Neural Information Processing Systems (NIPS), volume 24, pages1143?1151.Adam Pauls and Dan Klein.
2011.
Faster and smaller n-gram language models.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguistics: Human Language Technologies, pages 258?267.Association for Computational Linguistics.John C. Platt.
1999.
Probabilistic outputs for support vector machines and comparisons to regularized likelihoodmethods.
In Advances In Large Margin Classifiers, pages 61?74.
MIT Press.Marcus Rohrbach, Qiu Wei, Ivan Titov, Stefan Thater, Manfred Pinkal, and Bernt Schiele.
2013.
Translating videocontent to natural language descriptions.
In IEEE International Conference on Computer Vision (ICCV).Anna Senina, Marcus Rohrbach, Wei Qiu, Annemarie Friedrich, Sikandar Amin, Mykhaylo Andriluka, ManfredPinkal, and Bernt Schiele.
2014.
Coherent multi-sentence video description with variable level of detail.
arXivpreprint arXiv:1403.6173.Heng Wang, Alexander Klaser, Cordelia Schmid, and Cheng-Lin Liu.
2011.
Action recognition by dense trajec-tories.
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3169?3176.
IEEE.Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba.
2010.
Sun database: Large-scale scene recognition from abbey to zoo.
IEEE Conference on Computer Vision and Pattern Recognition(CVPR), pages 3485?3492.Yezhou Yang, Ching Lik Teo, Hal Daum?e, III, and Yiannis Aloimonos.
2011.
Corpus-guided sentence generationof natural images.
In Conference on Emperical Methods in Natural Language Processing (EMNLP), pages444?454.Haonan Yu and Jeffrey Mark Siskind.
2013.
Grounded language learning from video described with sentences.
InProceedings of the Association for Computational Linguistics (ACL), pages 53?63.Jianguo Zhang, Marcin Marsza?ek, Svetlana Lazebnik, and Cordelia Schmid.
2007.
Local features and kernelsfor classification of texture and object categories: A comprehensive study.
International Journal of ComputerVision (IJCV), 73(2):213?238.1227
