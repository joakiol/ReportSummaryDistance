Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 292?300,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsMulti-Task Learning for Improved Discriminative Training in SMTPatrick Simianer and Stefan RiezlerDepartment of Computational LinguisticsHeidelberg University69120 Heidelberg, Germany{simianer,riezler}@cl.uni-heidelberg.deAbstractMulti-task learning has been shown to beeffective in various applications, includingdiscriminative SMT.
We present an exper-imental evaluation of the question whethermulti-task learning depends on a ?natu-ral?
division of data into tasks that bal-ance shared and individual knowledge, orwhether its inherent regularization makesmulti-task learning a broadly applicableremedy against overfitting.
To investi-gate this question, we compare ?natural?tasks defined as sections of the Interna-tional Patent Classification versus ?ran-dom?
tasks defined as random shards inthe context of patent SMT.
We find thatboth versions of multi-task learning im-prove equally well over independent andpooled baselines, and gain nearly 2 BLEUpoints over standard MERT tuning.1 IntroductionMulti-task learning is motivated by situationswhere a number of statistical models need to be es-timated from data belonging to different tasks.
It isassumed that the data are not completely indepen-dent of one another as they share some common-alities, yet they differ enough to counter a simplepooling of data.
The goal of multi-task learning isto take advantage of commonalities among tasksby learning a shared model without neglecting in-dividual knowledge.
For example, Obozinski etal.
(2010) present an optical character recognitionscenario where data consist of samples of hand-written characters from several writers.
While thestyles of different writers vary, it is expected thatthere are also commonalities on a pixel- or stroke-level that are shared across writers.
Chapelle et al(2011) present a scenario where data from searchengine query logs are available for different coun-tries.
While the rankings for some queries willhave to be country-specific (they cite ?football?as a query requiring different rankings in the USand the UK), a large fraction of queries will becountry-insensitive.
Wa?schle and Riezler (2012b)present multi-task learning for statistical machinetranslation (SMT) of patents from different classes(so-called sections) according to the InternationalPatent Classification (IPC)1.
While the vocabularymay differ between the different IPC sections, spe-cific legal jargon and a typical textual structurewill be shared across IPC sections.
As shown inthe cited works, treating data from different writ-ers, countries, or IPC classes as data from differ-ent tasks, and applying generic multi-task learningto the specific scenario, improves learning resultsover learning independent or pooled models.The research question we ask in this paper isas follows: Is multi-task learning dependent on a?natural?
task structure in the data, where sharedand individual knowledge is properly balanced?Or can multi-task learning be seen as a generalregularization technique that prevents overfittingirrespective of the task structure in the data?We investigate this research question on the ex-ample of discriminative training for patent trans-lation, using the algorithm for multi-task learn-ing with `1/`2 regularization presented by Simi-aner et al(2012).
We compare multi-task learningon ?natural?
tasks given by IPC sections to multi-task learning on ?random?
tasks given by randomshards and to baseline models trained on indepen-dent tasks and pooled tasks.
We find that bothversions of multi-task learning improve over inde-pendent or pooled training.
However, differencesbetween multi-task learning on IPC tasks and ran-dom tasks are small.
This points to a more generalregularization effect of multi-task learning and in-dicates a broad applicability of multi-task learningtechniques.
Another advantage of the `1/`2 reg-1http://wipo.int/classifications/ipc/en/292ularization technique of Simianer et al(2012) isa considerable efficiency gain due to paralleliza-tion and iterative feature selection that makes thealgorithm suitable for big data applications andfor large-scale training with millions of sparse fea-tures.
Last but not least, our best result for multi-task learning improves by nearly 2 BLEU pointsover the standard MERT baseline.2 Related WorkMulti-task learning is an active area in machinelearning, dating back at least to Caruana (1997).
Aregularization perspective was introduced by Ev-geniou and Pontil (2004), who formalize the cen-tral idea of trading off optimality of parameter vec-tors for each task-specific model and closeness ofthese model parameters to the average parame-ter vector across models in an SVM framework.Equivalent formalizations replace parameter reg-ularization by Bayesian prior distributions on theparameters (Finkel and Manning, 2009) or by aug-mentation of the feature space with domain inde-pendent features (Daume?, 2007).
Besides SVMs,several learning algorithms have been extendedto the multi-task scenario in a parameter regu-larization setting, e.g., perceptron-type algorithms(Dredze et al 2010) or boosting (Chapelle etal., 2011).
Further variants include different for-malizations of norms for parameter regularization,e.g., `1/`2 regularization (Obozinski et al 2010)or `1/`?
regularization (Quattoni et al 2009),where only the features that are most importantacross all tasks are kept in the model.Early research on multi-task learning for SMThas investigated pooling of IPC sections, withlarger pools improving results (Utiyama and Isa-hara, 2007; Tinsley et al 2010; Ceaus?u et al2011).
Wa?schle and Riezler (2012b) apply multi-task learning to tasks defined as IPC sections andcompare patent translation on independent tasks,pooled tasks, and multi-task learning, using same-sized training data.
They show small but sta-tistically significant improvements for multi-tasklearning over independent and pooled training.Duh et al(2010) introduce random tasks as n-bestlists of translations and showed significant im-provements by applying various multi-task learn-ing techniques to discriminative reranking.
Songet al(2011) define tasks as bootstrap samplesfrom the development set and show significant im-provements for a bagging-based system combina-tion over individual MERT training.In this paper we apply the multi-task learningtechnique of Simianer et al(2012) to tasks de-fined as IPC sections and to random tasks.
Theiralgorithm can be seen as a weight-based back-ward feature elimination variant of Obozinski etal.
(2010)?s gradient-based forward feature selec-tion algorithm for `1/`2 regularization.
The lat-ter approach is related to the general methodol-ogy of using block norms to select entire groupsof features jointly.
For example, such groups canbe defined as non-overlapping subsets of features(Yuan and Lin, 2006), or as hierarchical groupsof features (Zhao et al 2009), or they can begrouped by the general structure of the predictionproblem (Martins et al 2011).
However, theseapproaches are concerned with grouping featureswithin a single prediction problem whereas multi-task learning adds an orthogonal layer of multipletask-specific prediction problems.
By virtue of av-eraging selected weights after each epoch, the al-gorithm of Simianer et al(2012) is related to Mc-Donald et al(2010)?s iterative mixing procedure.This algorithm is itself related to the bagging pro-cedure of Breiman (1996), if random shards areconsidered from the perspective of random sam-ples.
In both cases averaging helps to reduce thevariance of the per-sample classifiers.3 Multi-task Learning for DiscriminativeTraining in SMTIn multi-task learning, we have data points{(xiz, yiz), i = 1, .
.
.
, Nz, z = 1, .
.
.
, Z}, sampledfrom a distribution Pz on X ?
Y .
The subscriptz indexes tasks and the superscript i indexes i.i.d.data for each task.
For the application of discrimi-native ranking in SMT, the spaceX can be thoughtof as feature representations of n-best translations,and the space Y denotes corresponding sentence-level BLEU scores.2 We assume that Pz is differ-ent for each task but that the Pz?s are related as,for example, considered in Evgeniou and Pontil(2004).
The standard approach is to fit an inde-pendent model involving a D-dimensional param-eter vector wz for each task z.
In multi-task learn-ing, we consider a Z-by-D matrix W = (wdz)z,dof stacked D-dimensional row vectors wz , and Z-dimensional column vectors wd of weights asso-ciated with feature d across tasks.
The central al-2See Duh et al(2010) for a similar formalization for thecase of n-best reranking via multi-task learning.293gorithms in most multi-task learning techniquescan be characterized as a form of regularizationthat enforces closeness of task-specific parametervectors to shared parameter vectors, or promotessparse models that only contain features that areshared across tasks.
In this paper, we will fol-low the approach of Simianer et al(2012), whoformalize multi-task learning as a distributed fea-ture selection algorithm using `1/`2 regulariza-tion.
`1/`2 regularization can be described as pe-nalizing weights W by the weighted `1/`2 norm,which is defined following Obozinski et al(2010),as?||W||1,2 = ?D?d=1||wd||2.Each `2 norm of a weight column wd representsthe relevance of the corresponding feature acrosstasks.
The `1 sum of the `2 norms enforces aselection of features by encouraging several fea-ture columns wd to be 0 and others to have highweights across all tasks.
This results in shrinkingthe matrix to the features that are useful across alltasks.Simianer et al(2012) achieve this behavior bythe following weight-based iterative feature elimi-nation algorithm that is wrapped around a stochas-tic gradient descent (SGD) algorithm for pairwiseranking (Shen and Joshi, 2005):Algorithm 1 Multi-task SGDGet data for Z tasks, each including S sentences;distribute to machines.Initialize v?
0.for epochs t?
0 .
.
.
T ?
1: dofor all tasks z ?
{1 .
.
.
Z}: parallel dowz,t,0,0 ?
vfor all sentences i ?
{0 .
.
.
S ?
1}: doDecode ith input with wz,t,i,0.for all pairs j ?
{0 .
.
.
P ?
1}: dowz,t,i,j+1 ?
wz,t,i,j ?
?
?lj(wz,t,i,j)end forwz,t,i+1,0 ?
wz,t,i,Pend forend forStack weights W?
[w1,t,S,0| .
.
.
|wZ,t,S,0]TSelect topK feature columns of W by `2 normfor k ?
1 .
.
.K dov[k] = 1ZZ?z=1W[z][k].end forend forreturn vThe innermost loop of the algorithm computesan SGD update based on the subgradient ?lj of apairwise loss function.
`1/`2-based feature selec-tion is done after each epoch of SGD training foreach task in parallel.
The `2 norm of the weightsis computed for each feature column across tasks;features are sorted by this value; K top featuresare kept in the model; reduced weight vectors aremixed and the result is re-sent to each task-specificmodel to start another epoch of parallel trainingfor each task.We compare two different loss functions forpairwise ranking, one corresponding to the orig-inal perceptron algorithm (Rosenblatt, 1958), andan improved version called the margin perceptron(Collobert and Bengio, 2004).
To create train-ing data for a pairwise ranking setup, we gener-ate preference pairs by ordering translations ac-cording to smoothed sentence-wise BLEU score(Nakov et al 2012).
Let each translation candi-date in the n-best list be represented by a featurevector x ?
IRD: For notational convenience, wedenote by xj a preference pair xj = (x(1)j ,x(2)j )where x(1)j is ordered above x(2)j w.r.t.
BLEU.
Fur-thermore, we use the shorthand x?j = x(1)j ?
x(2)jto denote aD-dimensional difference vector repre-senting an input pattern.
For completeness, a labely = +1 can be assigned to patterns x?j where x(1)jis ordered above x(2)j (y = ?1 otherwise), how-ever, since the ordering relation is antisymmetric,we can consider an ordering in one direction andomit the label entirely.The original perceptron algorithm is based onthe following hinge loss-type objective function:lj(w) = (?
?w, x?j ?
)+where (a)+ = max(0, a) , w ?
IRD is a weightvector, and ?
?, ??
denotes the standard vector dotproduct.
Instantiating SGD to the stochastic sub-gradient?lj(w) ={?x?j if ?w, x?j?
?
0,0 else.leads to the perceptron algorithm for pairwiseranking (Shen and Joshi, 2005).Collobert and Bengio (2004) presented a ver-sion of perceptron learning that includes a marginterm in order to control the capacity and thus thegeneralization performance.
Their margin percep-tron algorithm follows from applying SGD to theloss functionlj(w) = (1?
?w, x?j ?
)+294with the following stochastic subgradient?lj(w) ={?x?j if ?w, x?j?
< 1,0 else.Collobert and Bengio (2004) argue that the use ofa margin term justifies not using an explicit regu-larization, thus making the margin perceptron anefficient and effective learning machine.4 Experiments4.1 Data & System SetupFor training, development and testing, we use dataextracted from the PatTR3 corpus for the experi-ments in Wa?schle and Riezler (2012b).
Trainingdata consists of about 1.2 million German-Englishparallel sentences.
We translate from German intoEnglish.
German compound words were split us-ing the technique of Koehn and Knight (2003).
Weuse the SCFG decoder cdec (Dyer et al 2010)4and build grammars using its implementation ofthe suffix array extraction method described inLopez (2007).
Word alignments are built from allparallel data using mgiza5 and the Moses scripts6.SCFG models use the same settings as describedin Chiang (2007).
We built a modified Kneser-Ney smoothed 5-gram language model using theEnglish side of the training data and performedquerying with KenLM (Heafield, 2011)7.The International Patent Classification (IPC)categorizes patents hierarchically into 8 sections,120 classes, 600 subclasses, down to 70,000 sub-groups at the leaf level.
The eight top classes(called sections) are listed in Table 1.Typically, a patent belongs to more than onesection, with one section chosen as main classi-fication.
Our development and test sets for eachof the classes, A to H, comprise 2,000 sentenceseach, originating from a patent with the respec-tive class.
These sets were built so that there is nooverlap of development sets and test sets, and nooverlap between sets of different classes.
Theseeight test sets are referred to as independent testsets.
Furthermore, we test on a combined set,3http://www.cl.uni-heidelberg.de/statnlpgroup/pattr4https://github.com/redpony/cdec5http://www.kyloo.net/software/doku.php/mgiza:overview6http://www.statmt.org/moses/?n=Moses.SupportTools7http://kheafield.com/code/kenlm/estimation/A Human NecessitiesB Performing Operations, TransportingC Chemistry, MetallurgyD Textiles, PaperE Fixed ConstructionsF Mechanical Engineering, Lighting,Heating, WeaponsG PhysicsH ElectricityTable 1: IPC top level sections.called pooled-cat, that is constructed by concate-nating the independent sets.
Additionally we usetwo pooled sets for development and testing, eachcontaining 2,000 sentences with all classes evenlyrepresented.Our tuning baseline is an implementation of hy-pergraph MERT (Kumar et al 2009), directly op-timizing IBM BLEU4 (Papineni et al 2002).
Fur-thermore, we present a regularization baseline byapplying `1 regularization with clipping (Carpen-ter, 2008; Tsuruoka et al 2009) to the standardpairwise ranking perceptron.
All pairwise rankingmethods use a smoothed sentence-wise BLEU+1score (Nakov et al 2012) to create gold standardrankings.
Our multi-task learning experiments arebased on pairwise ranking perceptrons that differin their objective, corresponding either to the orig-inal perceptron or to the margin-perceptron.
Bothversions of the perceptron are used for single-tasktuning and multi-task tuning.
In the multi-tasksetting, we compare three different methods fordefining a task: ?natural?
tasks given by IPC sec-tions where each independent data set is consid-ered as task; ?random?
tasks, defined by shardingwhere data is shuffled and split once, tasks are keptfixed throughout, and by resharding where after anepoch data is shuffled and new random tasks areconstructed.
In all cases a task/shard is defined tocontain 2,000 sentences8, resulting in eight shardsfor each setting.
The number of features selectedafter each epoch was set to K = 100, 000.For all perceptron runs, the following meta pa-rameters were fixed: A cube pruning pop limit of200 and non-terminal span limit of 15; 100-bestlists with unique entries; constant learning rate;multipartite pair selection.
Single-task perceptronruns on independent and pooled tasks were done8This number is determined by the size of the original de-velopment sets; variations of this size did not change results.295single-task tuningindep.
0 pooled 1 pooled-cat 2pooled test ?
51.18 51.22A 54.92 0255.27 055.17B 51.53 51.48 0151.69C 1256.31 255.90 55.74D 49.94 050.33 050.26E 149.19 48.97 149.13F 1251.26 51.02 51.12G 149.61 49.44 49.55H 49.38 49.50 0149.67average test 51.52 51.49 51.54Table 2: BLEU4 results of MERT baseline using densefeatures for three different tuning sets: independent (sepa-rate tuning sets for each IPC class), pooled and pooled-cat(concatenated independent sets).
Significant superior per-formance over other systems in the same row is denoted byprefixed numbers.
The first row shows, e.g., that the resultof pooled 1 is significantly better than independent 0, andpooled-cat 2.for 15 epochs; multi-task perceptron runs used10 epochs.
Single-task tuning on pooled-cat dataincreases computation time by a factor of eightwhich makes this setup infeasible in practice.
Forthe sake of comparison we performed 10 epochsin this setup.MERT (with default parameters) is used to op-timize the weights of 12 dense default features;eight translation model features, a word penalty,the passthrough weight, the language model (LM)score, and an LM out-of-vocabulary penalty.
Per-ceptron training allows to add millions of sparsefeatures which are directly derived from grammarrules: rule shape, rule identifier, bigrams in rulesource and target.
For a further explanation ofthese features see Simianer et al(2012).For testing we measured IBM BLEU4 on tok-enized and lowercased data.
Significance resultswere obtained by approximate randomization testsusing the approach of Clark et al(2011)9 to ac-count for optimizer instability.
Tuning methodswith a random component (MERT, randomizedexperiments) were repeated three times, scores re-ported in the tables are averaged over optimizerruns.4.2 Experimental ResultsIn single-task tuning mode, systems are tunedon the eight independent data sets separately, thepooled data set, and the independent data sets con-9https://github.com/jhclark/multevalsingle-task tuningindep.
0 pooled 1 pooled-cat 2pooled test ?
50.75 1 52.08A 1 55.11 54.32 01 55.94B 1 52.61 50.84 1 52.57C 56.18 56.11 01 56.75D 1 50.68 49.48 01 51.22E 1 50.27 48.69 1 50.01F 1 51.68 50.71 1 51.95G 1 49.90 49.06 01 50.51H 1 50.48 49.16 1 50.53average test 52.11 51.05 52.44model size 430,092.5 457,428 1,574,259Table 3: BLEU4 results for standard perceptron with `1 reg-ularization baseline using sparse rule features, tuned on in-dependent, pooled and pooled-cat sets.
Prefixed superscriptsdenote a significant improvement over the result in the samerow indicated by the superscript.catenated (pooled-cat).
Testing is done on each ofthe eight IPC sections separately, and on a pooledtest set of 2,000 sentences where all sections areequally represented.
Furthermore, we report aver-age test results over runs for all independent datasets.Results for the MERT baseline are shown inTable 2: Neither pooling nor concatenating inde-pendent sets leads to significant performance im-provements on all sets with averaged scores beingnearly identical.Evaluation results obtained with the standardperceptron algorithm (Table 4) show improve-ments over MERT in single-task tuning mode.
Thegain on pooled-cat data shows that in contrast toMERT training on 12 dense features, discrimi-native training using large feature sets is able tobenefit from large data sets.
However, since thepooled-cat scenario increases computation timeby a factor of 8, it is quite infeasible when usedwith large sets of sparse features.
Single-task tun-ing on a small set of pooled data seems to showoverfitting behavior.Table 3 shows evaluation results for a regular-ization baseline that applies `1 regularization withclipping to the the single-task tuned standard per-ceptron in Table 4.
We see gains in BLEU on in-dependent and pooled-cat tuning data, but not onthe small pooled data set.Multi-task tuning for the standard perceptronis shown in the right half of Table 4.
Becauseof parallelization, this scenario is as efficient as296single-task tuning multi-task tuningindep.
0 pooled 1 pooled-cat 2 IPC 3 sharding 4 resharding 5pooled test ?
51.33 1 51.77 12 52.56 12 52.54 12 52.60A 54.79 54.76 01 55.31 012 56.35 012 56.22 012 56.21B 12 52.45 51.30 1 52.19 012 52.78 0123 52.98 012 52.96C 2 56.62 56.65 1 56.12 01245 57.76 012 57.30 012 57.44D 1 50.75 49.88 1 50.63 01245 51.54 012 51.33 012 51.20E 1 49.70 49.23 01 49.92 012 50.51 012 50.52 012 50.38F 1 51.60 51.09 1 51.71 012 52.28 012 52.43 012 52.32G 1 49.50 49.06 01 49.97 012 50.84 012 50.88 012 50.74H 1 49.77 49.50 01 50.64 012 51.16 012 51.07 012 51.10average test 51.90 51.42 52.06 52.90 52.84 52.79model size 366,869.4 448,359 1,478,049 100,000 100,000 100,000Table 4: BLEU4 results for standard perceptron algorithm using sparse rule features, tuned in single-task mode on independent,pooled, and pooled-cat sets, and in multi-task mode on eight tasks taken from IPC sections or by random (re)sharding.
Prefixedsuperscripts denote a significant improvement over the result in the same row indicated by the superscript.single-task tuning multi-task tuningindep.
0 pooled 1 pooled-cat 2 IPC 3 sharding 4 resharding 5pooled test ?
51.33 1 52.58 12 52.98 12 52.95 12 52.99A 1 56.09 55.33 1 55.92 0124556.78 012 56.62 012 56.53B 1 52.45 51.59 1 52.44 01253.31 012 53.35 012 53.21C 1 57.20 56.85 01 57.54 0157.46 1 57.42 1 57.43D 1 50.51 50.18 01 51.38 0124552.14 0125 51.82 012 51.66E 1 50.27 49.36 01 50.72 012451.13 012 50.89 012 51.02F 1 52.06 51.20 01 52.61 0124553.07 012 52.80 012 52.87G 1 50.00 49.58 01 50.90 0124551.36 012 51.19 012 51.11H 1 50.57 49.80 01 51.32 01251.57 012 51.62 01 51.47average test 52.39 51.74 52.85 53.35 53.21 53.16model size 423,731.5 484,483 1,697,398 100,000 100,000 100,000Table 5: BLEU4 results for margin-perceptron algorithm using sparse rule features, tuned in single-task mode on independenttasks, and in multi-task mode on eight tasks taken from IPC sections or by random (re)sharding.
Prefixed superscripts denotea significant improvement over the result in the same row indicated by the superscript.single-task tuning on small data.
We see improve-ments in BLEU over single-task tuning on smalland large tuning data sets.
Concerning our initialresearch questions, we see that the performancedifference between ?natural?
tasks (IPC) and ?ran-dom?
tasks is not conclusive.
However, multi-task learning using `1/`2 regularization consis-tently outperforms the standard perceptron under`1 regularization as shown in Table 3 and MERTtuning as shown in Table 2.Table 5 shows the evaluation results of themargin-perceptron algorithm.
Evaluation resultson single-task tuning show that this algorithm im-proves over the standard perceptron (Table 4),even in its `1-regularized version (Table 3), onall tuning sets.
Results for multi-task tuningshow improvements over the same scenario for thestandard perceptron (Table 4).
This means thatthe improvements due to the orthogonal regular-ization techniques in example space and featurespace, namely large-margin learning and multi-task learning, add up.
A comparison betweensingle-task and multi-task tuning modes of themargin-perceptron shows a gain for the latter sce-narios.
Differences between multi-task learningon IPC classes versus random sharding or re-sharding are again small, with the best overall re-sult obtained by multi-task learning of the margin-perceptron on IPC classes.Overall, our best multi-task learing result isnearly 2 BLEU points better than MERT training.The algorithm to achieve this result is efficient due297to parallelization and due to iterative feature se-lection.
As shown in the last rows of Tables 3-5 ,the average size is around 400K features for inde-pendently tuned models and around 1.6M featuresfor models tuned on pooled-cat data.
In multi-tasklearning, models can be iteratively cut to 100Kshared features whose weights are tuned in par-allel.5 ConclusionWe presented an experimental investigation of thequestion whether the power of multi-task learningdepends on data structured along tasks that exhibita proper balance of shared and individual knowl-edge, or whether its inherent feature selection andregularization makes multi-task learning a widelyapplicable remedy against overfitting.
We com-pared multi-task patent SMT for ?natural?
tasksof IPC sections and ?random?
tasks of shards indistributed learning.
Both versions of multi-tasklearning yield significant improvements over in-dependent and pooled training, however, the dif-ference between ?natural?
and ?random?
tasks ismarginal.
This is an indication for the useful-ness of multi-task learning as a generic regulariza-tion tool.
Considering also the efficiency gainedby iterative feature selection, the `1/`2 regulariza-tion algorithm presented in Simianer et al(2012)presents itself as an efficient and effective learningalgorithm for general big data and sparse featureapplications.
Furthermore, the improvements bymulti-task feature selection add up with improve-ments by large-margin learning, delivering overallimprovements of nearly 2 BLEU points over thestandard MERT baseline.Our research question regarding the superiorityof ?natural?
or ?random?
tasks was shown to beundetermined for the application of patent trans-lation.
The obvious question for future work is ifand how a task division can be found that improvesmulti-task learning over our current results.
Suchan investigation will have to explore various sim-ilarity metrics and clustering techniques for IPCsub-classes (Wa?schle and Riezler, 2012a), e.g., forthe goal of optimizing clustering with respect tothe ratio of between-cluster to within-cluster sim-ilarity for a given metric.
However, the final crite-rion for the usefulness of a clustering is necessar-ily application specific (von Luxburg et al 2012),in our case specific to patent translation perfor-mance.
Nevertheless, we hope that the presentedand future work will prove useful and generaliz-able for related multi-task learning scenarios.AcknowledgmentsThe research presented in this paper was supportedin part by DFG grant ?Cross-language Learning-to-Rank for Patent Retrieval?.ReferencesLeo Breiman.
1996.
Bagging predictors.
MachineLearning, 24:123?140.Bob Carpenter.
2008.
Lazy sparse stochastic gradientdescent for regularized multinomial logistic regres-sion.
Technical report, Alias-i.Rich Caruana.
1997.
Multitask learning.
Journal ofMachine Learning Research, 28.Alexandru Ceaus?u, John Tinsley, Jian Zhang, and AndyWay.
2011.
Experiments on domain adaptation forpatent machine translation in the PLuTO project.
InProceedings of the 15th Conference of the EuropeanAssociation for Machine Translation (EAMT 2011),Leuven, Belgium.Olivier Chapelle, Pannagadatta Shivaswamy, SrinivasVadrevu, Kilian Weinberger, Ya Zhang, and BelleTseng.
2011.
Boosted multi-task learning.
MachineLearning.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33.Jonathan Clark, Chris Dyer, Alon Lavie, and NoahSmith.
2011.
Better hypothesis testing for statis-tical machine translation: Controlling for optimizerinstability.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics(ACL?11), Portland, OR.Ronan Collobert and Samy Bengio.
2004.
Links be-tween perceptrons, MLPs, and SVMs.
In Proceed-ings of the 21st International Conference on Ma-chine Learning (ICML?04), Banff, Canada.Hal Daume?.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguistics(ACL?07), Prague, Czech Republic.Mark Dredze, Alex Kulesza, and Koby Crammer.2010.
Multi-domain learning by confidence-weighted parameter combination.
Machine Learn-ing, 79:123?149.Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, HidekiIsozaki, and Masaaki Nagata.
2010.
N-best rerank-ing by multitask learning.
In Proceedings of the 5thJoint Workshop on Statistical Machine Translationand MetricsMATR, Uppsala, Sweden.298Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Seti-awan, Vladimir Eidelman, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In Proceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics (ACL?10).Theodoros Evgeniou and Massimiliano Pontil.
2004.Regularized multi-task learning.
In Proceedings ofthe 10th ACM SIGKDD conference on knowledgediscovery and data mining (KDD?04), Seattle, WA.Jenny Rose Finkel and Christopher D. Manning.
2009.Hierarchical Bayesian domain adaptation.
In Pro-ceedings of the Conference of the North AmericanChapter of the Association for Computational Lin-guistics - Human Language Technologies (NAACL-HLT?09), Boulder, CO.Kenneth Heafield.
2011.
KenLM: faster and smallerlanguage model queries.
In Proceedings of theSixth Workshop on Statistical Machine Translation(WMT?11), Edinburgh, UK.Philipp Koehn and Kevin Knight.
2003.
Empiricalmethods for compound splitting.
In Proceedings ofthe 10th conference on European chapter of the As-sociation for Computational Linguistics (EACL?03),Budapest, Hungary.Shankar Kumar, Wolfgang Macherey, Chris Dyer,and Franz Och.
2009.
Efficient minimum errorrate training and minimum Bayes-risk decoding fortranslation hypergraphs and lattices.
In Proceedingsof the 47th Annual Meeting of the Association forComputational Linguistics and the 4th IJCNLP ofthe AFNLP (ACL-IJCNLP?09, Suntec, Singapore.Adam Lopez.
2007.
Hierarchical phrase-based trans-lation with suffix arrays.
In Proceedings of EMNLP-CoNLL, Prague, Czech Republic.Andre?
F. T. Martins, Noah A. Smith, Pedro M. Q.Aguiar, and Ma?rio A. T. Figueiredo.
2011.
Struc-tured sparsity in structured prediction.
In Proceed-ings of the 2011 Conference on Empirical Methodsin Natural Language Processing, Edinburgh, Scot-land.Ryan McDonald, Keith Hall, and Gideon Mann.
2010.Distributed training strategies for the structured per-ceptron.
In Proceedings of Human Language Tech-nologies: The 11th Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL-HLT?10), Los Angeles,CA.Preslav Nakov, Francisco Guzma?n, and Stephan Vogel.2012.
Optimizing for sentence-level bleu+1 yieldsshort translations.
In Proceedings of the 24th Inter-national Conference on Computational Linguistics(COLING 2012), Bombay, India.Guillaume Obozinski, Ben Taskar, and Michael I. Jor-dan.
2010.
Joint covariate selection and joint sub-space selection for multiple classification problems.Statistics and Computing, 20:231?252.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Ariadna Quattoni, Xavier Carreras, Michael Collins,and Trevor Darrell.
2009.
An efficient projec-tion for `1,?
regularization.
In Proceedings of the26th International Conference on Machine Learning(ICML?09), Montreal, Canada.Frank Rosenblatt.
1958.
The perceptron: A probabilis-tic model for information storage and organization inthe brain.
Psychological Review, 65(6).Libin Shen and Aravind K. Joshi.
2005.
Rankingand reranking with perceptron.
Journal of MachineLearning Research, 60(1-3):73?96.Patrick Simianer, Stefan Riezler, and Chris Dyer.2012.
Joint feature selection in distributed stochas-tic learning for large-scale discriminative training inSMT.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (ACL2012), Jeju, Korea.Linfeng Song, Haitao Mi, Yajuan Lu?, and Qun Liu.2011.
Bagging-based system combination for do-main adaptation.
In Proceedings of MT Summit XIII,Xiamen, China.John Tinsley, Andy Way, and Paraic Sheridan.
2010.PLuTO: MT for online patent translation.
In Pro-ceedings of the 9th Conference of the Association forMachine Translation in the Americas (AMTA 2010),Denver, CO.Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-niadou.
2009.
Stochastic gradient descent trainingfor `1-regularized log-linear models with cumulativepenalty.
In Proceedings of the 47th Annual Meet-ing of the Association for Computational Linguistics(ACL-IJCNLP?09), Singapore.Masao Utiyama and Hitoshi Isahara.
2007.
AJapanese-English patent parallel corpus.
In Pro-ceedings of MT Summit XI, Copenhagen, Denmark.Ulrike von Luxburg, Robert C. Williamson, and Is-abelle Guyon.
2012.
Clustering: Science or art?In Proceedings of the ICML 2011 Workshop on Un-supervised and Transfer Learning, Bellevue, WA.Katharina Wa?schle and Stefan Riezler.
2012a.
An-alyzing parallelism and domain similarities in theMAREC patent corpus.
In Proceedings of the 5thInformation Retrieval Facility Conference (IRFC2012), Vienna, Austria.299Katharina Wa?schle and Stefan Riezler.
2012b.
Struc-tural and topical dimensions in multi-task patenttranslation.
In Proceedings of the 13th Conferenceof the European Chapter of the Association for Com-putational Linguistics, Avignon, France.Ming Yuan and Yi Lin.
2006.
Model selectionand estimation in regression with grouped variables.J.R.Statist.Soc.B, 68(1):49?67.Peng Zhao, Guilherme Rocha, and Bin Yu.
2009.
Thecomposite absolute penalties family for grouped andhierarchical variable selection.
The Annals of Statis-tics, 37(6A):3468?3497.300
