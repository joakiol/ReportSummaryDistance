Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 974?983,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsCross-Lingual Lexico-Semantic Transfer in Language LearningEkaterina KochmarThe ALTA InstituteUniversity of Cambridgeek358@cam.ac.ukEkaterina ShutovaComputer LaboratoryUniversity of Cambridgees407@cam.ac.ukAbstractLexico-semantic knowledge of our nativelanguage provides an initial foundation forsecond language learning.
In this paper,we investigate whether and to what extentthe lexico-semantic models of the nativelanguage (L1) are transferred to the sec-ond language (L2).
Specifically, we focuson the problem of lexical choice and in-vestigate it in the context of three typolog-ically diverse languages: Russian, Span-ish and English.
We show that a statisticalsemantic model learned from L1 data im-proves automatic error detection in L2 forthe speakers of the respective L1.
Finally,we investigate whether the semantic modellearned from a particular L1 is portable toother, typologically related languages.1 IntroductionLexico-semantic knowledge of our native lan-guage is one of the factors that underlie our abilityto communicate and reason about the world.
It isalso the knowledge that guides us in the process ofsecond language learning.
Lexico-semantic vari-ation across languages (Bach and Chao, 2008)makes lexical choice a challenging task for sec-ond language learners (Odlin, 1989).
For instance,the meaning of the English expression pull thetrigger is realised as *push the trigger in Russianand Spanish, possibly leading to errors of lexicalchoice by Russian and Spanish speakers learningEnglish.
Our native language (L1) plays an essen-tial role in the process of lexical choice.
Whenchoosing between several linguistic realisations inL2, non-native speakers may rely on the lexico-semantic information from L1 and select a trans-lational equivalent that they deem to match theircommunicative intent best.
For example, Russianspeakers *do exceptions and offers instead of mak-ing them, and *find decisions instead of finding so-lutions, since in Russian do and make have a sin-gle translational equivalent (delat?
), and so do de-cision and solution (resheniye).
As a result, non-native speakers who tend to fall back to their L1translate phrases word-for-word, violating Englishlexico-semantic conventions.The effect of L1 interference on lexical choicein L2 has been pointed out in a number of stud-ies (Chang et al, 2008; Rozovskaya, 2010; Ro-zovskaya, 2011; Dahlmeier and Ng, 2011).
Someof these studies also demonstrated that using L1-specific properties, such as the error patterns ofspeakers of a given L1 or L1-induced paraphrases,improves the performance of automatic error cor-rection in non-native writing.
However, neither ofthe approaches has constructed a semantic modelfrom L1 data and systematically studied the effectsof its transfer onto L2.
In addition, most previouswork has focused on error correction, bypassingthe task of error detection for lexical choice.
Lex-ical choice is one of the most challenging tasksfor both non-native speakers and automated errordetection and correction (EDC) systems.
The re-sults of the most recent shared task on EDC, whichspanned all error types including lexical choice,show that most teams either did not propose anyalgorithms for this type of errors or did not per-form well on them (Ng, 2014).In this paper, we experimentally investigatethe influence of L1 on lexical choice in L2and whether lexico-semantic models from L1 aretransferred to L2 during language learning.
Forthis purpose, we induce L1 and L2 semantic mod-els from corpus statistics in each language in-dependently, and then use the discrepancies be-tween the two models to identify errors of lexi-cal choice.
We focus on two types of verb?nouncombinations, VERB?DIRECT OBJECT (dobj) and974SUBJECT?VERB (subj), and consider two widelyspoken L1s from different language families ?Russian and Spanish.
We conduct our experimentsusing the Cambridge Learner Corpus (Nicholls,2003), containing writing samples of non-nativespeakers of English.
Spanish speakers account foraround 24.6% of the non-native speakers repre-sented in this corpus and Russian speakers for 4%.Our experiments test two hypotheses: (1) thatL1 effects in the lexical choice in L2 reveal them-selves in the difference of the word associationstrength in the L1 and L2; and (2) that L1 lexico-semantic models are portable to other, typologi-cally related languages.
To the best of our knowl-edge, our paper is the first one to experimentallyinvestigate these questions.
Our results demon-strate that L1-induced information improves auto-matic error detection for lexical choice, confirm-ing the hypothesis that L1 speakers rely on se-mantic knowledge from their native language dur-ing L2 learning.
We test the second hypothesisby verifying that Russian speakers exhibit similartrends in errors with the speakers of other Slaviclanguages, and Spanish speakers with the speakersof other Romance languages.
We find that the L1-induced information from Russian and Spanish iseffective in assessing lexical choice of the speak-ers of other languages for both language groups.2 Related work2.1 Error detection in content wordsEarly approaches to collocation error detection re-lied on manually created databases of correct andincorrect word combinations (Shei and Pain, 2000;Wible et al, 2003; Chang et al, 2008).
Con-structing such databases is expensive and time-consuming, and therefore, more recent researchturned to the use of machine learning techniques.Leacock et al (2014) note that most approachesto detection and correction of collocation errorscompare the writer?s word choice to the set of al-ternatives using association strength measures andchoose the combination with the highest score, re-porting an error if this combination does not coin-cide with the original choice (Futagi et al, 2008;?Ostling and Knutsson, 2009; Liu et al, 2009).This strategy is expensive as it relies on compar-ison with a set of alternatives, limited in capac-ity as it depends on the quality of the alternativesgenerated and circular as the detection cannot beperformed independently of the correction.
Ourapproach alleviates these problems, since error de-tection depends on the original combination only.Some previous approaches focused on correc-tion only (Dahlmeier and Ng, 2011; Kochmarand Briscoe, 2015), and although they showpromising results, they have not attempted to per-form error detection in lexical choice.
Kochmarand Briscoe (2014) focus on error detection, buttheir system addresses adjective?noun combina-tions and does not use L1-induced information.2.2 L1 factors in L2 writingThe influence of an L1 on lexical choice in L2and the resulting errors have been previously stud-ied (Chang et al, 2008;?Ostling and Knutsson,2009; Dahlmeier and Ng, 2011).
These works fo-cus on errors in particular L1s and use the trans-lational equivalents directly to improve candidateselection and quality of corrections.
Dahlmeierand Ng (2011) show that L1-induced paraphrasesoutperform approaches based on edit distance, ho-mophones, and WordNet synonyms in selectingthe appropriate corrections.Rozovskaya and Roth (2010) show that an errorcorrection system for prepositions benefits fromrestricting the set of possible corrections to thoseobserved in the non-native data.
Rozovskaya andRoth (2011) further demonstrate that the modelsperform better when they use knowledge about er-ror patterns of the non-native writers.
Accordingto their results, an error correction algorithm thatrelies on a set of priors dependent on the writer?spreposition and the writer?s L1 outperforms othermethods.
Madnani et al (2008) show promisingresults in whole-sentence grammatical error cor-rection using round-trip translations from GoogleTranslate via 8 different pivot languages.The results of these studies suggest that L1 is avaluable source of information in EDC.
However,all these works use isolated translational equiva-lents and focus on error correction only.
In con-trast, we construct holistic semantic models of L1from L1 corpora and use these models to performthe more challenging task of error detection.3 DataWe first use large monolingual corpora in Span-ish, Russian and English to build word associationmodels for each of the languages.
We then applythe resulting models for error detection in the En-glish learner data.9753.1 L1 DataSpanish data The Spanish data was extractedfrom the Spanish Gigaword corpus (Mendonca etal., 2011), a one billion-word collection of newsarticles in Spanish.
The corpus was parsed usingthe Spanish Malt parser (Nivre et al, 2007; Balles-teros et al, 2010).
We extracted VERB?SUBJECTand VERB?DIRECT OBJECT relations from theoutput of the parser, which we then used to buildan L1 word association model for Spanish.Russian data The Russian data was extractedfrom the RU-WaC corpus (Sharoff, 2006), atwo billion-word representative collection of textsfrom the Russian Web.
The corpus was parsed us-ing Malt dependency parser for Russian (Sharoffand Nivre, 2011), and the VERB?SUBJECT andVERB?DIRECT OBJECT relations were extractedfrom the parser output to create an L1 word as-sociation model for Russian.Dictionaries and translation Once the L1 wordassociations have been computed for the verb?noun pairs, we identify possible translations forverbs and nouns (in each pair) in isolation, as alanguage learner might do.
To create the trans-lation dictionaries, we extracted translations fromthe English?Spanish and English?Russian edi-tions of Wiktionary, both from the translation sec-tions and the gloss sections if the latter containedsingle words as glosses.
We focus on verb?nounpairs, therefore multi-word expressions were uni-versally removed.
We added inverse translationsfor every original translation.
We then createdseparate translation dictionaries for each languageand part-of-speech tag combination from the re-sulting collection of translations.3.2 L2 dataTo build the English word association model, wehave used a combination of the British NationalCorpus (Burnard, 2007) and the UKWaC (Baroniet al, 2009).
The corpora were parsed by theRASP parser (Briscoe et al, 2006) and VERB?SUBJECT and VERB?DIRECT OBJECT relationswere extracted from the parser output.
Since theUKWaC is a Web corpus, we assume that the datacontains a certain amount of noise, e.g.
typograph-ical errors, slang and non-words.
We filter theseout by checking that the verbs and nouns in the ex-tracted relations are included in WordNet (Miller,1995) with the appropriate part of speech.3.3 Learner dataTo extract the verb?noun combinations that havebeen used by non-native speakers in practice,we use the Cambridge Learner Corpus (CLC),which is a 52.5 million-word corpus of learner En-glish collected by Cambridge University Press andCambridge English Language Assessment since1993 (Nicholls, 2003).
It comprises English ex-amination scripts written by learners of Englishwith 148 different L1s, ranging across multipleexaminations and covering all levels of languageproficiency.
A 25.5 million-word component ofthe CLC has been manually error-annotated.We have preprocessed the CLC with the RASPparser (Briscoe et al, 2006), as it is robust whenapplied to ungrammatical sentences.
We have thenextracted all dobj and subj combinations: in total,we have extracted 187, 109 dobj and 225, 716 subjcombinations.
We have used the CLC error anno-tation to split the data into correct combinationsand errors.
We note that some verb?noun com-binations are annotated both as being correct andas errors, depending on their wider context of use.To ensure that the annotation we use in our exper-iments is reliable and not context-dependent, wehave empirically set a threshold to filter out am-biguously annotated instances.
The set of correctword combinations includes only those word pairsthat are used correctly in at least 70% of the casesthey occur in the CLC; the set of errors includesonly those that are used incorrectly at least 70% ofthe time.3.4 Experimental datasetsWe split the annotated CLC data by language andrelation type.
Table 1 presents the statistics onthe datasets collected.1We extract the verb?nouncombinations from the CLC texts written by nativespeakers of Russian (RU) and Spanish (ES) to testour first hypothesis, as well as by speakers of ALLL1s in the CLC to test our second hypothesis.
Wethen filter the extracted relations using the trans-lated verb?noun pairs from Russian and Spanishcorpora.We note that Russian and Spanish have compa-rable number of word combinations in L1-specificsubsets ?
10K-12K for dobj and subj combina-tions ?
and comparable error rates (ERR).
Wealso note that the error rates in the dobj sub-1The data is available at http://www.cl.cam.ac.uk/?ek358/cross-ling-data.html976Source CLC Total ERR (%) verbs nounsRUdobjRU 11, 184 12.55 786 1, 918ALL 62, 923 14.02 1, 387 4, 168RUsubjRU 10, 417 7.90 734 1, 775ALL 63, 649 9.49 1, 403 4, 374ESdobjES 11, 959 14.66 705 1, 926ALL 32, 966 15.17 1, 072 2, 928ESsubjES 9, 899 8.09 573 1, 733ALL 26, 766 9.42 877 2, 762Table 1: Statistics on the datasets collected.sets are higher than in subj subsets, presumably,because VERB?SUBJECT combinations allow formore flexibility in lexical choice.
We find a largenumber of translated word combinations in otherL1s, and it is interesting to note that the errorrates are higher across multiple languages than inthe same L1s, which corroborates our second hy-pothesis that the lexico-semantic models from L1stransfer to L2.
The last two columns of Table1 show how diverse our datasets are in terms ofverbs and nouns used in the constructions: for ex-ample, RUdobjsubset contains combinations with786 different verbs and 1, 918 different nouns.4 MethodsOur approach to detecting lexico-semantic trans-fer errors relies on the intuition that a mismatchbetween the lexico-semantic models in two lan-guages reveals itself in the difference in word as-sociation scores.
We argue that a high associationscore of a verb?noun combination in L1 showsthat it is a collocation in L1, but low associationscore of its translational equivalent in L2 signalsan error in L2 stemming from the lexico-semantictransfer.
Following previous research (Baldwinand Kim, 2010), we measure the strength of verb?noun association using pointwise mutual informa-tion (PMI).
Figure 1 illustrates this intuition.
InRussian, both *find decision vs. find solution havea high PMI score.
However, in English the latterhas a high PMI while the former has a negativePMI.
We expect such a discrepancy in word asso-ciation to be an indicator of error of lexical choice,driven by the L1 semantics.We treat the task of lexico-semantic transfer er-ror detection as a binary classification problem andtrain a classifier for this task.
The classifier usesa combination of L1 and L2 semantic features.
Ifour hypothesis holds, we expect to see an improve-ment in the classifier?s performance when addingL1 semantic features.Figure 1: Russian to English interface for *finddecision.4.1 L2 lexico-semantic featuresWe experiment with two types of L2 features:lexico-semantic features and semantic vectorspace features.Lexico-semantic features include:?
pmi in L2: we estimate the associationstrength between the noun and verb using thecombined BNC and UKWaC corpus;?
verb and noun: the identity of the verb andthe noun in the pair, encoded in a numericalform in the range of (0, 1).
The motivationbehind that step is that certain words are moreerror-prone than others and converting theminto numerical features helps the classifier touse this information.Semantic vector space features Kochmar andBriscoe (2014) obtained state-of-the-art results inerror detection by using the semantic componentof the content word combinations.
We reimple-ment these features and test their impact on ourtask.
We extracted the noun and verb vectors fromthe publicly available word2vec dataset of wordembeddings for 3 million words and phrases.2The300-dimensional vectors have been trained on apart of Google News dataset (about 100 billionwords) using word2vec (Mikolov et al, 2013).The dobj and subj vectors are then built usingelement-wise addition on the vectors (Mitchelland Lapata, 2008; Mikolov et al, 2013; Kochmarand Briscoe, 2014).Once the compositional vectors are created, themethod relies on the idea that correct combina-tions can be distinguished from the erroneous onesby certain vector properties (Vecchi et al, 2011;Kochmar and Briscoe, 2014).
We implement a setof numerical features based on the following prop-erties of the vectors:2code.google.com/archive/p/word2vec/977?
length of the additive (vn) vector?
cosvn?n?
cosine between the vn vector andthe noun vector?
cosvn?v?
cosine between the vn vector andthe verb vector?
dist10?
distance to the 10 nearest neigh-bours of the vn vector?
lex-overlap ?
proportion of the 10 near-est neighbours of the vn vector containing theverb/noun?
comp-overlap ?
overlap between the 10neighbours of the vn vector and 10 neigh-bours of the verb/noun vector?
cosv?n?
cosine between the verb and thenoun vectors.The 10 nearest neighbours are retrieved in thecombined semantic space containing word embed-dings and additive phrase vectors.
All features, ex-cept for the last one, have been introduced in pre-vious work and showed promising results (Vecchiet al, 2011; Kochmar and Briscoe, 2014).
For ex-ample, it has been shown that the distance from theconstructed word combination vector to its nearestneighbours is one of the discriminative features ofthe error detection classifier.
Manual inspection ofthe vectors and nearest neighbours shows that theclosest neighbour to *find decision is see decisionwith the similarity of 0.8735 while the closest oneto find solution is discover solution with the simi-larity of 0.9048.We implement an additional cosv?nfeaturebased on the intuition that the distance between theverb and noun vectors themselves may indicate asemantic mismatch and thus help in detecting lex-ical choice errors.4.2 L1 lexico-semantic featuresWe first quantified the strength of association be-tween the L1 verbs and nouns in the original L1data, using PMI.
We then generated a set of possi-ble translations for each verb?noun pair in L1 us-ing the translation dictionaries.
Each verb?nounpair in the CLC was then mapped to one of thetranslated L1 pairs and its L1 features.
We usedthe following L1 features in classification:?
pmi in L1: we estimate the strength of asso-ciation on the original L1 corpora;?
difference between the PMI of the verb?noun pair in L1 and in L2.4.3 ClassificationClassifier settings We treat the task as a binaryclassification problem and apply a linear SVMclassifier using scikit-learn LinearSVCimplementation.3The error rates in Table 1 showthat we are dealing with a two-class problemwhere one class (correct word combinations) sig-nificantly outnumbers the other class (errors) byup to 11:1 (on RUsubj).
To address the problemof class imbalance, we use subsampling: we ran-domly split the set of correct word combinationsin n samples keeping the majority class baselineunder 0.60, and run n experiments over the sam-ples.
We apply 10-fold cross-validation withineach sample.
The results reported in the follow-ing sections are averaged across the samples foreach dataset.Evaluation The goal of the classifier is to detecterrors, therefore we primarily focus on its perfor-mance on the error class and, in addition to ac-curacy, report precision (P), recall (R) and F1onthis class.
Previous studies (Nagata and Nakatani,2010) suggest that systems with high precision indetecting errors are more helpful for L2 learningthan systems with high recall as non-native speak-ers find misidentified errors very misleading.
Inline with this research, we focus on maximisingprecision on the error class.Baseline We compare the performance of ourdifferent feature sets to the baseline classifierwhich uses L2 co-occurrence frequency of theverb and noun in the pair as a single feature.
Fre-quency sets a competitive baseline as it is oftenjudged to be the measure of acceptability of an ex-pression and many previous works relied on thefrequency of occurrence as an evidence of accept-ability (Shei and Pain, 2000; Futagi et al, 2008).5 Experimental ResultsTo test our hypothesis that lexico-semantic mod-els are transferred from L1 to L2, we first run theset of experiments on the L1 subsets of the CLCdata, that is RU ?
RUCLCand ES ?
ESCLC,where the left-hand side of the notation denotesthe lexico-semantic model and the right-hand sidethe L1 of the speakers that produced the word pairsextracted from the CLC.
We incrementally add thefeatures, starting with the set of lexico-semantic3scikit-learn.org/978L1 Features Acc PeReF1eRUdobjbaseline 55.68 47.77 61.44 53.55pmiEn64.74 59.76 47.55 52.96+verb 64.79 59.87 47.56 53.01RUsubjbaseline 54.48 46.30 63.96 53.17pmiEn67.02 58.86 62.74 60.74+verb 67.64 59.84 62.17 60.98ESdobjbaseline 56.74 52.25 74.44 61.36pmiEn64.28 61.75 59.55 60.63+verb 64.34 61.80 59.67 60.71ESsubjbaseline 54.45 46.71 70.31 56.00pmiEn69.22 61.35 68.83 64.87+verb 69.51 61.79 68.58 65.00Table 2: System performance (in %) using L2lexico-semantic features, L1?
L1CLC.features in L2 that are readily available withoutreference to the L1, and later adding L1 semanticfeatures, and measure their contribution.5.1 L2 lexico-semantic featuresThe first system configuration we experiment withuses the set of lexico-semantic features from L2.Table 2 reports the results.
Our experiments showthat a classifier that uses L2 PMI (pmiEn) as asingle feature performs with relatively high accu-racy: on all four datasets it outperforms the base-line classifier achieving an increase from 7.54%(on ESdobj) up to 14.77% (on ESsubj) in accuracy.Adding the noun as a feature decreases perfor-mance of the classifier and we do not further usethis feature.
The verb used as an additional fea-ture consistently improves classifier performance.5.2 L2 semantic vector space featuresNext, we test the combination of the semantic vec-tor space features (sem) and combine them withtwo L2 lexico-semantic features including pmiEnand verb (denoted as ftEnhereafter for brevity).Table 3 reports the results.We note that the semantic vector space featureson their own yield precision of 50%?
52% on theerror class in dobj combinations and lower than50% on subj combinations.
This suggests that theclassifier misidentifies correct combinations as er-rors more frequently than it correctly detects er-rors.
Moreover, recall of this system configura-tion is also low on all datasets.
Adding the seman-tic vector space features to the other L2 semanticfeatures, however, improves the performance, asshown in Table 3.
As both groups of features referto the phenomena in L2, the results suggest thatthey complement each other.L1 Features Acc PeReF1eRUdobjsem 58.36 50.72 6.98 12.22+ftEn65.90 58.64 62.18 60.35RUsubjsem 58.62 36.07 3.40 6.12+ftEn68.37 60.05 66.48 63.07ESdobjsem 54.51 52.01 20.78 29.48+ftEn66.87 63.36 67.08 65.16ESsubjsem 58.63 49.37 9.27 15.47+ftEn70.75 62.21 74.31 67.72Table 3: System performance (in %) using a com-bination of L2 semantic features, L1?
L1CLC.L1 Features Acc PeReF1eRUdobjftEn64.79 59.87 47.56 53.01+pmiL166.05 58.74 62.72 60.67RUsubjftEn67.64 59.88 62.17 60.98+pmiL168.68 62.10 69.61 64.38ESdobjftEn64.34 61.80 59.67 60.71+pmiL166.89 63.01 68.61 65.68ESsubjftEn69.51 61.79 68.58 65.00+pmiL171.19 62.10 77.66 69.00Table 4: System performance (in %) using L1 andL2 lexico-semantic features, L1?
L1CLC.5.3 L1 lexico-semantic featuresFinally, we add the L1 lexico-semantic features tothe well-performing L2 features (pmi and verb).The combination of L1 lexico-semantic featureswith the L2 lexico-semantic and semantic vec-tor space features achieves lower results, there-fore we do not report them here.
The use of L1pmi improves both the accuracy and the F-scoreof the error class (see Table 4).
For the ease ofcomparison, we also include the results obtainedusing a combination of L1 lexico-semantic fea-tures (denoted ftEn).
The addition of the explicitdifference feature between the two PMIs hasnot yielded further improvement.
This is likely tobe due to the fact that the classifier already implic-itly captures the knowledge of this difference inthe form of individual L1 and L2 PMIs.We note that the system using a combination ofL1 and L2 lexico-semantic features gains an ab-solute improvement in accuracy from 1.04% forRUsubjto 2.55% on ESdobj.
The performance onthe error class improves in all but one case (Peon RUdobj), with an absolute increase in F1up to7.66%.
The system has both a higher coverage inerror detection (a rise in recall) and a higher pre-cision.
The improvement in performance acrossall four datasets is statistically significant at 0.05level.
These results demonstrate the effect oflexico-semantic model transfer from L1 to L2.9796 Effect on different L1sNext, we test our second hypothesis that a lexico-semantic model from one L1 is portable acrossseveral L1s, in particular, typologically relatedones.
We first experiment with the data repre-senting all L1s in the CLC and then with the datarepresenting a specific language group.
We com-pare the performance of the baseline system us-ing verb?noun co-occurrence frequency as a singlefeature, the system that uses L2 semantic featuresonly and the system that combines both L2 and L1semantic features.6.1 Experiments on all L1sTable 1 shows that using the translated verb?nouncombinations from our L1s (RU and ES) we areable to find a large amount of both correct and er-roneous combinations in different L1s in the CLCincluding RU and ES (see ALL).
This gives us aninitial confirmation that the lexico-semantic mod-els may be shared across multiple languages.We then experiment with error detection acrossall L1s represented in the CLC.
The results areshown in Table 5.
The baseline system achievessimilar performance on RU ?
ALLCLCas onRU ?
RUCLC, and better performance on ES ?ALLCLCthan on ES ?
ESCLC.
The results ob-tained with the L2 lexico-semantic features arealso comparable: the system achieves an absoluteincrease in accuracy of up to 9.86% for the modeltransferred from RUsubj, reaching an accuracy ofaround 65 ?
66% with balanced performance interms of precision and recall on errors.When the L1 lexico-semantic features are addedto the model, we observe an absolute increase inthe accuracy ranging from 0.57% (for RUsubj) to1.43% (for ESdobj).
The Spanish lexico-semanticmodel has a higher positive effect on all measures,including precision on the error class.
Althoughthe addition of the L1 lexico-semantic featuresdoes not have a significant effect on the accuracyand precision, the system achieves an absolute im-provement in recall of up to 12.71% (on RUdobj).That is, the system that uses L1 lexico-semanticfeatures is able to find more errors in the data orig-inating with a set of different L1s.
Generally, theresults of the Spanish model are more stable andcomparable to the results in the previous Section,which may be explained by the fact that Spanish ismore well-represented in the CLC.L1 Features Acc PeReF1eRUdobjbaseline 55.13 50.17 72.14 58.99ftEn63.58 59.73 57.98 58.85+pmiL164.60 58.81 70.69 64.20RUsubjbaseline 54.56 47.95 71.10 56.71ftEn64.42 57.27 62.64 59.83+pmiL164.99 57.24 68.17 62.21ESdobjbaseline 59.35 55.38 71.87 62.51ftEn64.32 61.89 63.47 62.67+pmiL165.75 61.90 71.37 66.30ESsubjbaseline 58.34 50.90 66.97 57.48ftEn65.57 58.32 64.09 61.06+pmiL166.54 58.80 68.72 63.36Table 5: System performance (in %) using L1 andL2 lexico-semantic features, L1?
all L1s.6.2 Experiments on related L1sThe results on ALL L1s confirm our expectations:since we have extracted verb?noun combinationsthat originate with two particular L1s from the setof all different L1s in the CLC, and then used theL1 lexico-semantic features, the system is able toidentify more errors thus we observe an improve-ment in recall.
The precision, however, does notimprove, possibly because the set of errors in ALLL1s is different from that in the two L1s we relyon to build the lexico-semantic models.
The finalquestion that we investigate is whether the lexico-semantic models of our L1s are directly portableto typologically related languages.
If this is thecase, we expect to see an effect on the precision ofthe classifier as well as on the recall.We experiment with the following groups of re-lated languages ordered by the number of verb?noun pairs we found in the CLC data:?
RU group: Russian, Polish, Czech, Slovak,Serbian, Croatian, Bulgarian, Slovene;?
ES group: Spanish, Italian, Portuguese,French, Catalan, Romanian, Romansch.In addition to investigating the effect of theL1 lexico-semantic model on the whole languagegroup, we also consider its effects on individuallanguages.
We chose Polish for the RU model, andItalian for the ES model as these two languageshave the most data representing their native speak-ers in the CLC.
Table 6 shows the number of verb?noun combinations and error rates for the languagegroups and these individual languages.The results are presented in Tables 7 and 8.They exhibit similar trends in the change of thesystem performance on L1 ?
L1 GROUP as we980Source Targets Total ERRRUdobjSlavic 18, 721 9.19Polish 11, 327 8.16RUsubjSlavic 18, 511 6.80Polish 11, 204 6.42ESdobjRomance 18, 898 12.81Italian 6, 375 10.92ESsubjRomance 15, 871 7.57Italian 5, 300 6.98Table 6: Statistics on the L1 groups and relatedlanguages.L1 Features Acc PeReF1eRUdobjbaseline 57.08 51.80 71.58 59.78ftEn64.20 60.99 55.36 58.04+pmiL165.77 61.06 64.78 62.86RUsubjbaseline 56.43 49.52 62.04 54.24ftEn62.26 55.84 50.02 52.76+pmiL162.78 56.02 54.48 55.21ESdobjbaseline 59.18 51.44 72.31 59.97ftEn65.14 59.82 53.83 56.66+pmiL166.24 58.92 67.00 62.70ESsubjbaseline 58.10 52.95 77.43 62.45ftEn66.29 61.24 68.45 64.64+pmiL167.00 61.68 70.50 65.78Table 7: System performance (in %) using L1 andL2 lexico-semantic features, L1?
L1 GROUP.see for L1 ?
ALL L1s.
Adding the L1 lexico-semantic features has only a minor effect on accu-racy and precision, and a more pronounced effecton recall.
On the contrary, when we test the systemon one particular related L1 (Table 8) we observethe opposite effect: with the exception of ESsubjdata, precision and accuracy improve, suggestingthat the error detection system using L1-inducedinformation identifies errors more precisely.Overall, the observed gains in performance in-dicate that L1 semantic models contribute infor-mation to lexical choice error detection in L2 forthe speakers of typologically related languages.This in turn suggests that there may be less seman-tic variation within a language group than acrossdifferent language groups.7 Discussion and data analysisThe best accuracy achieved in our experimentsis 71.19% on ESsubjcombinations.
However,previous research suggests that error detection inlexical choice is a difficult task.
For instance,Kochmar and Briscoe (2014) report that the agree-ment between human annotators on error detectionin adjective?noun combinations is 86.50%.We then qualitatively assessed the performanceof our systems by analysing what types of errorsL1 Features Acc PeReF1eRUdobjbaseline 55.04 47.68 63.87 53.81ftEn64.73 59.76 46.05 52.01+pmiL165.15 60.63 45.77 52.16RUsubjbaseline 53.30 44.77 61.09 51.29ftEn61.84 54.63 35.81 43.22+pmiL162.53 57.24 35.11 43.18ESdobjbaseline 55.25 51.67 76.79 61.21ftEn64.06 62.30 56.01 58.98+pmiL165.21 63.44 58.13 60.66ESsubjbaseline 54.34 47.76 68.73 56.23ftEn62.71 58.80 43.09 49.69+pmiL162.44 58.46 41.71 48.60Table 8: System performance (in %) using L1 andL2 lexico-semantic features, L1?
REL L1.the classifiers reliably detect and what types of er-rors the classifiers miss across all runs over thesamples.
Some of the most reliably identified er-rors in both RU and ES datasets include:?
verbs offer, propose and suggest which areoften confused with each other.
Correctlyidentified errors include *offer plan vs. sug-gest plan, *propose work vs. offer work and*suggest cost vs. offer cost;?
verbs demonstrate and show where demon-strate is often used instead of show as in*chart demonstrates;?
verbs say and tell particularly well identifiedwith the ES model.
Examples include *sayidea instead of tell idea and *tell goodbye in-stead of say goodbye.These examples represent lexical choice errorswhen selecting among near-synonyms, and viola-tions of verb subcategorization frames.
The errorin *find solution discussed throughout the paper isalso reliably identified by the classifier across allruns.
It is interesting to note that in the pair ofverbs do and make, which are often confused witheach other by both Russian and Spanish L1 speak-ers, errors involving make are identified more reli-ably than errors involving do: for example, *makebusiness is correctly identified as an error, while*do joke is missed by the classifier.Many of the errors missed by the classifier arecontext-dependent.
Some of the most problematicerrors involve errors in combinations with verbslike be and become.
Such errors do not result froman L1 lexico-semantic transfer and it is not surpris-ing that the classifiers miss them.9818 ConclusionWe have investigated whether lexico-semanticmodels from the native language are transferredto the second language, and what effect this trans-fer has on lexical choice in L2.
We focused on twotypologically different L1s ?
Russian and Spanish,and experimentally confirmed the hypothesis thatstatistical semantic models learned from these L1ssignificantly improve automatic error detection inL2 data produced by the speakers of the respec-tive L1s.
We also investigated whether the seman-tic models learned from particular L1s are portableto other languages, and in particular to languagesthat are typologically close to the investigated L1s.Our results demonstrate that L1 models improvethe coverage of the error detection system on arange of other L1s.AcknowledgmentsWe are grateful to the ACL reviewers for theirhelpful feedback.
Ekaterina Kochmar?s researchis supported by Cambridge English LanguageAssessment via the ALTA Institute.
EkaterinaShutova?s research is supported by the LeverhulmeTrust Early Career Fellowship.ReferencesBach E. and Chao W. 2008.
Semantic universals andtypology.
In Chris Collins, Morten Christiansen andShimon Edelman, eds., Language Universals (Ox-ford: Oxford University Press).Baldwin T. and Kim S. N. 2010.
Multiword Expres-sions.
In Handbook of Natural Language Process-ing, Second Edition, N. Indurkhya and F. J.
Damerau(eds.
), pp.
267?292.Ballesteros M., Herrera J., Francisco V., and Gerv?as P.2010.
A Feasibility Study on Low Level Techniquesfor Improving Parsing Accuracy for Spanish UsingMaltparser.
In Proceedings of the 6th Hellenic Con-ference on Artificial Intelligence: Theories, Modelsand Applications, pp.
39?48.Baroni M., Bernardini S., Ferraresi A., andZanchetta E. 2009.
The WaCky Wide Web: ACollection of Very Large Linguistically ProcessedWeb-Crawled Corpora.
Language Resources andEvaluation, 43(3): 209?226.Briscoe E., Carroll J., and Watson R. 2006.
The Sec-ond Release of the RASP System.
In Proceedingsof the COLING/ACL-2006 Interactive PresentationSessions, pp.
59?68.Burnard L. 2007.
The British National Corpus, ver-sion 3 (BNC XML Edition).
Distributed by Ox-ford University Computing Services on behalf of theBNC Consortium.
http://www.natcorp.ox.ac.uk/.Chang Y.C., Chang J.S., Chen H.J., and Liou H.C.2012.
An automatic collocation writing assistantfor Taiwanese EFL learners: A case of corpus-based NLP technology.
Computer Assisted Lan-guage Learning, 21(3), pp.
283?299.Dahlmeier D. and Ng H.T.
2011.
Correcting Se-mantic Collocation Errors with L1-induced Para-phrases.
In Proceedings of the EMNLP-2011, pp.107?117.Futagi Y., Deane P., Chodorow M., and Tetreault J.2009.
A computational approach to detecting col-location errors in the writing of non-native speakersof English.
Computer Assisted Language Learning,21(4), pp.
353?367.Joachims T. 1999.
Making Large-Scale SVM Learn-ing Practical.
Advances in Kernel Methods ?
Sup-port Vector Learning.
B. Sch?olkopf and C. Burgesand A. Smola (ed.
), MIT-Press.Kochmar E. and Briscoe T. 2014.
Detecting LearnerErrors in the Choice of Content Words Using Com-positional Distributional Semantics.
In Proceedingsof the 25th International Conference on Computa-tional Linguistics: Technical Papers, pp.
1740?1751Kochmar E. and Briscoe T. 2015.
Using Learner Datato Improve Error Correction in AdjectiveNoun Com-binations.
In Proceedings of the Tenth Workshopon Innovative Use of NLP for Building EducationalApplications, pp.
233-242.Leacock C., Chodorow M., Gamon M. and Tetreault J.2014.
Automated Grammatical Error Detection forLanguage Learners.
Morgan and Claypool Publish-ers.Liu A. L.-E., Wible D., and Tsao N.-L. 2009.
Auto-mated suggestions for miscollocations.
In Proceed-ings of the 4th Workshop on Innovative Use of NLPfor Building Educational Applications, pp.
47?50.Madnani N., Tetreault J., and Chodorow M. 2012.
Ex-ploring Grammatical Error Correction with Not-So-Crummy Machine Translation.
In Proceedings ofthe 7th Workshop on the Innovative Use of NLP forBuilding Educational Applications, pp.
44-53.Mendonca A., Jaquette D., Graff D., and DiPersio D.2011.
Spanish Gigaword Third Edition.
LinguisticData Consortium, Philadelphia.Mikolov T., Sutskever I., Chen K., Corrado G., andDean J.
2013.
Distributed Representations of Wordsand Phrases and their Compositionality.
In Pro-ceedings of NIPS.982Mikolov T., Yih W.-T., and Zweig G. 2013.
LinguisticRegularities in Continuous Space Word Representa-tions.
In Proceedings of NAACL HLT.Miller G. A.
1995.
WordNet: A Lexical Database forEnglish.
Communications of the ACM, 38(11): 39?41.Mitchell J. and Lapata M. 2008.
Vector-based modelsof semantic composition.
In Proceedings of ACL,pp.
236?244.Mitchell J. and Lapata M. 2010.
Composition in dis-tributional models of semantics.
Cognitive Science,34, pp.
1388?1429.Nagata, R. and Nakatani, K. 2010.
Evaluating Per-formance of Grammatical Error Detection to Max-imize Learning Effect.
In Proceedings of COLING(Posters), pp.
894-900.Ng, H.T., Wu, S. M., Briscoe, T., Hadiwinoto, C., Su-santo, R. H., Bryant, C. 2014.
The CoNLL-2014Shared Task on Grammatical Error Correction.
InProceedings of the Eighteenth Conference on Com-putational Natural Language Learning: Shared Task,pp.
1?14.Nicholls D. 2003.
The Cambridge Learner Cor-pus: Error coding and analysis for lexicography andELT.
In Proceedings of the Corpus Linguistics con-ference, pp.
572?581.Nivre J., Hall J., Nilsson J., Chanev A., Eryigit G.,K?ubler S., Marinov S., and Marsi E. 2007.
Malt-Parser: A language-independent system for data-driven dependency parsing.
Natural Language En-gineering, 2(13):95?135.Odlin T. 1989.
Language transfer: Cross-linguistic in-fluence in language learning.
Cambridge UniversityPress.
?Ostling R. and Knutsson O.
2009.
A corpus-basedtool for helping writers with Swedish collocations.In Proceedings of the Workshop on Extracting andUsing Constructions in NLP, NODALIDA, pp.
28?33.Park T., Lank E., Poupart P., and Terry M. 2008.
Isthe sky pure today?
AwkChecker: an assistive toolfor detecting and correcting collocation errors.
InProceedings of the 21st annual ACM symposium onUser interface software and technology, pp.
121?130.Rozovskaya A. and Roth D. 2010.
Generating Confu-sion Sets for Context-Sensitive Error Correction.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pp.
961-970.Rozovskaya A. and Roth D. 2011.
Algorithm Selectionand Model Adaptation for ESL Correction Tasks.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies ?
Volume 1, pp.
924?933.Sharoff S. 2006.
Creating General-Purpose CorporaUsing Automated Search Engine Queries.
WaCky!Working papers on the Web as Corpus, Marco Ba-roni and Silvia Bernardini (ed.
).Sharoff S. and Nivre J.
2011.
The proper place ofmen and machines in language technology Process-ing Russian without any linguistic knowledge.
Dia-logue 2011, Russian Conference on ComputationalLinguistics.Shei C.C.
and Pain H. 2000.
An ESL Writer?s Collo-cation Aid.
Computer Assisted Language Learning,13(2), pp.
167?182.Vecchi E., Baroni M. and Zamparelli R. 2011.
(Lin-ear) maps of the impossible: Capturing semanticanomalies in distributional space.
In Proceedingsof the DISCO Workshop at ACL-2011, pp.
1?9.Wible H., Kwo C.-H., Tsao N.-L., Liu A., and Lin H.-L. 2003.
Bootstrapping in a language-learning en-vironment.
Journal of Computer Assisted Learning,19(4), pp.
90?102.983
