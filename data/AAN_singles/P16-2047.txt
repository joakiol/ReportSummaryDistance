Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 287?292,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsAn Unsupervised Method for Automatic Translation Memory CleaningMasoud Jalili Sabet(1), Matteo Negri(2), Marco Turchi(2), Eduard Barbu(3)(1)School of Electrical and Computer Engineering, University of Tehran, Iran(2)Fondazione Bruno Kessler, Trento, Italy(3)Translated srl, Rome, Italyjalili.masoud@ut.ac.ir{negri,turchi}@fbk.eueduard@translated.netAbstractWe address the problem of automaticallycleaning a large-scale Translation Mem-ory (TM) in a fully unsupervised fash-ion, i.e.
without human-labelled data.We approach the task by: i) designinga set of features that capture the similar-ity between two text segments in differ-ent languages, ii) use them to induce re-liable training labels for a subset of thetranslation units (TUs) contained in theTM, and iii) use the automatically labelleddata to train an ensemble of binary clas-sifiers.
We apply our method to clean atest set composed of 1,000 TUs randomlyextracted from the English-Italian versionof MyMemory, the world?s largest publicTM.
Our results show competitive perfor-mance not only against a strong baselinethat exploits machine translation, but alsoagainst a state-of-the-art method that relieson human-labelled data.1 IntroductionTranslation Memories (TMs) are one of the mainsources of knowledge supporting human transla-tion with the so-called Computer-assisted Transla-tion (CAT) tools.
A TM is a database that stores(source, target) segments called translation units(TUs).
These segments can be sub-sentential frag-ments, full sentences or even paragraphs in twolanguages and, ideally, they are perfect transla-tions of each other.
Their use in a CAT frameworkis based on computing a ?fuzzy match?
score be-tween an input sentence to be translated and theleft-hand side (the source) of each TU stored inthe TM.
If the score is above a certain threshold,the right-hand side (the target) is presented to theuser as a translation suggestion.
When translatinga document with a CAT tool, the user can storeeach translated (source, target) pair in the TM forfuture use.
Each newly added TU contributes tothe growth of the TM which, as time goes by, willbecome more and more useful to the user.
Dueto such constant growth, in which they evolve in-corporating users style and terminology, the so-called private TMs represent an invaluable assetfor individual translators and translation compa-nies.
Collaboratively-created public TMs grow ina less controlled way (e.g.
incorporating poten-tially noisy TUs supplied by anonymous contribu-tors or automatically extracted from the Web) butstill remain a practical resource for the translators?community at large.Together with the quantity, the quality of thestored material is a crucial factor that determinesthe usefulness of the TM and, all in all, its value.For this reason, the growth of the TM should gohand in hand with its continuous maintenance.This problem is usually addressed through man-ual (hence costly) revision, or by applying simple(hence approximate) automatic filtering routines.Advanced automatic methods for tidying up an ex-isting TM would contribute to reduce managementcosts, increase its quality, speed-up and simplifythe daily work of human translators.Focusing on TM maintenance, we explore anautomatic method to clean a large-scale TM byidentifying the TUs in which the target is a poortranslation of the source.
Its main strength isthe reliance on a fully unsupervised approach,which makes it independent from the availabilityof human-labelled data.
As it allows us to avoidthe burden of acquiring a (possibly large) set ofannotated TUs, our method is cost-effective andhighly portable across languages and TMs.
Thiscontrasts with supervised strategies like the onepresented in (Barbu, 2015) or those applied inclosely-related tasks such as cross-lingual seman-287ENGLISHITALIAN(EN translation)a traditional costumes of Icelandcostumi tradizionali dell?islanda(traditional costumes of iceland)b Active substances: per dose of 2 ml:Principi attivi Per ogni dose da 2 ml:(Active substances Per dose of 2 ml:)c The length of time of ...La durata delperiodo di ...(The length oftime of ...)d ... 4 weeks after administration ...... 4 settimane dopo la somministarzione ...(... 4 weeks after somministarzione ...)e 5. ensure the organization of ...5.5.f Read package leafletPer lo smaltimento leggere il foglio illustrativoFor disposal read the package leafletg beef chuck roastchuck carne assada?chuck meat ?assadah is an integral part of the contractrisultato della stagione(result of the season)Table 1: Examples of problematic translation units mined from the English-Italian version of MyMemory.tic textual similarity,1cross-lingual textual entail-ment (Negri et al, 2013), and quality estimation(QE) for MT (Specia et al, 2009; Mehdad et al,2012; C. de Souza et al, 2014; Turchi et al, 2014;C. de Souza et al, 2015).
Also most of the previ-ous approaches to bilingual data mining/cleaningfor statistical MT rely on supervised learning(Resnik and Smith, 2003; Munteanu and Marcu,2005; Jiang et al, 2009).
Unsupervised solutions,like the one proposed by Cui et al (2013) usuallyrely on redundancy-based approaches that rewardparallel segments containing phrase pairs that arefrequent in a training corpus.
This idea is well-motivated in the SMT framework but scarcely ap-plicable in the CAT scenario, in which it is crucialto manage and reward rare phrases as a source ofuseful suggestions for difficult translations.2 The problemWe consider as ?problematic TUs?
those contain-ing translation errors whose correction during thetranslation process can reduce translators?
produc-tivity.
Table 1 provides some examples extractedfrom the English-Italian training data recently re-leased for the NLP4TM 2016 shared task on clean-ing translation memories.2As can be seen in thetable, TU quality can be affected by a variety ofproblems.
These include: 1. minor formatting er-rors like the casing issue in example (a), the cas-ing+punctuation issue in (b) and the missing spacein (c), 2. misspelling errors like the one in (d),33.missing or extra words in the translation, as in (e)1http://alt.qcri.org/semeval2016/task1/2http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/3?somministARzione?
instead of ?somministRAzione?.and (f), 4. situations in which the translation isawkward (due to mistranslations and/or untrans-lated terms) like in (g) or it is completely unrelatedto the source sentence like in (h).Especially in the case of collaboratively-createdpublic TMs, these issues are rather frequent.
Forinstance, in the NLP4TM shared task trainingdata (randomly sampled from MyMemory) the in-stances affected by any of these error types areabout 38% of the total.3 MethodOur unsupervised TM cleaning method ex-ploits the independent views of three groups ofsimilarity-based features.
These allow us to in-fer a binary label for a subset of the TUs storedin a large-scale TM.
The inferred labels are usedto train an ensemble of binary classifiers, special-ized to capture different aspects of the general no-tion of translation quality.
Finally, the ensembleof classifiers is used to label the rest of the TM.To minimize overfitting issues, each base classi-fier exploits features that are different from thoseused to infer the label of the training instances.3.1 General workflowGiven a TM to be cleaned, our approach consistsof two main steps: i) label inference and ii) train-ing of the base classifiers.Label inference.
The first step aims to infer a re-liable binary label (1 or 0, respectively for ?good?and ?bad?)
for a subset Z of unlabelled TUs ran-domly selected from the input TM.
To this aim, thethree groups of features described in ?3.2 (say A,B, C) are first organised into combinations of two288groups (i.e.
AB, AC, BC).
As the features are dif-ferent in nature, each combination reflects a par-ticular ?view?
of the data, which is different fromthe other combinations.Then, for each TU in Z, we extract the fea-tures belonging to each combination.
Being de-signed and normalized to return a similarity scorein the [0-1] interval, the result of feature extrac-tion is a vector of numbers whose average valuecan be computed to sort each TU from the best(avg.
close to 1, indicating a high similarity be-tween source and target) to the worst (avg.
close to0).
This is done separately for each feature com-bination, so that the independent views they pro-vide will produce three different ranked lists forthe TUs in Z.Finally, the three ranked lists are processed toobtain different sets of positive/negative examples,whose variable size depends on the amount of TUstaken from the top and the bottom of the lists.Training of the base classifiers.
Each of thethree inferred annotations of Z (say z1, z2, z3) re-flects the specific view of the two groups of fea-tures used to obtain it (i.e.
AB for z1, AC for z2,BC for z3).
Based on each view, we train a binaryclassifier using the third group of features (i.e.
Cfor z1, B for z2, A for z3).
This results in threebase classifiers:?A,?B and?C that, in spite of thesame shared purpose, are by construction differentfrom each other.
This allows us to create an en-semble of base classifiers and to minimize the riskof overfitting, in which we would have incurredby training one single classifier with the same fea-tures (A,B,C) used as labelling criterion.3.2 FeaturesOur features capture different aspects of the sim-ilarity between the source and the target of a TU.The degree of similarity is mapped into a numericscore in the [0-1] interval.
The full set consists of31 features, which are organized in three groups.4Basic features (8).
This group represents aslightly improved variant of those proposed byBarbu (2015).
They aim to capture translationquality by looking at surface aspects, such as thepossible mismatches in the number of dates, num-bers, URLs and XML tags present in the sourceand target segments.5The consistency between4Implemented in TMop: https://github.com/hlt-mt/TMOP5Being these feature very sparse, we collapsed them intoa single one, which is set to 1 if any feature has value 1.the actual source and target languages and thoseindicated in the TM is also verified.
Languageidentification, carried out with the Langid tool(Lui and Baldwin, 2012), is a highly predictivefeature since sometimes the two languages are in-verted or even completely different.
Other featuresmodel the similarity between source and target bycomputing the direct and inverse ratio between thenumber of characters and words, as well as the av-erage word length in the two segments.
Finally,two features look at the presence of uncommoncharacter or word repetitions.QE-derived features (18).
This group containsfeatures borrowed from the closely-related task ofMT quality estimation, in which the complexity ofthe source, the fluency of the target and the ade-quacy between source and target are modeled asquality indicators.
Focusing on the adequacy as-pect, we exploit a subset of the features proposedby Camargo de Souza et al (2013).
They use wordalignment information to link source and targetwords and capture the quantity of meaning pre-served by the translation.
For each segment of aTU, word alignment information is used to calcu-late: i) the proportion of aligned and unalignedword n-grams (n=1,2), ii) the ratio between thelongest aligned/unaligned word sequence and thelength of the segment, iii) the average length ofthe aligned/unaligned word sequences, and iv) theposition of the first/last unaligned word, normal-ized by the length of the segment.
Word align-ment models were trained on the whole TM, usingMGIZA++ (Gao and Vogel, 2008).Word embeddings (5).
This is a newly devel-oped group of features that rely on cross-lingualword embeddings to identify ?good?
and ?bad?TUs.
Cross-lingual word embeddings provide acommon vector representation for words in dif-ferent languages and allow us to build featuresthat look at the same time at the source and tar-get segments.
Cross-lingual word embeddings arecomputed using the method proposed in (S?gaardet al, 2015).
Differently from the original pa-per, which takes advantage of bilingual documentsas atomic concepts to bridge the two languages,we use the TUs contained in the whole TM tobuild the embeddings.
Given a TU and a 100-dimensional vector representation of each word inthe source and target segments, the new featuresare: i) the cosine similarity between source and289target segment vectors obtained by averaging (orusing the median) the source and target word vec-tors; ii) the average embedding alignment scoreobtained by computing the cosine similarity be-tween each source word and all the target wordsand averaging over the largest cosine score of eachsource word; iii) the average cosine similarity be-tween source/target word alignments; iv) a scorethat merges features (ii) and (iii) by complement-ing word alignments (obtained using MGIZA++)with the alignments obtained from word embed-ding and averaging all the alignment weights.4 ExperimentsData.
We experiment with the English-Italianversion of MyMemory,6the world?s largest publicTM.
This collaboratively built TM contains about11M TUs coming from heterogeneous sources:aggregated private TMs or automatically extractedfrom the web/corpora, and anonymous contribu-tions of (source, target) bi-segments.
Being largeand free, the TM is of great utility for profes-sional translators.
Its uncontrolled sources, how-ever, call for accurate cleaning methods (e.g.
tomake it more accurate, smaller and manageable).From the TM we randomly extracted: i) subsets ofvariable size to automatically obtain training datafor the base classifiers and ii) a collection of 2,500TUs manually annotated with binary labels.
Dataannotation was done by two Italian native speakersproperly trained with the same guidelines preparedby the TM owner for periodic manual revisions.After agreement computation (Cohen?s kappa is0.7838), a reconciliation ended up with about 65%positive and 35% negative examples.
This pool israndomly split in two parts.
One (1,000 instances)is used as test set for our evaluation.
The other(1,500 instances) is used to replicate the approachof Barbu (2015) used as term of comparison.Learning algorithm.
Our base classifiers aretrained with the Extremely Randomized Trees al-gorithm (Geurts et al, 2006), optimized using 10-fold cross-validation in a randomized search pro-cess and combined in a majority voting schema.Evaluation metric.
To handle the imbalanced(65%-35%) data distribution, and equally rewardthe correct classification on both classes, we eval-uate performance in terms of balanced accuracy6https://mymemory.translated.net/(BA), computed as the average of the accuracieson the two classes (Brodersen et al, 2010).Terms of comparison.
We evaluate our ap-proach against two terms of comparison, bothstronger than the trivial random baseline achievinga BA of 50.0%.
The first competitor (MT-based)is a translation-based solution that exploits Bingtranslator7to render the source segment of a TUin the same language of the target.
Then, the sim-ilarity between the translated source and the tar-get segment is measured in terms of TranslationEdit Rate (TER (Snover et al, 2006)).
The TUis marked as ?good?
if the TER is smaller than0.4 (?bad?
otherwise).
This value is chosen basedon the findings of Turchi et al (2013), which sug-gests that only for TER values lower than 0.4 hu-man translators consider MT suggestions as goodenough for being post-editable.
In our scenario wehence assume that ?good?
TUs are those featuringa small TER distance between the target and anautomatic translation of the source.The second competitor (Barbu15) is the su-pervised approach proposed by Barbu (2015),which leverages human-labelled data to train anSVM binary classifier.
To the best of our knowl-edge, it represents the state-of-the-art in this task.1500 5000 10000 150000.690.70.710.720.730.740.750.760.770.780.79k (Training Set Size)Balanced AccuracyZ = 50KZ = 100KZ = 500KZ = 1MBarbu15MT?basedFigure 1: BA results as a function of Z and k.5 Results and DiscussionThe result of the ?label inference?
step describedin ?3.1 is a set of automatically labelled TUs totrain the base classifiers.
Positive and negativeexamples are respectively the top and the bottomk elements extracted from a list of TUs (of sizeZ) ranked according to the inferred similarity be-tween source and target.
In this process, the size7https://www.bing.com/translator/290of the list and the value of k clearly have influenceon the separability between the training instancesbelonging to the two classes.
Long lists and smallvalues of k will result in highly polarized trainingdata, with a very high similarity between the in-stances assigned to each class and feature valuesrespectively close to 1 and 0.
Vice-versa, shortlists and large values of k will result in less sepa-rable training data, with higher variability in thepoints assigned to each class and in the respec-tive feature values.
In light of this trade-off, weanalyse performance variations as a function of:i) the amount (Z) of data considered to initialisethe label inference step, and ii) the amount (k) oftraining instances used to learn the base classifiers.For the first dimension, we consider four values:50K (a value compatible with the size of mostof the existing TMs), 100K, 500K and 1M units(a value compatible only with a handful of large-scale TMs).
For the second dimension we ex-periment with four balanced training sets, respec-tively containing: 1.5K (the same amount used in(Barbu, 2015)), 5K, 10K and 15K instances.Figure 1 illustrates the performance of our TMcleaning method for different values of Z and k.Each of the four dashed learning curves refers toone of the four chosen values of Z. BA varia-tions for the same line are obtained by increasingthe number of training instances k and averagingover three random samples of size Z.
As can beseen from the figure, the results obtained by ourclassifiers trained with the inferred data alwaysoutperform the MT-based system and, in onecase (Z=50K, k=15K), also the Barbu15 classi-fier trained with human labelled data.8Consid-ering that all our training data are collected with-out any human intervention, hence eliminating theburden and the high costs of the annotation pro-cess, this is an interesting result.Overall, for the same value of k, smaller valuesof Z consistently show higher performance.
At thesame time, for the same value of Z, increasing kconsistently yields higher results.
Such improve-ments, however, are less evident when the poolof TUs used for the label inference step is larger(Z>100K).
These observations confirm the intu-ition that classifiers?
performance is highly influ-enced by the relation between the amount and thepolarization of the training data.
Indeed, looking8Improvements are statistically significant with ?
< 0.05,measured by approximate randomization (Noreen, 1989).at the average feature values used to infer the pos-itive and negative instances, we noticed that, forthe considered values of k, these scores are closerto 0 and 1 for the 1M curve than for the 50K curve.In the former case, highly polarized training datalimit the generalisation capability of the base clas-sifiers (and their ability, for instance, to correctlylabel the borderline test instances), which resultsin lower BA results.Nevertheless, it?s worth remarking that ourlarger value of k (15K) represents 30% of the datain the case of Z=50K, but just 1.5% of the datain case of Z=1M.
This suggests that for large val-ues of Z, more training points would be probablyneeded to introduce enough variance in the dataand improve over the almost flat curves shown inFigure 1.
Exploring this possibility was out of thescope of this initial analysis but would be doableby applying scalable algorithms capable to man-age larger quantities of training data (up to 300K,in the case of Z=1M).
For the time being, a sta-tistically significant improvement of ?1 BA pointover a supervised method in the most normal con-ditions (Z=50K) is already a promising step.6 ConclusionWe presented a fully unsupervised method to re-move useless TUs from a large-scale TM.
Focus-ing on the identification of wrongly translated seg-ments, we exploited the independent views of dif-ferent sets of features to: i) infer a binary labelfor a certain amount of TUs, and ii) use the au-tomatically labelled units as training data for anensemble of binary classifiers.
Such independentlabelling/training routines exploit the ?wisdom ofthe features?
to bypass the need of human anno-tations and obtain competitive performance.
Ourresults are not only better than a strong MT-basedbaseline, but they also outperform a state-of-the-art approach relying on human-labelled data.AcknowledgmentsThis work has been partially supported by the EC-funded projects ModernMT (H2020 grant agree-ment no.
645487) and EXPERT (FP7 grant agree-ment no.
317471).
The work carried out atFBK by Masoud Jalili Sabet was sponsored by theEAMT Summer internships 2015 program.
Theauthors would like to thank Anders S?gaard forsharing the initial version of the code for comput-ing word embeddings.291ReferencesEduard Barbu.
2015.
Spotting False TranslationSegments in Translation Memories.
In Proceed-ings of the Workshop Natural Language Processingfor Translation Memories, pages 9?16, Hissar, Bul-garia, September.Kay Henning Brodersen, Cheng Soon Ong, Klaas EnnoStephan, and Joachim M. Buhmann.
2010.
The Bal-anced Accuracy and Its Posterior Distribution.
InProceedings of the 2010 20th International Confer-ence on Pattern Recognition, ICPR ?10, pages 3121?3124, Istanbul, Turkey, August.Jos?e G. C. de Souza, Jes?us Gonz?alez-Rubio, Chris-tian Buck, Marco Turchi, and Matteo Negri.
2014.FBK-UPV-UEdin participation in the WMT14Quality Estimation shared-task.
In Proceedings ofthe Ninth Workshop on Statistical Machine Trans-lation, pages 322?328, Baltimore, Maryland, USA,June.Jos?e G. C. de Souza, Matteo Negri, Elisa Ricci, andMarco Turchi.
2015.
Online Multitask Learningfor Machine Translation Quality Estimation.
In Pro-ceedings of the 53rd Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 219?228, Beijing, China, July.Jos?e Guilherme Camargo de Souza, Christian Buck,Marco Turchi, and Matteo Negri.
2013.
FBK-UEdin Participation to the WMT13 Quality Esti-mation Shared Task.
In Proceedings of the EighthWorkshop on Statistical Machine Translation, pages352?358, Sofia, Bulgaria, August.Lei Cui, Dongdong Zhang, Shujie Liu, Mu Li, andMing Zhou.
2013.
Bilingual data cleaning forsmt using graph-based random walk.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Pa-pers), pages 340?345, Sofia, Bulgaria, August.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In In Proceedingsof the ACL 2008 Software Engineering, Testing, andQuality Assurance Workshop, pages 49?57, Colum-bus, Ohio, USA, June.Pierre Geurts, Damien Ernst, and Louis Wehenkel.2006.
Extremely randomized trees.
Machine learn-ing, 63(1):3?42.Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu,and Qingsheng Zhu.
2009.
Mining bilingual datafrom the web with adaptively learnt patterns.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP, pages 870?878, Suntec, Singapore, August.Marco Lui and Timothy Baldwin.
2012. langid.py: AnOff-the-shelf Language Identification Tool.
In Pro-ceedings of the ACL 2012 System Demonstrations,pages 25?30, Jeju Island, Korea, July.Yashar Mehdad, Matteo Negri, and Marcello Fed-erico.
2012.
Match without a Referee: Eval-uating MT Adequacy without Reference Transla-tions.
In Proceedings of the Machine TranslationWorkshop (WMT2012), pages 171?180, Montr?eal,Canada, June.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguis-tics, 31(4):477?504, December.Matteo Negri, Alessandro Marchetti, Yashar Mehdad,Luisa Bentivogli, and Danilo Giampiccolo.
2013.Semeval-2013 Task 8: Crosslingual Textual Entail-ment for Content Synchronization.
In Proceedingsof the 7th International Workshop on Semantic Eval-uation (SemEval 2013), pages 25?33, Atlanta, Geor-gia, USA, June.Erik W. Noreen.
1989.
Computer-intensive methodsfor testing hypotheses: an introduction.
Wiley Inter-science.Philip Resnik and Noah A. Smith.
2003.
The webas a parallel corpus.
Computational Linguistics,29(3):349?380.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In Proceedings of Association for MachineTranslation in the Americas, pages 223?231, Cam-bridge, Massachusetts, USA, August.Anders S?gaard,?Zeljko Agi?c, H?ector Mart?
?nez Alonso,Barbara Plank, Bernd Bohnet, and Anders Jo-hannsen.
2015.
Inverted indexing for cross-lingualNLP.
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Linguisticsand the 7th International Joint Conference on Natu-ral Language Processing (Volume 1: Long Papers),pages 1713?1722, Beijing, China, July.Lucia Specia, Nicola Cancedda, Marc Dymetman,Marco Turchi, and Nello Cristianini.
2009.
Estimat-ing the Sentence-level Quality of Machine Trans-lation Systems.
In Proceedings of the 13th An-nual Conference of the European Association forMachine Translation (EAMT-2009), pages 28?35,Barcelona, Spain.Marco Turchi, Matteo Negri, and Marcello Federico.2013.
Coping with the Subjectivity of HumanJudgements in MT Quality Estimation.
In Proceed-ings of the Eighth Workshop on Statistical MachineTranslation, pages 240?251, Sofia, Bulgaria, Au-gust.Marco Turchi, Antonios Anastasopoulos, Jos?e G. C. deSouza, and Matteo Negri.
2014.
Adaptive Qual-ity Estimation for Machine Translation.
In Proceed-ings of the 52nd Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 710?720, Baltimore, Maryland, USA,June.292
