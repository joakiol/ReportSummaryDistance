Proceedings of NAACL HLT 2009: Short Papers, pages 137?140,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsTaking into Account the Differences between Actively and PassivelyAcquired Data: The Case of Active Learning with Support Vector Machinesfor Imbalanced DatasetsMichael Bloodgood?Human Language TechnologyCenter of ExcellenceJohns Hopkins UniversityBaltimore, MD 21211 USAbloodgood@jhu.eduK.
Vijay-ShankerComputer and InformationSciences DepartmentUniversity of DelawareNewark, DE 19716 USAvijay@cis.udel.eduAbstractActively sampled data can have very differentcharacteristics than passively sampled data.Therefore, it?s promising to investigate usingdifferent inference procedures during AL thanare used during passive learning (PL).
Thisgeneral idea is explored in detail for the fo-cused case of AL with cost-weighted SVMsfor imbalanced data, a situation that arises formany HLT tasks.
The key idea behind theproposed InitPA method for addressing im-balance is to base cost models during AL onan estimate of overall corpus imbalance com-puted via a small unbiased sample rather thanthe imbalance in the labeled training data,which is the leading method used during PL.1 IntroductionRecently there has been considerable interest in us-ing active learning (AL) to reduce HLT annotationburdens.
Actively sampled data can have differ-ent characteristics than passively sampled data andtherefore, this paper proposes modifying algorithmsused to infer models during AL.
Since most AL re-search assumes the same learning algorithms will beused during AL as during passive learning1 (PL),this paper opens up a new thread of AL research thataccounts for the differences between passively andactively sampled data.The specific case focused on in this paper isthat of AL with SVMs (AL-SVM) for imbalanced?This research was conducted while the first author was aPhD student at the University of Delaware.1Passive learning refers to the typical supervised learningsetup where the learner does not actively select its training data.datasets2.
Collectively, the factors: interest in AL,widespread class imbalance for many HLT tasks, in-terest in using SVMs, and PL research showing thatSVM performance can be improved substantially byaddressing imbalance, indicate the importance of thecase of AL with SVMs with imbalanced data.Extensive PL research has shown that learningalgorithms?
performance degrades for imbalanceddatasets and techniques have been developed thatprevent this degradation.
However, to date, rela-tively little work has addressed imbalance during AL(see Section 2).
In contrast to previous work, thispaper advocates that the AL scenario brings out theneed to modify PL approaches to dealing with im-balance.
In particular, a new method is developedfor cost-weighted SVMs that estimates a cost modelbased on overall corpus imbalance rather than theimbalance in the so far labeled training data.
Sec-tion 2 discusses related work, Section 3 discussesthe experimental setup, Section 4 presents the newmethod called InitPA, Section 5 evaluates InitPA,and Section 6 concludes.2 Related WorkA problem with imbalanced data is that the classboundary (hyperplane) learned by SVMs can be tooclose to the positive (pos) examples and then recallsuffers.
Many approaches have been presented forovercoming this problem in the PL setting.
Manyrequire substantially longer training times or ex-2This paper focuses on the fundamental case of binary clas-sification where class imbalance arises because the positive ex-amples are rarer than the negative examples, a situation that nat-urally arises for many HLT tasks.137tra training data to tune parameters and thus arenot ideal for use during AL.
Cost-weighted SVMs(cwSVMs), on the other hand, are a promising ap-proach for use with AL: they impose no extra train-ing overhead.
cwSVMs introduce unequal cost fac-tors so the optimization problem solved becomes:Minimize:12?~w?2 + C+?i:yi=+1?i + C?
?i:yi=?1?i (1)Subject to:?k : yk [~w ?
~xk + b] ?
1?
?k, (2)where (~w, b) represents the learned hyperplane, ~xkis the feature vector for example k, yk is the labelfor example k, ?k = max(0, 1 ?
yk(~wk ?
~xk + b))is the slack variable for example k, and C+ and C?are user-defined cost factors.The most important part for this paper are the costfactors C+ and C?.
The ratio C+C?
quantifies theimportance of reducing slack error on pos train ex-amples relative to reducing slack error on negative(neg) train examples.
The value of the ratio is cru-cial for balancing the precision recall tradeoff well.
(Morik et al, 1999) showed that during PL, set-ting C+C?
= # of neg training examples# of pos training examples is an effec-tive heuristic.
Section 4 explores using this heuris-tic during AL and explains a modified heuristic thatcould work better during AL.
(Ertekin et al, 2007) propose using the balancingof training data that occurs as a result of AL-SVMto handle imbalance and do not use any further mea-sures to address imbalance.
(Zhu and Hovy, 2007)used resampling to address imbalance and based theamount of resampling, which is the analog of ourcost model, on the amount of imbalance in the cur-rent set of labeled train data, as PL approaches do.In contrast, the InitPA approach in Section 4 basesits cost models on overall (unlabeled) corpus imbal-ance rather than the amount of imbalance in the cur-rent set of labeled data.3 Experimental SetupWe use relation extraction (RE) and text classifica-tion (TC) datasets and SVMlight (Joachims, 1999)for training the SVMs.
For RE, we use AImed,previously used to train protein interaction extrac-tion systems ((Giuliano et al, 2006)).
As in previ-ous work, we cast RE as a binary classification taskFigure 1: Hyperplane B was trained with a higher C+C?ratio than hyperplane A was trained with.
(14.94% of the examples in AImed are positive).
Weuse the KGC kernel from (Giuliano et al, 2006), oneof the highest-performing systems on AImed to dateand perform 10-fold cross validation.
For TC, weuse the Reuters-21578 ModApte split.
Since a doc-ument may belong to more than one category, eachcategory is treated as a separate binary classificationproblem, as in (Joachims, 1998).
As in (Joachims,1998), we use the ten largest categories, which haveimbalances ranging from 1.88% to 29.96%.4 AL-SVM Methods for Addressing ClassImbalanceThe key question when using cwSVMs is how to setthe ratio C+C?
.
Increasing it will typically shift thelearned hyperplane so recall is increased and preci-sion is decreased (see Figure 1 for a hypothetical ex-ample).
Let PA= C+C?
.3 How should the PA be setduring AL-SVM?We propose two approaches: one sets the PAbased on the level of imbalance in the labeled train-ing data and one aims to set the PA based on an es-timate of overall corpus imbalance, which can dras-tically differ from the level of imbalance in activelysampled training data.
The first method is calledCurrentPA, depicted in Figure 2.
Note that in step0 of the loop, PA is set based on the distribution ofpositive and negative examples in the current set oflabeled data.
However, observe that during AL theratio # neg labeled examples# pos labeled examples in the current set of la-beled data gets skewed from the ratio in the entire3PA stands for positive amplification and gives us a conciseway to denote the fraction C+C?
, which doesn?t have a standardname.138Input:L = small initial set of labeled dataU = large pool of unlabeled dataLoop until stopping criterion is met:0.
Set PA = |{x?Labeled:f(x)=?1}||{x?L:f(x)=+1}|where f is the function we desire to learn.1.
Train an SVM with C+ and C?
set suchthat C+C?
= PA and obtain hyperplane h .42. batch?
select k points from U that areclosest to h and request their labels.53.
U = U ?
batch .4.
L = L ?
batch .End LoopFigure 2: The CurrentPA algorithm0 500 1000 1500 2000 2500 3000 3500 4000 4500 50001.522.533.544.555.5Empirical Evidence of CurrentPA creating a Skewed Distribution (Fold Avg)Number of Points for which Annotations Have Been RequestedRatioof#ofNegative to #ofPositivePointsRatio with CurrentPARatio with Entire SetFigure 3: Illustration of AL skewing the distribution ofpos/neg points on AImed.corpus because AL systematically selects the exam-ples that are closest to the current model?s hyper-plane and this tends to select more positive exam-ples than random selection would select (see also(Ertekin et al, 2007)).Empirical evidence of this distribution skew is il-lustrated in Figure 3.
The trend toward balanceddatasets during AL could mislead and cause us tounderestimate the PA.Therefore, our next algorithm aims to set the PAbased on the ratio of neg to pos instances in the en-tire corpus.
However, since we don?t have labels forthe entire corpus, we don?t know this ratio.
But byusing a small initial sample of labeled data, we can4We use SVMlight?s default value for C?.5In our experiments, batch size is 20.0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000202530354045505560AImed Average F Measure versus Number of AnnotationsNumber of Points for which Annotations Have Been RequestedPerformance(FMeasure)InitPAOversampling(Zhu and Hovy,2007)CurrentPAEHG2007(Ertekin et al 2007)Figure 4: AImed learning curves.
y-axis is from 20% to60%.estimate this ratio with high confidence.
This esti-mate can then be used for setting the PA throughoutthe AL process.
We call this method of setting thePA based on a small initial set of labeled data theInitPA method.
It is like CurrentPA except we moveStep 0 to be executed one time before the loop andthen use that same PA value on each iteration of theAL loop.To guide what size to make the initial set of la-beled data, one can determine the sample size re-quired to estimate the proportion of positives in afinite population to within sampling error e with adesired level of confidence using standard statisti-cal techniques found in many college-level statisticsreferences such as (Berenson et al, 1988).
For ex-ample, carrying out the computations on the AImeddataset shows that a size of 100 enables us to be95% confident that our proportion estimate is within0.0739 of the true proportion.
In our experiments,we used an initial labeled set of size 100.5 EvaluationIn addition to InitPA and CurrentPA, we also imple-mented the methods from (Ertekin et al, 2007; Zhuand Hovy, 2007).
We implemented oversampling byduplicating points and by BootOS (Zhu and Hovy,2007).
To avoid cluttering the graphs, we only showthe highest-performing oversampling variant, whichwas by duplicating points.
Learning curves are pre-sented in Figures 4 and 5.Note InitPA is the highest-performing method forall datasets, especially in the practically importantarea of where the learning curves begin to plateau.1390 1000 2000 3000 4000 5000 6000 7000 8000 9000 100007677787980818283Reuters Average F Measure versus Number of AnnotationsNumber of Points for which Annotations Have Been RequestedPerformance(FMeasure)InitPAOversampling(Zhu and Hovy,2007)CurrentPAEHG2007(Ertekin et al 2007)Figure 5: Reuters learning curves.
y-axis is from 76% to83%.This area is important because this is around wherewe would want to stop AL (Bloodgood and Vijay-Shanker, 2009).Observe that the gains of InitPA over CurrentPAare smaller for Reuters.
For some Reuters cate-gories, InitPA and CurrentPA have nearly identicalperformance.
Applying the models learned by Cur-rentPA at each round of AL on the data used totrain the model reveals that the recall on the train-ing data is nearly 100% for those categories whereInitPA/CurrentPA perform similarly.
Increasing therelative penalty for slack error on positive trainingpoints will not have much impact if (nearly) all ofthe pos train points are already classified correctly.Thus, in situations where models are already achiev-ing nearly 100% recall on their train data, InitPA isnot expected to outperform CurrentPA.The hyperplanes learned during AL-SVM servetwo purposes: sampling - they govern which unla-beled points will be selected for human annotation,and predicting - when AL stops, the most recentlylearned hyperplane is used for classifying test data.Although all AL-SVM approaches we?re aware ofuse the same hyperplane at each round of AL forboth of these purposes, this is not required.
We com-pared InitPA with hybrid approaches where hyper-planes trained using an InitPA cost model are usedfor sampling and hyperplanes trained using a Cur-rentPA cost model are used for predicting, and vice-versa, and found that InitPA performed better thanboth of these hybrid approaches.
This indicates thatthe InitPA cost model yields hyperplanes that arebetter for both sampling and predicting.6 ConclusionsWe?ve made the case for the importance of AL-SVMfor imbalanced datasets and showed that the AL sce-nario calls for modifications to PL approaches to ad-dressing imbalance.
For AL-SVM, the key idea be-hind InitPA is to base cost models on an estimate ofoverall corpus imbalance rather than the class imbal-ance in the so far labeled data.
The practical utilityof the InitPA method was demonstrated empirically;situations where InitPA won?t help that much weremade clear; and analysis showed that the sources ofInitPA?s gains were from both better sampling andbetter predictive models.InitPA is an instantiation of a more general ideaof not using the same inference algorithms duringAL as during PL but instead modifying inference al-gorithms to suit esoteric characteristics of activelysampled data.
This is an idea that has seen relativelylittle exploration and is ripe for further investigation.ReferencesMark L. Berenson, David M. Levine, and David Rind-skopf.
1988.
Applied Statistics.
Prentice-Hall, Engle-wood Cliffs, NJ.Michael Bloodgood and K. Vijay-Shanker.
2009.
Amethod for stopping active learning based on stabiliz-ing predictions and the need for user-adjustable stop-ping.
In CoNLL.Seyda Ertekin, Jian Huang, Le?on Bottou, and C. LeeGiles.
2007.
Learning on the border: active learningin imbalanced data classification.
In CIKM.Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.2006.
Exploiting shallow linguistic information for re-lation extraction from biomedical literature.
In EACL.Thorsten Joachims.
1998.
Text categorization with su-port vector machines: Learning with many relevantfeatures.
In ECML, pages 137?142.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In Advances in Kernel Methods ?Support Vector Learning, pages 169?184.Katharina Morik, Peter Brockhausen, and ThorstenJoachims.
1999.
Combining statistical learning with aknowledge-based approach - a case study in intensivecare monitoring.
In ICML, pages 268?277.Jingbo Zhu and Eduard Hovy.
2007.
Active learning forword sense disambiguation with methods for address-ing the class imbalance problem.
In EMNLP-CoNLL.140
