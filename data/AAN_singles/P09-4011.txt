Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 41?44,Suntec, Singapore, 3 August 2009.c?2009 ACL and AFNLPCombining POMDPs trained with User Simulations andRule-based Dialogue Management in a Spoken Dialogue SystemSebastian Varges, Silvia Quarteroni, Giuseppe Riccardi, Alexei V. Ivanov, Pierluigi RobertiDepartment of Information Engineering and Computer ScienceUniversity of Trento38050 Povo di Trento, Italy{varges|silviaq|riccardi|ivanov|roberti}@disi.unitn.itAbstractOver several years, we have developed anapproach to spoken dialogue systems thatincludes rule-based and trainable dialoguemanagers, spoken language understandingand generation modules, and a compre-hensive dialogue system architecture.
Wepresent a Reinforcement Learning-baseddialogue system that goes beyond standardrule-based models and computes on-linedecisions of the best dialogue moves.
Thekey concept of this work is that we bridgethe gap between manually written dia-log models (e.g.
rule-based) and adaptivecomputational models such as PartiallyObservable Markov Decision Processes(POMDP) based dialogue managers.1 Reinforcement Learning-basedDialogue ManagementIn recent years, Machine Learning techniques,in particular Reinforcement Learning (RL), havebeen applied to the task of dialogue management(DM) (Levin et al, 2000; Williams and Young,2006).
A major motivation is to improve robust-ness in the face of uncertainty, for example dueto speech recognition errors.
A further motivationis to improve adaptivity w.r.t.
different user be-haviour and application/recognition environments.The Reinforcement Learning framework is attrac-tive because it offers a statistical model represent-ing the dynamics of the interaction between sys-tem and user.
This is in contrast to the super-vised learning approach of learning system be-haviour based on a fixed corpus (Higashinaka etal., 2003).
To explore the range of dialogue man-agement strategies, a simulation environment isrequired that includes a simulated user (Schatz-mann et al, 2006) if one wants to avoid the pro-hibitive cost of using human subjects.We demonstrate the various parameters that in-fluence the learnt dialogue management policy byusing pre-trained policies (section 4).
The appli-cation domain is a tourist information system foraccommodation and events in the local area.
Thedomain of the trained DMs is identical to that of arule-based DM that was used by human users (sec-tion 2), allowing us to compare the two directly.The state of the POMDP keeps track of the SLUhypotheses in the form of domain concepts (10 inthe application domain, e.g.
main activity, star rat-ing of hotels, dates etc.)
and their values.
Thesevalues may be abstracted into ?known/unknown,?for example, increasing the likelihood that the sys-tem re-visits a dialogue state which it can exploit.Representing the verification status of the con-cepts in the state, influences ?
in combination withthe user model (section 1.2) and N best hypotheses?
if the system learns to use clarification questions.1.1 The exploration/exploitation trade-off inreinforcement learningThe RL-DM maintains a policy, an internal datastructure that keeps track of the values (accumu-lated rewards) of past state-action pairs.
The goalof the learner is to optimize the long-term rewardby maximizing the ?Q-Value?Qpi(st, a) of a policypi for taking action a at time t. The expected cu-mulative value V of a state s is defined recursivelyas Vpi(st) =?api(st, a)?st+1Past,st+1[Rast,st+1+ ?Vpi(st+1)].Since an analytic solution to finding an optimalvalue function is not possible for realistic dialoguescenarios, V (s) is estimated by dialogue simula-tions.To optimize Q and populate the policy with ex-pected values, the learner needs to explore un-tried actions (system moves) to gain more expe-riences, and combine this with exploitation of the41???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
?0 2000 4000 6000 8000 10000?8?6?4?20x8greedy0.0_fixed_error_sessions10000_maxsessionlength4_runs10reward# sessions(a) 0% exploration, 100% exploitation: learner does not findoptimal dialogue strategy????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
?0 2000 4000 6000 8000 10000?8?6?4?20x8greedy0.2_fixed_error_sessions10000_maxsessionlength4_runs10# sessionsreward(b) 20% exploration, 80% exploitation: noticeable increase inreward, hitting upper boundFigure 1: Exploration/exploitation trade-offalready known successful actions to also ensurehigh reward.
In principle there is no distinctionbetween training and testing.
Learning in the RL-based dialogue manager is strongly dependent onthe chosen exploration/exploitation trade-off.
Thisis determined by the action selection policy, whichfor each system turn decides probabilistically (-greedy, softmax) if to exploit the currently knownbest action of the policy for the believed dialoguestate, or to explore an untried action.
Figure 1(a)shows, for a subdomain of the application domain,how the reward (expressed as minimizing costs)reaches an upper bound early during 10,000 sim-ulated dialogue sessions (each dot represents theaverage of 10 rewards at a particular session num-ber).
Note that if the policy provides no matchingstate, the system can only explore, and thus a cer-tain amount of exploration always takes place.
Incontrast, with exploration the system is able to findlower cost solutions (figure 1(b)).1.2 User SimulationIn order to conduct thousands of simulated dia-logues, the DM needs to deal with heterogeneousbut plausible user input.
For this purpose, we havedesigned a User Simulator (US) which bootstrapslikely user behaviors starting from a small corpusof 74 in-domain dialogs, acquired using the rule-based version of the SDS (section 2).
The task ofthe US is to simulate the output of the SLU mod-ule to the DM, hence providing it with a rankedlist of SLU hypotheses.A list of possible user goals is stored in adatabase table (section 3) using a frame/slot rep-resentation.
For each simulated dialogue, one ormore user goals are randomly selected.
The UserSimulator?s task is to mimic a user wanting to per-form such task(s).
At each turn, the US mines theprevious system dialog act to obtain the conceptsrequired by the DM and obtains the correspondingvalues (if any) from the current user goal.The output of the user model proper is passedto an error model that simulates the ?noisy chan-nel?
recognition errors based on statistics from thedialogue corpus.
These concern concept values aswell as other dialogue phenomena such as noIn-put, noMatch and hangUp.
If the latter phenomenaoccur, they are propagated to the DM directly; oth-erwise, the following US step is to attach plausibleconfidences to concept-value pairs, also based onthe dialogue corpus.
Finally, concept-value pairsare combined in an SLU hypothesis and, as in theregular SLU module, a cumulative utterance-levelconfidence is computed, determining the rank ofeach of the n hypotheses.
The probability of agiven concept-value observation at time t+1 giventhe system act at time t, named as,t, and the ses-sion user goal gu, P (ot+1|as,t, gu), is obtained bycombining the error model and the user model:P (ot+1|au,t+1) ?
P (au,t+1|as,t, gu)where au,t+1is the true user action.2 Rule-based Dialogue ManagementA rule-based dialogue manager was developed as ameaningful comparison to the trained DM, to ob-tain training data from human-system interactionfor the user simulator, and to understand the prop-erties of the domain (Varges et al, 2008).
Rule-based dialog management works in two stages:retrieving and preprocessing facts (tuples) takenfrom a dialogue state database (section 3), andinferencing over those facts to generate a systemresponse.
We distinguish between the ?contextmodel?
of the first phase ?
essentially allowing42more recent values for a concept to override lessrecent ones ?
and the ?dialog move engine?
(DME)of the second phase.
In the second stage, accep-tor rules match SLU results to dialogue context,for example perceived user concepts to open ques-tions.
This may result in the decision to verify theapplication parameter in question, and the actionis verbalized by language generation rules.
If theparameter is accepted, application dependent taskrules determine the next parameter to be acquired,resulting in the generation of an appropriate re-quest.3 Data-centric System ArchitectureAll data is continuously stored in a database whichweb-service based processing modules (such asSLU, DM and language generation) access.
Thisarchitecture also allows us to access the databasefor immediate visualization.
The system presentsan example of a ?thick?
inter-module informa-tion pipeline architecture.
Individual componentsexchange data by means of sets of hypothesescomplemented by the detailed conversational con-text.
The database concentrates heterogeneoustypes of information at various levels of descrip-tion in a uniform way.
This facilitates dialog eval-uation, data mining and online learning becausedata is available for querying as soon as it hasbeen stored.
There is no need for separate loggingmechanisms.
Multiple systems/applications areavailable on the same infrastructure due to a cleanseparation of its processing modules (SLU, DM,NLG etc.)
from data storage (DBMS), and moni-toring/analysis/visualization and annotation tools.4 Visualization ToolWe developed a live web-based dialogue visual-ization tool that displays ongoing and past di-alogue utterances, semantic interpretation confi-dences and distributions of confidences for incom-ing user acts, the dialogue manager state, andpolicy-based decisions and updating.
An exam-ple of the visualization tool is given in figures 3(dialogue logs) and 4 (annotation view).
We arecurrently extending the visualization tool to dis-play the POMDP-related information that is al-ready present in the dialogue database.The visualization tool shows how our dedicatedSLU module produces a number of candidate se-mantic parses using the semantics of a domain on-tology and the output of ASR.The visualization of the internal representationof the POMDP-DM includes the N best dialoguestates after each user utterance and the rerankingof the action set.
At the end of each dialogue ses-sion, the reward and the policy updates are shown,i.e.
new or updated state entries and action val-ues.
Another plot relates the current dialogue?sreward to the reward of previous dialogues (as inplots 1(b) and 1(a)).Users are able to talk with several systems(via SIP phone connection to the dialogue systemserver) and see their dialogues in the visualizationtool.
They are able to compare the rule-basedsystem, a randomly exploring learner that hasnot been trained yet, and several systems thatuse various pre-trained policies.
These policiesare obtained by dialogue simulations with usermodels based on data obtained from human-machine dialogues with the original rule-baseddialogue manager.
The web tool is availableat http://cicerone.dit.unitn.it/DialogStatistics/.AcknowledgmentsThis work was partially supported by the Euro-pean Commission Marie Curie Excellence Grantfor the ADAMACH project (contract No.
022593)and by LUNA STREP project (contract No.33549).ReferencesR.
Higashinaka, M. Nakano, and K. Aikawa.
2003.Corpus-based discourse understanding in spoken di-alogue systems.
In ACL-03, Sapporo, Japan.E.
Levin, R. Pieraccini, and W. Eckert.
2000.
Astochastic model of human-machine interaction forlearning dialog strategies.
IEEE Transactions onSpeech and Audio Processing, 8(1).J.
Schatzmann, K. Weilhammer, M. Stuttle, andS.
Young.
2006.
A Survey of Statistical User Sim-ulation Techniques for Reinforcement-Learning ofDialogue Management Strategies.
Knowledge En-gineering Review, 21(2):97?126.S.
Varges, G. Riccardi, and S. Quarteroni.
2008.
Per-sistent Information State in a Data-Centric Architec-ture.
In SIGDIAL-08, Columbus, Ohio.J.
D. Williams and S. Young.
2006.
Partially Ob-servable Markov Decision Processes for Spoken Di-alog Systems.
Computer Speech and Language,21(2):393?422.43ASRTTSTurnSetupDBSLUDMNLGhttp-reqhttp-reqhttp-reqhttp-reqIdsVXMLpageASR resultsHTTP requestSLU resultsDM context/resultsVXMLgenhttp-reqNLG context/results(a) Turn-level information flow in the data-centric SDS ar-chitectureDBSimulationEnvironmentDMNLGUser ModelError ModelUser GoalsCorpus(b) User simulator interface with the dialogue managerFigure 2: Architecture for interacting with human user (left) and simulated user (right)Figure 3: Left pane: overview of all dialogues.
Right pane: visualization of a system opening prompt fol-lowed by the user?s activity request.
All distinct SLU hypotheses (concept-value combinations) derivingfrom ASR are ranked based on concept-level confidence (2 in this turn).Figure 4: Turn annotation of task success based on previously filled dialog transcriptions (left box).44
