Learning Dependency Translation Modelsas Collections of Finite-State HeadTransducersHiyan Alshawi*Shannon Laboratory, AT&T LabsShona Douglas*Shannon Laboratory, AT&T LabsSrinivas Bangalore*Shannon Laboratory, AT&T LabsThe paper defines weighted head transducers,finite-state machines that perform middle-out stringtransduction.
These transducers are strictly more expressive than the special case of standard left-to-right finite-state transducers.
Dependency transduction models are then defined as collectionsof weighted head transducers that are applied hierarchically.
A dynamic programming searchalgorithm is described for finding the optimal transduction of an input string with respect to adependency transduction model.
A method for automatically training a dependency transduc-tion model from a set of input-output example strings is presented.
The method first searchesfor hierarchical alignments of the training examples guided by correlation statistics, and thenconstructs the transitions of head transducers that are consistent with these alignments.
Experi-mental results are given for applying the training method to translation from English to Spanishand Japanese.1.
IntroductionWe will define a dependency transduction model in terms of a collection of weightedhead transducers.
Each head transducer is a finite-state machine that differs from"standard" finite-state transducers in that, instead of consuming the input string leftto right, it consumes it "middle out" from a symbol in the string.
Similarly, the outputof a head transducer is built up middle out at positions relative to a symbol in theoutput string.
The resulting finite-state machines are more expressive than standardleft-to-right transducers.
In particular, they allow long-distance movement with fewerstates than a traditional finite-state ransducer, a useful property for the translation taskto which we apply them in this paper.
(In fact, finite-state head transducers are capableof unbounded movement with a finite number of states.)
In Section 2, we introducehead transducers and explain how input-output positions on state transitions resultin middle-out transduction.When applied to the problem of translation, the head transducers forming the de-pendency transduction model operate on input and output strings that are sequencesof dependents of corresponding headwords in the source and target languages.
Thedependency transduction model produces ynchronized dependency trees in whicheach local tree is produced by a head transducer.
In other words, the dependency* 180 Park Avenue, Florham Park, NJ 07932t 180 Park Avenue, Florham Park, NJ 07932180 Park Avenue, Florham Park, NJ 07932@ 2000 Association for Computational LinguisticsComputational Linguistics Volume 26, Number 1model applies the head transducers ecursively, imposing a recursive decompositionof the source and target strings.
A dynamic programming search algorithm finds op-timal (lowest total weight) derivations of target strings from input strings or wordlattices produced by a speech recognizer.
Section 3 defines dependency transductionmodels and describes the search algorithm.We construct the dependency transduction models for translation automaticallyfrom a set of unannotated examples, each example comprising a source string and acorresponding target string.
The recursive decomposition of the training examplesresults from an algorithm for computing hierarchical alignments of the examples,described in Section 4.2.
This alignment algorithm uses dynamic programming searchguided by source-target word correlation statistics as described in Section 4.1.Having constructed a hierarchical alignment for the training examples, a set ofhead transducer t ansitions are constructed from each example as described in Sec-tion 4.3.
Finally, the dependency transduction model is constructed by aggregating theresulting head transducers and assigning transition weights, which are log probabili-ties computed from the training counts by simple maximum likelihood estimation.We have applied this method of training statistical dependency transduction mod-els in experiments on English-to-Spanish and English-to-Japanese translations of tran-scribed spoken utterances.
The results of these experiments are described in Section 5;our concluding remarks are in Section 6.2.
Head Transducers2.1 Weighted Finite-State Head TransducersIn this section we describe the basic structure and operation of a weighted head trans-ducer.
In some respects, this description is simpler than earlier presentations (e.g.,Alshawi 1996); for example, here final states are simply a subset of the transducerstates whereas in other work we have described the more general case in which finalstates are specified by a probability distribution.
The simplified escription is adequatefor the purposes of this paper.Formally, aweighted head transducer is a 5-tuple: an alphabet W of input symbols;an alphabet V of output symbols; a finite set Q of states q0 .
.
.
.
.
qs; a set of final statesF c Q; and a finite set T of state transitions.
A transition from state q to state q' hasthe form(q,q',w,v,o~,fl, clwhere w is a member of W or is the empty string c; v is a member of V or ?
; the integero~ is the input position; the integer fl is the output position; and the real number c isthe weight or cost of the transition.
A transition in which oz = 0 and fl = 0 is called ahead transition.The interpretation f q, q', w, and v in transitions i similar to left-to-right transduc-ers, i.e., in transitioning from state q to state qt, the transducer "reads" input symbolw and "writes" output symbol v, and as usual if w (or v) is e then no read (respec-tively write) takes place for the transition.
The difference lies in the interpretation fthe read position c~ and the write position ft. To interpret the transition positions astransducer actions, we consider notional input and output apes divided into squares.On such a tape, one square is numbered 0,and the other squares are numbered 1,2 .
.
.
.rightwards from square 0, and -1 , -2  .
.
.
.
leftwards from square 0 (Figure 1).A transition with input position ~ and output position fl is interpreted as readingw from square c~ on the input tape and writing v to square fl of the output tape; ifsquare fl is already occupied, then v is written to the next empty square to the left of46Alshawi, Bangalore, and Douglas Learning Dependency Translation Models<q, q' ,  w, v, a, fl,, c>@ o p -@CI lw l  w0 I I-4 -3a,=--2-1 0 1 2 3 4-4 -3 -2 -1 0 1 2 ,0=3 4Figure 1Transition symbols and positions.fl if fl < 0, or to the right of fl if fl > 0, and similarly, if input was already read fromposition a, w is taken from the next unread square to the left of a if a < 0 or to theright of c~ if a ~ 0.The operation of a head transducer is nondeterministic.
It starts by taking a headtransition{q, q', w0, v0, 0, 0, c}where w0 is one of the symbols (not necessarily the leftmost) in the input string.
(Thevalid initial states are therefore implicitly defined as those with an outgoing headtransition.)
w0 is considered to be at square 0 of the input tape and v0 is output atsquare 0 of the output tape.
Further state transitions may then be taken until a finalstate in F is reached.
For a derivation to be valid, it must read each symbol in theinput string exactly once.
At the end of a derivation, the output string is formed bytaking the sequence of symbols on the target ape, ignoring any empty squares on thistape.The cost of a derivation of an input string to an output string by a weightedhead transducer is the sum of the costs of transitions taken in the derivation.
We cannow define the string-to-string transduction function for a head transducer to be thefunction that maps an input string to the output string produced by the lowest-costvalid derivation taken over all initial states and initial symbols.
(Formally, the functionis partial in that it is not defined on an input when there are no derivations or whenthere are multiple outputs with the same minimal cost.
)In the transducers produced by the training method described in this paper, thesource and target positions are in the set {-1,0,1},  though we have also used hand-coded transducers (Alshawi and Xia 1997) and automatically trained transducers (A1-shawl and Douglas 2000) with a larger range of positions.2.2 Relationship to Standard FSTsThe operation of a traditional eft-to-right ransducer can be simulated by a headtransducer by starting at the leftmost input symbol and setting the positions of thefirst transition taken to a = 0 and fl = 0, and the positions for subsequent transitionsto o~ = 1 and fl = 1.
However, we can illustrate the fact that head transducers are more47Computational Linguistics Volume 26, Number 1a:aa:a ~ b:b0:0Figure 2Head transducer to reverse an input string of arbitrary length in the alphabet {a, b}.expressive than left-to-right transducers by the case of a finite-state head transducerthat reverses a string of arbitrary length.
(This cannot be performed by a traditionaltransducer with a finite number of states.
)For example, the head transducer described below (and shown in Figure 2) withinput alphabet {a, b} will reverse an input string of arbitrary length in that alphabet.The states of the example transducer are Q = {ql, q2} and F = {q2}, and it has thefollowing transitions (costs are ignored here):{ql, q2,a,a,O,O}<ql, q2, b, b, 0, 0><q2,q2,a,a,-1,1}(q2, q2, b, b, -1,1}The only possible complete derivations of the transducer read the input string rightto left, but write it left to right, thus reversing the string.Another similar example is using a finite-state head transducer to convert a palin-drome of arbitrary length into one of its component halves.
This clearly requires theuse of an empty string on some of the output transitions.3.
Dependency Transduction Models3.1 Dependency Transduction using Head TransducersIn this section we describe dependency transduction models, which can be used formachine translation and other transduction tasks.
These models consist of a collectionof head transducers that are applied hierarchically.
Applying the machines hierarchi-cally means that a nonhead transition is interpreted not simply as reading an input-output pair (w, v), but instead as reading and writing a pair of strings headed by (w, v)according to the derivation of a subnetwork.For example, the head transducer shown in Figure 3 can be applied recursively inorder to convert an arithmetic expression from infix to prefix (Polish) notation (as notedby Lewis and Stearns \[1968\], this transduction cannot be performed by a pushdowntransducer).In the case of machine translation, the transducers derive pairs of dependencytrees, a source language dependency tree and a target dependency tree.
A dependencytree for a sentence, in the sense of dependency grammar (for example Hays \[1964\] andHudson \[1984\]), is a tree in which the words of the sentence appear as nodes (we donot have terminal symbols of the kind used in phrase structure grammar).
In such atree, the parent of a node is its head and the child of a node is the node's dependent.The source and target dependency trees derived by a dependency transductionmodel are ordered, i.e., there is an ordering on the nodes of each local tree.
This48Alshawi, Bangalore, and Douglas Learning Dependency Translation Modelsb b ~C~ b:b b:bFigure 3Dependency transduction network mapping bracketed arithmetic expressions from infix toprefix notation.I I want to make a collect call I?
,\[ quiero hac~ una llamada de cobr~ IFigure 4Synchronized dependency trees derived for transducing I want to make a collect call into quierohacer una llamada de cobrar.means, in particular, that the target sentence can be constructed directly by a simplerecursive traversal of the target dependency tree.
Each pair of source and target reesgenerated is synchronized in the sense to be formalized in Section 4.2.
An example isgiven in Figure 4.Head transducers and dependency transduction models are thus related as fol-lows: Each pair of local trees produced by a dependency transduction derivation is theresult of a head transducer derivation.
Specifically, the input to such a head transduceris the string corresponding to the flattened local source dependency tree.
Similarly, theoutput of the head transducer derivation is the string corresponding to the flattenedlocal target dependency tree.
In other words, the head transducer is used to converta sequence consisting of a headword w and its left and right dependent words to asequence consisting of a target word v and its left and right dependent words (Fig-ure 5).
Since the empty string may appear in a transition in place of a source or targetsymbol, the number of source and target dependents can be different.The cost of a derivation produced by a dependency transduction model is thesum of all the weights of the head transducer derivations involved.
When applying adependency transduction model to language translation, we choose the target stringobtained by flattening the target ree of the lowest-cost dependency derivation thatalso generates the source string.We have not yet indicated what weights to use for head transducer t ansitions.The definition of head transducers as such does not constrain these.
However, for adependency transduction model to be a statistical model for generating pairs of strings,we assign transition weights that are derived from conditional probabilities.
Several49Computational Linguistics Volume 26, Number 1Iw1 ..- wk.ll ~?1 ..-'~nlIv ,Figure 5Head transducer converts the sequences of left and right dependents (wl ... wk-l/ and(wk+i ?
?
?
w,) of w into left and right dependents (vl... vj-1) and {Vj+I... Vp) of v.probabilistic parameterizations can be used for this purpose including the followingfor a transition with headwords w and v and dependent words w' and v':P(q', w', v', fllw, v, q).Here q and q' are the from-state and to-state for the transition and a and fl are thesource and target positions, as before.
We also need parameters P(q0, ql\]w, v) for theprobability of choosing a head transition(qo, ql, w,v,O,O)given this pair of headwords.
To start the derivation, we need parametersP(roots(wo, vo)) for the probability of choosing w0,v0 as the root nodes of the twotrees.These model parameters can be used to generate pairs of synchronized epen-dency trees starting with the topmost nodes of the two trees and proceeding recur-sively to the leaves.
The probability of such a derivation can be expressed as:P( oots(wo, vo) )P(Dwo,vo)where P(Dw,v) is the probability of a subderivation headed by w and v, that isP(Dw,v) = P(qo, qllw, v) H P(qi+l, Wi, Vi,~i, fli\]w,v, qi)P(Dwi,vl)1K i lnfor a derivation in which the dependents of w and v are generated by n transitions.3.2 Transduction AlgorithmTo carry out translation with a dependency transduction model, we apply a dynamicprogramming search to find the optimal derivation.
This algorithm can take as inputeither word strings, or word lattices produced by a speech recognizer.
The algorithmis similar to those for context-free parsing such as chart parsing (Earley 1970) andthe CKY algorithm (Younger 1967).
Since word string input is a special case of wordlattice input, we need only describe the case of lattices.We now present a sketch of the transduction algorithm.
The algorithm worksbottom-up, maintaining a set of configurations.
A configuration has the formIn1, n2, w, v, q, c, t\]corresponding to a bottom-up artial derivation currently in state q covering an inputsequence between nodes nl and n2 of the input lattice, w and v are the topmost50Alshawi, Bangalore, and Douglas Learning Dependency Translation Modelsnodes in the source and target derivation trees.
Only the target ree t is stored in theconfiguration.The algorithm first initializes configurations for the input words, and then per-forms transitions and optimizations to develop the set of configurations bottom-up:Initialization: For each word edge between odes n and n ~ in the latticewith source word w0, an initial configuration is constructed for any headtransition of the form(q, q', w0, v0, 0, 0, c}Such an initial configuration has the form:\[n, n t, w0, v0, q~, c, v0\]Transition: We show the case of a transition in which a new configurationresults from consuming a source dependent wl to the left of a headwordw and adding the corresponding target dependent Vl to the right of thetarget head v. Other cases are similar.
The transition applied is:(q, q~, Wl, Vl, -1,1, c'}It is applicable when there are the following head and dependentconfigurations:\[n2,n3,w,v,q,c,t\]\[nl, n2, Wl, Vl, qf, Cl, tl\]where the dependent configuration is in a final state qf.
The result ofapplying the transition is to add the following to the set ofconfigurations:In1, n3, w, v, q', c + Cl q- C', t'\]where Y is the target dependency tree formed by adding tl as therightmost dependent of t.Optimization: We also require a dynamic programming condition toremove suboptimal (sub)derivations.
Whenever there are twoconfigurations\[n, n', w, v, q, Cl, tl\]\[n, n', w, v, q, C2, t2\]and c2 > Cl, the second configuration is removed from the set ofconfigurations.If, after all applicable transitions have been taken, there are configurations span-ning the entire input lattice, then the one with the lowest cost is the optimal derivation.When there are no such configurations, we take a pragmatic approach in the trans-lation application and simply concatenate the lowest costing of the minimal lengthsequences of partial derivations that span the entire lattice.
A Viterbi-like search ofthe graph formed by configurations i used to find the optimal sequence of deriva-tions.
One of the advantages ofmiddle-out transduction is that robustness i improvedthrough such use of partial derivations when no complete derivations are available.51Computational Linguistics Volume 26, Number 14.
Training MethodOur training method for head transducer models only requires a set of training exam-ples.
Each example, or bitext, consists of a source language string paired with a targetlanguage string.
In our experiments, the bitexts are transcriptions of spoken Englishutterances paired with their translations into Spanish or Japanese.It is worth emphasizing that we do not necessarily expect he dependency repre-sentations produced by the training method to be traditional dependency structuresfor the two languages.
Instead, the aim is to produce bilingual (i.e., synchronized, seebelow) dependency representations that are appropriate to performing the translationtask for a specific language pair or specific bilingual corpus.
For example, headwordsin both languages are chosen to force a synchronized alignment (for better or worse)in order to simplify cases involving so-called head-switching.
This contrasts with oneof the traditional approaches (e.g., Dorr 1994; Watanabe 1995) to posing the transla-tion problem, i.e., the approach in which translation problems are seen in terms ofbridging the gap between the most natural monolingual representations underlyingthe sentences of each language.The training method has four stages: (i) Compute co-occurrence statistics from thetraining data.
(ii) Search for an optimal synchronized hierarchical alignment for eachbitext.
(iii) Construct a set of head transducers that can generate these alignments withtransition weights derived from maximum likelihood estimation.4.1 Computing Pairing CostsFor each source word w in the data set, assign a cost, the translation pairing costc(w, v) for all possible translations v into the target language.
These translations of thesource word may be zero, one, or several target language words (see Section 4.4 fordiscussion of the multiword case).
The assignment of translation pairing costs (effec-tively a statistical bilingual dictionary) may be done using various statistical measures.For this purpose, a suitable statistical function needs to indicate the strength of co-occurrence correlation between source and target words, which we assume is indicativeof carrying the same semantic ontent.
Our preferred choice of statistical measure forassigning the costs is the ~ correlation measure (Gale and Church 1991).
We applythis statistic to co-occurrence of the source word with all its possible translations inthe data set examples.
We have found that, at least for our data, this measure leads tobetter performance than the use of the log probabilities of target words given sourcewords (cf.
Brown et al 1993).In addition to the correlation measure, the cost for a pairing includes a distancemeasure component that penalizes pairings proportionately to the difference betweenthe (normalized) positions of the source and target words in their respective sentences.4.2 Computing Hierarchical AlignmentsAs noted earlier, dependency transduction models are generative probabilistic models;each derivation generates a pair of dependency trees.
Such a pair can be representedas a synchronized hierarchical alignment of two strings.
A hierarchical alignmentconsists of four functions.
The first two functions are an alignment mapping f fromsource words w to target words f(w) (which may be the empty string ~), and aninverse alignment mapping from target words v to source words fr(v).
The inversemapping is needed to handle mapping of target words to ~; it coincides wi thf  for pairswithout source ~.
The other two functions are a source head-map g mapping sourcedependent words w to their heads g(w) in the source string, and a target head-maph mapping target dependent words v to their headwords h(v) in the target string.
An52Alshawi, Bangalore, and Douglas Leaning Dependency Translation Modelsgshow me nonstop flights to bostonmuestreme los vuelos sin escalas a bostongshow me z ' \muestr~menonstop flights to bostonlos vuelos sin escalas a bostonFigure 6A hierarchical alignment: alignment mappings f and f', and head-maps g and h.example hierarchical alignment is shown in Figure 6 (f and f '  are shown separatelyfor clarity).A hierarchical alignment is synchronized (i.e., it corresponds to synchronized e-pendency trees) if these conditions hold:Nonover lap :  If wl # w2, thenf(wl) f(w2), and similarly, if Vl  V2, thenf'(vl) #d'(v2).Synchron izat ion :  if f (w) = v and v # e, then f(g(w)) = h(v), and f'(v) = w.Similarly, ifd'(v) = w and w # e, thend'(h(v)) = g(w), andf(w) = v.Phrase  cont igu i ty :  The image under f of the maximal substring dominated by aheadword w is a contiguous egment of the target string.
(Here w and v refer to word tokens not symbols (types).
We hope that the context ofdiscussion will make the type-token distinction clear in the rest of this article.)
Thehierarchical alignment in Figure 6 is synchronized.Of course, translations of phrases are not always transparently related by a hier-archical alignment.
In cases where the mapping between a source and target phrase isunclear (for example, one of the phrases might be an idiom), then the most reasonablechoice of hierarchical alignment may be for f and f '  to link the heads of the phrasesonly, all the other words being mapped to e, with no constraints on the monolingualhead mappings h and g. (This is the approach we take to compound lexical pairings,discussed in Section 4.4.
)In the hierarchical alignments produced by the training method described here,the source and target strings of a bitext are decomposed into three aligned regions,as shown in Figure 7: a head region consisting of headword w in the source and itscorresponding targetf(w) in the target string, a left substring region consisting of thesource substring to the left of w and its projection under f on the target string, anda right substring region consisting of the source substring to the right of w and itsprojection under f  on the target string.
The decomposition is recursive in that the leftsubstring region is decomposed around a left headword wl, and the right substring53Computational Linguistics Volume 26, Number 1\[Figure 7Decomposing source and target strings around heads w and f(w).region is decomposed around a right headword Wr.
This process of decompositioncontinues for each left and right substring until it only contains a single word.For each bitext there are, in general, multiple such recursive decompositions thatsatisfy the synchronization constraints for hierarchical alignments.
We wish to findsuch an alignment hat respects the co-occurrence statistics of bitexts as well as thephrasal structure implicit in the source and target strings.
For this purpose we definea cost function on hierarchical alignments.
The cost function is the sum of three terms.The first term is the total of all the translation pairing costs c(w,f(w)) of each sourceword w and its translation f(w) in the alignment; the second term is proportional tothe distance in the source string between dependents wd and their heads g(wa); and thethird term is proportional to the distance in the target string between target dependentwords va and their heads h(va).The hierarchical alignment hat minimizes this cost function is computed usinga dynamic programming procedure.
In this procedure, the pairing costs are first re-trieved for each possible source-target pair allowed by the example.
Adjacent sourcesubstrings are then combined to determine the lowest-cost subalignments for suc-cessively larger substrings of the bitext satisfying the constraints tated above.
Thesuccessively larger substrings eventually span the entire source string, yielding theoptimal hierarchical alignment for the bitext.
This procedure has O(n 6) complexityin the number of words in the source (or target) sentence.
In Alshawi and Douglas(2000) we describe a version of the alignment algorithm in which heads may havean arbitrary number of dependents, and in which the hierarchical alignments for thetraining corpus are refined by iterative reestimation.4.3 Constructing TransducersBuilding a head transducer involves creating appropriate head transducer states andtracing hypothesized head transducer transitions between them that are consistentwith the hierarchical alignment of a bitext.The main transitions that are traced in our construction are those that map heads,wl and Wr, of the right and left dependent phrases of w to their translations as indi-cated by the alignment function f in the hierarchical alignment.
The positions of thedependents in the target string are computed by comparing the positions off(wt) andf(Wr) to the position of v = f(w).In order to generalize from instances in the training data, some model states aris-ing from different raining instances are shared.
In particular, in the construction de-scribed here, for a given pair (w, v) there is only one final state.
(We have also triedusing automatic word-clustering techniques to merge states further, but for the lim-ited domain corpora we have used so far, the results are inconclusive.)
To specify54Alshawi, Bangalore, and Douglas Learning Dependency Translation Models?
\ooFigure 8+1 :+1 -1 :+ 1States and transitions constructed for the "swapping" decomposition shown in Figure 7.the sharing of states we make use of a one-to-one state-naming function ?
from se-quences of strings to transducer states.
The same state-naming function is used forall examples in the data set, ensuring that the transducer fragments recorded forthe entire data set will form a complete collection of head transducer transition et-works.Figure 7 shows a decomposition i which w has a dependent to either side, vhas both dependents to the right, and the alignment is "swapping" (f(wl) is to theright off(wr)).
The construction for this decomposition case is illustrated in Figure 8as part of a finite-state transition diagram, and described in more detail below.
(Theother transition arrows shown in the diagram will arise from other bitext alignmentscontaining (w,f(w)) pairings.)
Other cases covered by our algorithm (e.g., a single leftsource dependent but no right source dependent, or target dependents on either sideof the target head) are simple variants.The detailed construction is as follows:1.
Construct a transition from sl = ?
(initial) to S 2 = O ' (w, f (w) ,  head) mappingthe source headword w to the target head f(w) at position 0 in sourceand target.
(In our training construction there is only one initial state sl.)2.
Since the target dependentf(wr) is to the left of target dependentf(wl)(and we are restricting positions to {-1, 0, +1}) the Wr transition isconstructed first in order that the target dependent nearest he head isoutput first.Construct a transition from s2 to s3 = c~(w,f(w), swapping, Wr,f(Wr)mapping the source dependent Wr at position +1 to the target dependentf(Wr) at position +1.3.
Construct a transition from s3 to s4 = cr(w,f(w),final) mapping the sourcedependent wl at position -1 to the target dependentf(wl) at position +1.If instead the alignment had been as in Figure 9, in which the source dependentsare mapped to target dependents in a parallel rather than swapping configuration(the configuration of sin escalas and Boston around flights:los vuelos in Figure 6), theconstruction is the same, except for the following differences:..Since the target dependentf(wl) is to the left of target dependentf(Wr),the wl transition is constructed first in order that the target dependentnearest he head is output first.The source and target positions are as shown in Figure 10.
Instead ofs ta te  s3, we use a different state ss = ?
(w,f(w),parallel, wl,f(wl)).55Computational Linguistics Volume 26, Number 1\[ "'" l \ [ \ ] \ [  ..-4..- \]j JFigure 9Decomposing source and target strings around heads w and f(w)--"parallel'.w :f(w) ?\ooFigure 10-1 :+1w / f (w , )+1 :+ 1States and transitions constructed for the "parallel" decomposition shown in Figure 9.Other states are the same as for the first case.
The resulting states and transitions areshown in Figure 10.After the construction described above is applied to the entire set of aligned bi-texts in the training set, the counts for transitions are treated as event observationcounts of a statistical dependency transduction model with the parameters describedin Section 3.1.
More specifically, the negated logs of these parameters are used as theweights for transducer t ansitions.4.4 Mult iword PairingsIn the translation application, source word w and target word v are generalized sothey can be short substrings (compounds) of the source and target strings.
Exam-ples of such multiword pairs are show me:muestrdme and nonstop:sin escalas in Fig-ure 6.
The cost for such pairings still uses the same ~ statistic, now taking the ob-servations to be the co-occurrences of the substrings in the training bitexts.
How-ever, in order that these costs can be comparable to the costs for simple pairings,they are multiplied by the number of words in the source substring of the pair-ing.The use of compounds in pairings does not require any fundamental changes tothe hierarchical lignment dynamic programming algorithm, which simply producesdependency trees with nodes that may be compounds.
In the transducer constructionphase of the training method, one of the words of a compound is taken to be the pri-mary or "real" headword.
(In fact, we take the least common word of a compound tobe its head.)
An extra chain of transitions i constructed to transduce the other wordsof compounds, if necessary using transitions with epsilon strings.
This compilationmeans that the transduction algorithm is unaffected by the use of compounds whenaligning training data, and there is no need for a separate compound identificationphase when the transduction algorithm is applied to test data.
Some results for dif-ferent choices of substring lengths can be found in Alshawi, Bangalore, and Douglas(1998).56Alshawi, Bangalore, and Douglas Learning Dependency Translation Models5.
Experiments5.1 Evaluation MethodIn order to reduce the time required to carry out training evaluation experiments,we have chosen two simple, string-based evaluation metrics that can be calculatedautomatically.
These metrics, simple accuracy and translation accuracy, are used tocompare the target string produced by the system against a reference human transla-tion from held-out data.Simple accuracy is computed by first finding a transformation f one string intoanother that minimizes the total weight of insertions, deletions, and substitutions.
(Weuse the same weights for these operations as in the NIST ASR evaluation software\[National Institute of Standards and Technology 1997\].)
Translation accuracy includestranspositions (i.e., movement) of words as well as insertions, deletions, and substi-tutions.
We regard the latter metric as more appropriate for evaluation of translationsystems because the simple metric would count a transposition as two errors: an in-sertion plus a deletion.
(This issue does not arise for speech recognizers because thesesystems do not normally make transposition errors.
)For the lowest edit-distance transformation between the reference translation andsystem output, if we write I for the number of insertions, D for deletions, S for substi-tutions, and R for number of words in the reference translation string, we can expresssimple accuracy assimple accuracy = 1 - ( I  + D + S) /R .Similarly, if T is the number of transpositions in the lowest weight transformationincluding transpositions, we can express translation accuracy astranslation accuracy = 1 - ( I  ~ + D ~ + S + T) /R .Since a transposition corresponds toan insertion and a deletion, the values of I ~ and D ~for translation accuracy will, in general, be different from I and D in the computation ofsimple accuracy.
For Spanish, the units for string operations in the evaluation metricsare words, whereas for Japanese they are Japanese characters.5.2 English-to-SpanishThe training and test data for the English-to-Spanish experiments were taken froma set of transcribed utterances from the Air Travel Information System (ATIS) corpustogether with a translation of each utterance to Spanish.
An utterance is typically asin-gle sentence but is sometimes more than one sentence spoken in sequence.
Alignmentsearch and transduction training was carried out only on bitexts with sentences upto length 20, a total of 13,966 training bitexts.
The test set consisted of 1,185 held-outbitexts at all lengths.
Table 1 shows the word accuracy percentages ( ee Section 5.1)for the trained model, e2s, against he original held-out ranslations at various sourcesentence l ngths.
Scores are also given for a "word-for-word" baseline, sww, in whicheach English word is translated by the most highly correlated Spanish word.5.3 English-to-JapaneseThe training and test data for the English-to-Japanese experiments was a set of tran-scribed utterances of telephone service customers talking to AT&T operators.
Theseutterances, collected from real customer-operator interactions, tend to include frag-mented language, restarts, etc.
Both training and test partitions were restricted to bi-texts with at most 20 English words, giving 12,226 training bitexts and 3,253 held-outtest bitexts.
In the Japanese text, we introduce "word" boundaries that are convenient57Computational Linguistics Volume 26, Number 1Table 1Simple accuracy/translation accuracy (percent) for the trainedEnglish-to-Spanish model (e2s) against he word-for-word baseline(sww).Length < 5 < 10 G 15 < 20 Allsww 45.1/45.8 46.7/48.6 46.5/48.2 45.5/47.1 45.2/46.9e2s 75.4/75.8 76.3/78.0 75.4/77.0 74.4/76.0 73.3/75.0Table 2Simple accuracy/translation accuracy as percentages of Japanesecharacters, for the trained English-to-Japanese model (e2j) and theword-for-word baseline (jww).Length G 5 < 10 G 15 ~ 20 Alljww 75.8/78.0 45.2/50.4 40.0/45.4 37.2/42.8 37.2/42.8e2j 89.2/89.7 74.0/76.6 68.6/72.2 66.4/70.1 66.4/70.1for the training process.
These word boundaries are parasitic on the word boundariesin the English transcriptions: the translators are asked to insert such a word boundarybetween any two Japanese characters that are taken to have arisen from the translationof distinct English words.
This results in bitexts in which the number of multichar-acter Japanese "words" is at most the number of English words.
However, as notedabove, evaluation of the Japanese output is done with Japanese characters, i.e., withthe Japanese text in its natural format.
Table 2 shows the Japanese character accuracypercentages for the trained English-to-Japanese model, e2j, and a baseline model, jww,which gives each English word its most highly correlated translation.5.4 Note on Experimental SettingThe vocabularies in these English-Spanish and English-Japanese experiments are onlya few thousand words; the utterances are fairly short (an average of 7.3 words per utter-ance) and often contain errors typical of spoken language.
So while the domains maybe representative of task-oriented ialogue settings, further experimentation wouldbe needed to assess the effectiveness of our method in situations uch as translat-ing newspaper articles.
In terms of the training data required, Tsukada et al (1999)provide indirect empirical evidence suggesting accuracy can be further improved byincreasing the size of our training sets, though also suggesting that the learning curveis relatively shallow beyond the current size of corpus.6.
Concluding RemarksFormalisms for finite-state and context-free transduction have a long history (e.g.,Lewis and Stearns 1968; Aho and Ullman 1972), and such formalisms have been ap-plied to the machine translation problem, both in the finite-state case (e.g., Vilar et al1996) and the context-free case (e.g., Wu 1997).
In this paper we have added to thisline of research by providing a method for automatically constructing fully lexicalizedstatistical dependency transduction models from training examples.Automatically training a translation system brings important benefits in terms ofmaintainability, robustness, and reducing expert coding effort as compared with tra-58Alshawi, Bangalore, and Douglas Learning Dependency Translation Modelsditional rule-based translation systems (a number of which are described in Hutchinsand Somers \[1992\]).
The reduction of effort results, in large part, from being ableto do without artificial intermediate representations of meaning; we do not requirethe development of semantic mapping rules (or indeed any rules) or the creation ofa corpus including semantic annotations.
Compared with left-to-right ransduction,middle-out ransduction also aids robustness because, when complete derivations arenot available, partial derivations tend to have meaningful headwords.At the same time, we believe our method has advantages over the approach de-veloped initially at IBM (Brown et al 1990; Brown et al 1993) for training translationsystems automatically.
One advantage is that our method attempts to model the nat-ural decomposition of sentences into phrases.
Another is that the compilation of thisdecomposition i to lexically anchored finite-state head transducers produces imple-mentations that are much more efficient han those for the IBM model.
In particular,our search algorithm finds optimal transductions of test sentences in less than "realtime" on a 300MHz processor, that is, the time to translate an utterance is less thanthe time taken to speak it, an important consideration for our speech translation ap-plication.ReferencesAho, Alfred V. and Jeffrey D. Ullman.
1972.The Theory o/Parsing, Translation, andCompiling.
Prentice-Hall, EnglewoodCliffs, NJ.Alshawi, H. 1996.
Head automata forspeech translation.
In Proceedings oftheInternational Conference on Spoken LanguageProcessing, pages 2360-2364, Philadelphia,PA.Alshawi, H., S. Bangalore, and S. Douglas.1998.
Learning phrase-based headtransduction models for translation ofspoken utterances.
In Proceedings oftheInternational Conference on Spoken LanguageProcessing, pages 2767-2770, Sydney,Australia.Alshawi, H. and S. Douglas.
2000.
Learningdependency transduction models fromunannotated examples.
PhilosophicalTransactions ofthe Royal Society (Series A:Mathematical, Physical and EngineeringSciences).
To appear.Alshawi, Hiyan and Fei Xia.
1997.English-to-Mandarin speech translationwith head transducers.
In Proceedings ofthe Workshop on Spoken LanguageTranslation, Madrid, Spain.Brown, P. J., J. Cocke, S. A. Della Pietra,V.
J. Della Pietra, J. Lafferty, R. L. Mercer,and P. Rossin.
1990.
A statistical approachto machine translation.
ComputationalLinguistics, 16(2):79-85.Brown, P. J., S. A. Della Pietra, V. J. DellaPietra, and R. L. Mercer.
1993.
Themathematics ofmachine translation:Parameter estimation.
ComputationalLinguistics, 16(2):263-312.Dorr, B. J.
1994.
Machine translationdivergences: A formal description andproposed solution.
ComputationalLinguistics, 20(4):597-634.Earley, J.
1970.
An efficient context-freeparsing algorithm.
Communications of theACM, 13(2):94-102.Gale, W. A. and K. W. Church.
1991.Identifying word correspondences inparallel texts.
In Proceedings ofthe FourthDARPA Speech and Natural LanguageProcessing Workshop, ages 152-157, PacificGrove, CA.Hays, D. G. 1964.
Dependency theory: Aformalism and some observations.Language, 40:511-525.Hudson, R. A.
1984.
Word Grammar.Blackwell, Oxford.Hutchins, W. J. and H. L. Somers.
1992.
AnIntroduction to Machine Translation.Academic Press, New York.Lewis, P. M. and R. E. Stearns.
1968.Syntax-directed transduction.
Journal of theAssociation for Computing Machinery,15(3):465-488.National Institute of Standards andTechnology.
1997.
Spoken NaturalLanguage Processing Group Web page.http://www.itl.nist.gov/div894.Tsukada, Hajime, Hiyan Alshawi, ShonaDouglas, and Srinivas Bangalore.
1999.Evaluation of machine translation systembased on a statistical method by usingspontaneous speech transcription.
InProceedings ofthe Fall Meeting of theAcoustical Society of Japan, pages 115-116,September.Vilar, J. M., V. M. Jim~nez, J. C. Amengual,A.
Castellanos, D. Llorens, and E. Vidal.1996.
Text and speech translation by59Computational Linguistics Volume 26, Number 1means of subsequential transducers.Natural Language Engineering, 2(4):351-354.Watanabe, Hideo.
1995.
A model of abi-directional transfer mechanism usingrule combination.
Machine Translation,10(4):269-291.Wu, Dekai.1997.
Stochastic inversiontransduction grammars and bilingualparsing of parallel corpora.
ComputationalLinguistics, 23(3):377-404.Younger, D. 1967.
Recognition and Parsingof Context-Free Languages in Time n 3.Information and Control, 10:189-208.60
