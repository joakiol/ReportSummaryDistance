Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 2326?2334, Dublin, Ireland, August 23-29 2014.Unsupervised Word Segmentation in ContextGabriel Synnaeve and Isabelle DautricheLSCP, DECENS Ulm, Paris, Francegabriel.synnaeve@gmail.comisabelle.dautriche@gmail.comBenjamin B?orschingerInstitut f?ur ComputerlinguistikUniversit?at Heidelberg, Heidelberg, Germany.benjamin.boerschinger@gmail.comMark JohnsonDepartment of Computer ScienceMacquarie University, Sydney, Australiamark.johnson@mq.edu.auEmmanuel DupouxLSCP, DECEHESS, Paris, Franceemmanuel.dupoux@gmail.comAbstractThis paper extends existing word segmentation models to take non-linguistic context into ac-count.
It improves the token F-score of a top performing segmentation models by 2.5% on a 27kutterances dataset.
We posit that word segmentation is easier in-context because the learner isnot trying to access irrelevant lexical items.
We use topics from a Latent Dirichlet Allocationmodel as a proxy for ?activities?
contexts, to label the Providence corpus.
We present AdaptorGrammar models that use these context labels, and we study their performance with and withoutcontext annotations at test time.1 Introduction and Previous WorksSegmentation of the speech stream into lexical units plays a central role in early language acquisition.Because words are generally not uttered in isolation, one of the first task for infants learning a language isto extract the words that make up the utterances they hear.
Experimental research has shown that infantsare able to segment fluent speech into word-like units within the first year of life (Jusczyk and Aslin,1995).
How does this ability emerge?
There is evidence that infants use a broad array of linguistic cuesto perform word segmentation (e.g., phonotactics (Jusczyk et al., 1993a), prosodic information (Jusczyket al., 1993b), statistical regularities (Saffran et al., 1996)).
Past experimental and modeling research onspeech segmentation has mainly focused on linguistic cues, treating them as independent from other non-linguistic cues naturally occurring in the child learning environment.
Yet, language appears in contextand is constrained by the events occurring in the daily life of the child.
For example, during an eatingevent one is most likely to speak about food, while during a zoo-visit event, people are more likelyto talk about the animals they see.
Activity contexts may provide a natural structure to speech thatwould be readily be accessible to children.
A recent study using dense recordings of a single child?slanguage development (Roy et al., 2006) showed that words appearing in specific activity contexts arelearned faster (Roy et al., 2012).
Relatedly, Johnson et al.
(2010) showed that Adaptor Grammars (AGs)performed better on a segmentation task when the model has access to a hand-annotated set of objectspresent in the environment, that it can use to learn simultaneously word-object associations (see also(Frank et al., 2009)).
This supports the view that integrating multiple sources of information, linguisticand non-linguistic, can improve learning.Following this idea, we posit that information from the broader context in which a word has beenuttered may simplify the learning problem faced by the child.
In particular, our hypothesis postulatesthat speech segmentation is easier when using vocabularies that are related to a specific activity (eating,This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/2326Table 1: Most probable words in the 7 final topicsegg book ball truck name color blockapple shape cat car school bear batterybanana square hat fire time crayon minutemilk circle tree piece today hair phonebutter triangle fish train day head puzzle?food ?shapes ?playing ?toys ?time ?drawing ?garbage?playing...), or place (kitchen, bedroom...).
To evaluate this hypothesis, we applied topic modeling (Bleiet al., 2003) to automatically derive activity contexts on a corpus of child directed speech, the Provi-dence corpus (Demuth et al., 2006), and tested the influence of such topics on a word segmentation taskextending the AG models used in (B?orschinger et al., 2012).
We found that a model augmented withthe assumption that words are dependent upon the topic of the discourse (as a proxy for activity context)performs better than the same model without access to the discourse topic.
This suggests that the broadercontext in which sentences are uttered may help in the word segmentation process, and could presumablybe used at various stages of language development.The paper is structured as follows.
Section 2 presents a novel approach to augment a corpus withcontextual annotations derived from topic models.
Section 3 quickly explains Adaptor Grammars, theframework that we used to express all our models.
Section 4 presents all the models that were used inthe results.
Section 5 describes the Providence corpus and the experimental setup.
Section 6 shows ourquantitative and qualitative results.
Finally, we discuss the implications for models of language learning.2 Topics as Proxies for ContextsRoy et al.
(2012) found high correlations between human-annotated activity contexts and topics from alatent Dirichlet allocation model (LDA) (Blei et al., 2003), thus showing that using topics as proxies forcontexts is a sound approach.
Topic modeling infers a topic distribution for each ?document?
(a bag ofwords) in the corpus.
Since ?documents?
were not annotated in our corpus, we developed the following3-step approach to automatically segment it into documents.Firstly, for all the children of the Providence corpus, we used recording sessions as hard documentboundaries.
We considered as a ?possible document?
every contiguous sequence of sentences separatedby at least 10 seconds of silence, according to the orthographic transcript.
We also identified ?possibledocuments?
using cues such as ?bye/hi?, indicating a change of participants.
This segmentation resultedin an over-segmented corpus (compared to context switches), yielding a total of 16, 742 documents.Secondly, we used the gensim software (?Reh?u?rek and Sojka, 2010) to train a topic model (LDA)1,and get the topic distributions for each of these documents.
We used the symmetric KL-divergence tomeasure the distance between two topic distributions before and after a ?possible document?
boundary.If the distance was above a threshold, we considered this boundary as a document boundary.
Otherwisewe merged both ?possible documents?
through this silence.
The threshold was set empirically to dis-criminate between two topic distributions that correspond to different activity contexts.
After this step,we assume that each of the resulting 8, 634 documents maps to an activity context.Thirdly, we applied LDA again on this new segmentation to get the topic distribution, hence the activitycontext, of each document.
The number of topics is qualitatively chosen to correspond to the number ofmain activity contexts (eating / playing / drawing / etc.)
that occur in the Providence dataset (we used 7topics), the resulting most topic specific words are shown in Table 1.
Finally, for each document, we gota distribution on topics, and we annotated the document with the most probable topic.
By doing that, wethrow away graded information about the distribution on topics for each document.
We could make useof the full distribution, but here we are only interested in the most probable topic as a proxy for activitycontext.
We do not posit that the infants learn the topic models on linguistic cues while bootstrappingspeech and segmentation, but rather that they get activity context from non-linguistic cues.1We did LDA only on nouns (as they contain most of the semantics), weighted by TF-IDF.23273 Adaptor GrammarsAdaptor Grammars (Johnson et al., 2007) are an extension of probabilistic context-free gram-mars (PCFGs) that learn probability of entire subtrees as well as probabilities of rules.
A PCFG(N,W,R, S, ?)
consists of a start symbol S, N and W disjoints sets of nonterminals and terminal sym-bols respectively.
R is a set of rules producing elements of N or W .
Finally, ?
is a set of distributionsover the rules RX,?X ?
N (RXare the rules that expand X).
An AG (N,W,R, S, ?, A,C) extendsthe above PCFG with a subset (A ?
N ) of adapted nonterminals, each of them (X ?
A) having anassociated adaptor (CX?
C).
An AG defines a distribution over trees GX, ?X ?
N ?W .
If X /?
A,then GXis defined exactly as for a PCFG:GX=?X?Y1...Yn?RX?X?Y1...YnTDX(GY1.
.
.
GYn)With TDX(G1.
.
.
Gn) the distribution over trees with root node X and each subtree ti?
Gii.i.d.
IfX ?
A, then there is an additional indirection (composition) with the distribution HX:GX=?X?Y1...Yn?RX?X?Y1...YnTDX(HY1.
.
.
HYn)HX?
CX(GX)We used CXadaptors following the Pitman-Yor process (PYP) (Perman et al., 1992; Teh, 2006) withparameters a and b.
The PYP generates (Zipfian) type frequencies that are similar to those that occurin natural language (Goldwater et al., 2011).
Metaphorically, if there are n customers and m tables, then+ 1th customer is assigned to table zn+1according to (?kis the Kronecker delta function):zn+1|z1.
.
.
zn?ma+ bn+ b?m+1+m?k=1nk?
an+ b?kFor an AG, this means that adapted non-terminals (X ?
A) either expand to a previously generatedsubtree (T (X)k) with probability proportional to how often it was visited (nk), or to a new subtree(T (X)m+1) generated through the PCFG with probability proportional to ma+ b.4 Word segmentation models4.1 Unigram modelThis most basic model just generates words as sequences of phonemes.
AsWord is underlined, it meansit is adapted, and thus we learn a ?word unit -like?
vocabulary.
Phon is a nonterminal that expands toall the phonemes of the language under consideration.Sentence?Word+Word?
Phon+where :Word+?
{Words?WordWords?Word Words4.2 Collocations and SyllabificationThe baseline that we are using is commonly called the ?colloc-syll?
model (Johnson, 2008; B?orschingeret al., 2012) and is reported at 78% token F-score on the standard Brent version of the Bernstein-Ratnercorpus corpus (Johnson, 2008).
It posits that sentences are collocations of words, and words are com-posed of syllables.
(Goldwater et al., 2009) showed how an assumption of independence between words(a unigram model) led to under-segmentation.
So, above the Word level, we take the collocations (co-occurring sequences) of words into account.2328Furthermore, there is evidence that 8-month-old infants track syllable frequencies (Saffran et al.,1996), and the ?colloc-syll?
model can take that into account.
Word splits into general syllables andinitial- or final- specific syllables.
Syllables consist of onsets or codas (producing consonants), and nu-clei (vowels).
Onsets, nuclei and codas are adapted, thus allowing this model to memorize sequences orconsonants or sequences of vowels, dependent on their position in the word.
Consonants and vowels arethe pre-terminals, their derivation is specified in the grammar into phonemes of the language.Sentence?
Colloc+Colloc?Word+Word?
StructSyllFor notations purposes, all this syllabification is appended after Word by Word ?
StructSyll.All details about the collocations and syllabification grammars can be found in (Johnson, 2008).
Hereis an example of a (good) parse of ?yuwanttusiD6bUk?
with this model, skipping the StructSyllderivations:SentenceCollocWordbUkWordD6CollocWordsiCollocWordtuWordwantWordyu4.3 Including topics (contexts)To allow for the model to make use of the topics (used as proxies for contexts), we modify the grammarby prefixing utterances with topic number (similarly to (Johnson et al., 2010)), ?K ?
#topics:Sentence?
tK Colloc+tKColloctK?Word+tKFor each WordtK, we can derive it into a common adapted Word by WordtK?Word.
Consider thislower level adaptor (Word): it learns a shared vocabulary independently of the topic (all contexts thatwill derive b U k will increment the Word(b U k) pseudo-count).
This Word-hierarchical model iscalled share vocab.Alternatively, we can learn a separate vocabulary for each topic, by having directly: WordtK?StructSyll (note that all words then share the same syllabic structure).
Words are split across differenttopics and need to be adapted for each topic in which they appear.
This flat structure vocabulary modelis called split vocab.4.4 Allowing for non context-specific wordsSentences are not composed only of context-specific words, thus we need a third type of extension thatallows for topic-independent and topic-specific words to mix.
For this, we add topic-independent typesof Colloc and Word that can be used across all topics, but we force each sentence to have at least onetopical collocation:Sentence ?
tK (Colloc+|Colloc+tK) Colloc+tK(Colloc+|Colloc+tK)ColloctK?
Word+tKColloc ?
Word+WordtK?
StructSyllWord ?
StructSyll2329Parentheses denote that these terms are optionals, and ?|?
denotes ?or?.
Both WordtKand Word areadapted, but this time on the same level of hierarchy.
This model allows the use of both topic-specific andcommon words in sentences, and it learns #topics + 1 vocabularies.
We call this model with common.An example of a correct parse with this model is given by:SentenceColloc t3Word t3bUkWord t3D6Word t3siCollocWordtuWordwantWordyut35 Experimental setupThe Providence corpus (Demuth et al., 2006) consists of audio and video, weekly or bi-weekly, record-ings of 6 monolingual English-speaking children home interactions.
Each recording is approximatively1 hour long.
This corpus spans approximatively from their first to third year.
We used the whole corpusto extract the topics to get more stable and general activity contexts.
For all the following results, weused only the Naima portion between 11 months and 24 months, consisting in 26,425 utterances (sen-tences) and 135,389 tokens (words).
The input consist in DARPABET-encoded sequences of phonemeswith about 4200 word-types in the Naima subset.
We followed the same preparation procedure as in(B?orschinger et al., 2012), where more details about the corpus can be found.We used the last version of Mark Johnson?s Adaptor Grammars software2.
All the additional code(preparation, topics, grammars, learning) to reproduce these experiments and results is freely availableonline3, along with the datasets annotations derived from topic modeling4.
For the adaptors, we used aBeta(1, 1) (uniform) prior on the PYP a parameter, and a sparse Gamma(100, 0.01) prior on the PYPb parameter.
We ran 500 iterations (finishing at ?
0.05% of log posterior variation between the lastsiterations) with several runs for each subset of the Naima dataset.6 Results6.1 Unsupervised words segmentationTable 2: Mean (token and boundary) F-scores (f), precisions (p), and recalls (r) for different modelsdepending on the size of dataset (age range).months baseline share vocab split vocab with commontoken f p r f p r f p r f p r11-12 .80 .79 .81 .77 .76 .78 .77 .75 .78 .77 .75 .7811-15 .81 .81 .82 .76 .78 .75 .81 .79 .82 .82 .81 .8311-19 .82 .82 .83 .77 .78 .76 .81 .81 .82 .83 .82 .8411-22 .81 .82 .81 .77 .79 .75 .82 .81 .83 .83 .82 .84boundary f p r f p r f p r f p r11-12 .90 .88 .91 .88 .87 .89 .87 .85 .90 .88 .85 .9011-15 .91 .91 .92 .89 .91 .86 .91 .89 .92 .91 .90 .9311-19 .92 .92 .93 .90 .92 .88 .92 .91 .93 .92 .91 .9411-22 .92 .93 .91 .90 .93 .87 .92 .91 .93 .93 .91 .94The key metric of interest is the token F-score (harmonic mean of precision and recall of words).Table 2 gives all the scores for an increasingly large dataset (as in (B?orschinger et al., 2012)).
Figure 1shows the month-by-month evolution of the token F-score of the different models.
We can see that2http://web.science.mq.edu.au/?mjohnson/3https://github.com/SnippyHolloW/contextual_word_segmentation4https://github.com/SnippyHolloW/contextual_word_segmentation/tree/master/ProvidenceFinal/Final2330Figure 1: Token F-scores (and standard deviations) evolution with an increasingly bigger and richerdataset (11 months to ?X-axis value?
months), computed on 8 runs of 500 iterations per data point.0.7500.7750.8000.82524232221201918171615141312 age in monthstoken f?score modelbaselineshare vocabsplit vocabwith commoncontext-based models need more data to get good performances (several vocabularies to learn), but theyseem more resilient to over-segmentation.Preliminary results confirm the trend of baseline scores getting slowly worse at 25 and 26 monthswhile with common and split vocab stabilize (not plotted here).
We also tried models for which wecan have the ?common vocabulary?
derived only at the level of the collocations (making topic-specificcollocations topic-pure as in split vocab for instance), or only at the level of the words (allowing fortopic-specific collocations deriving in only common words if needed).
Both models are worse than splitvocab and with common.Using a shared global vocabulary while being able to learn (through adaptation) different topic-specificvocabularies does not seem to be a solution: share vocab performs worse than the baseline.
Token recalland boundary recall are worse off (see Table 2), suggesting that fewer words are correctly adapted.Maybe that is because this is the only model with two levels of adapted word hierarchies (WordtKandWord).
Sharing a lower-level vocabulary (Word) still does not allow for context vocabularies (WordtK)to mix, thus is simply harder to train.
Having only one vocabulary per context (split vocab) is a slightimprovement over the baseline, even though it is not significant (95% confidence interval) before 22months.
Models allowing for both topic-specific vocabularies and a common vocabulary to be learnedare the best: with common is significantly (95% confidence interval) better than the baseline, startingfrom 20 months (Figure 1).
The improvement seems to be due to better token (and boundary) recall(Table 2), suggesting that more words are learned.
By looking at their lexicons at 24 months, topic-dependent models have slightly larger lexicon recalls and worse lexicon precisions than the baseline.This means that the additional true word-types that they learn are more frequently correctly used than thefalse word-types (otherwise the token F-scores would be reversed, e.g.
between split vocab and baseline).2331Figure 2: Mean token F-scores (and standard deviations) on 20% held-out test data for 6 different randomsplits of Naima from 11 to 22 months, 500 iterations each.
Grey for baseline on test, green and blue forcontext-dependent models on test and no prefix conditions respectively.Table 3: Most probable words (?
P (word|topic = k)) in the 7 recovered topics at test time without topicannotations (no prefix condition) for the with common model (we omitted phonemes clusters yieldingnon-words).bread elephant lego Michael skinny stick bubbledelicious owl doctor shorts massage remember pastaavocado wearing brush towel ostrich track spiralsporridge turkey change shirts nurse forget squirrelsraisin haircut squeeze pirates hammer oink thumbbiscuit turtle music tangled ruby towed pentagonfood animals play clothes (messy) verbs ?shapes6.2 Recovery of the topics on held-out dataTo check whether these models generalize to unseen utterances, and possibly unseen vocabulary, welooked at the scores of held-out data (80/20% train/test split of the Naima 11 to 22 months dataset).Token F-scores for this test condition are shown in green and grey in Figure 2.
This separates low-frequency collocations to be used at test time and those seen at training time, both for context awaremodels and the basic baseline model.
The F-scores show the same pattern as in the previous experiment,with context-aware models (with common and split vocab here) performing better than the baseline.The topics are learned on the orthographic transcription of the whole Providence corpus (6 children),while we test only on the Naima dataset.
Still, to check that these results are not simply due to additionalinformation (leaked somehow in the form of the tK prefix), we produced another held-out condition,without topic ( tK) prefixes.
Models can use topic-specific vocabularies learned during training, butthey are given no context information at test time.
Token F-scores for this no prefix condition are shownin blue (and grey for the baseline) in Figure 2.
The fact that no prefix performance is on par with thetest condition means that contextual cues are not only important at test time, but particularly so whilelearning the vocabulary.
In other words, the model acquires its vocabularies making use of the additionalcontext.
In the test setting, it is evaluated on novel utterances for which additional context informationis available.
In the no prefix condition it is evaluated on novel utterances for which no additional contextinformation is available.
This means that topic-specific vocabulary learned during training is successfullyused in a consistent way at test time.
To confirm this qualitatively, we looked at the most probable words(after unsupervised segmentation from the phonemic input) in recovered topics at test time in the noprefix condition.
They are shown in Table 3, and they exhibit some of the topics that were found on theorthographic transcript (as they are not limited to nouns, a topic for ?verbs?
appears).23327 ConclusionWe have shown that contextual information helps segmenting speech into word-like units.
We usedtopic modeling as a proxy for richer contextual annotations, as (Roy et al., 2012) have shown high cor-relation between contexts and automatically derived topics.
We modified existing Adaptor Grammarsegmentation models (Johnson, 2008; Johnson and Goldwater, 2009), to be able to learn topic-specificvocabularies.
We applied this approach to a large child directed speech corpus that was previously usedfor segmentation (B?orschinger et al., 2012).
Our model with the capacity to use both a topic-specificvocabulary and a common vocabulary (with common) produces better segmentation scores, ending upwith at least 2.5% better absolute F-scores than its context-oblivious counterpart (baseline).
More gen-erally, both models that learn specialized vocabularies do not get worse F-scores with increasing data(Figure 1).
Particularly, they seem to fix a well-known problem of previous models like ?colloc-syll?
(our baseline), that ?overlearn?
by over-segmenting frequent morphemes as single words (B?orschingeret al., 2012).
We have controlled for the additional information of giving the topic ( tK), and we havefound out that contextual information helps at training time.It would be interesting to look into the link between semantics and syntax in recovered topics.
Fur-ther work should integrate syntax (e.g.
function words), stress cues and prosody from the audio signal(B?orschinger and Johnson, 2014), use even less supervision for contexts, and be applied to other lan-guages.
We believe that language acquisition is not a simple sequential process and that segmentation,syntax, and word meaning bootstrap each others.
This is only a first step towards integrating multiplesources of information and different modalities at all steps of language acquisition.AcknowledgmentsThis project is funded in part by the European Research Council (ERC-2011-AdG-295810 BOOT-PHON), the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, ANR-10-IDEX-0001-02PSL*), the Fondation de France, the Ecole de Neurosciences de Paris, and the Region Ile de France (DIMcerveau et pense).ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003.
Latent dirichlet allocation.
J. Mach.
Learn.
Res.,3:993?1022, March.Benjamin B?orschinger and Mark Johnson.
2014.
Exploring the role of stress in Bayesian word segmentation usingAdaptor Grammars.
Transactions of the Association of Computational Linguistics, 2:93?104, February.Benjamin B?orschinger, Katherine Demuth, and Mark Johnson.
2012.
Studying the effect of input size for bayesianword segmentation on the providence corpus.
In COLING, pages 325?340.Katherine Demuth, Jennifer Culbertson, and Jennifer Alter.
2006.
Word-minimality, epenthesis and coda licensingin the early acquisition of english.
Language and Speech, 49(2):137?173.Michael C Frank, Noah D Goodman, and Joshua B Tenenbaum.
2009.
Using speakers?
referential intentions tomodel early cross-situational word learning.
Psychological Science, 20(5):578?585.Sharon Goldwater, Thomas L Griffiths, and Mark Johnson.
2009.
A bayesian framework for word segmentation:Exploring the effects of context.
Cognition, 112(1):21?54.Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson.
2011.
Producing power-law distributions and dampingword frequencies with two-stage language models.
Journal of Machine Learning Research, 12(Jul):2335?2382.Mark Johnson and Sharon Goldwater.
2009.
Improving nonparameteric bayesian inference: experiments onunsupervised word segmentation with adaptor grammars.
In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,pages 317?325.
Association for Computational Linguistics.Mark Johnson, Thomas L Griffiths, and Sharon Goldwater.
2007.
Adaptor grammars: A framework for specifyingcompositional nonparametric bayesian models.
Advances in neural information processing systems, 19:641.2333Mark Johnson, Katherine Demuth, Michael Frank, and Bevan Jones.
2010.
Synergies in learning words and theirreferents.
In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S.
Zemel, and A. Culotta, editors, Advances inNeural Information Processing Systems 23, pages 1018?1026.Mark Johnson.
2008.
Using adaptor grammars to identify synergies in the unsupervised acquisition of linguisticstructure.
In ACL, pages 398?406.Peter W. Jusczyk and Richard N. Aslin.
1995.
Infants detection of the sound patterns of words in fluent speech.Cognitive psychology, 29(1):123.Peter W. Jusczyk, Anne Cutler, and Nancy J. Redanz.
1993a.
Infants?
preference for the predominant stresspatterns of english words.
Child development, 64(3):675687.Peter W. Jusczyk, Angela D. Friederici, Jeanine MI Wessels, Vigdis Y. Svenkerud, and Ann Marie Jusczyk.1993b.
Infants sensitivity to the sound patterns of native language words.
Journal of Memory and Language,32(3):402420.Mihael Perman, Jim Pitman, and Marc Yor.
1992.
Size-biased sampling of poisson point processes and excursions.Probability Theory and Related Fields, 92(1):21?39.Radim?Reh?u?rek and Petr Sojka.
2010.
Software Framework for Topic Modelling with Large Corpora.
In Proceed-ings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45?50, Valletta, Malta, May.ELRA.
http://is.muni.cz/publication/884893/en.Deb Roy, Rupal Patel, Philip DeCamp, Rony Kubat, Michael Fleischman, Brandon Roy, Nikolaos Mavridis, Ste-fanie Tellex, Alexia Salata, Jethran Guinness, et al.
2006.
The human speechome project.
In Symbol Groundingand Beyond, pages 192?196.
Springer.Brandon C Roy, Michael C Frank, and Deb Roy.
2012.
Relating activity contexts to early word learning in denselongitudinal data.
In Proceedings of the 34th Annual Cognitive Science Conference.Jenny R. Saffran, Richard N. Aslin, and Elissa L. Newport.
1996.
Statistical learning by 8-month old infants.Science, 274(5294):1926?1928.Yee Whye Teh.
2006.
A hierarchical Bayesian language model based on Pitman-Yor processes.
In Proceedingsof the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Associationfor Computational Linguistics, pages 985?992.2334
