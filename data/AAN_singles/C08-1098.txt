Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 777?784Manchester, August 2008Estimation of Conditional Probabilities With Decision Trees and anApplication to Fine-Grained POS TaggingHelmut Schmid and Florian LawsIMS, University of Stuttgart{schmid,lawsfn}@ims.uni-stuttgart.deAbstractWe present a HMM part-of-speech tag-ging method which is particularly suitedfor POS tagsets with a large number offine-grained tags.
It is based on three ideas:(1) splitting of the POS tags into attributevectors and decomposition of the contex-tual POS probabilities of the HMM into aproduct of attribute probabilities, (2) esti-mation of the contextual probabilities withdecision trees, and (3) use of high-orderHMMs.
In experiments on German andCzech data, our tagger outperformed state-of-the-art POS taggers.1 IntroductionA Hidden-Markov-Model part-of-speech tagger(Brants, 2000, e.g.)
computes the most probablePOS tag sequence?tN1=?t1, ...,?tNfor a given wordsequence wN1.
?tN1= argmaxtN1p(tN1, wN1)The joint probability of the two sequences is de-fined as the product of context probabilities andlexical probabilities over all POS tags:p(tN1, wN1) =N?i=1p(ti|ti?1i?k)?
??
?context prob.p(wi|ti)?
??
?lexical prob.
(1)HMM taggers are fast and were successfully ap-plied to a wide range of languages and training cor-pora.c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.POS taggers are usually trained on corpora withbetween 50 and 150 different POS tags.
Tagsetsof this size contain little or no information aboutnumber, gender, case and similar morphosyntac-tic features.
For languages with a rich morphol-ogy such as German or Czech, more fine-grainedtagsets are often considered more appropriate.
Theadditional information may also help to disam-biguate the (base) part of speech.
Without genderinformation, for instance, it is difficult for a taggerto correctly disambiguate the German sentence Istdas Realit?at?
(Is that reality?).
The word das isambiguous between an article and a demonstrative.Because of the lack of gender agreement betweendas (neuter) and the noun Realit?at (feminine), thearticle reading must be wrong.The German Tiger treebank (Brants et al, 2002)is an example of a corpus with a more fine-grainedtagset (over 700 tags overall).
Large tagsets aggra-vate sparse data problems.
As an example, take theGerman sentence Das zu versteuernde Einkommensinkt (?The to be taxed income decreases?
; Thetaxable income decreases).
This sentence shouldbe tagged as shown in table 1.Das ART.Def.Nom.Sg.Neutzu PART.Zuversteuernde ADJA.Pos.Nom.Sg.NeutEinkommen N.Reg.Nom.Sg.Neutsinkt VFIN.Full.3.Sg.Pres.Ind.
SYM.Pun.SentTable 1: Correct POS tags for the German sentenceDas zu versteuernde Einkommen sinkt.Unfortunately, the POS trigram consisting ofthe tags of the first three words does not occurin the Tiger corpus.
(Neither does the pair con-sisting of the first two tags.)
The unsmoothed777context probability of the third POS tag is there-fore 0.
If the probability is smoothed with thebackoff distribution p(?|PART.Zu), the mostprobable tag is ADJA.Pos.Acc.Sg.Fem rather thanADJA.Pos.Nom.Sg.Neut.
Thus, the agreement be-tween the article and the adjective is not checkedanymore.A closer inspection of the Tiger corpus revealsthat it actually contains all the information neededto completely disambiguate each component of thePOS tag ADJA.Pos.Nom.Sg.Neut:?
All words appearing after an article (ART)and the infinitive particle zu (PART.zu) are at-tributive adjectives (ADJA) (10 of 10 cases).?
All adjectives appearing after an article anda particle (PART) have the degree positive(Pos) (39 of 39 cases).?
All adjectives appearing after a nominativearticle and a particle have nominative case (11of 11 cases).?
All adjectives appearing after a singular arti-cle and a particle are singular (32 of 32 cases).?
All adjectives appearing after a neuter articleand a particle are neuter (4 of 4 cases).By (1) decomposing the context probability ofADJA.Pos.Nom.Sg.Neut into a product of attributeprobabilitiesp(ADJA | 2:ART, 2:ART.Def, 2:ART.Nom,2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu)?
p(Pos| 2:ART, 2:ART.Def, 2:ART.Nom,2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu,0:ADJA)?
p(Nom | 2:ART, 2:ART.Def, 2:ART.Nom,2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu,0:ADJA, 0:ADJA.Pos)?
p(Sg | 2:ART, 2:ART.Def, 2:ART.Nom,2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu,0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom)?
p(Neut | 2:ART, 2:ART.Def, 2:ART.Nom,2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu,0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom, 0:ADJA.Sg)and (2) selecting the relevant context attributesfor the prediction of each attribute, we obtain thefollowing expression for the context probability:p(ADJA | ART, PART.Zu)?
p(Pos | 2:ART, 1:PART, 0:ADJA)?
p(Nom | 2:ART.Nom, 1:PART.Zu, 0:ADJA)?
p(Sg | 2:ART.Sg, 1:PART.Zu, 0:ADJA)?
p(Neut | 2:ART.Neut, 1:PART.Zu, 0:ADJA)The conditional probability of each attribute is1.
Hence the context probability of the whole tag isalso 1.
Without having observed the given context,it is possible to deduce that the observed POS tagis the only possible tag in this context.These considerations motivate an HMM taggingapproach which decomposes the POS tags into aset of simple attributes, and uses decision trees toestimate the probability of each attribute.
Deci-sion trees are ideal for this task because the iden-tification of relevant attribute combinations is atthe heart of this method.
The backoff smoothingmethods of traditional n-gram POS taggers requirean ordering of the reduced contexts which is notavailable, here.
Discriminatively trained taggers,on the other hand, have difficulties to handle thehuge number of features which are active at thesame time if any possible combination of contextattributes defines a separate feature.2 Decision TreesDecision trees (Breiman et al, 1984; Quinlan,1993) are normally used as classifiers, i.e.
they as-sign classes to objects which are represented as at-tribute vectors.
The non-terminal nodes are labeledwith attribute tests, the edges with the possible out-comes of a test, and the terminal nodes are labeledwith classes.
An object is classified by evaluatingthe test of the top node on the object, following therespective edge to a daughter node, evaluating thetest of the daughter node, and so on until a termi-nal node is reached whose class is assigned to theobject.Decision Trees are turned into probability esti-mation trees by storing a probability for each pos-sible class at the terminal nodes instead of a singleresult class.
Figure 1 shows a probability estima-tion tree for the prediction of the probability of thenominative attribute of nouns.2.1 Induction of Decision TreesDecision trees are incrementally built by first se-lecting the test which splits the manually anno-tated training sample into the most homogeneoussubsets with respect to the class.
This test, whichmaximizes the information gain1wrt.
the class, is1The information gain measures how much the test de-creases the uncertainty about the class.
It is the differencebetween the entropy of the empirical distribution of the classvariable in the training set and the weighted average entropy7782:N.Regp=0.571 p=0.938p=0.9990:N.Name1:ART.Nom0:N.Name 0:N.Namep=0.948 p=0.998 ....1:ADJA.Nomyesyes nonoyes noyes nonoyesFigure 1: Probability estimation tree for the nomi-native case of nouns.
The test 1:ART.Nom checksif the preceding word is a nominative article.assigned to the top node.
The tree is recursivelyexpanded by selecting the best test for each sub-set and so on, until all objects of the current subsetbelong to the same class.
In a second step, the de-cision tree may be pruned in order to avoid overfit-ting to the training data.Our tagger generates a predictor for each feature(such as base POS, number, gender etc.)
Instead ofusing a single tree for the prediction of all possiblevalues of a feature (such as noun, article, etc.
forbase POS), the tagger builds a separate decisiontree for each value.
The motivation was that a treewhich predicts a single value (say verb) does notfragment the data with tests which are only rele-vant for the distinction of two other values (e.g.
ar-ticle and possessive pronoun).2Furthermore, weobserved that such two-class decision trees requireno optimization of the pruning threshold (see alsosection 2.2.
)The tree induction algorithm only considers bi-nary tests, which check whether some particularattribute is present or not.
The best test for eachnode is selected with the standard information gaincriterion.
The recursive tree building process ter-minates if the information gain is 0.
The decisiontree is pruned with the pruning criterion describedbelow.Since the tagger creates a separate tree for eachattribute, the probabilities of a set of competing at-tributes such as masculine, feminine, and neuterwill not exactly sum up to 1.
To understand why,assume that there are three trees for the gender at-tributes.
Two of them (say the trees for mascu-line and feminine) consist of a single terminal nodein the two subsets.
The weight of each subset is proportionalto its size.2We did not directly compare the two alternatives (two-valued vs. multi-valued tests), because the implementationaleffort required would have been too large.which returns a probability of 0.3.
The third treefor neuter has one non-terminal and two terminalnodes returning a probability of 0.3 and 0.5, re-spectively.
The sum of probabilities is thereforeeither 0.9 or 1.1, but never exactly 1.
This problemis solved by renormalizing the probabilities.The probability of an attribute (such as ?Nom?
)is always conditioned on the respective base POS(such as ?N?)
(unless the predicted attribute is thebase POS) in order to make sure that the probabil-ity of an attribute is 0 if it never appeared with therespective base POS.
All context attributes otherthan the base POS are always used in combinationwith the base POS.
A typical context attribute is?1:ART.Nom?
which states that the preceding tagis an article with the attribute ?Nom?.
?1:ART?
isalso a valid attribute specification, but ?1:Nom?
isnot.The tagger further restricts the set of possibletest attributes by requiring that some attribute ofthe POS tag at position i-k (i=position of the pre-dicted POS tag, k ?
1) must have been used be-fore an attribute of the POS tag at position i-(k+1)may be examined.
This restriction improved thetagging accuracy for large contexts.2.2 Pruning CriterionThe tagger applies3the critical-value pruning strat-egy proposed by (Mingers, 1989).
A node ispruned if the information gain of the best test mul-tiplied by the size of the data subsample is below agiven threshold.To illustrate the pruning, assume that D is thedata of the current node with 50 positive and 25negative elements, and that D1(with 20 positiveand 20 negative elements) and D2(with 30 posi-tive and 5 negative elements) are the two subsetsinduced by the best test.
The entropy of D is?2/3 log22/3 ?
1/3 log21/3 = 0.92, the entropyofD1is?1/2 log21/2?1/2 log21/2 = 1, and theentropy of D2is ?6/7 log26/7 ?
1/7 log21/7 =0.59.
The information gain is therefore 0.92 ?
(8/15 ?
1 ?
7/15 ?
0.59) = 0.11.
The resultingscore is 75 ?
0.11 = 8.25.
Given a threshold of 6,the node is therefore not pruned.We experimented with pre-pruning (where anode is always pruned if the gain is below the3We also experimented with a pruning criterion based onbinomial tests, which returned smaller trees with a slightlylower accuracy, although the difference in accuracy was neverlarger than 0.1% for any context size.
Thus, the simpler prun-ing strategy presented here was chosen.779threshold) as well as post-pruning (where a nodeis only pruned if its sub-nodes are terminal nodesor pruned nodes).
The performance of pre-pruningwas slightly better and it was less dependent onthe choice of the pruning threshold.
A thresholdof 6 consistently produced optimal or near optimalresults for pre-pruning.
Thus, pre-pruning with athreshold of 6 was used in the experiments.3 Splitting of the POS TagsThe tagger treats dots in POS tag labels as attributeseparators.
The first attribute of a POS tag is themain category.
The number of additional attributesis fixed for each main category.
The additionalattributes are category-specific.
The singular at-tribute of a noun and an adjective POS tag aretherefore two different attributes.4Each position in the POS tags of a given cate-gory corresponds to a feature.
The attributes oc-curring at a certain position constitute the value setof the feature.4 Our TaggerOur tagger is a HMM tagger which decomposesthe context probabilities into a product of attributeprobabilities.
The probability of an attribute giventhe attributes of the preceding POS tags as well asthe preceding attributes of the predicted POS tagis estimated with a decision tree as described be-fore.
The probabilities at the terminal nodes of thedecision trees are smoothed with the parent nodeprobabilities (which themselves were smoothed inthe same way).
The smoothing is implemented byadding the weighted class probabilities pp(c) of theparent node to the frequencies f(c) before normal-izing them to probabilities:p(c) =f(c) + ?pp(c)?
+?cf(c)The weight ?
was fixed to 1 after a few experi-ments on development data.
This smoothing strat-egy is closely related to Witten-Bell smoothing.The probabilities are normalized by dividing themby the total probability of all attribute values of therespective feature (see section 2.1).The best tag sequence is computed with theViterbi algorithm.
The main differences of our tag-ger to a standard trigram tagger are that the order ofthe Markov model (the k in equation 1) is not fixed4This is the reason why the attribute tests in figure 1 usedcomplex attributes such as ART.Nom rather than Nom.and that the context probability p(ti|ti?1i?k) is inter-nally computed as a product of attribute probabili-ties.
In order to increase the speed, the tagger alsoapplies a beam-search strategy which prunes allsearch paths whose probability is below the prob-ability of the best path times a threshold.
With athreshold of 10?3or lower, the influence of prun-ing on the tagging accuracy was negligible.4.1 Supplementary LexiconThe tagger may use an external lexicon which sup-plies entries for additional words which are notfound in the training corpus, and additional tags forwords which did occur in the training data.
If anexternal lexicon is provided, the lexical probabili-ties are smoothed as follows: The tagger computesthe average tag probabilities of all words with thesame set of possible POS tags.
The Witten-Bellmethod is then applied to smooth the lexical prob-abilities with the average probabilities.If the word w was observed with N differenttags, and f(w, t) is the joint frequency of w andPOS tag t, and p(t|[w]) is the average probabilityof t among words with the same set of possibletags as w, then the smoothed probability of t givenw is defined as follows:p(t|w) =f(w, t) + Np(t|[w])f(w) + NThe smoothed estimates of p(tag|word) are di-vided by the prior probability p(tag) of the tag andused instead of p(word|tag).54.2 Unknown WordsThe lexical probabilities of unknown words areobtained as follows: The unknown words are di-vided into four disjoint classes6with numeric ex-pressions, words starting with an upper-case letter,words starting with a lower-case letter, and a fourthclass for the other words.
The tagger builds a suf-fix trie for each class of unknown words using theknown word types from that class.
The maximallength of the suffixes is 7.The suffix tries are pruned until (i) all suffixeshave a frequency of at least 5 and (ii) the informa-tion gain multiplied by the suffix frequency and di-5p(word|tag) is equal to p(tag|word)p(word)/p(tag)and p(word) is a constant if the tokenization is unambiguous.Therefore dropping the factor p(word) has no influence onthe ranking of the different tag sequences.6In earlier experiments, we had used a much larger num-ber of word classes.
Decreasing their number to 4 turned outto be better.780vided by the number of different POS tags is abovea threshold of 1.
More precisely, if T?is the set ofPOS tags that occurred with suffix ?, |T | is thesize of the set T , f?is the frequency of suffix ?,and p?
(t) is the probability of POS tag t among thewords with suffix ?, then the following conditionmust hold:fa?|Ta?|?t?Ta?pa?
(t) logpa?(t)p?
(t)< 1The POS probabilities are recursively smoothedwith the POS probabilities of shorter suffixes us-ing Witten-Bell smoothing.5 EvaluationOur tagger was first evaluated on data from theGerman Tiger treebank.
The results were com-pared to those obtained with the TnT tagger(Brants, 2000) and the SVMTool (Gim?enez andM`arquez, 2004), which is based on support vec-tor machines.7The training of the SVMTool tookmore than a day.
Therefore it was not possible tooptimize the parameters systematically.
We tookstandard features from a 5 word window and M4-LRL training without optimization of the regular-ization parameter C.In a second experiment, our tagger was alsoevaluated on the Czech Academic corpus 1.0(Hladk?a et al, 2007) and compared to the TnT tag-ger.5.1 Tiger CorpusThe German Tiger treebank (Brants et al, 2002)contains over 888,000 tokens.
It is annotated withPOS tags from the coarse-grained STTS tagsetand with additional features encoding informa-tion about number, gender, case, person, degree,tense, and mood.
After deleting problematic sen-tences (e.g.
with an incomplete annotation) and au-tomatically correcting some easily detectable er-rors, 885,707 tokens were left.
The first 80% wereused as training data, the first half of the rest asdevelopment data, and the last 10% as test data.Some of the 54 STTS labels were mapped tonew labels with dots, which reduced the numberof main categories to 23.
Examples are the nom-inal POS tags NN and NE which were mapped toN.Reg and N.Name.
Some lexically decidable dis-tinctions missing in the Tiger corpus have been7It was planned to include also the Stanford tagger(Toutanova et al, 2003) in this comparison, but it was notpossible to train it on the Tiger data.automatically added.
Examples are the distinc-tion between definite and indefinite articles, andthe distinction between hyphens, slashes, left andright parentheses, quotation marks, and other sym-bols which the Tiger treebank annotates with ?$(?.A supplementary lexicon was created by analyz-ing a word list which included all words from thetraining, development, and test data with a Germancomputational morphology.
The analyses gener-ated by the morphology were mapped to the Tigertagset.
Note that only the words, but not the POStags from the test and development data were used,here.
Therefore, it is always possible to create asupplementary lexicon for the corpus to be pro-cessed.In case of the TnT tagger, the entries of the sup-plementary lexicon were added to the regular lex-icon with a default frequency of 1 if the word/tag-pair was unknown, and with a frequency propor-tional to the prior probability of the tag if the wordwas unknown.
This strategy returned the best re-sults on the development data.
In case of the SVM-Tool, we were not able to successfully integrate thesupplementary lexicon.5.1.1 Refined TagsetPrepositions are not annotated with case in theTiger treebank, although this information is impor-tant for the disambiguation of the case of the nextnoun phrase.
In order to provide the tagger withsome information about the case of prepositions,a second training corpus was created in whichprepositions which always select the same case,such as durch (through), were annotated with thiscase (APPR.Acc).
Prepositions which select gen-itive case, but also occur with dative case8, weretagged with APPR.Gen.
The more frequent onesof the remaining prepositions, such as in (in), werelexicalized (APPR.in).
The refined tagset alo dis-tinguished between the auxiliaries sein, haben, andwerden, and used lexicalized tags for the coor-dinating conjunctions aber, doch, denn, wie, bis,noch, and als whose distribution differs from thedistribution of prototypical coordinating conjunc-tions such as und (and) or oder (or).For evaluation purposes, the refined tags aremapped back to the original tags.
This mappingis unambiguous.8In German, the genitive case of arguments is more andmore replaced by the dative.781tagger default refined ref.+lexiconbaseline 67.3 67.3 69.4TnT 86.3 86.9 90.4SVMTool 86.6 86.6 ?2 tags 87.0 87.9 91.510 tags 87.6 88.5 92.2Table 2: Tagging accuracies on development datain percent.
Results for 2 and for 10 preceding POStags as context are reported for our tagger.5.1.2 ResultsTable 2 summarizes the results obtained withdifferent taggers and tagsets on the developmentdata.
The accuracy of a baseline tagger whichchooses the most probable tag9ignoring the con-text is 67.3% without and 69.4% with the supple-mentary lexicon.The TnT tagger achieves 86.3% accuracy on thedefault tagset.
A tag is considered correct if allattributes are correct.
The tagset refinement in-creases the accuracy by about 0.6%, and the ex-ternal lexicon by another 3.5%.The SVMTool is slightly better than the TnTtagger on the default tagset, but shows little im-provement from the tagset refinement.
Apparently,the lexical features used by the SVMTool encodemost of the information of the tagset refinement.With a context of two preceding POS tags (sim-ilar to the trigram tagger TnT), our tagger outper-forms TnT by 0.7% on the default tagset, by 1%on the refined tagset, and by 1.1% on the refinedtagset plus the additional lexicon.
A larger contextof up to 10 preceding POS tags further increasedthe accuracy by 0.6, 0.6, and 0.7%, respectively.default refined ref.+lexiconTnT STTS 97.28TnT Tiger 97.17 97.26 97.5110 tags 97.39 97.57 97.97Table 3: STTS accuracies of the TnT tagger trainedon the STTS tagset, the TnT tagger trained on theTiger tagset, and our tagger trained on the Tigertagset.These figures are considerably lower thane.g.
the 96.7% accuracy reported in Brants (2000)for the Negra treebank which is annotated withSTTS tags without agreement features.
This is to9Unknown words are tagged by choosing the most fre-quent tag of words with the same capitalization.be expected, however, because the STTS tagset ismuch smaller.
Table 3 shows the results of an eval-uation based on the plain STTS tagset.
The firstresult was obtained with TnT trained on Tiger datawhich was mapped to STTS before.
The secondrow contains the results for the TnT tagger whenit is trained on the Tiger data and the output ismapped to STTS.
The third row gives the corre-sponding figures for our tagger.91.491.591.691.791.891.99292.192.292.32 3 4 5 6 7 8 9 10Figure 2: Tagging accuracy on development datadepending on context sizeFigure 2 shows that the tagging accuracy tendsto increase with the context size.
The best resultsare obtained with a context size of 10.
What typeof information is relevant across a distance of tenwords?
A good example is the decision tree for theattribute first person of finite verbs, which looksfor a first person pronoun at positions -1 through-10 (relative to the position of the current word) inthis order.
Since German is a verb-final language,these tests clearly make sense.Table 4 shows the performance on the test data.Our tagger was used with a context size of 10.
Thesuffix length parameter of the TnT tagger was setto 6 without lexicon and to 3 with lexicon.
Thesevalues were optimal on the development data.
Theaccuracy of our tagger is lower than on the devel-opment data.
This could be due to the higher rateof unknown words (10.0% vs. 7.7%).
Relative tothe TnT tagger, however, the accuracy is quite sim-ilar for test and development data.
The differencesbetween the two taggers are significant.10tagger default refined ref.+lexiconTnT 83.45 84.11 89.14our tagger 85.00 85.92 91.07Table 4: Tagging accuracies on test data.By far the most frequent tagging error was theconfusion of nominative and accusative case.
If10726 sentences were better tagged by TnT (i.e.
with fewerrors), 1450 sentences were better tagged by our tagger.
Theresulting score of a binomial test is below 0.001.782this error is not counted, the tagging accuracyon the development data rises from 92.17% to94.27%.Our tagger is quite fast, although not as fast asthe TnT tagger.
With a context size of 3 (10), it an-notates 7000 (2000) tokens per second on a com-puter with an Athlon X2 4600 CPU.
The trainingwith a context size of 10 took about 4 minutes.5.2 Czech Academic CorpusWe also evaluated our tagger on the Czech Aca-demic corpus (Hladk?a et al, 2007) which contains652.131 tokens and about 1200 different POS tags.The data was divided into 80% training data, 10%development data and 10% test data.88.588.688.788.888.9892 3 4 5 6 7 8 9 10?context-data2?Figure 3: Accuracy on development data depend-ing on context sizeThe best accuracy of our tagger on the develop-ment set was 88.9% obtained with a context of 4preceding POS tags.
The best accuracy of the TnTtagger was 88.2% with a maximal suffix length of5.
The corresponding figures for the test data are89.53% for our tagger and 88.88% for the TnT tag-ger.
The difference is significant.6 DiscussionOur tagger combines two ideas, the decompositionof the probability of complex POS tags into a prod-uct of feature probabilities, and the estimation ofthe conditional probabilities with decision trees.
Asimilar idea was previously presented in Kempe(1994), but apparently never applied again.
Thetagging accuracy reported by Kempe was belowthat of a traditional trigram tagger.
Unlike him,we found that our tagging method out-performedstate-of-the-art POS taggers on fine-grained POStagging even if only a trigram context was used.Schmid (1994) and M`arquez (1999) used deci-sion trees for the estimation of contextual tag prob-abilities, but without a decomposition of the tagprobability.
Magerman (1994) applied probabilis-tic decision trees to parsing, but not with a genera-tive model.Provost & Domingos (2003) noted that well-known decision tree induction algorithms such asC4.5 (Quinlan, 1993) or CART (Breiman et al,1984) fail to produce accurate probability esti-mates.
They proposed to grow the decision trees totheir maximal size without pruning, and to smooththe probability estimates with add-1 smoothing(also known as the Laplace correction).
Ferriet al (2003) describe a more complex backoffsmoothing method.
Contrary to them, we ap-plied pruning and found that some pruning (thresh-old=6) gives better results than no pruning (thresh-old=0).
Another difference is that we used N two-class trees with normalization to predict the prob-abilities of N classes.
These two-class trees can bepruned with a fixed pruning threshold.
Hence thereis no need to put aside training data for parametertuning.An open question is whether the SVMTool (orother discriminatively trained taggers) could out-perform the presented tagger if the same decompo-sition of POS tags and the same context size wasused.
We think that this might be the case if theSVM features are restricted to the set of relevantattribute combinations discovered by the decisiontree, but we doubt that it is possible to train theSVMTool (or other discriminatively trained tag-gers) without such a restriction given the difficul-ties to train it with the standard context size.Czech POS tagging has been extensively stud-ied in the past (Haji?c and Vidov?a-Hladk?a, 1998;Haji?c et al, 2001; Votrubec, 2006).
Spoustov etal.
(2007) compared several POS taggers includ-ing an n-gram tagger and a discriminatively trainedtagger (Mor?ce), and evaluated them on the PragueDependency Treebank (PDT 2.0).
Mor?ce?s tag-ging accuracy was 95.12%, 0.3% better than then-gram tagger.
A hybrid system based on fourdifferent tagging methods reached an accuracy of95.68%.
Because of the different corpora used andthe different amounts of lexical information avail-able, a direct comparison to our results is difficult.Furthermore, our tagger uses no corpus-specificheuristics, whereas Mor?ce e.g.
is optimized forCzech POS tagging.The German tagging results are, to the best ofour knowledge, the first published results for fine-grained POS tagging with the Tiger tagset.7837 SummaryWe presented a HMM POS tagger for fine-grainedtagsets which splits the POS tags into attributevectors and estimates the conditional probabili-ties of the attributes with decision trees.
In ex-periments with German and Czech corpora, thismethod achieved a higher tagging accuracy thantwo state-of-the-art general-purpose POS taggers(TnT and SVMTool).ReferencesBrants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERtreebank.
In Proceedings of the Workshop on Tree-banks and Linguistic Theories, Sozopol.Brants, Thorsten.
2000.
TnT - a statistical part-of-speech tagger.
In Proceedings of the Sixth AppliedNatural Language Processing Conference ANLP-2000, Seattle, WA.Breiman, L., J. H. Friedman, R. A. Olshen, and C. J.Stone.
1984.
Classification and Regression Trees.Wadsworth and Brooks, Pacific Grove CA.Ferri, C., P. Flach, and J. Hern?andez-Orallo.
2003.
Im-proving the AUC of probabilistic estimators trees.
InProceedings of 14th European Conference on Ma-chine Learning (ECML?03), pages 121?132.Gim?enez, Jes?us and Llu?
?s M`arquez.
2004.
SVMTool:A general POS tagger generator based on supportvector machines.
In Proceedings of the IV Interna-tional Conference on Language Resources and Eval-uation (LREC?04), pages 43?46, Lisbon, Portugal.Haji?c, Jan and Barbora Vidov?a-Hladk?a.
1998.
Tag-ging inflective languages: Prediction of morpholog-ical categories for a rich, structured tagset.
In Pro-ceedings of ACL-COLING?98, Montreal, Canada.Haji?c, Jan, Pavel Krbec, Karel Oliva, Pavel Kv?eto?n,and Vladim?
?r Petkevi?c.
2001.
Serial combination ofrules and statistics: A case study in czech tagging.
InProceedings of the 39th Annual Meeting of the ACL,Toulouse, France.Hladk?a, Barbora Vidov?a, Jan Hajic, Jir??
Hana, JaroslavaHlav?acov?a, Jir??
M?
?rovsk?y, and Jan Votrubec.
2007.Czech Academic Corpus 1.0 Guide.
KarolinumPress, Prag, Czechia.Kempe, Andr?e.
1994.
Probabilistic tagging with fea-ture structures.
In Proceedings of the 15th Inter-national Conference on Computational Linguistics(COLING 1994), pages 161?165, Kyoto, Japan.Magerman, David M. 1994.
Natural Language Pro-cessing as Statistical Pattern Recognition.
Ph.D.thesis, Stanford University.M`arquez, Llu??s.
1999.
POS Tagging : A Ma-chine Learning Approach based on Decision Trees.Ph.D.
thesis, Dep.
LSI, Universitat Politecnica deCatalunya (UPC), Barcelona, Spain, July.Mingers, John.
1989.
An empirical comparison ofpruning methods for decision tree induction.
Ma-chine Learning, 4:227?243.Provost, Foster and Pedro Domingos.
2003.
Treeinduction for probability-based ranking.
MachineLearning, 52(3):199?215.Quinlan, J. Ross.
1993.
C4.5 : Programs for MachineLearning.
Morgan Kaufmann, San Mateo , CA.Schmid, Helmut.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing, pages 44?49, Manchester, UK.Spoustov?a, Drahom?
?ra, Jan Haji?c, Jan Votrubec, PavelKrbec, and Pavel Kv?eto?n.
2007.
The best of twoworlds: Cooperation of statistical and rule-based tag-gers for czech.
In Proceedings of the Workshop onBalto-Slavonic Natural Language Processing, pages67?74, Prague, Czech Republic, June.Toutanova, Kristina, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.
InProceedings of HLT-NAACL 2003, pages 252?259,Edmonton, Canada.Votrubec, Jan. 2006.
Morphological tagging based onaveraged perceptron.
In Proceedings of the 15th An-nual Conference of Doctoral Students (WDS), pages191?195.784
