A Preference-first Language ProcessorIntegrating the Unification Grammar and Markov Language Modelfor Speech Recognition-ApplicationSLee-Feng Chien**, K. J. Chen** and Lin-Shan Lee** Dept.
o f  Computer  Science and Information Engineering,National Ta iwan University,Taipei,  Taiwan,  Rep. o f  China,  Tel: (02) 362-2444.
** The Institute of  Information Science, Academia  Sinica, Taipei,  Taiwan, Rep. of  China.A language processor is to find out a most promisingsentence hypothesis for a given word lattice obtainedfrom acoustic signal recognition.
In this paper a newlanguage processor is proposed, in which unificationgranunar and Markov language model are integrated in aword lattice parsing algorithm based on an augmentedchart, and the island-driven parsing concept is combinedwith various preference-first parsing strategies defined bydifferent construction principles and decision rules.
Testresults"show that significant improvements in bothcorrect rate of recognition and computation speed can beachieved .1.
IntroductionIn many speech recognition applications, a wordlattice is a partially ordered set of possible wordhypotheses obtained from an acoustic signalprocessor.
The purpose of a language processor isthen, for an input word lattice, to find the mostpromising word sequence or sentence hypothesis asthe output (Hayes, 1986; Tomita, 1986;O'Shaughnessy, 1989).
Conventionally eithergrammatical or statistcal approaches were used insuch language processors.
However, the high degreeof ambiguity and large number of noisy wordhypotheses in the word lattices usually make thesearch space huge and correct identification of theoutput sentence hypothesis difficult, and thecapabilities of a language processor based on eithergrammatical orstatistical pproaches alone were veryoften limited.
Because the features of these twoapproaches are basically complementary, Derouaultand Merialdo (Derouault, 1986) first proposed aunified model to combine them.
But in this modelthese two approaches were applied primarilyseparately, selecting the output sentence hypothesisbased on the product of two probabilitiesindependently obtained from these two approaches.293In this paper a new language processor based on arecently proposed augmented chart parsing algorithm(Chien, 1990a) is presented, in which thegrammatical approach of unification grammar(Sheiber, 1986) and the statistical approach ofMarkov language model (Jelinek, 1976) are properlyintegrated in a preference-first word lattice parsingalgorithm.
The augmented chart (Chien, 1990b) wasextended from the conventional chart.
It can representa very complicated word lattice, so that the difficultword lattice parsing problem can be reduced toessentially a well-known chart parsing problem.Unification grammars, compared with othergrarnmal~cal approaches, are more declarative and canbetter integrate syntactic and semantic information toeliminate illegal combinations; while Markovlanguage models are in general both effective andsimple.
The new language processor proposed in thispaper actually integrates the unification grammar andthe Markov language model by a new preference-f'u-stparsing algorithm with various preference-firstparsing strategies defined by different constituentconstruction principles and decision rules, such thatthe constituent selection and search directions in theparsing process can be more appropriately determinedby Markovian probabilities, thus rejecting mostnoisy word hypotheses and significantly reducing thesearch space.
Therefore the global structural synthesiscapabilities of the unification grammar and the localrelation estimation capabilities of the Markovlanguage model are properly integrated.
This makesthe present language processor not sensitive at all tothe increased number of noisy word hypotheses in avery large vocabulary environment.
An experimentalsystem for Mandarin speech recognition has beenimplemented (Lee, 1990) and tested, in which a veryhigh correct rate of recognition (93.8%) was obtainedat a very high processing speed (about 5 sec persentence on an IBM PC/AT).
This indicatessignificant improvements a compared to previouslyproposed models.
The details of this new languageprocessor will be presented in the following sections.2.
The ProposedLanguage ProcessorThe language processor proposed in this paperis shown in Fig.
1, where an acoustic signalpreprocessor is included to form a complete speechrecognition system.
The language processor consistsof a language model and a parser.
The language modelproperly integrates the unification grammar and theMarkov language model, while the parser is definedbased on the augmented chart and the preference-firstparsing algorithm.
The input speech signal is firstprocessed by the acoustic signal preprocessor; thecorresponding word lattice will thus be generated andconstructed onto the augmented chart.
The parser willthen proceed to build possible constituents from theword lattice on the augmented chart in accordancewith the language model and the preference-firstparsing algorithm.
Below, except he preference-firstparsing algorithm presented in detail in the nextsection, all of other elements are briefly summarized.The Laneua~e ModelThe goal of the language model is to participatein the selection of candidate constituents for asentence to be identified.
The proposed languagemodel is composed of a PATR-II-like unificationgrammar (Sheiber, 1986; Chien, 1990a) and afirst-order Markov language model (Jelinek, 1976) andthus, combines many features of the grammatical ndstatistical anguage modeling approaches.
ThePATR-II-Iike unification grammar is used primarilyto distinguish between well-formed, acceptable wordsequences against ill-formed ones, and then torepresent the structural phrases and categories, or tofred the intended meaning depending on differentapplications.
The first-order Markov kmguage model,on the other hand, is used to guide the parser towardcorrect search directions, such that many noisy wordhypotheses can be rejected and many unnecessaryconstituents can be avoided, and the most promisingsentence hypothesis can thus be easily found.
In thisway the weakness in either the PATR-II-likeunification grammar (Sheiber, 1986), e.g., the heavyreliance on rigid linguistic information, or thefirst-order Markov language model (Jelinek, 1976),e.g., the need for a large training corpus and the localprediction scope can also be effectively remedied.The Augmented  Char t  andthe Word  l~attic?
Pars ing SchemeChart is an efficient and widely used workingstructure in many natural language processingsystems (Kay, 1980; Thompson, 1984), but it isbasically designed to parse a sequence of fixed andknown words instead of an ambiguous word lattice.The concept of the augmented chart has recently beensuccessfully developed such that it can be used torepresent and parse a word lattice (Chien, 1990b).Any given input word lattice for parsing can berepresented by the augmented chart through amapping procedure, in which a minimum number ofvertices are used to indicate the end points for all wordhypotheses in the lattice, and an inactive dge is usedto represent every word hypotheses.
Also, speciallydesigned jump edges are constructed to link someedges whose corresponding word hypotheses canpossibly be connected but themselves are physicallyseparated inthe chart.
In this way the basic operationof a chart parser can thus be properly performed on aword lattice.
The difference isthat wo separated geslinked by a jump edge can also be combined as longas the required condition is satisfied.
Note that in sucha scheme, every constituents (edge) will beconstructed only once, regardless of the fact that itmay be shared by many different sentence hypotheses.A Sl~-ech r~ogn i t ion  systemSpeeeh-lnpu|Acoust ic  signal | V0\]~t~qroo~.,88or JThe  proposed |an, ,mlal~e processorThe lan~rua~e mode l,rd latticesThe parserparsing ITh?
I Lost promisingsent?
:ce hypothesisFig.
1 An abstract diagram of the proposed language processor.2943.
The Preference-first ParsingAlgorithmThe preference-first parsing algorithm isdeveloped based on the augmented chart summarizedabove, so that the difficult word lattice parsingproblem is reduced to essentially a well-known chartparsing problem.
This parsing algorithm is a generalalgorithm, in which various preference-first parsingstrategies defined by different construction principlesand decision rules can be combined with theisland-driven parsing concept, so that the constituentselection and search directions can be appropriatelydetermined by Markovian probabilities, thus rejectingmany noisy word hypotheses and significantlyreducing the search space.
In this way, not only canthe features of the grammatical and statisticalapproaches be combined, but the effects of the twodifferent approaches are reflected and integrated in asingle algorithm such that overall performance an beappropriately optimized.
Below, more details aboutthe algorithm will be given.Example  Const ruct ion  pr inc ip les :random mincit)le: at 1my ~ nmd~ly  select It c~adidatc conslJt ucnt to be constttlct~probability selection l~rinciole: at any dmc the candi~llt?
consdtucnt with file highestprobability will b?
constnlcte.d ftrstlength ~,cleclion ~Hnc~ole: at any time the candidate constituent with the largest numtcomponent word hypoth~es will be constructed ftrstlen~,th~robabilltv xe/ection Drlnci~le: at any tlm?
the c~mdldat?
constituent with thehighest probability among those with the largest number of component "~tdhypotheses wltt b~ ?otts~ctcd t intExample  Decis ion rules:hi~hcst nrc, bab~titv rule; ~fft~r lilt grammatical scntoncc onstituents have been {ound,one with the higher probability L~ taken as tlc re~uh~rst- 1 rulG: the rtrst grlunmatlcal ~:ntcnc?
constilucnt obtained during the con~ ofparsing is ulkcn as the Rsuhfirst-k rule: the sontcnc?
constltmmt with ~hc highest probability among the first kc.o~s~ct?d ?rammadcal scnunac~ constituents obkaincd during thc course ol'parsi;~is taken as the resultThe performance of these various constructionprinciples and decision rules will be discussed inSections 5 and 6 based on experimental results.Probabilitv Estimationfor Constructed ConstituentsIn order to make the unification-based parsingalgorithm also capable of handling the Markovlanguage model, every constructed constituent has tobe assigned a probability.
In general, for each givenconstituent C a probability P(C) = P(W c) is assigned,where W c is the component word hypothesis sequenceof C and P('W c) can be evaluated from the Markovlanguage model.
Now, when an active constituent Aand an inactive constituent I form a new constituentN, the probability P(N) can be evaluated fromprobabilities P(A) and P(I).
Let W n, W a, W i be thecomponent word hypothesis sequences of N, A, and Irespectively.
Without loss of generality, assume A isto the left of I, thereby Wn = WaWi =Wal ..... Wam,Wil .....
Win, where wak is the k-th wordhypothesis of Wa and Wik the k-th word hypothesisof Wi.
Then,P(Wn) = P(WaWi) =P(Wal ) * 71~ P(waklWak.1) * P(WillWarn) * TI~ P(wiklWik_l) 2 < k_<.
n 2~k~rn-- P(Wa)*PfWi)* I P(wil Iwam)lP(wi 1) }-This can be easily evaluated ineach parsing step.The Preference-first ConstructionPrincinles and Decision RulesSince P(C) is assigned to every constituent C inthe augmented chart, various parsing strategies can bedeveloped for the preference-first parsing algorithm fordifferent applications.
For example, there can bevarious construction principles to determine the orderof constituent construction for all possible candidateconstituents.
There can also be various decision rulesto choose the output sentence among all of theconstructed sentence constituents.
Some examples forsuch construction principles and decision rules arelisted in the following.2954.
The Experimental SystemAn experimental system based on the proposedlanguage processor has been developed and tested on asmall lexicon, a Markov language model, and a simpleset of unification grammar ules for the Chineselanguage, although the present model is in factlanguage independent.
The system is written in Clanguage and performed on an IBM PC/AT.The lexicon used has a total of 1550 words.
Theyare extracted from the primary school Chinese textbooks currently used in Taiwan area, which arcbelieved to cover the most frequently used words andmost of the syntactic and semantic structures in th~everyday Chinese sentences.
Each word stored inlexicon (word entry) contains uch information as the.word name, the pronunciations (the phonemes), thelexical categories and the corresponding featurestructures.
Information contained in each word entry isrelatively simple except for the verb words, becauseverbs have complicated behavior and will play a centralrole in syntactic analysis, The unification grammarconstructed includes about 60 rules.
It is believed thatthese rules cover almost all of the sentences used in theprimary school Chinese text books.
The Markovlanguage model is trained using the primary schoolChinese text books as training corpus.
Since there areno boundary markers between adjacent words in writtenChinese sentences, each sentence in the corpus wasfirst segmented into a corresponding word string beforeused in the model training.
Moreover, the test datainclude 200 sentences randomly selected from 20articles taken from several different magazines,newspapers and books published in Taiwan area.
Allthe words used in the test sentences are included in thelexicon.5.
Test Results (I) -- InitialPreference-first ParsingStrategiesThe present preference-first languageprocessor is a general model on which differentparsing strategies defined by different constructionprinciples and decision rules can be implemented.
Inthis and the next sections, several attractive parsingstrategies are proposed, tested and discussed under thetest conditions presented above.
Two initial tests,test I and II, were first performed to be used as thebaseline for comparison in the following.
In test I,the conventional unification-based grammaticalanalysis alone is used, in which all the sentencehypotheses obtained from the word lattice wereparsed exhaustively and a grammatical sentenceconstituent was selected randomly as the result;while in test II the first-order Markov modelingapproach alone is used, and a sentence hypothesiswith the highest probability was selected as theresult regardless of the grammatical structure.
Thecorrect rate of recognition is defined as the averagedpercentage of the correct words in the outputsentences.
The correct rate of recognition and theapproximated average time required are found to be73.8% and 25 see for Test I ,  as well as 82.2% and3 see for Test II, as indicated in the first two rows ofTable 1.
In all the following parsing strategies, boththe unification grammar and the Markov languagemodel will be integrated in the language model toobtain better esults.The parsing strategy 1 uses the randomselection principle and the highest probability rule (as listed in Section 3), and the entire word latticewill be parsed exhaustively.
The total number ofconstituents constructed during the course of parsingfor each test sentence are also recorded.
The resultsshow that the correct rate of recognition can be ashigh as 98.3%.
This indicates that the languageprocessor based on the integration of the unificationgrammar and the Markov language model can in factbe very reliable.
That is, most of the interferencesdue to the noisy word hypotheses are actuallyrejected by such an integration.
However, thecomputation load required for such an exhaustiveparsing strategy turns out to be very high (similar tothat in Test 13, i.e., for each test sentence in average305.9 constituents have to be constructed and ittakes about 25 sec to process a sentence on the IBMPC/AT.
Such computation requirements will makethis strategy practically difficult for manyapplications.
All these test data together with the?
results for the other three parsing strategies 2-4 arelisted in Table 1 for comparison.The basic concept of parsing strategy 2(using the probability selection principle and thefirst-1 rule, as listed in Section 3 ) is to use theprobabilities of the constituents o select he searchdirection such that significant reduction incomputation requirements can be achieved.
The testresults (in the fourth row of Table 1) show that withthis strategy for each test sentence in average only152.4 constituents are constructed and it takes onlyabout 12 see to process a sentence on the PC~AT,and the high correct rate of recognition of parsingstrategy 1is almost preserved, i.e., 96.0%.
Thereforethis strategy represents a very good made, off, i.e., thecomputation requirements are reduced by a factor of0.50 ( the constituent reduction ratio in the lastsecond column of Table 1 is the ration of the averagenumber of built constituents o that of Strategy 1),while the correct rate is only degraded by 2.3%.However, such a speed (12 sac for a sentence) is stillvery low especially if real-time operation isconsidered.6.
Test Results (1I) --Improved Best-first ParsingStrategiesIn a further analysis all of the constituentsconstructed by parsing strategy 1 were first dividedinto two classes: correct constituents and noisyconstituents.
A correct constituent is a constituentwithout any component noisy word hypothesis;while a noisy constituent is a constituent which isnot correct.
These two classes of constituents werethen categorized according to their length (number ofword hypotheses in the constituents).
The averageprobability values for each category of correct andnoisy constituents were then evaluated.
The resultsare plotted in Fig.
2, where the vertical axis showsthe average probability values and the horizontal axisdenotes the length of the constituent.
Someobservations can be made as in the following.
First,it can be seen that the two curves in Fig.
2 apparentlydiverge, especially for longer constituents, whichimplies that the Markovian probabilities caneffectively discriminate the noisy constituents againstthe correct constituents (note that all of thozeconstituents are grammatical), especially for longerconstituents.
This is exactly why parsing strateg~ :Iand 2 can provide very high correct rat~,~.Furthermore, Fig.
2 also shows that in gene~lthe probabilities for shorter constituents wo~(iusually be much higher than those for longerconstituents.
This means with parsing strategy 2almost all short constituents; no matter noisy or296correct, would be constructed first, and only thoselong noisy constituents with lower probability valuescan be rejected by the parsing strategy 2.
This thusleads to the parsing strategies 3 and 4 discussedbelow.In parsing strategy 3 (using thelength/probability selection principle and First-1 rule,as listed in Section 3), the length of a constituent isconsidered first, because it is found that the correctconstituents have much better chance to be obtainedvery quickly by means of the Markovian probabilitiesfor longer constituents than shorter correctconstituents, as discussed in the above.
In this way,the construction of the desired constituents would bemuch more faster and very significant reduction incomputation requirements can be achieved.
The testresults in the fifth row of Table 1 show that with thisstrategy in average only 70.2 constituents wereconstructed for a sentence, a constituent reductionratio of 0.27 is found, and it takes only about 4 sec toprocess a sentence on PC/AT, which is now veryclose to real-time.
However, the correct rate ofrecognition is seriously degraded to as low as 85.8%,apparently because some correct constituents havebeen missed due to the high speed constructionprinciple.
Fortunately, after a series of experiments, itwas found that in this case the correct sentences veryoften appeared as the second or the third constructedsentences, if not the first.
Therefore, the parsingstrategy 4 is proposed below, in which everything isthe same as parsing strategy 3 except hat the first-1decision rule is replaced by the first-3 decision rule.
Inother words, those missed correct constituents canvery possibly be picked up in the next few steps, ifthe final decision can be slightly delayed.The test results for parsing strategy 4 listed inthe sixth row of Table 1 show that with this strategythe correct rate of recognition has been improved to93.8% and the computation complexity is still closeto that of parsing strategy 3, i.e., the average numberof constructed constituents for a sentence is 91.0, ittakes about 5 sec to process a sentence, and aconstituent reduction ratio of 0.29 is achieved.
This isapparently a very attractive approach considering boththe accuracy and the computation complexity.
Infact, with the parsing strategy 4, only those noisyword hypotheses which both have relatively highprobabilities and can be unified with theirneighboring word hypotheses can cause interferences.This is why the noisy word hypothesis nterferencescan be reduced, and the present approach is thereforenot sensitive at all to the increased number of noisyword hypotheses in a very large vocabularyenvironment.
Note that although intuitively theintegration of grammatical nd statistical approacheswould imply more computation requirements, buthere in fact the preference-first algorithm providescorrect directions of search such that many noisyconstituents are simply rejected and the reduction o fthe computation complexity makes such ahintegration also very attractive in terms ofcomputation requirements.7.
Concluding RemarksIn this paper, we have proposed an efficientlanguage processor  for speech recognit ionapplications, in which the unification grammar andthe Markov language model are properly integrated inTest I(Unification grammar only)Test II(Markov languag,model only)construction decision Correct rates o Number of Constituent Approximated avq rage time requirecprinciples rules recognition built constituent reduction ratio (See/Sentence)73.8 % 305.9 1,00  2582.2 %parsing ~u'ategy 1 the random the highestselection prineipl probability 98.3 % 305.9 1.00 25parsing strategy 2 the probability First-1 96.0 % 152.4 0:50 12~eleetion principle rule 'First-1parsing strategy 3 rule 85.8 % 70.2 0,27 4the length/pro-bability selectionprinciplethe length/pro-bability selectionprincipleFirst-3 93.8 % 91.0 0.29 5rule parsing strategy 4Table 1 Test results for the two initial tests and four parsing strategies.297a preference-first parsing algorithm defined on anaugmented chart.
Because the unification-basedanalysis eliminates all illegal combinations and theMarkovian probabilities of constituents indicates thecorrect direction of processing, a very high correct rateof recognition can be obtained.
Meanwhile, manyunnecessary computations can be effectivelyeliminated and very high processing speed obtaineddue to the significant reduction of the huge searchspace.
This preference-first language processor isquite general, in which many different parsingstrategies defined by appropriately chosenconstruction principles and decision rules can beeasily implemented for different speech recognitionapplications.References :Chien, L. F., Chen, K. J. and Lee, L. S. (1990b).
AnAugmented Chart Parsing Algorithm IntegratingUnification Grammar and Markov Language Modelfor Continuous Speech Recognition.
Proceedings ofthe IEEE 990 International Conference on Acoustics,Speech and Signal Processing, Albuquerque, NM,USA, Apr.
1990.Chien, L. F., Chert, K. J. and Lee, L. S. (1990a).
AnAugmented Chart Data Structure with Efficient WordLattice Parsing Scheme in Speech RecognitionApplications.
To appear on Speech Communication.,also in Proceedings of the 13th InternationalConference on Computational Linguistics, July1990, pp.
60-65.Derouault A. and Merialdo B.
(1986).
NaturalLanguage Modeling for Phoneme-to-TextTranscription, IEEE Trans.
on PAM1, Vol.
PAMI-8,pp.
742-749.Hayes, P. J. et al (1986).
Parsing SpokenLanguage:A Semantic Caseframe Approach.Proceedings of the l \ ]  th International Conference on1(7:10-:Average 1 O"l~ob~tbilityvalue, s10 "$a61'mmmmm.
-Computational Linguistics, University of Bonn, pp.587-592.Jelinek, F. (1976).
Continuous Speech Recognitionby Statistical Methods, Prec.
IEEE, Vol.
64(4), pp.532-556, Apr.
1976.Kay M. (1980).
Algorithm Schemata nd DataStructures in Syntactic Processing.
Xerox ReportCSL-80-12, pp.
35-70, Pala Alto.Lee, L. S. et al (1990).
A Mandarin DictationMachine Based Upon A Hierarchical RecognitionApproach and Chinese Natural Language Analysis,IEEE Trans.
on Pattern Analysis and MachineIntelligence, Vol.
12, No.
7.
July 1990, pp.
695-704.O'Shaughnessy, D. (1989).
Using SyntacticInformation to Improve Large Vocabulary WordRecognition, ICASSP'89, pp.
715-718.Sheiber, S. M. (1986).
An Introduction toUnification-Based Approaches to Grammar.University of Chicago Press, Chicago.Thompson, H. and Ritchie, G. (1984).
ImplementingNatural Language Parsers, in Artificial Intelligence,Tools, Techniques, and Applications, O'shea, T. andElsenstadt, M. (eds), Harper&Row, Publishers, Inc.Tomita, M. (1986).
An Efficient Word LatticeParsing Algorithm for Continuous SpeechRecognition.
Proceedings of the 1986 InternationalConference on Acoustic, Speech and SignalProcessing, pp.
1569-1572.Cox~ct  constituentsNoisy constituentsFig.
2Constituent lengthThe average probability values for the correct and noisy constituents with differentlengths constructed by parsing strategy 1.298
