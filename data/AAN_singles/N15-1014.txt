Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 133?142,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsHEADS: Headline Generation as Sequence Prediction Using an AbstractFeature-Rich SpaceCarlos A. Colmenares?Google Inc.Brandschenkestrasse 1108002 Zurich, Switzerlandcrcarlos@google.comMarina LitvakShamoon Collegeof EngineeringBeer Sheva, Israelmarinal@sce.ac.ilAmin Mantrach Fabrizio SilvestriYahoo Labs.Avinguda Diagonal 17708018 Barcelona, Spain{amantrach,silvestr}@yahoo-inc.comAbstractAutomatic headline generation is a sub-taskof document summarization with many re-ported applications.
In this study we presenta sequence-prediction technique for learninghow editors title their news stories.
The intro-duced technique models the problem as a dis-crete optimization task in a feature-rich space.In this space the global optimum can be foundin polynomial time by means of dynamic pro-gramming.
We train and test our model on anextensive corpus of financial news, and com-pare it against a number of baselines by us-ing standard metrics from the document sum-marization domain, as well as some new onesproposed in this work.
We also assess thereadability and informativeness of the gener-ated titles through human evaluation.
The ob-tained results are very appealing and substan-tiate the soundness of the approach.1 IntroductionDocument summarization, also known as text sum-marization, is the process of automatically abridgingtext documents.
Although traditionally the final ob-jective of text summarization is to produce a para-graph or abstract that summarizes a rather large col-lection of texts (Mani and Maybury, 1999; Das andMartins, 2007; Nenkova and McKeown, 2012), thetask of producing a very short summary comprisedof 10?15 words has also been broadly studied.
Therehave been many reported practical applications forthis endeavor, most notably, efficient web browsing?Work done during an internship at Yahoo Labs.on hand-held devices (Buyukkokten et al, 2001),generation of TV captions (Linke-Ellis, 1999), dig-itization of newspaper articles that have uninforma-tive headlines (De Kok, 2008), and headline gener-ation in one language based on news stories writtenin another (Banko et al, 2000; Zajic et al, 2002).In general terms, a headline of a news article canbe defined as a short statement that gives a readera general idea about the main contents of the storyit entitles (Borko and Bernier, 1987; Gattani, 2007).The objective of our study is to develop a novel tech-nique for generating informative headlines for newsarticles, albeit to conduct experiments we focusedon finance articles written in English.
In this workwe make a number of contributions concerning sta-tistical models for headline generation, training ofthe models, and their evaluation, specifically:?
We propose a model that learns how an edi-tor generates headlines for news articles, wherea headline is regarded as a compression ofits article?s text.
Our model significantly dif-fers from others in the way it represents pos-sible headlines in a feature-rich space.
Themodel tries to learn how humans discern be-tween good and bad compressions.
Further-more, our model can be trained with any mono-lingual corpus consisting of titled articles, be-cause it does not request special conditions onthe headlines?
structure or provenance.?
We suggest a slight change of the Mar-gin Infused Relaxed Algorithm (Crammer andSinger, 2003) to fit our model, which yields bet-ter empirical results.133?
We present a simple and elegant algorithm thatruns in polynomial time and finds the globaloptimum of our objective function.
This rep-resents an important advantage of our pro-posal because many former techniques resort toheuristic-driven search algorithms that are notguaranteed to find the global optimum.?
With the intention of overcoming several prob-lems suffered by traditional metrics for auto-matically evaluating the quality of proposedheadlines, we propose two new evaluation met-rics that correlate with ratings given by humanannotators.2 Related workThere has been a significant amount of researchabout headline generation.
As noted by Gattani(2007), it is possible to identify three main trends oftechniques broadly employed through different stud-ies:Rule-based approaches.
These methods makeuse of handcrafted linguistically-based rules for de-tecting or compressing important parts in a docu-ment.
They are simple and lightweight, but fail atexploring complex relationships in the text.
Themost representative model for this group is theHedge Trimmer (Dorr et al, 2003).Statistics-based approaches.
These methodsmake use of statistical models for learning correla-tions between words in headlines and in the articles.The models are fit under supervised learning envi-ronments and therefore need large amounts of la-belled data.
One of the most influential works in thiscategory is the Na?
?ve Bayes approach presented byBanko et al (2000), and augmented in works such asJin and Hauptmann (2001; Zajic et al (2002).
Theuse of statistical models for learning pruning-rulesfor parse trees has also been studied, the most no-table work on this area is presented in Knight andMarcu (2001) and extended by Unno et al (2006).Summarization-based approaches.
Headlinescan be regarded as very short summaries, thereforetraditional summarization methods could be adaptedfor generating one-line compressions; the commontrend consists in performing multiple or combinedsteps of sentence selection and compression (Ha-jime et al, 2013; Martins and Smith, 2009).
Themain problem with these approaches is that theymake use of techniques that were not initiallydevised for generating compressions of less than10% of the original content, which directly affectsthe quality of the resulting summary (Banko etal., 2000).
It is noteworthy to highlight that mostof the modern summarization-based techniquesopt for generating headlines just by recycling andreordering words present in the article, which alsoraises the risk of losing or changing the contextualmeaning of the reused words (Berger and Mittal,2000).An area that deals with a target similar to headlinegeneration is multi-sentence compression, where itsobjective is to produce a single short phrase thatabridges a set of sentences that conform a document.The main difference between both practices is thatheadline generation is more strict about the length ofthe generated output, which should consist of abouteight tokens (Banko et al, 2000), whereas the latteraccepts longer results.
One of the most recent andcompetitive approaches for multi-sentence compres-sion is described by Filippova (2010).3 Background on sequence predictionSequence models have been broadly used for manyNatural Language Processing tasks, such as iden-tification of sentence boundaries (Reynar and Rat-naparkhi, 1997), named entity recognition (McCal-lum and Li, 2003), part of speech tagging (Kupiec,1992), dependency tree parsing (McDonald et al,2005), document summarization (Shen et al, 2007),and single-sentence compression (McDonald, 2006;Nomoto, 2007).
These models are formalizations ofrelationships between observed sequences of vari-ables and predicted categories for each one.
Math-ematically, let X = {x1, x2, ..., xN} be a finiteset of possible atomic observations, and let Y ={y1, y2, ..., yM} be a finite set of possible categoriesthat each atomic observation could belong to.Statistical sequence models try to approximate aprobability distribution P with parameters ?
capa-ble of predicting for any sequence of n observationsx ?
Xn, and any sequence of assigned categoriesper observation y ?
Yn, the probability P (y|x;?
).The final objective of these models is to predict the134most likely sequence of categories?y ?
Ynfor anyarbitrary observation sequence, which can be ex-pressed as:?y = argmaxy?YnP (y|x;?
)There have been many proposals for modelling theprobability distribution P .
Some of the most pop-ular proposals are Hidden Markov Models (Rabinerand Juang, 1986), local log-linear classifiers, Max-imum Entropy Markov Models (McCallum et al,2000), and Conditional Random Fields (Lafferty etal., 2001).
The following two sections will brieflyintroduce the latter, together with a widely used im-provement of the model.3.1 Conditional Random Fields (CRF)As presented by Lafferty et al (2001), CRF aresequence prediction models where no Markov as-sumption is made on the sequence of assigned cate-gories y, but a factorizable global feature function isused so as to transform the problem into a log-linearmodel in feature space.
Formally, CRF model theprobability of a sequence in the following way:P (y|x;?)
=exp{w ?
F (x,y)}Z(x)Where ?
= {w} and w ?
Rmis a weight vec-tor, F : Xn?
Yn?
Rmis a global feature func-tion of m dimensions, and Z(x) is a normalizationfunction.
Moreover, the global feature function isdefined in the following factored way:F (x,y) =n?i=1f(x, i, yi-1, yi)where f : X?
?N+?Y?Y ?
Rmis a local featurefunction.
Due to this definition, it can be shown thatthe decoding of CRF is equivalent to:?y = argmaxy?Ynw ?
F (x,y)Which is a linear classification in a feature space.The fact that the local feature function f only de-pends on the last two assigned categories allows theglobal optimum of the model to be found by meansof a tractable algorithm, whereas otherwise it wouldbe necessary to explore all the |Y|npossible solu-tions.3.2 CRF with state sequencesSince CRF do not assume independence between as-signed categories, it is possible to extend the localfeature function for enabling it to keep more infor-mation about previous assigned categories and notjust the last category.
These models are derived fromthe work on weighted automata and transducers pre-sented in studies such as Mohri et al (2002).
Let Sbe a state space, s0be a fixed initial empty state, andlet function g : S ?X?
?N+?Y ?
S model statetransitions.
Then the global feature function can beredefined as:F (x,y) =n?i=1f(x, i, si-1, yi), si= g(si-1,x, i, yi)This slight change adds a lot of power to CRF be-cause it provides the model with much more infor-mation that it can use for learning complex relations.Finally, the best candidate can be found by solving:?y = argmaxy?Ynw ?
[n?i=1f(x, i, si-1, yi)](1)4 Model description: headlines as bitmapsWe model headline generation as a sequence predic-tion task.
In this manner a news article is seen as aseries of observations, where each is a possible to-ken in the document.
Furthermore, each observa-tion can be assigned to one of two categories: in-headline, or not in-headline.
Note that this approachallows a generated headline to be interpreted as abitmap over the article?s tokens.If this set-up was used for a CRF model, the stan-dard local feature function f(x, i, yi-1, yi) wouldonly be able to know whether the previous tokenwas taken or not, which would not be very infor-mative.
For solving the problem we integrate a statesequence into the model, where a state s ?
S holdsthe following information:?
The last token that was chosen as part of theheadline.?
The part-of-speech tag1of the second-to-lastword that was selected as part of the headline.1We used the set of 45 tags from the Penn Treebank Tag-Set.135?
The number of words already chosen to be partof the headline, which could be zero.Therefore, the local feature function f(x, i, si-1, yi)will not only know about the whole text and the cur-rent token xi, whose category yiis to be assigned,but it will also hold information about the headlineconstructed so far.
In our model, the objective of thelocal feature function is to return a vector that de-scribes in an abstract euclidean space the outcomeof placing, or not placing, token xiin the headline,provided that the words previously chosen form thestate si-1.
We decide to make this feature vector con-sist of 23 signals, which only fire if the token xiisplaced in the headline (i.e., yi= 1).
The signals canbe grouped into the following five sets:Language-model features: they assess the gram-maticality of the headline being built.
The first fea-ture is the bigram probability of the current token,given the last one placed on the headline.
The sec-ond feature is the trigram probability of the PoS tagof the current token, given the tags of the two lasttokens on the headline.Keyword features: binary features that help themodel detect whether the token under analysis is asalient word.
The document?s keywords are calcu-lated as a preprocessing step via TF-IDF weighting,and the features fire depending on how good or badthe current token xiis ranked with respect to the oth-ers on the text.Dependency features: in charge of informing themodel about syntactical dependencies among the to-kens placed on the headline.
For this end the depen-dency tree of all the sentences in the news article arecomputed as a pre-processing step2.Named-entity features: help the system identifynamed entities3in the text, including those that arecomposed of contiguous tokens.Headline-length features: responsible for en-abling the model to decide whether a headline istoo short or too long.
As many previous studies re-port, an ideal headline must have from 8 to 10 tokens(Banko et al, 2000).
Thus, we include three binaryfeatures that correspond to the following conditions:(1) if the headline length so far is less than or equalto seven; (2) if the headline length so far is greater2We use the Stanford toolkit for computing parse trees.3We only use PER, LOC, and ORG as entity annotations.than or equal to 11; (3) if the token under analysis isassigned to the headline, which is a bias feature.5 Decoding the modelDecoding the model involves solving equation (1)being given a weight vector w, whose value staysunchanged during the process.
A naive way of solv-ing the optimization problem would be to try all pos-sible |Y|nsequence combinations, which would leadto an intractable procedure.
In order to design apolynomial algorithm that finds the global optimumof the aforementioned formula, the following fourobservations must be made:(1) Our model is designed so that the local fea-ture function only fires when a token is selected forbeing part of a headline.
Then, when evaluating anarbitrary solution y, only the tokens placed on theheadline must be taken into account.
(2) When applying the local feature function toa particular token xi(assuming yi= 1), the resultof the function will vary only depending on the pro-vided previous state si-1; all the other parameters arefixed.
Moreover, a new state siwill be generated,which in turn will include token xi.
This implies thatthe entire evaluation of a solution can be completelymodeled as a sequence of state transitions; i.e., it be-comes possible to recover a solution?s bitmap froma sequence of state transitions and vice-versa.
(3) When analyzing the local feature function atany token xi, the amount of different states si-1thatcan be fed to the function depend solely on the to-kens taken before, for which there are 2i-1differentcombinations.
Nevertheless, because a state onlyholds three pieces of information, a better upper-bound to the number of possible reachable states isequal to i2?
|PoS|, which accounts to: all possiblecandidates for the last token chosen before xi, timesall possible combinations of total number of tokenstaken before xi, times all possible PoS tags of theone-before-last token taken before xi.
(4) The total amount of producible states inthe whole text is equal to?ni=1i2?
|PoS| =O(n3?
|PoS|).
If the model is also con-strained to produce headlines containing no morethan H tokens, the asymptotic bound drops toO(H ?
n2?
|PoS|).136The final conclusion of these observations is asfollows: since any solution can be modelled as achain of state sequences, the global optimum canbe found by generating all possible states and fetch-ing the one that, when reached from the initial state,yields the maximum score.
This task is achievablewith a number of operations linearly proportionalto the number of possible states, which at the sametime is polynomial with respect to the number of to-kens in the document.
In conclusion, the model canbe decoded in quadratic time.
The pseudo-code inalgorithm 1 gives a sketch of a O(H ?
n2?
|PoS|)bottom-up implementation.6 Training the model: learning whathuman-generated headlines look likeThe global feature function F is responsible for tak-ing a document and a bitmap, and producing a vec-tor that describes the candidate headline in an ab-stract feature space.
We defined the feature functionso it only focuses on evaluating how a series of to-kens that comprise a headline relate to each otherand to the document as a whole.
This implies thatif h = {h1, h2, ..., hk} is the tokenized form of anyarbitrary headline consisting of k tokens, and we de-fine vectors a ?
Xk+nand b ?
Yk+nas:a = {h1, h2, ..., hk, x1, x2, ..., xn}b = {11, 12, ..., 1k, 01, 02, ..., 0n}where a is the concatenation of h and x, and b isa bitmap for only selecting the actual headline to-kens, it follows that the feature vector that resultsfrom calling the global feature function, which wedefine asu = F (a, b) (2)is equivalent to a description of how headline h re-lates to document x.
This observation is the core ofour learning algorithm, because it implies that it ispossible to ?insert?
a human-generated headline inthe text and get its description in the abstract fea-ture space induced by F .
The objective of the learn-ing process will consist in molding a weight vec-tor w, such that it makes the decoding algorithm fa-vor headlines whose descriptions in feature space re-semble the characteristics of human-generated titles.For training the model we follow the on-linelearning schemes presented by Collins (2002) andAlgorithm 1 Sketch of a bottom-up algorithm for find-ing the top-scoring state s?that leads to the global op-timum of our model?s objective function.
It iterativelycomputes two functions: pi(i, l), which returns the setof all reachable states that correspond to headlines hav-ing token-length l and finishing with token xi, and ?
(s),which returns the maximum score that can be obtainedby following a chain of state sequences that ends in theprovided state, and starts on s0.1: //Constants.2: H ?
Max.
number of allowed tokens in headlines.3: n?
Number of tokens in the document.4: x?
List of n tokens (document).5: w ?Weight vector.6: g ?
State transition function.7: f ?
Local feature function.8: s0?
Init state.9: //Variables.10: pi ?
new Set<State>[n+ 1][H + 1]({})11: ??
new Float[|State|](??
)12: s??
s013: //Base cases.14: ?(s0)?
015: for i in {0, ..., n} do16: pi(i, 0)?
{s0}17: //Bottom-up fill of pi and ?.18: for l in {1, ...,H} do19: for i in {l, ..., n} do20: for j in {l ?
1, ..., i?
1} do21: for z in pi(j, l ?
1) do22: s?
g(z, x, i, 1)23: sscore?
?
(z) +w ?
f(x, i, z, 1)24: pi(i, l)?
pi(i, l) ?
{s}25: ?(s)?
max(?
(s), sscore)26: if ?
(s) > ?(s?)
then27: s??
sapplied in studies that deal with CRF models withstate sequences, such as the dependency parsingmodel of McDonald et al (2005).
The learningframework consists of an averaged perceptron thatiteratively sifts through the training data and per-forms the following error-correction update at eachstep:w??
w + ?
?
(u?
?v),?v = F (x,?y)whereu is the vector defined in equation (2),?y is theresult of solving equation (1) with the current weight137vector, and ?
?
R is a learning factor.
We try threedifferent values for ?
, which lead to the followinglearning algorithms:?
Perceptron: ?
= 1?
MIRA: ?
= max(0,1?w?(u??v)||u??v||2)?
Forced-MIRA: ?
=1?w?(u??v)||u?
?v||2The first value is a simple averaged perceptron aspresented by Collins (2002), the second value is aMargin Infused Relaxed Algorithm (MIRA) as pre-sented by Crammer and Singer (2003), and the thirdvalue is a slight variation to the MIRA update.
Wepropose it for making the algorithm acknowledgethat the objective feature vector u cannot be pro-duced from document x, and thus force an updateat every step.
The reason for this is that if w ?
u >w ?
?v, then MIRA sets ?
= 0, because an error wasnot made (i.e.
the human-generated headline got ahigher score than all of the others).
Nevertheless,we observed in our experiments that this behaviourbiases the process towards learning weights that ex-ploit patterns that can occur in human-generated ti-tles, but are almost never observed in the titles thatcan be generated by our model, which hinders thequality of the final headlines.7 Automatic evaluation set-up7.1 Evaluation metricsFor performing an automatic evaluation of the head-lines generated by our system we follow the pathtaken by related work such as Zajic et al (2004) anduse a subset of the ROUGE metrics for comparingcandidate headlines with reference ones, which havebeen proven to strongly correlate with human evalu-ations (Lin, 2003; Lin, 2004).We decide to use as metrics ROUGE-1, ROUGE-2, and ROUGE-SU.
We also propose as an experi-mental new metric a weighted version of ROUGE-SU, which we name ROUGE-WSU.
The rationale ofour proposal is that ROUGE-SU gives the same im-portance to all skip-bigrams extracted from a phraseno matter how far apart they are.
We address theproblem by weighting each shared skip-gram be-tween phrases by the inverse of the token?s averagegap distance.
Formally:Sim(R,C) =?(a,b)?su(R)?su(C)2distR(a,b)+distC(a,b)?
(a,b)?su(R)1distR(a,b)Where function distH(a, b) returns the skip distancebetween tokens ?a?
and ?b?
in headline H , andsu(H) returns all skip-bigrams in headline H .With the objective of having a metric capable ofdetecting abstract concepts in phrases and compar-ing headlines at a semantic level, we resort to LatentSemantic Indexing (LSI) (Deerwester et al, 1990).We use the method for extracting latent conceptsfrom our training corpus so as to be able to repre-sent text in an abstract latent space.
We then com-pute the similarity of a headline with respect to anews article by calculating the cosine similarity oftheir vector representations in latent space.7.2 BaselinesIn order to have a point of reference for interpretingthe performance of our model, we implement fourbaseline models.
We arbitrarily decide to make allthe baselines generate, if possible, nine-token-longheadlines, where the last token must always be a pe-riod.
This follows from the observation that goodheadlines must contain about eight tokens (Banko etal., 2000).
The implemented baselines are the fol-lowing:Chunked first sentence: the first eight tokensfrom the article, plus a period at the end.Hidden Markov Model: as proposed by Zajicet al (2002), but adapted for producing eight-tokensentences, plus an ending period.Word Graphs: as proposed by Filippova (2010).This is a state-of-the-art multi-sentence compressionalgorithm.
To ensure it produces headlines as out-put, we keep the shortest path in the graph withlength equal to or greater than eight tokens.
An end-ing period is appended if not already present.
Notethat the original algorithm would produce the top-k shortest paths and keep the one with best averageedge weight, not caring about its length.Keywords: the top eight keywords in the article,as ranked by TF-IDF weighting, sorted in descend-ing order of relevance.
This is not a real baseline138because it does not produce proper headlines, but itis used for naively trying to maximize the achievablevalue of the evaluation metrics.
This is based on theassumption that keywords are the most likely tokensto occur in human-generated headlines.7.3 Experiments and ResultsWe trained our model with a corpus consisting ofroughly 1.3 million financial news articles fetchedfrom the web, written in English, and published onthe second half of 2012.
We decided to add three im-portant constraints to the learning algorithm whichproved to yield positive empirical results:(1) Large news articles are simplified by eliminat-ing their most redundant or least informative sen-tences.
For this end, the text ranking algorithm pro-posed by Mihalcea and Tarau (2004) is used for dis-criminating salient sentences in the article.
Further-more, a news article is considered large if it has morethan 300 tokens, which corresponds to the averagenumber of words per article in our training set.
(2) Because we observed that less than 2% of theheadlines in the training set contained more than 15tokens, we constraint the decoding algorithm to onlygenerate headlines consisting of 15 or fewer tokens.
(3) We restrain the decoding algorithm fromplacing symbols such as commas, quotation marks,and question marks on headlines.
Nonetheless, onlyheadlines that end with a period are considered assolutions; otherwise the model tends to generatenon-conclusive phrases as titles.For automated testing purposes we use a trainingset consisting of roughly 12,000 previously unseenarticles, which were randomly extracted from theinitial dataset before training.
The evaluation con-sisted in producing seven candidate headlines per ar-ticle: one for each of the four baselines, plus one foreach of the three variations of our model (each dif-fering solely on the scheme used to learn the weightvector).
Then each candidate is compared againstthe article?s reference headline by means of the fiveproposed metrics.Table 1 summarizes the obtained results of themodels with respect to the ROUGE metrics.
Theresults show that our model, when trained with ourproposed forced-MIRA update, outperforms all theother baselines on all metrics, except for ROUGE-2, where all differences are statistically significantwhen assessed via a paired t-test (p < 0.001).
Also,as initially intended, the keywords baseline doesproduce better scores than all the other methods,therefore it is considered as a naive upper-bound.
Itmust be highlighted that all the numbers on the tableare rather low, this occurs because, as noted by Zajicet al (2002), humans tend to use a very different vo-cabulary and writing-style on headlines than on arti-cles.
The effect of this is that our methods and base-lines are not capable of producing headlines withwordings strongly similar to human-written ones,which as a consequence makes it almost impossibleto obtain high ROUGE scores.R-1 R-2 R-SU R-WSUPerceptron 0.157 0.056 0.053 0.082MIRA 0.172 0.042 0.057 0.084f-MIRA 0.187 0.054 0.065 0.0951stsent.
0.076 0.021 0.025 0.038HMM 0.090 0.009 0.023 0.038Word graphs 0.174 0.060 0.060 0.084Keywords 0.313 0.021 0.112 0.148Table 1: Result of the evaluation of our models and base-lines with respect to ROUGE metrics.For having a more objective assessment of ourproposal, we carried out a human evaluation of theheadlines generated by our model when trained withthe f-MIRA scheme and the word graphs approachby Filippova (2010).
For this purpose, 100 arti-cles were randomly extracted from the test set andtheir respective candidate headlines were generated.Then different human raters were asked to evalu-ate on a Likert scale, from 1 to 5, both the gram-maticality and informativeness of the titles.
Eacharticle-headline pair was annotated by three differ-ent raters.
The median of their ratings was chosenas a final mark.
As a reference, the raters were alsoasked to annotate the actual human-generated head-lines from the articles, although they were not in-formed about the provenance of the titles.
We mea-sured inter-judge agreement by means of their Intra-Class Correlation (ICC) (Cicchetti, 1994).
The ICCfor grammaticality was 0.51 ?
0.07, which repre-sents fair agreement, and the ICC for informative-ness was 0.63 ?
0.05, which represents substantialagreement.Table 2 contains the results of the models with139H.
Len.
LSI Gram.
Inf.Perceptron 10.096 0.446 ?
?MIRA 13.045 0.463 ?
?f-MIRA 11.737 0.491 3.45 2.941stsent.
8.932 0.224 ?
?HMM 9.000 0.172 ?
?Word graphs 10.973 0.480 3.69 2.32Keywords 9.000 0.701 ?
?Reference 11.898 0.555 4.49 4.14Table 2: Result of the evaluation of our models and base-lines with LSI document similarity, grammaticality andinformativeness as assessed by human raters, and aver-age headline length.respect to the LSI document similarity metric, andthe human evaluations for grammaticality and in-formativeness.
For exploratory purposes the tablealso contains the average length for the generatedheadlines of each of the models (which also countsthe imposed final period).
The results in this ta-ble are satisfying: with respect to LSI documentsimilarity, our model outperforms all of the base-lines and its value is close to the one achieved byhuman-generated headlines.
On the other hand, thehuman evaluations are middling: the word-graphsmethod produces more readable headlines, but ourmodel proves to be more informative because it doesbetter work at detecting abstract word relationshipsin the text.
All differences in this table are statis-tically significant when computed as paired t-tests(p < 0.001).It is worth noting that the informativeness ofhuman-generated headlines did not get a high score.The reason for this is the fact that editors tend toproduce rather sensationalist or partially informativetitles so as to attract the attention of readers and en-gage them to read the whole article; human raters pe-nalized the relevance of such headlines, which wasreflected on this final score.Finally, table 3 contains the mutual correlation be-tween automated and manual metrics.
The first thingto note is that none of the used metrics proved to begood for assessing grammaticality of headlines.
It isalso worth noting that our proposed metric ROUGE-WSU performs as well as the other ROUGE metrics,and that the proposed LSI document similarity doesnot prove to be as strong a metric as the others.R-1 R-2 R-SU R-WSU LSI-DSGram.
-0.130 -0.084 -0.131 -0.132 -0.015Inf.
0.561 0.535 0.557 0.542 0.370Table 3: Spearman correlation between human-assessedmetrics and automatic ones.8 Conclusions and DiscussionIn this study we proposed a CRF model with statetransitions.
The model tries to learn how humanstitle their articles.
The learning is performed bymeans of a mapping function that, given a doc-ument, translates headlines to an abstract feature-rich space, where the characteristics that distinguishhuman-generated titles can be discriminated.
Thisabstraction allows our model to be trained with anymonolingual corpus of news articles because it doesnot impose conditions on the provenance of stories?headlines ?i.e, our model maps reference headlinesto a feature space and only learns what abstract prop-erties characterize them.
Furthermore, our model al-lows defining the task of finding the best possibleproducible headline as a discrete optimization prob-lem.
By doing this each candidate headline is mod-elled as a path in a graph of state sequences, thusallowing the best-scoring path to be found in poly-nomial time by means of dynamic programming.Our results, obtained through reliable automatic andhuman-assessed evaluations, provide a proof of con-cept for the soundness of our model and its capabili-ties.
Additionally, we propose a new evaluation met-ric, ROUGE-WSU, which, as shown in table 3, cor-relates as good as traditional ROUGE metrics withhuman evaluations for informativeness of headlines.The further work we envisage for augmenting ourresearch can be grouped in the following areas:?
Exploring more advanced features that manageto detect abstract semantic relationships or dis-course flows in the compressed article.?
Complementing our system with a separatetranslation model capable of transforming to?Headlinese?
the titles generated with the lan-guage used in the bodies of articles.?
Attempting to achieve a more objective evalua-tion of our generated headlines, through the useof semantic-level measures.140AcknowledgmentsWe thank Jordi Atserias, Horacio Saggion, Hora-cio Rodr?
?guez, and Xavier Carreras, who helped andsupported us during the development of this study.References[Banko et al2000] Michele Banko, Vibhu O. Mittal, andMichael J. Witbrock.
2000.
Headline generationbased on statistical translation.
Proceedings of the38th Annual Meeting on Association for Computa-tional Linguistics.
[Berger and Mittal2000] B. Berger and N. Mittal.
2000.Discourse segmentation in aid of document summa-rization.
Proceedings of the Hawaii InternationalConference on System Sciences, Minitrack on DigitalDocuments Understanding.
[Borko and Bernier1987] H. Borko and C. Bernier.
1987.Abstracting concepts and methods.
New York: Aca-demic Press.
[Buyukkokten et al2001] Orkut Buyukkokten, HectorGarcia-Molina, and Andreas Paepcke.
2001.
Seeingthe whole in parts: text summarization for web brows-ing on handheld devices.
Proceedings of the 10th in-ternational conference on World Wide Web.
ACM.
[Cicchetti1994] Domenic V. Cicchetti.
1994.
Guidelines,criteria, and rules of thumb for evaluating normed andstandardized assessment instruments in psychology.Psychological assessment 6.4.
[Collins2002] Michael Collins.
2002.
Discriminativetraining methods for hidden markov models: Theoryand experiments with perceptron algorithms.
Proceed-ings of the ACL-02 conference on Empirical methodsin natural language processing-Volume 10.
[Crammer and Singer2003] Koby Crammer and YoramSinger.
2003.
Ultraconservative online algorithms formulticlass problems.
The Journal of Machine Learn-ing Research 3.
[Das and Martins2007] Dipanjan Das and Andr?e FT Mar-tins.
2007.
A survey on automatic text summariza-tion.
Literature Survey for the Language and StatisticsII course at CMU 4, pages 192?195.
[De Kok2008] D. De Kok.
2008.
Headline generationfor dutch newspaper articles through transformation-based learning.
Master Thesis.
[Deerwester et al1990] Scott C. Deerwester et al 1990.Indexing by latent semantic analysis.
JASIS 41.6.
[Dorr et al2003] Bonnie Dorr, David Zajic, and RichardSchwartz.
2003.
Hedge trimmer: a parse-and-trim approach to headline generation.
Proceedings ofthe HLT-NAACL 03 on Text summarization workshop,pages 1?8.
[Filippova2010] Katja Filippova.
2010.
Multi-sentencecompression: Finding shortest paths in word graphs.Proceedings of the 23rd International Conference onComputational Linguistics.
[Gattani2007] Akshay Kishore Gattani.
2007.
Auto-mated natural language headline generation using dis-criminative machine learning models.
Diss.
SimonFraser University.
[Hajime et al2013] Morita Hajime et al 2013.
Subtreeextractive summarization via submodular maximiza-tion.
Proceedings of 51st Annual Meeting of the As-sociation for Computational Linguistics.
[Jin and Hauptmann2001] Rong Jin and Alexander G.Hauptmann.
2001.
Title generation using a trainingcorpus.
CICLing ?01: Proceedings of the Second In-ternational Conference on Computational Linguisticsand Intelligent Text Processing, pages 208?215.
[Knight and Marcu2001] Kevin Knight and DanielMarcu.
2001.
Summarization beyond sentenceextraction: A probabilistic approach to sentencecompression.
Artificial Intelligence 139.1, pages91?107.
[Kupiec1992] Julian Kupiec.
1992.
Robust part-of-speech tagging using a hidden markov model.
Com-puter Speech and Language 6.3.
[Lafferty et al2001] John Lafferty, Andrew McCallum,and Fernando CN Pereira.
2001.
Conditional randomfields: Probabilistic models for segmenting and label-ing sequence data.
[Lin2003] C.Y.
Lin.
2003.
Cross-domain study of n-gram co-occurrence metrics.
Proceedings of the Work-shop on Machine Translation Evaluation, New Or-leans, USA.
[Lin2004] Chin-Yew Lin.
2004.
Rouge: A package forautomatic evaluation of summaries.
Text Summariza-tion Branches Out: Proceedings of the ACL-04 Work-shop.
[Linke-Ellis1999] N. Linke-Ellis.
1999.
Closed caption-ing in america: Looking beyond compliance.
Pro-ceedings of the TAO Workshop on TV Closed Captionsfor the Hearing Impaired People, Tokyo, Japan, pages43?59.
[Mani and Maybury1999] Inderjeet Mani and MarkT.
Maybury Maybury.
1999.
Advances in automatictext summarization, volume 293.
Cambridge: MITpress.
[Martins and Smith2009] Andr?e F.T.
Martins andNoah A. Smith.
2009.
Summarization with ajoint model for sentence extraction and compres-sion.
NAACL-HLT Workshop on Integer LinearProgramming for NLP, Boulder, USA.
[McCallum and Li2003] Andrew McCallum and Wei Li.2003.
Early results for named entity recognition with141conditional random fields, feature induction and web-enhanced lexicons.
Proceedings of the seventh con-ference on Natural language learning at HLT-NAACL2003-Volume 4.
[McCallum et al2000] Andrew McCallum, Dayne Fre-itag, and Fernando CN Pereira.
2000.
Maximum en-tropy markov models for information extraction andsegmentation.
ICML.
[McDonald et al2005] Ryan McDonald, Koby Crammer,and Fernando Pereira.
2005.
Online large-margintraining of dependency parsers.
Proceedings of the43rd Annual Meeting on Association for Computa-tional Linguistics.
[McDonald2006] Ryan T. McDonald.
2006.
Discrimi-native sentence compression with soft syntactic evi-dence.
EACL.
[Mihalcea and Tarau2004] Rada Mihalcea and Paul Ta-rau.
2004.
Textrank: Bringing order into texts.
As-sociation for Computational Linguistics.
[Mohri et al2002] Mehryar Mohri, Fernando Pereira, andMichael Richard.
2002.
Weighted finite-state trans-ducers in speech recognition.
Computer Speech andLanguage 16.1.
[Nenkova and McKeown2012] Ani Nenkova and Kath-leen McKeown.
2012.
A survey of text summariza-tion techniques.
Mining Text Data.
Springer US, pages43?76.
[Nomoto2007] Tadashi Nomoto.
2007.
Discriminativesentence compression with conditional random fields.Information processing and management 43.6.
[Rabiner and Juang1986] Lawrence Rabiner and Biing-Hwang Juang.
1986.
An introduction to hiddenmarkov models.
ASSP Magazine, IEEE 3.1, pages 4?16.
[Reynar and Ratnaparkhi1997] Jeffrey C. Reynar and Ad-wait Ratnaparkhi.
1997.
A maximum entropy ap-proach to identifying sentence boundaries.
Proceed-ings of the fifth conference on Applied natural lan-guage processing.
[Shen et al2007] Dou Shen et al 2007.
Document sum-marization using conditional random fields.
IJCAI.Vol.
7.
[Unno et al2006] Yuya Unno et al 2006.
Trimmingcfg parse trees for sentence compression using ma-chine learning approaches.
Proceedings of the COL-ING/ACL on Main conference poster sessions.
[Zajic et al2002] David Zajic, Bonnie Dorr, and RichardSchwartz.
2002.
Automatic headline generation fornewspaper stories.
Workshop on Automatic Summa-rization.
[Zajic et al2004] David Zajic, Bonnie Dorr, and RichardSchwartz.
2004.
Bbn/umd at duc-2004: Topiary.
Pro-ceedings of the HLT-NAACL 2004 Document Under-standing Workshop, Boston.142
