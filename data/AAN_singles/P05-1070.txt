Proceedings of the 43rd Annual Meeting of the ACL, pages 565?572,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsInstance-based Sentence Boundary Determination by Optimization forNatural Language GenerationShimei Pan and James C. ShawIBM T. J. Watson Research Center19 Skyline DriveHawthorne, NY 10532, USA{shimei,shawjc}@us.ibm.comAbstractThis paper describes a novel instance-based sentence boundary determinationmethod for natural language generationthat optimizes a set of criteria based onexamples in a corpus.
Compared to exist-ing sentence boundary determination ap-proaches, our work offers three signifi-cant contributions.
First, our approachprovides a general domain independentframework that effectively addresses sen-tence boundary determination by balanc-ing a comprehensive set of sentence com-plexity and quality related constraints.Second, our approach can simulate thecharacteristics and the style of naturallyoccurring sentences in an application do-main since our solutions are optimizedbased on their similarities to examplesin a corpus.
Third, our approach canadapt easily to suit a natural language gen-eration system?s capability by balancingthe strengths and weaknesses of its sub-components (e.g.
its aggregation and re-ferring expression generation capability).Our final evaluation shows that the pro-posed method results in significantly bet-ter sentence generation outcomes than awidely adopted approach.1 IntroductionThe problem of sentence boundary determination innatural language generation exists when more thanone sentence is needed to convey multiple conceptsand propositions.
In the classic natural languagegeneration (NLG) architecture (Reiter, 1994), sen-tence boundary decisions are made during the sen-tence planning stage in which the syntactic struc-ture and wording of sentences are decided.
Sentenceboundary determination is a complex process thatdirectly impacts a sentence?s readability (Gunning,1952), its semantic cohesion, its syntactic and lex-ical realizability, and its smoothness between sen-tence transitions.
Sentences that are too complex arehard to understand, so are sentences lacking seman-tic cohesion and cross-sentence coherence.
Furthermore, bad sentence boundary decisions may evenmake sentences unrealizable.To design a sentence boundary determinationmethod that addresses these issues, we employ aninstance-based approach (Varges and Mellish, 2001;Pan and Shaw, 2004).
Because we optimize our so-lutions based on examples in a corpus, the outputsentences can demonstrate properties, such as simi-lar sentence length distribution and semantic group-ing similar to those in the corpus.
Our approachalso avoids problematic sentence boundaries by op-timizing the solutions using all the instances in thecorpus.
By taking a sentence?s lexical and syntac-tic realizability into consideration, it can also avoidsentence realization failures caused by bad sentenceboundary decisions.
Moreover, since our solutioncan be adapted easily to suit the capability of a natu-ral language generator, we can easily tune the algo-rithm to maximize the generation quality.
To the bestof our knowledge, there is no existing comprehen-sive solution that is domain-independent and pos-sesses all the above qualities.
In summary, our workoffers three significant contributions:1.
It provides a general and flexible sentence565boundary determination framework whichtakes a comprehensive set of sentence com-plexity and quality related criteria into consid-eration and ensures that the proposed algorithmis sensitive to not only the complexity of thegenerated sentences, but also their semantic co-hesion, multi-sentence coherence and syntacticand lexical realizability.2.
Since we employ an instance-based method,the proposed solution is sensitive to the styleof the sentences in the application domain inwhich the corpus is collected.3.
Our approach can be adjusted easily to suita sentence generation system?s capability andavoid some of its known weaknesses.Currently, our work is embodied in a multimodalconversation application in the real-estate domain inwhich potential home buyers interact with the sys-tem using multiple modalities, such as speech andgesture, to request residential real-estate informa-tion (Zhou and Pan, 2001; Zhou and Chen, 2003;Zhou and Aggarwal, 2004).
After interpreting therequest, the system formulates a multimedia pre-sentation, including automatically generated speechand graphics, as the response (Zhou and Aggarwal,2004).
The proposed sentence boundary determi-nation module takes a set of propositions selectedby a content planner and passes the sentence bound-ary decisions to SEGUE (Pan and Shaw, 2004), aninstance-based sentence generator, to formulate thefinal sentences.
For example, our system is calledupon to generate responses to a user?s request: ?Tellme more about this house.?
Even though not all ofthe main attributes of a house (more than 20) will beconveyed, it is clear that a good sentence boundarydetermination module can greatly ease the genera-tion process and improve the quality of the output.In the rest of the paper, we start with a discussionof related work, and then describe our instance-baseapproach to sentence boundary determination.
Fi-nally, we present our evaluation results.2 Related WorkExisting approaches to sentence boundary determi-nation typically employ one of the following strate-gies.
The first strategy uses domain-specific heuris-tics to decide which propositions can be combined.For example, Proteus (Davey, 1979; Ritchie, 1984)produces game descriptions by employing domain-specific sentence scope heuristics.
This approachcan work well for a particular application, however,it is not readily reusable for new applications.The second strategy is to employ syntactic, lex-ical, and sentence complexity constraints to con-trol the aggregation of multiple propositions (Robin,1994; Shaw, 1998).
These strategies can generatefluent complex sentences, but they do not take othercriteria into consideration, such as semantic cohe-sion.
Further more, since these approaches do notemploy global optimization as we do, the content ofeach sentence might not be distributed evenly.
Thismay cause dangling sentence problem (Wilkinson,1995).Another strategy described in Mann andMoore(1981) guided the aggregation process byusing an evaluation score that is sensitive to thestructure and term usage of a sentence.
Similar toour approach, they rely on search to find an optimalsolution.
The main difference between this approachand ours is that their evaluation score is computedbased on preference heuristics.
For example, allthe semantic groups existing in a domain have tobe coded specifically in order to handle semanticgrouping.
In contrast, in our framework, the score iscomputed based on a sentence?s similarity to corpusinstances, which takes advantage of the naturallyoccurring semantic grouping in the corpus.Recently, Walker (2002) and Stent (2004) usedstatistical features derived from corpus to rank gen-erated sentence plans.
Because the plan ranker wastrained with existing examples, it can choose a planthat is consistent with the examples.
However, de-pending on the features used and the size of the train-ing examples, it is unclear how well it can capturepatterns like semantic grouping and avoid problemslikes dangling sentences.3 ExamplesBefore we describe our approach in detail, we startwith a few examples from the real-estate domainto demonstrate the properties of the proposed ap-proach.First, sentence complexity impacts sentenceboundary determination.
As shown in Table 1, af-ter receiving a user?s request (U1) for the details of ahouse, the content planner asked the sentence plan-ner to describe the house with a set of attributes in-cluding its asking price, style, number of bedrooms,number of bathrooms, square footage, garage, lotsize, property tax, and its associated town and school566Example Turn SentenceE1 U1 Tell me more about this houseS1 This is a 1 million dollar 3 bedroom, 2 bathroom, 2000 square foot colonialwith 2 acre of land, 2 car garage, annual taxes 8000 dollars in Armonkand in the Byram Hills school district.S2 This is a 1 million dollar house.
This is a 3 bedroom house.
This is a 2 bathroomhouse.
This house has 2000 square feet.
This house has 2 acres of land.This house has 2 car garage.
This is a colonial house.
The annual taxes are 8000 dollars.This house is in Armonk.
This house is in the Byram Hills school district.S3 This is a 3 bedroom, 2 bathroom, 2000 square foot colonial located in Armonkwith 2 acres of land.
The asking price is 1 million dollar and the annual taxesare 8000 dollars.
The house is located in the Byram Hills School District.E2 S4 This is a 1 million dollar 3 bedroom house.
This is a 2 bathroom house withannual taxes of 8000 dollars.S5 This is a 3 bedroom and 2 bathroom house.
Its price is 1 million dollar andits annual taxes are 8000 dollars.E3 S6 The tax rate of the house is 3 percent.S7 The house has an asphalt roof.E4 S8 This is a 3 bedroom, 2 bathroom colonial with 2000 square feet and 2 acres of land.S9 The house has 2 bedrooms and 3 bathrooms.
This house is a colonial.It has 2000 square feet.
The house is on 2 acres of land.Table 1: Examplesdistrict name.
Without proper sentence boundarydetermination, a sentence planner may formulate asingle sentence to convey all the information, as inS1.
Even though S1 is grammatically correct, itis too complex and too exhausting to read.
Simi-larly, output like S2, despite its grammatical correct-ness, is choppy and too tedious to read.
In contrast,our instance-based sentence boundary determinationmodule will use examples in a corpus to partitionthose attributes into several sentences in a more bal-anced manner (S3).Semantic cohesion also influences the quality ofoutput sentences.
For example, in the real-estatedomain, the number of bedrooms and number ofbathrooms are two closely related concepts.
Basedon our corpus, when both concepts appear, they al-most always conveyed together in the same sen-tence.
Given this, if the content planner wants toconvey a house with the following attributes: price,number of bedrooms, number of bathrooms, andproperty tax, S4 is a less desirable solution than S5because it splits these concepts into two separatesentences.
Since we use instance-based sentenceboundary determination, our method generates S5 tominimize the difference from the corpus instances.Sentence boundary placement is also sensitive tothe syntactic and lexical realizability of groupeditems.
For example, if the sentence planner asks thesurface realizer to convey two propositions S6 andS7 together in a sentence, a realization failure willbe triggered because both S6 and S7 only exist inthe corpus as independent sentences.
Since neitherof them can be transformed into a modifier based onthe corpus, S6 and S7 cannot be aggregated in oursystem.
Our method takes a sentence?s lexical andsyntactic realizability into consideration in order toavoid making such aggregation request to the sur-face realizer in the first place.A generation system?s own capability may alsoinfluence sentence boundary determination.
Goodsentence boundary decisions will balance a system?sstrengths and weaknesses.
In contrast, bad decisionswill expose a system?s venerability.
For example, ifa sentence generator is good at performing aggre-gations and weak on referring expressions, we mayavoid incoherence between sentences by preferringaggregating more attributes in one sentence (like inS8) rather than by splitting them into multiple sen-tences (like in S9).In the following, we will demonstrate how our ap-proach can achieve all the above goals in a unifiedinstance-based framework.4 Instance-based boundary determinationInstance-based generation automatically createssentences that are similar to those generated by hu-mans, including their way of grouping semantic con-tent, their wording and their style.
Previously, Panand Shaw (2004) have demonstrated that instance-based learning can be applied successfully in gen-erating new sentences by piecing together existingwords and segments in a corpus.
Here, we want todemonstrate that by applying the same principle, wecan make better sentence boundary decisions.567The key idea behind the new approach is to find asentence boundary solution that minimizes the ex-pected difference between the sentences resultingfrom these boundary decisions and the examples inthe corpus.
Here we measure the expected differ-ence based a set of cost functions.4.1 Optimization CriteriaWe use three sentence complexity and quality re-lated cost functions as the optimization criteria: sen-tence boundary cost, insertion cost and deletion cost.Sentence boundary cost (SBC): Assuming P isa set of propositions to be conveyed and S is a col-lection of example sentences selected from the cor-pus to convey P .
Then we say P can be realizedby S with a sentence boundary cost that is equal to(|S| ?
1) ?
SBC in which |S| is the number of sen-tences and SBC is the sentence boundary cost.
Touse a specific example from the real-estate domain,the input P has three propositions:p1.
House1 has-attr (style=colonial).p2.
House1 has-attr(bedroom=3).p3.
House1 has-attr(bathroom=2).One solution, S, contains 2 sentences:s1.
This is a 3 bedroom, 2 bathroom house.s2.
This is a colonial house.Since only one sentence boundary is involved, S is asolution containing one boundary cost.
In the aboveexample, even though both s1and s2are grammati-cal sentences, the transition from s1to s2is not quitesmooth.
They sound choppy and disjointed.
To pe-nalize this, whenever there is a sentence break, thereis a SBC.
In general, the SBC is a parameter that issensitive to a generation system?s capability such asits competence in reference expression generation.If a generation system does not have a robust ap-proach for tracking the focus across sentences, it islikely to be weak in referring expression generationand adding sentence boundaries are likely to causefluency problems.
In contrast, if a generation sys-tem is very capable in maintaining the coherence be-tween sentences, the proper sentence boundary costwould be lower.Insertion cost: Assume P is the set of propo-sitions to be conveyed, and Ci is an instance inthe corpus that can be used to realize P by insert-ing a missing proposition pj to Ci, then we say Pcan be realized using Ci with an insertion cost oficost(CH , pj), in which CH is the host sentence inthe corpus containing proposition pj .
Using an ex-ample from our real-estate domain, assume the inputP=(p2, p3, p4), wherep4.
House1 has-attr (square footage=2000).Assume Ci is a sentence selected from the cor-pus to realize P : ?This is 3 bedroom 2 bathroomhouse?.
Since Ci does not contain p4, p4 needs tobe added.
We say that P can be realized using Ciby inserting a proposition p4with an insertion costof icost(CH , p4), in which CH is a sentence in thecorpus such as ?This is a house with 2000 squarefeet.
?The insertion cost is influenced by two main fac-tors: the syntactic and lexical insertability of theproposition pj and a system?s capability in aggre-gating propositions.
For example, if in the corpus,the proposition pj is always realized as an indepen-dent sentence and never as a modifier, icost(?, pj)should be extremely high, which effectively pro-hibit pj from becoming a part of another sen-tence.
icost(?, pj) is defined as the minimum in-sertion cost among all the icost(CH , pj).
Currentlyicost(CH , pj) is computed dynamically based onproperties of corpus instances.
In addition, sincewhether a proposition is insertable depends on howcapable an aggregation module can combine propo-sitions correctly into a sentence, the insertion costshould be assigned high or low accordingly.Deletion cost: Assume P is a set of input proposi-tions to be conveyed and Ci is an instance in the cor-pus that can be used to convey P by deleting an un-needed proposition pj in Ci.
Then, we say P can berealized using Ci with a deletion cost dcost(Ci, pj).As a specific example, assuming the input is P=(p2,p3, p4), Ci is an instance in the corpus ?This is a3 bedroom, 2 bathroom, 2000 square foot colonialhouse.?
In addition to the propositions p2, p3andp4, Ci also conveys a proposition p1.
Since p1 isnot needed when conveying P , we say that P can berealized using Ci by deleting proposition p1 with adeletion cost of dcost(Ci, p1).The deletion cost is affected by two main fac-tors as well: first the syntactic relation betweenpj and its host sentence.
Given a new instanceCi, ?This 2000 square foot 3 bedroom, 2 bathroomhouse is a colonial?, deleting p1, the main object568of the verb, will make the rest of the sentence in-complete.
As a result, dcost(Ci, p1) is very expen-sive.
In contrast, dcost(Ci, p4) is low because theresulting sentence is still grammatically sound.
Cur-rently dcost(Ci, pj) is computed dynamically basedon properties of corpus instances.
Second, the ex-pected performance of a generation system in dele-tion also impacts the deletion cost.
Depending onthe sophistication of the generator to handle variousdeletion situations, the expected deletion cost canbe high if the method employed is naive and errorprone, or is low if the system can handle most casesaccurately.Overall cost: Assume P is the set of propositionsto be conveyed and S is the set of instances in thecorpus that are chosen to realize P by applying a setof insertion, deletion and sentence breaking opera-tions, the overall cost of the solutionCost(P ) =?Ci(Wi ?
?jicost(CHj , pj)+Wd ?
?kdcost(Ci, pk))+(Nb ?
1) ?
SBCin which Wi, Wd and SBC are the insertion weight,deletion weight and sentence boundary cost; Nb isthe number of sentences in the solution, Ci is a cor-pus instance been selected to construct the solutionand CHj is the host sentence that proposition pj be-longs.4.2 Algorithm: Optimization based on overallcostWe model the sentence boundary determination pro-cess as a branch and bound tree search problem.
Be-fore we explain the algorithm itself, first a few no-tations.
The input P is a set of input propositionschosen by the content planner to be realized.
?
isthe set of all possible propositions in an applicationdomain.
Each instance Ci in the corpus C is repre-sented as a subset of ?.
Assume S is a solution toP , then it can be represented as the overall cost plusa list of pairs like (Cis, Ois), in which Cis is oneof the instances selected to be used in that solution,Ois is a set of deletion, insertion operations that canbe applied to Cis to transform it to a subsolution Si.To explain this representation further, we use a spe-cific example in which P=(a, d, e, f), ?=(a, b, c, d,e, f g, h, i).
One of the boundary solution S can berepresented asS = (Cost(S), (S1, S2))S1= (C1= (a, b, d, i), delete(b, i)),S2= (C2= (e), insert(f as in C3= (f, g)))Cost(S) = Wd ?
(dcost(C1, b) + dcost(C1, i)) +Wi ?
icost(C3, f) + 1 ?
SBCin which C1and C2are two corpus instances se-lected as the bases to formulate the solution and C3is the host sentence containing proposition f .The general idea behind the instance-basedbranch and bound tree search algorithm is that givenan input, P , for each corpus instance Ci, we con-struct a search branch, representing all possibleways to realize the input using the instance plusdeletions, insertions and sentence breaks.
Sinceeach sentence break triggers a recursive call toour sentence boundary determination algorithm, thecomplexity of the algorithm is NP-hard.
To speed upthe process, for each iteration, we prune unproduc-tive branches using an upper bound derived by sev-eral greedy algorithms.
The details of our sentenceboundary determination algorithm, sbd(P ), are de-scribed below.
P is the set of input propositions.1.
Set the current upper bound, UB, to the mini-mum cost of solutions derived by greedy algo-rithms, which we will describe later.
This valueis used to prune unneeded branches to make thesearch more efficient.2.
For each instance Ci in corpus C in which (Ci?P ) 6= ?, loop from step 3 to 9.
The goal hereis to identify all the useful corpus instances forrealizing P .3.
Delete all the propositions pj ?
D in whichD = Ci ?
P (D contains propositions in Cibut not exist in P) with cost Costd(P ) = Wd ?
?Pj?D dcost(Ci, pj).
This step computes thedeletion operators and their associated costs.4.
Let I = P ?
Ci (I contains propositions in Pbut not in Ci).
For each subset Ej ?
I (Ej in-cludes ?
and I itself), iterate through step 5 to9.
These steps figure out all the possible waysto add the missing propositions, including in-serting into the instance Ci and separating therest as independent sentence(s).5695.
Generate a solution in which ?pk ?
Ej , insertpk to Ci.
All the propositions in Q = I ?
Ejwill be realized in different sentences, thus in-curring a SBC.6.
We update the cost Cost(P ) toCostd(P ) + Wi ?
?pk?Ejicost(?, pk)+SBC + Cost(Q)in which Cost(Q) is the cost of sbd(Q) whichrecursively computes the best solution for inputQ and Q ?
P .
To facilitate dynamic program-ming, we remember the best solution for Q de-rived by sbd(Q) in case Q is used to formulateother solutions.7.
If the lower bound for Cost(P) is greater thanthe established upper bound UB, prune thisbranch.8.
Using the notation described in the beginningof Sec.
4.2, we update the current solution tosbd(P ) = (Cost(P ), (Ci, delete?pj?D(pj),insert?pk?Ej(pk)))?sbd(Q)in which?is an operator that composes twopartial solutions.9.
If sbd(P) is a complete solution (either Q isempty or have a known best solution) andCost(P ) < UB, update the upper boundUB = Cost(P ).10.
Output the solution with the lowest overall cost.To establish the initial UB for pruning, we use theminimum of the following three bounds.
In general,the tighter the UB is, the more effective the pruningis.Greedy set partition: we employ a greedy setpartition algorithm in which we first match the setS ?
P with the largest |S|.
Repeat the same processfor P ?
where P ?
= P ?
S. The solution cost isCost(P ) = (N ?
1) ?
SBC , and N is the numberof sentences in the solution.
The complexity of thiscomputation is O(|P |), where |P | is the number ofpropositions in P .Revised minimum set covering: we employ agreedy minimum set covering algorithm in whichwe first find the set S in the corpus that maximizesthe overlapping of propositions in the input P .
Theunwanted propositions in S ?
P are deleted.
As-sume P ?
= P ?
S, repeat the same process to P?until P ?
is empty.
The only difference between thisand the previous approach is that S here might notbe a subset of P .
The complexity of this computa-tion is O(|P |).One maximum overlapping sentence: we firstidentify the instance Ci in corpus that covers themaximum number of propositions in P .
To arriveat a solution for P , the rest of the propositions notcovered by Ci are inserted into Ci and all the un-wanted propositions in Ci are deleted.
The cost ofthis solution isWd ?
?pj?Ddcost(Ci, pj) + Wi ?
?pk?Iicost(?, pk)in which D includes proposition in Ci but not in P ,and I includes propositions in P but not in Ci.Currently, we update UB only after a completesolution is found.
It is possible to derive better UBby establishing the upper bound for each partial so-lution, but the computational overhead might notjustify doing so.4.3 Approximation AlgorithmEven with pruning and dynamic programming, theexact solution still is very expensive computation-ally.
Computing exact solution for an input sizeof 12 propositions has over 1.6 millions states andtakes more than 30 minutes (see Figure 1).
To makethe search more efficient for tasks with a large num-ber of propositions in the input, we naturally seeka greedy strategy in which at every iteration the al-gorithm myopically chooses the next best step with-out regard for its implications on future moves.
Onegreedy search policy we implemented explores thebranch that uses the instance with maximum over-lapping propositions with the input and ignores allbranches exploring other corpus instances.
The in-tuition behind this policy is that the more overlapan instance has with the input, the less insertions orsentence breaks are needed.Figure 1 and Figure 2 demonstrate the trade-off between computation efficiency and accuracy.In this graph, we use instances from the real-estate corpus with size 250, we vary the input sen-tence length from one to twenty and the resultsshown in the graphs are average value over sev-eral typical weight configurations ((Wd,Wi,SBC)=570(1,3,5),(1,3,7),(1,5,3),(1,7,3),(1,1,1)).
Figure 2 com-pares the quality of the solutions when using exactsolutions versus approximation.
In our interactivemultimedia system, we currently use exact solutionfor input size of 7 propositions or less and switch togreedy for any larger input size to ensure sub-secondperformance for the NLG component.0204060801001201401601802002 4 6 8 9 10 12 14 16 18 20# of Propositions in InputExecutionTime(Seconds)GreedyExactFigure 1: Speed difference between exact solutionsand approximations024681012141618202 4 6 8 9 10 12 14 16 18 20# of Proposition in InputCost GreedyExactFigure 2: Cost difference between exact solutionsand approximationsMeasures Ours B-3 B-6Dangling sentence (7) 0 100% 100%Split Semantic Group 1% 61% 21%Realization Failure 0 56% 72%Fluency 59% 4% 8%Table 2: Comparisons5 EvaluationsTo evaluate the quality of our sentence boundary de-cisions, we implemented a baseline system in whichboundary determination of the aggregation moduleis based on a threshold of the maximum numberof propositions allowed in a sentence (a simplifiedversion of the second strategy in Section 2.
Wehave tested two threshold values, the average (3) andmaximum (6) number of propositions among cor-pus instances.
Other sentence complexity measures,such as the number of words and depth of embed-ding are not easily applicable for our comparisonbecause they require the propositions to be realizedfirst before the boundary decisions can be made.We tune the relative weight of our approach tobest fit our system?s capability.
Currently, theweights are empirically established to Wd = 1,Wi = 3 and SBC = 3.
Based on the output gen-erated from both systems, we derive four evaluationmetrics:1.
Dangling sentences: We define dangling sen-tences as the short sentences with only oneproposition that follow long sentences.
Thismeasure is used to verify our claim that becausewe use global instead of local optimization,we can avoid generating dangling sentences bymaking more balanced sentence boundary de-cisions.
In contrast, the baseline approacheshave dangling sentence problem when the in-put proposition is 1 over the multiple of thethreshold values.
The first row of Table 2 showsthat when the input proposition length is setto 7, a pathological case, among the 200 inputproposition sets randomly generated, the base-line approach always produce dangling sen-tences (100%).
In contrast, our approach al-ways generates more balanced sentences (0%).2.
Semantic group splitting.
Since we use aninstance-based approach, we can maintain thesemantic cohesion better.
To test this, werandomly generated 200 inputs with up to 10propositions containing semantic grouping ofboth the number of bedrooms and number ofbathrooms.
The second row, Split SemanticGroup, in Table 2 shows that our algorithm canmaintain semantic group much better than thebaseline approach.
Only in 1% of the outputsentences, our algorithm generated number ofbedrooms and number of bathrooms in separatesentences.
In contrast, the baseline approachesdid much worse (61% and 21%).3.
Sentence realization failure.
This measure isused to verify that since we also take a sen-tence?s lexical and syntactical realizability intoconsideration, our sentence boundary decisionswill result in less sentence realization failures.571An realization failure occurs when the aggre-gation module failed to realize one sentencefor all the propositions grouped by the sentenceboundary determination module.
The third rowin Table 2, Realization Failure, indicates thatgiven 200 randomly generated input proposi-tion sets with length from 1 to 10, howmany re-alization happened in the output.
Our approachdid not have any realization failure while for thebaseline approaches, there are 56% and 72%outputs have one or more realization failures.4.
Fluency.
This measure is used to verify ourclaim that since we also optimize our solutionsbased on boundary cost, we can reduce incoher-ence across multiple sentences.
Given 200 ran-domly generated input propositions with lengthfrom 1 to 10, we did a blind test and presentedpairs of generated sentences to two human sub-jects randomly and asked them to rate whichoutput is more coherent.
The last row, Flu-ency, in Table 2 shows how often the humansubjects believe that a particular algorithm gen-erated better sentences.
The output of our al-gorithm is preferred for more than 59% of thecases, while the baseline approaches are pre-ferred 4% and 8%, respectively.
The other per-centages not accounted for are cases where thehuman subject felt there is no significant differ-ence in fluency between the two given choices.The result from this evaluation clearly demon-strates the superiority of our approach in gener-ating coherent sentences.6 ConclusionIn the paper, we proposed a novel domain indepen-dent instance-based sentence boundary determina-tion algorithm that is capable of balancing a com-prehensive set of generation capability, sentencecomplexity, and quality related constraints.
Thisis the first domain-independent algorithm that pos-sesses many desirable properties, including balanc-ing a system?s generation capabilities, maintainingsemantic cohesion and cross sentence coherence,and preventing severe syntactic and lexical realiza-tion failures.
Our evaluation results also demon-strate the superiority of the approach over a rep-resentative domain independent sentence boundarysolution.ReferencesAnthony C. Davey.
1979.
Discourse Production.
Edin-burgh University Press, Edinburgh.Robert Gunning.
1952.
The Technique of Clear Writing.McGraw-Hill.William C. Mann and James A. Moore.
1981.
Computergeneration of multiparagraph English text.
AmericanJournal of Computational Linguistics, 7(1):17?29.Shimei Pan and James Shaw.
2004.
SEGUE: A hy-brid case-based surface natural language generator.
InProc.
of ICNLG, Brockenhurst, U.K.Ehud Reiter.
1994.
Has a consensus NL generationarchitecture appeared, and is it psycholinguisticallyplausible?
In Proc.
of INLG, Kennebunkport, Maine.Graeme D. Ritchie.
1984.
A rational reconstruction ofthe Proteus sentence planner.
In Proc.
of the COLINGand the ACL, Stanford, CA.Jacques Robin.
1994.
Automatic generation and revi-sion of natural language summaries providing histori-cal background.
In Proc.
of the Brazilian Symposiumon Artificial Intelligence, Fortaleza, CE, Brazil.James Shaw.
1998.
Segregatory coordination and ellipsisin text generation.
In Proc.
of the COLING and theACL., Montreal, Canada.Amanda Stent, Rashmi Prasad, and Marilyn Walker.2004.
Trainable sentence planning for complex in-formation presentation in spoken dialog systems.
InProc.
of the ACL, Barcelona, Spain.Sebastian Varges and Chris Mellish.
2001.
Instance-based natural language generation.
In Proc.
of theNAACL, Pittsburgh, PA.Marilyn Walker, Owen Rambow, and Monica Rogati.2002.
Training a sentence planner for spoken dialogueusing boosting.
Computer Speech and Language.John Wilkinson.
1995.
Aggregation in natural languagegeneration: Another look.
Co-op work term report,Dept.
of Computer Science, University of Waterloo.Michelle Zhou and Vikram Aggarwal.
2004.
Anoptimization-based approach to dynamic data contentselection in intelligent multimedia interfaces.
In Proc.of the UIST, Santa Fe, NM.Michelle X. Zhou and Min Chen.
2003.
Automatedgeneration of graphic sketches by example.
In IJCAI,Acapulco, Mexico.Michelle X. Zhou and Shimei Pan.
2001.
Automatedauthoring of coherent multimedia discourse in conver-sation systems.
In ACM Multimedia, Ottawa, Canada.572
