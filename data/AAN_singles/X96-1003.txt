EVALUATION DRIVEN RESEARCH:The Foundation of the TIPSTER Text ProgramDr.
John D. PrangeDepartment ofDefensejdprang @ afterlife.ncsc, milINTRODUCTION:I have been fortunate to have had theopportunity to be associated with the TIPSTER TextProgram since its inception in 1989.
Preliminarydiscussions among government researchers whowere interested in establishing a major, new inter-agency text handling, processing, and exploitationprogram began in the Summer of that year andcontinued in earnest during the months thatfollowed.
Most of our frequent day-long planningmeetings during the first year of our TIPSTERProgram planning were held at DARPAheadquarters in Arlington, VA and were chaired bythe Program Manger of DARPA's Speech and TextR&D efforts.There were clearly two different sets ofexperience and expertise present during thesemeetings.The DARPA Program Manager was astrong proponent and advocate of an EvaluationDriven Research Paradigm that he was following inthe Speech component of his R&D program.
Andeven though this Program Manager had spent onlyslightly more than one year at DARPA, he clearlyunderstood how DARPA established, funded andmanaged new R&D Programs.The rest of us around the table had little orno previous exposure to either the details of anEvaluation Driven Research Paradigm or to theinner workings of DARPA programs.
What webrought to the table were strong credentials andexperience in artificial intelligence, natural anguageprocessing and computational linguistics.
We alsocame with numerous challenging problems to besolved along with an understanding and appreciationof the text handling, processing, and exploitationneeds of our individual agency's analysts andlinguists.The TIPSTER Text Program was born outof the best combination of these two camps.
Sinceits creation in 1989, TIPSTER has developed, grownand evolved into its current role as a major drivingforce within both the Information Retrieval andInformation Extraction R&D communities.TIPSTER has just completed its second, two-yearPhase and is poised to begin Phase III, a three-yeareffort his coming October.
Rather than running outof steam, TIPSTER has continued to pick upmomentum and to broaden its area of interest andcoverage as it proceeded through Phases I and II andnow heads into Phase III.Why has this happened?
Looking back nowfrom the perspective and vantage point of sevenyears of rich history, it is very clear to me that thoseof us who participated in these early, formativeTIPSTER Text Program discussions collectively laida very solid foundation.
That foundation was built,"TIPSTER style", out of an Evaluation DrivenParadigm, heavily borrowed from DARPA's SpeechR&D Program and it has continued to grow andevolve over the past seven years in a "TIPSTERunique" way.EVALUATION DRIVEN RESEARCH:What is it and what does it take to make itwork?In the late 1980's the Speech and TextTechnology Program at the Defense AdvanceResearch Project Agency (DARPA) was heavilyweighted and dominated by its Speech component.Those of us in the Government who were attemptingto focus increasing attention and resources on texthandling, processing and exploitation problemsbeing encountered by our Agency's linguists andanalysts looked with envy at the Speech componentof this DARPA program.
Increasingly, we wonderedwhat it was that gave the DARPA Speech R&DProgram its focus, its momentum, its continuity, itslongevity, and most importantly its ability todramatically move forward the state-of-the-art intheir very challenging technical field.While there were surely many reasons forthe on-going success of DARPA's Speech R&D13Program, a single answer emerged to us as textresearchers during our early TIPSTER discussions.From our vantage point, the continuing vitality ofDARPA's Speech R&D Program could be directlyauributed to its enthusiastic implementation of an"Evaluation Driven Research" paradigm.During these TIPSTER Program planningmeetings in late 1989 and early 1990, we took acloser look at this implementation by DARPA'sSpeech R&D Community with an eye towardsattempting to duplicate and tailor, as needed, their"Evaluation Driven Research" paradigm in our newText Processing R&D Program.
We identified thefollowing distinct components which we hoped tocarry over to our new text program:?
A clearly defined final objective for theoverall R&D program.?
A series of specific tasks which whensuccessfully accomplished would move theR&D community significantly closer to theprogram's final objective.?
An agreed upon and specifically tailoredmetric and evaluation methodology forperiodically measuring progress towardsaccomplishing each of the chosen tasks.?
Sufficient quantifies of training and testingdata.
Each data collection should be carefullyselected, formatted, annotated, and otherwiseprepared to directly support a specific task.?
A group of several (in fact, the more themerrier) leading-edge research institutionswho are willing to:0 Aggressively investigate solutions to eachassigned task.0 Periodically participate in formalevaluations of how well their systems areperforming on the current ask.0 Openly discuss their successes andfailures in the forum of a technicalworkshop attended by researchers fromthe other participating institutions and byinterested government sponsors.?
A multi-year program budget with sufficient,programmed government funding to coverthe cost of.
"0 Obtaining and preparing the training andtest data collections.0 Fully funding the R&D activities of acore group of research institutions.0 Providing strong encouragement andeven some limited financial support toother non-core group research institutionsto participate to the greatest extentpossible in these periodic task evaluationsand related open forums.0 Conducting regularly scheduled formalevaluations of each task according to itsagreed upon metric.0 Sponsoring regularly scheduled openworkshops to discuss and share results,approaches, and techniques.On the surface, there appeared to us to benothing revolutionary about these components andtheir description.
We concluded that simplyincluding them in a new R&D program would not,by themselves, guarantee its success.
Rather we feltthat the real key to the successful implementation fthe "Evaluation Driven Research" paradigm lay inthe careful and thoughtful selection of variousprogram design choices and then in their actualexecution.
In particular several implementationconsiderations directly related to the paradigmcomponents listed above seemed to be particularlyimportant.?
Choosing an appropriate series of tasks.0 Each task must be focused and clearlydefined, ff more than one task is beingpursued simultaneously, there needs to bean overarching concept or framework intowhich these multiple tasks couldmeaningfully fit.0 Each task must be technicallychallenging, and clearly a significant stepbeyond the current state-of-the-art.
Eachtask must be far enough away so that noparticipant can reach it easily but not sofar away that no one will make anattempt.
The goal here is to spark theintellectual curiosity and interest of theworld-class researchers in the field.
Onceyou have attracted these pace setters andhave enticed them to work on your task,then you have, in effect, indirectly movedmany of the other institutions in thistechnical field who will independentlytake up this same task so that they can14Choosingstay up with this forward moving, state-of-the-art standard bearer.0 Another key consideration is to chooseeach task so that more than one technicalapproach can be followed andinvestigated.
Clearly, stimulating a senseof friendly competition betweenparticipating research institutions withina single approach can produce improvedresults, but if this same sense ofcompetition is established across multipleapproaches, the net, positive result can besignificantly multiplied.
Behef in the"superiority" of a specific technicalapproach can be a tremendous motivator.0 And finally, the task must be stated anddefined in such a way that it immediatelylends itself to measurement andevaluation.
In particular the best tasksare those whose rationale or case foraction is stated in terms of the measurableamount of improvement which is beinganticipated and sought.a metric and evaluationmethodology.
The strategy here is to choose ametric which is sufficiently close to the coreresearch problem associated with each taskthat progress as measured by this metric willclearly imply that similar progress on the corretechnology which lies at the heart of the giventask is also being made.0 Choosing an easily accomplished yetinappropriate metric and evaluationmethodology is clearly wasted energy.You can produce lots of data but yetproduce few, if any, insights into eitherthe effectiveness or efficiency of theunderlying algorithm or in the progressbeing made on the actual, desired task.0 Evaluation shines a bright light andattracts a lot of attention.
We mustunderstand the current ask well enoughto insure that our selected metrics will dothe job they are intended to do; namelymeasuring the rate and degree of progressin achieving a satisfactory solution to thecurrent task.
Whatever we ultimatelydecide to evaluate is almost certainlyguaranteed toreceive significantly greaterattention during the execution of thistask.
That's great if this is whereattention needs to be applied, but16devastating if the "real problem" to besolved lies elsewhere.
This latterpossibility is reminiscent of a story abouta man who is walking home late at nightand encounters a man, down on his handsand knees underneath a street light,carefully and systematically searching theground with his hands.
The first manasks the man on his knees what he issearching for.
"I'm trying to find my lostwatch", he replies.
The first manwillingly joins in the search.
Afterseveral minutes have passed inunsuccessful search, the first man asks asecond question, "Now, exactly where didyou lose your watch?"
The originalsearcher points across the street andreplies, "Over there."
"Then why are wesearching here?"
the first man responds.To which the original searcher eplies,"The light is so much better here.
"People will naturally search longer andmore completely where the light isstrongest.
Likewise, metrics andevaluation shine very bright lights.
Wemust make sure that they illuminate thoseparts of the problem which deserve andwill benefit from this special form ofattention.The selected metrics and evaluationmethodology must simultaneously satisfysignificantly different audiences.
A highlevel view of the selected metrics must besimple enough that they can be easilyunderstood by analyst end-users aud byoperational managers who areknowledgeable about the environmentand domain into which these researchresults will eventually be applied but whoare not necessarily technically savvyabout the details of the underlyingtechnology and algorithmic approachesbeing used.
These analyst end-users andoperational managers must be able to seethe connection between the current ask,the metrics being applied and the futureimpact that this emerging technologycould have against their operationalproblems.
Second, a subset of thesemetrics must allow the governmentsponsors to measure progress over time sothat the government's return oninvestment can be tracked andappropriate programmatic decisionsreached.
And finally, the full set ofmetrics must be sufficiently detailed andspecific, so that the participatingresearchers can make adjustments in theiralgorithms and techniques based upon theinsights that they have gained.Simultaneously satisfying these variedconditions and requirements can bedifficult o achieve but it can be done andit is well worth the effort.Making available relevant training and testingdata, in sufficient quantities and ofappropriate quality.
This is an area in whichDARPA's Speech R&D Program has placedhigh value and importance and has clearlyexcelled in its execution.
The key idea here isthat the parameters associated with eachcollection of training and testing data must becarefully considered and selected prior to thebeginning of each new R&D task.
Theseparameters then became the guidingprinciples during the data preparation phase.0 If the data does not already exist, then itmust be created.
Successful execution ofthe Evaluation Driven ResearchParadigm is totally dependent upon theavailability of high quality data insufficient quantities which has beenspecifically prepared and formatted tosupport he current R&D tasks.
Whilethe creation of appropriate training andtesting databases can be expensive andtime consuming, these negatives must beaccepted as part of the cost of doingbusiness and must be factored into thebudget and time schedule for thatparticular R&D task.0 The optimal situation is one in which thedata collection effort is 100% completedprior to the start of the associatedresearch task.
This may, however, be anunrealistic expectation.
So when this isnot practical, it is still technically possiblefor the data collection efforts to be donesimultaneously with the task executionwithout significantly impacting theresearch efforts on the given taskprovided that the bare minimum dataquantity and quality requirements are meton-time and that the parametersassociated with the data preparation task16remain constant hroughout he entireeffort.
However, keeping this approachon track is easier said than done.
TheDARPA's Speech R&D Program has onmore than one occasion used thissimultaneous program and datapreparation approach with mixed results.More on this subject later when wediscuss this component in terms of theTIPSTER Program.Happily, these specifically createdtraining and testing databases can oftenbe made available to other researchersand hence support other, initiallyunplanned R&D investigations.
That is,the utility of these specially produceddatabases can be extended beyond theiroriginal purposes and yield important,unplanned side benefits.The requisite preparation of data may beviewed as a mundane, routine, unexcitingactivity.
But as previously stated,satisfactory completion of this datapreparation task is absolutely essential.The degree of care and attention to detailthat is applied during this activity directlytranslate into the quantity and quality ofthe R&D results produced by thoseinvestigations which utilize these datacollections.
There are no short cuts here.Fostering a cooperative, corporate programviewpoint among participating institutionsand sponsoring overnment agencies.0 The objective here is to make theprogram participants truly believe thateach task to be solved takes precedenceover who achieves the solution or by whatmethod.
While this may be overlyidealistic, this objective's entiment isexactly what is needed.0 During the pursuit of each task, eachparticipating institution will need todevelop solutions to peripheral problemswhich are common to other technicalapproaches Every participant will needto input the same training and test data,to access imilar collections of supportinginformation (e.g.
lexicons, word lists,gazetteers), and to use functionallysimilar software tools (e.g.
part of speechtaggers, text annotation tools,segmentors).
The participants should beencouraged to equitably share these toolsand data resources with other programparticipants.
All stand to gain in the longrun from such free and open exchanges.A great motivation for this viewpoint isthe fact that continuing governmentalfunding support can best be secured byclearly demonstrating progress across abroad technological front.
And thisobjective is easier to achieve in acooperative, sharing environment, than inan isolated, proprietary one.Fostering and maintaining a cooperateand cooperative viewpoint applies to theteam of government sponsors as well.
Itis hard enough for multiple offices withinthe same Agency is work together on thesame program over an extended period oftime, but this becomes much harder whenyou must factor in the cultural differencesthat will surely arises across severalAgencies.
When we looked closely at howDARPA's Speech R&D Program wasmanaged we saw a fairly loose andunstructured confederation approach ofinterested government sponsors.
Byadopting this loose confederation, theSpeech Program had avoided the need toconfront the cultural differences acrossthe sponsoring Agencies.
On the otherhand, as TIPSTER planners we hoped toestablish a program which was equallyplanned, funded, managed, and executedby multiples Agencies.
We were going toconfront our Agencies' culturaldifferences head on.
We knew that wehad bitten off a lot.EVALUATION DRIVEN RESEARCH:TIPSTER StyleSo how well has TIPSTER adhered to theEvaluation Driven Research Paradigm as describedin this preceding section?
My assessment, in aphrase, is very well.
Unfortunately a detailedresponse to this question is beyond the scope of thispaper since the full answers to this question lies inthe collective papers contained in the Proceedings ofthe TIPSTER Text Program (Phase 1), theProceedings for each of the recent MessageUnderstanding Conferences (MUC) and for each ofthe Text Retrieval Conferences (TREC) and the restof this Proceedings for Phase II.
So my objective forthe remainder of this paper is to give a high levelsummary response to each paradigm component andmaybe in the process to give a perspective withwhich you can read and interpret hese individualpapers.Components of the Evaluation Driven ResearchParadigm:?
A clearly defined final objective for the overallR&D program.During the 1989-90 DARPA planningmeetings a large number of important, yet diversetext handling, processing, and exploitationrequirements surfaced.
To make matters worse, eachof these requirements ook on many different formswhen we took into account specific applications.Early on we opted to focus the TIPSTER Program ontwo core problems which seemed to be central to alarge number of different operational problems.These two enabling technology areas are now wellknown and closely associated with the TIPSTERProgram: Document Detection and InformationExtraction.
In Phase I the research goal was tosignificantly push the state-of-the-art in both fieldsusing multiple, different echnical approaches.
InPhase II the research goals shifted.
The main focuswas now placed on investigating ways in which thetwo separate technology areas of document detectionand information extraction could synergisticallyinteract within a single, modular TIPSTER systemarchitecture, on developing and deployingoperational prototypes based upon the mostpromising TIPSTER algorithms, and on thecontinuing advancement of the overall performanceof the best TIPSTER algorithms.
In Phase III, wewill add a third enabling technology area; textsummarization while continuing to pursue naturalextensions of these Phase II goals.
* A series of specific tasks which when successfullyaccomplished would move the R&D communitysignificantly closer to the program's finalobjective.The manner in which the TIPSTERProgram has incorporated this component is mosteasily seen in the design of the multiple tasks thatunderwrote Phase I, Our evaluation of the pre-TIPSTER state-of-the-art in document detectionsystems concluded that there was:6 Heavy reliance on Boolean-logic key wordsystems170 Poor system performance0 Query construction required asystem expert0 Bulk of the Information Retrieval (IR)research community efforts were directed atEnglish documents and retrospective (or adhoc) retrieval applications0 Many IR research systems were automatic,statistically-based systems whoseperformance on small, homogeneousresearch collections was promising butwhose performance on real-world sizecollections was untested and unknown.Similarly our evaluation of the pre-TIPSTER state-of-the-art in information extractionconcluded that:0 Almost all work had beeen done in Engfish0 The subject domains were focused primarilyon military domains0 The input texts were typically highlystructured and stylized0 The input text volumes were very small andsystem throughput was very slow0 Systems were designed to solve veryspecific applications.
As a result systemdesigns were highly "stove-piped".
Systemportability was virtually non-existent.0 Systems failed "hard" when theyencountered previously unseen vocabulary,linguistic structures, formats, etc.0 Practical applications were limited to highlyconstrained domains with high enoughpriority to warrant he development expenseassociated with a highly tailored systemsolutionIn response to these conclusions, Phase I ofTIPSTER established multiple, inter-related tasks.All participants were required to demonstratelanguage portability by performing the same basictasks in both English and in Japanese and systemrobustness by successfully handling and processingtext documents which contained ungrammaticalusage, garbles, new words, and structures.In addition the document detectionparticipants were required to perform both routingand ad hoc retrieval tasks, to automatically convertdetailed, lengthy natural anguage information eedstatements covering a wide range of topics intosystem specific queries without human intervention,to return relevant documents in priority order basedupon the document's perceived egree of relevance,to highlight he most relevant passages within theseretrieved ocuments, and to perform all of their taskson large (now over 1 million documents andmultiple gigabytes), heterogeneous, complexdocument collections.Similarly the information extractionparticipants were additionally required toautomatically locate, identify and standardizeinformation contained in newspaper style documentswithin two distinct subject domains; the formation ofbusiness joint ventures and microelectronic chipfabrication.
This entire extraction task wassignificantly more difficult than previous extractiontasks when measured along several dimensions (i.e.text corpus complexity, text corpus size, template fallcomplexity, and the overall nature of the task).One of the most challenging informationextraction tasks which was first articulated uringPhase I (namely, system extensibifity by analyst end-users) has still not been completely satisfied.Extraction systems are still best extended andmodified by the system developers themselves or byindividuals who have received significant training.An agreed upon and specifically tailored metricand evaluation methodology for periodicallymeasuring progress towards accomplishing eachof the chosen tasks.Frequent formal metric-based evaluationshave been a hallmark of the TIPSTER TextProgram.
The relevant evaluations are onlyhighlighted in the following paragraphs.
Each ofthese evaluations has been reported on in detail ineither the Proceedings of the TIPSTER TextProgram (Phase I), in this Proceedings for Phase IIor in the separately pubfished Proceedings for theMessage Understanding Conferences (MUC-3 toMUC-6) and Text Retrieval Conferences (TREC- 1 toTREC-4).
A reader wanting additional details isdirected to one or more of these references.During Phase I, all TIPSTER participantswere formally evaluated shortly before the 12, 18,and 24 month Workshops.In addition, the TIPSTER Text Programestablished close ties with the MessageUnderstanding Conference (MUC) beginning withMUC-3.
All of the TIPSTER Information Extractioncontractors were required to participate in MUC-418where the subject domain consisted of news reportson terrorism events.
MUC-5 coincided with theTIPSTER Phase 124-month evaluation and consistedof the same information extraction tasks that hadbeen assigned to the Phase I participants (Formationof business joint ventures and microelectronic chipfabrication; each domain in two languages, Englishand Japanese).
The non-TIPSTER MUC-5participants could choose which of the 4 domain-language pairs they wished to be evaluated against.In November 1995, a redesigned MUC-6 has held inwhich each participant could choose to be evaluatedin one or more of the following tasks; a named entitytask, a template lement ask, a scenario templatetask, and a co-reference task.
All four of these taskswere done using English source texts.
In May 1996,TIPSTER sponsored a new information extractionevaluation program; the Multilingual EvaluationTask (MET).
In MET, the participants performed theMUC-6 named entity task in one or more of thefollowing foreign languages; Spanish, Chinese, andJapanese.Early into Phase I of the TIPSTER TextProgram, the decision was made to establish acompanion evaluation program based initially on theTIPSTER Phase I document detection tasks.
Thiscompanion evaluation program became known as theText Retrieval Conference (TREC).
To date, fourTREC's have been held and the fifth is currently inprogress.
During TREC-1 to TREC-3, eachparticipant was evaluated against both a routing taskand an ad hoc retrieval task, each consisting of 50test cases.
Beginning with TREC-4, severaladditional specialty subtasks (referred to withinTREC as Tracks) were added.
These included amultiple database merging track, a confusion track toexamine the effect of corrupted ata, a multilingualtrack to examine retrieval of Spanish languagedocuments, an interactive track, and a filtering track.These TREC Tracks are being continued in TREC-5.The major addition here is that the retrieval ofChinese language documents has been added to themultilingual track.As part of TIPSTER Phase III, theTIPSTER R&D investigations will be expanded intothe field of text summarization.
Planning is alreadyunderway to determine an appropriate metric-basedevaluation strategy for text summarization.The impact of the TIPSTER Text Programmetric-based evaluations can be readily seen fromthe single statistic that over 100 institutions havealready participated in either a TIPSTER TextProgram internal evaluation, or one or more of theMUC, MET, and TREC evaluation programs.
Infact a significant majority of these institutions haveparticipated at least twice and many haveparticipated with even greater frequency.?
Sufficient quantities of training and testing data.Each data collection should be carefully selected,formatted, annotated, and otherwise prepared todirectly support a specific task.The thirteen different formal metric-basedevaluations conducted variously under the banners ofthe TIPSTER Text Program Phase I (3), MUC (4),MET (1), and TREC (5) could not have beenexecuted without sufficient quantities of training andtesting data.
The collection, annotation, tagging,and formatting of the base document collectionsalong with the creation of the appropriate answerkeys to support each separate valuation programhas beeen a costly, time consuming, human analystintensive process.
The bulk of these data preparationtasks were concentrated into Phase I, but additionaldata preparation efforts to support MUC, MET andTREC have continued, as needed, since thecompletion of Phase I in 1993.
The performance ofhuman analysts in completing their tasks has beenroutinely measured and have subsequently been usedas a benchmark against which the performance ofthe information extraction and document detectionalgorithms can be compared.As indicated earlier in this paper, theoptimal situation is one in which the data collectioneffort is 100% completed prior to the start of theassociated research task.
This did not happen duringTIPSTER Phase I.
The collection, formatting andpreparation of appropriate document databases andthe creation of topic statements and pooled relevancejudgments to support the document detectionresearch tasks and of complex scenario templates,detailed fill rule descriptions, and appropriateanswer keys to support he information extractionresearch task turned out to be a monumentalundertaking.
These data preparation tasks in bothareas were several orders of magnitude greater thanprevious efforts.
The TIPSTER governmentsponsors did not fully appreciate this fact until thedata collection efforts were underway.
We soonfound ourselves in the situation where TIPSTERPhase I Program execution and data preparationwere occurring simultaneously.
It quickly provedvery difficult, particularly on the information19extraction side, to maintain sufficient raining andtesting data throughput and at the same time,maintain high data consistency.
While the job waseventually completed, it was only through the tirelessand sometimes even heroic efforts of a small numberof highly motivated and dedicated governmentresearchers that this data preparation effort wasbrought to a successful conclusion in Phase I.
To saythe least, this is not a recommended mode ofoperation.Again all of these TIPSTER datadevelopment activities have been previously reportedon in the Proceedings associated with each of theevaluation programs identified earlier.
Theinterested reader is directed to these sources foradditional information and details.A group of several (in fact, the more the merrier)leading-edge research institutions who arewilling to participate in a cooperative, corporateprogram.The cooperativeness and corporateness ofthe TIPSTER Text Program participants has beenrepeatedly demonstrated in a wide variety of ways.A few examples are listed below to demonstrate hedegree to which this statement has been played out.0 One participant in the Document Detectioncomponent of TIPSTER has participated in allthree TIPSTER Phase I evaluations, in TREC-1 to TREC-4, and is currently participating inTREC-5.
Likewise one participant in theInformation Extraction component hasparticipated in all three TIPSTER Phase Ievaluations, MUC-3 to MUC-6, and MET.
Anumber of other participants come close tomatching these participation levels.0 Throughout the entire TIPSTER TextProgram all of the contractors have willingshared data files and software modules withthe other participants.
This clearly allowedthe collective program to cover more groundand to move forward faster.0 Since its beginning the TIPSTER TextProgram has held technical workshops at 6month intervals.
The Phase II 24-monthWorkshop was the 10th such workshop.
Aportion of each workshop has been devoted toeach contractor describing the technicaldetails of their underlying algorithms andapproaches, the results of their internallyconducted evaluations and experiments, aswell as their successes and failures on theTIPSTER sponsored formal evaluations.
Theopenness of these presentations has alwaysbeen highly commendable.
To the degree thattime permits, the same openness has beenevident during each MUC, MET, and TREC.The importance of these forums and opendiscussions has been repeatedly demonstrated.A report outlining the details of successfullyimplemented techniques and approaches ismade at one workshop by a single participant.Inevitably at the next workshop, reports aregiven by several other participants concerninghow they were able to successfully andbeneficially incorporate these new ideas intotheir own systems.
In this way, a singlesuccess has been quickly multiplied.0 Establishing and maintaining a cooperative,corporate viewpoint among the program'sexternal participants is made considerablyeasier if it is evident hat there is a similarcooperative and corporate viewpoint beingregularly demonstrated by the Governmentsponsors.
Over the past seven years a uniquebonding chemistry has developed among thelarge number of Government personnel whohave had an active hand in the TIPSTERProgram.
Since October 1993 theintroductory briefing of the TIPSTER TextProgram has regularly been given as a jointbriefing by Dr. Sarah Taylor of the Office ofResearch and Development and myself.
Thisbriefing has been frequently opened with theobservation that "Multiple agencies have beenworking closely together on this Programsince 1989.
Why, in the process, we've evenbecome friends."
The line usually sparks asnicker or two, because those in the audienceseem to know that previous joint programsbetween these Agencies have not always beenso amicable.
Almost from day one, there hasbeen an underlying current of give and take,of teamwork, of consensus building.
Thisatmosphere has proven to be quite contagiousas new Government participants have joinedthe TIPSTER Program team and it has clearlyrubbed off onto the other TIPSTERparticipants.0 In the Spring of 1994 the TIPSTER TextProgram was nominated by the CommunityManagement Staff as a "ReinventionLaboratory" in recognition of "its teamwork,its customer focus, and the fact that it has20broken down exiting bureaucratic barriers.
"Then in March 1996 Vice President Gorepresented the National Performance ReviewHammer Award to the TIPSTER TextProgram in the reinvention of government.
Inhis remarks, the Vice President lauded theTIPSTER Program's teamwork for spanningthe Intelligence Community and partneringwith the private sector and leadinguniversities.?
Sufficient government funding to cover the costof all aspects of the Evaluation Driven ResearchParadigm.From its inception the TIPSTER TextProgram has been a jointly planned, funded, andmanaged program.
It is unlikely that any of theindividual participating Agencies could have startedand sustained a program of this magnitude by itself.In addition to the three principal funding agencies,additional funds were obtained from a variety ofother sources at critical junctures in the program.The most notable example of this came from theCongressionally funded Dual Use TechnologyProgram which provided over $5 million insupplement funds in early 1992, about a quarter ofthe way through Phase I.
This infusion of fundshelped raised the TIPSTER Program to a higherlevel, insured that its extensive program to collectand prepare sufficient quantities of training andtesting data could be completed as planned and atthe desired level of quality and provided the impetusfor the TIPSTER Text Program to undertake thedevelopment of its first operational prototype systembased upon TIPSTER technology (i.e., the HOOKAHProject at the Drug Enforcement Administration).0 The implementation f the TIPSTER Phase IIArchitecture Demonstration System, requiredextensive, detailed coordination between allseven of the TIPSTER Phase II contractors.The timetable which was established forcompletion of this effort was extremely tight.Any single contractor who chose to drag hisor her feet or not fully and openly participatewould have put the completion of the wholeeffort in serious jeopardy.
This did nothappen and as a result, the TIPSTER TextProgram Phase II 12-month Workshop wastreated to several demonstrations of thisworking prototype system built in compliancewith the specifications of the TIPSTERArchitecture.EVALUATION DRIVEN RESEARCH:How Has It Performed in T IPSTER?Very well indeed.
Following the EvaluationDriven Research Paradigm has served the TIPSTERText Program exceedingly well.
Throughout itsseven year history, TIPSTER has achieved manyexciting and important research results, but listingthem here is beyond the intended purpose of thispaper.
All of the Proceedings listed in the referencesection directly below are filled with excellent paperswhich describe in full detail what each TIPSTERText Program participant has discovered, learned,and accomplished while investigating TIPSTERtasks under an Evaluation Driven ResearchParadigm.
These papers make for exciting andinteresting reading and the reader is happily directedto them for further details.SUMMARY:During the past seven year history of theTIPSTER Text Program, there has been dramaticimprovements in the current state-of-the-art in texthandling, processing and exploitation.
ClearlyTIPSTER has been a major driving force behindthese improvements within both the InformationRetrieval and Information Extraction R&Dcommunities.While some of these advances would havehappened without TIPSTER, TIPSTER was probablyinstrumental in accelerating their emergence.
Inother cases TIPSTER prodded and encouraged theseR&D communities to investigate problems whichthey might not have considered on their ownmlUaUve.So why has TIPSTER been able to exertsuch a dramatic impact over these two fields?
Thispaper argues that this success has been madepossible in large part by TIPSTER's early adoptionof and continuing adherence toan Evaluation DrivenResearch Paradigm.REFERENCES:Harman D.
(Ed.).
The First Text REtrievalConference (TREC-1).
National Institute ofStandards and Technology Special Publication 500-207, 1993.Harman D.
(Ed.).
The Second Text REtrievalConference (TREC-2).
National Institute of21Standards and Technology Special Publication 500-215, 1994.Hannah D.
(Ed.).
The Third Text REtrievalConference (TREC-3).
National Institute ofStandards and Technology Special Publication 500-225, 1995.Harman D.
(Ed.).
The Fourth Text REtrievalConference (TREC-4).
National Institute ofStandards and Technology Special Publication (ToAppear).Proceedings of the Third Message UnderstandingConference (MUC-3), May 1991, San Francisco:Morgan Kaufmann.Proceedings of the Fourth Message UnderstandingConference (MUC-4), June 1992, San Francisco:Morgan Kaufmann.Proceedings of the Fifth Message UnderstandingConference (MUC-5), August 1993, San Francisco:Morgan Kaufmann.Proceedings of the Sixth Message UnderstandingConference (MUC-6), November 1995, SanFrancisco: Morgan Kaufmann (To Appear).Proceedings of the TIPSTER Text Program (Phase/), September 1993, San Francisco: MorganKaufmann.Proceedings of the TIPSTER Text Program (PhaseII), May 1996, San Francisco: Morgan Kaufmann.22
