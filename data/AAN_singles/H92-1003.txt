Multi-Site Data Collection for a Spoken Language CorpusMADCOW *Contact: Lynette HirschmanNE43-643 Spoken Language Systems GroupMIT Laboratory for Computer Science, Cambridge, MA 02139.e-maih lynette~goldilocks.lcs.mit.eduABSTRACTThis paper describes a recently collected spoken language cor-pus for the ATIS (Air Travel Information System) domain.This data collection effort has been co-ordinated by MAD-COW (Multi-site ATIS Data COllection Working group).
Wesummarize the motivation for this effort, the goals, the imple-mentation of a multi-site data collection paradigm, and theaccomplishments of MADCOW in monitoring the collectionand distribution of 12,000 utterances of spontaneous speechfrom five sites for use in a multi-site common evaluation ofspeech, natural anguage and spoken language.1.
IntroductionFollowing the February 1991 DARPA Speech and Natu-ral Language Workshop, the DARPA Spoken Languagecontractors decided to institute a multi-site data collec-tion paradigm in order to:?
support a common evaluation on speech, naturallanguage and spoken language;?
maximize the amount of data collected;?
provide some diversity in data collection paradigms;?
reduce cost to any one site by sharing the data col-lection activity across multiple participating sites.To co-ordinate this effort, MADCOW was formed in May1991 with a representative from each of the participat-ing sites.
This included the six sites planning to collectand evaluate on the data: AT&T, BBN, CMU, MIT, SRIand Paramax (formerly Unisys), it included NIST, whichwas responsible for data validation, distribution and se-lection and scoring of test material, and it included theAnnotation group at SRI, responsible under a separatecontract for annotating the data with database referenceanswers.
*This paper was written under the auspices of the Multi-Site ATIS Data Collection Working group (MADCOW) by L.Hirschman, M. Bates, D. Dahl, W. Fisher, J. Garofolo, K. Hunicke-Smith, D. Pallett, C. Pao, P. Price, and A. Rudnicky.
In addition,many other people made important contributions to this work andare listed in the Acknowledgements section.The charter of MADCOW was to implement the multi-site data collection paradigm, to monitor the distributionof the data, and to agree on a test paradigm for themulti-site data.
The original goals for the data collectionactivity were to collect 10,000 training utterances and1,000 test utterances, plus material for a dry-run testto be held in October 1991.
Between May 1991 andFebruary 1992, the following data have been collectedunder the MADCOW effort:10,400 training utterances collected from 280 speak-ers at 5 sites, with speech and transcriptions dissem-inated to testing sites;?
5,600 utterances annotated with database referenceanswers, distributed to testing sites;?
300 annotated utterances used for October dry-runtest;?
1,000 annotated utterances used for the February1992 test;?
1,000 utterances set aside for a later test.Significant data collection and evaluation infrastructurewere already in place prior to the formation of MAD-COW.
This included the definition of the air travel plan-ning task \[11\], the database (a relational version of aneleven city subset of the Official Airline Guide, contain-ing airline, flight and ground transportation i formation,initially set up by C. Hemphill at TI and revised and ex-tended by It.
Moore and others at SRI), a comparator-based evaluation methodology for comparing databasetuples to reference answers \[1, 12\], and several earlierATIS corpora collected at TI  \[2\] and SRI.To implement the multi-site data collection effort, eachsite agreed to collect a corpus of 2200 utterances and toprovide this corpus to NIST in a standard format, includ-ing speech data, transcriptions, and a logfile recordingth e subject's interaction with the data collection system.David Pallett's group at NIST was responsible for vali-dation and distribution of the training data as well as forrunning the common evaluation.
As data were submit-ted, NIST checked the data for conformity to the stan-dard formats and randomly set aside 20% of each site'sincoming data for test sets.
For the common evaluation,NIST was responsible for the release of the test data, andthe collection, scoring and analysis of the results, as wellas for adjudication of questions about reference answerson the Spoken Language and Natural Language tests.The Annotation group under Jared Bernstein at SRIwas responsible for providing the database reference an-swers and for categorization of the data into context-independent (class A), context-dependent (class D) andunanswerable (class X) utterances.
To facilitate timelyagreement on specific issues, a special subgroup, chairedby Deborah Dahl, was formed under MADCOW, withresponsibility for the Principles of Interpretation 1.2.
Co l lec t ing  the  DataData collection procedures were not standardized acrosssites.
We know that variation in these procedures canlead to vast differences in the resulting data.
Thoughstandardizing is often important and has played a crucialrole in other areas of the DARPA SLS program, it is alsodifficult and costly.
Spoken language understanding asahuman-computer interface tool is a new technology, andthe space of potential variations is enormous and largelyunexplored.
We therefore chose to sample various pointsin this space, and to document he differences.
Thisdecision may be revised as we learn from this experiment.We outline in this section those aspects of data collectionshared by all systems, and provide a separate section foreach data collection site to highlight the unique aspectsof that site.The original data collected at TI  and SRI used two hu-man "wizards."
As the subject spoke a sentence, oneperson provided a fast transcription, while the otherused NLParse 2 to generate an SQL query to access thedatabase.
At all sites subjects were led to believe theyare talldng to a fully automated system.
For data col-lected at SRI, this was true; all other sites used someautomatic speech recognition and/or natural languageunderstanding, with varying amounts of human tran-scription and error correction.
AT&T used only audiooutputs; all other sites used a computer screen to displaytables of data and other information to the subject.
Thetwo standard microphones were the Sennheiser HMD-410 close talking microphone and the Crown PCC-1601The Principles of Interpretation are the set of rules governingthe interpretation f various types of queries, e.g, the meaning of"around 10 AM", or the definition of what constitutes a "red-eyeflight"; see the Principles of Interpretation section below.2NLParse is a Texas Instruments propriety system, made avail-able to the DARPA research community for the ATIS applicationYou have only three days for job hunting, and you havearranged job interviews in two different cities!
(Theinterview times will depend on your flight schedule.
)Start from City-A and plan the flight and groundtransportation itinerary to City-B and City-C, and backhome to City-A.Figure 1: Common ATIS scenariotable-top microphone.
Table 1 shows the total data col-lected by site, including training and test data 3.All sites used a set of air travel planning "scenarios"(problems) for subjects to solve; BBN supplementedthese with problems more like general database querytasks.
The scenarios varied greatly in complexity and inthe number of queries required for a solution.
For sitesusing a wizard, the wizard was constrained in behavior,and did not represent human-like capabilities, thoughthe wizard's role varied from site to site.
By agreement,one "common scenario" was designated, shown in Fig-ure 1, and sites agreed to collect 10% of their data usingthis common scenario.All sites (except BBN) used a debriefing questionnairewhich explained the nature of the experiment, unveiledthe deception of the wizard, and elicited comments fromthe subject on the experience.
All sites automaticallygenerated logfiles documenting subject queries, systemresponses and time stamps for all key events.
A samplelog file is shown in Figure 2; the user input is marked as"Utterance", the SQL is marked as "Query", the wizardinput as "Sentence", the system display as "Result", andthere are timestamps to mark when speech recordingbegins, when the sentence is sent for processing, andwhen the answer is returned.Site Speakers Scenarios UtterancesAT&TBBNCMUMITMIT: old DBSRI506243759672176307196250320134188722772480226529402301TOTAL 398 1383 14150Table h Multi-site ATIS data summary2 .1 .
BBN Data  Co l lec t ionThe BBN data collection setup employed an interactivesubject and wizard interface based on X-windows.
The3The numbers in this table reflect otal data collected, whichdiffers from the amount of data released by NIST, quoted in theprevious ection.\[UtteranceID:\] 1\[Timestamp: Sent speech for utterance 1 at 10:38:36\]\[Begin Utterance: 1\]do you have any flights from Pittsburgh to Boston onWednesday ofnext week .
in the morning\[End Utterance: 1\]\[Begin Sentence: 1\]pit to boston wednesday morning\[End Sentence: 1\]\[Timestamp: Sent sentence for utterance 2 at 10:38.
"56\]\[Begin Query: 1\]select distinct flight.airline_flight,flight.from_airport,flight.to_airport,flight.departure_time,flight.axrival_time,flight.time_elapsed,flight.stops from flightwhere flight.from_airport in ('PIT')and flight.to_airport in ('BOS')and((flight_days like '%WE%'and flight_days not like 'NOT%')or (flight_days like 'DAILY'))and(flight.depaxture_time<=1200)\[End Query: 1\]\[Begin Result: 1\]AIRL# FROM TO LEAVE ARRIVE DURA STOPSUS674 PIT BOS 1200 1328 88 0US732 PIT BOS 710 839 89 0US736 PIT BOS 840 1006 86 0\[End Result: 1\]\[Timestamp: Sent answer for utterance 1 at 10:39:00\]Figure 2: Sample log file (excerpt)subject's queries and answers were stacked on the colorscreen for later examination or other manipulation bythe subject.
The system also used BBN's real-time BY-BLOS speech recognition system as the front-end; thewizard had the choice of using the speech recognitionoutput or correcting it.
This choice allowed the wizardto give feedback (in terms of errorful speech recognition)to the subject hat may have encouraged the subject ospeak more clearly.
Certainly there would be such feed-back in a real system.The scenarios included not only trip planning scenarios,but also problem solving scenarios involving more gen-eral kinds of database access, e.g., finding the hub cityfor an airline X.
This was done to try to elicit a richerrange of language use.2.2.
CMU Data  Co l lec t ionThe Carnegie Mellon University (CMU) data collectionsystem incorporated a working ATIS system \[15\] and awizard.
The subject sat at a computer that displayed awindow containing system output, and another windowthat acted as an interface to the "recognition" systemwhich used a push-and-hold protocol to record speech.Two channels of data were recorded, using both theSennheiser and the Crown microphones.
An Ariel DM-N digitizer and a Symetrix 202 microphone pre-amplifiercompleted the equipment.
The wizard, sitting two cu-bicles away in an open-plan lab, listened to the sub-ject directly through headphones.
A modified versionof the CMU ATIS system was used to assist the wiz-ard in database access.
The wizard could paraphrasethe subject's query or correct recognition errors beforedatabase access.
Retrieved information was previewedby the wizard before being sent to the subject's display.The wizard also had available a set of standard "error"replies to be sent to the subject when appropriate ( .g.,when the subject asked questions outside the domain).Subjects were recruited from the university environment;they ranged in age from 18 to 38, with a mean of 24years.
The subjects were introduced to the system byan experimenter who explained the procedure and satwith the subject during the first scenario.
Standard airtravel planning scenarios were used.
The experimenterthen left the enclosure, but was available i f  problemsarose.
Subjects completed as many scenarios as fit intoan hour-long session.
A maximum of 6 scenarios wereavailable; an average of 4.6 were completed in the datacollected to date.2.3.
M IT  Data  Co l lec t ionThe MIT data collection paradigm emphasized interac-tive data collection and dialogue, using the MIT ATISsystem \[10, 13\].
Data were collected by asking subjectsto solve scenarios using the system; the experimentersat in another room and transcribed a "clean" version ofthe subject's peech input.
The transcriber eliminatedhesitations, "urns" and false starts, but otherwise sim-ply transmitted a transcription ofwhat the subject said.The natural anguage component then translated thetranscribed input into a database query and returned thedisplay to the user.
The MIT system produced severalforms of output for the subject, including a summary ofthe question being answered (in both written and spokenform) and a reformatted tabular display without crypticabbreviations.
The system also supported a capabilityfor system-initiated clarification dialogue to handle caseswhere the user underspecified a query.
For example, ifthe user specified only a destination, the system wouldask where the subject was departing from.Subjects were recruited mainly from MIT and consistedof undergraduates, graduate students and employees.Each subject was given a $10 gift certificate to a lo-cal store.
A data collection session lasted approximately45 minutes; it included an introduction by the experi-menter (who also acted as transcriber); practice with thepush-and-hold-to-talk mechanism; the solution of threeor four scenarios (moving from simple scenarios to morecomplex ones involving booking a flight); and completionof a debriefing questionnaire.
The data were collected inan office-noise nvironment using an Ariel Pro-Port A/Dsystem connected to a Sun Sparcstation.2.4.
AT&T Data  Col lect ionThe AT&T ATIS data were collected using a partiallysimulated, speech-in/speech-out spoken language system\[9\].
The natural anguage and database access com-ponents of the AT&T system were essentially identi-cal to those of the MIT ATIS system \[10\].
The inter-face with the the subject was designed to simulate anactual telephone-based dialogue: the system providedall information in the form of synthesized speech, asopposed to displaying information on a computer ter-minal.
Speech data were captured simultaneously us-ing (1) the Sennheiser microphone amplified by a ShureFP l l  microphone-to-line amplifier, and (2) a standardcarbon button-based telephone handset (over local tele-phone lines).
Digitization was performed by an ArielPro-Port A/D system.Before each recording session, the experimenter providedthe subject with a brief verbal explanation of the task,a page of written instructions, a summary of the ATISdatabase domain, and a list of travel planning scenarios.The system initiated the dialogue at the beginning ofthe recording session, and responded after every utter-ance with information or with an error message.
Theexperimenter controlled recording from the keyboard,starting recording as soon as the system response nded,and stopping recording when the subject appeared tohave completed a sentence.
The experimenter then tran-scribed what the subject said, excluding false starts, andsent he transcription tothe system, which automaticallygenerated the synthesized response.
A complete sessionlasted about an hour, including initial instruction, a two-part recording session with a five minute break, and adebriefing questionnaire.Subjects for data collection were recruited from localcivic organizations, and collection took place duringworking hours.
As a result, 82 percent of the subjectswere female, and subjects ranged in age from 29 to 77,with a median age of 55.
In return for each subject's par-ticipation, a donation was made to the civic organizationthrough which he or she was recruited.2.5.
SRI  Data  Col lect ionThe SKI data collection system used SKI's SLS system;there was no wizard in the loop.
The basic character-istics of the DECIPHER speech recognition componentare described in \[4\], \[6\], and the basic characteristics ofthe natural anguage understanding component are de-scribed in \[3\].
Two channels of data were recorded, usingboth the Sennheiser and the Crown microphones.
Sub-jects clicked a mouse button to talk, and the system de-cided when the utterance was complete.
The data werecollected in an office-noise nvironment, using a SonitechSpirit-30 DSP board for A/D connected to a Sun Sparc-station.Subjects were recruited from other groups at SKI, froma nearby university, and from a volunteer organization.They were given a brief overview of the system and itscapabilities, and were then asked to solve one or sev-eral air travel planning scenarios.
The interface allowedthe user to move to the context of a previous question.Some subjects used the real-time hardware version ofthe DECIPHER system \[5\], \[16\]; others used the soft-ware version of the system.
Other parameters that werevaried included: instructions to subjects regarding whatthey should do when the system made errors, the inter-face to the context-setting mechanism, and the numberof scenarios and sessions.
See \[14\] for details on the in-terface and the conditions that were varied from subjectto subject.3.
D is t r ibut ing  the  DataDuring the MADCOW collection effort, NIST was pri-marily responsible for two steps in the data pipeline: (1)quality control and distribution of "initial" unannotateddata received from the collection sites; and (2) qualitycontrol and distribution of annotated data from the SRIannotators.3.1.
D is t r ibut ion  o f  In i t ia l  DataInitial (unannotated) ata were received on 8ram tar-formatted tapes from the collection sites, logged intothe file "madcow-tapes.log", and placed in queue for dis-tribution.
The initial data consisted of a .log file foreach subject-scenario, and .way (NIST-headered speechwaveform with standard header fields) and .sro (speechrecognition detailed transcription) files for each utter-ance.
The 8ram tapes were downloaded and the initialdata and filename/directory structure were verified forformat compliance using a suite of shell program verifi-10cation programs.
Non-compliant data were either fixedat NIST or returned to the site for correction, dependingon the degree and number of problems.
Twenty percentof the utterances from each collection site was then setaside as potential test data.
The remaining data fortraining were assigned an initial release ID (date) andthe textual non-waveform data were then made avail-able to the collection and annotation sites via anony-mous ftp.
The tape log file, "madcow-tapes.log" wasupdated with the release date.
A cumulative l xicon inthe file "lexicon.doc.<DATE>" was also updated witheach new release.
During the peak of data collection ac-tivity, these releases occurred at weekly intervals.
Whenenough waveforms (.way) had accumulated to fill a CD-ROM (630 Mb), the waveforms were premastered onanIS0-9660 8ram tape which was then sent to MIT for"one-off" (recordable) CD-ROM production.
Upon re-ceipt of each CD-ROM from MIT, the initial releaseID(s) of the data~on the CD-ROM were recorded inthe file "madcow-waves.log", and the CD-ROMs wereshipped overnight to the MADCOW sites.3.2.
D is t r ibut ion  o f  Annotated  DataAnnotated ata from SRI were downloaded at NIST viaftp.
The data were organized by initial release date in thestandard ATIS file and directory structure and containedfiles for the query categorization (.cat), wizard input toNLParse (.win) a, the SQL for the minimal answer (.sql),the SQL for the maximal answer (.sq2, generated fromthe minimal SQL) and the corresponding minimal andmaximal reference answers (.ref, .rf2).The .cat, .ref, and .rf2 files in the release were verified forformat compliance using a suite of verification programs.A classification summary was then generated for the re-lease and the data made available to the MADCOW sitesvia anonymous ftp.
The "madcow-answers.log" file wasupdated with the release date.3.3.
Data  D is t r ibut ion  SummaryTable 2 shows a summary by site and class of the an-notated MADCOW data distributed by NIST as of De-cember 20, 1991.3.4.
Common Documentat ionTo facilitate common data exchange, MADCOW devel-oped a set of documents which specify the formats foreach common file type, listed below:/ -?
.wav - NIST-headered speech waveform with stan-dard header fields4This was used to produce the minimal SQL query; see sectionon AnnotationSite Class A Class XATT 164 34.8% 118 25.1%BBN 850 55.6% 334 21.8%CMU 430 38.3% 403 35.9%MIT 671 38.2% 406 23.1%SRI 335 46.8% 82 11.5%Total 2450 43.8% 1343 24.0%Class D Total189 40.1% 471 8.4%345 22.6% 1529 27.3%289 25.8% 1122 20.1%680 38.7% 1757 31.4%299 41.8% 716 12.8%1802 32.2% 5595 100.0%Table 2: Distribution of the annotated training datasummary?
.log - Session log?
.sro - SR-output detailed transcription?
.cat - Query categorization?
.ref- Minimal reference answer?
.rf2 - Maximal reference answerIn addition, documentation was developed to specify di-rectory and filename structures, as well as file contents.To insure conformity, NIST created and distributed for-mat verification software for each file type and for direc-tory/filename structures.
The specifications documentsand verification software are maintained for public dis-tribution in NIST's anonymous ftp directory.
NIST alsomaintains documentation for the transcription conven-tions, logfile formats, categorization principles and prin-ciples of database interpretation, also available in NIST'sanonymous ftp directory.To track the flow of data through the distribution"pipeline" during data collection, NIST maintained andpublished the data flow logs and documentation modifi-cations in weekly electronic mail reports to MADCOW.4.
The  Eva luat ion  Parad igmThe diversity of data collection paradigms was a con-cern for MADCOW.
To control for potential effects in-troduced by this diversity, it was agreed that test setswould consist of comparable amounts of data from eachsite (regardless of the amount of training material avail-able from that site).
In addition, benchmark test resultswould be displayed in an N * M matrix form (for the Nsystems under test from the M data collecting sites).
Forthe February 1992 tests, the number of collecting sites(M) was 5.
This format was intended to indicate if datafrom one collecting site were "outliers" and whether asite performed particularly well on locally collected ata.The February 1992 Evaluation required sites to gener-ate answers for data presented in units consisting of a"subject-scenario'.
The utterances from the scenario11were presented in sequence, with no annotation as tothe class of the utterances.
For scoring purposes, as inprevious ATIS Benchmark tests \[7\], test queries weregrouped into several classes on the basis of annotations.Results for the context-independent sentences (Class A)and context-dependent sentences (class D) were com-puted and tabulated separately, along with an overallscore (A + D).
Class X queries ("unanswerable" queries)were not included in the NL or SLS tests, but were in-cluded in the SPREC tests (since valid .lsn transcriptionsexisted for these utterances).
The matrix tabulations re-ported on % correct, % incorrect and % weighted error 5defined as \[2 ?
(%Fa lse)  + (%No_Answer) \ ] .The February 1992 results also reflected a new methodof computing answer "correctness" using both a mini-real and a maximal database reference answer.
The ob-jective was to guard against "overgeneration": gettinganswers correct by including all possible facts about agiven flight or fare, rather than by understanding whatspecific information was requested.
This method (pro-posed by R. Moore and implemented by Moore and E.Jackson of SRI) specified the maximum relevant informa-tion for any query, and required that the correct answercontain at least the minimal correct information, and nomore than the maximum.
This method was first usedduring the October 1991 "dry run" and was adopted asthe standard scoring procedure by the DARPA SpokenLanguage Coordinating Committee.Three types of performance assessment tests were com-puted on the ATIS MADCOW Benchmark Test Data:SPeech RECognition (SPREC), Natural Language (NL),and Spoken Language Systems (SLS) tests.
Details ofthese tests, and a summary of "official" reported results,are to be found elsewhere in these Proceedings \[8\].5.
Annotat ionThe goal of annotation was to classify utterances andprovide database reference answers for the subjects'queries in the ATIS domain.
These reference answerswere used by the system developers and by NIST to eval-uate the responses of the MADCOW natural languageand spoken language systems.The annotators began with the transcribed .sro files, anddetermined the possible interpretations of each utter-ance, classifying them as one of the following:?
A: context-independent?
D: context-dependent (classification includes tag(s)pointing to the context-setting query or queries).5This single number performance measure was first introducedfor the February 1991 tests..sro #1: show me flights from Pittsburgh to Boston on septemberfourth in the morning.cat #1: A:.win #1: List morning flights from Pittsburgh and to Boston andflying on 9/4/91.ref #1:(138860 138861 38862).rf2 #1: Very long; may contain the following information:flight ID, flight#, airline, times, date, day name, frequency, citynames, city codes, airport codes..sro #2: what classes of service are there on flight U S seventhirty -.cat #2: X: trunc-utt.sro #3: are there meals on that flight.eat #3: X: context-dependent: Q002.sro #4: are there meals on U S seven thirty two.cat #4: D: testably-ambiguousinterp#l: yes/no context-dep:Q1interp#2: wh-ques context-dep:Q1.win #4: List food services erved on flights from Pittsburgh andto Boston and flying on 9/4/91 and whose airline code is US andwhose flight number is 732.ref #4: (YES OR(("B" 1 "COACH")( "B" 1 "FIRST"))).r/2 #4:(YES OR(('B" 1 "COACH ....
PIT .... BOS ....
WED ....
US" 732 "PPIT"BBOS" 9/4/91 138860 "PITTSBURGH" "BOSTON" "DAILY")("B" 1 "FIRST" "PIT .
.
.
.
BOS" "WED" "US" 732 "PPIT""BBOS" 9/4/91 138860 "PITTSBURGH" "BOSTON""DAILY")))Figure 3: Annotation files from a sample ATIS session?
X: unevaluable (explanation provided by a tag inthe classification).Those utterances which were evaluable (class A or D)were translated into an English-like form (.win for wizardinput) that could be interpreted by NLParse, a menu-driven program that converts English-like sentences intodatabase queries expressed in SQL.
Annotation deci-sions about how to translate the .sros were guided bythe "Principles of Interpretation" (see the next section).After the .sro form of an utterance was classified andtranslated, the work was checked thoroughly by anotherannotator and by various checking programs.
NLParsewas then run to generate an SQL form in a .sql file.
Fi-nally a series of batch programs was run on each .sql fileto produce the minimal and maximal reference answers(.ref and .rf2 files) for the corresponding utterance.Figure 3 shows the annotation files created for a sam-ple ATIS dialogue.
Each line in italics identifies the file;the .sro file is the input; the .cat, .win, .ref and .rf2 filesare created during the annotation procedure.
Sentence#1 is class A, and has as its minimal reference answerthe set of flight IDs for flights meeting the constraints.The maximal answer contains all of the columns used inthe .sql query to constrain the answer; the answer is too12large to be displayed here.
The .sro for sentence #2 endswith a truncation (marked by a tilde ~ ), which causesit to be classified as X (unevaluable).
Thus no .win, .refor .rf2 files are generated.
Sentence #3 is a context-dependent utterance, due to the anaphoric expressionthat flight.
It depends on #2, but since #2 is class X,#3 is also classified as X, following the principle thatanything that depends on a class X (unevaiuable) sen-tence must itself be unevaluable.
Finally, sentence #4is a yes-no question, which may have two answers: ei-ther YES or the set of entities atisfying the constraints.This sentence is also context-dependent, since it refers toflight US 732 between Pittsburgh and Boston.
(Flight732 may go to other cities, thus context is needed to es-tablish the segment of interest).
The minimal referenceanswer to the question about meals is defined to be thetriple (meal,number,class).
The maximal answer can in-clude any information used in the .sql to generate theminimal answer.6.
Principles of InterpretationIn order to carry out an objective valuation, it was nec-essary to be able to say whether an answer was right orwrong.
In turn, deciding on the right answer often de-pended on how particular words and constructions in thequery were interpreted.
Thus, it was recognized early onin the development of the ATIS common task that itwould be necessary to agree on specific definitions forcertain vague expressions.
In addition, given the cur-rent database, there was often more than one reason-able way of relating particular queries to the database.To insure objectivity in the evaluation, decisions abouthow to interpret queries had to be documented in sucha way that all participants in the evaluation had accessto them.
The Principles of Interpretation document de-scribes the interpretation of queries with respect o theATIS database.
This document was used both by systemdevelopers to train their systems and by the annotatorsfor developing reference answers.Examples of decisions in the Principles of Interpretationinclude: the meaning of terms like early morning, classi-fication of a snack as a meal for the purposes of answeringquestions about meals, and the meaning of constructionssuch as between X and Y, defined for ATIS to mean "fromX to Y".A subgroup on the Principles of Interpretation wasformed to discuss and make decisions on new issues ofinterpretation as they arose data collection and anno-tation.
A representative from each site served on thissubgroup.
This insured that all sites were notified whenchanges or additions occurred in the Principles, and al-lowed each site to have input into the decision process.It was important o make careful decisions, because anyrevision could cause previously annotated ata to be-come inconsistent with the revised Principles of Inter-pretation.
On the other hand, in many cases there wasno one "correct" way of interpreting something, for ex-ample, the classification of a snack as a meal.
In caseslike this, the main goal was to make sure that all partic-ipants understood the chosen interpretation.It Was agreed that reference answers should emphasizeliteral understanding of an utterance, rather than a co-operative answer which might cause more informationto be included than what was actually requested.
How-ever, to support systems used for demonstrations and fordata collection as well as for evaluation, answers neededto be minimally cooperative, since otherwise demonstra-tion systems would have to answer differently from eval-uation systems.
Thus the main criterion was how wellthe proposed interpretation reflected understanding ofthe query, with some consideration for providing a coop-erative answer.7.
ConclusionThe MADC0W experiment in multi-site data collectionand evaluation has been successful.
The participatingsites have collected a rich corpus of training data, haveput in place methods for distributing the data, and havedevised test procedures to evaluate speech, natural an-guage, and spoken language results on a test corpus.
Theresources made available by the multi-site paradigm haveallowed us to collect more data and to learn more aboutdata collection than would have been possible with onlyone or two sites collecting data under a special contract.Some difficult problems till remain.
Our shared goal isto build interactive spoken language systems; however,our evaluation methods rely on a canned corpus andevaluate a system's recognition performance under staticconditions that are not representative of the interactiveenvironments in which these systems will eventually beused.
In addition, the ATIS task has been limited so farto a small, static subset of the air travel domain.
Thesedifficulties will increase as research sites develop differ-ent approaches to actively managing interaction with theuser: different processing strategies will generate diver-gent behaviors on the part of users, but this divergencewill lessen the validity of tests that assume comparableresponses to a sequence of queries.The MADCOW collection and evaluation procedureshave provided effective tools for assessing the current ca-pabilities of interactive spoken language systems.
How-ever, we must continue to improve our methods ofdata collection and evaluation.
For example, we have13only just begun to explore the use of real-time spo-ken language systems for data collection and evaluation.We also need to more towards a larger, more realis-tic, database.
As our spoken language systems evolve,data collection and evaluation methods must evolve withthem.8.
AcknowledgementsThe Multi-site ATIS Data Collection Working group in-cludes a representative from each site involved in theATIS data collection effort: Lynette Hirschman, Chair(MIT), Jay Wilpon (AT&T), Madeleine Bates (BBN),Alexander Rudnicky (CMU), David Pallett (NIST),Deborah Dahl (Paramax), Patti Price (SRI) and JaredBernstein (SRI-Annotation).
Many more people havemade key contributions and have been an integral partof the multi-site effort.
This includes the staff at NIST(Nancy Dahlgren, William Fisher, Jon Fiscus, KathyGallo, John Garofolo, Brett Tjaden) and the AnnotationGroup at SRI (Kate Hunicke-Smith, Harry Bratt, andBeth Bryson).
In addition, Robert Moore and Eric Jack-son (SRI) provided critical help in designing and debug-ging the code to support the minimal/maximal nswercriterion.
Individuals responsible for the data collec-tion at the various ites include: Christine Pao (AT&T),Robert Bobrow, Robert Ingria, David Stallard, andVarda Shaked (BBN), Robert Weide, Cynthia Neelanand Sondra Ahlen (CMU), Joseph Polifroni, StephanieSeneff, and Christie Clark Winterton (MIT), and Eliz-abeth Shriberg, Elizabeth Wade, and Steven Tepper(SRI).
Deborah Dahl (Paramax), together with WilliamFisher (NIST), took on responsibility for maintaining thePrinciples of Interpretation i conjunction with the SRtAnnotators.
The Principles of Interpretation WorkingGroup included Evelyne Tzoukermann (AT&T), DavidStallard (BBN), Wayne Ward (CMU), Stephanie Sen-eft (MIT), and Robert Moore (SRI).
Michael Phillipsand Christie Clark Winterton (MIT) were responsiblefor creation of the CD-ROMs at MIT.
Finally, many in-dividuals at the various sites played an important rolein checking and debugging the data and their anno-tations, including Robert Bobrow, Robert Ingria, andDavid Stailard (BBN), Joseph Polifroni and StephanieSeneff (MIT), William Fisher, Nancy Dahlgren, Jon Fis-cus, John Garofolo, Brett Tjaden (NIST), Lewis Norton(Paramax), and Eric Jackson (SRI).References1.
Bates, M., S. Boisen, and J. Makhoul, "Developingan Evaluation Methodology for Spoken Language Sys-tems," Proe.
Third DARPA Speech and Language Work-shop, R. Stern (ed.
), Morgan Kaufmann, June 1990.2.
Hemphill, C. T., J. J. Godfrey, and G. R. Doddington,"The ATIS Spoken Language System Pilot Corpus,"Proc.
Third DARPA Speech and Language Workshop,R.
Stern (ed.
), Morgan Kaufmann, June 1990.3.
Jackson, E., D. Appelt, J.
Bear, R. Moore, A. Podlozny,"A Template Marcher for Robust NL Interpretation,"Proc.
DARPA Speech and Natural Language Workshop,P.
Price (ed.
), Morgan Kaufmann, 1991.4.
Murveit, H., J. Butzberger, and M. Weintraub,"SpeechRecognition in SRI's Resource Management and ATISSystems," Proc.
DARPA Speech and Natural LanguageWorkshop, P. Price (ed.
), Morgan Kanfmann, 1991.5.
Murveit, H. and M. Weintraub, "Real-Time SpeechRecognition System," Proc.
DARPA Speech and NaturalLanguage Workshop, P. Price (ed.
), Morgan Kanfmann,1991.6.
Murveit, H., J. Butzberger, and M. Weintraub, "Perfor-mance of SRI's Decipher Speech Recognition System onDARPA's ATIS Task," Proc.
DARPA Speech and Natu-ral Language Workshop, M. Marcus (ed.
), Morgan Kanf-mann, 1992.7.
Pallett, D., "Session 2: DARPA Resource Manage-ment and ATIS Benchmark Test Poster Session", Proc.DARPA Speech and Natural Language Workshop Work.shop, P. Price (ed.
), Morgan Kaufmann, 1991.8.
Pallett, D., et al "February 1992 DARPA ATIS Bench-mark Test Results Summary," Proc.
DARPA Speech andNatural Language Workshop, M. Marcus (ed.
), MorganKaufmann, 1992.9.
Pao, C. and J. Wilpon, "Spontaneous Speech Collec-tion for the ATIS Domain with an Aural User-FeedbackParadigm," Proc.
DARPA Speech and Natural LanguageWorkshop, M. Marcus (ed.
), Morgan Kaufmann, 1992.10.
Poliffroni, J., S. Seneff, V. W. Zue, and L. Hirschman,,ATIS Data Collection at MIT," DARPA SLS Note 8,Spoken Language Systems Group, MIT Laboratory forComputer Science, Cambridge, MA, November, 1990.11.
Price P., "Evaluation of Spoken Language Systems: TheATIS Domain," Proc.
Third DARPA Speech and Lan-guage Workshop, P. Price (ed.
), Morgan Kanfmann,June 1990.12.
Ramshaw, L. A. and S. Boisen, "An SLS Answer Com-parator," SLS Note 7, BBN Systems and TechnologiesCorporation, Cambridge, MA, May 1990.13.
Seneff, S., L. Hirschman, and V. Zue, "interactiveProblem Solving and Dialogue in the ATIS Domain,"Proc.
Fourth DARPA Speech and Language Workshop,P.
Price (ed.
), Morgan Kaufmann, February 1991.14.
Shriberg, E., E. Wade, and P. Price, "Human-MachineProblem Solving Using Spoken Language Systems(SLS): Factors Affecting Performance and User Satisfac-tion" Proc.
DARPA Speech and Natural Language Work-shop, M. Marcus (ed.
), Morgan Kaufmann, 1992.15.
Ward, W., "Evaluation of the CMU ATIS system"Proc.
Fourth DARPA Speech and Language Workshop,P.
Price (ed.
), Morgan Kaufmann, February 1991.16: Weintraub, M., G. Chen, P. Mankoski, H. Murveit, A.Stolzle, S. Narayanaswamy, R. Yu, B. Richards, M.Srivastava, J. Rabay, R. Broderson, "The SRI/UCBReal-Time Speech Recognition System," Proc.
DARPASpeech and Natural Language Workshop, M.
Marcus(ed.
), Morgan Kanfmann, 1992.14
