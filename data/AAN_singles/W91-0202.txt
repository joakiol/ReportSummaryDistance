Syntax-Driven and Ontology-Driven LexicalSemanticsSergei Nirenburg and Lori LevinCenter for Machine TranslationSchool of Computer ScienceCarnegie Mellon University{ sergei, lsl} @nl.cs.cmu.eduAbstractIn this position paper we describe the scopes of two schools in lexicM semantics,which we call syntax-driven lexical semantics and ontology-driven lexical semantics,respectively.
Both approaches arc used in various applications at The Center forMachine Translation.
We believe that a comparative analysis of these positions andclarification of claims and coverage is essential for the field as a whole.There are different traditions in the study of lexical semantics.
Two of them seem tobe the most current in computational linguistics and its applications -- the one basedon syntactic theory studies and the other, on the artificial intelligence approaches.
Theformer seeks to discover semantic properties of lexical items from which syntactic behavior(such as subcategorization and participation in transitivity alternations) is predictable.
(See Grimshaw, 1990; B. Levin & Rappaport Hovav, 1990.)
The latter tries to establishthe meaning of natural language texts with the help of an independently constructed"world model" (often called "ontology") which explicates relations among entities in theworld rather than lexical units.It is customary to pitch the two approaches as competing.
And in practice, researcherstypically develop lexical-semantic theories and computer systems based on them usingonly one of the two.
We would like to argue, however, that these approaches to semanticsare much closer to one another in their aims and research methodologies than to anyother schools of semantics ("logical," "philosophical," "formal," etc.)
From a practicalstandpoint, we also have a different experience to report.
We believe that neither ontheoretical grounds nor with respect to computational applications is there a necessityto make one of the approaches prominent at the expense of the other.
We have carriedout both theoretical and practical work in which the approaches seem to coexist.
Thecentral role of the syntax-driven lexical semantics in the process of NLP  is to decode thenature of the dependency between heads of phrases and their arguments in a particularlanguage.
This knowledge is then used in ontology-driven lexical semantics as a necessaryset of heuristics which allow us to represent the meaning of a text in terms of a language-independent conceptual model.
Thus, we believe that a comprehensive approach shouldcombine the benefits of both approaches and that neither will on its own be sufficient forrealistic NLP.1 Syntax-Driven Lexical SemanticsIn a knowledge-based system, the primary role of syntactic analysis is to decode knowl-edge that is syntactically encoded.
An important component of such knowledge is therelationship between predicates and their arguments.
For example, in order to know thatthe English sentence Max interviewed Hesler for a job means that Max was consideringhiring Hester and not that Hester was in the position to hire Max, it is necessary to knowthat the interviewer role is expressed as the subject of the sentence, that the intervieweerole is expressed as the object, that the subject precedes the verb and that the objectfollows it.
Much of the information about the syntactic encoding of arguments can beplaced in lexical entries of argument-taking predicates.
Classes of predicates with simi-lar argument encodings usually have some semantic similarity, and thus define a kind oflexical semantics based on semantic features that are predictors of syntactic behavior.In light of these observations, our theory of lexical semantics includes a componentof lexical knowledge that describes the syntactic encoding of arguments.
This lexicalknowledge consists of Lexical Conceptual Structures (LCS) and Linking Rules.
We willassume that the LCSs of any given language are composed from a universal set of buildingblocks (including, but not limited to those described by Jackendoff, 1983, 1990).
However,the presence of lexical gaps--meanings that are lexicalized in some languages but must beexpressed phrasMly in another language--suggests that languages do not all share exactlythe same inventory of LCSs.
The most important property of the LCSs of any individuallanguage is that they contain all of the information ecessary for Linking Rules to calculatethe syntactic encoding of predicates and arguments.
Thus they contain syntacticallyrelevant semantic information.Linking Rules describe how LCSs are syntactically encoded in a language and theypartition LCSs into classes of predicates that have similar syntactic encodings.
LinkingRules and the verb classes they define are language-specific (Mitamura, 1989), but gov-erned by language-universal principles.
The particular version of linking rules and verbclasses we have implemented is based on Lexicai Functional Grammar (L. Levin, 1987;Bresnan & Kanerva, 1989), and includes the treatment of the phenomena briefly discussedin the rest of this section.1.1 Which  Grammat ica l  Funct ion  Is Assoc ia ted  w i th  Each  Argu-ment?Linking Rules associate grammatical functions with arguments in a LCS.
For example,the English verb interview associates its interviewer role with the grammatical functionSUBJECT and its interviewee role with the grammatical function OBJECT.
Argument-taking predicates themselves fall into subcategoriies (or subcategorization classes) basedon the grammatical functions that they assign to their arguments.
For example, thosethat assign SUBJECT and OBJECT, those that assign SUBJECT and OBLIQUE, etc.1.2 Which  Syntact i c  Pos i t ion ,  Case  Marker ,  o r  Agreement  Mor -pheme Encodes  Each  Argument?After linking LCS arguments to grammatical functions, the next step in the syntacticencoding of LCS arguments i to assign a syntactic position or morphological marker toeach argument.
For example, in English, the interviewer ole is syntactically encodedto the left of the verb.
In another language, the verb might specify a case marking forthe interviewer ole.
Default information about syntactic encoding of arguments (e.g.,SUBJECTs have nominative case) can be specified in the syntactic rules of the language,instead of in the lexicon.
However, after these defaults are specified, there remains a10considerable amount of lexically specific information--which prepositions mark obliquearguments, which verbs take dative objects, and so on.1.3 Which Predicates Participate in Alternate Assignments ofGrammatical Functions?Many predicates allow multiple assignments of grammatical functions or case markings totheir arguments.
For example, the well-known spray/load alternation i volves the assign-ment of the grammatical function OBJECT or OBLIQUE to an argument that is filled orcovered, as in spray the wall with paint or spray paint on the wall.
The passive construc-tion also results from an alternation in grammatical function assignment to arguments.An argument that has the OBJECT function in an active sentence has the SUBJECTfunction in a passive sentence.Argument-taking predicates fall into classes according to which alternations they allow.When a set of LCSs is identified that all allow the same alternations, it is usually possible toidentify a semantic feature that they have in common that correlates with the transitivityalternation.
Hence LCSs that undergo the same transitivity alternations appear to formsemantic lasses - -  for example, the change of state verbs, the spray-load verbs, verbs ofremoving, verbs of creation, etc.
(See B. Levin (1989) for an extensive list of such classes.
)1.4 In Which Argument-Structure-Building Operations Does thePredicate Participate?Talmy (1975, 1985), Jackendoff (1990), and B. Levin & Rapoport (1988) among othershave proposed rules that conflate two lexical meanings to create a new lexical meaning.Such argument-structure-building rules in English include incorporation ofmanner of mo-tion and directed motion (the argument structure for jump and the argument structure forgo combine to form the argument structure for jump into the room); resultative secondarypredication (the argument structure for become and the argument structure for hammercombine to form the argument structure for hammer the nail fiat); and many others.
Forthese operations, we are assuming that two LCSs are combined to form a new LCS, andthis new argument structure undergoes assignment ofgrammatical functions and syntac-tic encoding of arguments.
The allowable types of LCS conflation and the classes of verbsthat undergo them vary from language to language.All of the operations mentioned in this section identify syntactic lasses of verbs--theverbs that undergo the same transitivity alternations, the verbs that map their argumentsonto the same grammatical functions, the verbs that undergo a certain argument s ructurebuilding operation, the verbs that encode their arguments syntactically in a certian way.It is usually possible to find semantic features of LCSs that correlate with these classes--e.g., a change of state, an effect on a patient, etc.
In this sense syntactic patterns of verbclasses define a lexical semantics.
This lexical semantics i language-specific in that themembership ofverb classes and the semantic features that they are based on are differentin different languages.112 Ontology-Driven Lexical SemanticsWhile the focus of attention in syntax-driven lexical semantics i on components of lexicalmeaning that determine syntactic behavior, the task of ontology-driven lexical semantics ito determine and specify meanings of lexieal units, suggest how they must be represented(in the lexicon) and how they contribute to the overall representation f text meaning.
Weview this task as a component of the more general task of representing the meaning of atext in a computational application.
This view has been developed mostly in the traditionof natural anguage processing within artificial intelligence.
(It is impossible to referenceall of the contributors to this, by no means monolithic, paradigm; a very incomplete list ofreferences might include Wilks (1975), Schank (1973), Sowa (1984), Hirst (1987), articlesin Hobbs and Moore (1985) or Lehnert and Ringle (1982)).Some of the desiderata for an ontology-driven lexical semantics are discussed in theremainder of this section.2.1 I t  Must  Be  Const ruct iveAmong our differences from a typical practice in model-theoretical semantics i our "con-structive" attitude to the models.
We believe that in order to be able to discuss model-theoretic semantics, the models (that is, the ontologies) must first be constructed and putin correspondence with the lexicon.
This premise also sets us apart from some work inlinguistic semantics, for instance, Jackendoff's.
At the same time, we do not strive to usea very limited set of primitive concepts.One way of constructing a model is to come up with a set of properties describingthings in the world, define value sets for these properties and then describe ach conceptin the world as a set of particular property-value pairs.
For reference purposes uch setscan be named.
The result is a world model, or computational ontology.
A number ofontologies has been recently suggested and built, though not al of them with languageprocessing in mind (cf.
Lenat and Guha, 1990, Dahlgren and McDoweil, 1989, etc.
).If some of the properties reflect links among ontological units, then the ontology canbe viewed as a multiply interconnected network.
Such a network can support "reasoning"about the world encoded in it, if special heuristic rules for network traversal (includinginheritance) are defined (see, e.g., Woods (1975) Brachman (1983), Horty et hi.
(1990)).2.2 I t  Must  Suppor t  D isambiguat ionIn a computational environment, syntax-driven lexical semantics helps distinguish mean-ings but not represent them.
Indeed, in some cases knowledge of predicate-argumentstructure can disambiguate a lexical unit entirely.
Thus, the three senses of spray inLDOCE (illustrated by spray paint on a wall; the water sprayed out over the garden andspray a wall with paint) can be completely disambiguated on the basis of differences intheir predicate-argument structure.
However, after the choice has been made, the correctmeaning needs to actually be represented.
That is, we need a representation f mean-ing that shows how the different senses of spray are different, not simply that they aredifferent.However, in some cases a lexical unit cannot be completely disambiguated after thepredicate-argument a alysis is performed.
Consider the English verb hack.
We call dis-tinguish two senses of the word - -  the cutting and tile programming one.
The difference12between these senses cannot be expressed in terms of the predicate argument structure.We will need to specify, for instance, that in one of the senses the instrument of the eventis a knife or an axe and in the other, a computer.
This type of information is, in fact, afamiliar selectional restriction.
Specifying selectional restrictions i different from specify-ing predicate-argument relationships.
The specification of selectional restriction falls intothe purview of ontology-driven lexical semantics.In the lexis realm, ontology-driven lexical semantics provides the residue of mean-ing beyond the predicate-argument structure.
Additional disambiguating power is to beprovided on top of the verb class distinctions of the syntax-driven lexical semantics-- forinstance, selectional restrictions have to be formulated.
Therefore, the lexicon should con-tain knowledge that is not required for decoding the syntactic realization of arguments.Naturally, this calls for additional expressive power in the formalism and, crucially, forreliance on an independently developed model of the world as the source of meaninginterpretation for lexical units (see next item).2 .3  Meta language Must  Be  D i f fe rent  f rom Ob ject  LanguageNo matter how extensive a computational dictionary is prepared for an application, therewill always be cases when a given set of selectional restrictions will fail to help disambiguatea text.
This means that even more disambiguating power should be added to the arsenal ofa lexical-semantic theory.
The cases where selectional restrictions are not powerful enoughare typically cases of metonymy or metaphor.
Note that classification of phenomena streatable by selectional restrictions or only by metonymy/metaphor processors cruciallydepends on the content of the dictionary - -  the less selectional restriction information inthe dictionary and the fewer word senses distinguished, the more work remains for themetonymy/metaphor processor.Since processing metaphor and metonymy is more effort-consuming than using selec-tional restrictions, it is natural that lexical semanticists have been searching for ways ofreducing the need for the former.
One way is to require very large and detailed lexicons.In part, this is the reason why some researchers suggested that word senses from a naturallanguage could be used as elements of a metalanguage.We believe in the distinction of metalanguage and object language and therefore claimthat the meanings of natural language units should not be represented using unadornedlexical units of the same language.
Ill the most general case, the meaning-encoding met-alanguage should be a full-blown language-independent conceptual world model.Constraints like selectional restrictions and distinctions among word senses have to beexpressed using symbols - -  either words of a natural language or elements of an artificial"language of thought" (in the sense of Fodor, 1975).
One problem with using words of anatural anguage is that they are typically themselves ambiguous and therefore a computerprogram which has to use them as disambiguation heuristics will run into additionaldisambiguation requirements.Another reason for not basing an ontology on word senses of a natural anguage is thatthere are cases where different word senses, as specified in a human-oriented dictionary,seem to be divided quite artificially.
Consider, for instance, the entry for preempt inLDOCE.
It has four senses all of which conform to a single predicate-argument structure.All of them have something to do with having authority for preferential treatment withrespect o goods or services.
Ill fact, if one is so bent, one can think about distinguishingmore than four such senses, at about the same level of generality.
We believe that such13cases are pr ime candidates for merging the senses into one and treat ing the semanticanalysis process for them in the metonymy/metaphor  mode.The above seems to argue that  if automat ic  seeding of a computat iona l  lexicon isundertaken from an MRD,  a nontr iv ial  manual  pass over the result ing lexicon should becarried out  to make sure that  the grain size of sense dist inction is commensurate  with thelexical and conceptual  detail  in the corresponding domain model.2.4 P rocedures  fo r  Ass ignment  o f  Mean ing  Are  an  In tegra l  Par to f  the  TheoryOntology-driven lexical semantics defines the meaning of a lexical unit through a mappingbetween a word sense and an ontological concept (or a part thereof).
The process of textanalysis, then, consists essentially of instantiating the appropriate ontological concepts ormodifying property values in concepts that have already been instantiated.
12.5 I t  Must  Of fe r  a T reatment  o f  the  Phenomena o f  Synonymy,Hyponymy and AntonymyIn a theory where lexical meaning is expressed in terms of mappings into instances ofontological concepts, these standard lexical-semantic relations are explained in terms ofrelations among elements of a world model.
Thus, synonymy will be defined as a relationamong the lexical units at least some of whose senses map into the same ontologicalconcept.
2 Lexical unit A is defined to be a hyponym of lexical unit B if the concept inwhich at least some of the senses of A map stands in the taxonomical is-a relation to theconcept into which lexical unit B is mapped.
Antonyms are defined on lexical units whichmap into regions on an attribute scale.
Specifically, antonyms map into regions that aresymmetrically positioned around the center point of the scale.
32 .6  Text  Mean ing  Representat ion  Language Shou ld  Not  Be  TooC lose  to  the  Syntax  and  Lex is  o f  Any  Natura l  LanguageSince the purpose of an LCS in syntax-driven lexical semantics is to reflect the syntacti-cally relevant semantic properties, that is those properties that determine how an LCS ismapped into a syntactic dependency representation, the LCSs are usually closely tied tothe syntax and lexis of a specific language.
They are therefore not as useful in multilin-gual computational environments as in monolingual ones.
This is because languages differin their lexicalization patterns and in patterns of syntactic dependency.
Following aresome examples that call for departure from the lexical semantics of individual anguagesin muir-lingual processing enviroments.1 Lexical-semantic information is, of course, only one of several components ofa text meaning represen-tation.
In practice, additional information (e.g., pragmatic and discourse-related) is encoded in lexiconentries and is represented aspart of text meaning.2It should be noted that in our application of lexlcal semantics, the I~ltmvsus project, such mappingsare allowed to be constrained, not just univocal (for instance, the English verb taxi maps, in one of i tssenses ,  to the  concept move-on-surface but only if the theme of this instance is constrained to the conceptof airplane or one of its ontological descendents) - -  see Onyshkevich and Nirenburg (1991) for additionaldetail.aNote that synonymy on scalars is defined as mapping onto the same region of a scale.14A head-modifier relationship can be realized in opposing ways in different languages;thus, a morpheme carrying the aspectual meaning can be realized as the head ofa phrase (like the verb continue or finish), with a word denoting an event as itscomplement; alternatively, the word denoting the event can be realized as the mainverb while the aspectual meaning can be realized through an adverb or even a affix.Lexical Gaps: Russian has no word that corresponds exactly to the English wordafford (as in I can't afford X or I can't afford to Y).
In a multilingual process-ing environment, there might be a concept corresponding to a sense of the Englishword afford.
A Russian sentence Jane mogu sebe etogo pozvolit' (I can't allow my-self this), uttered in a context of acquisition - -  which could have been assigned astraightforward lexical-semantic representation if we were building a lexical semanrtics for Russian alone - -  should involve the concept hat represents afford.
Thismeans that if the units of the representation language are chosen so that they arebased on Russian lexis, the meaning of afford will be missing.
But this meaningseems ufficiently basic to be included in an ontology.
As a result, if lexical patternsof many languages are used as the clues for building ontology, the quality of thelatter should increase.Different languages describe the same event using different lexical semantics.
Forexample, the ritual act of washing one's body in Islam is expressed with a change ofstate verb in Arabic (to get washed), as a verb of receiving in Indonesian (to receiveablutions), and with an agentive verb in French (to make ablutions) (Mahmoud,1989).For reasons uch as the above, we argue that, at least in some applications, uch asmultilingual MT, a more language-independent r presentation scheme is preferable.3 Our Pos i t ion  on Some of the Quest ions  Posed inthe Call  for PapersOn the basis of the above discussion, our opinions on some of the questions posed in thecall for papers are as follows.3.1 What  is Wor ld  Knowledge  and  What  Is Knowledge  o f  Lan-guageLexical knowledge of a language is relevant for mapping phrases in syntactic structuresonto argument positions of a predicate.
World knowledge is relevant for representing thedistinctions among the senses of particular predicates and arguments which cannot bedistinguished by distributional characteristics.3.2 Cross -L ingu is t i c  Ev idence  for  the  Spec i f i c i ty  o f  Lex ica l  Se-mant ic  Representat ionOur experience in medium-scale KBMT projects hows that verb classes defined by linkingrules in different languages are not identical.
This means that the syntax-driven lexicalsemantics component of work on a computational pplication will be different for every15language involved.
For example, in Arabic there is a transitivity alternation that is similarto the causative/inchoative alt rnation (Someone broke the glass / The glass broke) in En-glish.
However, The English and Arabic transitivity alternations apply to different classesof verbs.
The Arabic rule, for instance, applies systematically to verbs of psychologicalchange of state (excite, please, stun, entertain, confuse, etc.
), but the English rule doesnot apply to these verbs.
Since our goal in developing KBMT systems is to support mul-tilinguality, we preferred to create a single, deeper, interlingual meaning representationwhich will not be subject o the divergences between verb classes in individual languages(Mitamura, 1989).4 An Example:  Two Semant ic  StructuresIn this section we will show the differences in analyzing a single sentence of English ac-cording to each of the two methods of semantic analysis.
This sentence has been extractedfrom a journalistic text, in which it was actually a clause in a compound sentence.Revenues and earnings will begin to post double digit growth in 1992./,From the standpoint of syntax-driven lexical semantics, the desiderata of analysisinclude preserving argument s ructure and producing asemantic analysis in such a mannerthat syntactic realization of predicates and their arguments will be attainable throughlinking rules.
We will concentrate solely on verbs.Following Jackendoff (1976, 1983, 1990), we will analyze begin as a change of circum-stance.
In other words, revenues and earnings enter tile circumstance of posting doubledigit growth.
For verbs in the change-of-circumstance class, the argument that changescircumstance links with the grammatical function SUBJECT.
The circumstance can berealized as a nonfinite complement.
We will treat post as a conflation whose meaning issimilar to "show by posting" and which is a member of the same verb class as show ordisplay in the sense of to have an observable property.
4 The linking rules for this classlink the property with the OBJECT grammatical function and link the object that hasthe property with the SUBJECT grammatical function.The LCS for the above example shows two predicates - -  BEGIN and SHOW-BY-POSTING.
Notice that we have avoided the habitual thematic role names for their argu-ments and adjuncts.
Linking to grammatical functions i shown by markers in parentheses.BEGI|thing-that-changes-circumstance: revenues and earnings (SUB J)nee-circumstance: SHOW-B?-POSTIIG (KCONP)thing-that-has-a-property: revenues and earnings (SUB J)property:  double d ig i t  g ro .
th  (0BJ)t ime: 1992 (Adjunct)/,From the standpoint of ontology-driven lexical semantics, the desiderata of analysisinclude a) independence of the syntax of representation from the syntax of the source text,b) explicitly representing all implied meanings which are referred to in the source text -either lexically or through deixis or ellipsis or other means, c) keeping the set of atomic4 A conflatlon is a combination of two LCSs, A and B, into a new LCS, C. The type of conflation in thisexmnple is that C has the meaning of "B by A-lug" and follows the linking rules and subcategorizationpatterns of B.
More on the phenomenon of conflatlon see in Jackendoff (1990), B. Levln and Rapoport(1988), Talmy (1975, 1985).16meaning representation elements as low as possible (this latter goal does not imply a desir?to postulate a small fixed set of meaning primitives).
An ontology-driven lexical-semanticrepresentation for the above example can look as follows: ~c lausehead: inc rease-1aspect:phase:  beg indurat ion :  p ro longedi te ra t ion :  s ing let ime re la t ive :  ' ' a f te r  t ime o f  speech ' 'abso lu te :  ' 'dur ing  1992' 'inc rease-1theme: ' ' revenues  and earn ings ' 'ra te :  ' 'g reater  than 10~ and less  than 100~' 're la t ion -1type :  tempora l -be fore.
?
:  .
, .f rom: ' 'time of speech'' .-:?"
, ,,, . "
'..:!~'-to 1992 .
-.
~ .
.
.
.
.
.
.
.
,,:.....~ .- -.Note that the verbal meanings are factored out rote propos!t!oa_~l $~ga i~!~/~i ; ;  ( 'clause head and the aspectual and temporal meanings.
Note also that the ClaUse head" i s -increase  which means that the meanings of begin and post got i~a~rpo%ated i~to othe?
_representations - -  the former gave rise to the value beg:i.a~f thei:'~pectga~"ir~a~gi~i'Of ;i "~ .the clause.
The latter was understood as a functional verb whose only perpoae:w~::t0~}: .allow the English realization of the concept of increasing in a nominalized f~h ig~:~~!~' : !~ i:~{post is treated as a collocant of growth (see e.g., Mel'~uk, 1981, "Nirenbu~g et~ g~! '
i~ \ [~~!~i~: ;  ;::Smadja and McKeown, 1989 for various treatments of colloe0.ti0ns): Proper~i~e~ ?O~d to  ' :describe events and objects (such as :i.ncrease) are predefined in an on\[ologi~al g!0m,a~ia .....model.
Phrases in double quotes are inserted as placeholders, ~ince for the ~!~rpos,~s 0fief::-}.
:this comparison their further analysis is immaterial.
?
:.?
::::!o.~ .5 Conc lus ionOur approach to lexical semantics combines the benefits of both syntax-driven and ontology-driven semantics.
We have been able to use both types of knowledge in several integratedapplications.
We plan to use this experience to formulate a distinct lexical-semantic the-ory based on this work.
As a preparatory step toward formulating such a theory we havesuggested the structure of ontological domain models, lexicons for particular languagesand a text meaning representation language (see Carlson and Nirenburg, 1990; Meyer ethi., 1990, Nirenburg and Defrise, 1991a, b).References\[1\] Brachman, R. 1983.
What IS-A and Isn't: An Analysis of Taxonomic Links inSemantic Networks.
Computer, 16(10), October, 30-36.Sin this representation we didn't ry to follow any particular knowledge r presentation formalism.J\[2\] Bresnan, J. and J. Kanerva.
1989.
Locative Inversion in Chichewa: A Case Studyof Factorization i Grammar.
Linguistic Inquiry, 1-50.\[3\] Carlson, L. and S. Nirenburg.
1990.
World Modeling for NLP.
CMU CMT TechnicalReport 90-121.\[4\] Dahlgren, K. and J. McDowell.
1989.
Knowledge Representation for CommonsenseReasoning with Text.
Computational Linguistics, 15, 149-170.\[5\] Fodor, J.
A.
1975.
The Language of Thought.
New York: Crowell.\[6\] Grimshaw, J.
1990.
Argument  Structure.
Cambridge, MA: MIT Press.\[7\] Hirst, G. 1987.
Semantic Interpretat ion and Resolut ion of Ambiguity.
Cam-bridge University Press.\[8\] Hobbs, J. and R. Moore (eds.)
1985.
Formal Theories of tile CommonsenseWorld.
Norwood, N J: Ablex.\[9\] Horty, J. F., Thomason, R. H., and Touretzky, D. S. (1990) A skeptical theory ofinheritance in nonmonotonic semantic nets.
Artificial Intelligence 42(2-3), 311-348.\[10\] Jackendoff, R. 1976.
Toward an Explanatory Semantic Representation.
LinguisticInquiry, 89-150.\[11\] Jackendoff, R. 1983.
Semantics and Cognition.
Cambridge, MA: MIT Press.\[12\] Jackendoff, R. 1990.
Semantic Structures.
Cambridge, MA: MIT Press.\[13\] Lenat, D. and R. Guha.
Building Large Knowledge-Based Systems.
Reading,MA: Addison-Wesley.\[14\] Lehnert, W. and M. Ringle (eds.)
1982.
Strategies for Natural  Language Pro-cessing.
Hillsdale, N J: Lawrence Er lbaum.\[15\] Levin, B.
1989.
English Verbal Diathesis.
Lexicon Pro ject  Working Papers 32.Center for Cognitive Science, MIT.\[16\] Levin, B. and M. Rappaport Hovav.
1990.
Wiping the Slate Clean: A Lexical Se-mantic Exploration.
ms. Northwestern University and Bar Ilan University.\[17\] Levin, B. and T. Rapoport.
1988.
Lexical Subordination.
Proceedings of the 24thAnnual Meeting of tile Chicago Linguistic Society.\[18\] Levin, L. 1986.
Operat ions on Lexical Forms: Unaccusative Rules in Ger-manic Languages.
Ph.D. Dissertation.
MIT.\[19\] Levin, L. 1987.
Toward a Linking Theory of Relation Changing Rules in LFG.
CSLIReport  No.
CSLI-87-115, November 1987, Center for the Study of Languageand Information, Stanford, California.\[20\] Mahmoud, A.T. 1989.
A Comparative Study of Middle and Inchoative Alternationsin Arabic and English.
Ph.D. Dissertation.
University of Pittsburgh.18\[21\]\[22\]\[23\]\[24\]\[25\]\[26\]\[27\]\[28\]\[29\]\[30\]\[31\]\[32\]\[33\]\[34\]Mel'6.uk, I.A.
1981.
Meaning-Text Models: A Recent Trend in SOviet Linguistics! '
iThe Annual Review of Anthropology.Meyer, I., B. Onyshkevych and L. Carlson.
1990.
~Lexicographic Principles amdDesign for Knowledge-Based Machine Translation.
CMU CMT Technical Report90-118.Mitamura, T. 1989.
The Hierarchical Organizat ion of Pred icate  F ramesf0rInterpret ive Mapping in Natura l  Language Processing.
Ph.D. Dissertation.University of Pittsburgh.Nirenburg, S., E. Nyberg, R. McCardell, S. Huffmann, E. Kenschaft, and I. Niren,burg.
1988.
DIOGENES-88.
Technical Report CMU-CMT-88-107.
Center for Ma-chine Translation, Carnegie Mellon University, Pittsburgh, PA. .
.
,  %iii:~i~.
i~ !~ ~iNirenburg, S. and C. Defrise.
1991a.
Application-Oriented Computational Sem~n~::~i~'i !'
!tics.
In: R. Johnson and M. Rosner (eds.)
Formal Semantics and Computa-  ~tional Linguistics.
Cambridge University Press (in press).Nirenburg, S. and C. Defrise.
1991b.
Aspects of Text Meaning.
In: J, PusteJ0ysky :(ed.).
Semantics and the Lexicon.
Dordrecht, Holland: Kluwer.Onyshkevych, B. and S. Nirenburg.
1991.
Lexicon, Ontology and Text Meaning\]"~; : i ,This volume.Schank, R. 1973.
Identification of Conceptualizations Underlying Natural Language.In: R. Schank and K. Colby (eds.
), Computer  Models of  Language andThought.
San Francisco: Freeman.
~: ..... ~ : " ~'i ~Smadja, F. and K. McKeown.
1990.
Automatically Extracting and RepresentingCollocations for Language Generation.
Proceedings of the 28th Annual Meeting ofthe ACL.Sown, J.
1984.Conceptual Structures:  In format ion PRocessing in Mind andMachine.
Reading, MA: Addison-Wesley.Talmy, L. 1975.
Semantics and Syntax of Motion.
In: J.P.Kimball, ed.
Syntax andSemantics,  4.
New York: Academic Press.
181-238.Talmy, L. 1985.
Lexicalization Patterns: Semantic Structure in Lexical Forms.
In:T. Shopen (ed.)
Language Typology and Syntact ic  Description, vol.
3.
Cam-bridge University Press.
57-149.Wilks, Y.
1975.
A Preferential, Pattern-Seeking Semantics for Natural LanguageInference.
Artificial Intelligence, 6, 144-147.Woods, W. 1975.
What's in a Link: Foundations of Semantic Networks.
In:D.Bobrow and A. Collins (eds.)
Representat ion and Understanding:  Stud-ies in Cognit ive Science.
New York: Academic Press.19
