Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 41?48,Sydney, July 2006. c?2006 Association for Computational LinguisticsTaxonomy Learning using Term Specificity and SimilarityPum-Mo RyuComputer Science Division, KAISTKORTERM/BOLAKoreapmryu@world.kaist.ac.krKey-Sun ChoiComputer Science Division, KAISTKORTERM/BOLAKoreakschoi@cs.kaist.ac.krAbstractLearning taxonomy for technical terms isdifficult and tedious task, especiallywhen new terms should be included.
Thegoal of this paper is to assign taxonomicrelations among technical terms.
We pro-pose new approach to the problem thatrelies on term specificity and similaritymeasures.
Term specificity and similarityare necessary conditions for taxonomylearning, because highly specific termstend to locate in deep levels and semanti-cally similar terms are close to each otherin taxonomy.
We analyzed various fea-tures used in previous researches in viewof term specificity and similarity, and ap-plied optimal features for term specificityand similarity to our method.1 IntroductionTaxonomy is a collection of controlled vocabu-lary terms organized into a hierarchical structure.Each term in a taxonomy is one or more parent-child relationships to other terms in the taxon-omy.
Taxonomies are useful artifacts for orga-nizing many aspects of knowledge.
As compo-nents of ontologies, taxonomies can provide anorganizational model for a domain (domain on-tology), or a model suitable for specific tasks(task ontologies) (Burgun & Bodenreider, 2001).However their wide usage is still hindered bytime-consuming, cost-ineffective building proc-esses.The main paradigms of taxonomy learning areon the one hand pattern based approaches and onthe other hand distributional hypothesis basedapproaches.
The former is approaches based onmatching lexico-syntactic patterns which conveytaxonomic relations in a corpus (Hearst, 1992;Iwanska et al, 2000), and the latter is statisticalapproaches based on the distribution of contextin corpus (Cimiano et al, 2005; Yamamoto et al,2005; Sanderson & Croft, 1999).
The former fea-tures a high precision and low recall compared tothe latter.
The quality of learned relations ishigher than those of statistical approaches, whilethe patterns are rarely applied in real corpus.
It isalso difficult to improve performance of patternbased approaches because they are simple andclear.
So, many researches have been focused onraising precision of statistical approaches.We introduce new distributional hypothesisbased taxonomy learning method using termspecificity and term similarity.
Term specificityis a measure of information quantity of terms ingiven domain.
When a term has much domaininformation, the term is highly specific to thedomain, and vice versa (Ryu & Choi, 2005).
Be-cause highly specific terms tend to locate in lowlevel in domain taxonomy, term specificity canbe used as a necessary condition for taxonomylearning.
Term similarity is degree of semanticoverlap among terms.
When two terms sharemany common characteristics, they are semanti-cally similar to each other.
Term similarity canbe another necessary condition for taxonomylearning, because semantically similar terms lo-cate near by in given domain taxonomy.
The twoconditions are generally valid for terms in a taxo-nomic relation, while terms satisfying the condi-tions do not always have taxonomic relation.
Sothey are necessary conditions for taxonomylearning.Based on these conditions, it is highly prob-able that term t1 is an ancestor of term t2 in do-main taxonomy TD, when t1 and t2 are semanti-cally similar enough and the specificity of t1 islower than that of t2 in D as in Figure 1.
However,t1 is not an ancestor of t3 even though the speci-41ficity of t1 is lower than that of t3 because t1 is notsimilar to t3 on the semantic level.t1t2 t3SimilaritySpecificityhighlowDepthhighlowFigure 1.
Term specificity and term similarity ina domain taxonomy TDThe strength of this method lies in its ability toadopt different optimal features for term specific-ity and term similarity.
Most of current re-searches relied on single feature such as adjec-tives of terms, verb-argument relation, or co-occurrence ratio in documents according to theirmethods.
Firstly, we analyze characteristics offeatures for taxonomy learning in view of termspecificity and term similarity to show that thefeatures embed characteristics of specificity andsimilarity, and finally apply optimal features toour method.Additionally we tested inside information ofterms to measure term specificity and similarity.As multiword terms cover the larger part of tech-nical terms, lexical components are featuringinformation representing semantics of terms(Cerbah, 2000).The remainder of this paper is organized fol-lows.
Characteristics of term specificity are de-scribed in Section 2, while term similarity and itsfeatures are addressed in Section 3.
Our taxon-omy learning method is discussed in Section 4.Experiment and evaluation are discussed in Sec-tion 5, and finally, conclusions are drawn in Sec-tion 6.2 Term SpecificitySpecificity is degree of detailed information ofan object about given target object.
For example,if an encyclopedia contains detailed informationabout ?IT domain?, then the encyclopedia is ?ITspecific encyclopedia?.
In this context, specificityis a function of objects and target object to realnumber.
Traditionally term specificity is widelyused in information retrieval systems to weightindex terms in documents (S. Jones, 1972; Ai-zawa, 2003; Wong & Yao, 1992).
In informationretrieval context, term specificity is function ofindex terms and documents.
On the other hand,term specificity is the function of terms and tar-get domains in taxonomy learning context (Ryu& Choi 2005).
Term specificity to a domain isquantified to a positive real number as shown inEq.
(1).
( | )Spec t D R+?
(1)where t is a term, and Spec(t|D) is the specificityof t in a given domain D. We simply use Spec(t)instead of Spec(t|D) assuming a particular do-main D in this paper.Understanding the relation between domainconcepts and their lexicalization methods isneeded, before we describe term specificitymeasuring methods.
Domain specific conceptscan be distinguished by a set of what we call?characteristics?.
More specific concepts are cre-ated by adding characteristics to the set of char-acteristics of existing concepts.
Let us considertwo concepts: C1 and C2.
C1 is an existing con-cept and C2 is a newly created concept by com-bining new characteristics to the characteristicset of C1.
In this case, C1 is an ancestor of C2(ISO, 2000).
When domain specific concepts arelexicalized as terms, the terms' word-formation isclassified into two categories based on the com-position of component words.
In the first cate-gory, new terms are created by adding modifiersto existing terms.
Figure 2 shows a subtree offinancial ontology.
For example ?current asset?was created by adding the modifier ?current?
toits hypernym ?asset?.
In this case, inside informa-tion is a good evidence to represent the charac-teristics.
In the second category, new terms arecreated independently of existing terms.
For ex-ample, ?cache?, ?inventory?, and ?receivable?share no common words with their hypernyms?current asset?
and ?asset?.
In this case, outsideinformation is used to differentiate the character-istics of the terms.assetcurrent asset fixed assetcache inventory receivable intangibleassetFigure 2.
Subtree of financial ontologyThere are many kinds of inside and outside in-formation to be used in measuring term specific-ity.
Distribution of adjective-term relation andverb-argument dependency relation are colloca-tion based statistics.
Distribution of adjective-term relation refers to the idea that specific nounsare rarely modified, while general nouns are fre-42quently modified in text.
This feature has beendiscussed to measure specificity of nouns in(Caraballo, 1999; Ryu & Choi, 2005) and tobuild taxonomy of Japanese nouns (Yamamoto etal., 2005).
Inversed specificity of a term can bemeasured by entropy of adjectives as shown Eq.
(2).1( ) ( | ) log ( | )adjadjSpec t P adj t P adj t?
= ??
(2)where P(adj|t), the probability that adj modifies t,is estimated as freq(adj,t)/freq(t).
The entropy isthe average information quantity of all (adj,t)pairs for term t. Specific terms have low entropy,because their adjective distributions are simple.For verb-argument distribution, we assumethat domain specific terms co-occur with selectedverbs which represent special characteristics ofterms while general terms are associated withmultiple verbs.
Under this assumption, we makeuse of syntactic dependencies between verbs ap-pearing in the corpus and their arguments such assubjects and objects.
For example, ?inventory?1,in Figure 2, shows a tendency to be objects ofspecific verbs like ?increase?
and ?reduce?.
Thisfeature was used in (Cimiano et al, 2005) tolearn concept hierarchy.
Inversed specificity of aterm can be measured by entropy of verb-argument relations as Eq.
(3).1( ) ( | ) log ( | )argargv arg argvSpec t P t v P t v?
= ??
(3)where P(t|varg), the probability that t is argumentof varg, is estimated as freq(t,varg)/freq(varg).
Theentropy is the average information quantity of all(t,varg) pairs for term t.Conditional probability of term co-occurrencein documents was used in (Sanderson & Croft,1999) to build term taxonomy.
This statistics isbased on the assumption that, for two terms, tiand tj, ti is said to subsume tj if the following twoconditions hold,P(ti|tj) = 1 and P(tj|ti)<1                                     (4)In other words, ti subsumes tj if the documentswhich tj occurs in are a subset of the documentswhich ti occurs in, therefore ti can be parent of tjin taxonomy.
Although a good number of termpairs are found that adhere to the two subsump-1 ?Inventory?
consists of a list of goods and materials heldavailable in stock (http://en.wikipedia.org/wiki/Inventory).tion conditions, it is noticed that many are justfailing to be included because a few occurrencesof the subsumed term, tj, does not co-occur withti.
Subsequently, the conditions are relaxed andsubsume function is defined as Eq.
(5).
In case ofP(ti|tj)>P(tj|ti), subsume(ti,tj) returns 1, otherwisereturns 0.1  if ( | ) ( | )( , )0  otherwisei j j ii jP t t P t tsubsume t t>?= ??
(5)We apply this function to calculate term speci-ficity as shown Eq.
(6) where a term is specificwhen it is subsumed by most of other terms.Specificity of t is determined by the ratio ofterms that subsume t over all co-occurring terms.1( , )( )jj ncoldocsubsume t tSpec tn?
?= ?
(6)where n is number of terms co-occurring termswith t.Finally, inside-word information is importantto compute specificity for multiword terms.
Con-sider a term t that consists of two words like t =w1w2.
Two words, w1 and w2, have their uniquecharacteristics and the characteristics aresummed up to the characteristic of t. Mutual in-formation is used to estimate the association be-tween a term and its component words.
LetT={t1,?,tN} be a set of terms found in a corpus,and W={w1,?,wM} be a set of component wordscomposing the terms in T. Assume a joint prob-ability distribution P(ti,wj), probability of wj is acomponent of ti, is given for ti and wj.
Mutualinformation between ti and wj compares the prob-ability of observing ti and wj together and theprobability of observing ti and wj independently.The mutual information represents the reductionof uncertainty about ti when wj is observed.
Thesummed mutual information between ti and W, asin Eq.
(7), is total reduction of uncertainty aboutti when all component words are observed.
( , )( ) log( ) ( )ji jin iw W i jP t wSpec tP t P w?= ?
(7)This equation indicates that wj which is highlyassociated to ti contributes specificity of ti.
Forexample, ?debenture bond?
is more specific con-cept than ?financial product?.
Intuitively, ?deben-ture?
is highly associated to ?debenture bond?43compared with ?bond?
to ?debenture bond?
or?financial?, ?product?
to ?financial product?.3 Term SimilarityWe evaluate four statistical and lexical features,related to taxonomy learning, in view of termsimilarity.
Three statistical features have beenused in existing taxonomy learning researches.
(Sanderson & Croft, 1999) used conditionalprobability of co-occurring terms in same docu-ment in taxonomy learning process as shown inEq.
(4).
This feature can be used to measuresimilarity of terms.
If two terms co-occur incommon documents, they are semantically simi-lar to each other.
Based on this assumption, wecan calculate term similarity by comparing thefrequency of co-occurring ti and tj together andthe frequency of occurring ti and tj independently,as Eq.
(8).2* ( , )( , )( ) ( )i jcoldoc i ji jdf t tSim t tdf t df t= +(8)where df(ti,tj) is number of documents in whichboth ti and tj co-occur, df(ti) is number of docu-ments in which ti occurs.
(Yamamoto et al, 2005) used adjective pat-terns to make characteristics vectors for terms inComplementary Similarity Measure (CSM).
Al-though CSM was initially designed to extractsuperordinate-subordinate relations, it is a simi-larity measure by itself.
They proposed two CSMmeasures; one is for binary images in which val-ues in feature vectors are 0 or 1, and the other isfor gray-scale images in which values in featurevectors are 0 through 1.
We adapt gray-scalemeasure in similarity calculation, because itshowed better performance in their research.
(Cimiano et al, 2005) applied Formal ConceptAnalysis (FCA) to extract taxonomies from atext corpus.
They modeled the context of a termas a vector representing syntactic dependencies.Similarity based on verb-argument dependenciesis calculated using cosine measure as Eq.
(9).2 2( | ) ( | )( , )( | ) ( | )argargarg argi arg j argv Vv i ji arg j argv V v VP t v P t vSim t tP t v P t v??
?= ??
?
(9)where P(t|varg), the probability that t is argumentof varg, is estimated as freq(t,varg)/freq(varg).Above three similarity measures are valid whenterms, ti and tj, appear in corpus one or moretimes.The last similarity measure is based on insideinformation of terms.
Because many domainterms are multiword terms, component words areclues for term similarity.
If two terms sharemany common words, they share common char-acteristics in given domain.
For example, fourwords ?asset?, ?current asset?, ?fixed asset?
and?intangible asset?
share characteristics related to?asset?
as in Figure 2.
This similarity measure isshown in Eq.
(10).2* ( , )( , )| | | |i jin i ji jcwc t tSim t tt t= +(10)where |t| is word count of t, and cwc(ti,tj) iscommon word count in ti and tj.
Simin(ti,tj) isvalid when cwc(ti,tj)>0.
Because cwc(ti,tj)=0 formost of term pairs, it is difficult to catch reliableresults for all possible term pairs.4 Taxonomy Learning ProcessWe model taxonomy learning process as a se-quential insertion of new terms to current taxon-omy.
New taxonomy starts with empty state, andchanges to rich taxonomic structure with the re-peated insertion of terms as depicted in Figure 3.Terms to be inserted are sorted by term specific-ity values.
Term insertion based on the increas-ing order of term specificity is natural, becausethe taxonomy grows from top to down with terminsertion process in increasing specificity se-quence.
?SpecificityHigh LowSpecificityHighLowTerm sequenceTaxonomytnewtnewFigure 3.
Terms are inserted to taxonomy in thesequence of specificityAccording to above assumption, our systemselects possible hypernyms of a new term, tnew incurrent taxonomy as following steps:?
Step 1: Select n-most similar terms to tnewfrom current taxonomy?
Step 2: Select candidate hypernyms of tnewfrom n-most similar terms.
Specificity ofcandidate hypernyms is less than that of tnew.44?
Step 3: Insert tnew as hyponyms of candidatehypernymsFor example, suppose t2, t4, t5 and t6, are fourmost similar terms to tnew in Figure 4.
Two termst2 and t4 are selected as candidate hypernyms oftnew, because specificity of the terms is less thanspecificity of tnew.t1t2 t3t4 t5 t6t7 t8 t9tnewt10Spec(t1) = 1.0Spec(t3) = 1.5Spec(t2) = 1.5Spec(t4) = 2.0 Spec(t5) = 3.0Spec(t7) = 4.0 Spec(t8) = 3.5Spec(t6) = 2.4Spec(t9) = 2.5Spec(tnew) = 2.3Spec(t10) = 3.0SpecificityHighLowFigure 4.
Selection of candidate hypernyms oftnew from taxonomy using term specificity andsimilarity5 Experiment and EvaluationWe applied our taxonomy learning method to setof terms in existing taxonomy.
We removed allrelations from the taxonomy, and made newtaxonomic relations among the terms.
Thelearned taxonomy was then compared to originaltaxonomy.
Our experiment is composed of foursteps.
Firstly, we calculated term specificity us-ing specificity measures discussed in chapter 2,secondly, we calculated term similarity usingsimilarity measures described in chapter 3,thirdly, we applied the best specificity and simi-larity features to our taxonomy building process,and finally, we evaluated our method and com-pared with other taxonomy learning methods.Finance ontology 2  which was developedwithin the GETESS project (Staab et al, 1999)was used in our experiment.
We slightly modi-fied original ontology.
We unified different ex-pressions of same concept to identical expression.For example, 'cd-rom drive' and 'cdrom drive' areunified as 'cd-rom drive' because the former ismore usual expression than the latter.
We alsoremoved terms that are not descendents of 'root'node to make the taxonomy have single rootnode.
The taxonomy consists of total 1,819nodes and 1,130 distinct nodes.
Maximum andaverage depths are 15 and 5.5 respectively, and2 The ontology can be downloaded at http://www.aifb.uni-karlsruhe.de/WBS/pci/FinanceGoldStandard.isa.
P. Cimianoand his colleagues added English labels for the originallyGerman labeled nodes (Cimiano et al, 2005)maximum and average children nodes are 32 and3.5 respectively.We considered Reuters215783 corpus, over 3.1million words in title and body fields.
We parsedthe corpus using Connexor functional depend-ency parser4 and extracted various statistics: termfrequency, distribution of adjectives, distributionof co-occurring frequency in documents, andverb-argument distribution.5.1 Term SpecificityTerm specificity was evaluated based on threecriteria: recall, precision and F-measure.
Recallis the fraction of the terms that have specificityvalues by the given measuring method.
Precisionis the fraction of relations with correct specificityvalues.
F-measure is a harmonic mean of preci-sion and recall into a single measure of overallperformance.
Precision (Pspec), recall (Rspec), F-measure (Fspec) is defined as follows:###   ( , )#   ( , )specvalidspecvalidof terms with specificityRof all termsof R p c with correct specificityPof R p c==(11)where Rvalid(p,c) is a valid parent-child relation inoriginal taxonomy, and a relation is valid whenthe specificity of two terms are measured by thegiven method.
If the specificity of child term, c,is larger than that of parent term, p, then the rela-tion is correct.We tested four specificity measuring methodsdiscussed in section 2 and the result is shown inTable 1.
Specadj showed the highest precision aswe anticipated.
Because domain specific termshave sufficient information in themselves; theyare rarely modified by other words in real text.However, Specadj showed the lowest recall fordata sparseness problem.
As mentioned above, itis hard to collect sufficient adjectives for domainspecific terms from text.
Specvarg showed thelowest precision.
This result indicates that distri-bution of verb-argument relation is less corre-lated to term specificity.
Specin showed the high-est recall because it measures term specificityusing component words contrary to other meth-ods.
Speccoldoc showed comparable precision andrecall.3http://www.daviddlewis.com/resources/testcollections/reuters21578/4 http://www.connexor.com/45We harmonized Specin and Specadj to Specin/adjas described in (Ryu & Choi, 2005) to take ad-vantages of both inside and outside information.Harmonic mean of two specificity values wasused in Specin/adj method.
Specin/adj showed thehighest F-measure because precision was higherthan that of Specin and recall was equal to that ofSpecin.Table 1.
Precision, recall and F-measure for termspecificityMethod Precision Recall F-measureSpecadj 0.795 0.609  0.689Specvarg 0.663 0.702  0.682Speccoldoc 0.717 0.702  0.709Specin 0.728 0.907  0.808Specin/adj 0.731 0.907  0.8105.2 Term SimilarityWe evaluated similarity measures by comparingwith taxonomy based similarity measure.
(Bu-danitsky & Hirst, 2006) calculated correlationcoefficients (CC) between human similarity rat-ings and the five WordNet based similaritymeasures.
Among the five computational meas-ures, (Leacock & Chodorow, 1998)?s methodshowed the highest correlation coefficients, eventhough all of the measures showed similar rang-ing from 0.74 to 0.85.
This result means that tax-onomy based similarity is highly correlated tohuman similarity ratings.
We can indirectlyevaluate our similarity measures by comparing totaxonomy based similarity measure, instead ofdirect comparison to human rating.
If appliedsimilarity measure is qualified, the calculatedsimilarity will be highly correlated to taxonomybased similarity.
Leacock and Chodorow pro-posed following formula for computing thescaled semantic similarity between terms t1 and t2in taxonomy.1 21 2( , )( , ) log2 max ( )LCt Taxonomylen t tSim t tdepth t?= ?
?
(12)where the denominator includes the maximumdepth of given taxonomy, and len(t1, t2) is num-ber of edges in the shortest path between word t1and t2 in the taxonomy.Besides CC with ontology based similaritymeasures, recall of a similarity measures is alsoimportant evaluation factor.
We defined recall ofsimilarity measure, RSim, as the fraction of theterm pairs that have similarity values by thegiven measuring method as Eq.
(13).##     Simsimilarity measured term pairsRall possible term pairs=           (13)We also defined F-measure for a similaritymeasure, Fsim, as harmonic means of CC and Rsim.Because CC is a kind of precision, Fsim is overallmeasure of precision and recall.We calculated term similarity between all pos-sible term pairs in finance ontology using themeasures described in section 3.
Additionally weintroduced new similarity measure Simin/vargwhich is combined similarity of Simvarg and Simin.Simvarg and Simin between two terms are harmo-nized to Simin/varg.
We also calculated SimLCbased on finance ontology, and calculated CCbetween SimLC and results of other measures.Figure 5 shows variation of CC and recall asthreshold of similarity changes from 0.0 to 1.0for five similarity measures.
Threshold is directlyproportional to CC and inversely proportional torecall in ideal case.
We normalized all similarityvalues to [0.0, 1.0] in each measure.
CC grows asthreshold increases in Simcoldoc and Simvarg as weexpected.
CC of CSM measure, Simcsm, increasedas threshold increased and decreased whenthreshold is over 0.6.
For example two terms ?as-set?
and ?current asset?
are very similar to eachother based on SimLC measure, because edgecount between two terms is one in finance ontol-ogy.
The former can be modified many adjec-tives such as ?intangible?, ?tangible?, ?new?
and?estimated?, while the latter is rarely modified byother adjectives in corpus because it was alreadyextended from ?asset?
by adding adjective ?cur-rent?.
Therefore, semantically similar terms donot always have similar adjective distributions.CC between Simin and SimLC showed high curvein low threshold, but downed as threshold in-creased.
Similarity value above 0.6 is insignifi-cant, because it is hard to be over 0.6 using Eq.(10).
For example, similarity between ?executiveboard meeting?
and ?board meeting?
is 0.8, themaximum similarity in our test set.
The averageof inside-word similarity is 0.41.Simvarg showed higher recall than other meas-ures.
This means that verb-argument relation ismore abundant than other features in corpus.SimIn showed the lowest recall because we couldget valid similarity using Eq.
(10).
Simvargshowed higher F-measure when threshold is over0.2.
This result illustrate that verb-argument rela-tion is adequate feature to similarity calculation.46The combined similarity measure, Simin/varg,complement shortcomings of SimIn and Simvarg.SimIn showed high CC but low recall.
ContrarilySimvarg showed low CC but high recall.
Simin/vargshowed the highest F-measure.5.3 Taxonomy learningIn order to evaluate our approach we need to as-sess how good the automatically learned tax-onomies reflect a given domain.
The goodness isevaluated by the similarity of automaticallylearned taxonomy to reference taxonomy.
Weused (Cimiano et al, 2005)?s ontology evaluationmethod in which lexical recall (LRTax), precision(PTax) and F-measure (FTax) of learned taxonomyare defined based on the notion of taxonomyoverlap.
LRTax is defined as the ratio of numberof common terms in learned taxonomy and refer-ence taxonomy over number of terms in refer-ence taxonomy.
PTax is defined as ratio of taxon-omy overlap of learned taxonomy to referencetaxonomy.
FTax is harmonic mean of LRTax andPTax.0.00.20.40.60.81.00.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0ThresholdCC0.000.040.080.120.160.200.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0ThresholdFmeasureSim(coldoc) Sim(CSM) Sim(varg)Sim(In) Sim(In/Varg)0.000.040.080.120.160.200.240.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0ThresholdRecallFigure 5 Correlation coefficient between SimLCand other similarity measures.
Recall and F-measure of similarity measuresWe generated four taxonomies, Tcoldoc, Tcsm,Tfca, Tspec/sim, using four taxonomy learning meth-ods: term co-occurring method, CSM method,FCA method and our method.
We applied Spe-cin/adj in specificity measuring and Simin/varg insimilarity calculation because they showed thehighest F-measure.
In our method, the mostprobable one term was selected as hypernym ofnewly inserted term in each learning step.Figure 6 shows variations of lexical recall,precision and F-measure of four methods asthreshold changes.
Threshold in each methodrepresent different information to each other.Threshold in Tcsm is variation of CSM values.Threshold in Tcoldoc is variation of probability oftwo terms co-occur in a document.
Threshold inTfca is normalized frequency of contexts.
Thresh-old in Tspec/sim, is variation of similarity.Tspec/sim showed the highest lexical recall.Lexical recall is tightly related to recall in simi-larity measures.
Simin/varg showed the highest re-call in similarity measures.
Tfca and Tcsm showedhigher precision than other taxonomies.
It is as-sumed that  precision  of  taxonomy  depends  on00.20.40.60.810 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9ThresholdLexical Recall00.20.40.60.810 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9ThresholdF-MeasureCSM COLDOC SPEC/SIM FCA00.20.40.60.810 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9ThresholdPrecisionFigure 6.
Lexical recall, precision and F-measureof taxonomy learning methods47the precision of specificity measures and the CCof similarity measures.
In actual case, Simvargshowed the most plausible curve in CC and Spe-cadj showed the highest precision in specificity.Verb-argument relation and adjective-term rela-tion are used in FCA and CSM methods respec-tively.
Tspec/sim and Tcoldoc showed higher F-measure curve than other two taxonomies due tohigh lexical recall.
Although our method showedplausible F-measure, it showed the lowest preci-sion.
So other combination of similarity andspecificity measures are needed to improve pre-cision of learned taxonomy.6 ConclusionWe have presented new taxonomy learningmethod with term similarity and specificity takenfrom domain-specific corpus.
It can be applied todifferent domains as it is; and, if we have a syn-tactic parser available, to different languages.
Weanalyzed the features used in previous researchesin view of term specificity and similarity.
In thisanalysis, we found that the features embed thecharacteristics of both conditions.Compared to previous approaches, our methodhas advantages in that we can use different fea-tures for term specificity and similarity.
It makeseasy to analyze errors in taxonomy learning step,whether the wrong relations are caused by speci-ficity errors or by similarity errors.
The maindrawback of our method, as it is now, is that theeffect of wrong located terms in upper levelpropagates to lower levels.Until now, researches on automatic ontologylearning especially taxonomic relation showedvery low precision.
Human experts?
interventionis inevitable in automatic learning process tomake applicable taxonomy.
Future work is tomake new model where human experts and sys-tem work interactively in ontology learningprocess in order to balance cost and precision.ReferenceS.
Caraballo, E. Charniak.
1999.
Determining theSpecificity of Nouns from Text.
Proceedings of the1999 Joint SIGDAT Conference on EmpiricalMethods in Natural Language Processing and VeryLarge Corpora, pp.
63-70P.
Cimiano, A. Hotho, S.Staab.
2005.
Learning Con-cept Hierarchies from Text Corpora using FormalConcept Analysis.
Journal of AI Research, Vol.
24,pp.
305-339M.
Hearst.
1992.
Automatic Acquisition of Hypo-nyms from Large Text Corpora.
Proceedings of the14th International Conference on ComputationalLinguisticsL.
Iwanska, N. Mata and K. Kruger.
2000.
Fullyautomatic acquisition of taxonomic knowledgefrom large corpora of texts.
In Iwanska, L. &Shapiro, S.
(Eds.
), Natural Language Processingand Knowledge Processing, pp.
335-345,MIT/AAAI Press.E.
Yamamoto, K. Kanzaki and H. Isahara.
2005.
Ex-traction of Hierarchies Based on Inclusion of Co-occurring Words with Frequency Information.Proceedings of 9th International Joint Conferenceon Artificial Intelligence, pp.
1160-1167A.
Burgun, O. Bodenreider.
2001.
Aspects of theTaxonomic Relation in the Biomedical Domain,Proceedings of International Conference on For-mal Ontology in Information Systems, pp.
222-233Mark Sanderson and Bruce Croft.
1999.
Derivingconcept hierarchies from text.
Proceedings of the22th Annual ACM S1GIR Conference on Researchand Development in Information Retrieval, pp.206-213, 1999Karen Sparck Jones.
1972.
Exhausitivity and Speci-ficity Journal of Documentation Vol.
28, Num.
1,pp.
11-21S.K.M.
Wong, Y.Y.
Yao.
1992.
An Information-Theoretic Measure of Term Specificity, Journal ofthe American Society for Information Science, Vol.43, Num.
1. pp.54-61ISO 704.
2000.
Terminology work-Principle andmethods.
ISO 704 Second EditionA.
Aizawa.
2003.
An information-theoretic perspec-tive of tf-idf measures.
Journal of InformationProcessing and Management, vol.
39Alexander Budanitsky, Graeme Hirst.
2006 Evaluat-ing WordNet-based Measures of Lexical SemanticRelatedness.
Computational Linguistics.
Vol.
32NO.
1, pp.
13-47(35)Claudia Leacock, Martin Chodorow.
1998.
Combin-ing local context and WordNet similarity for wordsense identification.
In Christian Fellbaum, editor,WordNet: An Electronic Lexical Database.
TheMIT Press, pp.
265-283Pum-Mo Ryu, Key-Sun Choi.
2005.
An Information-Theoretic Approach to Taxonomy Extraction forOntology Learning, In P. Buitelaar et al (eds.
), On-tology Learning from Text: Methods, Evaluationand Applications, Vol.
123, Frontiers in ArtificialIntelligence and Applications, IOS PressFarid Cerbah.
2000.
Exogeneous and EndogeneousApproaches to Semantic Categorization of Un-known Technical Terms.
Proceedings of the 18thInternational Conference on Computational Lin-guistics, vol.
1, pp.
145-15148
