Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 206?214,Beijing, August 2010Simplicity is Better: Revisiting Single Kernel PPI ExtractionSung-Pil ChoiInformation Technology LaboratoryKorea Institute of Science and Technol-ogy Informationspchoi@kisti.re.krSung-Hyon MyaengDepartment of Computer ScienceKorea Advanced Institute of Science andTechnologymyaeng@kaist.ac.krAbstractIt has been known that a combination ofmultiple kernels and addition of variousresources are the best options for im-proving effectiveness of kernel-basedPPI extraction methods.
These supple-ments, however, involve extensive ker-nel adaptation and feature selectionprocesses, which attenuate the originalbenefits of the kernel methods.
This pa-per shows that we are able to achievethe best performance among the state-of-the-art methods by using only a sin-gle kernel, convolution parse tree kernel.In-depth analyses of the kernel revealthat the keys to the improvement are thetree pruning method and considerationof tree kernel decay factors.
It is note-worthy that we obtained the perfor-mance without having to use any addi-tional features, kernels or corpora.1 IntroductionProtein-Protein Interaction (PPI) Extractionrefers to an automatic extraction of the interac-tions between multiple protein names from nat-ural language sentences using linguistic featuressuch as lexical clues and syntactic structures.
Asentence may contain multiple protein namesand relations, i.e., multiple PPIs.
For example,the sentence in Fig.1 contains a total of six pro-tein names of varying word lengths and threeexplicit interactions (relations).
The interactiontype between phosphoprotein and the acronymP in the parentheses is ?EQUAL.?
A longer pro-tein name phosphoprotein of vesicular stomati-tis virus is related to nucleocapsid protein via?INTERACT?
relation.
Like the first PPI, nuc-leocapsid protein is equivalent to the abbre-viated term N.It is not straightforward to extract PPIs froma sentence or textual segment.
There may bemultiple protein names and their relationships,which are intertwined in a sentence.
An interac-tion type may be expressed in a number of dif-ferent ways.Figure 1.
An example sentence containing mul-tiple PPIs involving different names of varyingscopes and relations1A significant amount of efforts have beendevoted to kernel-based approaches to PPI ex-tractions (PPIE) as well as relation extractions2(Zhang et al, 2006; Pyysalo et al, 2008; Guo-Dong et al, 2007; Zhang et al, 2008; Airola etal., 2008; Miwa et al, 2009).
They includeword feature kernels, parse tree kernels, andgraph kernels.
One of the benefits of using akernel method is that it can keep the original1 BioInfer, Sentence ID:BioInfer.d10.s02 Relation extraction has been studied massively with thehelp of the ACE (www.nist.gov/tac) competition work-shop and its corpora.
The ACE corpora contain valuableinformation showing the traits of target entities (e.g., ent-ity types, roles) for relation extraction in single sentences.Since all target entities are of the same type, proteinname, in PPIE, however, we cannot use relational infor-mation that exists among entity types.
This makes PPIEmore challenging.206formation of target objects such as parse trees,not requiring extensive feature engineering forlearning algorithms (Zelenko et al, 2003).In an effort to improve the performance ofPPIE, researchers have developed not only newkernels but also methods for combining them(GuoDong et al, 2007; Zhang et al, 2008; Air-ola et al, 2008; Miwa et al, 2009a; Miwa et al,2009b).
While the intricate ways of combingvarious kernels and using extra resources haveplayed the role of establishing strong baselineperformance for PPIE, however, they areviewed as another form of engineering efforts.After all, one of the reasons the kernel methodshave become popular is to avoid such engineer-ing efforts.Instead, we focus on a state-of-the-art kerneland investigate how it can be best utilized forenhanced performance.
We show that even witha single kernel, convolution parse tree kernel inthis case, we can achieve superior performancein PPIE by devising an appropriate preprocess-ing and factor adjustment method.
The keys tothe improvement are tree pruning and consider-ation of a tree kernel decay factor, which areindependent of the machine learning modelused in this paper.
The main contribution of ourwork is the extension and application of theparticular convolution tree kernel method forPPIE, which gives a lesson that a deep analysisand a subsequent extension of a kernel for max-imal performance can override the gains ob-tained from engineering additional features orcombining other kernels.The remaining part of the paper is organizedas follows.
In section 2, we survey the existingapproaches.
Section 3 introduces the parse treekernel model and its algorithm.
Section 4 ex-plains the performance improving factors ap-plied to the parse tree kernel.
The architectureof our system is introduced in section 5.
Section6 shows the improvements in effectiveness inmultiple PPI corpora and finally we concludeour work in section 7.2 Related WorkIn recent years, numerous studies have at-tempted to extract PPI automatically from text.Zhou and He (2008) classified various PPIEapproaches into three categories: linguistic,rule-based and machine learning and statisticalmethods.Linguistic approaches involve constructingspecial grammars capable of syntactically ex-pressing the interactions in sentences and thenapplying them to the language analyzers such aspart-of-speech taggers, chunkers and parsers toextract PPIs.
Based on the level of linguisticanalyses, we can divide the linguistic approach-es into two categories: shallow parsing (Seki-mizu et al, 1998; Gondy et al, 2003) and fullparsing methods (Temkin & Gilder, 2003; Ni-kolai et al, 2004).Rule-based approaches use manually definedsets of lexical patterns and find text segmentsthat match the patterns.
Blaschke et al (1996)built a set of lexical rules based on clue wordsdenoting interactions.
Ono et al (2001) defineda group of lexical and syntactic interaction pat-terns, embracing negative expressions, and ap-plied them to extract PPIs from documentsabout ?Saccharomyces cerevisiae?
and ?Esche-richia coli?.
Recently, Fundel et al (2007) pro-posed a PPI extraction model based on moresystematic rules using a dependency parser.Machine learning and statistical approacheshave been around for a while but have recentlybecome a dominant approach for PPI extraction.These methods involve building supervised orsemi-supervised models based on training setsand various feature extraction methods (An-drade & Valencia, 1998; Marcotte et al, 2001;Craven & Kumlien, 1999).
Among them, ker-nel-based methods have been studied extensive-ly in recent years.
Airola et al (2008) attemptedto extract PPIs using a graph kernel by convert-ing dependency parse trees into the correspond-ing dependency graphs.Miwa et al (2009a) utilized multiple kernelssuch as word feature kernels, parse tree kernels,and even graph kernels in order to improve theperformance of PPI extraction.
Their experi-ments based on five PPI corpora, however,showed that combining multiple kernels gaveonly minor improvements compared to othermethods.
To further improve the performanceof the multiple kernel system, the same groupcombined multiple corpora to exploit additionalfeatures for a modified SVM model (Miwa etal., 2009b).
While they achieved the best per-formance in PPI extraction, it was possible only207with additional kernels and corpora from whichadditional features were extracted.Unlike the aforementioned approaches tryingto use all possible resources for performanceenhancement, this paper aims at maximizing theperformance of PPIE using only a single kernelwithout any additional resources.
Without lo-wering the performance, we attempt to stick tothe initial benefits of the kernel methods: sim-plicity and modularity (Shawe-Taylor & Cris-tianini, 2004).3 Convolution Parse Tree KernelModel for PPIEThe main idea of a convolution parse tree ker-nel is to sever a parse tree into its sub-trees andtransfer it as a point in a vector space in whicheach axis denotes a particular sub-tree in theentire set of parse trees.
If this set contains Munique sub-trees, the vector space becomes M-dimensional.
The similarity between two parsetrees can be obtained by computing the innerproduct of the two corresponding vectors,which is the output of the parse tree kernel.There are two types of parse tree kernels ofdifferent forms of sub-trees: one is SubTreeKernel (STK) proposed by Vishwanathan andSmola (2003), and the other is SubSet TreeKernel (SSTK) developed by Collins and Duffy(2001).
In STK, each sub-tree should be a com-plete tree rooted by a specific node in the entiretree and ended with leaf nodes.
All the sub-treesmust obey the production rules of the syntacticgrammar.
Meanwhile, SSTK can have anyforms of sub-trees in the entire parse tree giventhat they should obey the production rules.
Itwas shown that SSTK is much superior to STKin many tasks (Moschitti, 2006).
He also intro-duced a fast algorithm for computing a parsetree kernel and showed its beneficial effects onthe semantic role labeling problem.A parse tree kernel can be computed by thefollowing equation:(1)where Ti is ith parse tree and n1 and n2 are nodesin NT, the set of the entire nodes of T. ?represents a tree kernel decay factor, which willbe explained later, and ?
decides the way thetree is severed.
Finally ?
(n1, n2, ?, ?)
counts thenumber of the common sub-trees of the twoparse trees rooted by n1 and n2.
Figure 2 showsthe algorithm.In this algorithm, the get_children_numberfunction returns the number of the direct childnodes of the current node in a tree.
The functionnamed get_node_value gives the value of anode such as part-of-speeches, phrase tags andwords.
The get_production_rule function findsthe grammatical rule of the current node and itschildren by inspecting their relationship.1234567891011121314151617181920212223242526272829FUNCTION delta(TreeNode n1, TreeNode n2, ?, ?
)n1 = one node of T1;  n2 = one node of T2;?
= tree kernel decay factor;  ?
= tree division me-thod;BEGINnc1 = get_children_number(n1);nc2 = get_children_number(n2);IF nc1 EQUAL 0 AND nc2 EQUAL 0 THENnv1 = get_node_value(n1);nv2 = get_node_value(n2);IF nv1 EQUAL nv2 THEN RETURN 1;ENDIFnp1 = get_production_rule(n1);np2 = get_production_rule(n2);IF np1 NOT EQUAL np2 THEN RETURN 0;IF np1 EQUAL np2 AND nc1 EQUAL 1AND nc2 EQUAL 1 THENRETURN ?
;END IFmult_delta = 1;FOR I = 1 TO nc1nch1 = Ith child of n1;   nch2 = Ith child of n2;mult_delta = mult_delta ?(?
+ delta(nch1, nch2, ?, ?
));END FORRETURN ?
?
mult_delta;ENDFigure 2. ?
(n1, n2, ?, ?)
algorithm4 Performance Improving Factors4.1 Tree Pruning MethodsTree pruning for relation extraction was firstlyintroduced by Zhang et al (2006) and also re-ferred to as ?tree shrinking task?
for removingless related contexts.
They suggested five typesof the pruning methods and later invented twomore in Zhang et al (2008).
Among them, thepath-enclosed tree (PT) method was shown togive the best result in the relation extractiontask based on ACE corpus.
We opted for thispruning method in our work.208Figure 3 shows how the PT method prunes atree.
To focus on the pivotal context, it pre-serves only the syntactic structure encompass-ing the two proteins at hand and the words inbetween them (the part enclosed by the dottedlines).
Without pruning, all the words like addi-tion, increased and activity would intricatelyparticipate in deciding the interaction type ofthis sentence.Figure 3.
Path-enclosed Tree (PT) MethodAnother important effect of the tree pruningis its ability to separate features when two ormore interactions exist in a sentence.
As in Fig-ure 1, each interaction involves its unique con-text even though a sentence has multiple inte-ractions.
With tree pruning, it is likely to extractcontext-sensitive features by ignoring externalfeatures.4.2 Tree Kernel Decay FactorCollins and Duffy (2001) addressed two prob-lems of the parse tree kernel.
The first one isthat its kernel value tends to be largely domi-nated by the size of two input trees.
If they arelarge in size, it is highly probable for the kernelto accumulate a large number of overlappingcounts in computing their similarity.
Secondly,the kernel value of two identical parse trees canbecome overly large while the value of two dif-ferent parse trees is much tiny in general.
Thesetwo aspects can cause a trouble during a train-ing phase because pairs of large parse trees thatare similar to each other are disproportionatelydominant.
Consequently, the resulting modelscould act like nearest neighbor models (Collinsand Duffy, 2001).To alleviate the problems, Collins and Duffy(2001) introduced a scalability parameter calleddecay factor, 0 < ?
?
1 which scales the relativeimportance of tree fragments with their sizes asin line 33 of Fig.
2.
Based on the algorithm, adecay factor decreases the degree of contribu-tion of a large sub-tree exponentially in kernelcomputation.
Figure 4 illustrates both the way atree kernel is computed and the effect of a de-cay factor.
In the figure, T1 and T2 share fourcommon sub-trees (S1, S2, S3, S5).
Let us assumethat there are only two trees in a training set andonly five unique sub-trees exist.
Then each treecan be expressed by a vector whose elementsare the number of particular sub-trees.
Kernelvalue is obtained by computing the inner prod-uct of the two vectors.
As shown in the figure,S1 is a large sub-sub-trees, S1, S2 S3, and S4, twoof which (S2, and S3) are duplicated in the innerproduct computation.
It is highly probable forlarge sub-trees to contain many smaller sub-trees, which lead to an over-estimated similarityvalue between two parse trees.
As mentionedabove, therefore, it is necessary to rein thoselarge sub-trees with respect to their sizes incomputing kernel values by using decay factors.In this paper, we treat the decay factor as one ofthe important optimization parameters for a PPIextraction task.Figure 4.
The effect of decaying in comparing two trees.
n(?)
denotes #unique subtrees in a tree.2095 Experimental ResultsIn order to show the superiority of the simplekernel based method using the two factors usedin this paper, compared to the resent results forPPIE using additional resources, we ran a seriesof experiments using the same PPI corporacited in the literature.
In addition, we show thatthe method is robust especially for cross-corpusexperiments where a classifier is trained andtested with entirely different corpora.5.1 Evaluation CorporaTo evaluate our approach for PPIE, we used?Five PPI Corpora3?
organized by Pyysalo et al(2008).
It contains five different PPI corpora:AImed, BioInfer, HPRD50, IEPA and LLL.They have been combined in a unified XMLformat and ?binarized?
in case of involvingmultiple interaction types.Table 1.
Five PPI CorporaTable 1 shows the size of each corpus in?Five PPI Corpora.?
As mentioned before, asentence can have multiple interactions, whichresults in the gaps between the number of sen-tences and the sum of the number of instances.Negative instances have been automaticallygenerated by enumerating sentences with mul-tiple proteins but not having interactions be-tween them (Pyysalo et al, 2008).5.2 Evaluation SettingsIn order to parse each sentence, we used Char-niak Parser4.
For kernel-based learning, we ex-panded the original libsvm 2.895 (Chang & Lin,2001) so that it has two additional kernels in-cluding parse tree kernel and composite kernel6along with four built-in kernels7Our experiment uses both macro-averagedand micro-averaged F-scores.
Macro-averaging3 http://mars.cs.utu.fi/PPICorpora/eval-standard.html4 http://www.cs.brown.edu/people/ec/#software5 http://www.csie.ntu.edu.tw/~cjlin/libsvm/6 A kernel combining built-in kernels and parse tree kernel7 Linear, polynomial, radial basis function, sigmoid ker-nelscomputes F-scores for all the classes indivi-dually and takes average of the scores.
On theother hand, micro-averaging enumerates bothpositive results and negative results on thewhole without considering the score of eachclass and computes total F-score.In 10-fold cross validation, we apply thesame split used in Airola et al, (2008), Miwa etal., (2009a) and Miwa et al, (2009b) for com-parisons.
Also, we empirically estimate the re-gularization parameters of SVM (C-values) byconducting 10-fold cross validation on eachtraining data.
We do not adjust the SVM thre-sholds to the optimal value as in Airola et al,(2008) and Miwa et al, (2009a).5.3 PPI Extraction PerformanceTable 2 shows the best scores of our system.The optimal decay factor varies with each cor-pus.
In LLL, the optimal decay factor is 0.28indicating that the shortage of data has forcedour system to normalize parse trees more inten-sively with a strong decay factor in kernel com-putation in order to cover various syntacticstructures.DF AC ma-P ma-R ma-F ?ma-FA 0.6 83.672.8(55.0)62.1(68.8)67.0(60.8)4.5(6.6)B 0.5 79.874.5(65.7)70.9(71.1)72.6(68.1)2.7(3.2)H 0.7 74.575.3(68.5)71.0(76.1)73.1(70.9)10.2(10.3)I 0.6 74.274.1(67.5)72.2(78.6)73.1(71.7)6.0(7.8)L 0.2 82.283.2(77.6)81.2(86.0)82.1(80.1)10.4(14.1)Table 2.
The highest results of the proposedsystem w.r.t.
decay factors.
DF: Decay Factor,AC: accuracy, ma-F: macro-averaged F1, ?ma-F:standard deviation of F-scores in CV.
A:AIMed,B:BioInfer, H:HPRD50, I:IEPA, L:LLL.
Thenumbers in parentheses refer to the scores ofMiwa et al, (2009a).Our system outperforms the previous resultsas in Table 2.
Even using rich feature vectorsincluding Bag-Of-Words and shortest path trees8 It was determined by increasing it by 0.1 progressivelythrough 10-fold cross validation.AIMed BioInfer HPRD50 IEPA LLL#Sentence 1,955 1,100 145 486 77#Positive  1,000 2,534 163 335 164#Negative  4,834 7,132 270 482 166210generated from multiple corpora, Miwa et al,(2009b) reported 64.0% and 66.7% in AIMedand BioInfer, respectively.
Our system, howev-er, produced 67.0% in AIMed and 72.6% inBioInfer with a single parse tree kernel.
We didnot have to perform any intensive feature gen-eration tasks using various linguistic analyzersand more importantly, did not use any addition-al corpora for training as done in Miwa et al,(2009b).
While the performance differences arenot very big, we argue that obtaining higherperformance values is significant because theproposed system did not use any of the addi-tional efforts and resources.To investigate the effect of the scaling para-meter of the parse tree kernel in PPI extraction,we measure how the performance changes asthe decay factor varies (Figure 5).
It is obviousthat the decay factor influences the overall per-formance of PPI extraction.
Especially, the F-scores of the small-scale corpora such asHPRD50 and LLL are influenced by the decayfactor.
The gaps between the best and worstscores in LLL and HPRD50 are 19.1% and5.2%, respectively.
The fluctuation in F-scoresof the large-scale corpora (AIMed, BioInfer,IEPA) is not so extreme, which seems to stemfrom the abundance in syntactic and lexicalforms that reduce the normalizing effect of thedecay factor.
The increase in the decay factorleads to the increase in the precision values ofall the corpora except for LLL.
The phenome-non is fairly plausible because the decreasednormalization power causes the system to com-pute the tree similarities more intensively andtherefore it classifies each instance in a strictand detailed manner.
On the contrary, the recallvalues slightly decrease with respect to the de-cay factor, which indicates that the tree pruning(PT) has already conducted the normalizationprocess to reduce the sparseness problem ineach corpus.Most importantly, along with tree pruning,decay factor could boost the performance of oursystem by controlling the rigidness of the parsetree kernel in PPI extraction.Table 3 shows the results of the cross-corpusevaluation to measure the generalization powerof our system as conducted in Airola et al,(2008) and Miwa et al, (2009a).
Miwa et al,(2009b) executed a set of combinatorial expe-riments by mixing multiple corpora and pre-sented their results.
Therefore, it is not reasona-ble to compare our results with them due to thesize discrepancy between training corpora.Nevertheless, we will compare our results withtheir approaches in later based on AIMed cor-pus.As seen in Table 3, our system outperformsthe existing approaches in almost all pairs ofcorpora.
In particular, in the multiple corpora-based evaluations aimed at AIMed which hasbeen frequently used as a standard set in PPIextraction, our approach shows prominent re-sults compared with others.
While other ap-proaches showed the performance ranging from33.3% to 60.8%, our approach achieved muchhigher scores between 55.9% and 67.0%.
Morespecific observations are:(1) Our PPIE method trained on any corpus ex-cept for IEPA outperforms the other approachesregardless of the test corpus only with a fewexceptions with IEPA and LLL.
(2) Even when using LLL or HPRD50, twosmallest corpora, as training sets, our systemperforms well with every other corpus for test-ing.
It indicates that our approach is much lessvulnerable to the sizes of training corpora thanother methods.
(3) The degree of score fluctuation of our sys-tem across different testing corpora is muchsmaller than other regardless of the training da-ta set.
When trained on LLL, for example, therange for our system (55.9% ~ 82.1%) is small-er than the others (38.6% ~ 83.2% and 33.3% ~76.8%).
(4) The cross-corpus evaluation reveals that ourmethod outperforms the others significantly.This is more visibly shown especially when thelarge-scale corpora (AIMed and BioInfer) areused.
(5) PPI extraction model trained on AIMedshows lower scores in IEPA and LLL as com-pared with other methods, which could triggerfurther investigation.In order to convince ourselves further the su-periority of the proposed method, we compareit with other previously reported approaches.Table 4 lists the macro-averaged precision, re-call and F-scores of the nine approaches testedon AIMed.
While the experimental settings aredifferent as reported in the literature, they arequite close in terms of the numbers of positiveand negative documents.211As seen in the table, the proposed method issuperior to all the others in F-scores.
The im-provement in precision (12.8%) is most signifi-cant, especially in comparison with the work ofMiwa et al, (2009b), which used multiple cor-pora (AIMed + IEPA) for training and com-bined various kernels such as bag-of-words,parse trees and graphs.
It is natural that the re-call value is lower since a less number of pat-terns (features) must have been learned.
What?simportant is that the proposed method has ahigher or at least comparable overall perfor-mance without additional resources.Our approach is significantly better than thatof Airola et al, (2008), which employed twodifferent forms of graph kernels to improve theinitial model.
Since they did not use multiplecorpora for training, the comparison shows thedirect benefit of using the extension of the ker-nel.6 Conclusion and Future WorksTo improve the performance of PPIE, recentresearch activities have had a tendency of in-creasing the complexity of the systems by com-bining various methods and resources.
In thispaper, however, we argue that by paying moreTrainingcorporaSystemsF-Scores in the test corporaAIMed BioInfer HPRD50 IEPA LLLAIMedOur System 67.0  64.2  72.9  59.0  62.7(Miwa et al, 2009a) 60.8  53.1  68.3  68.1  73.5(Airola et al, 2008) 56.4  47.1  69.0  67.4  74.5BioInferOur System 65.2  72.6  71.9  72.9  78.4(Miwa et al, 2009a) 49.6  68.1  68.3  71.4  76.9(Airola et al, 2008) 47.2  61.3  63.9  68.0  78.0HPRD50Our System 63.1  65.5  73.1  69.3  73.7(Miwa et al, 2009a) 43.9  48.6  70.9  67.8  72.2(Airola et al, 2008) 42.2  42.5  63.4  65.1  67.9IEPAOur System 57.8  66.1  66.3  73.1  78.4(Miwa et al, 2009a) 40.4  55.8  66.5  71.7  83.2(Airola et al, 2008) 39.1  51.7  67.5  75.1  77.6LLLOur System 55.9  64.4  69.4  71.4  82.1(Miwa et al, 2009a) 38.6  48.9  64.0  65.6  83.2(Airola et al, 2008) 33.3  42.5  59.8  64.9  76.8Table 3.
Macro-averaged F1 scores in cross-corpora evaluation.
Rows and columns correspond tothe training and test corpora, respectively.
We parallel our results with other recently reported re-sults.
All the split methods in 10-fold CV are the same for fair comparisons.Figure 5.
Performance variation with respect to decay factor in Five PPI Corpora.
Macro-averaged F1 (left), Precision (middle), Recall (right) evaluated by 10-fold CV212attention to a single model and adjusting para-meters more carefully, we can obtain at leastcomparable performance if not better.This paper indicates that a well-tuned parsetree kernel based on decay factor can achievethe superior performance in PPIE when it ispreprocessed by the path-enclosed tree pruningmethod.
It was shown in a series of experimentsthat our system produced the best scores in sin-gle corpus evaluation as well as cross-corporavalidation in comparison with other state-of-the-art methods.
Contribution points of this pa-per are as follows:(1) We have shown that the benefits of usingadditional resources including richer featurescan be obtained by tuning a single tree kernelmethod with tree pruning and decaying factors.
(2) We have newly found that the decay factorinfluences precision enhancement of PPIE andhence its overall performance as well.
(3) We have also revealed that the parse treekernel method equipped with decay factorsshows superior generalization power even withsmall corpora while presenting significant per-formance increase on cross-corpora experi-ments.As a future study, we leave experiments withtraining the classifier with multiple corpora anddeeper analysis of what aspects of the corporagave different magnitudes of the improvements.AcknowledgmentWe want to thank the anonymous reviewersfor their valuable comments.
This work hasbeen supported in part by KRCF Grant, the Ko-rean government.ReferenceAirola, A., Pyysalo, S., Bjorne, J., Pahikkala, T.,Ginter, F. & Salakoski, T. (2008).
All-paths graphkernel for protein-protein interaction extractionwith evaluation of cross-corpus learning.
BMCBioinformatics, 9(S2), doi:10.1186/1471-2105-9-S11-S2.Andrade, M. A.
& Valencia, A.
(1998).
Automaticextraction of keywords from scientific text: appli-cation to the knowledge domain of protein fami-lies.
Bioinformatics, 14(7), 600-607.Blaschke, C., Andrade, M., Ouzounis, C. & Valencia,A.
(1999).
Automatic extraction of biological in-formation from scientific text: protein-protein in-teractions.
Proc.
Int.
Conf.
Intell.
Syst.
Mol.
Biol.,(pp.
60-67).Bunescu, R., Ge, R., Kate, R., Marcotte, E., Mooney,R., Ramani, A.
& Wong, Y.
(2005).
ComparativeExperiments on Learning Information Extractorsfor Proteins and their Interactions.
Artif.
Intell.Med., Summarization and Information Extractionfrom Medical Documents, 33, 139-155Collins, M. & Duffy, N. (2001).
Convolution Ker-nels for Natural Language.
NIPS-2001, (pp.
625-632).Craven, M. & Kumlien, J.
(1999).
Constructing bio-logical knowledge bases by extracting informa-tion from text sources.
Proceedings of the 7th In-ternational conference on intelligent systems formolecular biology, (pp.77-86), Heidelberg, Ger-many.Ding, J., Berleant, D., Nettleton, D. & Wurtele, E.(2002).
Mining MEDLINE: abstracts, sentences,or phrases?.
Proceedings of PSB'02, (pp.
326-337)Erkan, G., Ozgur, A., & Radev, D. R., (2007).
Semi-supervised classification for extracting protein in-POS NEG ma-P ma-R ma-F ?FOur System 1,000 4,834 72.8 62.1 67.0 4.5(Miwa et al, 2009b) 1,000 4,834 60.0 71.9 65.2(Miwa et al, 2009a) 1,000 4,834 58.7 66.1 61.9 7.4(Miwa et al, 2008) 1,005 4,643 60.4 69.3 61.5(Miyao et al, 2008) 1,059 4,589 54.9 65.5 59.5(Giuliano et al, 2006) - - 60.9 57.2 59.0(Airola et al, 2008) 1,000 4,834 52.9 61.8 56.4 5.0(S?
tre et al, 2007) 1,068 4,563 64.3 44.1 52.0(Erkan et al, 2007) 951 4,020 59.6 60.7 60.0(Bunescu & Mooney, 2005) - - 65.0 46.4 54.2Table 4.
Comparative results in AIMed.
The number of positive instances (POS) and negative in-stances (NEG) and macro-averaged precision (ma-P), recall (ma-R) and F1-score (ma-F) are shown.213teraction sentences using dependency parsing.
InEMNLP 2007.Fundel, K., K?ffner, R. & Zimmer, R. (2007).
RelEx?
Relation extraction using dependency parsetrees.
Bioinformatics, 23, 365-371.Giuliano, C., Lavelli, A., Romano, L., (2006).
Ex-ploiting Shallow Linguistic Information for Rela-tion Extraction From Biomedical Literature.
Pro-ceedings of the 11th Conference of the EuropeanChapter of the Association for ComputationalLinguistics.Gondy, L., Hsinchun C. & Martinez Jesse D. (2003).A shallow parser based on closed-class words tocapture relations in biomedical text.
J. Biomed.Informatics.
36(3), 145-158.GuoDong, Z., Min, Z., Dong, H. J.
& QiaoMing, Z.(2007).
Tree Kernel-based Relation Extractionwith Context-Sensitive Structured Parse Tree In-formation.
Proceedings of the 2007 Joint Confe-rence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, Prague, (pp.
728?736)Marcotte, E. M., Xenarios, I.
& Eisenberg D. (2001).Mining literature for protein-protein interactions.Bioinformatics, 17(4), 359-363.Miwa, M., S?
tre, R., Miyao, Y.
& Tsujii J.
(2009a).Protein-protein interaction extraction by leverag-ing multiple kernels and parsers.
InternationalJournal of Medical Informatics, 78(12), e39-e46.Miwa, M., S?
tre, R., Miyao, Y.
& Tsujii J.
(2009b).A Rich Feature Vector for Protein-Protein Inte-raction Extraction from Multiple Corpora.
Pro-ceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, (pp.121-130)Miwa, M., S?
tre, R., Miyao, Y., Ohta,  T., & Tsujii,J.
(2008).
Combining multiple layers of syntacticinformation for protein-protein interaction extrac-tion.
In Proceedings of the Third InternationalSymposium on Semantic Mining in Biomedicine(SMBM 2008), (pp.
101?108)Miyao, Y., S?
tre, R., Sagae, K., Matsuzaki, T., &Tsujii, J.
(2008).
Task-oriented evaluation of syn-tactic parsers and their representations.
Proceed-ings of the 45th Meeting of the Association forComputational Linguistics (ACL?08:HLT).Moschitti, A.
(2006).
Making tree kernels practicalfor natural language learning.
Proceedings ofEACL?06, Trento, Italy.Nikolai, D., Anton, Y., Sergei, E., Svetalana, N.,Alexander, N. & llya, M. (2004).
Extracting hu-man protein interactions from MEDLINE using afull-sentence parser.
Bioinformatics, 20(5), 604-611.Ono, T., Hishigaki, H., Tanigam, A.
& Takagi, T.(2001).
Automated extraction of information onprotein-protein interactions from the biological li-terature.
Bioinformatics, 17(2), 155-161.Pyysalo, S., Airola, A., Heimonen, J., Bj?rne, J.,Ginter, F. & Salakoski, T. (2008).
Comparativeanalysis of five protein-protein interaction corpo-ra.
BMC Bioinformatics, 9(S6),doi:10.1186/1471-2105-9-S3-S6.S?
tre, R., Sagae, K., & Tsujii, J.
(2007).
Syntacticfeatures for protein-protein interaction extraction.In LBM 2007 short papers.Sekimizu, T., Park H. S. & Tsujii J.
(1998).
Identify-ing the interaction between genes and gene prod-ucts based on frequently seen verbs in MEDLINEabstracts.
Workshop on genome informatics, vol.9, (pp.
62-71).Shawe-Taylor, J., Cristianini, N., (2004).
KernelMethods for Pattern Analysis, Cambridge Univer-sity Press.Temkin, J. M. & Gilder, M. R. (2003).
Extraction ofprotein interaction information from unstructuredtext using a context-free grammar.
Bioinformatics,19(16), 2046-2053.Vishwanathan, S. V. N., Smola, A. J.
(2003).
FastKernels for String and Tree Matching.
Advancesin Neural Information Processing Systems, 15,569-576, MIT Press.Zhang, M., GuoDong, Z.
& Aiti, A.
(2008).
Explor-ing syntactic structured features over parse treesfor relation extraction using kernel methods.
In-formation Processing and Management, 44, 687-701Zhang, M., Zhang, J., Su, J.
& Zhou, G. (2006).
AComposite Kernel to Extract Relations betweenEntities with both Flat and Structured Features.21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the ACL,(pp.825-832).Zhou, D. & He, Y.
(2008).
Extracting interactionsbetween proteins from the literature.
Journal ofBiomedical Informatics, 41, 393-407.214
