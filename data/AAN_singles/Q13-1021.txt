Transactions of the Association for Computational Linguistics, 1 (2013) 255?266.
Action Editor: Kristina Toutanova.Submitted 11/2012; Published 5/2013.
c?2013 Association for Computational Linguistics.Minimally-Supervised Morphological Segmentationusing Adaptor GrammarsKairit SirtsInstitute of CyberneticsTallinn University of Technologysirts@phon.ioc.eeSharon GoldwaterSchool of InformaticsThe University of Edinburghsgwater@inf.ed.ac.ukAbstractThis paper explores the use of Adaptor Gram-mars, a nonparametric Bayesian modellingframework, for minimally supervised morpho-logical segmentation.
We compare three train-ing methods: unsupervised training, semi-supervised training, and a novel model selec-tion method.
In the model selection method,we train unsupervised Adaptor Grammars us-ing an over-articulated metagrammar, then usea small labelled data set to select which poten-tial morph boundaries identified by the meta-grammar should be returned in the final output.We evaluate on five languages and show thatsemi-supervised training provides a boost overunsupervised training, while the model selec-tion method yields the best average results overall languages and is competitive with state-of-the-art semi-supervised systems.
Moreover,this method provides the potential to tune per-formance according to different evaluation met-rics or downstream tasks.1 IntroductionResearch into unsupervised learning of morphologyhas a long history, starting with the work of Harris(1951).
While early research was mostly motivatedby linguistic interests, more recent work in NLP oftenaims to reduce data sparsity in morphologically richlanguages for tasks such as automatic speech recogni-tion, statistical machine translation, or automatic textgeneration.
For these applications, however, com-pletely unsupervised systems may not be ideal ifeven a small amount of segmented training data isavailable.
In this paper, we explore the use of Adap-tor Grammars (Johnson et al 2007) for minimallysupervised morphological segmentation.Adaptor Grammars (AGs) are a nonparametricBayesian modelling framework that can learn latenttree structures over an input corpus of strings.
Forexample, they can be used to define a morpholog-ical grammar where each word consists of zero ormore prefixes, a stem, and zero or more suffixes; theactual forms of these morphs (and the segmentationof words into morphs) are learned from the data.
Inthis general approach AGs are similar to many otherunsupervised morphological segmentation systems,such as Linguistica (Goldsmith, 2001) and the Mor-fessor family (Creutz and Lagus, 2007).
A majordifference, however, is that the morphological gram-mar is specified as an input to the program, ratherthan hard-coded, which allows different grammarsto be explored easily.
For the task of segmentingutterances into words, for example, Johnson and col-leagues have experimented with grammars encodingdifferent kinds of sub-word and super-word structure(e.g., syllables and collocations), showing that thebest grammars far outperform other systems on thesame corpora (Johnson, 2008a; Johnson and Goldwa-ter, 2009; Johnson and Demuth, 2010).These word segmentation papers demonstratedboth the power of the AG approach and the syner-gistic behavior that occurs when learning multiplelevels of structure simultaneously.
However, the best-performing grammars were selected using the samecorpus that was used for final testing, and each paperdealt with only one language.
The ideal unsuper-vised learner would use a single grammar tuned on255one or more development languages and still performwell on other languages where development data isunavailable.
Indeed, this is the basic principle be-hind Linguistica and Morfessor.
However, we knowthat different languages can have very different mor-phological properties, so using a single grammar forall languages may not be the best approach if thereis a principled way to choose between grammars.Though AGs make it easy to try many different pos-sible grammars, the process of proposing and testingplausible options can still be time-consuming.In this paper, we propose a novel method for au-tomatically selecting good morphological grammarsfor different languages (English, Finnish, Turkish,German, and Estonian) using a small amount ofgold-segmented data (1000 word types).
We usethe AG framework to specify a very general binary-branching grammar of depth four with which welearn a parse tree of each word that contains severalpossible segmentation splits for the word.
Then, weuse the gold-segmented data to learn, for each lan-guage, which of the proposed splits from the originalgrammar should actually be used in order to bestsegment that language.We evaluate our approach on both a small devel-opment set and the full Morpho Challenge test setfor each language?up to three million word types.In doing so, we demonstrate that using the posteriorgrammar of an AG model to decode unseen data isa feasible way to scale these models to large datasets.
We compare to several baselines which use theannotated data to different degrees: parameter tuning,grammar tuning, supervised training, or no use ofannotated data.
In addition to existing approaches?unsupervised and semi-supervised Morfessor, unsu-pervised Morsel (Lignos, 2010), and unsupervisedAGs?we also show how to use the annotated data totrain semi-supervised AGs (using the data to accumu-late rule statistics rather than for grammar selection).The grammar selection method yields comparableresults to the best of these other approaches.To summarize, our contributions in this paper are:1) scaling AGs to large data sets by using the poste-rior grammar to define an inductive model; 2) demon-strating how to train semi-supervised AG models, andshowing that this improves morphological segmenta-tion over unsupervised training; and 3) introducinga novel grammar selection method for AG modelswhose segmentation results are competitive with thebest existing systems.Before providing details of our methods and re-sults, we first briefly review Adaptor Grammars.
Fora formal definition, see Johnson et al(2007).2 Adaptor GrammarsAdaptor Grammars are a framework for specifyingnonparametric Bayesian models that can be used tolearn latent tree structures from a corpus of strings.There are two components to an AG model: the basedistribution, which is just a PCFG, and the adaptor,which ?adapts?
the probabilities assigned to individ-ual subtrees under the PCFG model, such that theprobability of a subtree under the complete modelmay be considerably higher than the product of theprobabilities of the PCFG rules required to constructit.
Although in principle the adaptor can be any func-tion that maps one distribution onto another, Johnsonet al(2007) use a Pitman-Yor Process (PYP) (Pit-man and Yor, 1997) as the adaptor because it actsas a caching model.
Under a PYP AG model, theposterior probability of a particular subtree will beroughly proportional to the number of times that sub-tree occurs in the current analysis of the data (withthe probability of unseen subtrees being computedunder the base PCFG distribution).An AG model can be defined by specifying theCFG rules (the support for the base distribution) andindicating which non-terminals are ?adapted?, i.e.,can serve as the root of a cached subtree.
Given thisdefinition and an input corpus of strings, Markovchain Monte Carlo samplers can be used to infer theposterior distribution over trees (and all hyperparam-eters of the model, including PCFG probabilities inthe base distribution and PYP hyperparameters).
Anyfrequently recurring substring (e.g., a common pre-fix) will tend to be parsed consistently, as this permitsthe model to treat the subtree spanning that string asa cached subtree, assigning it higher probability thanunder the PCFG distribution.Adaptor Grammars have been applied to a widevariety of tasks, including segmenting utterancesinto words (Johnson, 2008a; Johnson and Goldwa-ter, 2009; Johnson and Demuth, 2010), classifyingdocuments according to perspective (Hardisty et al2010), machine transliteration of names (Huang et256al., 2011), native language identification (Wong etal., 2012), and named entity clustering (Elsner et al2009).
There have also been AG experiments withmorphological segmentation, but more as a proof ofconcept than an attempt to achieve state-of-the-artresults (Johnson et al 2007; Johnson, 2008b).3 Using AGs for Learning MorphologyOriginally, the AG framework was designed for un-supervised learning.
This section first describes howAGs can be used for unsupervised morphologicalsegmentation, and then introduces two ways to usea small labelled data set to improve performance:semi-supervised learning and grammar selection.3.1 Unsupervised Adaptor GrammarsWe define three AG models to use as unsupervisedbaselines in our segmentation experiments.
The firstof these is very simple:Word?
Morph+Morph?
Char+ (1)The underline notation indicates an adapted non-terminal, and + abbreviates a set of recursive rules,e.g., Word?
Morph+ is short forWord?
MorphsMorphs?
Morph MorphsMorphs?
MorphGrammar 1 (MorphSeq) is just a unigram modelover morphs: the Morph symbol is adapted, so theprobability of each Morph will be roughly propor-tional to its (inferred) frequency in the corpus.
Thegrammar specifies no further structural relationshipsbetween morphs or inside of morphs (other than ageometric distribution on their length in characters).Experiments with AGs for unsupervised word seg-mentation suggest that adding further latent structurecan help with learning.
Here, we add another layerof structure below the morphs,1 calling the resulting1Because the nonterminal labels are arbitrary, this grammarcan also be interpreted as adding another layer on top of morphs,allowing the model to learn morph collocations that encode de-pendencies between morphs (which themselves have no substruc-ture).
However preliminary experiments showed that the morph-submorph interpretation scored better than the collocation-morphinterpretation, hence we chose the corresponding nonterminalnames.grammar SubMorphs:Word?
Morph+Morph?
SubMorph+SubMorph?
Char+(2)For capturing the rules of morphotactics, a gram-mar with linguistically motivated non-terminals canbe created.
There are many plausible options andthe best-performing grammar may be somewhatlanguage-dependent.
Rather than experimenting ex-tensively, we designed our third grammar to replicateas closely as possible the grammar that is implicitlyimplemented in the Morfessor system.
This Com-pounding grammar distinguishes between prefixes,stems and suffixes, allows compounding, defines theorder in which the morphs can occur and also allowsthe morphs to have inner latent structure:Word?
Compound+Compound?
Prefix?
Stem Suffix?Prefix?
SubMorph+Stem?
SubMorph+Suffix?
SubMorph+SubMorph?
Char+(3)3.2 Semi-Supervised Adaptor GrammarsThe first new use of AGs we introduce is the semi-supervised AG, where we use the labelled data to ex-tract counts of the different rules and subtrees used inthe gold-standard analyses.
We then run the MCMCsampler as usual over both the unlabelled and la-belled data, treating the counts from the labelled dataas fixed.We assume that the labelled data provides a con-sistent bracketing (no two spans in the bracketingcan partially overlap) and the labels of the spansmust be compatible with the grammar.
However,the bracketing may not specify all levels of structurein the grammar.
In our case, we have morphemebracketings but not, e.g., submorphs.
Thus, usingthe SubMorphs grammar in semi-supervised modewill constrain the sampler so that Morph spans in thelabelled data will remain fixed, while the SubMorphsinside those Morphs will be resampled.257The main change made to the AG inference pro-cess2 for implementing the semi-supervised AG wasto prune out from the sampling distribution any non-terminals that are inconsistent with the spans/labelsin the given labelling.3.3 AG SelectBoth the unsupervised and semi-supervised methodsdescribed above assume the definition of a grammarthat adequately captures the phenomena being mod-elled.
Although the AG framework makes it easyto experiment with different grammars, these experi-ments can be time-consuming and require some goodguesses as to what a plausible grammar might be.These problems can be overcome by automating thegrammar development process to systematically eval-uate different grammars and find the best one.We propose a minimally supervised model selec-tion method AG Select that uses the AG framework toautomatically identify the best grammar for differentlanguages and data sets.
We first define a very gen-eral binary-branching CFG grammar for AG trainingthat we call the metagrammar.
The metagrammarlearns a parse tree for each word where each branchcontains a different structure in the word.
The granu-larity of these structures is determined by the depth ofthis tree.
For example, Grammar 4 generates binarytrees of depth two and can learn segmentations of upto four segments.Word?
M1Word?
M1 M2M1?
M11M1?
M11 M12M2?
M21M2?
M21 M22M11?
Chars+M12?
Chars+M21?
Chars+M22?
Chars+(4)Next we introduce the notion of a morphologi-cal template, which is an ordered sequence of non-terminals whose concatenated yields constitute theword and which are used to parse out a specific seg-mentation of the word.
For example, using Gram-mar 4 the parse tree of the word saltiness is shown inFigure 1.
There are four possible templates with four2We started with Mark Johnson?s PYAG implementa-tion, http://web.science.mq.edu.au/?mjohnson/code/py-cfg.tgz,which we also used for our unsupervised and grammar selectionexperiments.WordM1M11s a lM12tM2M21iM22n e s sFigure 1: The parse tree generated by the metagrammarof depth 2 for the word saltiness.different segmentations: M1 M2 (salt iness), M11M12 M2 (sal t iness), M1 M21 M22 (salt i ness),and M11 M12 M21 M22 (sal t i ness).The morphological template consisting only ofnon-terminals from the lowest cached level of theparse tree is expected to have high recall, whereasthe template containing the non-terminals just belowthe Word is expected to have high precision.
Ourgoal is to find the optimal template by using a smalllabelled data set.
The grammar selection process iter-ates over the set of all templates.
For each template,the segmentations of the words in the labelled dataset are parsed out and the value of the desired evalua-tion metric is computed.
The template that obtainedthe highest score is then chosen.For each language we use a single template to seg-ment all the words in that language.
However, evenusing (say) a four-morph template such as M11 M12M21 M22, some words may contain fewer morphsbecause the metagrammar permits either unary orbinary branching rules, so some parses may not con-tain M12 or M2 (and thus M21 M22) spans.
Thus,we can represent segmentations of different lengths(from 1 to 2n, where n is the depth of the metagram-mar) with a single template.3For our experiments we use a metagrammar ofdepth four.
This grammar allows words to consist ofup to 16 segments, which we felt would be enough forany word in the training data.
Also, iterating over allthe templates of a grammar with bigger depth wouldnot be feasible as the number of different templatesincreases very rapidly.43We also experimented with selecting different templates forwords of different length but observed no improvements over thesingle template approach.4The number of templates of each depth can be expressedrecursively as Ni = (Ni?1 + 1)2, where Ni?1 is the number of2583.4 Inductive LearningPrevious work on AGs has used relatively small datasets and run the sampler on the entire input corpus(some or all of which is also used for evaluation)?atransductive learning scenario.
However, our largerdata sets contain millions of word types, where sam-pling over the whole set is not feasible.
For example,1000 training iterations on 50k word types took abouta week on one 2.67 GHz CPU.
To solve this problem,we need an inductive learner that can be trained on asmall set of data and then used to segment a differentlarger set.To create such a learner, we run the sampler onup to 50k word types, and then extract the posteriorgrammar as a PCFG.5 This grammar contains all theinitial CFG rules, plus rules to generate each of thecached subtrees inferred by the sampler.
The samplercounts of all rules are normalized to obtain a PCFG,and we can then use a standard CKY parser to decodethe remaining data using this PCFG.4 Experiments4.1 DataWe test on languages with a range of morphologi-cal complexity: English, Finnish, Turkish, German,and Estonian.
For each language we use two smallsets of gold-annotated data?a labelled set for semi-supervised training or model selection and a devset for development results?and one larger gold-annotated dataset for final tests.
We also have a largeunlabelled training set for each language.
Table 1gives statistics.The data sets for English, Finnish, Turkish andGerman are from the Morpho Challenge 2010 com-petition6 (MC2010).
We use the MC2010 trainingset of 1000 annotated word types as our labelled data,and for our dev sets we collate together the devel-opment data from all years of the MC competition.Final evaluation is done on the official MC2010 testsets, which are not public, so we rely on the MCorganizers to perform the evaluation.
The words intemplates in the grammar of depth one less and N0 = 0.5This can be seen as a form of Structure Compilation (Lianget al 2008), where the solution found by a more costly modelis used to define a less costly model.
However in Liang et alscase both models were already inductive.6http://research.ics.aalto.fi/events/morphochallenge2010/datasets.shtmlUnlab.
Lab.
Dev TestEnglish 0.9M 1000 1212 16KFinnish 2.9M 1000 1494 225KTurkish 0.6M 1000 1531 64KGerman 2.3M 1000 785 62KEstonian 2.1M 1000 1500 74KTable 1: Number of word types in our data sets.each test set are an unknown subset of the words inthe unlabelled corpus, so to evaluate we segmentedthe entire unlabelled corpus and sent the results tothe MC team, who then computed scores on the testwords.The Estonian wordlist is gathered from the news-paper texts of a mixed corpus of Estonian.7 Goldstandard segmentations of some of these words areavailable from the Estonian morphologically disam-biguated corpus;8 we used these for the test set, withsmall subsets selected randomly for the labelled anddev sets.For semi-supervised tests of the AG Compoundinggrammar we annotated the morphemes in the English,Finnish and Estonian labelled sets as prefixes, stemsor suffixes.
We could not do so for Turkish becausenone of the authors knows Turkish.4.2 EvaluationWe evaluate our results with two measures: segmentborder F1-score (SBF1) and EMMA (Spiegler andMonson, 2010).
SBF1 is one of the simplest andmost popular evaluation metrics for morphologicalsegmentations.
It computes F1-score from the preci-sion and recall of ambiguous segment boundaries?i.e., word edges are not counted.
It is easy and quickto compute but has the drawback that it gives nocredit for one-morpheme words that have been seg-mented correctly (i.e., are assigned no segment bor-ders).
Also it can only be used on systems and goldstandards where the output is just a segmentation ofthe surface string (e.g., availabil+ity) rather than amorpheme analysis (e.g., available+ity).
For thisreason we cannot report SBF1 on our German data,which annotations contain only analyses.EMMA is a newer measure that addresses both7http://www.cl.ut.ee/korpused/segakorpus/epl8http://www.cl.ut.ee/korpused/morfkorpus/259of these issues?correctly segmented one-morphemewords are reflected in the score, and it can evalu-ate both concatenative and non-concatenative mor-phology.
EMMA works by finding the best one-to-one mapping between the hypothesized and true seg-ments.
The induced segments are then replaced withtheir mappings and based on that, F1-score on match-ing segments is calculated.
Using EMMA we canevaluate the induced segmentations of German wordsagainst gold standard analyses.
EMMA has a freelyavailable implementation,9 but is slow to computebecause it uses Integer Linear Programming.For our dev results, we computed both scores us-ing the entire dev set, but for the large test sets, theevaluation is done on batches of 1000 word types se-lected randomly from the test set.
This procedure isrepeated 10 times and the average is reported, just asin the MC2010 competition (Kohonen et al 2010a).4.3 Baseline ModelsWe compare our AG models to several other mor-phology learning systems.
We were able to obtainimplementations of two of the best unsupervised sys-tems from MC2010, Morfessor (Creutz and Lagus,2007) and Morsel (Lignos, 2010), and we use thesefor comparisons on both the dev and test sets.
Wealso report test results from MC2010 for the onlysemi-supervised system in the competition, semi-supervised Morfessor (Kohonen et al 2010a; Ko-honen et al 2010b).
No dev results are reported onthis system since we were unable to obtain an imple-mentation.
This section briefly reviews the systems.4.3.1 Morfessor Categories-MAPMorfessor Categories-MAP (Morfessor) is a state-of-the-art unsupervised morphology learning system.Its implementation is freely available10 so it is widelyused both as a preprocessing step in tasks requiringmorphological segmentations, and as a baseline forevaluating morphology learning systems.Morfessor uses the Minimum Description Length(MDL) principle to choose the optimal segment lexi-con and the corpus segmentation.
Each morph in thesegment lexicon is labelled as a stem, prefix, suffix9http://www.cs.bris.ac.uk/Research/MachineLearning/Morphology/resources.jsp#eval10http://www.cis.hut.fi/projects/morpho/morfessorcatmapdownloadform.shtmlor non-morph.
The morphotactic rules are encodedas an HMM, which specifies the allowed morph se-quences with respect to the labels (e.g., a suffix can-not directly follow a prefix).The morphs in the segment lexicon can have ahierachical structure, containing submorphs whichthemselves can consist of submorphs etc.
We hypoth-esize that this hierarchical structure is one of the keyreasons why Morfessor has been so successful, as theexperiments also in this paper with different gram-mars show that the ability to learn latent structures iscrucial for learning good segmentations.One essential difference between Morfessor andthe proposed AG Select is that while we use the la-belled data to choose which levels of the hierarchyare to be used as morphs, Morfessor makes this de-cision based on the labels of the segments, choosingthe most fine-grained morph sequence that does notcontain the non-morph label.Morfessor includes a free parameter, perplexitythreshold, which we found can affect the SBF1 scoreconsiderably (7 points or more).
The best value forthis parameter depends on the size of the trainingset, characteristics of the language being learned, andalso the evaluation metric being used, as in somecases the best SBF1 and EMMA scores are obtainedwith completely different values.Thus, we tuned the value of the perplexity thresh-old on the labelled set for each language and evalua-tion metric for different unlabelled training set sizes.4.3.2 Semi-Supervised MorfessorRecently, the Morfessor system has been adaptedto allow semi-supervised training.
Four versions ofthe system were evaluated in MC2010, using differ-ent degrees of supervision.
Results reported here arefrom the Morfessor S+W system, which performedbest of those that use the same kind of labelled dataas we do.11 This system uses the Morfessor Base-line model (not Cat-MAP), which incorporates alexicon prior and data likelihood term.
The semi-supervised version maintains separate likelihoods forthe labelled and unlabelled data, and uses the devel-opment set to tune two parameters that weight theseterms with respect to each other and the prior.11Morfessor S+W+L performs better, but uses training datawith morpheme analyses rather than surface segmentations.2604.3.3 MorselMorsel (Lignos, 2010) is an unsupervised mor-phology learning system introduced in MC2010; weobtained the implementation from the author.
Morsellearns morphological analyses rather than segmenta-tions, so it can be evaluated only using EMMA.
Thereare two options provided for running Morsel: aggres-sive and conservative.
We used the development setto choose the best in each experimental case.The MC data sets contain gold standard morpho-logical analyses (as well as segmentations) so wecould compute Morsel?s EMMA scores using theanalyses.
However, we found that Morsel obtainshigher EMMA scores when evaluated against goldstandard segmentations and thus we used this optionin all the experiments.
(EMMA scores for other sys-tems were also computed using the segmentations.
)4.4 MethodThe experiments were conducted in two parts.
First,we evaluated different aspects of the AG models andcompared to all baseline models using the dev setdata.
Then we evaluated the most competitive modelson the final test data.For the development experiments, we compiled un-labelled training sets with sizes ranging from 10k to50k word types (using the most frequent word typesin each case).
For the AG results, we report the aver-age of five different runs made on the same trainingset.
We let the sampler run for 1000 iterations.
Noannealing was used as it did not seem to help.
Thetable label resampling option was turned on and thehyperparameter values were inferred.We trained all AG and baseline models on each ofthese training sets.
For AG Select, the words fromthe labelled data set were added to the training set toallow for template selection.12 To compute results intransductive mode, the words from the dev set werealso added to the training data.
In inductive mode,the dev set was instead parsed with the CKY parser.Preliminary experiments showed that the perfor-mance of unsupervised AG and AG Select improvedwith larger training sets, though the effect is small(see Figure 2 for results of AG Select in transductive12We also experimented with smaller sets of labelled data.
Inmost cases, the template selected based on only 300 word typeswas the same than the one selected with 1000 word types.mode; the trend in inductive mode is similar).
Basedon these and similar results with other baseline sys-tems, all results reported later for unsupervised mod-els (AG and baseline) and AG Select were obtainedusing training sets of 50k words.In contrast to the above models, the semi-supervised AG does not always improve with moreunlabelled data (see Figure 2) and in the limit, itwill match the performance of the same grammarin the unsupervised setting.
Other semi-supervisedapproaches often solve this problem by weightingthe labelled data more heavily than the unlabelleddata when estimating model parameters?effectively,assuming that each labelled item has actually beenobserved more than once.
However, duplicating thelabelled data does not make sense in the AG frame-work, because duplicate items will in most cases justbe cached at the root (Word) node, providing no addi-tional counts of Morphs (which are where the usefulinformation is).
It might be possible to come up witha different way to weight the labelled data more heav-ily when larger unlabelled sets are used, howeverfor this paper we instead kept the labelled data thesame and tuned the amount of unlabelled data.
Weused the dev set to choose the amount of unlabelleddata (in the range from 10k to 50k types); results forsemi-supervised AG are reported using the optimalamount of unlabelled data for each experiment.For test set experiments with semi-supervised AG,we evaluated each language using whichever gram-mar performed best on that language?s dev set.
Fortest set experiments with AG Select, we chose thetemplates with a two-pass procedure.
First, wetrained 5 samplers on the 50k training set with la-belled set added, and used the labelled data to choosethe best template for each inferred grammar.
Then,we decoded the dev set using each of the 5 gram-mar/template pairs and based on these results, chosethe best of these pairs to decode the test set.4.5 ResultsWe present the dev set results in Table 2(a) for trans-ductive and in Table 2(b) for inductive learning.
Ineach table, unsupervised models are shown in theupper section and the semi-supervised models andAG Select below.
Morsel appears only in Table 2(a)since it only works transductively.
Semi-supervisedgrammars cannot be trained on German, since we261556065707580859010000  20000  30000  40000  50000F-score# of word typesEnglishEstonianFinnishTurkish556065707580859010000  20000  30000  40000  50000F-score# of word typesEnglishEstonianFinnishTurkishFigure 2: Effect of training data size on dev set SBF1 for AG Select (left) and semi-supervised SubMorphs grammar(right) in transductive mode.only have gold standard analyses, not segmentations.The SubMorphs grammar performs the best of theunsupervised AG models, with the Compoundinggrammar being only slightly worse.
We also triedthe Compounding grammar without the sub-morphstructures but the results were even worse than thoseof MorphSeq.
This shows that the latent structuresare important for learning good segmentations.In all cases, the semi-supervised AGs perform bet-ter (ofen much better) than the corresponding unsu-pervised grammars.
Even though their average scoresare not as high as AG Select?s, they give the best devset results in many cases.
This shows that althoughfor semi-supervised AG the grammar must be cho-sen manually, with a suitable choice of the grammarand only a small set of labelled data it can improveconsiderably over unsupervised AG.In transductive mode, the AG Select performs thebest in several cases.
In both transductive and induc-tive mode, the results of AG Select are close to thebest results obtained and are consistently good acrossall languages?it achieves the best average scoresof all models, suggesting that the model selectionmethod is robust to different types of morphologyand annotation schemes.Table 3 presents the test set results.
We includescores for unsupervised Morfessor in both transduc-tive and inductive mode, where transductive modetrains on the entire unlabelled corpus and inductivemode trains on the 50k subset.
The semi-supervisedMorfessor scores are taken from the MC resultspage13 after verifying that the evaluation method-13http://research.ics.aalto.fi/events/morphochallenge/ology and labelled data used is the same as ours.14There is a good deal of variation between devel-opment and test results, indicating that the dev setsmay not be a representative sample.
The most no-table differences are in Turkish, where all modelsperform far worse on the test than dev set.
However,AG Select performs slightly better on the test set forthe other languages.
Thus its average SBF1 score ac-tually improves on the test set and is not much worsethan semi-supervised Morfessor.
While its averageperformance drops somewhat on test set EMMA, itis still as good as any other model on that measure.Again, these results support the idea that AG Selectis robust to variations in language and data set.We also note the surprisingly good performanceof Morfessor in transductive mode on Estonian; thiscould possibly be due to the larger amount of trainingdata used for the test set results, but it is not clearwhy this would improve performance so much onEstonian and not on the other languages.5 DiscussionTo give a sense of what the AG Select model is learn-ing, we provide some examples of both correctly andincorrectly induced segmentations in Table 4.
Theseexamples suggest that for example in English, M1 isused to model the stem, M21 is for the suffix or thesecond stem in the compound word, and the rest ofthe elements in the template are for the remainingsuffixes (if any).Table 5 presents examples of some of the mostfrequently used metagrammar rules and cached rules14Sami Virpioja, personal communication.262(a) Transductive mode Border F1-score EMMAEng Est Fin Tur Avg Eng Est Fin Tur Ger AvgAG MorphSeq 61.5 54.0 56.9 59.5 58.0 74.7 74.1 63.7 53.5 59.4 65.1AG SubMorphs 66.2 66.9 60.5 59.5 63.3 79.1 83.4 66.8 53.4 57.4 68.0AG Compounding 63.0 64.8 60.9 60.9 62.4 75.4 81.6 65.5 53.7 62.2 67.7Morfessor 69.5 55.7 65.0 69.3 64.9 81.3 75.3 67.8 62.2 62.7 69.9Morsel - - - - - 76.8 74.4 66.1 50.1 55.9 64.7AG ssv MorphSeq 64.4 57.3 63.0 68.9 63.4 74.4 75.9 65.6 59.6 - -AG ssv SubMorphs 67.6 69.1 64.4 63.4 66.1 79.5 84.4 69.2 56.1 - -AG ssv Compounding 70.0 67.5 71.8 - - 79.5 82.8 74.0 - - -AG Select 71.9 68.5 70.2 72.6 70.8 77.5 81.8 73.2 63.0 62.4 71.6(b) Inductive mode Border F1-score EMMAEng Est Fin Tur Avg Eng Est Fin Tur Ger AvgAG MorphSeq 57.6 54.0 55.4 59.8 56.7 72.0 73.8 62.6 53.7 58.9 64.2AG SubMorphs 66.1 67.5 61.6 59.8 63.7 78.6 83.7 67.4 53.4 56.0 67.8AG Compounding 62.0 64.8 57.4 61.1 61.3 73.5 81.1 61.9 53.2 61.0 66.2Morfessor 68.9 51.1 63.5 68.2 62.9 80.9 72.0 68.1 60.6 60.8 68.5AG ssv MorphSeq 64.6 56.9 63.1 70.3 63.8 72.7 73.3 65.9 61.2 - -AG ssv SubMorphs 70.1 69.7 66.3 67.9 68.4 80.4 83.7 70.5 59.0 - -AG ssv Compounding 70.5 67.2 70.0 - - 77.3 81.9 70.5 - - -AG Select 69.8 68.8 67.5 70.1 69.1 77.3 81.9 71.1 59.7 62.6 70.5Table 2: Dev set results for all models in (a) transductive and (b) inductive mode.
Unsupervised AG models andbaselines are shown in the top part of each table; semi-supervised AG models and grammar selection method are below.Border F1-score EMMAEng Est Fin Tur Avg -Est Eng Est Fin Tur Ger Avg -Est/GerMorf.
trans 67.3 73.9 61.2 57.1 64.9 61.9 78.4 78.8 61.8 49.8 65.2 66.8 63.3Morf.
ind 65.7 57.7 60.8 60.1 61.1 62.2 76.5 70.5 59.6 47.0 64.1 63.5 61.0Morsel - - - - - - 81.9 77.2 63.3 47.8 59.0 65.8 64.3Morf.
ssv 77.8 - 71.7 68.9 - 72.8 80.6 - 62.1 49.9 - - 64.2AG ssv best 70.3?
68.6?
64.9?
58.2?
65.5 64.5 75.9?
80.3?
61.3?
46.1?
- - 61.1AG Select 74.4 71.7 70.0 61.4 69.4 68.6 81.3 81.0 64.0 47.5 63.8 67.5 64.3Table 3: Test set results for unsupervised baselines Morfessor CatMAP (in transductive and inductive mode) and Morsel;semi-supervised Morfessor; and AG semi-supervised (?
marks the Compounding grammar, ?
denotes SubMorphsgrammar, and ?
is the MorphSeq grammar) and grammar selection methods.
Results are shown for each language,averaged over all languages (when possible: Avg), and averaged over just the languages where scores are available forall systems (-Est, -Est/Ger).for English, together with their relative frequencies.It shows that at the Word level the binary rule isselected over three times more frequently than theunary rule.
Also, most of the more frequently usedgrammar rules expand the first branch (rooted in M1)into more finegrained structures.
The second branch(M2) is mostly modelled with the unary rule.Among the frequently cached rules we see thecommon English prefixes and suffixes.
One of themost frequent cached rule stores the single letter e atthe end of a word, which often causes oversegmen-tation of words ending in e (as seen in the incorrectexamples in Table 4).
This problem is common inunsupervised morphological segmentation of English(Goldwater et al 2006; Goldsmith, 2001).We also took a look at the most frequent cachedrules learned by the semi-supervised AG with theSubMorphs grammar, and observed that Morphs263Correct Segmentations Incorrect SegmentationsWord Segmentation Induced Correcttreatable [tr.ea.t]M1 [a.b.le]M21 disagree s dis agree sdisciplined [dis.cip.l.i.n]M1 [e.d]M21 reduc e reducemonogamous [mon.o.g.a.m]M1 [o.u.s]M21 revalu e re valuestreakers [st.r.e.a.k]M1 [e.r]M21 [s]M2211 derid e deridetollgate [t.o.l.l.
]M1 [g.a.t.e]M21 [s]M2211 accompani ed ac compani edfoxhunting [f.o.x]M1 [h.u.n.t]M21 [ing]M2211 war y warymuscovites [m.u.sc.o.v]M1 [i.t.e]M21 [s]M2211 indescrib able in describ ablestandardizes [st.a.n.d.a.rd]M1 [i.z.e]M21 [s]M2211 orat es orate sslavers?
[sl.a.v]M1 [e.r]M21 [s]M2211 [?
]M2212 alger ian s algeri an searthiness?
[e.ar.th]M1 [i]M2111 [ness]M2211 [?
]M2212 disput e s dispute sinstinkt [in.st.in.kt]M1 meister likkust meisterlikkus trebis [re.b.i]M1 [s]M2 min a minatoitsid [to.it]M1 [s.id]M2 teiste teis tearmuavaldus [a.rm.u]M11 [ava.ld.u.s]M12 kuritegu de sse kuri tegu dessema?a?givale [ma?a?.g.i]M11 [v.a]M12 [l.e]M2 liharoa ga liha roa gakeskuskoulussa [kesk.us]M11 [koul.u]M12 [s.sa]M2 polte tti in polte tt i inperusla?hteille [per.u.s]M11 [l.a?.ht.e]M12 [i]M211 [ll.e]M212 kulttuuri se lt a kin kulttuurise lta kinperunakaupoista [per.u.n.a]M11 [k.au.p.o]M12 [i]M211 [st.a]M212 tuote palki ntoja tuote palkinto j ayo?paikkaani [yo?
]M11 [p.ai.kk.a]M12 [a]M21 [n.i]M22 veli puo lt a veli puol tanimetta?ko?o?n [ni.m.e]M11 [tt.a?
]M12 [k.o?
]M21 [o?.n]M22 ota ttava otatta vaTable 4: Examples of segmented words in English (top), Estonian (middle) and Finnish (bottom).
Correctly segmentedwords are in the left part of the table.
The identified segments are in brackets indexed by the respective templatenonterminal; dots separate the metagrammar generated parse tree leaves.
Examples of incorrectly segmented wordstogether with the correct segmentation are on the right.Freq (%) Rule Freq (%) Cached Rule9.9 Word?M1 M2 1.2 (M2 (M21 (M211 (M2111 s)))))5.7 M1?M11 M12 0.9 (M2 (M21 (M211 (M2111 e)) (M212 (M2121 d))))3.1 Word?M1 0.7 (M2 (M21 (M211 (M2111 i)) (M212 (M2121 n g))))2.5 M11?M111 0.6 (M2 (M21 (M211 (M2111 e)))1.8 M2?M21 0.4 (M2 (M21 (M211 (M2111 ?)))
(M22 (M221 (M2211 s))))1.4 M12?M121 M122 0.3 (M1112 a)1.4 M111?M1111 M1112 0.3 (M2 (M21 (M211 (M2111 y))))0.9 M12?M121 0.3 (M2 (M21 (M211 (M2111 e))) (M212 (M2121 r)))0.9 M11?M111 M112 0.2 (M2 (M21 (M211 (M2111 a))))Table 5: Examples from English most frequently used metagrammar rules and cached rules together with their relativeoccurrence frequencies (in percentages).tended to contain only a single SubMorph.
Thishelps to explain why the SubMorphs grammar insemi-supervised AG improved less over the unsuper-vised AG as compared to the MorphSeq grammar?the rules with only a single SubMorph under theMorph are essentially the same as they would be inthe MorphSeq grammar.Finally, we examined the consistency of the tem-plates chosen for each of the 5 samplers during modelselection for the test set (Section 4.4).
We found thatthere was some variability in the templates, but inmost experiments the same template was chosen forthe majority of the samplers (see Table 6).
While thismajority template is not always the optimal one onthe dev set, we observed that it does produce con-sistently good results.
It is possible that using themajority template, rather than the optimal templatefor the dev set, would actually produce better results264Majority templateEnglish M1 M21 M2211 M2212 M222Finnish M11 M12 M211 M212 M22Turkish M11 M12 M211 M212 M22German M11 M121 M122 M21 M221 M222Estonian M11 M12 M2Table 6: Majority templates for each language.
Notethat the Estonian gold standard contains less fine-grainedsegmentations than some of the other languages.on the test set, especially if (as appears to be the casehere, and may often be the case in real applications)the dev and test sets are from somewhat differentdistributions.It must be noted that both AG Select and semi-supervised AG are computationally more demandingthan the comparison systems.
Since we do inferenceover tree structures, the complexity is cubic in theinput word length, while most segmentation systemsare quadratic or linear.
Even compared to the unsu-pervised AG, AG Select is more expensive, becauseof the larger grammar and number of cached symbols.Nevertheless, our systems can feasibly be run on thelarge Morpho Challenge datasets.Other recent unsupervised systems have reportedstate-of-the art results by incorporating additional in-formation from surrounding words (Lee et al 2011),multilingual alignments (Snyder and Barzilay, 2008),or overlapping context features in a log-linear model(Poon et al 2009), but they have only been run onSemitic languages and English (and in the latter case,a very small corpus).
Since they explicitly enumerateand sample from all possible segmentations of eachword (often with some heuristic constraints), theycould have trouble with the much longer words ofthe agglutinative languages tested here.
In any casethe results are not directly comparable to ours.6 ConclusionIn this paper we have introduced three new meth-ods for Adaptor Grammars and demonstrated theirusefulness for minimally supervised morphologicalsegmentation.
First, we showed that AG models canbe scaled to large data sets by using the posteriorgrammar for defining an inductive model, that onaverage results in the same accuracy as compared tofull transductive training.Second, we implemented semi-supervised AG in-ference, which uses labelled data to constrain thesampler, and showed that in all cases it performsmuch better than the unsupervised AG on the samegrammar.
Semi-supervised AG could benefit fromlabelled data reweighting techniques frequently usedin semi-supervised learning, and studying the properways of doing so within the AG framework would bea potential topic for future research.Our final contribution is the AG Select method,where the initial model is trained using a very generalgrammar that oversegments the data, and the labelleddata is used to select which granularity of segments touse.
Unlike other morphological segmentation mod-els, this method can adapt its grammar to languageswith different structures, rather than having to usethe same grammar for every language.
Indeed, wefound that AG Select performs well across a rangeof languages and also seems to be less sensitive todifferences between data sets (here, dev vs. test).
Inaddition, it can be trained on either morphologicalanalyses or segmentations.
Although we tuned allresults to optimize the SBF1 metric, in principle thesame method could be used to optimize other mea-sures, including extrinsic measures on downstreamapplications such as machine translation or informa-tion retrieval.
In future we hope to show that thismethod can be used to improve performance on suchapplications, and also to explore its use for relatedsegmentation tasks such as stemming or syllabifica-tion.
Also, the method itself could potentially beimproved by designing a classifier to determinine thebest template for each word based on a set of features,rather than using a single template for all words inthe language.AcknowledgmentsThis work was supported by the Tiger University pro-gram of Estonian Information Technology Founda-tion for the first author.
We thank Constantine Lignosfor releasing his Morsel code to us, Sami Virpioja forevaluating test set results, and Federico Sangati forproviding useful scripts.ReferencesMathias Creutz and Krista Lagus.
2007.
Unsupervisedmodels for morpheme segmentation and morphology265learning.
ACM Transactions of Speech and LanguageProcessing, 4(1):1?34, February.Micha Elsner, Eugene Charniak, and Mark Johnson.
2009.Structured generative models for unsupervised named-entity clustering.
In Proceedings of NAACL, pages164?172.
Association for Computational Linguistics.John Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
Computational Lin-guistics, 27(2):153?198, June.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2006.
Interpolating between types and tokens byestimating power-law generators.
In Advances in Neu-ral Information Processing Systems 18, pages 459?466,Cambridge, MA.
MIT Press.Eric A. Hardisty, Jordan Boyd-Graber, and Philip Resnik.2010.
Modeling perspective using adaptor grammars.In Proceedings of EMNLP, pages 284?292.
Associationfor Computational Linguistics.Zellig Harris.
1951.
Structural Linguistics.
University ofChicago Press.Yun Huang, Min Zhang, and Chew Lim Tan.
2011.
Non-parametric bayesian machine transliteration with syn-chronous adaptor grammars.
In Proceedings of ACL:short papers - Volume 2, pages 534?539.
Associationfor Computational Linguistics.Mark Johnson and Katherine Demuth.
2010.
Unsuper-vised phonemic chinese word segmentation using adap-tor grammars.
In Proceedings of COLING, pages 528?536.
Association for Computational Linguistics.Mark Johnson and Sharon Goldwater.
2009.
Improvingnonparameteric bayesian inference: experiments on un-supervised word segmentation with adaptor grammars.In Proceedings of NAACL, pages 317?325.
Associationfor Computational Linguistics.Mark Johnson, Thomas L. Griffiths, and Sharon Gold-water.
2007.
Adaptor grammars: A framework forspecifying compositional nonparametric bayesian mod-els.
In B. Scho?lkopf, J. Platt, and T. Hoffman, editors,Advances in Neural Information Processing Systems19, pages 641?648.
MIT Press, Cambridge, MA.Mark Johnson.
2008a.
Unsupervised word segmentationfor sesotho using adaptor grammars.
In Proceedingsof ACL Special Interest Group on Computational Mor-phology and Phonology, pages 20?27.
Association forComputational Linguistics.Mark Johnson.
2008b.
Using adaptor grammars to iden-tify synergies in the unsupervised acquisition of linguis-tic structure.
In Proceedings of ACL, pages 398?406.Association for Computational Linguistics.Oskar Kohonen, Sami Virpioja, and Krista Lagus.
2010a.Semi-supervised learning of concatenative morphology.In Proceedings of ACL Special Interest Group on Com-putational Morphology and Phonology, pages 78?86.Association for Computational Linguistics.Oskar Kohonen, Sami Virpioja, Laura Leppa?nen, andKrista Lagus.
2010b.
Semi-supervised extensions tomorfessor baseline.
In Proceedings of the MorphoChallenge 2010 Workshop, pages 30?34.
Aalto Univer-sity School of Science and Technology.Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.2011.
Modeling syntactic context improves morpho-logical segmentation.
In Proceedings of CoNLL, pages1?9.
Association for Computational Linguistics.Percy Liang, Hal Daume?, III, and Dan Klein.
2008.
Struc-ture compilation: trading structure for features.
InProceedings of ICML, pages 592?599.
Association forComputing Machinery.Constantine Lignos.
2010.
Learning from Unseen Data.In Mikko Kurimo, Sami Virpioja, and Ville T. Turunen,editors, Proceedings of the Morpho Challenge 2010Workshop, pages 35?38.
Aalto University School ofScience and Technology.Jim Pitman and Marc Yor.
1997.
The two-parameterPoisson-Dirichlet distribution derived from a stable sub-ordinator.
Annals of Probability, 25(2):855?900.Hoifung Poon, Colin Cherry, and Kristina Toutanova.2009.
Unsupervised morphological segmentation withlog-linear models.
In Proceedings of NAACL, pages209?217.
Association for Computational Linguistics.Benjamin Snyder and Regina Barzilay.
2008.
Unsuper-vised multilingual learning for morphological segmen-tation.
In Proceedings of ACL, pages 737?745.
Associ-ation for Computational Linguistics.Sebastian Spiegler and Christian Monson.
2010.
Emma:A novel evaluation metric for morphological analysis.In Proceedings of COLING, pages 1029?1037.
Associ-ation for Computational Linguistics.Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.2012.
Exploring adaptor grammars for native languageidentification.
In Proceedings of EMNLP, pages 699?709.
Association for Computational Linguistics.266
