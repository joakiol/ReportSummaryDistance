Semantic Role Labeling withBoosting, SVMs, Maximum Entropy, SNOW, and Decision ListsGrace NGAI?1 , Dekai WU?2Marine CARPUAT?
, Chi-Shing WANG?
, Chi-Yung WANG??
Dept.
of ComputingHK Polytechnic UniversityHong Kong?
HKUST, Dept of Computer ScienceHuman Language Technology CenterHong Kongcsgngai@polyu.edu.hk, dekai@cs.ust.hkmarine@cs.ust.hk, wcsshing@netvigator.com, cscywang@comp.polyu.edu.hkAbstractThis paper describes the HKPolyU-HKUST sys-tems which were entered into the Semantic Role La-beling task in Senseval-3.
Results show that thesesystems, which are based upon common machinelearning algorithms, all manage to achieve goodperformances on the non-restricted Semantic RoleLabeling task.1 IntroductionThis paper describes the HKPolyU-HKUST sys-tems which participated in the Senseval-3 SemanticRole Labeling task.
The systems represent a diversearray of machine learning algorithms, from decisionlists to SVMs to Winnow-type networks.Semantic Role Labeling (SRL) is a task thathas recently received a lot of attention in the NLPcommunity.
The SRL task in Senseval-3 usedthe Framenet (Baker et al, 1998) corpus: given asentence instance from the corpus, a system?s jobwould be to identify the phrase constituents andtheir corresponding role.The Senseval-3 task was divided into restrictedand non-restricted subtasks.
In the non-restrictedsubtask, any and all of the gold standard annotationscontained in the FrameNet corpus could be used.Since this includes information on the boundariesof the parse constituents which correspond to someframe element, this effectively maps the SRL taskto that of a role-labeling classification task: given aconstituent parse, identify the frame element that itbelongs to.Due to the lack of time and resources, we chose toparticipate only in the non-restricted subtask.
Thisenabled our systems to take the classification ap-proach mentioned in the previous paragraph.1The author would like to thank the Hong Kong PolytechnicUniversity for supporting this research in part through researchgrants A-PE37 and 4-Z03S.2The author would like to thank the Hong Kong ResearchGrants Council (RGC) for supporting this research in partthrough research grants RGC6083/99E, RGC6256/00E, andDAG03/04.EG09.2 Experimental FeaturesThis section describes the features that were usedfor the SRL task.
Since the non-restricted SRL taskis essentially a classification task, each parse con-stituent that was known to correspond to a frameelement was considered to be a sample.The features that we used for each sample havebeen previously shown to be helpful for the SRLtask (Gildea and Jurafsky, 2002).
Some of thesefeatures can be obtained directly from the Framenetannotations:?
The name of the frame.?
The lexical unit of the sentence ?
i.e.
the lex-ical identity of the target word in the sentence.?
The general part-of-speech tag of the targetword.?
The ?phrase type?
of the constituent ?
i.e.
thesyntactic category (e.g.
NP, VP) that the con-stituent falls into.?
The ?grammatical function?
(e.g.
subject, ob-ject, modifier, etc) of the constituent, with re-spect to the target word.?
The position (e.g.
before, after) of the con-stituent, with respect to the target word.In addition to the above features, we also ex-tracted a set of features which required the use ofsome statistical NLP tools:?
Transitivity and voice of the target word ?The sentence was first part-of-speech taggedand chunked with the fnTBL transformation-based learning tools (Ngai and Florian, 2001).Simple heuristics were then used to deduce thetransitivity voice of the target word.?
Head word (and its part-of-speech tag) of theconstituent ?
After POS tagging, a syntacticparser (Collins, 1997) was then used to ob-tain the parse tree for the sentence.
The headword (and the POS tag of the head word) ofAssociation for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemsthe syntactic parse constituent whose span cor-responded most closely to the candidate con-stituent was then assumed to be the head wordof the candidate constituent.The resulting training data set consisted of 51,366constituent samples with a total of 151 frame ele-ment types.
These ranged from ?Descriptor?
(3520constituents) to ?Baggage?
and ?Carrier?
(1 con-stituent each).
This training data was randomly par-titioned into a 80/20 ?development training?
and?validation?
set.3 MethodologyThe previous section described the features thatwere extracted for each constituent.
This sectionwill describe the experiment methodology as wellas the learning systems used to construct the mod-els.Our systems had originally been trained on theentire development training (devtrain) set, gener-ating one global model per system.
However, oncloser examination of the task, it quickly becameevident that distinguishing between 151 possibleoutcomes was a difficult task for any system.
Itwas also not clear that there was going to be alot of information that could be generalized acrossframe types.
We therefore partitioned the data byframe, so that one model would be trained for eachframe.
(This was also the approach taken by (Gildeaand Jurafsky, 2002).)
Some of our individual sys-tems tried both approaches; the results are com-pared in the following subsections.
For compar-ison purposes, a baseline model was constructedby simply classifying all constituents with the mostfrequently-seen (in the training set) frame elementfor the frame.In total, five individual systems were trained forthe SRL task, and four ensemble models were gen-erated by using various combinations of the indi-vidual systems.
With one exception, all of the indi-vidual systems were constructed using off-the-shelfmachine learning software.
The following subsec-tions describe each system; however, it should benoted that some of the individual systems were notofficially entered as competing systems; therefore,their scores are not listed in the final rankings.3.1 BoostingThe most successful of our individual systems isbased on boosting, a powerful machine learningalgorithm which has been shown to achieve goodresults on NLP problems in the past.
Our sys-tem was constructed around the Boostexter soft-ware (Schapire and Singer, 2000), which imple-Model Prec.
Recall AttemptedSingle Model 0.891 0.795 89.2%Frame Separated 0.894 0.798 89.2%Baseline 0.444 0.396 89.2%Table 1: Boosting Models: Validation Set Resultsments boosting on top of decision stumps (decisiontrees of one level), and was originally designed fortext classification.
The same system also partici-pated in the Senseval-3 lexical sample tasks for Chi-nese and English, as well as the Multilingual lexicalsample task (Carpuat et al, 2004).Table 1 compares the results of training one sin-gle overall boosting model (Single) versus trainingseparate models for each frame (Frame).
It can beseen that training frame-specific models producesa small improvement over the single model.
Theframe-specific model was used in all of the ensem-ble systems, and was also entered into the competi-tion as an individual system (hkpust-boost).3.2 Support Vector MachinesThe second of our individual systems was basedon support vector machines, and implemented usingthe TinySVM software package (Boser et al, 1992).Since SVMs are binary classifiers, we used aone-against-all method to reduce the SRL task toa binary classification problem.
One model is con-structed for each possible frame element and thetask of the model is to decide, for a given con-stituent, whether it should be classified with thatframe element.
Since it is possible for all the bi-nary classifiers to decide on ?NOT-<element>?, themodel is effectively allowed to pass on samples thatit is not confident about.
This results in a very pre-cise model, but unfortunately at a significant hit torecall.A number of kernel parameter settings were in-vestigated, and the best performance was achievedwith a polynomial kernel of degree 4.
The rest ofthe parameters were left at the default values.
Table2 shows the results of the best SVM model on thevalidation set.
This model participated in the all ofthe ensemble systems, and was also entered into thecompetition as an individual system.System Prec.
Recall AttemptedSVM 0.945 0.669 70.8%Baseline 0.444 0.396 89.2%Table 2: SVM Models: Validation Set Results3.3 Maximum EntropyThe third of our individual systems was based onthe maximum entropy model, and implemented ontop of the YASMET package (Och, 2002).
Like theboosting model, the maximum entropy system alsoparticipated in the Senseval-3 lexical sample tasksfor Chinese and English, as well as the Multilinguallexical sample task (Carpuat et al, 2004).Our maximum entropy models can be classi-fied into two main approaches.
Both approachesused the frame-partitioned data.
The more conven-tional approach (?multi?)
then trained one modelper frame; that model would be responsible for clas-sifying a constituent belonging to that frame withone of several possible frame elements.
The secondapproach (binary) used the same approach as theSVM models, and trained one binary one-against-all classifier for each frame type-frame elementcombination.
(Unlike the boosting models, a singlemaximum entropy model could not be trained for allpossible frame types and elements, since YASMETcrashed on the sheer size of the feature space.
)System Prec.
Recall Attemptedmulti 0.856 0.764 89.2%binary 0.956 0.539 56.4%Baseline 0.444 0.396 89.2%Table 3: Maximum Entropy Models: Validation SetResultsTable 3 shows the results for the maximum en-tropy models.
As would have been expected, thebinary model achieves very high levels of precision,but at considerable expense of recall.
Both systemswere eventually used in the some of the ensemblemodels but were not submitted as individual contes-tants.3.4 SNOWThe fourth of our individual systems is based onSNOW ?
Sparse Network Of Winnows (Mun?oz etal., 1999).The development approach for the SNOW mod-els was similar to that of the boosting models.
Twomain model types were generated: one which gener-ated a single overall model for all the possible frameelements, and one which generated one model perframe type.
Due to a bug in the coding which wasnot discovered until the last minute, however, theresults for the frame-separated model were invali-dated.
The single model system was eventually usedin some of the ensemble systems, but not entered asan official contestant.
Table 4 shows the results.System Prec.
Recall AttemptedSingle Model 0.764 0.682 89.2%Baseline 0.444 0.396 89.2%Table 4: SNOW Models: Validation Set Results3.5 Decision ListsThe final individual system was a decision list im-plementation contributed from the Swarthmore Col-lege team (Wicentowski et al, 2004), which partic-ipated in some of the lexical sample tasks.The Swarthmore team followed the frame-separated approach in building the decision listmodels.
Table 5 shows the result on the validationset.
This system participated in some of the finalensemble systems as well as being an official par-ticipant (hkpust-swat-dl).System Prec.
Recall AttemptedDL 0.837 0.747 89.2%Baseline 0.444 0.396 89.2%Table 5: Decision List Models: Validation Set Re-sults3.6 Ensemble SystemsClassifier combination, where the results of differ-ent models are combined in some way to make anew model, has been well studied in the literature.A successful combined classifier can result in thecombined model outperforming the best base mod-els, as the advantages of one model make up for theshortcomings of another.Classifier combination is most successful whenthe base models are biased differently.
That condi-tion applies to our set of base models, and it wasreasonable to make an attempt at combining them.Since the performances of our systems spanneda large range, we did not want to use a simple ma-jority vote in creating the combined system.
Rather,we used a set of heuristics which trusted the mostprecise systems (the SVM and the binary maximumentropy) when they made a prediction, or a combi-nation of the others when they did not.Table 6 shows the results of the top-scoring com-bined systems which were entered as official con-testants.
As expected, the best of our combined sys-tems outperformed the best base model.4 Test Set ResultsTable 7 shows the test set results for all systemswhich participated in some way in the official com-petition, either as part of a combined system or asan individual contestant.Model Prec.
Recall Attemptedsvm, boosting, maxent (binary) (hkpolyust-all(a)) 0.874 0.867 99.2%boosting (hkpolyust-boost) 0.859 0.852 0.846%svm, boosting, maxent (binary), DL (hkpolyust-swat(a)) 0.902 0.849 94.1%svm, boosting, maxent (binary), DL, snow (hkpolyust-swat(b)) 0.908 0.846 93.2%svm, boosting, maxent (multi), DL, snow (hkpolyust-all(b)) 0.905 0.846 93.5%decision list (hkpolyust-swat-dl) 0.819 0.812 99.2%maxent (multi) 0.827 0.735 88.8%svm (hkpolyust-svm) 0.926 0.725 76.1%snow 0.713 0.499 70.0%maxent (binary) 0.935 0.454 48.6%Baseline 0.438 0.388 88.6%Table 7: Test set results for all our official systems, as well as the base models used in the ensemble system.Base Models Prec.
Recall Attemptedsvm, boosting,maxent (bin)0.901 0.803 89.2%svm, boosting,maxent (bin), snow0.938 0.8 85.2%svm, boosting,maxent (bin), DL0.926 0.783 84.6%svm, boosting,maxent (multi),DL, snow0.935 0.797 85.2%Baseline 0.444 0.396 89.2%Table 6: Combined Models: Validation Set ResultsThe top-performing system is the combined sys-tem that uses the SVM, boosting and the binary im-plementation of maximum entropy.
Of the individ-ual systems, boosting performs the best, even out-performing 3 of the combined systems.
The SVMsuffers from its high-precision approach, as does thebinary implementation of maximum entropy.
Therest of the systems fall somewhere in between.5 ConclusionThis paper presented the HKPolyU-HKUST sys-tems for the non-restricted Semantic Role Labelingtask for Senseval-3.
We mapped the task to thatof a simple classification task, and used featuresand systems which were easily extracted and con-structed.
Our systems achieved good performanceon the SRL task, easily beating the baseline.6 AcknowledgmentsThe ?hkpolyust-swat-*?
systems are the result ofjoint work between our team and Richard Wicen-towski?s team at Swarthmore College.
The authorswould like to express their immense gratitude to theSwarthmore team for providing their decision listsystem as one of our models.ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Chris-tian Boitet and Pete Whitelock, editors, Proceedingsof the Thirty-Sixth Annual Meeting of the Associa-tion for Computational Linguistics and SeventeenthInternational Conference on Computational Linguis-tics, pages 86?90, San Francisco, California.
MorganKaufmann Publishers.Bernhard E. Boser, Isabelle Guyon, and Vladimir Vap-nik.
1992.
A training algorithm for optimal marginclassifiers.
In Computational Learing Theory, pages144?152.Marine Carpuat, Weifeng Su, and Dekai Wu.
2004.Augmenting Ensemble Classification for Word SenseDisambiguation with a Kernel PCA Model.
In Pro-ceedings of Senseval-3, Barcelona.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of the35th Annual Meeting of the ACL (jointly with the 8thConference of the EACL), Madrid.Daniel Gildea and Dan Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):256?288.Marcia Mun?oz, Vasin Punyakanok, Dan Roth, and DavZimak.
1999.
A learning approach to shallow pars-ing.
In Proceedings of EMNLP-WVLC?99, pages168?178, College Park.
Association for Computa-tional Linguistics.G.
Ngai and R. Florian.
2001.
Transformation-basedlearning in the fast lane.
In Proceedings of the 39thConference of the Association for Comp utational Lin-guistics, Pittsburgh, PA.Franz Josef Och.
2002.
Yet Another Small Max-ent Toolkit: Yasmet.
http://www-i6.informatik.rwth-aachen.de/Colleagues/och.Robert E. Schapire and Yoram Singer.
2000.
Boostex-ter: A boosting-based system for text categorization.Machine Learning, 39(2/3):135?168.Richard Wicentowski, Emily Thomforde, and AdrianPackel.
2004.
The Swarthmore College Senseval-3system.
In Proceedings of Senseval-3, Barcelona.
