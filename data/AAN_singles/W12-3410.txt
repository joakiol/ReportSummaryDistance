Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 72?77,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsUsing an SVM Ensemble System for Improved Tamil Dependency ParsingNathan Green, Loganathan Ramasamy and Zdene?k Z?abokrtsky?Charles University in PragueInstitute of Formal and Applied LinguisticsFaculty of Mathematics and PhysicsPrague, Czech Republic{green,ramasamy,zabokrtsky}@ufal.mff.cuni.czAbstractDependency parsing has been shown to im-prove NLP systems in certain languages andin many cases helps achieve state of the art re-sults in NLP applications, in particular appli-cations for free word order languages.
Mor-phologically rich languages are often short ontraining data or require much higher amountsof training data due to the increased size oftheir lexicon.
This paper examines a newapproach for addressing morphologically richlanguages with little training data to start.Using Tamil as our test language, we cre-ate 9 dependency parse models with a lim-ited amount of training data.
Using thesemodels we train an SVM classifier using onlythe model agreements as features.
We usethis SVM classifier on an edge by edge deci-sion to form an ensemble parse tree.
Usingonly model agreements as features allows thismethod to remain language independent andapplicable to a wide range of morphologicallyrich languages.We show a statistically significant 5.44%improvement over the average dependencymodel and a statistically significant 0.52% im-provement over the best individual system.1 IntroductionDependency parsing has made many advancementsin recent years.
A prime reason for the quick ad-vancement has been the CoNLL shared task compe-titions, which gave the community a common train-ing/testing framework along with many open sourcesystems.
These systems have, for certain languages,achieved high accuracy ranging from on averagefrom approximately 60% to 80% (Buchholz andMarsi, 2006).
The range of scores are more of-ten language dependent rather than system depen-dent, as some languages contain more morpholog-ical complexities.
While some of these languagesare morphologically rich, we would like to addition-ally address dependency parsing methods that mayhelp under-resourced languages as well, which oftenoverlaps with morphologically rich languages.
Forthis reason, we have chosen to do the experimentsin this paper using the Tamil Treebank (Ramasamyand Z?abokrtsky?, 2012).Tamil belongs to Dravidian family of languagesand is mainly spoken in southern India and also inparts of Sri Lanka, Malaysia and Singapore.
Tamilis agglutinative and has a rich set of morphologi-cal suffixes.
Tamil has nouns and verbs as two ma-jor word classes, and hundreds of word forms canbe produced by the application of concatenative andderivational morphology.
Tamil?s rich morphologymakes the language free word order except that it isstrictly head final.When working with small datasets it is often verydifficult to determine which dependency model willbest represent your data.
One can try to pick themodel through empirical means on a tuning set butas the data grows in the future this model may nolonger be the best choice.
The change in the bestmodel may be due to new vocabulary or through adomain shift.
If the wrong single model is chosenearly on when training is cheap, when the model isapplied in semi supervised or self training it couldlead to significantly reduced annotation accuracy.72For this reason, we believe ensemble combinationsare an appropriate direction for lesser resourced lan-guages, often a large portion of morphologicallyrich languages.
Ensemble methods are robust asdata sizes grow, since the classifier can easily be re-trained with additional data and the ensemble modelchooses the best model on an edge by edge basis.This cost is substantially less than retraining multi-ple dependency models.2 Related WorkEnsemble learning (Dietterich, 2000) has been usedfor a variety of machine learning tasks and recentlyhas been applied to dependency parsing in variousways and with different levels of success.
(Surdeanuand Manning, 2010; Haffari et al, 2011) showeda successful combination of parse trees through alinear combination of trees with various weight-ing formulations.
Parser combination with depen-dency trees have been examined in terms of accu-racy (Sagae and Lavie, 2006; Sagae and Tsujii,2007; Zeman and Z?abokrtsky?, 2005; S?gaard andRish?j, 2010).
(Sagae and Lavie, 2006; Green andZ?abokrtsky?, 2012) differ in part since their methodguarantees a tree while our system can, in some sit-uations, produce a forest.
POS tags were used inparser combination in (Hall et al, 2007) for combin-ing a set of Malt Parser models with an SVM clas-sifier with success, however we believe our work isnovel in its use of an SVM classifier solely on modelagreements.
Other methods of parse combinationshave shown to be successful such as using one parserto generate features for another parser.
This wasshown in (Nivre and McDonald, 2008; Martins etal., 2008), in which Malt Parser was used as a fea-ture to MST Parser.Few attempts were reported in the literature on thedevelopment of a treebank for Tamil.
Our exper-iments are based on the openly available treebank(TamilTB) (Ramasamy and Z?abokrtsky?, 2012).
De-velopment of TamilTB is still in progress and the ini-tial results for TamilTB appeared in (Ramasamy andZ?abokrtsky?, 2011).
Previous parsing experiments inTamil were done using a rule based approach whichutilized morphological tagging and identification ofclause boundaries to parse the sentences.
The resultswere also reported for Malt Parser and MST parser.Figure 1: Process Flow for one run of our SVM Ensemblesystem.
This Process in its entirety was run 100 times foreach of the 8 data set splits.When the morphological tags were available duringboth training and testing, the rule based approachperformed better than Malt and MST parsers.
Forother Indian languages, treebank development is ac-tive mainly for Hindi and Telugu.
Dependency pars-ing results for them are reported in (Husain et al,2010).3 Methodology3.1 Process FlowWhen dealing with small data sizes it is oftennot enough to show a simple accuracy increase.This increase can be very reliant on the train-ing/tuning/testing data splits as well as the sam-pling of those sets.
For this reason our experi-ments are conducted over 7 training/tuning/testingdata split configurations.
For each configurationwe randomly sample without replacement the train-ing/tuning/testing data and rerun the experiment 100times.
These 700 runs, each on different samples,allow us to better show the overall effect on the ac-curacy metric as well as the statistically significantchanges as described in Section 3.5.
Figure 1 showsthis process flow for one run of this experiment.733.2 ParsersA dependency tree is a special case of a depen-dency graph that spawns from an artificial root, isconnected, follows a single-head constraint and isacyclic.
Because of this we can look at a large his-tory of work in graph theory to address finding thebest spanning tree for each dependency graph.
Themost common form of this type of dependency pars-ing is Graph-Based parsing also called arc-factoredparsing and deals with the parameterization of theedge weights.
The main drawback of these meth-ods is that for projective trees, the worst case sce-nario for most methods is a complexity of O(n3)(Eisner, 1996).
However, for non-projective pars-ing Chu-Liu-Edmond?s algorithm has a complexityof O(n2) (McDonald et al, 2005).
The most com-mon tool for doing this is MST parser (McDonald etal., 2005).
For this parser we generate two models,one projective and one non-projective to use in ourensemble system.Transition-based parsing creates a dependencystructure that is parameterized over the transitions.This is closely related to shift-reduce constituencyparsing algorithms.
The benefit of transition-basedparsing is the use greedy algorithms which have alinear time complexity.
However, due to the greedyalgorithms, longer arc parses can cause error propa-gation across each transition (Ku?bler et al, 2009).We make use of Malt Parser (Nivre et al, 2007),which in the CoNLL shared tasks was often tiedwith the best performing systems.
For this parserwe generate 7 different models using different train-ing parameters, seen in Table 1, and use them asinput into our ensemble system along with the twoGraph-based models described above.
Each parserhas access to gold POS information as supplied bythe TamilTB described in 3.4.Dependency parsing systems are often optimizedfor English or other major languages.
This opti-mization, along with morphological complexities,lead other languages toward lower accuracy scoresin many cases.
The goal here is to show thatwhile the corpus is not the same in size or scope ofmost CoNLL data, a successful dependency parsercan still be trained from the annotated data throughmodel combination for morphologically rich lan-guages.Training Parameter Model Descriptionnivreeager Nivre arc-eagernivrestandard Nivre arc-standardstackproj Stack projectivestackeager Stack eagerstacklazy Stack lazyplanar Planar eager2planar 2-Planar eagerTable 1: Table of the Malt Parser Parameters used duringtraining.
Each entry represents one of the parsing algo-rithms used in our experiments.
For more information seehttp://www.maltparser.org/options.html3.3 Ensemble SVM SystemWe train our SVM classifier using only model agree-ment features.
Using our tuning set, for each cor-rectly predicted dependency edge, we create(N2)features where N is the number of parsing models.We do this for each model which predicted the cor-rect edge in the tuning data.
So for N = 3 thefirst feature would be a 1 if model 1 and model 2agreed, feature 2 would be a 1 if model 1 and model3 agreed, and so on.
This feature set is novel andwidely applicable to many languages since it doesnot use any additional linguistic tools.For each edge in the ensemble graph, we use ourclassifier to predict which model should be correct,by first creating the model agreement feature setfor the current edge of the unknown test data.
TheSVM predicts which model should be correct andthis model then decides to which head the currentnode is attached.
At the end of all the tokens in asentence, the graph may not be connected and willlikely have cycles.
Using a Perl implementation ofminimum spanning tree, in which each edge has auniform weight, we obtain a minimum spanning for-est, where each subgraph is then connected and cy-cles are eliminated in order to achieve a well formeddependency structure.
Figure 2 gives a graphicalrepresentation of how the SVM decision and MSTalgorithm create a final Ensemble parse tree whichis similar to the construction used in (Hall et al,2007; Green and Z?abokrtsky?, 2012).
Future itera-tions of this process could use a multi-label SVMor weighted edges based on the parser?s accuracy ontuning data.74Figure 2: General flow to create an Ensemble parse tree3.4 Data SetsTable 2 shows the statistics of the TamilTB Tree-bank.
The last 2 rows indicate how many word typeshave unique tags and how many have two tags.
Also,Table 2 illustrates that most of the word types canbe uniquely identified with single morphological tagand only around 120 word types take more than onemorphological tag.Description Value#Sentences 600#Words 9581#Word types 3583#Tagset size 234#Types with unique tags 3461#Types with 2 tags 112Table 2: TamilTB: data statisticsSince this is a relatively small treebank and in or-der to confirm that our experiments are not heavilyreliant on one particular sample of data we try a va-riety of data splits.
To test the effects of the train-ing, tuning, and testing data we try 7 different datasplits.
The tuning data in the Section 4 use the for-mat training-tuning-testing.
So 70-20-10 means weused 70% of the TamilTB for training, 20% for tun-ing the SVM classifier, and 10% for evaluation.3.5 EvaluationMade a standard in the CoNLL shared tasks com-petition, two standard metrics for comparing depen-dency parsing systems are typically used.
Labeledattachment score (LAS) and unlabeled attachmentscore (UAS).
UAS studies the structure of a depen-dency tree and assesses whether the output has thecorrect head and dependency arcs.
In addition to thestructure score in UAS, LAS also measures the accu-racy of the dependency labels on each arc (Buchholzand Marsi, 2006).
Since we are mainly concernedwith the structure of the ensemble parse, we reportonly UAS scores in this paper.To test statistical significance we use Wilcoxonpaired signed-rank test.
For each data split we have100 iterations each with different sampling.
Eachmodel is compared against the same samples so apaired test is appropriate in this case.
We report sta-tistical significance values for p < 0.01 and p <0.05.4 Results and DiscussionData Average % Increase % IncreaseSplit SVM UAS over Avg over Best70-20-10 76.50% 5.13% 0.52%60-20-20 76.36% 5.68% 0.72%60-30-10 75.42% 5.44% 0.52%60-10-30 75.66% 4.83% 0.10%85-5-10 75.33% 3.10% -1.21%90-5-5 75.42% 3.19% -1.10%80-10-10 76.44% 4.84% 0.48%Table 3: Average increases and decreases in UAS scorefor different Training-Tuning-Test samples.
The averagewas calculated over all 9 models while the best was se-lected for each data splitFor each of the data splits, Table 3 shows the per-cent increase in our SVM system over both the av-erage of the 9 individual models and over the bestindividual model.
As the Table 3 shows, our ap-proach seems to decrease in value along with the de-crease in tuning data.
In both cases when we onlyused 5% tuning data we did not get any improve-ment in our average UAS scores.
Examining Table4, shows that the decrease in the 90-5-5 split is notstatistically significant however the decrease in 85-5-10 is a statistically significant drop.
However, theincreases in all data splits are statistically significantexcept for the 60-20-20 data split.
It appears that75Model 70-20-10 60-20-20 60-30-10 60-10-30 85-5-10 90-5-5 80-10-102planar * * * * * * **mstnonproj * * * * * * **mstproj * * * * * * **nivreeager * * * * ** x *nivrestandard * * ** x * * *planar * * * * * * **stackeager * * * x * ** *stacklazy * * * x * ** *stackproj ** * * x ** ** **Table 4: Statistical Significance Table for different Training-Tuning-Test samples.
Each experiment was sampled100 times and Wilcoxon Statistical Significance was calculated for our SVM model?s increase/decrease over eachindividual model.
?
= p < 0.01 , ?
?
p =< 0.05, x = p ?
0.05the size of the tuning and training data matter morethan the size of the test data given the low variancein Table 5.
Since the TamilTB is relatively smallwhen compared to other CoNLL treebanks, we ex-pect that this ratio may shift more when additionaldata is supplied since the amount of out of vocab-ulary, OOV, words will decrease as well.
As OOVwords decrease, we expect the use of additional testdata to have less of an effect.Data Splits SVM Variance70-20-10 0.001160-20-20 0.000560-30-10 0.001060-10-30 0.000385-5-10 0.001090-5-5 0.002880-10-10 0.0010Table 5: Variance of the UAS Scores of our EnsembleSVM System over 100 data splitsThe traditional approach of using as much data aspossible for training does not seem to be as effec-tive as partitioning more data for tuning an SVM.For instance the highest training percentage we useis 90% applied to training with 5% for tuning andtesting each.
In this case the best individual modelhad a UAS of 76.25% and the SVM had a UAS of75.42%.
One might think using 90% of the datawould achieve a higher overall UAS than using lesstraining data.
On the contrary, we achieve a betterUAS score on average using only 60%, 70%, 80%,and 85% of the data towards training.
This addi-tional data spent for tuning appears to be worth thecost.5 ConclusionWe have shown a new SVM based ensemble parserthat uses only dependency model agreement fea-tures.
The ability to use only model agreements al-lows us to keep this approach language independentand applicable to a wide range of morphologicallyrich languages.
We show a statistically significant5.44% improvement over the average dependencymodel and a statistically significant 0.52% improve-ment over the best individual system.In the future we would like to examine how ourdata splits?
results change as more data is added.This might be a prime use for self training.
Sincethe tuning data size for the SVM seems most impor-tant, the UAS may be improved by only adding selftraining data to our tuning sets.
This would have theadditional benefit of eliminating the need to retrainthe individual parsers, thus saving computation time.The tuning size may have a reduced effect for largertreebanks but in our experiments it is critical to thesmaller treebank.
Additionally, a full comparison ofvarious ensemble parsing error distributions will beneeded.6 AcknowledgmentsThis research has received funding from the Euro-pean Commission?s 7th Framework Program (FP7)under grant agreement n?
238405 (CLARA)76ReferencesSabine Buchholz and Erwin Marsi.
2006.
CoNLL-X shared task on multilingual dependency parsing.In Proceedings of the Tenth Conference on Compu-tational Natural Language Learning, CoNLL-X ?06,pages 149?164, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Thomas G. Dietterich.
2000.
Ensemble methods in ma-chine learning.
In Proceedings of the First Interna-tional Workshop on Multiple Classifier Systems, MCS?00, pages 1?15, London, UK.
Springer-Verlag.Jason Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Proceed-ings of the 16th International Conference on Com-putational Linguistics (COLING-96), pages 340?345,Copenhagen, August.Nathan Green and Zdene?k Z?abokrtsky?.
2012.
HybridCombination of Constituency and Dependency Treesinto an Ensemble Dependency Parser.
In Proceedingsof the Workshop on Innovative Hybrid Approaches tothe Processing of Textual Data, pages 19?26, Avignon,France, April.
Association for Computational Linguis-tics.Gholamreza Haffari, Marzieh Razavi, and Anoop Sarkar.2011.
An ensemble model that combines syntacticand semantic clustering for discriminative dependencyparsing.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 710?714, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryigit,Bea?ta Megyesi, Mattias Nilsson, and Markus Saers.2007.
Single Malt or Blended?
A Study in Mul-tilingual Parser Optimization.
In Proceedings of theCoNLL Shared Task Session of EMNLP-CoNLL 2007,pages 933?939.Samar Husain, Prashanth Mannem, Bharat Ram Ambati,and Phani Gadde.
2010.
The icon-2010 tools conteston indian language dependency parsing.
In Proceed-ings of ICON-2010 Tools Contest on Indian LanguageDependency Parsing, pages 1?8.Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.2009.
Dependency parsing.
Synthesis lectures on hu-man language technologies.
Morgan & Claypool, US.Andre?
F. T. Martins, Dipanjan Das, Noah A. Smith, andEric P. Xing.
2008.
Stacking dependency parsers.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?08,pages 157?166, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedings ofHuman Language Technology Conference and Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 523?530, Vancouver, British Columbia,Canada, October.
Association for Computational Lin-guistics.Joakim Nivre and Ryan McDonald.
2008.
Integratinggraph-based and transition-based dependency parsers.In Proceedings of ACL-08: HLT, pages 950?958,Columbus, Ohio, June.
Association for ComputationalLinguistics.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gulsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
MaltParser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(2):95?135.Loganathan Ramasamy and Zdene?k Z?abokrtsky?.
2011.Tamil dependency parsing: results using rule basedand corpus based approaches.
In Proceedings of the12th international conference on Computational lin-guistics and intelligent text processing - Volume Part I,CICLing?11, pages 82?95, Berlin, Heidelberg.Loganathan Ramasamy and Zdene?k Z?abokrtsky?.
2012.Prague dependency style treebank for Tamil.
In Pro-ceedings of LREC 2012, I?stanbul, Turkey.Kenji Sagae and Alon Lavie.
2006.
Parser combina-tion by reparsing.
In Proceedings of the Human Lan-guage Technology Conference of the NAACL, Com-panion Volume: Short Papers, pages 129?132, NewYork City, USA, June.
Association for ComputationalLinguistics.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependency pars-ing and domain adaptation with LR models and parserensembles.
In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL 2007, pages 1044?1050,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Anders S?gaard and Christian Rish?j.
2010.
Semi-supervised dependency parsing using generalized tri-training.
In Proceedings of the 23rd InternationalConference on Computational Linguistics (Coling2010), pages 1065?1073, Beijing, China, August.Mihai Surdeanu and Christopher D. Manning.
2010.
En-semble models for dependency parsing: cheap andgood?
In HLT: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, HLT ?10, pages 649?652,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Daniel Zeman and Zdene?k Z?abokrtsky?.
2005.
Improvingparsing accuracy by combining diverse dependencyparsers.
In In: Proceedings of the 9th InternationalWorkshop on Parsing Technologies.77
