Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 29?32,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsShared task system description:Frustratingly hard compositionality predictionAnders Johannsen, Hector Martinez Alonso, Christian Rish?j and Anders S?gaardCenter for Language TechnologyUniversity of Copenhagen{ajohannsen|alonso|crjensen|soegaard}@hum.ku.dkAbstractWe considered a wide range of features forthe DiSCo 2011 shared task about composi-tionality prediction for word pairs, includingCOALS-based endocentricity scores, compo-sitionality scores based on distributional clus-ters, statistics about wordnet-induced para-phrases, hyphenation, and the likelihood oflong translation equivalents in other lan-guages.
Many of the features we consideredcorrelated significantly with human compo-sitionality scores, but in support vector re-gression experiments we obtained the best re-sults using only COALS-based endocentric-ity scores.
Our system was nevertheless thebest performing system in the shared task, andaverage error reductions over a simple base-line in cross-validation were 13.7% for En-glish and 50.1% for German.1 IntroductionThe challenge in the DiSCo 2011 shared task is toestimate and predict the semantic compositionalityof word pairs.
Specifically, the data set consists ofadjective-noun, subject-verb and object-verb pairsin English and German.
The organizers also pro-vided the Wacky corpora for English and Germanwith lowercased lemmas.1 In addition, we also ex-perimented with wordnets and using Europarl cor-pora for the two languages (Koehn, 2005), but noneof the features based on these resources were usedin the final submission.Semantic compositionality is an ambiguous termin the linguistics litterature.
It may refer to the po-sition that the meaning of sentences is built from1http://wacky.sslmit.unibo.it/the meaning of its parts through very general prin-ciples of application, as for example in type-logicalgrammars.
It may also just refer to a typically notvery well defined measure of semantic transparencyof expressions or syntactic constructions, best illus-trated by examples:(1) pull the plug(2) educate peopleThe verb-object word pair in example (1) is inthe training data rated as much less compositionalthan example (2).
The intuition is that the mean-ing of the whole is less related to the meaning ofthe parts.
The compositionality relation is not de-fined more precisely, however, and this may in partexplain why compositionality prediction seems frus-tratingly hard.2 FeaturesMany of our features were evaluated with differentamounts of slop.
The slop parameter permits non-exact matches without resorting to language-specificshallow patterns.
The words in the compounds areallowed to move around in the sentence one positionat a time.
The value of the parameter is the maxi-mum number of steps.
Set to zero, it is equivalentto an exact match.
Below are a couple of exampleconfigurations.
Note that in order for w1 and w2 toswap positions, we must have slop > 1 since slop=1would place them on top of each other.x x w1 w2 x x (slop=0)x x w1 x w2 x (slop=1)x x w1 x x w2 (slop=2)x x w2 w1 x x (slop=2)292.1 LEFT-ENDOC, RIGHT-ENDOC andDISTR-DIFFThese features measure the endocentricity of a wordpair w1 w2.
The distribution of w1 is likely to besimilar to the distribution of ?w1 w2?
if w1 is thesyntactic head of ?w1 w2?.
The same is to be ex-pected for w2, when w2 is the head.Syntactic endocentricity is related to composi-tionality, but the implication is one-way only.
Ahighly compositional compound is endocentric, butan endocentric compound need not be highly com-positional.
For example, the distribution of ?oliveoil?, which is endocentric and highly compositional,is very similar to the distribution of ?oil?, the headword.
On the other hand, ?golden age?
which isranked as highly non-compositional in the trainingdata, is certainly endocentric.
The distribution of?golden age?
is not very different from that of ?age?.We used COALS (Rohde et al, 2009) to cal-culate word distributions.
The COALS algorithmbuilds a word-to-word semantic space from a cor-pus.
We used the implementation by Jurgens andStevens (2010), generating the semantic space fromthe Wacky corpora for English and German with du-plicate sentences removed and low-frequency wordssubstituted by dummy symbols.
The word pairshave been fed to COALS as compounds that have tobe treated as single tokens, and the semantic spacehas been generated and reduced using singular valuedecompositon.
The vectors for w1, w2 and ?w1 w2?are calculated, and we compute the cosine distancebetween the semantic space vectors for the wordpair and its parts, and between the parts themselves,namely for ?w1 w2?
and w1, for ?w1 w2?
and w2,and for w1 and w2, say for ?olive oil?
and ?olive?,for ?olive oil?
and ?oil?, and for ?olive?
and ?oil?.LEFT-ENDOC is the cosine distance between the leftword and the compound.
RIGHT-ENDOC is the co-sine distance between the right word and the com-pound.
Finally, DISTR-DIFF is the cosine distancebetween the two words, w1 and w2.2.2 BR-COMPTo accommodate for the weaknesses of syntactic en-docentricity features, we also tried introducing com-positionality scores based on hierarchical distribu-tional clusters that would model semantic composi-tionality more directly.
The scores are referred tobelow as BR-COMP (compositionality scores basedon Brown clusters), and the intuition behind thesescores is that a word pair ?w1 w2?, e.g.
?hot dog?, isnon-compositional if w1 and w2 have high colloca-tional strength, but if w1 is replaced with a differentword w?1 with similar distribution, e.g.
?warm?, then?w?1 w2?
is less collocational.
Similarly, if w2 is re-placed with a different word w?2 with similar distri-bution, e.g.
?terrier?, then ?w1 w?2?
is also much lesscollocational than ?w1 w2?.We first induce a hierarchical clustering of thewords in the Wacky corpora cl : W ?
2W withW the set of words in our corpora, using publiclyavailable software.2 Let the collocational strength ofthe two words w1 and w2 be G2(w1, w2).
We thencompute the average collocational strength of distri-butional clusters, BR-CS (collocational strength ofBrown clusters):BR-CS(w1, w2) =?Nx?cl(w1),x?
?cl(w2)G2(x, x?
)Nwith N = |cl(w1)| ?
|cl(w2)|.
We now letBR-COMP(w1, w2) =BR-CS(w1,w2)G2(w1,w2).The Brown clusters were built with C = 1000and a cut-off frequency of 1000.
With these settingsthe number of word types per cluster is quite high,which of course has a detrimental effect on the se-mantic coherence of the cluster.
To counter this wechoose to restrict cl(w) and cl(w?)
to include onlythe 50 most frequently occurring terms.2.3 PARAPHRThese features have to do with alternative phrasingsusing synonyms from Princeton WordNet 3 and Ger-maNet4.
One word in the compound is held con-stant while the other is replaced with its synonyms.The intuition is again that non-compositional com-pounds are much more frequent than any compoundthat results from replacing one of the constituentwords with one of its synonyms.
For ?hot dog?
wethus generate ?hot terrier?
and ?warm dog?, but not?warm terrier?.
Specifically, PARAPHR?100 means2http://www.cs.berkeley.edu/?pliang/software/3http://wordnet.princeton.edu/4GermaNet Copyright c?
1996, 2008 by University ofTu?bingen.30that at least one of the alternative compounds hasa document count of more than 100 in the cor-pus.
PARAPHRav is the average count for all para-phrases, PARAPHRsum is the sum of these counts,and PARAPHRrel is the average count for all para-phrases over the count of the word pair in question.2.4 HYPHThe HYPH features were inspired by Bergsma etal.
(2010).
It was only used for English.
Specif-ically, we used the relative frequency of hyphen-ated forms as features.
For adjective-noun pairswe counted the number of hyphenated occurrences,e.g.
?front-page?, and divided that number by thenumber of non-hyphenated occurrences, e.g.
?frontpage?.
For subject-verb and object-verb pairs, weadd -ing to the verb, e.g.
?information-collecting?,and divided the number of such forms with non-hyphenated equivalents, e.g.
?information collect-ing?.2.5 TRANS-LENThe intuition behind our bilingual features is thatnon-compositional words typically translate into asingle word or must be paraphrased using multiplewords (circumlocution or periphrasis).
TRANS-LENis the probability that the phrase?s translation, possi-bly with intervening articles and markers, is longerthan lmin and shorter than lmax , i.e.
:TRANS-LEN(w1, w2, lmin , lmax ) =??
?trans(w1 w2),l1?|?
|?l2P (?|w1 w2)??
?trans(w1 w2)P (?|w1 w2)We use English and German Europarl (Koehn,2005) to train our translation models.
In particular,we use the phrase tables of the Moses PB-SMT sys-tem5 trained on a lemmatized version of the WMT11parallel corpora for English and German.
BelowTRANS-LEN-n will be the probability of the trans-lation of a word pair being n or more words.
Wealso experimented with average translation length asa feature, but this did not correlate well with seman-tic compositionality.5http://statmt.orgfeat ?English Germanrel-type = ADJ NN 0.0750 *0.1711rel-type = V SUBJ 0.0151 **0.2883rel-type = V OBJ 0.0880 0.0825LEFT-ENDOC **0.3257 *0.1637RIGHT-ENDOC **0.3896 0.1379DISTR-DIFF *0.1885 0.1128HYPH (5) 0.1367 -HYPH (5) reversed *0.1829 -G2 0.1155 0.0535BR-CS *0.1592 0.0242BR-COMP 0.0292 0.0024Count (5) 0.0795 *0.1523PARAPHR?|w1 w?2| 0.1123 0.1242PARAPHRrel (5) 0.0906 0.0013PARAPHRav (1) 0.1080 0.0743PARAPHRav (5) 0.1313 0.0707PARAPHRsum (1) 0.0496 0.0225PARAPHR?100 (1) **0.2434 0.0050PARAPHR?100 (5) **0.2277 0.0198TRANS-LEN-1 0.0797 0.0509TRANS-LEN-2 0.1109 0.0158TRANS-LEN-3 0.0935 0.0489TRANS-LEN-5 0.0240 0.0632Figure 1: Correlations.
Coefficients marked with * aresignificant (p < 0.05), and coefficients marked with **are highly significant (p < 0.01).
We omit features withdifferent slop values if they perform significantly worsethan similar features.3 CorrelationsWe have introduced five different kinds of features,four of which are supposed to model semantic com-positionality directly.
For feature selection, wetherefore compute the correlation of features withcompositionality scores and select features that cor-relate significantly with compositionality.
The fea-tures are then used for regression experiments.4 Regression experimentsFor our regression experiments, we use support vec-tor regression with a high (7) degree kernel.
Other-wise we use default parameters of publicly availablesoftware.6 In our experiments, however, we werenot able to produce substantially better results thanwhat can be obtained using only the features LEFT-ENDOC and RIGHT-ENDOC.
In fact, for Germanusing only LEFT-ENDOC gave slightly better resultsthan using both.
These features are also those thatcorrelate best with human compositionality scoresaccording to Figure 1.
Consequently, we only use6http://www.csie.ntu.edu.tw/?cjlin/libsvm/31these features in our official runs.
Our evaluationsbelow are cross-validation results on training and de-velopment data using leave-one-out.
We compareusing only LEFT-ENDOC and RIGHT-ENDOC (forEnglish) with using all significant features that seemrelatively independent.
For English, we used LEFT-ENDOC, RIGHT-ENDOC, DISTR-DIFF, HYPH (5)reversed, BR-CS, PARAPHR?100 (1).
For German,we used rel-type = ADJ NN, rel-type=V SUBJ andRIGHT-ENDOC.
We only optimized on numericscores.
The submitted coarse-grained scores wereobtained using average +/- average deviation.7English Germandev test dev testBL 18.395 47.123all sign.
indep.
19.22 23.02L-END+R-END 15.89 16.19 23.51 24.03err.red (L+R) 0.137 0.5015 DiscussionOur experiments have shown that the DiSCo 2011shared task about compositionality prediction was atough challenge.
This may be because of the fine-grained compositionality metric or because of in-consistencies in annotation, but note also that thesyntactically oriented features seem to perform alot better than those trying to single out semanticcompositionality from syntactic endocentricity andcollocational strength.
For example, LEFT-ENDOC,RIGHT-ENDOC and BR-CS correlate with compo-sitionality scores, whereas BR-COMP does not, al-though it is supposed to model compositionalitymore directly.
Could it perhaps be that annotationsreflect syntactic endocentricity or distributional sim-ilarity to a high degree, rather than what is typicallythought of as semantic compositionality?Consider a couple of examples of adjective-nounpairs in English in Figure 2 for illustration.
Theseexamples are taken from the training data, but wehave added our subjective judgments about semanticand syntactic markedness and collocational strength(peaking at G2 scores).
It seems that semanticmarkedness is less important for scores than syntac-7These thresholds were poorly chosen, by the way.
Had wechosen less balanced cut-offs, say 0 and 72, our improved accu-racy on coarse-grained scores (59.4) would have been compara-ble to and slightly better than the best submitted coarse-grainedscores (58.5).sem syn coll scorefloppy disk X 61free kick X 77happy birthday X X 47large scale X X 55old school X X X 37open source X X 49real life X 69small group 91Figure 2: Subjective judgments about semantic and syn-tactic markedness and collocational strength.tic markedness and collocational strength.
In partic-ular, the combination of syntactic markedness andcollocational strength makes annotators rank wordpairs such as happy birthday and open source asnon-compositional, although they seem to be fullycompositional from a semantic perspective.
Thismay explain why our COALS-features are so predic-tive of human compositionality scores, and why G2correlates better with these scores than BR-COMP.6 ConclusionsIn our experiments for the DiSCo 2011 shared taskwe have considered a wide range of features andshowed that some of them correlate significantly andsometimes highly significantly with human compo-sitionality scores.
In our regression experiments,however, our best results were obtained with onlyone or two COALS-based endocentricity features.We report error reductions of 13.7% for English and50.1% for German.ReferencesShane Bergsma, Aditya Bhargava, Hua He, and GrzegorzKondrak.
2010.
Predicting the semantic composition-ality of prefix verbs.
In EMNLP.David Jurgens and Keith Stevens.
2010.
The S-Spacepackage: an open source package for word space mod-els.
In ACL.Philipp Koehn.
2005.
Europarl: a parallel corpus forstatistical machine translation.
In MT-Summit.Douglas Rohde, Laura Gonnerman, and David Plaut.2009.
An improved model of semantic similaritybased on lexical co-occurrence.
In Cognitive Science.32
