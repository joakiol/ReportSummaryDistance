Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 975?980,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsThe Effects of Data Size and Frequency Rangeon Distributional Semantic ModelsMagnus SahlgrenGavagai and SICSSlussplan 9, Box 1263111 30 Stockholm, 164 29 KistaSwedenmange@[gavagai|sics].seAlessandro LenciUniversity of Pisavia Santa Maria 3656126 PisaItalyalessandro.lenci@unipi.itAbstractThis paper investigates the effects of data sizeand frequency range on distributional seman-tic models.
We compare the performance ofa number of representative models for severaltest settings over data of varying sizes, andover test items of various frequency.
Our re-sults show that neural network-based modelsunderperform when the data is small, and thatthe most reliable model over data of varyingsizes and frequency ranges is the inverted fac-torized model.1 IntroductionDistributional Semantic Models (DSMs) have be-come a staple in natural language processing.
Thevarious parameters of DSMs ?
e.g.
size of con-text windows, weighting schemes, dimensionalityreduction techniques, and similarity measures ?have been thoroughly studied (Weeds et al, 2004;Sahlgren, 2006; Riordan and Jones, 2011; Bulli-naria and Levy, 2012; Levy et al, 2015), and arenow well understood.
The impact of various pro-cessing models ?
matrix-based models, neural net-works, and hashing methods ?
have also enjoyedconsiderable attention lately, with at times conflict-ing conclusions (Baroni et al, 2014; Levy et al,2015; Schnabel et al, 2015; O?sterlund et al, 2015;Sahlgren et al, 2016).
The consensus interpretationof such experiments seems to be that the choice ofprocessing model is less important than the parame-terization of the models, since the various processingmodels all result in more or less equivalent DSMs(provided that the parameterization is comparable).One of the least researched aspects of DSMs isthe effect on the various models of data size andfrequency range of the target items.
The only pre-vious work in this direction that we are aware ofis Asr et al (2016), who report that on small data(the CHILDES corpus), simple matrix-based mod-els outperform neural network-based ones.
Unfor-tunately, Asr et al do not include any experimentsusing the same models applied to bigger data, mak-ing it difficult to compare their results with previousstudies, since implementational details and parame-terization will be different.There is thus still a need for a consistent and faircomparison of the performance of various DSMswhen applied to data of varying sizes.
In this pa-per, we seek an answer to the question: which DSMshould we opt for if we only have access to lim-ited amounts of data?
We are also interested in therelated question: which DSM should we opt for ifour target items are infrequent?
The latter ques-tion is particularly crucial, since one of the major as-sets of DSMs is their applicability to create seman-tic representations for ever-expanding vocabulariesfrom text feeds, in which new words may continu-ously appear in the low-frequency ranges.In the next section, we introduce the contend-ing DSMs and the general experiment setup, beforeturning to the experiments and our interpretation ofthe results.
We conclude with some general advice.2 Distributional Semantic ModelsOne could classify DSMs in many different ways,such as the type of context and the method to builddistributional vectors.
Since our main goal here is975to gain an understanding of the effect of data sizeand frequency range on the various models, we fo-cus primarily on the differences in processing mod-els, hence the following typology of DSMs.Explicit matrix modelsWe here include what could be referred to as ex-plicit models, in which each vector dimension cor-responds to a specific context (Levy and Goldberg,2014).
The baseline model is a simple co-occurrencematrix F (in the following referred to as CO for Co-Occurrence).
We also include the model that resultsfrom applying Positive Pointwise Mutual Informa-tion (PPMI) to the co-occurrence matrix.
PPMI is de-fined as simply discarding any negative values of thePMI, computed as:PMI(a, b) = log fab ?
Tfafb (1)where fab is the co-occurrence count of word a andword b, fa and fb are the individual frequencies ofthe words, and T is the number of tokens in thedata.1Factorized matrix modelsThis type of model applies an additional factor-ization of the weighted co-occurrence counts.
Wehere include two variants of applying Singular ValueDecomposition (SVD) to the PPMI-weighting co-occurrence matrix; one version that discards all butthe first couple of hundred latent dimensions (TSVDfor truncated SVD), and one version that instead re-moves the first couple of hundred latent dimensions(ISVD for inverted SVD).
SVD is defined in the stan-dard way:F = U?V T (2)where U holds the eigenvectors of F , ?
holds theeigenvalues, and V ?
U(w) is a unitary matrix map-ping the original basis of F into its eigenbasis.
SinceV is redundant due to invariance under unitary trans-formations, we can represent the factorization of F?in its most compact form F?
?
U?.1We also experimented with smoothed PPMI, which raisesthe context counts to the power of ?
and normalizes them (Levyet al, 2015), thereby countering the tendency of mutual infor-mation to favor infrequent events: f(b) = #(b)?
?b#(b)?, but it didnot lead to any consistent improvements compared to PPMI.Hashing modelsA different approach to reduce the dimensionality ofDSMs is to use a hashing method such as RandomIndexing (RI) (Kanerva et al, 2000), which accumu-lates distributional vectors ~d(a) in an online fashion:~d(a)?
~d(ai)+c?j=?c,j 6=0w(x(i+j))pij~r(x(i+j)) (3)where c is the extension of the context window, w(b)is a weight that quantifies the importance of contextterm b,2 ~rd(b) is a sparse random index vector thatacts as a fingerprint of context term b, and pij is a per-mutation that rotates the random index vectors onestep to the left or right, depending on the position ofthe context items within the context windows, thusenabling the model to take word order into account(Sahlgren et al, 2008).Neural network modelsThere are many variations of DSMs that use neuralnetworks as processing model, ranging from simplerecurrent networks (Elman, 1990) to more complexdeep architectures (Collobert and Weston, 2008).The incomparably most popular neural networkmodel is the one implemented in the word2vec li-brary, which uses the softmax for predicting b givena (Mikolov et al, 2013):p(b|a) = exp(~b ?
~a)?b?
?C exp(~b?
?
~a)(4)where C is the set of context words, and~b and ~a arethe vector representations for the context and targetwords, respectively.
We include two versions of thisgeneral model; Continuous Bag of Words (CBOW)that predicts a word based on the context, and Skip-Gram Negative Sampling (SGNS) that predicts thecontext based on the current word.3 Experiment setupSince our main focus in this paper is the perfor-mance of the above-mentioned DSMs on data of2We use w(b) = e???
f(b)V where f(b) is the frequency ofcontext item b, V is the total number of unique context itemsseen thus far (i.e.
the current size of the growing vocabulary),and ?
is a constant that we set to 60 (Sahlgren et al, 2016).976varying sizes, we use one big corpus as startingpoint, and split the data into bins of varying sizes.We opt for the ukWaC corpus (Ferraresi et al, 2008),which comprises some 1.6 billion words after to-kenization and lemmatization.
We produce sub-corpora by taking the first 1 million, 10 million, 100million, and 1 billion words.Since the co-occurrence matrix built from the1 billion-word ukWaC sample is very big (morethan 4,000,000 ?
4,000,000), we prune the co-occurrence matrix to 50,000 dimensions before thefactorization step by simply removing infrequentcontext items.3 As comparison, we use 200 di-mensions for TSVD, 2,800 (3,000-200) dimensionsfor ISVD, 2,000 dimensions for RI, and 200 dimen-sions for CBOW and SGNS.
These dimensionalitieshave been reported to perform well for the respec-tive models (Landauer and Dumais, 1997; Sahlgrenet al, 2008; Mikolov et al, 2013; O?sterlund et al,2015).
All DSMs use the same parameters as far aspossible with a narrow context window of?2 words,which has been shown to produce good results in se-mantic tasks (Sahlgren, 2006; Bullinaria and Levy,2012).We use five standard benchmark tests in theseexperiments; two multiple-choice vocabulary tests(the TOEFL synonyms and the ESL synonyms),and three similarity/relatedness rating benchmarks(SimLex-999 (SL) (Hill et al, 2015), MEN (Bruniet al, 2014), and Stanford Rare Words (RW) (Luonget al, 2013)).
The vocabulary tests measure the syn-onym relation, while the similarity rating tests mea-sure a broader notion of semantic similarity (SL andRW) or relatedness (MEN).4 The results for the vo-cabulary tests are given in accuracy (i.e., percentageof correct answers), while the results for the similar-ity tests are given in Spearman rank correlation.4 Comparison by data sizeTable 1 summarizes the results over the different testsettings.
The most notable aspect of these results3Such drastic reduction has a negative effect on the perfor-mance of the factorized methods for the 1 billion word data, butunfortunately is necessary for computational reasons.4It is likely that the results on the similarity tests could beimproved by using a wider context window, but such improve-ment would probably be consistent across all models, and isthus outside the scope of this paper.DSM TOEFL ESL SL MEN RW1 million wordsCO 17.50 20.00 ?1.64 10.72 ?3.96PPMI 26.25 18.00 8.28 21.49 ?2.57TSVD 27.50 20.00 4.43 22.15 ?1.56ISVD 22.50 14.00 14.33 19.74 5.31RI 20.00 16.00 5.65 17.94 1.92SGNS 15.00 8.00 3.64 12.34 1.46CBOW 15.00 10.00 ?0.16 11.59 1.3910 million wordsCO 40.00 22.00 4.77 15.20 0.95PPMI 52.50 38.00 26.44 39.83 4.00TSVD 38.75 30.00 19.27 34.33 5.53ISVD 45.00 44.00 30.19 44.21 9.88RI 47.50 24.00 20.44 34.56 3.32SGNS 43.75 42.00 28.30 26.59 2.38CBOW 40.00 30.00 22.22 28.33 3.04100 million wordsCO 45.00 30.00 10.00 19.36 3.12PPMI 66.25 54.00 33.75 46.74 15.05TSVD 46.25 34.00 25.11 42.49 13.00ISVD 66.25 66.00 40.98 54.55 21.27RI 55.00 48.00 32.31 45.71 10.15SGNS 65.00 58.00 40.75 52.83 11.73CBOW 61.25 46.00 36.15 48.30 15.621 billion wordsCO 55.00 40.00 11.85 21.83 6.82PPMI 71.25 54.00 35.69 52.95 24.29TSVD 56.25 46.00 31.36 52.05 13.35ISVD 71.25 66.00 44.77 60.11 28.46RI 61.25 50.00 35.35 50.51 18.58SGNS 76.25 66.00 41.94 67.03 24.50CBOW 75.00 56.00 38.31 59.84 22.80Table 1: Results for DSMs trained on data of varying sizes.is that the neural networks models do not producecompetitive results for the smaller data, which cor-roborates the results by Asr et al (2016).
The bestresults for the smallest data are produced by the fac-torized models, with both TSVD and ISVD produc-ing top scores in different test settings.
It shouldbe noted, however, that even the top scores for thesmallest data set are substandard; only two models(PPMI and TSVD) manage to beat the random base-line of 25% for the TOEFL tests, and none of themodels manage to beat the random baseline for theESL test.The ISVD model produces consistently good re-sults; it yields the best overall results for the 10 mil-9770204060801001M 10M 100M 1G,COPMITSVDISVDRISGNSCBOWFigure 1: Average results and standard deviation over all tests.lion and 100 million-word data, and is competitivewith SGNS on the 1 billion word data.
Figure 1shows the average results and their standard devi-ations over all test settings.5 It is obvious that thereare no huge differences between the various models,with the exception of the baseline CO model, whichconsistently underperforms.
The TSVD and RI mod-els have comparable performance across the differ-ent data sizes, which is systematically lower than thePPMI model.
The ISVD model is the most consis-tently good model, with the neural network-basedmodels steadily improving as data becomes bigger.Looking at the different datasets, SL and RW arethe hardest ones for all the models.
In the case ofSL, this confirms the results in (Hill et al, 2015),and might be due to the general bias of DSMs to-wards semantic relatedness, rather than genuine se-mantic similarity, as represented in SL.
The substan-dard performance on RW might instead be due to thelow frequency of the target items.
It is interesting tonote that these are benchmark tests in which neuralmodels perform the worst even when trained on thelargest data.5 Comparison by frequency rangeIn order to investigate how each model handles dif-ferent frequency ranges, we split the test items intothree different classes that contain about a third ofthe frequency mass of the test items each.
This5Although rank correlation is not directly comparable withaccuracy, they are both bounded between zero and one, whichmeans we can take the average to get an idea about overall per-formance.split was produced by collecting all test items intoa common vocabulary, and then sorting this vo-cabulary by its frequency in the ukWaC 1 billion-word corpus.
We split the vocabulary into 3 equallylarge parts; the HIGH range with frequencies rang-ing from 3,515,086 (?do?)
to 16,830 (?organism?
),the MEDIUM range with frequencies ranging be-tween 16,795 (?desirable?)
and 729 (?prickly?
), andthe LOW range with frequencies ranging between728 (?boardwalk?)
to hapax legomenon.
We thensplit each individual test into these three ranges, de-pending on the frequencies of the test items.
Testpairs were included in a given frequency class if andonly if both the target and its relatum occur in thefrequency range for that class.
For the constituentwords in the test item that belong to different fre-quency ranges, which is the most common case, weuse a separate MIXED class.
The resulting fourclasses contain 1,387 items for the HIGH range,656 items for the MEDIUM range, 350 items forthe LOW range, and 3,458 items for the MIXEDrange.6Table 2 (next side) shows the average results overthe different frequency ranges for the various DSMstrained on the 1 billion-word ukWaC data.
We alsoinclude the highest and lowest individual test scores(signified by ?
and ?
), in order to get an idea aboutthe consistency of the results.
As can be seen inthe table, the most consistent model is ISVD, whichproduces the best results in both the MEDIUMand MIXED frequency ranges.
The neural net-work models SGNS and CBOW produce the best re-sults in the HIGH and LOW range, respectively,with CBOW clearly outperforming SGNS in the lat-ter case.
The major difference between these mod-els is that CBOW predicts a word based on a con-text, while SGNS predicts a context based on a word.Clearly, the former approach is more beneficial forlow-frequent items.The PPMI, TSVD and RI models perform simi-larly across the frequency ranges, with RI produc-ing somewhat lower results in the MEDIUM range,and TSVD producing somewhat lower results in theLOW range.
The CO model underperforms in allfrequency ranges.
Worth noting is the fact that allmodels that are based on an explicit matrix (i.e.
CO,6233 test terms did not occur in the 1 billion-word corpus.978DSM HIGH MEDIUM LOW MIXEDCO 32.61 (?62.5,?04.6) 35.77 (?66.6,?21.2) 12.57 (?35.7,?00.0) 27.14 (?56.6,?07.9)PPMI 55.51 (?75.3,?28.0) 57.83 (?88.8,?18.7) 25.84 (?50.0,?00.0) 47.73 (?83.3,?27.1)TSVD 50.52 (?70.9,?23.2) 54.75 (?77.9,?24.1) 17.85 (?50.0,?00.0) 41.08 (?56.6,?19.6)ISVD 63.31 (?87.5,?36.5) 69.25 (?88.8,?46.3) 10.94 (?16.0,?00.0) 57.24 (?83.3,?33.0)RI 53.11 (?62.5,?30.1) 48.02 (?72.2,?20.4) 23.29 (?39.0,?00.0) 46.39 (?66.6,?21.0)SGNS 68.81 (?87.5,?36.4) 62.00 (?83.3,?27.4) 18.76 (?42.8,?00.0) 56.93 (?83.3,?30.2)CBOW 62.73 (?81.2,?31.9) 59.50 (?83.3,?32.4) 27.13 (?78.5,?00.0) 52.21 (?76.6,?25.9)Table 2: Average results for DSMs over four different frequency ranges for the items in the TOEFL, ESL, SL, MEN, and RW tests.All DSMs are trained on the 1 billion words data.PPMI, TSVD and ISVD) produce better results in theMEDIUM range than in the HIGH range.The arguably most interesting results are in theLOW range.
Unsurprisingly, there is a gen-eral and significant drop in performance for lowfrequency items, but with interesting differencesamong the various models.
As already mentioned,the CBOW model produces the best results, closelyfollowed by PPMI and RI.
It is noteworthy that thelow-dimensional embeddings of the CBOW modelonly gives a modest improvement over the high-dimensional explicit vectors of PPMI.
The worst re-sults are produced by the ISVD model, which scoreseven lower than the baseline CO model.
This mightbe explained by the fact that ISVD removes the la-tent dimensions with largest variance, which are ar-guably the most important dimensions for very low-frequent items.
Increasing the number of latent di-mensions with high variance in the ISVD model im-proves the results in the LOW range (16.59 whenremoving only the top 100 dimensions).6 ConclusionOur experiments confirm the results of Asr etal.
(2016), who show that neural network-basedmodels are suboptimal to use for smaller amounts ofdata.
On the other hand, our results also show thatnone of the standard DSMs work well in situationswith small data.
It might be an interesting novel re-search direction to investigate how to design DSMsthat are applicable to small-data scenarios.Our results demonstrate that the inverted factor-ized model (ISVD) produces the most robust resultsover data of varying sizes, and across several dif-ferent test settings.
We interpret this finding as fur-ther corroborating the results of Bullinaria and Levy(2012), and O?sterlund et al (2015), with the con-clusion that the inverted factorized model is a robustcompetitive alternative to the widely used SGNS andCBOW neural network-based models.We have also investigated the performance of thevarious models on test items in different frequencyranges, and our results in these experiments demon-strate that all tested models perform optimally in themedium-to-high frequency ranges.
Interestingly, allmodels based on explicit count matrices (CO, PPMI,TSVD and ISVD) produce somewhat better results foritems of medium frequency than for items of highfrequency.
The neural network-based models andISVD, on the other hand, produce the best results forhigh-frequent items.None of the tested models perform optimallyfor low-frequent items.
The best results for low-frequent test items in our experiments were pro-duced using the CBOW model, the PPMI model andthe RI model, all of which uses weighted contextitems without any explicit factorization.
By contrast,the ISVD model underperforms significantly for thelow-frequent items, which we suggest is an effect ofremoving latent dimensions with high variance.This interpretation suggests that it might be inter-esting to investigate hybrid models that use differentprocessing models ?
or at least different parame-terizations ?
for different frequency ranges, and fordifferent data sizes.
We leave this as a suggestion forfuture research.7 AcknowledgementsThis research was supported by the Swedish Re-search Council under contract 2014-28199.979ReferencesFatemeh Asr, Jon Willits, and Michael Jones.
2016.Comparing predictive and co-occurrence based mod-els of lexical semantics trained on child-directedspeech.
In Proceedings of CogSci.Marco Baroni, Georgiana Dinu, and Germa?n Kruszewski.2014.
Don?t count, predict!
a systematic compari-son of context-counting vs. context-predicting seman-tic vectors.
In Proceedings of ACL, pages 238?247.Elia Bruni, Nam Khanh Tran, and Marco Baroni.
2014.Multimodal distributional semantics.
Journal of Arti-ficial Intelligence Research, 49(1):1?47, January.John Bullinaria and Joseph P. Levy.
2012.
Extract-ing semantic representations from word co-occurrencestatistics: stop-lists, stemming, and svd.
Behavior Re-search Methods, 44:890?907.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceedingsof ICML, pages 160?167.Jeffrey L. Elman.
1990.
Finding structure in time.
Cog-nitive Science, 14:179?211.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, andSilvia Bernardini.
2008.
Introducing and evaluatingukwac, a very large web-derived corpus of english.Proceedings of WAC-4, pages 47?54.Felix Hill, Roi Reichart, and Anna Korhonen.
2015.Simlex-999: Evaluating semantic models with (gen-uine) similarity estimation.
Computational Linguis-tics, 41(4):665?695.Pentti Kanerva, Jan Kristofersson, and Anders Holst.2000.
Random indexing of text samples for latent se-mantic analysis.
In Proceedings of CogSci, page 1036.Thomas K Landauer and Susan T. Dumais.
1997.
A so-lution to platos problem: The latent semantic analysistheory of acquisition, induction, and representation ofknowledge.
Psychological Review, 104(2):211?240.Omer Levy and Yoav Goldberg.
2014.
Linguistic regu-larities in sparse and explicit word representations.
InProceedings of CoNLL, pages 171?180.Omer Levy, Yoav Goldberg, and Ido Dagan.
2015.
Im-proving distributional similarity with lessons learnedfrom word embeddings.
Transactions of the Associa-tion for Computational Linguistics, 3:211?225.Minh-Thang Luong, Richard Socher, and Christopher D.Manning.
2013.
Better word representations with re-cursive neural networks for morphology.
In Proceed-ings of CoNLL, pages 104?113.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-rado, and Jeff Dean.
2013.
Distributed representationsof words and phrases and their compositionality.
InProceedings of NIPS, pages 3111?3119.Arvid O?sterlund, David O?dling, and Magnus Sahlgren.2015.
Factorization of latent variables in distributionalsemantic models.
In Proceedings of EMNLP, pages227?231.Brian Riordan and Michael N. Jones.
2011.
Redun-dancy in perceptual and linguistic experience: Com-paring feature-based and distributional models of se-mantic representation.
Topics in Cognitive Science,3(2):303?345.Magnus Sahlgren, Anders Holst, and Pentti Kanerva.2008.
Permutations as a means to encode order inword space.
In Proceedings of CogSci, pages 1300?1305.Magnus Sahlgren, Amaru Cuba Gyllensten, FredrikEspinoza, Ola Hamfors, Anders Holst, Jussi Karl-gren, Fredrik Olsson, Per Persson, and AkshayViswanathan.
2016.
The Gavagai Living Lexicon.
InProceedings of LREC.Magnus Sahlgren.
2006.
The Word-Space Model.
Phdthesis, Stockholm University.Tobias Schnabel, Igor Labutov, David Mimno, andThorsten Joachims.
2015.
Evaluation methods forunsupervised word embeddings.
In Proceedings ofEMNLP, pages 298?307.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributional simi-larity.
In Proceedings of COLING, pages 1015?1021.980
