Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 815?823,Beijing, August 2010Nonparametric Word Segmentation for Machine TranslationThuyLinh Nguyen Stephan Vogel Noah A. SmithLanguage Technologies InstituteCarnegie Mellon University{thuylinh,vogel,nasmith}@cs.cmu.eduAbstractWe present an unsupervised word seg-mentation model for machine translation.The model uses existing monolingual seg-mentation techniques and models the jointdistribution over source sentence segmen-tations and alignments to the target sen-tence.
During inference, the monolin-gual segmentation model and the bilin-gual word alignment model are coupledso that the alignments to the target sen-tence guide the segmentation of the sourcesentence.
The experiments show improve-ments on Arabic-English and Chinese-English translation tasks.1 IntroductionIn statistical machine translation, the smallest unitis usually the word, defined as a token delimitedby spaces.
Given a parallel corpus of source andtarget text, the training procedure first builds aword alignment, then extracts phrase pairs fromthis word alignment.
However, in some languages(e.g., Chinese) there are no spaces between words.The same problem arises when translating be-tween two very different languages, such as froma language with rich morphology like Hungarianor Arabic to a language with poor morphologylike English or Chinese.
A single word in a mor-phologically rich language is often the composi-tion of several morphemes, which correspond toseparate words in English.11We will use the terms word segmentation, morphologi-cal analysis, and tokenization more or less interchangeably.Often some preprocessing is applied involvingword segmentation or morphological analysis ofthe source and/or target text.
Such preprocess-ing tokenizes the text into morphemes or words,which linguists consider the smallest meaning-bearing units of the language.
Take as an ex-ample the Arabic word ?fktbwha?
and its En-glish translation ?so they wrote it?.
The preferredsegmentation of ?fktbwha?
would be ?f-ktb-w-ha(so-wrote-they-it),?
which would allow for a one-to-one mapping between tokens in the two lan-guages.
However, the translation of the phrase inHebrew is ?wktbw ath?.
Now the best segmen-tation of the Arabic words would be ?fktbw-ha,?corresponding to the two Hebrew words.
This ex-ample shows that there may not be one correctsegmentation that can be established in a prepro-cessing step.
Rather, tokenization depends on thelanguage we want to translate into and needs tobe tied in with the alignment process.
In short,we want to find the tokenization yielding the bestalignment, and thereby the best translation sys-tem.We propose an unsupervised tokenizationmethod for machine translation by formulating agenerative Bayesian model to ?explain?
the bilin-gual training data.
Generation of a sentence pairis described as follows: first a monolingual to-kenization model generates the source sentence,then the alignment model generates the target sen-tence through the alignments with the source sen-tence.
Breaking this generation process into twosteps provides flexibility to incorporate existingmonolingual morphological segmentation mod-els such as those of Mochihashi et al (2009) orCreutz and Lagus (2007).
Using nonparametric815models and the Bayesian framework makes it pos-sible to incorporate linguistic knowledge as priordistributions and obtain the posterior distributionthrough inference techniques such as MCMC orvariational inference.As new test source sentences do not have trans-lations which can help to infer the best segmenta-tion, we decode the source string according to theposterior distribution from the inference step.In summary, our segmentation technique con-sists of the following steps:?
A joint model of segmented source text andits target translation.?
Inference of the posterior distribution of themodel given the training data.?
A decoding algorithm for segmenting sourcetext.?
Experiments in translation using the prepro-cessed source text.Our experiments show that the proposed seg-mentation method leads to improvements onArabic-English and Chinese-English translationtasks.In the next section we will discuss related work.Section 3 will describe our model in detail.
Theinference will be covered in Section 4, and decod-ing in Section 5.
Experiments and results will bepresented in Section 6.2 Related WorkThe problem of segmentation for machine trans-lation has been studied extensively in recent lit-erature.
Most of the work used some linguisticknowledge about the source and the target lan-guages (Nie?en and Ney, 2004; Goldwater andMcClosky, 2005).
Sadat and Habash (2006) ex-perimented with a wide range of tokenizationschemes for Arabic-English translation.
Theseexperiments further show that even for a singlelanguage pair, different tokenizations are neededdepending on the training corpus size.
The ex-periments are very expensive to conduct and donot generalize to other language pairs.
Recently,Dyer (2009) created manually crafted lattices fora subset of source words as references for seg-mentation when translating into English, and thenlearned the segmentation of the source words tooptimize the translation with respect to these ref-erences.
He showed that the parameters of themodel can be applied to similar languages whentranslating into English.
However, manually cre-ating these lattices is time-consuming and requiresa bilingual person with some knowledge of the un-derlying statistical machine translation system.There have been some attempts to apply un-supervised methods for tokenization in machinetranslation (Chung and Gildea, 2009; Xu et al,2008).
The alignment model of Chung andGildea (2009) forces every source word to alignwith a target word.
Xu et al (2008) mod-eled the source-to-null alignment as in the sourceword to target word model.
Their models arespecial cases of our proposed model when thesource model2 is a unigram model.
Like Xu etal.
(2008), we use Gibbs sampling for inference.Chung and Gildea (2009) applied efficient dy-namic programming-based variational inferencealgorithms.We benefit from existing unsupervised mono-lingual segmentation.
The source model uses thenested Pitman-Yor model as described by Mochi-hashi et al (2009).
When sampling each potentialword boundary, our inference technique is a bilin-gual extension of what is described by Goldwateret al (2006) for monolingual segmentation.Nonparametric models have received attentionin machine translation recently.
For example,DeNero et al (2008) proposed a hierarchicalDirichlet process model to learn the weights ofphrase pairs to address the degeneration in phraseextraction.
Teh (2006) used a hierarchical Pitman-Yor process as a smoothing method for languagemodels.Recent work on multilingual language learningsuccessfully used nonparametric models for lan-guage induction tasks such as grammar induction(Snyder et al, 2009; Cohen et al, 2010), morpho-logical segmentation (Goldwater et al, 2006; Sny-der and Barzilay, 2008), and part-of-speech tag-ging (Goldwater and Griffiths, 2007; Snyder et al,2Note that ?source model?
here means a model of sourcetext, not a source model in the noisy channel paradigm.8162008).3 ModelsWe start with the generative process for a sourcesentence and its alignment with a target sentence.Then we describe individual models employed bythis generation scheme.3.1 Generative StoryA source sentence is a sequence of word tokens,and each word is either aligned or not aligned.
Wefocus only on the segmentation problem and notreordering source words; therefore, the model willnot generate the order of the target word tokens.A sentence pair and its alignment are captured byfour components:?
a sequence of words in the source sentence,?
a set of null-aligned source tokens,?
a set of null-aligned target tokens, and?
a set of (source word to target word) align-ment pairs.We will start with a high-level story of how thesegmentation of the source sentence and the align-ment are generated.1.
A source language monolingual segmenta-tion model generates the source sentence.2.
Generate alignments:(a) Given the sequence of words of thesource sentence already generated instep 1, the alignment model marks eachsource word as either aligned or un-aligned.
If a source word is aligned, themodel also generates the target word.
(b) Unaligned target words are generated.The model defines the joint probability of a seg-mented source language sentence and its align-ment.
During inference, the two parts are cou-pled, so that the alignment will influence whichsegmentation is selected.
However, there are sev-eral advantages in breaking the generation processinto two steps.First of all, in principle the model can incor-porate any existing probabilistic monolingual seg-mentation to generate the source sentence.
Forexample, the source model can be the nestedPitman-Yor process as described by Mochihashi etal.
(2009), the minimum description length modelpresented by Creutz and Lagus (2007), or some-thing else.
Also the source model can incorporatelinguistic knowledge from a rule-based or statisti-cal morphological disambiguator.The model generates the alignment after thesource sentence with word boundaries alreadygenerated.
Therefore, the alignment model canbe any existing word alignment model (Brownet al, 1993; Vogel et al, 1996).
Even thoughthe choices of source model or alignment modelcan lead to different inference methods, the modelwe propose here is highly extensible.
Note thatwe assume that the alignment consists of at mostone-to-one mappings between source and targetwords, with null alignments possible on bothsides.Another advantage of a separate source modellies in the segmentation of an unseen test set.
Insection 5 we will show how to apply the sourcemodel distribution learned from training data tofind the best segmentation of an unseen test set.Notation and ParametersWe will use bold font for a sequence or bagsof words and regular font for an individual word.A source sentence s is a sequence of |s| wordssi:(s1, .
.
.
, s|s|); the translation of sentence s isthe target sentence t of |t| words (t1, .
.
.
, t|t|).In sentence s the list of unaligned words is snaland the list of aligned source words is sal.
Inthe target sentence t the list of unaligned wordsis tnal and the list of target words having one-to-one alignment with source words sal is tal.The alignment a of s and t is represented by{?si, null?
| si ?
snal} ?
{?si, tai?
| si ?
sal; tai ?tal} ?
{?null, tj?
| tj ?
tnal} where ai denotesthe index in t of the word aligned to si.The probability of a sequence or a set is denotedby P (.
), probability at the word level is p (.).
Forexample, the probability of sentence s is P (s), theprobability of a word s is p (s), the probabilitythat the target word t aligns to an aligned source817word s is p (t |s).A sentence pair and its alignment are generatedfrom the following models:?
The source model generates sentence s withprobability P (s).?
The source-to-null alignment model de-cides independently for each word swhether it is unaligned with probabilityp (null | si) or aligned with probabil-ity: 1 ?
p (null | si).
The probabilityof this step, for all source words, is:P (snal, sal | s) = ?si?snal p (null | si) ?
?si?sal (1 ?
p (null | si)) .We will also refer to the source-to-null modelas the deletion model, since words in snal areeffectively deleted for the purposes of align-ment.?
The source-to-target algnment model gen-erates a bag of target words tal alignedto the source words sal with probability:P (tal |sal) = ?si?sal;tai?tal p (tai |si).
Notethat we do not need to be concerned withgenerating a explicitly, since we do notmodel word order on the target side.?
The null-to-target algnment model gen-erates the list of unaligned target wordstnal given aligned target words tal withP (tnal |tal) as follows:?
Generate the number of unaligned tar-get words |tnal| given the number ofaligned target words |tal| with probabil-ity P (|tnal| | |tal|).?
Generate |tnal| unaligned words t ?tnal independently, each with probabil-ity p (t |null).The resulting null-to-target proba-bility is therefore: P (tnal | tal) =P (|tnal| | |tal|)?t?tnal p (t |null) .We also call the null-to-target model the in-sertion model.The above generation process defines the jointprobability of source sentence s and its alignmenta as follows:P (s, a) = P (s)???
?source model?
P (a | s)?
??
?alignment model(1)P (a | s) = P (tal |sal) ?
P (tnal |tal) (2)?
?si?snalp (null | si) ?
?si?sal(1 ?
p (null | si))3.2 Source ModelOur generative process provides the flexibility ofincorporating different monolingual models intothe probability distribution of a sentence pair.In particular we use the existing state-of-the-artnested Pitman-Yor n-gram language model as de-scribed by Mochihashi et al (2009).
The proba-bility of s is given byP (s) = P (|s|)|s|?i=1p (si |si?n, .
.
.
, si?1) (3)where the n-gram probability is a hierarchicalPitman-Yor language model using (n ?
1)-gramas the base distribution.At the unigram level, the model uses the basedistribution p (s) as the infinite-gram character-level Pitman-Yor language model.3.3 Modeling Null-Aligned Source WordsThe probability that a source word aligns to nullp (null | s) is defined by a binomial distributionwith Beta prior Beta (?p, ?
(1 ?
p)), where ?and p are model parameters.
When p ?
0 and?
?
?
the probability p (null | s) converges to 0forcing each source words align to a target word.We fixed p = 0.1 and ?
= 20 in our experiment.Xu et al (2008) view the null word as anothertarget word, hence in their model the probabilitythat a source word aligns to null can only dependon itself.By modeling the source-to-null alignment sep-arately, our model lets the distribution dependon the word?s n-gram context as in the sourcemodel.
p (null | si?n, .
.
.
, si) stands for the prob-ability that the word si is not aligned given its con-text (si?n, .
.
.
, si?1).The n-gram source-to-null distributionp (null | si?n, .
.
.
, si) is defined similarly to818p (null | si) definition above in which the basedistribution p now becomes the (n ?
1)-gram:p (null | si?n+1, .
.
.
, si).33.4 Source-Target Alignment ModelThe probability p (t |s) that a target word t alignsto a source word s is a Pitman-Yor process:t | s ?
PY (d, ?, p0 (t |s))here d and ?
are the input parameters, andp0 (t |s) is the base distribution.Let |s, ?| denote the number of times s is alignedto any t in the corpus and let |s, t| denote the num-ber of times s is aligned to t anywhere in the cor-pus.
And let ty(s) denote the number of differenttarget words t the word s is aligned to anywherein the corpus.
In the Chinese Restaurant Processmetaphor, there is one restaurant for each sourceword s, the s restaurant has ty(s) tables and total|s, ?| customers; table t has |s, t| customers.Then, at a given time in the generative processfor the corpus, we can write the probability that tis generated by the word s as:?
if |s, t| > 0:p (t |s) =|s, t| ?
d + [?
+ dty(s)]p0 (t |s)|s, ?| + ??
if |s, t| = 0:p (t |s) = [?
+ dty(s)]p0 (t |s)|s, ?| + ?For language pairs with similar character setssuch as English and French, words with similarsurface form are often translations of each other.The base distribution can be defined based onthe edit distance between two words (Snyder andBarzilay, 2008).We are working with diverse language pairs(Arabic-English and Chinese-English), so weuse the base distribution as the flat distributionp0 (t |s) = 1T ; T is the number of distinct targetwords in the training set.
In our experiment, themodel parameters are ?
= 20 and d = .5.3We also might have conditioned this decision on wordsfollowing si, since those have all been generated already atthis stage.3.5 Modeling Null-Aligned Target WordsThe null-aligned target words are modeled condi-tioned on previously generated target words as:P (tnal |tal) = P (|tnal| | |tal|)?t?tnalp (t |null)This model uses two probability distributions:?
the number of unaligned target words:P (|tnal| | |tal|), and?
the probability that each word in tnal is gen-erated by null: p (t |null).We model the number of unaligned targetwords similarly to the distribution in the IBM3word alignment model (Brown et al, 1993).IBM3 assumes that each aligned target words gen-erates a null-aligned target word with probabil-ity p0 and fails to generate a target word withprobability 1 ?
p0.
So the parameter p0 canbe used to control the number of unaligned tar-get words.
In our experiments, we fix p0 =.05.
Following this assumption, the probability of|tnal| unaligned target words generated from |tal|words is: P (|tnal| | |tal|) =( |tal||tnal|)p|tnal|0 (1 ?p0)|tal|?|tnal|.The probability that a target word t aligns tonull, p (t |null), also has a Pitman-Yor processprior.
The base distribution of the model is similarto the source-to-target model?s base distributionwhich is the flat distribution over target words.4 InferenceWe have defined a probabilistic generative modelto describe how a corpus of alignments and seg-mentations can be generated jointly.
In this sec-tion we discuss how to obtain the posterior distri-butions of the missing alignments and segmenta-tions given the training corpus, using Gibbs sam-pling.Suppose we are provided a morphologicaldisambiguator for the source language such asMADA morphology tokenization toolkit (Sadatand Habash, 2006) for Arabic.4 The morpho-logical disambiguator segments a source word to4MADA provides several segmentation schemes; amongthem the MADA-D3 scheme seeks to separate all mor-phemes of each word.819morphemes of smallest meaning-bearing units ofthe source language.
Therefore, a target word isequivalent to one or several morphemes.
Givena morphological disambiguation toolkit, we useits output to bias our inference by not consider-ing word boundaries after every character but onlyconsidering potential word boundaries as a subsetof the morpheme boundaries set.
In this way, theinference uses the morphological disambiguationtoolkit to limit its search space.The inference starts with an initial segmenta-tion of the source corpus and also its alignmentto the target corpus.
The Gibbs sampler consid-ers one potential word boundary at a time.
Thereare two hypotheses at any given boundary posi-tion of a sentence pair (s, t): the merge hypothe-sis stands for no word boundary and the resultingsource sentence smerge has a word s spanning overthe sample point; the split hypothesis indicates theresulting source sentence ssplit has a word bound-ary at the sample point separating two words s1s2.Similar to Goldwater et al (2006) for monolingualsegmentation, the sampler randomly chooses theboundary according to the relative probabilities ofthe merge hypothesis and the split hypothesis.The model consists of source and alignmentmodel variables; given the training corpora size ofa machine translation system, the number of vari-ables is large.
So if the Gibbs sampler samplesboth source variables and alignment variables, theinference requires many iterations until the sam-pler mixes.
Xu et al (2008) fixed this by repeat-edly applying GIZA++ word alignment after eachsampling iteration through the training corpora.Our inference technique is not precisely Gibbssampling.
Rather than sampling the alignment orattempting to collapse it out (by summing overall possible alignments when calculating the rel-ative probabilities of the merge and split hypothe-ses), we seek the best alignment for each hypoth-esis.
In other words, for each hypothesis, we per-form a local search for a high-probability align-ment of the merged word or split words, giventhe rest of alignment for the sentence.
Up to oneword may be displaced and realigned.
This ?local-best?
alignment is used to score the hypothesis,and after sampling merge or split, we keep thatbest alignment.This inference technique is motivated by run-time demands, but we do not yet know of a the-oretical justification for combining random stepswith maximization over some variables.
A morecomplete analysis is left to future work.5 Decoding for Unseen Test SentencesSection 4 described how to get the model?s pos-terior distribution and the segmentation and align-ment of the training data under the model.
We areleft with the problem of decoding or finding thesegmentation of test sentences where the transla-tions are not available.
This is needed when wewant to translate new sentences.
Here, tokeniza-tion is performed as a preprocessing step, decou-pled from the subsequent translation steps.The decoding step uses the model?s posteriordistribution for the training data to segment un-seen source sentences.
Because of the clear sep-aration of the source model and the alignmentmodel, the source model distribution learned fromthe Gibbs sampling directly represents the distri-bution over the source language and can thereforealso handle the segmentation of unknown wordsin new test sentences.
Only the source model isused in preprocessing.The best segmentation s?
of a string of charac-ters c = (c1, .
.
.
, c|c|) according to the n-gramsource model is:s?
= argmaxs from cp (|s|)i=|s|?i=1p (si |si?n, .
.
.
, si?1)We use a stochastic finite-state machine for de-coding.
This is possible by composition of the fol-lowing two finite state machines:?
Acceptor Ac.
The string of characters c isrepresented as an finite state acceptor ma-chine where any path through the machinerepresents an unweighted segmentation of c.?
Source model weighted finite state trans-ducer Lc.
Knight and Al-Onaizan (1998)show how to build an n-gram languagemodel by a weighted finite state machine.The states of the transducer are (n ?
1)-gram history, the edges are words from thelanguage.
The arc si coming from state820(si?n, .
.
.
, si?1) to state (si?n+1, .
.
.
, si) hasweight p (si |si?n, .
.
.
, si?1).The best segmentation s?
is given as s?
=BestPath(Ac ?
Lc).6 ExperimentsThis section presents experimental results onArabic-English and Chinese-English translationtasks using the proposed segmentation technique.6.1 Arabic-EnglishAs a training set we use the BTEC corpus dis-tributed by the International Workshop on Spo-ken Language Translation (IWSLT) (Matthias andChiori, 2005).
The corpus is a collection ofconversation transcripts from the travel domain.The ?Supplied Data?
track consists of nearly 20KArabic-English sentence pairs.
The developmentset consists of 506 sentences from the IWSLT04evaluation test set and the unseen set consists of500 sentences from the IWSLT05 evaluation testset.
Both development set and test set have 16 ref-erences per Arabic sentence.6.2 Chinese-EnglishThe training set for Chinese-English translationtask is also distributed by the IWSLT evaluationcampaign.
It consists of 67K Chinese-Englishsentence pairs.
The development set and the testset each have 489 Chinese sentences and each sen-tence has 7 English references.6.3 ResultsWe will report the translation results where thepreprocessing of the source text are our unigram,bigram, and trigram source models and source-to-null model.The MCMC inference algorithm starts with aninitial segmentation of the source text into fullword forms.
For Chinese, we use the originalword segmentation as distributed by IWSLT.
Toget an initial alignment, we generate the IBM4Viterbi alignments in both directions using theGIZA++ toolkit (Och and Ney, 2003) and com-bine them using the ?grow-diag-final-and?
heuris-tic.
The output of combining GIZA++ align-ment for a sentence pair is a sequence of si-tjentries where i is an index of the source sen-tence and j is an index of the target sentence.As our model allows only one-to-one mappingsbetween the words in the source and target sen-tences, we remove si-tj from the sequence if ei-ther the source word si or target word tj is al-ready in a previous entry of the combined align-ment sequence.
The resulting alignment is our ini-tial alignment for the inference.We also apply the MADA morphology seg-mentation toolkit (Habash and Rambow, 2005) topreprocess the Arabic corpus.
We use the D3scheme (each Arabic word is segmented into mor-phemes in sequence [CONJ+ [PART+ [Al+ BASE+PRON]]]), mark the morpheme boundaries, andthen combine the morphemes again to have wordsin their original full word form.
During inference,we only sample over these morpheme boundariesas potential word boundaries.
In this way, welimit the search space, allowing only segmenta-tions consistent with MADA-D3.The inference samples 150 iterations throughthe whole training set and uses the posterior prob-ability distribution from the last iteration for de-coding.
The decoding process is then appliedto the entire training set as well as to the devel-opment and test sets to generate a consistent to-kenization across all three data sets.
We usedthe OpenFST toolkit (Allauzen et al, 2007) forfinite-state machine implementation and opera-tions.
The output of the decoding is the pre-processed data for translation.
We use the opensource Moses phrase-based MT system (Koehn etal., 2007) to test the impact of the preprocessingtechnique on translation quality.56.3.1 Arabic-English Translation ResultsWe consider the Arabic-English setting.
Weuse two baselines: original full word formand MADA-D3 tokenization scheme for Arabic-English translation.
Table 1 compares the trans-lation results of our segmentation methods withthese baselines.
Our segmentation method showsimprovement over the two baselines on both thedevelopment and test sets.
According to Sadatand Habash (2006), the MADA-D3 scheme per-5The Moses translation alignment is the output ofGIZA++, not from our MCMC inference.821Dev.
TestOriginal 59.21 54.00MADA-D3 58.28 54.92Unigram 59.44 56.18Bigram 58.88 56.18Trigram 58.76 56.82Table 1: Arabic-English translation results(BLEU).forms best for their Arabic-English translation es-pecially for small and moderate data sizes.
In ourexperiments, we see an improvement when usingthe MADA-D3 preprocessing over using the orig-inal Arabic corpus on the unseen test set, but noton the development set.The Gibbs sampler only samples on the mor-phology boundary points of MADA-D3, so theimprovement resulting from our segmentationtechnique does not come from removing unknownwords.
It is due to a better matching betweenthe source and target sentences by integrating seg-mentation and alignment.
We therefore expect thesame impact on a larger training data set in futureexperiments.6.3.2 Chinese-English Translation ResultsDev.
TestWhole word 23.75 29.02Character 23.39 27.74Unigram 24.90 28.97Trigram 23.98 28.20Table 2: Chinese-English translation result inBLEU score metric.We next consider the Chinese-English setting.The translation performance using our word seg-mentation technique is shown in Table 2.
Thereare two baselines for Chinese-English translation:(a) the source text in the full word form distributedby the IWSLT evaluation and (b) no segmentationof the source text, which is equivalent to interpret-ing each Chinese character as a single word.Taking development and test sets into account,the best Chinese-English translation system re-sults from our unigram model.
It is significantlybetter than other systems on the development setand performs almost equally well with the IWSLTsegmentation on the test set.
Note that the seg-mentation distributed by IWSLT is a manual seg-mentation for the translation task.Chung and Gildea (2009) and Xu et al (2008)also showed improvement over a simple mono-lingual segmentation for Chinese-English trans-lation.
Our character-based translation result iscomparable to their monolingual segmentations.Both trigram and unigram translation results out-perform the character-based translation.We also observe that there are no additionalgains for Chinese-English translation when usinga higher n-gram model.
Our Gibbs sampler hasthe advantage that the samples are guaranteed toconverge eventually to the model?s posterior dis-tributions, but in each step the modification to thecurrent hypothesis is small and local.
In itera-tions 100?150, the average number of boundarychanges for the unigram model is 14K boundariesversus only 1.5K boundary changes for the tri-gram model.
With 150 iterations, the inferenceoutput of trigram model might not yet representits posterior distribution.
We leave a more de-tailed investigation of convergence behavior to fu-ture work.Conclusion and Future WorkWe presented an unsupervised segmentationmethod for machine translation and presentedexperiments for Arabic-English and Chinese-English translation tasks.
The model can incor-porate existing monolingual segmentation mod-els and seeks to learn a segmenter appropriate fora particular translation task (target language anddataset).AcknowledgementsWe thank Kevin Gimpel for interesting discus-sions and technical advice.
We also thank theanonymous reviewers for useful feedback.
Thiswork was supported by DARPA Gale project,NSF grants 0844507 and 0915187.822ReferencesAllauzen, C., M. Riley, J. Schalkwyk, W. Skut, andM.
Mohri.
2007.
OpenFst: A General and EfficientWeighted Finite-State Transducer Library.
In Pro-ceedings of the CIAA 2007, volume 4783 of LectureNotes in Computer Science, pages 11?23.
Springer.http://www.openfst.org.Brown, Peter F., Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation: Pa-rameter Estimation.
Comput.
Linguist., 19(2):263?311.Chung, T. and D. Gildea.
2009.
Unsupervised Tok-enization for Machine Translation.
In Proceedingsof EMNLP 2009, pages 718?726, Singapore, Au-gust.
Association for Computational Linguistics.Cohen, S. B., D. M. Blei, and N. A. Smith.
2010.
Vari-ational Inference for Adaptor Grammars.
In Pro-ceedings of NAACL-HLT, pages 564?572, June.Creutz, Mathias and Krista Lagus.
2007.
Unsu-pervised Models for Morpheme Segmentation andMorphology Learning.
ACM Trans.
Speech Lang.Process., 4(1):1?34.DeNero, J., A.
Bouchard-Co?te?, and D. Klein.
2008.Sampling Alignment Structure under a BayesianTranslation Model.
In Proceedings of EMNLP2008, pages 314?323, Honolulu, Hawaii, October.Association for Computational Linguistics.Dyer, C. 2009.
Using a Maximum Entropy model tobuild segmentation lattices for MT.
In Proceedingsof HLT 2009, pages 406?414, Boulder, Colorado,June.Goldwater, S. and T. L. Griffiths.
2007.
A FullyBayesian Approach to Unsupervised Part-of-SpeechTagging.
In Proceedings of ACL.Goldwater, S. and D. McClosky.
2005.
Improving Sta-tistical Machine Translation Through Morphologi-cal Analysis.
In Proc.
of EMNLP.Goldwater, S., T. L. Griffiths, and M. Johnson.
2006.Contextual Dependencies in Unsupervised WordSegmentation.
In Proc.
of COLING-ACL.Habash, N. and O. Rambow.
2005.
Arabic Tok-enization, Part-of-Speech Tagging, and Morpholog-ical Disambiguation in One Fell Swoop.
In Proc.
ofACL.Knight, K. and Y. Al-Onaizan.
1998.
Translationwith Finite-State Devices.
In Proceedings of AMTA,pages 421?437.Koehn, P., H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Con-stantin, and E. Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Proc.of ACL (demo session).Matthias, E. and H. Chiori.
2005.
Overview of theIWSLT 2005 Evaluation Campaign.
In Proceedingsof IWSLT.Mochihashi, D., T. Yamada, and N. Ueda.
2009.Bayesian Unsupervised Word Segmentation withNested Pitman-Yor Language Modeling.
In Pro-ceedings of 47th ACL, pages 100?108, Suntec, Sin-gapore, August.Nie?en, S. and H. Ney.
2004.
Statistical MachineTranslation with Scarce Resources Using Morpho-Syntactic Information.
Computational Linguistics,30(2), June.Och, F. and H. Ney.
2003.
A Systematic Comparisonof Various Statistical Alignment Models.
Computa-tional Linguistics, 29(1).Sadat, F. and N. Habash.
2006.
Combination of Ara-bic Preprocessing Schemes for Statistical MachineTranslation.
In Proceedings of the ACL, pages 1?8.Snyder, B. and R. Barzilay.
2008.
Unsupervised Mul-tilingual Learning for Morphological Segmentation.In Proceedings of ACL-08: HLT, pages 737?745,June.Snyder, B., T. Naseem, J. Eisenstein, and R. Barzilay.2008.
Unsupervised Multilingual Learning for POSTagging.
In Proceedings of EMNLP.Snyder, B., T. Naseem, and R. Barzilay.
2009.
Unsu-pervised Multilingual Grammar Induction.
In Pro-ceedings of ACL-09, pages 73?81, August.Teh, Y. W. 2006.
A Hierarchical Bayesian LanguageModel Based On Pitman-Yor Processes.
In Pro-ceedings of ACL, pages 985?992, July.Vogel, S., H. Ney, and C. Tillmann.
1996.
HMM-Based Word Alignment in Statistical Translation.
InProceedings of COLING, pages 836?841.Xu, J., J. Gao, K. Toutanova, and H. Ney.
2008.Bayesian Semi-Supervised Chinese Word Segmen-tation for Statistical Machine Translation.
InProceedings of (Coling 2008), pages 1017?1024,Manchester, UK, August.823
