Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 141?144,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005A Generalized Alignment-Free Phrase ExtractionBing ZhaoLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA-15213bzhao@cs.cmu.eduStephan VogelLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA-15213vogel+@cs.cmu.eduAbstractIn this paper, we present a phrase ex-traction algorithm using a translation lex-icon, a fertility model, and a simple dis-tortion model.
Except these models, wedo not need explicit word alignments forphrase extraction.
For each phrase pair (ablock), a bilingual lexicon based score iscomputed to estimate the translation qual-ity between the source and target phrasepairs; a fertility score is computed to es-timate how good the lengths are matchedbetween phrase pairs; a center distortionscore is computed to estimate the relativeposition divergence between the phrasepairs.
We presented the results and ourexperience in the shared tasks on French-English.1 IntroductionPhrase extraction becomes a key component in to-day?s state-of-the-art statistical machine translationsystems.
With a longer context than unigram, phrasetranslation models have flexibilities of modelling lo-cal word-reordering, and are less sensitive to the er-rors made from preprocessing steps including wordsegmentations and tokenization.
However, most ofthe phrase extraction algorithms rely on good wordalignments.
A widely practiced approach explainedin details in (Koehn, 2004), (Och and Ney, 2003)and (Tillmann, 2003) is to get word alignments fromtwo directions: source to target and target to source;the intersection or union operation is applied to getrefined word alignment with pre-designed heuristicsfixing the unaligned words.
With this refined wordalignment, the phrase extraction for a given sourcephrase is essentially to extract the target candidatephrases in the target sentence by searching the leftand right projected boundaries.In (Vogel et al, 2004), they treat phrase align-ment as a sentence splitting problem: given a sourcephrase, find the boundaries of the target phrase suchthat the overall sentence alignment lexicon probabil-ity is optimal.
We generalize it in various ways, esp.by using a fertility model to get a better estimation ofphrase lengths, and a phrase level distortion model.In our proposed algorithm, we do not need ex-plicit word alignment for phrase extraction.
Therebyit avoids the burden of testing and comparing differ-ent heuristics especially for some language specificones.
On the other hand, the algorithm has such flex-ibilities that one can incorporate word alignment andheuristics in several possible stages within this pro-posed framework to further improve the quality ofphrase pairs.
In this way, our proposed algorithmis more generalized than the usual word alignmentbased phrase extraction algorithms.The paper is structured as follows: in section 2,The concept of blocks is explained; in section 3, adynamic programming approach is model the widthof the block; in section 4, a simple center distortionof the block; in section 5, the lexicon model; thecomplete algorithm is in section 6; in section 7, ourexperience and results using the proposed approach.2 BlocksWe consider each phrase pair as a block within agiven parallel sentence pair, as shown in Figure 1.The y-axis is the source sentence, indexed wordby word from bottom to top; the x-axis is the targetsentence, indexed word by word from left to right.The block is defined by the source phrase and its pro-jection.
The source phrase is bounded by the startand the end positions in the source sentence.
Theprojection of the source phrase is defined as the leftand right boundaries in the target sentence.
Usually,the boundaries can be inferred according to wordalignment as the left most and right most alignedpositions from the words in the source phrase.
In141StartEndRight boundaryLeft boundaryWidthsrc centertgt centerFigure 1: Blocks with ?width?
and ?centers?this paper, we provide another view of the block,which is defined by the centers of source and targetphrases, and the width of the target phrase.Phrase extraction algorithms in general searchfor the left and right projected boundaries of eachsource phrase according to some score metric com-puted for the given parallel sentence pairs.
Wepresent here three models: a phrase level fertilitymodel score for phrase pairs?
length mismatch, asimple center-based distortion model score for thedivergence of phrase pairs?
relative positions, anda phrase level translation score to approximate thephrase pairs?
translational equivalence.
Given asource phrase, we can search for the best possibleblock with the highest combined scores from thethree models.3 Length Model: Dynamic ProgrammingGiven the word fertility definitions in IBM Mod-els (Brown et al, 1993), we can compute a prob-ability to predict phrase length: given the candi-date target phrase (English) eI1, and a source phrase(French) of length J , the model gives the estima-tion of P (J |eI1) via a dynamic programming algo-rithm using the source word fertilities.
Figure 2shows an example fertility trellis of an English tri-gram.
Each edge between two nodes represents oneEnglish word ei.
The arc between two nodes rep-resents one candidate non-zero fertility for ei.
Thefertility of zero (i.e.
generating a NULL word) cor-responds to the direct edge between two nodes, andin this way, the NULL word is naturally incorpo-rated into this model?s representation.
Each arc ise1 e2 e31320 020e1 e2 e3??
?.123431312Figure 2: An example of fertility trellis for dynamicprogrammingassociated with a English word fertility probabilityP (?i|ei).
A path ?I1 through the trellis representsthe number of French words ?i generated by eachEnglish word ei.
Thus, the probability of generatingJ words from the English phrase along the Viterbipath is:P (J |eI1) = max{?I1,J=?Ii=1 ?i}I?i=1P (?i|ei) (1)The Viterbi path is inferred via dynamic program-ming in the trellis of the lower panel in Figure 2:?
[j, i] = max????????
[j, i ?
1] + log PNULL(0|ei)?
[j ?
1, i ?
1] + log P?(1|ei)?
[j ?
2, i ?
1] + log P?(2|ei)?
[j ?
3, i ?
1] + log P?
(3|ei)where PNULL(0|ei) is the probability of generatinga NULL word from ei; P?
(k = 1|ei) is the usualword fertility model of generating one French wordfrom the word ei; ?
[j, i] is the cost so far for gener-ating j words from i English words ei1 : e1, ?
?
?
, ei.After computing the cost of ?
[J, I], we can traceback the Viterbi path, along which the probabilityP (J |eI1) of generating J French words from the En-glish phrase eI1 as shown in Eqn.
1.142With this phrase length model, for every candidateblock, we can compute a phrase level fertility scoreto estimate to how good the phrase pairs are matchin their lengthes.4 Distortion of CentersThe centers of source and target phrases are both il-lustrated in Figure 1.
We compute a simple distor-tion score to estimate how far away the two centersare in a parallel sentence pair in a sense the block isclose to the diagonal.In our algorithm, the source center fj+lj of thephrase f j+lj with length l +1 is simply a normalizedrelative position defined as follows:fj+lj =1|F |j?=j+l?j?=jj?l + 1 (2)where |F | is the French sentence length.For the center of English phrase ei+ki in the targetsentence, we first define the expected correspondingrelative center for every French word fj?
using thelexicalized position score as follows:ei+ki (fj?)
=1|E| ??
(i+k)i?=i i?
?
P (fj?
|ei?)?
(i+k)i?=i P (fj?
|ei?
)(3)where |E| is the English sentence length.
P (fj?
|ei)is the word translation lexicon estimated in IBMModels.
i is the position index, which is weightedby the word level translation probabilities; the termof ?Ii=1 P (fj?
|ei) provides a normalization so thatthe expected center is within the range of target sen-tence length.
The expected center for ei+ki is simplya average of ei+ki (fj?
):ei+ki =1l + 1j+l?j?=jei+ki (fj?)
(4)This is a general framework, and one can certainlyplug in other kinds of score schemes or even wordalignments to get better estimations.Given the estimated centers of fj+lj andei+ki , we can compute how close they are bythe probability of P (ei+ki |fj+lj ).
To estimateP (ei+ki |fj+lj ), one can start with a flat gaussianmodel to enforce the point of (ei+ki ,fj+lj ) not toofar off the diagonal and build an initial list of phrasepairs, and then compute the histogram to approxi-mate P (ei+ki |fj+lj ).5 Lexicon ModelSimilar to (Vogel et al, 2004), we compute for eachcandidate block a score within a given sentence pairusing a word level lexicon P (f |e) as follows:P (f j+lj |ei+ki ) =?j??[j,j+l]?i??
[i,i+k]P (fj?
|ei?
)k + 1??j?
/?[j,j+l]?i?
/?
[i,i+k]P (fj?
|ei?
)|E| ?
k ?
16 AlgorithmOur phrase extraction is described in Algorithm1.
The input parameters are essentially from IBMModel-4: the word level lexicon P (f |e), the Englishword level fertility P?
(?e = k|e), and the centerbased distortion P (ei+ki |fj+lj ).Overall, for each source phrase f j+lj , the algo-rithm first estimates its normalized relative centerin the source sentence, its projected relative cen-ter in the target sentence.
The scores of the phraselength, center-based distortion, and a lexicon basedscore are computed for each candidate block A lo-cal greedy search is carried out for the best scoredphrase pair (f j+lj , ei+ki ).In our submitted system, we computed thefollowing seven base scores for phrase pairs:Pef (f j+lj |ei+ki ), Pfe(ei+ki |f j+lj ), sharing similarfunction form in Eqn.
5.Pef (f j+lj |ei+ki ) =?j?
?i?P (fj?
|ei?
)P (ei?
|ei+ki )=?j?
?i?P (fj?
|ei?
)k + 1 (5)We compute phrase level relative frequency in bothdirections: Prf (f j+lj |ei+ki ) and Prf (ei+ki |f j+lj ).
Wecompute two other lexicon scores which were alsoused in (Vogel et al, 2004): S1(f j+lj |ei+ki ) andS2(ei+ki |fj+lj ) using the similar function in Eqn.
6:S(f j+lj |ei+ki ) =?j?
?i?P (fj?
|ei?)
(6)143In addition, we put the phrase level fertility scorecomputed in section 3 via dynamic programming tobe as one additional score for decoding.Algorithm 1 A Generalized Alignment-free PhraseExtraction1: Input: Pre-trained models: P?
(?e = k|e) ,P (E |F ) , and P (f |e).2: Output: PhraseSet: Phrase pair collections.3: Loop over the next sentence pair4: for j : 0 ?
|F | ?
1,5: for l : 0 ?
MaxLength,6: foreach f j+lj7: compute f and E8: left = E ?
|E|-MaxLength,9: right= E ?
|E|+MaxLength,10: for i : left ?
right,11: for k : 0 ?
right,12: compute e of ei+ki ,13: score the phrase pair (f j+lj , ei+ki ), wherescore = P (e|f )P (l|ei+ki )P (f j+lj |ei+ki )14: add top-n {(f j+lj , ei+ki )} into PhraseSet.7 Experimental ResultsOur system is based on the IBM Model-4 param-eters.
We train IBM Model 4 with a scheme of1720h73043 using GIZA++ (Och and Ney, 2003).The maximum fertility for an English word is 3.
Allthe data is used as given, i.e.
we do not have anypreprocessing of the English-French data.
The wordalignment provided in the workshop is not used inour evaluations.
The language model is providedby the workshop, and we do not use other languagemodels.The French phrases up to 8-gram in the devel-opment and test sets are extracted with top-3 can-didate English phrases.
There are in total 2.6 mil-lion phrase pairs 1 extracted for both developmentset and the unseen test set.
We did minimal tuningof the parameters in the pharaoh decoder (Koehn,2004) settings, simply to balance the length penaltyfor Bleu score.
Most of the weights are left as theyare given: [ttable-limit]=20, [ttable-threshold]=0.01,1Our phrase table is to be released to public in this workshop[stack]=100, [beam-threshold]=0.01, [distortion-limit]=4, [weight-d]=0.5, [weight-l]=1.0, [weight-w]=-0.5.
Table 1 shows the algorithm?s performanceon several settings for the seven basic scores pro-vided in section 6.settings Dev.Bleu Tst.Bleus1 27.44 27.65s2 27.62 28.25Table 1: Pharaoh Decoder SettingsIn Table 1, setting s1 was our submissionwithout using the inverse relative frequency ofPrf (ei+ki |fj+lj ).
s2 is using all the seven scores.8 DiscussionsIn this paper, we propose a generalized phrase ex-traction algorithm towards word alignment-free uti-lizing the fertility model to predict the width of theblock, a distortion model to predict how close thecenters of source and target phrases are, and a lex-icon model for translational equivalence.
The algo-rithm is a general framework, in which one couldplug in other scores and word alignment to get bet-ter results.ReferencesP.F.
Brown, Stephen A. Della Pietra, Vincent.
J.Della Pietra, and Robert L. Mercer.
1993.
The mathe-matics of statistical machine translation: Parameter es-timation.
In Computational Linguistics, volume 19(2),pages 263?331.Philip Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based smt.
In Proceedings of the Confer-ence of the Association for Machine Translation in theAmericans (AMTA).Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.
InComputational Linguistics, volume 29, pages 19?51.Christoph Tillmann.
2003.
A projection extension algo-rithm for statistical machine translation.
In Proceed-ings of the Conference on Empirical Methods in Natu-ral Language Processing (EMNLP).Stephan Vogel, Sanjika Hewavitharana, Muntsin Kolss,and Alex Waibel.
2004.
The ISL statistical translationsystem for spoken language translation.
In Proc.
of theInternational Workshop on Spoken Language Transla-tion, pages 65?72, Kyoto, Japan.144
