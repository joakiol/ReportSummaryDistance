Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2114?2123,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsTopic Extraction from Microblog Posts Using Conversation StructuresJing Li1,2?, Ming Liao1,2, Wei Gao3, Yulan He4and Kam-Fai Wong1,21The Chinese University of Hong Kong, Shatin, N.T., Hong Kong2MoE Key Laboratory of High Confidence Software Technologies, China3Qatar Computing Research Institute, Hamad Bin Khalifa University, Doha, Qatar4School of Engineering and Applied Science, Aston University, UK{lijing,mliao,kfwong}@se.cuhk.edu.hk1,2wgao@qf.org.qa3, y.he9@aston.ac.uk4AbstractConventional topic models are ineffec-tive for topic extraction from microblogmessages since the lack of structure andcontext among the posts renders poormessage-level word co-occurrence pat-terns.
In this work, we organize microblogposts as conversation trees based on re-posting and replying relations, which en-rich context information to alleviate datasparseness.
Our model generates wordsaccording to topic dependencies derivedfrom the conversation structures.
In spe-cific, we differentiate messages as leadermessages, which initiate key aspects ofpreviously focused topics or shift the focusto different topics, and follower messagesthat do not introduce any new informationbut simply echo topics from the messagesthat they repost or reply.
Our model cap-tures the different extents that leader andfollower messages may contain the keytopical words, thus further enhances thequality of the induced topics.
The resultsof thorough experiments demonstrate theeffectiveness of our proposed model.1 IntroductionThe increasing popularity of microblog platformsresults in a huge volume of user-generated shortposts.
Automatically modeling topics out of suchmassive microblog posts can uncover the hid-den semantic structures of the underlying collec-tion and can be useful to downstream applicationssuch as microblog summarization (Harabagiu andHickl, 2011), user profiling (Weng et al, 2010),event tracking (Lin et al, 2010) and so on.Popular topic models, like Probabilistic La-tent Semantic Analysis (pLSA) (Hofmann, 1999)?
* Part of this work was conducted when the first authorwas visiting Aston University.and Latent Dirichlet Allocation (LDA) (Blei etal., 2003b), model the semantic relationships be-tween words based on their co-occurrences in doc-uments.
They have demonstrated their successin conventional documents such as news reportsand scientific articles, but perform poorly whendirectly applied to short and colloquial microblogcontent due to severe sparsity in microblog mes-sages (Wang and McCallum, 2006; Hong andDavison, 2010).A common way to deal with short text sparsityis to aggregate short messages into long pseudo-documents.
Most of the studies heuristicallyaggregate messages based on authorship (Zhaoet al, 2011; Hong and Davison, 2010), sharedwords (Weng et al, 2010), or hashtags (Ram-age et al, 2010; Mehrotra et al, 2013).
Someworks directly take into account the word re-lations to alleviate document-level word sparse-ness (Yan et al, 2013; Sridhar, 2015).
Morerecently, a self-aggregation-based topic modelcalled SATM (Quan et al, 2015) was proposed toaggregate texts jointly with topic inference.However, we argue that the existing aggrega-tion strategies are suboptimal for modeling top-ics in short texts.
Microblogs allow users to shareand comment on messages with friends throughreposting or replying, similar to our everyday con-versations.
Intuitively, the conversation structurescan not only enrich context, but also provide use-ful clues for identifying relevant topics.
Thisis nonetheless ignored in previous approaches.Moreover, the occurrence of non-topic words suchas emotional, sentimental, functional and evenmeaningless words are very common in microblogposts, which may distract the models from recog-nizing topic-related key words and thus fail to pro-duce coherent and meaningful topics.We propose a novel topic model by utilizing thestructures of conversations in microblogs.
We linkmicroblog posts using reposting and replying rela-2114tions to build conversation trees.
Particularly, theroot of a conversation tree refers to the originalpost and its edges represent the reposting/replyingrelations.
[O] Just an hour ago, a series of coordinatedterrorist attacks occurred in Paris !!!
[R2] Gunmen and suicide bombershit a concert hall.
More than 100are killed already.
[R1] OMG!
I can?t believe it?sreal.
Paris?!
I?ve just been therelast month.
[R3] Oh no!
@BonjourMarcr u OK?
please reply me forgod?s sake!!!
[R4] My gosh!!!
thatsucks// Poor on uguys?
[R7] For the safety of US, I?m for Trump to be president,especially after this.
[R8] I repost to support @realDonaldTrump.
Can?t agree more -[R10] R U CRAZY?!
Trump is just a bigot sexistand racist.??
??
??
[R9] thanks dude, you?d never regret -??
[R5] Don?t worry.
I was home.
[R6] poor guys, terribleFigure 1: An example of conversation tree.
[O]:the original post; [Ri]: the i-th repost/reply; Ar-row lines: reposting/replying relations; Dark blackposts: leaders to be detected; Underlined italicwords: key words representing topicsFigure 1 illustrates an example of a conversa-tion tree, in which messages can initiate a newtopic such as [O] and [R7] or raise a new aspect(subtopic) of the previously discussed topics suchas [R2] and [R10].
These messages are named asleaders, which contain salient content in topic de-scription, e.g., the italic and underlined words inFigure 1.
The remaining messages, named as fol-lowers, do not raise new issues but simply respondto their reposted or replied messages followingwhat has been raised by the leaders and often con-tain non-topic words, e.g., OMG, OK, agree, etc.Conversation tree structures from microblogshave been previously shown helpful to microblogsummarization (Li et al, 2015), but have neverbeen explored for topic modeling.
We follows Liet al (2015) to detect leaders and followers acrosspaths of conversation trees using Conditional Ran-dom Fields (CRF) trained on annotated data.
Thedetected leader/follower information is then in-corporated as prior knowledge into our proposedtopic model.Our experimental results show that our model,which captures parent-child topic correlations inconversation trees and generates topics by consid-ering messages being leaders or followers sepa-rately, is able to induce high-quality topics andoutperforms a number of competitive baselines.
Insummary, our contributions are three-fold:?
We propose a novel topic model, which ex-plicitly exploits the topic dependencies containedin conversation structures to enhance topic assign-ments.?
Our model differentiates the generative pro-cess of topical and non-topic words, according tothe message where a word is drawn from beinga leader or a follower.
This helps the model dis-tinguish the topic-specific information from back-ground noise.?
Our model outperforms state-of-the-art topicmodels when evaluated on a large real-world mi-croblog dataset containing over 60K conversationtrees, which is publicly available1.2 Related WorksTopic models aim to discover the latent seman-tic information, i.e., topics, from texts and havebeen extensively studied.
One of the most popu-lar and well-known topic models is LDA (Blei etal., 2003b).
It utilizes Dirichlet priors to generatedocument-topic and topic-word distributions, andhas been shown effective in extracting topics fromconventional documents.Nevertheless, prior research has demonstratedthat standard topic models, essentially focusingon document-level word co-occurrences, are notsuitable for short and informal microblog mes-sages due to severe data sparsity exhibited inshort texts (Wang and McCallum, 2006; Hong andDavison, 2010).
Therefore, how to enrich and ex-ploit context information becomes a main concern.Weng et al (2010), Hong et al (2010) and Zhaoet al (2011) first heuristically aggregated mes-sages posted by the same user or sharing the samewords before applying classic topic models to ex-tract topics.
However, such a simple strategy posessome problems.
For example, it is common that auser has various interests and posts messages cov-ering a wide range of topics.
Ramage et al (2010)and Mehrotra et al (2013) used hashtags as labelsto train supervised topic models.
But these mod-els depend on large-scale hashtag-labeled data formodel training, and their performance is inevitablycompromised when facing unseen topics irrelevantto any hashtag in training data due to the rapidchange and wide variety of topics in social media.SATM (Quan et al, 2015) combined short textsaggregation and topic induction into a unifiedmodel.
But in their work, no prior knowledge1http://www1.se.cuhk.edu.hk/?lijing/data/microblog-topic-extraction-data.zip2115was given to ensure the quality of text aggrega-tion, which therefore can affect the performanceof topic inference.
In this work, we organize mi-croblog messages as conversation trees based onreposting/reply relations, which is a more advan-tageous message aggregation strategy.Another line of research tackled the wordsparseness by modeling word relations instead ofword occurrences in documents.
For example,Gaussian Mixture Topic Model (GMTM) (Srid-har, 2015) utilized word embeddings to model thedistributional similarities of words and then in-ferred clusters of words represented by word dis-tributions using Gaussian Mixture Model (GMM)that capture the notion of latent topics.
However,GMTM heavily relies on meaningful word embed-dings that require a large volume of high-qualityexternal resources for training.Biterm Topic Model (BTM) (Yan et al,2013) directly explores unordered word-pair co-occurrence patterns in each individual message.Our model learns topics from aggregated mes-sages based on conversation trees, which naturallyprovide richer context since word co-occurrencepatterns can be captured from multiple relevantmessages.3 LeadLDA Topic ModelIn this section, we describe how to extract top-ics from a microblog collection utilizing conversa-tion tree structures, where the trees are organizedbased on reposting and replying relations amongthe messages2.To identify key topic-related content from collo-quial texts, we differentiate the messages as lead-ers and followers.
Following Li et al (2015), weextract all root-to-leaf paths on conversation treesand utilize the state-of-the-art sequence learningmodel CRF (Lafferty et al, 2001) to detect theleaders3.
As a result, the posterior probability ofeach node being a leader or follower is obtainedby averaging the different marginal probabilitiesof the same node over all the tree paths that containthe node.
Then, the obtained probability distribu-tion is considered as the observed prior variableinput into our model.2Reposting/replying relations are straightforward to ob-tain by using microblog APIs from Twitter and Sina Weibo.3The CRF model for leader detection was trained on apublic corpus with all the messages annotated on the treepaths.
Details are described in Section 4.3.1 Topics and Conversation TreesPrevious works (Zhao et al, 2011; Yan et al,2013; Quan et al, 2015) have proven that assum-ing each short post contains a single topic is usefulto alleviate the data sparsity problem.
Thus, givena corpus of microblog posts organized as conver-sation trees and the estimated leader probabilitiesof tree nodes, we assume that each message onlycontains a single topic and a tree covers a mixtureof multiple topics.
Since leader messages subsumethe content of their followers, the topic of a leadercan be generated from the topic distribution of theentire tree.
Consequently, the topic mixture of aconversation tree is determined by the topic as-signments to the leader messages on it.
The topicsof followers, however, exhibit strong and explicitdependencies on the topics of their ancestors.
So,their topics need to be generated in considerationof local constraints.
Here, we mainly address howto model the topic dependencies of followers.Enlighten by the general Structural Topic Model(strTM) (Wang et al, 2011), which incorporatesdocument structures into topic model by explic-itly modeling topic dependencies between adja-cent sentences, we exploit the topical transitionsbetween parents and children in the trees for guid-ing topic assignments.Intuitively, the emergence of a leader results inpotential topic shift.
It tends to weaken the topicsimilarities between the emerging leaders and theirpredecessors.
For example, [R7] in Figure 1 trans-fers the topic to a new focus, thus weakens the tiewith its parent.
We can simplify our case by as-suming that followers are topically responsive justup to (hence not further than) their nearest ances-tor leaders.
Thus, we can dismantle each conver-sation tree into forest by removing the links be-tween leaders and their parents hence producinga set of subgraphs like [R2]?
[R6] and [R7]?
[R9]in Figure 1.
Then, we model the internal topicdependencies within each subgraph by inferringthe parent-child topic transition probabilities sat-isfying the first-order Markov properties in a simi-lar way as estimating the transition distribution ofadjacent sentences in strTM (Wang et al, 2011).At topic assignment stage, the topic of a followerwill be assigned by referring to its parent?s topicand the transition distribution that captures topicsimilarities of followers to their parents (see Sec-tion 3.2).In addition, every word in the corpus is either2116a topical or non-topic (i.e., background) word,which highly depends on whether it occurs in aleader or a follower message.Figure 2 illustrates the graphical model of ourgenerative process, which is named as LeadLDA.TMt??K????????????,?Nt,m?????,?(?)????,????,???K??????,?,????,?,???2????
?Figure 2: Graphical Model of LeadLDA3.2 Topic ModelingFormally, we assume that the microblog posts areorganized as T conversation trees.
Each tree tcontains Mtmessage nodes and each message mcontains Nt,mwords in the vocabulary.
The vo-cabulary size is V and there are K topics em-bedded in the corpus represented by word distri-bution ?k?
Dir(?)
(k = 1, 2, ...,K).
Also, abackground word distribution ?B?
Dir(?)
is in-cluded to capture the general information, which isnot topic specific.
?kand ?Bare multinomial dis-tributions over the vocabulary.
A tree t is modeledas a mixture of topics ?t?
Dir(?)
and any mes-sage m on t is assumed to contain a single topiczt,m?
{1, 2, ...,K}.
(1) Topic assignments: The topic assignmentsof LeadLDA is inspired by Griffiths et al (2004)that combines syntactic and semantic dependen-cies between words.
LeadLDA integrates the out-comes of leader detection with a binomial switcheryt,m?
{0, 1} indicating whether m is a leader(yt,m= 1) or a follower (yt,m= 0), given eachmessage m on the tree t. yt,mis parameterized byits leader probability lt,m, which is the posteriorprobability output from the leader detection modeland serves as an observed prior variable.According to the notion of leaders, they initiatekey aspects of previously discussed topics or sig-nal a new topic shifting the focus of its descendantfollowers.
So, the topics of leaders on tree t aredirectly sampled from the topic mixture ?t.To model the internal topic correlations withinthe subgraph of conversation tree consisting of aleader and all its followers, we capture parent-child topic transitions pik?
Dir(?
), which is adistribution over K topics, and use pik,jto denotethe probability of a follower assigned topic j whenthe topic of its parent is k. Specifically, if messagem is sampled as a follower and the topic assign-ment to its parent message is zt,p(m), where p(m)indexes the parent ofm, then zt,m(i.e., the topic ofm) is generated from topic transition distributionpizt,p(m).
In particular, since the root of a conver-sation tree has no parent and can only be a leader,we make the leader probability lt,root= 1 to forceits topic only to be generated from the topic distri-bution of tree t.(2) Topical and non-topic words: We sep-arately model the distributions of leader andfollower messages emitting topical or non-topicwords with ?0and ?1, respectively, both of whichare drawn from a symmetric Beta prior parame-tererized by ?.
Specifically, for each word n inmessage m on tree t, we add a binomial back-ground switcher xt,m,ncontrolled by whether mis a leader or a follower, i.e., xt,m,n?
Bi(?yt,m),which indicates n is a topical word if xt,m,n= 0 ora background word if xt,m,n= 1, and xt,m,ncon-trols n to be generated from the topic-word dis-tribution ?zt,m, where zt,mis the topic of m, orfrom background word distribution ?Bmodelingnon-topic information.
(3) Generation process: To sum up, condi-tioned on the hyper-parameters ?
= (?, ?, ?, ?
),the generation process of a conversation tree t canbe described as follows:?
Draw ?t?
Dir(?)?
For message m = 1 to Mton tree t?
Draw yt,m?
Bi(lt,m)?
If yt,m== 1?
Draw zt,m?Mult(?t)?
If yt,m== 0?
Draw zt,m?Mult(pizt,p(m))?
For word n = 1 to Nt,min m?
Draw xt,m,n?
Bi(?yt,m)?
If xt,m,n== 0?
Draw wt,m,n?Mult(?zt,m)?
If xt,m,n== 1?
Draw wt,m,n?Mult(?B)2117CLBs,(r)# of words with background switchers assigned as r and oc-curring in messages with leader switchers s.CLBs,(?
)# of words occurring in messages whose leader switchers ares, i.e.,?r?
{0,1}CLBs,(r).NB(r)# of words occurring in message (t,m) and with backgroundswitchers assigned as r.NB(?
)# of words in message (t,m), i.e., NB(?)=?r?
{0,1}NB(r).CTWk,(v)# of words indexing v in vocabulary, sampled as topic (non-background) words, and occurring in messages assigned topick.CTWk,(?
)# of words assigned as topic (non-background) word andoccurring in messages assigned topics k, i.e., CTWk,(?
)=?Vv=1CTWk,(v).NW(v)# of words indexing v in vocabulary that occur in message(t,m) and are assigned as topic (non-background) word.NW(?
)# of words assigned as topic (non-background) words and oc-curring in message (t,m), i.e., NW(?
)=?Vv=1NW(v).CTRi,(j)# of messages sampled as followers and assigned topic j,whose parents are assigned topic i.CTRi,(?
)# of messages sampled as followers whose parents are as-signed topic i, i.e., CTRi,(?)=?Kj=1CTRi,(j).I(?
)An indicator function, whose value is 1 when its argumentinside () is true, and 0 otherwise.NCT(j)# of messages that are children of message (t,m), sampledas followers and assigned topic j.NCT(?
)# of message (t,m)?s children sampled as followers, i.e.,NCT(?
)=?Kj=1NCT(j)CTTt,(k)# of messages on conversation tree t sampled as leaders andassigned topic k.CTTt,(?
)# of messages on conversation tree t sampled as leaders, i.e.,CTTt,(?
)=?Kk=1CTTt,(k)CBW(v)# of words indexing v in vocabulary and assigned as back-ground (non-topic) wordsCBW(?
)# of words assigned as background (non-topic) words, i.e.,CBW(?
)=?Vv=1CBW(v)Table 1: The notations of symbols in the samplingformulas (1) and (2).
(t,m): message m on con-versation tree t.3.3 Inference for ParametersWe use collapsed Gibbs Sampling (Griffiths,2002) to carry out posterior inference for param-eter learning.
The hidden multinomial variables,i.e., message-level variables (y and z) and word-level variables (x) are sampled in turn, conditionedon a complete assignment of all other hidden vari-ables.
Due to the space limitation, we leave outthe details of derivation but give the core formulasin the sampling steps.We first define the notations of all variablesneeded by the formulation of Gibbs sampling,which are described in Table 1.
In particular, thevarious C variables refer to counts excluding themessage m on conversation tree t.For each message m on a tree t, we samplethe leader switcher yt,mand topic assignment zt,maccording to the following conditional probabilitydistribution:p(yt,m= s, zt,m= k|y?
(t,m), z?
(t,m),w,x, l,?)??(CLBs,(?
)+ 2?)?(CLBs,(?)+NB(?
)+ 2?)?r?{0,1}?
(CLBs,(r)+NB(r)+ ?)?
(CLBs,(r)+ ?)??(CTWk,(?
)+ V ?)?(CTWk,(?)+NW(?
)+ V ?)V?v=1?
(CTWk,(v)+NW(v)+ ?)?
(CTWk,(v)+ ?
)?g(s, k, t,m)(1)where g(s, k, t,m) takes different forms depend-ing on the value of s:g(0, k, t,m) =?(CTRzt,p(m),(?)+K?)?(CTRzt,p(m),(?
)+ I(zt,p(m)6= k) +K?)??(CTRk,(?)+K?)?(CTRk,(?
)+ I(zt,p(m)= k) +NCT(?)+K?)?K?j=1?
(CTRk,(j)+NCT(j)+ I(zt,p(m)= j = k) + ?)?
(CTRk,(j)+ ?)??
(CTRzt,p(m),(k)+ I(zt,p(m)6= k) + ?)?
(CTRzt,p(m),(k)+ ?)?
(1?
lt,m)andg(1, k, t,m) =CTTt,(k)+ ?CTTt,(?)+K??
lt,mFor each word n in m on t, the sampling for-mula of its background switcher is given as thefollowing:p(xt,m,n= r|x?
(t,m,n),y, z,w, l,?
)?CLByt,m,(r)+ ?CLByt,m,(?
)+ 2??
h(r, t,m, n)(2)whereh(r, t,m, n) =?????CTWzt,m,(wt,m,n)+?CTWzt,m,(?
)+V ?if r = 0CBW(wt,m,n)+?CBW(?
)+V ?if r = 14 Data Collection and Experiment SetupTo evaluate our LeadLDA model, we conductedexperiments on real-world microblog dataset col-lected from Sina Weibo that has the same 140-character limitation and shares the similar mar-ket penetration as Twitter (Rapoza, 2011).
Forthe hyper-parameters of LeadLDA, we fixed ?
=50/K, ?
= 0.1, following the common practicein previous works (Griffiths and Steyvers, 2004;Quan et al, 2015).
Since there is no analogue of?
and ?
in prior works, where ?
controls topicdependencies of follower messages to their an-cestors and ?
controls the different tendencies of2118Month # of trees # of messages Vocab sizeMay 10,812 38,926 6,011June 29,547 98,001 9,539July 26,103 102,670 10,121Table 2: Statistics of our three evaluation datasetsleaders and followers covering topical and non-topic words.
We tuned ?
and ?
by grid search ona large development set containing around 120Kposts and obtained ?
= 50/K, ?
= 0.5.Because the content of posts are often incom-plete and informal, it is difficult to manually an-notate topics in a large scale.
Therefore, we fol-low Yan et al (2013) to utilize hashtags led by ?#?,which are manual topic labels provided by users,as ground-truth categories of microblog messages.We collected the real-time trending hashtags onSina Weibo and utilized the hashtag-search API4to crawl the posts matching the given hashtagqueries.
In the end, we built a corpus containing596,318 posts during May 1 ?
July 31, 2014.To examine the performance of models on var-ious topic distributions, we split the corpus into 3datasets, each containing messages of one month.Similar to Yan et al (2013), for each dataset, wemanually selected 50 frequent hashtags as topics,e.g.
#mh17, #worldcup, etc.
The experimentswere conducted on the subsets of posts with theselected hashtags.
Table 2 shows the statistics ofthe three subsets used in our experiments.We preprocessed the datasets before topic ex-traction in the following steps: 1) Use FudanNLPtoolkit (Qiu et al, 2013) for word segmentation,stop words removal and POS tagging for ChineseWeibo messages; 2) Generate a vocabulary foreach dataset and remove words occurring less than5 times; 3) Remove all hashtags in texts before in-put them to models, since the models are expectedto extract topics without knowing the hashtags,which are ground-truth topics; 4) For LeadLDA,we use the CRF-based leader detection model (Liet al, 2015) to classify messages as leaders andfollowers.
The leader detection model was im-plemented by using CRF++5, which was trainedon the public dataset composed of 1,300 conversa-tion paths and achieved state-of-the-art 73.7% F1-score of classification accuracy (Li et al, 2015).4http://open.weibo.com/wiki/2/search/topics5https://taku910.github.io/crfpp/5 Experimental ResultsWe evaluated topic models with two sets ofK, i.e.,the number of topics.
One isK = 50, to match thecount of hashtags following Yan et al (2013), andthe other is K = 100, much larger than the ?real?number of topics.
We compared LeadLDA withthe following 5 state-of-the-art basedlines.TreeLDA: Analogous to Zhao et al (2011),where they aggregated messages posted by thesame author, TreeLDA aggregates messages fromone conversation tree as a pseudo-document.
Ad-ditionally, it includes a background word distribu-tion to capture non-topic words controlled by ageneral Beta prior without differentiating leadersand followers.
TreeLDA can be considered as adegeneration of LeadLDA, where topics assignedto all messages are generated from the topic distri-butions of the conversation trees they are on.StructLDA: It is another variant of LeadLDA,where topics assigned to all messages are gener-ated based on topic transitions from their parents.The strTM (Wang et al, 2011) utilized a similarmodel to capture the topic dependencies of adja-cent sentences in a document.
Following strTM,we add a dummy topic Tstartemitting no word tothe ?pseudo parents?
of root messages.
Also, weadd the same background word distribution to cap-ture non-topic words as TreeLDA does.BTM: Biterm Topic Model (BTM)6(Yan etal., 2013) directly models topics of all word pairs(biterms) in each post, which outperformed LDA,Mixture of Unigrams model, and the model pro-posed by Zhao et al (2011) that aggregated postsby authorship to enrich context.SATM: A general unified model proposed byQuan et al (2015) that aggregates documents andinfers topics simultaneously.
We implementedSATM and examined its effectiveness specificallyon microblog data.GMTM: To tackle word sparseness, Sridharet al (2015) utilized Gaussian Mixture Model(GMM) to cluster word embeddings generated bya log-linear word2vec model7.The hyper-parameters of BTM, SATM andGMTM were set according to the best hyper-parameters reported in their original papers.
ForTreeLDA and StructLDA, the parameter settingswere kept the same as LeadLDA since they are its6https://github.com/xiaohuiyan/BTM7https://code.google.com/archive/p/word2vec/2119variants.
And the background switchers were pa-rameterized by symmetric Beta prior on 0.5, fol-lowing Chemudugunta et al (2006).
We ran Gibbssamplings (in LeadLDA, TreeLDA, StructLDA,BTM and SATM) and EM algorithm (in GMTM)with 1,000 iterations to ensure convergence.Topic model evaluation is inherently difficult.In previous works, perplexity is a popular metricto evaluate the predictive abilities of topic mod-els given held-out dataset with unseen words (Bleiet al, 2003b).
However, Chang et al (2009)have demonstrated that models with high perplex-ity do not necessarily generate semantically co-herent topics in human perception.
Therefore, weconducted objective and subjective analysis on thecoherence of produced topics.5.1 Objective AnalysisThe quality of topics is commonly measured bycoherence scores (Mimno et al, 2011), assumingthat words representing a coherent topic are likelyto co-occur within the same document.
However,due to the severe sparsity of short text posts, wemodify the calculation of commonly-used topiccoherence measure based on word co-occurrencesin messages tagged with the same hashtag, namedas hashtag-document, assuming that those mes-sages discuss related topics8.Specifically, we calculate the coherence score ofa topic given the topN words ranked by likelihoodas below:C =1K?K?k=1N?i=2i?1?j=1logD(wki, wkj) + 1D(wkj), (3)where wkirepresents the i-th word in topic kranked by p(w|k), D(wki, wkj) refers to the countof hashtag-documents where word wkiand wkjco-occur, and D(wki) denotes the number of hashtag-documents that contain word wki.Table 3 shows the absolute values of C scoresfor topics produced on three evaluation datasets(May, June and July), and the top 10, 15, 20 wordsof topics were selected for evaluation.
Lowerscores indicate better coherence in the inducedtopic.We have the following observations:?
GMTM gave the worst coherence scores,which may be ascribed to its heavy reliance on rel-evant large-scale high-quality external data, with-8We sampled posts and their corresponding hashtags inour evaluation set and found only 1% mismatch.N ModelMay June JulyK50 K100 K50 K100 K50 K10010TREE 27.9 30.5 24.0 23.8 23.9 26.1STR 29.9 30.8 24.0 24.1 24.4 26.4BTM 26.7 28.9 27.8 25.5 25.4 25.2SATM 30.6 29.9 23.8 23.7 24.3 27.5GMTM 40.8 40.1 44.0 44.2 41.7 40.8LEAD 28.4 26.9 19.8 23.4 22.6 25.115TREE 71.9 76.4 55.3 60.4 61.2 66.2STR 76.4 74.1 57.6 62.2 58.1 61.1BTM 69.6 71.4 58.5 60.3 59.1 63.0SATM 74.3 73.0 54.8 60.4 61.2 65.3GMTM 96.4 93.1 100.4 105.1 94.6 94.9LEAD 67.4 65.2 52.8 57.7 55.3 57.820TREE 138.8 138.6 102.0 115.0 115.8 119.7STR 134.0 136.9 104.3 112.7 111.0 117.3BTM 125.2 131.1 109.4 115.7 115.3 120.2SATM 134.6 131.9 105.5 114.3 113.5 118.9GMTM 173.5 169.0 184.7 190.9 167.4 171.2LEAD 120.9 127.2 101.6 106.0 97.2 104.9Table 3: Absolute values of coherence scores.Lower is better.
K50: 50 topics; K100: 100 topics;N: # of top words ranked by topic-word probabil-ities; TREE: TreeLDA; STR: StructLDA; LEAD:LeadLDA.out which the trained word embedding modelfailed to capture meaningful semantic features forwords, and hence could not yield coherent topics.?
TreeLDA and StructLDA produced competi-tive results compared to the state-of-the-art base-line models, which indicates the effectiveness ofusing conversation structures to enrich context andthus generate topics of reasonably good quality.?
The coherence of topics generated byLeadLDA outperformed all the baselines on thethree datasets, most of time by large marginsand was only outperformed by BTM on the Maydataset when K = 50 and N = 10.
The gen-erally higher performance of LeadLDA is dueto three reasons: 1) It effectively identifies top-ics using the conversation tree structures, whichprovide richer context information; 2) It jointlymodels the topics of leaders and the topic depen-dencies of other messages on a tree.
TreeLDAand StructLDA, each only considering one ofthe factors, performed worse than LeadLDA; 3)LeadLDA separately models the probabilities ofleaders and followers containing topical or non-topic words while the baselines only model thegeneral background information regardless of thedifferent types of messages.
This implies thatleaders and followers do have different capaci-ties in covering key topical words or backgroundnoise, which is useful to identify key words fortopic representation.2120TreeLDA StructLDA BTM SATM LeadLDA???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
?Hong Kong, microblog,family, confirm,immigration, airliner,news, Grey Chow, vote,second baby, choice, godfather, nourish, airplane,foreign, feeling, crash,man, fellowmanUkraine, airline, dear,national, bypass, fly,flight, airspace, all, avoid,announce, airspace,eastern, Russia, finally,forget, company,disappointed, look,valuableHong Kong, immigration,family, confirm, man,passport, foreign, news,crash, Malaysia Airlines,report, contact, broadcaststation, airliner, airplane,fellowman, confirm,event, Fok?s family,directlyMalaysia Airlines, prey,rest in peace, life, dead,world, AIDS, terror,Guangzhou, airplane,innocent, silent tribute,keep away from, event,shoot down, bus, Chinese,international, wish thedead, reallyUkraine, MalaysiaAirlines, airliner, shootdown, airplane, crash,missile, Russia, news,passenger, China,Malaysia, Hong Kong,killed, event, militant,flight, terror, current,confirmFigure 3: The extracted topics describing MH17 crash.
Each column represents the similar topic gener-ated by the corresponding model with the top 20 words.
The 2nd row: original Chinese words; The 3rdrow: English translations.5.2 Subjective AnalysisTo evaluate the coherence of induced topics fromhuman perspective, we invited two annotators tosubjectively rate the quality of every topic (by dis-playing the top 20 words) generated by differentmodels on a 1-5 Likert scale.
A higher rating in-dicates better quality of topics.
The Fless?s Kappaof annotators?
ratings measured for various modelson different datasets given K = 50 and 100 rangefrom 0.62 to 0.70, indicating substantial agree-ments (Landis and Koch, 1977).Table 4 shows the overall subjective ratings.We noticed that humans preferred topics pro-duced given K = 100 to K = 50, but coher-ence scores gave generally better grades to mod-els for K = 50, which matched the number oftopics in ground truth.
This is because modelsmore or less mixed more common words whenK is larger.
Coherence score calculation (Equa-tion (3)) penalizes common words that occur inmany documents, whereas humans could some-how ?guess?
the meaning of topics based on therest of words thus gave relatively good ratings.Nevertheless, annotators gave remarkably higherratings to LeadLDA than baselines on all datasetsregardless of K being 50 or 100, which con-firmed that LeadLDA effectively yielded good-quality topics.For a detailed analysis, Figure 3 lists the top 20words about ?MH17 crash?
induced by differentmodels9when K = 50.
We have the following9As shown in Table 3 and 4, the topic coherence scoresof GMTM were the worst.
Hence, the topic generated byModelMay June JulyK50 K100 K50 K100 K50 K100TREE 3.12 3.41 3.42 3.44 3.03 3.48STR 3.05 3.45 3.38 3.48 3.08 3.53BTM 3.04 3.26 3.40 3.37 3.15 3.57SATM 3.08 3.43 3.30 3.55 3.09 3.54GMTM 2.02 2.37 1.99 2.27 1.97 1.90LEAD 3.40 3.57 3.52 3.63 3.55 3.72Table 4: Subjective ratings of topics.
The mean-ings of K50, K100, TREE, STR and LEAD are thesame as in Table 3.observations:?
BTM, based on word-pair co-occurrences,mistakenly grouped ?Fok?s family?
(a tycoon fam-ily in Hong Kong), which co-occurred frequentlywith ?Hong Kong?
in other topics, into the topic of?MH17 crash?.
?Hong Kong?
is relevant here as aHong Kong passenger died in the MH17 crash.?
The topical words generated by SATM weremixed with words relevant to the bus explosion inGuangzhou, since it aggregated messages accord-ing to topic affinities based on the topics learned inthe previous step.
Thus the posts about bus explo-sion and MH17 crash, both pertaining to disasters,were aggregated together mistakenly, which gen-erated spurious topic results.?
Both TreeLDA and StructLDA generatedtopics containing non-topic words like ?mi-croblog?
and ?dear?.
This means that withoutdistinguishing leaders and followers, it is diffi-cult to filter out non-topic words.
The topic qual-ity of StructLDA nevertheless seems better thanGMTM is not shown due to space limitation.2121TreeLDA, which implies the usefulness of exploit-ing topic dependencies of posts in conversationstructures.?
LeadLDA not only produced more semanti-cally coherent words describing the topic, but alsorevealed some important details, e.g., MH17 wasshot down by a missile.6 Conclusion and Future WorksThis paper has proposed a novel topic model byconsidering the conversation tree structures of mi-croblog posts.
By rigorously comparing our pro-posed model with a number of competitive base-lines on real-world microblog datasets, we havedemonstrated the effectiveness of using conversa-tion structures to help model topics embedded inshort and colloquial microblog messages.This work has proven that detecting leaders andfollowers, which are coarse-grained discourse de-rived from conversation structures, is useful tomodel microblogging topics.
In the next step, weplan to exploit fine-grained discourse structures,e.g., dialogue acts (Ritter et al, 2010), and proposea unified model that jointly inferring discourseroles and topics of posts in context of conversa-tion tree structures.
Another extension is to ex-tract topic hierarchies by integrating the conversa-tion structures into hierarchical topic models likeHLDA (Blei et al, 2003a) to extract fine-grainedtopics from microblog posts.AcknowledgmentThis work is supported by General ResearchFund of Hong Kong (417112), the Innova-tion and Technology Fund of Hong Kong SAR(ITP/004/16LP), Shenzhen Peacock Plan Re-search Grant (KQCX20140521144507925) andInnovate UK (101779).
We would like to thankShichao Dong for his efforts on data process-ing and anonymous reviewers for the useful com-ments.ReferencesDavid M. Blei, Thomas L. Griffiths, Michael I. Jor-dan, and Joshua B. Tenenbaum.
2003a.
Hierarchi-cal topic models and the nested chinese restaurantprocess.
In Proceedings of the 17th Annual Con-ference on Neural Information Processing Systems,NIPS, pages 17?24.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003b.
Latent dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.Jonathan Chang, Jordan L. Boyd-Graber, Sean Gerrish,Chong Wang, and David M. Blei.
2009.
Reading tealeaves: How humans interpret topic models.
In Pro-ceedings of the 23rd Annual Conference on NeuralInformation Processing Systems, NIPS, pages 288?296.Chaitanya Chemudugunta, Padhraic Smyth, and MarkSteyvers.
2006.
Modeling general and specificaspects of documents with a probabilistic topicmodel.
In Proceedings of the 20th Annual Con-ference on Neural Information Processing Systems,NIPS, pages 241?248.Thomas L Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101(suppl 1):5228?5235.Thomas L. Griffiths, Mark Steyvers, David M. Blei,and Joshua B. Tenenbaum.
2004.
Integrating top-ics and syntax.
In Proceedings of the 18th AnnualConference on Neural Information Processing Sys-tems, NIPS, pages 537?544.Tom Griffiths.
2002.
Gibbs sampling in the generativemodel of latent dirichlet alocation.Sanda M. Harabagiu and Andrew Hickl.
2011.
Rel-evance modeling for microblog summarization.
InProceedings of the 5th International Conference onWeb and Social Media, ICWSM.Thomas Hofmann.
1999.
Probabilistic latent seman-tic indexing.
In In Proceedings of the 22nd AnnualInternational, ACM SIGIR, pages 50?57.Liangjie Hong and Brian D Davison.
2010.
Empiricalstudy of topic modeling in twitter.
In Proceedings ofthe first workshop on social media analytics, pages80?88.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labelingsequence data.
In Proceedings of the 18th Inter-national Conference on Machine Learning, ICML,pages 282?289.J Richard Landis and Gary G Koch.
1977.
The mea-surement of observer agreement for categorical data.biometrics, pages 159?174.Jing Li, Wei Gao, Zhongyu Wei, Baolin Peng, andKam-Fai Wong.
2015.
Using content-level struc-tures for summarizing microblog repost trees.
InProceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing, EMNLP,pages 2168?2178.Cindy Xide Lin, Bo Zhao, Qiaozhu Mei, and JiaweiHan.
2010.
PET: a statistical model for popular2122events tracking in social communities.
In Proceed-ings of the 16th International Conference on Knowl-edge Discovery and Data Mining, ACM SIGKDD,pages 929?938.Rishabh Mehrotra, Scott Sanner, Wray L. Buntine, andLexing Xie.
2013.
Improving LDA topic models formicroblogs via tweet pooling and automatic label-ing.
In Proceedings of the 36th International con-ference on research and development in InformationRetrieval, ACM SIGIR, pages 889?892.David M. Mimno, Hanna M. Wallach, Edmund M.Talley, Miriam Leenders, and Andrew McCallum.2011.
Optimizing semantic coherence in topic mod-els.
In Proceedings of the 2011 Conference on Em-pirical Methods in Natural Language Processing,EMNLP, pages 262?272.Xipeng Qiu, Qi Zhang, and Xuanjing Huang.
2013.Fudannlp: A toolkit for chinese natural languageprocessing.
In 51st Annual Meeting of the Asso-ciation for Computational Linguistics, ACL, pages49?54.Xiaojun Quan, Chunyu Kit, Yong Ge, and Sinno JialinPan.
2015.
Short and sparse text topic modeling viaself-aggregation.
In Proceedings of the 24th Inter-national Joint Conference on Artificial Intelligence,IJCAI, pages 2270?2276.Daniel Ramage, Susan T. Dumais, and Daniel J.Liebling.
2010.
Characterizing microblogs withtopic models.
In Proceedings of the 4th Inter-national Conference on Web and Social Media,ICWSM.Kenneth Rapoza.
2011.
China?s weibos vs us?s twitter:And the winner is?
Forbes (May 17, 2011).Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Unsu-pervised modeling of twitter conversations.
In Pro-ceedings of the 2010 Conference of the North Amer-ican Chapter of the Association of ComputationalLinguistics, NAACL, pages 172?180.Vivek Kumar Rangarajan Sridhar.
2015.
Unsupervisedentity linking with abstract meaning representation.In Proceedings of the 2015 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,NAACL-HLT, pages 1130?1139.Xuerui Wang and Andrew McCallum.
2006.
Top-ics over time: a non-markov continuous-time modelof topical trends.
In Proceedings of the 12th Inter-national Conference on Knowledge Discovery andData Mining, ACM SIGKDD, pages 424?433.Hongning Wang, Duo Zhang, and ChengXiang Zhai.2011.
Structural topic model for latent topical struc-ture analysis.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics, ACL, pages 1526?1535.Jianshu Weng, Ee-Peng Lim, Jing Jiang, and Qi He.2010.
Twitterrank: finding topic-sensitive influen-tial twitterers.
In Proceedings of the 3rd Interna-tional Conference on Web Search and Web DataMining, WSDM, pages 261?270.Xiaohui Yan, Jiafeng Guo, Yanyan Lan, and XueqiCheng.
2013.
A biterm topic model for short texts.In Proceedings of the 22nd International World WideWeb Conference, WWW, pages 1445?1456.Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He,Ee-Peng Lim, Hongfei Yan, and Xiaoming Li.
2011.Comparing twitter and traditional media using topicmodels.
In Advances in Information Retrieval - 33rdEuropean Conference on IR Research, ECIR, pages338?349.2123
