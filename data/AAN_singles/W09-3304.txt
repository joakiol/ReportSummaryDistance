Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 28?31,Suntec, Singapore, 7 August 2009.c?2009 ACL and AFNLPUsing the Wiktionary Graph Structure for Synonym DetectionTimothy Weale, Chris Brew, Eric Fosler-LussierDepartment of Computer Science and EngineeringThe Ohio State University{weale,cbrew,fosler}@cse.ohio-state.eduAbstractThis paper presents our work on usingthe graph structure of Wiktionary for syn-onym detection.
We implement seman-tic relatedness metrics using both a directmeasure of information flow on the graphand a comparison of the list of verticesfound to be ?close?
to a given vertex.
Ouralgorithms, evaluated on ESL 50, TOEFL80 and RDWP 300 data sets, perform bet-ter than or comparable to existing seman-tic relatedness measures.1 IntroductionThe recent creation of large-scale, collabora-tively constructed semantic resources provides re-searchers with cheap, easily accessible informa-tion.
Previous metrics used for synonym detec-tion had to be built using co-occurrence statisticsof collected corpora (Higgins, 2004) or expensive,expert-created resources such as WordNet or Ro-get?s Thesaurus (Jarmasz and Szpakowicz, 2003).Here, we evaluate the effectiveness of Wiktionary,a collaboratively constructed resource, as a sourceof semantic relatedness information for the syn-onym detection problem.Researching these metrics is important becausethey have been empirically shown to improve per-formance in a variety of NLP applications, includ-ing word sense disambiguation (Turdakov and Ve-likhov, 2008), real-world spelling errors (Budan-itsky and Hirst, 2006) and coreference resolution(Strube and Ponzetto, 2006).Synonym detection is a recognized testbedfor comparing semantic relatedness metrics (e.g(Zesch et al, 2008)).
In this task, a target wordor phrase is presented to the system, which is thenpresented with four alternative words or phrases.The goal of the system is to pick the alternativemost related to the target.
Example questions canbe found in Figure 1.Through the Wikimedia Foundation,1volun-teers have created two large-scale, collaborativeresources that have been used in previous related-ness research ?
Wikipedia (an encyclopedia) andWiktionary (a dictionary).
These sources havebeen used for synonym detection and replicatinghuman relatedness evaluations using the categorystructure (Strube and Ponzetto, 2006), local linkstructure (Milne and Witten, 2008) and (Turdakovand Velikhov, 2008) and global features (Zesch etal., 2008).
They contain related information butfocus on different information needs; which infor-mation source provides better results depends onthe needs of the task.
We use Wiktionary which,due to its role as a dictionary, focuses on commonwords and definitions ?
the type of informationfound in our synonym detection problems.Both Wikipedia and Wiktionary are organizedaround a basic ?page?
unit, containing informa-tion about an individual word, phrase or entityin the world ?
definitions, thesaurus entries, pro-nunciation guides and translations in Wiktionaryand general biographical, organizational or philo-sophical information in Wikipedia.
In both datasets, pages are linked to each each other and toa user-created category structure ?
a graph struc-ture where pages are vertices of the graph and pagelinks are the graph edges.
We will leverage thisgraph for determining relatedness.1http://www.wikimedia.org/Source Word Alternative Wordsmake earn, print, trade, borrowflawed imperfect, tiny, lustrous, crudesolitary alone, alert, restless, fearlessFigure 1: Example TOEFL Questions282 Extracting Relatedness MeasuresWe define relatedness based on information flowthrough the entire Wiktionary graph, rather thanby any local in-bound or out-bound link structure.This provides a global measurement of vertex im-portance, as we do not limit the approach to com-paring immediate neighbors.To do this, we first run the PageRank algorithm(Brin and Page, 1998) iteratively over the graphuntil convergence to measure the a priori impor-tance of each vertex in graph:~PRt+1= ??(~PRt?
E)+ (1 ?
?)
?~J (1)In this, E contains the edge transition probabilities,set to a uniform out-bound probability.~PR holdsthe PageRank value for each vertex and~J is uni-form vector used to randomly transition betweenvertices.
Traditionally, ?
= 0.85 and is used totradeoff between a strict transition model and therandom-walk model.We then adopt the extensions proposed in (Ol-livier and Senellart, 2007) (OS) to determine re-latedness given a source vertex:~Rt+1= ??(~Rt?
E + (~S ?~PR))+(1??
)?~J(2)~S is a vector that contains zeros except for a oneat our source vertex, and~PR removes an overallvalue of 1 based on the a priori PageRank value ofthe vertex.
In this way, vertices close to the sourceare rewarded with weight and vertices that have ahigh a priori importance are penalized.
When~Rconverges, it contains measures of importance forvertices based on the source vertex.Final relatedness values are then calculatedfrom the vector generated by Equation 2 and thea priori importance of the vector based on thePageRank from Equation 1:relOS(w, a) =~Rw[a] ?
log(1PR[a])(3)w is the vertex for the source word and a is thealternative word vertex.
ThePR[a] penalty is usedto further ensure that our alternative vertex is nothighly valued simply because it is well-connected.Applying Equation 3 provides comparable se-mantic relatedness performance (see Tables 1 and2).
However, cases exist where a single data valueis insufficient to make an adequate determinationof word relatedness because of small differencesfor candidate words.
We can incorporate addi-tional relatedness information about our verticesby leveraging information about the set of verticesdeemed ?most related?
to our current vertex.2.1 Integrating N-Best NeighborsWe add information by looking at the similaritybetween the n-best related words for each vertex.Intuitively, given a source word w and candidatealternatives a1and a2,2we look at the set of wordsthat are semantically related to each of the can-didates (represented as vectors W , A1and A2).If the overlap between elements of W and A1isgreater thanW andA2, A1is more likely to be thesynonym of W .Highly-ranked shared elements are good indi-cators of relatedness and should contribute morethan low-ranked related words.
Lists with manylow-ranked words could be an artifact of the dataset and should not be ranked higher than ones con-taining a few high-ranked words.Our ranked-list comparison metric (NB) is a se-lective mean reciprocal ranking function:relNB(~W,~A, n) =n?r=11r?
?
(Wr?~A) (4)~W is the n-best list based on the source vertexand~A is the n-best list based on the alternativevertex.
Values are added to our relatedness metricbased on the position of a vertex in the target listand the traditional Dirac ?-function, which has avalue of one if the target vertex appears anywherein our candidate list and a zero in all other cases.Each metric (OS and NB) will have differentranges.
We therefore normalize the reported valueby scaling each based on the maximum value forthat portion in order to achieve a uniform scale.Our final metric (OS+NB) is created by aver-aging the two normalized scores.
In this work,both scores are given equal weighting.
Derivingweightings for combining the two scores will bepart of our future work.relOS+NB(wi,j) =OS(ci, cj) + NB(ci, cj, n)2(5)In this, OS() returns the normalized relOS()value and NB() returns the normalized relNBvalue.
The maximum relP+N() value of 1.0 isachieved if cjhas the highest PageRank-basedvalue and the highest N-Best value.2See Figure 129SourceESL TOEFLAcc.
(%) Acc.
(%)JPL 82 78.8LC-IR 78 81.3OS 86 88.8NB 80 88.8OS+NB 88 93.8Table 1: ESL and TOEFL Performance3 EvaluationWe present performance results on three data sets.The first, ESL, uses 50 questions from the Englishas a Second Language test (Turney, 2001).
Next,an 80 question data set from the Test of Englishas a Foreign Language (TOEFL) is used (Lan-dauer and Dumais, 1997).
Finally, we evaluateon the Reader?s Digest WordPower (RDWP) dataset (Jarmasz and Szpakowicz, 2003).
This is a setof 300 synonym detection problems gathered fromthe Word Power game of the Canadian edition ofReader?s Digest Word from 2000 ?
2001.We use the Feb. 03, 2009 version of the EnglishWiktionary data set3for extracting graph structureand relatedness information.Table 1 presents the performance of our algo-rithm on the ESL and TOEFL test sets.
Our resultsare compared to Jarmasz and Szpakowicz (2003),which uses a path-based cost on the structureof Roget?s Thesaurus (JPL) and a cooccurence-based metric, LC-IR (Higgins, 2004), which con-strained context to only consider adjacent words instructured web queries.Information about our algorithm?s performanceon the RDWP test set is found in Table 2.
Our re-sults are compared to the previously mentioned al-gorithms and also the work of Zesch et al (2008).Their first metric (ZPL) uses the path length be-tween two graph vertices for relatedness determi-nation.
The second, (ZCV), creates concept vec-tors based on a distribution of pages that contain aparticular word.RDWP is not only larger then the previous two,but also more complictated.
TOEFL and ESLaverage 1.0 and 1.008 number of words in eachsource and alternative, respectively.
For RDWPeach entry averages 1.4 words.We map words and phrases to graph vertices byfirst matching against the page title.
If there is no3http://download.wikimedia.orgmatch, we follow the approach outlined in (Zeschet al, 2008).
Common words are removed fromthe phrase4and for every remaining word in thephrase, we determine the page mapping for thatindividual word.
The relatedness of the phraseis then set to be the maximum relatedness valueattributed to any of the individual words in thephrase.Random guessing by an algorithm could in-crease algorithm performance through randomchance.
Therefore, we present both a overallpercentage and also a precision-based percentage.The first (Raw) is defined as the correct number ofguesses over all questions.
The second (Prec) isdefined as the correct number of guesses dividedby only those questions that were attempted.3.1 DiscussionFor NB and OS+NB, we set n = 3000 based onTOEFL data set training.5Testing was then per-formed on the ESL and RDWP data set.As shown in Table 1, the OS algorithm per-forms better on the task than the comparison sys-tems.
On its own, NB relatedness performs well ?at or slightly worse than OS.
Combining the twomeasures increases performance on both data sets.While our TOEFL results are below the reportedperformance of (Turney et al, 2003) (97.5%), wedo not use any task-dependent learning for our re-sults and our algorithms have better performancethan any individual module in their system.Combining OS with NB mitigates the influenceof OS when it is not confident.
OS correctly picks?pinnacle?
as a synonym of ?zenith?
with a relat-edness value 126,000 times larger than its nextcompetitor.
For ?consumed?, OS is wrong, giving?bred?
a higher score than ?eaten?
?
but only bya value 1.2 times that of ?eaten?.
The latter caseis overcome by the addition of n-best informationwhile the former is unaffected.Table 2 demonstrates that we have results com-parable to existing state-of-the-art measures.
Ourchoice of n resulted in reduced scores on this taskwhen compared to using the OS metric by itself.But, our algorithm still outperforms both the ZPLand ZCV metrics for our data set in raw scores andin three out of the four precision measures.
Fur-ther refinement of the RDWP data set mapping orchanging our metric score to a weighted sum of4Defined here as: {and, or, to, be, the, a, an, of, on, in, for,with, by, into, is, no}5Out of 1.1 million vertices30Metric Source Attempted Score # Ties Raw PrecJPL Roget?s 300 223 0 .74 .74LC-IR Web 300 224.33 - .75 .75ZPLWikipedia226 88.33 96 .29 .39ZCV 288 165.83 2 .55 .58ZPLWiktionary201 103.7 55 .35 .52ZCV 174 147.3 3 .49 .85OSWiktionary300 234 0 .78 .78NB 300 212 0 .71 .71OS+NB 300 227 0 .76 .76Table 2: Reader?s Digest WordPower 300 Overall Performancesorts (rather than a raw maximum) could result inincreased performance.Wiktionary?s coverage enables all words in thefirst two tasks to be found (with the exception of?bipartisanly?).
Enough of the words in the RDWPtask are found to enable the algorithm to attemptall synonym detection questions.4 Conclusion and Future WorkIn this paper, we have demonstrated the effective-ness of Wiktionary as a source of relatedness in-formation when coupled with metrics based oninformation flow using synonym detection as ourevaluation testbed.Our immediate work will be in learning weightsfor the combination measure, using (Turney et al,2003) as our guideline.
Additional work will be inautomatically determining an effective value for nacross all data sets.Long-term work will be in modifying the pagetransition values to achieve non-uniform transitionvalues.
Links are of differing quality, and the tran-sition probabilities should reflect that.ReferencesSergey Brin and Lawrence Page.
1998.
The Anatomyof a Large-Scale Hypertextual Web Search En-gine.
Computer Networks and ISDN Systems, 30(1?7):107?117.Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating WordNet-based Measures of Lexical Se-mantic Relatedness.
Computational Linguistics,32(1):13?47.Derrick Higgins.
2004.
Which Statistics Reflect Se-mantics?
Rethinking Synonymy and Word Similar-ity.
In Proceedings of the International Conferenceon Linguistic Evidence.Mario Jarmasz and Stan Szpakowicz.
2003.
Roget?sThesaurus and Semantic Similarity.
In Proceedingsof Conference on Recent Advances in Natural Lan-guage Processing (RANLP 2003).Thomas K. Landauer and Susan T. Dumais.
1997.
ASolution to Plato?s Problem: The Latent SemanticAnalysis Theory of Acquisition, Induction, and Rep-resentation of Knowledge.
Psychological Review.David Milne and Ian H. Witten.
2008.
An Effec-tive, Low-Cost Measure of Semantic RelatednessObtained from Wikipedia Links.
In Proceedings ofAAAI 2008.Yann Ollivier and Pierre Senellart.
2007.
Finding Re-lated Pages Using Green Measures: An Illustrationwith Wikipedia.
In Proceedings of AAAI 2007.Michael Strube and Simone Paolo Ponzetto.
2006.WikiRelate!
Computing Semantic Relatedness Us-ing Wikipedia.
In AAAI.Denis Turdakov and Pavel Velikhov.
2008.
Semanticrelatedness metric for Wikipedia concepts based onlink analysis and its application to word sense dis-ambiguation.
In Proceedings of CEUR.Peter D. Turney, Michael L. Littman, Jeffrey Bigham,and Victor Shnayder.
2003.
Combining Indepen-dent Modules in Lexical Multiple-Choice Problems.In Recent Advances in Natural Language ProcessingIII: Selected Papers from RANLP 2003.Peter D. Turney.
2001.
Mining the Web for Synonyms:PMI-IR versus LSA on TOEFL.
In Proceedingsof the Twelfth European Conference on MachineLearning (ECML-2001), pages 491?502, Freidburg,Germany.Torsten Zesch, Christof Muller, and Iryna Gurevych.2008.
Using Wiktionary for Computing SemanticRelatedness.
In Proceedings of AAAI 2008.31
