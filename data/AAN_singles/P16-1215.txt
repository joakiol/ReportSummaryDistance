Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2276?2286,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsComposing Distributed Representations of Relational PatternsSho Takase Naoaki Okazaki Kentaro InuiGraduate School of Information Sciences, Tohoku University{takase, okazaki, inui}@ecei.tohoku.ac.jpAbstractLearning distributed representations forrelation instances is a central techniquein downstream NLP applications.
In or-der to address semantic modeling of rela-tional patterns, this paper constructs a newdataset that provides multiple similarityratings for every pair of relational patternson the existing dataset (Zeichner et al,2012).
In addition, we conduct a compar-ative study of different encoders includ-ing additive composition, RNN, LSTM,and GRU for composing distributed rep-resentations of relational patterns.
Wealso present Gated Additive Composition,which is an enhancement of additive com-position with the gating mechanism.
Ex-periments show that the new dataset doesnot only enable detailed analyses of thedifferent encoders, but also provides agauge to predict successes of distributedrepresentations of relational patterns in therelation classification task.1 IntroductionKnowledge about entities and their relations (re-lation instances) are crucial for a wide spectrumof NLP applications, e.g., information retrieval,question answering, and recognizing textual en-tailment.
Learning distributed representations forrelation instances is a central technique in down-stream applications as a number of recent studiesdemonstrated the usefulness of distributed repre-sentations for words (Mikolov et al, 2013; Pen-nington et al, 2014) and sentences (Sutskever etal., 2014; Cho et al, 2014; Kiros et al, 2015).In particular, semantic modeling of relationsand their textual realizations (relational patternshereafter) is extremely important because a rela-Zeichner+ (2012)(increase the risk of, cause):(be open from, close at):.........5 6 6 6 71 1 2 2 2..........Similarity ratings of pattern pairsincrease:risk:open:increase the risk of:cause:Corpus (ukWaC)... evidence that passive smoking increases the risk of lung cancer by 10%.
The research also ...                  ......Cigarette smoking causes breathing problems ...Usually, pubs closes at midnight, and peopleincrease the risk of passive smoking lung cancercause cigarette smoking heart attackclose at pub midnightbe open from department store 10amRelational pattern X Y... ... ...Encoder forrelational patterns (?3)Additive composition,RNN, LSTM, GRU, GACbe open from:Annotating human judgmentsusing crowd sourcing (?2)Embeddings of relational patternsWord embeddings SemEval 2010 Task 8(word2vec)TrainingOpen IE (Reverb)Relational pattern similarity Evaluation (?4.1)Relation classification Evaluation (?4.2)Figure 1: Overview of this study.tion (e.g., causality) can be mentioned by variousexpressions (e.g., ?X cause Y?, ?X lead to Y?, ?Y isassociated with X?).
To make matters worse, rela-tional patterns are highly productive: we can pro-duce a emphasized causality pattern ?X increasethe severe risk of Y?
from ?X increase the risk ofY?
by inserting severe to the pattern.To model the meanings of relational patterns,the previous studies built a co-occurrence matrixbetween relational patterns (e.g., ?X increase therisk of Y?)
and entity pairs (e.g., ?X: smoking,Y: cancer?)
(Lin and Pantel, 2001; Nakashole etal., 2012).
Based on the distributional hypothe-sis (Harris, 1954), we can compute a semantic vec-tor of a relational pattern from the co-occurrencematrix, and measure the similarity of two rela-tional patterns as the cosine similarity of the vec-tors.
Nowadays, several studies adopt distributedrepresentations computed by neural networks forsemantic modeling of relational patterns (Yih etal., 2014; Takase et al, 2016).Notwithstanding, the previous studies paid lit-tle attention to explicitly evaluate semantic mod-eling of relational patterns.
In this paper, we con-struct a new dataset that contains a pair of rela-tional patterns with five similarity ratings judgedby human annotators.
The new dataset shows a2276high inter-annotator agreement, following the an-notation guideline of Mitchell and Lapata (2010).The dataset is publicly available on the Web site1.In addition, we conduct a comparative studyof different encoders for composing distributedrepresentations of relational patterns.
During thecomparative study, we present Gated AdditiveComposition, which is an enhancement of addi-tive composition with the gating mechanism.
Weutilize the Skip-gram objective for training the pa-rameters of the encoders on a large unlabeled cor-pus.
Experiments show that the new dataset doesnot only enable detailed analyses of the differentencoders, but also provides a gauge to predict suc-cesses of distributed representations of relationalpatterns in another task (relation classification).Figure 1 illustrates the overview of this study.2 Data Construction2.1 Target relation instancesWe build a new dataset upon the work of Ze-ichner et al (2012), which consists of relationalpatterns with semantic inference labels annotated.The dataset includes 5,555 pairs2extracted by Re-verb (Fader et al, 2011), 2,447 pairs with infer-ence relation and 3,108 pairs (the rest) withoutone.Initially, we considered using this high-qualitydataset as it is for semantic modeling of relationalpatterns.
However, we found that inference rela-tions exhibit quite different properties from thoseof semantic similarity.
Take a relational patternpair ?X be the part of Y?
and ?X be an essentialpart of Y?
filled with ?X = the small intestine, Y =the digestive system?
as an instance.
The pattern?X be the part of Y?
does not entail ?X be an essen-tial part of Y?
because the meaning of the formerdoes not include ?essential?.
Nevertheless, bothstatements are similar, representing the same rela-tion (PART-OF).
Another uncomfortable pair is ?Xfall down Y?
and ?X go up Y?
filled with ?X = thedude, Y = the stairs?.
The dataset indicates that theformer entails the latter probably because fallingdown from the stairs requires going up there, butthey present the opposite meaning.
For this rea-son, we decided to re-annotate semantic similarity1http://github.com/takase/relPatSim2More precisely, the dataset includes 1,012 meaninglesspairs in addition to 5,555 pairs.
A pair of relational patternswas annotated as meaningless if the annotators were unableto understand the meaning of the patterns easily.
We ignorethe meaningless pairs in this study.judgments on every pair of relational patterns onthe dataset.2.2 Annotation guidelineWe use instance-based judgment in a similar man-ner to that of Zeichner et al (2012) to securea high inter-annotator agreement.
In instance-based judgment, an annotator judges a pair ofrelational patterns whose variable slots are filledwith the same entity pair.
In other words, heor she does not make a judgment for a pair ofrelational patterns with variables, ?X prevent Y?and ?X reduce the risk of Y?, but two instantiatedstatements ?Cephalexin prevent the bacteria?
and?Cephalexin reduce the risk of the bacteria?
(?X =Cephalexin, Y = the bacteria?).
We use the entitypairs provided in Zeichner et al (2012).We asked annotators to make a judgment for apair of relation instances by choosing a rating from1 (dissimilar) to 7 (very similar).
We providedthe following instructions for judgment, which iscompatible with Mitchell and Lapata (2010): (1)rate 6 or 7 if the meanings of two statements arethe same or mostly the same (e.g., ?Palmer teamwith Jack Nicklaus?
and ?Palmer join with JackNicklaus?
); (2) rate 1 or 2 if two statements aredissimilar or unrelated (e.g., ?the kids grow upwith him?
and ?the kids forget about him?
); (3)rate 3, 4, or 5 if two statements have some rela-tionships (e.g., ?Many of you know about the site?and ?Many of you get more information about thesite?, where the two statements differ but also rea-sonably resemble to some extent).2.3 Annotation procedureWe use a crowdsourcing service CrowdFlower3to collect similarity judgments from the crowds.CrowdFlower has the mechanism to assess the re-liability of annotators using Gold Standard Data(Gold, hereafter), which consists of pairs of re-lational patterns with similarity scores assigned.Gold examples are regularly inserted throughoutthe judgment job to enable measurement of theperformance of each worker4.
Two authors ofthis paper annotated 100 pairs extracted randomlyfrom 5,555 pairs, and prepared 80 Gold examplesshowing high agreement.
Ratings of the Gold ex-amples were used merely for quality assessmentof the workers.
In other words, we discarded the3http://www.crowdflower.com/4We allow ?1 differences in rating when we measure theperformance of the workers.2277Figure 2: Number of judgments for each similarityrating.
The total number of judgments is 27, 775(5, 555 pairs?
5 workers).similarity ratings of the Gold examples, and usedthose judged by the workers.To build a high quality dataset, we use judg-ments from workers whose confidence values (re-liability scores) computed by CrowdFlower aregreater than 75%.
Additionally, we force everypair to have at least five judgments from the work-ers.
Consequently, 60 workers participated in thisjob.
In the final version of this dataset, each pairhas five similarity ratings judged by the five mostreliable workers who were involved in the pair.Figure 2 presents the number of judgments foreach similarity rating.
Workers seldom rated 7for a pair of relational patterns, probably becausemost pairs have at least one difference in contentwords.
The mean of the standard deviations ofsimilarity ratings of all pairs is 1.16.
Moreover, wecomputed Spearman?s ?
between similarity judg-ments from each worker and the mean of five judg-ments in the dataset.
The mean of Spearman?s ?of workers involved in the dataset is 0.728.
Thesestatistics show a high inter-annotator agreement ofthe dataset.3 Encoder for Relational PatternsThe new dataset built in the previous section raisestwo new questions ?
What is the reasonablemethod (encoder) for computing the distributedrepresentations of relational patterns?
Is thisdataset useful to predict successes of distributedrepresentations of relational patterns in real ap-plications?
In order to answer these questions, thissection explores various methods for learning dis-tributed representations of relational patterns.3.1 Baseline methods without supervisionA na?
?ve approach would be to regard a rela-tional pattern as a single unit (word) and totrain word/pattern embeddings as usual.
In fact,Mikolov et al (2013) implemented this approachas a preprocessing step, mining phrasal expres-sions with strong collocations from a training cor-pus.
However, this approach might be affected bydata sparseness, which lowers the quality of dis-tributed representations.Another simple but effective approach is ad-ditive composition (Mitchell and Lapata, 2010),where the distributed representation of a relationalpattern is computed by the mean of embeddings ofconstituent words.
Presuming that a relational pat-tern consists of a sequence of T words w1, ..., wT,then we let xt?
Rdthe embedding of the wordwt.
This approach computes1T?Tt=1xtas the em-bedding of the relational pattern.
Muraoka et al(2014) reported that the additive composition is astrong baseline among various methods.3.2 Recurrent Neural NetworkRecently, a number of studies model seman-tic compositions of phrases and sentences byusing (a variant of) Recurrent Neural Network(RNN) (Sutskever et al, 2014; Tang et al, 2015).For a given embedding xtat position t, the vanillaRNN (Elman, 1990) computes the hidden stateht?
Rdby the following recursive equation5,ht= g(Wxxt+ Whht?1).
(1)Here, Wxand Whare d?d matrices (parameters),g(.)
is the elementwise activation function (tanh).We set h0= 0 at t = 1.
In essence, RNN com-putes the hidden state htbased on the one at theprevious position (ht?1) and the word embeddingxt.
Applying Equation 1 from t = 1 to T , weuse hTas the distributed representation of the re-lational pattern.3.3 RNN variantsWe also employ Long Short-Term Memory(LSTM) (Hochreiter and Schmidhuber, 1997) andGated Recurrent Unit (GRU) (Cho et al, 2014) asan encoder for relational patterns.
LSTM has beenapplied successfully to various NLP tasks includ-ing word segmentation (Chen et al, 2015), depen-dency parsing (Dyer et al, 2015), machine trans-lation (Sutskever et al, 2014), and sentiment anal-ysis (Tai et al, 2015).
GRU is also successful inmachine translation (Cho et al, 2014) and various5We do not use a bias term in this study.
We set the num-ber of dimensions of hidden states identical to that of wordembeddings (d) so that we can adapt the objective functionof the Skip-gram model for training (Section 3.5).2278passive smoking increases the risk of lung cancerxs xs+1 xs+2 xs+L-1 xs+L xs+L+1xs-2 xs-1 (3) (4) (5)hs hs+1 hs+2hs+L-1(3)ws ws+1 ws+2 ws+L-1(3) ws+L ws+L+1(4) (5)ws-2 ws-1f s f s+1 f s+2i s i s+1 i s+2 i s+3~ ~ ~ ~Parameter updateby Skip-gram model Parameter update bySkip-gram modelPattern vectorT = L = 4 ?
= 2?
= 2Context window Context windowRelation pattern(word vectors)(context vectors) (context vectors)(hidden vectors)Gated Additive Composition (GAC)Figure 3: Overview of GAC trained with Skip-gram model.
GAC computes the distributed rep-resentation of a relational pattern using the inputgate and forget gate, and learns parameters by pre-dicting surrounding words (Skip-gram model).tasks including sentence similarity, paraphrase de-tection, and sentiment analysis (Kiros et al, 2015).LSTM and GRU are similar in that the both ar-chitectures have gates (input, forget, and outputfor LSTM; reset and update for GRU) to rem-edy the gradient vanishing or explosion problemin training RNNs.
Although some researchers re-ported that GRU is superior to LSTM (Chung etal., 2014), we have no consensus about the supe-riority.
Besides, we are not sure whether LSTMor GRU is really necessary for relational patterns,which ususlly consist of a few words.
Thus, wecompare RNN, LSTM, and GRU empirically withthe same training data and the same training pro-cedure.
Similarly to RNN, we use the hidden statehTof LSTM6or GRU as the distributed represen-tation of a relation pattern.3.4 Gated Additive Composition (GAC)In addition to the gradient problem, LSTM orGRU may be suitable for relational patterns, hav-ing the mechanism of adaptive control of gates forinput words and hidden states.
Consider the rela-tional pattern ?X have access to Y?, whose mean-ing is mostly identical to that of ?X access Y?.Because ?have?
in the pattern is a light verb, itmay be harmful to incorporate the semantic vectorof ?have?
into the distributed representation of thepattern.
The same may be true for the functionalword ?to?
in the pattern.
However, the additivecomposition nor RNN does not have a mechanismto ignore the semantic vectors of these words.
Itis interesting to explore a method somewhere be-tween additive composition and LSTM/GRU: ad-ditive composition with the gating mechanism.For this reason, we present an another variantof RNN in this study.
Inspired by the input and6We omitted peephole connections and bias terms.forget gates in LSTM, we compute the input gateit?
Rdand forget gate ft?
Rdat position t. Weuse them to control the amount to propagate to thehidden state htfrom the current word xtand theprevious state ht?1.it= ?
(Wixxt+ Wihht?1) (2)ft= ?
(Wfxxt+ Wfhht?1) (3)ht= g(ft?
ht?1+ it?
xt) (4)Here, Wix, Wih, Wfx, Wfhare d ?
d matri-ces.
Equation 4 is interpreted as a weighted ad-ditive composition between the vector of the cur-rent word xtand the vector of the previous hid-den state ht?1.
The elementwise weights are con-trolled by the input gate itand forget gate ft; weexpect that input gates are closed (close to zero)and forget gates are opened (close to one) whenthe current word is a control verb or function word.We name this architecture gated additive compo-sition (GAC).3.5 Parameter estimation: Skip-gram modelTo train the parameters of the encoders (RNN,LSTM, GRU, and GAC) on an unlabeled text cor-pus, we adapt the Skip-gram model (Mikolov etal., 2013).
Formally, we designate an occurrenceof a relational pattern p as a subsequence of Lwords ws, ..., ws+L?1in a corpus.
We define?
words appearing before and after pattern p asthe context words, and let Cp= (s ?
?, ..., s ?1, s + L, ..., s + L + ?)
denote the indices ofthe context words.
We define the log-likelihoodof the relational pattern lp, following the objec-tive function of Skip-gram with negative sampling(SGNS) (Levy and Goldberg, 2014).lp=??
?Cp(log ?(h?p?x?)
+K?k=1log ?(?h?p?x??
))(5)In this formula: K denotes the number of nega-tive samples; hp?
Rdis the vector for the rela-tional pattern p computed by each encoder such asRNN; x???
Rdis the context vector for the wordw?7; x????
Rdis the context vector for the word7The Skip-gram model has two kinds of vectors xtandx?tassigned for a word wt.
Equation 2 of the original pa-per (Mikolov et al, 2013) denotes xt(word vector) as v(input vector) and x?t(context vector) as v?
(output vector).The word2vec implementation does not write context (out-put) vectors but only word (input) vectors to a model file.Therefore, we modified the source code to save context vec-tors, and use them in Equation 5.
This modification ensuresthe consistency of the entire model.2279that were sampled from the unigram distribution8at every iteration of?k.At every occurrence of a relational pattern inthe corpus, we use Stochastic Gradient Descent(SGD) and backpropagation through time (BPTT)for training the parameters (matrices) in encoders.More specifically, we initialize the word vectors xtand context vectors x?twith pre-trained values, andcompute gradients for Equation 5 to update the pa-rameters in encoders.
In this way, each encoderis trained to compose a vector of a relational pat-tern so that it can predict the surrounding contextwords.
An advantage of this parameter estimationis that the distributed representations of words andrelational patterns stay in the same vector space.Figure 3 visualizes the training process for GAC.4 ExperimentsIn Section 4.1, we investigate the performance ofthe distributed representations computed by differ-ent encoders on the pattern similarity task.
Section4.2 examines the contribution of the distributedrepresentations on SemEval 2010 Task 8, and dis-cusses the usefulness of the new dataset to predictsuccesses of the relation classification task.4.1 Relational pattern similarityFor every pair in the dataset built in Section 2, wecompose the vectors of the two relational patternsusing an encoder described in Section 3, and com-pute the cosine similarity of the two vectors.
Re-peating this process for all pairs in the dataset, wemeasure Spearman?s ?
between the similarity val-ues computed by the encoder and similarity ratingsassigned by humans.4.1.1 Training procedureWe used ukWaC9as the training corpus for theencoders.
This corpus includes the text of 2 bil-lion words from Web pages crawled in the .ukdomain.
Part-of-speech tags and lemmas are an-notated by TreeTagger10.
We used lowercasedlemmas throughout the experiments.
We applyword2vec to this corpus to pre-train word vec-tors xtand context vectors x?t.
All encoders useword vectors xtto compose vectors of relationalpatterns; and the Skip-gram model uses context8We use the probability distribution of words raised to the3/4 power (Mikolov et al, 2013).9http://wacky.sslmit.unibo.it10http://www.cis.uni-muenchen.de/?schmid/tools/TreeTagger/Figure 4: Performance of each method on the rela-tional pattern similarity task with variation in thenumber of dimensions.vectors x?tto compute the objective function andgradients.
We fix the vectors xtand x?twith pre-trained values during training.We used Reverb (Fader et al, 2011) to theukWaC corpus to extract relational pattern can-didates.
To remove unuseful relational patterns,we applied filtering rules that are compatible withthose used in the publicly available extraction re-sult11.
Additionally, we discarded relational pat-terns appearing in the evaluation dataset through-out the experiments to assess the performance un-der which an encoder composes vectors of unseenrelational patterns.
This preprocessing yielded127, 677 relational patterns.All encoders were implemented on Chainer12, aflexible framework of neural networks.
The hyper-parameters of the Skip-gram model are identicalto those in Mikolov et al (2013): the width ofcontext window ?
= 5, the number of negativesamples K = 5, the subsampling of 10?5.
Foreach encoder that requires training, we tried 0.025,0.0025, and 0.00025 as an initial learning rate, andselected the best value for the encoder.
In contrastto the presentation of Section 3, we compose a pat-tern vector in backward order (from the last to thefirst) because preliminary experiments showed aslight improvement with this treatment.4.1.2 Results and discussionsFigure 4 shows Spearman?s rank correlations ofdifferent encoders when the number of dimensionsof vectors is 100?500.
The figure shows that GACachieves the best performance on all dimensions.Figure 4 includes the performance of the na?
?veapproach, ?NoComp?, which regards a relationalpattern as a single unit (word).
In this approach,11http://reverb.cs.washington.edu/12http://chainer.org/2280Length # NoComp Add LSTM GRU RNN GAC1 636 0.324 0.324 0.324 0.324 0.324 0.3242 1,018 0.215 0.319 0.257 0.274 0.285 0.3213 2,272 0.234 0.386 0.344 0.370 0.387 0.4044 1,206 0.208 0.306 0.314 0.329 0.319 0.323> 5 423 0.278 0.315 0.369 0.384 0.394 0.357All 5,555 0.215 0.340 0.336 0.356 0.362 0.370Table 1: Spearman?s rank correlations on different pattern lengths (number of dimensions d = 500).we allocated a vector hpfor each relational pat-tern p in Equation 5 instead of the vector compo-sition, and trained the vectors of relational patternsusing the Skip-gram model.
The performance waspoor for two reasons: we were unable to computesimilarity values for 1,744 pairs because relationalpatterns in these pairs do not appear in ukWaC;and relational patterns could not obtain sufficientstatistics because of data sparseness.Table 1 reports Spearman?s rank correlationscomputed for each pattern length.
Here, the lengthof a relational-pattern pair is defined by the maxi-mum of the lengths of two patterns in the pair.
Inlength of 1, all methods achieve the same corre-lation score because they use the same word vec-tor xt.
The table shows that additive composition(Add) performs well for shorter relational patterns(lengths of 2 and 3) but poorly for longer ones(lengths of 4 and 5+).
GAC also exhibits the sim-ilar tendency to Add, but it outperforms Add forshorter patterns (lengths of 2 and 3) probably be-cause of the adaptive control of input and forgetgates.
In contrast, RNN and its variants (RNN,GRU, and LSTM) enjoy the advantage on longerpatterns (lengths of 4 and 5+).To examine the roles of input and forget gates ofGAC, we visualize the moments when input/forgetgates are wide open or closed.
More precisely, weextract the input word and scanned words when|it|2or |ft|2is small (close to zero) or large (closeto one) on the relational-pattern dataset.
We re-state that we compose a pattern vector in backwardorder (from the last to the first): GAC scans ?of?,?author?, and ?be?
in this order for composing thevector of the relational pattern ?be author of?.Table 2 displays the top three examples iden-tified using the procedure.
The table shows twogroups of tendencies.
Input gates open and forgetgates close when scanned words are only a prepo-sition and the current word is a content word.
Inthese situations, GAC tries to read the semanticwtwt+1wt+2...large itreimburse for(input payable inopen) liable tosmall ita charter member of(input a valuable member ofclose) be an avid reader oflarge ftbe eligible to participate in(forget be require to submitopen) be request to submitsmall ftcoauthor of(forget capital ofclose) center ofTable 2: Prominent moments for input/forgetgates.vector of the content word and to ignore the se-mantic vector of the preposition.
In contrast, inputgates close and forget gates open when the currentword is ?be?
or ?a?
and scanned words form a nounphrase (e.g., ?charter member of?
), a complement(e.g., ?eligible to participate in?
), or a passivevoice (e.g., ?require(d) to submit?).
This behavioris also reasonable because GAC emphasizes infor-mative words more than functional words.4.2 Relation classification4.2.1 Experimental settingsTo examine the usefulness of the dataset and dis-tributed representations for a different application,we address the task of relation classification onthe SemEval 2010 Task 8 dataset (Hendrickx etal., 2010).
In other words, we explore whetherhigh-quality distributed representations of rela-tional patterns are effective to identify a relationtype of an entity pair.The dataset consists of 10, 717 relation in-stances (8, 000 training and 2, 717 test instances)with their relation types annotated.
The dataset2281Method Feature set F1SVM BoW, POS 77.3SVM + NoComp embeddings, BoW, POS 79.9SVM + LSTM embeddings, BoW, POS 81.1SVM + Add embeddings, BoW, POS 81.1SVM + GRU embeddings, BoW, POS 81.4SVM + RNN embeddings, BoW, POS 81.7SVM + GAC embeddings, BoW, POS 82.0+ dependency, WordNet, NE 83.7Ranking loss + GAC w/ fine-tuning embeddings, BoW, POS+ dependency, WordNet, NE 84.2SVM (Rink and Harabagiu, 2010) BoW, POS, dependency, Google n-gram, etc.
82.2MV-RNN (Socher et al, 2012) embeddings, parse trees 79.1+ WordNet, POS, NE 82.4FCM (Gormley et al, 2015) w/o fine-tuning embeddings, dependency 79.4+ WordNet 82.0w/ fine-tuning embeddings, dependency 82.2+ NE 83.4RelEmb (Hashimoto et al, 2015) embeddings 82.8+ dependency, WordNet, NE 83.5CR-CNN (dos Santos et al, 2015) w/ Other embeddings, word position embeddings 82.7w/o Other embeddings, word position embeddings 84.1depLCNN (Xu et al, 2015) embeddings, dependency 81.9+ WordNet 83.7depLCNN + NS embeddings, dependency 84.0+ WordNet 85.6Table 3: F1 scores on the SemEval 2010 dataset.defines 9 directed relations (e.g.,CAUSE-EFFECT)and 1 undirected relation OTHER.
Given a pairof entity mentions, the task is to identify a rela-tion type in 19 candidate labels (2 ?
9 directed +1 undirected relations).
For example, given thepair of entity mentions e1= ?burst?
and e2=?pressure?
in the sentence ?The burst has beencaused by water hammer pressure?, a system isexpected to predict CAUSE-EFFECT(e2, e1).We used Support Vector Machines (SVM) witha Radial Basis Function (RBF) kernel imple-mented in libsvm13.
Basic features are: part-of-speech tags (predicted by TreeTagger), surfaceforms, lemmas of words appearing between an en-tity pair, and lemmas of the words in the entitypair.
Additionally, we incorporate distributed rep-resentations of a relational pattern, entities, and aword before and after the entity pair (number ofdimensions d = 500).
In this task, we regardwords appearing between an entity pair as a re-13https://www.csie.ntu.edu.tw/?cjlin/libsvm/lational pattern.
We compare the vector represen-tations of relational patterns computed by the fiveencoders presented in Section 4.1: additive com-position, RNN, GRU, LSTM, and GAC.
Hyper-parameters related to SVM were tuned by 5-foldcross validation on the training data.4.2.2 Results and discussionsTable 3 presents the macro-averaged F1 scores onthe SemEval 2010 Task 8 dataset.
The first groupof the table provides basic features and enhance-ments with the distributed representations.
Wecan observe a significant improvement even fromthe distributed representation of NoComp (77.3to 79.9).
Moreover, the distributed representationthat exhibited the high performance on the patternsimilarity task was also successful on this task;GAC, which yielded the highest performance onthe pattern similarity task, also achieved the bestperformance (82.0) of all encoders on this task.It is noteworthy that the improvements broughtby the different encoders on this task roughly cor-2282respond to the performance on the pattern similar-ity task.
This fact implies two potential impacts.First, the distributed representations of relationalpatterns are useful and easily transferable to othertasks such as knowledge base population.
Second,the pattern similarity dataset provides a gauge topredict successes of distributed representations inanother task.We could further improve the performance ofSVM + GAC by incorporating external resourcesin the similar manner as the previous studiesdid.
Concretely, SVM + GAC achieved 83.7 F1score by adding features for WordNet, named en-tities (NE), and dependency paths explained inHashimoto et al (2015).
Moreover, we could ob-tain 84.2 F1 score, using the ranking based lossfunction (dos Santos et al, 2015) and fine-tuningof the distributed representations initially trainedby GAC.
Currently, this is the second best scoreamong the performance values reported in the pre-vious studies on this task (the second group of Ta-ble 3).
If we could use the negative sampling tech-nique proposed by Xu et al (2015), we might im-prove the performance further14.5 Related WorkMitchell and Lapata (2010) was a pioneering workin semantic modeling of short phrases.
They con-structed the dataset that contains two-word phrasepairs with semantic similarity judged by humanannotators.
Korkontzelos et al (2013) provided asemantic similarity dataset with pairs of two wordsand a single word.
Wieting et al (2015) annotateda part of PPDB (Ganitkevitch et al, 2013) to eval-uate semantic modeling of paraphrases.
Althoughthe target unit of semantic modeling is differentfrom that for these previous studies, we follow theannotation guideline and instruction of Mitchelland Lapata (2010) to build the new dataset.The task addressed in this paper is also re-lated to the Semantic Textual Similarity (STS)task (Agirre et al, 2012).
STS is the task to mea-sure the degree of semantic similarity between twosentences.
Even though a relational pattern ap-pears as a part of a sentence, it may be difficultto transfer findings from one to another: for exam-ple, the encoders of RNN and its variants explored14In fact, we made substantial efforts to introduce the nega-tive sampling technique.
However, Xu et al (2015) omits thedetail of the technique probably because of the severe pagelimit of short papers.
For this reason, we could not reproducetheir method in this study.in this study may exhibit different characteristics,influenced by the length and complexity of inputtext expressions.In addition to data construction, this paper ad-dresses semantic modeling of relational patterns.Nakashole et al (2012) approached the similartask by constructing a taxonomy of relational pat-terns.
They represented a vector of a relational pat-tern as the distribution of entity pairs co-occurringwith the relational pattern.
Grycner et al (2015)extended Nakashole et al (2012) to generalize di-mensions of the vector space (entity pairs) by in-corporating hyponymy relation between entities.They also used external resources to recognize thetransitivity of pattern pairs and applied transitivi-ties to find patterns in entailment relation.
Thesestudies did not consider semantic composition ofrelational patterns.
Thus, they might suffer fromthe data sparseness problem, as shown by No-Comp in Figure 4.Numerous studies have been aimed at encod-ing distributed representations of phrases and sen-tences from word embeddings by using: Recur-sive Neural Network (Socher et al, 2011), MatrixVector Recursive Neural Network (Socher et al,2012), Recursive Neural Network with differentweight matrices corresponding to syntactic cate-gories (Socher et al, 2013) or word types (Takaseet al, 2016), RNN (Sutskever et al, 2011),LSTM (Sutskever et al, 2014), GRU (Cho et al,2014), PAS-CLBLM (Hashimoto et al, 2014), etc.As described in Section 3, we applied RNN, GRU,and LSTM to compute distributed representationsof relational patterns because recent papers havedemonstrated their superiority in semantic compo-sition (Sutskever et al, 2014; Tang et al, 2015).In this paper, we presented a comparative study ofdifferent encoders for semantic modeling of rela-tional patterns.To investigate usefulness of the distributed rep-resentations and the new dataset, we adopted therelation classification task (SemEval 2010 Task 8)as a real application.
On the SemEval 2010 Task8, several studies considered semantic composi-tion.
Gormley et al (2015) proposed Feature-richCompositional Embedding Model (FCM) that cancombine binary features (e.g., positional indica-tors) with word embeddings via outer products.dos Santos et al (2015) addressed the task usingConvolutional Neural Network (CNN).
Xu et al(2015) achieved a higher performance than dos2283Santos et al (2015) by application of CNN on de-pendency paths.In addition to the relation classification task,we briefly describe other applications.
To popu-late a knowledge base, Riedel et al (2013) jointlylearned latent feature vectors of entities, relationalpatterns, and relation types in the knowledge base.Toutanova et al (2015) adapted CNN to capturethe compositional structure of a relational patternduring the joint learning.
For open domain ques-tion answering, Yih et al (2014) proposed themethod to map an interrogative sentence on an en-tity and a relation type contained in a knowledgebase by using CNN.Although these reports described good perfor-mance on the respective tasks, we are unsure of thegenerality of distributed representations trainedfor a specific task such as the relation classifica-tion.
In contrast, this paper demonstrated the con-tribution of distributed representations trained in ageneric manner (with the Skip-gram objective) tothe task of relation classification.6 ConclusionIn this paper, we addressed the semantic model-ing of relational patterns.
We introduced the newdataset in which humans rated multiple similar-ity scores for every pair of relational patterns onthe dataset of semantic inference (Zeichner et al,2012).
Additionally, we explored different en-coders for composing distributed representationsof relational patterns.
The experimental resultsshows that Gated Additive Composition (GAC),which is a combination of additive compositionand the gating mechanism, is effective to composedistributed representations of relational patterns.Furthermore, we demonstrated that the presenteddataset is useful to predict successes of the dis-tributed representations in the relation classifica-tion task.We expect that several further studies will usethe new dataset not only for distributed represen-tations of relational patterns but also for other NLPtasks (e.g., paraphrasing).
Analyzing the internalmechanism of LSTM, GRU, and GAC, we plan toexplore an alternative architecture of neural net-works that is optimal for relational patterns.AcknowledgmentsWe thank the reviewers and Jun Suzuki for valu-able comments.
This work was partially sup-ported by Grant-in-Aid for JSPS Fellows Grantno.
26.5820, JSPS KAKENHI Grant number15H05318, and JST, CREST.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: A pi-lot on semantic textual similarity.
In The First JointConference on Lexical and Computational Seman-tics (*SEM 2012), pages 385?393.Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu,and Xuanjing Huang.
2015.
Long short-term mem-ory neural networks for chinese word segmentation.In Proceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2015), pages 1197?1206.Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, and Yoshua Bengio.
2014.
Learningphrase representations using rnn encoder?decoderfor statistical machine translation.
In Proceedings ofthe 2014 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP 2014), pages1724?1734.Junyoung Chung, C?aglar G?ulc?ehre, KyungHyun Cho,and Yoshua Bengio.
2014.
Empirical evaluation ofgated recurrent neural networks on sequence model-ing.Cicero dos Santos, Bing Xiang, and Bowen Zhou.2015.
Classifying relations by ranking with con-volutional neural networks.
In Proceedings of the53rd Annual Meeting of the Association for Compu-tational Linguistics and the 7th International JointConference on Natural Language Processing (ACL-IJCNLP 2015), pages 626?634.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependency parsing with stack long short-term memory.
In Proceedings of the 53rd An-nual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Con-ference on Natural Language Processing (ACL-IJCNLP 2015), pages 334?343.Jeffrey L Elman.
1990.
Finding structure in time.Cognitive science, 14(2):179?211.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing(EMNLP 2011), pages 1535?1545.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
Ppdb: The paraphrasedatabase.
In Proceedings of the 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies (NAACL-HLT 2013), pages 758?764.2284Matthew R. Gormley, Mo Yu, and Mark Dredze.
2015.Improved relation extraction with feature-rich com-positional embedding models.
In Proceedings of the2015 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2015), pages 1774?1784.Adam Grycner, Gerhard Weikum, Jay Pujara, JamesFoulds, and Lise Getoor.
2015.
Relly: Inferringhypernym relationships between relational phrases.In Proceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2015), pages 971?981.Zellig Harris.
1954.
Distributional structure.
Word,10(23):146?162.Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa,and Yoshimasa Tsuruoka.
2014.
Jointly learn-ing word representations and composition functionsusing predicate-argument structures.
In Proceed-ings of the 2014 Conference on Empirical Methodsin Natural Language Processing (EMNLP 2014),pages 1544?1555.Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa,and Yoshimasa Tsuruoka.
2015.
Task-orientedlearning of word embeddings for semantic relationclassification.
In Proceedings of the 19th Confer-ence on Computational Natural Language Learning(CoNLL 2015), pages 268?278.Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,Preslav Nakov, Diarmuid?O S?eaghdha, SebastianPad?o, Marco Pennacchiotti, Lorenza Romano, andStan Szpakowicz.
2010.
Semeval-2010 task 8:Multi-way classification of semantic relations be-tween pairs of nominals.
In Proceedings of the5th International Workshop on Semantic Evaluation,pages 33?38.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural Computation,9(8):1735?1780.Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,Richard S. Zemel, Antonio Torralba, Raquel Urta-sun, and Sanja Fidler.
2015.
Skip-thought vectors.In Advances in Neural Information Processing Sys-tems 28 (NIPS 2015), pages 3276?3284.Ioannis Korkontzelos, Torsten Zesch, Fabio MassimoZanzotto, and Chris Biemann.
2013.
Semeval-2013task 5: Evaluating phrasal semantics.
In SecondJoint Conference on Lexical and Computational Se-mantics (*SEM 2013), pages 39?47.Omer Levy and Yoav Goldberg.
2014.
Neural wordembedding as implicit matrix factorization.
In Ad-vances in Neural Information Processing Systems 27(NIPS 2014), pages 2177?2185.Dekang Lin and Patrick Pantel.
2001.
Dirt ?
discoveryof inference rules from text.
In Proceedings of theSeventh ACM SIGKDD International Conference onKnowledge Discovery and Data Mining (KDD 01),pages 323?328.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems 26 (NIPS 2013), pages 3111?3119.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1439.Masayasu Muraoka, Sonse Shimaoka, Kazeto Ya-mamoto, Yotaro Watanabe, Naoaki Okazaki, andKentaro Inui.
2014.
Finding the best model amongrepresentative compositional models.
In Proceed-ings of the 28th Pacific Asia Conference on Lan-guage, Information, and Computation (PACLIC 28),pages 65?74.Ndapandula Nakashole, Gerhard Weikum, and FabianSuchanek.
2012.
Patty: A taxonomy of rela-tional patterns with semantic types.
In Proceedingsof the 2012 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning (EMNLP-CoNLL2012), pages 1135?1145.Jeffrey Pennington, Richard Socher, and ChristopherManning.
2014.
Glove: Global vectors for wordrepresentation.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2014), pages 1532?1543.Sebastian Riedel, Limin Yao, Andrew McCallum, andBenjamin M. Marlin.
2013.
Relation extractionwith matrix factorization and universal schemas.
InProceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(NAACL-HLT 2013), pages 74?84.Bryan Rink and Sanda Harabagiu.
2010.
Utd: Clas-sifying semantic relations by combining lexical andsemantic resources.
In Proceedings of the 5th Inter-national Workshop on Semantic Evaluation, pages256?259.Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,and Christopher D. Manning.
2011.
Parsing natu-ral scenes and natural language with recursive neu-ral networks.
In Proceedings of the 28th Inter-national Conference on Machine learning (ICML2011), pages 129?136.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic com-positionality through recursive matrix-vector spaces.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL 2012), pages 1201?1211.Richard Socher, John Bauer, Christopher D. Manning,and Ng Andrew Y.
2013.
Parsing with composi-tional vector grammars.
In Proceedings of the 51stAnnual Meeting of the Association for Computa-tional Linguistics (ACL 2013), pages 455?465.2285Ilya Sutskever, James Martens, and Geoffrey Hinton.2011.
Generating text with recurrent neural net-works.
In Proceedings of the 28th InternationalConference on Machine Learning (ICML 2011),pages 1017?1024.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to sequence learning with neural net-works.
In Advances in Neural Information Process-ing Systems 27 (NIPS 2014), pages 3104?3112.Kai Sheng Tai, Richard Socher, and Christopher D.Manning.
2015.
Improved semantic representa-tions from tree-structured long short-term memorynetworks.
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Linguis-tics and the 7th International Joint Conference onNatural Language Processing (ACL-IJCNLP 2015),pages 1556?1566.Sho Takase, Naoaki Okazaki, and Kentaro Inui.
2016.Modeling semantic compositionality of relationalpatterns.
Engineering Applications of Artificial In-telligence, 50(C):256?264.Duyu Tang, Bing Qin, and Ting Liu.
2015.
Docu-ment modeling with gated recurrent neural networkfor sentiment classification.
In Proceedings of the2015 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2015), pages 1422?1432.Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-fung Poon, Pallavi Choudhury, and Michael Ga-mon.
2015.
Representing text for joint embeddingof text and knowledge bases.
In Proceedings of the2015 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2015), pages 1499?1509.John Wieting, Mohit Bansal, Kevin Gimpel, and KarenLivescu.
2015.
From paraphrase database to com-positional paraphrase model and back.
Transac-tions of the Association for Computational Linguis-tics (TACL 2015), 3:345?358.Kun Xu, Yansong Feng, Songfang Huang, andDongyan Zhao.
2015.
Semantic relation classifica-tion via convolutional neural networks with simplenegative sampling.
In Proceedings of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2015), pages 536?540.Wen-tau Yih, Xiaodong He, and Christopher Meek.2014.
Semantic parsing for single-relation ques-tion answering.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Lin-guistics (ACL 2014), pages 643?648.Naomi Zeichner, Jonathan Berant, and Ido Dagan.2012.
Crowdsourcing inference-rule evaluation.
InProceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2012),pages 156?160.2286
