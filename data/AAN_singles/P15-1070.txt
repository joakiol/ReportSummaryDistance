Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 719?729,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsAutomatic disambiguation of English punsTristan Miller and Iryna GurevychUbiquitous Knowledge Processing Lab (UKP-TUDA)Department of Computer Science, Technische Universit?at Darmstadthttps://www.ukp.tu-darmstadt.de/AbstractTraditional approaches to word sense dis-ambiguation (WSD) rest on the assump-tion that there exists a single, unambigu-ous communicative intention underlyingevery word in a document.
However, writ-ers sometimes intend for a word to be in-terpreted as simultaneously carrying mul-tiple distinct meanings.
This deliberateuse of lexical ambiguity?i.e., punning?is a particularly common source of humour.In this paper we describe how traditional,language-agnostic WSD approaches can beadapted to ?disambiguate?
puns, or ratherto identify their double meanings.
We eval-uate several such approaches on a manuallysense-annotated collection of English punsand observe performance exceeding thatof some knowledge-based and supervisedbaselines.1 IntroductionWord sense disambiguation, or WSD, is the task ofidentifying a word?s meaning in context.
No matterwhether it is performed by a human or a machine,WSD usually rests on the assumption that thereis a single unambiguous communicative intentionunderlying each word in the document.1However,there exists a class of language constructs known1Under this assumption, lexical ambiguity arises due tothere being a plurality of words with the same surface formbut different meanings, and the task of the interpreter is toselect correctly among them.
An alternative view is that eachword is a single lexical entry whose specific meaning is un-derspecified until it is activated by the context (Ludlow, 1996).In the case of systematically polysemous terms (i.e., wordsthat have several related senses shared in a systematic way bya group of similar words), it may not be necessary to disam-biguate them at all in order to interpret the communication(Buitelaar, 2000).
While there has been some research in mod-elling intentional lexical-semantic underspecification (Jurgens,2014), it is intended for closely related senses such as those ofsystematically polysemous terms, not those of coarser-grainedhomonyms which are the subject of this paper.as paronomasia and syllepsis, or more generally aspuns, in which homonymic (i.e., coarse-grained)lexical-semantic ambiguity is a deliberate effect ofthe communication act.
That is, the writer intendsfor a certain word or other lexical item to be in-terpreted as simultaneously carrying two or moreseparate meanings, or alternatively for it to be un-clear which meaning is the intended one.
There area variety of motivations writers have for employingsuch constructions, and in turn for why such usesare worthy of scholarly investigation.Perhaps surprisingly, this sort of intentional lex-ical ambiguity has attracted little attention in thefields of computational linguistics and natural lan-guage processing.
What little research has beendone is confined largely to computational mecha-nisms for pun generation (in the context of natu-ral language generation for computational humour)and to computational analysis of phonological prop-erties of puns.
A fundamental problem which hasnot yet been as widely studied is the automaticdetection and identification of intentional lexicalambiguity?that is, given a text, does it containany lexical items which are used in a deliberatelyambiguous manner, and if so, what are the intendedmeanings?We consider these to be important research ques-tions with a number of real-world applications.For instance, puns are particularly common in ad-vertising, where they are used not only to createhumour but also to induce in the audience a va-lenced attitude toward the target (Valitutti et al,2008).
Recognizing instances of such lexical am-biguity and understanding their affective connota-tions would be of benefit to systems performingsentiment analysis on persuasive texts.
Wordplayis also a perennial topic of scholarship in literarycriticism and analysis.
To give just one example,puns are one of the most intensively studied as-pects of Shakespeare?s rhetoric, and laborious man-ual counts have shown their frequency in certain719of his plays to range from 17 to 85 instances perthousand lines (Keller, 2009).
It is not hard toimage how computer-assisted detection, classifi-cation, and analysis of puns could help scholarsin the digital humanities.
Finally, computationalpun detection and understanding hold tremendouspotential for machine-assisted translation.
Some ofthe most widely disseminated and translated popu-lar discourses?particularly television shows andmovies?feature puns and other forms of wordplayas a recurrent and expected feature (Schr?oter, 2005).These pose particular challenges for translators,who need not only to recognize and comprehendeach instance of humour-provoking ambiguity, butalso to select and implement an appropriate trans-lation strategy.2NLP systems could assist transla-tors in flagging intentionally ambiguous words forspecial attention, and where they are not directlytranslatable (as is usually the case), the systemsmay be able to propose ambiguity-preserving alter-natives which best match the original pun?s doublemeaning.In the present work, we discuss the adaptation ofautomatic word sense disambiguation techniquesto intentionally ambiguous text and evaluate theseadaptations in a controlled setting.
We focus onhumorous puns, as these are by far the most com-monly encountered and more readily available in(and extractable from) existing text corpora.The remainder of this paper is structured as fol-lows: In the following section we give a brief intro-duction to puns, WSD, and related previous workon computational detection and comprehension ofhumour.
In ?3 we describe the data set producedfor our experiments.
In ?
?4 and 5 we describe howdisambiguation algorithms, evaluation metrics, andbaselines from traditional WSD can be adapted tothe task of pun identification, and in ?6 we reportand discuss the performance of our adapted sys-tems.
Finally, we conclude in ?7 with a review ofour research contributions and an outline of ourplans for future work.2 Background2.1 PunsPunning is a form of wordplay where a word isused in such a way as to evoke several indepen-dent meanings simultaneously.
Humorous and non-2The problem is compounded in audio-visual media suchas films; often one or both of the pun?s meanings appears inthe visual channel, and thus cannot be freely substituted.humorous puns have been the subject of extensivestudy in the humanities and social sciences, whichhas led to insights into the nature of language-basedhumour and wordplay, including their role in com-merce, entertainment, and health care; how they areprocessed in the brain; and how they vary over timeand across cultures (Monnot, 1982; Culler, 1988;Lagerwerf, 2002; Bell et al, 2011; Bekinschtein etal., 2011).
Study of literary puns imparts a greaterunderstanding of the cultural or historical contextin which the literature was produced, which is of-ten necessary to properly interpret and translate it(Delabastita, 1997).Puns can be classified in various ways (Attardo,1994), though from the point of view of our par-ticular natural language processing application themost important distinction is between homographicand homophonic puns.
A homographic pun ex-ploits distinct meanings of the same written word,and a homophonic pun exploits distinct meaningsof the same spoken word.
Puns can be homo-graphic, homophonic, both, or neither, as the fol-lowing examples illustrate:(1) A lumberjack?s world revolves on its axes.
(2) She fell through the window but felt nopane.
(3) A political prisoner is one who stands be-hind her convictions.
(4) The sign at the nudist camp read, ?Clotheduntil April.
?In (1), the pun on axes is homographic but nothomophonic, since the two meanings (?more thanone axe?
and ?more than one axis?)
share the samespelling but have different pronunciations.
In (2),the pun on pane (?sheet of glass?)
is homophonicbut not homographic, since the word for the sec-ondary meaning (?feeling of injury?)
is properlyspelled pain but pronounced the same.
The pun onconvictions (?strongly held beliefs?
and ?findingsof criminal guilt?)
in (3) is both homographic andhomophonic.
Finally, the pun on clothed in (4) isneither homographic nor homophonic, since theword for the secondary meaning, closed, differsin both spelling and pronunciation.
Such puns arecommonly known as imperfect puns.Other characteristics of puns important for ourwork include whether they involve compounds,multiword expressions, or proper names, andwhether the pun?s multiple meanings involve mul-720tiple parts of speech.
We elaborate on the signifi-cance of these characteristics in the next section.2.2 Word sense disambiguationWord sense disambiguation (WSD) is the task ofdetermining which sense of a polysemous term isthe one intended when that term is used in a givencommunicative act.
Besides the target term itself,a WSD system generally requires two inputs: thecontext (i.e., the running text containing the target),and a sense inventory which specifies all possiblesenses of the target.Approaches to WSD can be categorized accord-ing to the type of knowledge sources used to helpdiscriminate senses.
Knowledge-based approachesrestrict themselves to using pre-existing lexical-semantic resources (LSRs), or such additional infor-mation as can be automatically extracted or minedfrom raw text corpora.
Supervised approaches, onthe other hand, use manually sense-annotated cor-pora as training data for a machine learning sys-tem, or as seed data for a bootstrapping process.Supervised WSD systems generally outperformtheir knowledge-based counterparts, though thiscomes at the considerable expense of having hu-man annotators manually disambiguate hundredsor thousands of example sentences.
Moreover, su-pervised approaches tend to be such that they candisambiguate only those words for which they haveseen sufficient training examples to cover all senses.That is, most of them cannot disambiguate wordswhich do not occur in the training data, nor canthey select the correct sense of a known word ifthat sense was never observed in the training data.Regardless of the approach, all WSD systemswork by extracting contextual information for thetarget word and comparing it against the senseinformation stored for that word.
A seminalknowledge-based example is the Lesk algorithm(Lesk, 1986) which disambiguates a pair of tar-get terms in context by comparing their respectivedictionary definitions and selecting the two withthe greatest number of words in common.
Thoughsimple, the Lesk algorithm performs surprisinglywell, and has frequently served as the basis of moresophisticated approaches.
In recent years, Leskvariants in which the contexts and definitions aresupplemented with entries from a distributional the-saurus (Lin, 1998) have achieved state-of-the-artperformance for knowledge-based systems on stan-dard data sets (Miller et al, 2012; Basile et al,2014).In traditional word sense disambiguation, thepart of speech and lemma of the target word areusually known a priori, or can be determined withhigh accuracy using off-the-shelf natural languageprocessing tools.
The pool of candidate senses cantherefore be restricted to those whose lexicaliza-tions exactly match the target lemma and part ofspeech.
No such help is available for puns, at leastnot in the general case.
Take the following twoexamples:(5) Tom moped.
(6) ?I want a scooter,?
Tom moped.In the first of these sentences, the word mopedis unambiguously a verb with the lemma mope,and would be correctly recognized as such by anyautomatic lemmatizer and part-of-speech tagger.The moped of the second example is a pun, one ofwhose meanings is the same inflected form of theverb mope (?to sulk?)
and the other of which is thenoun moped (?motorized scooter?).
For such casesan automated pun identifier would therefore needto account for all possible lemmas for all possibleparts of speech of the target word.
The situationbecomes even more onerous for heterographic andimperfect puns, which may require the use of pro-nunciation dictionaries, and application of phono-logical theories of punning, in order to recover thelemmas (Hempelmann, 2003).As our research interests are in lexical semanticsrather than phonology, we focus on puns which arehomographic and monolexemic.
This allows us toinvestigate the problem of pun identification in ascontrolled a setting as possible.2.3 Previous work2.3.1 Computational humourThere is some previous research on computationaldetection and comprehension of humour, though byand large it is not concerned specifically with puns;those studies which do analyze puns tend to havea phonological or syntactic rather than semanticbent.
In this subsection we briefly review someprior work which is relevant to ours.Yokogawa (2002) describes a system for detect-ing the presence of puns in Japanese text.
However,this work is concerned only with puns which areboth imperfect and ungrammatical, relying on syn-tactic cues rather than the lexical-semantic informa-tion we propose to use.
Taylor and Mazlack (2004)721describe an n-gram?based approach for recogniz-ing when imperfect puns are used for humorouseffect in a certain narrow class of English knock-knock jokes.
Their focus on imperfect puns andtheir use of a fixed syntactic context makes theirapproach largely inapplicable to perfect puns inrunning text.
Mihalcea and Strapparava (2005)treat humour recognition as a classification task,employing various machine learning techniqueson humour-specific stylistic features such as al-literation and antonymy.
Of particular interest istheir follow-up analysis (Mihalcea and Strapparava,2006), where they specifically point to their sys-tem?s failure to resolve lexical-semantic ambiguityas a stumbling block to better accuracy, and specu-late that deeper semantic analysis of the text, suchas via word sense disambiguation or domain disam-biguation, could aid in the detection of humorousincongruity and opposition.The previous work which is perhaps most rele-vant to ours is that of Mihalcea et al (2010).
Theybuild a data set consisting of 150 joke set-ups, eachof which is followed by four possible ?punchlines?,only one of which is actually humorous (but notnecessarily due to a pun).
They then comparethe set-ups against the punchlines using variousmodels of incongruity detection, including manyexploiting knowledge-based semantic relatednesssuch as Lesk.
The Lesk model had an accuracyof 56%, which is lower than that of a na?
?ve pol-ysemy model which simply selects the punchlinewith the highest mean polysemy (66%) and evenof a random-choice baseline (62%).
However, itshould be stressed here that the Lesk model did notdirectly account for the possibility that any givenword might be ambiguous.
Rather, for every wordin the setup, the Lesk measure was used to select aword in the punchline such that the lexical overlapbetween each one of their possible definitions wasmaximized.
The overlap scores for all word pairswere then averaged, and the punchline with the low-est average score selected as the most humorous.2.3.2 CorporaThere are a number of English-language corporaof intentional lexical ambiguity which have beenused in past work, usually in linguistics or the so-cial sciences.
In their work on computer-generatedhumour, Lessard et al (2002) use a corpus of 374?Tom Swifty?
puns taken from the Internet, plusa well-balanced corpus of 50 humorous and non-humorous lexical ambiguities generated program-matically (Venour, 1999).
Hong and Ong (2009)also study humour in natural language generation,using a smaller data set of 27 punning riddles de-rived from a mix of natural and artificial sources.In their study of wordplay in religious advertis-ing, Bell et al (2011) compile a corpus of 373puns taken from church marquees and literature,and compare it against a general corpus of 1515puns drawn from Internet websites and a special-ized dictionary.
Zwicky and Zwicky (1986) con-duct a phonological analysis on a corpus of severalthousand puns, some of which they collected them-selves from advertisements and catalogues, and theremainder of which were taken from previouslypublished collections.
Two studies on cognitivestrategies used by second language learners (Ka-plan and Lucas, 2001; Lucas, 2004) used a data setof 58 jokes compiled from newspaper comics, 32of which rely on lexical ambiguity.
Bucaria (2004)conducts a linguistic analysis of a set of 135 hu-morous newspaper headlines, about half of whichexploit lexical ambiguity.Such data sets?particularly the larger ones?provided us good evidence that intentionally lexicalambiguous exemplars exist in sufficient numbers tomake a rigorous evaluation of our task feasible.
Un-fortunately, none of the above-mentioned corporahave been published in full, and moreover many ofthem contain (sometimes exclusively) the sort ofimperfect or otherwise heterographic puns whichwe mean to exclude from consideration.
This hasmotivated us to produce our own corpus of puns,the construction and analysis of which is describedin the following section.3 Data setAs in traditional WSD, a prerequisite for our re-search is a corpus of examples, where one or morehuman annotators have already identified the am-biguous words and marked up their various mean-ings with reference to a given sense inventory.
Sucha corpus is sufficient for evaluating what we termpun identification or pun disambiguation?that is,identifying the senses of a term known a priori tobe a pun.3.1 ConstructionThough several prior studies have produced corporaof puns, none of them are systematically sense-annotated.
We therefore compiled our own corpusby pooling together some of the aforementioned722corpora, the user-submitted puns from the Pun ofthe Day website,3and private collections providedto us by some professional humorists.
This rawcollection of 7750 one-liners was then filtered bytrained human annotators to those instances meet-ing the following four criteria:One pun per instance: Of all the lexical units inthe instance, one and only one may be a pun.
(This criterion simplifies the task detecting thepresence and location of puns in a text, a clas-sification task which we intend to investigatein future work.
)One content word per pun: The lexical unit thatforms the pun must consist of, or contain, onlya single content word (i.e., a noun, verb, adjec-tive, or adverb), excepting adverbial particlesof phrasal verbs.
This criterion is importantbecause, in our observations, it is often onlyone word which carries ambiguity in puns oncompounds and multi-word expressions.
Ac-cepting lexical units containing more than onecontent word would have required our annota-tors to laboriously partition the pun into (pos-sibly overlapping) sense-bearing units and toassign sense sets to each of them, inflating thecomplexity of the annotation task to unaccept-able levels.Two meanings per pun: The pun must have ex-actly two distinct meanings.
Though manysources state that puns have only two senses(Redfern, 1984; Attardo, 1994), our annota-tors identified a handful of corpus exampleswhere the pun could plausibly be analyzed ascarrying three distinct meanings.
To simplifyour manual annotation procedure and our eval-uation metrics we excluded these rare outliers.Weak homography: The lexical units corre-sponding to the two distinct meanings must bespelled exactly the same way, except that parti-cles and inflections may be disregarded.
Thissomewhat softer definition of homography al-lows us to admit a good many morphologi-cally interesting cases which were nonethelessreadily recognized by our human annotators.The filtering reduced the number of instancesto 1652, whose puns two human judges anno-tated with sense keys from WordNet 3.1 (Fellbaum,3http://www.punoftheday.com/1998).
Using an online annotation tool speciallyconstructed for this study, the annotators appliedtwo sets of sense keys to each instance, one for eachof the two meanings of the pun.
For cases where thedistinction between WordNet?s fine-grained senseswas irrelevant, the annotators had the option oflabelling the meaning with more than one sensekey.
Annotators also had the option of marking ameaning as unassignable if WordNet had no cor-responding sense key.
Further details of our anno-tation tool and its use can be found in Miller andTurkovi?c (2015).3.2 AnalysisOur judges agreed on which word was the punin 1634 out of 1652 cases, a raw agreement of98.91%.
For the agreed cases, we used DKProAgreement (Meyer et al, 2014) to compute Krip-pendorff?s ?
(Krippendorff, 1980) for the senseannotations.
This is a chance-correcting metricof inter-annotator agreement ranging in (?1,1],where 1 indicates perfect agreement, ?1 perfectdisagreement, and 0 the expected score for ran-dom labelling.
Our distance metric for ?
is astraightforward adaptation of the MASI set compar-ison metric (Passonneau, 2006).
Whereas standardMASI, dM(A,B), compares two annotation sets Aand B, our annotations take the form of unorderedpairs of sets {A1,A2} and {B1,B2}.
We thereforefind the mapping between elements of the twopairs that gives the lowest total distance, and halveit: dM?
({A1,A2} ,{B1,B2}) =12min(dM(A1,B1) +dM(A2,B2),dM(A1,B2) + dM(A2,B1)).
With thismethod we observe a Krippendorff?s ?
of 0.777;this is only slightly below the 0.8 threshold recom-mended by Krippendorff, and far higher than whathas been reported in other sense annotation studies(Passonneau et al, 2006; Jurgens and Klapaftis,2013).Where possible, we resolved sense annotationdisagreements automatically by taking the intersec-tion of corresponding sense sets.
Where the an-notators?
sense sets were disjoint or contradictory(including the cases where the annotators disagreedon the pun word), we had a human adjudicator at-tempt to resolve the disagreement in favour of oneannotator or the other.
This left us with 1607 in-stances,4of which we retained only the 1298 thathad successful (i.e., not marked as unassignable)4Pending clearance of the distribution rights, we will makesome or all of our annotated data set available on our websiteat https://www.ukp.tu-darmstadt.de/data/.723annotations for the present study.
The contexts inthis data set range in length from 3 to 44 words,with an average length of 11.9.
The 2596 meaningscarry sense key annotations corresponding to any-where from one to seven WordNet synsets, withan average of 1.08.
As expected, then, WordNet?ssense granularity proved to be somewhat finer thannecessary to characterize the meanings in the dataset, though only marginally so.Of the 2596 individual meanings, 1303 (50.2%)were annotated with noun senses only, 877 (33.8%)with verb senses only, 340 (13.1%) with adjectivesenses only, and 41 (1.6%) with adverb senses only.Only 35 individual meanings (1.3%) carry sense an-notations corresponding to multiple parts of speech.However, for 297 (22.9%) of our puns, the twomeanings had different parts of speech.
Similarly,sense annotations for each individual meaning cor-respond to anywhere from one to four differentlemmas, with a mean of 1.25.
These observationsconfirm the concerns we raised in ?2.2 that pundisambiguators, unlike traditional WSD systems,cannot always rely on the output of a lemmatizeror part-of-speech tagger to narrow down the list ofsense candidates.4 Pun disambiguationIt has long been observed that gloss overlap?basedWSD systems, such as those based on the Leskalgorithm, fail to distinguish between candidatesenses when their definitions have a similar over-lap with the target word?s context.
In some casesthis is because the overlap is negligible or nonexis-tent; this is known as the lexical gap problem, andvarious solutions to it are discussed in (inter alia)Miller et al (2012).
In other cases, the indecisionarises because the definitions provided by the senseinventory are too fine-grained; this problem hasbeen addressed, with varying degrees of success,through sense clustering or coarsening techniques(a short but reasonably comprehensive survey ofwhich appears in Matuschek et al (2014)).
A thirdcondition under which senses cannot be discrimi-nated is when the target word is used in an under-specified or intentionally ambiguous manner.
Wehold that for this third scenario a disambiguator?sinability to discriminate senses should not be seenas a failure condition, but rather as a limitationof the WSD task as traditionally defined.
By re-framing the task so as to permit the assignmentof multiple senses (or groups of senses), we canallow disambiguation systems to sense-annotate in-tentionally ambiguous constructions such as puns.Many approaches to WSD, including Lesk-likealgorithms, involve computing some score for allpossible senses of a target word, and then select-ing the single highest-scoring one as the ?correct?sense.
The most straightforward modification ofthese techniques to pun disambiguation, then, isto have the systems select the two top-scoringsenses, one for each meaning of the pun.
Accord-ingly we applied this modification to the followingknowledge-based WSD algorithms:Simplified Lesk (Kilgarriff and Rosenzweig,2000) disambiguates a target word by examin-ing the definitions5for each of its candidatesenses and selecting the single sense?or inour case, the two senses?which have thegreatest number of words in common withthe context.
As we previously demonstratedthat puns often transcend part of speech, ourpool of candidate senses is constructed asfollows: we apply a morphological analyzerto recover all possible lemmas of the targetword without respect to part of speech, andfor each lemma we add all its senses to thepool.Simplified extended Lesk (Ponzetto and Navigli,2010) is similar to simplified Lesk, except thatthe definition for each sense is concatenatedwith those of neighbouring senses in Word-Net?s semantic network.Simplified lexically expanded Lesk (Miller etal., 2012) is also based on simplified Lesk,with the extension that every word in thecontext and sense definitions is expanded withup to 100 entries from a large distributionalthesaurus.The above algorithms fail to make a sense as-signment when more than two senses are tied forthe highest lexical overlap, or when there is a singlehighest-scoring sense but multiple senses are tiedfor the second-highest overlap.
We therefore de-vised two pun-specific tie-breaking strategies.
Thefirst is motivated by the informal observation that,though the two meanings of a pun may have dif-ferent parts of speech, at least one of the parts5In our implementation, the sense definitions are formedby concatenating the synonyms, gloss, and example sentencesprovided by WordNet.724of speech is grammatical in the context of thesentence, and so would probably be the one as-signed by a stochastic or rule-based POS tagger.Our ?POS?
tie-breaker therefore preferentially se-lects the best sense, or pair of senses, whose POSmatches the one applied to the target by the Stan-ford POS tagger (Toutanova et al, 2003).
For oursecond tie-breaking strategy, we posit that sincehumour derives from the resolution of semantic in-congruity (Raskin, 1985; Attardo, 1994), puns aremore likely to exploit coarse-grained homonymythan than fine-grained systematic polysemy.
Thus,following Matuschek et al (2014), we induced aclustering of WordNet senses by aligning WordNetto the more coarse-grained OmegaWiki LSR.6Our?cluster?
fallback works the same as the ?POS?
one,with the addition that any remaining ties amongsenses with the second-highest overlap are resolvedby preferentially selecting those which are not inthe same induced cluster as, and which in Word-Net?s semantic network are at least three edgesdistant from, the sense with the highest overlap.5 Evaluation5.1 ScoringIn traditional word sense disambiguation, in vitroevaluations are conducted by comparing the sensesassigned by the disambiguation system to the gold-standard senses assigned by the human annotators.For the case that the system and gold-standard as-signments consist of a single sense each, the exact-match criterion is used: the system receives a scoreof 1 if it chose the sense specified by the gold stan-dard, and 0 otherwise.
Where the system selectsa single sense for an instance for which there ismore than one correct gold standard sense, the mul-tiple tags are interpreted disjunctively?that is, thesystem receives a score of 1 if it chose any one ofthe gold-standard senses, and 0 otherwise.
Overallperformance is reported in terms of coverage (thenumber of targets for which a sense assignmentwas attempted), precision (the sum of scores di-vided by the number of attempted targets), recall(the sum of scores divided by the total number oftargets in the data set), and F1(the harmonic meanof precision and recall) (Palmer et al, 2006).The traditional approach to scoring individualtargets is not usable as-is for pun disambiguation,because each pun carries two disjoint but equallyvalid sets of sense annotations.
Instead, since our6http://www.omegawiki.org/systems assign exactly one sense to each of thepun?s two sense sets, we count this as a match(scoring 1) only if each chosen sense can be foundin one of the gold-standard sense sets, and no twogold-standard sense sets contain the same chosensense.
(As with traditional WSD scoring, variousapproaches could be used to assign credit for par-tially correct assignments, though we leave explo-ration of these to future work.
)5.2 BaselinesSystem performance in WSD is normally inter-preted with reference to one or more baselines.
Toour knowledge, ours is the very first study of auto-matic pun disambiguation on any scale, so at thispoint there are no previous systems against whichto compare our results.
However, traditional WSDsystems are often compared with two na?
?ve base-lines (Gale et al, 1992) which can be adapted forour purposes.The first of these na?
?ve baselines is to randomlyselect from among the candidate senses.
In tradi-tional WSD, the score for a random disambiguatorwhich selects a single sense for a given target t isthe number of gold-standard senses divided by thenumber of candidate senses: score(t) = g(t)??
(t).In our pun disambiguation task, however, a ran-dom disambiguator must select two senses?onefor each of the sense sets g1(t) and g2(t)?andthese senses must be distinct.
There are(?
(t)2)pos-sible ways of selecting two unique senses, so therandom score for any given instance is score(t) =g1(t) ?g2(t)?(?
(t)2).The second na?
?ve baseline for WSD, known asmost frequent sense (MFS), is a supervised base-line, meaning that it depends on a manually sense-annotated background corpus.
As its name sug-gests, it involves always selecting from the candi-dates that sense which has the highest frequency inthe corpus.
As with our test algorithms, we adaptthis technique to pun disambiguation by havingit select the two most frequent senses (accordingto WordNet?s built-in sense frequency counts).
Intraditional WSD, MFS baselines are notoriouslydifficult to beat, even for supervised disambigua-tion systems, and since they rely on expensivesense-tagged data they are not normally considereda benchmark for the performance of knowledge-based disambiguators.725system C P R F1SL 35.52 19.74 7.01 10.35SEL 42.45 19.96 8.47 11.90SLEL 98.69 13.43 13.25 13.34SEL+POS 59.94 21.21 12.71 15.90SEL+cluster 68.10 20.70 14.10 16.77random 100.00 9.31 9.31 9.31MFS 100.00 13.25 13.25 13.25Table 1: Coverage, precision, recall, and F1forvarious pun diasmbiguation algorithms.6 ResultsUsing the freely available DKPro WSD framework(Miller et al, 2013), we implemented our pun dis-ambiguation algorithms, ran them on our full dataset, and compared their annotations against thoseof our manually produced gold standard.
Table 1shows the coverage, precision, recall, and F1forsimplified Lesk (SL), simplified extended Lesk(SEL), simplified lexically expanded Lesk (SLEL),and the random and most frequent sense baselines;for SEL we also report results for each of our pun-specific tie-breaking strategies.
All metrics arereported as percentages, and the highest score foreach metric (excluding baseline coverage, which isalways 100%) is highlighted in boldface.Accuracy for the random baseline annotator wasabout 9%; for the MFS baseline it was just over13%.
These figures are considerably lower thanwhat is typically seen with traditional WSD cor-pora, where random baselines achieve accuracies of30 to 60%, and MFS baselines 65 to 80% (Palmeret al, 2001; Snyder and Palmer, 2004; Navigli etal., 2007).
Our baselines?
low figures are the re-sult of them having to consider senses from everypossible lemmatization and part of speech of thetarget, and underscore the difficulty of our task.The simplest knowledge-based algorithm wetested, simplified Lesk, was over twice as accu-rate as the random baseline in terms of precision(19.74%), but predictably had very low cover-age (35.52%), leading in turn to very low recall(7.01%).
Manual examination of the unassignedinstances confirmed that failure was usually dueto the lack of any lexical overlap whatsoever be-tween the context and definitions.
The use of atie-breaking strategy would not help much here,though some way of bridging the lexical gap would.This is, in fact, the strategy employed by the ex-tended and lexically expanded variants of simpli-fied Lesk, and we observed that both were success-ful to some degree.
Simplified lexically expandedLesk almost completely closed the lexical gap, withnearly complete coverage (98.69%), though thiscame at the expense of a large drop in precision (to13.43%).
Given the near-total coverage, use of a tie-breaking strategy here would have no appreciableeffect on the accuracy.Simplified extended Lesk, on the other hand,saw significant increases in coverage, precision,and recall (to 42.45%, 19.96%, and 8.47%, respec-tively).
Its recall is statistically indistinguishable7from the random baseline, though spot-checks ofits unassigned instances show that the problem isvery frequently not the lexical gap but rather mul-tiple senses tied for the greatest overlap with thecontext.
We therefore tested our two pun-specificbackoff strategies to break this system?s ties.
Us-ing the ?POS?
strategy increased coverage by 41%,relatively speaking, and gave us our highest ob-served precision of 21.21%.
Our ?cluster?
strategyeffected a relative increase in coverage of over 60%,and gave us the best recall (14.10%).
This strategyalso had the best tradeoff between precision andrecall, with an F1of 16.77%.Significance testing shows the recall scores forSLEL, SEL+POS, and SEL+cluster to be signifi-cantly better than the random baseline, and statisti-cally indistinguishable from that of MFS.
This isexcellent news, especially in light of the fact thatsupervised approaches (even baselines like MFS)usually outperform their knowledge-based counter-parts.
Though the three knowledge-based systemsare not statistically distinguishable from each otherin terms of recall, they do show a statistically sig-nificant improvement over SL and SEL, and thetwo implementing pun-specific tie-breaking strate-gies were markedly more accurate than SLEL forthose targets where they attempted an assignment.These two systems would therefore be preferablefor applications where precision is more importantthan recall.We also examined the results of our gener-ally best-performing system, SEL+cluster, to seewhether there was any relationship with the targets?part of speech.
We filtered the results according towhether both gold-standard meanings of the puncontain senses for nouns only, verbs only, adjec-7All significance statements in this section are based onMcNemar?s test at a confidence level of 5%.726POS C P R Rrandnoun 66.60 20.89 13.91 10.44verb 65.61 14.54 9.54 5.12adj.
68.87 39.73 27.36 16.84adv.
100.00 75.00 75.00 46.67pure 66.77 21.44 14.31 9.56mult.
72.58 18.43 13.38 12.18Table 2: Coverage, precision, and recall forSEL+cluster, and random baseline recall, accord-ing to part of speech.tives only, or adverbs only; these amounted to 539,346, 106, and 8 instances, respectively.
These re-sults are shown in Table 2.
Also shown there is arow which aggregates the 999 targets with ?pure?POS, and another for the remaining 608 instances(?mult.?
), where one or both of the two mean-ings contain senses for multiple parts of speech,or where the two meanings have different parts ofspeech.
The last column of each row shows therecall of the random baseline for comparison.Accuracy was lowest on the verbs, which had thehighest candidate polysemy (21.6) and are knownto be particularly difficult to disambiguate even intraditional WSD.
Still, as with all the other sin-gle parts of speech, performance of SEL+clusterexceeded the random baseline.
While recall waslower on targets with mixed POS than those withpure POS, coverage was significantly higher.
Nor-mally such a disparity could be attributed to a dif-ference in polysemy: Lesk-like systems are morelikely to attempt a sense assignment for highly pol-ysemous targets, since there is a greater likelihoodof one of the candidate definitions matching thecontext, though the probability of the assignmentbeing correct is reduced.
In this case, however,the multi-POS targets actually had lower averagepolysemy than the single-POS ones (13.2 vs. 15.8).7 ConclusionIn this paper we have introduced the novel task ofpun disambiguation and have proposed and evalu-ated several computational approaches for it.
Themajor contributions of this work are as follows:First, we have produced a new data set consistingof manually sense-annotated homographic puns.The data set is large enough, and the manual an-notations reliable enough, for a principled eval-uation of automatic pun disambiguation systems.Second, we have shown how evaluation metrics,baselines, and disambiguation algorithms from tra-ditional WSD can be adapted to the task of pundisambiguation, and we have tested these adapta-tions in a controlled experiment.
The results showpun disambiguation to be a particularly challeng-ing task for NLP, with baseline results far belowwhat is commonly seen in traditional WSD.
Weshowed that knowledge-based disambiguation al-gorithms na?
?vely adapted from traditional WSDperform poorly, but that extending them with strate-gies that rely on pun-specific features brings aboutdramatic improvements in accuracy: their recall be-comes comparable to that of a supervised baseline,and their precision greatly exceeds it.There are a number of avenues we intend to ex-plore in future work.
First, we would like to tryadapting and evaluating some additional WSD al-gorithms for use with puns.
Though our data set isprobably too small to use with machine learning?based approaches, we are particularly interestedin testing knowledge-based disambiguators whichrely on measures of graph connectivity rather thangloss overlaps.
Second, we would like to investi-gate alternative tie-breaking strategies, such as thedomain similarity measures used by Mihalcea etal.
(2010).
Finally, whereas in this paper we havetreated only the task of sense disambiguation forthe case where a word is known a priori to be apun, we are interested in exploring the requisiteproblem of pun detection, where the object is todetermine whether or not a given context containsa pun, and more precisely whether any given wordin a context is a pun.AcknowledgmentsThe work described in this paper is supportedby the Volkswagen Foundation as part of theLichtenberg Professorship Program under grantNo.
I/82806.
The authors thank John Black,Matthew Collins, Don Hauptman, ChristianF.
Hempelmann, Stan Kegel, Andrew Lamont,Beatrice Santorini, Mladen Turkovi?c, and AndreasZimpfer for helping us build our data set.ReferencesSalvatore Attardo.
1994.
Linguistic Theories of Humor.Mouton de Gruyter.Pierpaolo Basile, Annalina Caputo, and Giovanni Se-meraro.
2014.
An enhanced Lesk word sense disam-727biguation algorithm through a distributional seman-tic model.
In Proceedings of the 25th InternationalConference on Computational Linguistics (COLING2014), pages 1591?1600.Tristan A. Bekinschtein, Matthew H. Davis, Jennifer M.Rodd, and Adrian M. Owen.
2011.
Why clownstaste funny: The relationship between humor andsemantic ambiguity.
The Journal of Neuroscience,31(26):9665?9671, June.Nancy D. Bell, Scott Crossley, and Christian F. Hempel-mann.
2011.
Wordplay in church marquees.
Humor:International Journal of Humor Research, 24(2):187?202, April.Chiara Bucaria.
2004.
Lexical and syntactic ambigu-ity as a source of humor: The case of newspaperheadlines.
Humor: International Journal of HumorResearch, 17(3):279?309.Paul Buitelaar.
2000.
Reducing lexical semantic com-plexity with systematic polysemous classes and un-derspecification.
In Proceedings of the 2000 NAACL-ANLP Workshop on Syntactic and Semantic Com-plexity in Natural Language Processing Systems, vol-ume 1, pages 14?19.Jonathan D. Culler, editor.
1988.
On Puns: The Foun-dation of Letters.
Basil Blackwell, Oxford.Dirk Delabastita, editor.
1997.
Traductio: Essays onPunning and Translation.
St. Jerome, Manchester.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,MA.William Gale, Kenneth Ward Church, and DavidYarowsky.
1992.
Estimating upper and lower boundson the performance of word-sense disambiguationprograms.
In Proceedings of the 30th Annual Meet-ing of the Association of Computational Linguistics(ACL 1992), pages 249?256.Christian F. Hempelmann.
2003.
Paronomasic Puns:Target Recoverability Towards Automatic Generation.Ph.D.
thesis, Purdue University.Bryan Anthony Hong and Ethel Ong.
2009.
Automati-cally extracting word relationships as templates forpun generation.
In Proceedings of the 1st Workshopon Computational Approaches to Linguistic Creativ-ity (CALC 2009), pages 24?31, June.David Jurgens and Ioannis Klapaftis.
2013.
SemEval-2013 Task 13: Word sense induction for graded andnon-graded senses.
In Proceedings of the 7th Interna-tional Workshop on Semantic Evaluation (SemEval2013), pages 290?299, June.David Jurgens.
2014.
An analysis of ambiguity inword sense annotations.
In Proceedings of the 9th In-ternational Conference on Language Resources andEvaluation (LREC 2014), pages 3006?3012, May.Nora Kaplan and Teresa Lucas.
2001.
Comprensi?ondel humorismo en ingl?es: Estudio de las estrategiasde inferencia utilizadas por estudiantes avanzados deingl?es como lengua extranjera en la interpretaci?onde los retru?ecanos en historietas c?omicas en lenguainglesa.
Anales de la Universidad Metropolitana,1(2):245?258.Stefan Daniel Keller.
2009.
The Development of Shake-speare?s Rhetoric: A Study of Nine Plays, volume136 of Swiss Studies in English.
Narr, T?ubingen.Adam Kilgarriff and Joseph Rosenzweig.
2000.
Frame-work and results for English SENSEVAL.
Computersand the Humanities, 34:15?48.Klaus Krippendorff.
1980.
Content Analysis: An In-troduction to its Methodology.
Sage, Beverly Hills,CA.Luuk Lagerwerf.
2002.
Deliberate ambiguity in slo-gans: Recognition and appreciation.
Document De-sign, 3(3):245?260.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: How to tell apine cone from a ice cream cone.
In Proceedings ofthe 5th Annual International Conference of SystemsDocumentation (SIGDOC 1986), pages 24?26.Greg Lessard, Michael Levison, and Chris Venour.2002.
Cleverness versus funniness.
In Proceedingsof the 20th Twente Workshop on Language Technol-ogy, pages 137?145.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th An-nual Meeting of the Association for ComputationalLinguistics (ACL 1998) and the 17th InternationalConference on Computational Linguistics (COLING1998), volume 2, pages 768?774.Teresa Lucas.
2004.
Deciphering the Meaning of Punsin Learning English as a Second Language: A Studyof Triadic Interaction.
Ph.D. thesis, Florida StateUniversity.Peter J. Ludlow.
1996.
Semantic Ambiguity and Under-specification (review).
Computational Linguistics,3(23):476?482.Michael Matuschek, Tristan Miller, and Iryna Gurevych.2014.
A language-independent sense clustering ap-proach for enhanced WSD.
In Proceedings of the12th Konferenz zur Verarbeitung nat?urlicher Sprache(KONVENS 2014), pages 11?21, October.Christian M. Meyer, Margot Mieskes, Christian Stab,and Iryna Gurevych.
2014.
DKPro Agreement: Anopen-source Java library for measuring inter-rateragreement.
In Proceedings of the 25th InternationalConference on Computational Linguistics (SystemDemonstrations) (COLING 2014), pages 105?109,August.728Rada Mihalcea and Carlo Strapparava.
2005.
Mak-ing computers laugh: Investigations in automatic hu-mor recognition.
In Proceedings of the 11th HumanLanguage Technology Conference and the 10th Con-ference on Empirical Methods in Natural LanguageProcessing (HLT-EMNLP 2005), pages 531?538, Oc-tober.Rada Mihalcea and Carlo Strapparava.
2006.
Learn-ing to laugh (automatically): Computational modelsfor humor recognition.
Computational Intelligence,22(2):126?142.Rada Mihalcea, Carlo Strapparava, and Stephen Pul-man.
2010.
Computational models for incongruitydetection in humour.
In Proceedings of the 11thInternational Conference on Computational Linguis-tics and Intelligent Text Processing (CICLing 2010),volume 6008 of Lecture Notes in Computer Science,pages 364?374.
Springer, March.Tristan Miller and Mladen Turkovi?c.
2015.
Towardsthe automatic detection and identification of Englishpuns.
European Journal of Humour Research.
Toappear.Tristan Miller, Chris Biemann, Torsten Zesch, and IrynaGurevych.
2012.
Using distributional similarity forlexical expansion in knowledge-based word sensedisambiguation.
In Proceedings of the 24th Inter-national Conference on Computational Linguistics(COLING 2012), pages 1781?1796, December.Tristan Miller, Nicolai Erbs, Hans-Peter Zorn, TorstenZesch, and Iryna Gurevych.
2013.
DKPro WSD: Ageneralized UIMA-based framework for word sensedisambiguation.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (System Demonstrations) (ACL 2013), pages37?42, August.Michel Monnot.
1982.
Puns in advertising: Ambiguityas verbal aggression.
Maledicta, 6:7?20.Roberto Navigli, Kenneth C. Litkowski, and Orin Har-graves.
2007.
SemEval-2007 Task 07: Coarse-grained English All-words Task.
In Proceedingsof the 4th International Workshop on Semantic Eval-uations (SemEval-2007), pages 30?35, June.Martha Palmer, Christiane Fellbaum, Scott Cotton, Lau-ren Delfs, and Hoa Trang Dang.
2001.
English tasks:All-words and verb lexical sample.
In Proceedingsof Senseval-2: 2nd International Workshop on Eval-uating Word Sense Disambiguation Systems, pages21?24, July.Martha Palmer, Hwee Tou Ng, and Hoa Trang Dang.2006.
Evaluation of WSD systems.
In Eneko Agirreand Philip Edmonds, editors, Word Sense Disam-biguation: Algorithms and Applications, volume 33of Text, Speech, and Language Technology.
Springer.Rebecca J. Passonneau, Nizar Habash, and Owen Ram-bow.
2006.
Inter-annotator agreement on a multilin-gual semantic annotation task.
In Proceedings of the5th International Conference on Language Resourcesand Evaluations (LREC 2006), pages 1951?1956.Rebecca J. Passonneau.
2006.
Measuring agreement onset-valued items (MASI) for semantic and pragmaticannotation.
In Proceedings of the 5th InternationalConference on Language Resources and Evaluations(LREC 2006), pages 831?836.Simone Paolo Ponzetto and Roberto Navigli.
2010.Knowledge-rich word sense disambiguation rivalingsupervised systems.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics (ACL 2010), pages 1522?1531.Vitor Raskin.
1985.
Semantic Mechanisms of Humor.D.
Reidel, Dordrecht, the Netherlands.Walter Redfern.
1984.
Puns.
Basil Blackwell, Oxford.Thorsten Schr?oter.
2005.
Shun the Pun, Rescue theRhyme?
The Dubbing and Subtitling of Language-Play in Film.
Ph.D. thesis, Karlstad University.Benjamin Snyder and Martha Palmer.
2004.
The En-glish all-words task.
In Proceedings of the 3rd In-ternational Workshop on the Evaluation of Systemsfor the Semantic Analysis of Text (Senseval-3), pages41?43, July.Julia M. Taylor and Lawrence J. Mazlack.
2004.
Com-putationally recognizing wordplay in jokes.
In Pro-ceedings of the 26th Annual Conference of the Cog-nitive Science Society (CogSci 2004), pages 1315?1320, August.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the 3rd Conference of the North AmericanChapter of the Association for Computational Lin-guistics and the 9th Human Language TechnologiesConference (HLT-NAACL 2003), pages 252?259.Alessandro Valitutti, Carlo Strapparava, and OlivieroStock.
2008.
Textual affect sensing for computa-tional advertising.
In Proceedings of the AAAI SpringSymposium on Creative Intelligent Systems, pages117?122, March.Chris Venour.
1999.
The computational generation ofa class of puns.
Master?s thesis, Queen?s University,Kingston, ON.Toshihiko Yokogawa.
2002.
Japanese pun analyzerusing articulation similarities.
In Proceedings ofthe 11th IEEE International Conference on FuzzySystems (FUZZ 2002), volume 2, pages 1114?1119,May.Arnold M. Zwicky and Elizabeth D. Zwicky.
1986.Imperfect puns, markedness, and phonological simi-larity: With fronds like these, who needs anemones?Folia Linguistica, 20(3?4):493?503.729
