Proceedings of NAACL HLT 2007, pages 284?291,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsAn Exploration of Eye Gaze in Spoken Language Processing for MultimodalConversational InterfacesShaolin Qu Joyce Y. ChaiDepartment of Computer Science and EngineeringMichigan State UniversityEast Lansing, MI 48824{qushaoli,jchai}@cse.msu.eduAbstractMotivated by psycholinguistic findings,we are currently investigating the role ofeye gaze in spoken language understand-ing for multimodal conversational sys-tems.
Our assumption is that, during hu-man machine conversation, a user?s eyegaze on the graphical display indicatessalient entities on which the user?s atten-tion is focused.
The specific domain infor-mation about the salient entities is likelyto be the content of communication andtherefore can be used to constrain speechhypotheses and help language understand-ing.
Based on this assumption, this paperdescribes an exploratory study that incor-porates eye gaze in salience modeling forspoken language processing.
Our empiri-cal results show that eye gaze has a poten-tial in improving automated language pro-cessing.
Eye gaze is subconscious and in-voluntary during human machine conver-sation.
Our work motivates more in-depthinvestigation on eye gaze in attention pre-diction and its implication in automatedlanguage processing.1 IntroductionPsycholinguistic experiments have shown that eyegaze is tightly linked to human language process-ing.
Eye gaze is one of the reliable indicators ofwhat a person is ?thinking about?
(Henderson andFerreira, 2004).
The direction of gaze carries infor-mation about the focus of the users attention (Justand Carpenter, 1976).
The perceived visual contextinfluences spoken word recognition and mediatessyntactic processing (Tanenhaus et al, 1995; Royand Mukherjee, 2005).
In addition, directly beforespeaking a word, the eyes move to the mentionedobject (Griffin and Bock, 2000).Motivated by these psycholinguistic findings, weare currently investigating the role of eye gaze inspoken language understanding during human ma-chine conversation.
Through multimodal interfaces,a user can look at a graphic display and conversewith the system at the same time.
Our assumptionis that, during human machine conversation, a user?seye gaze on the graphical display can indicate saliententities on which the user?s attention is focused.
Thespecific domain information about the salient enti-ties is likely linked to the content of communicationand therefore can be used to constrain speech hy-potheses and influence language understanding.Based on this assumption, we carried out an ex-ploration study where eye gaze information is in-corporated in a salience model to tailor a languagemodel for spoken language processing.
Our prelim-inary results show that eye gaze can be useful in im-proving spoken language processing and the effectof eye gaze varies among different users.
Becauseeye gaze is subconscious and involuntary in humanmachine conversation, our work also motivates sys-tematic investigations on how eye gaze contributesto attention prediction and its implications in auto-mated language processing.2 Related WorkEye gaze has been mainly used in human machineinteraction as a pointing mechanism in direct manip-ulation interfaces (Jacob, 1990; Jacob, 1995; Zhaiet al, 1999), as a facilitator in computer supportedhuman human communication (Velichkovsky, 1995;Vertegaal, 1999); or as an additional modality dur-ing speech or multimodal communication (Starkerand Bolt, 1990; Campana et al, 2001; Kaur et al,2842003; Qvarfordt and Zhai, 2005).
This last area ofinvestigation is more related to our work.In the context of speech and multimodal commu-nication, studies have shown that speech and eyegaze integration patterns can be modeled reliably forusers.
For example, by studying patterns of eye gazeand speech in the phrase ?move it there?, researchersfound that the gaze fixation closest to the intendedobject begins, with high probability, before the be-ginning of the word ?move?
(Kaur et al, 2003).
Re-cent work has also shown that eye gaze has a poten-tial to improve reference resolution in a spoken dia-log system (Campana et al, 2001).
Furthermore, eyegaze also plays an important role in managing dia-log in conversational systems (Qvarfordt and Zhai,2005).Salience modeling has been used in both naturallanguage and multimodal language processing.
Lin-guistic salience describes entities with their accessi-bility in a hearer?s memory and their implications inlanguage production and interpretation.
Linguisticsalience modeling has been used for language in-terpretations such as reference resolution (Huls etal., 1995; Eisenstein and Christoudias, 2004).
Vi-sual salience measures how much attention an en-tity attracts from a user based on its visual proper-ties.
Visual salience can tailor users?
referring ex-pressions and thus can be used for multimodal refer-ence resolution (Kehler, 2000).
Our recent work hasalso investigated salience modeling based on deic-tic gestures to improve spoken language understand-ing (Chai and Qu, 2005; Qu and Chai, 2006).3 Data CollectionWe conducted user studies to collect speech and eyegaze data.
In the experiments, a static 3D bedroomscene was shown to the user.
The system verballyasked a user a list of questions one at a time aboutthe bedroom and the user answered the questions byspeaking to the system.
Fig.1 shows the 14 questionsin the experiments.
The user?s speech was recordedthrough an open microphone and the user?s eye gazewas captured by an Eye Link II eye tracker.
From 7users?
experiments, we collected 554 utterances witha vocabulary of 489 words.
Each utterance was tran-scribed and annotated with entities that were beingtalked about in the utterance.1 Describe this room.2 What do you like/dislike about the arrangement?3 Describe anything in the room that seems strange toyou.4 Is there a bed in this room?5 How big is the bed?6 Describe the area around the bed.7 Would you make any changes to the area around thebed?8 Describe the left wall.9 How many paintings are there in this room?10 Which is your favorite painting?11 Which is your least favorite painting?12 What is your favorite piece of furniture in the room?13 What is your least favorite piece of furniture in theroom?14 How would you change this piece of furniture to makeit better?Figure 1: Questions for users in experimentsThe collected raw gaze data consists of the screencoordinates of each gaze point sampled at 4 ms.As shown in Fig.2a, this raw data is not very use-ful for identifying fixated entities.
The raw gazedata are processed to eliminate invalid and saccadicgaze points, leaving only pertinent eye fixations.Invalid gaze points occur when users look off thescreen.
Saccadic gaze points occur during ballis-tic eye movements between fixations.
Vision stud-ies have shown that no visual processing occurs dur-ing saccades (i.e., saccadic suppression).
It is wellknown that eyes do not stay still, but rather makesmall, frequent jerky movements.
In order to bestdetermine fixation locations, nearby gaze points areaveraged together to identify fixations.
The pro-cessed eye gaze fixations can be seen in Fig.2b.Fig.3 shows an excerpt of the collected speechand gaze fixation with fixated entities.
In the speechstream, each word starts at a particular timestamp.
Inthe gaze stream, each gaze fixation f has a startingtimestamp tf and a duration Tf .
Gaze fixations canhave different durations.
An entity e on the graphi-cal display is fixated by gaze fixation f if the area ofe contains the fixation point of f .
One gaze fixationcan fall on multiple entities or no entity.4 Salience Driven Language ModelingOur goal is to use the domain specific informationabout the salient entities on a graphical display, asindicated by the user?s eye gaze, to help recognitionof the user?s utterances.
In particular, we incorporatethis salient domain information in speech recogni-tion via salience driven language modeling.285(a) Raw gaze points (b) Processed gaze fixationsFigure 2: Gaze fixations on a scene8 596 968 1668 2096 32522692tf Tf[19] [ ] [17] [19] [22] [ ] [10][11][10][11][10][11]This room has a chandelier2572 2872 3170 3528 3736( [19] ?
bed_8; [17] ?
lamp_2; [22] ?
door_1; [10] ?
bedroom; [11] ?
chandelier_1 )speech streamgaze stream(ms)(ms)[fixated entity]f: gaze fixationFigure 3: An excerpt of speech and gaze stream dataWe first briefly introduce speech recognition.
Thetask of speech recognition is to, given an observedspoken utterance O, find the word sequence W ?such that W ?
= argmaxWp(O|W )p(W ), wherep(O|W ) is the acoustic model and p(W ) is thelanguage model.
The acoustic model provides theprobability of observing the acoustic features givenhypothesized word sequences while the languagemodel provides the probability of a word sequence.The language model is represented as:p(W ) = p(wn1 ) =n?k=1p(wk|wk?11 ) (1)Using first-order Markov assumption, the above lan-guage model can be approximated by a bigrammodel:p(wn1 ) =n?k=1p(wk|wk?1) (2)In the following sections, we first introduce thesalience modeling based on eye gaze, then presenthow the gaze-based salience models can be used totailor language models.4.1 Gaze-based Salience ModelingWe first define a gaze fixation set F t0+Tt0 (e), whichcontains all gaze fixations that fall on entity e withina time window t0 ?
(t0 + T ):F t0+Tt0 (e) = {f |f falls on e within t0 ?
(t0 + T )}We model gaze-based salience in two ways.4.1.1 Gaze Salience Model 1Salience model 1 is based on the assumption thatwhen an entity has more gaze fixations on it thanother entities, this entity is more likely attended bythe user and thus has higher salience:pt0,T (e) =#elements in F t0+Tt0 (e)?e(#elements in Ft0+Tt0 (e))(3)Here, pt0,T (e) tells how likely it is that the user isfocusing on entity ewithin time period t0 ?
(t0+T )based on how many gaze fixations are on e amongall gaze fixations that fall on entities within t0 ?
(t0 + T ).4.1.2 Gaze Salience Model 2Salience model 2 is based on the assumption thatwhen an entity has longer gaze fixations on it thanother entities, this entity is more likely attended bythe user and thus has higher salience:pt0,T (e) =Dt0+Tt0 (e)?e Dt0+Tt0 (e)(4)whereDt0+Tt0 (e) =?f?Ft0+Tt0(e)Tf (5)Here, pt0,T (e) tells how likely it is that the user isfocusing on entity e within time period t0 ?
(t0+ t)286based on how long e has been fixated by gaze fixa-tions among the overall time length of all gaze fixa-tions that fall on entities within t0 ?
(t0 + T ).4.2 Salience Driven N-gram ModelSalience models can be incorporated in different lan-guage models, such as bigram models, class-basedbigram models, and probabilistic context free gram-mar.
Among these language models, the saliencedriven bigram model based on deictic gesture hasbeen shown to achieve best performance on speechrecognition (Qu and Chai, 2006).
In our initial in-vestigation of gaze-based salience, we incorporatethe gaze-based salience in a bigram model.The salience driven bigram probability is givenby:ps(wi|wi?1) = (1 ?
?
)p(wi|wi?1) +?
?e p(wi|wi?1, e)pt0,T (e) (6)where pt0,T (e) is the salience distribution as mod-eled in equations (3) and (4).
In applying thesalience driven bigram model for speech recogni-tion, we set t0 as the starting timestamp of the ut-terance and T as the duration of the utterance.
Thepriming weight ?
decides how much the originalbigram probability will be tailored by the saliententities indicated by eye gaze.
Currently, we set?
= 0.67 empirically.
We also tried learning thepriming weight with an EM algorithm.
However,we found out that the learned priming weight per-formed worse than the empirical one in our exper-iments.
This is probably due to insufficient devel-opment data.
Bigram probabilities p(wi|wi?1) wereestimated by the maximum likelihood estimation us-ing Katz?s backoff method (Katz, 1987) with a fre-quency cutoff of 1.
The samemethod was used to es-timate p(wi|wi?1, e) from the users?
utterance tran-scripts with entity annotation of e.5 Application of Salience Driven LMsThe salience driven language models can be inte-grated into speech processing in two stages: an earlystage before a word lattice (n-best list) is generated(Fig.4a), or in a late stage where the word lattice(n-best list) is post-processed (Fig.4b).For the early stage integration, the gaze-basedsalience driven language model is used together withword lattice(n-best list)speecheye gazeSpeech DecoderLanguageModelAcousticModel(a) Early stage integrationword lattice(n-best list) n-best listeye gazeRescorerspeechSpeech DecoderLanguageModelAcousticModelLanguageModel(b) Late stage integrationFigure 4: Integration of gaze-based salience drivenlanguage model in speech processingthe acoustic model to generate the word lattice, typ-ically by Viterbi search.For the late stage integration, the gaze-basedsalience driven language model is used to rescore theword lattice generated by a speech recognizer witha basic language model not involving salience mod-eling.
A* search can be applied to find the n-bestpaths in the word lattice.6 EvaluationThe evaluations were conducted on data collectedfrom user studies (Sec.
3).
We evaluated the gaze-based salience driven bigram models when appliedfor speech recognition at early and late stages.6.1 Evaluation ResultsUsers?
speech was first segmented, then recognizedby the CMU Sphinx-4 speech recognizer using dif-ferent language models.
Evaluation was done bya 14-fold cross validation.
We compare the per-formances of the early and late applications of twogaze-based salience driven language models:?
S-Bigram1 ?
salience driven language modelbased on salience modeling 1 (Sec.
4.1.1)?
S-Bigram2 ?
salience driven language modelbased on salience modeling 2 (Sec.
4.1.2)Table 1 and Table 2 show the results of early andlate application of the salience driven language mod-els based on eye gaze.
We can see that all word errorrates (WERs) are high.
In the experiments, userswere instructed to only answer systems questionsone by one.
There was no flow of a real conversa-tion.
In this setting, users were more free to express287themselves than in the situation where users believedthey were conversing with a machine.
Thus, we ob-serve much longer sentences that often contain dis-fluencies.
Here is one example:System: ?How big is the bed?
?User: ?I would to have to offer a guess that the bed,if I look the chair that?s beside it [pause] in a rel-ative angle to the bed, it?s probably six feet long,possibly, or shorter, slightly shorter.
?The high WER was mainly caused by the com-plexity and disfluencies of users?
speech.
Poorspeech recording quality is another reason for thebad recognition performance.
It was found thatthe trigram model performed worse than the bigrammodel in the experiment.
This is probably due to thesparseness of trigrams in the corpus.
The amount ofdata available is too small considering the vocabu-lary size.Language Model Lattice-WER WERBigram 0.613 0.707Trigram 0.643 0.719S-Bigram 1 0.605 0.690S-Bigram 2 0.604 0.689Table 1: WER of early application of LMsLanguage Model Lattice-WER WERS-Bigram 1 0.643 0.709S-Bigram 2 0.643 0.710Table 2: WER of late application of LMsThe S-Bigram1 and S-Bigram2 achieved similarresults in both early application (Table 1) and lateapplication (Table 2).
In early application, the S-Bigram1 model performed better than the trigrammodel (t = 5.24, p < 0.001, one-tailed) and thebigram model (t = 3.31, p < 0.001, one-tailed).The S-Bigram2model also performed better than thetrigram model (t = 5.15, p < 0.001, one-tailed)and the bigram model (t = 3.33, p < 0.001, one-tailed) in early application.
In late application, theS-Bigram1 model performed better than the trigrammodel (t = 2.11, p < 0.02, one-tailed), so didthe S-Bigram2 model (t = 1.99, p < 0.025, one-tailed).
However, compared to the bigram model,the S-Bigram1 model did not change the recogni-tion performance significantly (t = 0.38, N.S., two-tailed) in late application, neither did the S-Bigram2model (t = 0.50, N.S., two-tailed).We also compare performances of the saliencedriven language models for individual users.
In earlyapplication (Fig.5a), both the S-Bigram1 and the S-Bigram2 model performed better than the baselinesof the bigram and trigrammodels for all users exceptuser 2 and user 7.
T-tests have shown that these aresignificant improvements.
For user 2, the S-Bigram1model achieved the sameWER as the bigrammodel.For user 7, neither of the salience driven languagemodels improved recognition compared to the bi-gram model.
In late application (Fig.5b), only foruser 3 and user 4, both salience driven languagemodels performed better than the baselines of the bi-gram and trigrammodels.
These improvements havealso been confirmed by t-tests as significant.1 2 3 4 5 6 70.40.50.60.70.80.91User IDWERbigram trigram s?bigram1 s?bigram2(a) WER of early application1 2 3 4 5 6 70.40.50.60.70.80.91User IDWERbigram trigram s?bigram1 s?bigram2(b) WER of Late applicationFigure 5: WERs of LMs for individual usersComparing early and late application of thesalience driven language models, it is observed thatearly application performed better than late applica-tion for all users except user 3 and user 4.
T-testshave confirmed that these differences are significant.288It is interesting to see that the effect of gaze-basedsalience modeling is different among users.
Fortwo users (i.e., user 3 and user 4), the gaze-basedsalience driven language models consistently out-performed the bigram and trigram models in bothearly application and late application.
However, forsome other users (e.g., user 7), this is not the case.
Infact, the gaze-based salience driven language mod-els performed worse than the bigram model.
Thisobservation indicates that during language produc-tion, a user?s eye gaze is voluntary and unconscious.This is different from deictic gesture, which is moreintentionally delivered by a user.
Therefore, incor-porating this ?unconscious?
mode of modality insalience modeling requires more in-depth researchon the role of eye gaze in attention prediction duringmultimodal human computer interaction.6.2 DiscussionGaze-based salience driven language models arebuilt on the assumption that when a user is fixat-ing on an entity, the user is saying something re-lated to the entity.
With this assumption, gaze-basedsalience driven language models have the potentialto improve speech recognition by biasing the speechdecoder to favor the words that are consistent withthe entity indicated by the user?s eye gaze, especiallywhen the user?s utterance contains words describingunique characteristics of the entity.
These particularcharacteristics could be the entity?s name or physicalproperties (e.g., color, material, size).Utterance: ?a tree growing from the floor?Gaze salience:p(bedroom) = 0.2414 p(plant willow) = 0.2414p(chair soft) = 0.2414 p(door 1) = 0.1378p(bed 8) = 0.1378Bigram n-best list:sheet growing from a foursheet growing from a forsheet growing from a floor.
.
.S-Bigram2 n-best list:a tree growing from the floora tree growing from the fora tree growing from the floor a. .
.Figure 6: N-best lists of utterance ?a tree growingfrom the floor?Fig.6 shows an example where the S-Bigram2model in early application improved recognition ofthe utterance ?a tree growing from the floor?.
Inthis example, the user?s gaze fixations accompany-ing the utterance resulted in a list of candidate enti-ties with fixating probabilities (cf.
Eqn.
(4)), amongwhich entities bedroom and plant willow were as-signed higher probabilities.
Two n-best lists, the Bi-gram n-best list and the S-Bigram2 n-best list, weregenerated by the speech recognizer when the bigrammodel and the S-Bigram2 model were applied sep-arately.
The speech recognizer did not get the cor-rect recognition when the bigram model was used,but got the correct result when the S-Bigram2 modelwas used.Fig.7a and 7b show the word lattices of the ut-terance generated by the speech recognizer usingthe bigram model and the S-Bigram2 model respec-tively.
The n-best lists in Fig.6 were generated fromthose word lattices.
In the word lattices, each pathgoing from the start node<s> to the end node</s>forms a recognition hypothesis.
The bigram proba-bilities along the edges are in the logarithm of base10.
In the bigram case, the path ?<s> a tree?
has ahigher language score (summation of bigram prob-abilities along the path) than ?<s> sheet?, and ?afloor?
has a higher language score than ?a full?.However, these correct paths ?<s> a tree?
and ?afloor?
(not exactly correct, but better than ?a full?
)do not appear in the best hypothesis in the result-ing n-best list.
This is because the system tries tofind an overall best hypothesis by considering bothlanguage and acoustic score.
Because of the noisyspeech, the incorrect hypotheses may happen to havehigher acoustic confidence than the correct ones.
Af-ter tailoring the bigram model with gaze salience,the salient entity plant willow significantly increasesthe probability of ?a tree?
(from -1.3594 to -0.9913)and ?tree growing?
(from -3.1009 to -1.1887), whileit decreases the probability of ?sheet growing?
(from-3.0962 to -3.4534).
This probability change is madeby the entity conditional probability p(wi|wi?1, e)in tailoring of bigram by salience (cf.
Eqn.
(6)).Probability p(wi|wi?1, e), trained from the anno-tated utterances, reflects what words are more likelyto be spoken by a user while talking about an entitye.
The increased probabilities of ?a tree?
and ?treegrowing?
show that word ?tree?
appears more likelythan ?sheet?
when the user is talking about entity289</s>i-1.5043forest-0.8615floor -0.3552four-0.5322for-0.9768full-1.9490a-3.1284-2.8274-3.0035-2.9066-3.0035of-3.3940-3.2691-1.0280from-3.6386-3.3376-3.5137-3.4168-3.5137-1.9339kind-0.2312growing-2.2662-3.2942going-1.5911sheet-2.4272-3.0962tree -3.1009-3.5780a-1.3594<s>-3.9306-3.0275-1.5987(a) Word lattice with bigram model</s>a-1.2861floor-0.2570-1.9165forest-0.7782for-1.2683a-2.4966-2.9278-3.6009the-1.2151-3.2468-3.7353further-3.6961from-3.9011-4.2022-3.3477-1.9934-0.1622growing-3.4626-3.6233tree-1.1887sheet-3.4534a-0.9913<s>-2.3655-3.8964-1.5618(b) Word lattice with S-Bigram 2Figure 7: Word lattices of utterance ?a tree growing from the floor?
?plant willow.
This is in accordance with our com-mon sense.
Likewise, the salient entity bedroom, ofwhich floor is a component, makes the probability ofthe correct hypothesis ?the floor?
much higher thanother hypotheses (?the for?
and ?the forest?).
Theseenlarged language score differences make the cor-rect hypotheses ?a tree?
and ?the floor?
win out inthe searching procedure despite the noisy speech.Utterance: ?I like the picture with like a forest in it?Gaze salience:p(bedroom) = 0.5960 p(chandelier 1) = 0.4040Bigram n-best list:and i eight that picture rid like got fiveand i eight that picture rid identifiableand i eight that picture rid like got forest.
.
.S-Bigram2 n-best list:and i that bedroom it like upsideand i that bedroom it like a fiveand i that bedroom it like a forest.
.
.Figure 8: N-best lists of utterance ?I like the picturewith like a forest in it?Unlike the active input mode of deictic gesture,eye gaze is a passive input mode.
The salience in-formation indicated by eye gaze is not as reliableas the one indicated by deictic gesture.
When thesalient entities indicated by eye gaze are not thetrue entities the user is referring to, the saliencedriven language model can worsen speech recogni-tion.
Fig.8 shows an example where the S-Bigram2model in early application worsened the recogni-tion of a user?s utterance ?I like the picture with likea forest in it?
because of wrong salience informa-tion.
In this example, the user was talking about apicture entity picture bamboo.
However, this entitywas not salient, only entities bedroom and chande-lier 1 were salient.
As a result, the recognition withthe S-Bigram2 model becomes worse than the base-line.
The correct word ?picture?
is missing and thewrong word ?bedroom?
appears in the result.The failure to identify the actual referred entitypicture bamboo as salient in the above example canalso be caused by the visual properties of entities.Smaller entities on the screen are harder to be fix-290ated by eye gaze than larger entities.
To address thisissue, more reliable salience modeling that takes intoaccount the visual features is needed.7 ConclusionThis paper presents an empirical exploration of in-corporating eye gaze in spoken language processingvia salience driven language modeling.
Our prelim-inary results have shown the potential of eye gaze inimproving spoken language processing.
Neverthe-less, this exploratory study is only the first step inour investigation.
Many interesting research ques-tions remain.
During human machine conversation,how is eye gaze aligned with speech production?How reliable is eye gaze for attention prediction?Are there any other factors such as interface designand visual properties that will affect eye gaze behav-ior and therefore attention prediction?
The answersto these questions will affect how eye gaze should beappropriately modeled and used for language pro-cessing.Eye-tracking systems are no longer bulky, sta-tionary systems that prevent natural human ma-chine communication.
Recently developed dis-play mounted gaze-tracking systems (e.g., Tobii) arecompletely non-intrusive, can tolerate head motion,and provide high tracking quality.
These featureshave been demonstrated in several successful appli-cations (Duchowski, 2002).
Integrating eye trackingwith conversational interfaces is no longer beyondreach.
We believe it is time to conduct systematicinvestigations and fully explore the additional chan-nel provided by eye gaze in improving robustness ofhuman machine conversation.8 AcknowledgmentsThis work was supported by a Career Award IIS-0347548 and IIS-0535112 from the National Sci-ence Foundation.
The authors would like to thankZahar Prasov for his contribution on data collectionand thank anonymous reviewers for their valuablecomments and suggestions.ReferencesE.
Campana, J. Baldridge, J. Dowding, B. Hockey, R. Reming-ton, and L. Stone.
2001.
Using eye movements to determinereferents in a spoken dialogue system.
In Proceedings of theWorkshop on Perceptive User Interface.J.
Chai and S. Qu.
2005.
A salience driven approach to ro-bust input interpretation in multimodal conversational sys-tems.
In Proceedings of HLT/EMNLP?05.A.
T. Duchowski.
2002.
A breath-first survey of eye trackingapplications.
Behavior Research methods, Instruments, andComputers, 33(4).J.
Eisenstein and C. M. Christoudias.
2004.
A salience-basedapproach to gesture-speech alignment.
In Proceedings ofHLT/NAACL?04.Z.
M. Griffin and K. Bock.
2000.
What the eyes say aboutspeaking.
Psychological Science, 11:274?279.J.
M. Henderson and F. Ferreira.
2004.
The interface of lan-guage, vision, and action: Eye movements and the visualworld.
New York: Taylor & Francis.C.
Huls, E. Bos, and W. Classen.
1995.
Automatic referent res-olution of deictic and anaphoric expressions.
ComputationalLinguistics, 21(1):59?79.R.
J. K. Jacob.
1990.
What you look is what you get: Eyemovement-based interaction techniques.
In Proceedings ofCHI?90.R.
J. K. Jacob.
1995.
Eye tracking in advanced interface design.In W. Barfield and T. Furness, editors, Advanced InterfaceDesign and Virtual Environments, pages 258?288.
OxfordUniversity Press.M.
Just and P. Carpenter.
1976.
Eye fixations and cognitiveprocesses.
Cognitive Psychology, 8:441?480.S.
Katz.
1987.
Estimation of probabilities from sparse data forthe language model component of a speech recogniser.
IEEETrans.
Acous., Speech and Sig.
Processing, 35(3):400?401.M.
Kaur, M. Termaine, N. Huang, J. Wilder, Z. Gacovski,F.
Flippo, and C. S. Mantravadi.
2003.
Where is ?it??
eventsynchronization in gaze-speech input systems.
In Proceed-ings of ICMI?03.A.
Kehler.
2000.
Cognitive status and form of reference inmultimodal human-computer interaction.
In Proceedings ofAAAI?00.S.
Qu and J. Chai.
2006.
Salience modeling based on non-verbal modalities for spoken language understanding.
InProceedings of ICMI?06.P.
Qvarfordt and S. Zhai.
2005.
Conversing with the user basedon eye-gaze patterns.
In Proceedings of CHI?05.D.
Roy and N. Mukherjee.
2005.
Towards situated speechunderstanding: Visual context priming of language models.Computer Speech and Language, 19(2):227?248.I.
Starker and R. A. Bolt.
1990.
A gaze-responsive self-disclosing display.
In Proceedings of CHI?90.M.
K. Tanenhaus, M. J. Spivey-Knowlton, K. M. Eberhard,and J. E. Sedivy.
1995.
Integration of visual and linguis-tic information in spoken language comprehension.
Science,268:1632?1634.B.
M. Velichkovsky.
1995.
Communicating attention-gaze po-sition transfer in cooperative problem solving.
Pragmaticsand Cognition, 3:99?224.R.
Vertegaal.
1999.
The gaze groupware system: Mediatingjoint attention in multiparty communication and collabora-tion.
In Proceedings of CHI?99.S.
Zhai, C. Morimoto, and S. Ihde.
1999.
Manual and gazeinput cascaded (magic) pointing.
In Proceedings of CHI?99.291
