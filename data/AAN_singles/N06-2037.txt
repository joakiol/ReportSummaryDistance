Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 145?148,New York, June 2006. c?2006 Association for Computational LinguisticsSelecting relevant text subsets from web-data for building topic speciclanguage modelsAbhinav Sethy, Panayiotis G. Georgiou, Shrikanth NarayananSpeech Analysis and Interpretation LabIntegrated Media Systems CenterViterbi School of EngineeringDepartment of Electrical Engineering-SystemsUniversity of Southern CaliforniaAbstractIn this paper we present a scheme to se-lect relevant subsets of sentences from alarge generic corpus such as text acquiredfrom the web.
A relative entropy (R.E)based criterion is used to incrementally se-lect sentences whose distribution matchesthe domain of interest.
Experimental re-sults show that by using the proposed sub-set selection scheme we can get signif-icant performance improvement in bothWord Error Rate (WER) and Perplexity(PPL) over the models built from the en-tire web-corpus by using just 10% of thedata.
In addition incremental data selec-tion enables us to achieve significant re-duction in the vocabulary size as well asnumber of n-grams in the adapted lan-guage model.
To demonstrate the gainsfrom our method we provide a compar-ative analysis with a number of methodsproposed in recent language modeling lit-erature for cleaning up text.1 IntroductionOne of the main challenges in the rapid deploymentof NLP applications is the lack of in-domain datarequired for training statistical models.
Languagemodels, especially n-gram based, are key compo-nents of most NLP applications, such as speechrecognition and machine translation, where theyserve as priors in the decoding process.
To estimatea n-gram language model we require examples ofin-domain transcribed utterances, which in absenceof readily available relevant corpora have to be col-lected manually.
This poses severe constraints interms of both system turnaround time and cost.This led to a growing interest in using the WorldWide Web (WWW) as a corpus for NLP (Lapata,2005; Resnik and Smith, 2003).
The web can serveas a good resource for automatically gathering datafor building task-specific language models.
Web-pages of interest can be identified by generatingquery terms either manually or automatically froman initial set of in-domain sentences by measuressuch as TFIDF or Relative Entropy (R.E).
Thesewebpages can then be converted to a text corpus(which we will refer to as web-data) by appropri-ate preprocessing.
However text gathered from theweb will rarely fit the demands or the nature of thedomain of interest completely.
Even with the bestqueries and web crawling schemes, both the styleand content of the web-data will usually differ sig-nificantly from the specific needs.
For example, aspeech recognition system requires conversationalstyle text whereas most of the data on the web isliterary.The mismatch between in-domain data and web-data can be seen as a semi-supervised learning prob-lem.
We can model the web-data as a mix of sen-tences from two classes: in-domain (I) and noise(N) (or out-of-domain).
The labels I and N are la-tent and unknown for the sentences in web-data butwe usually have a small number of examples of in-domain examples I.
Selecting the right labels forthe unlabeled set is important for benefiting from it.145Recent research on semi-supervised learning showsthat in many cases (Nigam et al, 2000; Zhu, 2005)poor preprocessing of unlabeled data might actuallylower the performance of classifiers.
We found sim-ilar results in our language modeling experimentswhere the presence of a large set of noisy N ex-amples in training actually lowers the performanceslightly in both perplexity and WER terms.
Recentliterature on building language models from text ac-quired from the web addresses this issue partly byusing various rank-and-select schemes for identify-ing the set I (Ostendorf et al, 2005; Sethy, 2005;Sarikaya, 2005).
However we believe that simi-lar to the question of balance (Zhu, 2005) in semi-supervised learning for classification, we need to ad-dress the question of distributional similarity whileselecting the appropriate utterances for building alanguage model from noisy data.
The subset of sen-tences from web-data which are selected to build theadaptation language should have a distribution sim-ilar to the in-domain data model.To address the issue of distributional similarity wepresent an incremental algorithm which comparesthe distribution of the selected set and the in-domainexamples by using a relative entropy (R.E) criterion.We will review in section 2 some of the rankingschemes which provide baselines for performancecomparison and in section 3 we describe the pro-posed algorithm.
Experimental results are providedin section 4, before we conclude with a summary ofthis work and directions for the future.2 Rank and select methods for textcleaningThe central idea behind text cleanup schemes in re-cent literature, on using web-data for language mod-eling, is to use a scoring function that measures thesimilarity of each observed sentence in the web-data to the in-domain set and assigns an appropri-ate score.
The subsequent step is to set a thresholdin terms of either the minimum score or the num-ber of top scoring sentences.
The threshold can usu-ally be fixed using a heldout set.
Ostendorf (2005)use perplexity from an in-domain n-gram languagemodel as a scoring function.
More recently, a mod-ified version of the BLEU metric which measuressentence similarity in machine translation has beenproposed by Sarikaya (2005) as a scoring function.Instead of explicit ranking and thresholding it is alsopossible to design a classifier in a learning from pos-itive and unlabeled examples framework (LPU) (Liuet al, 2003).
In this system, a subset of the unla-beled set is selected as the negative or the noise setN.
A two class classifier is then trained using thein-domain set and the negative set.
The classifieris then used to label the sentences in the web-data.The classifier can then be iteratively refined by us-ing a better and larger subset of the I/N sentencesselected in each iteration.Rank ordering schemes do not address the issue ofdistributional similarity and select many sentenceswhich already have a high probability in the in-domain text.
Adapting models on such data has thetendency to skew the distribution even further to-wards the center.
For example, in our doctor-patientinteraction task short sentences containing the word?okay?
such as ?okay?,?yes okay?, ?okay okay?
werevery frequent in the in-domain data.
Perplexity orother similarity measures give a high score to allsuch examples in the web-data boosting the prob-ability of these words even further while other perti-nent sentences unseen in the in-domain data such as?Can you stand up please??
are ranked low and getrejected.3 Incremental SelectionTo address the issue of distributional similarity wedeveloped an incremental greedy selection schemebased on relative entropy which selects a sentenceif adding it to the already selected set of sentencesreduces the relative entropy with respect to the in-domain data distribution.Let us denote the language model built from in-domain data as P and let Pinit be a language modelfor initialization purposes which we estimate bybagging samples from the same in-domain data.
Todescribe our algorithm we will employ the paradigmof unigram probabilities though the method general-izes to higher n-grams also.Let W (i) be a initial set of counts for the wordsi in the vocabulary V initialized using Pinit.
We de-note the count of word i in the j th sentence sj ofweb-data with mij .
Let nj = ?i mij be the num-ber of words in the sentence and N = ?i W (i) be146the total number of words already selected.
The rel-ative entropy of the maximum likelihood estimate ofthe language model of the selected sentences to theinitial model P is given byH(j ?
1) = ?
?iP (i) ln P (i)W (i)/NIf we select the sentence sj , the updated R.EH(j) = ?
?iP (i) ln P (i)(W (i) + mij)/(N + nj)Direct computation of R.E using the above ex-pressions for every sentence in the web-data willhave a very high computational cost since O(V )computations per sentence in the web-data are re-quired.
However given the fact that mij is sparse,we can split the summation H(j) intoH(j) = ?
?iP (i) ln P (i) ++?iP (i) ln W (i) + mijN + nj= H(j ?
1) + ln N + njN?
??
?T1?
?i,mij 6=0P (i) ln (W (i) + mij)W (i)?
??
?T2Intuitively, the term T1 measures the decreasein probability mass because of adding nj wordsmore to the corpus and the term T2 measures thein-domain distribution P weighted improvement inprobability for words with non-zero mij .For the R.E to decrease with selection of sentencesj we require T1 < T2.
To make the selection morerefined we can impose a condition T1 + thr(j) <T2 where thr(j) is a function of j.
A good choicefor thr(j) based on empirical study is a function thatdeclines at the same rate as the ratio ln (N+nj)N ?nj/N ?
1/kj where k is the average number ofwords for every sentence.The proposed algorithm is sequential and greedyin nature and can benefit from randomization of theorder in which it scans the corpus.
We generate per-mutes of the corpus by scanning through the corpusand randomly swapping sentences.
Next we do se-quential selection on each permutation and mergethe selected sets.The choice of using maximum likelihood estima-tion for estimating the intermediate language mod-els for W (j) is motivated by the simplification inthe entropy calculation which reduces the order fromO(V ) to O(k).
However, maximum likelihood esti-mation of language models is poor when comparedto smoothing based estimation.
To balance the com-putation cost and estimation accuracy, we modifythe counts W (j) using Kneser-Ney smoothing pe-riodically after fixed number of sentences.4 ExperimentsOur experiments were conducted on medical do-main data collected for building the English ASRof our English-Persian Speech to Speech translationproject (Georgiou et al, 2003).
We have 50K in-domain sentences for this task available.
We down-loaded around 60GB data from the web using au-tomatically generated queries which after filteringand normalization amount to 150M words.
The testset for perplexity evaluations consists of 5000 sen-tences(35K words) and the heldout set had 2000sentences (12K words).
The test set for word er-ror rate evaluation consisted of 520 utterances.
Ageneric conversational speech language model wasbuilt from the WSJ, Fisher and SWB corpora in-terpolated with the CMU LM.
All language modelsbuilt from web-data and in-domain data were inter-polated with this language model with the interpola-tion weight determined on the heldout set.We first compare our proposed algorithm againstbaselines based on perplexity(PPL), BLEU and LPUclassification in terms of test set perplexity.
As thecomparison shows the proposed algorithm outper-forms the rank-and-select schemes with just 1/10thof data.
Table 1 shows the test set perplexity withdifferent amounts of initial in-domain data.
Table 2shows the number of sentences selected for the bestperplexity on the heldout set by the above schemes.The average relative perplexity reduction is around6%.
In addition to the PPL and WER improvementswe were able to acheive a factor of 5 reduction inthe number of estimated language model parameters(bigram+trigram) and a 30% reduction in the vocab-14710K 20K 30K 40KNo Web 60 49.6 42.2 39.7AllWeb 57.1 48.1 41.8 38.2PPL 56.1 48.1 41.8 38.2BLEU 56.3 48.2 42.0 38.3LPU 56.3 48.2 42.0 38.3Proposed 54.8 46.8 40.7 38.1Table 1: Perplexity of testdata with the web adaptedmodel for different number of initial sentences.ulary size.
No Web refers to the language model builtfrom just in-domain data with no web-data.
All-Web refers to the case where the entire web-data wasused.The WER results in Table 3 show that adding datafrom the web without proper filtering can actuallyharm the performance of the speech recognition sys-tem when the initial in-domain data size increases.This can be attributed to the large increase in vo-cabulary size which increases the acoustic decoderperplexity.
The average reduction in WER using theproposed scheme is close to 3% relative.
It is inter-esting to note that for our data selection scheme theperplexity improvments correlate surprisingly wellwith WER improvments.
A plausible explanationis that the perplexity improvments are accompaniedby a significant reduction in the number of languagemodel parameters.5 Conclusion and Future WorkIn this paper we have presented a computationallyefficient scheme for selecting a subset of data froman unclean generic corpus such as data acquiredfrom the web.
Our results indicate that with thisscheme, we can identify small subsets of sentences(about 1/10th of the original corpus), with which wecan build language models which are substantiallysmaller in size and yet have better performance in10K 20K 30K 40KPPL 93 92 91 91BLEU 91 90 89 89LPU 90 88 87 87Proposed 12 11 11 12Table 2: Percentage of web-data selected for differ-ent number of initial sentences.10K 20K 30K 40KNo Web 19.8 18.9 18.3 17.9AllWeb 19.5 19.1 18.7 17.9PPL 19.2 18.8 18.5 17.9BLEU 19.3 18.8 18.5 17.9LPU 19.2 18.8 18.5 17.8Proposed 18.3 18.2 18.2 17.3Table 3: Word Error Rate (WER) with web adaptedmodels for different number of initial sentences.both perplexity and WER terms compared to modelsbuilt using the entire corpus.
Although our focus inthe paper was on web-data, we believe the proposedmethod can be used for adaptation of topic specificmodels from large generic corpora.We are currently exploring ways to use multiplebagged in-domain language models for the selectionprocess.
Instead of sequential scan of the corpus, weare exploring the use of rank-and-select methods togive a better search sequence.ReferencesAbhinav Sethy and Panayiotis Georgiou et al.
Building topicspecific language models from web-data using competitivemodels.
Proceedings of Eurospeech.
2005Bing Liu and Yang Dai et al.
Building Text Classifiers UsingPositive and Unlabeled Examples.
Proceedings of ICDM.2003Kamal Nigam and Andrew Kachites McCallum et al.
TextClassification from Labeled and Unlabeled Documents usingEM.
Journal of Machine Learning.
39(2:3)103?134.
2000Mirella Lapata and Frank Keller.
Web-based models for natu-ral language processing.
ACM Transactions on Speech andLanguage Processing.
2(1),2005.Philip Resnik and Noah A. Smith.
The Web as a parallel cor-pus.
Computational Linguistics.
29(3),2003.P.G.
Georgiou and S.Narayanan et al.
Transonics: A speech tospeech system for English-Persian Interactions.
Proceedingsof IEEE ASRU.
2003Ruhi Sarikaya and Agustin Gravano et al Rapid LanguageModel Development Using External Resources For NewSpoken Dialog Domains Proceedings of ICASSP.
2005Tim Ng and Mari Ostendorf et al.
Web-data Augmented Lan-guage Model for Mandarin Speech Recognition.
Proceed-ings of ICASSP.
2005Xiaojin Zhu.
Semi-Supervised Learning Literature Survey.Computer Science, University of Wisconsin-Madison.148
