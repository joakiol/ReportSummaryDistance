Study of Practical Effectiveness for Machine Translation UsingRecursive Chain-link-type LearningHiroshi Echizen-yaDept.
of Electronics and InformationHokkai-Gakuen UniversityS 26-Jo, W 11-Chome, Chuo-kuSapporo, 064-0926 Japanechi@eli.hokkai-s-u.ac.jpKenji ArakiDivision of Electronics and InformationHokkaido UniversityN 13-Jo, W 8-Chome, Kita-kuSapporo, 060-8628 Japanaraki@media.eng.hokudai.ac.jpYoshio MomouchiDept.
of Electronics and InformationHokkai-Gakuen UniversityS 26-Jo, W 11-Chome, Chuo-kuSapporo, 064-0926 Japanmomouchi@eli.hokkai-s-u.ac.jpKoji TochinaiDivision of Business AdministrationHokkai-Gakuen University4-Chome, Asahi-machi, Toyohira-kuSapporo, 060-8790 Japantochinai@econ.hokkai-s-u.ac.jpAbstractA number of machine translation systems basedon the learning algorithms are presented.
Thesemethods acquire translation rules from pairsof similar sentences in a bilingual text cor-pora.
This means that it is difficult for thesystems to acquire the translation rules fromsparse data.
As a result, these methods requirelarge amounts of training data in order to ac-quire high-quality translation rules.
To over-come this problem, we propose a method of ma-chine translation using a Recursive Chain-link-type Learning.
In our new method, the systemcan acquire many new high-quality translationrules from sparse translation examples based onalready acquired translation rules.
Therefore,acquisition of new translation rules results inthe generation of more new translation rules.Such a process of acquisition of translation rulesis like a linked chain.
From the results of evalua-tion experiments, we confirmed the effectivenessof Recursive Chain-link-type Learning.1 IntroductionRule-Based Machine Translation(MT)(Hutchinsand Somers, 1992) requires large-scale knowl-edge to analyze both source language(SL)sentences and target language(TL) sentences.Moreover, it is difficult for a developer to com-pletely describe large-scale knowledge that cananalyze various linguistic phenomena.
There-fore, Rule-Based MT is time-consuming andexpensive.
Statistical MT and Example-BasedMT have been proposed to overcome the dif-ficulties of Rule-Based MT.
These approachescorrespond to Corpus-Based approach.
Corpus-Based approach uses translation examples thatkeep including linguistic knowledge.
Thismeans that the system can improve the qualityof its translation only by adding new translationexamples.
However, in Statistical MT(Brownet al, 1990), large amounts of translationexamples are required in order to obtainhigh-quality translation.
Moreover, Example-Based MT(Sato and Nagao, 1990; Watanabeand Takeda, 1998; Brown, 2001; Carl, 2001)which relies on various knowledge resourcesresults in the same difficulties as Rule-BasedMT.
Therefore, Example-Based MT, whichautomatically acquires the translation rulesfrom only bilingual text corpora, is very effec-tive.
However, existing Example-Based MTsystems using the learning algorithms requirelarge amounts of translation pairs to acquirehigh-quality translation rules.In Example-Based MT based on analogicalreasoning(Malavazos, 2000; Guvenir, 1998), thedifferent parts are replaced by variables to gen-eralize translation examples as shown in (1) ofFigure 1.
However, the number of differentparts of the two SL sentences must be sameas the number of different parts of the two TLsentences.
This means that the condition of ac-quisition of translation rules is very strict be-cause this method allows only n:n mappings inthe number of the different parts between theSL sentences and the TL sentences.
As a re-sult, many translation rules cannot be acquired.
(McTait, 2001) generalizes both the differentparts and the common parts as shown in Fig-ure 1(2).
This means that (McTait, 2001) al-lows m:n mappings in the number of the differ-ent parts, or the number of the common parts.However, it is difficult to acquire the translationrules that correspond to the lexicon level.
Onthe other hand, we have proposed a method ofMachine Translation using Inductive Learningwith Genetic Algorithms(GA-ILMT)(Echizen-ya et al, 1996).
This method automaticallygenerates the similar translation examples fromonly given translation examples by applying ge-netic algorithms(Goldberg, 1989) as shown in(3a) of Figure 1.
Moreover, the system per-forms Inductive Learning.
By using InductiveLearning, the abstract translation rules are ac-quired by performing phased extraction of dif-ferent parts as shown in Figure 1(3b) and (3c).In all methods shown in Figure 1, the condi-tion of acquisition of translation rules is thattwo similar translation examples must exist.
Asa result, the systems require large amounts oftranslation examples.We propose a method of MT using Recur-sive Chain-link-type Learning as a method toovercome the above problem.
In our method,the system acquires new translation rules fromsparse data using other already acquired trans-lation rules.
For example, first, translationrule B is acquired by using translation rule Awhen the translation rule A exists in the dictio-nary.
Moreover, translation rule C is acquiredby using the translation rule B.
Such a pro-cess of acquisition of translation rules is like achain where each ring is linked.
Therefore, wecall this mechanism Recursive Chain-link-typeLearning(RCL).
This method can effectively ac-quire many translation rules from sparse datawithout depending on the different parts of sim-ilar translation pairs.
In this paper, we describethe effectiveness of RCL through evaluation ex-periments.2 Basic IdeaRCL is a method with an ability that automat-ically acquires translation knowledge in a com-puter without any analytical knowledge, such asGA-ILMT.
This is the ability to extract corre-	 		 	!"#$ "$ 	!"#$ "$%	!"
#&!"''!
'"#$"$'(			)*(++(			 	&, 	&,(%	-	-&,!
-	%		 -(	* -,-!
.
-,/&-0-123.
)*4 2#	 56!
 	%		  !
		 	%		 !
7&-&8$	59:; ;<=;>;?
@;ABC8D  59:E; ;FGH;>;?
@;ABC1	*	4I	,%#,,#8$59:;J;FGH;>;?
@;ABC8D 	59:E;J;<=;>;?
@;ABC1%	4I		%#3&.	8$59:;J;;>;?
@;ABC8$59:;J; ;>;?@;ABC8$K+59:;J;K+;>;?
@;ABC1)	4I		%#8!K+59";J;K+;>;?@;ABC8#K+59"$;J;K+;>;?@;ABC8KK+59K;J;K+;>;?
@;ABC*	4I	,*	4I	,89FGHLM8	9<=L8$9:LM8D 9:EL3&.	Figure 1: Previous works.sponding parts from pairs of objects with whichit corresponds.
In this paper, we apply this abil-ity to a translation example that consists of SLand TL sentences.
A system with RCL can ac-quire translation rules from sparse translationexamples.
Figure 2 shows how translation rulesare acquired using this method1.Figure 2 shows the process where translationrules B, C and D are acquired one after anotherusing RCL.
In this paper, source parts are thoseparts that are extracted from the SL sentencesof translation examples, and target parts arethose parts that are extracted from the TL sen-tences of translation examples.
Moreover, parttranslation rules are pairs of source parts and1In Figure 2, the use of a Greek character means thatall language characters correspond to unknown characterstrings for a computer.!"#$%&'()*+(,-.)/01(23+0/4.5Process&'()*+(,-.)/01(23+0/4.6&'()*+(,-.)/01(23+0/4.74.5Process4.6Process4.7!"#$#"!%&'!#()%$"*'+$,!"#$#"!%&'!#()%$"*'+$,!"#$#"!%&'!#()%$"*'+$,!"#$#"!%&'!#()%$"*'+$,-+%#+%.+$#"!%&'!#()%$$$$"*'+$/-+%#+%.+$#"!%&'!#()%$$$$"*'+$/-+%#+%.+$#"!%&'!#()%$$$$"*'+$/-+%#+%.+$#"!%&'!#()%$$$$"*'+$/!"#$#"!%&'!#()%$"*'+$0!"#$#"!%&'!#()%$"*'+$0!"#$#"!%&'!#()%$"*'+$0!"#$#"!%&'!#()%$"*'+$0-+%#+%.+$#"!%&'!#()%$"*'+$1-+%#+%.+$#"!%&'!#()%$"*'+$1-+%#+%.+$#"!%&'!#()%$"*'+$1-+%#+%.+$#"!%&'!#()%$"*'+$1-)*".+$2!"#-)*".+$2!"#-)*".+$2!"#-)*".+$2!"#3!"4+#$2!"#3!"4+#$2!"#3!"4+#$2!"#3!"4+#$2!"#3"!%&'!#()%$"*'+3"!%&'!#()%$"*'+3"!%&'!#()%$"*'+3"!%&'!#()%$"*'+Figure2:SchemainprocessofacquisitionoftranslationrulesusingRCL.targetparts,extractedaspartsliketranslationrulesAandCinFigure2.Sentencetranslationrulesarepairsofsourceandtargetpartsex-tractedassentencesliketranslationrulesBandDinFigure2.Ontheotherhand,translationrulesthatareusedasstartingpointsintheac-quisitionprocessoftranslationrules,liketrans-lationruleAinFigure2,areacquiredbyusingGA-ILMT.Thereasonbeingthatthesystemcanperformtranslationbasedononlylearningabilitywithoutanyanalyticalknowledge,byus-ingGA-ILMTandRCL.AsystemwithRCLacquiresparttranslationrulesandsentencetranslationrulestogether.Asaresult,achainreactioncausestheac-quisitionoftranslationrules.IntheprocessNo.1ofFigure2,translationruleAhasinfor-mationthatthesystemcanextract?Z?fromtheSLsentencesoftranslationexamples,orthesourcepartsoftranslationrules,andcanextract???fromtheTLsentencesorthetargetparts.Therefore,thesystemcanacquirethesentencetranslationruleBbyextracting?Z?fromtheSLsentenceoftranslationexampleNo.1and???fromtheTLsentenceoftranslationexam-pleNo.1.TheacquiredtranslationruleBhasinformationthatthesystemcanextractfromtherightof?E?totheleftof?H?intheSLsentencesoftranslationexamples,orthesourcepartsoftranslationrules,andcanextractfromtherightof???totheleftof???intheTLsentencesorthetargetparts.Byusingthisin-formation,inprocess2,thesystemcanacquireparttranslationruleC?N?????byextract-ing?N??fromtheSLsentenceoftranslationexampleNo.2and???
?fromtheTLsentence.Moreover,inprocess3,translationruleDisac-quiredbasedontranslationruleC.Suchprocessisperformedbydecidingthecommonanddif-ferentpartsincharacterstringsoftranslationexamples(Arakietal.,1995).Therefore,thesystempossessesanabilitytodecidecommonanddifferentpartsbetweentwoobjects.3OutlineFigure3:Processflow.Figure3showstheoutlineofanEnglish-to-JapaneseMTsystemwithRCL.First,auserin-putsaSLsentenceinEnglish.Inthetranslationprocess,thesystemgeneratestranslationresultsusingtranslationrulesacquiredinthelearningprocess.Theuserthenproofreadsthetrans-latedsentencescheckingforerrors.Inthefeed-backprocess,thesystemevaluatesthetransla-tionrulesusedinthetranslationprocess.Inthelearningprocess,thetranslationrulesareacquiredbyusingtwolearningalgorithms.OneisGA-ILMT,theotherisRCL.Thesetwoal-gorithmsworkeachother.Namely,thetrans-lationrulesacquiredbyGA-ILMTareusedinRCL,andthetranslationrulesacquiredbyRCLareusedinInductiveLearningofGA-ILMT.Inthispaper,weimplementedanewsystembasedonFigure3asabootstrappingsystem,andwethenevaluatedthissystem.4 Process4.1 Translation processIn the translation process, the system gener-ates translation results using acquired transla-tion rules.
First, the system selects the sen-tence translation rules that can be applied tothe SL sentence.
Second, the system generatesthe translation results by replacing the variablesin the sentence translation rules with the parttranslation rules.4.2 Feedback processIn the feedback process, the system evaluatesthe translation rules used.
First, the systemevaluates the translation rules without variablesby using the results of combinations betweenthe translation rules with variables and thetranslation rules without variables(Echizen-yaet al, 1996).
Next, the system evaluates trans-lation rules with variables by using the processesof combinations between the translations ruleswith variables and the translation rules withoutvariables(Echizen-ya et al, 2000).
As a result,the system increases the correct translation fre-quencies, or the erroneous translation frequen-cies, of the translation rules by using these eval-uation methods for the translation rules.4.3 Learning process4.3.1 GA-ILMTIn this paper, by using the process of acquisi-tion of translation rules in GA-ILMT, the sys-tem acquires both sentence and part transla-tion rules.
These rules are then used as startingpoints when the system performs RCL.4.3.2 Recursive Chain-link-typeLearning(RCL)In this section, we describe the process of acqui-sition of translation rules using RCL.
The de-tails of the process of acquisition of part trans-lation rules are as follows.
(1)The system selects translation examplesthat have common parts with the sentencetranslation rules.
(2)The system extracts the parts that corre-spond to the variables in the source partsand in the target parts of the sentencetranslation rules from the SL sentences,and the TL sentences of the translation ex-amples.
(3)The system registers pairs, of the parts ex-tracted from the SL sentences and the partsextracted from the TL sentences, as thepart translation rules.
(4)The system gives the correct and erroneousfrequencies of sentence translation rules tothe acquired part translation rules.Figure 42 shows an example of the acquisi-tion of a part translation rule using the sentencetranslation rule.
In Figure 4, (thirty;30[sanju])as the part translation rule is acquired be-cause ?thirty?
corresponds to the variable inthe source part of sentence translation rule and?30[sanju]?
corresponds to the variable in thetarget part of sentence translation rule.   	  	 							!
"#	 $%	&' (			)*+,-.-/012/+ ----3-45-678-39:;<%=>----9-?>-@A-B8-39:;<%C			#        $#       $Figure 4: Example of the acquisition of a parttranslation rule using the sentence translationrule.The details of the process of acquisition ofsentence translation rules are as follows:(1)The system selects the part translation rulesin which the source parts are included inthe SL sentences of the translation exampleor in the source parts of sentence transla-tion rules, and in which the target parts areincluded in the TL sentences of the trans-lation examples or in the target parts ofsentence translation rules.
(2)The system acquires new sentence transla-tion rules by replacing the parts which aresame as the part translation rules with thevariables to the translation examples or thesentence translation rules.
(3)The system gives the correct and erroneousfrequencies of the part translation rules tothe acquired sentence translation rules.2Italics are the pronunciation in Japanese.Figure 5 shows examples of the acquisitionof the sentence translation rules using the parttranslation rules.
In Figure 5, the systemacquires?It starts in @0 minutes.???/?/@0/?/??/?/???/???
[Sore wa @0 puntate ba hajimari masu.
]?as a sentence trans-lation rule by using the part translation rule(thirty;30[sanju]) acquired in Figure 4, and?
@1starts in @0 minutes.?@1/?/@0/?/??/?/???/???
[@1 wa @0 pun tate ba hajimarimasu.
]?as the sentence translation rule, that ismore abstracted, is acquired by using the parttranslation rule (it;??
[sore]).	 											 				 !"!
!#!$%!&!'()!
(*+ !"!
!#!$%!&!'()!
(*+			,								-		-!"!
!#!$%!&!'()!
(*+ 		!"!
!#!$%!&!'()!
(*+.				/	 	   01/	 	   01/ 	   01/	  	   01Figure 5: Examples of the acquisition of a sen-tence translation rule using the part translationrule.5 Experiments for performanceevaluation5.1 Experimental procedureThere are two kinds of data as experimentaldata.
One is learning data and the other isevaluation data.
In these experiments, 1,759translation examples were used as learning data.These translation examples were taken fromtextbooks(Nihon Kyozai(1), 2001; Nihon Ky-ozai(2), 2001; Hoyu Shuppan, 2001) for second-grade junior high school students.
As well,1,097 translation examples were used as eval-uation data.
These translation examples weretaken from textbooks(Bunri, 2001; Sinko Shup-pan, 2001) for second-grade junior high schoolstudents.
All of these translation examples wereprocessed by the method outlined in Figure 3.The initial condition of the dictionary is empty.Moreover, we used three other commercial Rule-Based MT systems, comparing our system withthose systems.
We call these three MT systemsA, B and C respectively.5.2 Evaluation standardsThe correct translation results are grouped intotwo categories:(1) The correct translationThis means that the translation results cor-respond to the correct translation resultstaken from textbooks respectively(Bunri,2001; Sinko Shuppan, 2001).
(2) A correct translation which includes un-known wordsThis means that the translation resultswith substituted nouns or adjectives asvariables correspond to the correct trans-lation results taken from textbooks respec-tively(Bunri, 2001; Sinko Shuppan, 2001).In this paper, the effective translation resultsare the translation results that correspond to(1) and (2), and the ineffective translation re-sults are the translation results that do not cor-respond to (1) and (2).
Moreover, the effec-tive translation rate is the rate of the effectivetranslation results in all the evaluation data.The translation results are ranked when severaltranslation results are generated.
The transla-tion results using the translation rules whoserate of correct translation frequency is high, areranked at the top.
We evaluated the translationresults that are ranked from No.1 to No.3.5.3 Experimental results and discussionTable 1 shows examples of effective translationresults in our system with RCL.
Table 2 showsthe results of comparative experiments of oursystem and the three Rule-Based MT systems.We excluded 309 SL sentences from 1,097 SLsentences used as evaluation data in Table 2.
Inour system, the 309 SL sentences became the in-effective translation results because of a lack oflearning data.
Therefore, the 309 SL sentencesare not inadequate as evaluation data.
Table 2shows the effective translation rates in 788 SLsentences, which were left after excluding 309SL sentences from the 1,097 SL sentences usedas evaluation data.
In the other three Rule-Based MT systems, the same 788 SL sentenceswere used as evaluation data and the transla-tion results which correspond to (1) and (2)Table 1: Examples of effective translation results.Examples of the correct translation resultsSL sentences TL sentencesThis bag was made in France.
??????????????
[Kono baggu wa furansu sei desu.
]We went there to play ??????????????????????baseball.
[Watashi tachi wa yakyu wo suru tame soko e iki mashi ta.
]Examples of the correct translation results which includes the unknown wordsSL sentences TL sentences@0??????????????
?Shall I take you to the [@0 e tsure te itte age masho ka?
]amusement park?
?
@0 requires ????
[yuen chi]?
which is equivalent for?the noun ?the amusement park?.@0?????????????????????
[@0 karaHow far is it from Kyoto to hiroshima made dono kurai no kyori ga ari masu ka?]Hiroshima?
?
@0 requires ???
[kyoto]?
which is equivalent for?the noun ?Kyoto?.described in section 5.2 were evaluated as thecorrect translation results.
The effective trans-lation rate in the system with only GA-ILMTwas 45.1%.
In Table 2, the effective translationrate of system with RCL is almost the same asthe effective translation rates of system A, butis higher than systems B and C.Table 2: Results of comparative experiments.Effective trans- DetailsSystem lation rates (1) (2)Our system 85.0% 41.6% 58.4%system A 85.8% 84.0% 16.0%system B 81.7% 83.7% 16.3%system C 76.9% 82.7% 17.3%Table 3: Comparison of effective translationrates based on quality.Effective trans- DetailsSystem lation rates (1) (2)Our system 73.7% 7.5% 52.5%system A 70.3% 84.2% 15.8%system B 63.8% 85.0% 15.0%system C 58.7% 82.8% 17.2%Moreover, we evaluated translation resultsmore strictly in terms of the quality of trans-lation.
Meaning that only translation resultsthat had almost the same character strings asthe correct translation results taken from thetextbooks(Bunri, 2001; Sinko Shuppan, 2001)were effective translation results.
For exam-ple, ?????
10??????
[Sore wa yakujuppun kakari masu.]?
is an ineffective trans-lation result because of the correct transla-tion results for ?It takes about ten minutes.
?is ??
10 ??????
[Yaku juppun kakarimasu.]?
in textbook(Bunri, 2001; Sinko Shup-pan, 2001).
In this Japanese sentence, phrase????
[sore wa]?
results in needlessly long.Therefore, we evaluate the translation resultsthat have different phrases to the correct trans-lation results as the ineffective translation re-sults in terms of the quality of translation.
Ta-ble 3 shows a comparison of effective translationrates based on quality.
In Table 3, we confirmedthat the system with RCL can generate morehigh-quality translation results than the threeother Rule-Based MT systems.In the system with RCL, the erroneous trans-lation rules are also acquired like a linked chain.For example, in Figure 2, the translation rulesB, C and D are acquired as the erroneous trans-lation rules when the translation rule A is theerroneous translation rule.
Namely, a chain re-action causes the acquisition of erroneous trans-lation rules.
In learning data, the rate of er-roneous part translation rules to the acquiredpart translation rules was 47.9%, and the rateof erroneous sentence translation rules to theacquired sentence translation rules was 38.2%.However, such erroneous translation rules areautomatically decided as being erroneous trans-lation rules in the feedback process resultingfrom the ineffective translation results.6 ConclusionIn existing Example-Based MT systems basedon learning algorithms, similar translation pairsmust exist to acquire high-quality translationrules.
This means that the systems requirelarge amounts of translation examples to ac-quire high-quality translation rules.
On theother hand, a system with RCL can acquiremany new translation rules from sparse trans-lation examples because it uses other alreadyacquired translation rules based on the learn-ing algorithms described in section 2.
As a re-sult, the quality of the translation and the ef-fective translation rate of our system is higherthan other Rule-Based MT systems.
However,our system still does not reach the level of apractical MT system and requires more transla-tion rules to realize the goal of a practical MTsystem.
Although our system is not a practicalenough MT system, the system can effectivelyacquire the translation rules from sparse databy using RCL.
Therefore, we consider that thequality of translation improves only by addingnew translation examples without the difficultyof Rule-Based MT systems in which a developermust completely describe large-scale knowledge.In the future, we plan to add a mechanismthat effectively combines the acquired transla-tion rules so that the system realizes the trans-lation of practical SL sentences.7 AcknowledgementsThis work was partially supported by theGrants from the High-Tech Research Cen-ter of Hokkai-Gakuen University and a Gov-ernment subsidy for aiding scientific research(No.14658097) of the Ministry of Education,Culture, Sports, Science and Technology ofJapan.ReferencesHutchins, W. J and H. L. Somers.
1992.
AnIntroduction to Machine Translation.
ACA-DEMIC PRESS.Sato, S and M. Nagao.
1990.
Toward Memory-based Translation.
In proceedings of the Col-ing?90.Brown, P., J. Cocke, S. Della Pietra, V. J. DellaPietra, F. Jelinek, J. D. Lafferty, R. L. Mer-cer and P. S. Roossin.
1990.
A StatisticalApproach to Machine Translation.
Computa-tional Linguistics Vol.16, No.2.Watanabe, H and K. Takeda.
1998.
A Pattern-based Machine Translation System Extendedby Example-based Processing.
In proceedingsof the Coling-ACL?98.Brown, R.D.
2001.
Transfer-Rule Inductionfor Example-Based Translation.
In proceed-ings of the Workshop on EBMT, MT SummitVIII.Carl, M. 2001.
Inducing Translation Grammarsfrom Bracketed Alignments.
In proceedings ofthe Workshop on EBMT, MT Summit VIII.Malavazos, C and S. Piperidis.
2000.
Appli-cation of Analogical Modelling to ExampleBased Machine Translation.
In proceedings ofthe Coling2000.Gu?venir, H.A and I. Cicekli.
1998.
LearningTranslation Templates from Examples.
Infor-mation Systems Vol.23, No.6.McTait, K. 2001.
Linguistic Knowledge andComplexity in an EBMT System Based onTranslation Patterns.
In proceedings of theWorkshop on EBMT, MT Summit VIII.Echizen-ya, H., K. Araki, Y. Momouchi and K.Tochinai.
1996.
Machine Translation Methodusing Inductive Learning with Genetic Algo-rithms.
In proceedings of the Coling?96.Goldberg, D. E. 1989.
Genetic Algorithms inSearch, Optimization, and Machine Learning.Addison-Wesley.Araki, K., Y. Momouchi and K. Tochinai.
1995.Evaluation for Adaptability of Kana-kanjiTranslation of Non-segmented Japanese KanaSentences using Inductive Learning.
In pro-ceedings of the PACLING?95.Echizen-ya, H., K. Araki, Y. Momouchi andK.
Tochinai.
2000 Effectiveness of LayeringTranslation Rules based on Transition Net-works in Machine Translation using InductiveLearning with Genetic Algorithms.
In pro-ceedings of the MT and Multilingual Applica-tions in the New Millennium.Nihon-Kyozai(1).
2001.
One World EnglishCourse 1 new edition.
Tokyo.Nihon-Kyozai(2).
2001.
One World EnglishCourse 2 new edition.
Tokyo.Hoyu Shuppan.
2001.
System English Course 2new edition.
Tokyo.Bunri.
2001.
Work English Course 2 new edi-tion.
Tokyo.Sinko Shuppan 2001.
Training English Course2 new edition.
Osaka.
