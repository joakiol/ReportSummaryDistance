Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 881?891,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsResponse-based Learning for Grounded Machine TranslationStefan Riezler and Patrick Simianer and Carolin HaasDepartment of Computational LinguisticsHeidelberg University, 69120 Heidelberg, Germany{riezler,simianer,haas1}@cl.uni-heidelberg.deAbstractWe propose a novel learning approach forstatistical machine translation (SMT) thatallows to extract supervision signals forstructured learning from an extrinsic re-sponse to a translation input.
We showhow to generate responses by groundingSMT in the task of executing a seman-tic parse of a translated query againsta database.
Experiments on the GEO-QUERY database show an improvement ofabout 6 points in F1-score for response-based learning over learning from refer-ences only on returning the correct an-swer from a semantic parse of a translatedquery.
In general, our approach alleviatesthe dependency on human reference trans-lations and solves the reachability problemin structured learning for SMT.1 IntroductionIn this paper, we propose a novel approachfor learning and evaluation in statistical ma-chine translation (SMT) that borrows ideas fromresponse-based learning for grounded semanticparsing.
In this framework, the meaning of a sen-tence is defined in the context of an extrinsic task.Successful communication of meaning is mea-sured by a successful interaction in this task, andfeedback from this interaction is used for learning.We suggest that in a similar way the preser-vation of meaning in machine translation shouldbe defined in the context of an interaction in anextrinsic task.
For example, in the context of agame, a description of a game rule is translatedsuccessfully if correct game moves can be per-formed based only on the translation.
In the con-text of a question-answering scenario, a questionis translated successfully if the correct answer isreturned based only on the translation of the query.We propose a framework of response-basedlearning that allows to extract supervision signalsfor structured learning from the response of anextrinsic task to a translation input.
Here, learn-ing proceeds by ?trying out?
translation hypothe-ses, receiving a response from interacting in thetask, and converting this response into a supervi-sion signal for updating model parameters.
In caseof positive feedback, the predicted translation canbe treated as reference translation for a structuredlearning update.
In case of negative feedback, astructural update can be performed against transla-tions that have been approved previously by pos-itive task feedback.
This framework has severaladvantages:?
The supervision signal in response-basedlearning has a different quality than super-vision by human-generated reference transla-tions.
While a human reference translationis generated independently of the SMT task,conversion of predicted translations into ref-erences is always done with respect to a spe-cific task.
In this sense we speak of ground-ing meaning transfer in an extrinsic task.?
Response-based learning can repeatedly tryout system predictions by interacting in theextrinsic task.
Instead of and in additionto learning from human reference transla-tions, response-based learning allows to con-vert multiple system translations into refer-ences.
This alleviates the supervision prob-lem in cases where parallel data are scarce.?
Task-specific response acts upon systemtranslations.
This avoids the problem of un-reachability of independently generated ref-erence translations by the SMT system.The proposed approach of response-basedlearning opens the doors for various extrinsic tasks881in which SMT systems can be trained and evalu-ated.
In this paper, we present a proof-of-conceptexperiment that uses feedback from a simulatedworld environment.
Building on prior work ingrounded semantic parsing, we generate transla-tions of queries, and receive feedback by execut-ing semantic parses of translated queries againstthe database.
Successful response is defined as re-ceiving the same answer from the semantic parsesfor the translation and the original query.
Our ex-perimental results show an improvement of about6 points in F1-score for response-based learningover standard structured learning from referencetranslations.
We show in an error analysis thatthis improvement can be attributed to using struc-tural and lexical variants of reference translationsas positive examples in response-based learning.Furthermore, translations produced by response-based learning are found to be grammatical.
Thisis due to the possibility to boost similarity to hu-man reference translations by the additional use ofa cost function in our approach.2 Related WorkThe key idea of grounded language learningis to study natural language in the context ofa non-linguistic environment, in which meaningis grounded in perception and/or action.
Thispresents an analogy to human learning, where alearner tests her understanding in an actionablesetting.
Such a setting can be a simulated worldenvironment in which the linguistic representa-tion can be directly executed by a computer sys-tem.
For example, in semantic parsing, the learn-ing goal is to produce and successfully executea meaning representation.
Executable system ac-tions include access to databases such as the GEO-QUERY database on U.S. geography (Wong andMooney (2006), inter alia), the ATIS travel plan-ning database (Zettlemoyer and Collins (2009),inter alia), robotic control in simulated naviga-tion tasks (Chen and Mooney (2011), inter alia),databases of simulated card games (Goldwasserand Roth (2013), inter alia), or the user-generatedcontents of FREEBASE (Cai and Yates (2013), in-ter alia).
Since there are many possible correctparses, matching against a single gold standardfalls short of grounding in a non-linguistic envi-ronment.
Rather, the semantic context for inter-pretation, as well as the success criterion in evalua-tion is defined by successful execution of an actionin the extrinsic environment, e.g., by receiving thecorrect answer from the database or by successfulnavigation to the destination.
Recent attempts tolearn semantic parsing from question-answer pairswithout recurring to annotated logical forms havebeen presented by Kwiatowski et al (2013), Be-rant et al (2013), or Goldwasser and Roth (2013).The algorithms presented in these works are vari-ants of structured prediction that take executabilityof semantic parses into account.
Our work buildsupon these ideas, however, to our knowledge thepresented work is the first to embed translationsinto grounded scenarios in order to use feedbackfrom interactions in these scenarios for structuredlearning in SMT.A recent important research direction in SMThas focused on employing automated translationas an aid to human translators.
Computer as-sisted translation (CAT) subsumes several modesof interaction, ranging from binary feedback onthe quality of the system prediction (Saluja etal., 2012), to human post-editing operations on asystem prediction resulting in a reference transla-tion (Cesa-Bianchi et al, 2008), to human accep-tance or overriding of sentence completion pre-dictions (Langlais et al, 2000; Barrachina et al,2008; Koehn and Haddow, 2009).
In all inter-action scenarios, it is important that the systemlearns dynamically from its errors in order to of-fer the user the experience of a system that adaptsto the provided feedback.
Since retraining theSMT model after each interaction is too costly,online adaptation after each interaction has be-come the learning protocol of choice for CAT.
On-line learning has been applied in generative SMT,e.g., using incremental versions of the EM algo-rithm (Ortiz-Mart?
?nez et al, 2010; Hardt and Elm-ing, 2010), or in discriminative SMT, e.g., usingperceptron-type algorithms (Cesa-Bianchi et al,2008; Mart?
?nez-G?omez et al, 2012; W?aschle etal., 2013; Denkowski et al, 2014).
In a simi-lar way to deploying human feedback, extrinsicloss functions have been used to provide learn-ing signals for SMT.
For example, Nikoulina etal.
(2012) propose a setup where an SMT systemfeeds into cross-language information retrieval,and receives feedback from the performance oftranslated queries with respect to cross-languageretrieval performance.
This feedback is used totrain a reranker on an n-best list of translations or-der with respect to retrieval performance.
In con-882Figure 1: Response-based learning cycle for grounding SMT in virtual trivia gameplay.trast to our work, all mentioned approaches to in-teractive or adaptive learning in SMT rely on hu-man post-edits or human reference translations.Our work differs from these approaches in thatexactly this dependency is alleviated by learningfrom responses in an extrinsic task.Interactive scenarios have been used for eval-uation purposes of translation systems for nearly50 years, especially using human reading compre-hension testing (Pfafflin, 1965; Fuji, 1999; Joneset al, 2005), and more recently, using face-to-face conversation mediated via machine transla-tion (Sakamoto et al, 2013).
However, despite of-fering direct and reliable prediction of translationquality, the cost and lack of reusability has con-fined task-based evaluations involving humans totesting scenarios, but prevented a use for interac-tive training of SMT systems as in our work.Lastly, our work is related to cross-lingual nat-ural language processing such as cross-lingualquestion answering or cross-lingual informationretrieval as conducted at recent evaluation cam-paigns of the CLEF initiative.1While these ap-proaches focus on improvements of the respectivenatural language processing task, our goal is to im-prove SMT by gathering feedback from the task.1http://www.clef-initiative.eu3 Grounding SMT in Semantic ParsingIn this paper, we present a proof-of-concept of ourideas of embedding SMT into simulated world en-vironments as used in semantic parsing.
We usethe well-known GEOQUERY database on U.S. ge-ography for this purpose.
Embedding SMT in asemantic parsing scenario means to define transla-tion quality by the ability of a semantic parser toconstruct a meaning representation from the trans-lated query, which returns the correct answer whenexecuted against the database.
If viewed as simu-lated gameplay, a valid game move in this scenarioreturns the correct answer to a translated query.The diagram in Figure 1 gives a sketch ofresponse-based learning from semantic parsing inthe geographical domain.
Given a manual Ger-man translation of the English query as source sen-tence, the SMT system produces an English targettranslation.
This sentence is fed into a semanticparser that produces an executable parse represen-tation ph.
Feedback is generated by executing theparse against the database of geographical facts.Positive feedback means that the correct answer isreceived, i.e., exec(pg)?= exec(ph) indicates thatthe same answer is received from the gold standardparse pgand the parse for the hypothesis transla-tion ph; negative feedback results in case a differ-ent or no answer is received.The key advantage of response-based learning883is the possibility to receive positive feedback evenfrom predictions that differ from gold standardreference translations, but yet receive the cor-rect answer when parsed and matched against thedatabase.
Such structural and lexical variationbroadens the learning capabilities in contrast tolearning from fixed labeled data.
For example,assume the following English query in the geo-graphical domain, and assume positive feedbackfrom executing the corresponding semantic parseagainst the geographical database:Name prominent elevations in theUSAThe manual translation of the English originalreadsNenne prominente Erhebungen inden USAAn automatic translation2of the German stringproduces the resultGive prominent surveys in the USThis translation will trigger negative task-basedfeedback: A comparison with the original allowsthe error to be traced back to the ambiguity ofthe German word Erhebung.
Choosing a gen-eral domain translation instead of a translation ap-propriate for the geographical domain hinders theconstruction of a semantic parse that returns thecorrect answer from the database.
An alternativetranslation might look as follows:Give prominent heights in the USDespite a large difference to the original En-glish string, key terms such as elevations andheights, or USA and US, can be mapped into thesame predicate in the semantic parse, thus allow-ing to receive positive feedback from parse execu-tion against the geographical database.4 Response-based Online LearningRecent approaches to machine learning for SMTformalize the task of discriminating good frombad translations as a structured prediction prob-lem.
Assume a joint feature representation ?
(x, y)of input sentences x and output translations y ?Y (x), and a linear scoring function s(x, y;w) forpredicting a translation y?
(where ?
?, ??
denotes thestandard vector dot product) s.t.y?
= argmaxy?Y (x)s(x, y;w) = argmaxy?Y (x)?w, ?
(x, y)?
.2http://translate.google.comThe structured perceptron algorithm (Collins,2002) learns an optimal weight vector w by updat-ing w on input x(i)by the following rule, in casethe predicted translation y?
is different from andscored higher than the reference translation y(i):w = w + ?
(x(i), y(i))?
?
(x(i), y?
).This stochastic structural update aims to demoteweights of features corresponding to incorrect de-cisions, and to promote weights of features for cor-rect decisions.An application of structured prediction to SMTinvolves more than a straightforward replacementof labeled output structures by reference transla-tions.
Firstly, update rules that require to com-pute a feature representation for the referencetranslation are suboptimal in SMT, because of-ten human-generated reference translations can-not be generated by the SMT system.
Such ?un-reachable?
gold-standard translations need to bereplaced by ?surrogate?
gold-standard translationsthat are close to the human-generated translationsand still lie within the reach of the SMT sys-tem.
Computation of distance to the referencetranslation usually involves cost functions basedon sentence-level BLEU (Nakov et al (2012), in-ter alia) and incorporates the current model score,leading to various ramp loss objectives describedin Gimpel and Smith (2012).An alternative approach to alleviate the depen-dency on labeled training data is response-basedlearning.
Clarke et al (2010) or Goldwasser andRoth (2013) describe a response-driven learningframework for the area of semantic parsing: Herea meaning representation is ?tried out?
by itera-tively generating system outputs, receiving feed-back from world interaction, and updating themodel parameters.
Applied to SMT, this meansthat we predict translations and use positive re-sponse from acting in the world to create ?surro-gate?
gold-standard translations.
This decreasesthe dependency on a few (mostly only one) refer-ence translations and guides the learner to promotetranslations that perform well with respect to theextrinsic task.In the following, we will present a frameworkthat combines standard structured learning fromgiven reference translations with response-basedlearning from task-approved references.
We needto ensure that gold-standard translations lead topositive task-based feedback, that means they can884be parsed and executed successfully against thedatabase.
In addition, we can use translation-specific cost functions based on sentence-levelBLEU in order to boost similarity of translationsto human reference translations.We denote feedback by a binary execution func-tion e(y) ?
{1, 0} that tests whether executingthe semantic parse for the prediction against thedatabase receives the same answer as the parsefor the gold standard reference.
Our cost functionc(y(i), y) = (1?BLEU(y(i), y)) is based on a ver-sion of sentence-level BLEU Nakov et al (2012).Define y+as a surrogate gold-standard translationthat receives positive feedback, has a high modelscore, and a low cost of predicting y instead ofy(i):y+= argmaxy?Y (x(i)):e(y)=1(s(x(i), y;w)?
c(y(i), y)).The opposite of y+is the translation y?that leadsto negative feedback, has a high model score, anda high cost.
It is defined as follows:y?= argmaxy?Y (x(i)):e(y)=0(s(x(i), y;w) + c(y(i), y)).Update rules can be derived by minimization ofthe following ramp loss objective:minw(?
maxy?Y (x(i)):e(y)=1(s(x(i), y;w)?
c(y(i), y))+ maxy?Y (x(i)):e(y)=0(s(x(i), y;w) + c(y(i), y))).Minimization of this objective using stochastic(sub)gradient descent (McAllester and Keshet,2011) yields the following update rule:w = w + ?
(x(i), y+)?
?
(x(i), y?
).The intuition behind this update rule is to discrim-inate the translation y+that leads to positive feed-back and best approximates (or is identical to) thereference within the means of the model from atranslation y?which is favored by the model butdoes not execute and has high cost.
This is doneby putting all the weight on the former.Algorithm 1 presents pseudo-code for ourresponse-driven learning scenario.
Upon predict-ing translation y?, in case of positive feedback fromthe task, we treat the prediction as surrogate refer-ence by setting y+?
y?, and by adding it to theset of reference translations for future use.
Thenwe need to compute y?, and update by the differ-ence in feature representations of y+and y?, ata learning rate ?.
If the feedback is negative, wewant to move the weights away from the predic-tion, thus we treat it as y?.
To perform an update,we need to compute y+.
If either y+or y?cannotbe computed, the example is skipped.Algorithm 1 Response-based Online Learningrepeatfor i = 1, .
.
.
, n doReceive input string x(i)Predict translation y?Receive task feedback e(y?)
?
{1, 0}if e(y?)
= 1 theny+?
y?Store y?
as reference y(i)for x(i)Compute y?elsey??
y?Receive reference y(i)Compute y+end ifw ?
w + ?(?
(x(i), y+)?
?
(x(i), y?
))end foruntil ConvergenceThe sketched algorithm allows several varia-tions.
In the form depicted above, it allowsto use human reference translations in additionto task-approved surrogate references.
The costfunction can be implemented by different ver-sions of sentence-wise BLEU, or it can be omittedcompletely so that learning relies on task-basedfeedback alone, similar to algorithms recentlysuggested for semantic parsing (Goldwasser andRoth, 2013; Kwiatowski et al, 2013; Berant etal., 2013).
Lastly, regularization can be intro-duced by using update rules corresponding to pri-mal form optimization variants of support vectormachines (Collobert and Bengio, 2004; Chapelle,2007; Shalev-Shwartz et al, 2007).5 Experiments5.1 Experimental SetupIn our experiments, we use the GEOQUERYdatabase on U.S. geography as provided by Jones885method precision recall F1 BLEU1CDEC 63.67 58.21 60.82 46.532EXEC 70.36 63.57 66.79148.0013RAMPION 75.58 69.64 72.491256.64124REBOL 81.15 75.36 78.1512355.6612Table 1: Experimental results using extended parser for returning answers from GEOQUERY (precision,recall, F1) and n-gram match to original English query (BLEU) on 280 re-translated test examples.
Bestresults for each column are highlighted in bold face.
Superscripts1234denote a significant improvementover the respective method.method precision recall F1 BLEU1CDEC 65.59 57.86 61.48 46.532EXEC 66.54 61.79 64.07 46.003RAMPION 67.68 63.57 65.56 55.67124REBOL 70.68 67.14 68.861255.6712Table 2: Experimental results using the original parser for returning answers from GEOQUERY (preci-sion, recall, F1) and n-gram match to original English query (BLEU) on 280 re-translated test examples.et al (2012).3The dataset includes 880 Englishquestions and their logical forms.
The Englishstrings were manually translated into German bythe authors of Jones et al (2012)), and correctedfor typos by the authors of this paper.
We followthe provided split into 600 training examples and280 test examples.For response-based learning, we retrained thesemantic parser of Andreas et al (2013)4on thefull 880 GEOQUERY examples in order to reachfull parse coverage.
This parser is itself based onSMT, trained on parallel data consisting of Englishqueries and linearized logical forms, and on a lan-guage model trained on linearized logical forms.We used the hierarchical phrase-based variant ofthe parser.
Note that we do not use GEOQUERYtest data in SMT training.
Parser training includesGEOQUERY test data in order to be less depen-dent on parse and execution failures in the eval-uation: If a translation system, response-based orreference-based, translates the German input intothe gold standard English query it should be re-warded by positive task feedback.
To double-check whether including the 280 test examplesin parser training gives an unfair advantage toresponse-based learning, we also present experi-mental results using the original parser of Andreas3http://homepages.inf.ed.ac.uk/s1051107/geoquery-2012-08-27.zip4https://github.com/jacobandreas/smt-semparseet al (2013) that is trained only on the 600 GEO-QUERY training examples.The bilingual SMT system used in our experi-ments is the state-of-the-art SCFG decoder CDEC(Dyer et al, 2010)5.
We built grammars us-ing its implementation of the suffix array extrac-tion method described in Lopez (2007).
For lan-guage modeling, we built a modified Kneser-Neysmoothed 5-gram language model using the En-glish side of the training data.
We trained the SMTsystem on the English-German parallel web dataprovided in the COMMON CRAWL6(Smith et al,2013) dataset.5.2 Compared SystemsMethod 1 is the baseline system, consisting ofthe CDEC SMT system trained on the COMMONCRAWL data as described above.
This system doesnot use any GEOQUERY data for training.
Meth-ods 2-4 use the 600 training examples from GEO-QUERY for discriminative training only.Variants of the response-based learning algo-rithm described above are implemented as a stand-alone tool that operates on CDEC n-best lists of10,000 translations of the GEOQUERY trainingdata.
All variants use sparse features of CDEC asdescribed in Simianer et al (2012) that extract rule5https://github.com/redpony/cdec6http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz886prediction: how many inhabitants has new yorkreference: how many people live in new yorkprediction: how big is the population of texasreference: how many people live in texasprediction: which are the cities of the state with the highest elevationreference: what are the cities of the state with the highest pointprediction: how big is the population of states , through which the mississippi runsreference: what are the populations of the states through which the mississippi river runsprediction: what state borders californiareference: what is the adjacent state of californiaprediction: what are the capitals of the states which have cities with the name durhamreference: what is the capital of states that have cities named durhamprediction: what rivers go through states with the least citiesreference: which rivers run through states with fewest citiesTable 3: Predicted translations by response-based learning (REBOL) leading to positive feedback versusgold standard references.shapes, rule identifiers, and bigrams in rule sourceand target directly from grammar rules.
Method4, named REBOL, implements REsponse-BasedOnline Learning by instantiating y+and y?tothe form described in Section 4: In addition tothe model score s, it uses a cost function c basedon sentence-level BLEU (Nakov et al, 2012) andtests translation hypotheses for task-based feed-back using a binary execution function e. Thisalgorithm can convert predicted translations intoreferences by task-feedback, and additionally usethe given original English queries as references.Method 2, named EXEC, relies on task-executionby function e and searches for executable or non-executable translations with highest score s to dis-tinguish positive from negative training examples.It does not use a cost function and thus cannotmake use of the original English queries.We compare response-based learning with astandard structured prediction setup that omits theuse of the execution function e in the definitionof y+and y?.
This algorithm can be seen as astochastic (sub)gradient descent variant of RAM-PION (Gimpel and Smith, 2012).
It does not makeuse of the semantic parser, but defines positive andnegative examples based on score s and cost cwithrespect to human reference translations.We report BLEU (Papineni et al, 2001) oftranslation system output measured against theoriginal English queries.
Furthermore, we reportprecision, recall, and F1-score for executing se-mantic parses built from translation system out-puts against the GEOQUERY database.
Precisionis defined as the percentage of correctly answeredexamples out of those for which a parse could beproduced; recall is defined as the percentage of to-tal examples answered correctly; F1-score is theharmonic mean of both.
Statistical significanceis measured using Approximate Randomization(Noreen, 1989) where result differences with a p-value smaller than 0.05 are considered statisticallysignificant.Methods 2-4 perform structured learning forSMT on the 600 GEOQUERY training examplesand re-translate the 280 unseen GEOQUERY testdata, following the data split of Jones et al (2012).Training for RAMPION, REBOL and EXEC was re-peated for 10 epochs.
The learning rate ?
is set toa constant that is adjusted by cross-validation onthe 600 training examples.5.3 Empirical ResultsWe present an experimental comparison of thefour different systems according to BLEU and887reference RAMPION REBOLhow many colorado rivers aretherehow many rivers with the namecolorado gives ithow many rivers named col-orado are therewhat are the populations ofstates which border texashow big are the populations ofthe states , which in texas bor-dershow big are the populations ofthe states which on texas borderwhat is the biggest capital city inthe uswhat is the largest city in the usa what is the largest capital in theusawhat state borders new york what states limits of new york what states border new yorkwhich states border the statewith the smallest areawhat states boundaries of thestate with the smallest surfaceareawhat states border the state withthe smallest surface areaTable 4: Predicted translations by response-based learning (REBOL) leading to positive feedback versustranslations by supervised structured learning (RAMPION) leading to negative feedback.F1, using an extended semantic parser (trainedon 880 GEOQUERY examples) and the originalparser (trained on 600 GEOQUERY training exam-ples).
The extended parser reaches and F1-scoreof 99.64% on the 280 GEOQUERY test examples;the original parser yields an F1-score of 82.76%.Table 1 reports results for the extended seman-tic parser.
A system ranking according to F1-score shows about 6 points difference between therespective methods, ranking REBOL over RAM-PION, EXEC and CDEC.
The exploitation of task-feedback allows both EXEC and REBOL to im-prove task-performance over the baseline.
RE-BOL?s combination of task feedback with a costfunction achieves the best results since positivelyexecutable hypotheses and reference translationscan both be exploited to guide the learning pro-cess.
Since all English reference queries lead topositively executable parses in the setup that usesthe extended semantic parser, RAMPION implic-itly also has access to task feedback.
This allowsRAMPION to improve F1 over the baseline.
Allresult differences are statistically significant.In terms of BLEU score measured against theoriginal English GEOQUERY queries, the bestnominal result is obtained by RAMPION whichuses them as reference translations.
REBOL per-forms worse since BLEU performance is opti-mized only implicitly in cases where original En-glish queries function as positive examples.
How-ever, the result differences between these twosystems do not score as statistically significant.Despite not optimizing for BLEU performanceagainst references, the fact that positively exe-cutable translations include the references allowseven EXEC to improve BLEU over CDEC whichdoes not use GEOQUERY data at all in training.This result difference is statistically significant.Table 2 compares the same systems using theoriginal parser trained on 600 training examples.The system ranking according to F1-score showsthe same ordering that is obtained when using anextended semantic parser.
However, the respec-tive methods are separated only by 3 or less pointsin F1 score such that only the result difference ofREBOL over the baseline CDEC and over EXEC isstatistically significant.
We conjecture that this isdue to a higher number of empty parses on the testset which makes this comparison unstable.In terms of BLEU measured against the originalqueries, the result differences between REBOL andRAMPION are not statistically significant, and nei-ther are the result differences between EXEC andCDEC.
The result differences between systems ofthe former group and the systems of latter groupare statistically significant.5.4 Error AnalysisFor a better understanding of the differences be-tween the results produced by supervised andresponse-based learning, we conducted an er-888reference RAMPION REBOLhow many states have a higherpoint than the highest point ofthe state with the largest capitalcity in the ushow many states have a highernearby point as the highest pointof the state with the largest capi-tal in the usahow many states have a highpoint than the highest point ofthe state with the largest capitalin the usahow tall is mount mckinley how high is mount mckinley what is mount mckinleywhat is the longest river thatflows through a state that bordersindianahow is the longest river , whichruns through a state , borders theof indianawhat is the longest river whichruns through a state of indianaborderswhat states does the mississippiriver run throughthrough which states runs themississippithrough which states is the mis-sissippiwhich is the highest peak not inalaskahow is the highest peaks of notin alaska iswhat is the highest peak inalaska isTable 5: Predicted translations where supervised structured learning (RAMPION) leads to positive feed-back versus translations by response-based learning (REBOL) leading to negative feedback.ror analysis on the test examples.
Table 3shows examples where the translation predicted byresponse-based learning (REBOL) differs from thegold standard reference translation, but yet leadsto positive feedback via a parse that returns thecorrect answer from the database.
The examplesshow structural and lexical variation that leads todifferences on the string level at equivalent posi-tive feedback from the extrinsic task.
This can ex-plain the success of response-based learning: Lex-ical and structural variants of reference transla-tions can be used to boost model parameters to-wards translations with positive feedback, whilethe same translations might be considered as neg-ative examples in standard structured learning.Table 4 shows examples where translationsfrom REBOL and RAMPION differ from the goldstandard reference, and predictions by REBOLlead to positive feedback, while predictions byRAMPION lead to negative feedback.
Table 5shows examples where translations from RAM-PION outperform translations from REBOL interms of task feedback.
We see that predictionsfrom both systems are in general grammatical.This can be attributed to the use of sentence-level BLEU as cost function in RAMPION andREBOL.
Translation errors of RAMPION can betraced back to mistranslations of key terms (cityversus capital, limits or boundaries versusborder).
Translation errors of REBOL more fre-quently show missing translations of terms.6 ConclusionWe presented a proposal for a new learning andevaluation framework for SMT.
The central ideais to ground meaning transfer in successful in-teraction in an extrinsic task, and use task-basedfeedback for structured learning.
We presented aproof-of-concept experiment that defines the ex-trinsic task as executing semantic parses of trans-lated queries against the GEOQUERY database.Our experiments show an improvement of about6 points in F1-score for response-based learningover structured learning from reference transla-tions.
Our error analysis shows that response-based learning generates grammatical translationswhich is due to the additional use of a cost func-tion that boosts similarity of translations to humanreference translations.In future work, we would like to extend ourwork on embedding SMT in virtual gameplay tolarger and more diverse datasets, and involve hu-man feedback in the response-based learning loop.ReferencesJacob Andreas, Andreas Vlachos, and Stephen Clark.2013.
Semantic parsing as machine translation.
In889Proceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics (ACL?13),Sofia, Bulgaria.Sergio Barrachina, Oliver Bender, Francisco Casacu-berta, Jorge Civera, Elsa Cubel, Shahram Khadivi,Antonio Lagarda, Hermann Ney, Jes?us Tom?as, En-rique Vidal, and Juan-Miguel Vilar.
2008.
Sta-tistical approaches to computer-assisted translation.Computational Linguistics, 35(1):3?28.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on freebase fromquestion-answer pairs.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP?13), Seattle, WA.Qingqing Cai and Alexander Yates.
2013.
Large-scalesemantic parsing via schema matching and lexiconextenstion.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics(ACL?13), Sofia, Bulgaria.Nicol`o Cesa-Bianchi, Gabriele Reverberi, and San-dor Szedmak.
2008.
Online learning algorithmsfor computer-assisted translation.
Technical report,SMART (www.smart-project.eu).Olivier Chapelle.
2007.
Training a support vec-tor machine in the primal.
Neural Computation,19(5):1155?1178.David L. Chen and Raymond J. Mooney.
2011.Learning to interpret natural language navigationinstructions from observations.
In Proceedings ofthe 25th AAAI Conference on Artificial Intelligence(AAAI?11), pages 859?866, San Francisco, CA.James Clarke, Dan Goldwasser, Wing-Wei Chang, andDan Roth.
2010.
Driving semantic parsing from theworld?s response.
In Proceedings of the 14th Con-ference on Natural Language Learning (CoNLL?10),pages 18?27, Uppsala, Sweden.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: theory and experi-ments with perceptron algorithms.
In Proceedingsof the conference on Empirical Methods in Nat-ural Language Processing (EMNLP?02), Philadel-phia, PA.Ronan Collobert and Samy Bengio.
2004.
Links be-tween perceptrons, MLPs, and SVMs.
In Proceed-ings of the 21st International Conference on Ma-chine Learning (ICML?04), Banff, Canada.Michael Denkowski, Chris Dyer, and Alon Lavie.2014.
Learning from post-editing: Online modeladaptation for statistical machine translation.
InProceedings of the 14th Conference of the EuropeanChapter of the Association for Computational Lin-guistics (EACL?14), Gothenburg, Sweden.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Ferhan Ture, Phil Blunsom, Hendra Seti-awan, Vladimir Eidelman, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In Proceedings of the ACL 2010 System Demonstra-tions, Uppsala, Sweden.Masaru Fuji.
1999.
Evaluation experiment for readingcomprehension of machine translation outputs.
InProceedings of the Machine Translation Summit VII,Singapore.Kevin Gimpel and Noah A. Smith.
2012.
Structuredramp loss minimization for machine translation.In Proceedings of 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(NAACL-HLT 2012), Montreal, Canada.Dan Goldwasser and Dan Roth.
2013.
Learning fromnatural instructions.
Machine Learning, 94(2):205?232.Daniel Hardt and Jakob Elming.
2010.
Incrementalre-training for post-editing SMT.
In Proceedings ofthe 9th Conference of the Association for MachineTranlation in the Americas (AMTA?10), Denver, CO.Douglas Jones, Wade Shen, Neil Granoien, MarthaHerzog, and Clifford Weinstein.
2005.
Measuringtranslation quality by testing english speakers witha new defense language proficiency test for arabic.In Proceedings of 2005 International Conference onIntelligence Analysis, McLean, VA.Bevan K. Jones, Mark Johnson, and Sharon Goldwater.2012.
Semantic parsing with bayesion tree trans-ducers.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguistics(ACL?12), Jeju Island, Korea.Philipp Koehn and Barry Haddow.
2009.
Interactiveassistance to human translators using statistical ma-chine translation methods.
In Proceedings of MTSummit XII, Ottawa, Ontario, Canada.Tom Kwiatowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling semantic parsers withon-the-fly ontology matching.
In Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP?13), Seattle, WA.Philippe Langlais, George Foster, and Guy Lapalme.2000.
Transtype: a computer-aided translation typ-ing system.
In Proceedings of the ANLP-NAACL2000 Workshop on Embedded Machine TranslationSystems, Seattle, WA.Adam Lopez.
2007.
Hierarchical phrase-based trans-lation with suffix arrays.
In Proceedings of the JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL 2007), Prague,Czech Republic.Pascual Mart?
?nez-G?omez, Germ?an Sanchis-Trilles, andFrancisco Casacuberta.
2012.
Online adaptation890strategies for statistical machine translation in post-editing scenarios.
Pattern Recognition, 45(9):3193?3202.David McAllester and Joseph Keshet.
2011.
General-ization bounds and consistency for latent structuralprobit and ramp loss.
In Proceedings of the 25th An-nual Conference on Neural Information ProcessingSytems (NIPS 2011), Granada, Spain.Preslav Nakov, Francisco Guzm?an, and Stephan Vogel.2012.
Optimizing for sentence-level bleu+1 yieldsshort translations.
In Proceedings of the 24th Inter-national Conference on Computational Linguistics(COLING 2012), Bombay, India.Vassilina Nikoulina, Bogomil Kovachev, Nikolaos La-gos, and Christof Monz.
2012.
Adaptation of statis-tical machine translation model for cross-lingual in-formation retrieval in a service context.
In Proceed-ings of the 13th Conference of the European Chap-ter of the Association for Computational Linguistics(EACL?12), Avignon, France.Eric W. Noreen.
1989.
Computer Intensive Meth-ods for Testing Hypotheses.
An Introduction.
Wiley,New York.Daniel Ortiz-Mart?
?nez, Ismal Garc?
?a-Varea, and Fran-cisco Casacuberta.
2010.
Online learning for in-teractive statistical machine translation.
In Proceed-ings of the Human Language Technologies confer-ence and the 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (HLT-NAACL?10), Los Angeles,CA.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automaticevaluation of machine translation.
Technical ReportIBM Research Division Technical Report, RC22176(W0190-022), Yorktown Heights, N.Y.Sheila M. Pfafflin.
1965.
Evaluation of machine trans-lations by reading comprehension tests and subjec-tive judgements.
Mechanical Translation and Com-putational Linguistics, 8(2):2?8.Akiko Sakamoto, Nayuko Watanabe, Satoshi Ka-matani, and Kazuo Sumita.
2013.
Development of asimultaneous interpretation system for face-to-faceservices and its evaluation experiment in real situ-ation.
In Proceedings of the Machine TranslationSummit XIV, Nice, France.Avneesh Saluja, Ian Lane, and Ying Zhang.
2012.Machine translation with binary feedback: A large-margin approach.
In Proceedings of the 10th Bi-ennial Conference of the Association for MachineTranslation in the Americas (AMTA?12), San Diego,CA.Shai Shalev-Shwartz, Yoram Singer, and Nathan Sre-bro.
2007.
Pegasos: Primal Estimated sub-GrAdient SOlver for SVM.
In Proceedings of the24th International Conference on Machine Learning(ICML?07), Corvallis, OR.Patrick Simianer, Stefan Riezler, and Chris Dyer.2012.
Joint feature selection in distributed stochas-tic learning for large-scale discriminative training inSMT.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (ACL2012), Jeju, Korea.Jason R. Smith, Herve Saint-Amand, Magdalena Pla-mada, Philipp Koehn, Chris Callison-Burch, andAdam Lopez.
2013.
Dirt cheap web-scale paral-lel text from the common crawl.
In Proceedings ofthe 51st Annual Meeting of the Association for Com-putational Linguistics (ACL?13), Sofia, Bulgaria.Katharina W?aschle, Patrick Simianer, Nicola Bertoldi,Stefan Riezler, and Marcello Federico.
2013.
Gen-erative and discriminative methods for online adap-tation in SMT.
In Proceedings of the MachineTranslation Summit XIV, Nice, France.Yuk Wah Wong and Raymond J. Mooney.
2006.Learning for semantic parsing with statistical ma-chine translation.
In Proceedings of the Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics (HLT/NAACL?06), New York City, NY.Luke S. Zettlemoyer and Michael Collins.
2009.Learning context-dependent mappings from sen-tences to logical form.
In Proceedings of the 47thAnnual Meeting of the Association for Computa-tional Linguistics (ACL-IJCNLP?09), Singapore.891
