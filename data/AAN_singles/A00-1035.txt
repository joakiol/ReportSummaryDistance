Spell ing and Grammar  Correct ion for Danish in SCARRIEPat r i z ia  Pagg ioCenter for SprogteknologiCopenhagen (DK)patrizia@cst, ku.
dkAbst rac tThis paper reports on work carried out to de-velop a spelling and grammar corrector for Dan-ish, addressing in particular the issue of how aform of shallow parsing is combined with er-ror detection and correction for the treatmentof context-dependent spelling errors.
The syn-tactic grammar for Danish used by the systemhas been developed with the aim of dealing withthe most frequent error types found in a parallelcorpus of unedited and proofread texts specif-ically collected by the project's end users.
Byfocussing on certain grammatical constructionsand certain error types, it has been possibleto exploit the linguistic 'intelligence' providedby syntactic parsing and yet keep the systemrobust and efficient.
The system described isthus superior to other existing spelling checkersfor Danish in its ability to deal with context-dependent errors.1 In t roduct ionIn her much-quoted and still relevant review oftechnologies for automatic word correction (Ku-kich, 1992), Kukich observes that "research incontext-dependent spelling correction is in itsinfancy" (p. 429), and that the task of treatingcontext-dependent rrors is still an elusive onedue to the complexity of the linguistic knowl-edge often necessary to analyse the context insufficient depth to find and correct such er-rors.
But progress in parsing technology andthe growing speed of computers eem to havemade the task less of a chimera.
The '90shave in fact seen a renewed interest in gram-mar checking, and proposals have been madefor systems covering English (Bernth, 1997) andother languages uch as Italian (Bolioli et al,1992), Spanish and Greek (Bustamante andLdon, 1996), Czech (Holan et al, 1997) andSwedish (Hein, 1998).This paper describes the prototype of aspelling and grammar corrector for Danishwhich combines traditional spelling checkingfunctionalities with the ability to carry out com-pound analysis and to detect and correct cer-tain types of context-dependent spelling errors(hereafter simply "grammar errors").
Gram-mar correction is carried out by parsing thetext, making use of feature overriding and errorweights to accommodate he errors.
Althougha full parse of each sentence is attempted, thegrammar has been developed with the aim ofdealing only with the most frequent error typesfound in a parallel corpus of unedited and proof-read texts specifically collected by the project'send users.
By focussing on certain grammati-cal constructions and certain error types, it hasbeen possible to exploit the linguistic 'intelli-gence' provided by syntactic parsing and yetkeep the system robust and efficient.
The sys-tem described is thus superior to other existingspelling checkers for Danish in its ability to dealwith certain types of grammar errors.We begin by giving an overview of the sys-tem's components in Section 2.
In Section 3 wedescribe the error types we want to deal with:Section 4 gives an overview of the grammar:in particular, the methods adopted for treatingfeature mismatches and structural errors are ex-plained.
Finally, in Section 5 evaluation resultsare presented and a conclusion is drawn.2 The  proto typeThe prototype is a system for high-qualityproofreading for Danish which has been de-veloped in the context of a collaborative EU-project 1.
Together with the Danish prototype,1Main contractors in the consortium were:WordFinder Software AB (Sweden), Center for255the project has also produced similar systemsfor Swedish and Norwegian, all of them tailoredto meet the specific needs of the Scandinavianpublishing industry.
They all provide writingsupport in the form of word and grammar check-ing.The Danish version of the system 2 constitutesa further development of the CORRie prototype(Vosse, 1992) (Vosse, 1994), adapted to dealwith the Danish language, and to the needs ofthe project's end users.
The system processestext in batch mode and produces an annotatedoutput text where errors are flagged and re-placements suggested where possible.
Text cor-rection is performed in two steps: first the sys-tem deals with spelling errors and typos result-ing in invalid words, and then with grammarerrors.Invalid words are identified on the basis ofdictionary lookup.
The dictionary presentlyconsists of 251,000 domain-relevant word formsextracted from a collection of 68,000 newspa-per articles.
A separate idiom list allowing forthe identification of multi-word expressions isalso available.
Among the words not found inthe dictionary or the idiom list, those occurringmost frequently in the text (where frequency isassessed relative to the length of the text) aretaken to be new words or proper names 3.
Theremaining unknown words are passed on to thecompound analysis grammar, which is a set ofregular expressions covering the most commontypes of compound nominals in Danish.
This isan important feature, as in Danish compound-ing is very productive, and compounds are writ-ten as single words.Words still unknown at this point are takento be spelling errors.
The System flags them asSprogteknologi (Denmark), Department of Linguisticsat Uppsala University (Sweden), Institutt for lingvistikkog litteraturvitenskab at the University of Bergen (Nor-way), and Svenska Dagbladet (Sweden).
A number ofsubcontractors also contributed to the project.
Subcon-tractors in Denmark were: Munksgaard InternationalPublishers, Berlingske Tidende, Det Danske Sprog- ogLitteraturselskab, and Institut for Almen og AnvendtSprogvidenskab t the University of Copenhagen.2In addition to the author of the present paper,tlle Danish SCARRIE team at CST consisted of ClausPovlsen, Bart Kongejan and Bradley Music.3The system also checks whether a closely matchingalternative can be found in the dictionary, to avoid mis-taking a consistently misspelt word for a new word.such and tries to suggest a replacement.
Thealgorithm used is based on trigram and tri-phone analysis (van Berkel and Smedt, 1988),and takes into account the orthographic stringscorresponding to the invalid word under con-sideration and its possible replacement, as wellas the phonetic representations of the same twowords.
Phonetic representations are generatedby a set of grapheme-to-phoneme rules (Hansen,1999) the aim of which is to assign phoneticallymotivated misspellings and their correct coun-terparts identical or similar phonetic represen-tations.Then the system tries to identify context-dependent spelling errors.
This is done by pars-ing the text.
Parsing results are passed on toa corrector to find replacements for the errorsfound.
The parser is an implementation of theTomita algorithm with a component for errorrecognition whose job is to keep track of errorweights and feature mismatches as described in(Vosse, 1991).
Each input sentence is assignedthe analysis with the lowest error weight.
If theerror is due to a feature mismatch, the offendingfeature is overridden, and if a dictionary entrysatisfying the grammar constraints expressed bythe context is found in the dictionary, it is of-fered as a replacement.
If the structure is in-complete, on the other hand, an error messageis generated.
Finally, if the system identifies anerror as a split-up or a run-on, it will suggesteither a possible concatenation, or a sequenceof valid words into which the misspelt word canbe split up.3 The  er rorsTo ensure the coverage of relevant error types,a set of parallel unedited and proofread textsprovided by the Danish end users has been col-lected.
This text collection consists of newspa-per and magazine articles published in 1997 fora total of 270,805 running words.
The articleshave been collected in their raw version, as wellas in the edited version provided by the pub-lisher's own proofreaders.
Although not verylarge in number of words, th@ corpus consistsof excerpts from 450 different articles to en-sure a good spread of lexical domains and errortypes.
The corpus has been used to constructtest suites for progress evaluation, and also toguide grammar development.
The aim set for256Error type No.Context independent errors 386Context dependent errors 308Punctuation problems 212Style problems 89Graphical problems 24Total 1019%38302192100Figure 1: Error distribution in the Danish cor-pusgrammar development was then to enable thesystem to identify and analyse the grammati-cal constructions in which errors typically occur,whilst to some extent disregarding the remain-der of the text.The errors occurring in the corlbus have beenanalysed according to the taxonomy in (Ram-bell, 1997).
Figure 1 shows the distribution ofthe various error types into the five top-levelcategories of the taxonomy.
As can be seen,grammar errors account for 30~0 of the errors.Of these, 70% fall into one of the following cat-egories (Povlsen, 1998):?
Too many finite verbal forms or missing fi-nite verb?
Errors in nominal phrases:- agreement errors,- wrong determination,- genitive errors,- errors concerning pronouns;?
Split-ups and run-ons.Another way of grouping the errors is by thekind of parsing failure they generate: they canthen be viewed as either feature mismatches,or as structural errors.
Agreement errors aretypical examples of feature mismatches.
In thefollowing nominal phrase, for example:(1) de *interessant projekter(the interesting projects)_the error can be formalised as a mismatch be-tween the definiteness of the determiner de (the)and the indefiniteness of the adjective interes-sant (interesting).
Adjectives have in fact bothan indefinite and a definite form in Danish.The sentence below, on the other hand, is anexample of structural error.
(2) i sin tid *skabet han skulpturer overatomkraften(during his time wardrobe/created hesculptures about nuclear power)Since the finite verb skabte (created) has beenmisspelt as skabet (the wardrobe), the syntacticstructure corresponding to the sentence is miss-ing a verbal head.Run-ons and split-ups are structural errors ofa particular kind, having to do with leaves in thesyntactic tree.
In some cases they can only bedetected on the basis of the context, because themisspelt word has the wrong category or bearssome other grammatical feature that is incorrectin the context.
Examples are given in (3) and(4) below, which like the preceding examples aretaken from the project's corpus.
In both cases,the error would be a valid word in a differentcontext.
More specifically, rigtignok (indeed) isan adverb, whilst rigtig nok (actually correct) isa modified adjective; and inden .for (inside) is apreposition, whilst indenfor (indoors) is an ad-verb.
In both examples the correct alternativeis indicated in parentheses.
(3) ... studerede rain gruppe *rigtig nok(rigtignok) under temaoverskrifter(studied my group indeed on the basisof topic headings)(4) *indenfor (inden for) de gule mute(inside the yellow walls)Although the system has a facility for identi-fying and correcting split-ups and run-ons basedon a complex interaction between the dictio-nary, the idiom list, the compound grammarand the syntactic grammar, this facility has notbeen fully developed yet, and will therefore notbe described any further here.
More details canbe found in (Paggio, 1999).4 The  grammarThe grammar is an augmented context-freegrammar consisting of rewrite rules where sym-bols are associated with features.
Error weightsand error messages can also be attached to ei-ther rules or single features.
The rules are ap-plied by unification, but in cases where one ormore features do not unify, the offending fea-tures will be overridden.257In the current version of the grammar~ onlythe structures relevant to the error types wewant the system to deal with - in other wordsnominal phrases and verbal groups - are ac-counted for in detail.
The analysis produced isthus a kind of shallow syntactic analysis wherethe various sentence constituents are attachedunder the topmost S node as fragments.For example, adjective phrases can be anal-ysed as fragments, as shown in the followingrule:Fragment ->AP "?Fragment AP rule":2To indicate that the fragment analysis is notoptimal, it is associated with an error weight,as well as an error message to be used for de-bugging purposes (the message is not visible tothe end user).
The weight penalises parse treesbuilt by applying the rule.
The rule is used e.g.to analyse an AP following a copula verb as in:(5) De projekter er ikke interessante.
(Those projects are not interesting)The main motivation for implementing agrammar based on the idea of fragments wasefficiency.
Furthermore, the fragment strategycould be implemented very quickly.
However,as will be clarified in Section 5, this strategy issometimes responsible for bad flags.4.1 Feature  mismatchesAs an alternative to the fragment analysis, APscan be attached as daughters in NPs.
This is ofcourse necessary for the treatment of agreementin NPs, one of the error types targeted in ourapplication.
This is shown in the following rule:NP(def Gender PersNumber) ->Det (def Gender PersNumber)AP(def _ _)N(indef Gender:9- PersNumber)The rule will parse a correct definite NP suchas :(6)but also(7)(S)de interessante projekter(the interesting projects)de *interessant projekterde interessante *projekterneThe  feature overriding mechanism makes itpossible for the system to suggest interessanteas the correct replacement in (7), and projekterin (8).
Let us see how this is done in more de-tail for example (7).
The parser tries to applythe NP rule to the input string.
The rule statesthat the adjective phrase must be definite (AP(def _ _)).
But the dictionary entry correspond-ing to interessant bears the feature 'indef'.
Theparser will override this feature and build anNP according to the constraints expressed bythe rule.
At this point, a new dictionary lookupis performed, and the definite form of the ad-jective can be suggested as a replacement.Weights are used to control rule interactionas well as to establish priorities among featuresthat may have to be overridden.
For examplein our NP rule, a weight has been attached tothe Gender feature in the N node.
The weightexpresses the fact that it costs more to over-ride gender on the head noun than on the de-terminer or adjective.
The rationale behind thisis the fact that if there is a gender mismatch,the parser should not try to find an alternative?
form of the noun (which does not exist), but ifnecessary override the gender feature either onthe adjective or the determiner.4.2.
Captur ing  s t ruc tura l  e r ro rs  ingrammar  ru lesTo capture structural errors, the formalism al-lows the grammar writer to write so-called errorrules.
The syntax of error rules is very similarto that used in 'normal' rules, the only differ-ence being that an error rule must have an er-?
ror weight and an error message attached to it.The purpose of the weight is to ensure that er-ror rules are applied only if 'normal' rules arenot applicable.
The error message can serve twopurposes.
Depending on whether it is stated asan implicit or an explicit message (i.e.
whetherit is preceded by a question mark or not), it willappear in the log file where it can be used fordebugging purposes, or in the output text as amessage to the end user.The following is an error rule example.VGroup(_ finite Tense) ->V(_ finite:4 Tense)V(_ finite:4 _)"Sequence of two finite verbs":4, ' j r -  O 258Error type Total Flagged Hits Misses No suggsErrors in single words 81 69 (85.2%) 35 (50.8%) 11 (15.9%) 23 (33.3%)Errors in compounds 42 28 (66.7%) 8 (28.6%) 8 (28.6%) 12 (42.8%)NP agreement errors 18 15 (83.3%) 15 (100.0%) 0 (0.0%) 0 (0.0%)VP errors 13 8 (61.6%) 8 (100.0%) 0 (0 .0%)0  (0.0%)Total 154 120 (78.0%)66 (55.0%) 19 (15.8%) 35 (29.2%)Figure 2: Error coverage and suggestion adequacy evaluated on test suitesA weight of 4 is attached to the rule as awhole, but there are also weights attached to the'finiteness' feature on the daughters: their func-tion is to make it costly for the system to applythe rule to non-finite forms.
In other words, thefeature specification 'finite' is made difficult tooverride to ensure that it is indeed a sequence offinite verbal forms the rule applies to and flags.The rule will for example parse the verbal se-quence in the following sentence:(9) Jeg vil *bevarer (berate) rain frihed.
(*I want keep my freedom)As a result of parsing, the system in this casewill not attempt to correct the wrong verbalform, but issue the error message "Sequence oftwo finite verbs".Error rules can thus be used to explicitlydescribe an error and to issue error messages.However, so far we have made very limited useof them, as controlling their interaction with'normal' rules and with the feature overridingmechanism is not entirely easy.
In fact, they areconsistently used only to identify incorrect se-quences of finite verbal forms or sentences miss-ing a finite verb.
To this sparse use of error rulescorresponds, on the other hand, an extensive x-ploitation of the feature overriding mechanism.This strategy allows us to keep the number ofrules in the  grammar elatively low, but relieson a careful manual adjustment of the weightsattached to the various features in the rules.5 Eva luat ion  and  Conc lus ionThe project's access to a set of parallel uneditedand proofread texts has made it possible to au-tomate the evaluation of the system's linguis-tic functionality.
A tool has been implementedto compare the results obtained by the sys-tem with the corrections suggested by the pub-lisher's human proofreaders in order to derivemeasures telling us how well the system per-formed on recall (lexical coverage as well as cov-erage of errors), precision (percentage of cor-rect flaggings), as well as suggestion adequacy(hits, misses and no suggestions offered).
Thereader is referred to (Paggio and Music, 1998)for more details on the evaluation methodology.The automatic procedure was used to evaluatethe system during development, and in connec-tion with the user validation.
Testing was doneon constructed test suites displaying examplesof the errors targeted in the project and withtext excerpts from the parallel corpora~Figure 2 shows error recall and suggestion ad-equacy figures for the various error types repre-sented in the test suites.
These figures are verypositive, especially with regard to the treatmentof grammar errors.
To make a comparison witha commercial product, the Danish version of thespelling and grammar checker provided by Mi-crosoft Word does not flag any of the grammarerrors.Figure 3 shows how the system performed onone of the test corpora.
The corpus was assem-bled by mixing short excerpts containing rele-vant grammar errors and randomly chosen text.Since unlike test suites, the corpus also containscorrect ext, the figure this time also shows lex-ical coverage and precision figures.
The corpusconsists of 278 sentences, with an average lengthof 15.18 words per sentence.
It may be sur-prising to see that it contains a limited numberof errors, but it must be remembered that thetexts targeted in the project are written by ex-perienced journalists.The corpus was processed in 58 cpu-secondson an HP 9000/B160.
As expected, the systemperforms less well than on the test suites, and ingeneral precision is clearly too low.
However, westill consider these results encouraging given therelatively small resources the project has beenable to spend on grammar development, and we259Recall4220 total words4157 valid words3996 (96.1%) valid words accepted161 (3.9%) valid words rejected63 invalid words (real errors)36 (57.1%) real errors potted (good flags)27 (42.9%) real errors missedPrecision175 flaggings36 (20.6%) good flags139 (79.4%) bad flags (false positives)Suggestion adequacy36 good flags15 (41.7%) hits (initial suggestion correct )8 (22.2%) misses (suggestions offered, none correct)13 (36.1%) with no suggestions offeredFigure 3: Test corpus evaluationbelieve they can be improved.We regard error coverage as quite satisfac-tory for a research prototype.
In a comparativetest made on a similar (slightly smaller) corpus,SCARR/E obtained 58.1% error coverage, andWord 53.5%.
To quote a figure from anotherrecently published test (Martins et al, 1998),the ReGra system is reported to miss 48.1%real errors.
It is worth noting that ReGra hasmuch more extensive linguistic resources avail-able than SCARRIE, i.e.
a dictionary of 1.5million words and a grammar of 600 productionrules.
Most of the errors not found by SCAR-RIE in the test have to do with punctuationand other stylistic matters not treated in theproject.
There are also, however, agreement er-rors which go unnoticed.
These failures are dueto one of two reasons: either that no parse hasbeen produced for the sentence in question, orthat the grammar has produced a wrong analy-sis.The precision obtained is at least at first sightmuch too low.
On the same test corpus, how-ever, Word only reached 15.9% precision.
Oncloser inspection, 72 of the bad flags producedby SCARRIE turned out to be due to unrecog-nised proper names.
Disregarding those, preci-sion goes up to 34.9%.
As was mentioned early,SCARRIE has a facility for guessing unknownproper names on the basis of their frequencyof occurrence in the text.
But since the testcorpus consists of Short unrelated excerpts, alarge number of proper names only occur onceor twice.
To get an impression of how the sys-tem would perform in a situation where thesame proper names and unknown words had ahigher frequency of occurrence, we doubled thetest corpus by simply repeating the same texttwice.
As expected, precision increased.
Thesystem produced 178 flags, 60 of which werecorrect (39.7%).
This compares well with the40% precision reported for instance for ReGra.In addition to the problem of unkown propernames, false flags are related to unrecognisedacronyms and compounds (typically forms con-taining acronyms or dashes), and a not very pre-cise treatment of capitalisation.
Only 13 falseflags are due to wrong grammar analyses causedeither by the fragment approach or by the gram-mar's limited coverage.
In particular, genitivephrases, which are not treated at the moment,are responsible for most of these false alarms.In conclusion, we consider the results ob-tained so far promising, and the problems re-vealed by the evaluation tractable within thecurrent system design.
In particular, futuredevelopment should focus on treating stylisticmatters uch as capitalisation and punctuationwhich have not been in focus in the currentprototype.
The coverage of the grammar, inparticular the treatment of genitive phrases,should also be further developed.
The data pro-260vided by the evaluation reported on in this pa-per, however, are much too limited to base fur-ther development on.
Therefore, more exten-sive testing and debugging should also be car-ried out.In addition, two aspects of the system thathave only be touched on in this paper wouldbe worth further attention: one is the mecha-nism for the treatment of split-ups and run-ons,which as mentioned earlier is not well-integratedat the moment; the other is the weight adjust-ment process, which is done manually at themoment, and for which the adoption of a semi-automatic tool could be considered.Re ferencesA.
Bernth.
1997.
EasyEnglish: a tool for im-proving document quality.
In Proceedings of?
the Fifth Conference on Applied Natural Lan-guage Processing.A.
Bolioli, L. Dini, and G. Malnati.
1992.JDII: Parsing Italian with a robust constraintgrammar.
In Proceedings of COLING:92,pages 1003-1007.Flora Ram~rez Bustamante and Fer-nando S?nchez L@on.
1996.
GramCheck: Agrammar and style checker.
In Proceedingsof COLING-96, pages 175-181, Copenhagen,Denmark.Peter Molb~ek Hansen.
1999.
Grapheme-to-phoneme rules for the Danish componentof the SCARRIE project.
In Hanne E.Thomsen and Sabine'Kirchmeier-Andersen,editors, Datalingvistisk Forenings drsmcde1998 i Kcbehavn, Proceedings, number 25 inLAMBDA, pages 79-91.
Institut for datal-ingvistik, Handelshcjskolen i Kcbenhaven.Anna S?gvall Hein.
1998.
A chart-based frame-work for grammar checking: Initial studies.In Proceedings of Nodalida-98.Tom~ Holan, Vladislav Kubofi, and Mar-tin Pl?tek.
1997.
A prototype of a gram-mar checker for Czech.
In Proceedings ofANLP'97.Karen Kukich.
1992.
Techniques for automati-cally correcting words in text.
A CM Comput-_ ing Surveys, 24(4):377-439.Ronaldo Teixeira Martins, Ricardo Hasegawa,Maria Volpe Nunes, Gisele Monthila, and Os-valdo Novais De Oliveira Jr. 1998.
Linguisticissues in the development ofReGra: a gram-mar Checker for Brazilian Portuguese.
Natu-ral Language Engineering, 4(4):287-307, De-cember.Patrizia' Paggio and Bradley Music.
1998.
Eval-uation in the SCARRIE project.
In Pro-ceedings of the First International Conferenceon Language Resources ~ Evaluation, pages277-282.
Granada, Spain.Patrizia Paggio.
1999.
Treatment of grammat-ical errors and evaluation in SCARRIE.
InHanne E. Thomsen and Sabine Kirchmeier-Andersen, editors, Datalingvistisk ForeningsdrsmCde 1998 i KCbehavn, Proceedings, num-ber 25 in LAMBDA, pages 65-78.
Insti-tut for datalingvistik, Handelshcjskolen iKcbenhaven.Claus Povlsen.
1998.
Three types of gram-matical errors in Danish.
Technical report,Copenhagen: Center for Sprogteknologi.Olga Rambell.
1997.
Error typology for auto-matic proof-reading purposes.
Technical re-port, Uppsala: Uppsala University.
:Brigitte van Berkel and Koenra~d De Smedt.1988.
Triphone analysis: a combined methodfor the correction of orthographical nd ty-pographical errors.
In Proceedings of the 2ndconference on Applied Natural Language Pro-cessing, pages 77-83.
ACL, Austin.Theo  Vosse.
1991.
Detection and correctionof morpho-syntactic errors in shift-reduceparsing.
In R. Heemels, A. Nijholt, andK.
Sikkel, editors, Tomita's Algorithm: Ex-tensions and Applications, number 91-68 inMemoranda Informatica, pages 69-78.
Uni-versity of Twente/Theo Vosse.
1992.
Detecting and correctingmorpho-syntactic errors in real texts.
In Pro-ceedings of the Third Conference on AppliedNatural Language Processing, pages 111-118,Trento, Italy.Theo G. Vosse.
1994.
The Word Connection -Grammar-based Spelling Error Correction inDutch.
Ph.D. thesis, Rijksuniversiteit a Lei-den: the Netherlands.
ISBN 90-75296-01-0.261
