Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 883?891,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsExploiting Social Information in Grounded Language Learning viaGrammatical ReductionsMark JohnsonDepartment of ComputingMacquarie UniversitySydney, AustraliaMark.Johnson@MQ.edu.auKatherine DemuthDepartment of LinguisticsMacquarie UniversitySydney, AustraliaKatherine.Demuth@MQ.edu.auMichael FrankDepartment of PsychologyStanford UniversityStanford, Californiamcfrank@Stanford.eduAbstractThis paper uses an unsupervised model ofgrounded language acquisition to study therole that social cues play in language acqui-sition.
The input to the model consists of (or-thographically transcribed) child-directed ut-terances accompanied by the set of objectspresent in the non-linguistic context.
Eachobject is annotated by social cues, indicatinge.g., whether the caregiver is looking at ortouching the object.
We show how to modelthe task of inferring which objects are be-ing talked about (and which words refer towhich objects) as standard grammatical in-ference, and describe PCFG-based unigrammodels and adaptor grammar-based colloca-tion models for the task.
Exploiting socialcues improves the performance of all mod-els.
Our models learn the relative importanceof each social cue jointly with word-objectmappings and collocation structure, consis-tent with the idea that children could discoverthe importance of particular social informa-tion sources during word learning.1 IntroductionFrom learning sounds to learning the meanings ofwords, social interactions are extremely importantfor children?s early language acquisition (Baldwin,1993; Kuhl et al, 2003).
For example, children whoengage in more joint attention (e.g.
looking at par-ticular objects together) with caregivers tend to learnwords faster (Carpenter et al, 1998).
Yet compu-tational or formal models of social interaction arerare, and those that exist have rarely gone beyondthe stage of cue-weighting models.
In order to studythe role that social cues play in language acquisition,this paper presents a structured statistical model ofgrounded learning that learns a mapping betweenwords and objects from a corpus of child-directedutterances in a completely unsupervised fashion.
Itexploits five different social cues, which indicatewhich object (if any) the child is looking at, whichobject the child is touching, etc.
Our models learnthe salience of each social cue in establishing refer-ence, relative to their co-occurrence with objects thatare not being referred to.
Thus, this work is consis-tent with a view of language acquisition in whichchildren learn to learn, discovering organizing prin-ciples for how language is organized and used so-cially (Baldwin, 1993; Hollich et al, 2000; Smith etal., 2002).We reduce the grounded learning task to a gram-matical inference problem (Johnson et al, 2010;Bo?rschinger et al, 2011).
The strings presented toour grammatical learner contain a prefix which en-codes the objects and their social cues for each ut-terance, and the rules of the grammar encode rela-tionships between these objects and specific words.These rules permit every object to map to everyword (including function words; i.e., there is no?stop word?
list), and the learning process decideswhich of these rules will have a non-trivial proba-bility (these encode the object-word mappings thesystem has learned).This reduction of grounded learning to grammat-ical inference allows us to use standard grammati-cal inference procedures to learn our models.
Herewe use the adaptor grammar package described inJohnson et al (2007) and Johnson and Goldwater(2009) with ?out of the box?
default settings; noparameter tuning whatsoever was done.
Adaptorgrammars are a framework for specifying hierarchi-cal non-parametric models that has been previouslyused to model language acquisition (Johnson, 2008).883Social cue Valuechild.eyes objects child is looking atchild.hands objects child is touchingmom.eyes objects care-giver is looking atmom.hands objects care-giver is touchingmom.point objects care-giver is pointing toFigure 1: The 5 social cues in the Frank et al (to appear)corpus.
The value of a social cue for an utterance is asubset of the available topics (i.e., the objects in the non-linguistic context) of that utterance.A semanticist might argue that our view of refer-ential mapping is flawed: full noun phrases (e.g., thedog), rather than nouns, refer to specific objects, andnouns denote properties (e.g., dog denotes the prop-erty of being a dog).
Learning that a noun, e.g., dog,is part of a phrase used to refer to a specific dog (say,Fido) does not suffice to determine the noun?s mean-ing: the noun could denote a specific breed of dog,or animals in general.
But learning word-object rela-tionships is a plausible first step for any learner: it isoften only the contrast between learned relationshipsand novel relationships that allows children to in-duce super- or sub-ordinate mappings (Clark, 1987).Nevertheless, in deference to such objections, wecall the object that a phrase containing a given nounrefers to the topic of that noun.
(This is also appro-priate, given that our models are specialisations oftopic models).Our models are intended as an ?ideal learner?
ap-proach to early social language learning, attempt-ing to weight the importance of social and structuralfactors in the acquisition of word-object correspon-dences.
From this perspective, the primary goal isto investigate the relationships between acquisitiontasks (Johnson, 2008; Johnson et al, 2010), lookingfor synergies (areas of acquisition where attemptingtwo learning tasks jointly can provide gains in both)as well as areas where information overlaps.1.1 A training corpus for social cuesOur work here uses a corpus of child-directedspeech annotated with social cues, described inFrank et al (to appear).
The corpus consistsof 4,763 orthographically-transcribed utterances ofcaregivers to their pre-linguistic children (ages 6, 12,and 18 months) during home visits where childrenplayed with a consistent set of toys.
The sessionswere video-taped, and each utterance was annotatedwith the five social cues described in Figure 1.Each utterance in the corpus contains the follow-ing information:?
the sequence of orthographic words uttered bythe care-giver,?
a set of available topics (i.e., objects in the non-linguistic objects),?
the values of the social cues, and?
a set of intended topics, which the care-giverrefers to.Figure 2 presents this information for an example ut-terance.
All of these but the intended topics are pro-vided to our learning algorithms; the intended top-ics are used to evaluate the output produced by ourlearners.Generally the intended topics consist of zero orone elements from the available topics, but not al-ways: it is possible for the caregiver to refer to twoobjects in a single utterance, or to refer to an objectnot in the current non-linguistic context (e.g., to atoy that has been put away).
There is a considerableamount of anaphora in this corpus, which our mod-els currently ignore.Frank et al (to appear) give extensive details onthe corpus, including inter-annotator reliability in-formation for all annotations, and provide detailedstatistical analyses of the relationships between thevarious social cues, the available topics and the in-tended topics.
That paper also gives instructions onobtaining the corpus.1.2 Previous workThere is a growing body of work on the role of socialcues in language acquisition.
The language acqui-sition research community has long recognized theimportance of social cues for child language acqui-sition (Baldwin, 1991; Carpenter et al, 1998; Kuhlet al, 2003).Siskind (1996) describes one of the first exam-ples of a model that learns the relationship betweenwords and topics, albeit in a non-statistical frame-work.
Yu and Ballard (2007) describe an associativelearner that associates words with topics and thatexploits prosodic as well as social cues.
The rela-tive importance of the various social cues are spec-ified a priori in their model (rather than learned, asthey are here), and unfortunately their training cor-pus is not available.
Frank et al (2008) describes aBayesian model that learns the relationship betweenwords and topics, but the version of their model thatincluded social cues presented a number of chal-lenges for inference.
The unigram model we de-scribe below corresponds most closely to the Frank884.dog # .pig child.eyes mom.eyes mom.hands # ## wheres the piggieFigure 2: The photograph indicates non-linguistic context containing a (toy) pig and dog for the utterance Where?s thepiggie?.
Below that, we show the representation of this utterance that serves as the input to our models.
The prefix (theportion of the string before the ?##?)
lists the available topics (i.e., the objects in the non-linguistic context) and theirassociated social cues (the cues for the pig are child.eyes, mom.eyes and mom.hands, while the dog is not associatedwith any social cues).
The intended topic is the pig.
The learner?s goals are to identify the utterance?s intended topic,and which words in the utterance are associated with which topic.SentenceTopic.pigT.None.dogNotTopical.child.eyesNotTopical.child.handsNotTopical.mom.eyesNotTopical.mom.handsNotTopical.mom.point#Topic.pigT.pig.pigTopical.child.eyeschild.eyesTopical.child.handsTopical.mom.eyesTopical.mom.handsmom.handsTopical.mom.point#Topic.None##Words.pigWord.NonewheresWords.pigWord.NonetheWords.pigWord.pigpiggieFigure 3: Sample parse generated by the Unigram PCFG.
Nodes coloured red show how the ?pig?
topic is propagatedfrom the prefix (before the ?##?
separator) into the utterance.
The social cues associated with each object are generatedeither from a ?Topical?
or a ?NotTopical?
nonterminal, depending on whether the corresponding object is topical ornot.885et al model.
Johnson et al (2010) reduces groundedlearning to grammatical inference for adaptor gram-mars and shows how it can be used to perform wordsegmentation as well as learning word-topic rela-tionships, but their model does not take social cuesinto account.2 Reducing grounded learning with socialcues to grammatical inferenceThis section explains how we reduce ground learn-ing problems with social cues to grammatical in-ference problems, which lets us apply a wide vari-ety of grammatical inference algorithms to groundedlearning problems.
An advantage of reducinggrounded learning to grammatical inference is thatit suggests new ways to generalise grounded learn-ing models; we explore three such generalisationshere.
The main challenge in this reduction is findinga way of expressing the non-linguistic informationas part of the strings that serve as the grammatical in-ference procedure?s input.
Here we encode the non-linguistic information in a ?prefix?
to each utteranceas shown in Figure 2, and devise a grammar suchthat inference for the grammar corresponds to learn-ing the word-topic relationships and the salience ofthe social cues for grounded learning.All our models associate each utterance with zeroor one topics (this means we cannot correctly anal-yse utterances with more than one intended topic).We analyse an utterance associated with zero topicsas having the special topic None, so we can assumethat every utterance has exactly one topic.
All ourgrammars generate strings of the form shown in Fig-ure 2, and they do so by parsing the prefix and thewords of the utterance separately; the top-level rulesof the grammar force the same topic to be associatedwith both the prefix and the words of the utterance(see Figure 3).2.1 Topic models and the unigram PCFGAs Johnson et al (2010) observe, this kind ofgrounded learning can be viewed as a specialisedkind of topic inference in a topic model, where theutterance topic is constrained by the available ob-jects (possible topics).
We exploit this observationhere using a reduction based on the reduction ofLDA topic models to PCFGs proposed by Johnson(2010).
This leads to our first model, the unigramgrammar, which is a PCFG.11In fact, the unigram grammar is equivalent to a HMM,but the PCFG parameterisation makes clear the relationshipSentence?
Topict Wordst ?t ?
T?TopicNone ?
##Topict ?
Tt TopicNone ?t ?
T?Topict ?
TNone Topict ?t ?
TTt ?
t Topicalc1 ?t ?
TTopicalci ?
(ci) Topicalci+1 i = 1, .
.
.
, `?
1Topicalc` ?
(c`) #TNone ?
t NotTopicalc1 ?t ?
TNotTopicalci ?
(ci) NotTopicalci+1 i = 1, .
.
.
, `?
1NotTopicalc` ?
(c`) #Wordst ?WordNone (Wordst) ?t ?
T ?Wordst ?Wordt (Wordst) ?t ?
TWordt ?
w ?t ?
T ?, w ?WFigure 4: The rule schema that generate the unigramPCFG.
Here (c1, .
.
.
, c`) is an ordered list of the so-cial cues, T is the set of all non-None available topics,T ?
= T ?
{None}, and W is the set of words appearingin the utterances.
Parentheses indicate optionality.Figure 4 presents the rules of the unigram gram-mar.
This grammar has two major parts.
The rulesexpanding the Topict nonterminals ensure that thesocial cues for the available topic t are parsed un-der the Topical nonterminals.
All other availabletopics are parsed under TNone nonterminals, so theirsocial cues are parsed under NotTopical nontermi-nals.
The rules expanding these non-terminals arespecifically designed so that the generation of the so-cial cues corresponds to a series of binary decisionsabout each social cue.
For example, the probabilityof the ruleTopicalchild.eyes ?
.child.eyes Topicalchild.handsis the probability of an object that is an utterancetopic occuring with the child.eyes social cue.
By es-timating the probabilities of these rules, the modeleffectively learns the probability of each social cuebeing associated with a Topical or a NotTopicalavailable topic, respectively.The nonterminals Wordst expand to a sequenceof Wordt and WordNone nonterminals, each ofwhich can expand to any word whatsoever.
In prac-tice Wordt will expand to those words most stronglyassociated with topic t, while WordNone will expandto those words not associated with any topic.between grounded learning and estimation of grammar ruleweights.886Sentence?
Topict Collocst ?t ?
T?Collocst ?
Colloct (Collocst) ?t ?
T ?Collocst ?
CollocNone (Collocst) ?t ?
TColloct ?Wordst ?t ?
T ?Wordst ?Wordt (Wordst) ?t ?
T ?Wordst ?WordNone (Wordst) ?t ?
TWordt ?Word ?t ?
T ?Word?
w ?w ?WFigure 5: The rule schema that generate the collocationadaptor grammar.
Adapted nonterminals are indicated viaunderlining.
Here T is the set of all non-None availabletopics, T ?
= T ?
{None}, and W is the set of words ap-pearing in the utterances.
The rules expanding the Topictnonterminals are exactly as in unigram PCFG.2.2 Adaptor grammarsOur other grounded learning models are based onreductions of grounded learning to adaptor gram-mar inference problems.
Adaptor grammars are aframework for stating a variety of Bayesian non-parametric models defined in terms of a hierarchy ofPitman-Yor Processes: see Johnson et al (2007) fora formal description.
Informally, an adaptor gram-mar is specified by a set of rules just as in a PCFG,plus a set of adapted nonterminals.
The set oftrees generated by an adaptor grammar is the sameas the set of trees generated by a PCFG with thesame rules, but the generative process differs.
Non-adapted nonterminals in an adaptor grammar expandjust as they do in a PCFG: the probability of choos-ing a rule is specified by its probability.
However,the expansion of an adapted nonterminal depends onhow it expanded in previous derivations.
An adaptednonterminal can directly expand to a subtree withprobability proportional to the number of times thatsubtree has been previously generated; it can also?back off?
to expand using a grammar rule, just asin a PCFG, with probability proportional to a con-stant.2Thus an adaptor grammar can be viewed ascaching each tree generated by each adapted non-terminal, and regenerating it with probability pro-portional to the number of times it was previouslygenerated (with some probability mass reserved togenerate ?new?
trees).
This enables adaptor gram-2This is a description of Chinese Restaurant Processes,which are the predictive distributions for Dirichlet Processes.Our adaptor grammars are actually based on the more generalPitman-Yor Processes, as described in Johnson and Goldwater(2009).SentenceTopic.pig...Collocs.pigColloc.NoneWords.NoneWord.NoneWordwheresCollocs.pigColloc.pigWords.pigWord.NoneWordtheWords.pigWord.pigWordpiggieFigure 6: Sample parse generated by the collocationadaptor grammar.
The adapted nonterminals Colloct andWordt are shown underlined; the subtrees they dominateare ?cached?
by the adaptor grammar.
The prefix (notshown here) is parsed exactly as in the Unigram PCFG.mars to generalise over subtrees of arbitrary size.Generic software is available for adaptor grammarinference, based either on Variational Bayes (Cohenet al, 2010) or Markov Chain Monte Carlo (Johnsonand Goldwater, 2009).
We used the latter softwarebecause it is capable of performing hyper-parameterinference for the PCFG rule probabilities and thePitman-Yor Process parameters.
We used the ?out-of-the-box?
settings for this software, i.e., uniformpriors on all PCFG rule parameters, a Beta(2, 1)prior on the Pitman-Yor a parameters and a ?vague?Gamma(100, 0.01) prior on the Pitman-Yor b pa-rameters.
(Presumably performance could be im-proved if the priors were tuned, but we did not ex-plore this here).Here we explore a simple ?collocation?
extensionto the unigram PCFG which associates multiwordcollocations, rather than individual words, with top-ics.
Hardisty et al (2010) showed that this signifi-cantly improved performance in a sentiment analy-sis task.The collocation adaptor grammar in Figure 5 gen-erates the words of the utterance as a sequence ofcollocations, each of which is a sequence of words.Each collocation is either associated with the sen-tence topic or with the None topic, just like words inthe unigram model.
Figure 6 shows a sample parsegenerated by the collocation adaptor grammar.We also experimented with a variant of the uni-gram and collocation grammars in which the topic-specific word distributions Wordt for each t ?
T887Model Social Utterance topic Word topic Lexiconcues acc.
f-score prec.
rec.
f-score prec.
rec.
f-score prec.
rec.unigram none 0.3395 0.4044 0.3249 0.5353 0.2007 0.1207 0.5956 0.1037 0.05682 0.5952unigram all 0.4907 0.6064 0.4867 0.8043 0.295 0.1763 0.9031 0.1483 0.08096 0.881colloc none 0.4331 0.3513 0.3272 0.3792 0.2431 0.1603 0.5028 0.08808 0.04942 0.4048colloc all 0.5837 0.598 0.5623 0.6384 0.4098 0.2702 0.8475 0.1671 0.09422 0.7381unigram?
none 0.3261 0.3767 0.3054 0.4914 0.1893 0.1131 0.5811 0.1167 0.06583 0.5122unigram?
all 0.5117 0.6106 0.4986 0.7875 0.2846 0.1693 0.891 0.1684 0.09402 0.8049colloc?
none 0.5238 0.3419 0.3844 0.3078 0.2551 0.1732 0.4843 0.2162 0.1495 0.3902colloc?
all 0.6492 0.6034 0.6664 0.5514 0.3981 0.2613 0.8354 0.3375 0.2269 0.6585Figure 7: Utterance topic, word topic and lexicon results for all models, on data with and without social cues.
Theresults for the variant models, in which Wordt nonterminals expand via WordNone, are shown under unigram?
andcolloc?.
Utterance topic shows how well the model discovered the intended topics at the utterance level, word topicshows how well the model associates word tokens with topics, and lexicon shows how well the topic most frequentlyassociated with a word type matches an external word-topic dictionary.
In this figure and below, ?colloc?
abbreviates?collocation?, ?acc.?
abbreviates ?accuracy?, ?prec.?
abbreviates ?precision?
and ?rec.?
abbreviates ?recall?.
(the set of non-None available topics) expand viaWordNone non-terminals.
That is, in the variantgrammars topical words are generated with the fol-lowing rule schema:Wordt ?WordNone ?t ?
TWordNone ?WordWord?
w ?w ?WIn these variant grammars, the WordNone nontermi-nal generates all the words of the language, so it de-fines a generic ?background?
distribution over all thewords, rather than just the nontopical words.
An ef-fect of this is that the variant grammars tend to iden-tify fewer words as topical.3 Experimental evaluationWe performed grammatical inference using theadaptor grammar software described in Johnson andGoldwater (2009).3 All experiments involved 4 runsof 5,000 samples each, of which the first 2,500 werediscarded for ?burn-in?.4 From these samples weextracted the modal (i.e., most frequent) analysis,3Because adaptor grammars are a generalisation of PCFGs,we could use the adaptor grammar software to estimate the un-igram model.4We made no effort to optimise the computation, but itseems the samplers actually stabilised after around a hundrediterations, so it was probably not necessary to sample so exten-sively.
We estimated the error in our results by running our mostcomplex model (the colloc?
model with all social cues) 20 times(i.e., 20?8 chains for 5,000 iterations) so we could compute thevariance of each of the evaluation scores (it is reasonable to as-sume that the simpler models will have smaller variance).
Thestandard deviation of all utterance topic and word topic mea-sures is between 0.005 and 0.01; the standard deviation for lex-icon f-score is 0.02, lexicon precision is 0.01 and lexicon recallis 0.03.
The adaptor grammar software uses a sentence-wisewhich we evaluated as described below.
The resultsof evaluating each model on the corpus with socialcues, and on another corpus identical except that thesocial cues have been removed, are presented in Fig-ure 7.Each model was evaluated on each corpus as fol-lows.
First, we extracted the utterance?s topic fromthe modal parse (this can be read off the Topictnodes), and compared this to the intended topics an-notated in the corpus.
The frequency with whichthe models?
predicted topics exactly matches theintended topics is given under ?utterance topic ac-curacy?
; the f-score, precision and recall of eachmodel?s topic predictions are also given in the table.Because our models all associate word tokenswith topics, we can also evaluate the accuracy withwhich word tokens are associated with topics.
Weconstructed a small dictionary which identifies thewords that can be used as the head of a phrase torefer to the topical objects (e.g., the dictionary in-dicates that dog, doggie and puppy name the topi-cal object DOG).
Our dictionary is relatively conser-vative; between one and eight words are associatedwith each topic.
We scored the topic label on eachword token in our corpus as follows.
A topic label isscored as correct if it is given in our dictionary andthe topic is one of the intended topics for the utter-ance.
The ?word topic?
entries in Figure 7 give theresults of this evaluation.blocked sampler, so it requires fewer iterations than a point-wise sampler.
We used 5,000 iterations because this is the soft-ware?s default setting; evaluating the trace output suggests itonly takes several hundred iterations to ?burn in?.
However, weran 8 chains for 25,000 iterations of the colloc?
model; as ex-pected the results of this run are within two standard deviationsof the results reported above.888Model Social Utterance topic Word topic Lexiconcues acc.
f-score prec.
rec.
f-score prec.
rec.
f-score prec.
rec.unigram none 0.3395 0.4044 0.3249 0.5353 0.2007 0.1207 0.5956 0.1037 0.05682 0.5952unigram +child.eyes 0.4573 0.5725 0.4559 0.7694 0.2891 0.1724 0.8951 0.1362 0.07415 0.8333unigram +child.hands 0.3399 0.4011 0.3246 0.5247 0.2008 0.121 0.5892 0.09705 0.05324 0.5476unigram +mom.eyes 0.338 0.4023 0.3234 0.5322 0.1992 0.1198 0.5908 0.09664 0.053 0.5476unigram +mom.hands 0.3563 0.4279 0.3437 0.5667 0.1984 0.1191 0.5948 0.09959 0.05455 0.5714unigram +mom.point 0.3063 0.3548 0.285 0.4698 0.1806 0.1086 0.5359 0.09224 0.05057 0.5238colloc none 0.4331 0.3513 0.3272 0.3792 0.2431 0.1603 0.5028 0.08808 0.04942 0.4048colloc +child.eyes 0.5159 0.5006 0.4652 0.542 0.351 0.2309 0.7312 0.1432 0.07989 0.6905colloc +child.hands 0.4827 0.4275 0.3999 0.4592 0.2897 0.1913 0.5964 0.1192 0.06686 0.5476colloc +mom.eyes 0.4697 0.4171 0.3869 0.4525 0.2708 0.1781 0.5642 0.1013 0.05666 0.4762colloc +mom.hands 0.4747 0.4251 0.3942 0.4612 0.274 0.1806 0.5666 0.09548 0.05337 0.4524colloc +mom.point 0.4228 0.3378 0.3151 0.3639 0.2575 0.1716 0.5157 0.09278 0.05202 0.4286Figure 8: Effect of using just one social cue on the experimental results for the unigram and collocation models.
The?importance?
of a social cue can be quantified by the degree to which the model?s evaluation score improves whenusing a corpus containing that social cue relative to its evaluation score when using a corpus without any social cues.The most important social cue is the one which causes performance to improve the most.Finally, we extracted a lexicon from the parsedcorpus produced by each model.
We counted howoften each word type was associated with each topicin our sampler?s output (including the None topic),and assigned the word to its most frequent topic.The ?lexicon?
entries in Figure 7 show how wellthe entries in these lexicons match the entries in themanually-constructed dictionary discussed above.There are 10 different evaluation scores, and nomodel dominates in all of them.
However, the top-scoring result in every evaluation is always for amodel trained using social cues, demonstrating theimportance of these social cues.
The variant colloca-tion model (trained on data with social cues) was thetop-scoring model on four evaluation scores, whichis more than any other model.One striking thing about this evaluation is that therecall scores are all much higher than the precisionscores, for each evaluation.
This indicates that allof the models, especially the unigram model, are la-belling too many words as topical.
This is perhapsnot too surprising: because our models completelylack any notion of syntactic structure and simplymodel the association between words and topics,they label many non-nouns with topics (e.g., woofis typically labelled with the topic DOG).3.1 Evaluating the importance of social cuesIt is scientifically interesting to be able to evalu-ate the importance of each of the social cues togrounded learning.
One way to do this is to studythe effect of adding or removing social cues fromthe corpus on the ability of our models to performgrounded learning.
An important social cue shouldhave a large impact on our models?
performance; anunimportant cue should have little or no impact.Figure 8 compares the performance of the uni-gram and collocation models on corpora containinga single social cue to their performance on the cor-pus without any social cues, while Figure 9 com-pares the performance of these models on corporacontaining all but one social cue to the corpus con-taining all of the social cues.
In both of these evalua-tions, with respect to all 10 evaluation measures, thechild.eyes social cue had the most impact on modelperformance.Why would the child?s own gaze be more impor-tant than the caregiver?s?
Perhaps caregivers are fol-lowing in, i.e., talking about objects that their chil-dren are interested in (Baldwin, 1991).
However, an-other possible explanation is that this result is due tothe general continuity of conversational topics overtime.
Frank et al (to appear) show that for the cur-rent corpus, the topic of the preceding utterance isvery likely to be the topic of the current one also.Thus, the child?s eyes might be a good predictor be-cause they reflect the fact that the child?s attentionhas been drawn to an object by previous utterances.Notice that these two possible explanations of theimportance of the child.eyes cue are diametricallyopposed; the first explanation claims that the cue isimportant because the child is driving the discourse,while the second explanation claims that the cue isimportant because the child?s gaze follows the topicof the caregiver?s previous utterance.
This sort ofquestion about causal relationships in conversationsmay be very difficult to answer using standard de-scriptive techniques, but it may be an interesting av-889Model Social Utterance topic Word topic Lexiconcues acc.
f-score prec.
rec.
f-score prec.
rec.
f-score prec.
rec.unigram all 0.4907 0.6064 0.4867 0.8043 0.295 0.1763 0.9031 0.1483 0.08096 0.881unigram ?child.eyes 0.3836 0.4659 0.3738 0.6184 0.2149 0.1286 0.6546 0.1111 0.06089 0.6341unigram ?child.hands 0.4907 0.6063 0.4863 0.8051 0.296 0.1769 0.9056 0.1525 0.08353 0.878unigram ?mom.eyes 0.4799 0.5974 0.4768 0.7996 0.2898 0.1727 0.9007 0.1551 0.08486 0.9024unigram ?mom.hands 0.4871 0.5996 0.4815 0.7945 0.2925 0.1746 0.8991 0.1561 0.08545 0.9024unigram ?mom.point 0.4875 0.6033 0.4841 0.8004 0.2934 0.1752 0.9007 0.1558 0.08525 0.9024colloc all 0.5837 0.598 0.5623 0.6384 0.4098 0.2702 0.8475 0.1671 0.09422 0.738colloc ?child.eyes 0.5604 0.5746 0.529 0.6286 0.39 0.2561 0.8176 0.1534 0.08642 0.6829colloc ?child.hands 0.5849 0.6 0.5609 0.6451 0.4145 0.273 0.8612 0.1662 0.09375 0.7317colloc ?mom.eyes 0.5709 0.5829 0.5457 0.6255 0.4036 0.2655 0.8418 0.1662 0.09375 0.7317colloc ?mom.hands 0.5795 0.5935 0.5571 0.6349 0.4038 0.2653 0.8442 0.1788 0.1009 0.7805colloc ?mom.point 0.5851 0.6006 0.5607 0.6467 0.4097 0.2685 0.8644 0.1742 0.09841 0.7561Figure 9: Effect of using all but one social cue on the experimental results for the unigram and collocation models.The ?importance?
of a social cue can be quantified by the degree to which the model?s evaluation score degrades whenthat just social cue is removed from the corpus, relative to its evaluation score when using a corpus without all socialcues.
The most important social cue is the one which causes performance to degrade the most.enue for future investigation using more structuredmodels such as those proposed here.54 Conclusion and future workThis paper presented four different grounded learn-ing models that exploit social cues.
These modelsare all expressed via reductions to grammatical in-ference problems, so standard ?off the shelf?
gram-matical inference tools can be used to learn them.Here we used the same adaptor grammar softwaretools to learn all these models, so we can be rel-atively certain that any differences we observe aredue to differences in the models, rather than quirksin the software.Because the adaptor grammar software performsfull Bayesian inference, including for model param-eters, an unusual feature of our models is that wedid not need to perform any parameter tuning what-soever.
This feature is particularly interesting withrespect to the parameters on social cues.
Psycholog-ical proposals have suggested that children may dis-cover that particular social cues help in establishingreference (Baldwin, 1993; Hollich et al, 2000), butprior modeling work has often assumed that cues,cue weights, or both are prespecified.
In contrast, themodels described here could in principle discover awide range of different social conventions.5A reviewer suggested that we can test whether child.eyeseffectively provides the same information as the previous topicby adding the previous topic as a (pseudo-) social cue.
We triedthis, and child.eyes and previous.topic do in fact seem to conveyvery similar information: e.g., the model with previous.topicand without child.eyes scores essentially the same as the modelwith all social cues.Our work instantiates the strategy of investigatingthe structure of children?s learning environment us-ing ?ideal learner?
models.
We used our models toinvestigate scientific questions about the role of so-cial cues in grounded language learning.
Becausethe performance of all four models studied in thispaper improve dramatically when provided with so-cial cues in all ten evaluation metrics, this paper pro-vides strong support for the view that social cues area crucial information source for grounded languagelearning.We also showed that the importance of the differ-ent social cues in grounded language learning canbe evaluated using ?add one cue?
and ?subtract onecue?
methodologies.
According to both of these, thechild.eyes cue is the most important of the five so-cial cues studied here.
There are at least two pos-sible reasons for this: the caregiver?s topic couldbe determined by the child?s gaze, or the child.eyescue could be providing our models with informationabout the topic of the previous utterance.Incorporating topic continuity and anaphoric de-pendencies into our models would be likely to im-prove performance.
This improvement might alsohelp us distinguish the two hypotheses about thechild.eyes cue.
If the child.eyes cue is just provid-ing indirect information about topic continuity, thenthe importance of the child.eyes cue should decreasewhen we incorporate topic continuity into our mod-els.
But if the child?s gaze is in fact determining thecare-giver?s topic, then child.eyes should remain astrong cue even when anaphoric dependencies andtopic continuity are incorporated into our models.890AcknowledgementsThis research was supported under the AustralianResearch Council?s Discovery Projects fundingscheme (project number DP110102506).ReferencesDare A. Baldwin.
1991.
Infants?
contribution to theachievement of joint reference.
Child Development,62(5):874?890.Dare A. Baldwin.
1993.
Infants?
ability to consult thespeaker for clues to word reference.
Journal of ChildLanguage, 20:395?395.Benjamin Bo?rschinger, Bevan K. Jones, and Mark John-son.
2011.
Reducing grounded learning tasks to gram-matical inference.
In Proceedings of the 2011 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 1416?1425, Edinburgh, Scotland, UK.,July.
Association for Computational Linguistics.M.
Carpenter, K. Nagell, M. Tomasello, G. Butterworth,and C. Moore.
1998.
Social cognition, joint attention,and communicative competence from 9 to 15 monthsof age.
Monographs of the society for research in childdevelopment.E.V.
Clark.
1987.
The principle of contrast: A constrainton language acquisition.
Mechanisms of language ac-quisition, 1:33.Shay B. Cohen, David M. Blei, and Noah A. Smith.2010.
Variational inference for adaptor grammars.In Human Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 564?572, Los Angeles, California, June.
Association forComputational Linguistics.Michael Frank, Noah Goodman, and Joshua Tenenbaum.2008.
A Bayesian framework for cross-situationalword-learning.
In J.C. Platt, D. Koller, Y.
Singer, andS.
Roweis, editors, Advances in Neural InformationProcessing Systems 20, pages 457?464, Cambridge,MA.
MIT Press.Michael C. Frank, Joshua Tenenbaum, and Anne Fernald.to appear.
Social and discourse contributions to thedetermination of reference in cross-situational wordlearning.
Language, Learning, and Development.Eric A. Hardisty, Jordan Boyd-Graber, and Philip Resnik.2010.
Modeling perspective using adaptor grammars.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages 284?292, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.G.J.
Hollich, K. Hirsh-Pasek, and R. Golinkoff.
2000.Breaking the language barrier: An emergentist coali-tion model for the origins of word learning.
Mono-graphs of the Society for Research in Child Develop-ment.Mark Johnson and Sharon Goldwater.
2009.
Improvingnonparameteric Bayesian inference: experiments onunsupervised word segmentation with adaptor gram-mars.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 317?325, Boulder, Colorado, June.
As-sociation for Computational Linguistics.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2007.
Adaptor Grammars: A framework for spec-ifying compositional nonparametric Bayesian models.In B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-vances in Neural Information Processing Systems 19,pages 641?648.
MIT Press, Cambridge, MA.Mark Johnson, Katherine Demuth, Michael Frank, andBevan Jones.
2010.
Synergies in learning wordsand their referents.
In J. Lafferty, C. K. I. Williams,J.
Shawe-Taylor, R.S.
Zemel, and A. Culotta, editors,Advances in Neural Information Processing Systems23, pages 1018?1026.Mark Johnson.
2008.
Using adaptor grammars to identi-fying synergies in the unsupervised acquisition of lin-guistic structure.
In Proceedings of the 46th AnnualMeeting of the Association of Computational Linguis-tics, pages 398?406, Columbus, Ohio.
Association forComputational Linguistics.Mark Johnson.
2010.
PCFGs, topic models, adaptorgrammars and learning topical collocations and thestructure of proper names.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 1148?1157, Uppsala, Sweden, July.Association for Computational Linguistics.Patricia K. Kuhl, Feng-Ming Tsao, and Huei-Mei Liu.2003.
Foreign-language experience in infancy: Effectsof short-term exposure and social interaction on pho-netic learning.
Proceedings of the National Academyof Sciences USA, 100(15):9096?9101.Jeffrey Siskind.
1996.
A computational study of cross-situational techniques for learning word-to-meaningmappings.
Cognition, 61(1-2):39?91.L.B.
Smith, S.S. Jones, B. Landau, L. Gershkoff-Stowe,and L. Samuelson.
2002.
Object name learning pro-vides on-the-job training for attention.
PsychologicalScience, 13(1):13.Chen Yu and Dana H Ballard.
2007.
A unified model ofearly word learning: Integrating statistical and socialcues.
Neurocomputing, 70(13-15):2149?2165.891
