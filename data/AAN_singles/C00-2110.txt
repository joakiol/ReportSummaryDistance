Japanese Dependency Analysisusing a Determinist ic Finite State TransducerSatosh i  Sek ineComputer  Science Depar tmentNew York University715 Broadway, 7th floorNew York, NY 10003, USAAbst ractA deternfinistic finite state transducer is a fastdevice fbr analyzing strings.
It takes O(n) timeto analyze a string of length n. In this 1)al)er, anapplication of this technique to Japanese depen-dency analysis will be described.
We achievedthe speed at; a small cost in accuracy.
It takesabout 0.1.7 nfillisecond to analyze one sentence(average length is 10 bunsetsu, 1)ased on Pen-t:tahiti 650MHz PC, Linux) and we actuallyobserved the analysis time to be proportionalto the sentence length.
Thb accuracy is about;81% even though very little lexical informationis used.
This is about 17% and 9% better thanthe default and a simple system, respectively.We believe the gap between our pertbrmm:ceand the best current 1)erforlnm:ce on the stonetask, about 7%, can be filled by introducing lex-ical or sen:antic infornmtion.1 I n t roduct ionSyntactic analysis or parsing based on tradi-tional methods, like Chm't parsing or the GLRparsing algorithm, takes cubic or greater tin:ein the sentence length to analyze natural lan-guage sentences.
For Japmmse, Sekine et al(Sekine et al, 2000) proposed a Japanese depen-dency analyzer which mmlyzes entences in timequadratic in the sentence length using a back-ward search algorithm.
Recently, a mnnber ofresearch efforts using Finite State Transducers(FST) have been reported.
Roche built an En-glish syntactic mmlyzer by finding a fixed pointin a non-deterministic FST (Roche, 1994).
Butit still can't anMyze a sentence in time linear inthe sentence length.In this paper, we will propose a Japanesedependency analyzer using a Deterministic Fi-nile State Transducer (DFST).
The Japmtesedependency structure is usually represented byrelationships between phrasal units called %un-sets::'.
A lmnsetsu sually contains one or morecontent words, like a noun, verb or adjective,and zero or more time:ion words, lil:e a post-position (case marker) or verb/noun suffix.
Adependency between two bunsetsu has a direc-tion fl'om a dependent to its head.
Figure 1shows examples of lmnsetsu and dependencies.Each lmnsetsu is separated by "1".
The frstseglnent "KANOJO-HA" consists of two words,KANOJO (She) and HA (subject case, ,narl:er).The nmnbers in the "head" line show the headID of corresponding bunsctsus.
Note that thelast segment does not have a head, and it is thehead lmnsetsu of the sentence.
The task of thedependency analysis is to find the head ID foreach lnmsetsu.2 Backward  beam search  a lgor i thml?irst, we wouhl lil:e to describe the backwm'dbeam search algoril:lm: tbr ,\]at)m:ese d pendencyanalysis proposed by Sekinc et sd.
(Sekine et al,2000).
Their experin:ents suggested the methodproposed in this paper.The following characteristics are known ibrJapanese dependency.
Sekine et al assumedthese characteristics in order to design the algo-rithm 1(1) l)ependencies are directed from left to right(2) Dependencies don't cross.
(3) Each bunsetsu except the rightmost onehas only one head(4) Left context is not necessary to deternfinea dependency.1We know that there are exceptions (Shirai, 1998),but the frequencies of such exceptions are very stuN1.Characteristic (4) is not well recognized, but based onour experiments with humans, it is true more than 90%of the time.761ID 1 2 3 4 5 6KANOJ0-HA I KARE-GA I TSUKUTTA I PAI-W0 I YOROKONDE I UKETOTTA.
(She-subj) (he-subj) (made) (pie-obj) (with pleasure) (received)Head 6 3 4 6 6 -Translation: She received the pie made by him with pleasure.Figure h Example of a Japanese sentence, bunsetsus and dependenciesSekine et al proposed a backward beam searchalgorithm (analyze a sentence froln the tail tothe head, or right to left).
Backward search hastwo merits.
Let's assume we have analyzed upto the/14 + l-st bunsetsu in a sentence of lengthN (0 < M < N).
Now we are deciding on thehead of the M-th bunsetsu.
The first merit isthat the head of the dependency of the /1J-thbunsetsu is one of the bunsetsus between t14 + 1and N, which are already analyzed.
Becmlse ofthis, we don't have to keel) a huge number ofpossible analyses, i.e.
we can avoid somethinglike active edges in a chart parser, or makingparallel stacks in GLR parsing, as we can makea decision at this time.
Also, wc can use thebemn search mechanism, t)y keeping a certainnmnber of candidates of analyses at each t)un-setsu.
The width of the beam search can bee~Lsily tuned and the memory size of the processis proportional to the product of the input sen-tence length and the beam search width.
Theother merit is that the 1)ossible heads of the de-pendency can be narrowed down because of theassmnption of non-crossing dependencies.
Forexample, if the K-th bunsetsu depends on theL-th bunsetsn (/1J < K < L), then the 21//-th bunsetsu can't depend on any bunsetsus be-tween I(  and L. According to our experiment,this reduced the nmnber of heads to consider toless than 50%.Uchimoto et al implemented a Japanese de-pendency analyzer based on this algorithm incombination with the Maxinmm Entropy learn-ing method (Uchimoto et al, 2000).
The an-alyzer demonstrated a high accuracy.
Table 1shows the relationship between the beam widthand the accuracy of the system.
Froln the table,we can see that the accuracy is not sensitive tothe beam width, and even when the width isBeam width Dependency SentenceAccuracy Accuracy\]25102087.1487.1687.1587.2086.2140.6040.7640.6040.6040.60Table h Bemn width and accuracy1, which Ineans that at each stage, the depen-dency is deterministieally decided, the accuracyis almost the same as the best accuracy.
Thismeans that the left, context of a dependency isnot necessary to decide the dependency, whichis closely related to characteristic (4).
This re-sult gives a strong motivation tbr st~rting theresearch reported in this paper.3 IdeaUntbrtunately, the fact that the beam width canbe 1 by itself can not lead to the analysis in timeproportional to the input length.
Theoretically,it takes time quadratic in the length and this isobserved experimentally as well (see Figure 3 in(Sekine et al, 2000)).
This is due to the processof finding the head among the candidate bun-setsns on its right.
The time to find the head isproportional to the number of candidates, whichis roughly proportional to the number of bun-setsus on its right.
The key idea of getting linearspeed is to make this time a constant.Table 2 shows the number of candidate bun-setsus and the relative location of the headamong the candidates (the number of candi-date bunsetsus from the bmlsetsu being ana-lyzed).
The data is derived fl'om 1st to 8th of762Nmn.of Location of headeand.
1 2 3 4 5 6 7 8 9 10 11 12 13 1.41234567891011121314791812824 494810232 32926774 22443692 12941843 614848 27834O 110137 3351 1117 55 13 ()() \]31261439 1968888 660 1114398 331.
291 551176 133 i33 12526483 47 ,78 57 68 12128 17 19 21 19 2/t 4610 7 3 10 3 6 8 222 0 2 2 /1 2 /, 2101 2 0 0 0 0 22  13I.
0 0 1 0 0 0 0 0 0 0\] 0 0 0 0 0 0 0 0 0 0 2rl'able 2: Nmnber of candidate, and location of headJanuary part of the Kyoto COl'pus, wlfich is ahand created cort)llS tagged.with POS, tmnsetsuand dependency relationships (l{urohashi, Na-gao, 1997).
It is the same 1)ortion used as tiletraining data of the system described late, r. 'J~henmnbc, r of cmMidates is shown vertically andthe location of the head is shown horizontally.l;br exalnple, 2244 in the fom'th row and these(:Olid (-ohmn~ means that there are 2244 bun-setsu which have 4 head Calldidates and the 2ndfl'om the left is the head of the bunsetsu.We can observe that the.
data.
is very biased.The head location is very lilnited.
98.7% ofinstances are covered 1)y the first 4 candidatesand the last candidate, combined.
In the table,the data which are not covered by the criteriaare shown in italics.
From this obserw~tion, wecome to the key idea.
We restrict he head can-didate locations to consider, mid we rememberall patterns of categories at the limited loca-tions.
For each remembered lmttern, we alsoremember where the.
head (answer) is.
Thisstrategy will give us a constant ime select;ionof the head.
For example, assmne, at ~ certainlmnsetsu in the training data, there are five Call-didates.
Then we.
will remember the categoriesof the current lmnsetsu, st\y "A", and five can-didates, "B C 1) E F", as wall as the head loca-tion, for example "2nd".
At the analysis phase,if we encounter the same sittmtion, i.e.
the samecategory bmlsetsu "A" to be mmlyzed, and thesame categories of five candidates in the sameorder "13 C D E F", then we can just return tileremenfl)ered head location, "2nd".
This processcan be done in constant tilne and eventually, theanalysis of a sentence can 1)e done in time 1)ro-portional to the sentence length.For the iml)lementation, we used a DFST inwhich each state represents the patterll of thecandidates, the input of an edge is the categoryof the.
l)llnsetsll 1)eing analyzed and the outputof a.n edge.
is the location of hea(t.4: Imp lementat ionWe still have several 1)rol)lelns whi('h have tobe solved in order to iint)lement the idea.
As-SUlning the number of candidates to be 5, thet)roblelns are to(l) define the c~tegories of head bunsetsu can-didates,(2) limit the nunlber of patterns (the lmmberof states in DFST) to a mallageable range,because the Colnbilmtion of five categoriescould t)e huge(3) define the categories of intmt bunsetsus,(4) deal with unseen events (sparseness l)rob-lem).hi this section, we l)resent how we imple-mented the, system, wlfich at the stone tinle763shows the solution to the problems.
At the end,we implemented the systeln in a very small size;1200 lines of C progrmn, 188KB data file andless than 1MB processing size.Structure of DFSTFor the categories of head bunsetsu candi-dates, we used JUMAN's POS categories as thebasis and optimized them using held-out data.JUMAN has 42 parts-of-speech (POS) includingthe minor level POSs, and we used the POS ofthe head word of a candidate bunsetsu.
We alsoused the information of whether the bunsetsuhas a colnma or not.
The nulnber of categories1)ecomes 18 after tuning it using the held-outdata.The input of all edge is the information aboutthe 1)unsetsu currently being analyzed.
Weused the inforination of the tail of the bunsetsu(mostly function words), and it becomes 40 cat-egories according to the stone tuning process.The output of an edge is the information ofwhich candidate is the head.
It is simply a nun>ber from 1 to 5.
The training data containsexamples which represent he same state andinput, but different output.
This is due to therough definition of the categories or inherent im-possibility of to disambiguating the dependencyrelationship fi'om the given infbrmation only.
Insuch cases, we pick the one which is the mostfrequent as the answer in that situation.
We willdiscuss the problems caused by this strategy.Data sizeBased on the design described above, thenumber of states (or number of patterns) is1,889,568 (18 ~) and the number of edges is75,582,720 as each state has 40 outgoing edges.If we implement it as is, we may need severalhundred megabytes of data.
In order to keepit fast, all the data should be in memory, andthe current memory requirement is too large toimplement.To solve the problem, two ideas are employed.First, the states are represented by the combi-nation of the candidate categories, i.e.
statescan be located by the 5 candidate categories.So, once it transfers from one state to another,the new state can be identified from the previ-ous state, input bunsetsu and the output of theedge.
Using this operation, the new state doesnot need to be remembered for each edge andthis leads to a large data reduction.
Second, weintroduced the default dependency relationship.In the Japanese dependency relationship, 64%of bunsetsu debend on the next bunsetsu.
Soif this is the output, we don't record the edgeas it is the default.
In other words, if there isno edge in~brmation for a particular input at aparticular state, the outtmt is the next bunsetsu(or 1).
This gave unexpectedly a large benefit.For unseen events, it is reasonable to guess thatthe bunsetsu depends the next bunsetsu.
Be-cause of the default, we don't have to keep suchinformation.
Actually the majority (more than99%) of states and edges are unseen events, sothe default strategy helps a lot to reduce thedata size.By the combination of the two ideas, therecould be a state whidl has absolutely no in-formation.
If this kind of state is reached inthe DFST, the output for any input is the nextbunsetsu and the next state can be calculatedfroln the information you have.
In fact, wehave a lot of states with absolutely no infor-mation.
Before implementing the supplemen-tation, explained in the next section, the num-ber of recorded states is only 1,006 and thereare 1,554 edges (among 1,889,568 possible statesand 75,582,720 possible edges).
After imple-menting the supplementation, westill have only10,457 states and 31.,316 edges.
The data sizes(file sizes) are about 15KB and 188KB, respec-tively.Sparseness problemTile amount of training data is about 8,000sentences.
As the average number of bunsetsuin a sentence is 10, there are about 72,000 datapoints in the training data.
This number is verymuch smaller than the nmnber of possible states(1,889,568), and it seems obvious that we willhave a sparseness problem 2.In order to supplement the unseen events,we use the system developed by Udfimotoet.al (Uchimoto et al, 2000).
A large cor-pus is parsed by the analyzer, and the results2However, we can make the system by using the de-fault strategy mM surprisingly the accuracy of the sys-tem is not so bad.
This will be reported in the Experi-ment section764are added to the training cori)us.
In prac-tice, we parsed two years of lmwspaper articles(2,536,229 sentences ofMainichi Shinbun 94 and95, excluding Jalmary 1-10, 95, as these are usedin the Kyoto corpus).5 Exper imentIn this section, we will report the experilnent.~?\r(, used the Kyoto corl)us (ve.rsion 2).
Thetraining is done using January 1-8, 95 (7,960sentences), the test is done using Jmmary 9,95 (1,246 sentences) mid the parameter tuningis done using Jmmary 10, 95 (1,519 sentences;held-out data).
The input sentences to the sys-rein are morl)hologically analyzed and bunsetsunre detected correctly.Dependency AccuracyTable 3 shows the accuracy of the systems.The 'dependency accuracy' metals the percent-age of the tmnsetsus whose head is correctlymmlyzed.
The bmlsetsus are all but the lastbunsetsu of the sentence, as the last 1)unsetsuhas no head.
The 'default nlethod' is the sys-rein in which the all bunsetsus are supt)osedto dei)end on the next bunsetsu.
~t'he 'base-line reel;hod' is a quite siml)le rule-based sys-toni which was created by hand an(t has a\])out30 rules.
'l'he details of the system are rel)ortedin (Uchimoto et al, 1999).
The 'Uchimoto'ssystem' is tit(; system rel)orted in (Uchimoto ctal., 2000).
They used the same training datam~d test (lata.
The del)endency accuracies ofSystem Det)en(tencyAccuracyOur system (with supp.)
81.231%Our systenl (without sut)p. ) 77.972 %Default method (i4.14: %Baseline method 72.57 %Uchimoto's ystem 87.93 %Table 3: Dependency Accuracyour systems are 81% with supl)lenlentation a d78% without supt)lementation.
The result isabout 17% and 9% better than tile default midthe baseline nlethods respectively.
Comparedwith Uchimoto's ystem, we are about 7% be-hind.
But as Uchimoto's ystem used about40,()00 features including lexical features, andthey also introduced combined features (up to5), it is natural that our system, which uses only18 categories and only combinations of two cat-cgories, has less accuracy a.Analysis speedThe main objective of the system is speed.Table 4 shows the analysis speed on three di ffcrent platforms.
On the fastest machine, it an-alyzes a sentence in 0.17 millisecond.
Tat)le 5shows a comlmrisou of the analysis speed ofthree difl'erent systems on SPARC-I.
Our systemruns about 100 times faster than Uchimoto'ssystem and KNP(Kurohashi, Nagao, 1994).Platform Analysis time(millisec./sent.
)PentiulnIII 650MHz 0.17SmlEnterprise, 400MHz 0.29Ultra SPARC-I, \]70MHz 1.
{)3Table 4: Analysis SpeedSystem Analysis time(millisec./sent.
)Our system 1.03Uchimoto's ystem 170KNP 86Tal)le 5: Coml)arison of Analysis SpeedFigure 2 shows tile relationship between tilesentence length and tile analysis tinle.
We usedtile slowest nlachine (Ultra SPARC-I, 170MHz)in order to minimize obserw~tional errors.
Wecml clearly observe that the anMysis tinle is pro-portional to the sentence length, as was pre-dicted by the algorithm.The speed of tile training is slightly slowerl:han that of tile analysis.
The training on thesmaller training data (about 8000 sentences)t;akes M)out 10 seconds on Ultra SPAR.C-I.aHowever, our system uses context information offly( ~,bunsctsus, which is not used in Uchimoto's system.7651.51.00.50.0Analysis time(millisec.
)1 I0 5 I'0 15 2'0Sentence length (Num.of bunsetsu)Figure 2: Analysis time and Sentence length6 Discuss ion6.1 The restrictionPeople may strongly argue that the main I)rol>lenl of the system is the restriction of the headlocation.
We eonlpletely agree.
We restrict thecandidate to five bunsetsus, as we described ear-lie:', and we thcoretically ignored 1.3% of accu-racy.
Obviously, this restriction can be loosenedby widelfing the range of the candidates.
For ex-ample, 1.3% will be reduced to 0.5% if we take6 instead of 5 candidates, hnplenlentation ofthis change is easy.
However, the.
problen: lieswith the sparseness.
As shown in Table 2, thereare fewer examples with a large number of bml-setsu candidates.
For examI)le, there are only 4instances which haw~ 14 candidates.
It may beimpossible to accumulate nough training ex-mnples of these kinds, even if we use a lot ofuntagged text fbr the supplementation.
In suchcases, we believe, we should have other kindsof remedies.
One of them is a generalization ofa large number candidates to a smaller numbercandidates by deleting nniniportant bunsetsus.This remains for fl:ture research.We can easily imagine that other systems maynot analyze accurately the cases where the cor-rect head is far fl'om the bunsetsu.
For example,Uchimoto's ystem achieves an accuracy of 41%in the cases shown in italics in Table 2, which ismuch lower than 88%, the system's ow~rall ac-curacy.
So, relative to the Uchimoto's system,the restriction caused a loss of accuracy of only0.5% (1.3% x 41%) instead of 1.3%.6.2 AccuracyThere are several things to do in order to achievebetter accuracy.
One of the major things is touse the information which has not been used,but is known to be useful to decide dependencyrelationships.
Because the accuracy of the sys-tem against the training data is only 81.653%(without supplementation), it, is clear that wemiss some important information,We believe the lexical relationships in verbfrmne element preference (VFEP) is one of themost important types of information.
Analyz-ing the data, we can find evidence that suchinformation is crucial.
For exmnplc, there are236 exmnples in the training corpus where thereare 4 head candidates and they are bunsetsuswhose heads are noun, verb, noun and verb, andthe current bunsetsu ends with a kaku- j  oshi ,a major particle.
Out of the 236 exami)les, thenulnber of cases where the first, second, thirdand last candidate is the head are 60, 142, 3 and31, respectively.
The answer is widely spread,and as the current system takes the most fl'e-quent head as the answer, 94 eases out of 236(40%) are ignored.
This is due to the level ofcategorization which uses only POS informa-tion.
Looking at the example sentences in thisease, we can observe that the VFEP could solvethe problenl.It is not straightfbrward to add such lexiealinformation to the current franlework.
If suchinformation is incorporated into the state inibr-marion, the nmnber of states will become enor-mous.
We can alternatively take an approachwhich was taken by augmented CFG.
In this ap-proach, the lexical infbrmation will be referredto only when needed.
Such a process m~\y slowdown the analyzer, but since the nuinber of in-vocation of the process needed in a :~entencemay be proportional to the sentence length, webelieve the entire process may still operate intiine proportional to the sentence length.7 Conc lus ionWe proposed a Japmiese dependency analy-sis using a Deterministic Finite State Trans-ducer (DFST).
The system analyzes a sentencewith fairly good accuracy of about 81% in 0.17millisecond on average.
It can be at)plied toa wide range of NLP applications, includingreal time Machine Translation, Information Ex-766traction and Speech Recognition.
There area number of eflbrts to improve the accuracyof Japanese dependency analysis (Haruno et.al,1997) (Fujio, Matsumoto, 1998) (Kanaymnaet.al, 2000).
In particular, it is interesting to seethat Kanwmna's method uses a similar idea,limiting the head candidates, in order to im-prove the accuracy.
We are plmming to incor-porate the fruit of these works to improve ouraccuracy.
We believe our resem'ch is comple-mentary to these research.
We also believe themethod proposed in this l)aper ln~\y be applic>ble to other languages which share the chaiac-teristics explained earlier, for example, Korean,~lhrkish and others.8 AcknowledgmentThe eXl)eriments in this paper were conductedusing the Kyoto corpus.
We really appreciatetheir effort alld their kindness in making it pub-lit: domain.
We thank l)r. Kuroluu~hi and otherpeople who worked in the project.
Mr.Uchimotohelped us to prepare the d~l;a and the experi-ment, mid we would like to thank him.ReferencesMasakazu Fujio, Yuuji Matsumoto.
1998 :"JaI)an(;se Dctmndency Strtlctlll'( ~,Analysisbase(l on l,cxicalizcd Statistics", P'rocccding.s"o.f Th, i'rd Co~:/'('.rcnce onEMNLP 1)t)87-9(iI:liroshi Kanayama, Kcntaro '\]brisawa, YutakaMitsuishi, .Jml'ichi Tsujii.
2000 : '"A hybrid,lalmnese Parser with Hand-crafl;ed Grmnmarand Statistics", Proceedings o/" COLING-O0,this pr'oceedingsMasahiko Haruno, Satoshi Shirai, YoshiflmfiOoymna.
1998 : "Using Decision ~l~'ees toConstruct a Practical Parser", Procccding.s o\[COLING-A CL-08 pp505-511Sadao Kurohashi, Makoto Nagao.
1994 : "KNParser : Japanese Dependency/Case Struc-ture Analyzer", Pwcccdin.qs of the Wor'k-shop on Sharable Natural Language l~.c,sourccspp48-55Sadao Kurohashi, Makoto Nagao.
1.997 : "Ky-oto University text corpus project", Proceed-ings of the ANLP-9Z Japan pp115-118Emmmmel R.oche.
1994 : "Two Parsing Algo-rithms by Means of Finite State Transduc-ers", Proceedings of COLING-9/j pp431-435,Satoshi Sekine, Kiyotaka Ucllimoto, Hitoshi Isa-hara.
2000 : "Statistical Dependency Anal-ysis using Backwm'd Beam Search", Proceed-ings of COLING-O0, this proceedingsSatoshi Shirai.
19!18 : "Heuristics and its lim-itation", ,JouTvtal of the ANLP, Japan Vol.5No.l, t)1)1-2Kiyotaka Uchimoto, Satoshi Sekine, Hitoshi Isa-hara.
1999 : "Jal)anese Dependency Struc-tllre Analysis Based on Maximum EntropyModels", ,\]our'nal of Information ProcessingSociety of Japan Vol.40, No.9, pp3397-3407Kiyotaka Uchimoto, Masaki Mm'ata, SatoshiSekine, Hitoshi Isahara.
2000 : "DependencyModel Using l?osterior Context", Pr'oceedingsof the IIYPT-O0767
