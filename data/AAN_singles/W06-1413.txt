Proceedings of the Fourth International Natural Language Generation Conference, pages 89?91,Sydney, July 2006. c?2006 Association for Computational LinguisticsThe Clarity-Brevity Trade-off in Generating Referring Expressions ?Imtiaz Hussain Khan and Graeme Ritchie and Kees van DeemterDepartment of Computing ScienceUniversity of AberdeenAberdeen AB24 3UE, U.K.{ikhan,gritchie,kvdeemte}@csd.abdn.ac.ukAbstractExisting algorithms for the Generation ofReferring Expressions (GRE) aim at gen-erating descriptions that allow a hearer toidentify its intended referent uniquely; thelength of the expression is also considered,usually as a secondary issue.
We explorethe possibility of making the trade-off be-tween these two factors more explicit, viaa general cost function which scores thesetwo aspects separately.
We sketch somemore complex phenomena which might beamenable to this treatment.1 IntroductionUntil recently, GRE algorithms have focussed onthe generation of distinguishing descriptions thatare either as short as possible (e.g.
(Dale, 1992;Gardent, 2002)) or almost as short as possible (e.g.
(Dale and Reiter, 1995)).
Since reductions in am-biguity are achieved by increases in length, thereis a tension between these factors, and algorithmsusually resolve this in some fixed way.
However,the need for a distinguishing description is usuallyassumed, and typically built in to GRE algorithms.We will suggest a way to make explicit this bal-ance between clarity (i.e.
lack of ambiguity) andbrevity, and we indicate some phenomena whichwe believe may be illuminated by this approach.The ideas in this paper can be seen as a loosen-ing of some of the many simplifying assumptionsoften made in GRE work.
?This work is supported by a University of AberdeenSixth Century Studentship, and the TUNA project (EPSRC,UK) under grant number GR/S13330/01.
We thank Ielka vander Sluis and Albert Gatt for valuable comments.2 Clarity, Brevity and CostWe consider only simple GRE, where the aim is toconstruct a conjunction of unary properties whichdistinguish a single target object from a set of po-tential distractors.
Our notation is as follows.
Adomain consists of a set D of objects, and a set Pof properties applicable to objects in D. A descrip-tion is a subset of P. The denotation of S, written[[ S ]], is {x ?
D | ?p ?
S : p(x)}.
(Krahmer et al, 2003) describe an approach toGRE in which a cost function guides search for asuitable description, and show that some existingGRE algorithms fit into this framework.
However,they follow the practice of concentrating solely ondistinguishing descriptions, treating cost as a mat-ter of brevity.
We suggest that decomposing costinto two components, for the clarity and brevityof descriptions, permits the examination of trade-offs.
For now, we will take the cost of a descriptionS to be the sum of two terms:cost(S) = fC(S) + fB(S).where fC counts ambiguity (lack of clarity) andfB counts size (lack of brevity).
Even with thisdecomposition of cost, some existing algorithmscan still be seen as cost-minimisation.
For exam-ple, the cost functions:fC(S) =| P | ?
| [[ S ]] |fB(S) = | S |allow the Full Brevity algorithm (Dale, 1992) tobe viewed as minimising cost(S), and the in-cremental algorithm (Dale and Reiter, 1995) ashill-climbing (strictly, hill-descending), guided bythe property-ordering which that algorithm re-quires.
Whereas Krahmer et al?s cost functionsare (brevity-based) heuristic guidance functions,our alternative here is a global quantity for opti-misation.
Hence their simulation of Full Brevity89relies on the details of their algorithm (rather thancost) to ensure clarity, while our own cost functionensures both brevity and clarity.3 Exploring the Trade-off3.1 Varying penalties for distractorsImagine the following situation.
You are prepar-ing a meal in a friend?s house, and you wish toobtain, from your own kitchen, a bottle of Italianextra virgin olive oil which you know is there.
Theonly way open to you is to phone home and askyour young child to bring it round for you.
Youknow that also in your kitchen cupboard are somedistractors: one bottle each of Spanish extra virginolive oil, Italian non-virgin olive oil, cheap veg-etable oil, linseed oil (for varnishing) and cam-phorated oil (medicinal).
It is imperative that youdo not get the linseed or camphorated oil, andpreferable that you receive olive oil.
A full ex-pression, Italian extra virgin olive oil, guaranteesclarity, but may overload your helper?s abilities.
Avery short expression, oil, is risky.
You might wellsettle for the intermediate olive oil.To model this situation, fC could take a muchhigher value if [[ S ]] contains a distractor whichmust not be selected (e.g.
varnish rather than cook-ing oil).
That is, instead of a simple linear functionof the size of [[ S ]], there is a curve where the costdrops more steeply as the more undesirable dis-tractors are excluded.
For example, each objectcould be assigned a numerical rating of how unde-sirable it is, with the target having a score of zero,and the fC value for a set A could be the maxi-mum rating of any element of A.
(This would, ofcourse, require a suitably rich domain model.
)The brevity cost function fB could still be a rel-atively simple linear function, providing fB valuesdo not mask the effect of the shape of the fC curve.3.2 Fuzziness of targetSuppose Mrs X has dropped a piece of rawchicken meat on the kitchen table, and immedi-ately removed the meat.
She would now like MrX to wipe the area clean.
The meat leaves no visi-ble stain, so she has to explain where it was.
In thiscase, it appears that there is no such thing as a dis-tinguishing description (i.e.
a description that pinsdown the area precisely), although Mrs X can ar-bitrarily increase precision, by adding properties:?
the edge of the table,?
the edge of the table, on the left (etc.
)The ideal description would describe the dirty areaand nothing more, but a larger area will also do,if not too large.
Here, the domain D is implic-itly defined as all conceivable subareas of the ta-ble, the target is again one element of D, but ?
un-like the traditional set-up with discrete elements ?a description (fuzzily) defines one such area, nota disjoint collection of individual items.
Our fCoperates on the description S, not just on the num-ber of distractors, so it can assess the aptness ofthe denotation of any potential S. However, it hasto ensure that this denotation (subarea of the sur-face) contains the target (contaminated area), anddoes not contain too much beyond that.
Hence,we may need to augment our clarity cost functionwith another argument: the target itself.
In gen-eral, more complex domains may need more com-plicated functions.3.3 Underspecification in dialogueStandard GRE algorithms assume that the speakerknows what the hearer knows (Dale and Reiter,1995).
In practice, speakers can often only guess.It has been observed that speakers sometimes pro-duce referring expressions that are only disam-biguated through negotiation with the hearer, asexemplified in the following excerpt (quoted in(Hirst, 2002)).1.
A: What?s that weird creature over there?2.
B: In the corner?3.
A: [affirmative noise]4.
B: It?s just a fern plant.5.
A: No, the one to the left of it.6.
B: That?s the television aerial.
It pulls out.A and B are in the same room, in an informal set-ting, so A can be relatively interactive in convey-ing information.
Also, the situation does not ap-pear to be highly critical, in comparison to a mil-itary officer directing gunfire, or a surgeon guid-ing an incision.
Initially, A produces an expres-sion which is not very detailed.
It may be that hethinks this is adequate (the object is sufficientlysalient that B will uniquely determine the refer-ent), or he doesn?t really know, but is willing tomake an opening bid in a negotiation to reach thegoal of reference.
In the former case, a GRE algo-rithm which took account of salience (e.g.
(Krah-mer and Theune, 1999)), operating withA?s modelof B?s knowledge, should produce this sort of ef-fect.
(A dialogue model might also be needed.)
Inthe latter case, we need an algorithm which can90relax the need for complete clarity.
This could bearranged by having fC give similar scores to deno-tations where there are no distractors and to deno-tations where there are just a few distractors, withfB making a large contribution to the cost.3.4 Over-specificationRecently, interest has been growing in ?overspec-ified?
referring expressions, which contain moreinformation than is required to identify their in-tended referent.
Some of this work is mainly or ex-clusively experimental (Jordan and Walker, 2000;Arts, 2004), but algorithmic consequences are alsobeing explored (Horacek, 2005; Paraboni and vanDeemter, 2002; van der Sluis and Krahmer, 2005).Over-specification could also arise in a dialoguesituation (comparable to that in Section 3.3) if aspeaker is unclear about the hearer?s knowledge,and so over-specifies (relative to his own knowl-edge) to increase the chances of success.This goes beyond the classical algorithms,where the main goal is total clarity, with no rea-son for the algorithm to add further properties toan already unambiguous expression.
That is, suchalgorithms assume that every description S forwhich | [[ S ]] |= 1 has the same level of clarity(fC value).
This assumption could be relaxed.
Forexample, the approach of (Horacek, 2005) to GREallows degrees of uncertainty about the effective-ness of properties to affect their selection.
Withinsuch a framework, one could separately computecosts for clarity (e.g.
likelihood of being under-stood) and brevity (which might include the com-plexity of expressing the properties).4 Conclusion and Future WorkWe have argued that the GRE task becomes verydifferent when some commonly-made assump-tions are abandoned: some distractors might beworse than others (section 3.1); the target may beimpossible to distinguish precisely (section 3.2);the speaker may be unsure what the hearer knows(section 3.3); or there may be a need for over-specification (section 3.4)).
As a result, it may benecessary to consider other aspects of the descrip-tions and their denotations, not simply countingdistractors or numbers of properties.
Some effectscould perhaps be modelled using costs which arenot simple linear functions, but which give varyingimportance to particular aspects of the denotationof a description, or of its content.
We hope thatthis approach will ultimately shed light not onlyon the effect of the discourse situation, but alsosome aspects of generating indefinite descriptions.ReferencesAnja Arts.
2004.
Overspecification in Instructive Text.Ph.D.
thesis, Tilburg University, The Netherlands.Robert Dale and Ehud Reiter.
1995.
Computationalinterpretations of the Gricean maxims in the gener-ation of referring expressions.
Cognitive Science,18:233?263.Robert Dale.
1992.
Generating Referring Expres-sions: Building Descriptions in a Domain of Objectsand Processes.
MIT Press.Claire Gardent.
2002.
Generating minimal distin-guishing descriptions.
In Proceedings of the 40thAnnual Meeting of the ACL (ACL?02), Philadelphia,USA.Graeme Hirst.
2002.
Negotiation, compromise, andcollaboration in interpersonal and human?computerconversations.
In Proceedings of Workshop onMeaning Negotiation, 18th National Conferenceon Artificial Intelligence, pages 1?4, Edmonton,Canada.Helmut Horacek.
2005.
Generating referential de-scriptions under conditions of uncertainty.
In Gra-ham Wilcock, Kristiina Jokinen, Chris Mellish, andEhud Reiter, editors, Proceedings of the 10th Eu-ropean Workshop on Natural Language Generation(ENLG-05), pages 58?67.Pamela Jordan and Marilyn Walker.
2000.
Learningattribute selections for non-pronominal expressions.In Proceedings of the 38th Annual Meeting of theACL (ACL-00), pages 181?190.Emiel Krahmer and Marie?t Theune.
1999.
Efficientgeneration of descriptions in context.
In Proceed-ings of the ESSLLI workshop on the generation ofnominals, Utrecht, The Netherlands.Emiel Krahmer, Sebastiaan van Erk, and Andre?
Verleg.2003.
Graph-based generation of referring expres-sions.
Computational Linguistics, 29(1):53?72.Ivandre?
Paraboni and Kees van Deemter.
2002.
Gener-ating easy references: the case of document deixis.In Proceedings of the Second International Confer-ence on Natural Language Generation, New York,USA.Ielka van der Sluis and Emiel Krahmer.
2005.
Towardsthe generation of overspecified multimodal referringexpressions.
In Proceedings of the Symposium onDialogue Modelling and Generation at the 15th An-nual Meeting of the ST & D (STD-05), Amsterdam,The Netherlands.91
