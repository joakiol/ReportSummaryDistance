Proceedings of the EACL 2009 Workshop on Language Technologies for African Languages ?
AfLaT 2009, pages 104?111,Athens, Greece, 31 March 2009. c?2009 Association for Computational LinguisticsMethods for Amharic Part-of-Speech TaggingBjo?rn Gamba?ck??
Fredrik Olsson?
Atelach Alemu Argaw?
Lars Asker?
?Userware Laboratory ?Dpt.
of Computer & Information Science ?Dpt.
of Computer & System SciencesSwedish Institute of Computer Science Norwegian University of Science & Technology Stockholm UniversityKista, Sweden Trondheim, Norway Kista, Sweden{gamback,fredriko}@sics.se gamback@idi.ntnu.no {atelach,asker}@dsv.su.seAbstractThe paper describes a set of experimentsinvolving the application of three state-of-the-art part-of-speech taggers to EthiopianAmharic, using three different tagsets.The taggers showed worse performancethan previously reported results for Eng-lish, in particular having problems withunknown words.
The best results wereobtained using a Maximum Entropy ap-proach, while HMM-based and SVM-based taggers got comparable results.1 IntroductionMany languages, especially on the African con-tinent, are under-resourced in that they havevery few computational linguistic tools or corpora(such as lexica, taggers, parsers or tree-banks)available.
Here, we will concentrate on the taskof developing part-of-speech taggers for Amharic,the official working language of the governmentof the Federal Democratic Republic of Ethiopia:Ethiopia is divided into nine regions, each withits own nationality language; however, Amharic isthe language for country-wide communication.Amharic is spoken by about 30 million peopleas a first or second language, making it the secondmost spoken Semitic language in the world (afterArabic), probably the second largest language inEthiopia (after Oromo), and possibly one of thefive largest languages on the African continent.The actual size of the Amharic speaking popula-tion must be based on estimates: Hudson (1999)analysed the Ethiopian census from 1994 and in-dicated that more than 40% of the population thenunderstood Amharic, while the current size of theEthiopian population is about 80 million.1182.5 million according to CIA (2009); 76.9 according toEthiopian parliament projections in December 2008 based onthe preliminary reports from the census of May 2007.In spite of the relatively large number of speak-ers, Amharic is still a language for which very fewcomputational linguistic resources have been de-veloped, and previous efforts to create languageprocessing tools for Amharic?e.g., Alemayehuand Willett (2002) and Fissaha (2005)?have beenseverely hampered by the lack of large-scale lin-guistic resources for the language.
In contrast, thework detailed in the present paper has been ableto utilize the first publicly available medium-sizedtagged Amharic corpus, described in Section 5.However, first the Amharic language as such isintroduced (in Section 2), and then the task of part-of-speech tagging and some previous work in thefield is described (Section 3).
Section 4 details thetagging strategies used in the experiments, the re-sults of which can be found in Section 6 togetherwith a short discussion.
Finally, Section 7 sums upthe paper and points to ways in which we believethat the results can be improved in the future.2 AmharicWritten Amharic (and Tigrinya) uses a uniquescript originating from the Ge?ez alphabet (theliturgical language of the Ethiopian OrthodoxChurch).
Written Ge?ez can be traced back to atleast the 4th century A.D., with the first versionsincluding consonants only, while the charactersin later versions represent consonant-vowel (CV)pairs.
In modern Ethiopic script each syllograph(syllable pattern) comes in seven different forms(called orders), reflecting the seven vowel sounds.The first order is the basic form; the others are de-rived from it by modifications indicating vowels.There are 33 basic forms, giving 7*33 syllographs,or fidels (?fidel?, lit.
?alphabet?
in Amharic, refersboth to the characters and the entire script).
UnlikeArabic and Hebrew, Amharic is written from leftto right.
There is no agreed upon spelling standardfor compound words and the writing system usesseveral ways to denote compounds104form patternroot sbr CCCperfect sa?bba?r CVCCVCimperfect sa?br CVCCgerund sa?br CVCCimperative sba?r CCVCcausative assa?bba?r as-CVCCVCpassive ta?sa?bba?r ta?s-CVCCVCTable 1: Some forms of the verb sbr (?break?
)2.1 Amharic morphologyA significantly large part of the vocabulary con-sists of verbs, and like many other Semitic lan-guages, Amharic has a rich verbal morphologybased on triconsonantal roots with vowel variantsdescribing modifications to, or supplementary de-tail and variants of the root form.
For example,the root sbr, meaning ?to break?
can have (amongothers!)
the forms shown in Table 1.
Subject, gen-der, number, etc., are also indicated as bound mor-phemes on the verb, as well as objects and posses-sion markers, mood and tense, beneficative, mal-factive, transitive, dative, negative, etc.Amharic nouns (and adjectives) can be inflectedfor gender, number, definiteness, and case, al-though gender is usually neutral.
The definite ar-ticle attaches to the end of a noun, as do conjunc-tions, while prepositions are mostly prefixed.2.2 Processing Amharic morphologyThe first effort on Amharic morphological pro-cessing was a rule-based system for verbs (andnouns derived from verbs) which used root pat-terns and affixes to determine lexical and in-flectional categories (Bayou, 2000), while Bayu(2002) used an unsupervised learning approachbased on probabilistic models to extract stems,prefixes, and suffixes for building a morphologicaldictionary.
The system was able to successfullyanalyse 87% of a small testdata set of 500 words.The first larger-scale morphological analyserfor Amharic verbs used XFST, the Xerox FiniteState Tools (Fissaha and Haller, 2003).
This waslater extended to include all word categories (Am-salu and Gibbon, 2005).
Testing with 1620 wordstext from an Amharic bible, 88?94% recall and54?94% precision (depending on the word-class)were reported.
The lowest precision (54%) wasobtained for verbs; Amsalu and Demeke (2006)thus describe ways to extend the finite-state sys-tem to handle 6400 simple verbal stems generatedfrom 1300 root forms.Alemayehu and Willett (2002) report on a stem-mer for Information Retrieval for Amharic, andtesting on a 1221 random word sample stated?Manual assessment of the resulting stems showedthat 95.5 percent of them were linguisticallymeaningful,?
but gave no evaluation of the cor-rectness of the segmentations.
Argaw and Asker(2007) created a rule-based stemmer for a similartask, and using 65 rules and machine readable dic-tionaries obtained 60.0% accuracy on fictional text(testing on 300 unique words) and 76.9% on newsarticles (on 1503 words, of which 1000 unique).23 Part-of-Speech TaggingPart-of-speech (POS) tagging is normally treatedas a classification task with the goal to assign lex-ical categories (word classes) to the words in atext.
Most work on tagging has concentrated onEnglish and on using supervised methods, in thesense that the taggers have been trained on anavailable, tagged corpus.
Both rule-based and sta-tistical / machine-learning based approaches havebeen thoroughly investigated.
The Brill Tagger(Brill, 1995) was fundamental in using a com-bined rule- and learning-based strategy to achieve96.6% accuracy on tagging the Penn Treebankversion of the Wall Street Journal corpus.
Thatis, to a level which is just about what humansnormally achieve when hand-tagging a corpus, interms of interannotator agreement?even thoughVoutilainen (1999) has shown that humans can getclose to the 100% agreement mark if the annota-tors are allowed to discuss the problematic cases.Later taggers have managed to improve Brill?sfigures a little bit, to just above 97% on the WallStreet Journal corpus using Hidden Markov Mod-els, HMM and Conditional Random Fields, CRF;e.g., Collins (2002) and Toutanova et al (2003).However, most recent work has concentrated onapplying tagging strategies to other languages thanEnglish, on combining taggers, and/or on usingunsupervised methods.
In this section we will lookat these issues in more detail, in particular with therelation to languages similar to Amharic.3.1 Tagging Semitic languagesDiab et al (2004) used a Support Vector Machine,SVM-based tagger, trained on the Arabic Penn2Other knowledge sources for processing Amharic in-clude, e.g., Gasser?s verb stem finder (available fromnlp.amharic.org) and wordlists as those collected byGebremichael (www.cs.ru.nl/?biniam/geez).105Treebank 1 to tokenize, POS tag, and annotateArabic base phrases.
With an accuracy of 95.5%over a set of 24 tags, the data-driven tagger per-formed on par with state-of-the-art results for En-glish when trained on similar-sized data (168k to-kens).
Bar-Haim et al (2008) developed a lexicon-based HMM tagger for Hebrew.
They report89.6% accuracy using 21 tags and training on 36ktokens of news text.
Mansour (2008) ported thistagger into Arabic by replacing the morphologicalanalyzer, achieving an accuracy of 96.3% over 26tags on a 89k token corpus.
His approach modifiesthe analyses of sentences receiving a low proba-bility by adding synthetically constructed analysesproposed by a model using character information.A first prototype POS tagger for Amharic useda stochastic HMM to model contextual dependen-cies (Getachew, 2001), but was trained and testedon only one page of text.
Getachew suggested atagset for Amharic consisting of 25 tags.
Morerecently, CRFs have been applied to segment andtag Amharic words (Fissaha, 2005), giving an ac-curacy of 84% for word segmentation, using char-acter, morphological and lexical features.
The bestresult for POS-tagging was 74.8%, when adding adictionary and bigrams to lexical and morphologi-cal features, and 70.0% without dictionary and bi-grams.
The data used in the experiments was alsoquite small and consisted of 5 annotated news ar-ticles (1000 words).
The tagset was a reduced ver-sion (10 tags) of the one used by Getachew (2001),and will be further discussed in Section 5.2.3.2 Unsupervised taggingThe desire to use unsupervised machine learningapproaches to tagging essentially originates fromthe wish to exploit the vast amounts of unlabelleddata available when constructing taggers.
The areais particularly vivid when it comes to the treatmentof languages for which there exist few, if any, com-putational resources, and for the case of adaptingan existing tagger to a new language domain.Banko and Moore (2004) compared unsuper-vised HMM and transformation-based taggerstrained on the same portions of the Penn Treebank,and showed that the quality of the lexicon used fortraining had a high impact on the tagging results.Duh and Kirchhoff (2005) presented a minimally-supervised approach to tagging for dialectal Ara-bic (Colloquial Egyptian), based on a morpholog-ical analyzer for Modern Standard Arabic and un-labeled texts in a number of dialects.
Using a tri-gram HMM tagger, they first produced a baselinesystem and then gradually improved on that in anunsupervised manner by adding features so as tofacilitate the analysis of unknown words, and byconstraining and refining the lexicon.Unsupervised learning is often casted as theproblem of finding (hidden) structure in unla-beled data.
Goldwater and Griffiths (2007) notedthat most recent approaches to this problem aimto identify the set of attributes that maximizessome target function (Maximum Likelihood Esti-mation), and then to select the values of these at-tributes based on the representation of the model.They proposed a different approach, based onBayesian principles, which tries to directly max-imize the probability of the attributes based onobservation in the data.
This Bayesian approachoutperformed Maximum Likelihood Estimationwhen training a trigram HMM tagger for English.Toutanova and Johnson (2007) report state-of-the-art results by extending the work on Bayesianmodelling for unsupervised learning of taggersboth in the way that prior knowledge can be incor-porated into the model, and in the way that possi-ble tags for a given word is explicitly modeled.3.3 Combining taggersA possible way to improve on POS tagging resultsis to combine the output of several different tag-gers into a committee, forming joint decisions re-garding the labeling of the input.
Roughly, thereare three obvious ways of combining multiple pre-dicted tags for a word: random decision, voting,and stacking (Dietterich, 1997), with the first waysuited only for forming a baseline.
Voting canbe divided into two subclasses: unweighted votes,and weighted votes.
The weights of the votes, ifany, are usually calculated based on the classifiers?performance on some initial dataset.
Stacking, fi-nally, is a way of combining the decisions madeby individual taggers in which the predicted tagsfor a given word are used as input to a subsequenttagger which outputs a final label for the word.Committee-based approaches to POS tagginghave been in focus the last decade: Brill and Wu(1998) combined four different taggers for Englishusing unweighted voting and by exploring contex-tual cues (essentially a variant of stacking).
Aireset al (2000) experimented with 12 different waysof combining the output from taggers for Brazilian106Portuguese, and concluded that some, but not all,combinations yielded better accuracy than the bestindividual tagger.
Shacham and Wintner (2007)contrasted what they refer to as being a na?
?ve wayof combining taggers with a more elaborate, hi-erarchical one for Hebrew.
In the end, the elabo-rated method yielded results inferior to the na??veapproach.
De Pauw et al (2006) came to simi-lar conclusions when using five different ways ofcombining four data-driven taggers for Swahili.The taggers were based on HMM, Memory-basedlearning, SVM, and Maximum Entropy, with thelatter proving most accurate.
Only in three offive cases did a combination of classifiers performbetter than the Maximum Entropy-based tagger,and simpler combination methods mostly outper-formed more elaborate ones.Spoustova?
et al (2007) report on work on com-bining a hand-written rule-based tagger with threestatistically induced taggers for Czech.
As an ef-fect of Czech being highly inflectional, the tagsetsare large: 1000?2000 unique tags.
Thus the ap-proach to combining taggers first aims at reducingthe number of plausible tags for a word by usingthe rule-based tagger to discard impossible tags.Precision is then increased by invoking one or allof the data-driven taggers.
Three different ways ofcombining the taggers were explored: serial com-bination, involving one of the statistical taggers;so called SUBPOS pre-processing, involving twoinstances of statistical taggers (possibly the sametagger); and, parallel combination, in which an ar-bitrary number of statistical taggers is used.
Thecombined tagger yielded the best results for CzechPOS tagging reported to date, and as a side-effectalso the best accuracy for English: 97.43%.34 The TaggersThis section describes the three taggers used in theexperiments (which are reported on in Section 6).4.1 Hidden Markov Models: TnTTnT, ?Trigrams?n?Tags?
(Brants, 2000) is a veryfast and easy-to-use HMM-based tagger whichpainlessly can be trained on different languagesand tagsets, given a tagged corpus.4 A Markov-based tagger aims to find a tag sequence whichmaximizes P (wordn|tagn) ?
P (tagn|tag1...n?1),where the first factor is the emit (or lexical) prob-3As reported on ufal.mff.cuni.cz/compost/en4www.coli.uni-saarland.de/?thorsten/tntability, the likelihood of a word given certain tag,and the second factor is the state transition (or con-textual) probability, the likelihood of a tag given asequence of preceding tags.
TnT uses the Viterbialgorithm for finding the optimal tag sequence.Smoothing is implemented by linear interpolation,the respective weights are determined by deletedinterpolation.
Unknown words are handled by asuffix trie and successive abstraction.Applying TnT to the Wall Street Journal cor-pus, Brants (2000) reports 96.7% overall accuracy,with 97.0% on known and 85.5% on unknownwords (with 2.9% of the words being unknown).4.2 Support Vector Machines: SVMToolSupport Vector Machines (SVM) is a linear learn-ing system which builds two class classifiers.
Itis a supervised learning method whereby the in-put data are represented as vectors in a high-dimensional space and SVM finds a hyperplane (adecision boundary) separating the input space intotwo by maximizing the margin between positiveand negative data points.SVMTool is an open source tagger based onSVMs.5 Comparing the accuracy of SVMToolwith TnT on the Wall Street Journal corpus,Gime?nez and Ma`rquez (2004) report a better per-formance by SVMTool: 96.9%, with 97.2% onknown words and 83.5% on unknown.4.3 Maximum Entropy: MALLETMaximum Entropy is a linear classificationmethod.
In its basic incarnation, linear classifi-cation combines, by addition, the pre-determinedweights used for representing the importance ofeach feature to a given class.
Training a Maxi-mum Entropy classifier involves fitting the weightsof each feature value for a particular class to theavailable training data.
A good fit of the weightsto the data is obtained by selecting weights to max-imize the log-likelihood of the learned classifica-tion model.
Using an Maximum Entropy approachto POS tagging, Ratnaparkhi (1996) reports a tag-ging accuracy of 96.6% on the Wall Street Journal.The software of choice for the experiments re-ported here is MALLET (McCallum, 2002), afreely available Java implementation of a range ofmachine learning methods, such as Na?
?ve Bayes,decision trees, CRF, and Maximum Entropy.65www.lsi.upc.edu/?nlp/SVMTool6mallet.cs.umass.edu1075 The DatasetThe experiments of this paper utilize the firstmedium-sized corpus for Amharic (available athttp://nlp.amharic.org).
The corpus consistsof all 1065 news texts (210,000 words) from theEthiopian year 1994 (parts of the Gregorian years2001?2002) from the Walta Information Center, aprivate news service based in Addis Ababa.
It hasbeen morphologically analysed and manually part-of-speech tagged by staff at ELRC, the EthiopianLanguages Research Center at Addis Ababa Uni-versity (Demeke and Getachew, 2006).The corpus is available both in fidel and tran-scribed into a romanized version known as SERA,System for Ethiopic Representation in ASCII (Ya-cob, 1997).
We worked with the transliteratedform (202,671 words), to be compatible with themachine learning tools used in the experiments.5.1 ?Cleaning?
the corpusUnfortunately, the corpus available on the net con-tains quite a few errors and tagging inconsisten-cies: nine persons participated in the manual tag-ging, writing the tags with pen on hard copies,which were given to typists for insertion into theelectronic version of the corpus?a procedure ob-viously introducing several possible error sources.Before running the experiments the corpus hadto be ?cleaned?
: many non-tagged items have beentagged (the human taggers have, e.g., often taggedthe headlines of the news texts as one item, end-of-sentence punctuation), while some double tagshave been removed.
Reflecting the segmentationof the original Amharic text, all whitespaces wereremoved, merging multiword units with a singletag into one-word units.
Items like ?"?
and ?/?have been treated consistently as punctuation, andconsistent tagging has been added to word-initialand word-final hyphens.
Also, some direct taggingerrors and misspellings have been corrected.Time expressions and numbers have not beenconsistently tagged at all, but those had to be leftas they were.
Finally, many words have been tran-scribed into SERA in several versions, with onlythe cases differing.
However, this is also difficultto account for (and in the experiments below weused the case sensitive version of SERA), sincethe SERA notation in general lets upper and lowercases of the English alphabet represent differentsymbols in fidel (the Amharic script).5.2 TagsetsFor the experiments, three different tagsets wereused.
Firstly, the full, original 30-tag set devel-oped at the Ethiopian Languages Research Centerand described by Demeke and Getachew (2006).This version of the corpus will be referred to as?ELRC?.
It contains 200, 863 words and differsfrom the published corpus in way of the correc-tions described in the previous section.Secondly, the corpus was mapped to 11 basictags.
This set consists of ten word classes: Noun,Pronoun, Verb, Adjective, Preposition, Conjunc-tion, Adverb, Numeral, Interjection, and Punctua-tion, plus one tag for problematic words (unclear:<UNC>).
The main differences between the twotagsets pertain to the treatment of prepositions andconjunctions: in ?ELRC?
there are specific classesfor, e.g., pronouns attached with preposition, con-junction, and both preposition and conjunction(similar classes occur for nouns, verbs, adjectives,and numerals).
In addition, numerals are dividedinto cardinals and ordinals, verbal nouns are sepa-rated from other nouns, while auxiliaries and rela-tive verbs are distinguished from other verbs.
Thefull tagset is made up of thirty subclasses of thebasic classes, based on type of word only: the tagscontain no information on grammatical categories(such as number, gender, tense, and aspect).Thirdly, for comparison reasons, the full tagsetwas mapped to the 10 tags used by Fissaha (2005).These classes include one for Residual (R) whichwas assumed to be equivalent to <UNC>.
In addi-tion, <CONJ> and <PREP> were mapped to Ad-position (AP), and both <N> and <PRON> to N.The other mappings were straight-forward, exceptthat the ?BASIC?
tagset groups all verbs together,while Fissaha kept Auxiliary (AUX) as its ownclass.
This tagset will be referred to as ?SISAY?.5.3 FoldsFor evaluation of the taggers, the corpus was splitinto 10 folds.
These folds were created by chop-ping the corpus into 100 pieces, each of about2000 words in sequence, while making sure thateach piece contained full sentences (rather thancutting off the text in the middle of a sentence),and then merging sets of 10 pieces into a fold.Thus the folds represent even splits over the cor-pus, to avoid tagging inconsistencies, but the se-quences are still large enough to potentially makeknowledge sources such as n-grams useful.108Fold TOTAL KNOWN UNKNOWNfold00 20,027 17,720 2,307fold01 20,123 17,750 2,373fold02 20,054 17,645 2,409fold03 20,169 17,805 2,364fold04 20,051 17,524 2,527fold05 20,058 17,882 2,176fold06 20,111 17,707 2,404fold07 20,112 17,746 2,366fold08 20,015 17,765 2,250fold09 20,143 17,727 2,416Average 20,086 17,727 2,359Percent ?
88.26 11.74Table 2: Statistics for the 10 foldsTable 2 shows the data for each of the folds, interms of total number of tokens, as well as splitinto known and unknown tokens, where the termUNKNOWN refers to tokens that are not in any ofthe other nine folds.
The figures at the bottomof the table show the average numbers of knownand unknown words, over all folds.
Notably, theaverage number of unknown words is about fourtimes higher than in the Wall Street Journal cor-pus (which, however, is about six times larger).6 ResultsThe results obtained by applying the three dif-ferent tagging strategies to the three tagsets areshown in Table 3, in terms of average accura-cies after 10-fold cross validation, over all thetokens (with standard deviation),7 as well as ac-curacy divided between the known and unknownwords.
Additionally, SVMTool and MALLET in-clude support for automatically running 10-foldcross validation on their own folds.
Figures forthose runs are also given.
The last line of the tableshows the baselines for the tagsets, given as thenumber of tokens tagged as regular nouns dividedby the total number of words after correction.6.1 TnTAs the bold face figures indicate, TnT achieves thebest scores of all three taggers, on all three tagsets,on known words.
However, it has problems withthe unknown words?and since these are so fre-quent in the corpus, TnT overall performs worsethan the other taggers.
The problems with the un-known words increase as the number of possibletags increase, and thus TnT does badly on the orig-inal tagging scheme (?ELRC?
), where it only gets7The standard deviation is given by?1n?ni=1(xi ?
x)2where x is the arithmetic mean ( 1n?ni=1 xi).ELRC BASIC SISAYTnT 85.56 92.55 92.60STD DEV 0.42 0.31 0.32KNOWN 90.00 93.95 93.99UNKNOWN 52.13 82.06 82.20SVM 88.30 92.77 92.80STD DEV 0.41 0.31 0.37KNOWN 89.58 93.37 93.34UNKNOWN 78.68 88.23 88.74Own folds 88.69 92.97 92.99STD DEV 0.33 0.17 0.26MaxEnt 87.87 92.56 92.60STD DEV 0.49 0.38 0.43KNOWN 89.44 93.26 93.27UNKNOWN 76.05 87.29 87.61Own folds 90.83 94.64 94.52STD DEV 1.37 1.11 0.69BASELINE 35.50 58.26 59.61Table 3: Tagging resultsa bit over 50% on the unknown words (and 85.6%overall).
For the two reduced tagsets TnT doesbetter: overall performance goes up to a bit over92%, with 82% on unknown words.Table 3 shows the results on the default configu-ration of TnT, i.e., using 3-grams and interpolatedsmoothing.
Changing these settings give no sub-stantial improvement overall: what is gained atone end (e.g., on unknown words or a particulartagset) is lost at the other end (on known words orother tagsets).
However, per default TnT uses asuffix trie of length 10 to handle unknown words.Extending the suffix to 20 (the maximum valuein TnT) gave a slight performance increase on?ELCR?
(0.13% on unknown words, 0.01% over-all), while having no effect on the smaller tagsets.6.2 SVMThe SVM-tagger outperforms TnT on unknownwords, but is a bit worse on known words.
Overall,SVM is slightly better than TnT on the two smallertagsets and clearly better on the large tagset, andsomewhat better than MaxEnt on all three tagsets.These results are based on SVMTool?s defaultparameters: a one-pass, left-to-right, greedy tag-ging scheme with a window size of 5.
Previousexperiments with parameter tuning and multiplepass tagging have indicated that there is room forperformance improvements by ?
2%.6.3 Maximum EntropyThe MaxEnt tagger gets results comparable to theother taggers on the predefined folds.
Its overall109Wordn ; Tag of WordnPrefixes of Wordn, length 1-5 charactersPostfixes of Wordn, length 1-5 charactersIs Wordn capitalized?Is Wordn all digits?Does Wordn contain digits?Does Wordn contain a hyphen?Wordn?1 ; Tag of Wordn?1Wordn?2 ; Tag of Wordn?2Wordn+1Wordn+2Table 4: Features used in the MaxEnt taggerperformance is equivalent to TnT?s on the smallertagsets, but significantly better on ?ELRC?.As can be seen in Table 3, the MaxEnt tag-ger clearly outperforms the other taggers on alltagsets, when MALLET is allowed to create itsown folds: all tagsets achieved classification ac-curacies higher than 90%, with the two smallertagsets over 94.5%.
The dramatic increase in thetagger?s performance on these folds is surprising,but a clear indication of one of the problems withn-fold cross validation: even though the resultsrepresent averages after n runs, the choice of theoriginal folds to suit a particular tagging strategyis of utmost importance for the final result.Table 4 shows the 22 features used to representan instance (Wordn) in the Maximum Entropy tag-ger.
The features are calculated per token withinsentences: the starting token of a sentence is notaffected by the characteristics of the tokens endingthe previous sentence, nor the other way around.Thus not all features are calculated for all tokens.6.4 DiscussionIn terms of accuracy, the MaxEnt tagger is byfar the best of the three taggers, and on all threetagsets, when allowed to select its own folds.
Still,as Table 3 shows, the variation of the results foreach individual fold was then substantially larger.It should also be noted that TnT is by far thefastest of the three taggers, in all respects: in termsof time to set up and learn to use the tagger, interms of tagging speed, and in particular in termsof training time.
Training TnT is a matter of sec-onds, but a matter of hours for MALLET/MaxEntand SVMTool.
On the practical side, it is worthadding that TnT is robust, well-documented, andeasy to use, while MALLET and SVMTool aresubstantially more demanding in terms of user ef-fort and also appear to be more sensitive to thequality and format of the input data.7 Conclusions and Future WorkThe paper has described experiments with apply-ing three state-of-the-art part-of-speech taggers toAmharic, using three different tagsets.
All tag-gers showed worse performance than previouslyreported results for English.
The best accuracywas obtained using a Maximum Entropy approachwhen allowed to create its own folds: 90.1% on a30 tag tagset, and 94.6 resp.
94.5% on two reducedsets (11 resp.
10 tags), outperforming an HMM-based (TnT) and an SVM-based (SVMTool) tag-ger.
On predefined folds all taggers got compa-rable results (92.5-92.8% on the reduced sets and4-7% lower on the full tagset).
The SVM-taggerperforms slightly better than the others overall,since it has the best performance on unknownwords, which are four times as frequent in the200K words Amharic corpus used than in the (sixtimes larger) English Wall Street Journal corpus.TnT gave the best results for known words, buthad the worst performance on unknown words.In order to improve tagging accuracy, we willinvestigate including explicit morphological pro-cessing to treat unknown words, and combiningtaggers.
Judging from previous efforts on com-bining taggers (Section 3.3), it is far from certainthat the combination of taggers actually ends upproducing better results than the best individualtagger.
A pre-requisite for successful combinationis that the taggers are sufficiently dissimilar; theymust draw on different characteristics of the train-ing data and make different types of mistakes.The taggers described in this paper use no otherknowledge source than a tagged training corpus.In addition to incorporating (partial) morpholog-ical processing, performance could be increasedby including knowledge sources such as machinereadable dictionaries or lists of Amharic stemforms (Section 2.2).
Conversely, semi-supervisedor unsupervised learning for tagging clearly areinteresting alternatives to manually annotate andconstruct corpora for training taggers.
Sincethere are few computational resources availablefor Amharic, approaches as those briefly outlinedin Section 3.2 deserve to be explored.AcknowledgementsThe work was partially funded by Sida, the Swedish Inter-national Development Cooperation Agency through SPIDER(the Swedish Programme for ICT in Developing Regions).Thanks to Dr. Girma Demeke, Mesfin Getachew, and theELRC staff for their efforts on tagging the corpus, and toThorsten Brants for providing us with the TnT tagger.110ReferencesRachel V. Xavier Aires, Sandra M.
Alu?
?sio, Denise C. S.Kuhn, Marcio L. B. Andreeta, and Osvaldo N. Oliveira Jr.2000.
Combining classifiers to improve part of speechtagging: A case study for Brazilian Portuguese.
In 15thBrazilian Symposium on AI, pp.
227?236, Atibaia, Brazil.Nega Alemayehu and Peter Willett.
2002.
Stemming ofAmharic words for information retrieval.
Literary andLinguistic Computing, 17:1?17.Saba Amsalu and Dafydd Gibbon.
2005.
Finite state mor-phology of Amharic.
In 5th Recent Advances in NaturalLanguage Processing, pp.
47?51, Borovets, Bulgaria.Saba Amsalu and Girma A. Demeke.
2006.
Non-concatinative finite-state morphotactics of Amharic sim-ple verbs.
ELRC Working Papers, 2:304-325.Atelach Alemu Argaw and Lars Asker.
2007.
An Amharicstemmer: Reducing words to their citation forms.
Compu-tational Approaches to Semitic Languages, pp.
104?110,Prague, Czech Rep.Michele Banko and Robert C. Moore.
2004.
Part of speechtagging in context.
In 20th Int.
Conf.
on ComputationalLinguistics, pp.
556?561, Geneva, Switzerland.Roy Bar-Haim, Khalil Simaan, and Yoad Winter.
2008.
Part-of-speech tagging of modern Hebrew text.
Natural Lan-guage Engineering, 14:223?251.Abiyot Bayou.
2000.
Design and development of wordparser for Amharic language.
MSc Thesis, Addis AbabaUniversity, Ethiopia.Tesfaye Bayu.
2002.
Automatic morphological analyser:An experiment using unsupervised and autosegmental ap-proach.
MSc Thesis, Addis Ababa University, Ethiopia.Thorsten Brants.
2000.
TnT ?
a statistical part-of-speechtagger.
In 6th Conf.
Applied Natural Language Process-ing, pp.
224?231, Seattle, Wash.Eric Brill and Jun Wu.
1998.
Classifier combination for im-proved lexical disambiguation.
In 17th Int.
Conf.
on Com-putational Linguistics, pp.
191?195, Montreal, Canada.Eric Brill.
1995.
Transformation-based error-driven learningand Natural Language Processing: A case study in part ofspeech tagging.
Computational Linguistics, 21:543?565.CIA.
2009.
The World Factbook ?
Ethiopia.
The Central In-telligence Agency, Washington, DC.
[Updated 22/01/09.
]Michael Collins.
2002.
Discriminative training methods forhidden Markov models: Theory and experiments with per-ceptron algorithms.
In Empirical Methods in Natural Lan-guage Processing, pp.
1?8, Philadelphia, Penn.Girma A. Demeke and Mesfin Getachew.
2006.
Manual an-notation of Amharic news items with part-of-speech tagsand its challenges.
ELRC Working Papers, 2:1?17.Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004.
Au-tomatic tagging of Arabic text: From raw text to basephrase chunks.
In HLT Conf.
North American ACL,pp.
149?152, Boston, Mass.Thomas G. Dietterich.
1997.
Machine-learning research:Four current directions.
AI magazine, 18:97?136.Kevin Duh and Katrin Kirchhoff.
2005.
POS tagging of di-alectal Arabic: A minimally supervised approach.
Com-putational Approaches to Semitic Languages, pp.
55?62,Ann Arbor, Mich.Sisay Fissaha and Johann Haller.
2003.
Amharic verblexicon in the context of machine translation.
In 10thTraitement Automatique des Langues Naturelles, vol.
2,pp.
183?192, Batz-sur-Mer, France.Sisay Fissaha.
2005.
Part of speech tagging for Amharic us-ing conditional random fields.
Computational Approachesto Semitic Languages, pp.
47?54, Ann Arbor, Mich.Mesfin Getachew.
2001.
Automatic part of speech tag-ging for Amharic: An experiment using stochastic hid-den Markov model (HMM) approach.
MSc Thesis, AddisAbaba University, Ethiopia.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2004.
SVMTool: A gen-eral POS tagger generator based on support vector ma-chines.
In 4th Int.
Conf.
Language Resources and Eval-uation, pp.
168?176, Lisbon, Portugal.Sharon Goldwater and Thomas L. Griffiths.
2007.
A fullyBayesian approach to unsupervised part-of-speech tag-ging.
In 45th ACL, pp.
744?751, Prague, Czech Rep.Grover Hudson.
1999.
Linguistic analysis of the 1994Ethiopian census.
Northeast African Studies, 6:89?107.Saib Mansour.
2008.
Combining character and morphemebased models for part-of-speech tagging of Semitic lan-guages.
MSc Thesis, Technion, Haifa, Israel.Andrew Kachites McCallum.
2002.
MALLET: A machinelearning for language toolkit.
Webpage.Guy De Pauw, Gilles-Maurice de Schryver, and Peter W.Wagacha.
2006.
Data-driven part-of-speech tagging ofKiswahili.
In 9th Int.
Conf.
Text, Speech and Dialogue,pp.
197?204, Brno, Czech Rep.Adwait Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Empirical Methods in NaturalLanguage Processing, pp.
133?142, Philadelphia, Penn.Danny Shacham and Shuly Wintner.
2007.
Morphologicaldisambiguation of Hebrew: A case study in classifier com-bination.
In Empirical Methods in Natural Language Pro-cessing, pp.
439?447, Prague, Czech Rep.Drahomira?
Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Krbec,and Pavel Kve?ton?.
2007.
The best of two worlds: Co-operation of statistical and rule-based taggers for Czech.Balto-Slavonic Natural Language Processing, pp.
67?74.Prague, Czech Rep.Kristina Toutanova and Mark Johnson.
2007.
A BayesianLDA-based model for semi-supervised part-of-speech tag-ging.
In 21st Int.
Conf.
Advances in Neural InformationProcessing Systems, pp.
1521?1528, Vancover, B.C.Kristina Toutanova, Dan Klein, Christopher D. Manning, andYoram Singer.
2003.
Feature-rich part-of-speech taggingwith a cyclic dependency network.
In HLT Conf.
NorthAmerican ACL, pp.
173?180, Edmonton, Alberta.Atro Voutilainen.
1999.
An experiment on the upper boundof interjudge agreement: The case of tagging.
In 9th Eu-ropean ACL, pp.
204?208, Bergen, Norway.Daniel Yacob.
1997.
The System for Ethiopic Representa-tion in ASCII ?
1997 standard.
Webpage.111
