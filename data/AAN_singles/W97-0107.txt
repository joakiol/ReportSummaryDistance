Reestimation and Best-First Parsing Algorithmfor Probabilistic Dependency GrammarsSeungmi  Lee and  Key-Sun ChoiDept.
of Computer ScienceKorea Advanced Institute of Science and Technology373-1, Kusung-Dong, YuSung-Gu, Taejon, 305-701, KOREAe-malh {leesm, kschoi}~orld, kaist, ac.
krAbstractThis paper presents areesthnation algorithm and a best-first parsing (BFP) algorithmfor probabilistic dependency grummars (PDG).
The proposed reestimation algorithm isa variation of the inside-outside algorithm adapted to probabilistic dependency gram-mars.
The inside-outside algorithm is a probabilistic parameter reestimation algorithmfor phrase structure grammars inChomsky Normal Form (CNF).
Dependency grammarrepresents a entence structure as a set of dependency links between arbitrary two wordsin the sentence, and can not be reestimated bythe inside-outside algorithm directly.In this paper, non-constituent objects, complete-llnk and complete-sequence aredefined as basic units of dependency structure, and the probabilities of them are rees-timated.
The reestimation and BFP algorithms utilize CYK-style chart and the non-constituent objects as chart entries.
Both algoritbrn~ have O(n s) time complexities.1 Int roduct ionThere have been many efforts to induce grammars automatically from corpus by utilizingthe vast amount of corpora with various degrees of annotations.
Corpus-based, stochasticgrammar induction has many profitable advantages such as simple acquisition and exten-sion of linguistic knowledges, easy treatment of ambiguities by virtue of its innate scoringmechanism, and fail-soi~ reaction to ill-formed or extra-grammatical sentences.Most of corpus-based grammar inductions have concentrated onphrase structure gram?mars (Black, Lafferty, and Roukos, 1992, Lari and Young, 1990, Magerman, 1994).
Thetypical works on phrase structure grammar induction are as follows(Lari and Young, 1990,Carroll, 1992b): (1) generating all the possible rules, (2) reestimating the probabilitiesof rules using the inside-outside algorithm, and (3) finally finding a stable grammar byeliminating the rules which have probability values close to 0.
Generating all the rulesis done by restricting the number of nonterminals and/or the number of the right handside symbols in the rules and enumerating all the possible combinations.
Chen extractsrules by some heuristics and reestimates the probabilities of rules using the inside-outsidealgorithm (Chen, 1995).
The inside-outside algorithm learns a grammar by iteratively ad-justing the rule probabilities to minimize the training corpus entropy.
It is extensively usedas reestimation algorithm for phrase structure grammars.Most of the works on phrase structure grammar induction, however, have partiallysucceeded.
Estimating phrase structure grammars by minimizing the training corpus on-41tropy does not lead to the desired grammars which is consistent with human intuitions(de Marcken, 1995).
To increase the correctness of the learned grammar, Marcken pro-posed to include lexical information to the phrase structure grammar.
A recent rend ofparsing is also to include lexiccal information to increase the correctness (Magerman, 1994,Collir~, 1996).
This means that the lack of lexical information i  phrase structure gram-mar is a major weak point for syntactic disambiguation.
Besides the lack of lexical in-formation, the induction of phrase structure grnmmar may suffer from structural datasparseness with medium sized training corpus.
The structural data sparseness means thelack of information on the grammar ules.
An approach to increase the correctness ofgrammar induction is to learn a grammar from a tree-tagged corpus or bracketed corpus(Pereira nd Schabes, 1992, Black, Lafferty, and Roukos, 1992).
But the construction ofvast sized tree-corpus orbracketed corpus is very labour-intensive and manual constructionof such corpus may produce serious inconsistencies.
And the structural-data sparsenessproblem still remains.The problems of structural-data sparseness and lack of lexical information can be less-ened with PDG.
Dependency grammar defines a language as a set of dependency relationsbetween any two words.
The basic units of sentence structure in DG, the dependency re-lations are much simpler than the rules in phrase structure grnmmar.
So, the search spaceof dependency grammar may be smaller and the grammar induction may be less affectedby the structural-data sparseness.
Dependency grammar induction has been studied byCarroll (Carroll, 1992b, Carroll, 1992a).
In the works, however, the dependency grammarwas rather a restricted form of phrase structure grarnrnarss.
Accordingly, they extensivelyused the inside-outside algorithm to reestimate he grnmmnr and have the same problemof structural-data sparseness.In this paper, we propose a reestimation algorithm and a best-first parsing algorithmfor PDG.
The reestimation algorithm is a variation of the inside-outside algorithm adaptedto PDG.
The inside-outside algorithm is a probabilistic parameter reestimation algorithmfor phrase structure grammars in CNF and thus can not be directly used for reestimationof probabilistic dependency grammnrs.
We define non-constituent objects, complete- l inkand complete-sequence as basic units of dependency structure.
Both of reestimationalgorithm and best-first parsing algorithm utilize a CYK-style chart and the non-constituentobjects as chart entries.
Both algorithms have O(n s) time complexities.The rest of the paper is organized as follows.
Section 2 defines the basic units and de-scribes best-first parsing algorithm.
Section 3 describes the reestimation algorithm.
Section4 shows the experimental results of reestimation algorithm on Korean and finally section 5concludes this paper.2 PDG Best First Parsing AlgorithmDependency grammar describes a language with a set of head-dependent relations betweenany two words in the language.
Head-dependent relations represent specific relations uchas modiflee-modifier, p edicate-argument, tc.
In general, a functional role is assigned toa dependency link and specifies the syntactic/semantic relation between the head and thedependent.
However, in this paper, we use the minimal definition of dependency grnmmarwith head-dependent relations only.
In the future we will extend our dependency grammarinto one with functions of dependency lln~.42IIIIIIIIIIIIIIA dependency tree of a n-word sentence is always composed of n-1 dependency links.Every word in the sentence must have its head, except he word which is the head of thesentence.
In a dependency tree, crossing links are not allowed.Salesperson sold the dog buiscuitsFigure 1: Dependency tree: link representationsFigure 1 shows a dependency tree as a hierarchical representation a d a link represen-tation respectively.
In both, the word ~sold" is the head of the sentence.Here, we define the non-constituent objects, complete-link and complete-sequence whichare used in PDG reestimation and BFP algorithms.
A set of dependency links constructedfor word sequence wid is defined as complete-link ff the set satisfies following conditions:?
The set has exclusively (wi ~ wj) or (wi ~ wj).?
There is neither link-crossing nor llnk-cyele.?
The set is composed of j - i dependency links.?
Every inner word of wij must have its head and thus a link from the head.Complete-link has directionality.
It is determined by the direction of the outermost depen-dency relation.
If the complete-llnk has (wi --> wj), it is rightward, and if the complete-linkhas (wi ~-- wj), then it is leftward.
Basic complete-link is a dependency link betweenadjacent two words.Complete-sequence is defined as a sequence of null or more adjacent complete-lknks ofsame direction.
Basic complete-sequence is null sequence of complete-links which is definedon one word, the smallest word sequence.
The direction of complete-sequence is determinedby the direction of component complete-links.
If the complete-sequence is composed ofleftward complete-links, the complete-sequence is l ftward, and vice versa.Figure 2 shows abstract rightward complete-llnk for wi,j, rightward complete-sequencefor Wi,m, and leftward complete-sequence for Wrn+ld.
Double-slashed line means a complete-sequence.
Whatever the direction is, a complete-link for wij is always constructed witha dependency link between wi and wj, a rightward complete-sequence from i to ra, anda leftward complete-sequence from j to m + 1, for an m between i and j - 1.
Rightwardcomplete-sequence is always composed with a combination of a rightward complete-sequenceand a rightward complete-link.
On the contrary, leftward complete-sequence is alwayscomposed with a combination of a leftward complete-link and a leftward complete-sequence.These restrictions on composition of complete-sequence is for the ease of description ofalgorithm~ The basic complete-link and complete-sequence ar  also shown in the Figure 2.Following notations are used to represent the four kinds of objects for a word sequence wijand for an m from i to j-1.?
Lr(i,j): rightward complete.link for wid, i.e.
{(wi ---r wj), Sr(i,m), St(rn + 1.n)}?
Lt(i,j): leftward complete-link for wi,a, i.e.
{(wi ~ wj), Sr(i,m), St(m+ 1,n)}43IIW ~ / W i + l  .................................. Wm Wm?l .................................... Wj I ~ ..\] I l W i  Wi+l,, comprehend li kRightward Complete-link IWi Wk WmRightward Complete-sequenceWm+ 1 WI WjLeftward Complete-sequencetmi complet~ sequenceFigure 2: Abstract complete-link and complete-sequence?
St(i, j): rightward complete-sequence for wit/, i.e.
{St (i, m), Lr (m, j)}?
Sz(i,j): leftward complete-sequence for wid, i.e.
{Lt(~,m), Sdm, j)}To generalize the structure of a dependency tree, we assume that there are markingtags, BOS (Begin Of Sentence) before w, and EOS (End Of Sentence) after wn and thatthere are always the dependency links, (wBos --+ WEos) and (wk ~-- wEos) when wk isthe head word of the sentence.
Then, by definition: any dependency tree of a sentence,wi.n can be uniquely represented with either a Lr(BOS, EOS) or a Sl(1, EOS) as depictedin Figure 3.
This is because Lr (BOS, EOS) for any sentence is always composed of nullSr(BOS, BOS) and St(1,EOS).
The head of a dependency tree Wk can be found in therightmost Lt( k, EO S) of &(1, EO S).BOS Wl .... Wi ...... Wj .......... Wk(head) ... Wn EOSLr(BOS,EOS)Figure 3: Abstract dependency tree of a sentenceThe probability of each object is defined as follows.pCLrCi,j)) = PCwi --~ wj)p(Sr(i, rn))p(SlC m + l,j))pCZ,~(i, j)) = pC,v~ .
-  wj)pCS.
(i, m))pC& (m + 1, j))p(Sr(i, j)) = p(SrCi, m))PCLr(rn, j))p(St(i, j)) = p(Ll(i, m))p(St (m, j))iiIIIIIIIIIII44 !The m varies from i to j - 1 for Lz, Lr and St, and from i + 1 to j for Si.
The best L~and the best Lr always share the same m. This is because both are composed of the samesub-St and sub-St with rnaYimum probabilities.
Basis probabilities are as follows:p(L,.
( i ,  i + 1)) = p(wp(.Lt(i, i + 1)) = p(tO i ~-- W/+I)p(Sr(i,i + 1)) - -p(Zr( i , i  ?
1))p(Sl(i,i + 1)) = p(Lt(i,i + 1))p(St ( i ,  i ) )  = p(Sr ( i ,  i ) )  = 1Thus, the probability of a dependency tree is defined either by p(Lr(BOS, EOS)) or byp(st(1, sos).Leftward/Rightward Complete Linki Lr/LISr  - -  L ~" I iLeftward Complete Sequence Rightward Complete SequenceI I F  I ISI \[ \[ SlFigure 4: Best first parsing entriesThe PDG best-first parsing algorithm constructs the best dependency tree in bottom-up manner, with dynamic programrrdng method using CYK style chart.
It is based oncomplete-link and complete-sequence of non-constituent concept.
The parsing algorithmconstructs the complete-link.q and complete-sequences for ubstring, and merges incremen-tally the complete-links into larger complete-sequence and complete-sequences into largercomplete-link until the Lr(BOS, EOS) with maximum probability is constructed.Eisner (Eisner, 1996) proposed an O(n 3) parsing algorithm for PDG.
In their work,basic unit of chart entry is span which is also of non-constituent concept.
But, the spanslightly differs from our complete-sequence andcomplete-link.
When two adjacent spans aremerged into a larger span, some conditional tests must be satisfied.
In our work, best-firstparsing is done by inscribing the four entries with maximum probabilities, Lt (i, j), L~ (i, j),St(i,j), and Sr(i,j) to each chart positions in bottom-up/left-to-right manner without anyextra condition checking.Figure 4 depicts the possible combinations of chart entries into a larger Lr, Lt, St,and Sr each.
The sub-entries under the white-headed arrow and the sub-entries under theblack-headed arrow are merged into a larger entries.
The larger entries are inscribed intothe bold box.There is an exception for chart entries of n+l th  column.
In the n+l th  column, onlythe Lt(k, EOS) whose sub Sl is null can be inscribed.
This is because there can be onlyone head word for a tree structure.
If Lt(k, EOS) whose sub St is not null is inscribed intothe chart, the overall tree structure will have two or more heads.45i?
i!
!The best parse is maximum Lr(BOS, EOS) in the chart position (0,n + 1).
The bestparse can also be found by the maximum St(l, EOS) because the Lr(BOS, EOS) is alwayscomposed of Sr(BOS, BOS) and Sl(l, EOS).The chart size is n2+4n+3 for n word sentence.
For four items (Lr, Ll, St, and Sl) of 2each chart position, there can be maximally n searches.
Thus, the time complexity of thebest-fRrst parsing algorithm is O(nS).3 PDG Reestimation AlgorithmFor reestimation of dependency probabilities of PDG, eight kinds of chart entries are de-fined based on three factors: inside/outside, complete-link/complete-sequence, a d left-ward/rightward.
In following definitions, f~ is for inside probability and a is for outside prob-ability.
Superscripts represent whether the object is complete-link or complete-sequence, lfor complete-link and s for complete-sequence.
Subscripts of f~ and a are for the direction-ality, r for rightward and l for leftward.Complete-link Inside Probabilities: jSlr, ~8~ Inside probability of a complete-linkis the probability that word sequence wij will be generated when there is a dependency mrelation between wi and wj.
I= p( i ILr(i,j))j-I Im--if (i,j) = pCwi,#lLiCi, Im=iIn Figure 5, ~(i,j), the inside probability of Lr(i,j) is depicted.
In the left part of thel .
.
.
.
.
i - I  i m+l  ... m ... j j+ 1 ...... nII tTFigure 5: Rightward complete-link Inside probabilityfigure~ the gray partitions indicate all the possible constructions of St(/,m) and all thepossible constructions of Sl(m + 1, j) respectively.
Double-slashed links depict complete-sequences which compose the Lr together with the outermost dependency (wi ~ wj).
Theright part of the figure represents the chart.
The bold box is the position where the jS~is to be inscribed.
Inside probability of a complete-link is the sum of the probabilities ofall the possible constructions of the complete-link.
As explained in the previous section, a46Lr( i , j )  is composed of the dependency link between word i and word j (either (wi -+ wj) or(wi ~-- wj)), S,.
(i, m) and S l ( rn+l , j )  for an m from i to j-1.
Inside probability of Lt( i , j )  canbe computed the same as that of Lr (i, j) except for the direction of dependency link betweenzvi and wj.
The outermost dependency (zoi ~ wj) must be replaced with (wi ~ wj).
Lr andLt are not defined on word sequence of 1 length, zvi.
The unit probabilities for ~ and ~are as follows:~( i ,  i + ~) = p(~ ~ ~,~+~)l~(i,i + i) = p(wi ~ wi+~)~r(BOS, ~OS) = p(~,~)Any dependency tree of a sentence always has the dependency (z~BOS -+ Z~EOS) as theoutermost dependency.
So the ~r(BOS, EOS) is the same as the sentence probabilitywhich is the sum of probabilities of all the possible parses.Complete-sequence Inside Probabilities: ~r s, ~ Inside probability of complete-sequence is the probability that word sequence wio is generated when there is St(i, j) orS,(i,j)./~(i , j )  = p(wijl&(i,j))j-1= ~ ~gCi,=)~Cm,#)m= p(wial&Ci, j))J= ~7 ~ICi,~)~t(m,#)n~.=i+lIn Figure 6 and 7, the double-slashed link means complete-sequence, a sequence of null ormore adjacent complete-links of same direction.
A complete-sequence is composed of sub-"  l/I i1 ..... i-1 i ..... m ..... J j+l  ...... n ~ n i li vt iFigure 6: Rightward complete-sequence Inside probabilitycomplete-sequence and sub-complete-link.
Figure 6 depicts rightward complete-sequencefor an m. The value of m varies from i to j-1.
In Figure 7, St is composed of sub-L~ andsub-St.
The inside probability of complete-sequence is the sum of the probabilities that thecomplete-sequences are constructed with.
The basis for inside probabilities of complete-sequence are as follows.~r s(i, i) = ~ (i, i) = 1471 ... .
.
i - I  i .
.
.
.. m ..... J j+ l  .
.
.
.
.
.
n ~ - -,iFigure 7: Leftward complete-sequence Inside probability~( i , i  + 1) = p(L~(i,i + 1)) = p(w~ -+ w~+~)~t(i,i + 1) = p(Ll(i,i + 1)) = p(wl ~ w/+t)Because n+lth word, wEos can not be a dependent of any other word, l~r(k, EOS) or~rS(k, EOS) for k from 1 to n is not computed.
And because there can be only one headof a tree, wEos must be head of only one word.
Thus, in computation of flt(x, EOS) andfl~(x, EOS), only the Lts whose sub St is null are considered.Complete - l ink  Outs ide Probabi l i ty :  atr, a~ This is the probability of producing thewords before i and after j of a sente;ace while complete-link(i.j) generates wid.a~(i, j) = n(w~,~-~, n (i, j), ~j+~,,,)i= ~ o4(,,, j)~(v,i)tr=la~(i, j) = p(w~,~_x, L,(i, j),w~+~,.
)= ~o4( i ,h )~t~,h)Figure 8 and 9 depict the cases of Lr and Ll to become a substructure of larger Sr and12 ....... v ........... i ............ j j+l ....... nI~s a sr'La lFFigure 8: Rightward complete-link Outside probabilitySt respectively.
In Figure 8, the outside probability of Lr which is inscribed in the boldbox is computed by summing the products of the inside probabilities in the boxes underthe white-headed arrow and the outside probabilities in the boxes under the black-headed48t i ~ i I \[\] .
.
.
.
.
i j h .
.
.
.
.
n I. .
.
.
.
.
.
o?
.o .IiFigure 9: Leftward complete-link Outside probabilityarrow.
Likewise, in Figure 9, the outside probability of Lt in the bold box is computedby slimming all the products of the inside probabilities under the white-headed arrow andthe outside probabilities under the black-headed arrow each.
This is because, in parsing,the subentries under the white-headed arrows and Lr/Lz in the bold boxes are merged intolarger entries which are to be inscribed in the boxes under the black-headed arrows.
Basisprobability for complete-link outside probability is as follow.~(BOS, EOS) =a~(k, EOS) is always 0, for k = 1,n + I(EOS) because wEos can not be a dependent ofany other word.Complete-sequence Outside Probabilities: aSr, ~/ This is the probability of produc-ing word sequence wl,i-1 and Wj+l,n while words i through j construct a complete-sequence.a,~(i,j) = p(w1,~-l,Sr(ij),w#+1,.
)n= ~ ~(i,h)~h#,h)h=j+l+a~(i, h)l~(\] + 1,h)p(wi ~ Wh)+a~(~, h)Zt(i + 1, h)p(wi ~ ~h)In the above expression, The first term is for the construction of larger St(i, h) from thecombination of Sr(i,j) and its adjacent Lr(j: h).
The second term me~n~ the constructionof larger Lr(i,h) from the combination of Sr(i,j), St(j + 1,h), and the dependency linkfrom wi to wh.
The third term is for the larger Lt(i,h) from the combination of Sr(i,\]),Si(j + 1,h) and the dependency link from wh to wi.
The three terms in the expression aredepicted in Figure 10.a~(i, j)  = p(w~,i.~,Sdi, j) ,w,+~,.
)i -1= E ~tCv,#)~Cv,~)v----1+a~C~,j)f~(~,  - 1 )p (~ +- w~)a~ is the sum of all the probabilities that Sl is to become a subentry of larger entries: St,L~, and Lt.
The first term in the above expression is for the combination of St(v,j) fromLt (v, i) and St (i, j).
The second is for the construction of Lr (v, j )  from St(v, i - 1), St(i, \]),49L____.
J  ~ I1 ... .
.
i j h .. .
.
.
n. .
.
.
.
O , .
.o .
?
*  Ia $r 3frI I m I1 ..... i j j+ l  .. .
.
.
h ..... nrBi talrI '  I ~ I1 ..... i J j+ l  .. .
.
.
h .. .
.
.
n IaS irFigure 10: Rightward complete-sequence Outside probabilityand the dependency link from wv to wj.
The third term is for the construction of L~(v,j)from Sr(v,i - 1), St(i,j), and dependency relation from w i to w~.
The three cases aredepicted in Figure 11.
The basis probabilities for complete-sequence outside probabilitiesare as follows.~(BOS, ZOS) = 4(BOS, EOS) = ~?
(1, EOS) : 1The reesthnation algorithm computes the inside probabilities(#r ~, #~, #r', and #t) inscribingthem into the chart in bottom-up and left-to-right.
The outside probabilities(c~r, a~, ar s,and ~)  are computed and inscribed into the chart in top-down and right-to-left.Training The training process is as foflow;1.
Initialize the probabilities of dependency relations between all the possible word pairs.2.
Compute initial entropy.3.
Analyze the training corpus using the known probabilities,and recalculate the frequency of each dependency relation based on the analysis result.4.
Compute the new probabilities based on the newly counted frequencies.5.
Compute the new entropy.6.
Continue 3 through 5 until the new entropy ~ previous entropy.IIIIIIIIIII!IIIII!50 !II!IIII1 2 .....v-----1v i J j+l......n12 ..... v ..... i-I i ....... J j+l ...... n12 ..... v ....i.-I i ....... J j+l ...... nt~r ~t ras.~1,I.
iTct s.F igure 11: Leftward complete-sequence Outside probabi l i tyThe  above iterat ion is cont inued unti l  all the probabil i t ies are sett led down or the tra in ingcorpus entropy converges to the min imum.
The  new usage count of  a dependency  relat ionis calculated as follows.
In  the following expression, the O?~ is 1 if the dependency  relat ionis used in the given tree and 0 otherwise.c(wi --+ wj)  = ~p(tree\]wl,=)O~(wi ~ wj, tree, wl,,)tree_ 1p(wl,,~)_ 1p(~,~)1p(wl,.
)_ 1p(wl,~)_ 1p(wl,,~)_ 1p(wl,n)~ p(~ee, wl,.
)O=(w~  wj, tree, wl,.
)- -  ~p(tree, wl,n,w~ ~ wj)t ree- -  (ilJl,n, W,  ~ Wj )j-1- -  ~ p(w~,n,  L r ( i ,  j ) ,  S~ (i, m), &(~ + l ,  j ) )rct=ij--1- -  ~ p(~,~,~_~, w~,,~, w,~+~,j,wj+~,,~, L,.
(i,j), S,.
(i, ~), &(m + 1,j))j -1m=ip(S~(i,m), ,~(rn + l:\])\[L~(i,j))51v(~,~ls~(i, m))v(w~+~jlsz(m + l,j))_ 1 j-z1 t- v(~,~la~(i,j)NCi, j)Similarly, the usage count of (w~ ~ w#), c(wi ~- w#) is ~a~( i , j )g ( i , j ) .a ~Chart has n2+4n+3 number of boxes.
The reestimation algorithm computes eight itemsfor each chart box and the computation ofeach item needs maximally n number of produc-tions and snmrnations respectively.
So the time complexity of the algorithm is O(nS).The algorithm can be used for class-based (or tag-based) dependency grammar.
Withthe concept of word class/tag, the complexity is affected by the class/tag size due to theclass/tag ambiguities of each word.
In the worst case, the time needed is 8 x t 2 x ns?4~ +sn,so the complexity will be O(~n a) with respect o t, the number of classes and n, the lengthof input string.4 Exper imenta l  resu l t s  on  KoreanWe performed experiment of the reestimation algorithm on Korean language.
Korean is apartially ordered language in which the head words are always placed to the right of theirdependent words.
Under such restriction on word order, an abstract dependency structurefor Korean is depicted in Figure 12.
Thus, in Korean, both of ` sr(i,j) and Lr(i,j) aremeaningless and not constructed.
Only ,St, Ll and null ` sr(i:i) are considered.
Ll(i,j) isalways composed with the combination of null Sr(i,i) and St(i + 1,j).Wi Wi+l  ........................ W jWi Wi+ 1it comp~e~d linkWi ............ Wk ........... W jI Icompleted sequence(null sequence)Figure 12: Abstract complete-link and complete-sequence for KoreanKorean sentence is spaced by "word-phrases" which is a sequence of content word andits functional words.
In this experiment, the final part of speech(POS) of a "word-phrase" isselected as the representative POS and the inter-word-phrase d pendencies are reestimated.We used 54 POS set.
The initial probabilities of all the possible dependencies were set equal.The experiment was performed on three kinds of training and test sets extracted from52IIlIIIIi!ilIl!liiTable 1: Train and test entropiesiteration I Set-1 (14,427 words) !
Set-2 (40,818 words) Set-3 (336,824 words)12341314151617184.4394222.2735622.1949522.1670792.1396252.1393982.1392252.1390932.1389892.1389064.4864442.3569332.2836012.2581572.2355722.2354142.2352972.2352074.4867172.3572492.2895912.2654822.2433962.2432322.2431092.2430151 Set-1 (2;170 words) i Set-2 (4,662 words) Set-3 ~i5,903 words).... test entropy I 2.476484 ' 2.1505'53 2.251653KAIST  corpora 1.
The convergence of entropies(bits/word) through training iterations andthe test corpus entropy are shown in the table 1.
Set-1 is extracted from information andcommunication related texts; the train corpus of set-1 is 1,124 sentences (14,427 words)and the test corpus is 162 sentences (2,170 words).
Set-2 is extracted from economy relatedtexts; the train corpus is 3,499 sentences (40,818 words) and the test corpus is 409 sentences(4,662 words).
Set-3 is not restricted to a domain; train corpus is 29,169 sentences (336,824words) and the test corpus is 1917 sentences (15,903 words).The experiment result shows that the proposed reestimation algorithm converges to a(local) minimum entropy.
It shows also that the train and test entropies are not affectedmuch by the domain or by the size of training corpus.
It may be because the reestimationwas done on inter-POS dependencies, which is relatively simple.
If the reestimation wouldbe done on the dependencies between POS sequences for ~word-phrase" or on the depen-dencies between lexical entities~ the entropies may be affected much by the domain and thesize of corpus.
We plan such experiments.Below we show the parsing results of two example Korean sentences.
We used theproposed best-first parsing algorithm to find the most probable parse of each sentence.
Theinter-word-phrase probabilities used for parsing are the reestimated ones for the trainingset-3.
To the right of each Korean word-phrase, the meaning of it in English is given in thesquare brackets.
In the parse representations, each individual inter-word-phrase probabilityis given to the right of the dependent word-phrase.
The probability of each parse is theproduct of all the inter-word-phrase probabilities in the parse and is given on the top ofeach parse.Input Sentence:1 KAIST(Korean Advanced Institute of Science and Technology) corpora have been under constructionsince 1994 under Korean Information Base System construction (KIBS) project.
It is supported by theMinistry of Culture and Sports, Korea.
The KAIST corpora consist of raw text corpus(45,000.000 word-phrases), POS tagged corpus(6,750,000 word-phrases), and tree tagged corpus(30,000 sentences) at present.For our experiment, we extracted each train/tesz set from the POS tagged corpus.531.
~!~el-o~\]-,~ - \[the laboratory\]2.
~71-:21- \[an electric generator\]3.
"~ l -~-~ \[equipped with\]4. oA-~4.
\[is\]Parse: 7.893022e-04(EOS(4.
~-r..1- \[is\] 9.996915e-01(Z.
~4"~-~\]'-'~" \[the laboratory\](3.
~\] .~.o~ \[equipped with\](2.
~'~Tl-yl- \[an electric generator\]7.129873e-02)1.150859e-019.622178e-02))))Input Sentence:1.
~z  \[tunnel\]2.
~--~- \[the front of\]s. ~I~-~ \[passing\]4.
~ \[when\]5.
~'xFT\] \[suddenly\]6.
~--71- \[the car\]7.
~- - ,~-~ \[stopped\]Parse: 3.293545e-06(EOS(7.
~--~-ru~ \[stopped\] 9.996915e-01(4.
--~-~ \[when\] S.S28747e-02(3.
~\]',.-},-e. \[passing\] 9.044279e-01(2. o~_.~.
\[the front of\] 1.785020e-01(1.
'~l~ \[tunnel\] 9.537951e-02))))(5.
~-x\]-y\] \[suddenly\] 5.879496e-02)(6.
~..-7}- \[the car\] 9.504489e-02)))5 Conclusion and Further WorksIn this paper we have proposed a reestimation algorithm and a best-first parsing algorithmfor probabilistic dependency grammars(PDG).
The reestimation algorithm is a variation ofthe inside-outside algorithm adapted to PDG.
In both of the reestimation algorithm andthe parsing algorithm, the non-constituent objects, complete-link and complete-sequence,are defined as basic units for dependency structures and a CYK-style chart is utilized.Both algorithms have O(n 3) time complexities and can be used for corpus-based, stochasticII|iI!IIIIIIIII!Ii54 IPDG induction.
By experiment on Korean: we have shown that the reestimation algorithmconverges to a local minimum and constitute a stable grammar.Compared to phrase structure grammars, PDG can be a useful and practical schemefor parsing model and language model.
It is because dependency tree is much simplerand easily understood than the structure constructed by the phrase structure grammars.Besides the search space of the grammar may be smaller and the effect of structural datasparseness may be less.
This also means that the reestimation algorithm for PDG canconverge with smaller training corpus.
We are planning to evaluate the parsing modelbased on the reestimated PDG and the PDG-based language model.ReferencesBlack, E., J. Lafferty, and S. Roukos.
1992.
"Development and evaluation of a broad-coverage probabilistic grammar of English-language computer manuals".
In 30th AnnualMeeting off the Association .for Computatinal Linguistics, pages 185-192.Carroll, G. 1992a.
"Learning probabilistic dependency grammars from labeled text".
InWorking Notes, Fall Symposium Series, AAAI, pages 25-31.Carroll, G. 1992b.
"Two Experiments on Learning Probabilistic Dependency Grammarsfrom Corpora".
Technical Report CS-92-16, Brown University.Chen, S.F.
1995.
"Bayesian grammar induction for language modeling".
In 33rd AnnealMeeting of the Association .for Computatinal Linguistics, pages 228-235.Collins, Michael John.
1996.
"A New Statistical Parser Based on Bigram Lexical Depen-dencies".
In COLING-96.de Marcken, C. 1995.
"Lexical heads, phrase structure and the induction of grammar".
InThird Workshop on Very Large Corpora.Eisner, Jason M. 1996.
"Three New Probabilistic Models for Dependency Parsing: AnExploration".
In COLING-96, pages 340-345.Lari, K. and S.J.
Young.
1990.
"The estimation of stochastic context-free grammars usingthe inside-outside algorithm".
Computer Speech and Language, 4:35--56.Magerman, David M. 1994.
"Natural Language Parsing as Statistical Pattern Recognition".Ph.D.
thesis, Stanford University.Pereira, F. and Y. Schabes.
1992.
"Inside-outside reestimation from partially bracketedcorpora".
In 50th Annual Meeting off the Association for Computational Linguistics,pages 128-135.55
