Language Engineering and the Pathway to Healthcare: A user-oriented viewHarold SomersSchool of InformaticsUniversity of ManchesterPO Box 88Manchester M61 0QD, EnglandHarold.Somers@manchester.ac.ukAbstractThis position paper looks critically at anumber of aspects of current research intospoken language translation (SLT) in themedical domain.
We first discuss the userprofile for medical SLT, criticizing de-signs which assume that the doctor willnecessarily need or want to control thetechnology.
If patients are to be users onan equal standing, more attention must bepaid to usability issues.
We focus brieflyon the issue of feedback in SLT systems,pointing out the difficulties of relying ontext-based paraphrases.
We consider thedelicate issue of evaluating medical SLTsystems, noting that some of the standardand much-used evaluation techniques forall aspects of the SLT chain might not besuitable for use with real users, even ifthey are role-playing.
Finally, we discussthe idea that the ?pathway to healthcare?involves much more than a face-to-face in-terview with a medical professional, andthat different technologies including butnot restricted to SLT will be appropriatealong this pathway.1 IntroductionThe doctor?patient consultation is a central elementof the ?pathway to healthcare?, and with languageproblems recognised as the single most significantbarrier on this pathway, spoken-language translation(SLT) of doctor?patient dialogues is an obvious andtimely and attractive application of language tech-nology.
As Bouillon et al (2005) state, the taskis both useful and manageable, particularly as inter-actions are highly constrained, and the domain canbe divided into smaller domains based on symptomtypes.
In this position paper, we wish to discuss anumber of aspects of this research area, and suggestthat we should broaden our horizons to look beyondthe central doctor?patient consultation to considerthe variety of interactions on the pathway to health-care, and beyond the confines of SLT as an appropri-ate technology for patient?provider communication.In particular we want to stress the importance ofthe users ?
both practitioners and patients ?
in thedesign, especially considering computer- and con-ventional literacy.
We will argue that the pathway tohealthcare involves a range of communicative activ-ities requiring different language skills and implyingdifferent technologies, not restricted to SLT.
We willcomment on the different situations which have beentargeted by research in this field so far, and the im-pact of different target languages on research, andhow the differing avilability of resources and soft-ware influences research.
We also need to considermore carefully the design of the feedback and verifi-cation elements of systems, and the need for realisticevaluations.2 Who are the users?We start by looking at the assumed profile of usersof medical SLT systems.
Systems that have beendeveloped so far can be divided into those for use inthe doctors office ?
notably, MedSLT (Rayner andBouillon, 2002), CCLINC (Lee et al, 2002), and(honourable mention) the early work done at CMU(Tomita et al, 1988)1 ?
and those for use for firstcontact with medical professionals ?in the field?, de-veloped under DARPA?s CAST programme:2 MAS-TOR (Zhou et al, 2004), Speechalator (Waibel etal., 2003), Transonics (Narayanan et al, 2004) andSRI?s system (Precoda et al, 2004).
This distinctionmainly motivates differences in hardware, overalldesign, and coverage, but there may be other moresubtle differences that result especially from the sit-uation in which it was envisaged that the CAST sys-tems would be used.Some descriptions of the systems talk of ?doc-tors?
and ?patients?
though others do use more in-clusive terms such as ?medical professional?.
A sig-nificant common factor in the descriptions of thesystems seems to be that it is the doctor who con-trols the device.
This may be because it can onlyhandle one-way translation, as is the case of Med-SLT, ?.
.
.
the dialogue can be mostly initiated by thedoctor, with the patient giving only non-verbal re-sponses?
(Bouillon et al, 2005), or may be an ex-plicit design decision:There is, however, an assymmetry in thedialogue management in control, given thedesire for the English-speaking doctor tobe in control of the device and the primary?director?
of the dialog.
(Ettelaie et al,2005, 89) [emphasis added]It is understandable that as a regular user, themedical professional may eventually have more fa-miliarity with the system, but this should be re-flected in there being different user-interfaces (seeSomers and Lovel 2003).
We find regrettable how-ever the assumption that ?the English speaker [.
.
.
]is expected to have greater technological familiar-ity?
(Precoda et al, 2004, 9) or thatthe medical care-giver will maintain theinitiative in the dialogue, will have soleaccess to the controls and display of thetranslation device, and will operate the1We give here one indicative reference for each system.2Formerly known as Babylon.
See www.darpa.mil/ipto/ pro-grams/cast/.push-to-talk controls for both him or her-self and the [P]ersian patient.
(Narayananet al, 2004, 101)In fact, although the early use of computers indoctor?patient consultations was seen as a threat,more recently the help of computers to increasecommunication and rapport has begun to be recog-nised (Mitchell and Sullivan, 2001).
This may be atthe expense of patient-initiated activities however,and many practitioners are suspicious of the nega-tive impact of technology on relationships with pa-tients, especially inasmuch as it increases the per-ceived power imbalance in the relationship.Figure 1, a snapshot from Transonics demo,3leaves in no doubt who is in control.Figure 1: Snapshot from Transonics?
demo movie.The patient is not even allowed to see the screen!Equipment whose use and ?ownership?
can beequally shared between the participants goes someway to redressing the perceived power-balance inthe consultation.
We have evidence of this effect inongoing experiments comparing (non-speech) com-munication aids on laptops and tablet PCs: with thelaptop, controlled by a mouse or mouse-pad, thepractitioner tends to take the initiative, while withthe tablet, which comes with a stylus, the patienttakes the lead.
Bouillon et al (2005) comment that?patients [.
.
. ]
will in general have had no previ-ous exposure to speech recognition technology, andmay be reluctant to try it.?
On the other hand, pa-tients also have suffered from failed consultations3http://sail.usc.edu/transonics/demo/transedit02lr.movwhich break down through inability to communi-cate, and in our experience are pleased to be in-volved in experiments to find alternatives.
In ourview, one should not underestimate patients?
adapt-ability, or their potential as users of technology onan equal status with the practitioners.This being the case, we feel that some effort needsto be devoted to usability issues.
We will return tothis below, but note that text-based interfaces are notappropriate for users with limited literacy (whichmay be due to low levels of education, visual im-pairment, or indeed the lack of a written standard forthe language).
Use of images and icons also needsto be evaluated for appropriateness, an issue not ad-dressed in any of the reports on research in medicalSLT that we have read.
For example, Bouillon et al(2005) show a screenshot which includes the graphicreproduced in Figure 2.
The text suggests that theuser (i.e.
the doctor?)
can click on the picture toset the topic domain.
It is not clear why a graphic ismore suitable for the doctor-user than a drop-downtext menu; there is no mention of whether the patientis encouraged to use the diagram, but if so one won-ders for what purpose, and if it is the best choice ofgraphic.
Research (e.g.
by Costantini et al 2002)suggests that multimodal interfaces are superior tospeech-only systems, so there is some scope for ex-ploration here.Figure 2: Graphic taken from screenshot in Bouillonet al (2005)Incorporating more symbolic graphics into an in-terface is an area of complexity, as Johnson et al(2006) report.
Iconic text-free symbols, for exam-ple to represent ?please repeat?, or ?next question?,or abstract concepts such as ?very?
are not alwaysas instantly understandable as some designers think.Considering the use of symbols from AAC (augmen-tative and alternative communication) designed forspeech-impaired disabled users by patients with lim-ited English, we noticed that AAC symbol sets havea systematic iconicity that regular users learn, butwhich may be opaque to first-time (or one-time) un-trained users (Johnson, 2004).3 Feedback and verificationTranslation accuracy is of course crucial in the med-ical domain, and sometimes problematic even withhuman interpreters, if not trained properly (Flores,2005).
Both speech recognition (SR) and translationare potential sources of error in the SLT chain, so itis normal and necessary to incorporate in SLT sys-tems the provision of feedback and verification forusers.
The standard method for SR is textual repre-sentation, often in the form of a list of choices, forexample as in Figure 3, from Precoda et al (2004).Figure 3: Choice of recognizer outputs, from Pre-coda et al (2004:10)For translation output, some form of paraphraseor back-translation is offered, often facilitated by theFigure 4: Choice of recognizer outputs, from Precoda et al (2004:10)particular design of the machine translation (MT)component (e.g.
use of an interlingua representa-tion, as in MedSLT, Speechalator).
In the Transonicssystem, the SR accuracy is automatically assessedby the MT component: SR output that conforms tothe expectations of the MT systems grammar is pre-ferred.For the literate English-speaking user, this ap-proach seems reasonable, although an interface suchas the one shown in Figure 4, detailing the output ofthe parse must be of limited utility to a doctor withno linguistics training, and we must assume that theprototype is designed more for the developers?
ben-efit than for the end-users.For the patient with limited or no English, the is-sue of feedback and verification is much more diffi-cult.
As mentioned above, and reiterated by Precodaet al (2004), the user may not be (wholly) liter-ate, or indeed the language (or dialect) may not havean established writing system.
For some languages,displaying text in the native orthography may be anadded burden.
Figure 5 shows Speechalator?s Ara-bic input screen (Waibel et al, 2003).
It is acknowl-edged that the users must ?know something aboutthe operation of the machine?, and although it isstated that the display uses the writing system ofthe language to be recognised, in the illustration theArabic is shown in transcription.Another issue concerns the ease with which a layuser can make any sense of a task in which theyare asked to judge a number of paraphrases, someungrammatical.
This is an intellectual task that isdifficult for someone with limited education or noexperience of linguistic ?games?.
For example, forthis reason we have rejected the use of semanticallyunpredictable sentences (SUS) (Beno?
?t et al, 1996)in our attempts to evaluate Somali speech synthesis(Somers et al, 2006).
This leads us to a considera-tion of how medical SLT can best be evaluated.4 EvaluationMT evaluation is notoriously difficult, and SLT eval-uation even more so.
Most researchers agree thatmeasures of translation fidelity in comparison with agold-standard translation, as seen in text MT evalu-ation, are largely irrelevant: a task-based evaluationis more appropriate.
In the case of medical SLT thispresumably means simulating the typical situationthat the technology will be used in, which involvespatients with medical problems seeking assistance.Since SLT is a pipeline technology, the individ-ual components could be evaluated separately, andindeed the effects of the contributing technologiesassessed (cf.
Somers and Sugita 2003).
Once again,literacy issues will cloud any evaluation of speechrecognition accuracy that relies on its speech-to-textfunction, and evaluation of speech synthesis mustsimulate a realistic task (cf.
comments on SUS,above).Evaluations that have been reported suggest us-ing real medical professionals and actors playingthe part of patients: this scenario is well establishedin the medical world, where ?standardized patients?
(SPs) ?
actors trained to behave like patients ?
havebeen used since the 1960s.
One problem with SPsfor systems handling ?low density?
languages likePersian, Pashto and so on, is the need for the vol-Figure 5: Speechalator?s Arabic input screen(Waibel et al, 2003, 372)unteers to understand English so that they can betrained as an SP, in conflict with the need for themto not understand English in order to give the sys-tem a realistic test.
Ettelaie et al (2005) for exam-ple report that their evaluation was somewhat com-promised by the fact that two of their patient role-players did speak some English, while a third partic-ipant did not adequately understand what they weresupposed to do.Another problem is that there is no obvious base-line against which evaluations can be assessed.
Onecould set up ?with and without?
trials, and mea-sure how much and how accurately information waselicited in either mode.
But this would be a waste ofeffort: it is widely, although anecdotally, reportedthat when patients with limited English arrive fora consultation where no provision for interpretationhas been made, the consultations simply halt.
It isalso reported, as already mentioned, that human in-terpreters are not 100% reliable (Flores, 2005).
Of-ten, an untrained interpreter is used, whether a fam-ily member or friend that the patient has broughtwith them, or even another health-seeker who hap-pens to be sitting in the waiting room.
The potentialfor an unreliably interpreted consultation (or worse)is massive.Ettelaie et al (2005) mention a number of metricsthat were used in their evaluation, but unfortunatelydo not have space for a full discussion.
The principlemetric is task completion, but they also mention anevaluation of a scripted dialogue, with translationsevaluated against model translations using a modi-fied version of BLEU, and SR evaluated with word-error rate.
These do not seem to me to be extremelyvaluable evaluation techniques.Starlander et al (2005) report an evaluation inwhich the translations were judged for acceptabilityby native speakers.
Given the goal-based nature ofthe task, rating for intelligibility rather than accept-ability might have been more appropriate, though itis widely understood that the two features are closelyrelated.
On the positive side, Starlander et al usedonly a three-point rating (?good?, ?ok?
or ?bad?
):evaluations of other target languages might be sub-ject to the problem, reported by Johnson et al (inprep.)
and by ADD REF that rating scales are highlyculture-dependent, so that for example Somali par-ticipants in an evaluation of the suitability of sym-bols in doctor?patient communication mostly usedonly points 1 and 7 of a 7-point scale.Another evaluation method4 is to assess the num-ber and type of translation or interpretation errorsmade, including whether there was any potential oractual error of clinical consequence.As Starlander et al (2005) say:In the long-term, the real question wewould like to answer when evaluating theprototype is whether this system is practi-cally useful for doctorsto which we can only add, reiterating our commentsin Section 2, ?.
.
.
and for patients?.5 The Pathway to HealthcareLet us move on finally to a more wide-ranging is-sue.
?Medical SLT?
is often assumed to focus ondoctorpatient consultations or, as we have seen in4Thanks to the anonymous reviewer for pointing this out.the case of systems developed under the CAST pro-gramme, interactions between medical professionalsand affected persons in the field.
Away from thatscenario, although it is natural to think of ?going tothe doctor?
as involving chiefly an interview with adoctor, and while everything in medical practice ar-guably derives from this consultation, the pathwayto healthcare in normal circumstances involves sev-eral other processes, all of which involve language-based encounters that present a barrier to patientswith limited English.
None of the medical SLT sys-tems that have been reported in the literature addressthis variety of scenarios, although the website for thePhraselator (which is of course not an SLT system assuch) does list a number of different scenes, such asthe front desk, labour ward and so on.In this section, we would like to survey the path-way to healthcare, and note the range of languagetechnologies ?
not always speech or translation ori-ented ?
that might be appropriate at any point.
Thepurpose of this is both to make a plea to widen ourvision of what ?medical SLT?
covers, but also tonote that SLT is not necessarily the most appropri-ate technology in every case.The pathway might begin with a person sus-pecting that there may be something wrong withthem.
Many people nowadays would in this situa-tion first try to find out something about their con-dition on their own, typically on the Web, thoughof course there is still a major ?digital divide?
forracial and ethnic minorities, and the poor, partlydue to the langauge barriers this research is address-ing.
If you need this information in your own lan-guage, and you have limited literacy skills, tech-nologies implied are multilingual information ex-traction.
MT perhaps coupled with text simplifica-tion, with synthesized speech output.
For specificconditions which may be treated at specialist clin-ics (our own experience is based on Somalis withrespiratory difficulties) it may be possible to iden-tify a series of frequently asked questions and setup a pre-consultation computer-mediated help-deskand interview (cf.
Osman et al 1994).
See Somersand Lovel (2003) for more details.Having decided that a visit to the doctor is indi-cated, the next step is to make an appointment.
Ap-pointment scheduling is the classical application ofSLT, as seen in most of the early work in the field,and is a typical case of a task-oriented cooperativedialogue.
Note that the ?practitioner?
?
the recep-tionist in the clinic ?
does not necessarily have anymedical expertise, nor possibly the high level of ed-ucation and openness to new technology that is oftenassumed in the literature on medical SLT which talksof the ?doctor?
controlling the device.If this is the patient?s first encounter with this par-ticular healthcare institution, there may be a processof gathering details of the patient?s medivcal his-tory and other details, done separately from themain doctor?patient consultation, to save the doc-tor?s time.
This might be a suitable application forcomputer-based interviewing (cf.
Bachman 2003).The next step might be the doctor?patient consul-tation, which has been the focus of much attention.For no doubt practical purposes, some medical SLTdevelopers have assumed that the patients role in thiscan be reduced to simple responses involving yes/noresponses, gestures and perhaps a limited vocabu-lary of simple answers at the limit.
This view un-fortunately ignores current clinical theory.
Patient-centred medicine (cf.
Stewart et al 2003) is widelypromoted nowadays.
The session will see the doctoreliciting information in order to make a diagnosis asforeseen, but also explaining the condition and thetreatment, and exploring the patients feelings aboutthe situation.
While it may be unrealistic at presentto envisage fully effective support for all these as-pects of the doctorpatient consultation, we feel thatits purpose should be explicitly appreciated, and thelimitations of current technology in this respect ac-knowledged.After the initial consultation, the next step mayinvolve a trip to the pharmacist to get some drugs orequipment.
Apart from the human interaction, thedrugs (or whatever) will include written instructionsand information: frequency and amount of use, con-traindications, warnings and so on.
This is an ob-vious application for controlled language MT: drugdose instructions are of the same order of complexityas weather bulletins.
For non-literate patients, ?talk-ing pill boxes?
are already available:5 why can?tthey talk in a variety of languages?Another outcome might involve another practi-tioner ?
a nurse or a therapist ?
and a series of meet-5Marketed by MedivoxRx.
See Orlovsky (2005).ings where the condition may be treated or managed.Apart from more scheduling, this will almost cer-tainly involve explanations and demonstrations bythe practitioner, and typically also elicitation of fur-ther information from the patient.
Hospital treat-ment would involve interaction with a wide rangeof staff, again not all medical experts.
If a commu-nication device is to be used, it makes more sensefor it to be under the control and ?ownership?
of theperson who is going to be using it regularly: the pa-tient.6 ConclusionSome of the comments made in this position papermay seem critical, but it has not been my intentionto be negative about the field.6 It has been my inten-tion in this paper to draw attention to the followingaspects of medical SLT which I believe so far havebeen somewhat neglected:?
What is the ideal user profile for medical SLT?Should the doctor control the system, or couldit be seen as a shared resource??
If the patient is also a user, devices need to bemore user-friendly, taking into account culturaldifferences, and problems of low literacy.?
This particularly applies to feedback and veri-fication modules in the system.?
Evaluation should focus on the ability of thetechnology to aid the completion of the task,from the perspective of both the practitionerand the patient.?
Evaluation methods should not involve partici-pants in meaningless or incomprehensible tasks(such as rating nonsensical output), nor rely onskills (such as literacy) that they may lack.?
The pathway to healthcare involves more thanthe one-way doctor?patient dialogues coveredby most systems.
A wide range of technologiescan be brought to bear on the problem.6In particular, it should perhaps be acknowledged that interms of practical accomplishment we have yet to match oth-ers in the field.AcknowledgmentsI would like to acknowledge the contribution to thispaper of my colleagues on the CANES project, AnnCaress, Gareth Evans, Marianne Johnson, HermioneLovel, Zeinab Mohamed.
Some of the work re-ported here is funded by the Economic and SocialResearch Council (ESRC), project reference RES-000-23-0610.
Thanks also to the anonymous refer-ees for their very useful comments.ReferencesBachman, J.W.
2003.
?The patient-computer interview: aneglected tool that can aid the clinician.?
Mayo ClinicProceedings, 78:67?78.Beno?
?t, Christian, Martine Grice and Vale?rie Hazan.1996.
?The SUS test: A method for the assessment oftext-to-speech synthesis intelligibility using Semanti-cally Unpredictable Sentences?.
Speech Communica-tion, 18:381?392.Bouillon, Pierrette, Manny Rayner, NikosChatzichrisafis, Beth Ann Hockey, MarianneSantaholma, Marianne Starlander, Yukie Nakao,Kyoko Kanzaki and Hitoshi Isahara.
2005.
?A genericmulti-lingual open source platform for limited-domain medical speech translation?.
In Proceedingsof the Tenth Conference on European Associationof Machine Translation, Budapest, Hungary, pp.CHECKCostantini, Erica, Fabio Pianesi and Susanne Burger.2002.
?The added value of multimodality in the NES-POLE!
speech-to-speech translation system: an exper-imental study?.
In Fourth IEEE International Confer-ence on Multimodal Interfaces (ICMI?02), Pittsburgh,PA, pp.
235?240.Ettelaie, Emil, Sudeep Gandhe, Panayiotis Georgiou,Robert Belvin, Kevin Knight, Daniel Marcu, ShrikanthNarayanan and D. Traum.
2005.
?Transonics: A practi-cal speech-to-speech translator for English-Farsi med-ical dialogues?.
In 43rd Annual Meeting of the Asso-ciation for Computational Linguistics: ACL-05 Inter-active Poster and Demonstration Sessions, Ann Arbor,MI, pp.
89?92.Flores, Glenn.
2005.
?The impact of medical interpreterservices on the quality of health care: a systematic re-view?.
Medical Care Research and Review, 62:255?299.Johnson, M.J. 2004.
?What can we learn from drawingparallels between people who use AAC and peoplewhose first language is not English??
CommunicationMatters, 18(2):15?17.Johnson, M.J., D.G.
Evans and Z. Mohamed.
2006.
?Apilot study to investigate alternative communicationstrategies in provider-patient interaction with Somalirefugees?.
In Current Perspectives in Healthcare Com-puting Conference, Harrogate, England, pp.
97?106.Johnson, M.J., G. Evans, Z. Mohamed and H. Somers (inprep.)
An investigation into the perception of symbolsby UK-based Somalis and English-speaking nursingstudents using a variety of symbol assessment tech-niques.Lee, Young-Suk, Daniel J. Sinder and Clifford J. Wein-stein.
2002.
?Interlingua-based English?Korean two-way speech translation of doctor?patient dialogueswith CCLINC?.
Machine Translation, 17:213?243.Mitchell, E. and F. Sullivan.
2001.
?A descriptive feast butan evaluative famine: systematic review of publishedarticles on primary care computing during 1980-97?.British Medical Journal, 322:279?282.Narayanan, S., S. Ananthakrishnan, R. Belvin, E. Et-telaie, S. Gandhe, S. Ganjavi, P. G. Georgiou, C.M.
Hein, S. Kadambe, K. Knight, D. Marcu, H. E.Neely, N. Srinivasamurthy, D. Traum, and D. Wang.2004.
?The Transonics spoken dialogue translator: anaid for English-Persian doctor-patient interviews?.
InTimothy Bickmore (ed.)
Dialogue Systems for HealthCommunication: Papers from the 2004 Fall Sympo-sium, American Association for Artificial Intelligence,Menlo Park, California, pp.
97?103.Orlovsky, Christina.
2005.
?Talking pill bot-tles let medications speak for them-selves?.
NurseZone.com (online magazine),www.nursezone.com/Job/DevicesandTechnology.asp?articleID=14396.
Accessed 15 March 2006.Osman, L., M. Abdalla, J. Beattie, S. Ross, I. Russell,J.
friend, J. Legge and J. Douglas.
1994.
?Reducinghospital admissions through computer supported edu-cation for asthma patients?.
British Medical Journal,308:568?571.Precoda, Kristin, Horacio Franco, Ascander Dost,Michael Frandsen, John Fry, Andreas Kathol, ColleenRichey, Susanne Riehemann, Dimitra Vergyri, JingZheng and Christopher Culy.
2004.
?Limited-domainspeech-to-speech translation between English andPashto?.
In HLT-NAACL 2004, Human LanguageTechnology Conference of the North American Chap-ter of the Association for Computational Linguistics:Demonstrations, pp.
9?12.Rayner, Manny and Pierrette Bouillon.
2002.
?A flexiblespeech to speech phrasebook translator?.
In Proceed-ings of the ACL-02 Workshop on Speech-to-SpeechTranslation: Algorithms and Systems, Philadelphia,PA, pp.
69?76.Somers, Harold, Gareth Evans and Zeinab Mo-hamed.
2006.
?Developing speech synthesis for under-resourced languages by ?faking it?
: an experimentwith Somali?.
In Proceedings of LREC: 5th Confer-ence on Language Resources and Evaluation, Genoa.Somers, Harold and Hermione Lovel.
2003.
?Computer-based support for patients with limited English?.
In As-sociation for Computational Linguistics EACL 2003,10th Conference of The European Chapter, Proceed-ings of the 7th International EAMT Workshop on MTand other language technology tools, Budapest, pp.41?49.Somers, Harold and Yuriko Sugita.
2003.
?Evaluatingcommercial spoken language translation software.?
InMT Summit IX: Proceedings of the Ninth MachineTranslation Summit, New Orleans, pp.
370?377.Starlander, Marianne, Pierrette Bouillon, Manny Rayner,Nikos Chatzichrisafis, Beth Ann Hockey, Hitoshi Isa-hara, Kyoko Kanzaki, Yukie Nakao and Marianne San-taholma.
2005.
?Breaking the language barrier: ma-chine assisted diagnosis using the medical speechtranslator?.
In Proceedings of the XIX InternationalCongress of the European Federation for Medical In-formatics, Geneva, Switzerland.Stewart, Moira, Judith Belle Brown, W. Wayne We-ston, Ian R. McWhinney, Carol L. McWilliam andThomas R. Freeman.
2003.
Patient-Centered Medi-cine: Transforming the Clinical Method (2nd ed.
).Radcliffe, Abingdon, Oxon.Tomita, Masaru, Marion Kee, Hiroaki Saito, Teruko Mi-tamura and Hideto Tomabechi.
1988.
?The universalparser compiler and its application to a speech trans-lation system?.
In Second International Conferenceon Theoretical and Methodological Issues in MachineTranslation of Natural Languages, Pittsburgh, Penn-sylvania, pages not numbered.Waibel, Alex, Ahmed Badran, Alan W. Black, RobertFrederking, Donna Gates, Alon Lavie, Lori Levin,Kevin Lenzo, Laura Mayfield Tomokiyo, Ju?rgenReichert, Tanja Schultz, Dorcas Wallace, MonikaWoszczyna, and Jing Zhang.
2003.
?Speechalator:two-way speech-to-speech translation on a consumerPDA?.
In Proceedings of EUROSPEECH 2003, 8thEuropean Conference on Speech Communication andTechnology, Geneva, pp.
369?372.Zhou, Bowen, Daniel De?chelotte and Yuqing Gao.
2004.?Two-way speech-to-speech translation on handhelddevices?.
In INTERSPEECH 2004 ?
ICSLP, 8th Inter-national Conference on Spoken Language Processing,Jeju Island, Korea, pp.
1637?1640.
