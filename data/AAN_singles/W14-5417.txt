Proceedings of the 25th International Conference on Computational Linguistics, pages 109?111,Dublin, Ireland, August 23-29 2014.Towards Succinct and Relevant Image DescriptionsDesmond ElliottInstitute of Language, Communication, and ComputationSchool of InformaticsUniversity of Edinburghd.elliott@ed.ac.ukWhat does it mean to produce a good description of an image?
Is a description good because itcorrectly identifies all of the objects in the image, because it describes the interesting attributes of theobjects, or because it is short, yet informative?
Grice?s Cooperative Principle, stated as ?Make yourcontribution such as is required, at the stage at which it occurs, by the accepted purpose or directionof the talk exchange in which you are engaged?
(Grice, 1975), alongside other ideas of pragmatics incommunication, have proven useful in thinking about language generation (Hovy, 1987; McKeown etal., 1995).
The Cooperative Principle provides one possible framework for thinking about the generationand evaluation of image descriptions.1The immediate question is whether automatic image description is within the scope of the CooperativePrinciple.
Consider the task of searching for images using natural language, where the purpose of theexchange is for the user to quickly and accurately find images that match their information needs.
In thisscenario, the user formulates a complete sentence query to express their needs, e.g.
A sheepdog chasingsheep in a field, and initiates an exchange with the system in the form of a sequence of one-shot con-versations.
In this exchange, both participants can describe images in natural language, and a successfuloutcome relies on each participant succinctly and correctly expressing their beliefs about the images.
Itfollows from this that we can think of image description as facilitating communication between peopleand computers, and thus take advantage of the Principle?s maxims of Quantity, Quality, Relevance, andManner in guiding the development and evaluation of automatic image description models.An overview of the image description literature from the perspective of Grice?s maxims can be foundin Table 1.
The most apparent ommission is the lack of research devoted to generating minimally infor-mative descriptions: the maxim of Quantity.
Attending to this maxim will become increasingly importantas the quality and coverage of object, attribute, and scene detectors increases.
It would be undesirable todevelop models that describe every detected object in an image because that would be likely to violate themaxim of Quantity (Spain and Perona, 2010).
Similarly, if it is possible to associate an accurate attributewith each object in the image, it will be important to be sparing in the application of those attributes: isit relevant to describe ?furry?
sheep when there are no sheared sheep in an image?How should image description models be evaluated with respect to the maxims of the CooperativePrinciple?
So far model evaulation has focused on automatic text-based measures, such as UnigramBLEU and human judgements of semantic correctness (see Hodosh et al.
(2013) for discussion of framingimage description as a ranking task, and Elliott and Keller (2014) for a correlation analysis of text-basedmeasures against human judgements).
The semantic correctness judgements task typically present avariant of ?Rate the relevance of the description for this image?, which only evaluates the description vis-`a-vis the maxim of Relevance.
One exception is the study of Mitchell et al.
(2012), in which judgementsabout the ordering of noun phrases (the maxim of Manner) were also collected.
The importance ofbeing able to evaluate according to multiple maxims becomes clearer as computer vision becomes moreaccurate.
It seems intuitive that a model that describes and relates every object in the image couldbe characterised as generating Relevant and Quality descriptions, but not necessarily descriptions ofThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Licence details: http://creativecommons.org/licenses/by/4.0/1This discussion primarily applies to image descriptions, and not to image captions.
See (Hodosh et al., 2013) and (Panof-sky, 1939) for a discussion of the differences between descriptions and captions.109CategoryMaxim Attention in the literatureQuantityBe as informative as required ??
?Do not be more informative thanrequired??
?QualityDo not say what youbelieve is falseAll models exploit some kind of corpus data toconstruct descriptions that are maximally probable(Yang et al., 2011; Li et al., 2011; Kuznetsova et al.,2012; Le et al., 2013).
These approaches typicallyuse language modelling to construct hypothesesbased on the available evidence, but may eventuallybe false.Do not say that for whichyou lack evidenceRelevanceBe relevantNo models try to generate irrelevant descriptions.Dodge et al.
(2012) explored the separation betweenwhat can be seen/not seen in an image/caption pair.MannerAvoid obscure expressionsNo model has been deliberately obscure.Avoid ambiguityKulkarni et al.
(2011) introduced visual attributes todescribe and distinguish objects.Be brief ??
?Be orderlyMitchell et al.
(2012) and Elliott and Keller (2013)explicitly try to predict the best ordering of objectsin the final description.Table 1: An overview of Grice?s maxims and the relevant image description models.
???
means that weare unaware of any models that implicitly or explicitly claim to address this type of maxim.adequate Quantity.
It is not clear that current human judgements capture this distinction, yet the gold-standard crowdsourced descriptions almost certainly do conform to the maxim of sufficient Quantity.
Afurther important consideration is how to obtain human judgements for multiple maxims without makingthe studies prohibitively expensive.Using Grice?s maxims to think about image description from the perspective of enabling effectivecommunication helps us reconsider the state of the art of automatic image description and directionsfor future research.
In particular, we identified the open problems of determining the minimum andmost relevant aspects of an image, and the challenges of conducting human evaluations along alternativedimensions to semantic correctness.AcknowledgmentsS.
Frank, D. Frassinelli, and the anonymous reviewers provided valuable feedback on this paper.
Theresearch is funded by ERC Starting Grant SYNPROC No.
203427.ReferencesJesse Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch, Margaret Mitchell, Karl Stratos, Kota Yamaguchi, YejinChoi, Hal Daum?e III, Alex Berg, and Tamara Berg.
2012.
Detecting visual text.
In Proceedings of the 2012Conference of the North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies, pages 762?772, Montr?eal, Canada.Desmond Elliott and Frank Keller.
2013.
Image Description using Visual Dependency Representations.
In Pro-ceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1292?1302,Seattle, Washington, U.S.A.110Desmond Elliott and Frank Keller.
2014.
Comparing Automatic Evaluation Measures for Image Description.In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 452?457,Baltimore, Maryland, U.S.A.H.
Paul Grice.
1975.
Logic and Conversation.
In P. Cole and J. L. Morgan, editors, Syntax and Semantics 3:Speech Arts, pages 41?58.
Academic Press, Inc.Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013.
Framing Image Description as a Ranking Task: Data,Models and Evaluation Metrics.
Journal of Artificial Intelligence Research, 47:853?899.E Hovy.
1987.
Generating natural language under pragmatic constraints.
Journal of Pragmatics, 11(6):689?719.Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg.2011.
Baby talk: Understanding and generating simple image descriptions.
In 2011 IEEE Conference onComputer Vision and Pattern Recognition, pages 1601?1608, Colorado Springs, Colorado, U.S.A.Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg, Tamara L. Berg, and Yejin Choi.
2012.
CollectiveGeneration of Natural Image Descriptions.
In Proceedings of the 50th Annual Meeting of the Association forComputational Linguistics, pages 359?368, Jeju Island, South Korea.Dieu Thu Le, Jasper Uijlings, and Raffaella Bernardi.
2013.
Exploiting language models for visual recognition.In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 769?779,Seattle, Washington, U.S.A.Siming Li, Girish Kulkarni, Tamara L. Berg, Alexander C. Berg, and Yejin Choi.
2011.
Composing simple imagedescriptions using web-scale n-grams.
In Proceedings of the Fifteenth Conference on Computational NaturalLanguage Learning, pages 220?228, Portland, Oregon, U.S.A.K McKeown, J Robin, and K Kukich.
1995.
Generating concise natural language summaries.
InformationProcessing & Management, 31(5):703?733.Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Yamaguchi, Karl Stratos, Alyssa Mensch, Alex Berg, TamaraBerg, and Hal Daum.
2012.
Midge : Generating Image Descriptions From Computer Vision Detections.
InProceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,pages 747?756, Avignon, France.Erwin Panofsky.
1939.
Studies in Iconology.
Oxford University Press.Merrielle Spain and Pietro Perona.
2010.
Measuring and Predicting Object Importance.
International Journal ofComputer Vision, 91(1):59?76.Yezhou Yang, Ching Lik Teo, Hal Daum?e III, and Yiannis Aloimonos.
2011.
Corpus-Guided Sentence Generationof Natural Images.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing,pages 444?454, Edinburgh, Scotland, UK.111
