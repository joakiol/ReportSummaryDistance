AUTOMATIC GRAMMAR ACQUISITIONScott Miller Heidi J. FoxCollege o f  Computer  ScienceNortheastern UniversityBoston, MA 02115BBN Systems and Technologies70 Fawcett  St.,Cambridge, MA 02138ABSTRACTWe describe a series of three experiments in which supervisedlearning techniques were used to acquire three different ypes ofgrammars for English news stories.
The acquired grammar typeswere: 1) context-free, 2) context-dependent, and 3) probabilisticcontext-free.
Training data were derived from University ofPennsylvania Treebank parses of 50 Wall Street Journal articles.In each case, the system started with essentially no grammaticalknowledge, and learned a set of grammar rules exclusively fromthe training data.
Performance for each gr~rnar type was thenevaluated on an independent set of test sentences using Parseval,a standard measure of parsing accuracy.
These experimentalresults yield a direct qtmntitative comparison between each ofthe three methods.1.
INTRODUCTIONDesigning and refining a natural anguage grammar is a difficultand time-intensive task, often consuming months or even yearsof skilled effort.
The resulting grammar is usually notcompletely satisfactory, failing to cover a significant fraction ofthe sentences in the intended omain.
Conversely, the grammaris likely to overgenerate, l ading to multiple interpretations for asingle sentence, many of which are incorrect.
With theincreasing availability of large, machine-readable, parsedcorpora such as the University of Pennsylvania Treebank\[Santorini, 90\], it has become worthwhile to consider automaticgrammar acquisition through the application of machine learningtechniques.
By learning a grammar that completely covers atraining set for some domain, it is hoped that coverage will alsobe increased for new sentences in that domain.
Additionally,machine learning techniques may be useful in reducingovergeneration through a variety of techniques that have beensuggested in recent literature.
One suggestion is to introducelocal contextual information into a grammar \[Simmons and Yu,92\], based on the premise that local context provides usefulinformation for selecting among competing rammar rules.
Asecond suggestion is to introduce probabilities in the form of aprobabilistic ontext-free grammar \[Chitaro and Gfishman, 9%based on the premise that a combination of local probabilitymeasures provides a useful estimate of the probability of anentire parse.JIn this work, we Investigate both of these suggestions andcompare them with a simple, automatically learned, context-freegrammar.
In each case, the grammar is acquired from a subsetof parsed Wall Street Journal articles taken from the Universityof Pennsylvania Treebank.
We then apply the acquired grammarto the problem of producing a single unambiguous parse for eachsentence of an independent test set derived from the sameSOUrce .2.
LEARNING A CONTEXT-FREEGRAMMARA context-free grammar is acquired from a parsed Treebankcorpus by straightforward memorization of the grammar ulesused in each training example.
Figure 1 shows a typical parsetree from our training corpus; Table 1 shows the grammar rulesused.SMAJNN NN NN VBD CD NN TO $ CD CD\[ I \[ I I I \ [ \ [ I  IGovernment construction spending rose 4.3 % to $ 88 billionFigure 1: A typical parse tree from the Treebank corpus.SMAJ---~ S : S---~ NPVPVP ---~ VBD NP PP NP --~ CD NNNP--} $ CD CDNP --~ NN NN NNPP ~ TO NPTable 1: Rules used for the parse tree shown in figure I.In order to parse new sentences, a simple bottom-up chart parseris combined with the acquired grammar.
In our experiments, theparser is run until a single parse tree is found that spans all ofthe words in the input sentence.
If no such tree is found, then268the minimum number of disjoint fragments i returned such thatthe set of fragments pans the entire input sentence.
Becausethe acquired grammar is highly ambiguous, the returned parsetree is dependent on the order in which the grammar rules areapplied.
To account for this sensitivity to rule order, we repeatour experiments several times with different rule orderings.
Wehave found that, although different orderings produce differentparse trees, the overall accuracy of the results do not differsignificantly.
As expected, the high degree of rule ambiguity,together with our procedure that returns a single parse tree foreach sentence, yields rather poor performance.
Nevertheless, theperformance of this system serves as a baseline which we use toassess the performance of other systems based on alternativegrammar types.3.
LEARNING A CONTEXT-DEPENDENTGRAMMARIn this experiment, we closely follow the approach of Simmonsand Yu, with extensions to accommodate grammar ules of aform derivable from the Treebank.
Unlike our otherexperiments, the grammar rules in this experiment are situation/ action rules for a shift-reduce parser.
In the following sectionswe consider:?
The general structure of the shift-reduce parser.?
The form of the context-dependent rules.?
The problem of learning context-dependent rules for ashift-reduce parser from Treebank examples.?
A parsing strategy that attempts to fred a single best parsebased on contextual information.3.1.
Sh i f t -Reduce  ParserThe shift-reduce parser consists of two primary data structures: afive position input buffer, and an unlimited depth push downstack.
New words arriving at the parser flow, in the order inwhich they are received, through the input buffer.
Shiftoperations remove the leading word from the input buffer andpush it onto the top of the stack.reduces /NNP VPFigure 2: Reduce operations construct tree structures.When this occurs, all other words are shifted one positiontoward the front of the buffer, and the next unprocessed word ismoved into the last buffer position.
Reduction operationsremove two or more elements from the top of the stack, andreplace them with a single constituent.
Reduction operations areequivalent to constructing parse subtrees, as in Figure 2.3.2.
Context -dependent  Ru lesThe determination f what action the parser takes in a particularsituation is governed by context-dependent rules.
Constraintsgiven by the rules are matched against actual situations in a twopart process.
First, for a rule to be applicable, a hard constraintspecified by two or more elements on the top of the stack mustbe satisfied.
Next, those rules that satisfy this condition areordered by preference based on soft constraints pecified bycontext elements of the stack and buffer.Hard constraints for reduction rules are determined irectly bythe reductions themselves.
For example, to apply a rulereducing {DT JJ NN ...} to {NP ...}, the top three stack elementsmust be NN, JJ, and DT.
For shift operations, the hardconstraints are always given by the top two stack elements.Soft constraints are specified by a'two part context comprised ofa stack portion and a buffer portion..
The stack portion iscomprised of the three stack positions directly below the hardconstraint, while the buffer portion is comprised of the entirefive element buffer.
Soft constraints are scored by a weightedsum of the number of matches between rule and situationcontexts.
These weights were hand tuned to maximize parsingaccuracy.hard constraint, rmust match |exactly.
Lstack \ [3  .............context 12 .............weights / / 1 ............. LSTACK~p~1top-n- 1top-n-2top-n-3BUFFERI w. I w.+,lw.+,lw.,Iw.,I15 4 3 2 Ibuffer context weightsFigure 3: The primary data structures of the shift-reduce parser.3.3.
Learn ing Sh i f t -Reduce RulesIn order to train the shift-reduce parser, it is first necessary toconvert he Treebank parse trees into sequences of shift andreduce operations.
A simple treewalk algorithm that performsthe conversion is shown in Figure 4.Training examples are presented successively to the parser.
Foreach example, all rules with satisfied hard constraints are269formed into a list.
Next, a shorter list is formed by extractingonly those rules that best satisfy the soft constraints.
If thecorrect parser action is among those specified by the shortenedlist of vales, then no action is taken.
Otherwise, a new rule isformed from the current context and parser action, and is storedin the hash table of rules.
When training is complete, the rulematching mechanism can present a short list of possible rules,one of which is guaranteed to be correct, for every situationpresented in the training examples.Convert(tree)BEGINIF tree is a leaf node THENemit a Shift operationELSE {has child nodes}FOR ALL child nodes of tree (from left to fight) DOConvert(child node)END FORemit a Reduce operation{reducing child nodes to a single symbol}ENDIFENDFigure 4: An algorithm for converting a parse tree intoshift-reduce parser actions.3.4.
Shift-Reduce Parser OperationParsing a sentence is considered as a search problem, the goal ofwhich is to fmd a sequence of actions that lead from an initialparser state to a final state.
The initial state for a sentence ischaracterized byan empty stack and a buffer filled with the firstfive words of the sentence.
The fmal state is characterized byanempty buffer and a single element at the top of the stack.
Validtransitions between states are determined by the rules acquiredduring training.Given a parser state, the rule matching mechanism returns a listof rules specifying actions that cause transitions to new states.The rules are guaranteed tobe legal by hard constraints, but varyto the degree to which soft constraints on context are satisfied.Each alternative rule corresponds to a different syntacticinterpretation of a phrase, only one of which is correct.
Thepremise, put forth by Simmons and Yu, is that the use of contextinformation significantly reduces ambiguity.To parse a sentence, a beam search is used to fred a path throughthe state space that maximizes the soft constraints pecifyingcontext.
Upon completion, a list of shift and reduce operationsis returned.
These operations correspond directly to a parse treefor the input sentence.4.
LEARNING A PROBABILISTICCONTEXT-FREE GRAMMARIn this experiment, probabilities are used to select one amongthe set of alternative parse trees derivable for an input sentence.A straightforward evaluation of the probability of a parse tree isobtained from the probabilities of the individual grammar rulesthat comprise the tree.
For each rule r of the form ct ~ t ,  therule probability is given by P(r)= P(fl lct).
Then, given aparse tree t constructed according to a derivation D(t), theprobability of t is the product of all the conditional nileprobabilities in the derivation:P(t)= H P(r).reD(t)Using the Treebank training corpus, P(~a)  is estimated bycounting the number of times the rule a ~ f l  appears in thetraining, divided by the number of times the nonterminal symbolOt appears.
In order to parse new sentences, a simple bottom-upchart parser is extended to include probability measures.
Inparticular, an extra field is added to the chart structure in orderto store a probability value corresponding to each completededge.
When multiple derivations result in the same edge, theprobability value stored is the maximum among the competingtheories.
When parsing a sentence, all possible derivations areconsidered, and the derivation with the highest probability isthen returned.5.
EXPERIMENTAL RESULTSEach of the grammars was learned from a training set of 731sentences (16,733 words) from the Wall Street Journal Treebankcorpus.
A separate test set of 49 sentences (1289 wordsi wascompiled from the same corpus.
Parse quality was evaluatedusing Parseval, which reports three different measures ofcorrectness: recall, precision, and crossings.
Each parse tree tobe evaluated (the candidate parse) is compared against thecorresponding parse as found in Treebank (the standard parse).Recall measures the percentage of the constituents in thestandard parse which are present in the candidate parse.Precision measures the percentage of the constituents in thecandidate parse which are correct (i.e., present in the standardparse).
Crossings measures the number of constituents in thecandidate parse which are incompatible with the constituents inthe standard parse, where incompatibility means that theconstituent,crosses brackets with a constituent in the standard.For more details on the evaluation procedure, see \[Black, et al,91\]The results of the test are shown in Table 2.
As expected, theperformance of the simple ccontext-free grammar is substantiallyworse than the performance of both the context-dependentgrammar and the probabilistic ontext-flee grammar.
It isinteresting to note that although recall for the P-CFG and CDGis essentially equal, the P-CFG has a higher precision.
This270suggests that probabilistic modeling is more successful atreducing overgeneration than simple examination of context.The P-CFG also shows a lesser average number of crossings persentence.Crossings Recall PrecisionP-CFG 4.94 :52.75 51.52CDG 6.61 51.20 42.16CFG 11.06 28.49 22.25Table 2: Parseval results for each grammar6.
CONCLUSIONSThese experiments provide aquantitative measure of the relativeeffectiveness of the three different types of grammars.
Using thestandard context-free grammar as a baseline, we see greatimprovement both with the addition of context information andwith the incorporation of a probabilistic model.
We also seeevidence that using context o disambiguate among rules is notas effective as using probabilities.There are still many problems to overcome.
Direct conversion ofTreebank parse trees into rules yields productions whose fight-hand sides can vary in size between I and approximately 10.This is suspected tohave significant impact on the performanceof the context-dependent sys em.More improvements will be necessary before a trainable parserwill be able to produce parses of high enough quality to beuseful in an understanding system.
This increase in accuracyshould be achievable by combining the strengths of the context-dependent model with those of the probabilistic context-freemodel, and by exploring ways to make use of other types ofinformation, such as semantic information.
It would also beworthwhile to fitrther experiment with varying the amount oftraining data, contrasting domain-dependent and domain-independent raining, and varying the amount and type ofcontext information used by the context-dependent model.1.2.3.4.REFERENCESBlack, E., Abney, S., Flickenger, D., Gdaniec, C.,Grishman, R., Harrison, P., Hindle, D., Ingria, R.,Jelinek, F., Klavans, J., Liberman, M., Marcus, M.,Roukos, S., Santorini, B., Strzalkowski, T., A Procedurefor Quantitatively Comparing the Syntactic Coverage ofEnglish Grammars: Proceedings of the Speech andNatural Language Workshop, Morgan KaufmannPublishers, pages 306-311, February 1991.Chitaro, M.V.
and Gfishman, R., Statistical Parsing ofMessages: Proceedings of the Speech and NaturalLanguage Workshop, Morgan Kaufmann Publishers,pages 263-276, June 1990.Santorini, B., Annotation Manual for the Penn TreebankProject, Technical Report, CIS Department, Universityof Pennsylvania, May 1990.Simmons, R. and Yu Y., The Acquisition and Use ofContext-Dependent Grammars for English:Computational Linguistics, Vol.
18, Number 4, pages391-416, Dec. 1992.ACKNOWLEDGEMENTSThe work reported here was supported in part by the DefenseAdvanced Research Projects Agency and was monitored by theRome Air Development Center under Contract No.
F30602-91-C-0051.
The views and conclusions contained in this documentare those of the authors and should not be interpreted asnecessarily representing the official policies, either expressed orimplied, of the Defense Advanced Research Projects Agency orthe United States Government.271
