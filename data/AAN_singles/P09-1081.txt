Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 719?727,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPA Graph-based Semi-Supervised Learning for Question-AnsweringAsli CelikyilmazEECS DepartmentUniversity of Californiaat BerkeleyBerkeley, CA, 94720asli@berkeley.eduMarcus ThintIntelligent Systems Research CentreBritish Telecom (BT Americas)Jacksonville, FL 32256, USAmarcus.2.thint@bt.comZhiheng HuangEECS DepartmentUniversity of Californiaat BerkeleyBerkeley, CA, 94720zhiheng@eecs.berkeley.eduAbstractWe present a graph-based semi-supervisedlearning for the question-answering (QA)task for ranking candidate sentences.
Us-ing textual entailment analysis, we obtainentailment scores between a natural lan-guage question posed by the user and thecandidate sentences returned from searchengine.
The textual entailment betweentwo sentences is assessed via features rep-resenting high-level attributes of the en-tailment problem such as sentence struc-ture matching, question-type named-entitymatching based on a question-classifier,etc.
We implement a semi-supervisedlearning (SSL) approach to demonstratethat utilization of more unlabeled datapoints can improve the answer-rankingtask of QA.
We create a graph for labeledand unlabeled data using match-scores oftextual entailment features as similarityweights between data points.
We applya summarization method on the graph tomake the computations feasible on largedatasets.
With a new representation ofgraph-based SSL on QA datasets usingonly a handful of features, and under lim-ited amounts of labeled data, we show im-provement in generalization performanceover state-of-the-art QA models.1 IntroductionOpen domain natural language question answer-ing (QA) is a process of automatically finding an-swers to questions searching collections of textfiles.
There are intensive research in this areafostered by evaluation-based conferences, such asthe Text REtrieval Conference (TREC) (Voorhees,2004), etc.
One of the focus of these research, aswell as our work, is on factoid questions in En-glish, whereby the answer is a short string that in-dicates a fact, usually a named entity.A typical QA system has a pipeline structurestarting from extraction of candidate sentencesto ranking true answers.
In order to improveQA systems?
performance many research focuson different structures such as question process-ing (Huang et al, 2008), information retrieval(Clarke et al, 2006), information extraction (Sag-gion and Gaizauskas, 2006), textual entailment(TE) (Harabagiu and Hickl, 2006) for ranking, an-swer extraction, etc.
Our QA system has a sim-ilar pipeline structure and implements a new TEmodule for information extraction phase of the QAtask.
TE is a task of determining if the truth of atext entails the truth of another text (hypothesis).Harabagui and Hickl (2006) has shown that usingTE for filtering or ranking answers can enhancethe accuracy of current QA systems, where the an-swer of a question must be entailed by the text thatsupports the correctness of this answer.We derive information from pair of texts, i.e.,question as hypothesis and candidate sentenceas the text, potentially indicating containment oftrue answer, and cast the inference recognitionas classification problem to determine if a ques-tion text follows candidate text.
One of the chal-lenges we face with is that we have very lim-ited amount of labeled data, i.e., correctly labeled(true/false entailment) sentences.
Recent researchindicates that using labeled and unlabeled data insemi-supervised learning (SSL) environment, withan emphasis on graph-based methods, can im-prove the performance of information extractionfrom data for tasks such as question classifica-tion (Tri et al, 2006), web classification (Liu etal., 2006), relation extraction (Chen et al, 2006),passage-retrieval (Otterbacher et al, 2009), vari-ous natural language processing tasks such as part-of-speech tagging, and named-entity recognition(Suzuki and Isozaki, 2008), word-sense disam-719biguation (Niu et al, 2005), etc.We consider situations where there are muchmore unlabeled data, XU , than labeled data, XL,i.e., nL  nU .
We construct a textual entail-ment (TE) module by extracting features fromeach paired question and answer sentence and de-signing a classifier with a novel yet feasible graph-based SSL method.
The main contributions are:?
construction of a TE module to extract match-ing structures between question and answer sen-tences, i.e., q/a pairs.
Our focus is on identifyinggood matching features from q/a pairs, concerningdifferent sentence structures in section 2,?
representation of our linguistic system by aform of a special graph that uses TE scores in de-signing a novel affinity matrix in section 3,?
application of a graph-summarization methodto enable learning from a very large unlabeled andrather small labeled data, which would not havebeen feasible for most sophisticated learning toolsin section 4.
Finally we demonstrate the results ofexperiments with real datasets in section 5.2 Feature Extraction for EntailmentImplementation of different TE models has pre-viously shown to improve the QA task using su-pervised learning methods (Harabagiu and Hickl,2006).
We present our recent work on the task ofQA, wherein systems aim at determining if a textreturned by a search engine contains the correctanswer to the question posed by the user.
The ma-jor categories of information extraction producedby our QA system characterizes features for ourTE model based on analysis of q/a pairs.
Here wegive brief descriptions of only the major modulesof our QA due to space limitations.2.1 Pre-Processing for Feature ExtractionWe build the following pre-processing modulesfor feature extraction to be applied prior to our tex-tual entailment analysis.Question-Type Classifier (QC): QC is the taskof identifying the type of a given question amonga predefined set of question types.
The type ofa question is used as a clue to narrow down thesearch space to extract the answer.
We used ourQC system presented in (Huang et al, 2008),which classifies each question into 6-coarse cat-egories (i.e., abbr., entity, human, location, num-ber, description) as well as 50-fine categories (i.e.,color, food, sport, manner, etc.)
with almost90% accuracy.
For instance, for question ?Howmany states are there in US?
?, the question-typewould be ?NUMBER?
as course category, and?Count?
for the finer category, represented jointlyas NUM:Count.
The QC model is trained via sup-port vector machines (SVM) (Vapnik, 1995) con-sidering different features such as semantic head-word feature based on variation of Collins rules,hypernym extraction via Lesk word disambigua-tion (Lesk, 1988), regular expressions for wh-word indicators, n-grams, word-shapes(capitals),etc.
Extracted question-type is used in connectionwith our Named-Entity-Recognizer, to formulatequestion-type matching feature, explained next.Named-Entity Recognizer (NER): This com-ponent identifies and classifies basic entities suchas proper names of person, organization, prod-uct, location; time and numerical expressions suchas year, day, month; various measurements suchas weight, money, percentage; contact informationlike address, web-page, phone-number, etc.
Thisis one of the fundamental layers of informationextraction of our QA system.
The NER moduleis based on a combination of user defined rulesbased on Lesk word disambiguation (Lesk, 1988),WordNet (Miller, 1995) lookups, and many user-defined dictionary lookups, e.g.
renown places,people, job types, organization names, etc.
Duringthe NER extraction, we also employ phrase analy-sis based on our phrase utility extraction methodusing Standford dependency parser ((Klein andManning, 2003)).
We can categorize entities upto 6 coarse and 50 fine categories to match themwith the NER types from QC module.Phrase Identification(PI): Our PI module un-dertakes basic syntactic analysis (shallow pars-ing) and establishes simple, un-embedded linguis-tic structures such as noun-phrases (NN), basicprepositional phrases (PP) or verb groups (VG).In particular PI module is based on 56 differentsemantic structures identified in Standford depen-dency parser in order to extract meaningful com-pound words from sentences, e.g., ?They heardhigh pitched cries.?.
Each phrase is identified witha head-word (cries) and modifiers (high pitched).Questions in Affirmative Form: To derive lin-guistic information from pair of texts (statements),we parse the question and turn into affirmativeform by replacing the wh-word with a place-holder and associating the question word with thequestion-type from the QC module.
For example:720?What is the capital of France??
is written in af-firmative form as ?
[X]LOC:City is the capital ofFranceLOC:Country.?.
Here X is the answer textof LOC:City NER-type, that we seek.Sentence Semantic Component Analysis: Us-ing shallow semantics, we decode the underlyingdependency trees that embody linguistic relation-ships such as head-subject (H-S), head-modifier(complement) (H-M), head-object (H-O), etc.
Forinstance, the sentence ?Bank of America acquiredMerrill Lynch in 2008.?
is partitioned as:?
Head (H): acquired?
Subject (S): Bank of America[Human:group]?
Object (O): Merrill Lynch[Human:group]?
Modifier (M): 2008[Num:Date]These are used as features to match components ofquestions like ?Who purchased Merrill Lynch?
?.Sentence Structure Analysis: In our questionanalysis, we observed that 98% of affirmed ques-tions did not contain any object and they are alsoin copula (linking) sentence form that is, theyare only formed by subject and information aboutthe subject as: {subject + linking-verb + subject-info.}.
Thus, we investigate such affirmed ques-tions different than the rest and call them copulasentences and the rest as non-copula sentences.
1For instance our system recognizes affirmed ques-tion ?
Fred Durst?s group name is [X]DESC:Def?.as copula-sentence, which consists of subject (un-derlined) and some information about it.2.2 Features from Paired Sentence AnalysisWe extract the TE features based on the above lex-ical, syntactic and semantic analysis of q/a pairsand cast the QA task as a classification problem.Among many syntactic and semantic features weconsidered, here we present only the major ones:(1) (QTCF) Question-Type-Candidate Sen-tence NER match feature: Takes on the value?1?
when the candidate sentence contains the fineNER of the question-type, ?0.5?
if it contains thecoarse NER or ?0?
if no NER match is found.
(2) (QComp) Question component match fea-tures: The sentence component analysis is appliedon both the affirmed question and the candidatesentence pairs to characterize their semantic com-ponents including subject(S), object(O), head (H)and modifiers(M).
We match each semantic com-ponent of a question to the best matching com-1One option would have been to leave out the non-copulaquestions and build the model for only copula questions.ponent of a candidate sentence.
For example forthe given question, ?When did Nixon die?
?, whenthe following candidate sentence, i.e., ?RichardNixon, 37th President of USA, passed away ofstroke on April 22, 1994.?
is considered, we ex-tract the following component match features:?
Head-Match: die?pass away?
Subject-Match: Nixon?Richard Nixon?
Object-Match: ??
Modifier-Match: [X]?April 22, 1994In our experiments we observed that convertedquestions have at most one subject, head, objectand a few modifiers.
Thus, we used one feature foreach and up to three for M-Match features.
Thefeature values vary based on matching type, i.e.,exact match, containment, synonym match, etc.For example, the S-Match feature will be ?1.0?due to head-match of the noun-phrase.
(3) (LexSem) Lexico-Syntactic AlignmentFeatures: They range from the ratio of consecu-tive word overlap between converted question (Q)and candidate sentence (S) including?Unigram/Bigram, selecting individual/pair of ad-jacent tokens in Q matching with the S?Noun and verb counts in common, separately.
?When words don?t match we attempt matchingsynonyms in WordNet for most common senses.
?Verb match statistics using WordNet?s cause andentailment relations.As a result, each q/a pair is represented as a fea-ture vector xi ?
<d characterizing the entailmentinformation between them.3 Graph Based Semi-SupervisedLearning for Entailment RankingWe formulate semi-supervised entailment rankscores as follows.
Let each data point inX = {x1, ..., xn}, xi ?
<d represents infor-mation about a question and candidate sentencepair and Y = {y1, ..., yn} be their output la-bels.
The labeled part of X is represented withXL = {x1, ..., xl} with associated labels YL ={y1, ..., yl}T .
For ease of presentation we concen-trate on binary classification, where yi can takeon either of {?1,+1} representing entailment ornon-entailment.
X has also unlabeled part, XU ={x1, ..., xu}, i.e., X = XL ?
XU .
The aim is topredict labels for XU .
There are also other testingpoints, XTe, which has the same properties as X .Each node V in graph g = (V,E) represents afeature vector, xi ?
<d of a q/a pair, characteriz-721ing their entailment relation information.
When allcomponents of a hypothesis (affirmative question)have high similarity with components of text (can-didate sentence), then entailment score betweenthem would be high.
Another pair of q/a sentenceswith similar structures would also have high en-tailment scores as well.
So similarity between twoq/a pairs xi, xj , is represented with wij ?
<n?n,i.e., edge weights, and is measured as:wij = 1?d?q=1|xiq?xjq |d (1)As total entailment scores get closer, the largertheir edge weights would be.
Based on our sen-tence structure analysis in section 2, given datasetcan be further separated into two, i.e., Xcp con-taining q/a pairs in which affirmed questions arecopula-type, and Xncp containing q/a pairs withnon-copula-type affirmed questions.
Since cop-ula and non-copula sentences have different struc-tures, e.g., copula sentences does not usually haveobjects, we used different sets of features for eachtype.
Thus, we modify edge weights in (1) as fol-lows:w?ij =????????????
?0 xi ?
Xcp, xj ?
Xncp1?dcp?q=1|xiq?xjq |dcpxi, xj ?
Xcp1?dncp?q=1|xiq?xjq |dncpxi, xj ?
Xncp(2)The diagonal degree matrix D is defined for graphg by D=?j w?ij .
In general graph-based SSL, afunction over the graph is estimated such that itsatisfies two conditions: 1) close to the observedlabels , and 2) be smooth on the whole graph by:argminf?i?L(fi ?
yi)2+?
?i,j?L?Uw?ij(fi ?
fj)2(3)The second term is a regularizer to represent thelabel smoothness, fTLf , where L = D?W is thegraph Laplacian.
To satisfy the local and globalconsistency (Zhou et al, 2004), normalized com-binatorial Laplacian is used such that the secondterm in (3) is replaced with normalized Laplacian,L = D?1/2LD?1/2, as follows:?i,j?L?Uwij(fi?di?
fj?dj)2 = fTLf (4)Setting gradient of loss function to zero, optimumf?, where Y = {YL ?
YU} , YU ={ynl+1 = 0};f?
= (1+ ?
(1?
L))?1 Y (5)Most graph-based SSLs are transductive, i.e., noteasily expendable to new test points outside L?U .In (Delalleau et al, 2005) an induction scheme isproposed to classify a new point xTe byf?
(xTe) =?i?L?U wxifi?i?L?U wxi(6)Thus, we use induction, where we can, to avoidre-construction of the graph for new test points.4 Graph SummarizationResearch on graph-based SSL algorithms pointout their effectiveness on real applications, e.g.,(Zhu et al, 2003), (Zhou and Scho?lkopf, 2004),(Sindhwani et al, 2007).
However, there is stilla need for fast and efficient SSL methods to dealwith vast amount of data to extract useful informa-tion.
It was shown in (Delalleau et al, 2006) thatthe convergence rate of the propagation algorithmsof SSL methods isO(kn2), which mainly dependson the form of eigenvectors of the graph Laplacian(k is the number of nearest neighbors).
As theweight matrix gets denser, meaning there will bemore data points with connected weighted edges,the more it takes to learn the classifier function viagraph.
Thus, the question is, how can one reducethe data points so that weight matrix is sparse, andit takes less time to learn?Our idea of summarization is to create repre-sentative vertices of data points that are very closeto each other in terms of edge weights.
Suffice tosay that similar data points are likely to representdenser regions in the hyper-space and are likely tohave same labels.
If these points are close enough,we can characterize the boundaries of these groupof similar data points with respect to graph andthen capture their summary information by newrepresentative vertices.
We replace each data pointwithin the boundary with their representative ver-tex, to form a summary graph.4.1 Graph Summarization AlgorithmLet each selected dataset be denoted as Xs ={xsi} , i = 1...m, s = 1, ..., q, where m is thenumber of data points in the sample dataset andq is the number of sample datasets drawn fromX .
The labeled data points, i.e., XL, are ap-pended to each of these selected Xs datasets,Xs ={xs1, ...xsm?l}?
XL.
Using a separatelearner, e.g., SVM (Vapnik, 1995), we obtain pre-dicted outputs, Y?
s =(y?s1, ..., y?sm?l)ofXs and ap-pend observed labels Y?
s = Y?
s ?
YL.722Figure 1: Graph Summarization.
(a) Actual data point with predicted class labels, (b) magnified view ofa single node (black) and its boundaries (c) calculated representative vertex, (d) summary dataset.We define the weight W s and degree Ds ma-trices of Xs using (1).
Diagonal elements of Dsis converted into a column vector and is sorted tofind the high degree vertices that are surroundedwith large number of close neighbors.The algorithm starts from the highest degreenode xsi ?
Xs, where initial neighbor nodes haveassumably the same labels.
This is shown in Fig-ure 1-(b) with the inner square around the mid-dle black node, corresponding high degree node.If its immediate k neighbors, dark blue colorednodes, have the same label, the algorithm contin-ues to search for the secondary k neighbors, thelight blue colored nodes, i.e., the neighbors of theneighbors, to find out if there are any opposite la-beled nodes around.
For instance, for the corre-sponding node (black) in Figure 1-(b) we can onlygo up to two neighbors, because in the third level,there are a few opposite labeled nodes, in red.
Thisindicates boundary Bsi for a corresponding nodeand unique nearest neighbors of same labels.Bsi ={xsi ?
{xsj}nmj=1}(7)In (7), nm denotes the maximum number of nodesof aBsi and ?xsj , xsj?
?
Bsi , ysj = ysj?
= yBsi , whereyBsi is the label of the selected boundary Bsi .We identify the edge weights wsij between eachnode in the boundary Bsi via (1), thus the bound-ary is connected.
We calculate the weighted av-erage of the vertices to obtain the representativesummary node of Bsi as shown in Figure 1-(c);XsBi =?nmi 6=j=112wsij(xsi + xsj)?nmi 6=j=1wsij(8)The boundaries of some nodes may only con-tain themselves because their immediate neigh-bors may have opposite class labels.
Similarlysome may have only k + 1 nodes, meaning onlyimmediate neighbor nodes have the same labels.For instance in Fig.
1 the boundary is drawn af-ter the secondary neighbors are identified (dashedouter boundary).
This is an important indicationthat some representative data points are better indi-cators of class labels than the others due to the factthat they represent a denser region of same labeledpoints.
We represent this information with the lo-cal density constraints.
Each new vertex is asso-ciated with a local density constraint, 0 ?
?j ?
1,which is equal to the total number of neighbor-ing nodes used to construct it.
We use the nor-malized density constraints for ease of calcula-tions.
Thus, for a each sample summary dataset,a local density constraint vector is identified as?s = {?s1, ..., ?snb}T .
The local density constraintsbecome crucial for inference where summarizedlabeled data are used instead of overall dataset.Algorithm 1 Graph Summary of Large Dataset1: Given X = {x1, ..., xn} , X = XL ?XU2: Set q ?
max number of subsets3: for s?
1, ..., q do4: Choose a random subset with repetitions5: Xs = {xs1, ..., xsm?l, xm?l+1, ..., xm}6: Summarize Xs to obtain Xsin (9)7: end for8: Obtain summary datasetX ={Xs}qs=1={Xi}pi=1andlocal density constrains, ?
= {?i}pi=1.After all data points are evaluated, the sampledataset Xs can now be represented with the sum-mary representative vertices asXs={XsB1 , ..., XsBnb}.
(9)and corresponding local density constraints as,?s = {?s1, ..., ?snb}T , 0 < ?si ?
1 (10)723The summarization algorithm is repeated for eachrandom subset Xs, s = 1, ..., q of very largedataset X = XL ?
XU , see Algorithm 1.
Asa result q number of summary datasets Xseachof which with nb labeled data points are com-bined to form a representative sample of X , X ={Xs}qs=1 reducing the number of data from n toa much smaller number of data, p = q ?
nb  n.So the new summary of the X can be representedwith X ={Xi}pi=1.
For example, an origi-nal dataset with 1M data points can be dividedup to q = 50 random samples of m = 5000data points each.
Then using graph summariza-tion each summarized dataset may be representedwith nb ?= 500 data points.
After merging sum-marized data, final summarized samples compileto 500 ?
50 ?= 25K  1M data points, reduced to1/40 of its original size.
Each representative datapoint in the summarized dataset X is associatedwith a local density constraints, a p = q ?
nbdimensional row vector as ?
= {?i}pi=1.We can summarize a graph separately for dif-ferent sentence structures, i.e., copula and non-copula sentences.
Then representative data pointsfrom each summary dataset are merged to form fi-nal summary dataset.
The Hybrid graph summarymodels in the experiments follow such approach.4.2 Prediction of New Testing DatasetInstead of using large dataset, we now use sum-mary dataset with predicted labels, and local den-sity constraints to learn the class labels of ntenumber of unseen data points, i.e., testing datapoints, XTe = {x1, ..., xnte}.
Using graph-basedSSL method on the new representative dataset,X ?
= X ?
XTe, which is comprised of sum-marized dataset, X ={Xi}pi=1, as labeled datapoints, and the testing dataset, XTe as unlabeleddata points.
Since we do not know estimated lo-cal density constraints of unlabeled data points, weuse constants to construct local density constraintcolumn vector for X ?
dataset as follows:??
= {1 + ?i}pi=1 ?
[1 ... 1]T ?
<nte (11)0 < ?i ?
1.
To embed the local density con-straints, the second term in (3) is replaced with theconstrained normalized Laplacian, Lc = ?TL?,?i,j?L?Twij(fi??
?i ?
di?fj??
?j ?
dj)2 = fTLcf(12)If any testing vector has an edge between a labeledvector, then with the usage of the local densityconstraints, the edge weights will not not only beaffected by that labeled node, but also how densethat node is within that part of the graph.5 ExperimentsWe demonstrate the results from three sets of ex-periments to explore how our graph representa-tion, which encodes textual entailment informa-tion, can be used to improve the performance ofthe QA systems.
We show that as we increasethe number of unlabeled data, with our graph-summarization, it is feasible to extract informationthat can improve the performance of QA models.We performed experiments on a set of 1449questions from TREC-99-03.
Using the search en-gine 2, we retrieved around 5 top-ranked candi-date sentences from a large newswire corpus foreach question to compile around 7200 q/a pairs.We manually labeled each candidate sentence astrue or false entailment depending on the contain-ment of the true answer string and soundness ofthe entailment to compile quality training set.
Wealso used a set of 340 QA-type sentence pairs fromRTE02-03 and 195 pairs from RTE04 by convert-ing the hypothesis sentences into question form tocreate additional set of q/a pairs.
In total, we cre-ated labeled training dataset XL of around 7600q/a pairs .
We evaluated the performance of graph-based QA system using a set of 202 questions fromthe TREC04 as testing dataset (Voorhees, 2003),(Prager et al, 2000).
We retrieved around 20 can-didate sentences for each of the 202 test questionsand manually labeled each q/a pair as true/false en-tailment to compile 4037 test data.To obtain more unlabeled training data XU,we extracted around 100,000 document headlinesfrom a large newswire corpus.
Instead of match-ing headline and first sentence of the document asin (Harabagiu and Hickl, 2006), we followed a dif-ferent approach.
Using each headline as a query,we retrieved around 20 top-ranked sentences fromsearch engine.
For each headline, we picked the1st and the 20th retrieved sentences.
Our assump-tion is that the first retrieved sentence may havehigher probability to entail the headline, whereasthe last one may have lower probability.
Each ofthese headline-candidate sentence pairs is used asadditional unlabeled q/a pair.
Since each head-2http://lucene.apache.org/java/724Features Model MRR Top1 Top5Baseline ?
42.3% 32.7% 54.5%QTCF SVM 51.9% 44.6% 63.4%SSL 49.5% 43.1% 60.9%LexSem SVM 48.2% 40.6% 61.4%SSL 47.9% 40.1% 58.4%QComp SVM 54.2% 47.5% 64.3%SSL 51.9% 45.5% 62.4%Table 1: MRR for different features and methods.line represents a converted question, in order toextract the question-type feature, we use a match-ing NER-type between the headline and candidatesentence to set question-type NER match feature.We applied pre-processing and feature extrac-tion steps of section 2 to compile labeled and un-labeled training and labeled testing datasets.
Weuse the rank scores obtained from the search en-gine as baseline of our system.
We present theperformance of the models using Mean Recipro-cal Rank (MRR), top 1 (Top1) and top 5 predic-tion accuracies (Top5) as they are the most com-monly used performance measures of QA systems(Voorhees, 2004).
We performed manual iterativeparameter optimization during training based onprediction accuracy to find the best k-nearest pa-rameter for SSL, i.e., k = {3, 5, 10, 20, 50} , andbest C ={10?2, .., 102}and ?
={2?2, .., 23}for RBF kernel SVM.
Next we describe three dif-ferent experiments and present individual results.Graph summarization makes it feasible to exe-cute SSL on very large unlabeled datasets, whichwas otherwise impossible.
This paper has no as-sumptions on the performance of the method incomparison to other SSL methods.Experiment 1.
Here we test individual con-tribution of each set of features on our QA sys-tem.
We applied SVM and our graph based SSLmethod with no summarization to learn modelsusing labeled training and testing datasets.
ForSSL we used the training as labeled and testingas unlabeled dataset in transductive way to pre-dict the entailment scores.
The results are shownin Table 1.
From section 2.2, QTCF representsquestion-type NER match feature, LexSem is thebundle of lexico-semantic features and QComp isthe matching features of subject, head, object, andthree complements.
In comparison to the baseline,QComp have a significant effect on the accuracyof the QA system.
In addition, QTCF has shownto improve the MRR performance by about 22%.Although the LexSem features have minimal se-mantic properties, they can improve MRR perfor-mance by 14%.Experiment 2.
To evaluate the performance ofgraph summarization we performed two separateexperiments.
In the first part, we randomly se-lected subsets of labeled training dataset XiL ?XL with different sample sizes, niL ={1% ?
nL,5% ?
nL, 10% ?
nL, 25% ?
nL, 50% ?
nL,100% ?
nL}, where nL represents the sample sizeof XL.
At each random selection, the rest of thelabeled dataset is hypothetically used as unlabeleddata to verify the performance of our SSL usingdifferent sizes of labeled data.
Table 2 reportsthe MRR performance of QA system on testingdataset using SVM and our graph-summary SSL(gSum SSL) method using the similarity functionin (1).
In the second part of the experiment, weapplied graph summarization on copula and non-copula questions separately and merged obtainedrepresentative points to create labeled summarydataset.
Then using similarity function in (2) weapplied SSL on labeled summary and unlabeledtesting via transduction.
We call these models asHybrid gSum SSL.
To build SVM models in thesame way, we separated the training dataset intotwo based on copula and non-copula questions,Xcp, Xncp and re-run the SVM method separately.The testing dataset is divided into two accordingly.Predicted models from copula sentence datasetsare applied on copula sentences of testing datasetand vice versa for non- copula sentences.
The pre-dicted scores are combined to measure overall per-formance of Hybrid SVM models.
We repeatedthe experiments five times with different randomsamples and averaged the results.Note from Table 2 that, when the number oflabeled data is small (niL < 10% ?
nL), graphbased SSL, gSum SSL, has a better performancecompared to SVM.
As the percentage of labeledpoints in training data increase, the SVM perfor-mance increases, however graph summary SSL isstill comparable with SVM.
On the other hand,when we build separate models for copula andnon-copula questions with different features, theperformance of the overall model significantly in-creases in both methods.
Especially in Hybridgraph-Summary SSL, Hybrid gSum SSL, whenthe number of labeled data is small (niL < 25% ?nL) performance improvement is better than rest725% SVM gSum SSL Hybrid SVM Hybrid gSum SSL#Labeled MRR Top1 Top5 MRR Top1 Top5 MRR Top1 Top5 MRR Top1 Top51% 45.2 33.2 65.8 56.1 44.6 72.8 51.6 40.1 70.8 59.7 47.0 75.25% 56.5 45.1 73.0 57.3 46.0 73.7 54.2 40.6 72.3 60.3 48.5 76.710% 59.3 47.5 76.7 57.9 46.5 74.2 57.7 47.0 74.2 60.4 48.5 77.225% 59.8 49.0 78.7 58.4 45.0 79.2 61.4 49.5 78.2 60.6 49.0 76.750% 60.9 48.0 80.7 58.9 45.5 79.2 62.2 51.0 79.7 61.3 50.0 77.2100% 63.5 55.4 77.7 59.7 47.5 79.7 67.6 58.0 82.2 61.9 51.5 78.2Table 2: The MRR (%) results of graph-summary SSL (gSum SSL) and SVM as well as Hybrid gSumSSL and Hybrid SVM with different sizes of labeled data.#Unlabeled MRR Top1 Top525K 62.1% 52.0% 76.7%50K 62.5% 52.5% 77.2%100K 63.3% 54.0% 77.2%Table 3: The effect of number of unlabeled dataon MRR from Hybrid graph Summarization SSL.of the models.
As more labeled data is introduced,Hybrid SVM models?
performance increase dras-tically, even outperforming the state-of-the artMRR performance on TREC04 datasets presentedin (Shen and Klakow, 2006) i.e., MRR=67.0%,Top1=62.0%, Top5=74.0%.
This is due to the factthat we establish two seperate entailment modelsfor copula and non-copula q/a sentence pairs thatenables extracting useful information and betterrepresentation of the specific data.Experiment 3.
Although SSL methods are ca-pable of exploiting information from unlabeleddata, learning becomes infeasible as the numberof data points gets very large.
There are vari-ous research on SLL to overcome the usage oflarge number of unlabeled dataset challenge (De-lalleau et al, 2006).
Our graph summarizationmethod, Hybrid gsum SSL, has a different ap-proach.
which can summarize very large datasetsinto representative data points and embed the orig-inal spatial information of data points, namely lo-cal density constraints, within the SSL summa-rization schema.
We demonstrate that as more la-beled data is used, we would have a richer sum-mary dataset with additional spatial informationthat would help to improve the the performanceof the graph summary models.
We gradually in-crease the number of unlabeled data samples asshown in Table 3 to demonstrate the effects on theperformance of testing dataset.
The results showthat the number of unlabeled data has positive ef-fect on performance of graph summarization SSL.6 Conclusions and DiscussionsIn this paper, we applied a graph-based SSL al-gorithm to improve the performance of QA taskby exploiting unlabeled entailment relations be-tween affirmed question and candidate sentencepairs.
Our semantic and syntactic features for tex-tual entailment analysis has individually shown toimprove the performance of the QA compared tothe baseline.
We proposed a new graph repre-sentation for SSL that can represent textual en-tailment relations while embedding different ques-tion structures.
We demonstrated that summariza-tion on graph-based SSL can improve the QA taskperformance when more unlabeled data is used tolearn the classifier model.There are several directions to improve ourwork: (1) The results of our graph summarizationon very large unlabeled data is slightly less thanbest SVM results.
This is largely due to usingheadlines instead of affirmed questions, whereinheadlines does not contain question-type and someof them are not in proper sentence form.
This ad-versely effects the named entity match of question-type and the candidate sentence named entities aswell as semantic match component feature extrac-tion.
We will investigate experiment 3 by usingreal questions from different sources and constructdifferent test datasets.
(2) We will use other dis-tance measures to better explain entailment be-tween q/a pairs and compare with other semi-supervised and transductive approaches.726ReferencesJinxiu Chen, Donghong Ji, C. Lim Tan, and ZhengyuNiu.
2006.
Relation extraction using label propaga-tion based semi-supervised learning.
In Proceedingsof the ACL-2006.Charles L.A. Clarke, Gordon V. Cormack, R. ThomasLynam, and Egidio L. Terra.
2006.
Question an-swering by passage selection.
In In: Advances inopen domain question answering, Strzalkowski, andHarabagiu (Eds.
), pages 259?283.
Springer.Oliver Delalleau, Yoshua Bengio, and Nicolas LeRoux.
2005.
Efficient non-parametric function in-duction in semi-supervised learning.
In Proceedingsof AISTAT-2005.Oliver Delalleau, Yoshua Bengio, and Nicolas LeRoux.
2006.
Large-scale algorithms.
In In: Semi-Supervised Learning, pages 333?341.
MIT Press.Sandra Harabagiu and Andrew Hickl.
2006.
Methodsfor using textual entailment in open-domain ques-tion answering.
In In Proc.
of ACL-2006, pages905?912.Zhiheng Huang, Marcus Thint, and Zengchang Qin.2008.
Question classification using headwords andtheir hypernyms.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP-08), pages 927?936.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Meeting of the ACL-2003, pages 423?430.Michael Lesk.
1988.
They said true things, but calledthem by wrong names - vocabulary problems in re-trieval systems.
In In Proc.
4th Annual Conferenceof the University of Waterloo Centre for the NewOED.Rong Liu, Jianzhong Zhou, and Ming Liu.
2006.
Agraph-based semi-supervised learning algorithm forweb page classification.
In Proc.
Sixth Int.
Conf.
onIntelligent Systems Design and Applications.George Miller.
1995.
Wordnet: A lexical database forenglish.
In Communications of the ACL-1995.Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan.2005.
Word sense disambiguation using labeledpropagation based semi-supervised learning.
InProceedings of the ACL-2005.Jahna Otterbacher, Gunes Erkan, and R. RadevDragomir.
2009.
Biased lexrank:passage retrievalusing random walks with question-based priors.
In-formation Processing and Management, 45:42?54.Eric W. Prager, John M.and Brown, Dragomir Radev,and Krzysztof Czuba.
2000.
One search engine ortwo for question-answering.
In Proc.
9th Text RE-trieval conference.Horacio Saggion and Robert Gaizauskas.
2006.
Ex-periments in passage selection and answer extrac-tion for question answering.
In Advances in naturallanguage processing, pages 291?302.
Springer.Dan Shen and Dietrich Klakow.
2006.
Exploring cor-relation of dependency relation paths for answer ex-traction.
In Proceedings of ACL-2006.Vikas Sindhwani, Wei Chu, and S. Sathiya Keerthi.2007.
Semi-supervised gaussian process classifiers.In Proceedings of the International Joint Conferenceon Artificial Intelligence (IJCAI-07), pages 1059?1064.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervisedsequential labeling and segmentation using giga-word scale unlabeled data.
In Proceedings of theACL-2008.Nguyen Thanh Tri, Nguyen Minh Le, and Akira Shi-mazu.
2006.
Using semi-supervised learning forquestion classification.
In ICCPOL, pages 31?41.LNCS 4285.Vilademir Vapnik.
1995.
The nature of statisticallearning theory.
In Springer-Verlag, New York.Ellen M. Voorhees.
2003.
Overview of the trec 2003question answering track.
In Proc.
12th Text RE-trieval conference.Ellen M. Voorhees.
2004.
Overview of trec2004 ques-tion answering track.Dengyong Zhou and Bernhard Scho?lkopf.
2004.Learning from labeled and unlabeled data using ran-dom walks.
In Proceedings of the 26th DAGM Sym-posium, (Eds.)
Rasmussen, C.E., H.H.
Blthoff, M.A.Giese and B. Schlkopf, pages 237?244, Berlin, Ger-many.
Springer.Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Ja-son Weston, and Bernhard Scho?lkopf.
2004.
Learn-ing with local and global consistency.
Advancesin Neural Information Processing Systems, 16:321?328.Xiaojin Zhu, John Lafferty, and Zoubin Ghahramani.2003.
Semi-supervised learning: From Gaus-sian Fields to Gaussian processes.
Technical Re-port CMU-CS-03-175, Carnegie Mellon University,Pittsburgh.727
