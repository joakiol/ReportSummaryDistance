Fully Lexicalized Head-Driven Syntactic GenerationT i lman BeckerGerman Research Center for Artificial Intell igence (DFKI  GmbH)Stuhlsatzenhausweg 3, 66123 Saarbriicken, Germanybecker@dfk i ,  deAbstractWe describe a new approach to syntactic generation with Head-Driven Phrase StructureGrammars (HPSG) that uses an extensive off-line preprocessing step.
Direct generation algo-?
rithms apply the phra~se-structure rul s (schemata) of the grammar on:line which is an com-putationally expensive step.
Instead, we collect off-line for every lexical type of the HPSGgrammar all minimally complete projections (called elementary trees) that can be derived withthe schemata.
This process is known as 'compiling HPSG to TAG' and derives a LexicalizedTree-Adjoining Grammar (LTAG).
The representation as an LTAG is 'fully lexicalized' in thesense that all grammatical information is directly encoded with the lexical item (as a set ofelementary trees) and the combination operations are reduced from schema pplications to theTAG primitives of adjunction and substitution.
Given this LTAG, the generation task has avery different search space that Can be traversed very efficiently, avoiding the costly on-lineapplications of HPSG unification.
The entire generation task from a semantic representationto a surface string is split into two tasks, a microplanner and a syntactic realizer.
This paperdiscusses the syntactic generator and the preprocessing steps as implemented in the Verbmobilsystem.1 Generat ion  in a Speech- to -Speech  SystemThe syntactic generation algorithm and the preprocessing steps presented in this paper are inte-grated into the Verbmobil system (see \[Wahlster 1993, Bub; Wahlster, and Waibel 1997\]).
It is asystem for speech-~to-speech dialog translation.
The input for the generation module VM-GECO 1is generated by a semantic-based transfer component (see \[Dorna nd Emele 1996\]).
The inter-face language chosen comprises the encoding of target language-specific semantic information in acombination of Underspecified Discourse Representation Theory and Minimal Recursion Semantics(see \[Bos et al 1996\] and \[Copestake, Flickinger, and Sag 1997\]).The internal architecture of the generation module is modularized: it is separated into two phases, amicroplanner and a syntactic generator.
Throughout the system, we emphasize declarativity, whichis also a necessary precondition for a comprehensive off-line preprocessing of external knowledgebases-in particular the preprocessing of the underlying Head-Driven Phrase Structure Grammar(HPSG, see \[Pollard and Sag 1994\]) which has been developed at CSLI, reflecting the latest devel-opments in the linguistic theory and with a fairly wide coverage and also covering phenomena ofspoken language.1VerbMobil GEneration COmponents?208,!!!
!i2 Microplanning and Syntactic GenerationStarting from the semantic representation, the microplanning component generates an annotateddependency structure which is used by the syntactic generation component to realize a surfacestring.
The microplanner also carries out word-choice.One goal of this modularization is a stepwise constraining of the search-space of alternative lin-guistic realizations, using different views in the different modules.
In each step, only an abstractionof the multitude of information contained in an alternative needs to be considered.Another aspect of this architecture is the separation into a kernel system, i.e., the language in-dependent core algorithms (a constraint-solver for microplanning and the search and combinationalgorithms for syntactic generation described in section'5) and declarative knowledge bases, e.g.,the language specific word-choice constraints in microplanning and the TAG grammars used in syn-tactic realization.
This separation allows for an easy adaptation of the system to other languagesand domains (see \[Becker et al 1998\]).3 Declarativity in the Syntactic GeneratorAll modules of the generator utilize external, declarative knowledge bases.
For the syntactic gen-erator, extensive off-line preprocessing of the highly declarative HPSG grammar for English 2 isapplied.
The grammar has not even been written exclusively as a generation grammar 3.
It isspecialized, however, in that it covers phenomena ofspoken language.
The high level of abstractionwhich is achieved in the hierarchically organized grammar description (see \[Flickinger 1987\]) allowsfor easy maintenance aswell as off-line preprocessing.The off-line preprocessing steps described in the next section keep the declarative nature of thegrammar intact, Le.
they retain explicitly the phrase structures and syntactic features as definedby the HPSG grammar.In general, declarative knowledge bases allow for an easier adaptation of the system to otherdomains and languages.
This is a huge benefit in the current second phase of the Verbmobil project\[Becker et al 1996\] where the generator is extended to cover German, English and Japanese as wellas additional and extended omains with a considerably arger vocabulary.4 Off-Line Preprocessing: HPSG to TAG Compilat ionThe subtasks in a direct syntactic generator based on an HPSG grammar will always include theapplication of schemata (the HPSG equivalent of phrase structure rules) such that all syntacticconstraints introduced by a lexical item (especially its SUBCAT l ist)are fulfilled.
This resultsin a constant repetition of, e.g., building up the projection of a verb in a declarative sentence.In preprocessing the HPSG grammar we aim at computing all possible partial phrase structureswhich can be derived from the information in a lexicon entry.
Given such sets of possible syn-tactic realization together with a set of selected lexicon entries for an utterance and finally theirdependencies, the task of a syntactic generator is simplified considerably.
Instead of exploring all2The HPSG grammar is being developed at CSLI, Stanford University.
Development is carried out on a grammardevelopment platform which is based on TDL \[Krieger and Sch?fer 1994\].3In fact, most of the testing during grammar development depends on the use of a parser.209possible, computationally expensive applications of HPSG schemata, it merely has to find suitableprecomputed syntactic structures for each lexical item and combine them appropriately.For this preprocessing of the HPSG grammar, we adapted the 'HPSG to TAG compilation' processdescribed in \[Kasper et al 1995\].
The basis for the compilation is an identification of syntacticallyrelevant selector features which express ubcategorization requirements of a lexical item, e.g.
theVALENCE features.
In general, a phrase structure is complete when these selector features areempty.Starting from the feature structure for a lexical item, HPSG schemata re applied such that thecurrent structure is unified with a daughter feature of the schema.
The resulting structure is againsubject to this process: This compilation process tops when certain termination criteria are met,e.g., when all selector features are empty.
Thus, all projections from the lexical item are collectedas a set of minimally complete phrase structures which can also be interpreted as elementary treesof a Tree-Adjoining Grammar (TAG).Instead of actually applying this compilation ?process to all lexical items, certain abstractions overthe lexical entries are specified in the HPSG grammar.
In fact, the needs of the compilation processhave led to a clear-cut separation of lexica!
types and lexical entries as shown in Figure 1.
Atypical exical entry is shown in Figure 2 and demonstrates that only three kinds of information arestored: the lexical type MV_NP_TRANS_LE 4, the semantic ontribution (th e relation _SUIT_REL)and morphological information (the stem and potentiallyirregular fo ms): By expanding the lexicaltype, the full feature structure can be obtained.Lexicon Hierarchy - Phrase Structure\[ syntac,,c \] \[ sem nt, c \]Types Types\[,Morphological L Infomation JI Lex iCa~x 200 types~__._\[ Lexical \]Instanceapprox.
2,900entriesHPSG \]PrinciplesSchemata \]approx.
25 schemataFigure 1: Organization of the HPSG grammar.Some of the trees which result from the preprocessing of the lexical type MV_NP_TRANS_LEare shown in Figure 3.
The figure Shows only the phrase structure and an abstraction of the4MV_NP_TRANS_LE is an abbreviation for"Main Verb, NP object, TRANSitive Lexical Entry" used in sentenceslike "Monday suits me.
"210II,I!Isuit_vl := mv_np_trans_le\[ STEM < "suit" >,SYNSEM.LOCAL.CONT.STEMLISZT < !
\[ PRED _suit_tel \] !>\ ] .Figure 2: Specification of a lexical instance for the verb "suit.
"node's categories.
All nodes still represent he full HPSG ?feature structures.
E.g., the treeMV_NP_TRANS_LE.2 of Figure 3 represents an imperative clause.
As a consequence PERSONhas the value SECOND and CL-MODE is set to IMPERATIVE.
Note that the compilation processstopped at this node since the selector features are empty.MV NP TRANS_LE.1 MV NP_TRANS_LE.2 MV NP TRANS_LE.3 MV NP TRANS_LE.4vPV NP$I?
MV NP TRANS_LEs s sVP NP.S.C P NP.S.COMP $ VPI I IMV NP TRANS_LE MV NP TRANS_LE MV NP TRANS_LEFigure 3: Some of the trees for transitive verbs.
They are compiled from the corresponding lexicaltype MV_NP_TRANS_/E as defined in the HPSG grammar.
Trees 3 and 4 differ only with respectto their feature structures which are not shown in this figure.From these trees, two kinds of knowledge bases are built.
For the microplanner, the relation betweenthe lexical and syntactic realization and the semantic representation (encoded in the SYNSEMLOCAl CONT feature) is extracted as a constraint.
For the syntactic generator, the relevantsyntactic information is extracted in the form of a Feature-Based Lexicalized TAG (FB-LTAG)grammar, see \[Joshi 1987, Vijay-Shanker and Joshi 1991, Schabes, Abeill4, and Joshi 1988\].
Thisincludes the phrase structure and a selected part of the feature structure (mainly the SYNSEMLOCAL CAT and SYNSEM NON-LOCAL features).
Figure 4 shows the bottom feature structureextracted from the root node of MV_NP_TRANSJE.2.
Note that some of the feature paths areabbreviated, e.g.
5LCI stands for SYNSEM LOCAL CONT INDEX.
The elementary TAG trees whichare built from the compilation result have so-called restricted ?feature structures which can beexploited for an efficient, specialized unification algorithm.The node names hown in the figures represent a disjunction of possible categories, e.g.
NP.S.COMPin tree MV_NP_TRANS_LE.3 implies that the subject of a transitive verb may be a nominal orsentential phrase.211Bottom Dag at selected node::ROOT: \[SLC: \[HEAD: \[FRD: (- .
)\]\[MOOD: (SUBJUNCTIVE MODAISUB} Ih~ICATIVE)\]\[VOICE: (PASSIVE ACTIVE)\]\['mNSE: (FUnma PAST PR~Sln, rO\]\[worv~t Bs~Ire'v: -I \[AUX'.-\]\[ROOT: .\]\[CL-MODE: IMPERATIVE\]\[RULE: IMPERATIVE_RULE\]\[SLCI: NIL\]\[SY~EM: It,tON-LOCAL: \[Qtm: -\]Figure 4: The bottom feature structure of the S node of tree MV.NP_TRANS_LE.2.Finally, the leaf nodes of the trees (except for the lexical item itself) are marked either as substi-tution nodes or as a foot node, thus creating an auxiliary tree.
In a TAG derivation, substitutionnodeS are replaced with trees bearing the correct category and a Unifiable feature structure at theirroot node.
Auxiliary trees can be inserted into other trees by the adjunction operation.5 The Syntactic Generator VM-GIFTThe task of the syntactic generator is the construction of a sentence (or phrase, given the oftenincomplete utterances in spoken dialogs) from the microplanning result which is then sent to aspeech-synthesis component.
It proceeds in three major steps which are also depicted in Fig.
5.?
A tree selection phase determines the set of relevant TAG trees.
A first tree retrieval stepmaps every object o f  the dependency tree into a set of applicable lementary TAG trees.
Themain tree selection phase uses information from the microplanner output to further efine theset of retrieved trees.?
A combination phase finds a successful combination of trees to build a (derived) phrasestructure tree.
* An inflection phase uses the information in the feature structures of the leaves (i.e.
the words)to apply appropriate morphological functions, including the use of irregular forms as providedby the HPSG lexiconand regular inflection function as supplied (as LISP code) by the HPSGgrammar.An initial preprocessing phase computes: the necessary auxiliary verbs from the tense, aspect,and sentence mood information.
It also rearranges the dependency tree accordingly (e.g.
subjectarguments are moved from the main verb to become dependents of the inflected auxiliary verb).The two core phases are the tree selection and the tree combination phase.
The tree selectionphase consists of two steps.
First, a set of possible trees is retrieved and then appropriate trees areselected from this set.
The retrieval is driven by the HPSG instance or word class that is suppliedby the microplanner.
It is mapped to a lexical type by a lexicon that is automatically compiled fromthe HPSG grammar.
The lexical types are then mapped to a tree family, i.e., a set of elementaryTAG trees representing all possible minimally complete phrase structures that can be  build fromthe instance.
The  additional information in the dependency tree is then used to add further feature212-t .otDe -e -a )Preprocessing I I Tree selection I I Tree combination { I(expand auxiliaries)~\] and sorting \ [ - -~ (adjoining and substitution) ~-~ Inflecti?n \[IrregularFigure 5: Steps of the syntactic generator.t : ) .Ola0t -values to the trees.
This additional information acts as a filter for selecting appropriate trees intwo stages:?
Some values are incompatible with values already present in the trees.
These trees cantherefore be filtered immediately from the set.
E.g., a syntactic structure for an imperativeclause is marked as such by a feature and can be discarded if a declarative sentence is to begenerated.?
Additional features can prevent he combination with other trees during the combinationphase.
This is the case, for example with agreement features.The combination phase explores the search space of all possible combinations of trees from thecandidate sets for each lexical item (instance).
An inefficient combination phase is a potentialdrawback of using the precomputed TAG trees.
However, there is sufficient information availablefl'om the microplanner result and from the trees such that a well:guided best-first search strategycan be employed in the current system.
The difference in run-time can be as dramatic as 24 seconds(comprehensive breadth-first) versus 1.5 seconds (best-first).As part of the tree selection phase, based on the rich annotation of the input structure, the tree setsare sorted locally.
Then a backtracking algorithm traverses the dependency tree in a bottom-upfashion s. At each node, and for each subtree in the dependency tree, a candidate for the phrasestructures of the subtree is constructed.
Then all possible adjunction or substitution sites arecomputed, possibly sorted (e.g.
allowing for preferences in word order) and the best candidate fora combined phrase structure is returned.
Since the combination of two partial phrase structuresby adjunction or substitution might fail due to incompatible feature structures , a backtracking5The algorithm stores intermediate results with a memoization technique.213algorithm must be used.
A partial phrase structure for a subtree of the dependency is finally checkedfor completeness.
These tests include the unifiability of all top and bottom feature structures andthe satisfaction of all other constraints (e.g.
obligatory adjunctions or open substitution odes)since no further adjunctions or substitutions will occur in this subtree.The  necessity of a spoken dialog translation system to produce output robustly calls for somerelaxations in these tests.
E.g., 'obligatory' arguments may be missing in the utterance and thetests in the syntactic generator must accept a sentence with a missing obligatory object if no othercomplete phrase can be generated.Figure 6 shows an example of the input of from the microplanner after the preprocessing phase hasinserted the entity LGV1 for the auxiliary will.
( (ENTITY LGVI((CAT V) (HEAD.WILL_AUX_POS) (INTENTION WH-QUESTION) (FUNC AUX)(TENSE?
FUTUP~) (MOOD INDICATIVE) (VOICE ACTIVE) (FORM OP, DINARY)(VPORM FIN)))(ENTITY LS-WORK_ACCEPTABLE.
((FORM OKI)INARY) (VFOKM BSE) (CAT V) (GOVE~-BY  WH-SENTENCE)(OPTIONAL-AGENT NO) (HEAD (OR SUIT_VI SUIT_V2)) (REALIZED LOCAL)(KEG LGVl)))(ENTITY LI3-PRON((REALIZED LOCAL) (CAT PPRON) (PERS 3) (NUM SG) (GENDER NTR)(TYPE NORMAL) (GOVERNED-BY V) (IS-COMPLEMENT T) (FORM CONTINUOUS)(KEG LGVI) (FUNC AGENT)))(ENTITY LI0-PRON((REALIZED LOCAL) (CAT PPRON) (PERS 2A) (NUM SG) (GENDER FEM) (TYPE NORMAL)(GOVERNED-BY (0R V PREP SENTENCE)) (FORM CONTINUOUS) (KEG L5-WORK_ACCEPTABLE)(FUNC ?
PATIENT) ) )(ENTITY L6-TEMP_LOC((CA T ADV) (REAL WH_QUEST) (SORT TIME) (POINTED'BY TEMP_LOC)(GOVERNED-BY (0R V N ADV SENTENCE)) (PRED TIME) (HEAD WHEN1)?
(REALIZED L0CAL) (WH-FOCUS T) (KEG L5-WORE_ACCEpTABLE) (FUNC TEMP-SPEC)))(ENTITY LI5-TEMP_LOC((CAT ADV) (BEAD THEN_ADV) (REALIZED GRouP-TIME-DEMONSTRATiVE)(REAL (0R ADV WH_QUEST YOFC)) (SORT (SUBSORT TIME)) (POINTED-BY TEMP_LOC)(GOVERNED-BY (OR V N ADV SENTENCE)) (BEG LS-WORK_ACCEPTABLE) (FUNC TEMP-SPEC))))?
.
.
.~  :Figure 6: Example of the input from microplanning after preprocessing for auxiliariesIn the tree retrieval phase for L5-WORK_ACCEPTABLE, first the HEAD information is used to deter-mine the lexical types of the possible realizations SUIT_V1 and SUIT_V2, namely MV_NP_TRANS_LE andMV_EXPL_PREP_TRANSIE respectively.
These types are then mapped to their respective sets of ele-mentary trees, a total of 25 trees.
In the tree selection phase (as described above), this numberis reduced to six.
For example, the tree MV_NP_TRANS_L?.2 in Figure 3 has a feature CL-MOD?with the value IMPERATIVE Now, the microplanner output for the root entity LGV1 contains theinformation (INTENTION WH-QUESTION) The NTENTION information is unified with all appropriateCk-MOD?
features, which in this case fails.
Thereforethe tree MV_NP_TRANS_k?.2 can be dis-carded in the tree selection phas e .214The combination phase uses the best-first bottom-up algorithm described above to determine onesuitable tree for every entity and also a target node in the tree that is selected for the governingentity.
For the above example, the selected trees and their combination odes are shown in Figure7 6 ,A0 " "% %,?
"~ %, V VP/ADV VP VPI ~ ' '~  1%I "  % " l. %ADV Y NP $ .
.  "
" NP Y NP J, ,' .NP VP ADVI I '-",- I I ''1,-, Iwhen will it suit you thenL6-TEMP_LOC LGVI L I3 -PRON L5-SUIT L I0-PRON LI5-TEMP_LOCFigure 7: The trees finally selected for the entities Of the example sentence.
The dashed linesconnect o suitable substitution or adjunction odes.
They correspond to the dependency tree.The inflection function finally uses attribute values like verb-form, number and person from thefinal tree to derive the correct inflections.
Information about the sentence mode WH-QUESTIONcan be used to annotate the resulting string for the speech-synthesis module.6 Conclusion and ComparisonWe have shown how preprocessing an HPSG grammar can be used to avoid the costly on-line ap-plication (unification) of HPSG schemata in a modularized generation system with a microplannerand a separate syntactic generator.
The compilation of an HPSG grammar to TAG grammar allowsthe use of an efficient syntactic generator without sacrificing the declarative nature of the HPSGgrammar.It is important o compare the generation strategy presented here with Semantic-head-driven g -eration \[Shieber et al 1990, van Noord 1990\] which is a direct generation algorithm froni logicalform encodings.
It improves previous algorithms in efficiency and in imposing less restrictions onthe type of grammar.
It is also applicable to HPSG and proceeds by applying the HPSG schematain a bottom-up fashion, driven from the lexical heads of the schemata.To a large ex.tend, the TAG-based generation algorithm presented here goes through the same stepsas semantic-head-driven g eration.
However, most of ?these steps will have been made during theoff-line preprocessing and are encoded in the elementary trees of the TAG grammar thns resulting6Note that  the node labels shown in Figures 7 are only a concession to readabil ity.
The  TAG requirement thatin an auxi l iary tree the foot node must  have the same category label as the root  node is formal ly fulfilled in ourimplementat ion.215in an important gain in efficiency.
Note though, that the generation task in the algorithm presentedhere is shared between the micr0planner and the syntactic generator,-so a formal comparison mustinclude both components.Work on generation with TAG generally assumes that there is a one, to~-one mapping between the in-formation in the generator input and the choice of elementary tree \[Mcdonald and Pustejovsky 1985,Yang, McCoy, and Vijay-Shanker 1991, D0ran and Stone 1997\].
In general, this will not be thecase.
In particular, in our system the input is not always sufficiently analyzed and the preprocess-ing froman HPSG grammar potentially ?creates more than one elementary tree that fits the inputparameters.One possible approach are choice nets-see \[Yang, McCoy, and Vijay-Shanker 1991\] who interpretsystemic grammar in this way.
Our approach as some similarity, though we have provided a moregeneral algorithm that does not require the specification of grammar specific choice nets but ratherexecutes tree Selection and combination from more declarative knowledge bases.
Tree selection isimplemented mainly by unification (adding feature values from the input specification to the treeswhere unifiable) and the best-first search algorithm is a general framework for handling sets ofpossible lementary trees, including backtracking steps when non-local tests (e.g.
unification in theresulting derived tree) fail.
This approach is also a precondition in our system since we have nodirect access to the TAG grammar as it is automatically preprocessed from an HPSG grammar.VM-GECO is fully implemented (in Common Lisp) and integrated into the speech-to-speechtranslation system Verbmobil for Enghsh and German.
For example, the underlying English HPSGgrammar has almost 3000 iexical entries with over 200 lexical types.
The resulting lexicalized TAGconsists of about 2800 trees.
The average overall generation time per sentence (up to length 24) is0.7 cpu ?seconds on a SUN ULTRA-1 machine, 68% of the runtime are used for tile microplanningwhile tile remaining 32% of the runtime are used for syntactic generation.7 Cur rent  WorkIn general, the task of finding appropriate lementary trees for the chosen words and consequentlya consistent phrase structure tree can exhibit constraints between an), two elementary trees inthe utterance (as expressed through feature equations).
?However, most of these constraints existbetween elementary trees that are combined irectly with each other (adjoined or substituted).
Toexploit thiSi we are currently experimenting with various well established binary constraint-solvingalgorithms to preselect elementary trees that are pairwise consistent w.r.t, feature equations.References\[Becket et al 1996\] Becket, T. W. Finkler, A. K!lger, and W. Wahlster.
1996.
Vorhabensbeschreibung zurSprachgenerierung i nerhalb des Teilprojektes 5 (Sprachgenerierung d -synthese) in Verbmobil, Phase2.
Document, German Research Center for Artificial Intelligence (DFKI GmbH), Saarbriicken, Germany,AUgust.\[Becke r et al 1998\] Becker, Tilman, Wolfgang Finkler, Anne Kilger, and Peter Poller.
1998.
An efficientkernel for multilingual generation i speech-to-speech dialogue translation.
In Proceedings of COLING-ACL 98, Montreal, Canada.IIII216\[Bos et al 1996\] Bos, J., B. Gamb~ick, C. Lieske, Y. Mori, M. Pinkal, and K. Worm.
1996.
Composi-tional semantics in verbmobil.
Technical report, University of the Saarland, Computational Linguistics,Saarbrficken, July.
Verbmobil Report 135.\[Bub, Wahlster, and Waibel 1997\] Bub, Th.
W. Wahlster, and A. Waibel.
1997.
Verbmobil: The combina-.tion of deep and shallow processing for spontaneous speech translation.
In Proceedings of ICASSP '97.
(forthcoming).\[Copestake, Flickinger, and Sag 1997\] Copestake, Ann, Dan Flickinger, and Ivan A.
Sag.
1997.
Minimalrecursion semantics: An introduction, available at f tp : / / cs l i - f tp ,  stanford, edu/ l ingu is t i cs /sag/ -mrs.ps, gz.
'\[Doran and Stone 1997\] Doran, Christy and Matthew Stone.
1997.
Sentence planning as description usingtree adjoining rammar.
In ACL-EACL, Madrid, Spain, July.\[Dorna nd Emele 1996\] Dorna, M. and M. Emele.
1996.
Semantic-based transfer.
In Proceedings of the16th International Conference on Computational Linguistics (COLING '96).\[Flickinger 1987\] Flickinger, Daniel P. 1987.
Lexical Rules in the Hierarchical Lexicon.
Ph.D. thesis, StanfordUniversity.\[Joshi 1987\] Joshi, Aravind K. 1987.
An introduction tO Tree Adjoining Grammars.
In A. Manaster-Ramer,?
editor, Mathematics of Language.
John Benjamins, Amsterdam.\[Kasper et al 1995\] Kasper, R., B. Kiefer, K. Netter, and K. Vijay-Shanker.
1995.
Compilation of HPSG toTAG.
In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages92-99, Cambridge, Mass.\[Krieger and Sch~fer 1994\] Krieger, Hans-Ulrich and Ulrich Sch~ifer.
1994.
779?--a type description languagefor constraint-based grammars.
In Proceedings of the 15th International Conference on ComputationalLinguistics, COLING-9g, pages 893-899.\[Mcdonald and Pustejovsky 1985\] Mcdonald, David D. and James D. Pust'ejovsky.
1985.
Tags as a gram-matical formalism for generation.
In Proc.
of the 23 th ACL, Chicago, IL.\[Pollard and Sag 1994\] Pollard, Carl and Ivan A.
Sag.
1994.
Head-Driven Phrase Structure Grammar.Studies in Contemporary Linguistics.
University of Chicago Press, Chicago.\[Schabes, Abeill4, and Joshi 1988\] Schabes, Y., A. Abeill4, and A.K.
Joshi.
1988.
Parsing strategies with'lexicalized' grammars: Application to Tree Adjoining Grammars.
In Proc.
12th bzternational Conferenceon Computational Linguistics (COLING-88}, pages 578-583, Budapest, August.\[Shieber et al 1990\] Shieber, Stuart, Gertjan Van Noord, Fernando Pereira, and Robert Moore.
1990.Semantic-head-driven g eration.
Computational Linguistics Vol.
16 No.
1, 16(1):30-43.\[van Noord 1990\] van Noord, Gertjan.
1990.
An overview of head-driven bottom-up generation.
In RobertDale, Chris Mellish, and Michael Zock, editors, Current Research in Natural Language Generation.
Aca-demic Press, New York, pages 141-165.\[Vijay-Shanker and Joshi 1991\] Vijay-Shanker, K. and Aravind K. Joshi.
1991.
Unification Based TreeAdjoining Grammars.
In J. Wedekind, editor, Unification-based Grammars.
MIT Press, Cambridge, Mas-sachusetts.\[Wahlster 1993.\] Wahlster, W. 1993.
Verbmobil: Translation of face-to-face dialoges.
In MT Summit IV,Kobe, Japan.\[Yang, McCoy, and Vijay-Shanker 1991\] Yang, Gijoo, Kathleen F. McCoy, and K. Vijay-Shanker.
1991.From functional specification to syntactic structures: Systemic grammar and tree adjoining grammar.Computational Intelligence, 7(4):207-219, November.217
