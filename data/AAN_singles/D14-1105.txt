Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 974?979,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsPOS Tagging of English-Hindi Code-Mixed Social Media ContentYogarshi Vyas?University of Marylandyogarshi@cs.umd.eduSpandana Gella?Xerox Research Centre Europespandanagella@gmail.comJatin Sharma Kalika Bali Monojit ChoudhuryMicrosoft Research India{jatin.sharma,kalikab,monojitc}@microsoft.comAbstractCode-mixing is frequently observed inuser generated content on social media,especially from multilingual users.
Thelinguistic complexity of such content iscompounded by presence of spelling vari-ations, transliteration and non-adheranceto formal grammar.
We describe ourinitial efforts to create a multi-level an-notated corpus of Hindi-English code-mixed text collated from Facebook fo-rums, and explore language identifica-tion, back-transliteration, normalizationand POS tagging of this data.
Our re-sults show that language identification andtransliteration for Hindi are two majorchallenges that impact POS tagging accu-racy.1 IntroductionCode-Switching and Code-Mixing are typicaland well-studied phenomena of multilingual so-cieties (Gumperz, 1964; Auer, 1984; Myers-Scotton, 1993; Danet and Herring, 2007;Cardenas-Claros and Isharyanti, 2009).
Lin-guists differentiate between the two, whereCode-Switching is juxtaposition within the samespeech exchange of passages of speech be-longing to two different grammatical systemsor sub-systems (Gumperz, 1982), and Code-Mixing (CM) refers to the embedding of linguis-tic units such as phrases, words and morphemesof one language into an utterance of another lan-guage (Myers-Scotton, 1993).
The first exam-ple in Fig.
1 features CM where English wordsare embedded in a Hindi sentence, whereas thesecond example shows codeswitching.
Here, wewill use CM to imply both.
Work on computa-?This work was done during authors?
internship at Mi-crosoft Research India.tional models of CM have been few and far be-tween (Solorio and Liu, 2008a; Solorio and Liu,2008b; Nguyen and Dogruoz, 2013), primarilydue to the paucity of CM data in conventionaltext-corpora which makes data-intensive methodshard to apply.
Solorio and Liu (2008a) in theirwork on English-Spanish CM use models built onsmaller datasets to predict valid switching pointsto synthetically generate data from monolingualcorpora, and in another work (2008b) describeparts-of-speech (POS) tagging of CM text.CM though typically observed in spoken lan-guage is now increasingly more common in text,thanks to the proliferation of the Computer Me-diated Communication channels, especially so-cial media like Twitter and Facebook (Crys-tal, 2001; Herring, 2003; Danet and Herring,2007; Cardenas-Claros and Isharyanti, 2009).Social media content is tremendously importantfor studying trends, reviews, events, human-behaviour as well as linguistic analysis, and there-fore in recent times has spurred a lot of interestin automatic processing of such data.
Neverthe-less, CM on social media has not been studiedfrom a computational aspect.
Moreover, socialmedia content presents additional challenges dueto contractions, non-standard spellings and non-grammatical constructions.
Furthermore, for lan-guages written in scripts other than Roman, likeHindi, Bangla, Japanese, Chinese and Arabic, Ro-man transliterations are typically used for repre-senting the words (Sowmya et al., 2010).
This canprove a challenge for language identification andsegregation of the two languages.In this paper, we describe our initial efforts toPOS tag social media content from English-Hindi(henceforth En-Hi) bilinguals while trying to ad-dress the challenges of CM, transliteration andnon-standard spelling, as well as lack of anno-tated data.
POS tagging is one of the fundamen-tal pre-processing steps for NLP, and while there974have been works on POS tagging of social mediadata (Gimpel et al., 2011; Owoputi et al., 2013)and of CM (Solorio and Liu, 2008b), but we donot know of any work on POS tagging of CMtext from social media that involves transliteration.The salient contributions of this work are in for-malizing the problem and related challenges forprocessing of En-Hi social media data, creationof an annotated dataset and some initial experi-ments for language identification, transliteration,normalization and POS tagging of this data.2 Corpus CreationFor this study, we collected data from Face-book public pages of three celebrities: AmitabhBachchan, Shahrukh Khan, Narendra Modi, andthe BBC Hindi news page.
All these pages arevery popular with 1.8 to 15.5 million ?likes?.
A to-tal of 40 posts were manually selected from thesepages, which were published between 22nd ?
28thOctober 2013.
The posts having a long thread ofcomments (50+) were preferred, because CM andnon-standard usage of language is more commonin the comments.
We shall use the term post to re-fer to either a post or a comment.
The corpus thuscreated has 6,983 posts and 113,578 words.
Thedata was semi-automatically cleaned and format-ted.
The user names were removed for anonymity,but the names appearing in comments, which aremostly of celebrities, were retained.2.1 AnnotationThere are various interesting linguistic as well associo-pragmatic features (e.g., user demograph-ics, presence of sarcasm or humor, polarity) forwhich this corpus could be annotated because CMis influenced by both linguistic as well as extra-linguistic features.
However, initial attempts atsuch detailed and layered annotation soon revealedthe resource-intensiveness of the task.
We, thus,scaled down the annotation to the following fourlayers:Matrix: The posts are split into contiguousfragments of words such that each fragment hasa unique matrix language (either En or Hi).
Thematrix language is defined as the language whichgoverns the grammatical relation between the con-stituents of the utterance.
Any other languagewords that are nested into the matrix constitute theembedded language(s).
Usually, matrix languagecan be assigned to clauses or sentences.Word origin: Every word is marked for its ori-gin or source language, En or Hi, depending onwhether it is an English or Hindi word.
Words thatare of neither Hindi nor English origin are markedas Ot or Other.
Here, we assume that code-mixingdoes not happen at sublexical levels, as it is un-common in this data; Hi and En have a sim-pler inflectional morphology and thus, sub-lexicalmixing though present (e.g., computeron hasa En root - computer and a Hi plural markeron) is relatively less common.
In languages withricher morphology and agglutination, like Banglaand most Dravidian languages, more frequent sub-lexical mixing may be observed.
Also note thatwords are borrowed extensively between Hi andEn such that certain English words (e.g., bus,party, vote etc) are no longer perceived as Englishwords by the Hindi speakers.
However, here wewill not distinguish between CM and borrowing,and such borrowed English words have also beenlabeled as En words.Normalization/Transliteration: Whenever theword is in a transliterated form, which is often thecase for the Hi words, it is labeled with the in-tended word in the native script (e.g., Devanagarifor Hi).
If the word is in native script, but usesa non-standard spelling, it is labeled with the cor-rect standard spelling.
We call this the spellingnormalization layer.Parts-of-Speech (POS): Finally, each word isalso labeled with its POS.
We use the UniversalPOS tagset proposed by Petrov et al.
(2011) whichhas 12 POS tags that are applicable to both Enand Hi.
The POS labels are decided based on thefunction of a word in the context, rather than adecontextualized lexical category.
This is an im-portant notion, especially for CM text, because of-ten the original lexical category of an embeddedword is lost in the context of the matrix language,and it plays the role of a different lexical category.Though the Universal POS tagset does not pre-scribe a separate tag for Named Entities, we feltthe necessity of marking three different kinds ofNEs - people, location and organization, becausealmost every comment has one or more NEs andstrictly speaking word origin does not make sensefor these words.Annotation Scheme: Fig.
1 illustrates the an-notation scheme through two examples.
Eachpost is enclosed within <s></s> tags.
Thematrices within a post are separated by the<matrix></matrix> tags which take the matrixlanguage as an argument.
Each word is anno-975Figure 1: Two example annotations.tated for POS, and the language (/E or /H for Enor Hi respectively) only if it is different from thelanguage of the matrix.
In case of non-standardspelling in English, the correct spelling is ap-pended as ?sol NOUN=soul?, while for theHindi words, the correct Devanagari translitera-tion is appended.
The NEs are marked with thetags P (person), L (location) or O (organization)and multiword NEs are enclosed within squarebrackets ?
[]?.A random subsample of 1062 posts consistingof 10171 words were annotated by a linguist whois a native speaker of Hi and proficient in En.
Theannotations were reviewed and corrected by twoexperts linguists.
During this phase, it was alsoobserved that a large number of comments werevery short, typically an eulogism of their favoritecelebrity and hence were not interesting from alinguistic point of view.
For our experiments, weremoved all posts that had fewer than 5 words.The resulting corpus had 381 comments/posts and4135 words.2.2 CM DistributionMost of the posts (93.17%) are in Roman script,and only 2.93% were in Devanagari.
Around 3.5%of the posts contain words in both the scripts (typ-ically a post in Devanagari with hashtags or urls inRoman script), and a very small fraction of the text(0.4% of comments/posts and 0.6% words) was insome other script.
The fraction of words presentin Roman and Devanagri scripts are 80.76% and15.32% respectively, which shows that the De-vanagari posts are relatively longer than the Ro-man posts.
Due to their relative rarity, the postscontaining words in Devanagari or any other scriptwere not considered for annotation.In the annotated data, 1102 sentences are in asingle matrix (398 Hi, 698 En and 6 Ot) and in45 posts there is at least one switch of matrix(mostly between Hi and En.
Thus, 4.2% of thedata shows code-switching.
This is a strict defi-nition of code-switching; if we consider a changein matrix within a conversation thread as a code-switch, then in this data all the threads exhibitcode-switching.
However, out of the 398 com-ments in Hi-matrix, 23.37% feature CM (i.e., theyhave at least one or more non-Hi (or rather, al-most always En) words embedded.
On the otherhand, only 7.34% En-matrix comments featureCM (again almost always with Hi).
Thus, a totalof 17.2% comments/posts, which contains a quar-ter of all the words in the annotated corpus, fea-ture either CM or code-switching or both.
We alsonote that more than 40% words in the corpus arein Hi or other Indian languages, but written in Ro-man script; hence, they are in transliterated form.See (Bali et al., 2014) for an in-depth discussionon the characteristics of the CM data.This analysis demonstrates the necessity of CMand transliterated text processing in the context ofIndian user-generated social media content.
Per-haps, the numbers are not too different for suchcontent generated by the users of any other bilin-gual and multilingual societies.3 Models and ExperimentsPOS tagging of En-Hi code-mixed data requireslanguage identification at both word and matrixlevel as well back-transliteration of the text into976Actual Predicted Label RecallLabel Hi EnHi 1057 515 0.672En 45 2023 0.978Precision 0.959 0.797Table 1: Confusion matrix, precision and recall ofthe language identification module.the native script.
Additionaly, since we are work-ing with content from social media, the usage ofnon-standard spelling is rampant and thus, nor-malization of text into some standard form is re-quired.
Ideally, these tasks should be performedjointly since they are interdependent.
However,due to lack of resources, we implement a pipelinedapproach in which the tasks - language identifica-tion, text normalization and POS tagging - are per-formed sequentially, in that order.
This pipelinedapproach also allows us to use various off-the-shelf tools for solving these subtasks and quicklycreate a baseline system.
The baseline results canalso provide useful insight into the inherent hard-ness of POS tagging of code-mixed social mediatext.
In this section, we first describe our approachto solve these three tasks, and then discuss the ex-periments and results.3.1 Language identificationLangauge identification is a well studied prob-lem (King and Abney, 2013; Carter et al., 2013;Goldszmidt et al., 2013; Nguyen and Dogruoz,2013), though for CM text, especially those in-volving transliterations and orthographic varia-tion, this is far from a solved problem (Nguyen andDogruoz, 2013).
There was a shared task in FIRE2013 (Saha Roy et al., 2013) on language iden-tification and back transliteration for En mixedwith Hi, Bangla and Gujarati.
Along the linesof Gella et al (Gella et al., 2013), which was thebest performing system in this shared task, weused the word-level logistic regression classifierbuilt by King and Abney (2013).
This system pro-vides a source language with a confidence prob-ability for each word in the test set.
We trainedthe classifier on 3201 English words extractedfrom the SMS corpus developed by Choudhuryet al (2007), while the Hindi data was obtainedby sampling 3218 Hindi transliterations out of theEn-Hi transliteration pairs developed by Sowmyaet al.
(Sowmya et al., 2010).
Ideally, the context ofa token is important for identifying the language.Again, following (Gella et al., 2013) we incorpo-rate context information through a code-switchingprobability, Ps.
A higher value of Psimplies alower probability of code-switching, i.e., adjacentwords are more likely to be in the same language.Table 1 shows the token (word) level confusionmatrix for the language identification task on ourdataset.
The language labels of 84.6% of the to-kens were correctly predicted by the system.
Ascan be seen from the Table, the precision for pre-dicting Hi is high, whereas that for En is low.
Thisis mainly due to the presence of a large number ofcontracted and distorted Hi words in the dataset,e.g.
h for hai (Fig.
1), which were tagged asEn by our system because the training exampleshad no contracted Hi words, but short and non-conventional spellings were in plenty in the Entraining examples as those were extracted from theSMS corpus.3.2 NormalizationIn our dataset, if a word is identified as Hi, thenit must be back-transliterated to Devanagari scriptso that any off-the-shelf Hindi POS tagger can beused.
We used the system by Gella et al.
(Gellaet al., 2013) for this task, which is part rule-basedand part statistical.
The system was trained on the35000 unique transliteration pairs extracted fromHindi song lyrics (Gupta et al., 2012).
This corpushas a reasonably wide coverage of Hindi words,and past researchers have also shown that translit-eration does not require a very large amount oftraining data.
Normalization of the En text wasnot needed because the POS tagger (Owoputi etal., 2013) could handle unnormalized text.3.3 POS taggingSolorio and Liu (2008b) describes a few ap-proaches to POS-tagging of code-switched Span-glish text, all of which primarily relies on twomonolingual taggers and certain heuristics to com-bine the output from the two.
One of the sim-pler heuristics is based on language identification,where the POS tag of a word is the output of themonolingual tagger of the language in which theword is.
In this initial study, we apply this ba-sic idea for POS tagging of CM data.
We dividethe text (which is already sentence-separated) intocontiguous maximal chunks of words which are inthe same language.
Then we apply a Hi POS tag-ger to the Hi chunks, and an En POS tagger to theEn chunks.977Model LI HN Tagger Hi Acc.
En Acc.
Total Acc.
Hi CA En CA Total CA1a K K Standard 75.14 81.91 79.02 27.34 39.67 34.051b K K Twitter 75.14 82.66 79.02 27.34 35.74 31.912 K NK Twitter 65.61 81.73 74.87 17.58 33.77 26.383 NK NK Twitter 44.74 80.68 65.39 40.00 13.17 25.00Table 2: POS Tagging accuracies for the different models.
K=Known, NK = Not Known.
LI = Languagelabels, HN = Hindi normalized forms, Acc.
= Token level accuracy, CA = Chunk level accuracy.We use a CRF++ based POS tagger for Hi,which is freely available from http://nltr.org/snltr-software/.
For En, we use theTwitter POS tagger (Owoputi et al., 2013).
Italso has an inbuilt tokenizer and can work di-rectly on unnormalized text.
This tagger has beenchosen because Facebook posts and commentsare more Twitter-like.
We also use the StanfordPOS Tagger (Toutanova et al., 2003) which, un-like the Twitter POS Tagger, has not been tunedfor Twitter-like text.
These taggers use differenttagsets - the ILPOST for Hi (Sankaran et al., 2008)and Penn-TreeBank for En (Marcus et al., 1993).The output tags are appropriately mapped to thesmaller Universal tagset (Petrov et al., 2011).3.4 Experiments and ResultsWe conducted three different experiments as fol-lows.
In the first experiment, we assume thatwe know the language identities and normal-ized/transliterated forms of the words, and only dothe POS tagging.
This experiment gives us an ideaof the accuracy of POS tagging task, if normal-ization, transliteration and language identificationcould be done perfectly.
We conduct this exper-iments with two different En POS taggers: theStanford POS tagger which is trained on formalEnglish text (Model 1a) and the Twitter POS tag-ger (Model 1b).
In the next experiment (Model2), we assume that only the language identity ofthe words are known, but for Hindi we apply ourmodel to generate the back transliterations.
ForEnglish, we apply the Twitter POS tagger directlybecause it can handle unnormalized social mediatext.
The third experiment (Model 3) assumes thatnothing is known.
So language identifier is firstapplied, and based on the language detected, weapply the Hi translitertaion module, and Hi POStagger, or the En tagger.
This is the most chal-lenging and realistic setting.
Note that the matrixinformation is not used in any of our experiments,though it could be potentially useful for POS tag-ging and could be explored in future.Table 2 gives a summary of the four modelsalong with the POS tagging accuracies (in %).
Itshows token level as well as chunk leve accuracies(CA), i.e., what percentage of chunks have beencorrectly POS tagged.
As can be seen, Hi POStagging has relatively low accuracies than En POStagging at word level for all cases.
This is primar-ily due to the errors of the transliteration module,which in turn, is because the transliteration doesnot address spelling contractions.
This is also re-flected in the drop in the accuracies for the casewhere LI is unknown.
The very low CA for Enfor model 3 is primarily because some of the Hichunks are incorrectly identified as En by the lan-guage identification module (see Table 1).
How-ever, the gradual drop of token and chunk levelaccuracies from model 1 to model 3 clearly showsthe effect of gradual error accumulation from eachof the modules.
We observe that Nouns wereusually confused most with Verbs and vice versa,while the Adj were mostly confused with Nouns,Pronouns with Determiners, and Adpositions withConjunctions.4 ConclusionThis is a work in progress.
We have identifiednormalization and transliteration as two very chal-lenging problems for En-Hi CM text.
Joint mod-elling of language identification, normalization,transliteration as well as POS tagging is expectedto yield better results.
We plan to continue ourwork in that direction, specifically for conversa-tional text in social media in a multilingual con-text.
CM is a common phenomenon found in allbilingual and multilingual societies.
The issue oftransliteration exist for most of the South Asianlanguages as well as many other languages such asArabic and Greek, which use a non-Roman basedscript (Gupta et al., 2014).
The challenges and is-sues identified in this study are likely to hold formany other languages as well, which makes this avery important and globally prevalent problem.978ReferencesPeter Auer.
1984.
The Pragmatics of Code-Switching:A Sequential Approach.
Cambridge UniversityPress.Kalika Bali, Yogarshi Vyas, Jatin Sharma, and MonojitChoudhury.
2014.
?
?i am borrowing ya mixing??
ananalysis of English-Hindi code mixing in Facebook.In Proceedings of the First Workshop on Computa-tional Approaches to Code Switching, EMNLP.M?onica Stella Cardenas-Claros and Neny Isharyanti.2009.
Code-switching and code-mixing in internetchatting: Between yes, ya, and si a case study.
InThe JALT CALL Journal, 5.Simon Carter, Wouter Weerkamp, and ManosTsagkias.
2013.
Microblog language identification:Overcoming the limitations of short, unedited andidiomatic text.
Language Resources and EvaluationJournal, 47:195?215.Monojit Choudhury, Rahul Saraf, Vijit Jain, AnimeshMukherjee, Sudeshna Sarkar, and Anupam Basu.2007.
Investigation and modeling of the structureof texting language.
IJDAR, 10(3-4):157?174.David Crystal.
2001.
Language and the Internet.Cambridge University Press.Brenda Danet and Susan Herring.
2007.
The Multilin-gual Internet: Language, Culture, and Communica-tion Online.
Oxford University Press., New York.Spandana Gella, Jatin Sharma, and Kalika Bali.
2013.Query word labeling and back transliteration for in-dian languages: Shared task system description.
InFIRE Working Notes.Kevin Gimpel, N. Schneider, B. O?Connor, D. Das,D.
Mills, J. Eisenstein, M. Heilman, D. Yogatama,J.
Flanigan, and N. A. Smith.
2011.
Part-of-speechtagging for twitter: Annotation, features, and exper-iments.
In Proceedings of ACL.Moises Goldszmidt, Marc Najork, and Stelios Papari-zos.
2013.
Boot-strapping language identifiers forshort colloquial postings.
In Machine Learning andKnowledge Discovery in Databases, volume 8189 ofLecture Notes in Computer Science, pages 95?111.John J. Gumperz.
1964.
Hindi-punjabi code-switchingin Delhi.
In Proceedings of the Ninth InternationalCongress of Linguistics.
Mouton:The Hague.John J. Gumperz.
1982.
Discourse Strategies.
OxfordUniversity Press.Kanika Gupta, Monojit Choudhury, and Kalika Bali.2012.
Mining Hindi-English transliteration pairsfrom online Hindi lyrics.
In Proceedings of LREC.Parth Gupta, Kalika Bali, Rafael E. Banchs, MonojitChoudhury, and Paolo Rosso.
2014.
Query ex-pansion for mixed-script information retrieval.
InProc.
of SIGIR, pages 677?686.
ACM Associationfor Computing Machinery.Susan Herring, editor.
2003.
Media and LanguageChange.
Special issue of Journal of Historical Prag-matics 4:1.Ben King and Steven Abney.
2013.
Labeling the lan-guages of words in mixed-language documents us-ing weakly supervised methods.
In Proceedings ofNAACL-HLT, pages 1110?1119.Mitchell P Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The penn treebank.
Compu-tational linguistics, 19(2):313?330.Carol Myers-Scotton.
1993.
Dueling Languages:Grammatical Structure in Code-Switching.
Clare-don, Oxford.Dong Nguyen and A. Seza Dogruoz.
2013.
Wordlevel language identification in online multilingualcommunication.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 857?862.Olutobi Owoputi, Brendan OConnor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah A.Smith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of NAACL.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2011.A universal part-of-speech tagset.
arXiv preprintarXiv:1104.2086.Rishiraj Saha Roy, Monojit Choudhury, Prasenjit Ma-jumder, and Komal Agarwal.
2013.
Overview anddatasets of fire 2013 track on transliterated search.In FIRE Working Notes.Bhaskaran Sankaran, Kalika Bali, Monojit Choudhury,Tanmoy Bhattacharya, Pushpak Bhattacharyya,Girish Nath Jha, S. Rajendran, K. Saravanan,L.
Sobha, and K. V. Subbarao.
2008.
A com-mon parts-of-speech tagset framework for indianlanguages.
In Proceedings of LREC.Thamar Solorio and Yang Liu.
2008a.
Learning topredict code-switching points.
In Proceedings of theEmpirical Methods in natural Language Processing.Thamar Solorio and Yang Liu.
2008b.
Parts-of-speechtagging for English-Spanish code-switched text.
InProceedings of the Empirical Methods in naturalLanguage Processing.V.
B. Sowmya, Monojit Choudhury, Kalika Bali,Tirthankar Dasgupta, and Anupam Basu.
2010.
Re-source creation for training and testing of translitera-tion systems for indian languages.
In Proceedings ofthe Language Resource and Evaluation Conference(LREC).Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of HLT-NAACL.979
