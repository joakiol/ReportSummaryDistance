Proceedings of NAACL HLT 2007, pages 540?547,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsToward Multimedia: A String Pattern-based Passage Ranking Model forVideo Question AnsweringYu-Chieh Wu Jie-Chi YangDept.
of Computer Science and Infor-mation EngineeringGraduate Institute of NetworkLearning TechnologyNational Central University National Central UniversityTaoyuan, Taiwan Taoyuan, Taiwanbcbb@db.csie.ncu.edu.tw yang@cl.ncu.edu.twAbstractIn this paper, we present a new string pat-tern matching-based passage ranking al-gorithm for extending traditional text-based QA toward videoQA.
Users interactwith our videoQA system through naturallanguage questions, while our system re-turns passage fragments with correspond-ing video clips as answers.
We collect75.6 hours videos and 253 Chinese ques-tions for evaluation.
The experimental re-sults showed that our methodoutperformed six top-performed rankingmodels.
It is 10.16% better than the sec-ond best method (language model) in rela-tively MRR score and 6.12% in precisionrate.
Besides, we also show that the use ofa trained Chinese word segmentation tooldid decrease the overall videoQA per-formance where most ranking algorithmsdropped at least 10% in relatively MRR,precision, and answer pattern recall rates.1 IntroductionWith the drastic growth of video sources, effectiveindexing and retrieving video contents has recentlybeen addressed.
The well-known Informedia pro-ject (Wactlar, 2000) and TREC-VID track (Over etal., 2005) are the two famous examples.
Althoughtext-based question answering (QA) has become akey research issue in past decade, to support mul-timedia such as video, it is still beginning.Over the past five years, several video QA stud-ies had investigated.
Lin et al (2001) presented anearlier work on combining videoOCR and termweighting models.
Yang et al (2003) proposed acomplex videoQA approach by employing abun-dant external knowledge such as, Web, WordNet,shallow parsers, named entity taggers, and human-made rules.
They adopted the term-weightingmethod (Pasca, and Harabagiu, 2001) to rank thevideo segments by weighting the pre-defined key-words.
Cao and Nunamaker (2004) developed alexical pattern matching-based ranking method fora domain-specific videoQA.
In the same year, Wuet al (2004) designed a cross-language (English-to-Chinese) video question answering systembased on extracting pre-defined named entitywords in captions.
On the other hand, Zhang andNunamaker (2004) made use of the simple TFIDFterm weighting schema to retrieve the manual-segmented clips for video caption word retrieval.They also manually developed the ontology to im-prove system performance.In this paper, we present a new string patternmatching-based passage ranking algorithm forvideo question answering.
We consider that thepassage is able to answer questions and also suit-able for videos because itself forms a very naturalunit.
Lin et al (2003) showed that users prefer pas-sage-level answers over short answer phrases sinceit contains rich context information.
Our methodmakes use of the string pattern searching in thesuffix trees to find common subsequences betweena passage and question.
The proposed term weight-ing schema is then designed to compute passagescore.
In addition, to avoid generating over-lengthsubsequence, we also present two algorithms forre-tokenization and weighting.2 The Framework of our VideoQA SystemAn overview of the proposed videoQA system canbe shown in Figure 1.
The video processing com-ponent recognizes the input video as an OCR docu-ment at the first stage.
Second, each threeconsecutive sentences were grouped into a passage.We tokenized the Chinese words with threegrained sizes: unigram, bigram, and trigram.
Simi-larly, the input question is also tokenized to uni-540gram, bigram, and trigram level of words.
To re-duce most irrelevant passages, we adopted the BM-25 ranking model (Robertson et al, 2000) to re-trieve top-1000 passages as the ?input passages?.Finally, the proposed passage ranking algorithmretrieved top-N passages as answers in response tothe question.
In the following parts, we briefly in-troduce the employed videoOCR approach.
Section2.2 presents the sentence and passage segmentationschemes.
The proposed ranking algorithms will bedescribed in Section 3.Figure1: System Architecture of the proposedvideoQA system2.1 Video ProcessingOur video processing takes a video and recognizesthe closed captions as texts.
An example of theinput and output associated with the whole videoprocessing component can be seen in Figure 2.
ThevideoOCR technique consists of four importantsteps: text detection, binarization, frame tracking,and OCR.
The goal of text detection is to locate thetext area precisely.
In this paper, we employ theedge-based filtering (Lyu et al, 2005) and slightlymodify the coarse-to-fine top-down block segmen-tation methods (Lienhart and Wernicke, 2002) tofind each text component in a frame.
The formerremoves most non-edge areas with global and localthresholding strategy (Fan et al, 2001) while thelatter incrementally segments and refines textblocks using horizontal and vertical projection pro-files.The next steps are text binarization and frametracking.
As we know, the main constituent ofvideo is a sequence of image frames.
A text com-ponent almost appears more than once.
To removeredundancy, we count the proportion of overlap-ping edge pixels between two consecutive frames.If the portion is above 70%, then the two frameswere considered as containing the same text com-ponents.
We then merge the two frames by averag-ing the gray-intensity for each pixel in the sametext component.
For the binarization stage, we em-ploy the Lyu?s text extraction algorithm (Lyu et al,2005) to binarize text pixels for the text compo-nents.
Unlike previous approaches (Lin et al, 2001;Chang et al, 2005), this method does not need toassume the text is in either bright or dark color (butassume the text color is stable).
At the end of thisstep, the output text components are prepared forOCR.The target of OCR is to identify the binarizedtext image to the ASCII text.
In this paper, we de-veloped a na?ve OCR system based on nearestneighbor classification algorithms and clusteringtechniques (Chang et al, 2005).
We also adoptedthe word re-ranking methods (Lin et al, 2001,strategy 3) to improve the OCR errors.Figure 2: Text extraction results of an input image2.2 Sentence and Passage SegmentationIn this paper, we treat all words appear in the sameframe as a sentence and group every three consecu-tive sentences as a passage.
Usually, words thatoccur in the same frame provide a sufficient andcomplete description.
We thus consider thesewords as a sentence unit for sentence segmentation.An example of a sentence can be found in Figure 2.The sentence of this frame is the cascading of thetwo text lines, i.e.
?speed-up to 17.5 thousandmiles per hour in less than six minutes?
For eachOCR document we grouped every three continuoussentences with one previous sentence overlappingto represent a passage.
Subsequently, we tokenizedChinese word with unigram, bigram, and trigramlevels.Searching answers in the whole video collectionis impractical since most of them are irrelevant tothe question.
By means of text retrieval technology,the search space can be largely reduced and limitedin a small set of relevant document.
The documentretrieval methods have been developed well andsuccessfully been applied for retrieving relevantpassages for question answering (Tellex et al,5412003).
We replicated the Okapi BM-25 (Robertsonet al, 2000), which is the effective and efficientretrieval algorithms to find the related segmentedpassages.
For each input question, the top-1000relevant passages are input to our ranking model.3 The AlgorithmTellex et al (2003) compared seven passage re-trieval models for text QA except for several ad-hoc approaches that needed either human-generated patterns or inference ontology whichwere not available.
In their experiments, theyshowed that the density-based methods (Lee et al,2001) achieved the best results, while the BM-25(Robertson, 2000) reached slightly worse retrievalresult than the density-based approaches, whichadopted named entity taggers, thesaurus, andWordNet.
Cui et al (2005) showed that their fuzzyrelation syntactic matching method outperformedthe density-based methods.
But the limitation isthat it required a dependency parser, thesaurus, andtraining data.
In many Asian languages like Chi-nese, Japanese, parsing is more difficult since it isnecessary to resolve the word segmentation prob-lem before part-of-speech (POS) tagging, and pars-ing (Fung et al, 2004).
This does not only makethe parsing task harder but also required to train ahigh-performance word segmentor.
The situation iseven worse when text contains a number of OCRerror words.
In addition, to develop a thesaurus andlabeled training set for QA is far time-consuming.In comparison to Cui?s method, the term weight-ing-based retrieval models are much less cost,portable and more practical.
Furthermore, the OCRdocument is not like traditional text articles thathave been human-typed well where some wordswere error predicted, unrecognizable, and false-alarm.
These unexpected words deeply affect theperformance of Chinese word segmentation, andfurther for parsing.
In our experiments (see Table 2and Table 3), we also showed that the use of awell-trained high-performance Chinese word seg-mentation tool gave the worse result than using theunigram-level of Chinese word (13.95% and13.92% relative precision and recall rates droppedfor language model method).To alleviate this problem, we treat the atomicChinese unigram as word and present a weightedstring pattern matching algorithm.
Our solution isto integrate the suffix tree for finding, and encod-ing important subsequence information in trees.Nevertheless, it is known that the suffix tree con-struction and pattern searching can be accom-plished in linear time (Ukkonen, 1995).
Beforeintroducing our method, we give the following no-tations.passage P = PW1, PW2, ?, PWTquestion Q = QW1, QW2, ?, QWT?a common subsequence for passagexixkkki == ?++ |Sub| if    PW,...,PW,PWSub P11Pa common subsequence for questionyjylllj == ?++ |Sub| if QW,...,QW,QWSub Q11QA common subsequence represents a continuousstring matching between P and Q.
We further im-pose two symbols on a subsequence.
For example,SubiP means i-th matched continuous string (com-mon subsequence) in the passage, while SubjQ in-dicates the j-th matched continuous string in thequestion.
The common subsequences can be ex-tracted through the suffix tree building and patternsearching.
For example, to extract the set of SubiP,we firstly build the suffix tree of P and incremen-tally insert substring of Q and label the matchedcommon string between P and Q.
Similarly, onecan apply a similar approach to generate the set ofSubjQ.
By extracting all subsequences for P and Q,we then compute the following score (see equation(1)) to rank passages.P) Q,QW_Weight() -(1P) (Q,QW_Density  ore(P)Passage_Sc?+?=??
(1)The first term of equation (1) ?QW_Density(Q,P)?
estimates the question word density degree inthe passage P, while ?QW_Weight(Q, P)?
meas-ures the matched question word weights in P. ?
is aparameter, which is used to adjust the importanceof the QW_Density(Q, P).
Both the two estima-tions make use of the subsequence information forP and Q.
In the following parts, we introduce thecomputation of QW_Density(Q,P) andQW_Weight(Q, P) separately.
The time complex-ity analysis of our method is then discussed in thetail of this section.The QW_Density(Q, P) is designed for quantify-ing ?how dense the matched question words in thepassage P?.
It also takes the term weight into ac-count.
By means of extracting common subse-quence in the question, the set of SubjQ can be usedto measures the question word density.
At the be-ginning, we define equation (2) for weighting asubsequence SubjQ.542)Sub(DP)Sub(length)Weight(Sub QQQ 1 jjj ?= ?
(2)Where length(SubjQ) is merely the length of QSub ji.e., the number of words in SubjQ.
?1 is a parameterthat controls the weight of length for SubjQ.
In thispaper, we consider the long subsequence match isuseful.
A long N-gram is usually much less am-biguous than its individual unigram.
The secondterm in equation (2) estimates the ?discriminativepower?
(DP) of the subsequence.
Some high-frequent and common words should be given lessweight.
To measure the DP score, we extend theBM-25 (Robertson et al, 2000) term weightingschema.
Equation (3), (4), and (5) list our DP scor-ing functions.
)Q ,Sub(TF)Q ,Sub(TF)1()P ,SubTF()P ,Sub(TF)1(')Sub(DP Q3Q3QQ1Qjjjjj kkKkW +?+?+?+?=   (3))5.0)Sub(PF5.0)Sub(PFlog(' QQ++?=jjPNW                                       (4)|)P(|AVG|P|)1( ?+?= bbK                                             (5)31  , , kbk  are constants, which empirically set as 1.2,0.75, 500 respectively (Robertson et al, 2000).
)P ,Sub(TF and )Q ,Sub(TF QQ jj  represent the termfrequency of SubjQ in question Q and passage P.Equation (4) computes the inverse ?passage fre-quency?
(PF) of SubjQ as against to the traditionalinverse ?document frequency?
(DF) where Np isthe total number of passages.
The collected Dis-covery video is a small but ?long?
OCR documentset, which results the estimation of DF value unre-liable.
On the contrary, a passage is more coherentthan a long document, thus we replace the DF es-timation with PF score.
It is worth to note thatsome SubjQ might be too long to be further re-tokenized into finer grained size.
We thereforepropose two algorithms to 1): re-tokenize an inputsubsequence, and 2): compute the DP score for asubsequence.
Figure 3, and Figure 4 list the pro-posed two algorithms.The proposed algorithm 1, and 2 can be used tocompute and tokenize the DP score of not onlySubjQ for question but also SubjP for passage.
Asseeing in Figure 4, it requires DP information fordifferent length of N-gram.
As noted in Section 2.2,the unigram, bigram, and trigram level of wordshad been stored in indexed files for efficient re-trieving and computing DP score at this step.
Byapplying algorithm 1 for the set of SubjQ, we canobtain all retokenized subsequences (TSubj).
Wethen use the re-tokenized subsequences to computethe final density score.
Equation (6) lists theQW_Density scoring function.?
?= +++=1_1 112)TSub,TSub(dist)TSub(Weight)TSub(WeightP)(Q,QW_DensityCNTTi iiii?
(6)1)_in_PTSub,(TSubce_betweenmin_distan)TSub,TSub(dist11+=++iiii                (7)T_CNT is the total number of retokenized subse-quences in Q, which can be extracted through ap-plying algorithm 1 for all SubjQ.
Equation (7)merely counts the minimum number of words be-tween two neighboring TSubi, and TSubi+1 in thepassage.
?2 is the parameter that controls the im-pact of distance measurement.Algorithm 1: Retokenizing_a_subsequenceInput:A subsequence SubjQ where startj is the position of first word inquestion and endj is the position of last word in questionOutput:A set of retokenized subsequence { ,.....TSub,TSub 21 }Nt: the number of retokenized subsequenceAlgorithm:Initially, we set Nt := 1; TSub1:=QWstartj;if (SubjQ??
){     /*** from the start to the end positions in the string ***/for ( k := startj+1 to endj){/***Check the two question words is bigram in the passage***/if (bigram(QWk-1,QWk) is_found_in_passage)add QWk into TSubNt;Otherwise{      Nt ++;;QW:TSub kNt =} /*** End otherwise***/} /*** End for ***/} /*** End if ***/elseNt := 0;Figure 3: An algorithm for retokenizing subsequenceAlgorithm 2: Copmuting_DP_scoreInput:A subsequence  SubjQ where startj is the position of first wordof SubjQ in question endj is the position of last word of SubjQ inquestionOutput:The score of DP(SubjQ)Algorithm:head := startj;tail := endj;Max_score := 0;for (k := head ~ tail){     let WORD := QWk, QWk+1,?, QWtail;/*** look-up WORD in the index files  ***/compute DP(WORD) using equation (3);if (DP(WORD) > Max_score)Max_score := DP(WORD);} /*** End for ***/DP(WORD) := Max_score;Figure 4: An algorithm for computing DP score for asubsequence543The density scoring can be thought as measuring?how much information the passage preserves inresponse to the question?.
On the contrary, theQW_Weight (second term in equation (1)) aims toestimate ?how much content information the pas-sage has given the question?.
To achieve this, wefurther take the other extracted common subse-quences, i.e., SubjP into account.
By means of thesame term weighting schema for the set of SubjP,the QW_Weight is then produced.
Equation (8)gives the overall QW_Weight measurement.??==?==CNTSiiiCNTSii_1PP_1P))Sub(DP)Sub((length)Weight(SubP)Q,QW_Weight(1?
(8)where the DP score of the input subsequence canbe obtained via the algorithm 2 (Figure 5).
S_CNTis the number of subsequence in P. The parameter?1 is also set as equal as equation (2).In addition, the neighboring contexts of a sen-tence, which contains high QW_Density scoremight include the answers.
Hence, we stress oneither head or tail fragments of the passage.
Inother words, the passage score is determined bycomputing equation (1) for head and tail parts ofpassage.
We thus extend equation (1) as follows.?????====+=+=?+?
?+?=12112211213222113212211S  P  P n,       the          S    :sentence 1 has P if elseS  P and S  P  then,          S ,S  :sentences 2 has P if elseSS  P and SS  P ,      thenS ,S ,S         :sentences 3 has P if)}P Q,QW_Weight() -(1)P (Q,QW_Density),P Q,QW_Weight() -(1)P (Q,QW_Density max{ ore(P)Passage_Sc???
?Instead of estimating the whole passage, the twodivided parts: P1, and P2 are used.
We select themaximum passage score from either head (P1) ortail (P2) part.
When the passage contains only onesentence, then this sentence is indispensable to beused for estimation.Now we turn to analyze the time complexity ofour algorithm.
It is known that the suffix tree con-struction costs is linear time (assume it requiresO(T), T: the passage length for passage and O(T?),T?
: the question length for question).
Assume thesearch time for a pattern in the suffix trees is atmost O(hlogm) where h is the tree height, and m isthe number of branch nodes.
To generate the setsof SubjQ and SubjP, it involves in building suffixtrees and incrementally searching substrings, i.e.,O((T+T?)+(T+T?)(hlogm)).
Intuitively, both algo-rithm 1, and algorithm 2 are linear time algorithms,which depends on the length of ?common?
subse-quence, i.e., at most O(min(T, T?)).
Consequently,the overall time complexity of our method forcomputing a passage is O((T+T?
)(1+hlogm)+min(T, T?
)).4 Experiments4.1 EvaluationWe should carefully select the use of videoQA col-lection for evaluation.
Unfortunately, there is nobenchmark corpus for this task.
Thus, we developan annotated collection by following the similartasks as TREC, CLEF, and NTCIR.
The Discoveryvideos are one of the popular raw video sourcesand widely evaluated in many literatures (Lin et al,2001; Wu et al, 2004; Lee et al, 2005).
Totally,75.6 hours of Discovery videos (93 video names)were used.
Table 1 lists the statistics of the Dis-covery films.The questions were created in two differentways: one set (about 73) was collected from previ-ous studies (Lin et al, 2001; Wu et al, 2004)which came from the ?Project: Assignment of Dis-covery?
; while the other was derived from a reallog from users.
Video collections are difficult to begeneral-purpose since hundreds hours of videosmight take tens of hundreds GB storage space.Therefore, general questions are quite difficult tobe found in the video database.
Hence, we providea list of short introductions collected from thecover-page of the videos and enable users tobrowse the descriptions.
Users were then asked forthe system with limited to the collected video top-ics.
We finally filter the (1) keyword-like queries(2) non-Chinese and (3) un-supported questions.Finally, there were 253 questions for evaluation.For the answer assessment, we followed theTREC-QA track (Voorhees, 2001) and NTCIR toannotate answers in the pool that collected fromthe outputs of different passage retrieval methods.Unlike traditional text QA task, most of the OCRsentences contain a number of OCR error words.Furthermore, some sentence did include the answerstring but error recognized as different words.
Thus,instead of annotating the recognized transcripts, weused the corresponding video frames for evaluationbecause users can directly find the answers in theretrieved video clips and recognized text.
Among253 questions, 56 of which did not have an answer,while 368 passage&frame segments (i.e., answerpatterns) in the pool were labeled as answers.
On544averagely, there are 1.45 labeled answers for eachquestion.The MRR (Voorhees, 2001) score, precision andpattern-recall are used for evaluation.
We measurethe MRR scores for both top1 and top5 ranks, andprecision and pattern-recall rates for top5 retrievedanswers.Table 1: Statistics of the collected Discovery videos# of videos # of sentence # of words # of passages93 49950 746276 25001AVG # ofwords persentenceAVG # ofwords perpassageAVG # ofsentencesper passageAVG # of wordsper video14.94 48.78 537.09 8024.474.2 ResultsIn this paper, we employed six top-performed yetportable ranking models, TFIDF, BM-25 (Robert-son et al, 2000), INQUERY, language model(Zhai and Lafferty, 2001), cosine, and density-based (Lee et al, 2001) approaches for compari-son1.
For the language model, the Jelinek-Mercersmoothing method was employed with the parame-ter settings ?=0.5 which was selected via severaltrials.
In our preliminary experiments, we foundthat the query term expansion does not improve butdecrease the overall ranking performance for allthe ranking models.
Thus, we only compare withthe ?pure?
retrieval performance without pseudo-feedback.The system performance was evaluated throughthe returned passages.
We set ?1=1.25, ?2= 0.25,and ?=0.8 which were observed via the followingparameter validations.
More detail parameter ex-periments are presented and discussed later.
Table2 lists the overall videoQA results with differentranking models.Among all ranking models, the proposed methodachieves the best system performance.
Our ap-proach produced 0.596 and 0.654 MRR scoreswhen evaluating the top1 and top5 passages andthe precision rate achieves 0.208.
Compared to thesecond best method (language model), our methodis 10.16% better in relatively percentage in termsof MRR(top1) score.
For the MRR(top5) score, ourmethod is 7.39 relative percentage better.
In termsof the non-answered questions, our method alsocovers the most questions (253-69=184) compared1 For the TFIDF/BM-25/INQUERY/Language Model approacheswere performed using the Lemur toolkitto the other ranking models.
Overall, the experi-ment shows that the proposed weighted string pat-tern matching algorithm outperforms the other sixmethods in terms of MRR, non-answered questionnumbers, precision and pattern recall rates.Table 2: Overall videoQA performance with differ-ent ranking models (using unigram Chinese word)Word-Level MRR (Top1)MRR(Top5)Non-answeredQuestions PrecisionPatternRecallTFIDF 0.498 0.572 81 0.189 0.649BM-25 0.501 0.581 78 0.186 0.638Language Model 0.541 0.609 74 0.196 0.671INQUERY 0.505 0.583 78 0.188 0.644Cosine 0.418 0.489 102 0.151 0.519Density 0.323 0.421 102 0.137 0.471Our Method 0.596 0.654 69 0.208 0.711Table 3: Overall videoQA performance with differ-ent ranking models using word segmentation toolsWord-Level MRR (Top1)MRR(Top5)Non-answeredQuestions PrecisionPatternRecallTFIDF 0.509 0.567 89 0.145 0.597BM-25 0.438 0.500 104 0.159 0.543Language Model 0.486 0.551 89 0.172 0.589INQUERY 0.430 0.503 97 0.164 0.562Cosine 0.403 0.480 100 0.158 0.548Density 0.304 0.380 125 0.133 0.451Our Method 0.509 0.561 89 0.181 0.608Next, we evaluate the performance with adopt-ing a trained Chinese word segmentation tool in-stead of unigram level of word.
In this paper, weemployed the Chinese word segmentation tool (Wuet al, 2006) that achieved about 0.93-0.96 re-call/precision rates in the SIGHAN-3 word seg-mentation task (Levow, 2006).
Table 3 lists theoverall experimental results with the adopted wordsegmentation tool.
In comparison to unigramgrained level (Table 2), it is shown that the use ofword segmentation tool does not improve thevideoQA result for most top-performed rankingmodels, BM-25, language model, INQUERY, andour method.
For example, our method is relatively17.92% and 16.57% worse in MRR(Top1) andMRR(Top5) scores.
In terms of precision and pat-tern-recall rates, it drops 14.91, and 16.94 relativepercentages, respectively.
For the TFIDF method,the MRR score is almost the same as previous re-sult whereas it decreased 30.34%, and 8.71% pre-cision and pattern-recall rates.
On averagely, thefour models, BM-25, language model, INQUERY,and our method dropped at least relatively 10% inMRR, precision, and pattern-recall rates.
In thisexperiment, our ranking algorithm also achieved545the best results in terms of precision and patternrecall rates while marginally worse than the TFIDFfor the MRR(top5) score.There are three parameters: ?, ?1, ?2, in our rank-ing algorithm.
?
controls the weight of theQW_Density(Q, P), while ?1, and ?2 were set forthe power of subsequence length and the distancemeasurement.
We randomly select 100 questionsfor parameter validations.
Firstly, we tried to verifythe optimal ?1 via different settings of the remain-ing two parameters.
The best ?1 is then set to verify?2 via various ?
values.
The optimal ?
is subse-quently confirmed through the observed ?1 and ?2values.
Figure 5, 6, 7 show the performanceevaluations of different settings for the three pa-rameters.As shown in Figure 5, the optimal settings of(?1=1.25) is obtained when and ?2=0.25, and?=0.75.
When ?1 is set more than 1.5, our methodquickly decreased.
In this experiment, we alsofound that large ?2 negatively affects the perform-ance.
The small ?2 values often lead to better rank-ing performance.
Thus, in the next experiment, welimited the ?2 value in 0.0~3.0.
As seeing in Figure6, again the abnormal high or zero ?2 values givethe poor results.
This implies the over-weight andno-weight on the distance measurement (equation(7)) is not useful.
Instead, a small ?2 value yields toimprove the performance.
In our experiment,?2=0.25 is quite effective.
Finally, in Figure 7, wecan see that both taking the QW_Density, andQW_Weight into account gives better ranking re-sult, especially QW_Density.
This experiment in-dicates that the combination of QW_Density andQW_Weight is better than its individual termweighting strategy.
When ?=0.8, the best rankingresult (MRR = 0.700) is reached.Next, we address on the impact of differentnumber of initial retrieved passages using BM-25ranking models.
Due to the length limitation of thispaper, we did not present the experiments over allthe compared ranking models, while we left thefurther results at our web site2.
For the three pa-rameters, we select the optimal settings derivedfrom previous experimental results, i.e., ?=0.8,?1=1.25, ?2=0.25.
Figure 8 shows the experimentalresults with different number of initial retrievedpassages.
When employing exactly five initial re-trieved passages, it can be viewed as the re-rankingimprovement over the BM-25 ranking model.
Asseeing in Figure 8, our method does improve theconventional BM-25 ranking approach (MRRscore 0.690 v.s.
0.627) with relatively 10.04%MRR value.
The best system performance isMRR=0.700 when there are merely 20 initial re-trieved passages.
The ranking result convergeswhen retrieving more than 40 passages.
Besides,2 http://140.115.112.118/bcbb/TVQS2/Figure 5: Experimental results with differentsettings of parameter ?1 using MRR evaluation0.20.250.30.350.40.450.50.550.60.650.70 0.5 1 1.5 2 2.5 3 3.5 4?2MRRLambda = 0.00Lambda = 0.25Lambda = 0.50Lambda = 0.75Lambda = 1.00Figure 6: Verify parameter ?2 with ?1=1.25, andvariant ?0.20.250.30.350.40.450.50.550.60.650.70 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 2.2 2.4 2.6 2.8 3?2MRRLambda = 0.00Lambda = 0.25Lambda = 0.50Lambda = 0.75Lambda = 1.00Figure 7: Verify parameter ?
in the two vali-dation sets with ?1=1.25 and ?2=0.250.50.550.60.650.70 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1?2MRRFigure 8: Experimental results with differentnumber of initial retrieval passages (TopN)0.670.680.690.70.715 20 40 100 300 500 1500 3000 5000TopNMRR546we also continue the experiments using only top-20 retrieved passages on the actual 253 testingquestions.
The ranking performance is then furtherenhanced from MRR=0.654 to 0.663 with 1.37%relatively improved.5 ConclusionMore and more users are interested in searching foranswers in videos, while existing question answer-ing systems do not support multimedia accessing.This paper presents a weighted string patternmatching-based passage ranking algorithm for ex-tending text QA toward video question answering.We compare our method with six top-performedranking models and show that our method outper-forms the second best approach (language model)in relatively 10.16 % MRR score, and 6.12% pre-cision rates.In the future, we plan to integrate the other use-ful features in videos to support multi-model-basedmultimedia question answering.
The video-demoversion of our videoQA system can be found at theweb site (http://140.115.112.118/bcbb/TVQS2/).ReferencesCao, J., and Nunamaker J. F. Question answering on lecturevideos: a multifaceted approach, International Conferenceon Digital Libraries, pages 214 ?
215, 2004.Chang, F., Chen, G. C., Lin, C. C., and Lin, W. H. Captionanalysis and recognition for building video indexing sys-tems.
Multimedia systems, 10: 344-355, 2005.Cui, H., Sun, R., Li, K., Kan, M., and Chua, T. Question an-swering passage retrieval using dependency relations.
InProceedings of the 28th ACM SIGIR Conference on Re-search and Development in Information Retrieval, pages400-407, 2005.Fan, J., Yau, D. K. Y., Elmagarmid, A. K., and Aref, W. G.Automatic image segmentation by integrating color-edgeextraction and seeded region growing.
IEEE Trans.
On Im-age Processing, 10(10): 1454-1464, 2001.Fung, P., Ngai, G., Yuan, Y., and Chen, B.
A maximum en-tropy Chinese parser augmented by transformation-basedlearning.
ACM Trans.
Asian Language Information Proc-essing, 3: 159-168, 2004.Lee et al SiteQ: Engineering high performance QA systemusing lexico-semantic pattern matching and shallow NLP.In Proceedings of the 10th Text Retrieval Conference,pages 437-446, 2001.Lee, Y. S., Wu, Y. C., and Chang, C. H. Integrating Web in-formation to generate Chinese video summaries.
In Pro-ceedings of 17th international conference on softwareengineering and knowledge engineering (SEKE), pages514-519, 2005.Levow, G. A.
The third international Chinese language proc-essing Bakeoff: word segmentation and named entity rec-ognition, In Proceedings of the 5th SIGHAN Workshop onChinese Language Processing, pages 108-117, 2006.Lin, C. J., Liu, C. C., and Chen, H. H. A simple method forChinese videoOCR and its application to question answer-ing.
Journal of Computational linguistics and Chinese lan-guage processing, 6: 11-30, 2001.Lin, J., Quan, D., Sinha, V., Bakshi, K., Huynh, D., Katz, B.,and Karger, D. R. What makes a good answer?
the role ofcontext in question answering.
In Proceedings of the 9th in-ternational conference on human-computer interaction(INTERACT), pages 25-32, 2003.Lienhart, R. and Wernicke, A. Localizing and segmenting textin images and videos.
IEEE Trans.
Circuits and Systemsfor Video Technology, 12(4): 243-255, 2002.Lyu, M. R., Song, J., and Cai, M. A comprehensive methodfor multilingual video text detection, localization, and ex-traction.
IEEE Trans.
Circuits and Systems for VideoTechnology, 15(2): 243-255, 2005.Over, P., Ianeva, T., Kraaij, W., and Smeaton, A. F.TRECVID 2005 - an overview.
In Proceedings of the 14thtext retrieval conference (TREC), 2005.Pasca, M., and Harabagiu, S. High-performance question an-swering.
In Proceedings of the 24th ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 366-374, 2001.Robertson, E., Walker, S., and Beaulieu, M. Experimentationas a way of life: Okapi at TREC.
Journal of Informationprocessing and management, 36: 95-108, 2000.Tellex, S., Katz, B., Lin, J. J., Fernandes, A., and Marton, G.Quantitative evaluation of passage retrieval algorithms forquestion answering.
In Proceedings of the 26th ACMSIGIR Conference on Research and Development in In-formation Retrieval, pages 41-47, 2003.Voorhees, E. M. Overview of the TREC 2001 question an-swering track.
In Proceedings of the 10th Text RetrievalConference , pages 42-52, 2001.Ukkonen, E. Constructing suffix trees on-line in linear time.
InProceedings of the international federation of informationprocessing, pages 484-492, 1995.Wactlar, H. D. Informedia search and summarization in thevideo medium, In Proceedings of Imagina 2000 Confer-ence, 2000.Wu, Y. C., Lee, Y. S., Chang, C. H. CLVQ: Cross-languagevideo question/answering system.
In Proceedings of 6thIEEE International Symposium on Multimedia SoftwareEngineering, pages 294-301, 2004.Wu, Y. C., Yang, J. C., and Lin, Q. X.
Description of the NCUChinese Word Segmentation and Named Entity Recogni-tion System for SIGHAN Bakeoff 2006.
In Proceedings ofthe 5th SIGHAN Workshop on Chinese Language Process-ing, pages 209-212, 2006.Yang, H., Chaison, L., Zhao, Y., Neo, S. Y., and Chua, T. S.VideoQA: Question answering on news video.
In Proceed-ings of the 11th ACM International Conference on Multi-media, pages 632-641, 2003.Zhai, C., and Lafferty, J.
A study of smoothing methods forlanguage models applied to ad hoc information retrieval, InProceedings of the 24th Annual International ACM SIGIRConference on Research and Development in InformationRetrieval (SIGIR), pages 334-342, 2001.Zhang, D., and Nunamaker, J.
A natural language approach tocontent-based video indexing and retrieval for interactiveE-learning.
IEEE Trans.
on Multimedia, 6: 450-458, 2004.547
