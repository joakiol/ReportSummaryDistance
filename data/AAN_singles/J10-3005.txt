Discourse Constraints forDocument CompressionJames Clarke?University of Illinois atUrbana-ChampaignMirella Lapata?
?University of EdinburghSentence compression holds promise for many applications ranging from summarization tosubtitle generation.
The task is typically performed on isolated sentences without taking thesurrounding context into account, even though most applications would operate over entiredocuments.
In this article we present a discourse-informed model which is capable of producingdocument compressions that are coherent and informative.
Our model is inspired by theories oflocal coherence and formulated within the framework of integer linear programming.
Experimen-tal results show significant improvements over a state-of-the-art discourse agnostic approach.1.
IntroductionRecent years have witnessed increasing interest in sentence compression.
The taskencompasses automatic methods for shortening sentences with minimal informationloss while preserving their grammaticality.
The popularity of sentence compression islargely due to its relevance for applications.
Summarization is a case in point here.
Mostsummarizers to date aim to produce informative summaries at a given compressionrate.
If we can have a compression component that reduces sentences to a minimallength and still retains the most important content, then we should be able to pack moreinformation content into a fixed size summary.
In other words, sentence compressionwould allow summarizers to increase the overall amount of information extractedwithout increasing the summary length (Lin 2003; Zajic et al 2007).
It could also beused as a post-processing step in order to render summaries more coherent and lessrepetitive (Mani, Gates, and Bloedorn 1999).Beyond summarization, a sentence compression module could be used to displaytext on small screen devices such as PDAs (Corston-Oliver 2001) or as a reading aidfor the blind (Grefenstette 1998).
Sentence compression could also benefit informationretrieval by eliminating extraneous information from the documents indexed by the?
Department of Computer Science, University of Illinois at Urbana-Champaign, 201 N Goodwin Ave,Urbana, IL 61801, USA.
E-mail: clarkeje@illinois.edu.??
School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh, EH8 9AB, UK.E-mail: mlap@inf.ed.ac.uk.Submission received: 10 September 2008; revised submission received: 27 October 2009; accepted forpublication: 6 March 2010.?
2010 Association for Computational LinguisticsComputational Linguistics Volume 36, Number 3retrieval engine.
This way it would be possible to store less information in the indexwithout dramatically affecting retrieval performance (Olivers and Dolan 1999).In theory, sentence compression may involve several rewrite operations such asdeletion, substitution, insertion, and word reordering.
In practice, however, the taskis commonly defined as a word deletion problem: Given an input sentence of wordsx = x1, x2, .
.
.
, xn, the aim is to produce a compression by removing any subset of thesewords (Knight and Marcu 2002).
Many sentence compression models aim to learn dele-tion rules from a parsed parallel corpus of source sentences and their target compres-sions (Knight and Marcu 2002; Turner and Charniak 2005; Galley and McKeown 2007;Cohn and Lapata 2009).
For example, Knight and Marcu (2002) learn a synchronouscontext-free grammar (Aho and Ullman 1969) from such a corpus.
The grammar ruleshave weights (essentially probabilities estimated using maximum likelihood) and areused to find the best compression from the set of all possible compressions for a givensentence.
Other approaches exploit syntactic information without making explicit useof a parallel grammar?for example, by learning which words or constituents to deletefrom a parse tree (Riezler et al 2003; Nguyen et al 2004; McDonald 2006; Clarke andLapata 2008).Despite differences in formulation and training requirements (some approachesrequire a parallel corpus, whereas others do not), existing models are similar in thatthey compress sentences in isolation without taking their surrounding context intoaccount.
This is in marked contrast with common practice in summarization.
Pro-fessional abstractors often rely on contextual cues while creating summaries (Endres-Niggemeyer 1998).
This is true of automatic summarization systems too, which considerthe position of a sentence in a document and how it relates to its surrounding sentences(Kupiec, Pedersen, and Chen 1995; Barzilay and Elhadad 1997; Marcu 2000; Teufel andMoens 2002).
Determining which information is important in a sentence is not merelya function of its syntactic position (e.g., deleting the verb or the subject of a sentence isless likely).
A variety of contextual factors can play a role, such as the discourse topic,whether the sentence introduces new entities or events that have not been mentionedbefore, or the reader?s background knowledge.A sentence-centric view of compression is also at odds with most relevant appli-cations which aim to create a shorter document rather than a single sentence.
Theresulting document must not only be grammatical but also coherent if it is to function asa replacement for the original.
However, this cannot be guaranteed without knowledgeof how the discourse progresses from sentence to sentence.
To give a simple example, acontextually aware compression system could drop a word or phrase from the currentsentence, simply because it is not mentioned anywhere else in the document and istherefore deemed unimportant.
Or it could decide to retain it for the sake of topiccontinuity.In this article we are interested in creating a compression model that is appropriatefor both documents and sentences.
Luckily, a variety of discourse theories have beendeveloped over the years (e.g., Mann and Thompson, 1988; Grosz, Weinstein, and Joshi1995; Halliday and Hasan 1976) and have found application in summarization (Barzilayand Elhadad 1997; Marcu 2000; Teufel and Moens 2002) and other text generationapplications (Scott and de Souza 1990; Kibble and Power 2004).
In creating a context-sensitive compression model we are faced with three important questions: (1) Whichtype of discourse information is useful for compression?
(2) Is it amenable to automaticprocessing (there is little hope for interfacing our compression model with applicationsif discourse-level cues cannot be identified robustly)?
and (3) How are sentence- anddocument-based information best integrated in a unified modeling framework?412Clarke and Lapata Discourse Constraints for Document CompressionIn building our compression model we borrow insights from two popular modelsof discourse, Centering Theory (Grosz, Weinstein, and Joshi 1995) and lexical chains(Morris and Hirst 1991).
Both approaches capture local coherence?the way adjacentsentences bind together to form a larger discourse.
They also both share the view thatdiscourse coherence revolves around discourse entities and the way they are intro-duced and discussed.
We first automatically augment our documents with annotationspertaining to centering and lexical chains, which we subsequently use to inform ourcompression model.
The latter is an extension of the integer linear programming for-mulation proposed by Clarke and Lapata (2008).
In a nutshell, sentence compression ismodeled as an optimization problem.
Given a long sentence, a compression is formedby retaining the words that maximize a scoring function coupled with a small numberof constraints ensuring that the resulting output is grammatical.
The constraints are en-coded as linear inequalities whose solution is found using integer linear programming(ILP; Winston and Venkataramanan 2003; Vanderbei 2001).
Discourse-level informationcan be straightforwardly incorporated by slightly changing the compression objective?we now wish to compress entire documents rather than isolated sentences?and aug-menting the constraint set with discourse-specific constraints.
We use our model tocompress whole documents (rather than sentences sequentially) and evaluate whetherthe resulting text is understandable and informative using a question-answering task.We show that our method yields significant improvements over discourse agnosticstate-of-the-art compression models (McDonald 2006; Clarke and Lapata 2008).The remainder of this article is organized as follows.
Section 2 provides an overviewof related work.
In Section 3 we present the ILP framework and compression model weemploy in our experiments.
We introduce our discourse-related extensions in Sections 4and 5.
Section 6 discusses our experimental set-up and evaluation methodology.
Ourresults are presented in Section 7.
Discussion of future work concludes the paper.2.
Related WorkSentence compression has been extensively studied across different modeling para-digms and has received both generative and discriminative formulations.
Most gen-erative approaches (Knight and Marcu 2002; Turner and Charniak 2005; Galley andMcKeown 2007) are instantiations of the noisy-channel model, whereas discriminativeformulations include decision-tree learning (Knight and Marcu 2002), maximum en-tropy (Riezler et al 2003), support vector machines (Nguyen et al 2004), and large-margin learning (McDonald 2006; Cohn and Lapata 2009).
These models are trainedon a parallel corpus and learn either which constituents to delete or which words toplace adjacently in the compression output.
Relatively few approaches dispense withthe parallel corpus and generate compressions in an unsupervised manner using eithera scoring function (Hori and Furui 2004; Clarke and Lapata 2008) or compression rulesthat are approximated from a non-parallel corpus such as the Penn Treebank (Turnerand Charniak 2005).The majority of sentence compression approaches only look at sentences in isolationwithout taking into account any discourse information.
However, there are two notableexceptions.
Jing (2000) uses information from the local context as evidence for andagainst the removal of phrases during sentence compression.
The idea here is thatwords or phrases which have more links to the surrounding context are more indicativeof its topic, and thus should not be dropped.
The topic is not explicitly identified;instead the importance of each phrase is determined by the number of lexical linkswithin the local context.
A link is created between two words if they are repetitions,413Computational Linguistics Volume 36, Number 3morphologically related, or associated in WordNet (Fellbaum 1998) through a lexicalrelation (e.g., hyponymy, synonymy).
Links have weights?for example, repetition isconsidered more important than hypernymy.
Each word is assigned a context weightbased on the number of links to the local context and the importance of each relationtype.
Phrases are scored by the sum of their children?s context scores.
The decision todrop a phrase is influenced by several factors, besides the local context, such as thephrase?s grammatical role and previous evidence from a parallel corpus.Daume?
III and Marcu (2002) generalize sentence compression to document com-pression.
Given a document D = w1, w2, .
.
.
, wn the goal is to produce a summary, S, bydropping any subset of words from D. Their system uses the discourse structure of adocument and the syntactic structure of each of its sentences in order to decide whichwords to drop.
Specifically, they extend Knight and Marcu?s (2002) noisy-channel modelso that it can be applied to entire documents.
In its simpler sentence compression instan-tiation, the noisy-channel model has two components, a language model and a channelmodel, both of which act on probabilistic context-free grammar (PCFG) representations.Daume?
III and Marcu define a noisy-channel model over syntax and discourse trees.Following Rhetorical Structure Theory (RST; Mann and Thompson 1988), they representdocuments by trees whose leaves correspond to elementary discourse units (edus) andwhose nodes specify how these and larger units (e.g., multi-sentence segments) arelinked to each other by rhetorical relations (e.g., Contrast, Elaboration).
Discourse unitsare further characterized in terms of their text importance: nuclei denote central seg-ments, whereas satellites denote peripheral ones.
Their model therefore learns not onlywhich syntactic constituents to drop but also which discourse units are unimportant.While Daume?
III and Marcu (2002) present a hybrid summarizer that can simulta-neously delete words and sentences from a document, the majority of summarizationsystems to date simply select and present to the user the most important sentences ina text (see Mani [2001] for a comprehensive overview of the methods used to achievethis).
Discourse-level information plays a prominent role here as the overall documentorganization can indicate whether a sentence should be included in the summary.
Avariety of approaches have focused on cohesion (Halliday and Hasan 1976) and theway it is expressed in discourse.
The term broadly describes a variety of linguisticdevices responsible for making the elements of a text appear unified or connected.Examples include word repetition, anaphora, ellipsis, and the use of synonyms orsuperordinates.
The underlying assumption is that sentences connected to many othersentences are likely to carry salient information and should therefore be includedin the summary (Sjorochod?ko 1972).
In exploiting cohesion for summarization, it isnecessary to somehow represent cohesive ties.
For instance, Boguraev and Kennedy(1997) represent cohesion in terms of anaphoric relations, whereas Barzilay and Elhadad(1997) operationalize cohesion via lexical chains?sequences of related words spanninga topical unit (Morris and Hirst 1991).
Besides repetition, they also examine semanticrelations based on synonymy, antonymy, hypernymy, and holonymy (we discuss theirapproach in more detail in Section 4.1).Other approaches characterize the document in terms of discourse structureand rhetorical relations.
Documents are commonly represented as trees (Mann andThompson 1988; Corston-Oliver 1998; Ono, Sumita, and Miike 1994; Carlson et al 2001)and the position of a sentence in a tree is indicative of its importance.
To give an ex-ample, Marcu (2000) proposes a summarization algorithm based on RST.
Assumingthat nuclei are more salient than satellites, the importance of sentential or clausal unitscan be determined based on tree depth.
Alternatively, discourse structure can be repre-sented as a graph (Wolf and Gibson 2004) and sentence importance is determined in414Clarke and Lapata Discourse Constraints for Document Compressiongraph-theoretic terms, by using graph connectivity measures such as in-degree orPageRank (Brin and Page 1998).
Although a great deal of research in summarizationhas focused on global properties of discourse structure, there is evidence that localcoherence may also be useful without the added complexity of computing discourserepresentations.
(Unfortunately, discourse parsers have yet to achieve levels of perfor-mance comparable to syntactic parsers.)
Teufel and Moens (2002) identify discourserelations on a sentence-by-sentence basis without presupposing an explicit discoursestructure.
Inspired by Centering Theory (Grosz, Weinstein, and Joshi 1995)?a theoryof local discourse structure that models the interaction of referential continuity andsalience of discourse entities?Ora?san (2003) proposes a summarization algorithm thatextracts sentences with at least one entity in common.
The idea here is that summariescontaining sentences referring to the same entity will be more coherent.
Other workhas relied on centering not so much to create summaries but to assess whether they arereadable (Barzilay and Lapata 2008).Our approach differs from previous sentence compression approaches in threekey respects.
First, we present a compression model that is contextually aware; decisionson whether to remove or retain a word (or phrase) are informed by its discourse prop-erties (e.g., whether it introduces a new topic, or whether it is semantically related to theprevious sentence).
Unlike Jing (2000) we explicitly identify topically important wordsand assume specific representations of discourse structure.
Secondly, in contrast toDaume?
III and Marcu (2002) and other summarization work, we adopt a less globaland more shallow representation of discourse based on Centering Theory and lexicalchains.
One of our aims is to exploit discourse features that can be computed efficientlyand relatively cheaply.
Thirdly, our compression model can be applied to isolatedsentences as well as to entire documents.
We claim the latter is more in the spirit of real-world applications where the goal is to generate a condensed and coherent text.
UnlikeDaume?
III and Marcu (2002) our model can delete words but not sentences, although itcould be used to compress documents of any type, even summaries.3.
The Compression ModelOur model is an extension of the approach put forward in Clarke and Lapata (2008)where they formulate sentence compression as an optimization problem.
Given a longsentence, a compression is created by retaining the words that maximize a scoring func-tion.
The latter is essentially a language model coupled with a few constraints ensuringthat the resulting output is grammatical.
The language model and the constraints areencoded as linear inequalities whose solution is found using ILP.1Their model is a good point of departure for studying document-based compres-sion.
As it does not require a parallel corpus, it can be ported across domains andtext genres, while delivering state-of-the-art results (see Clarke and Lapata [2008] fordetails).
Importantly, discourse-level information can be easily incorporated in twoways: Firstly, by applying the compression objective to entire documents rather thanindividual sentences; and secondly, by augmenting the constraint set with discourse-related information.
This is not the case for other approaches (e.g., those based onthe noisy channel model) where compression is modeled by grammar rules indicatingwhich constituents to delete in a syntactic context.
Moreover, ILP delivers a globally1 It is outside the scope of this article to provide an introduction to ILP.
We refer the interested reader toWinston and Venkataramanan (2003) and Vanderbei (2001) for comprehensive overviews.415Computational Linguistics Volume 36, Number 3optimal solution by searching over the entire compression space2 without employingheuristics or approximations during decoding (see Turner and Charniak [2005] andMcDonald [2006] for examples).Besides sentence compression, the ILP modeling framework has been applied toa wide range of natural language processing tasks demonstrating improvements overmore traditional methods.
Examples include reluctant paraphrasing (Dras 1997), rela-tion extraction (Roth and Yih 2004), semantic role labeling (Punyakanok et al 2004),concept-to-text generation (Marciniak and Strube 2005; Barzilay and Lapata 2006),dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009), andcoreference resolution (Denis and Baldridge 2007).In the following we describe Clarke and Lapata?s (2008) model in more detail.Sections 4?5 present our extensions and modifications.3.1 Language ModelLet x = x0, x1, x2, .
.
.
, xn denote a source sentence for which we wish to generate a targetcompression.
We use x0 to denote the ?start?
token.
We introduce a decision variablefor each word in the source and constrain it to be binary; a value of 0 represents a wordbeing dropped, whereas a value of 1 includes the word in the target compression.
Let:?i ={1 if xi is in the compression0 otherwise?i ?
[1 .
.
.
n]A trigram language model forms the backbone of the compression model.
The languagemodel is formulated as an integer linear program with the introduction of extra decisionvariables indicating which word sequences should be retained or dropped from thecompression.
Let:?i ={1 if xi starts the compression0 otherwise?i ?
[1 .
.
.
n]?ij =??
?1 if sequence xi, xj endsthe compression ?i ?
[0 .
.
.
n ?
1]0 otherwise ?j ?
[i + 1 .
.
.
n]?ijk =??
?1 if sequence xi, xj, xk ?i ?
[0 .
.
.
n ?
2]is in the compression ?j ?
[i + 1 .
.
.
n ?
1]0 otherwise ?k ?
[j + 1 .
.
.
n]The objective function is expressed in Equation (1).
It is the sum of all possible trigramsmultiplied by the appropriate decision variable where n is the length of the sentence(note all probabilities throughout this paper are log-transformed).
The objective func-tion also includes a significance score I(xi) for each word xi multiplied by the decision2 For a sentence of length n, there are 2n compressions.416Clarke and Lapata Discourse Constraints for Document Compressionvariable for that word (see the first summation term in Equation (1)).
This score high-lights important content words in a sentence and is defined in Section 3.2.max z =n?i=1?i ?
?I(xi) +n?i=1?i ?
P(xi|start)+n?2?i=1n?1?j=i+1n?k=j+1?ijk ?
P(xk|xi, xj)+n?1?i=0n?j=i+1?ij ?
P(end|xi, xj)?
?min ?
?
?
?max ?
?
(1)Note that we add a weighting factor, ?, to the objective, in order to counterbalance theimportance of the language model and the significance score.The final component of our objective function, ?
?
?, relates to the compression rate.As we explain shortly (Equations (7) and (8)) the compressions our model generatesare subject to a prespecified compression rate.
For instance we may wish to create com-pressions at a minimum rate of 40% and maximum rate of 70%.
The compression rateconstraint can be violated with a penalty, ?, which applies to each word.
?min counts thenumber of words under the compression rate and ?max the number of words over thecompression rate.
Thus, the more the output violates the compression rate, the largerthe penalty will be.
In other words, the term ?min ?
?
?
?max ?
?
acts as a soft constraintproviding a means to guide the compression towards the desired rate.
The violationpenalty ?
is tuned experimentally and may vary depending on the desired compressionrate or application.The objective function in Equation (1) allows any combination of trigrams to beselected.
As a result, invalid trigram sequences (e.g., two or more trigrams containingthe ?end?
token) could appear in the target compression.
We avoid this situation byintroducing sequential constraints (on the decision variables ?i, ?ijk, ?i, and ?ij) thatrestrict the set of allowable trigram combinations.Constraint 1.
Exactly one word can begin a sentence.n?i=1?i = 1 (2)Constraint 2.
If a word is included in the sentence it must either start the sentence or bepreceded by two other words or one other word and the ?start?
token x0.
?k ?
?k ?k?2?i=0k?1?j=i+1?ijk = 0 (3)?k : k ?
[1 .
.
.
n]417Computational Linguistics Volume 36, Number 3Constraint 3.
If a word is included in the sentence it must either be preceded by one wordand followed by another or it must be preceded by one word and end the sentence.
?j ?j?1?i=0n?k=j+1?ijk ?j?1?i=0?ij = 0 (4)?j : j ?
[1 .
.
.
n]Constraint 4.
If a word is in the sentence it must be followed by two words or followedby one word and then the end of the sentence or it must be preceded by one word andend the sentence.
?i ?n?1?j=i+1n?k=j+1?ijk ?n?j=i+1?ij ?i?1?h=0?hi = 0 (5)?i : i ?
[1 .
.
.
n]Constraint 5.
Exactly one word pair can end the sentence.n?1?i=0n?j=i+1?ij = 1 (6)Note that Equations (2)?
(6) are merely well-formedness constraints and differ from thecompression-specific constraints which we discuss subsequently.
Any language modelformulated as an ILP would require similar constraints.Compression rate constraints.
Depending on the application or the task at hand, wemay require that the compressions fall within a specific compression rate.
We assumehere that our model is given a compression rate range, cmin% ?
cmax%, and create twoconstraints that penalize compressions which do not fall within this range:n?i=0?i + ?min ?
cmin ?
n (7)n?i=0?i ?
?max ?
cmax ?
n (8)Here, ?i is still a decision variable for each word, n is the number of words in thesentence, ?
is the number of words over or under the compression rate, and cmin andcmax are the limits of the range.3.2 Significance ScoreThe significance score is an attempt at capturing the gist of a sentence.
The score hastwo components which correspond to document and sentence importance, respectively.Given a sentence and its syntactic parse, we define I(xi) as:I(xi) = fi logFaFi?
lN (9)418Clarke and Lapata Discourse Constraints for Document Compressionwhere xi is a topic word, fi is xi?s document frequency, Fi its corpus frequency, and Fa thesum of all topic words in the corpus; l is the number of clause constituents above xi, andN is the deepest level of clause embedding in the parse.The first term in Equation (9) is similar to tf ?
idf ; it highlights words that areimportant in the document and should therefore not be dropped.
The score is notapplied indiscriminately to all words in a sentence but solely to topic-related words,which are approximated by nouns and verbs.
This is offset by the importance of thesewords in the specific sentence being compressed.
Intuitively, in a sentence with multiplynested clauses, more deeply embedded clauses tend to carry more semantic content.This is illustrated in Figure 1, which depicts the clause embedding for the sentence MrField has said he will resign if he is not reselected, a move which could divide the party nationally.Here, the most important information is conveyed by clauses S3 (he will resign) and S4(if he is not reselected), which are embedded.
Accordingly, we should give more weightto words found in these clauses than in the main clause (S1 in Figure 1).
A simple wayto enforce this is to give clauses weight proportional to the level of embedding (see thesecond term in Equation (9)).
Therefore in Figure 1, the term lN is 1.0 (4/4) for clause S4,0.75 (3/4) for clause S3, and so on.
Individual words inherit their weight from theirclauses.
We obtain syntactic information in our experiments from RASP (Briscoe andCarroll 2002), a domain-independent, robust parsing system for English.
However, anyother parser with broadly similar output (e.g., Lin 2001) could also serve our purposes.Note that the significance score in Equation (9) does not weight differentially thecontribution of tf ?
idf versus level of embedding.
Although we found in our exper-iments that the latter term was as important as tf ?
idf in producing meaningful com-pressions, there may be applications or data sets where the contribution of the two termsvaries.
This could be easily remedied by introducing a weighting factor.3.3 Sentential ConstraintsIn its original formulation, the model also contains a small number of sentence-levelconstraints.
Their aim is to preserve the meaning and structure of the original sentenceas much as possible.
The majority of constraints revolve around modification andFigure 1The clause embedding of the sentence Mr Field has said he will resign if he is not reselected, a movewhich could divide the party nationally; nested boxes correspond to nested clauses.419Computational Linguistics Volume 36, Number 3argument structure and are defined over parse trees or grammatical relations whichas mentioned earlier we extract from RASP.Modifier Constraints.
Modifier constraints ensure that relationships between head wordsand their modifiers remain grammatical in the compression:?i ?
?j ?
0 (10)?i, j : xj ?
xi?s ncmods?i ?
?j ?
0 (11)?i, j : xj ?
xi?s detmodsEquation (10) guarantees that if we include a non-clausal modifier3 (ncmod) in thecompression (such as an adjective or a noun) then the head of the modifier must also beincluded; this is repeated for determiners (detmod) in Equation (11).Other modifier constraints ensure the meaning of the source sentence is preservedin the compression.
For example, Equation (12) enforces not in the compression whenthe head is included.
A similar constraint is added for possessive modifiers (e.g., his,our), including genitives (e.g., John?s gift), as shown in Equation (13).
?i ?
?j = 0 (12)?i, j : xj ?
xi?s ncmods ?
xj = not?i ?
?j = 0 (13)?i, j : xj ?
xi?s possessive modsArgument Structure Constraints.
Argument structure constraints make sure that the re-sulting compression has a canonical argument structure.
The first constraint (Equa-tion (14)) ensures that if a verb is present in the compression then so are its arguments,and if any of the arguments are included in the compression then the verb must also beincluded.
?i ?
?j = 0 (14)?i, j : xj ?
subject/object of verb xiAnother constraint forces the compression to contain at least one verb provided thesource sentence contains one as well:?i:xi?verbs?i ?
1 (15)3 Clausal modifiers (cmod) are adjuncts modifying entire clauses.
In the example he ate the cake because hewas hungry, the because-clause is a modifier of the sentence he ate the cake.420Clarke and Lapata Discourse Constraints for Document CompressionOther constraints apply to prepositional phrases and subordinate clauses and force theintroducing term (i.e., the preposition, or subordinator) to be included in the compres-sion if any word from within the syntactic constituent is also included:?i ?
?j ?
0 (16)?i, j : xj ?
PP/SUB ?
xi starts PP/SUBBy subordinator (SUB) we mean wh-words (e.g., who, which, how, where), the word that,and subordinating conjunctions (e.g., after, although, because).
The reverse is also true?that is, if the introducing term is included, at least one other word from the syntacticconstituent should also be included.
?i:xi?PP/SUB?i ?
?j ?
0 (17)?j : xj starts PP/SUBAll the constraints described thus far are mostly syntactic.
They operate overparse trees or dependency graphs.
In the following sections we present our discourse-specific constraints.
But first we discuss how we represent and automatically detectdiscourse-related information.4.
Discourse RepresentationObtaining an appropriate representation of discourse is the first step toward creating acompression model that exploits document-level information.
Our goal is to annotatedocuments automatically with discourse-level information which will subsequently beused to inform our compression procedure.
As mentioned in Section 2 previous summa-rization work has mainly focused on cohesion (Sjorochod?ko 1972; Barzilay and Elhadad1997) or global discourse structure (Marcu 2000; Daume?
III and Marcu 2002).
We alsoopt for a cohesion-based representation of discourse operationalized by lexical chains(Morris and Hirst 1991).
Computing global discourse structure robustly and accuratelyis far from trivial.
For example, Daume?
III and Marcu (2002) employ an RST parser4but find that it produces noisy output for documents containing longer sentences.We therefore focus on the less ambitious task of characterizing local coherence?theway adjacent sentences bind together to form a larger discourse.
Although it doesnot explicitly capture long distance relationships between sentences, local coherence isstill an important prerequisite for maintaining global coherence.
Specifically, we turnto Centering Theory (Grosz, Weinstein, and Joshi 1995) and adopt an entity-basedrepresentation of discourse.In the following sections we briefly introduce lexical chains and centering anddescribe our algorithms for obtaining discourse annotations.4 This is the decision-based parser described in Marcu (2000); it achieves an F1 of 38.2 for the identificationof elementary discourse units, 50.0 for hierarchical spans, 39.9 for nuclearity, and 23.4 for relationassignment.421Computational Linguistics Volume 36, Number 34.1 Lexical ChainsLexical cohesion refers to the degree of semantic relatedness observed among lexicalitems in a document.
The term was coined by Halliday and Hasan (1976), who observedthat coherent documents tend to have more related terms or phrases than incoherentones.
A number of linguistic devices can be used to signal cohesion; these range fromrepetition, to synonymy, hyponymy, and meronymy.
Lexical chains are a representationof lexical cohesion as sequences of semantically related words (Morris and Hirst 1991).There is a close relationship between discourse structure and cohesion.
Related wordstend to co-occur within the same discourse.
Thus, cohesion is a surface indicator ofdiscourse structure and can be identified through lexical chains.Lexical chains provide a useful means for describing the topic flow in discourse.For example, a document containing the chain {house, home, loft, house} will proba-bly describe a situation involving a house.
Documents often have multiple topics (orthemes) and consequently will contain many different lexical chains.
Some of thesetopics will be peripheral and thus represented by short chains whereas main topicswill correspond to dense longer chains.
Words participating in the latter chains areimportant for our compression task?they reveal what the document is about?and inall likelihood should not be deleted.Barzilay and Elhadad (1997) describe a technique for building lexical chains forextractive text summarization.
In their approach chains of semantically related expres-sions are used to select sentences for inclusion in a summary.
Their algorithm usesWordNet (Fellbaum 1998) to build chains of nouns (and noun compounds).
Nounsare considered related if they are repetitions or linked in WordNet via synonymy,antonymy, hypernymy, and holonymy.
Computing lexical chains would be relativelystraightforward if each word was always represented by a single sense.
However, dueto the high level of polysemy inherent in WordNet, algorithms developed for computinglexical chains must adopt some strategy for disambiguating word senses.
For example,Hirst and St-Onge (1998) greedily disambiguate a word as soon as it is encountered byselecting the sense most strongly related to existing chain members, whereas Barzilayand Elhadad (1997) consider all possible alternatives of word senses and then choosethe best one among them.Once created, lexical chains can serve to highlight which document sentences aremore topical, and should therefore be included in a summary.
Barzilay and Elhadad(1997) rank their chains heuristically by a score based on their length and homogeneity.They generate summaries by extracting sentences corresponding to strong chains, thatis, chains whose score is two standard deviations above the average score.
Analogously,we also wish to determine which lexical chains indicate the most prevalent discoursetopics.
Our assumption is that terms belonging to these chains are indicative of thedocument?s main focus and should therefore be retained in the compressed output.Barzilay and Elhadad?s (1997) scoring function aims to identify sentences (for inclusionin a summary) that have a high concentration of chain members.
In contrast, we areinterested in chains that span several sentences.
We thus score chains according to thenumber of sentences their terms occur in.
For example, the hypothetical chain {house3,home3, loft3, house5} (where wordi denotes word occurring in sentence i) would be given ascore of two as the terms occur only in two sentences.
We assume that a chain signals aprevalent discourse topic if it occurs throughout more sentences than the average chain.The scoring algorithm is outlined more formally as:1.
Compute the lexical chains for the document.422Clarke and Lapata Discourse Constraints for Document Compression2.
Score(Chain) = Sentences(Chain).3.
Discard chains for which Score(Chain) < Average(Score).4.
Mark terms from the remaining chains as being the focus of the document.We use the method of Galley and McKeown (2003) to compute lexical chains foreach document.5 It improves on Barzilay and Elhadad?s (1997) original algorithm byproviding better word sense disambiguation and linear runtime.
The algorithm pro-ceeds in three steps.
Initially, a graph is built representing all possible interpretationsof the document under consideration.
The text is processed sequentially, comparingeach word against all words previously read.
If a relation exists between the senses ofthe current word and any possible sense of a previous word, a connection is formedbetween the appropriate words and senses.
The strength of the connection is a functionof the type of relationship and of the distance between the words in the text (in termsof words, sentences, and paragraphs).
Words are represented as nodes in the graph andsemantic relations as weighted edges.
The relations considered by Galley and McKeown(2003) are all first-order WordNet relations, with the addition of siblings?two wordsare considered siblings if they are both hyponyms of the same hypernym.
Next, alloccurrences of a given word are collected together.
For each sense of a target word,the strength of all connections involving that sense are summed, giving that sense aunified score.
The sense with the highest unified score is chosen as the correct sensefor the target word.
Lastly, the lexical chains are constructed by collecting same sensewords into the same chain.Figure 2 illustrates the lexical chains created by our algorithm for three documents(taken from our test set).
Chains are shown in oval boxes; members of the same chainhave the same index.
The algorithm identifies three chains in the first document: {flow,rate}, {today, day, yesterday}, and {miles, ft}.
In the second document the chains are {body}and {month, night}, and in the third {policeman, police}, {woman, woman, boyfriend, man}.As can be seen, members of a chain represent a shared concept (e.g., ?time?, ?linearunit?, or ?person?).
In some cases important topics are missed.
For instance, in the firstdocument no chains were created with the words lava or debris.
The second documentis about Mrs Allan and contains many references to her.
However, because Mrs Allan isnot listed in WordNet it is not possible to create any chains for this word or any of itscoreferents (e.g., she, her).
A similar problem is observed in the third document whereAnderson is not included in any chain even though he is one of the main protagoniststhroughout the text.
We next turn to Centering Theory as a means of identifying whichentities are prominent in a document.4.2 Centering TheoryCentering Theory (Grosz, Weinstein, and Joshi 1995) is an entity-orientated theory oflocal coherence and salience.
One of the main ideas underlying centering is that certainentities mentioned in an utterance are more central than others.
This in turn imposesconstraints on the use of referring expressions and in particular on the use of pronouns.The theory begins by assuming that a discourse is broken into ?utterances.?
Thesecan be phrases, clauses, sentences, or even paragraphs.
At any point in discourse,some entities are considered more salient than others, and are expected to exhibit5 The software is available from http://www1.cs.columbia.edu/nlp/tools.cgi.423Computational Linguistics Volume 36, Number 3Figure 2Excerpts of documents from our test set with discourse annotations.
Centers are in double boxes;terms occurring in lexical chains are in oval boxes.
Words with the same subscript are membersof the same chain (e.g., police, policeman).different properties.
Specifically, although each utterance may contain several entities, itis assumed that a single entity is ?centered,?
thereby representing the current discoursefocus.
One of the main claims underlying centering is that discourse segments in whichsuccessive utterances contain common centers are more coherent than segments wherethe center repeatedly changes.Each utterance Uj in a discourse has a list of forward-looking centers, Cf (Uj), anda unique backward-looking center, Cb(Uj).
Cf (Uj) represents a ranking of the entitiesinvoked by Uj according to their salience.
Thus, some entities in the discourse aredeemed more important than others.
The Cb of the current utterance Uj is the highest-ranked element in Cf (Uj?1) that is also in Uj.
(Centering hypothesizes that the Cb islikely to be realized as a pronoun.)
Entities are commonly ranked in terms of theirgrammatical function, namely, subjects are ranked more highly than objects, which aremore highly ranked than the rest (Grosz, Weinstein, and Joshi 1995).
The Cb links Uj tothe previous discourse, but it does so locally since Cb(Uj) is chosen from Uj?1.Centering formalizes fluctuations in topic continuity in terms of transitions be-tween adjacent utterances.
Grosz, Weinstein, and Joshi (1995) distinguish between three424Clarke and Lapata Discourse Constraints for Document Compressiontypes of transitions.
In CONTINUE transitions, Cb(Uj) = Cb(Uj?1) and Cb(Uj) is themost highly ranked element entity in Uj.
In RETAIN transitions Cb(Uj) = Cb(Uj?1) butCb(Uj) is not the most highly ranked element entity in Uj.
And in SHIFT transitionsCb(Uj) = Cb(Uj?1).
These transitions are ordered: CONTINUEs are preferred over RE-TAINs, which are preferred over SHIFTs.
And discourses with many CONTINUE transi-tions are considered more coherent than those which repeatedly SHIFT from one centerto the other.We demonstrate these concepts in passages (1a)?
(1c) taken from Walker, Joshi, andPrince (1998).
(1) a. Jeff helped Dick wash the car.CF(Jeff, Dick, car)b.
He washed the windows as Dick waxed the car.CF(Jeff, Dick, car)CB=Jeffc.
He soaped a pane.CF(Jeff, pane)CB=JeffHere, the first utterance does not have a backward-looking center but has three forward-looking centers Jeff, Dick, and car.
To determine the backward-looking center of (1b) wefind the highest ranked entity among the forward-looking centers in (1a) which alsooccurs in (1b).
This is Jeff as it is the subject (and thus most salient entity) in (1a) andpresent (as a pronoun) in (1b).
The same procedure is applied for utterance (1c).
Alsonote that (1a) and (1b) are linked via a CONTINUE transition.
The same is true for (1b)and (1c).For the purposes of our document compression application, we are not so muchinterested in characterizing our texts in terms of entity transitions.
Because they are allwritten by humans, we can assume they are more or less coherent.
Nonetheless, identi-fying the centers in discourse seems important.
These will indicate what the documentis about, who the main protagonists are, and how the discourse focus progresses.
Wewould probably not want to delete entities functioning as backward-looking centers.As Centering is primarily a linguistic theory rather than a computational one,it is not explicitly stated how the concepts of ?utterance,?
?entities,?
and ?ranking?are instantiated.
A great deal of research has been devoted to fleshing these out andmany different instantiations have been developed in the literature (see Poesio et al[2004] for details).
In our case, the instantiation will have a bearing on the reliabilityof the algorithm to detect centers.
If the parameters are too specific then it may not bepossible to accurately determine the center for a given utterance.
Because our aim isto identify centers in discourse automatically, our parameter choice is driven by twoconsiderations: robustness and ease of computation.We therefore follow previous work (e.g., Miltsakaki and Kukich 2000) in assumingthat the unit of an utterance is the sentence (i.e., a main clause with accompanyingsubordinate and adjunct clauses).
This is a simplistic view of an utterance; however itis in line with our compression task, which also operates over sentences.
We determinewhich entities are invoked by a sentence using two methods.
First, we perform namedentity identification and coreference resolution on each document using LingPipe,6 a6 LingPipe can be downloaded from http://alias-i.com/lingpipe/.425Computational Linguistics Volume 36, Number 3publicly available system.
Named entities are not the only type of entity to occur in ourdata, thus to ensure a high entity recall we add named entities and all remaining nouns7to the Cf list.
Entity matching between sentences is required to determine the Cb of a sen-tence.
This is done using the named entity?s unique identifier (as provided by LingPipe)or by the entity?s surface form in the case of nouns not classified as named entities.We follow Grosz, Weinstein, and Joshi (1995) in ranking entities according to theirgrammatical roles; subjects are ranked more highly than objects, which are in turnranked higher than other grammatical roles; ties are broken using left-to-right orderingof the grammatical roles in the sentence (Tetreault 2001).
We identify grammatical rolesusing RASP (Briscoe and Carroll 2002).
Formally, our centering algorithm is as follows(where Uj corresponds to sentence j):1.
Extract entities from Uj.2.
Create Cf (Uj) by ranking the entities in Uj according to their grammaticalrole (subjects > objects > others, ties broken using left-to-right word orderof Uj).3.
Find the highest ranked entity in Cf (Uj?1) which occurs in Cf (Uj); set theentity to be Cb(Uj).This procedure involves several automatic steps (named entity recognition, coreferenceresolution, and identification of grammatical roles) and will unavoidably produce somenoisy annotations.
There is no guarantee, therefore, that the right Cb will be identified orthat all sentences will be marked with a Cb.
The latter situation also occurs in passagesthat contain abrupt changes in topic.
In such cases, none of the entities realized in Uj willoccur in Cf (Uj?1).
Hopefully, lexical chains will come to the rescue here as an alternativemeans of capturing local content within a document.Figure 2 shows the centers (in double boxes) identified by our algorithm.
In the firstdocument lava and debris are marked as centers, in the second document Mrs Allan (andits coreferents), and in the third one Peter Anderson and allotment.
When comparing theannotations produced by centering and the lexical chains, we observe that they tendto be complementary.
Proper nouns that lexical chains miss out on are often identi-fied by centering.
When the latter fails, due to errors in coreference resolution or theidentification of grammatical relations, lexical chains can be more robust because onlyWordNet is required for their computation.
As an example consider the third documentin Figure 2.
Here, lexical chains provide a better insight into the text.
Were we to relysolely on centering, we would obtain annotations only for two entities, namely, PeterAnderson and allotment.5.
The Discourse-Inspired Compression ModelWe now turn our attention to incorporating discourse information into our compressionmodel.
Before compression takes place, all documents are processed using the center-ing and lexical chain algorithms described earlier.
In each sentence we annotate thecenter Cb(Uj) if one exists.
Words (or phrases) that are present in the current sentenceand function as the center in the next sentence Cb(Uj+1) are also flagged.
Finally, words7 As determined by the word?s part-of-speech tag.426Clarke and Lapata Discourse Constraints for Document Compressionare marked if they are part of a prevalent (high scoring) chain.
Provided with thisadditional knowledge our model takes a (sentence-separated) source document as inputand generates a compressed version by applying sentence-level and discourse-levelconstraints to the entire document rather than to each sentence sequentially.
In ourearlier formulation of the compression task (Clarke and Lapata 2008), we create andsolve an ILP for every sentence, whereas now an ILP is solved for each document.This makes sense from a discourse perspective as compression decisions are not madeindependently of each other.
Also note that this latter formulation brings compressioncloser to summarization as we can manipulate the document compression rate directly,for example, by adding a constraint that forces the target document to be less than b to-kens.
This allows the model to choose how much to compress each individual sentencewithout requiring that they all have the same compression rate.
Accordingly, we modifyour objective function by introducing a sum over all sentences (assuming l sentences arepresent in the document) and adding an additional index g to each decision variable totrack the sentence it came from:max z =l?g=1[ ng?i=1?g,i ?
?I(xg,i) +ng?i=1?g,i ?
P(xg,i|start)+ng?2?i=1ng?1?j=i+1ng?k=j+1?g,ijk ?
P(xg,k|xg,i, xg,j)+ng?1?i=0ng?j=i+1?g,ij ?
P(end|xg,i, xg,j)???
?min ?
?
?
?max ?
?
(18)We also modify the compression rate soft constraint to act over the whole documentrather than sentences.
This allows some sentences to violate the compression rate with-out incurring a penalty, provided the compression rate of the document falls within thespecified range.Document Compression Rate Constraints.
We wish to penalize compressions which do notfall within a desired compression rate range (cmin% ?
cmax%).l?g=1ng?i=0?g,i + ?min ?
cmin ?l?g=1ng (19)l?g=1ng?i=0ng?i=0?g,i ?
?max ?
cmax ?l?g=1ng (20)Besides the new objective function and compression rate constraints, the modelmakes use of all the sentence-level constraints introduced in Section 3.3, but is cruciallyenhanced with three discourse constraints explained in the following.427Computational Linguistics Volume 36, Number 35.1 Discourse ConstraintsOur first goal to is preserve the focus of each sentence.
If the center, Cb, is identified inthe source sentence it must be retained in the target compression.
If present, the entityrealized as the Cb in the following sentence should also be retained to ensure the focusis preserved from one sentence to the next.
Such a condition is easily captured with thefollowing ILP constraint:?i = 1 (21)?i : xi ?
{Cb(Uj), Cb(Uj+1)}As an example, consider the first discourse in Figure 2.
The constraints generated fromEquation (21) will require the compression to retain lava in the first two sentences anddebris in the second and third sentences.As mentioned in the previous section, the centering algorithm relies on NLP tech-nology that is not 100% accurate (named entity detection, parsing, and coreferenceresolution).
Therefore, the algorithm can only approximate the center for each sen-tence and in some cases fails to identify any centers at all.
Lexical chains provide acomplementary annotation of the topic or theme of the document using informationwhich is not restricted to adjacent sentences.
Recall that once chains are created, theyare scored, and chains with scores less than the average are discarded.
We consider allremaining lexical chains as topical and require that words in these be retained in thecompression.
?i = 1 (22)?i : xi ?
document topical lexical chainConsider again the first text in Figure 2.
Here, flow and rate are members of the samechain (marked with subscript 1).
According to constraint (22) both words must beincluded in the compressed document.
In the third document the words relating to?police?
(police, policeman) and ?people?
(woman, boyfriend, man) also would be retainedin the compression.Our final discourse constraint concerns pronouns.
Specifically, we force per-sonal pronouns (whose antecedent may not always be identified) to be included in thecompression.
?i = 1 (23)?i : xi ?
personal pronounsThe constraints just described ensure that the compressed document will retainthe discourse flow of the source document and will preserve terms indicative ofimportant topics.
Document compression aside, the discourse constraints will alsobenefit sentence-level compression.
They provide our model, which so far relied onsyntactic evidence and surface level document characteristics (i.e., word frequencies),additional evidence for retaining (discourse) relevant words.428Clarke and Lapata Discourse Constraints for Document Compression5.2 Applying the ConstraintsAs explained earlier we apply the model and the constraints to each document.
In ourearlier sentence-based formulation, a significance score (see Section 3.2) was used tohighlight which nouns and verbs should be included in the compression.
As far asnouns are concerned, our discourse constraints perform a similar task.
Thus, when asentence contains discourse annotations, we are inclined to trust them more and onlycalculate the significance score for verbs.During development it was observed that applying all discourse constraints si-multaneously (see Equations (21)?
(23)) results in relatively long compressions.
Tocounteract this, we employ these constraints using a back-off strategy that relies onprogressively less reliable information.
Our back-off model works as follows: If center-ing information is present, we apply the appropriate constraints (Equation (21)).
If nocenters are present, we back off to the lexical chain information using Equation (22), andin the absence of the latter we back off to the pronoun constraint (Equation (23)).
Finally,if discourse information is entirely absent from the sentence, we default to the sig-nificance score.
Sentential constraints are applied throughout irrespective of discourseconstraints.
We determined this ordering (i.e., centering first, then lexical chains, andthen pronouns) on the development set.
Centering tends to be more precise, whereaslexical chains have high recall but lower precision in terms of identifying which entitiesare in focus and should therefore not be dropped.
In our test data (see Section 6 fordetails), the centering constraint was used in 68.6% of the sentences.
The model backedoff to lexical chains for 13.7% of the test sentences, whereas the pronoun constraintwas applied in 8.5%.
Finally, the noun and verb significance score was used on theremaining 9.2%.
Examples of our system?s output for the texts in Figure 2 are given inFigure 3.6.
Experimental Set-upIn this section we present our experimental set-up for assessing the performance ofthe compression model.
We describe the compression corpus used in our study, brieflyintroduce the model used for comparison with our approach, and explain how systemoutput was evaluated.6.1 Compression CorpusPrevious work on sentence compression has used almost exclusively the Ziff-Davis cor-pus, a compression corpus derived automatically from document?abstract pairs (Knightand Marcu 2002).
Unfortunately, this corpus is not suitable for our purposes because itconsists of isolated sentences taken from several different documents.
We thus createda document-based compression corpus manually.
Specifically, annotators were pre-sented with one document at a time and asked to compress sentences sequentiallyby removing tokens.
They were free to remove any words they deemed superfluous,provided their deletions (a) preserved the most important information in the source sen-tence, and (b) ensured the compressed sentence remained grammatical.
If they wished,they could leave a sentence uncompressed.
They were not allowed to delete wholesentences even if they believed they contained no information content with respect tothe story, as this would blur the task with summarization.
Following these guidelines,429Computational Linguistics Volume 36, Number 3Figure 3Compression output on excerpts from Figure 2 using the discourse model.
Words that aredropped are striken out.the annotators created compressions for 82 stories (1,629 sentences) from the BNC andthe LA Times and Washington Post.8 Forty-eight (48) documents (962 sentences) wereused for training, 3 for development (63 sentences), and 31 for testing (604 sentences).6.2 Comparison with State-of-the-ArtThe discourse-based compression model was evaluated against our earlier sentence-based ILP model (without the discourse constraints).
In addition, we compared our ap-proach against a state-of-the-art model which does not take discourse-level informationinto account, does not use ILP, and is sentence-based.
We give a brief description in thefollowing, and refer the interested reader to McDonald (2006) for details.McDonald (2006) formalizes sentence compression as a classification task in a dis-criminative large-margin learning framework: Pairs of words from the source sentenceare classified as being adjacent or not in the target compression.
Let x = x1, .
.
.
, xndenote a source sentence with a target compression y = y1, .
.
.
, ym where each yj oc-curs in x.
The function L(yi) ?
{1 .
.
.
n} maps word yi in the target to the index ofthe word in the source, x (subject to the constraint that L(yi) < L(yi+1)).
McDonalddefines the score of a compression y for a sentence x as the dot product between8 The corpus is available from http://homepages.inf.ed.ac.uk/s0460084/data/.430Clarke and Lapata Discourse Constraints for Document Compressiona high-dimensional feature representation over bigrams and a corresponding weightvector:s(x, y) =|y|?j=2w ?
f(x, L(yj?1), L(yj)) (24)Decoding in this framework amounts to finding the combination of bigrams that maxi-mize the scoring function in Equation (24).
The maximization is solved using dynamicprogramming (see McDonald [2006] for details).The model parameters are estimated using the Margin Infused Relaxed Algorithm(MIRA; Crammer and Singer 2003), a discriminative large-margin online learning tech-nique.
This algorithm learns by compressing each sentence and comparing the resultwith the gold standard.
The weights are updated so that the score of the correct com-pression (the gold standard) is greater than the score of all other compressions by amargin proportional to their loss.
The loss function is the number of words falsely re-tained or dropped in the incorrect compression relative to the gold standard.
McDonaldemploys a rich feature set defined over words, parts of speech, phrase structure trees,and dependencies.
These are gathered over adjacent words in the compression and thewords in between which were dropped.It is important to note that McDonald (2006) is not a straw-man system.
It achieveshighly competitive performance compared with Knight and Marcu?s (2002) noisy-channel and decision-tree models.
Due to its discriminative nature, the model is ableto use a large feature set and to optimize compression accuracy directly.
In other words,McDonald?s model has a head start against our own model which does not utilize alarge parallel corpus and has only a few constraints.
The comparison of the two systemsallows us to establish that we have a competitive state-of-the-art system, even withoutdiscourse constraints.We trained McDonald?s (2006) model on the full training set (48 documents, 962sentences).
Our implementation used an identical feature set, the only difference beingthat our phrase structure and dependency features were extracted from the outputof Roark?s (2001) parser.
McDonald uses Charniak?s (2000) parser, which performscomparably.
We also employed a slightly modified loss function to encourage compres-sion on our data set.
McDonald?s results were reported on the Ziff-Davis corpus.
Thelanguage model required for the ILP system was trained on 80 million tokens from theEnglish GigaWord corpus (LDC2007T07) using the SRI Language Modeling Toolkit withKneser-Ney discounting.
The significance score was calculated on 80 million tokensfrom the same corpus.
The ILP model presented in Equation (1) implements a weightedcombination of the significance score with a language model.
The weight was tunedon the development set which consisted of three source documents and their targetcompressions.
Our optimization procedure used Powell?s method (Press et al 1992) anda loss function based on the grammatical relations F1 between the gold standard andsystem output.
The optimal weight was approximately 9.0.
Note that the developmentset was the only source of parallel data our model had access to.In order to compare all three models (sentence-based ILP, discourse-based ILP, andMcDonald [2006]) on an equal footing, we ensured that their compression rates weresimilar.
To do this, we first run McDonald?s model on our data and then set the com-pression rate for our ILP models so that it is comparable to his output.
This can be donerelatively straightforwardly by adjusting the compression rate range soft constraint.
Inour experiments we set the minimum compression rate to 57%, the upper rate to 62%,431Computational Linguistics Volume 36, Number 3and the violation penalty (?)
to ?99.
In practice, the soft constraint controlling thecompression rate can be removed or specifically tuned to suit the application.6.3 EvaluationPrevious studies evaluate the well-formedness of automatically generated compres-sions out of context.
The target sentences are typically rated by naive subjects on twodimensions, grammaticality and importance (Knight and Marcu 2002).
Automatic eval-uation measures have also been proposed.
Riezler et al (2003) compare the grammaticalrelations found in the system output against those found in a gold standard using F1.Although F1 conflates grammaticality and importance into a single score, it neverthe-less has been shown to correlate reliably with human judgments (Clarke and Lapata2006).The aims of our evaluation study were twofold.
Firstly, we wanted to examinewhether our discourse constraints improve the compressions for individual sentences.There is no hope for generating shorter documents if the compressed sentences areeither too wordy or too ungrammatical.
Secondly and more importantly, our goal wasto evaluate the compressed documents as a whole by examining whether they arereadable and the degree to which they retain key information when compared to theoriginals.
We evaluated sentence-based compressions automatically using F1 and thegrammatical relations annotations provided by RASP (Briscoe and Carroll 2002).
Thisparser is suited to the compression task as it provides parses for both full sentencesand sentence fragments and is generally robust enough to analyze semi-grammaticalsentences.
We computed F1 over all the relations provided by RASP (e.g., subject,direct/indirect object, modifier; 17 in total).
We compared the output of our discoursesystem on the test set (31 documents, 604 sentences) against the sentence-based ILPmodel and McDonald (2006).Our document-level evaluation was motivated by two questions: (1) Are the com-pressed documents readable?
and (2) How much key information is preserved betweenthe source document and its target compression?
The readability of a document isfairly straightforward to measure by asking participants to provide a rating (e.g., on aseven-point scale).
Measuring how much information is preserved in the compresseddocument is more involved.
Under the assumption that the target document is tofunction as a replacement for the source, we can measure the extent to which thecompressed version can be used to find answers for questions which have been derivedfrom the source and are representative of its core content.
We thus created questionsfrom the source and then determined whether it was possible to find their answers byreading the compressed target.
The more questions a hypothetical compression systemcan answer, the better it is at compressing the document as a whole.A question-answering (Q&A) paradigm has been used previously to evaluatesummaries and text compression.
Morris, Kasper, and Adams (1992) performed oneof the first Q&A evaluations to investigate the degree to which documents could besummarized before reading comprehension diminished.
Their corpus consisted of fourpassages randomly selected from a set of sample Graduate Management Aptitude Test(GMAT) reading comprehension tests.
The texts covered a range of topics includingmedieval literature, 18th-century Japan, minority-operated businesses, and Florentineart.
Accompanying each text were eight multiple-choice questions, each containingfive possible answers.
The questions were provided by the Educational Testing Serviceand were designed to measure the subjects?
reading comprehension.
Subjects were432Clarke and Lapata Discourse Constraints for Document Compressiongiven various textual treatments: the full text, a human-authored abstract, three system-generated extracts, and a final treatment where merely the questions were presentedwithout any text.
The questions-only treatment was used as a control to investigate ifsubjects could answer questions without any source material.
Subjects were instructedto read the passage (if provided) and answer the multiple choice questions.The advantage of using standardized tests, such as the GMAT reading compre-hension test, is that Q&A pairs are provided along with a method for scoring answers(the correct answer is one among five possible choices).
However, our corpora do notcontain ready prepared Q&A pairs; thus we require a methodology for constructingquestions and their answers and scoring documents against the answers.
One suchmethodology is presented in the TIPSTER Text Summarization Evaluation (SUMMAC;Mani et al 2002).
SUMMAC was concerned with producing summaries tailored tospecific topics.
The Q&A task involved an evaluation where a topic-related summaryfor a document was evaluated in terms of its ?informativeness,?
namely, the degreeto which it contained answers found in the source document to a set of topic-relatedquestions.
For each topic (three in total), 30 relevant documents were chosen to generatea single summary.
One annotator per topic came up with no more than five questionsrelating to the obligatory aspects of the topic.
An obligatory aspect of a topic wasdefined as information that must be present in the document for the document to berelevant to the topic.
The annotators then created an answer key for their topic byannotating the passages and phrases from the documents which provided the answersto the questions.
In the SUMMAC evaluation, the annotator for each topic was taskedwith scoring the system summaries.
Scoring involved comparing the summaries againstthe answer key (annotated passages from the source documents) while judging whetherthe summary provided a Correct, Partially Correct, or Missing answer.
If a summary con-tained an answer key and sufficient context the summary was deemed correct; however,summaries would be considered partially correct if the answer key was present but withinsufficient context.
If context was completely missing, misleading, or the answer keywas absent then the summary was judged missing.Our methodology for constructing Q&A pairs and for scoring documents is in-spired by the SUMMAC evaluation exercise (Mani et al 2002).
Rather than creatingquestions for document sets (or topics) our questions were derived from individualdocuments.
Two annotators were independently instructed to read the documents fromour (test) corpus and create Q&A pairs.
Each annotator drafted no more than tenquestions and answers per document, related to its content.
Annotators were askedto create fact-based questions which required an unambiguous answer; these weretypically who, what, where, when, and how?style questions.
The purpose of using twoannotators per document was to allow annotators to compare and revise their Q&Apairs; this process was repeated until a common agreed-upon set of questions wasreached.
Revisions typically involved merging and simplifying questions to make themclearer, and in some cases splitting a question into multiple questions.
Documents forwhich too few questions were agreed upon and for which the questions and answerswere too ambiguous were removed.
This left an evaluation set of six documents withbetween five to eight concise questions per document.
Figure 4 shows a document fromour test set and the questions and answers our annotators created for it.For scoring our documents we adopt a more objective method than SUMMAC.Instead of asking the annotator who constructed the questions to check the documentcompressions for the answers, we ask naive participants to read the compressed doc-uments and answer the questions as best as they can.
During evaluation, the sourcedocument is not shown to our subjects; thus, if the compression is difficult to read, the433Computational Linguistics Volume 36, Number 3Figure 4Example document from our test set and questions with answer key created for this document.participants have no point of reference to help them understand the compression.
Thisis a departure from previous evaluations within text generation tasks, where the sourcetext is available at judgment time; in our case only the system output is available.The document-based evaluation was conducted remotely over the Internet usinga custom-built Web interface.
Upon loading the Web interface, participants were pre-sented with a set of instructions that explained the Q&A task and provided examples.434Clarke and Lapata Discourse Constraints for Document CompressionTable 1Compression results: compression rate and relation-based F1.Model CompR Precision Recall F1McDonald 60.1% 43.9% 36.5%?
37.9%?Sentence ILP 62.1% 40.7%?
39.4%?
39.0%?Discourse ILP 61.0% 46.2% 44.2% 42.2%Gold Standard 70.3% ??
??
???
Significantly different from Discourse ILP (p < 0.01 using the Wilcoxon test).Subjects were first asked to read the compressed document and then rate its readabilityon a seven-point scale where 7 = excellent, and 1 = terrible.
Next, questions werepresented one at a time (the order being is defined by the annotators) and participantswere encouraged to consult the document for the answer.
Answers were written directlyinto a text field on the Web interface which allowed free-form text to be submitted.
Oncea participant provided an answer and confirmed the answer, the interface locked theanswer to ensure it was not modified later.
This was necessary because later questionscould reveal information which would help answer previous questions.We elicited answers for six documents in four compression conditions: gold stan-dard, using the ILP sentence-based model, the ILP discourse model, and McDonald?s(2006) model.
A Latin square design was used to prevent participants from seeingmultiple treatments (compressions) of the same document thus removing any learningeffect.
A total of 116 unpaid volunteers completed the experiment.
They were recruitedthrough student mailing lists and the Language Experiments Web site.9 The answersprovided by our subjects were scored against an answer key.
A correct answer wasmarked with a score of one, and zero otherwise.
In cases where two answers wererequired, a score of 0.5 was awarded to each correct answer.
The score for a compresseddocument is the average of its question scores.
All subsequent tests and comparisonsare performed on the document score.7.
ResultsWe first assessed the compressions produced by the two ILP models (Discourse andSentence) and McDonald (2006) on a sentence-by-sentence basis.
Table 1 shows thecompression rates (CompR) for the three systems and evaluates the quality of theiroutput using grammatical relations F1.
As can be seen, all three systems producecomparable compression rates.
The Discourse ILP compressions are slightly longer thanMcDonald?s (2006) (61.0% vs. 60.1%) and slightly shorter than the Sentence ILP model(61.0% vs. 62.1%).
The Discourse ILP model is significantly better than McDonald (2006)and Sentence ILP in terms of F1, indicating that discourse-level information is generallyhelpful.
All three systems could use further improvement, as inter-annotator agreementon this data yields an F1 of 65.8% (Clarke 2008).Let us now consider the results of our document-based evaluation.
Table 2 showsthe mean readability ratings obtained for each system and the percentage of questionsanswered correctly.
We used an analysis of variance (ANOVA) to examine the effect9 Available at http://www.language-experiments.org.435Computational Linguistics Volume 36, Number 3Table 2Human evaluation results: average readability ratings and average percentage of questionsanswered correctly.Model Readability Q&A (%)McDonald 2.52?
51.42?
?Sentence ILP 2.76?
52.35?
?Discourse ILP 3.10?
71.38?Gold Standard 5.41?
85.48??
Significantly different from Gold Standard.?
Significantly different from Discourse ILP.of compression type (McDonald, Sentence ILP, Discourse ILP, Gold Standard).
TheANOVA revealed a reliable effect on both readability and Q&A.
Post hoc Tukey testsshowed that McDonald and the two ILP models do not differ significantly in termsof readability.
However, they are all significantly less readable than the gold standard(?
< 0.01).
For the Q&A task, we observe that our system is significantly better thanMcDonald (?
< 0.01) and Sentence ILP (?
< 0.01), but significantly worse than the goldstandard (?
< 0.05).
McDonald and Sentence ILP yield comparable performance (theirdifference is not statistically significant).These results indicate that the automatic systems lag behind the human gold stan-dard in terms of readability.
When reading entire documents, subjects are less tolerantof ungrammatical constructions.
We also find out that, despite relatively low readability,the documents are overall understandable.
The discourse-based model generates moreinformative documents?the number of questions answered correctly increases by 19%in comparison to McDonald and Sentence ILP.
This is an encouraging result suggestingthat there are advantages in developing compression models that exploit discourse-level information information.Figure 5 shows the output of the ILP systems (Discourse and Sentence) on twotest documents.
Words that are dropped have been stricken out.
As can be seen, thetwo systems produce different compressions, and the discourse-based output is morecoherent.
This is corroborated by the readability results where the discourse ILP modelreceived the highest rating.
Also note that some of the compressions produced by thesentence-based model distort the meaning of the original text, presumably leading thereader to make wrong inferences.
For example, in the second document (Sentence ILPversion) one infers that the victim was urged to report the incident.
Moreover, importantinformation is often omitted, for example, that the victim was indeed raped or that thestrike would be damaging not only to the company but also to its staff (see the SentenceILP version in the first document).8.
Conclusions and Future WorkIn this article we proposed a novel method for automatic sentence compression.
Centralin our approach is the use of discourse-level information, which we argue is an impor-tant prerequisite for document (as opposed to sentence) compression.
Our model usesinteger linear programming for inferring globally optimal compressions in the presenceof linguistically motivated constraints.
Our discourse constraints aim to capture localcoherence and are inspired by Centering Theory and lexical chains.
We showed that our436Clarke and Lapata Discourse Constraints for Document CompressionImprovements in certain allowances were made, described as divisive bythe unions, but the company has refused to compromise on a reduction inthe shorter working week.
Ford dismissed an immediate meeting with theunions but did not rule out talks after Christmas.
It said that a strike wouldbe damaging to the company and to its staff.
Production closed down at Fordlast night for the Christmas period.
Plants will open again on January 2.DiscourseILPImprovements in certain allowances were made, described as divisive bythe unions, but the company has refused to compromise on a reduction inthe shorter working week.
Ford dismissed an immediate meeting with theunions but did not rule out talks after Christmas.
It said that a strike wouldbe damaging to the company and to its staff.
Production closed down at Fordlast night for the Christmas period.
Plants will open again on January 2.SentenceILPHe threatened her by forcing his truncheon under her chin and then rapedher.
She said he only refrained from inserting his truncheon into her, after shebegged him not to.
Afterwards he told her not to report the incident becausehe could have her ?nicked?
for soliciting.
She did not report it because shedid not think she would be believed.
Police investigated after an anonymousreport.DiscourseILPHe threatened her by forcing his truncheon under her chin and then rapedher.
She said he only refrained from inserting his truncheon into her, after shebegged him not to.
Afterwards he told her not to report the incident becausehe could have her ?nicked?
for soliciting .
She did not report it because shedid not think she would be believed.
Police investigated after an anonymousreport.SentenceILPFigure 5Output of Discourse and Sentence ILP systems on two test documents.
Words that are strickenout have been dropped.model can be successfully employed to produce compressed documents that preservemost of the original core content.Our results confirm the conventional wisdom that discourse-level information ishelpful in summarization.
We also show that this type of information can be identifiedrobustly in free text.
Our experiments focused primarily on local discourse structure us-ing two complementary representations.
Centering tends to produce more annotationssince it tries to identify a center in every sentence.
Lexical chains tend to provide moregeneral information, such as the major topics in a document.
Due to their approximatenature, there is no one representation that is uniquely suited to the compression task.Rather, it is the synergy between lexical chains and centering that brings improvements.The discourse annotations proposed here are not specific to our model.
They couldbe easily translated into features and incorporated into discriminative modeling par-adigms (e.g., Nguyen et al 2004; McDonald 2006; Cohn and Lapata 2009).
The sameis true for the Q&A evaluation paradigm employed in our experiments.
It could bestraightforwardly adapted to assess the information content of shorter summaries andpotentially used to perform large-scale comparisons within and across systems.Our approach differs from most summarization work in that our summaries arefairly long.
However, we believe this is the first step to understanding how com-pression can help summarization.
An obvious extension would be to interface our437Computational Linguistics Volume 36, Number 3compression model with sentence extraction (see Martins and Smith [2009] for an ILPformulation of a model that jointly performs sentence extraction and compression,without, however, taking discourse level information into account).
The discourseannotations can help guide the extraction method into selecting topically related sen-tences which can consequently be compressed together.
More generally, formulating thesummarization process in the ILP framework outlined here would allow the integrationof varied and sometimes conflicting constraints during summary generation.
Examplesinclude the summary length, and whether it is coherent, grammatical, or repetitive.
Ad-ditional flexibility can be introduced by changing some of the constraints from hard tosoft (as we did with the compression rate constraints), although determining the penaltyfor constraint violation manually using prior knowledge is a non-trivial task (Chang,Ratinov, and Roth 2007) and automatically learning the constraint penalty results in aharder learning problem.
Importantly, under the ILP formulation such constraints canbe explicitly encoded and applied during inference while finding a globally optimalsolution.AcknowledgmentsWe are grateful to Ryan McDonald for hishelp with the re-implementation of hissystem, and our annotators Vasilis Karaiskosand Sarah Luger.
Thanks to Alex Lascarides,Sebastian Riedel, and Bonnie Webber forinsightful comments and suggestions, and tothe anonymous referees whose feedbackhelped to substantially improve the presentarticle.
Lapata acknowledges the support ofEPSRC (grant GR/T04540/01).ReferencesAho, A. V. and J. D. Ullman.
1969.
Syntaxdirected translations and the pushdownassembler.
Journal of Computer and SystemSciences, 3:37?56.Barzilay, R. and M. Elhadad.
1997.
Usinglexical chains for text summarization.
InProceedings of the ACL-97 Intelligent ScalableText Summarization Workshop, pages 10?17,Madrid.Barzilay, Regina and Mirella Lapata.
2006.Aggregation via set partitioning fornatural language generation.
In Proceedingsof the Human Language TechnologyConference of the North American Chapter ofthe Association for Computational Linguistics,pages 359?366, New York, NY.Barzilay, Regina and Mirella Lapata.
2008.Modeling local coherence: An entity-basedapproach.
Computational Linguistics,34(1):1?34.Boguraev, Branimir and Chris Kennedy.1997.
Salience-based contentcharacterization of text documents.
InProceedings of the ACL?97/EACL?97Workshop on Intelligent Scalable TextSummarization, pages 2?9, Madrid.Brin, Sergey and Michael Page.
1998.Anatomy of a large-scale hypertextualWeb search engine.
In Proceedings of the7th Conference on World Wide Web,pages 107?117, Brisbane.Briscoe, E. J. and J. Carroll.
2002.
Robustaccurate statistical annotation of generaltext.
In Proceedings of the 3rd InternationalConference on Language Resources andEvaluation (LREC?2002), pages 1499?1504,Las Palmas.Carlson, Lynn, John M. Conroy, DanielMarcu, Dianne P. O?Leary, Mary E.Okurowski, and Anthony Taylor.
2001.
Anempirical study on the relation betweenabstracts, extracts, and the discoursestructure of texts.
In Proceedings of theDUC-2001 Workshop on Text Summarization,New Orleans, LA.Chang, Ming-Wei, Lev Ratinov, and DanRoth.
2007.
Guiding semi-supervision withconstraint-driven learning.
In Proceedingsof the 22nd International Conference onComputational Linguistics and 44th AnnualMeeting of the Association for ComputationalLinguistics, pages 280?287, Prague.Charniak, Eugene.
2000.
A maximum-entropy-inspired parser.
In Proceedings ofthe 1st North American Annual Meeting of theAssociation for Computational Linguistics,pages 132?139, Seattle, WA.Clarke, James.
2008.
Global Inference forSentence Compression: An Integer LinearProgramming Approach.
Ph.D. thesis,University of Edinburgh.Clarke, James and Mirella Lapata.
2006.Models for sentence compression: Acomparison across domains, trainingrequirements and evaluation measures.In Proceedings of the 21st International438Clarke and Lapata Discourse Constraints for Document CompressionConference on Computational Linguistics and44th Annual Meeting of the Association forComputational Linguistics, pages 377?384,Sydney.Clarke, James and Mirella Lapata.
2008.Global inference for sentence compression:An integer linear programming approach.Journal of Artificial Intelligence Research,31:399?429.Cohn, Trevor and Mirella Lapata.
2009.Sentence compression as tree transduction.Journal of Artificial Intelligence Research,34:637?674.Corston-Oliver, Simon.
2001.
Textcompaction for display on very smallscreens.
In Proceedings of the NAACLWorkshop on Automatic Summarization,pages 89?98, Pittsburgh, PA.Corston-Oliver, Simon H. 1998.
Computingrepresentations of the structure of writtendiscourse.
Technical Report MSR-TR-98-15,Microsoft Research, Redmond, WA.Crammer, Koby and Yoram Singer.
2003.Ultraconservative online algorithms formulticlass problems.
Journal of MachineLearning Research, 3:951?991.Daume?
III, Hal and Daniel Marcu.
2002.A noisy-channel model for documentcompression.
In Proceedings of the 40thAnnual Meeting of the Association forComputational Linguistics, pages 449?456,Philadelphia, PA.Denis, Pascal and Jason Baldridge.
2007.Joint determination of anaphoricity andcoreference resolution using integerprogramming.
In Proceedings of HumanLanguage Technologies 2007: The Conferenceof the North American Chapter of theAssociation for Computational Linguistics,pages 236?243, Rochester, NY.Dras, Mark.
1997.
Reluctant paraphrase:Textual restructuring under anoptimisation model.
In Proceedings of theFifth Biannual Meeting of the PacificAssociation for Computational Linguistics,pages 98?104, Ohme.Endres-Niggemeyer, Brigitte.
1998.Summarising Information.
Springer, Berlin.Fellbaum, Christiane, editor.
1998.
WordNet:An Electronic Database.
MIT Press,Cambridge, MA.Galley, Michel and Kathleen McKeown.2003.
Improving word sense disambiguationin lexical chaining.
In Proceedings of 18thInternational Joint Conference on ArtificialIntelligence (IJCAI?03), pages 1486?1488,Acapulco, Mexico.Galley, Michel and Kathleen McKeown.2007.
Lexicalized Markov grammars forsentence compression.
In Proceedings ofHuman Language Technologies 2007: TheConference of the North American Chapter ofthe Association for Computational Linguistics,pages 180?187, Rochester, NY.Grefenstette, Gregory.
1998.
ProducingIntelligent Telegraphic Text Reduction toProvide an Audio Scanning Service for theBlind.
In Proceedings of the AAAI Symposiumon Intelligent Text Summarization,pages 111?117, Stanford, CA.Grosz, Barbara J., Scott Weinstein, andAravind K. Joshi.
1995.
Centering: aframework for modeling the localcoherence of discourse.
ComputationalLinguistics, 21(2):203?225.Halliday, M. A. K. and Ruqaiya Hasan.1976.
Cohesion in English.
Longman,London.Hirst, Graeme and David St-Onge.
1998.Lexical chains as representations ofcontext for the detection and correctionof malapropisms.
In Christiane Fellbaum,editor, WordNet: An Electronic Database.MIT Press, Cambridge, MA,pages 305?332.Hori, Chiori and Sadaoki Furui.
2004.Speech summarization: An approachthrough word extraction and a methodfor evaluation.
IEICE Transactions onInformation and Systems, E87-D(1):15?25, 1.Jing, Hongyan.
2000.
Sentence reductionfor automatic text summarization.
InProceedings of the 6th conference onApplied Natural Language Processing,pages 310?315, Seattle, WA.Kibble, Rodger and Richard Power.
2004.Optimising referential coherence in textgeneration.
Computational Linguistics,30(4):401?416.Knight, Kevin and Daniel Marcu.
2002.Summarization beyond sentenceextraction: a probabilistic approach tosentence compression.
Artificial Intelligence,139(1):91?107.Kupiec, Julian, Jan O. Pedersen, and FrancineChen.
1995.
A trainable documentsummarizer.
In Proceedings of SIGIR-95,pages 68?73, Seattle, WA.Lin, Chin-Yew.
2003.
Improvingsummarization performance by sentencecompression?A pilot study.
In Proceedingsof the 6th International Workshop onInformation Retrieval with Asian Languages,pages 1?8, Sapporo.Lin, Dekang.
2001.
LaTaT: Language and textanalysis tools.
In Proceedings of the firstHuman Language Technology Conference,pages 222?227, San Francisco, CA.439Computational Linguistics Volume 36, Number 3Mani, Inderjeet.
2001.
AutomaticSummarization.
John Benjamins,Amsterdam.Mani, Inderjeet, The?re`se Firmin, DavidHouse, Gary Klein, Beth Sundheim, andLynette Hirschman.
2002.
The TIPSTERSUMMAC Text SummarizationEvaluation.
Natural Language Engineering,8:43?68.Mani, Inderjeet, Barbara Gates, and EricBloedorn.
1999.
Improving summaries byrevising them.
In Proceedings of the 37thAnnual Meeting of the Association forComputational Linguistics, pages 558?565,College Park, MD.Mann, William C. and Sandra A. Thompson.1988.
Rhetorical structure theory: Toward afunctional theory of text organization.
Text,8(3):243?281.Marciniak, Tomasz and Michael Strube.2005.
Beyond the pipeline: Discreteoptimization in NLP.
In Proceedings ofthe Ninth Conference on ComputationalNatural Language Learning (CoNLL?2005),pages 136?143, Ann Arbor, MI.Marcu, Daniel.
2000.
The Theory and Practiceof Discourse Parsing and Summarization.
TheMIT Press, Cambridge, MA.Martins, Andre?
and Noah A. Smith.
2009.Summarization with a joint model forsentence extraction and compression.
InProceedings of the Workshop on Integer LinearProgramming for Natural LanguageProcessing, pages 1?9, Boulder, CO.Martins, Andre?, Noah Smith, and Eric Xing.2009.
Concise integer linear programmingformulations for dependency parsing.
InProceedings of the Joint Conference of the47th Annual Meeting of the ACL and the4th International Joint Conference onNatural Language Processing of the AFNLP,pages 342?350, Suntec.McDonald, Ryan.
2006.
Discriminativesentence compression with soft syntacticconstraints.
In Proceedings of the11th Conference of the European Chapter of theAssociation for Computational Linguistics,pages 297?304, Trento.Miltsakaki, Eleni and Karen Kukich.
2000.The role of centering theory?s rough-shiftin the teaching and evaluation of writingskills.
In Proceedings of the 38th AnnualMeeting of the Association for ComputationalLinguistics, pages 408?415, Hong Kong.Morris, A., G. Kasper, and D. Adams.
1992.The effects and limitations of automatedtext condensing on readingcomprehension performance.
InformationSystems Research, 3(1):17?35.Morris, Jane and Graeme Hirst.
1991.
Lexicalcohesion computed by thesaural relationsas an indicator of the structure of text.Computational Linguistics, 17(1):21?48.Nguyen, Minh Le, Akira Shimazu, SusumuHoriguchi, Tu Bao Ho, and MasaruFukushi.
2004.
Probabilistic sentencereduction using support vector machines.In Proceedings of the 20th InternationalConference on Computational Linguistics,pages 743?749, Geneva.Olivers, S. H. and W. B. Dolan.
1999.
Less ismore; eliminating index terms fromsubordinate clauses.
In Proceedings of the37th Annual Meeting of the Association forComputational Linguistics, pages 349?356,College Park, MD.Ono, Kenji, Kazuo Sumita, and Seiji Miike.1994.
Abstract generation based onrhetorical structure extraction.
InProceedings of the 15th InternationalConference on Computational Linguistics,pages 344?348, Kyoto.Ora?san, Constantin.
2003.
An evolutionaryapproach for improving the quality ofautomatic summaries.
In ACL Workshop onMultilingual Summarization and QuestionAnswering, pages 37?45, Sapporo, Japan.Poesio, Massimo, Rosemary Stevenson,Barbara Di Eugenio, and Janet Hitzeman.2004.
Centering: a parametric theory andits instantiations.
Computational Linguistics,30(3):309?363.Press, William H., Saul A. Teukolsky,William T. Vetterling, and Brian P.Flannery.
1992.
Numerical Recipes in C: TheArt of Scientific Computing.
CambridgeUniversity Press, Cambridge, UK.Punyakanok, Vasin, Dan Roth, Wen-tau Yih,and Dav Zimak.
2004.
Semantic rolelabeling via integer linear programminginference.
In Proceedings of the20th International Conference onComputational Linguistics, pages 1346?1352,Geneva.Riedel, Sebastian and James Clarke.
2006.Incremental integer linear programmingfor non-projective dependency parsing.In Proceedings of the 2006 Conference onEmpirical Methods in Natural LanguageProcessing, pages 129?137, Sydney.Riezler, Stefan, Tracy H. King, RichardCrouch, and Annie Zaenen.
2003.Statistical sentence condensation usingambiguity packing and stochasticdisambiguation methods forlexical-functional grammar.
In Proceedingsof the 2003 Human Language TechnologyConference of the North American Chapter of440Clarke and Lapata Discourse Constraints for Document Compressionthe Association for Computational Linguistics,pages 118?125, Edmonton.Roark, Brian.
2001.
Probabilistic top?downparsing and language modeling.Computational Linguistics, 27(2):249?276.Roth, Dan and Wen-tau Yih.
2004.
A linearprogramming formulation for globalinference in natural language tasks.In Proceedings of the 8th Conference onComputational Natural Language Learning,pages 1?8, Boston, MA.Scott, Donia and Clarisse Sieckeniusde Souza.
1990.
Getting the message acrossin RST-based text generation.
In RobertDale, Chris Mellish, and Michael Zock,editors, Current Research in NaturalLanguage Generation.
Academic Press,New York, pages 47?73.Sjorochod?ko, E. F. 1972.
Adaptive methodfor automatic abstracting and indexing.
InInformation Processing 71: Proceedings of theIFIP Congress 71, pages 1179?1182,Amsterdam.Tetreault, Joel R. 2001.
A corpus-basedevaluation of centering and pronounresolution.
Computational Linguistics,27(4):507?520.Teufel, Simone and Marc Moens.
2002.Summarizing scientific articles?Experiments with relevance and rhetoricalstatus.
Computational Linguistics,28(4):409?446.Turner, Jenine and Eugene Charniak.2005.
Supervised and unsupervisedlearning for sentence compression.
InProceedings of the 43rd Annual Meetingof the Association for ComputationalLinguistics, pages 290?297, AnnArbor, MI.Vanderbei, Robert J.
2001.
LinearProgramming: Foundations and Extensions.Kluwer Academic Publishers, Boston,2nd edition.Walker, Marilyn, Aravind Joshi, andEllen Prince.
1998.
Centering innaturally occurring discourse: Anoverview.
In Centering Theory inDiscourse.
Oxford University Press,Oxford, pages 1?28.Winston, Wayne L. and MunirpallamVenkataramanan.
2003.
Introduction toMathematical Programming.
Brooks/Cole,Independence, KY.Wolf, Florian and Edward Gibson.
2004.Paragraph-, word-, and coherence-basedapproaches to sentence ranking: Acomparison of algorithm and humanperformance.
In Proceedings of the42nd Meeting of the Association forComputational Linguistics, pages 383?390,Barcelona.Zajic, David, Bonnie J. Dorr, Jimmy J. Lin,and Richard M. Schwartz.
2007.Multi-candidate reduction: Sentencecompression as a tool for documentsummarization tasks.
InformationProcessing and Management,43(6):1549?1570.441
