Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 42?49,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsCapturing ?attrition intensifying?
structural traits from didacticinteraction sequences of MOOC learnersTanmay Sinha1, Nan Li2, Patrick Jermann3, Pierre Dillenbourg21Language Technologies Institute, Carnegie Mellon University, Pittsburgh PA 15213, USA2Computer-Human Interaction in Learning and Instruction, EPFL, CH 1015, Switzerland3Center for Digital Education, EPFL, CH 1015, Switzerland1tanmays@andrew.cmu.edu,2,3<firstname.lastname>@epfl.chAbstractThis work is an attempt to discover hiddenstructural configurations in learning activ-ity sequences of students in Massive OpenOnline Courses (MOOCs).
Leveragingcombined representations of video click-stream interactions and forum activities,we seek to fundamentally understand traitsthat are predictive of decreasing engage-ment over time.
Grounded in the inter-disciplinary field of network science, wefollow a graph based approach to success-fully extract indicators of active and pas-sive MOOC participation that reflect per-sistence and regularity in the overall in-teraction footprint.
Using these rich edu-cational semantics, we focus on the prob-lem of predicting student attrition, one ofthe major highlights of MOOC literaturein the recent years.
Our results indicate animprovement over a baseline ngram basedapproach in capturing ?attrition intensify-ing?
features from the learning activitiesthat MOOC learners engage in.
Implica-tions for some compelling future researchare discussed.1 IntroductionMassive Open Online Courses (MOOCs) have at-tracted millions of students, and yet, their peda-gogy is often less elaborated than the state of theart in learning sciences.
Scaling up learning activi-ties in MOOCs can be viewed as a sacrifice of ped-agogical support, made acceptable by the benefitsof giving broad access to education for a marginalincrease of costs.
Even with students volunteeringas teaching assistants in MOOCs, it is not possibleto provide at a distance the same support quality ina class of ten thousand as in a class of a hundred,because of the difficulty to collect and analyse datafrom such a high number of learners.
This meansthat MOOC instructors need to rely on rich com-putational methods that capture the formalism ofhow learners progress through the course and whattraits of decreasing engagement with the courseare predictive of attrition over time.
The interpre-tation of the state of the students can then either beperformed by the students themselves, by a humancoach or by an automated agent that can deliverrecommendations to the students.In this work, we model the sequence of learningactivities in the MOOC as a graph with specificproperties.
Describing the participants actions se-quence as a graph may initially sound as a futilecomplexity since most MOOCs are built as a sim-ple linear sequence of activities (watch video, doassignments, read forums).
However, when look-ing at the activity in more detail, some sequencesare richer and justify a more powerful descrip-tive modeling.
The descriptive power of the graphmodel is to capture the underlying structure of thelearning activity.
The hypothesis is that formaliz-ing the workflow of such heterogeneous behaviorin MOOCs, is one solution to be able to a) scale uplearning activities that may initially appear as nonscalable, b) help instructors reason out how educa-tional scenarios concretely unfold with time, suchas what happened during the course (at what timeswere learners active and performing well, lost, dis-oriented or trapped) and what needs to be repaired.2 Related WorkIn this section we outline perspectives on stu-dent attrition that have been explored so far in theliterature on MOOCs.
Much of this work suc-cessfully leverages effective feature engineeringand advanced statistical methods.
However, thebiggest limitation of most of these emerging worksis that they focus solely on discussion forum be-havior or video lecture activity, but do not fuseand take them into account.
Some of these works42have grown out of research on predicting academicprogress of students and identifying students thosewho are at dropout risk (Kotsiantis et al., 2003;Dekker et al., 2009; Pal, 2012; M?arquez-Vera etal., 2013; Manhaes et al., 2014).Some prior research has focused on deriving so-cial positioning metrics within discussion forumsto understand influencing factors that lead to dif-ferently motivated behaviors of students.
For ex-ample, (Yang et al., 2013; Ros?e et al., 2014) usedaggregate post-reply discussion forum graph perweek, with an aim to investigate posting behaviorand collaborative aspects of participation throughoperationalizations of social positioning.
How-ever, we work at a much finer granularity in thecurrent study and our focus is on individual stu-dent modeling instead.
We capture not only fo-rum participation trajectory, but also video lectureviewing activity of every student in their partici-pation week.
Modeling the combined interactionfootprint as an activity network, allows us to de-cipher the type of engagement and organization ofbehavior for each student, which are reflective ofattrition.Similarly (Ramesh et al., 2014; Wen et al.,2014a; Wen et al., 2014b) published results thatdescribe longitudinal discussion forum behavioraffecting student dropout, in terms of posting,viewing, voting activity, level of subjectivity (cog-nitive engagement) and positivity (sentiment) instudents?
posts.
Related to this, one recent work of(Rossi and Gnawali, 2014) have made an attemptto overcome the language dependency drawbackof these works and capture language indepen-dent discussion forum features related to structure,popularity, temporal dynamics of threads and di-versity of students.It is important to note, however, that all this sub-stantial research caters to only about 5% of stu-dents who participate in MOOC discussion forums(Huang et al., 2014).
Our recent work has laid apreliminary foundation for research investigatingstudents?
information processing behavior whileinteracting with MOOC video lectures (Sinha etal., 2014).
We apply a cognitive video watchingmodel to explain the dynamic process of cogni-tion involved in MOOC video clickstream interac-tion and develop a simple, yet potent informationprocessing index that can be effectively used as anoperationalization for making predictions regard-ing critical learner behavior, specifically in-videoand course dropouts.
In an attempt to better under-stand what features are predictive of students ceas-ing to actively participate in the MOOC, (Veera-machaneni et al., 2014) have integrated a crowdsourcing approach for effective feature engineer-ing at scale.
Among posting, assignment and grad-ing metrics, students?
cohort membership depend-ing on their MOOC engagement was identified asan influential feature for dropout prediction.3 Study ContextThe current study is a part of the shared taskfor EMNLP 2014 Workshop on Modeling LargeScale Social Interaction in Massively Open On-line Courses (Ros?e and Siemens, 2014).
We haveboth video clickstream data (JSON) and discus-sion forum activity data (SQL) from one Cours-era MOOC as training data, that we use in thiswork.
Our predictive models will also be testedon 5 other Coursera MOOCs.In general, Coursera forums, divided into var-ious subforums, have a thread starter post thatserves as a prompt for discussion.
The threadbuilds up as people start following up discus-sions by their posts and comments.
As far as ourforum dataset is concerned, we have 31532 in-stances of forum viewing and 35306 instances ofthread viewing.
In addition to this view data, wehave 4840 posts and 2652 comments among 1393threads initiated in the discussion forums duringthe span of the course, which received 5060 up-votes and 1763 downvotes in total.To supplement the forum data, we additionallyleverage rich video interaction data from the click-stream data.
The clickstream data contains manyerrors.
We obtained 82 unique video ids from theclickstream data, but only 45 of them are valid(watched by large number of unique students).The 37 invalid video ids may be simply due to log-ging errors.
They are also likely to be videos thatwere uploaded by the course staff for testing pur-poses.
There are in total 27739 students registeredthe course, however, only 14312 students had on-line video interactions.
The rest of the studentsmay have never logged in, or only have viewed thecourse pages, or have downloaded the videos with-out further online engagement.
Among the 14312students who have video interactions, 14264 ofthem have valid video events logged, which leadto 181100 valid video sessions for our analy-sis.
These valid video sessions further contain43462341 play events, 295103 pause events, 87585forward jumps, 98169 backward jumps, 6707 for-ward scrolls, 5311 backward scrolls, 18051 video-play rate increase and 16163 decrease events, re-spectively.Our dropout prediction approach that will be de-scribed in the next section is applied to studentinteractions comprising of only online forum andvideo viewing activities.
Currently, we do notmake use of the pageview click data.4 Technical Approach1.
To capture the behaviors exhibited in two pri-mary MOOC activities, namely video lec-ture viewing and forum interaction, we op-erationalize the following metrics:?
Video lecture clickstream activi-ties: Play (PL), Pause (PA), SeekFw(FW), SeekBw (BW), ScrollFw (FS),ScrollBw (BS), Ratechange Increase(RCI), Ratechange Decrease (RCD).When two seek events happen in < 1second, we group them into a scroll.We encode ratechange event based onwhether students sped up or sloweddown with respect to playrate of the lastclick event.?
Discussion forum activities: Post (Po),Comment (Co), Thread (Th), Upvote(Uv), Downvote (Dv), Viewforum (Vf),Viewthread (Vt)2.
Because timing of all such MOOC events arelogged in our data, we sort all these activitiesby timestamp to obtain the sequence of activ-ities done by students.
This gives us a sim-ple sequentially ordered time series that canbe used to reason about behavioral pattern ofstudents.3.
We form the interaction footprint sequencefor students by concatenating all their differ-ent timestamped MOOC activities for everyweek of MOOC activity.
For example, if astudent watched a video (PL, PA, FW, RCI,PA) at [time ?i?, week ?j?
], viewed a forum attime [?i+1?, week ?j?]
and consequently madea post at [time ?i+2?, week ?j?
], his interactionfootprint sequence for week ?j?
would be: PLPA FW RCI PA Vf Po.
Forming such a se-quence captures in some essence, the cogni-tive mind state that govern students?
interac-tion, as they progress through the MOOC byengaging with these multiple forms of com-puter mediated inputs.
Most MOOCs arebased on a weekly rhythm with a new set ofvideos and new assignments released everyweek.4.
To find subsequences that might help us topredict student dropout before it occurs, weextract the following set of features for eachstudent in each of his participation weeks:?
N-grams from the interaction footprintsequence (n = 2 to 5).
Such ?n?
consec-utively occurring MOOC activities notonly characterize suspicious behaviorsthat might lead to student attrition butalso help us to automatically determinethe elements of what might be consid-ered ?best MOOC interaction practices?that keep students engaged.?
Proportion of video viewing activitiesamong all video interactions, that areactive or passive.
We define passivevideo viewing as mere play and pause(PL, PA), while rest of the video lectureclickstream activities (FW, BW, FS, BS,RCD, RCI) are considered elements ofactive video viewing.?
Proportion of discussion forum activi-ties among all forum interactions, thatare active or passive.
We define passiveforum activities as viewing a forum orthread (Vf, Vt), upvoting and downvot-ing (Uv, Dv).
The forum activities ofstarting a thread (Th), posting (Po) andcommenting (Co) are indicative of ac-tive forum interaction.In general, because passive video lectureviewing is high (for example, 48% of allvideo clickstream activities in our datasetcomprise of activity sequences having onlyPL event), discussion forum conversationnetworks in MOOCs are sparse (only 10%of forum activities relate to explicitly post-ing, commenting or starting a thread) andpassive forum activities are very predominant(90% of forum interactions in our dataset arejust passively viewing a thread/forum, upvot-ing or downvoting), differentiating betweensuch active and passive forms of involvementmight clarify participation profiles that aremost likely to lead to disengagement of stu-44dents from the MOOC.5.
In an attempt to enrich the basic ngram rep-resentation and better infer traits of activeand passive participation, we extract the fol-lowing set of graph metrics from the over-all interaction footprint sequence.
Specifi-cally, in this modeling scheme, we extractconsecutive windows of length two and cre-ate a directed edge of weight one betweenthe activities appearing in sequential order.This results in a directed graph (having selfloops and parallel edges), with nodes repre-senting activities done by a student in particu-lar week, while the weighted edges represent-ing the frequencies of activities appearing af-ter one another.
For example, in a sequence,(Vt Po Vt Po Po), corresponding nodes in thegraph are Vt and Po, while edges are (Vt,Po), (Po, Vt), (Vt, Po) and (Po, Po).
Theactivity graph thus describes the visible partof the educational activities (who does whatand when) and models the structure of activ-ity sequences, rather than the details of eachactivity.
Features from the syntactic structureof the graph along with their educational se-mantics are described below.?
Number of nodes and edges: Indica-tive of whether overall participation ofstudents in different MOOC activities ishigh or low.?
Density: Graph density is a tight-knittedness indicator of how involvedstudents are in different MOOC activ-ities, how clustered their activities areor how frequently they switch back andforth between different activities.
Tech-nically, for a directed network, density =m/n(n?1), where m=number of edges,n=number of nodes.
For our multidi-graph representation, density can be >1,because self loops are counted in thetotal number of edges.
This also im-plies that values of density >1 denotehigh persistence in doing particular setof MOOC activities, because of greaternumber of self loops.?
Number of self loops: Though graphdensity provides meaningful interpreta-tions when > 1, we can?t conclusivelyinfer activity persistence in an activitygraph with low density.
So, we addition-ally extract number of self loops to referto the regularity in interaction behavior.?
Number of Strongly Connected Com-ponents (SCC): SCC define a specialrelationship among a set of graph ver-tices that can be exploited (each vertexcan be reached from every other vertexin the component via a directed path).
Ifthe number of SCC in an activity graphare high, there is a high probability thatstudents performs certain set of activ-ities frequently to successfully achievetheir desired learning outcomes in thecourse.
This might be an influential in-dicator for behavioral organization andcontinuity reflected in overall interac-tion footprint of students.
Dense net-works are more likely to have greaternumber of SCC.?
Central activity: We extract top threeactivities of students with maximum in-degree centrality, for each of their par-ticipation weeks.
Technically, indegreecentrality for a node ?v?
is the fraction ofnodes its incoming edges are connectedto.
Depending on which are the centralactivities of students, we can character-ize how active or passive is the partic-ipation.
For example, Viewthread andViewforum (Vt, Vf) are more passiveforms of participation than Upvote andDownvote (Uv, Dv), which are in turnmore passive than Posting, Comment-ing, Thread starting (Po, Co, Th) andother intense forms of video lecture par-ticipation that represent high grapplingwith the course material.?
Central transition: We extract the edge(activity transition) with maximum be-tweenness centrality, which acts like afacilitator in sustaining or decreasingparticipation.
Technically, betweennesscentrality of an edge ?e?
is the sum ofthe fraction of all-pairs shortest pathsthat pass through ?e?.
We normalize by1/n(n ?
1) for our directed graphicalrepresentation, where ?n?
is the numberof nodes.
For example, Vt-Po (viewthread-post) could be one of the centraledges for Th (thread starting activ-ity), which in turn is a strong student45(a) Active video viewing (b) Passive video viewing (c) Active forum activity (d) Passive forum activityFigure 1: Interaction graphs representing 4 contrasting MOOC scenarios in our datasetparticipation indicator.
Alternately,Po/Co/Th-Dv (post/comment/threadinitiate-downvote) could serve asdecision conduits that increase dis-satisfaction of students because ofothers?
off content/off-conduct post-ing.
Such lack of exposure to usefuland informative posts on forums canpotentially aggravate feelings of ?lackof peer support?
and ?healthy commu-nity involvement?, inturn leading todecreasing engagement.6.
We add certain control variables in our fea-ture set to account for inherently presentstudent characteristics, namely courseweek(number of weeks since the course has beenrunning), userweek (number of weeks sincethe student joined the course) and a nominalvariable indicating whether student activityin a week comprised of only video lectureviewing, only forum activity, both or none.Because we are interested in investigating a)howbehavior within a week affects students?
dropoutin the next course week, b)how cumulative be-havior exhibited up till a week affects students?dropout in the next course week, we create twoexperimental setups: one using data from the cur-rent participation week (Curr) and the second us-ing data from the beginning participation week tillthe current week (TCurr).
For the second setup,all feature engineering is done from the cumula-tive interaction footprint sequence.Some of the interaction graphs culled out fromthe footprint sequence, which are representativeof active and passive MOOC participation are de-picted in figure 1.
Each graph has a begin (Be)and end (En) node, with nodes sized by indegreecentrality and directed edges sized by tie strength.5 Results5.1 Evaluating Our FeaturesAs we would intuitively expect, mean and stan-dard deviations for all our extracted graph metricsare higher in the TCurr setup.
Another evidentpattern is that all these graph metrics follow longtailed distributions for both Curr and TCurr se-tups, with very few students exhibiting high val-ues.
These distributions concur with the 90-9-1rule in online communities which says that 90%of the participants only view content (for example,watch video, Vf, Vt), 9% of the participants editcontent (for example, Uv, Dv), and 1% of the par-ticipants actively create new content (for example,Po, Co, Th).
Moreover, we notice that the top threecentral activities with maximum frequency andcentral edges that describe interactions betweenthem, are passive interaction events.
Among thetop 20, we can observe central edges such as RCI-RCI or PL-FW that hint towards skipping videoand hence decreasing participation, while Th-PL,Po-PL, Th-Po that point towards facilitating par-ticipation.
Thus, in order to graphically visualizeinteractions among features and their relationshipto the class distribution (dropout and non dropout),we utilize mosaic plot representation.
The mo-tivating question being two-fold: a)How do theextracted features vary among dropouts and nondropouts?
b)When viewing more than one featurestogether, what can we say about association of dif-ferent feature combinations to survival of studentsin the MOOC?
After ranking feature projectionson basis of interaction gain (in % of class entropyremoved), we discern the following:?
For both Curr and TCurr setups, the mosaicplots reveal that dropout is higher for studentshaving low number of nodes, edges, SCC andself loops, low activity graph density, low46Model Performance Metric Setup Curr Setup TCurr1.
Baseline Accuracy/Kappa 0.623/0.297 0.647/0.173False Negative Rate 0.095 0.4852.
Graph Accuracy/Kappa 0.692/0.365 # 0.693/0.277 #False Negative Rate 0.157 0.3973.
Baseline + Graph Accuracy/Kappa 0.624/0.298 0.646/0.173False Negative Rate 0.095 0.482Table 1: Performance metrics for machine learning experiments.
Random classifier performance is 0.5.Values marked # are significantly better (p<0.01, pairwise t-test) than other results in same columnproportion of active forum and video viewingactivity.
This reflects that our operationaliza-tions drawn from overall interaction footprintare successfully able to capture features ex-pressing student behavior that might escalateattrition.?
Student dropout is higher if they join inlater course weeks and have a sparse activ-ity graph.
There could be 2 possible expla-nations: a)Students join later and do min-imal activity because they only have spe-cific information needs.
So, they do notstay after interacting with the course mate-rial in a short non linear fashion and satisfy-ing their needs, b)Students who join later areoverwhelmed with lots of introductory andprerequisite MOOC video lectures to watch,pending assignments to be completed to suc-cessfully pass the course and discussion fo-rum content already posted.
Finding diffi-culty in coping up with the ongoing pace ofthe MOOC, they do not stay for prolongedperiods in the course.5.2 Dropout Prediction and AnalysisWe leverage machine learning techniques to pre-dict student attrition along the way based on ourextracted feature set.
The dependent class variableis dropout, which is 0 for all active student partic-ipation weeks and 1 only for the last participationweek (student ceased to participate in the MOOCafter that week), leading to an extremely skewedclass distribution.
Note that by active student par-ticipation, we refer to only forum and video view-ing interactions.
We construct the following twomodels for validation.
For each model, there is aCurr and a TCurr setup:?
Baseline Ngram Model: Features used areCoursweek, Userweek, Ngrams from full in-teraction footprint sequence (2 to 5), Ngramlength, proportion of active/passive videoviewing and forum activity (dichotomized byequal width), nominal variable.?
Graph Model: Features used are Cour-sweek, Userweek, Ngram length, Graph met-rics (top 3 central activities, density (di-chotomized by equal frequency), central tran-sition, no.
of nodes (dichotomized by equalfrequency), no.
of edges (dichotomized byequal frequency), no.
of self loops (di-chotomized by equal frequency), no.
ofSCC), nominal variable.For both these models, we use cost sensitive Lib-SVM with radial basis kernel function (RBF) asthe learning algorithm (Hsu et al., 2003).
The ad-vantage of RBF is that it nonlinearly maps sam-ples into a higher dimensional space so it, unlikethe linear kernel, can handle the case when the re-lation between class labels and attributes is non-linear.
Rare threshold for feature extraction is setto 4, while cross validation is done using a sup-plied test set with held out students having sql id798619 through 1882807.The important take away messages from theseresults are:?
Graph model performs significantly betterthan Baseline ngram model for both Curr(t=-17.903, p<0.01) and TCurr (t=-11.834,p<0.01) setups, in terms of higher accu-racy/kappa and comparable false negativerates1.
This is because the graph modelsthe integration of heterogeneous MOOC ac-tivities into a structured activity.
The edgesof the graph, which connect consecutive ac-tivities represent a two-fold relationship be-tween these activities: how they relate to each1False negative rate of 0.x means that we correctly iden-tify (100-(100*0.x))% of dropouts47other from a pedagogical and from an oper-ational viewpoint.
In addition to capturingjust the order and mere presence of active andpassive MOOC events scatterred throughoutthe activity sequence, the activity networkrepresentation additionally captures differ-ent properties of MOOC interaction such asa)how recurring behaviors develop in the par-ticipation trajectory of students, and how themost central ones thrust towards increasingor decreasing engagement, b)how the num-ber and distribution of such activities are in-dicative of persistence in interaction behav-ior.
The baseline+graph approach does notlead to improvement in results over the base-line approach.?
TCurr setup does not necessarily lead to bet-ter results than Curr setup.
This indicatesthat students?
attrition is more strongly in-fluenced by the most recent week?s exhib-ited behavioral patterns, rather than aggre-gated MOOC interactions from the begin-ning of participation.
The extremely smallfalse negative rates in Curr setup indicate theeffectiveness of our feature engineering ap-proach in predicting attririon behavior, evenwith an extremely skewed class distribution.However, more studies would be required tocorroborate the relation between change ininteraction sequences from one week to an-other and factors such as students?
confusion(?I am unable to follow the course video lec-tures?)
or negative exposure (?I am not moti-vated enough to engage because of less pro-ductive discussion forums?
), which graduallybuild up like negative waves before dropouthappens (Sinha, 2014).6 Conclusion and Future WorkIn this work, we formed operationalizations thatquantify active and passive participation exhibitedby students in video lecture viewing and discus-sion forum behavior.
We were successful in de-veloping meaningful indicators of overall inter-action footprint that suggest systematization andcontinuity in behavior, which are in turn predictiveof student attrition.
In our work going forward,we seek to differentiate the interaction footprintsequences further using potent markov clusteringbased approaches.
The underlying motivation is todecipher sequences having lot of activity overlapas well as similar transition probabilities.
Thesecluster assignments can then serve as features thathelp segregating interaction sequences predictiveof dropout versus non-dropouts.Another interesting enhancement to our workwould include grouping commonly occurring ac-tivities that learners perform in conjunction witheach other and form higher level latent cate-gories indicative of different participation traits.In our computational work, we have recently beendeveloping techniques for operationalizing videolecture clickstreams of students into cognitivelyplausible higher level behaviors to aid instructorsto better understand MOOC hurdles and reasonabout unsatisfactory learning outcomes (Sinha etal., 2014).One limitation of the above work is that weare concerned merely with the timestamped orderof activities done by a student and not the timegap between activities appearing in the interac-tion footprint sequence.
The effect of an activ-ity on a subsequent activity often fades out withtime, i.e.
as the lag between two activities in-creases: learners forget what they learned in a pre-vious activity.
For example, the motivation cre-ated at the beginning of a lesson by presenting aninteresting application example does not last for-ever, so as to initiate productive forum discussions.Similarly, the situation of a thread being started(Th) and a post being made (Po) within 60 secsof completing video lecture viewing, might im-ply a different behavior, than if these forum activ-ities occur five days after video lecture viewing.Therefore, we seek to better understand contextof the most and least central activities of studentsin MOOCs, differentiating between subsequenceslying within and outside user specified temporalwindows.
Our goal is to view the interaction foot-print sequence formation in a sequential data min-ing perspective (Mooney and Roddick, 2013) anddiscover a)most frequently occurring interactionpathways that lead students to such central activ-ities, b)association rules with high statistical con-fidences that help MOOC instructors to trace whystudents engage in certain MOOC activities.
Forexample, a rule of the form AB?
C, such as ?Vf?,?Uv?
[15s] ?
?Po?
[30s] (confidence = 0.7), isread as if a student navigated and viewed a forumpage followed by doing an upvote within 15 sec-onds, then within the next 30 seconds he wouldmake a post 70% of the time.48ReferencesDekker, G. W., Pechenizkiy, M., & Vleeshouwers,J.
M. (2009).
?Predicting Students Drop Out: ACase Study?.
International Working Group on Ed-ucational Data Mining.Huang, J., Dasgupta, A., Ghosh, A., Manning, J., andSanders, M. 2014.
?Superposter behavior in MOOCforums?.
ACM Learing at Scale(L@S)Hsu, C. W., Chang, C. C., & Lin, C. J.
(2003).
?A prac-tical guide to support vector classification?Kotsiantis, S. B., Pierrakeas, C. J., & Pintelas, P. E.(2003, January).
?Preventing student dropout in dis-tance learning using machine learning techniques?.In Knowledge-Based Intelligent Information andEngineering Systems (pp.
267-274).
Springer BerlinHeidelberg.Manhaes, L. M. B., da Cruz, S. M. S., & Zimbrao, G.(2014, March).
?WAVE: an architecture for predict-ing dropout in undergraduate courses using EDM?.In Proceedings of the 29th Annual ACM Symposiumon Applied Computing (pp.
243-247).
ACM.M?arquez-Vera, C., Cano, A., Romero, C., & Ventura,S.
(2013).
?Predicting student failure at school us-ing genetic programming and different data miningapproaches with high dimensional and imbalanceddata?.
Applied intelligence, 38(3), 315-330.Mooney, C. H., & Roddick, J. F. (2013).
?Sequentialpattern mining?approaches and algorithms?.
ACMComputing Surveys (CSUR), 45(2), 19.Pal, S. (2012).
?Mining educational data to reducedropout rates of engineering students?.
InternationalJournal of Information Engineering and ElectronicBusiness (IJIEEB), 4(2), 1.Ramesh, A., Goldwasser, D., Huang, B., Daume III,H., & Getoor, L. (2014, June).
?Learning latent en-gagement patterns of students in online courses?.
InTwenty-Eighth AAAI Conference on Artificial Intel-ligence.Ros?e, C. P., Carlson, R., Yang, D., Wen, M., Resnick,L., Goldman, P., & Sherer, J.
(2014, March).
?Socialfactors that contribute to attrition in moocs.
In Pro-ceedings of the first ACM conference on Learning@scale conference (pp.
197-198).
ACM.Ros?e, C. P., Siemens, G. (2014).
?Shared Task on Pre-diction of Dropout Over Time in Massively OpenOnline Courses?, Proceedings of the 2014 EmpiricalMethods in Natural Language Processing Workshopon Modeling Large Scale Social Interaction in Mas-sively Open Online Courses, Qatar, October, 2014.Rossi, L. A., & Gnawali, O.
?Language IndependentAnalysis and Classification of Discussion Threadsin Coursera MOOC Forums?.Sinha, T., Jermann, P., Li, N., Dillenbourg, P.
(2014).
?Your click decides your fate: Inferring Informa-tion Processing and Attrition Behavior from MOOCVideo Clickstream Interactions?.
Proceedings of the2014 Empirical Methods in Natural Language Pro-cessing Workshop on Modeling Large Scale So-cial Interaction in Massively Open Online Courses,Qatar, October, 2014.Sinha, T. (2014).
?Who negatively influences me?
For-malizing diffusion dynamics of negative exposureleading to student attrition in MOOCs?.
LTI StudentResearch Symposium, Carnegie Mellon UniversityVeeramachaneni, K., O?Reilly, U. M., & Taylor, C.(2014).
?Towards Feature Engineering at Scale forData from Massive Open Online Courses?.
arXivpreprint arXiv:1407.5238.Wen, M., Yang, D., & Ros?e, C. P. (2014a).
?Linguis-tic Reflections of Student Engagement in MassiveOpen Online Courses?.
In Proceedings of the Inter-national Conference on Weblogs and Social MediaWen, M., Yang, D., & Ros?e, C. P. (2014b).
?SentimentAnalysis in MOOC Discussion Forums: What doesit tell us??.
In Proceedings of Educational Data Min-ingYang, D., Sinha T., Adamson D., and Rose, C. P.
2013.?Turn on, Tune in, Drop out: Anticipating studentdropouts in Massive Open Online Courses?
In NIPSWorkshop on Data Driven Education49
