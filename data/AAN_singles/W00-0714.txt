In: Proceedings of CoNLL-2000 and LLL-2000, pages 79-82, Lisbon, Portugal, 2000.Using Perfect Sampling in Parameter Estimation of a WholeSentence Maximum Entropy Language Model*F. Amaya t and J .
M.  BenedfDepartamento de Sistemas Inform?ticos y Computac idnUniversidad Polit6cnica de ValenciaCamino de vera s/n, 46022-Valencia (Spain){famaya, jbened i}@ds ic .upv .esAbst rac tThe Maximum Entropy principle (ME) is an ap-propriate framework for combining informationof a diverse nature from several sources into thesame language model.
In order to incorporatelong-distance information into the ME frame-work in a language model, a Whole SentenceMaximum Entropy Language Model (WSME)could be used.
Until now MonteCarlo MarkovChains (MCMC) sampling techniques has beenused to estimate the paramenters of the WSMEmodel.
In this paper, we propose the applica-tion of another sampling technique: the PerfectSampling (PS).
The experiment has shown a re-duction of 30% in the perplexity of the WSMEmodel over the trigram model and a reduc-tion of 2% over the WSME model trained withMCMC.1 Int roduct ionThe language modeling problem may be definedas the problem of calculating the probability ofa string, p(w) = p(wl,.
.
.
,  Wn).
The probabilityp(w) is usually calculated via conditional prob-abilities.
The n-gram model is one of the mostwidely used language models.
The power of then-gram model resides in its simple formulationand the ease of training.
On the other hand, n-grams only take into account local information,and important long-distance information con-tained in the string wl ... wn cannot be modeledby it.
In an attempt o supplement the local in-formation with long-distance information, hy-brid models have been proposed such us (Belle-* This work has been partially supported by the SpanishCYCIT under contract (TIC98/0423-C06).t Granted by Universidad el Cauca, Popay~n (Colom-bia)garda, 1998; Chelba, 1998; Benedl and Sanchez,2000).The Maximum Entropy principle is an ap-propriate framework for combining informationof a diverse nature from several sources intothe same model: the Maximum Entropy model(ME) (Rosenfeld, 1996).
The information is in-corporated as features which are submitted toconstraints.
The conditional form of the MEmodel is:1 (1)p(ulx) = z(x)where Ai are the parameters to be learned (onefor each feature), the fi are usually characteris-tic functions which are associated to the fea-tures and Z(x) = ~y exp{~i~l Aifi(x,y)} isthe normalization constant.
The main advan-tages of ME are its flexibility (local and globalinformation can be included in the model) andits simplicity.
The drawbacks are that the para-menter's estimation is computationally expen-sive, specially the evaluation of the normaliza-tion constant Z(x) andthat  the grammaticalinformation contained in the sentence is poorlyencoded in the conditional framework.
This isdue to the assumption of independence in theconditional events: in the events in the statespace, only a part of the information containedin the sentence influences de calculation of theprobability (Ristad, 1998).2 Who le  Sentence  Max imumEnt ropy  Language Mode lAn alternative to combining local, long-distanceand structural information contained in thesentence, within the maximum entropy frame-work, is the Whole Sentence Maximum En-tropy model (WSME) (Rosenfeld, 1997).
The79WSME is based in the calculation of unre-stricted ME probability p(w) of a whole sen-tence w = wl .
.
.
Wn.
The probability distribu-tion is the distribution p that has the maximumentropy relative to a prior distribution P0 (inother words: the distribution that minimize dedivergence D(pllpo)) (Della Pietra et al, 1995).The distribution p is given by:m .
.p(w) = 5po(w)eE~=l ~,:~(w) (2)where Ai and f~ are the same as in (1).
Z isa (global) normalization constant and P0 is aprior proposal distribution.
The Ai and Z areunknown and must be learned.The parameters Ai may be interpreted as be-ing weights of the features and could be learnedusing some type of iterative algorithm.
We haveused the Improved Iterative Scaling algorithm(IIS) (Berger et al, 1996).
In each iteration ofthe IIS, we find a 5i value such that adding thisvalue to Ai parameters, we obtain an increasein the the log-likelihood.
The 5i values are ob-tained as the solution of the m equations:1- Z = 0w wEN(3)where /  = 1,. .
.
,m, f#(w) = ~=l f i (w)  andf~ is a training corpus.
Because the domain ofWSME is not restricted to a part of the sen-tence (context) as in the conditional case, itallows us to combine global structural syntac-tic information which is contained in the sen-tence with local and other kinds of long rangeinformation such us triggers.
Furthermore, theWSME model is easier to train than the con-ditional one, because in the WSME model wedon't need to estimate the normalization con-stant Z during the training time.
In contrast,for each event (x, y) in the training corpus, wehave to calculate Z(x) in each iteration of theMEC model.The main drawbacks of the WSME model areits integration with other modules and the cal-culation of the expected value in the left part ofequation (3), because the event space is huge.Here we focus on the problem of calculatingthe expected value in (3).
The first sum in (3)is the expected value of fie ~::#, and it is obvi-ously not possible to sum over all the sentences.However, we can estimate the mean by usingthe empirical expected value:\[ fie~if# \] 1 M Z f/(sJ) (4) Ep k Jj= lwhere sl , .
?
?, SM is a random sample from p(w).Once the parameters have been learned it is pos-sible to estimate the value of the normalizationconstant, because Z = ~w e~l  ~f~(W)p0(w ) =F m |e~i=l if~|, and it can be estimated 1 byL .
Imeans of the sample mean with respect o P0(Chen and Rosenfeld, 1999).In each iteration of IIS, the calculation of (4)requires sampling from a probability distribu-tion which is partially known (Z is unknown),so the classical sampling techniques are not use-ful.
In the literature, there are some meth-ods like the MonteCarlo Markov Chain meth-ods (MCMC) that generate random samplesfrom p(w) (Sahu, 1997; Tierney, 1994).
Withthe MCMC methods, we can simulate a sampleapproximately from the probability distributionand then use the sample to estimate the desiredexpected value in (4).3 Per fec t  Sampl ingIn this paper, we propose the application of an-other sampling technique in the parameter esti-mation process of the WSME model which wasintroduced by Propp and Wilson (Propp andWilson, 1996): the Perfect Sampling (PS).
ThePS method produces samples from the exactlimit distribution and, thus, the sampling meangiven in (4) is less biased than the one obtainedwith the MCMC methods.
Therefore, we canobtain better estimations of the parameters Ai.In PS, we obtain a sample from the limitdistribution of an ergodic Markov Chain X ={Xn; n _> 0}, taking values in the state space S(in the WSME case, the state space is the set ofpossible sentences).
Because of the ergodicity,if the transition law of X is P(x, A) := P(Xn EAIXn_i = x), then it has a limit distribution ~-,that is: if we start a path on the chain in anystate at time n = 0, then as n ~ ~,  Xn ~ ~'.The first algorithm of the family of PS was pre-sented by Propp and Wilson (Propp and Wil-son, 1996) under the name Coupling From thePast (CFP) and is as follows: start a path in80every state of S at some time ( -T)  in the pastsuch that at time n = 0, all the paths collapseto a unique value (due to the ergodicity).
Thisvalue is a sample element.
In the majority ofcases, the state space is huge, so attemptingto begin a path in every state is not practical.Thus, we can define a partial stochastic orderin the state space and so we only need start twopaths: one in the minimum and one in the maxi-mum.
The two paths collapse at time n = 0 andthe value of the coalescence state is a sampleelement of ~-.
The CFP algorithm first deter-mines the time T to start and then runs the twopaths from time ( -T )  to 0.
Information aboutPS methods may be consulted in (Corcoran andTweedie, 1998; Propp and Wilson, 1998).4 Exper imenta l  workIn this work, we have made preliminary exper-iments using PS in the estimation of the ex-pected value (4) during the learning of the pa-rameters of a WSME model.
We have imple-mented the Cai algorithm (Cai, 1999) to obtainperfect samples.
The Cai algorithm has the ad-vantage that it doesn't need the definition of thepartial order.The experiments were carried out using apseudonatural corpus: "the traveler task "1.The traveler task consists in dialogs betweentravelers and hotel clerks.
The size of the vocab-ulary is 693 words.
The training set has 490,000sentences and 4,748,690 words.
The test set has10,000 sentences and 97,153 words.Three kinds of features were used in theWSME model: n-grams (1-grams, 2-grams, 3-grams), distance 2 n-grams (d2-2-grams, d2-3-grams) and triggers.
The proposal prior distri-bution used was a trigram model.We trained WSME models with different setsof features using the two sampling techniques:MCMC and PS.
We measured the perplexity(PP) of each of the models and obtained thepercentage of improvement in the PP with re-spect o a trigram base-line model (see table 1).The first model used MCMC techniques ( pecif-ically the Independence Metropolis-Hastings al-gorithm (IMH) 2) and features of n-grams anddistance 2 n-grams.
The second model used a1EuTrans ESPRIT-LTR Project 202682IMH has been reported recently as the most usefulMCMC algorithm used in the WSME training process.Method PP % ImprovementIMH 3.37115 28PS 3.46336 26IMH-T 3.37198 28PS-T 3.26964 30Trigram 4.66975Table h Test set perplexity of the WSMEmodel over the traveler task corpus: IMH withfeatures of n-grams and d-n-grams (IMH), PSwith n-grams and d-n-grams (PS) IMH withtriggers (IMH-T), PS with triggers (PS-T).
Thebase-line model is a trigram model (Trigram)PS algorithm and features of n-grams and dis-tance 2 n-grams.
The third model used the IMHalgorithm and features of triggers.
The fourthused PS and features of triggers.
Finally, in or-der to compare with the classical methods, weincluded the trigram base-line model.In all cases, the WSME had a better perfor-mance than the n-gram model.
From the resultsin Table 1, we see that the use of features oftriggers improves the performance of the modelmore than the use of n-gram features, this maybe due to the correlation between the triggersand the n-grams, the n-gram information hasbeen absorbed by the prior distribution and di-minishes the effects of the feature of n-grams.We believe this is the reason why PS-T in Ta-ble 1 is better than PS.
We also see how IMHand IHM-T shows the same improvement, i.e.the use of triggers does not seem improve theperplexity of the model but, this may be dueto the sampling technique: the parameter val-ues depends on the estimation of an expectedvalue, and the estimation depends on the sam-pling.
Finally, the PS-T has better perplexitythan the IMH-T.
The only difference betweenboth of these is the sampling technique,neitherof then has the correlation influence in the fea-tures, so we think that the improvement maybe due to the sampling technique.5 Conc lus ion  and  fu ture  worksWe have presented a different approach to thesampling step needed in the parameter estima-tion of a WSME model.
Using this technique,we have obtained a reduction of 30% in the per-plexity of the WSME model over the base-line81trigram model and an improvement of 2% overthe model trained with MCMC techniques.
Weare extending our experiments to a major cor-pus: the Wall Street Journal corpus and using aset of features which is more general, includingfeatures that reflect the global structure of thesentence.We are working on introducing the grammat-ical information contained into the sentence tothe model; we believe that such information im-proves the quality of the model significantly.Re ferencesJ.
R. Bellegarda.
1998.
A multispan languagemodeling framework for large vocabulary speechrecognition.
IEEE Transactions on Speech andAudio Processing, 6 (5):456-467.J.M.
Bened~ and J.A.
Sanchez.
2000.
Combinationof n-grams and stochastic ontext-free grammarsfor language modeling.
International conferenceon computational linguistics (COLIN-A CL).A.L.
Berger, V.J.
Della Pietra, and S.A. DellaPietra.
1996.
A Maximum Entropy approach tonatural anguage processing.
Computational Lin-guistics, 22(1):39-72.H.
Cai.
1999.
Exact Sampling using auxiliary vari-ables.
Statistical Computing Section, ASA Pro-ceedings.C.
Chelba.
1998.
A structured Language Model.PhD Dissertation Proposal, The Johns HopkinsUniversity.S.
Chen and R. Rosenfeld.
1999.
Efficient samplingand feature selection in whole sentence maximumentropy language models.
Proc.
IEEE Int.
Con-ference on Acoustics, Speech and Signal Process-ing (ICASSP).J.N.
Corcoran and R.L.
Tweedie.
1998.
Perfect sam-pling for Independent Metropolis-Hastings chains.preprint.
Colorado State University.S.
Della Pietra, V. Della Pietra, and J. Lafferty.1995.
Inducing features of random fields.
Tech-nical Report CMU-CS-95-144, Carnegie MellonUniversity.J.
G. Propp and D. B. Wilson.
1996.
Exact samplingwith coupled markov chains and applications tostatistical mechanics.
Random Structures and Al-gorithms, 9:223-252.J.
A. Propp and D. B. Wilson.
1998.
Coupling fromthe Past: User's Guide.
Dimacs series in discreteMathematics and Theoretical Computer Science,pages 181-192.E.
S. Ristad, 1998.
Maximum Entropy ModelingToolkit, Version 1.6 Beta.R.
Rosenfeld.
1996.
A Maximum Entropy approachto adaptive statistical language modeling.
Com-puter Speech and Language, 10:187-228.R.
Rosenfeld.
1997.
A whole sentence Maximum En-tropy language model.
IEEE workshop on SpeechRecognition and Understanding.S.
Sahu.
1997.
Bayesian data analysis.
Technical re-port, School of Mathematics, University of Walles.L.
Tierney.
1994.
Markov chains for exploring pos-terior distributions.
The Annals o/ Statistics,22:1701-1762.82
