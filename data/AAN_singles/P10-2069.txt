Proceedings of the ACL 2010 Conference Short Papers, pages 377?381,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsLearning Better Data Representation using Inference-Driven MetricLearningParamveer S. DhillonCIS Deptt., Univ.
of Penn.Philadelphia, PA, U.S.Adhillon@cis.upenn.eduPartha Pratim Talukdar?Search Labs, Microsoft ResearchMountain View, CA, USApartha@talukdar.netKoby CrammerDeptt.
of Electrical Engg.The Technion, Haifa, Israelkoby@ee.technion.ac.ilAbstractWe initiate a study comparing effective-ness of the transformed spaces learned byrecently proposed supervised, and semi-supervised metric learning algorithmsto those generated by previously pro-posed unsupervised dimensionality reduc-tion methods (e.g., PCA).
Through a va-riety of experiments on different real-world datasets, we find IDML-IT, a semi-supervised metric learning algorithm to bethe most effective.1 IntroductionBecause of the high-dimensional nature of NLPdatasets, estimating a large number of parameters(a parameter for each dimension), often from alimited amount of labeled data, is a challengingtask for statistical learners.
Faced with this chal-lenge, various unsupervised dimensionality reduc-tion methods have been developed over the years,e.g., Principal Components Analysis (PCA).Recently, several supervised metric learning al-gorithms have been proposed (Davis et al, 2007;Weinberger and Saul, 2009).
IDML-IT (Dhillon etal., 2010) is another such method which exploitslabeled as well as unlabeled data during metriclearning.
These methods learn a Mahalanobis dis-tance metric to compute distance between a pairof data instances, which can also be interpreted aslearning a transformation of the input data, as weshall see in Section 2.1.In this paper, we make the following contribu-tions:Even though different supervised and semi-supervised metric learning algorithms haverecently been proposed, effectiveness of thetransformed spaces learned by them in NLP?
Research carried out while at the University of Penn-sylvania, Philadelphia, PA, USA.datasets has not been studied before.
Inthis paper, we address that gap: we com-pare effectiveness of classifiers trained on thetransformed spaces learned by metric learn-ing methods to those generated by previ-ously proposed unsupervised dimensionalityreduction methods.
We find IDML-IT, asemi-supervised metric learning algorithm tobe the most effective.2 Metric Learning2.1 Relationship between Metric Learningand Linear ProjectionWe first establish the well-known equivalence be-tween learning a Mahalanobis distance measureand Euclidean distance in a linearly transformedspace of the data (Weinberger and Saul, 2009).
LetA be a d?d positive definite matrix which param-eterizes the Mahalanobis distance, dA(xi, xj), be-tween instances xi and xj , as shown in Equation1.
Since A is positive definite, we can decomposeit as A = P>P , where P is another matrix of sized?
d.dA(xi, xj) = (xi ?
xj)>A(xi ?
xj) (1)= (Pxi ?
Pxj)>(Pxi ?
Pxj)= dEuclidean(Pxi, Pxj)Hence, computing Mahalanobis distance pa-rameterized by A is equivalent to first projectingthe instances into a new space using an appropriatetransformation matrix P and then computing Eu-clidean distance in the linearly transformed space.In this paper, we are interested in learning a betterrepresentation of the data (i.e., projection matrixP ), and we shall achieve that goal by learning thecorresponding Mahalanobis distance parameterA.We shall now review two recently proposedmetric learning algorithms.3772.2 Information-Theoretic Metric Learning(ITML): SupervisedInformation-Theoretic Metric Learning (ITML)(Davis et al, 2007) assumes the availability ofprior knowledge about inter-instance distances.
Inthis scheme, two instances are considered simi-lar if the Mahalanobis distance between them isupper bounded, i.e., dA(xi, xj) ?
u, where uis a non-trivial upper bound.
Similarly, two in-stances are considered dissimilar if the distancebetween them is larger than certain threshold l,i.e., dA(xi, xj) ?
l. Similar instances are rep-resented by set S, while dissimilar instances arerepresented by set D.In addition to prior knowledge about inter-instance distances, sometimes prior informationabout the matrix A, denoted by A0, itself mayalso be available.
For example, Euclidean dis-tance (i.e., A0 = I) may work well in some do-mains.
In such cases, we would like the learnedmatrixA to be as close as possible to the prior ma-trix A0.
ITML combines these two types of priorinformation, i.e., knowledge about inter-instancedistances, and prior matrix A0, in order to learnthe matrix A by solving the optimization problemshown in (2).minA0Dld(A,A0) (2)s.t.
tr{A(xi ?
xj)(xi ?
xj)>} ?
u,?
(i, j) ?
Str{A(xi ?
xj)(xi ?
xj)>} ?
l,?
(i, j) ?
DwhereDld(A,A0) = tr(AA?10 )?
log det(AA?10 )?n, is the LogDet divergence.To handle situations where exactly solving theproblem in (2) is not possible, slack variables maybe introduced to the ITML objective.
To solve thisoptimization problem, an algorithm involving re-peated Bregman projections is presented in (Daviset al, 2007), which we use for the experiments re-ported in this paper.2.3 Inference-Driven Metric Learning(IDML): Semi-SupervisedNotations: We first define the necessary notations.Let X be the d ?
n matrix of n instances in ad-dimensional space.
Out of the n instances, nlinstances are labeled, while the remaining nu in-stances are unlabeled, with n = nl+nu.
Let S bea n ?
n diagonal matrix with Sii = 1 iff instancexi is labeled.
m is the total number of labels.
Yis the n?m matrix storing training label informa-tion, if any.
Y?
is the n?m matrix of estimated la-bel information, i.e., output of any classifier, withY?il denoting score of label l at node i.
.The ITML metric learning algorithm, which wereviewed in Section 2.2, is supervised in nature,and hence it does not exploit widely available un-labeled data.
In this section, we review Infer-ence Driven Metric Learning (IDML) (Algorithm1) (Dhillon et al, 2010), a recently proposed met-ric learning framework which combines an exist-ing supervised metric learning algorithm (such asITML) along with transductive graph-based la-bel inference to learn a new distance metric fromlabeled as well as unlabeled data combined.
Inself-training styled iterations, IDML alternates be-tween metric learning and label inference; withoutput of label inference used during next roundof metric learning, and so on.IDML starts out with the assumption that ex-isting supervised metric learning algorithms, suchas ITML, can learn a better metric if the numberof available labeled instances is increased.
Sincewe are focusing on the semi-supervised learning(SSL) setting with nl labeled and nu unlabeledinstances, the idea is to automatically label theunlabeled instances using a graph based SSL al-gorithm, and then include instances with low as-signed label entropy (i.e., high confidence labelassignments) in the next round of metric learning.The number of instances added in each iterationdepends on the threshold ?1.
This process is con-tinued until no new instances can be added to theset of labeled instances, which can happen wheneither all the instances are already exhausted, orwhen none of the remaining unlabeled instancescan be assigned labels with high confidence.The IDML framework is presented in Algo-rithm 1.
In Line 3, any supervised metriclearner, such as ITML, may be used as theMETRICLEARNER.
Using the distance metriclearned in Line 3, a new k-NN graph is constructedin Line 4 , whose edge weight matrix is stored inW .
In Line 5 , GRAPHLABELINF optimizes overthe newly constructed graph, the GRF objective(Zhu et al, 2003) shown in (3).minY?
?tr{Y??>LY??
}, s.t.
S?Y?
= S?Y??
(3)where L = D ?W is the (unnormalized) Lapla-1During the experiments in Section 3, we set ?
= 0.05378Algorithm 1: Inference Driven Metric Learn-ing (IDML)Input: instancesX , training labels Y , traininginstance indicator S, label entropy threshold ?,neighborhood size kOutput: Mahalanobis distance parameter A1: Y?
?
Y , S?
?
S2: repeat3: A?
METRICLEARNER(X, S?, Y?
)4: W ?
CONSTRUCTKNNGRAPH(X,A, k)5: Y???
GRAPHLABELINF(W, S?, Y?
)6: U ?
SELECTLOWENTINST(Y?
?, S?, ?
)7: Y?
?
Y?
+ UY?
?8: S?
?
S?
+ U9: until convergence (i.e., Uii = 0, ?i)10: return Acian, and D is a diagonal matrix with Dii =?jWij .
The constraint, S?Y?
= S?Y?
?, in (3)makes sure that labels on training instances are notchanged during inference.
In Line 6, a currentlyunlabeled instance xi (i.e., S?ii = 0) is consid-ered a new labeled training instance, i.e., Uii = 1,for next round of metric learning if the instancehas been assigned labels with high confidence inthe current iteration, i.e., if its label distributionhas low entropy (i.e., ENTROPY(Y?
?i:) ?
?).
Fi-nally in Line 7, training instance label informationis updated.
This iterative process is continued tillno new labeled instance can be added, i.e., whenUii = 0 ?i.
IDML returns the learned matrix Awhich can be used to compute Mahalanobis dis-tance using Equation 1.3 Experiments3.1 SetupDataset Dimension BalancedElectronics 84816 YesBooks 139535 YesKitchen 73539 YesDVDs 155465 YesWebKB 44261 YesTable 1: Description of the datasets used in Sec-tion 3.
All datasets are binary with 1500 total in-stances in each.Description of the datasets used during experi-ments in Section 3 are presented in Table 1.
Thefirst four datasets ?
Electronics, Books, Kitchen,and DVDs ?
are from the sentiment domain andpreviously used in (Blitzer et al, 2007).
WebKBis a text classification dataset derived from (Sub-ramanya and Bilmes, 2008).
For details regard-ing features and data pre-processing, we refer thereader to the origin of these datasets cited above.One extra preprocessing that we did was that weonly considered features which occurred more 20times in the entire dataset to make the problemmore computationally tractable and also since theinfrequently occurring features usually contributenoise.
We use classification error (lower is better)as the evaluation metric.
We experiment with thefollowing ways of estimating transformation ma-trix P :Original2: We set P = I , where I is thed ?
d identity matrix.
Hence, the data is nottransformed in this case.RP: The data is first projected into a lowerdimensional space using the Random Pro-jection (RP) method (Bingham and Mannila,2001).
Dimensionality of the target spacewas set at d?= logn2log 1, as prescribed in(Bingham and Mannila, 2001).
We use theprojection matrix constructed by RP as P .
was set to 0.25 for the experiments in Sec-tion 3, which has the effect of projecting thedata into a much lower dimensional space(84 for the experiments in this section).
Thispresents an interesting evaluation setting aswe already run evaluations in much higher di-mensional space (e.g., Original).PCA: Data instances are first projected intoa lower dimensional space using PrincipalComponents Analysis (PCA) (Jolliffe, 2002).
Following (Weinberger and Saul, 2009), di-mensionality of the projected space was setat 250 for all experiments.
In this case, weused the projection matrix generated by PCAas P .ITML: A is learned by applying ITML (seeSection 2.2) on the Original space (above),and then we decompose A as A = P>P toobtain P .2Note that ?Original?
in the results tables refers to orig-inal space with features occurring more than 20 times.
Wealso ran experiments with original set of features (withoutany thresholding) and the results were worse or comparableto the ones reported in the tables.379Datasets Original RP PCA ITML IDML-IT??
?
??
?
??
?
??
?
??
?Electronics 31.3?
0.9 42.5?
1.0 46.4?
2.0 33.0?
1.0 30.7?0.7Books 37.5?
1.1 45.0?
1.1 34.8?
1.4 35.0?
1.1 32.0?0.9Kitchen 33.7?
1.0 43.0?
1.1 34.0?
1.6 30.9?
0.7 29.0?1.0DVDs 39.0?
1.2 47.7?
1.2 36.2?
1.6 37.0?
0.8 33.9?1.0WebKB 31.4?
0.9 33.0?
1.0 27.9?
1.3 28.9?
1.0 25.5?1.0Table 2: Comparison of SVM % classification errors (lower is better), with 50 labeled instances (Sec.3.2).
nl=50.
and nu = 1450.
All results are averaged over ten trials.
All hyperparameters are tuned on aseparate random split.Datasets Original RP PCA ITML IDML-IT??
?
??
?
??
?
??
?
??
?Electronics 27.0?
0.9 40.0?
1.0 41.2?
1.0 27.5?
0.8 25.3?0.8Books 31.0?
0.7 42.9?
0.6 31.3?
0.7 29.9?
0.5 27.7?0.7Kitchen 26.3?
0.5 41.9?
0.7 27.0?
0.9 26.1?
0.8 24.8?0.9DVDs 34.7?
0.4 46.8?
0.6 32.9?
0.8 34.0?
0.8 31.8?0.9WebKB 25.7?
0.5 31.1?
0.5 24.9?
0.6 25.6?
0.4 23.9?0.4Table 3: Comparison of SVM % classification errors (lower is better), with 100 labeled instances (Sec.3.2).
nl=100.
and nu = 1400.
All results are averaged over ten trials.
All hyperparameters are tuned ona separate random split.IDML-IT: A is learned by applying IDML(Algorithm 1) (see Section 2.3) on the Orig-inal space (above); with ITML used asMETRICLEARNER in IDML (Line 3 in Al-gorithm 1).
In this case, we treat the set oftest instances (without their gold labels) asthe unlabeled data.
In other words, we essen-tially work in the transductive setting (Vap-nik, 2000).
Once again, we decompose A asA = P>P to obtain P .We also experimented with the supervisedlarge-margin metric learning algorithm (LMNN)presented in (Weinberger and Saul, 2009).
Wefound ITML to be more effective in practice thanLMNN, and hence we report results based onITML only.
Each input instance, x, is now pro-jected into the transformed space as Px.
Wenow train different classifiers on this transformedspace.
All results are averaged over ten randomtrials.3.2 Supervised ClassificationWe train a SVM classifier, with an RBF kernel, onthe transformed space generated by the projectionmatrix P .
SVM hyperparameter, C and RBF ker-nel bandwidth, were tuned on a separate develop-ment split.
Experimental results with 50 and 100labeled instances are shown in Table 2, and Ta-ble 3, respectively.
From these results, we observethat IDML-IT consistently achieves the best per-formance across all experimental settings.
We alsonote that in Table 3, performance difference be-tween ITML and IDML-IT in the Electronics andKitchen domains are statistically significant.3.3 Semi-Supervised ClassificationIn this section, we trained the GRF classifier (seeEquation 3), a graph-based semi-supervised learn-ing (SSL) algorithm (Zhu et al, 2003), usingGaussian kernel parameterized by A = P>P toset edge weights.
During graph construction, eachnode was connected to its k nearest neighbors,with k treated as a hyperparameter and tuned ona separate development set.
Experimental resultswith 50 and 100 labeled instances are shown inTable 4, and Table 5, respectively.
As before, weexperimented with nl = 50 and nl = 100.
Onceagain, we observe that IDML-IT is the most effec-tive method, with the GRF classifier trained on thedata representation learned by IDML-IT achievingbest performance in all settings.
Here also, we ob-serve that IDML-IT achieves the best performanceacross all experimental settings.380Datasets Original RP PCA ITML IDML-IT??
?
??
?
??
?
??
?
??
?Electronics 47.9?
1.1 49.0?
1.2 43.2?
0.9 34.9?
0.5 34.0?0.5Books 50.0?
1.0 49.4?
1.0 47.9?
0.7 42.1?
0.7 40.6?0.7Kitchen 49.8?
1.1 49.6?
0.9 48.6?
0.8 31.1?
0.5 30.0?0.5DVDs 50.1?
0.5 49.9?
0.7 49.4?
0.6 42.1?
0.4 41.2?0.5WebKB 33.1?
0.4 33.1?
0.3 33.1?
0.3 30.0?
0.4 28.7?0.5Table 4: Comparison of transductive % classification errors (lower is better) over graphs constructedusing different methods (see Section 3.3), with nl = 50 and nu = 1450.
All results are averaged overten trials.
All hyperparameters are tuned on a separate random split.Datasets Original RP PCA ITML IDML-IT??
?
??
?
??
?
??
?
??
?Electronics 43.5?
0.7 47.2?
0.8 39.1?
0.7 31.3?
0.2 30.8?0.3Books 48.3?
0.5 48.9?
0.3 43.3?
0.4 35.2?
0.5 33.3?0.6Kitchen 45.3?
0.6 48.2?
0.5 41.0?
0.7 30.7?
0.6 29.9?0.3DVDs 48.6?
0.3 49.3?
0.5 45.9?
0.5 42.6?
0.4 41.7?0.3WebKB 33.4?
0.4 33.4?
0.4 33.4?
0.3 30.4?
0.5 28.6?0.7Table 5: Comparison of transductive % classification errors (lower is better) over graphs constructedusing different methods (see Section 3.3), with nl = 100 and nu = 1400.
All results are averaged overten trials.
All hyperparameters are tuned on a separate random split.4 ConclusionIn this paper, we compared the effectivenessof the transformed spaces learned by recentlyproposed supervised, and semi-supervised metriclearning algorithms to those generated by previ-ously proposed unsupervised dimensionality re-duction methods (e.g., PCA).
To the best of ourknowledge, this is the first study of its kind in-volving NLP datasets.
Through a variety of ex-periments on different real-world NLP datasets,we demonstrated that supervised as well as semi-supervised classifiers trained on the space learnedby IDML-IT consistently result in the lowest clas-sification errors.
Encouraged by these early re-sults, we plan to explore further the applicabilityof IDML-IT in other NLP tasks (e.g., entity classi-fication, word sense disambiguation, polarity lexi-con induction, etc.)
where better representation ofthe data is a pre-requisite for effective learning.AcknowledgmentsThanks to Kuzman Ganchev for providing detailedfeedback on a draft of this paper.
This workwas supported in part by NSF IIS-0447972 andDARPA HRO1107-1-0029.ReferencesE.
Bingham and H. Mannila.
2001.
Random projec-tion in dimensionality reduction: applications to im-age and text data.
In ACM SIGKDD.J.
Blitzer, M. Dredze, and F. Pereira.
2007.
Biogra-phies, bollywood, boom-boxes and blenders: Do-main adaptation for sentiment classification.
InACL.J.V.
Davis, B. Kulis, P. Jain, S. Sra, and I.S.
Dhillon.2007.
Information-theoretic metric learning.
InICML.P.
S. Dhillon, P. P. Talukdar, and K. Crammer.
2010.Inference-driven metric learning for graph construc-tion.
Technical Report MS-CIS-10-18, CIS Depart-ment, University of Pennsylvania, May.IT Jolliffe.
2002.
Principal component analysis.Springer verlag.A.
Subramanya and J. Bilmes.
2008.
Soft-SupervisedLearning for Text Classification.
In EMNLP.V.N.
Vapnik.
2000.
The nature of statistical learningtheory.
Springer Verlag.K.Q.
Weinberger and L.K.
Saul.
2009.
Distance metriclearning for large margin nearest neighbor classifica-tion.
The Journal of Machine Learning Research.X.
Zhu, Z. Ghahramani, and J. Lafferty.
2003.
Semi-supervised learning using Gaussian fields and har-monic functions.
In ICML.381
