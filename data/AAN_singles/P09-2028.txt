Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 109?112,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPUsing Generation for Grammar Analysis and Error DetectionMichael Wayne Goodman?University of WashingtonDept.
of LinguisticsBox 354340 Seattle, WA 98195, USAgoodmami@u.washington.eduFrancis BondNICT Language Infrastructure Group3-5 Hikaridai, Seika-cho, So?raku-gun,Kyoto, 619-0289 Japanbond@ieee.orgAbstractWe demonstrate that the bidirectionalityof deep grammars, allowing them to gen-erate as well as parse sentences, can beused to automatically and effectively iden-tify errors in the grammars.
The system istested on two implemented HPSG gram-mars: Jacy for Japanese, and the ERG forEnglish.
Using this system, we were ableto increase generation coverage in Jacy by18% (45% to 63%) with only four weeksof grammar development.1 IntroductionLinguistically motivated analysis of text providesmuch useful information for subsequent process-ing.
However, this is generally at the cost of re-duced coverage, due both to the difficulty of pro-viding analyses for all phenomena, and the com-plexity of implementing these analyses.
In thispaper we present a method of identifying prob-lems in a deep grammar by exploiting the fact thatit can be used for both parsing (interpreting textinto semantics) and generation (realizing seman-tics as text).
Since both parsing and generation usethe same grammar, their performance is closelyrelated: in general improving the performance orcover of one direction will also improve the other.
(Flickinger, 2008)The central idea is that we test the grammar ona full round trip: parsing text to its semantic repre-sentation and then generating from it.
In general,any sentence where we cannot reproduce the orig-inal, or where the generated sentence significantlydiffers from the original, identifies a flaw in thegrammar, and with enough examples we can pin-point the grammar rules causing these problems.We call our system Egad, which stands for Erro-neous Generation Analysis and Detection.
?This research was carried out while visiting NICT.2 BackgroundThis work was inspired by the error mining ap-proach of van Noord (2004), who identified prob-lematic input for a grammar by comparing sen-tences that parsed and those that didn?t from alarge corpus.
Our approach takes this idea and fur-ther applies it to generation.
We were also inspiredby the work of Dickinson and Lee (2008), whose?variation n-gram method?
models the likelihooda particular argument structure (semantic annota-tion) is accurate given the verb and some context.We tested Egad on two grammars: Jacy (Siegel,2000), a Japanese grammar and the English Re-source Grammar (ERG) (Flickinger, 2000, 2008)from the DELPH-IN1 group.
Both grammars arewritten in the Head-driven Phrase Structure Gram-mar (HPSG) (Pollard and Sag, 1994) framework,and use Minimal Recursion Semantics (MRS)(Copestake et al, 2005) for their semantic rep-resentations.
The Tanaka Corpus (Tanaka, 2001)provides us with English and Japanese sentences.The specific motivation for this work was to in-crease the quality and coverage of generated para-phrases using Jacy and the ERG.
Bond et al(2008) showed they could improve the perfor-mance of a statistical machine translation systemby training on a corpus that included paraphrasedvariations of the English text.
We want to do thesame with Japanese text, but Jacy was not able toproduce paraphrases as well (the ERG had 83%generation coverage, while Jacy had 45%) Im-proving generation would also greatly benefit X-to-Japanese machine translation tasks using Jacy.2.1 Concerning Grammar PerformanceThere is a difference between the theoretical andpractical power of the grammars.
Sometimes the1Deep Linguistic Processing with HPSG Initiative ?
seehttp://www.delph-in.net for background informa-tion, including the list of current participants and pointers toavailable resources and documentation109parser or generator can reach the memory (i.e.edge) limit, resulting in a valid result not beingreturned.
Also, we only look at the top-ranked2parse and the first five generations for each item.This is usually not a problem, but it could causeEgad to report false positives.HPSG grammars are theoretically symmetricbetween parsing and generation, but in practicethis is not always true.
For example, to improveperformance, semantically empty lexemes are notinserted into a generation unless a ?trigger-rule?defines a context for them.
These trigger-rulesmay not cover all cases.3 Grammar AnalysisWhen analyzing a grammar, Egad looks at all in-put sentences, parses, and generations processedby the grammar and uses the information thereinto determine characteristics of these items.
Thesecharacteristics are encoded in a vector that can beused for labeling and searching items.
Some char-acteristics are useful for error mining, while othersare used for grammar analysis.3.1 Characteristic TypesEgad determines both general characteristics of anitem (parsability and generability), and character-istics comparing parses with generations.General characteristics show whether each itemcould: be parsed (?parsable?
), generate fromparsed semantics (?generable?
), generate the orig-inal parsed sentence (?reproducible?
), and gener-ate other sentences (?paraphrasable?
).For comparative characteristics, Egad com-pares every generated sentence to the parsed sen-tence whence its semantics originated, and deter-mines if the generated sentence uses the same setof lexemes, derivation tree,3 set of rules, surfaceform, and MRS as the original.3.2 Characteristic PatternsHaving determined all applicable characteristicsfor an item or a generated sentence, we encode thevalues of those characteristics into a vector.
Wecall this vector a characteristic pattern, or CP.An example CP showing general characteristics is:0010 -----2Jacy and the ERG both have parse-ranking models.3In comparing the derivation trees, we only look at phrasalnodes.
Lexemes and surface forms are not compared.The first four digits are read as: the item isparsable, generable, not reproducible, and is para-phrasable.
The five following dashes are for com-parative characteristics and are inapplicable exceptfor generations.3.3 Utility of CharacteristicsNot all characteristics are useful for all tasks.
Wewere interested in improving Jacy?s ability to gen-erate sentences, so we primarily looked at itemsthat were parsable but ungenerable.
In comparinggenerated sentences with the original parsed sen-tence, those with differing semantics often point toerrors, as do those with a different surface form butthe same derivation tree and lexemes (which usu-ally means an inflectional rule was misapplied).4 Problematic Rule DetectionOur method for detecting problematic rules is totrain a maximum entropy-based classifier4 with n-gram paths of rules from a derivation tree as fea-tures and characteristic patterns as labels.
Oncetrained, we do feature-selection to look at whatpaths of rules are most predictive of certain labels.4.1 Rule PathsWe extract n-grams over rule paths, or RPs,which are downward paths along the derivationtree.
(Toutanova et al, 2005) By creating sepa-rate RPs for each branch in the derivation tree, weretain some information about the order of rule ap-plication without overfitting to specific tree struc-tures.
For example, Figure 1 is the derivation treefor (1).
A couple of RPs extracted from the deriva-tion tree are shown in Figure 2.
(1) ?????shashin-utsuri-gapicture-taking-NOM?
?iigood(X is) good at taking pictures.4.2 Building a ModelWe build a classification model by using a parsedor generated sentence?s RPs as features and thatsentence?s CP as a label.
The set of RPs includesn-grams over all specified values of N. The labelsare, to be more accurate, regular expressions of4We would like to look at using different classifiers here,such as Decision Trees.
We initially chose MaxEnt becauseit was easy to implement, and have since had little motivationto change it because it produced useful results.110utterance rule-decl-finitehead subj rulehf-complement-rulequantify-n-lrulecompounds-ruleshashin?
?utsuri 1??ga?unary-vstem-vend-ruleadj-i-lexeme-infl-ruleii-adj?
?Figure 1: Derivation tree for (1)quantify-n-lrule ?
compounds-rule ?
shashinquantify-n-lrule ?
compounds-rule ?
utsuri 1Figure 2: Example RPs extracted from Figure 1CPs and may be fully specified to a unique CP orgeneralize over several.5 The user can weight theRPs by their N value (e.g.
to target unigrams).4.3 Finding Problematic RulesAfter training the model, we have a classifier thatpredicts CPs given a set of RPs.
What we want,however, is the RP most strongly associated witha given CP.
The classifier we use provides an easymethod to get the score a given feature has forsome label.
We iterate over all RPs, get their score,then sort them based on the score.
To help elim-inate redundant results, we exclude any RP thateither subsumes or is subsumed by a previous (i.e.higher ranked) RP.Given a CP, the RP with the highest scoreshould indeed be the one most closely associatedto that CP, but it might not lead to the greatestnumber of items affected.
Fixing the second high-est ranked RP, for example, may improve moreitems than fixing the top ranked one.
To help thegrammar developer decide the priority of prob-lems to fix, we also output the count of items ob-served with the given CP and RP.5 Results and EvaluationWe can look at two sets of results: how wellEgad was able to analyze a grammar and detecterrors, and how well a grammar developer coulduse Egad to fix a problematic grammar.
While thelatter is also influenced by the skill of the gram-mar developer, we are interested in how well Egad5For example, /0010 -----/ is fully specified./00.. -----/ marginalizes two general characteristicspoints to the most significant errors, and how it canhelp reduce development time.5.1 Error MiningTable 1 lists the ten highest ranked RPs associatedwith items that could parse but could not generatein Jacy.
Some RPs appear several times in differ-ent contexts.
We made an effort to decrease theredundancy, but clearly this could be improved.From this list of ten problematic RPs, thereare four unique problems: quantify-n-lrule (nounquantification), no-nspec (noun specification), to-comp-quotarg (?
to quotative particle), and te-adjunct (verb conjugation).
The extra rules listedin each RP show the context in which eachproblem occurs, and this can be informative aswell.
For instance, quantify-n-lrule occurs intwo primary contexts (above compounds-rule andnominal-numcl-rule).
The symptoms of the prob-lem occur in the interation of rules in each context,but the source of the problem is quantify-n-lrule.Further, the problems identified are not alwayslexically marked.
quantify-n-lrule occurs for allbare noun phrases (ie.
without determiners).
Thiskind of error cannot be accurately identified by us-ing just word or POS n-grams, we need to use theactual parse tree.5.2 Error CorrectionEgad greatly facilitated our efforts to find and fixa wide variety of errors in Jacy.
For example, werestructured semantic predicate hierarchies, fixednoun quantification, allowed some semanticallyempty lexemes to generate in certain contexts,added pragmatic information to distinguish be-tween politeness levels in pronouns, allowed im-peratives to generate, allowed more constructionsfor numeral classifiers, and more.Egad also identified some issues with the ERG:both over-generation (an under-constrained inflec-tional rule) and under-generation (sentences withthe construction take {care|charge|.
.
. }
of werenot generating).5.3 Updated Grammar StatisticsAfter fixing the most significant problems in Jacy(outlined in Section 5.2) as reported by Egad,we obtained new statistics about the grammar?scoverage and characteristics.
Table 2 shows theoriginal and updated general statistics for Jacy.We increased generability by 18%, doubled repro-ducibility, and increased paraphrasability by 17%.111Score Count Rule Path N-grams1.42340952569648 109 hf-complement-rule?
quantify-n-lrule?
compounds-rule0.960090299833317 54 hf-complement-rule?
quantify-n-lrule?
nominal-numcl-rule?
head-specifier-rule0.756227560530811 63 head-specifier-rule?
hf-complement-rule?
no-nspec?
??
?0.739668926140179 62 hf-complement-rule?
head-specifier-rule?
hf-complement-rule?
no-nspec0.739090261637851 22 hf-complement-rule?
hf-adj-i-rule?
quantify-n-lrule?
compounds-rule0.694215264789286 36 hf-complement-rule?
hf-complement-rule?
to-comp-quotarg?
??
?0.676244980660372 82 vstem-vend-rule?
te-adjunct?
??
?0.617621482523537 26 hf-complement-rule?
hf-complement-rule?
to-comp-varg?
??
?0.592260546433334 36 hf-adj-i-rule?
hf-complement-rule?
quantify-n-lrule?
nominal-numcl-rule0.564790702894285 62 quantify-n-lrule?
compounds-rule?
vn2n-det-lruleTable 1: Top 10 RPs for ungenerable itemsOriginal ModifiedParsable 82% 83%Generable 45% 63%Reproducible 11% 22%Paraphrasable 44% 61%Table 2: Jacy?s improved general statisticsAs an added bonus, our work focused on improv-ing generation also improved parsability by 1%.Work is now continuing on fixing the remainderof the identified errors.6 Future WorkIn future iterations of Egad, we would like to ex-pand our feature set (e.g.
information from failedparses), and make the system more robust, suchas replacing lexical-ids (specific to a lexeme) withlexical-types, since all lexemes of the same typeshould behave identically.
A more long-term goalwould allow Egad to analyze the internals of thegrammar and point out specific features within thegrammar rules that are causing problems.
Someof the errors detected by Egad have simple fixes,and we believe there is room to explore methodsof automatic error correction.7 ConclusionWe have introduced a system that identifies er-rors in implemented HPSG grammars, and furtherfinds and ranks the possible sources of those prob-lems.
This tool can greatly reduce the amountof time a grammar developer would spend find-ing bugs, and helps them make informed decisionsabout which bugs are best to fix.
In effect, we aresubstituting cheap CPU time for expensive gram-mar developer time.
Using our system, we wereable to improve Jacy?s absolute generation cover-age by 18% (45% to 63%) with only four weeksof grammar development.8 AcknowledgmentsThanks to NICT for their support, Takayuki Kurib-ayashi for providing native judgments, and Mar-cus Dickinson for comments on an early draft.ReferencesFrancis Bond, Eric Nichols, Darren Scott Appling, andMichael Paul.
2008.
Improving statistical machine trans-lation by paraphrasing the training data.
In InternationalWorkshop on Spoken Language Translation, pages 150?157.
Honolulu.Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A.Sag.
2005.
Minimal Recursion Semantics.
An introduc-tion.
Research on Language and Computation, 3(4):281?332.Markus Dickinson and Chong Min Lee.
2008.
Detectingerrors in semantic annotation.
In Proceedings of theSixth International Language Resources and Evaluation(LREC?08).
Marrakech, Morocco.Dan Flickinger.
2000.
On building a more efficient gram-mar by exploiting types.
Natural Language Engineering,6(1):15?28.
(Special Issue on Efficient Processing withHPSG).Dan Flickinger.
2008.
The English resource grammar.
Tech-nical Report 2007-7, LOGON, http://www.emmtee.net/reports/7.pdf.
(Draft of 2008-11-30).Carl Pollard and Ivan A.
Sag.
1994.
Head DrivenPhrase Structure Grammar.
University of Chicago Press,Chicago.Melanie Siegel.
2000.
HPSG analysis of Japanese.
In Wolf-gang Wahlster, editor, Verbmobil: Foundations of Speech-to-Speech Translation, pages 265 ?
280.
Springer, Berlin,Germany.Yasuhito Tanaka.
2001.
Compilation of a multilingual paral-lel corpus.
In Proceedings of PACLING 2001, pages 265?268.
Kyushu.
(http://www.colips.org/afnlp/archives/pacling2001/pdf/tanaka.pdf).Kristina Toutanova, Christopher D. Manning, Dan Flickinger,and Stephan Oepen.
2005.
Stochastic HPSG parse disam-biguation using the redwoods corpus.
Research on Lan-guage and Computation, 3(1):83?105.Gertjan van Noord.
2004.
Error mining for wide-coveragegrammar engineering.
In 42nd Annual Meeting of theAssociation for Computational Linguistics: ACL-2004.Barcelona.112
