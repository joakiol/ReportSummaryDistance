Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
1015?1023, Prague, June 2007. c?2007 Association for Computational LinguisticsImproving Word Sense Disambiguation Using Topic FeaturesJun Fu Cai, Wee Sun LeeDepartment of Computer ScienceNational University of Singapore3 Science Drive 2, Singapore 117543{caijunfu, leews}@comp.nus.edu.sgYee Whye TehGatsby Computational Neuroscience UnitUniversity College London17 Queen Square, London WC1N 3AR, UKywteh@gatsby.ucl.ac.ukAbstractThis paper presents a novel approach for ex-ploiting the global context for the task ofword sense disambiguation (WSD).
This isdone by using topic features constructed us-ing the latent dirichlet alocation (LDA) al-gorithm on unlabeled data.
The features areincorporated into a modified na?
?ve Bayesnetwork alongside other features such aspart-of-speech of neighboring words, singlewords in the surrounding context, local col-locations, and syntactic patterns.
In both theEnglish all-words task and the English lex-ical sample task, the method achieved sig-nificant improvement over the simple na?
?veBayes classifier and higher accuracy than thebest official scores on Senseval-3 for bothtask.1 IntroductionNatural language tends to be ambiguous.
A wordoften has more than one meanings depending on thecontext.
Word sense disambiguation (WSD) is a nat-ural language processing (NLP) task in which thecorrect meaning (sense) of a word in a given contextis to be determined.Supervised corpus-based approach has been themost successful in WSD to date.
In such an ap-proach, a corpus in which ambiguous words havebeen annotated with correct senses is first collected.Knowledge sources, or features, from the context ofthe annotated word are extracted to form the trainingdata.
A learning algorithm, like the support vectormachine (SVM) or na?
?ve Bayes, is then applied onthe training data to learn the model.
Finally, in test-ing, the learnt model is applied on the test data toassign the correct sense to any ambiguous word.The features used in these systems usually in-clude local features, such as part-of-speech (POS)of neighboring words, local collocations , syntac-tic patterns and global features such as single wordsin the surrounding context (bag-of-words) (Lee andNg, 2002).
However, due to the data scarcity prob-lem, these features are usually very sparse in thetraining data.
There are, on average, 11 and 28training cases per sense in Senseval 2 and 3 lexi-cal sample task respectively, and 6.5 training casesper sense in the SemCor corpus.
This problem isespecially prominent for the bag-of-words feature;more than hundreds of bag-of-words are usually ex-tracted for each training instance and each featurecould be drawn from any English word.
A directconsequence is that the global context information,which the bag-of-words feature is supposed to cap-ture, may be poorly represented.Our approach tries to address this problem byclustering features to relieve the scarcity problem,specifically on the bag-of-words feature.
In the pro-cess, we construct topic features, trained using thelatent dirichlet alocation (LDA) algorithm.
We trainthe topic model (Blei et al, 2003) on unlabeled data,clustering the words occurring in the corpus to a pre-defined number of topics.
We then use the resultingtopic model to tag the bag-of-words in the labeledcorpus with topic distributions.
We incorporate thedistributions, called the topic features, using a sim-ple Bayesian network, modified from na?
?ve Bayes1015model, alongside other features and train the modelon the labeled corpus.
The approach gives good per-formance on both the lexical sample and all-wordstasks on Senseval data.The paper makes mainly two contributions.
First,we are able to show that a feature that efficientlycaptures the global context information using LDAalgorithm can significantly improve the WSD ac-curacy.
Second, we are able to obtain this featurefrom unlabeled data, which spares us from any man-ual labeling work.
We also showcase the potentialstrength of Bayesian network in the WSD task, ob-taining performance that rivals state-of-arts meth-ods.2 Related WorkMany WSD systems try to tackle the data scarcityproblem.
Unsupervised learning is introduced pri-marily to deal with the problem, but with limitedsuccess (Snyder and Palmer, 2004).
In another ap-proach, the learning algorithm borrows training in-stances from other senses and effectively increasesthe training data size.
In (Kohomban and Lee,2005), the classifier is trained using grouped sensesfor verbs and nouns according to WordNet top-levelsynsets and thus effectively pooling training casesacross senses within the same synset.
Similarly,(Ando, 2006) exploits data from related tasks, usingall labeled examples irrespective of target words forlearning each sense using the Alternating StructureOptimization (ASO) algorithm (Ando and Zhang,2005a; Ando and Zhang, 2005b).
Parallel texts isproposed in (Resnik and Yarowsky, 1997) as po-tential training data and (Chan and Ng, 2005) hasshown that using automatically gathered paralleltexts for nouns could significantly increase WSD ac-curacy, when tested on Senseval-2 English all-wordstask.Our approach is somewhat similar to that of us-ing generic language features such as POS tags; thewords are tagged with its semantic topic that may betrained from other corpuses.3 Feature ConstructionWe first present the latent dirichlet alocation algo-rithm and its inference procedures, adapted from theoriginal paper (Blei et al, 2003).3.1 Latent Dirichlet AllocationLDA is a probabilistic model for collections of dis-crete data and has been used in document model-ing and text classification.
It can be representedas a three level hierarchical Bayesian model, showngraphically in Figure 1.
Given a corpus consisting ofM documents, LDA models each document using amixture over K topics, which are in turn character-ized as distributions over words.?wz?
?NMFigure 1: Graphical Model for LDAIn the generative process of LDA, for each doc-ument d we first draw the mixing proportion overtopics ?d from a Dirichlet prior with parameters ?.Next, for each of the Nd words wdn in document d, atopic zdn is first drawn from a multinomial distribu-tion with parameters ?d.
Finally wdn is drawn fromthe topic specific distribution over words.
The prob-ability of a word token w taking on value i giventhat topic z = j was chosen is parameterized usinga matrix ?
with ?ij = p(w = i|z = j).
Integratingout ?d?s and zdn?s, the probability p(D|?, ?)
of thecorpus is thus:M?d=1?p(?d|?
)(Nd?n=1?zdnp(zdn|?d)p(wdn|zdn, ?
))d?d3.1.1 InferenceUnfortunately, it is intractable to directly solve theposterior distribution of the hidden variables given adocument, namely p(?, z|w, ?, ?).
However, (Bleiet al, 2003) has shown that by introducing a set ofvariational parameters, ?
and ?, a tight lower boundon the log likelihood of the probability can be foundusing the following optimization procedure:(?
?, ??)
= argmin?,?D(q(?, z|?, ?
)?p(?, z|w, ?, ?
))1016whereq(?, z|?, ?)
= q(?|?)N?n=1q(zn|?n),?
is the Dirichlet parameter for ?
and the multino-mial parameters (?1 ?
?
?
?N ) are the free variationalparameters.
Note here ?
is document specific in-stead of corpus specific like ?.
Graphically, it is rep-resented as Figure 2.
The optimizing values of ?
and?
can be found by minimizing the Kullback-Leibler(KL) divergence between the variational distributionand the true posterior.??
?zNMFigure 2: Graphical Model for Variational Inference3.2 Baseline FeaturesFor both the lexical sample and all-words tasks,we use the following standard baseline features forcomparison.POS Tags For each training or testing word, w,we include POS tags for P words prior to as well asafter w within the same sentence boundary.
We alsoinclude the POS tag of w. If there are fewer thanP words prior or after w in the same sentence, wedenote the corresponding feature as NIL.Local Collocations Collocation Ci,j refers to theordered sequence of tokens (words or punctuations)surrounding w. The starting and ending position ofthe sequence are denoted i and j respectively, wherea negative value refers to the token position prior tow.
We adopt the same 11 collocation features as(Lee and Ng, 2002), namely C?1,?1, C1,1, C?2,?2,C2,2, C?2,?1, C?1,1, C1,2, C?3,?1, C?2,1, C?1,2,and C1,3.Bag-of-Words For each training or testing word,w, we get G words prior to as well as after w, withinthe same document.
These features are position in-sensitive.
The words we extract are converted backto their morphological root forms.Syntactic Relations We adopt the same syntacticrelations as (Lee and Ng, 2002).
For easy reference,we summarize the features into Table 1.POS of w FeaturesNoun Parent headword hPOS of hRelative position of h to wVerb Left nearest child word of w, lRight nearest child word of w, rPOS of lPOS of rPOS of wVoice of wAdjective Parent headword hPOS of hTable 1: Syntactic Relations FeaturesThe exact values of P and G for each task are setaccording to cross validation result.3.3 Topic FeaturesWe first select an unlabeled corpus, such as 20Newsgroups, and extract individual words from it(excluding stopwords).
We choose the number oftopics, K, for the unlabeled corpus and we apply theLDA algorithm to obtain the ?
parameters, where?
represents the probability of a word wi given atopic zj , p(wi|zj) = ?ij .
The model essentiallyclusters words that occurred in the unlabeled cor-pus according to K topics.
The conditional prob-ability p(wi|zj) = ?ij is later used to tag the wordsin the unseen test example with the probability ofeach topic.For some variants of the classifiers that we con-struct, we also use the ?
parameter, which is doc-ument specific.
For these classifiers, we may needto run the inference algorithm on the labeled corpusand possibly on the test documents.
The ?
param-eter provides an approximation to the probability of1017selecting topic i in the document:p(zi|?)
=?i?K ?k.
(1)4 Classifier Construction4.1 Bayesian NetworkWe construct a variant of the na?
?ve Bayes networkas shown in Figure 3.
Here, w refers to the word.s refers to the sense of the word.
In training, s isobserved while in testing, it is not.
The features f1to fn are baseline features mentioned in Section 3.2(including bag-of-words) while z refers to the la-tent topic that we set for clustering unlabeled corpus.The bag-of-words b are extracted from the neigh-bours of w and there are L of them.
Note that L canbe different from G, which is the number of bag-of-words in baseline features.
Both will be determinedby the validation result.?
?
??
??
?baselinefeatureswsfnf1bzLFigure 3: Graphical Model with LDA featureThe log-likelihood of an instance, `(w, s, F, b)where F denotes the set of baseline features, can bewritten as= logp(w) + logp(s|w) +?Flog(p(f |s))+?Llog(?Kp(zk|s)p(bl|zk)).The log p(w) term is constant and thus can be ig-nored.
The first portion is normal na?
?ve Bayes.
Andsecond portion represents the additional LDA plate.We decouple the training process into three separatestages.
We first extract baseline features from thetask training data, and estimate, using normal na?
?veBayes, p(s|w) and p(f |s) for all w, s and f .
Theparameters associated with p(b|z) are estimated us-ing LDA from unlabeled data.
Finally we estimatethe parameters associated with p(z|s).
We experi-mented with three different ways of both doing theestimation as well as using the resulting model andchose one which performed best empirically.4.1.1 Expectation Maximization ApproachFor p(z|s), a reasonable estimation method is touse maximum likelihood estimation.
This can bedone using the expectation maximization (EM) algo-rithm.
In classification, we just choose s?
that maxi-mizes the log-likelihood of the test instance, where:s?
= argmaxs`(w, s, F, b)In this approach, ?
is never used which means theLDA inference procedure is not used on any labeleddata at all.4.1.2 Soft Tagging ApproachClassification in this approach is done using thefull Bayesian network just as in the EM approach.However we do the estimation of p(z|s) differently.Essentially, we perform LDA inference on the train-ing corpus in order to obtain ?
for each document.We then use the ?
and ?
to obtain p(z|b) for eachword usingp(zi|bl, ?)
=p(bl|zi)p(zi|?
)?K p(bl|zk)p(zk|?
),where equation [1] is used for estimation of p(zi|?
).This effectively transforms b to a topical distri-bution which we call a soft tag where each softtag is probability distribution t1, .
.
.
, tK on topics.We then use this topical distribution for estimatingp(z|s).
Let si be the observed sense of instance iand tij1 , .
.
.
, tijK be the soft tag of the j-th bag-of-word feature of instance i.
We estimate p(z|s) asp(zjk|s) =?si=s tijk?si=s?k?
tijk?
(2)This approach requires us to do LDA inference onthe corpus formed by the labeled training data, but1018not the testing data.
This is because we need ?
toget transformed topical distribution in order to learnp(z|s) in the training.
In the testing, we only applythe learnt parameters to the model.4.1.3 Hard Tagging ApproachHard tagging approach no longer assumes that z islatent.
After p(z|b) is obtained using the same pro-cedure in Section 4.1.2, the topic zi with the high-est p(zi|b) among all K topics is picked to representz.
In this way, b is transformed into a single most?prominent?
topic.
This topic label is used in thesame way as baseline features for both training andtesting in a simple na?
?ve Bayes model.This approach requires us to perform the transfor-mation both on the training as well as testing data,since z becomes an observed variable.
LDA infer-ence is done on two corpora, one formed by thetraining data and the other by testing data, in orderto get the respective values of ?.4.2 Support Vector Machine ApproachIn the SVM (Vapnik, 1995) approach, we first form atraining and a testing file using all standard featuresfor each sense following (Lee and Ng, 2002) (oneclassifier per sense).
To incorporate LDA feature,we use the same approach as Section 4.1.2 to trans-form b into soft tags, p(z|b).
As SVM deals withonly observed features, we need to transform b bothin the training data and in the testing data.
Comparedto (Lee and Ng, 2002), the only difference is that foreach training and testing case, we have additionalL ?K LDA features, since there are L bag-of-wordsand each has a topic distribution represented by Kvalues.5 Experimental SetupWe describe here the experimental setup on the En-glish lexical sample task and all-words task.We use MXPOST tagger (Adwait, 1996) for POStagging, Charniak parser (Charniak, 2000) for ex-tracting syntactic relations, SVMlight1 for SVMclassifier and David Blei?s version of LDA2 for LDAtraining and inference.
All default parameters areused unless mentioned otherwise.
For all standard1http://svmlight.joachims.org2http://www.cs.princeton.edu/?blei/lda-c/baseline features, we use Laplace smoothing but forthe soft tag (equation [2]), we use a smoothing pa-rameter value of 2.5.1 Development Process5.1.1 Lexical Sample TaskWe use the Senseval-2 lexical sample task forpreliminary investigation of different algorithms,datasets and other parameters.
As the dataset is usedextensively for this purpose, only the Senseval-3 lex-ical sample task is used for evaluation.Selecting Bayesian Network The best achievableresult, using the three different Bayesian networkapproaches, when validating on Senseval-2 test datais shown in Table 2.
The parameters that are usedare P = 3 and G = 3.EM 68.0Hard Tagging 65.6Soft Tagging 68.9Table 2: Results on Senseval-2 English lexical sam-ple using different Bayesian network approaches.From the results, it appears that both the EM andthe Hard Tagging approaches did not yield as goodresults as the Soft Tagging approach did.
The EMapproach ignores the LDA inference result, ?, whichwe use to get our topic prior.
This information isdocument specific and can be regarded as globalcontext information.
The Hard Tagging approachalso uses less information, as the original topic dis-tribution is now represented only by the topic withthe highest probability of occurring.
Therefore, bothmethods have information loss and are disadvan-taged against the Soft Tagging approach.
We usethe Soft Tagging approach for the Senseval-3 lexicalsample and the all-words tasks.Unlabeled Corpus Selection The unlabeled cor-pus we choose to train LDA include 20 News-groups, Reuters, SemCor, Senseval-2 lexical sam-ple data and Senseval-3 lexical sample data.
Al-though the last three are labeled corpora, we onlyneed the words from these corpora and thus they canbe regarded as unlabeled too.
For Senseval-2 andSenseval-3 data, we define the whole passage foreach training and testing instance as one document.1019The relative effect using different corpus and com-binations of them is shown in Table 3, when validat-ing on Senseval-2 test data using the Soft Taggingapproach.Corpus |w| K L Senseval-220 Newsgroups 1.7M 40 60 67.9Reuters 1.3M 30 60 65.5SemCor 0.3M 30 60 66.9Senseval-2 0.6M 30 40 66.9Senseval-3 0.6M 50 60 67.6All 4.5M 60 40 68.9Table 3: Effect of using different corpus for LDAtraining, |w| represents the corpus size in terms ofthe number of words in the corpusThe 20 Newsgroups corpus yields the best resultif used individually.
It has a relatively larger corpussize at 1.7 million words in total and also a well bal-anced topic distribution among its documents, rang-ing across politics, finance, science, computing, etc.The Reuters corpus, on the other hand, focuses heav-ily on finance related articles and has a rather skewedtopic distribution.
This probably contributed to itsinferior result.
However, we found that the best re-sult comes from combining all the corpora togetherwith K = 60 and L = 40.Results for Optimized Configuration As base-line for the Bayesian network approaches, we usena?
?ve Bayes with all baseline features.
For the base-line SVM approach, we choose P = 3 and includeall the words occurring in the training and testingpassage as bag-of-words feature.The F-measure result we achieve on Senseval-2test data is shown in Table 4.
Our four systemsare listed as the top four entries in the table.
SoftTag refers to the soft tagging Bayesian network ap-proach.
Note that we used the Senseval-2 test datafor optimizing the configuration (as is done in theASO result).
Hence, the result should not be takenas reliable.
Nevertheless, it is worth noting that theimprovement of Bayesian network approach over itsbaseline is very significant (+5.5%).
On the otherhand, SVM with topic features shows limited im-provement over its baseline (+0.8%).Bayes (Soft Tag) 68.9SVM-Topic 66.0SVM baseline 65.2NB baseline 63.4ASO(best configuration)(Ando, 2006) 68.1Classifier Combination(Florian, 2002) 66.5Polynomial KPCA(Wu et al, 2004) 65.8SVM(Lee and Ng, 2002) 65.4Senseval-2 Best System 64.2Table 4: Results (best configuration) compared toprevious best systems on Senseval-2 English lexicalsample task.5.1.2 All-words TaskIn the all-words task, no official training data isprovided with Senseval.
We follow the commonpractice of using the SemCor corpus as our trainingdata.
However, we did not use SVM approach in thistask as there are too few training instances per sensefor SVM to achieve a reasonably good accuracy.As there are more training instances in SemCor,230, 000 in total, we obtain the optimal configura-tion using 10 fold cross validation on the SemCortraining data.
With the optimal configuration, wetest our system on both Senseval-2 and Senseval-3official test data.For baseline features, we set P = 3 and B = 1.
Wechoose a LDA training corpus comprising 20 News-groups and SemCor data, with number of topics K= 40 and number of LDA bag-of-words L = 14.6 ResultsWe now present the results on both English lexicalsample task and all-words task.6.1 Lexical Sample TaskWith the optimal configurations from Senseval-2,we tested the systems on Senseval-3 data.
Table 5shows our F-measure result compared to some of thebest reported systems.
Although SVM with topicfeatures shows limited success with only a 0.6%improvement, the Bayesian network approach hasagain demonstrated a good improvement of 3.8%over its baseline and is better than previous reportedbest systems except ASO(Ando, 2006).1020Bayes (Soft Tag) 73.6SVM-topic 73.0SVM baseline 72.4NB baseline 69.8ASO(Ando, 2006) 74.1SVM-LSA (Strapparava et al, 2004) 73.3Senseval-3 Best System(Grozea, 2004) 72.9Table 5: Results compared to previous best systemson Senseval-3 English lexical sample task.6.2 All-words TaskThe F-measure micro-averaged result for our sys-tems as well as previous best systems for Senseval-2and Senseval-3 all-words task are shown in Table 6and Table 7 respectively.
Bayesian network with softtagging achieved 2.6% improvement over its base-line in Senseval-2 and 1.7% in Senseval-3.
The re-sults also rival some previous best systems, exceptfor SMUaw (Mihalcea, 2002) which used additionallabeled data.Bayes (Soft Tag) 66.3NB baseline 63.7SMUaw (Mihalcea, 2002) 69.0Simil-Prime (Kohomban and Lee, 2005) 66.4Senseval-2 Best System 63.6(CNTS-Antwerp (Hoste et al, 2001))Table 6: Results compared to previous best systemson Senseval-2 English all-words task.Bayes (Soft Tag) 66.1NB baseline 64.6Simil-Prime (Kohomban and Lee, 2005) 66.1Senseval-3 Best System 65.2(GAMBL-AW-S(Decadt et al, 2004))Senseval-3 2nd Best System (SenseLearner 64.6(Mihalcea and Faruque, 2004))Table 7: Results compared to previous best systemson Senseval-3 English all-words task.6.3 Significance of ResultsWe perform the ?2-test, using the Bayesian networkand its na?
?ve Bayes baseline (NB baseline) as pairs,to verify the significance of these results.
The resultis reported in Table 8.
The results are significant at90% confidence level, except for the Senseval-3 all-words task.Senseval-2 Senseval-3All-word 0.0527 0.2925Lexical Sample <0.0001 0.0002Table 8: P value for ?2-test significance levels ofresults.6.4 SVM with Topic FeaturesThe results on lexical sample task show that SVMbenefits less from the topic feature than the Bayesianapproach.
One possible reason is that SVM base-line is able to use all bag-of-words from surround-ing context while na?
?ve Bayes baseline can only usevery few without decreasing its accuracy, due to thesparse representation.
In this sense, SVM baselinealready captures some of the topical information,leaving a smaller room for improvement.
In fact, ifwe exclude the bag-of-words feature from the SVMbaseline and add in the topic features, we are ableto achieve almost the same accuracy as we did withboth features included, as shown in Table 9.
Thisfurther shows that the topic feature is a better rep-resentation of global context than the bag-of-wordsfeature.SVM baseline 72.4SVM baseline - BAG + topic 73.5SVM-topic 73.6Table 9: Results on Senseval-3 English lexical sam-ple task6.5 Results on Different Parts-of-SpeechWe analyse the result obtained on Senseval-3 En-glish lexical sample task (using Senseval-2 optimalconfiguration) according to the test instance?s part-of-speech, which includes noun, verb and adjec-tive, compared to the na?
?ve Bayes baseline.
Ta-ble 10 shows the relative improvement on each part-of-speech.
The second column shows the numberof testing instances belonging to the particular part-of-speech.
The third and fourth column shows the10210.640.6450.650.6550.660.6650.670.6750.680 2 4 6 8 10 12 14 16 18LK=103333 33 333K=20++++ + ++++K=402 22222222K=60?
??
??
???
?K=804 4444 44 44Figure 4: Accuracy with varing L and K onSenseval-2 all-words taskaccuracy achieved by na?
?ve Bayes baseline and theBayesian network.
Adjectives show no improve-ment while verbs show a moderate +2.2% improve-ment.
Nouns clearly benefit from topical informa-tion much more than the other two parts-of-speech,obtaining a +5.7% increase over its baseline.POS Total NB baseline Bayes (Soft Tag)Noun 1807 69.5 75.2Verb 1978 71.1 73.5Adj 159 57.2 57.2Total 3944 69.8 73.6Table 10: Improvement with different POS onSenseval-3 lexical sample task6.6 Sensitivity to L and KWe tested on Senseval-2 all-words task using differ-ent L and K. Figure 4 is the result.6.7 Results on SemEval-1We participated in SemEval-1 English coarse-grained all-words task (task 7), English fine-grainedall-words task (task 17, subtask 3) and Englishcoarse-grained lexical sample task (task 17, subtask1), using the method described in this paper.
Forall-words task, we use Senseval-2 and Senseval-3all-words task data as our validation set to fine tunethe parameters.
For lexical sample task, we use thetraining data provided as the validation set.We achieved 88.7%, 81.6% and 57.6% for coarse-grained lexical sample task, coarse-grained all-words task and fine-grained all-words task respec-tively.
The results ranked first, second and fourth inthe three tasks respectively.7 Conclusion and Future WorkIn this paper, we showed that by using LDA algo-rithm on bag-of-words feature, one can utilise moretopical information and boost the classifiers accu-racy on both English lexical sample and all-wordstask.
Only unlabeled data is needed for this improve-ment.
It would be interesting to see how the featurecan help on WSD of other languages and other nat-ural language processing tasks such as named-entityrecognition.ReferencesY.
K. Lee and H. T. Ng.
2002.
An Empirical Evaluationof Knowledge Sources and Learning Algorithms forWord Sense Disambiguation.
In Proc.
of EMNLP.B.
Snyder and M. Palmer.
2004.
The English All-WordsTask.
In Proc.
of Senseval-3.U.
S. Kohomban and W. S. Lee 2005.
Learning SemanticClasses for Word Sense Disambiguation.
In Proc.
ofACL.R.
K. Ando.
2006.
Applying Alternating Structure Op-timization to Word Sense Disambiguation.
In Proc.
ofCoNLL.Y.
S. Chan and H. T. Ng 2005.
Scaling Up Word SenseDisambiguation via Parallel Texts.
In Proc.
of AAAI.R.
K. Ando and T. Zhang.
2005a.
A Framework forLearning Predictive Structures from Multiple Tasksand Unlabeled Data.
Journal of Machine Learning Re-search.R.
K. Ando and T. Zhang.
2005b.
A High-PerformanceSemi-Supervised Learning Method for Text Chunking.In Proc.
of ACL.P.
Resnik and D. Yarowsky.
1997.
A Perspective onWord Sense Disambiguation Methods and Their Eval-uation.
In Proc.
of ACL.D.
M. Blei and A. Y. Ng and M. I. Jordan.
2003.
La-tent Dirichlet Allocation.
Journal of Machine Learn-ing Research.1022A.
Ratnaparkhi 1996.
A Maximum Entropy Model forPart-of-Speech Tagging.
In Proc.
of EMNLP.E.
Charniak 2000.
A Maximum-Entropy-InspiredParser.
In Proc.
of the 1st Meeting of the North Ameri-can Chapter of the Association for Computational Lin-guistics.V.
N. Vapnik 1995.
The Nature of Statistical LearningTheory.
Springer-Verlag, New York.R.
Florian and D. Yarowsky 2002.
Modeling consensus:Classifier Combination for Word Sense Disambigua-tion.
In Proc.
of EMNLP.D.
Wu and W. Su and M. Carpuat.
2004.
A Kernel PCAMethod for Superior Word Sense Disambiguation.
InProc.
of ACL.C.
Strapparava and A. Gliozzo and C. Giuliano 2004.Pattern Abstraction and Term Similarity for WordSense Disambiguation: IRST at Senseval-3.
In Proc.of Senseval-3.C.
Grozea 2004.
Finding Optimal Parameter Settingsfor High Performance Word Sense Disambiguation.
InProc.
of Senseval-3.R.
Mihalcea 2002.
Bootstrapping Large Sense TaggedCorpora.
In Proc.
of the 3rd International Conferenceon Languages Resources and Evaluations.V.
Hoste and A. Kool and W. Daelmans 2001.
ClassifierOptimization and Combination in English All WordsTask.
In Proc.
of Senseval-2.B.
Decadt and V. Hoste and W. Daelmans 2004.GAMBL, Genetic Algorithm Optimization ofMemory-Based WSD.
In Proc.
of Senseval-3.R.
Mihalcea and E. Faruque 2004.
Sense-learner: Mini-mally Supervised Word Sense Disambiguation for AllWords in Open Text.
In Proc.
of Senseval-3.1023
