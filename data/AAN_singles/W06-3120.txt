Proceedings of the Workshop on Statistical Machine Translation, pages 142?145,New York City, June 2006. c?2006 Association for Computational LinguisticsTALP Phrase-based statistical translation system for European languagepairsMarta R. Costa-jussa`Patrik LambertJose?
B. Marin?oJosep M. CregoMaxim KhalilovJose?
A. R. FonollosaDepartment of Signal Theory and CommunicationsTALP Research Center (UPC)Barcelona 08034, Spain(mruiz,jmcrego,agispert,lambert,khalilov,canton,adrian, rbanchs)@gps.tsc.upc.eduAdria` de GispertRafael E. BanchsAbstractThis paper reports translation results forthe ?Exploiting Parallel Texts for Statis-tical Machine Translation?
(HLT-NAACLWorkshop on Parallel Texts 2006).
Wehave studied different techniques to im-prove the standard Phrase-Based transla-tion system.
Mainly we introduce two re-ordering approaches and add morphologi-cal information.1 IntroductionNowadays most Statistical Machine Translation(SMT) systems use phrases as translation units.
Inaddition, the decision rule is commonly modelledthrough a log-linear maximum entropy frameworkwhich is based on several feature functions (in-cluding the translation model), hm.
Each featurefunction models the probability that a sentence e inthe target language is a translation of a given sen-tence f in the source language.
The weights, ?i,of each feature function are typically optimized tomaximize a scoring function.
It has the advantagethat additional features functions can be easily in-tegrated in the overall system.This paper describes a Phrase-Based systemwhose baseline is similar to the system in Costa-jussa` and Fonollosa (2005).
Here we introducetwo reordering approaches and add morphologicalinformation.
Translation results for all six trans-lation directions proposed in the shared task arepresented and discussed.
More specifically, fourdifferent languages are considered: English (en),Spanish (es), French (fr) and German (de); andboth translation directions are considered for thepairs: EnEs, EnFr, and EnDe.
The paper is orga-nized as follows: Section 2 describes the system;0This work has been supported by the European Unionunder grant FP6-506738 (TC-STAR project) and the TALPResearch Center (under a TALP-UPC-Recerca grant).Section 3 presents the shared task results; and, fi-nally, in Section 4, we conclude.2 System DescriptionThis section describes the system procedure fol-lowed for the data provided.2.1 AlignmentGiven a bilingual corpus, we use GIZA++ (Och,2003) as word alignment core algorithm.
Duringword alignment, we use 50 classes per languageestimated by ?mkcls?, a freely-available tool alongwith GIZA++.
Before aligning we work with low-ercase text (which leads to an Alignment ErrorRate reduction) and we recover truecase after thealignment is done.In addition, the alignment (in specific pairs oflanguages) was improved using two strategies:Full verb forms The morphology of the verbsusually differs in each language.
Therefore, it isinteresting to classify the verbs in order to addressthe rich variety of verbal forms.
Each verb is re-duced into its base form and reduced POS tag asexplained in (de Gispert, 2005).
This transforma-tion is only done for the alignment, and its goalis to simplify the work of the word alignment im-proving its quality.Block reordering (br) The difference in wordorder between two languages is one of the mostsignificant sources of error in SMT.
Related workseither deal with reordering in general as (Kanthaket al, 2005) or deal with local reordering as (Till-mann and Ney, 2003).
We report a local reorder-ing technique, which is implemented as a pre-processing stage, with two applications: (1) to im-prove only alignment quality, and (2) to improvealignment quality and to infer reordering in trans-lation.
Here, we present a short explanation of thealgorithm, for further details see Costa-jussa` andFonollosa (2006).142Figure 1: Example of an Alignment Block, i.e.
apair of consecutive blocks whose target translationis swappedThis reordering strategy is intended to infer themost probable reordering for sequences of words,which are referred to as blocks, in order to mono-tonize current data alignments and generalize re-ordering for unseen pairs of blocks.Given a word alignment, we identify those pairsof consecutive source blocks whose translation isswapped, i.e.
those blocks which, if swapped,generate a correct monotone translation.
Figure 1shows an example of these pairs (hereinafter calledAlignment Blocks).Then, the list of Alignment Blocks (LAB) isprocessed in order to decide whether two consec-utive blocks have to be reordered or not.
By usingthe classification algorithm, see the Appendix, wedivide the LAB in groups (Gn, n = 1 .
.
.
N ).
In-side the same group, we allow new internal com-bination in order to generalize the reordering tounseen pairs of blocks (i.e.
new Alignment Blocksare created).
Based on this information, the sourceside of the bilingual corpora are reordered.In case of applying the reordering technique forpurpose (1), we modify only the source trainingcorpora to realign and then we recover the origi-nal order of the training corpora.
In case of usingBlock Reordering for purpose (2), we modify allthe source corpora (both training and test), and weuse the new training corpora to realign and buildthe final translation system.2.2 Phrase ExtractionGiven a sentence pair and a corresponding wordalignment, phrases are extracted following the cri-terion in Och and Ney (2004).
A phrase (orbilingual phrase) is any pair of m source wordsand n target words that satisfies two basic con-straints: words are consecutive along both sidesof the bilingual phrase, and no word on either sideof the phrase is aligned to a word out of the phrase.We limit the maximum size of any given phrase to7.
The huge increase in computational and storagecost of including longer phrases does not providea significant improvement in quality (Koehn et al,2003) as the probability of reappearance of largerphrases decreases.2.3 Feature functionsConditional and posterior probability (cp, pp)Given the collected phrase pairs, we estimate thephrase translation probability distribution by rela-tive frequency in both directions.The target language model (lm) consists of ann-gram model, in which the probability of a trans-lation hypothesis is approximated by the productof word n-gram probabilities.
As default languagemodel feature, we use a standard word-based 5-gram language model generated with Kneser-Neysmoothing and interpolation of higher and lowerorder n-grams (Stolcke, 2002).The POS target language model (tpos) con-sists of an N-gram language model estimated overthe same target-side of the training corpus but us-ing POS tags instead of raw words.The forward and backwards lexicon mod-els (ibm1, ibm1?1) provide lexicon translationprobabilities for each phrase based on the wordIBM model 1 probabilities.
For computing theforward lexicon model, IBM model 1 probabili-ties from GIZA++ source-to-target algnments areused.
In the case of the backwards lexicon model,target-to-source alignments are used instead.The word bonus model (wb) introduces a sen-tence length bonus in order to compensate the sys-tem preference for short output sentences.The phrase bonus model (pb) introduces a con-stant bonus per produced phrase.2.4 DecodingThe search engine for this translation system is de-scribed in Crego et al (2005) which takes into ac-count the features described above.Using reordering in the decoder (rgraph) Ahighly constrained reordered search is performedby means of a set of reordering patterns (linguisti-cally motivated rewrite patterns) which are used to143extend the monotone search graph with additionalarcs.
See the details in Crego et al (2006).2.5 OptimizationIt is based on a simplex method (Nelder andMead, 1965).
This algorithm adjusts the log-linear weights in order to maximize a non-linearcombination of translation BLEU and NIST: 10 ?log10((BLEU ?
100) + 1) + NIST.
The max-imization is done over the provided developmentset for each of the six translation directions underconsideration.
We have experimented an improve-ment in the coherence between all the automaticfigures by integrating two of these figures in theoptimization function.3 Shared Task Results3.1 DataThe data provided for this shared task correspondsto a subset of the official transcriptions of theEuropean Parliament Plenary Sessions, and itis available through the shared task website at:http://www.statmt.org/wmt06/shared-task/.The development set used to tune the systemconsists of a subset (500 first sentences) of theofficial development set made available for theShared Task.We carried out a morphological analysis of thedata.
The English POS-tagging has been carriedout using freely available TNT tagger (Brants,2000).
In the Spanish case, we have used theFreeling (Carreras et al, 2004) analysis toolwhich generates the POS-tagging for each inputword.3.2 Systems configurationsThe baseline system is the same for all tasks andincludes the following features functions: cp, pp,lm, ibm1, ibm1?1, wb, pb.
The POStag targetlanguage model has been used in those tasks forwhich the tagger was available.
Table 1 shows thereordering configuration used for each task.The Block Reordering (application 2) has beenused when the source language belongs to the Ro-manic family.
The length of the block is lim-ited to 1 (i.e.
it allows the swapping of singlewords).
The main reason is that specific errors aresolved in the tasks from a Romanic language toa Germanic language (as the common reorder ofNoun + Adjective that turns into Adjective +Noun).
Although the Block Reordering approachTask Reordering ConfigurationEs2En br2En2Es br1 + rgraphFr2En br2En2Fr br1 + rgraphDe2En -En2De -Table 1: Additional reordering models for eachtask: br1 (br2) stands for Block Reordering ap-plication 1 (application 2); and rgraph refers tothe reordering integrated in the decoderdoes not depend on the task, we have not donethe corresponding experiments to observe its ef-ficiency in all the pairs used in this evaluation.The rgraph has been applied in those caseswhere: we do not use br2 (there is no sense inapplying them simultaneously); and we have thetagger for the source language model available.In the case of the pair GeEn, we have not exper-imented any reordering, we left the application ofboth reordering approaches as future work.3.3 DiscussionTable 2 presents the BLEU scores evaluated on thetest set (using TRUECASE) for each configuration.The official results were slightly better because alowercase evaluation was used, see (Koehn andMonz, 2006).For both, Es2En and Fr2En tasks, br helpsslightly.
The improvement of the approach de-pends on the quality of the alignment.
The betteralignments allow to extract higher quality Align-ment Blocks (Costa-jussa` and Fonollosa, 2006).The En2Es task is improved when adding bothbr1 and rgraph.
Similarly, the En2Fr task seems toperform fairly well when using the rgraph.
In thiscase, the improvement of the approach depends onthe quality of the alignment patterns (Crego et al,2006).
However, it has the advantage of delay-ing the final decision of reordering to the overallsearch, where all models are used to take a fullyinformed decision.Finally, the tpos does not help much when trans-lating to English.
It is not surprising because it wasused in order to improve the gender and numberagreement, and in English there is no need.
How-ever, in the direction to Spanish, the tpos addedto the corresponding reordering helps more as theSpanish language has gender and number agree-ment.144Task Baseline +tpos +rc +tpos+rcEs2En 29.08 29.08 29.89 29.98En2Es 27.73 27.66 28.79 28.99Fr2En 27.05 27.06 27.43 27.23En2Fr 26.16 - 27.80 -De2En 21.59 21.33 - -En2De 15.20 - - -Table 2: Results evaluated using TRUECASE onthe test set for each conguration: rc stands forReordering Conguration and refers to Table 1.The bold results were the congurations submit-ted.4 ConclusionsReordering is important when using a Phrase-Based system.
Although local reordering is sup-posed to be included in the phrase structure, per-forming local reordering improves the translationquality.
In fact, local reordering, provided by thereordering approaches, allows for those general-izations which phrases could not achieve.
Re-ordering in the DeEn task is left as further work.ReferencesT.
Brants.
2000.
Tnt - a statistical part-of-speech tag-ger.
Proceedings of the Sixth Applied Natural Lan-guage Processing.X.
Carreras, I. Chao, L.
Padro?, and M. Padro?.
2004.Freeling: An open-source suite of language analyz-ers.
4th Int.
Conf.
on Language Resources and Eval-uation, LREC?04.M.
R. Costa-jussa` and J.A.R.
Fonollosa.
2005.
Im-proving the phrase-based statistical translation bymodifying phrase extraction and including new fea-tures.
Proceedings of the ACL Workshop on Build-ing and Using Parallel Texts: Data-Driven MachineTranslation and Beyond.M.
R. Costa-jussa` and J.A.R.
Fonollosa.
2006.
Usingreordering in statistical machine translation based onalignment block classification.
Internal Report.J.M.
Crego, J. Marin?o, and A. de Gispert.
2005.An Ngram-based statistical machine translation de-coder.
Proc.
of the 9th Int.
Conf.
on Spoken Lan-guage Processing, ICSLP?05.J.
M. Crego, A. de Gispert, P. Lambert, M. R.Costa-jussa`, M. Khalilov, J. Marin?o, J.
A. Fonol-losa, and R. Banchs.
2006.
Ngram-based smtsystem enhanced with reordering patterns.
HLT-NAACL06 Workshop on Building and Using Paral-lel Texts: Data-Driven Machine Translation and Be-yond, June.A.
de Gispert.
2005.
Phrase linguistic classification forimproving statistical machine translation.
ACL 2005Students Workshop, June.S.
Kanthak, D. Vilar, E. Matusov, R. Zens, and H.Ney.
2005.
Novel reordering approaches in phrase-based statistical machine translation.
Proceedingsof the ACL Workshop on Building and Using Par-allel Texts: Data-Driven Machine Translation andBeyond, pages 167?174, June.P.
Koehn and C. Monz.
2006.
Manual and automaticevaluation of machine translation between europeanlanguages.
June.P.
Koehn, F.J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
Proc.
of the Human Lan-guage Technology Conference, HLT-NAACL?2003,May.J.A.
Nelder and R. Mead.
1965.
A simplex methodfor function minimization.
The Computer Journal,7:308?313.F.J.
Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Compu-tational Linguistics, 30(4):417?449, December.F.J.
Och.
2003.
Giza++ software.
http://www-i6.informatik.rwth-aachen.de/?och/ soft-ware/giza++.html.A.
Stolcke.
2002.
Srilm - an extensible language mod-eling toolkit.
Proc.
of the 7th Int.
Conf.
on SpokenLanguage Processing, ICSLP?02, September.C.
Tillmann and H. Ney.
2003.
Word reordering anda dynamic programming beam search algorithm forstatistical machine translation.
Computational Lin-guistics, 29(1):97?133, March.A AppendixHere we describe the classification algorithm usedin Section 1.1.
Initialization: set n?
1 and LAB ?
?
LAB.2.
Main part: while LAB ?
is not empty do?
Gn = {(?k, ?k)} where (?k, ?k) is anyelement of LAB ?, i.e.
?k is the firstblock and ?k is the second block of theAlignment Block k of the LAB ?.?
Recursively, move elements (?i, ?i)from LAB?
to Gn if there is an element(?j , ?j) ?
Gn such that ?i = ?j or?i = ?j?
Increase n (i.e.
n?
n + 1)3.
Ending: For each Gn, construct the two setsAn and Bn which consists on the first andsecond element of the pairs in Gn, respec-tively.145
