Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1223?1233,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsInducing Probabilistic CCG Grammars from Logical Formwith Higher-Order UnificationTom Kwiatkowski?t.m.kwiatkowksi@sms.ed.ac.ukLuke Zettlemoyer?lsz@cs.washington.eduSharon Goldwater?sgwater@inf.ed.ac.ukMark Steedman?steedman@inf.ed.ac.uk?School of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UK?Computer Science & EngineeringUniversity of WashingtonSeattle, WA 98195AbstractThis paper addresses the problem of learn-ing to map sentences to logical form, giventraining data consisting of natural languagesentences paired with logical representationsof their meaning.
Previous approaches havebeen designed for particular natural languagesor specific meaning representations; here wepresent a more general method.
The approachinduces a probabilistic CCG grammar thatrepresents the meaning of individual wordsand defines how these meanings can be com-bined to analyze complete sentences.
Weuse higher-order unification to define a hy-pothesis space containing all grammars con-sistent with the training data, and developan online learning algorithm that efficientlysearches this space while simultaneously es-timating the parameters of a log-linear parsingmodel.
Experiments demonstrate high accu-racy on benchmark data sets in four languageswith two different meaning representations.1 IntroductionA key aim in natural language processing is to learna mapping from natural language sentences to for-mal representations of their meaning.
Recent workhas addressed this problem by learning semanticparsers given sentences paired with logical meaningrepresentations (Thompson & Mooney, 2002; Kateet al, 2005; Kate & Mooney, 2006; Wong &Mooney, 2006, 2007; Zettlemoyer & Collins, 2005,2007; Lu et al, 2008).
For example, the trainingdata might consist of English sentences paired withlambda-calculus meaning representations:Sentence: which states border texasMeaning: ?x.state(x) ?
next to(x, tex)Given pairs like this, the goal is to learn to map new,unseen, sentences to their corresponding meaning.Previous approaches to this problem have beentailored to specific natural languages, specific mean-ing representations, or both.
Here, we develop anapproach that can learn to map any natural languageto a wide variety of logical representations of lin-guistic meaning.
In addition to data like the above,this approach can also learn from examples such as:Sentence: hangi eyaletin texas ye siniri vardirMeaning: answer(state(borders(tex)))where the sentence is in Turkish and the meaningrepresentation is a variable-free logical expressionof the type that has been used in recent work (Kateet al, 2005; Kate & Mooney, 2006; Wong &Mooney, 2006; Lu et al, 2008).The reason for generalizing to multiple languagesis obvious.
The need to learn over multiple repre-sentations arises from the fact that there is no stan-dard representation for logical form for natural lan-guage.
Instead, existing representations are ad hoc,tailored to the application of interest.
For example,the variable-free representation above was designedfor building natural language interfaces to databases.Our approach works by inducing a combinatorycategorial grammar (CCG) (Steedman, 1996, 2000).A CCG grammar consists of a language-specificlexicon, whose entries pair individual words andphrases with both syntactic and semantic informa-tion, and a universal set of combinatory rules that1223project that lexicon onto the sentences and meaningsof the language via syntactic derivations.
The learn-ing process starts by postulating, for each sentencein the training data, a single multi-word lexical itempairing that sentence with its complete logical form.These entries are iteratively refined with a restrictedhigher-order unification procedure (Huet, 1975) thatdefines all possible ways to subdivide them, consis-tent with the requirement that each training sentencecan still be parsed to yield its labeled meaning.For the data sets we consider, the space of pos-sible grammars is too large to explicitly enumerate.The induced grammar is also typically highly am-biguous, producing a large number of possible anal-yses for each sentence.
Our approach discriminatesbetween analyses using a log-linear CCG parsingmodel, similar to those used in previous work (Clark& Curran, 2003, 2007), but differing in that the syn-tactic parses are treated as a hidden variable duringtraining, following the approach of Zettlemoyer &Collins (2005, 2007).
We present an algorithm thatincrementally learns the parameters of this modelwhile simultaneously exploring the space of possi-ble grammars.
The model is used to guide the pro-cess of grammar refinement during training as wellas providing a metric for selecting the best analysisfor each new sentence.We evaluate the approach on benchmark datasetsfrom a natural language interface to a database ofUS Geography (Zelle & Mooney, 1996).
We showthat accurate models can be learned for multiplelanguages with both the variable-free and lambda-calculus meaning representations introduced above.We also compare performance to previous methods(Kate & Mooney, 2006; Wong & Mooney, 2006,2007; Zettlemoyer & Collins, 2005, 2007; Lu et al,2008), which are designed with either language- orrepresentation- specific constraints that limit gener-alization, as discussed in more detail in Section 6.Despite being the only approach that is generalenough to run on all of the data sets, our algorithmachieves similar performance to the others, even out-performing them in several cases.2 Overview of the ApproachThe goal of our algorithm is to find a functionf : x ?
z that maps sentences x to logical ex-pressions z.
We learn this function by inducing aprobabilistic CCG (PCCG) grammar from a train-ing set {(xi, zi)|i = 1 .
.
.
n} containing example(sentence, logical-form) pairs such as (?New Yorkborders Vermont?, next to(ny, vt)).
The inducedgrammar consists of two components which the al-gorithm must learn:?
A CCG lexicon, ?, containing lexical itemsthat define the space of possible parses y foran input sentence x.
Each parse contains bothsyntactic and semantic information, and definesthe output logical form z.?
A parameter vector, ?, that defines a distribu-tion over the possible parses y, conditioned onthe sentence x.We will present the approach in two parts.
Thelexical induction process (Section 4) uses a re-stricted form of higher order unification along withthe CCG combinatory rules to propose new entriesfor ?.
The complete learning algorithm (Section 5)integrates this lexical induction with a parameter es-timation scheme that learns ?.
Before presenting thedetails, we first review necessary background.3 BackgroundThis section provides an introduction to the ways inwhich we will use lambda calculus and higher-orderunification to construct meaning representations.
Italso reviews the CCG grammar formalism and prob-abilistic extensions to it, including existing parsingand parameter estimation techniques.3.1 Lambda Calculus and Higher-OrderUnificationWe assume that sentence meanings are representedas logical expressions, which we will construct fromthe meaning of individual words by using the op-erations defined in the lambda calculus.
We use aversion of the typed lambda calculus (cf.
Carpenter(1997)), in which the basic types include e, for en-tities; t, for truth values; and i for numbers.
Thereare also function types of the form ?e, t?
that are as-signed to lambda expressions, such as ?x.state(x),which take entities and return truth values.
Werepresent the meaning of words and phrases using1224lambda-calculus expressions that can contain con-stants, quantifiers, logical connectors, and lambdaabstractions.The advantage of using the lambda calculuslies in its generality.
The meanings of individ-ual words and phrases can be arbitrary lambda ex-pressions, while the final meaning for a sentencecan take different forms.
It can be a full lambda-calculus expression, a variable-free expression suchas answer(state(borders(tex))), or any other log-ical expression that can be built from the primitivemeanings via function application and composition.The higher-order unification problem (Huet,1975) involves finding a substitution for the freevariables in a pair of lambda-calculus expressionsthat, when applied, makes the expressions equaleach other.
This problem is notoriously complex;in the unrestricted form (Huet, 1973), it is undecid-able.
In this paper, we will guide the grammar in-duction process using a restricted version of higher-order unification that is tractable.
For a given ex-pression h, we will need to find expressions for fand g such that either h = f(g) or h = ?x.f(g(x)).This limited form of the unification problem will al-low us to define the ways to split h into subpartsthat can be recombined with CCG parsing opera-tions, which we will define in the next section, toreconstruct h.3.2 Combinatory Categorial GrammarCCG (Steedman, 2000) is a linguistic formalismthat tightly couples syntax and semantics, andcan be used to model a wide range of languagephenomena.
For present purposes a CCG grammarincludes a lexicon ?
with entries like the following:New York ` NP : nyborders ` S\NP/NP : ?x?y.next to(y, x)Vermont ` NP : vtwhere each lexical item w`X : h has words w, asyntactic categoryX , and a logical form h expressedas a lambda-calculus expression.
For the first exam-ple, these are ?New York,?
NP , and ny.
CCG syn-tactic categories may be atomic (such as S, NP ) orcomplex (such as S\NP/NP ).CCG combines categories using a set of com-binatory rules.
For example, the forward (>) andbackward (<) application rules are:X/Y : f Y : g ?
X : f(g) (>)Y : g X\Y : f ?
X : f(g) (<)These rules apply to build syntactic and semanticderivations under the control of the word order infor-mation encoded in the slash directions of the lexicalentries.
For example, given the lexicon above, thesentence New York borders Vermont can be parsedto produce:New York borders VermontNP (S\NP )/NP NPny ?x?y.next to(y, x) vt>(S\NP )?y.next to(y, vt)<Snext to(ny, vt)where each step in the parse is labeled with the com-binatory rule (?
> or ?
<) that was used.CCG also includes combinatory rules of forward(> B) and backward (< B) composition:X/Y : f Y/Z : g ?X/Z : ?x.f(g(x)) (> B)Y \Z : g X\Y : f ?X\Z : ?x.f(g(x)) (< B)These rules provide for a relaxed notion of con-stituency which will be useful during learning as wereason about possible refinements of the grammar.We also allow vertical slashes in CCG categories,which act as wild cards.
For example, with thisextension the forward application combinator (>)could be used to combine the category S/(S|NP )with any of S\NP , S/NP , or S|NP .
Figure 1shows two parses where the composition combina-tors and vertical slashes are used.
These parsesclosely resemble the types of analyses that will bepossible under the grammars we learn in the experi-ments described in Section 8.3.3 Probabilistic CCGsGiven a CCG lexicon ?, there will, in general, bemany possible parses for each sentence.
We selectthe most likely alternative using a log-linear model,which consists of a feature vector ?
and a parame-ter vector ?.
The joint probability of a logical formz constructed with a parse y, given a sentence x is1225hangi eyaletin texas ye siniri vardirS/NP NP/NP NP NP\NP?x.answer(x) ?x.state(x) tex ?x.border(x)<NPborder(tex)>NPstate(border(tex))>Sanswer(state(border(tex)))what states border texasS/(S|NP ) S|NP/(S|NP ) S\NP/NP NP?f?x.f(x) ?f?x.state(x)?f(x) ?y?x.next to(x, y) tex>BS|NP/NP?y?x.state(x) ?
next to(x, y)>S|NP?x.state(x) ?
next to(x, tex)>S?x.state(x) ?
next to(x, tex)Figure 1: Two examples of CCG parses with different logical form representations.defined as:P (y, z|x; ?,?)
=e???(x,y,z)?(y?,z?)
e???(x,y?,z?
)(1)Section 7 defines the features used in the experi-ments, which include, for example, lexical featuresthat indicate when specific lexical items in ?
areused in the parse y.
For parsing and parameter es-timation, we use standard algorithms (Clark & Cur-ran, 2007), as described below.The parsing, or inference, problem is to find themost likely logical form z given a sentence x, as-suming the parameters ?
and lexicon ?
are known:f(x) = arg maxzp(z|x; ?,?)
(2)where the probability of the logical form is found bysumming over all parses that produce it:p(z|x; ?,?)
=?yp(y, z|x; ?,?)
(3)In this approach the distribution over parse trees yis modeled as a hidden variable.
The sum overparses in Eq.
3 can be calculated efficiently usingthe inside-outside algorithm with a CKY-style pars-ing algorithm.To estimate the parameters themselves, weuse stochastic gradient updates (LeCun et al,1998).
Given a set of n sentence-meaning pairs{(xi, zi) : i = 1...n}, we update the parameters ?
it-eratively, for each example i, by following the localgradient of the conditional log-likelihood objectiveOi = logP (zi|xi; ?,?).
The local gradient of theindividual parameter ?j associated with feature ?jand training instance (xi, zi) is given by:?Oi?
?j= Ep(y|xi,zi;?,?
)[?j(xi, y, zi)]?Ep(y,z|xi;?,?
)[?j(xi, y, z)](4)As with Eq.
3, all of the expectations in Eq.
4 arecalculated through the use of the inside-outside al-gorithm on a pruned parse chart.
In the experiments,each chart cell was pruned to the top 200 entries.4 Splitting Lexical ItemsBefore presenting a complete learning algorithm, wefirst describe how to use higher-order unification todefine a procedure for splitting CCG lexical entries.This splitting process is used to expand the lexiconduring learning.
We seed the lexical induction witha multi-word lexical item xi`S :zi for each trainingexample (xi, zi), consisting of the entire sentence xiand its associated meaning representation zi.
For ex-ample, one initial lexical item might be:New York borders Vermont `S:next to(ny, vt) (5)Although these initial, sentential lexical itemscan parse the training data, they will not generalizewell to unseen data.
To learn effectively, we willneed to split overly specific entries of this type intopairs of new, smaller, entries that generalize better.For example, one possible split of the lexical entrygiven in (5) would be the pair:New York borders ` S/NP : ?x.next to(ny, x),Vermont `NP : vtwhere we broke the original logical expression intotwo new ones ?x.next to(ny, x) and vt, and pairedthem with syntactic categories that allow the newlexical entries to be recombined to produce the orig-inal analysis.
The next three subsections define theset of possible splits for any given lexical item.
Theprocess is driven by solving a higher-order unifica-tion problem that defines all of the ways of splittingthe logical expression into two parts, as described inSection 4.1.
Section 4.2 describes how to construct1226syntactic categories that are consistent with the twonew fragments of logical form and which will allowthe new lexical items to recombine.
Finally, Sec-tion 4.3 defines the full set of lexical entry pairs thatcan be created by splitting a lexical entry.As we will see, this splitting process is overly pro-lific for any single language and will yield manylexical items that do not generalize well.
Forexample, there is nothing in our original lexicalentry above that provides evidence that the splitshould pair ?Vermont?
with the constant vt and not?x.next to(ny, x).
Section 5 describes how weestimate the parameters of a probabilistic parsingmodel and how this parsing model can be used toguide the selection of items to add to the lexicon.4.1 Restricted Higher-Order UnificationThe set of possible splits for a logical expressionh is defined as the solution to a pair of higher-order unification problems.
We find pairs of logi-cal expressions (f, g) such that either f(g) = h or?x.f(g(x)) = h. Solving these problems createsnew expressions f and g that can be recombined ac-cording to the CCG combinators, as defined in Sec-tion 3.2, to produce h.In the unrestricted case, there can be infinitelymany solution pairs (f, g) for a given expression h.For example, when h = tex and f = ?x.tex, theexpression g can be anything.
Although it would besimple enough to forbid vacuous variables in f andg, the number of solutions would still be exponen-tial in the size of h. For example, when h contains aconjunction, such as h = ?x.city(x)?major(x)?in(x, tex), any subset of the expressions in the con-junction can be assigned to f (or g).To limit the number of possible splits, we enforcethe following restrictions on the possible higher-order solutions that will be used during learning:?
No Vacuous Variables: Neither g or f can be afunction of the form ?x.e where the expressione does not contain the variable x.
This rules outfunctions such as ?x.tex.?
Limited Coordination Extraction: The ex-pression g cannot contain more than N of theconjuncts that appear in any coordination inh.
For example, with N = 1 the expressiong = ?x.city(x)?major(x) could not be usedas a solution given the h conjuction above.
Weuse N = 4 in our experimental evaluation.?
Limited Application: The function f can-not contain new variables applied to any non-variable subexpressions from h. For example,if h = ?x.in(x, tex), the pair f = ?q.q(tex)and g = ?y?x.in(x, y) is forbidden.Together, these three restrictions guarantee thatthe number of splits is, in the worst case, an N -degree polynomial of the number of constants in h.The constraints were designed to increase the effi-ciency of the splitting algorithm without impactingperformance on the development data.4.2 Splitting CategoriesWe define the set of possible splits for a categoryX :h with syntax X and logical form h by enumer-ating the solution pairs (f, g) to the higher-orderunification problems defined above and creatingsyntactic categories for the resulting expressions.For example, given X :h = S\NP :?x.in(x, tex),f = ?y?x.in(x, y), and g = tex, we wouldproduce the following two pairs of new categories:( S\NP/NP :?y?x.in(x, y) , NP :tex )( NP :tex , S\NP\NP :?y?x.in(x, y) )which were constructed by first choosing the syntac-tic category for g, in this caseNP , and then enumer-ating the possible directions for the new slash in thecategory containing f .
We consider each of thesetwo steps in more detail below.The new syntactic category for g is determinedbased on its type, T (g).
For example, T (tex) = eand T (?x.state(x)) = ?e, t?.
Then, the functionC(T ) takes an input type T and returns the syntacticcategory of T as follows:C(T ) =??
?NP if T = eS if T = tC(T2)|C(T1) when T = ?T1, T2?The basic types e and t are assigned syntacticcategories NP and S, and all functional typesare assigned categories recursively.
For exam-ple C(?e, t?)
= S|NP and C(?e, ?e, t??)
=S|NP |NP .
This definition of CCG categories isunconventional in that it never assigns atomic cate-gories to functional types.
For example, there is no1227distinct syntactic category N for nouns (which havesemantic type ?e, t?).
Instead, the more complex cat-egory S|NP is used.Now, we are ready to define the set of all categorysplits.
For a category A = X:h we can defineSC(A) = {FA(A) ?
BA(A) ?
FC(A) ?
BC(A)}which is a union of sets, each of which includessplits for a single CCG operator.
For example,FA(X:h) is the set of category pairsFA(X:h) = {(X/Y :f, Y :g) | h=f(g) ?
Y=C(T (g))}where each pair can be combined with the forwardapplication combinator, described in Section 3.2, toreconstruct X:h.The remaining three sets are defined similarly,and are associated with the backward applicationand forward and backward composition operators,respectively:BA(X:h) = {(Y :g,X\Y :f) | h=f(g) ?
Y=C(T (g))}FC(X/Y :h) = {(X/W :f,W/Y :g) |h=?x.f(g(x)) ?W=C(T (g(x)))}BC(X\Y :h) = {(W\Y :g,X\W :f) |h=?x.f(g(x)) ?W=C(T (g(x)))}where the composition sets FC and BC only acceptinput categories with the appropriate outermost slashdirection, for example FC(X/Y :h).4.3 Splitting Lexical ItemsWe can now define the lexical splits that will be usedduring learning.
For lexical entry w0:n ` A, withword sequence w0:n = ?w0, .
.
.
, wn?
and CCG cat-egory A, define the set SL of splits to be:SL(w0:n`A) = {(w0:i`B,wi+1:n`C) |0 ?
i < n ?
(B,C) ?
SC(A)}where we enumerate all ways of splitting the wordssequence w0:n and aligning the subsequences withcategories in SC(A), as defined in the last section.5 Learning AlgorithmThe previous section described how a splitting pro-cedure can be used to break apart overly specificlexical items into smaller ones that may generalizebetter to unseen data.
The space of possible lexi-cal items supported by this splitting procedure is toolarge to explicitly enumerate.
Instead, we learn theparameters of a PCCG, which is used both to guidethe splitting process, and also to select the best parse,given a learned lexicon.Figure 2 presents the unification-based learningalgorithm, UBL.
This algorithm steps through thedata incrementally and performs two steps for eachtraining example.
First, new lexical items are in-duced for the training instance by splitting and merg-ing nodes in the best correct parse, given the currentparameters.
Next, the parameters of the PCCG areupdated by making a stochastic gradient update onthe marginal likelihood, given the updated lexicon.Inputs and Initialization The algorithm takes asinput the training set of n (sentence, logical form)pairs {(xi, zi) : i = 1...n} along with an NP list,?NP , of proper noun lexical items such as Texas`NP :tex.
The lexicon, ?, is initialized with a singlelexical item xi`S :zi for each of the training pairsalong with the contents of the NP list.
It is possibleto run the algorithm without the initial NP list; weinclude it to allow direct comparisons with previousapproaches, which also included NP lists.
Featuresand initial feature weights are described in Section 7.Step 1: Updating the Lexicon In the lexical up-date step the algorithm first computes the best cor-rect parse tree y?
for the current training exam-ple and then uses y?
as input to the procedureNEW-LEX, which determines which (if any) newlexical items to add to ?.
NEW-LEX begins by enu-merating all pairs (C,wi:j), for i < j, where C is acategory occurring at a node in y?
and wi:j are the(two or more) words it spans.
For example, in theleft parse in Figure 1, there would be four pairs: onewith the category C = NP\NP :?x.border(x) andthe phrase wi:j =?ye siniri vardir?, and one for eachnon-leaf node in the tree.For each pair (C,wi:j), NEW-LEX considers in-troducing a new lexical item wi:j`C, which allowsfor the possibility of a parse where the subtree rootedat C is replaced with this new entry.
(If C is a leafnode, this item will already exist.)
NEW-LEX alsoconsiders adding each pair of new lexical items thatis obtained by splitting wi:j`C as described in Sec-tion 4, thereby considering many different ways ofreanalyzing the node.
This process creates a set ofpossible new lexicons, where each lexicon expands1228?
in a different way by adding the items from eithera single split or a single merge of a node in y?.For each potential new lexicon ?
?, NEW-LEXcomputes the probability p(y?|xi, zi; ??,??)
of theoriginal parse y?
under ??
and parameters ??
that arethe same as ?
but have weights for the new lexicalitems, as described in Section 7.
It also finds thebest new parse y?
= arg maxy p(y|xi, zi; ??,??
).1Finally, NEW-LEX selects the ??
with the largestdifference in log probability between y?
and y?, andreturns the new entries in ??.
If y?
is the best parsefor every ?
?, NEW-LEX returns the empty set; thelexicon will not change.Step 2: Parameter Updates For each training ex-ample we update the parameters ?
using the stochas-tic gradient updates given by Eq.
4.Discussion The alternation between refining thelexicon and updating the parameters drives the learn-ing process.
The initial model assigns a conditionallikelihood of one to each training example (thereis a single lexical item for each sentence xi, andit contains the labeled logical form zi).
Althoughthe splitting step often decreases the probability ofthe data, the new entries it produces are less spe-cific and should generalize better.
Since we initiallyassign positive weights to the parameters for newlexical items, the overall approach prefers splitting;trees with many lexical items will initially be muchmore likely.
However, if the learned lexical itemsare used in too many incorrect parses, the stochasticgradient updates will down weight them to the pointwhere the lexical induction step can merge or re-splitnodes in the trees that contain them.
This allows theapproach to correct the lexicon and, hopefully, im-prove future performance.6 Related WorkPrevious work has focused on a variety of differentmeaning representations.
Several approaches havebeen designed for the variable-free logical repre-sentations shown in examples throughout this pa-per.
For example, Kate & Mooney (2006) present amethod (KRISP) that extends an existing SVM learn-ing algorithm to recover logical representations.
The1This computation can be performed efficiently by incre-mentally updating the parse chart used to find y?.Inputs: Training set {(xi, zi) : i = 1 .
.
.
n} where eachexample is a sentence xi paired with a logical formzi.
Set of NP lexical items ?NP .
Number of iter-ations T .
Learning rate parameter ?0 and coolingrate parameter c.Definitions: The function NEW-LEX(y) takes a parsey and returns a set of new lexical items found bysplitting and merging categories in y, as describedin Section 5.
The distributions p(y|x, z; ?,?)
andp(y, z|x; ?,?)
are defined by the log-linear model,as described in Section 3.3.Initialization:?
Set ?
= {xi ` S : zi} for all i = 1 .
.
.
n.?
Set ?
= ?
?
?NP?
Initialize ?
using coocurrence statistics, as de-scribed in Section 7.Algorithm:For t = 1 .
.
.
T, i = 1 .
.
.
n :Step 1: (Update Lexicon)?
Let y?
= arg maxy p(y|xi, zi; ?,?)?
Set ?
= ?
?NEW-LEX(y?)
and expand theparameter vector ?
to contain entries for thenew lexical items, as described in Section 7.Step 2: (Update Parameters)?
Let ?
= ?01+c?k where k = i+ t?
n.?
Let ?
= Ep(y|xi,zi;?,?)[?
(xi, y, zi)]?Ep(y,z|xi;?,?)[?
(xi, y, z)]?
Set ?
= ?
+ ?
?Output: Lexicon ?
and parameters ?.Figure 2: The UBL learning algorithm.WASP system (Wong & Mooney, 2006) uses statis-tical machine translation techniques to learn syn-chronous context free grammars containing bothwords and logic.
Lu et al (2008) (Lu08) developeda generative model that builds a single hybrid treeof words, syntax and meaning representation.
Thesealgorithms are all language independent but repre-sentation specific.Other algorithms have been designed to re-cover lambda-calculus representations.
For exam-ple, Wong & Mooney (2007) developed a variantof WASP (?-WASP) specifically designed for thisalternate representation.
Zettlemoyer & Collins(2005, 2007) developed CCG grammar inductiontechniques where lexical items are proposed accord-ing to a set of hand-engineered lexical templates.1229Our approach eliminates this need for manual effort.Another line of work has focused on recover-ing meaning representations that are not based onlogic.
Examples include an early statistical methodfor learning to fill slot-value representations (Milleret al, 1996) and a more recent approach for recover-ing semantic parse trees (Ge & Mooney, 2006).
Ex-ploring the extent to which these representations arecompatible with the logic-based learning approachwe developed is an important area for future work.Finally, there is work on using categorial gram-mars to solve other, related learning problems.For example, Buszkowski & Penn (1990) describea unification-based approach for grammar discov-ery from bracketed natural language sentences andVillavicencio (2002) developed an approach formodeling child language acquisition.
Additionally,Bos et al (2004) consider the challenging problemof constructing broad-coverage semantic representa-tions with CCG, but do not learn the lexicon.7 Experimental SetupFeatures We use two types of features in ourmodel.
First, we include a set of lexical features:For each lexical item L ?
?, we include a feature?L that fires when L is used.
Second, we include se-mantic features that are functions of the output logi-cal expression z.
Each time a predicate p in z takesan argument a with type T (a) in position i it trig-gers two binary indicator features: ?
(p,a,i) for thepredicate-argument relation; and ?
(p,T (a),i) for thepredicate argument-type relation.Initialization The weights for the semantic fea-tures are initialized to zero.
The weights for the lex-ical features are initialized according to coocurrancestatistics estimated with the Giza++ (Och & Ney,2003) implementation of IBM Model 1.
We com-pute translation scores for (word, constant) pairs thatcooccur in examples in the training data.
The initialweight for each ?L is set to ten times the averagescore over the (word, constant) pairs in L, except forthe weights of seed lexical entries in ?NP which areset to 10 (equivalent to the highest possible coocur-rence score).
We used the learning rate ?0 = 1.0and cooling rate c = 10?5 in all training scenar-ios, and ran the algorithm for T = 20 iterations.These values were selected with cross validation onthe Geo880 development set, described below.Data and Evaluation We evaluate our systemon the GeoQuery datasets, which contain natural-language queries of a geographical database pairedwith logical representations of each query?s mean-ing.
The full Geo880 dataset contains 880 (English-sentence, logical-form) pairs, which we split into adevelopment set of 600 pairs and a test set of 280pairs, following Zettlemoyer & Collins (2005).
TheGeo250 dataset is a subset of Geo880 containing250 sentences that have been translated into Turk-ish, Spanish and Japanese as well as the original En-glish.
Due to the small size of this dataset we use10-fold cross validation for evaluation.
We use thesame folds as Wong & Mooney (2006, 2007) and Luet al (2008), allowing a direct comparison.The GeoQuery data is annotated with bothlambda-calculus and variable-free meaning rep-resentations, which we have seen examples ofthroughout the paper.
We report results for both rep-resentations, using the standard measures of Recall(percentage of test sentences assigned correct log-ical forms), Precision (percentage of logical formsreturned that are correct) and F1 (the harmonic meanof Precision and Recall).Two-Pass Parsing To investigate the trade-off be-tween precision and recall, we report results with atwo-pass parsing strategy.
When the parser fails toreturn an analysis for a test sentence due to novelwords or usage, we reparse the sentence and allowthe parser to skip words, with a fixed cost.
Skip-ping words can potentially increase recall, if the ig-nored word is an unknown function word that doesnot contribute semantic content.8 Results and DiscussionTables 1, 2, and 3 present the results for all of the ex-periments.
In aggregate, they demonstrate that ouralgorithm, UBL, learns accurate models across lan-guages and for both meaning representations.
Thisis a new result; no previous system is as general.We also see the expected tradeoff between preci-sion and recall that comes from the two-pass parsingapproach, which is labeled UBL-s. With the abil-ity to skip words, UBL-s achieves the highest recallof all reported systems for all evaluation conditions.1230SystemEnglish SpanishRec.
Pre.
F1 Rec.
Pre.
F1WASP 70.0 95.4 80.8 72.4 91.2 81.0Lu08 72.8 91.5 81.1 79.2 95.2 86.5UBL 78.1 88.2 82.7 76.8 86.8 81.4UBL-s 80.4 80.8 80.6 79.7 80.6 80.1SystemJapanese TurkishRec.
Pre.
F1 Rec.
Pre.
F1WASP 74.4 92.0 82.9 62.4 97.0 75.9Lu08 76.0 87.6 81.4 66.8 93.8 78.0UBL 78.5 85.5 81.8 70.4 89.4 78.6UBL-s 80.5 80.6 80.6 74.2 75.6 74.9Table 1: Performance across languages on Geo250 withvariable-free meaning representations.SystemEnglish SpanishRec.
Pre.
F1 Rec.
Pre.
F1?-WASP 75.6 91.8 82.9 80.0 92.5 85.8UBL 78.0 93.2 84.7 75.9 93.4 83.6UBL-s 81.8 83.5 82.6 81.4 83.4 82.4SystemJapanese TurkishRec.
Pre.
F1 Rec.
Pre.
F1?-WASP 81.2 90.1 85.8 68.8 90.4 78.1UBL 78.9 90.9 84.4 67.4 93.4 78.1UBL-s 83.0 83.2 83.1 71.8 77.8 74.6Table 2: Performance across languages on Geo250 withlambda-calculus meaning representations.However, UBL achieves much higher precision andbetter overall F1 scores, which are generally compa-rable to the best performing systems.The comparison to the CCG induction techniquesof ZC05 and ZC07 (Table 3) is particularly striking.These approaches used language-specific templatesto propose new lexical items and also required as in-put a set of hand-engineered lexical entries to modelphenomena such as quantification and determiners.However, the use of higher-order unification allowsUBL to achieve comparable performance while au-tomatically inducing these types of entries.For a more qualitative evaluation, Table 4 shows aselection of lexical items learned with high weightsfor the lambda-calculus meaning representations.Nouns such as ?state?
or ?estado?
are consistentlylearned across languages with the category S|NP ,which stands in for the more conventional N .
Thealgorithm also learns language-specific construc-tions such as the Japanese case markers ?no?
and?wa?, which are treated as modifiers that do not addsemantic content.
Language-specific word order isalso encoded, using the slash directions of the CCGSystemVariable Free Lambda CalculusRec.
Pre.
F1 Rec.
Pre.
F1Cross Validation ResultsKRISP 71.7 93.3 81.1 ?
?
?WASP 74.8 87.2 80.5 ?
?
?Lu08 81.5 89.3 85.2 ?
?
?
?-WASP ?
?
?
86.6 92.0 89.2Independent Test SetZC05 ?
?
?
79.3 96.3 87.0ZC07 ?
?
?
86.1 91.6 88.8UBL 81.4 89.4 85.2 85.0 94.1 89.3UBL-s 84.3 85.2 84.7 87.9 88.5 88.2Table 3: Performance on the Geo880 data set, with variedmeaning representations.categories.
For example, ?what?
and ?que?
taketheir arguments to the right in the wh-initial Englishand Spanish.
However, the Turkish wh-word ?nel-erdir?
and the Japanese question marker ?nan desuka?
are sentence final, and therefore take their argu-ments to the left.
Learning regularities of this typeallows UBL to generalize well to unseen data.There is less variation and complexity in thelearned lexical items for the variable-free represen-tation.
The fact that the meaning representation isdeeply nested influences the form of the inducedgrammar.
For example, recall that the sentence?what states border texas?
would be paired with themeaning answer(state(borders(tex))).
For thisrepresentation, lexical items such as:what ` S/NP : ?x.answer(x)states `NP/NP : ?x.state(x)border `NP/NP : ?x.borders(x)texas `NP : texcan be used to construct the desired output.
Inpractice, UBL often learns entries with only a sin-gle slash, like those above, varying only in the di-rection, as required for the language.
Even themore complex items, such as those for quantifiers,are consistently simpler than those induced fromthe lambda-calculus meaning representations.
Forexample, one of the most complex entries learnedin the experiments for English is the smallest `NP\NP/(NP |NP ):?f?x.smallest one(f(x)).There are also differences in the aggregate statis-tics of the learned lexicons.
For example, the aver-age length of a learned lexical item for the (lambda-calculus, variable-free) meaning representations is:1231(1.21,1.08) for Turkish, (1.34,1.19) for English,(1.43,1.25) for Spanish and (1.63,1.42) for Japanese.For both meaning representations the model learnssignificantly more multiword lexical items for thesomewhat analytic Japanese than the agglutinativeTurkish.
There are also variations in the averagenumber of learned lexical items in the best parsesduring the final pass of training: 192 for Japanese,206 for Spanish, 188 for English and 295 for Turk-ish.
As compared to the other languages, the mor-pologically rich Turkish requires significantly morelexical variation to explain the data.Finally, there are a number of cases where theUBL algorithm could be improved in future work.In cases where there are multiple allowable word or-ders, the UBL algorithm must learn individual en-tries for each possibility.
For example, the followingtwo categories are often learned with high weight forthe Japanese word ?chiisai?
:NP/(S|NP )\(NP |NP ):?f?g.argmin(x, g(x), f(x))NP |(S|NP )/(NP |NP ):?f?g.argmin(x, g(x), f(x))and are treated as distinct entries in the lexicon.
Sim-ilarly, the approach presented here does not modelmorphology, and must repeatedly learn the correctcategories for the Turkish words ?nehri,?
?nehir,??nehirler,?
and ?nehirlerin?, all of which correspondto the logical form ?x.river(x).9 Conclusions and Future WorkThis paper has presented a method for inducingprobabilistic CCGs from sentences paired with log-ical forms.
The approach uses higher-order unifi-cation to define the space of possible grammars ina language- and representation-independent manner,paired with an algorithm that learns a probabilisticparsing model.
We evaluated the approach on fourlanguages with two meaning representations each,achieving high accuracy across all scenarios.For future work, we are interested in exploringthe generality of the approach while extending it tonew understanding problems.
One potential limi-tation is in the constraints we introduced to ensurethe tractability of the higher-order unification proce-dure.
These restrictions will not allow the approachto induce lexical items that would be used with,among other things, many of the type-raised combi-nators commonly employed in CCG grammars.
WeEnglishpopulation of ` NP/NP : ?x.population(x)smallest ` NP/(S|NP ) : ?f.arg min(y, f(y), size(y))what ` S|NP/(S|NP ) : ?f?x.f(x)border ` S|NP/NP : ?x?y.next to(y, x)state ` S|NP : ?x.state(x)most ` NP/(S|NP )\(S|NP )\(S|NP |NP ) :?f?g?h?x.argmax(y, g(y), count(z, f(z, y) ?
h(z)))Japaneseno ` NP |NP/(NP |NP ) : ?f?x.f(x)shuu ` S|NP : ?x.state(x)nan desu ka ` S\NP\(NP |NP ) : ?f?x.f(x)wa ` NP |NP\(NP |NP ) : ?f?x.f(x)ikutsu ` NP |(S|NP )\(S|NP |(S|NP )) :?f?g.count(x, f(g(x)))chiiki ` NP\NP :?x.area(x)Turkishnedir ` S\NP\(NP |NP ) : ?f?x.f(x)sehir ` S|NP : ?x.city(x)nufus yogunlugu ` NP |NP : ?x.density(x)siniri` S|NP/NP : ?x?y.next to(y, x)kac tane ` S\NP/(S|NP |NP )\(S|NP ) :?f?g?x.count(y, f(y) ?
g(y, x))ya siniri ` S|NP\NP : ?x?y.next to(y, x)Spanishen ` S|NP/NP : ?x?y.loc(y, x)que es la ` S/NP/(NP |NP ): ?f?x.f(x)pequena ` NP\(S|NP )\(NP |NP ) :?g?f.arg min(y, f(y), g(y))estado ` S|NP : ?x.state(x)mas ` S\(S|NP )/(S|NP )\(NP |NP |(S|NP )) :?f?g?h.argmax(x, h(x), f(g, x))mayores `S|NP\(S|NP ) :?f?x.f(x) ?major(x)Table 4: Example learned lexical items for each languageon the Geo250 lambda-calculus data sets.are also interested in developing similar grammarinduction techniques for context-dependent under-standing problems, such as the one considered byZettlemoyer & Collins (2009).
Such an approachwould complement ideas for using high-order unifi-cation to model a wider range of language phenom-ena, such as VP ellipsis (Dalrymple et al, 1991).AcknowledgementsWe thank the reviewers for useful feedback.
Thiswork was supported by the EU under IST Cog-nitive Systems grant IP FP6-2004-IST-4-27657?Paco-Plus?
and ERC Advanced Fellowship 249520?GRAMPLUS?
to Steedman.
Kwiatkowski wassupported by an EPRSC studentship.
Zettlemoyerwas supported by a US NSF International ResearchFellowship.1232ReferencesBos, J., Clark, S., Steedman, M., Curran, J. R., & Hock-enmaier, J.
(2004).
Wide-coverage semantic represen-tations from a CCG parser.
In Proceedings of the In-ternational Conference on Computational Linguistics.Buszkowski, W. & Penn, G. (1990).
Categorial grammarsdetermined from linguistic data by unification.
StudiaLogica, 49, 431?454.Carpenter, B.
(1997).
Type-Logical Semantics.
The MITPress.Clark, S. & Curran, J. R. (2003).
Log-linear modelsfor wide-coverage CCG parsing.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing.Clark, S. & Curran, J. R. (2007).
Wide-coverage effi-cient statistical parsing with CCG and log-linear mod-els.
Computational Linguistics, 33(4), 493?552.Dalrymple, M., Shieber, S., & Pereira, F. (1991).
Ellipsisand higher-order unification.
Linguistics and Philoso-phy, 14, 399?452.Ge, R. & Mooney, R. J.
(2006).
Discriminative rerank-ing for semantic parsing.
In Proceedings of the COL-ING/ACL 2006 Main Conference Poster Sessions.Huet, G. (1975).
A unification algorithm for typed ?-calculus.
Theoretical Computer Science, 1, 27?57.Huet, G. P. (1973).
The undecidability of unification inthird order logic.
Information and Control, 22(3), 257?267.Kate, R. J.
& Mooney, R. J.
(2006).
Using string-kernelsfor learning semantic parsers.
In Proceedings of the44th Annual Meeting of the Association for Computa-tional Linguistics.Kate, R. J., Wong, Y. W., & Mooney, R. J.
(2005).
Learn-ing to transform natural to formal languages.
In Pro-ceedings of the National Conference on Artificial In-telligence.LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998).Gradient-based learning applied to document recogni-tion.
Proceedings of the IEEE, 86(11), 2278?2324.Lu, W., Ng, H. T., Lee, W. S., & Zettlemoyer, L. S.(2008).
A generative model for parsing natural lan-guage to meaning representations.
In Proceedings ofThe Conference on Empirical Methods in Natural Lan-guage Processing.Miller, S., Stallard, D., Bobrow, R. J., & Schwartz, R. L.(1996).
A fully statistical approach to natural languageinterfaces.
In Proc.
of the Association for Computa-tional Linguistics.Och, F. J.
& Ney, H. (2003).
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1), 19?51.Steedman, M. (1996).
Surface Structure and Interpreta-tion.
The MIT Press.Steedman, M. (2000).
The Syntactic Process.
The MITPress.Thompson, C. A.
& Mooney, R. J.
(2002).
Acquiringword-meaning mappings for natural language inter-faces.
Journal of Artificial Intelligence Research, 18.Villavicencio, A.
(2002).
The acquisition of a unification-based generalised categorial grammar.
Ph.D. thesis,University of Cambridge.Wong, Y. W. & Mooney, R. (2006).
Learning for seman-tic parsing with statistical machine translation.
In Pro-ceedings of the Human Language Technology Confer-ence of the NAACL.Wong, Y. W. & Mooney, R. (2007).
Learning syn-chronous grammars for semantic parsing with lambdacalculus.
In Proceedings of the Association for Com-putational Linguistics.Zelle, J. M. & Mooney, R. J.
(1996).
Learning to parsedatabase queries using inductive logic programming.In Proceedings of the National Conference on Artifi-cial Intelligence.Zettlemoyer, L. S. & Collins, M. (2005).
Learning tomap sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In Pro-ceedings of the Conference on Uncertainty in ArtificialIntelligence.Zettlemoyer, L. S. & Collins, M. (2007).
Online learningof relaxed CCG grammars for parsing to logical form.In Proc.
of the Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning.Zettlemoyer, L. S. & Collins, M. (2009).
Learningcontext-dependent mappings from sentences to logicalform.
In Proceedings of The Joint Conference of theAssociation for Computational Linguistics and Inter-national Joint Conference on Natural Language Pro-cessing.1233
