IiI/I/lIlI//INatural Language Concept AnalysisVera KamphuisDept.
of Language and SpeechUniversity of NijmegenThe Netherlandsv.
kamphuis@let, kun.
nlJanos Sarbo ~Computing Science InstituteUniversity of NijmegenThe Netherlandsj anos@cs, ktm.
n lAbstractCan we do text analysis without phrase struc-ture?
We think the answer could be positive.In this paper we outline the basics of an under-lying theory which yields hierarchical structureas the result of more abstract principles relat-ing to the combinatorial properties oflinguisticunits.
We discuss the nature of these proper-ties and illustrate our model with examples.Keywords Language processing, relationalmodel.1 Introduct ionIn most mainstream approaches tonatural languagemodelling and parsing, some form of hierarchicalstructure plays a central role.
The most obviouscase in point is phrase structure.
However, whilethe latter notion has shown its theoretical relevancein many ways, practical applications based on phrasestructure description are not without problems.
Themain reason for this is the high flexibility of naturallanguage.
In performance data (i.e.
actual anguageuse), many disruptions of, and variations on stan-dard phrase structure patterns occur.
As a result,application of phrase structure-based parsers in nat-ural language processing shows only limited success.This has inspired a search for alternative methods,such as statistical based or lexicon-driven parsing.In our search for a solution to the problems men-tioned we decided to take one step back, and exam-ine the underlying nature of hierarchical structure ingeneral, and phrase structure in particular.
Our aimin doing so was to find a more principled solutionto the problems of linguistic analysis and parsing.We looked for ways to derive structural informationfrom input, and to incorporate this in a mathemat-ically well-founded theory of knowledge representa-tion.
As a result we found a level of abstractnessthat, in principle, allows language-independent mod-elling and analysis.In our approach we capitalise on the property thatthe information carriers, the lexical items, are 'will-ing' to combine.
These combinatorial propertiesare determined by inherent characteristics of lexi-ca/ items.
Hierarchical structure follows naturallyfrom the interaction of these properties, while leav-ing room for variation and flexibility in structuralpatterning.In our model of natural anguage (NL) the inputis represented as a binary relation.
This is due tothe dichotomy oflanguage, meaning that a classifica-tion of lexical items as objects and attributes can bemade (we use the term "dichotomy" in a restrictedsense: a division into two mutually exclusive parts).The two classes are interrelated, and their relationcan be dete ~rrnined merely on the basis of lexicai n-formation and some general principles, like word or-der (e.g.
SVO).
The relation between the classesis due to the principle of relatedness.
This princi-ple entails that any non-empty set of objects impliesthe existence of a non-empty set of attributes (prop-erties) it is related to, and vice versa.
Minimally,an observable entity (object) has the property of ex-istence (attribute).
This principle gives rise to arelation representing the semantics of the 'thought'described by the sentence in terms of a set of relateditems, called observations.
An observation capturesa set of objects and properties that are mutuallycharacteristic of each other.
Such a notion corre-sponds to a/ormal concept in lattice theory.
We willshow that the above relation is supported by linguis-tic considerations.Our approach to language, Natural LanguageConcept Analysis (NLCA), constitutes a linguisti-cally and mathematically based theory.
This is re-flected by the different readings of the acronym, asfollows.
NLC(:A): the analysis of concepts that playa role in natural anguage; (NL)CA: the lattice the-Karaphuis and Sarbo 205 Natural Language Concept AnalysisV.
Kamphuis and J.J. Sarbo (1998) Natural Language Concept Analysis.
In D.M.W.
Powers (ed.)
NeMLaP3/CoNLL98: NewMethods in Language Processing and Computational N tural Language Learning, ACL, pp 205-214,oretical model of formal concept analysis applied tonatural language; N(LCA): a natural transformationon language (in concrete, on functor-argument rela-tions).1.1 Re la ted  researchOur theory goes together with a movement of mod-ern formalisms in computational linguistics whichcan be characterised by a shift of emphasis from alarge, detailed syntax and simple lexicon, to a com-pact syntax and rich lexicon.
Amongst other works,one can cite HPSG (Pollard and Sag, 1994), andmost recently, a proposal by Berwick and Epstein(Berwick and Epstein, 1995).Berwick and Epstein outline a model that, in ac-cordance with Minimalist principles, does not posit"any syntactic entities at all beyond what \[is\] ab-solutely necessary for linguistic description and ex-?
planation."
The necessary machinery, as they pointout, is one based on categorial grammar (Lambek,1988).
Their argument follows from the fundamen-tal idea that natural anguages are limited to rulesspecifying how constituents can be concatenated toform larger constituents.
Berwick and Epstein intro-duce a single syntactic operation, Hierarchical Com-position (HC), for the realization of such syntacticconstraints.With respect o the above mentioned movementin natural anguage processing, we note that the en-deavour to move (almost) all information to the lex-icon can be theoretically justified.
Intuitively, prac-tical NL formalisms like HPSG can be seen as vari-ants of two-level, e.g.
attribute grammars.
The-oretically, for such a grammar, a weakly equivalentgrammar using only a single nonterminal symbol ex-ists (Franzen, 1983).
In such a grammar all struc-tural information is specified by attribute functions.These functions can be defined by the lexicon.2 A suppor t ing  theoryNatural language modelling usually assumes omeform of hierarchical structure as given.
Experienceshows that practical application of such an approachto a non-trivial subset of the language can be ahighly complex task (Aarts, 1991).
In our search fora more flexible basis we arrived at the question: Howdoes phrase and clause structure merge in naturallanguage?
It appeared that this question is relatedto a more general one: How can knowledge aboutreal world be structured?We found a philosophical background in C.S.Peirce's pragmatism (Peirce, 1931) and a mathe-matical formalisation of Peirce's ideas in R. Wille'stheory on Formal Concept Analysis (FCA) Wille,1982).
Relatedness, for example, relies on Peirce'sepistemological rgument saying that "... there isno judgment of pure observation without reasoning'(Houser and Kloesel, 1992).
This means that anobservation is always tied to "judgment"; in otherwords, in our case, observation of an object alwaysimplies the presence of an attribute, and some inter-pretation of their relation.In the FCA framework, observable world is de-scribed by a binary relation between the sets of ob-jects and attributes.
These sets give a dichotomouscharacterisation f observable entities, and togetherwith their relation formalise Peirce's universal cat-egories: firstness, secondness and thirdness.
Theseare defined as follows: "The first is that whose be-hag is simply in itself, not referring to anything norlying behind anything.
The second is that which iswhat it is by force of something to which it is second.The third is that which is what it is owing to thingsbetween which it mediates and which it brings intorelation to each other" (Houser and Kloesel, 1992).For the time being we adopt the interpretationof Lehmann and Wille (Lehmann and Wille, 1995)who state that "the object g is a \[f\]irst ... to whichthe attribute m is a \[s\]econd...".
According toLehmann and Wille, this interpretation is compati-ble with Peirce's general understanding of firstnessand secondness.In FCA, observations, orconcepts, are mathemat-ically formalised.
Traditionally, the philosophicalnotion of a concept is determined by its extensionand its intension.
The extension consists of all ele-ments (set of objects) belonging to the concept whilethe intension covers all properties (set of attributes)valid for all those elements.In the mathematical model, the triple consistingof the sets, objects (G; Gegenst~tnde) and attributes(M; Merkmale), and the relation between them (R),is called the context (we assume that G and M arefinite sets).
We say, for g E G, m E M, (g, m) ER or equivalently, (gRin), iff the object g has theattribute m.For a context he following mappings are defined:A' = {m e M I gRrn for all g e A} for A C_ G; andB' = {ge  G I gRrn fora l lme B} fo rB  C M.A (formal) concept of a context (G, M, R) is a pair(A,B) with A C G, B C M, which satisfies theconditions (i) A' = B and (ii) A = B'.Informally, A ~ is calculated from A by consideringthe elements of A and accumulating the propertiescommon to them all.
B' is calculated ually.
Wesay (A, B) is a concept if, by the above calculation,A and B mutually determine ach other.For any concepts (A1,Bt) and (A2,B2) of a con-1II11IIIIIIII11i lIIKaraphuis and Sarbo 206 Natural Language Concept AnalysisIIIIIIIIIIIItex~ the hierarchy of concepts i captured by the def-inition: (A1,B1) < (A2,B2) iffA1 C A2 (or equiva-lently, iff B1 _D B2).
When the above order relationholds, (A1,A~) is called the subconcept of (A2, A~),and (Ae,A'~) the superconcept of (A1,A'I).
The setof all concepts of a context with this order relationis called the concept lattice.3 L ingu is t i c  re la t ionsNLCA applies Wille's theory to natural anguage bythe equivalence: attributes are functors, and ob-jects are arguments.
Functor-argument relations,the manifestations of the (combinatorial) propertiesof lexical items, have various realizations on the lin-guistic level.
For example, the verb-complement re-lation is not the same as the relation of modifica-tion.
This becomes clear when we look at the op-tionality of modifiers.
In English, we cannot say, onthe basis of encountering a noun, that it needs anadjectival modifier; however, when we encounter anadjective, we do know that at some level it needsa noun because it is a semantic predicate (functor)taking an argument of which it is predicated.
In thiscase, then, there is an asymmetrical relation betweenfunctor and argument.
On the other hand, the re-lation between a verb and its complementation is asymmetrical one.In NLCA, we distinguish between two kinds of re-lations: major and minor.
These types of relationscan be recursively nested, and their sum uniquelycharacterises the input.
The first type of relation,the major relation, or predication, is a pair (p,a),where a functions as an argument to the predicatep.
A major relation may involve the distinctionbetween an action/state and its participants (sym-metrical relation: each requires the presence of theother) and between an action/state orparticipant onthe one hand and its properties on the other (asym-metrical relation, or modification: the predicate re-quires the argument of which it is predicated, but thereverse does not hold).
We call predicates of the firsttype major predicates, and predicates of the secondtype minor predicates.It is interesting to note that this distinction re-flects the difference between constituency on the onehand, and dependency on the other.
In linguistics,these two relations are often treated as (formally)equivalent alternatives.
~ In the current view, theyentail a difference in status of the units that are in-volved in the relation.
The nature of the relationin both cases is that of predication; however, in the1However, see (Fraser, 1996) for some qualifying re-marks on this topic.constituency case each part assumes the presence ofthe other, whereas in the dependency ase, the pred-icate is optional.There are Various distinguishing factors betweenmajor and minor predicates.
In English, major pred-icates (usually) relate to the noun-verb division; mi-nor predicates do not.
Major predicates are typicallyrealized by verbs; minor predicates by adjectives andadverbs.
There is never more than one major predi-cate associated with an argument; here may be sev-eral minor predicates related to the same argument.
(This reflects the possibility of having zero or moremodifiers of an action or participant.)
Both ma-jor and minor predicates can provide semantic roles,but major predicates introduce participant roles fortheir arguments; minor predicates can introduce ad-ditional roles (such as location or manner) or prop-erties of their arguments.The second type of relation, the minor relation, orqualification, distinguishes between the core contentof a linguistic expression and some qualification ofit.
At the level of an action and its participants, forexample, this qualification may relate to referentialstatus of NPs (e.g.
definite vs. indefinite article),or to tense and aspect information at clause level.Intensifying adverbs (e.g.
very, extremely, deeply)and comparative adverbs (e.g.
more and most) alsobelong to the class of qualifiers.
These examples sug-gest that qualification may also have a symmetricaland asymmetrical variant: article, tense, aspect etc.being of the first type, and intensifying and compar-ative adverbs of the second.
However, this is stillan object of further study.
In this paper we will re-strict ourselves to the distinction between qualifierand core in general.The difference between a minor predicate and aqualifier is that the latter does not introduce amean-ing that is independent of the element it qualifies.
2The presence of a qualifier of a specific type, there-fore, also signals the presence of its counterpart.Furthermore, there can be several modifiers associ-ated with an argument or predicate; typically, how-ever, there will only be a single (possibly compos-ite) quali~er.
In the case of referential information,for instance, the qualifier situates the argument orpredicate in its referential context of which there willonly be one.
In some cases different aspects of the2By contrast, a minor predicate has some aspect ofmeaning that is independent ofthe element i combineswith.
This is illustrated by the fact that minor predicatescan be used in different contexts.
For example, a prepo-sitional phrase can modify an argument (e.g.
noun) butalso a predicate (e.g.
verb).
An adjective phrase can beused as a modifier of a noun, but also in the complemen-ration of a verb.Kamphuis and Sarbo 207 Natural Language Concept Analysisqualifier can be expressed separately (such as tenseand aspect); in that case these different aspects mustbe unifiable but there cannot be more than a singlequalifier elating to the same domain.The qualifier evokes its COUnterpart; nevertheless,the semantic 'core' is also complete in itself, in thatit forms a full account of semantic relationships.Therefore it does not require realization of the quali-fier as such: cf.
the use of such bare relations in cap-tions or telegram style speech (e.g.
"Lion attackedwoman.t" ).Summing up, we distinguish between the followingrelations:?
major predication?
minor predication?
qualification.A schematic representation f these possibilities igiven in Fig.
1./Minor relation:Qualification/ \Qualifier CoreLinguistic relations\Major relation:Predication/ \Major predication Minor predication(symmetrical) (asymmetrical)/ \ / \Major Argu- Minor Argu-predicate ment(s) predicate meatFigure 1: Inventory of linguistic relations in NLCAIt is important to note that this diagram does notrepresent the hierarchical structure of sentences, orthe organisation of conceptual content within thesentence (we will come back to this below); it merelyshows the different ypes of relations that our ap-proach identifies.
These relations lie at the heartof structure formation in NL.
How phrase structureemerges as a result of their interaction is explainedin Sect.
6.As mentioned, the different relations can be recur-sively nested.
For example, at the level of argument,a modifying predicate may be added in the form ofan adjective phrase, or a qualifier may be presentin the form of a determiner.
Each element hat isadded stands in a certain relation to its counterpart,based on the type of relation that was applied.There is a potential mapping between the linguis-tic relations displayed in Fig.
1 and the hierarchicalorganisation of information structure.
For example,it is likely that the major predication relation is themost important information carrier with respect othe semantic ontent of the sentence, and that theminor predication relation reflects additional infor-mation of less importance.
This illustrates the rela-tive contribution of the different linguistic relationsto information content.
In information retrieval thiscould help to generate a concise representation f re-trieved text.
It would also be in line with the use,already mentioned, of the major predication relationin captions or telegram style speech; furthermore,it could be a possible explanation for the ability ofspeed-reading that readers may develop.The qualification gives concrete reference to allthe items involved in the predication relation, andas such is relevant for all levels.
The presence of thequalifier at all levels of representation is a matterof some importance: word order, for instance, mayalso be classified as part of the qualification relation(e.g.
in English, word order is relevant for identify-hag questions, and also in assigning thematic roles toparticipants).
The relationship between qualifiers inNLCA, and operators in the semantically based hi-erarchy of Role and Reference Grammar (Van Volin~1993) would be a potentially useful area to investi-gate.4 A f i r s t  sketch  o f  the  mode lThe distinctions made above have been incorporatedinto the NLCA-model on the basis of the abstractionof FCA: the context.
In the dyadic model of FCA, acontext allows only two kinds of entities: object andattribute.
Therefore, each lexical item has:to be clas-sifted as one these, based on its lexical type.
Typicalobjects are nouns; typical attributes are verbs (ma-jor predicates), and adjectives and adverbs (minorpredicates).
We refer to these attributes uniformlyas major attributes (involving predication).
Qual-ifiers are classified as attributes, as well.
We callthem minor attributes (involving qualification).A comment with respect o the classification oflexical items and its relation to Peirce's universalcategories i in place.
We mentioned that objectsand attributes formalise the categories firstness andsecondiaess.
Each item of these classes may evoke adifferent relation (called interpretant) depending onthe item's syntactic and semantic properties, and ingeneral, the properties of the item as a sign (Liszka,1996)..In NLCA these interpretants are instantia-tions of the linguistic relations formalising the cat-egory of thirdness.
For example, the interpretantIIIIIIIIIIKamphuis and Sarbo 208 Natural Language Concept AnalysisiiiniiiIIIIiiiicreated by a verb, an instance of a major predica-tion, may 'explain' how that verb binds its argu-ments together '~in a bundle of interlocking relation-ships" (Sowa, 1996).The surjective mapping from lexical types to thesets of the dyadic model can be defined without caus-ing confusion.
The set of lexical types defines a par-tition of L, the set of lexical items and semantic rolesinvolved in the analysis, which is further partitionedaccording to the dyadic model, yielding the sets Gand M. From G C_ L and M C L follows that thereis an embedding of the relation R C G x M in L x L.This means that any pair (g, m) E R can be definedas the unique yield of 11 and l~ (ll, 12 E L) by theassignments ll ~ g and 12 ~-r m, where ~ respectsthe mapping of lexical types.As said above, each lexical item is classified ac-cording to its type.
Furthermore, with each lexicalitem is associated a number of positions for inter-nal and external arguments, denoted as suffixes, in tand _ext, respectively, z Internal arguments containinformation regarding the item itself.
External ar-guments relate to combinatorial demands to make acomplex linguistic unit, according to the linguisticrelations described above.
We say the input is well-formed if the combinatorial demands of each lexicalitem are satisfied.The internal argument positions are filled (i.e.
as-signed) by modifiers and qualifiers, which refer todistinct domains of analysis.
For each domain holdsthat when an argument position is filled by morethan a single element, hese different elements haveto be compatible (possibly depending on the con-text).
For example, with multiple modifiers, e.g.
twoor more adjectives modifying a noun, the modifiershave to be semantically compatible in order to makea sensible construct: cf.
the tall happy girl vs. ?thetall short girl.The external arguments of a verb (major predi-cate) are determined by the verb's valency: the sub-ject is also an external argument.
These externalarguments are involved in a symmetrical relation:an object fills the external argument position of anattribute, and vice versa, the attribute fills the ex-ternal argument position of that object.The external argument of a modifier (minor pred-icate) is involved in an asymmetrical relation: anobject fills the external argument position of an at-tribute, and the attribute fills the (modifier) internalargument position of that object.The qualifier-domain of the internal argumentSin procedural terms, argument position and ar-gument correspond to formal and actual parameter,respectively.contains pecific information that relates to the typeof lexical item.
For nouns, it is information re-garding reference: specific/generic/unique ref rence;number.
For verbs, it is information regarding finite-ness/tense/aspect, etc.
Thus, when the qualifier-domain of the internal argument of both the objectand the major attribute is filled, there is explicitreference with respect o the action and the partic-ipants involved.
Since qualifiers contain a specifictype of information, they can be regarded as a syn-tactic pointer to the qualified element i self: if thisdomain of the internal argument is filled, there mustalso be an object/attribute of the type that the inter-nal argument belongs to.
In case the qualifier pre-cedes its argument, this feature is reflected in thecomputational model by introducing placeholders,called Proto-items (cf.
Sect.
6).
Proto-items canonly be introduced by qualifiers.
When the argu-ment of the qualifier is found, it replaces the Proto-item and fills the external argument position of thatqualifier.
The relations that the Proto-item is in-volved in are inherited by that argument.Besides these relations, NLCA applies a set ofgeneral principles, like word order (e.g.
SVO), in-heritance of relation and 'greedy' binding of lexicalitems.
By the latter principle, the input string, form-ing the context of each lexical item, is evaluated fromthe perspective of that item and its needs: functorstake the textually nearest arguments available, andvice versa.
In NLCA input is analysed from left toright.Summarising thus far, we have incorporated thefollowing aspects in our model:?
each linguistic unit is classified according to theobject/attribute dichotomy?
each linguistic unit has positions for internaland external argument(s)?
the internal argument positions are filled byqualifiers and modifiersthe external argument positions are filled by el-ements that are involved in the predication re-lation.5 Re la t ion  Mat r ixThe analysis of examples in NLCA is represented interms of a so-called Relation Matrix (RM).
A Rela-tion Matrix shows the relation between objects (rep-resented in rows) and attributes (columns).Conform to our definition in Sect.
4, we say a Rela-tion Matrix is well-formed if each external argumentposition of each attribute is filled (meaning that theKamphuis and Sarbo 209 Natural Language Concept Analysissemantic roles of major predicates are realized andthat all other combinatorial demands of attributeshave been fulfilled) and each object is the externalargument of some attribute.
This implies that theinput corresponding to a well-formed RM must con-sist of one or more clauses.
In this paper we focuson the case that there is only a single clause?We represent a symmetrical relation by a pair ofasymmetrical relations, and an asymmetrical rela-tion by a directed relation, called a pointer (PTR).Technically, the value of a matrix element, RM\[i,j\],is a tuple encoding a Boolean variable and a set ofPTRs.In the graphical representation the value of aBoolean variable is represented by a '+' (true) orthe empty string (false).
We may also use the nota-tion '+i' referring to the ith true-value assignment.A PTR is depicted as a directed edge?
Internal ar-gument positions of objects and attributes are dis-played to their left-hand side (there is one argumentposition for the qualifiers, and one for the modifiers);external argument positions to their right.
Emptyargument positions are omitted.If the external argument position of an object (at-tribute) is filled by an attribute (object), we assigntrue to the Boolean variable of the corresponding cellin the RM.
These variables will be used for the rep-resentation of linguistic structure.
The assignmentscan take place after the analysis is completed, or, inmost cases, during the analysis.Attributes may have more than one external argu-ment position, and each of these may be involved ina different relation.
Therefore, we use the conven-tion that external argument positions of verbs aredisplayed in separate columns.
The relation of at-tributes and their external argument positions canbe traced back in the Relation Matrix, however, inthe examples, we do not graphically represent i .6 ExamplesIt is now possible to illustrate the model by dis-cussing some examples in detail.
The language ofillustration will be English.Example  1 The door squeaks.the Minor attribute; it generates a column.
Articlesbelong to the class of qualifiers, and thereby re-quire the presence of their counterpart.
Theycreate a Proto-object that needs to be filled,of which they themselves are the internal argu-ment.
The Proto-object allocates a row in theRaM; 'the' points at its internal argument.?
the --4 Proto-object_intdoor Object; fills the Proto-object slot created by'the' and thus finds 'the' pointing at its inter-nal argument position.
'door' itself points tothe external argument position of 'the' (on thebasis of the combinatorial demands of the lat-ter), leading to the linguistic unit 'the door' andyielding a '+' in the RM.This leads to the following postulate: when-ever the external argument position of an at-tribute (except for a verb) is filled, the elementstransitively involved in the relation constitute aphrase.
'door' replaces Proto-objectdoor ~ the_ext%' in RM in cell door/thesqueaks Major attribute; its internal argument isthe present ense marker 's'; its external argu-ment is the 0-role of THEME (but see below),filled by 'door'; hence there is a pointer from'door' to this role, 4 and a '+' is put in theRM.
'squeaks' itself is the external argumentof 'door'; again a '+' in the RM.
This timethe relation involves the external argument po-sitions of both attribute and object (as opposedto before, when the internal argument positionof the object was involved).
There are no exter-nal argument positions left unfilled; this signalsa clause.Again we can formulate a postulate: there isa well-formed clause when all external argu-ment positions of a major predicate (attribute)are filled by objects, and the attribute fills theexternal argument positions of those objects,and no external argument positions of items in-volved in the major predication are left unfilled.(N.B.
aVP can be found as a subset of a clause.)?
squeak -4 door_ext?
door -4 squeak_ext (THEME)?
%'  in RM in cell door/squeakThe precise labelling of thematic roles variesacross different models.
The current role issuggested by Haegeman's THEME2 (Haegeman,1991), in Role and Reference Grammar (RRG)(Van V~\]in, 1993) it would probably be called4There could in fact also be pointer from the thirdperson present tense marker to the object or THEME-role:this can be relevant especially in head-marking languageswhere the verb carries morphemes indicating the personand number of its arguments.
Note, incidentally, thatthe latter situation is totally tmproblematic for NLCA,since it is based on the abstract linguistic relations ratherthan on concrete syntactic realizations.Kamphuis and Sarbo 210 Natural Language Concept AnalysisIIIIII!1IIIIIIIiIIIIIIIIImIImB|mm|Im|II|mart EFFECTOR.
Where Van Valin and Haege-man are in conflict, we shall at this point choosethe more general role of the two.
However, amore detailed analysis of predicates into differ-ent types (accomplishment, activity, state andachievement) with associated logical structure(as in RRG)  is a desirability for the develop-ment of the NLCA-lexicon.The Relation Matrix for this sentence is displayedin Fig.
2 below.the .s.squeak ~_~.+2 ~3Figure 2: The door squeaksNote that objects may have pointers to several ex-ternal argument positions of attributes.
This corre-sponds to saying that the object may fulfil the argu-ment role of more than one attribute.
This is in factthe case: cf.
the occurrence of multiple modifiersof a single head, but also, in the current example,'door' functioning as argument to both the article'the' and the verb 'squeaks'.
That the two have dif-ferent status is not a problem and is in fact relevant:when 'door' fulfils the external argument role of averb it is involved in the major predication, becauseit itself requires a verb, but when it fulfils the ex-ternal argument role of adjectives or of articles it isnot; it just completes their demands.There is another aspect of the model that can beillustrated on the basis of a sentence of this kind.For this purpose, let us use the sentence The moonrose.
The lexical item 'rose' is ambiguous: it caneither be the past tense of the verb 'rise', or it canbe a noun referring to a flower, s In the former case,the analysis will take place in the same way as inthe example above.
But let us look at what wouldhappen in the latter case.thel l l?kon ~ + 1J 'OSe~Figure 3: The moon rose5We disregard for the moment he possibility ofanalysing the two nouns as a compound.In this example, we can see that assigning 'rose'to the object class leaves the analysis incomplete:the object is not connected to any attribute.
Hence,under this reading, the sentence is ungrammatical(cf.
Fig.
3).Example  2 The happy girl bought some flowers.the cf.
E,x.
I.?
the ~ Proto-object_inthappy  Attribute involved in the predication rela-tion (minor predicate); generates a column.
Itsinternal argument position is not filled.
Its ex-ternal argument position needs to be filled withan element of type Object (as an adjective, itis predicated of nominal elements).
There is aProto-object present, hence there is a pointerfrom the Proto-object to the external argumentposition of the attribute (greedy binding), so wecan put a '+' in the RM (under 'happy').
Theattribute itself points to the internal argumentof the Proto-object.
Note that this leads to achain of PTRs  from 'the' via the Proto-objectto an external argument that has been filled;such a chain gives rise to inheritance of the '+'to all relations involved in it.
Therefore, therewill also be a '+' under 'the'.
This, in fact, cre-ates a nominal adjective phrase with an impliedhead (the Proto-object).Proto-object ~ happy_ext?
happy --+ Proto-object_int'+'  in RM in cell Proto-object/happy'+'  in RM in cell Proto-object/theThere is an important difference here betweenthe role and treatment of the article and the ad-jective.
Note that the nominal adjective phrasewould not be created without the article: it isthe article that supplies the Proto-object thatfunctions in the nominal adjective phrase.
Itcan do so because it belongs to the class of qual-ifiers; they require the presence of their counter-part and therefore can be said to create a Proto-object.
The adjective, on the other hand, be-longs to the class of modifiers that are involvedin the relation of minor predication.
For thisreason, they can be said to have an implicit ob-ject required to fill the place of their externalargument position?
The Proto-object generatedby the qualifier can fill this role.
Both qualifierand modifier belong to the class of internal argu-ments; however, they do not have the same sta-tus and are treated ifferently.
The qualifier cangenerate a Proto-object (or a Proto-attribute ifKamphuis and Sarbo 211 Natural Language Concept Analysisit is the qualifier of an attribute) but this Proto-item does not fulfil its external argument need:otherwise, it would be wrongly assumed thata string consisting of the qualifier only wouldbe grammatical.
Hence there is no pointer fromthe Proto-object to the external argument of thequalifier.
With the modifying adjective, exactlythe reverse situation holds.
As adjectives can beused in different ypes of contexts (e.g.
attribu-tively or predicatively), they do not create aProto-object.
However, since they are involvedin the relation of predication, their external ar-gument position can be filled by a Proto-objectgenerated by a qualifier.
They themselves then,also may point to the internal argument of thatProto-object.girl Object; replaces the Proto-object.
The ob-ject points at the external argument positionof 'the'.
There is still a phrase, but now it is afull noun phrase rather than a nominal adjec-tive phrase.
Since there is not yet a pointer tothe external argument of the noun, we still donot have a clause, only a phrase.?
'girl' replaces Proto-object- girl --~ the_extbought Major attribute; generates a column.
Itsinternal argument is filled by the feature PAST;its external arguments are AGENT and THEME.Since 'buy' is a major predicate, it is the ex-ternal argument of the object 'girl', and 'girl'points to the AGENT role.
As a result, there is a'+' in the Relation Matrix in cells ~rl/AGENTand girl/buy.
However, since only one of the ex-ternai argument positions of the transitive verbis filled, the clause is not yet complete?- buy ~ girl_ext- girl -~ buy_ext (THEME)?
'+'  in Pt23/i in cell girl/AGENT?
'+'  in RM in cell girl/buysome A quantifying pronoun which may functionas a determiner or as an independent pronoun.We can make a unified account if we treat it asan attribute that, like the article, introduces aProto-object; however, unlike with articles theProto-object now also points to the externalargument of the attribute?
As a result, thereis a '+' in the Relation Matrix in cell Proto-object/some.
This explains the possibility ofe.g.
She bought some, which, indeed, is com-plete but has an implicit object.
In this view,then, quantifying pronouns are treated as an in-termediate ype between the pure qualifier-classof articles and the class of adjectives, which doallow a Proto-object as their external argument.The Proto-object also realizes the external ar-gument THEME, causing a '+' to be placed inthe appropriate cell of the Relation Matrix.?
some -~ Proto-object_int?
Proto-object -~ some_ext?
Proto-object --, buy_ext (THEME)- '+' in RM in cell ProW-object/some?
'+' in RM in cell PrOW-object/THEME?
'+' in RM in cell Proto-object/buyflowers Object; replaces the Proto-object.
's' canbe regarded as part of the internal argument(qualifier).
Note that this does not conflict withthe fact that 'some' also is an internal argument:they are unifiable within the same domain (bothcan signify plural; together they are plural in-definite)?
'flowers' replaces Proto-objectThe Relation Matrix for this sentence is displayedin Fig.
4.the happy~.
.+1~~sflo~er t,PASTbuY+7+4 ~3 V sOmeT+5 JFigure 4: The happy girl bought some flowersThis treatment of quantifying pronouns has twoimportant advantages.
First, it does not require am-biguous lexical entries.
The same can be said fordemonstrative pronouns, numerals and other func-tion words that are ambiguous between indepen-dent and adjectival use.
Second, the use of Proto-objects makes it unnecessary tohave a rule definingnoun phrase heads as realized either by nouns, or bynumerals, quantifying pronouns, demonstrative pro-nouns etc.
In fact, this also applies to nominal ad-jective phrases: there is no need to define adjectivesas possible realizations of noun phrase heads.
Thenominal adjective phrase follows naturally from thepresence of the article (creating the Proto-object)and the adjective (combining with the Proto-object).Furthermore, this approach also accounts for the po-tential structural ambiguity of a quantifying pro-noun or a nominal adjective phrase followed by aKamphuis and Sarbo 212 Natural Language Concept Analysism|m|BmmmmmmmmmmmmIIilIIilIIIIilIIplural noun phrase, as in apposition.
(Example: 'OnMonday she got a big bunch of flowers.
The white,lilies, wiited after a mere few days.
').Going through the sentence from left to right, wesee the following structure merge:?
At word 'happy' we obtain the nominal adjec-tive phrase (+1 and, through inheritance, +2);?
At word 'girl' we obtain the noun phrase (PTRfrom 'girl' to the_ext );?
At word 'some' we obtain the clause with anindependent pronoun (+5, +6 and +r);?
At word 'flowers' we obtain the clause with'some' as determiner.As shown in these examples, the phrases andclauses can be found in the Relation Matrix.
Theconcept lattice representation is especially valuablefor the observations, which are essential for informa-tion retrieval (Sarbo and Farkas, 1995).
Informationpresent in the Relation Matrix is accessible from theconcept lattice and can be used in the explanationof the concepts and sublattices.
We note that in ourapplication of FCA a concept containing the emptyset is meaningless, because it is in conflict with re-latedness.
The concept lattice of Fig.
4 is shown inFig.
5.C3=({girl,flower(s) }, {buy(PAST)})Cl=({girl}, {the,happy, C2=({flower(s)},{buy(PAST),buy(PAST) ,AGENT} ) THEME,some } )CO=({}, {the,happy, buy(PAST),AGENT,THEME .some} )Figure 5: Concept lattice of Fig.4For example, the concept C1 denotes the observa-tion: a particular ('the') object ('girl') has a prop-erty ('happy') and is the AGENT participant of theaction 'buy(PAST)'.
The concept C3 denotes theobservation that 'girl' and 'flower(s)' are related tothe action 'buy(PAST)'.
It is interesting that theseconcepts correspond to the focus of WH-questions:(C1) Who bought (something)?
(C3) What hap-pened?
This suggests a potential correspondencebetween the linguistic device of question-formationand the information reflected in the lattice.
More-over, it exemplifies the postulate (Sarbo, 1997) thata concept lattice is an appropriate representation forwhat is referred to in artificial intelligence as 'coop-erative communication' (Grice, 1975).In addition to the individual concepts, a(sub)lattice also has information content, compara-ble to that of a clause.
In this example, C0-C1-C2-C3 represents the clause.
The use of sublattices ipotentially relevant for interpreting discourse rela-tions.7 Summary  and  conc lus ionsIn this paper we have focused on the underlyingprinciples of hierarchical structure in language.
Wehave discussed the theoretical foundations of Nat-ural Language Concept Analysis.
We have shownthat hierarchical structure, which is commonly takenas given in linguistics, emerges as the result ofmore abstract principles relating to the combinato-rial properties of linguistic units of different ypes.These properties derive from the inherent charac-teristics of lexical items and the different linguisticrelations that they can take part in.
The majorrelation, predication, has a symmetrical instantia-tion (predicate-participants) andan asymmetricalinstantiation (predicate/participant-modifier).
Theminor relation distinguishes between semantic oreand a qualification of that core.
The applicationof NLCA was illustrated on the basis of examples.Current research on NLCA focuses on an elabora-tion of the linguistic and philosophical foundationson the one hand and algorithmic implementationthe other.AcknowledgementWe are grateful to J6zsef Farkas for his pioneer-ing work and inspiration in the initial stages of thisproject.Re ferencesJan Aarts.
1991.
Intuition-based and observation-based grammars.
In K. Aijmer and B. Altenberg,editors, English Corpus Linguistics, pages 44--62.Longman, London and New York.Robert C. Berwick and Samuel D. Epstein.
1995.On the convergence of 'minimalist' syntax andcategorial grammar.
In A. Nijholt, G. Scollo,and R. Steetskamp, editors, Algebraic Methods inLanguage Processing (T WL T 10), pages 143-148,Universiteit Twente, Enschede.Helmut Franzen.
1983.
Compiler generation:From compiler descriptions to efficient compilers.Bericht nr.
83 - 20, Technische Universit~t Berlin,May.Kamphuis and Sarbo 213 Natural Language Concept AnalysisNorman M. Fraser.
1996.
Dependency Grammar.
InK.
Brown and J. Miller, editors, Concise Encyclo-pedia of Syntactic Theories, pages 71-75, Perga-mon, Oxford etc.Herbert P. Grice.
1975.
Logic and conversation.
InP.
Cole and J. Morgan, editors, Syntax and semi-otics: Speech acts, volume 3, pages 41-58, Aca-demic Press, New York.Liliane Haegeman.
1991.
Introduction to Govern-ment and Binding Theory.
Basil Blackwell, Inc.,Cambridge, MA.Nathan Houser and Christian Kloesel, editors.
1992.The Essential Peirce: Selected Philosophical Writ-ings (1867-1893).
Indiana University Press,Bloomington.Joachim Lambek.
1988.
Categorial and Categori-cal Grammars.
In R.T. Oehrle, E. Bach, andD.
Wheeler, editors, Categorial Grammars andNatural Language Structures, D. Reidel Publish-ing Company, Dordrecht-Boston.Fritz Lehmann and Rudolf Wille.
1995.
A triadicapproach to formal concept analysis.
In G. El-lis, R. Levinson, W. Rich, and J.F.
Sowa, edi-tors, Third lnt.
Con\].
on Conceptual Structures,ICCS'gS.
Springer-Verlag.James J. Liszl~.
1996.
A General Introduction tothe Semeiotic o\] Charles Sanders Peirce.
IndianaUniversity Press, Bloomington and Indianapolis.Charles S. Peirce.
1931-35.
Collected Papers.
Har-vard University Press, Cambridge.Carl Pollard and Ivan A.
Sag.
1994.
Head-drivenPhrase Structure Grammar.
The University ofChicago Press, Cambridge, MA.Janos J. Sarbo.
1997.
Building Sub-KnowledgeBases Using Concept Lattices.
The ComputerJournal, volume 39, no.
10, pages 868-875, Ox-ford University Press.Janos J. Sarbo and Jozsef I. Farkas.
1995.
Know-ledge representation a d acquisition by conceptlattices.
In Shanl Markovitch, editor, Proc.
o\] the11th Izraeli Symposium on Artificial Intelligence(ISAI'95), Hebrew University of Jerusalem, Izrael.John F. Sowa.
1996.
Processes and participants.
InP.W.
Eklund, Gerard Ellis, and Graham Mann,editors, Conceptual Structures: Knowledge Repre-sentation as Interlingua (ICCS'96), volume 1115,pages 1-22, Springer-Verlag.Robert D. Van Vz\]in~ Jr. 1993.
A synopsis ofRole and Reference Grammar.
In Van Valin, ed-itor, Advances in Role and Re\]erence Grammar,pages 1-164, John Benjamins Publishing Com-pany, Amsterdam-Philadelphia.Rudolf WiUe.
1982.
Restructuring lattice theory:an approach based on hierarchies of concepts.
InI.
Rival, editor, Ordered sets, pages 445-470.
D.Reidel Publishing Company, Dordrecht-Boston.Kamphuis and Sarbo 214 Natural Language Concept AnalysisIIIIIIIIIIIIII
