Workshop on Computational Linguistics for Literature, pages 36?44,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsAligning Bilingual Literary Works: a Pilot StudyQian Yu and Aure?lien Max and Franc?ois YvonLIMSI/CNRS and Univ.
Paris Sudrue John von Neumann F-91 403 Orsay, France{fistname.lastname}@limsi.frAbstractElectronic versions of literary works abound on the In-ternet and the rapid dissemination of electronic read-ers will make electronic books more and more com-mon.
It is often the case that literary works exist inmore than one language, suggesting that, if properlyaligned, they could be turned into useful resources formany practical applications, such as writing and lan-guage learning aids, translation studies, or data-basedmachine translation.
To be of any use, these bilin-gual works need to be aligned as precisely as possible,a notoriously difficult task.
In this paper, we revisitthe problem of sentence alignment for literary worksand explore the performance of a new, multi-pass,approach based on a combination of systems.
Ex-periments conducted on excerpts of ten masterpiecesof the French and English literature show that ourapproach significantly outperforms two open sourcetools.1 IntroductionThe alignment of bitexts, i.e.
of pairs of texts as-sumed to be mutual translations, consists in find-ing correspondences between logical units in the in-put texts.
The set of such correspondences is calledan alignment.
Depending on the logical units thatare considered, various levels of granularity for thealignment are obtained.
It is usual to align para-graphs, sentences, phrases or words (see (Wu, 2010;Tiedemann, 2011) for recent reviews).
Alignmentsare used in many fields, ranging from TranslationStudies and Computer Assisted Language Learn-ing (CALL) to Multilingual Natural Language Pro-cessing (NLP) applications (Cross-Lingual Informa-tion Retrieval, Writing Aids for Translators, Multi-lingual Terminology Extraction and Machine Trans-lation (MT)).
For all these applications, sentencealignments have to be computed.Sentence alignment is generally thought to befairly easy and many efficient sentence alignmentprograms are freely available1.
Such programs relyon two main assumptions: (i) the relative order ofsentences is the same on the two sides of the bi-text, and (ii) sentence parallelism can be identifiedusing simple surface cues.
Hypothesis (i) warrantsefficient sentence alignment algorithms based ondynamic programming techniques.
Regarding (ii),various surface similarity measures have been pro-posed: on the one hand, length-based measures(Gale and Church, 1991; Brown et al, 1991) relyon the fact that the translation of a short (resp.
long)sentence is short (resp.
long).
On the other hand,lexical matching approaches (Kay and Ro?scheisen,1993; Simard et al, 1993) identify sure anchorpoints for the alignment using bilingual dictionar-ies or surface similarities of word forms.
Length-based approaches are fast but error-prone, while lex-ical matching approaches seem to deliver more re-liable results.
Most state-of-the-art approaches useboth types of information (Langlais, 1998; Simardand Plamondon, 1998; Moore, 2002; Varga et al,2005; Braune and Fraser, 2010).In most applications, only high-confidence one-to-one sentence alignments are considered usefuland kept for subsequent processing stages.
Indeed,when the objective is to build subsentential align-1See, for instance, the Uplug toolbox which integrates sev-eral sentence alignment tools in a unified framework:http://sourceforge.net/projects/uplug/36ments (at the level of words, terms or phrases), othertypes of mappings between sentences are deemedto be either insufficiently reliable or inappropriate.As it were, the one-to-one constraint is viewed as aproxy to literalness/compositionality of the transla-tion and warrants the search of finer-grained align-ments.
However, for certain types of bitexts2, suchas literary texts, translation often departs from astraight sentence-by-sentence alignment and usingsuch a constraint can discard a significant propor-tion of the bitext.
For MT, this is just a regrettablewaste of potentially useful training material (Uszko-reit et al, 2010), all the more so as parallel liter-ary texts constitute a very large reservoir of par-allel texts online.
For other applications implyingto mine, visualize or read the actual translations intheir context (second language learning (Nerbonne,2000; Kraif and Tutin, 2011), translators training,automatic translation checking (Macklovitch, 1994),etc.
), the entire bitext has to be aligned.
Further-more, areas where the translation is only partial orapproximative need to be identified precisely.The work reported in this study aims to explorethe quality of existing sentence alignment tech-niques for literary work and to explore the usabilityof a recently proposed multiple-pass approach, espe-cially designed for recovering many-to-one pairings.In a nutshell, this approach uses sure one-to-onemappings detected in a first pass to train a discrim-inative sentence alignment system, which is thenused to align the regions which remain problem-atic.
Our experiments on the BAF corpus (Simard,1998) and on a small literary corpus consisting of tenbooks show that this approach produces high qualityalignments and also identifies the most problematicpassages better than its competitors.The rest of this paper is organized as follows:we first report the results of a pilot study aimed ataligning our corpus with existing alignment meth-ods (Section 2).
In Section 3, we briefly describe ourtwo-pass method, including some recent improve-ments, and present experimental performance on theBAF corpus.
Attempts to apply this technique to ourlarger literary corpus are reported and discussed in2Actual literary bitexts are not so easily found over the Inter-net, notably due to (i) issues related to variations in the sourcetext and (ii) issues related to the variations, over time, of thevery notion of what a translation should be like.Section 4.
We discuss further prospects and con-clude in Section 5.2 Book alignment with off-the-shelf tools2.1 A small bilingual libraryThe corpus used in this study contains a random se-lection of ten books written mostly in the 19th andin the early 20th century: five are English classicstranslated into French, and five are French classicstranslated into English.
These books and their trans-lation are freely available3 from sources such as theGutenberg project4 or wikisource5, and are repre-sentative of the kinds of collections that can be easilycollected from the Internet.
These texts have beenpreprocessed and tokenized using in-house tools,yielding word and sentence counts in Table 1.2.2 Baseline sentence alignments2.2.1 Public domain toolsBaseline alignments are computed using twoopen-source sentence alignment packages, the sen-tence alignment tool of Moore (2002)6, and Hu-nalign (Varga et al, 2005).
These two tools werechosen as representative of the current state-of-the-art in sentence alignment.
Moore?s approach im-plements a two-pass, coarse-to-fine, strategy: a firstpass, based on sentence length cues, computes afirst alignment according to the principles of length-based approaches (Brown et al, 1991; Gale andChurch, 1991).
This alignment is used to train a sim-plified version of IBM model 1 (Brown et al, 1993),which provides the alignment system with lexicalassociation scores; these scores are then used to re-fine the measure of association between sentences.This approach is primarily aimed at delivering highconfidence, one-to-one, sentence alignments to beused as training material for data-intensive MT.
Sen-tences that cannot be reliably aligned are discardedfrom the resulting alignment.3Getting access to more recent books (or their translation) isproblematic, due to copyright issues: literary works fall in thepublic domain 70 years after the death of their author.4http://www.gutenberg.org5http://wikisource.org6http://research.microsoft.com/en-us/downloads/aafd5dcf-4dcc-49b2-8a22-f7055113e656/37French side English side# sents # words # sents # wordsEnglish books and their French translationEmma, J. Austen EM 5,764 134,950 7,215 200,223Jane Eyre, C. Bronte?
JE 9,773 240,032 9,441 237,487The last of the Mohicans, F. Cooper LM 6,088 189,724 5,629 177,303Lord Jim, J. Conrad LJ 7962 175,876 7,685 162,498Vanity fair, W. Thackeray VF 14,534 395,702 12,769 372,027French books and their English translationLes confessions, J.J. Rousseau CO 9,572 324,597 8,308 318,6585 semaines en ballon, J. Verne 5S 7,250 109,268 7,894 121,231La faute de l?Abbe?
Mouret, E. Zola AM 8,604 156,514 7,481 156,692Les travailleurs de la mer, V. Hugo TM 10,331 170,015 9,613 178,427Du co?te?
de chez Swann, M. Proust SW 4,853 208,020 4,738 232,514Total 84,731 2,104,698 80,773 2,157,060Table 1: A small bilingual libraryHunalign7, with default settings, also implementsa two-pass strategy which resembles the approach ofMoore.
Their main difference is that Hunalign alsoproduces many-to-one and one-to-many alignmentlinks, which are needed to ensure that all the inputsentences appear in the final alignment.Both systems also deliver confidence measuresfor the automatic alignment: a value between 0 and1 for Moore?s tool, which can be interpreted as aposterior probability; the values delivered by Hu-nalign are less easily understood, and range from?1to some small positive real values (greater than 1).2.2.2 Evaluation metricsSentence alignment tools are usually evaluatedusing standard recall [R] and precision [P] mea-sures, combined in the F-measure [F], with respectto some manually defined gold alignment (Ve?ronisand Langlais, 2000).
These measures can be com-puted at various levels of granularity: the level ofalignment links, of sentences, of words, and of char-acters.
As gold references only specify alignmentlinks, the other references are automatically derivedin the most inclusive way.
For instance, if the refer-ence alignment links state that the pair of source sen-tences f1, f2 is aligned with target e, the referencesentence alignment will contain both (f1, e) and7ftp://ftp.mokk.bme.hu/Hunglish/src/hunalign; we haveused the version that ships with Uplug.
(f2, e); likewise, the reference word alignment willcontain all the possible word alignments betweentokens in the source and the target side.
For suchmetrics, missing the alignment of a large ?block?of sentences gets a higher penalty than missing asmall one; likewise, misaligning short sentences isless penalized than misaligning longer ones.
As aside effect, all metrics, but the more severe one, ig-nore null alignments.
Our results are therefore basedon the link-level and sentence-level F-measure, toreflect the importance of correctly predicting un-aligned sentences in our applicative scenario.2.2.3 ResultsPrevious comparisons of these alignment toolson standard benchmarks have shown that both typ-ically yield near state-of-the-art performance.
Forinstance, experiments conducted using the literarysubpart of the BAF corpus (Simard, 1998), con-sisting of a hand-checked alignment of the Frenchnovel De la Terre a` la Lune (From the Earth tothe Moon), by Jules Verne, with a slightly abridgedtranslation available from the Gutenberg project8,have yielded the results in Table 2 (Moore?s systemwas used with its default parameters, Hunalign withthe --realign option).All in all, for this specific corpus, Moore?s strat-egy delivers slightly better sentence alignments than8http://www.gutenberg.org/ebooks/8338P R F % 1-1 linksAlignment based metricsHunalign 0.51 0.60 0.55 0.77Moore 0.85 0.65 0.74 1.00Sentence based metricsHunalign 0.76 0.70 0.73 -Moore 0.98 0.62 0.76 -Table 2: Baseline alignment experimentsFigure 1: Percentage of one-to-one links and pseudo-paragraph size for various baselinesHunalign does; in particular, it is able to identify 1-to-1 links with a very high precision.2.3 Aligning a small libraryIn a first series of experiments, we simply run thetwo alignment tools on our small collection to seehowmuch of it can be aligned with a reasonable con-fidence.
The main results are reproduced in Figure 1,where we display both the number of 1-to-1 linksextracted by the baselines (as dots on the Figure), aswell as the average size of pseudo-paragraphs (seedefinition below) in French and English.
As ex-pected, less 1-to-1 links almost always imply largerblocks.As expected, these texts turn out to be ratherdifficult to align: in the best case (Swann?s way(SW)), only about 80% of the total sentences arealigned by Moore?s system; in the more problem-atic cases (Emma (EM) and Vanity Fair (VF)), morethan 50% of the book content is actually thrownaway when one only looks at Moore?s alignments.Hunalign?s results look more positive, as a signifi-cantly larger number of one-to-one correspondencesis found.
Given that this system is overall less reli-able than Moore?s approach, it might be safe to filterthese alignments and keep only the surer ones (here,keeping only links having a score greater than 0.5).The resulting number of sentences falls way belowwhat is obtained by Moore?s approach.To conclude, both systems seem to have more dif-ficulties with the literary material considered herethan with other types of texts.
In particular, theproportion of one-to-one links appears to be signif-icantly smaller than what is typically reported forother genres; note, however, that even in the worstcase, one-to-one links still account for about 50% ofthe text.
Another finding is that the alignment scoreswhich are output are not very useful: for Moore, fil-tering low scoring links has very little effect; for Hu-nalign, there is a sharp transition (around a thresholdof 0.5): below this value, filtering has little effect;above this value, filtering is too drastic, as shown onFigure 1.3 Learning sentence alignmentsIn this section, we outline the main principles ofthe approach developed in this study to improve thesentence alignments produced by our baseline tools,with the aim to salvage as many sentences as possi-ble, which implies to come up with a way for betterdetecting many-to-one and one-to-many correspon-dences.
Our starting point is the set of alignmentsdelivered by Moore?s tool.
As discussed above,these alignments have a very high precision, at theexpense of an unsatisfactory recall.
Our sentencealignment method considers these sentence pairs asbeing parallel and uses them to train a binary classi-fier for detecting parallel sentences.
Using the pre-dictions of this tool, it then attempts to align the re-maining portions of the bitext (see Figure 2).In Figure 2, Moore?s links are displayed withsolid lines; these lines delineate parallel pseudo-paragraphs in the bitexts (appearing in boxed areas),which we will try to further decompose.
Note thattwo configurations need to be distinguished: (i) oneside of a paragraph is empty: no further analysisis performed and a 0-to-many alignment is output;(ii) both sides of a paragraph are non-empty and de-fine a i-to-j alignment that will be processed by theblock alignment algorithm described below.39Figure 2: Filling alignment gaps3.1 Detecting parallelismAssuming the availability of a set of example paral-lel sentences, the first step of our approach consistsin training a function for scoring candidate align-ments.
Following (Munteanu and Marcu, 2005), wetrain a Maximum Entropy classifier9 (Rathnaparkhi,1998); in principle, many other binary classifierswould be possible here.
Our motivation for usinga maxent approach was to obtain, for each possiblepair of sentences (f ,e), a link posterior probabilityP (link|f , e).We take the sentence alignments of the first stepas positive examples.
Negative examples are artifi-cially generated as follows: for all pairs of positiveinstances (e, f) and (e?, f ?)
such that e?
immediatelyfollows e, we select the pair (e, f ?)
as a negative ex-ample.
This strategy produced a balanced corpuscontaining as many negative pairs as positive ones.However, this approach may give too much weighton the length ratio feature and it remains to be seenwhether alternative approaches are more suitable.Formally, the problem is thus to estimate a con-ditional model for deciding whether two sentencese and f should be aligned.
Denoting Y the corre-sponding binary variable, this model has the follow-9Using the implementation available from http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html.ing form:P (Y = 1|e, f) =11 + exp[?
?Kk=1 ?kFk(e, f)],where {Fk(e, f), k = 1 .
.
.K} denotes a set of fea-ture functions testing arbitrary properties of e and f ,and {?k, k = 1 .
.
.K} is the corresponding set ofparameter values.Given a set of training sentence pairs, the opti-mal values of the parameters are set by optimizingnumerically the conditional likelihood; optimizationis performed here using L-BFGS (Liu and Nocedal,1989); a Gaussian prior over the parameters is usedto ensure numerical stability of the optimization.In this study, we used the following set of featurefunctions:?
lexical features: for each pair of words10 (e, f)occurring in Ve ?
Vf , there is a correspondingfeature Fe,f which fires whenever e ?
e andf ?
f .?
length features: denoting le (resp.
lf ) thelength of the source (resp.
target) sentence,measured in number of characters, we in-clude features related to length ratio, definedas Fr(e, f) =|le?lf |max(le,lf ).
Rather than taking thenumerical value, we use a simple discretizationscheme based on 6 bins.?
cognate features: we loosely define cog-nates11 as words sharing a common prefix oflength at least 3.
This gives rise to 4 features,which are respectively activated when the num-ber of cognates in the parallel sentence is 0, 1,2, or greater than 2.?
copy features: an extreme case of similarityis when a word is copied verbatim from thesource to the target.
This happens with propernouns, dates, etc.
We again derive 4 features,depending on whether the number of identicalwords in f and e is 0, 1, 2 or greater than 2.10A word is an alphabetic string of characters, excludingpunction marks.11Cognates are words that share a similar spelling in two ormore different languages, as a result of their similar meaningand/or common etymological origin, e.g.
(English-Spanish):history - historia, harmonious - armonioso.403.2 Filling alignment gapsThe third step uses the posterior alignment proba-bilities computed in the second step to fill the gapsin the first pass alignment.
The algorithm can beglossed as follows.
Assume a bitext block compris-ing the sentences from index i to j in the sourceside of the bitext, and from k to l in the target sidesuch that sentences ei?1 (resp.
ej+1) and fk?1 (resp.el+1) are aligned12.The first case is when j < i or k > l, in whichcase we create a null alignment for fk:l or for ei:j .
Inall other situations, we compute:?i?, j?, k?, l?, i ?
i?
?
j?
?
j, k ?
k?
?
l?
?
l,ai?,j?,k?,l?
= P (Y = 1|ei?:j?
, fk?:l?)
?
?S(i?, j?, k?, l?
)where ei?:j?
is obtained by concatenation of all thesentences in the range [i?:j?
], and S(i, j, k, l) = (j ?i+1)(l?k+1)?1 is proportional to the block size.The factor ?S(i?, j?, k?, l?)
aims at penalizing largeblocks, which, for the sentence-based metrics, yieldmuch more errors than the small ones.
This strategyimplies to compute O(|j ?
i + 1|2 ?
|k ?
l + 1|2)probabilities, which, given the typical size of theseblocks (see above), can be performed very quickly.These values are then iteratively visited by de-creasing order in a greedy fashion.
The top-scoringblock i?
: j?, k?
: l?
is retained in the final alignment;all overlapping blocks are subsequently deleted fromthe list and the next best entry is then considered.This process continues until all remaining blocksimply null alignments, in which case these n ?
0 or0 ?
n alignments are also included in our solution.This process is illustrated in Figure 3: assumingthat the best matching link is f2-e2, we delete allthe links that include f2 or e2, as well as links thatwould imply a reordering of sentences, meaning thatwe also delete links such as f1-e3.3.3 ExperimentsIn this section, we report the results of experimentsrun using again Jules Verne?s book from the BAFcorpus.
Figures are reported in Table 3 where wecontrast our approach with two simple baselines:(i) keep only Moore?s links; (ii) complete Moore?slinks with one single many-to-many alignment for12We enclose the source and target texts between begin andend markers to enforce alignment of the first and last sentences.Figure 3: Greedy alignment searchP R F(maxent) (all) (all) (all)link basedMoore only - 0.85 0.65 0.74Moore+all links - 0.78 0.75 0.76Maxent, ?
= 0 0.44 0.74 0.81 0.77Maxent, ?
= 0.06 0.42 0.72 0.82 0.77sentence basedMoore only - 0.98 0.62 0.76Moore+all links - 0.61 0.88 0.72Maxent, ?
= 0 0.80 0.93 0.80 0.86Maxent, ?
= 0.06 0.91 0.97 0.79 0.87Table 3: Performance of maxent-based alignmentseach block.
For the maxent-based approach, we alsoreport the precision on just those links that are notpredicted by Moore.
A more complete set of experi-ments conducted with other portions of the BAF arereported elsewhere (Yu et al, 2012) and have shownto deliver state-of-the-art results.As expected, complementing the very accurateprediction of Moore?s systems with our links sig-nificantly boosts the sentence-based alignment per-formance: recall rises from 0.62 to 0.80 for ?
= 0,which has a clear effect on the corresponding F-measure (from 0.76 to 0.86).
The performance dif-ferences with the default strategy of keeping thoseblocks unsegmented are also very clear.
Sentence-wise, maxent-based alignments are also quite pre-cise, especially when the value of ?
is chosen withcare (P=0.91 for ?=0.06); however, this optimiza-tion has a very small overall effect, given that only alimited number of alignment links are actually com-puted by the maxent classifier.414 Sentence alignment in the real worldIn this section, we analyze the performance obtainedwith our combined system, using excerpts of oursmall corpus as test set.
For this experiment, thefirst two to three hundreds sentences in each book,corresponding to approximately two chapters, weremanually aligned (by one annotator), using the sameguidelines that were used for annotating the BAFcorpus.
Except for two books (EM and VF), produc-ing these manual alignments was found to be quitestraightforward.
Results are in Table 4.A first comment is that both baselines are signifi-cantly outperformed by our algorithm for almost allconditions and books.
For several books (LM, AM,SW), the obtained sentence alignments are almostas precise as those predicted by Moore and have amuch higher recall, resulting in very good overallalignments.
The situation is, of course, much lesssatisfactory for other books (EM, VF, 5S).
All in all,our method salvages many useful sentence pairs thatwould otherwise be left unaligned.Moore?s method remains remarkably accuratethroughout the whole collection, even for the mostdifficult books.
It also outputs a significant propor-tion of wrong links, which, for lack of reliable confi-dence estimators, are difficult to spot and contributeto introduce noise into the maxent training set.The variation of performance can mostly be at-tributed to idiosyncrasies in the translation.
For in-stance, Emma (EM) seems very difficult to align,which can be attributed to the use of an old transla-tion dating back to 1910 (by P. de Puliga), and whichoften looks more like an adaptation than a transla-tion.
Some passages even question the possibility ofproducing any sensible (human) alignment betweensource and target13:(en) Her sister, though comparatively but little removed bymatrimony, being settled in London, only sixteen miles off,was much beyond her daily reach; and many a long Octoberand November evening must be struggled through at Hart-field, before Christmas brought the next visit from Isabellaand her husband, and their little children, to fill the house,and give her pleasant society again.
(fr) La s?ur d?Emma habitait Londres depuis son mariage,c?est-a`-dire, en re?alite?, a` peu de distance; elle se trouvait13In this excerpt, in addition to several approximations, theend of the last sentence (and their children...) is not translatedin French.ne?anmoins hors de sa porte?e journalie`re, et bien des longuessoire?es d?automne devraient e?tre passe?es solitairement a`Hartfield avant que Noe?l n?amena?t la visite d?Isabelle et deson mari.Les confessions (CO) is much most faithful to thecontent, yet, the translator has significantly departedfrom Rousseau?s style14, mostly made up of shortsentences, and it is often the case that several Frenchsentences align with one single English sentence,which is detrimental to Moore, and by ricochet, tothe quality of maxent predictions.
A typical excerpt:(fr) Pendant deux ans entiers je ne fus ni te?moin ni victimed?un sentiment violent.
Tout nourrissait dans mon coeur lesdispositions qu?il rec?ut de la nature.
(en) Everything contributed to strengthen those propensitieswhich nature had implanted in my breast, and during thetwo years I was neither the victim nor witness of any violentemotions.The same goes for Thackeray (VF), with a lot of re-structurations of the sentences as demonstrated bythe uneven number of sentences on both sides of thebitext.
Lord Jim (LJ) poses another type of diffi-culty: approximately 100 sentences are missing onthe French side, the rest of the text being fairly paral-lel (more than 82% of the reference links are actually1-to-1).
Du co?te?
de chez Swann (SW) represents theother extreme of the spectrum, where the translationsticks as much as possible to the very peculiar styleof Proust: nearly 90% of the reference alignmentsare 1-to-1, which explains the very good F-measurefor this book.It is difficult to analyze more precisely our er-rors; however, a fairly typical pattern is the infer-ence of a 1-to-1 link rather than a 2-to-1 link madeup of a short and a long sentence.
An example fromHugo (TM), where our approach prefers to leavethe second English sentence unaligned, even thoughthe corresponding segment (un enfant...) is the inFrench sentence:(fr) Dans tout le tronc?on de route qui se?pare la premie`re tourde la seconde tour, il n?y avait que trois passants, un enfant,un homme et une femme.
(en) Throughout that portion of the highway which separatesthe first from the second tower, only three foot-passengerscould be seen.
These were a child, a man, and a woman.A possible walk around for this problem would beto also add a penalty for null alignments.14Compare the number of sentences in Table 1.42Moore Hunalign Moore+maxentlinks P R F links F S 6= 0 S = 0 P R Ffr en links link basedEM 160 217 164 84 0.76 0.39 0.52 173 0.43 72 10 0.52 0.53 0.52JE 229 205 174 104 0.86 0.51 0.64 198 0.40 95 5 0.64 0.75 0.69LM 232 205 197 153 0.97 0.76 0.85 203 0.63 64 2 0.79 0.87 0.83LJ 580 682 515 403 0.94 0.73 0.82 616 0.60 155 15 0.82 0.81 0.76VF 321 248 219 129 0.92 0.54 0.68 251 0.39 133 3 0.58 0.70 0.63CO 326 236 213 104 0.86 0.42 0.56 256 0.28 135 3 0.62 0.70 0.665S 182 201 153 107 0.76 0.53 0.62 165 0.52 72 10 0.60 0.74 0.66AM 258 226 222 179 1.00 0.81 0.90 222 0.71 55 0 0.88 0.93 0.90TM 404 388 358 284 0.89 0.71 0.79 374 0.69 86 16 0.79 0.85 0.82SW 492 495 463 431 0.94 0.87 0.90 474 0.80 59 9 0.85 0.92 0.88fr en links sentence basedEM 160 217 206 84 0.85 0.34 0.49 199 0.60 124 0 0.62 0.63 0.62JE 229 205 270 104 0.92 0.36 0.52 235 0.60 125 0 0.90 0.76 0.82LM 232 205 238 153 0.99 0.64 0.78 234 0.79 62 0 0.97 0.88 0.92LJ 580 682 645 403 0.96 0.60 0.74 625 0.78 212 0 0.85 0.81 0.83VF 321 248 363 129 0.98 0.35 0.52 318 0.62 163 0 0.88 0.71 0.79CO 326 236 380 104 0.94 0.26 0.41 306 0.48 226 0 0.88 0.76 0.825S 182 201 260 107 0.98 0.40 0.57 224 0.70 81 0 0.93 0.67 0.78AM 258 226 264 179 1.00 0.68 0.81 262 0.84 72 0 0.98 0.94 0.96TM 404 388 445 284 0.96 0.61 0.75 418 0.82 134 0 0.93 0.87 0.90SW 492 495 532 431 0.99 0.80 0.88 512 0.88 55 0 0.99 0.90 0.94Table 4: Evaluating alignment systems on a sample of ?real-world?
booksFor each book, we report the number of French and English test sentences, the number of reference links and standard performancemeasures.
For the maxent approach, we also report separately the number of empty (S = 0) and non-empty (S 6= 0) paragraphs.5 Conclusions and future workIn this paper, we have presented a novel two-pass ap-proach aimed at improving existing sentence align-ment methods in contexts where (i) all sentencesneed to be aligned and/or (ii) sentence alignmentconfidence need to be computed.
By running ex-periments with several variants of this approach, wehave been able to show that it was able to signif-icantly improve the bare results obtained with thesole Moore alignment system.
Our study showsthat the problem of sentence alignment for literarytexts is far from being solved and additional workis needed to obtain alignments that could be used inreal applications, such as bilingual reading aids.The maxent-based approach proposed here is thusonly a first step, and we intend to explore variousextensions: an obvious way to go is to use moreresources (larger training corpora, bilingual dictio-naries, etc.)
and add more features, such as part-of-speech, lemmas, or alignment features as was donein (Munteanu and Marcu, 2005).
We also plan toprovide a much tighter integration with Moore?s al-gorithm, which already computes such alignments,so as to avoid having to recompute them.
Finally,the greedy approach to link selection can easily bereplaced with an exact search based on dynamic pro-gramming techniques, including dependencies withthe left and right alignment links.Regarding applications, a next step will be to pro-duce and evaluate sentence alignments for a muchlarger and more diverse set of books, comprisingmore than 100 novels, containing books in 7 lan-guages (French, English, Spanish, Italian, German,Russian, Portuguese) from various origins.
Mostwere collected on the Internet from Gutenberg, wik-isource and GoogleBooks15, and some were col-lected in the course of the Carmel project (Kraif etal., 2007).
A number of these books are translatedin more than one language, and some are raw OCRoutputs and have not been cleaned from errors.AcknowledgmentsThis work has been partly funded through the?Google Digital Humanities Award?
program.15http://books.google.com43ReferencesFabienne Braune and Alexander Fraser.
2010.
Im-proved unsupervised sentence alignment for symmet-rical and asymmetrical parallel corpora.
In Coling2010: Posters, pages 81?89, Beijing, China.
Coling2010 Organizing Committee.Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer.1991.
Aligning sentences in parallel corpora.
In Pro-ceedings of the 29th annual meeting on Associationfor Computational Linguistics, 1991, Berkeley, Cali-fornia, pages 169?176.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.William A. Gale and Kenneth W. Church.
1991.
A pro-gram for aligning sentences in bilingual corpora.
InProceedings of the 29th annual meeting of the Associ-ation for Computational Linguistics, pages 177?184,Berkeley, California.Martin Kay and Martin Ro?scheisen.
1993.
Text-translation alignement.
Computational Linguistics,19(1):121?142.Olivier Kraif and Agne`s Tutin.
2011.
Using a bilingualannotated corpus as a writing aid: An application foracademic writing for efl users.
In In Natalie Ku?bler(Ed.
), editor, Corpora, Language, Teaching, and Re-sources: From Theory to Practice.
Selected papersfrom TaLC7, the 7th Conference of Teaching and Lan-guage Corpora.
Peter Lang, Bruxelles.Olivier Kraif, Marc El-Be`ze, Re?gis Meyer, and ClaudeRichard.
2007.
Le corpus Carmel: un corpus multi-lingue de re?cits de voyages.
In Proceedings of Teach-ing and Language Corpora : TaLC?200, Paris.Philippe Langlais.
1998.
A System to Align Com-plex Bilingual Corpora.
Technical report, CTT, KTH,Stockholm, Sweden, Sept.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical Programming, 45:503?528.Elliot Macklovitch.
1994.
Using bi-textual alignment fortranslation validation: the TransCheck system.
In Pro-ceedings of the First Conference of the Association forMachine Translation in the Americas (AMTA), pages157?168, Columbia.Robert C. Moore.
2002.
Fast and accurate sen-tence alignment of bilingual corpora.
In Stephen D.Richardson, editor, Proceedings of the annual meet-ing of tha Association for Machine Translation inthe Americas (AMTA?02), Lecture Notes in ComputerScience 2499, pages 135?144, Tiburon, CA, USA.Springer Verlag.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguistics,31(4):477?504.John Nerbonne, 2000.
Parallel Texts in Computer-Assisted Language Learning, chapter 15, pages 354?369.
Text Speech and Language Technology Series.Kluwer Academic Publishers.Ardwait Rathnaparkhi.
1998.
Maximum Entropy Mod-els for Natural Language Ambiguity Resolution.
Ph.D.thesis, University of Pennsylvania.Michel Simard and Pierre Plamondon.
1998.
Bilingualsentence alignment: Balancing robustness and accu-racy.
Machine Translation, 13(1):59?80.Michel Simard, George F. Foster, and Pierre Isabelle.1993.
Using cognates to align sentences in bilingualcorpora.
In Ann Gawman, Evelyn Kidd, and Per-A?ke Larson, editors, Proceedings of the 1993 Confer-ence of the Centre for Advanced Studies on Collabora-tive Research, October 24-28, 1993, Toronto, Ontario,Canada, 2 Volume, pages 1071?1082.Michel Simard.
1998.
The BAF: a corpus of English-French bitext.
In First International Conference onLanguage Resources and Evaluation, volume 1, pages489?494, Granada, Spain.Jo?rg Tiedemann.
2011.
Bitext Alignment.
Number 14in Synthesis Lectures on Human Language Technolo-gies, Graeme Hirst (ed).
Morgan & Claypool Publish-ers.Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, andMoshe Dubiner.
2010.
Large scale parallel documentmining for machine translation.
In Proceedings ofthe 23rd International Conference on ComputationalLinguistics, COLING ?10, pages 1101?1109, Beijing,China.Da?niel Varga, La?szlo?
Ne?meth, Pe?ter Hala?csy, Andra?s Ko-rnai, Viktor Tro?n, and Viktor Nagy.
2005.
Parallel cor-pora for medium density languages.
In Proceedings ofRANLP 2005, pages 590?596, Borovets, Bulgaria.Jean Ve?ronis and Philippe Langlais.
2000.
Evaluationof Parallel Text Alignment Systems.
In Jean Ve?ronis,editor, Parallel Text Processing, Text Speech and Lan-guage Technology Series, chapter X, pages 369?388.Kluwer Academic Publishers.Dekai Wu.
2010.
Alignment.
In Nitin Indurkhyaand Fred Damerau, editors, CRC Handbook of Natu-ral Language Processing, number 16, pages 367?408.CRC Press.Qian Yu, Aure?lien Max, and Franc?ois Yvon.
2012.Revisiting sentence alignment algorithms for align-ment visualization and evaluation.
In Proceedings ofthe Language Resource and Evaluation Conference(LREC), Istambul, Turkey.44
