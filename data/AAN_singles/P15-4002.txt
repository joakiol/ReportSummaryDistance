Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 7?12,Beijing, China, July 26-31, 2015. c?2015 ACL and AFNLPIMI ?AMultilingual Semantic Annotation EnvironmentFrancis Bond, Lu?s Morgado da Costa and Tu?n Anh L?Linguistics and Multilingual StudiesNanyang Technological University, Singapore{bond@ieee.org, luis.passos.morgado@gmail.com, tuananh.ke@gmail.com}AbstractSemantic annotated parallel corpora,though rare, play an increasingly impor-tant role in natural language processing.These corpora provide valuable data forcomputational tasks like sense-basedmachine translation and word sensedisambiguation, but also to contrastivelinguistics and translation studies.
Inthis paper we present the ongoing devel-opment of a web-based corpus semanticannotation environment that uses the OpenMultilingual Wordnet (Bond and Foster,2013) as a sense inventory.
The systemincludes interfaces to help coordinatingthe annotation project and a corpus brows-ing interface designed specifically to meetthe needs of a semantically annotatedcorpus.
The tool was designed to buildthe NTU-Multilingual Corpus (Tan andBond, 2012).
For the past six years, ourtools have been tested and developed inparallel with the semantic annotation of aportion of this corpus in Chinese, English,Japanese and Indonesian.
The annotationsystem is released under an open sourcelicense (MIT).1 IntroductionPlain text parallel corpora are relatively widelyavailable andwidely used in NLP, such asmachinetranslation system development (Koehn, 2005,e.g., ).
In contrast, there are very few parallel sensetagged corpora due to the expense of tagging thecorpora and creating the sense inventories in mul-tiple languages.
The one exception is the trans-lations of English SemCor (Landes et al., 1998)for Italian (Bentivogli and Pianta, 2005), Roma-nian (Lupu et al., 2005) and Japanese (Bond et al.,2012).
Even for this corpus, not all of the origi-nal English texts have been translated and tagged,and not all words are tagged in the translated text(typically only those with a corresponding Englishsense).In this paper we present IMI, a web-based mul-tilingual semantic annotation system designed forthe task of sense annotation.
The main goals ofits design were to decrease the cost of productionof these resources by optimizing the speed of tag-ging, and to facilitate the management of this kindof project.
To accomplish this, we aimed at devel-oping a simple and intuitive web-based system thatallows parallel tagging by many users at a time,optimized for speed by requiring minimum inputfrom the annotators.We centered our development around the an-notation of the NTU-Multilingual Corpus (NTU-MC: Tan and Bond, 2012).
The NTU-MC isan open multilingual parallel corpus originally de-signed to include many layers of syntactic and se-mantic annotation.
We selected a portion of thiscorpus based on 7,093 sentences of English, total-ing 22,762 sentences of Chinese, Japanese and In-donesian parallel text.
A series of undergraduatelinguistics students were trained on the tool andannotated the corpus over several years.
They alsooffered extensive qualitative and quantitative feed-back on the usage of our system.The remainder of this paper is arranged as fol-lows.
In Section 2 we introduce related work.
Sec-tion 3 describes the main functionality of our sys-tem then we finish with Section 4, which summa-rizes and discusses our current and future work.2 Related WorkIn this sectionwe introduce the corpus (NTU-MC),the sense inventory (OMW), and a brief overviewof currently available tools.72.1 The NTU-Multilingual Corpus(NTU-MC)The NTU-MC (Tan and Bond, 2012) has dataavailable for eight languages from seven languagefamilies (Arabic, Chinese, English, Indonesian,Japanese, Korean, Vietnamese and Thai), dis-tributed across four domains (story, essay, news,and tourism).
The corpus started off with mono-lingual part-of-speech (POS) annotation and cross-lingual linking of sentences.
We are extending itto includemonolingual sense annotation and cross-lingual word and concept alignments (Bond et al.,2013).
Out of the available languages, Chinese,English, Japanese and Indonesian were chosen forfurther processing and annotation (due to the avail-ability of lexical and human resources).
As partof the annotation, we are also expanding the senseand concept inventory of the wordnets: PrincetonWordnet (PWN: Fellbaum, 1998), the JapaneseWordnet (Isahara et al., 2008), the Chinese OpenWordnet (Wang and Bond, 2013) and the Word-net Bahasa (Nurril Hirfana et al.
2011) throughthe Open Multingual Wordnet (Bond and Foster,2013).2.2 The Open Multilingual WordnetThe task of semantic annotating a corpus involvesthe manual (and often automated) disambiguationof words using lexical semantic resources ?
select-ing, for each word, the best match in a pool ofavailable concepts.
Among this type of resources,the PWNhas, perhaps, attained the greatest visibil-ity.
As a resource, a wordnet is simply a huge netof concepts, senses and definitions linked throughmany different types of relations.
Because of pop-ularity and confirmed utility, many projects havedeveloped wordnets for different languages.The Open Multilingual Wordent (OMW) (Bondand Foster, 2013) is an open source multilin-gual resource that combinesmany individual open-source wordnet projects, along with data extractedfromWiktionary and theUnicodeCommonLocaleData Repository.
It contains over 2 million sensesdistributed over more than 150 languages, linkedthrough PWN.
Browsing can be done monolin-gual or multilingually, and it incorporates a full-fledged wordnet editing system which our systemuses (OMWEdit: da Costa and Bond, 2015).2.3 Other Available SystemsThere are many text annotation tools available forresearch (e.g., Stenetorp et al., 2012).
However,sense annotation has some features that differ frommost common annotation tasks (such asNE or POSannotation).
In particular, the number of tags, andthe information associated with each tag is verylarge.
Sense tagging for English using the PWN,for example, when unrestricted, defaults at overa hundred thousand possible tags to chose from:even constrained by the lemma, there may be over40 tags and the set of tags will very from lemma tolemma.There are only a few annotation tools designedspecifically for sense annotation.
We were ableto find the following: the tools to tag the HinokiCorpus (Bond et al., 2008), for Japanese, and theSense Annotation Tool for the American NationalCorpus (SATANiC: Passonneau et al., 2009), forEnglish.
Both of these tools were developed to beused in a monolingual environment, and have notbeen released.The only open source tool that we could findwasChooser (Koeva et al., 2008), a multi-task annota-tion tool that was used to tag the Bulgarian SenseTagged Corpus (Koeva et al., 2006).
This tool isopen source, language independent and is capableof integrating a wordnet as a sense inventory.
Un-fortunately, it was not designed to be a web-servicewhich means it is difficult to coordinate the workof multiple users.3 System Overview and ArchitectureGiven the scenario of available systems, we de-cided we had enough motivation to start the de-velopment of a new Semantic Annotation Environ-ment (IMI).Because a large part of sense-tagging is addingnew senses to the inventory, we integrated IMIwiththe existing tools for editing and displaying theOpen Multilingual Wordnet.
This integration wasdone mainly through the development of a sin-gle web-based environment, with a common login,and API communications between interfaces.
Wealso designed a custom mode to display OMW re-sults in a condensed way.Sharing a common loginsystem allows our annotators to access the OMWwordnet editing mode (right hand of Figure 1) sothat, when needed, annotators can add new sensesand concepts to fit the data in the corpus.Our system is written in Python and uses SQLite8Figure 1: Sequential/Textual Tagger Interfaceto store the data.
It is tested on Firefox, Chromeand Safari browsers.
In the remainder of this sec-tion we discuss its main functionality.13.1 The Annotation InterfacesThe sequential/textual tagger (Figure 1) was de-signed for concept-by-concept sequential tagging.It shows a short context around the sentence cur-rently being tagged.
Clicking a word generates anautomated query in the OMW frame (on the rightof Figure 1).As it is costly to remember the set of senses foreach word, we normally tag with a lexical/targetedtagger (Figure 2 displays only the left side of thistagging interface, as the OMW frame is identicalto that of Figure 1).
Querying the OMW with thistagger is very similar to the description above.
Themain difference of this interface is that it focuseson a single lexical unit across the corpus.
In theexample provided in Figure 2, every occurrence ofthe lemma wire is tagged at the same time.
For fre-quent words, the number of results displayed canbe restricted.
In this interface, only the sentencewhere the word occurs is provided as context, buta larger context can also be accessed by clickingon the sentence ID.
Since the concept inventoryis the same for the full list of words to be tagged,time is saved by keeping the concepts fresh in theannotator?s mind, and quality is ensured by com-1The annotation interface software and corpora are avail-able from the NTU-MC page: <http://compling.hss.ntu.edu.sg/ntumc/>.paring different usages of different senses at thesame time.Figure 2: Targeted/Lexical TaggerIn both tagging interfaces, a tag is selectedamong an array of radio buttons displayed next tothe words being tagged.
Besides the numerical op-tions that match the results retrieved by the OMW,the interface also allows tagging with a set of metatags for named entities and to flag other issues.
Weuse a similar set to that of Bond et al.
(2013).
Withevery tag, a comment field is provided as an op-tional field, where annotators can leave notes ordescribe errors.9Missing senses are one of the major problemsduring the semantic annotation.
We overcome thisby integrating the wordnet editing interface pro-vided by the OMW.
Depending on the annotationtask at hands, the annotation of a corpus can bedone in parallel with the expansion of the respec-tive wordnet?s concept and sense inventory.A third tagging interface (not shown) allowsalso the direct manipulation of the corpus struc-ture.
Its major features include creating, deletingand editing sentences, words and concepts.
It istoo generalized to be used as an efficient tagger,but it is useful to correct POS tags, tokenization er-rors and occasional spelling mistakes.
It can alsobe used to correct or create complex concept struc-tures of multi-word expressions, that could not beautomatically identified.The minimal input required by our interfaces (inthe typical case, just clicking a radio button), espe-cially the lexical tagger, ensures time isn?t wastedwith complex interfaces.
It also guarantees thatthrough the automated linking of the databases,we avoid typos and similar noise in the produceddata.
An earlier version allowed annotators to tagdirectly with synset IDs, but it turned out that itwas very common for the ID to be mangled insome way, so we now only allow entering a synsetthrough the linking to the OMW.3.2 Annotation AgreementIMI also includes a tool to measure inter-annotatoragreement (Figure 3).
Up to four annotations canbe compared, for any section of the corpus.
Thetool also calculates the majority tag (MajTag).
Av-erage agreements scores are then computed be-tween annotators and between annotators and themajority tag.
Results are displayed by sentence andfor the selected portion (e.g.
the entire corpus).Agreement with the MajTag is color coded foreach annotation so that the annotators can quicklyspot disagreements.
The interface provides quickaccess to database editing for all taggers, and to theOMW editing tools.
The elected MajTag can alsobe automatically propagated as the final tag for ev-ery instance.For some texts up to three annotators have beenused, with one being a research assistant and twobeing students in a semantics class.
These studentsonly had a half hour of training, and used the se-quential tagger to tag around 250 concepts each.The average inter-annotator agreement was 67.5%.Tagging speed was around 60 concepts/hour (selfreported time).
Note that roughly 25% of the po-tential concepts were pre-marked as x: entries suchas preposition in, which should only be tagged onthe very rare cases it is an adjective (This is very inthis year or noun (I live in Lafayette, IN).
Becausethe students were minimally trained (and not allhighlymotivated) we expected a low agreement.
Iftwo out of three annotators agreed then the wordswere tagged with the majority tag.
Where all threeannotators disagreed the students were required todiscuss and re-tag those entries, and submit a re-port on them.
An expert (the first author) thenread (andmarked) all the reports and fixed any tagswhere he disagreed with their proposed solution.Adjudicating and marking the reports takes about30 minutes each, with some difficult to fix prob-lems left for later.
As a result of this process, allwords have been seen by multiple annotators, andall hard ones by an expert (and our students havea much better understanding of the issues in repre-senting word meaning using a fixed sense inven-tory)For most texts, we only have enough funding topay for a single annotator.
Targetted tagging (an-notating by word type) is known to be more accu-rate (Langone et al., 2004; Bond et al., 2008) andwe use this for the single annotator.
We expectto catch errors when we compare the annotationsacross languages: the annotation of the translationcan serve as another annotator (although of coursenot all concepts match across languages).3.3 JournalingWe take advantage of the relational database anduse SQL triggers to keep track of every committedchange, time-stamping and recording the annota-tor on every commit (true for both scripted and hu-manmanipulated data).
The system requires goingthrough a login system before granting access tothe tools, hence permitting a detailed yet automaticjournaling system.
A detailed and tractable historyof the annotation is available to control both thework-flow and check the quality of annotation.
Wecan export the data into a variety of formats, suchas RDF compatible XML and plain text triples.3.4 Corpus Search InterfaceSnapshots of the corpus are made availablethrough an online corpus look up (Figure 4: avail-able here: <http://compling.hss.ntu.edu.sg/ntumc/cgi-bin/showcorpus.cgi>).
Thissearch tool can query the corpus by concept key,10Figure 3: Inter-annotator Agreement ReportsFigure 4: Corpus Search Interface (results for the regular expression ?multi*?
as concept lemma, usingsentence ids to restrict the search to the Kyoto News Corpus, in bitext mode for Mandarin Chinese)concept lemma, word, lemma, sentence id andPOS, as well as any combination of these fields.Mousing over a word shows its lemma, pos, senseand annotators?
comments (if any), clicking on aword pops up more information about the lemma,pos and sense (such as definitions) that can beclicked for even more information.
Further, itis possible to see aligned sentences (for as manylanguages as selected), and color coded sentimentscores using two freely available sentiment lexi-cons, the SentiWordNet (Baccianella et al., 2010)and the ML-SentiCon (Cruz et al., 2014) (individ-ually or intersected).
Further improvements willallow highlighting cross-lingual word and conceptalignments (inspired by Nara: Song and Bond,2009).4 Summary and Future WorkWe have described the main interfaces and func-tionality of IMI.
It has undergone almost six yearsof development, and is now a mature annotationplatform.
The improvement of its interfaces andfunctionality have not only greatly boosted thespeed of the NTU-MC annotation, but has alsogreatly facilitated its coordination - making it eas-ier to maintain both consistency and quality of thecorpus.In the near future we intend to:?
refine the cross-lingual word and conceptalignment tool (not shown here)?
develop a reporting interface, where theproject coordinators can easily review thehistory of changes committed to the corpusdatabase?
add a simple corpus import tool for addingnew texts in different languages?
further develop the corpus search interface,to allow highlighting cross-lingual word andconcept links?
implement more automated consistencychecks (e.g.
match lemmas of words with11the lemmas of concepts, verify that conceptlemmas are still senses of the concept used totag a word, etc.)?
improve graphical coherence, as differentparts of the toolkit have originally been devel-oped separately, as a whole, our system cur-rently lacks graphical coherenceWe hope that the open release of our system canmotivate other projects to embrace semantic anno-tation projects, especially projects that are less ori-ented towards development of systems.
We wouldlike every wordnet to be accompanied by a sense-tagged corpus!AcknowledgmentsThis research was supported in part by the MOETier 2 grant That?s what you meant: a Rich Rep-resentation for Manipulation of Meaning (MOEARC41/13).
We would also like to thank our an-notators for their hard work and patience duringthis system?s development.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani.2010.
Sentiwordnet 3.0: An enhanced lexical resource forsentiment analysis and opinionmining.
In Nicoletta Calzo-lari (Conference Chair), Khalid Choukri, Bente Maegaard,JosephMariani, Jan Odijk, Stelios Piperidis, Mike Rosner,and Daniel Tapias, editors, Proceedings of the Seventh In-ternational Conference on Language Resources and Eval-uation (LREC?10).
European Language Resources Asso-ciation (ELRA), Valletta, Malta.Luisa Bentivogli and Emanuele Pianta.
2005.
Exploiting par-allel texts in the creation of multilingual semantically an-notated resources: the multisemcor corpus.
Natural Lan-guage Engineering, 11(3):247?261.Francis Bond, Timothy Baldwin, Richard Fothergill, andKiy-otaka Uchimoto.
2012.
Japanese SemCor: A sense-taggedcorpus of Japanese.
In Proceedings of the 6th GlobalWordNet Conference (GWC 2012), pages 56?63.
Matsue.Francis Bond and Ryan Foster.
2013.
Linking and extend-ing an open multilingual wordnet.
In 51st Annual Meetingof the Association for Computational Linguistics: ACL-2013, pages 1352?1362.
Sofia.
URL http://aclweb.org/anthology/P13-1133.Francis Bond, Sanae Fujita, and Takaaki Tanaka.
2008.
TheHinoki syntactic and semantic treebank of Japanese.
Lan-guage Resources and Evaluation, 42(2):243?251.
URLhttp://dx.doi.org/10.1007/s10579-008-9062-z,(Re-issue of DOI 10.1007/s10579-007-9036-6 as Springerlost the Japanese text).Francis Bond, Shan Wang, Eshley Huini Gao, Hazel ShuwenMok, and Jeanette Yiwen Tan.
2013.
Developing parallelsense-tagged corpora with wordnets.
In Proceedings of the7th Linguistic Annotation Workshop and Interoperabilitywith Discourse (LAW 2013), pages 149?158.
Sofia.
URLhttp://www.aclweb.org/anthology/W13-2319.Ferm?n L Cruz, Jos?
A Troyano, Beatriz Pontes, and F JavierOrtega.
2014.
Building layered, multilingual sentimentlexicons at synset and lemma levels.
Expert Systems withApplications, 41(13):5984?5994.Lu?s Morgado da Costa and Francis Bond.
2015.
OMWEdit- the integrated open multilingual wordnet editing system.In ACL-2015 System Demonstrations.
(this volume).Christine Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Hitoshi Isahara, Francis Bond, Kiyotaka Uchimoto, MasaoUtiyama, and Kyoko Kanzaki.
2008.
Development of theJapanese WordNet.
In Sixth International conference onLanguage Resources and Evaluation (LREC 2008).
Mar-rakech.Philipp Koehn.
2005.
Europarl: A parallel corpus for statisti-cal machine translation.
InMT Summit X.Svetla Koeva, Sv Leseva, and Maria Todorova.
2006.
Bul-garian sense tagged corpus.
In Proceedings of the 5thSALTMIL Workshop on Minority Languages: Strategiesfor Developing Machine Translation for Minority Lan-guages, Genoa, Italy, pages 79?87.Svetla Koeva, Borislav Rizov, and Svetlozara Leseva.
2008.Chooser: a multi-task annotation tool.
In LREC.Shari Landes, Claudia Leacock, and Christiane Fellbaum.1998.
Building semantic concordances.
In Fellbaum(1998), chapter 8, pages 199?216.Helen Langone, Benjamin R. Haskell, and George A. Miller.2004.
Annotating wordnet.
In Workshop On Frontiers InCorpus Annotation, pages 63?69.
ACL, Boston.Monica Lupu, Diana Trandabat, and Maria Husarciuc.
2005.ARomanian semcor aligned to the English and Italianmul-tisemcor.
In Proceedings 1st ROMANCEFrameNetWork-shop at EUROLAN 2005 Summer School, pages 20?27.EUROLAN, Cluj-Napoca, Romania.Nurril Hirfana Mohamed Noor, Suerya Sapuan, and FrancisBond.
2011.
Creating the open Wordnet Bahasa.
In Pro-ceedings of the 25th Pacific Asia Conference on Language,Information and Computation (PACLIC 25), pages 258?267.
Singapore.Rebecca J Passonneau, Ansaf Salleb-Aouissi, and Nancy Ide.2009.
Making sense of word sense variation.
In Proceed-ings of the Workshop on Semantic Evaluations: RecentAchievements and Future Directions, pages 2?9.
Associa-tion for Computational Linguistics.Sanghoun Song and Francis Bond.
2009.
Online search inter-face for the Sejong Korean-Japanese bilingual corps andauto-interpolation of phrase alignment.
In Proceedingsof the Third Linguistic Annotation Workshop (LAW III),pages 146?149.
Singapore.Pontus Stenetorp, Sampo Pyysalo, Goran Topi?, TomokoOhta, Sophia Ananiadou, and Jun?ichi Tsujii.
2012. brat:a web-based tool for NLP-assisted text annotation.
In Pro-ceedings of the Demonstrations Session at EACL 2012.Liling Tan and Francis Bond.
2012.
Building and annotat-ing the linguistically diverse NTU-MC (NTU-multilingualcorpus).
International Journal of Asian Language Pro-cessing, 22(4):161?174.Shan Wang and Francis Bond.
2013.
Building the ChineseOpen Wordnet (COW): Starting from core synsets.
InProceedings of the 11th Workshop on Asian LanguageResources, a Workshop at IJCNLP-2013, pages 10?18.Nagoya.12
