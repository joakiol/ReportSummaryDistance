Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777?785,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsSemi-supervised Dependency Parsing using Lexical AffinitiesSeyed Abolghasem Mirroshandel?,?
Alexis Nasr?
Joseph Le Roux?Laboratoire d?Informatique Fondamentale de Marseille- CNRS - UMR 7279Universite?
Aix-Marseille, Marseille, FranceLIPN, Universite?
Paris Nord & CNRS,Villetaneuse, France?Computer Engineering Department, Sharif university of Technology, Tehran, Iran(ghasem.mirroshandel@lif.univ-mrs.fr, alexis.nasr@lif.univ-mrs.fr,leroux@univ-paris13.fr)AbstractTreebanks are not large enough to reliablymodel precise lexical phenomena.
This de-ficiency provokes attachment errors in theparsers trained on such data.
We proposein this paper to compute lexical affinities,on large corpora, for specific lexico-syntacticconfigurations that are hard to disambiguateand introduce the new information in a parser.Experiments on the French Treebank showeda relative decrease of the error rate of 7.1% La-beled Accuracy Score yielding the best pars-ing results on this treebank.1 IntroductionProbabilistic parsers are usually trained on treebankscomposed of few thousands sentences.
While thisamount of data seems reasonable for learning syn-tactic phenomena and, to some extent, very frequentlexical phenomena involving closed parts of speech(POS), it proves inadequate when modeling lexicaldependencies between open POS, such as nouns,verbs and adjectives.
This fact was first recognizedby (Bikel, 2004) who showed that bilexical depen-dencies were barely used in Michael Collins?
parser.The work reported in this paper aims at a bettermodeling of such phenomena by using a raw corpusthat is several orders of magnitude larger than thetreebank used for training the parser.
The raw cor-pus is first parsed and the computed lexical affinitiesbetween lemmas, in specific lexico-syntactic config-urations, are then injected back in the parser.
Twooutcomes are expected from this procedure, the firstis, as mentioned above, a better modeling of bilexi-cal dependencies and the second is a method to adapta parser to new domains.The paper is organized as follows.
Section 2 re-views some work on the same topic and highlightstheir differences with ours.
In section 3, we describethe parser that we use in our experiments and givea detailed description of the frequent attachment er-rors.
Section 4 describes how lexical affinities be-tween lemmas are calculated and their impact is thenevaluated with respect to the attachment errors madeby the parser.
Section 5 describes three ways to in-tegrate the lexical affinities in the parser and reportsthe results obtained with the three methods.2 Previous WorkCoping with lexical sparsity of treebanks using rawcorpora has been an active direction of research formany years.One simple and effective way to tackle this prob-lem is to put together words that share, in a largeraw corpus, similar linear contexts, into word clus-ters.
The word occurrences of the training treebankare then replaced by their cluster identifier and a newparser is trained on the transformed treebank.
Us-ing such techniques (Koo et al, 2008) report signi-ficative improvement on the Penn Treebank (Marcuset al, 1993) and so do (Candito and Seddah, 2010;Candito and Crabbe?, 2009) on the French Treebank(Abeille?
et al, 2003).Another series of papers (Volk, 2001; Nakovand Hearst, 2005; Pitler et al, 2010; Zhou et al,2011) directly model word co-occurrences.
Co-occurrences of pairs of words are first collected in a777raw corpus or internet n-grams.
Based on the countsproduced, lexical affinity scores are computed.
Thedetection of pairs of words co-occurrences is gen-erally very simple, it is either based on the directadjacency of the words in the string or their co-occurrence in a window of a few words.
(Bansaland Klein, 2011; Nakov and Hearst, 2005) rely onthe same sort of techniques but use more sophisti-cated patterns, based on simple paraphrase rules, foridentifying co-occurrences.Our work departs from those approaches by thefact that we do not extract the lexical informationdirectly on a raw corpus, but we first parse it andthen extract the co-occurrences on the parse trees,based on some predetermined lexico-syntactic pat-terns.
The first reason for this choice is that the lin-guistic phenomena that we are interested in, such asas PP attachment, coordination, verb subject and ob-ject can range over long distances, beyond what isgenerally taken into account when working on lim-ited windows.
The second reason for this choice wasto show that the performances that the NLP commu-nity has reached on parsing, combined with the useof confidence measures allow to use parsers to ex-tract accurate lexico-syntactic information, beyondwhat can be found in limited annotated corpora.Our work can also be compared with self train-ing approaches to parsing (McClosky et al, 2006;Suzuki et al, 2009; Steedman et al, 2003; Sagaeand Tsujii, 2007) where a parser is first trained ona treebank and then used to parse a large raw cor-pus.
The parses produced are then added to the ini-tial treebank and a new parser is trained.
The maindifference between these approaches and ours is thatwe do not directly add the output of the parser to thetraining corpus, but extract precise lexical informa-tion that is then re-injected in the parser.
In the selftraining approach, (Chen et al, 2009) is quite closeto our work: instead of adding new parses to the tree-bank, the occurrence of simple interesting subtreesare detected in the parses and introduced as new fea-tures in the parser.The way we introduce lexical affinity measures inthe parser, in 5.1, shares some ideas with (Anguianoand Candito, 2011), who modify some attachmentsin the parser output, based on lexical information.The main difference is that we only take attachmentsthat appear in an n-best parse list into account, whilethey consider the first best parse and compute all po-tential alternative attachments, that may not actuallyoccur in the n-best forests.3 The ParserThe parser used in this work is the second ordergraph based parser (McDonald et al, 2005; Ku?bleret al, 2009) implementation of (Bohnet, 2010).
Theparser was trained on the French Treebank (Abeille?et al, 2003) which was transformed into dependencytrees by (Candito et al, 2009).
The size of the tree-bank and its decomposition into train, developmentand test sets is represented in table 1.nb of sentences nb of wordsFTB TRAIN 9 881 278 083FTB DEV 1 239 36 508FTB TEST 1 235 36 340Table 1: Size and decomposition of the French TreebankThe part of speech tagging was performed withthe MELT tagger (Denis and Sagot, 2010) and lem-matized with the MACAON tool suite (Nasr et al,2011).
The parser gave state of the art results forparsing of French, reported in table 2.pred.
POS tags gold POS tagspunct no punct punct no punctLAS 88.02 90.24 88.88 91.12UAS 90.02 92.50 90.71 93.20Table 2: Labeled and unlabeled accuracy score for auto-matically predicted and gold POS tags with and withouttaking into account punctuation on FTB TEST.Figure 1 shows the distribution of the 100 mostcommon error types made by the parser.
In thisfigure, x axis shows the error types and y axisshows the error ratio of the related error type( number of errors of the specific typetotal number of errors ).
We define an errortype by the POS tag of the governor and the POStag of the dependent.
The figure presents a typicalZipfian distribution with a low number of frequenterror types and a large number of unfrequent errortypes.
The shape of the curve shows that concen-trating on some specific frequent errors in order toincrease the parser accuracy is a good strategy.77800.020.040.060.080.10.120.1410  20  30  40  50  60  70  80  90  100errorratioError TypeFigure 1: Distribution of the types of errorsTable 3 gives a finer description of the most com-mon types of error made by the parser.
Here wedefine more precise patterns for errors, where somelexical values are specified (for prepositions) and, insome cases, the nature of the dependency is takeninto account.
Every line of the table corresponds toone type of error.
The first column describes theerror type.
The second column indicates the fre-quency of this type of dependency in the corpus.
Thethird one displays the accuracy for this type of de-pendency (the number of dependencies of this typecorrectly analyzed by the parser divided by the to-tal number of dependencies of this type).
The fourthcolumn shows the contribution of the errors made onthis type of dependency to the global error rate.
Thelast column associates a name with some of the errortypes that will prove useful in the remainder of thepaper to refer to the error type.Table 3 shows two different kinds of errors thatimpact the global error rate.
The first one concernsvery common dependencies that have a high accu-racy but, due to their frequency, hurt the global er-ror rate of the parser.
The second one concerns lowfrequency, low accuracy dependency types.
Lines 2and 3, respectively attachment of the preposition a` toa verb and the subject dependency illustrate such acontrast.
They both impact the total error rate in thesame way (2.53% of the errors).
But the first oneis a low frequency low accuracy type (respectively0.88% and 69.11%) while the second is a high fre-quency high accuracy type (respectively 3.43% and93.03%).
We will see in 4.2.2 that our method be-haves quite differently on these two types of error.dependency freq.
acc.
contrib.
nameN?N 1.50 72.23 2.91V?
a` 0.88 69.11 2.53 VaNV?suj?
N 3.43 93.03 2.53 SBJN?
CC 0.77 69.78 2.05 NcNN?
de 3.70 92.07 2.05 NdeNV?
de 0.66 74.68 1.62 VdeNV?obj?
N 2.74 90.43 1.60 OBJV?
en 0.66 81.20 1.24V?
pour 0.46 67.78 1.10N?
ADJ 6.18 96.60 0.96 ADJN?
a` 0.29 70.64 0.72 NaNN?
pour 0.12 38.64 0.67N?
en 0.15 47.69 0.57Table 3: The 13 most common error types4 Creating the Lexical ResourceThe lexical resource is a collection of tuples?C, g, d, s?
where C is a lexico-syntactic configu-ration, g is a lemma, called the governor of theconfiguration, d is another lemma called the depen-dent and s is a numerical value between 0 and 1,called the lexical affinity score, which accounts forthe strength of the association between g and d inthe context C. For example the tuple ?
(V, g)obj?
(N, d), eat , oyster , 0.23?
defines a simple configu-ration (V, g)obj?
(N, d) that is an object depen-dency between verb g and noun d. When replac-ing variables g and d in C respectively with eatand oyster , we obtain the fully specified lexico syn-tactic pattern(V, eat)obj?
(N, oyster), that we callan instantiated configuration.
The numerical value0.23 accounts for how much eat and oyster liketo co-occur in the verb-object configuration.
Con-figurations can be of arbitrary complexity but theyhave to be generic enough in order to occur fre-quently in a corpus yet be specific enough to modela precise lexico syntactic phenomenon.
The context(?, g)??
(?, d), for example is very generic but doesnot model a precise linguistic phenomenon, as selec-tional preferences of a verb, for example.
Moreover,configurations need to be error-prone.
In the per-spective of increasing a parser performances, thereis no point in computing lexical affinity scores be-tween words that appear in a configuration for which779the parser never makes mistakes.The creation of the lexical resource is a three stageprocess.
The first step is the definition of configura-tions, the second one is the collection of raw countsfrom the machine parsed corpora and the third one isthe computation of lexical affinities based on the rawcounts.
The three steps are described in the follow-ing subsection while the evaluation of the createdresource is reported in subsection 4.2.4.1 Computing Lexical AffinitiesA set of 9 configurations have been defined.
Theirselection is a manual process based on the analysisof the errors made by the parser, described in sec-tion 3, as well as on the linguistic phenomena theymodel.
The list of the 9 configurations is describedin Table 4.
As one can see on this table, configu-rations are usually simple, made up of one or twodependencies.
Linguistically, configurations OBJand SBJ concern subject and object attachments,configuration ADJ is related to attachments of ad-jectives to nouns and configurations NdeN, VdeN,VaN, and NaN indicate prepositional attachments.We have restricted ourselves here to two commonFrench prepositions a` and de.
Configurations NcNand VcV deal respectively with noun and verb coor-dination.Name DescriptionOBJ (V, g)obj?
(N, d)SBJ (V, g)subj?
(N, d)ADJ (N, g) ?
ADJNdeN (N, g) ?
(P, de)?
(N, d)VdeN (V, g) ?
(P, de)?
(N, d)NaN (N, g) ?
(P, a`)?
(N, d)VaN (V, g) ?
(P, a`)?
(N, d)NcN (N, g) ?
(CC, ?)?
(N, d)VcV (V, g) ?
(CC, ?)?
(V, d)Table 4: List of the 9 configurations.The computation of the number of occurrences ofan instantiated configuration in the corpus is quitestraightforward, it consists in traversing the depen-dency trees produced by the parser and detect theoccurrences of this configuration.At the end of the counts collection, we have gath-CORPUS Sent.
nb.
Tokens nb.AFP 1 024 797 31 486 618EST REP 1 103 630 19 635 985WIKI 1 592 035 33 821 460TOTAL 3 720 462 84 944 063Table 5: sizes of the corpora used to gather lexical countsered for every lemma l its number of occurrences asgovernor (resp.
dependent) of configurationC in thecorpus, noted C(C, l, ?)
(resp.
C(C, ?, l)), as well asthe number of occurrences of configuration C withlemma lg as a governor and lemma ld as a depen-dent, noted C(C, lg, ld).
We are now in a positionto compute the score s(C, lg, ld).
This score shouldreflect the tendency of lg and ld to appear togetherin configuration C. It should be maximal if when-ever lg occurs as the governor of configuration C,the dependent position is occupied by ld and, sym-metrically, if whenever ld occurs as the dependent ofconfiguration C, the governor position is occupiedby lg.
A function that conforms such a behavior isthe following:s(C, lg, ld) =12(C(C, lg, ld)C(C, lg, ?
)+C(C, lg, ld)C(C, ?, ld))it takes its values between 0 (lg and ld neverco-occur) and 1 (g and d always co-occur).
Thisfunction is close to pointwise mutual information(Church and Hanks, 1990) but takes its values be-tween 0 and 1.4.2 EvaluationLexical affinities were computed on three corpora ofslightly different genres.
The first one, is a collectionof news report of the French press agency AgenceFrance Presse, the second is a collection of news-paper articles from a local French newspaper : l?EstRe?publicain.
The third one is a collection of articlesfrom the French Wikipedia.
The size of the differentcorpora are detailed in table 5.
The corpus was firstPOS tagged, lemmatized and parsed in order to getthe 50 best parses for every sentence.
Then the lexi-cal resource was built, based on the 9 configurationsdescribed in table 4.The lexical resource has been evaluated onFTB DEV with respect to two measures: coverage780and correction rate, described in the next two sec-tions.4.2.1 CoverageCoverage measures the instantiated configura-tions present in the evaluation corpus that are in theresource.
The results are presented in table 6.
Everyline represents a configuration, the second columnindicates the number of different instantiations ofthis configuration in the evaluation corpus, the thirdone indicates the number of instantiated configura-tions that were actually found in the lexical resourceand the fourth column shows the coverage for thisconfiguration, which is the ratio third column overthe second.
Last column represents the coverage ofthe training corpus (the lexical resource is extractedon the training corpus) and the last line representsthe same quantities computed on all configurations.Table 6 shows two interesting results: firstly thehigh variability of coverage with respect to configu-rations, and secondly the low coverage when the lex-ical resource is computed on the training corpus, thisfact being consistent with the conclusions of (Bikel,2004).
A parser trained on a treebank cannot be ex-pected to reliably select the correct governor in lex-ically sensitive cases.Conf.
occ.
pres.
cov.
T cov.OBJ 1017 709 0.70 0.21SBJ 1210 825 0.68 0.24ADJ 1791 1239 0.69 0.33NdeN 1909 1287 0.67 0.31VdeN 189 107 0.57 0.16NaN 123 61 0.50 0.20VaN 422 273 0.65 0.23NcN 220 55 0.25 0.10VcV 165 93 0.56 0.04?
7046 4649 0.66 0.27Table 6: Coverage of the lexical resource over FTB DEV.4.2.2 Correction RateWhile coverage measures how many instantiatedconfigurations that occur in the treebank are actu-ally present in the lexical resource, it does not mea-sure if the information present in the lexical resourcecan actually help correcting the errors made by theparser.We define Correction Rate (CR) as a way to ap-proximate the usefulness of the data.
Given a wordd present in a sentence S and a configuration C, theset of all potential governors of d in configurationC, in all the n-best parses produced by the parser iscomputed.
This set is noted G = {g1, .
.
.
, gj}.
Letus note GL the element of G that maximizes the lex-ical affinity score.
When the lexical resource givesno score to any of the elements of G, GL is left un-specified.Ideally, G should not be the set of governors inthe n-best parses but the set of all possible governorsfor d in sentence S. Since we have no simple wayto compute the latter, we will content ourselves withthe former as an approximation of the latter.Let us note GH the governor of d in the (first)best parse produced and GR the governor of d in thecorrect parse.
CR measures the effect of replacingGH with GL.We have represented in table 7 the different sce-narios that can happen when comparing GH , GRand GL.GL = GR or GL unspec.
CCGH = GR GL 6= GR CEGL = GR ECGH 6= GR GL 6= GR or GL unspec.
EEGR /?
G NATable 7: Five possible scenarios when comparing thegovernor of a word produced by the parser (GH ), inthe reference parse (GR) and according to the lexical re-source (GL).In scenarios CC and CE, the parser did not makea mistake (the first letter, C, stands for correct).
Inscenario CC, the lexical affinity score was compat-ible with the choice of the parser or the lexical re-source did not select any candidate.
In scenario CE,the lexical resource introduced an error.
In scenar-ios EC and EE, the parser made an error.
In EC,the error was corrected by the lexical resource whilein EE, it wasn?t.
Either because the lexical resourcecandidate was not the correct governor or it was un-specified.
The last case, NA, indicates that the cor-rect governor does not appear in any of the n-bestparses.
Technically this case could be integrated inEE (an error made by the parser was not correctedby the lexical resource) but we chose to keep it apart781since it represents a case where the right solutioncould not be found in the n-best parse list (the cor-rect governor is not a member of set G).Let?s note nS the number of occurrences of sce-nario S for a given configuration.
We compute CRfor this configuration in the following way:CR =old error number - new error numberold error number=nEC ?
nCEnEE + nEC + nNAWhen CR is equal to 0, the correction did not haveany impact on the error rate.
When CR> 0, the errorrate is reduced and if CR < 0 it is increased1.CR for each configuration is reported in table 8.The counts of the different scenarios have also beenreported.Conf.
nCC nCE nEC nEE nNA CROBJ 992 30 51 5 17 0.29SBJ 1131 35 61 16 34 0.23ADJ 2220 42 16 20 6 -0.62NdeN 2083 93 42 44 21 -0.48VdeN 150 2 49 1 13 0.75NaN 89 5 21 10 2 0.48VaN 273 19 132 8 11 0.75NcN 165 17 12 31 12 -0.09VcN 120 21 14 11 5 -0.23?
7223 264 398 146 121 0.20Table 8: Correction Rate of the lexical resource with re-spect to FTB DEV.Table 8 shows very different results among con-figurations.
Results for PP attachments VdeN, VaNand NaN are quite good (a CR of 75% for a givenconfiguration, as VdeN indicates that the number oferrors on such a configuration is decreased by 25%).It is interesting to note that the parser behaves quitebadly on these attachments: their accuracy (as re-ported in table 3) is, respectively 74.68, 69.1 and70.64.
Lexical affinity helps in such cases.
Onthe other hand, some attachments like configurationADJ and NdeN, for which the parser showed verygood accuracy (96.6 and 92.2) show very poor per-formances.
In such cases, taking into account lexicalaffinity creates new errors.1One can note, that contrary to coverage, CR does not mea-sure a characteristic of the lexical resource alone, but the lexicalresource combined with a parser.On average, using the lexical resource with thissimple strategy of systematically replacing GH withGL allows to decrease by 20% the errors made onour 9 configurations and by 2.5% the global errorrate of the parser.4.3 Filtering Data with Ambiguity ThresholdThe data used to extract counts is noisy: it con-tains errors made by the parser.
Ideally, we wouldlike to take into account only non ambiguous sen-tences, for which the parser outputs a single parsehypothesis, hopefully the good one.
Such an ap-proach is obviously doomed to fail since almost ev-ery sentence will be associated to several parses.Another solution would be to select sentences forwhich the parser has a high confidence, using confi-dence measures as proposed in (Sa?nchez-Sa?ez et al,2009; Hwa, 2004).
But since we are only interestedin some parts of sentences (usually one attachment),we don?t need high confidence for the whole sen-tence.
We have instead used a parameter, defined onsingle dependencies, called the ambiguity measure.Given the n best parses of a sentence and a depen-dency ?, present in at least one of the n best parses,let us note C(?)
the number of occurrences of ?
inthe n best parse set.
We note AM(?)
the ambiguitymeasure associated to ?.
It is computed as follows:AM(?)
= 1?C(?
)nAn ambiguity measure of 0 indicates that ?
is nonambiguous in the set of the n best parses (the wordthat constitutes the dependent in ?
is attached to theword that constitutes the governor in ?
in all the n-best analyses).
When n gets large enough this mea-sure approximates the non ambiguity of a depen-dency in a given sentence.Ambiguity measure is used to filter the data whencounting the number of occurrences of a configura-tion: only occurrences that are made of dependen-cies ?
such that AM(?)
?
?
are taken into account.?
is called the ambiguity threshold.The results of coverage and CR given above werecomputed for ?
equal to 1, which means that, whencollecting counts, all the dependencies are taken intoaccount whatever their ambiguity is.
Table 9 showscoverage and CR for different values of ?
.
As ex-pected, coverage decreases with ?
.
But, interest-782ingly, decreasing ?
, from 1 down to 0.2 has a posi-tive influence on CR.
Ambiguity threshold plays therole we expected: it allows to reduce noise in thedata, and corrects more errors.?
= 1.0 ?
= 0.4 ?
= 0.2 ?
= 0.0cov/CR cov/CR cov/CR cov/CROBJ 0.70/0.29 0.58/0.36 0.52/0.36 0.35/0.38SBJ 0.68/0.23 0.64/0.23 0.62/0.23 0.52/0.23ADJ 0.69/-0.62 0.61/-0.52 0.56/-0.52 0.43/-0.38NdeN 0.67/-0.48 0.58/-0.53 0.52/-0.52 0.38/-0.41VdeN 0.57/0.75 0.44/0.73 0.36/0.73 0.20/0.30NaN 0.50/0.48 0.34/0.42 0.28/0.45 0.15/0.48VaN 0.65/0.75 0.50/0.8 0.41/0.80 0.26/0.48NcN 0.25/-0.09 0.19/0 0.16/0.02 0.07/0.13VcV 0.56/-0.23 0.42/-0.07 0.28/0.03 0.08/0.07Avg 0.66/0.2 0.57/0.23 0.51/0.24 0.38/0.17Table 9: Coverage and Correction Rate on FTB DEV forseveral values of ambiguity threshold.5 Integrating Lexical Affinity in the ParserWe have devised three methods for taking into ac-count lexical affinity scores in the parser.
The firsttwo are post-processing methods, that take as inputthe n-best parses produced by the parser and mod-ify some attachments with respect to the informationgiven by the lexical resource.
The third method in-troduces the lexical affinity scores as new features inthe parsing model.
The three methods are describedin 5.1, 5.2 and 5.3.
They are evaluated in 5.4.5.1 Post Processing MethodThe post processing method is quite simple.
It isvery close to the method that was used to computethe Correction Rate of the lexical resource, in 4.2.2:it takes as input the n-best parses produced by theparser and, for every configuration occurrence Cfound in the first best parse, the set (G) of all po-tential governors of C, in the n-best parses, is com-puted and among them, the word that maximizes thelexical affinity score (GL) is identified.Once GL is identified, one can replace the choiceof the parser (GH ) with GL.
This method is quitecrude since it does not take into account the confi-dence the parser has in the solution proposed.
Weobserved, in 4.2.2 that CR was very low for configu-rations for which the parser achieves good accuracy.In order to introduce the parser confidence in the fi-nal choice of a governor, we compute C(GH) andC(GL) which respectively represent the number oftimes GH and GL appear as the governor of config-uration C. The choice of the final governor, notedG?, depends on the ratio of C(GH) and C(GL).
Thecomplete selection strategy is the following:1. if GH = GL or GL is unspecified, G?
= GH .2. if GH 6= GL, G?
is determined as follows:G?
={GH ifC(GH)C(GL)> ?GL otherwisewhere ?
is a coefficient that is optimized on thedevelopment data set.We have reported, in table 10 the values of CR,for the 9 different features, using this strategy, for?
= 1.
We do not report the values of CR for othervalues of ?
since they are very close to each other.The table shows several noticeable facts.
First, thenew strategy performs much better than the formerone (crudely replacing GH by GL), the value of CRincreased from 0.2 to 0.4, which means that the er-rors made on the nine configurations are now de-creased by 40%.
Second, CR is now positive for ev-ery configuration: the number of errors is decreasedfor every configuration.Conf.
OBJ SUJ ADJ NdeN VdeNCR 0.45 0.46 0.14 0.05 0.73Conf.
NaN VaN NcN VcV ?CR 0.12 0.8 0.12 0.1 0.4Table 10: Correction Rate on FTB DEV when taking intoaccount parser confidence.5.2 Double Parsing MethodThe post processing method performs better than thenaive strategy that was used in 4.2.2.
But it has animportant drawback: it creates inconsistent parses.Recall that the parser we are using is based on a sec-ond order model, which means that the score of a de-pendency depends on some neighboring ones.
Sincewith the post processing method only a subset of thedependencies are modified, the resulting parse is in-consistent: the score of some dependencies is com-puted on the basis of other dependencies that havebeen modified.783In order to compute a new optimal parse treethat preserves the modified dependencies, we haveused a technique proposed in (Mirroshandel andNasr, 2011) that modifies the scoring function of theparser in such a way that the dependencies that wewant to keep in the parser output get better scoresthan all competing dependencies.The double parsing method is therefore a threestage method.
First, sentence S is parsed, producingthe n-best parses.
Then, the post processing methodis used, modifying the first best parse.
Let?s noteD the set of dependencies that were changed in thisprocess.
In the last stage, a new parse is produced,that preserves D.5.3 Feature Based MethodIn the feature based method, new features areadded to the parser that rely on lexical affinityscores.
These features are of the following form:?C, lg, ld, ?C(s)?, where C is a configuration num-ber, s is the lexical affinity score (s = s(C, lg, ld))and ?c(?)
is a discretization function.Discretization of the lexical affinity scores is nec-essary in order to fight against data sparseness.
Inthis work, we have used Weka software (Hall et al,2009) to discretize the scores with unsupervised bin-ning.
Binning is a simple process which dividesthe range of possible values a parameter can takeinto subranges called bins.
Two methods are im-plemented in Weka to find the optimal number ofbins: equal-frequency and equal-width.
In equal-frequency binning, the range of possible values aredivided into k bins, each of which holds the samenumber of instances.
In equal-width binning, whichis the method we have used, the range are dividedinto k subranges of the same size.
The optimal num-ber of bins is the one that minimizes the entropy ofthe data.
Weka computes different number of binsfor different configurations, ranging from 4 to 10.The number of new features added to the parser isequal to?C B(C) where C is a configuration andB(C) is the number of bins for configuration C.5.4 EvaluationThe three methods described above have been evalu-ated on FTB TEST.
Results are reported in table 11.The three methods outperformed the baseline (thestate of the art parser for French which is a secondorder graph based method) (Bohnet, 2010).
The bestperformances were obtained by the Double Parsingmethod that achieved a labeled relative error reduc-tion of 7, 1% on predicted POS tags, yielding thebest parsing results on the French Treebank.
It per-forms better than the Post Processing method, whichmeans that the second parsing stage corrects someinconsistencies introduced in the Post Processingmethod.
The performances of the Feature Basedmethod are disappointing, it achieves an error reduc-tion of 1.4%.
This result is not easy to interpret.
Itis probably due to the limited number of new fea-tures introduced in the parser.
These new featuresprobably have a hard time competing with the largenumber of other features in the training process.pred.
POS tags gold POS tagspunct no punct punct no punctBL LAS 88.02 90.24 88.88 91.12UAS 90.02 92.50 90.71 93.20PP LAS 88.45 90.73 89.46 91.78UAS 90.61 93.20 91.44 93.86DP LAS 88.87 91.10 89.72 91.90UAS 90.84 93.30 91.58 93.99FB LAS 88.19 90.33 89.29 91.43UAS 90.22 92.62 91.09 93.46Table 11: Parser accuracy on FTB TEST using thestandard parser (BL) the post processing method (PP),the double parsing method (DP) and the feature basedmethod.6 ConclusionComputing lexical affinities, on large corpora, forspecific lexico-syntactic configurations that are hardto disambiguate has shown to be an effective wayto increase the performances of a parser.
We haveproposed in this paper one method to compute lexi-cal affinity scores as well as three ways to introducethis new information in a parser.
Experiments on aFrench corpus showed a relative decrease of the er-ror rate of 7.1% Labeled Accuracy Score.AcknowledgmentsThis work has been funded by the French AgenceNationale pour la Recherche, through the projectsSEQUOIA (ANR-08-EMER-013) and EDYLEX(ANR-08-CORD-009).784ReferencesA.
Abeille?, L. Cle?ment, and F. Toussenel.
2003.
Buildinga treebank for french.
In Anne Abeille?, editor, Tree-banks.
Kluwer, Dordrecht.E.H.
Anguiano and M. Candito.
2011.
Parse correctionwith specialized models for difficult attachment types.In Proceedings of EMNLP.M.
Bansal and D. Klein.
2011.
Web-scale features forfull-scale parsing.
In Proceedings of ACL, pages 693?702.D.
Bikel.
2004.
Intricacies of Collins?
parsing model.Computational Linguistics, 30(4):479?511.B.
Bohnet.
2010.
Very high accuracy and fast depen-dency parsing is not a contradiction.
In Proceedingsof ACL, pages 89?97.M.
Candito and B. Crabbe?.
2009.
Improving generativestatistical parsing with semi-supervised word cluster-ing.
In Proceedings of the 11th International Confer-ence on Parsing Technologies, pages 138?141.M.
Candito and D. Seddah.
2010.
Parsing word clusters.In Proceedings of the NAACL HLT Workshop on Sta-tistical Parsing of Morphologically-Rich Languages,pages 76?84.M.
Candito, B.
Crabbe?, P. Denis, and F. Gue?rin.
2009.Analyse syntaxique du franc?ais : des constituants auxde?pendances.
In Proceedings of Traitement Automa-tique des Langues Naturelles.W.
Chen, J. Kazama, K. Uchimoto, and K. Torisawa.2009.
Improving dependency parsing with subtreesfrom auto-parsed data.
In Proceedings of EMNLP,pages 570?579.K.W.
Church and P. Hanks.
1990.
Word associationnorms, mutual information, and lexicography.
Com-putational linguistics, 16(1):22?29.P.
Denis and B. Sagot.
2010.
Exploitation d?uneressource lexicale pour la construction d?un e?tiqueteurmorphosyntaxique e?tat-de-l?art du franc?ais.
In Pro-ceedings of Traitement Automatique des Langues Na-turelles.M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I.H.
Witten.
2009.
The WEKA data min-ing software: an update.
ACM SIGKDD ExplorationsNewsletter, 11(1):10?18.R.
Hwa.
2004.
Sample selection for statistical parsing.Computational Linguistics, 30(3):253?276.T.
Koo, X. Carreras, and M. Collins.
2008.
Simple semi-supervised dependency parsing.
In Proceedings of theACL HLT, pages 595?603.S.
Ku?bler, R. McDonald, and J. Nivre.
2009.
Depen-dency parsing.
Synthesis Lectures on Human Lan-guage Technologies, 1(1):1?127.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of en-glish: The penn treebank.
Computational linguistics,19(2):313?330.D.
McClosky, E. Charniak, and M. Johnson.
2006.
Ef-fective self-training for parsing.
In Proceedings ofHLT NAACL, pages 152?159.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic?.2005.
Non-projective dependency parsing using span-ning tree algorithms.
In Proceedings of HLT-EMNLP,pages 523?530.S.A.
Mirroshandel and A. Nasr.
2011.
Active learningfor dependency parsing using partially annotated sen-tences.
In Proceedings of International Conference onParsing Technologies.P.
Nakov and M. Hearst.
2005.
Using the web as animplicit training set: application to structural ambigu-ity resolution.
In Proceedings of HLT-EMNLP, pages835?842.A.
Nasr, F. Be?chet, J-F. Rey, B. Favre, and Le Roux J.2011.
MACAON: An NLP tool suite for processingword lattices.
In Proceedings of ACL.E.
Pitler, S. Bergsma, D. Lin, and K. Church.
2010.
Us-ing web-scale N-grams to improve base NP parsingperformance.
In Proceedings of COLING, pages 886?894.K.
Sagae and J. Tsujii.
2007.
Dependency parsing anddomain adaptation with lr models and parser ensem-bles.
In Proceedings of the CoNLL shared task sessionof EMNLP-CoNLL, volume 7, pages 1044?1050.R.
Sa?nchez-Sa?ez, J.A.
Sa?nchez, and J.M.
Bened??.
2009.Statistical confidence measures for probabilistic pars-ing.
In Proceedings of RANLP, pages 388?392.M.
Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,J.
Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.2003.
Bootstrapping statistical parsers from smalldatasets.
In Proceedings of EACL, pages 331?338.J.
Suzuki, H. Isozaki, X. Carreras, and M. Collins.
2009.An empirical study of semi-supervised structured con-ditional models for dependency parsing.
In Proceed-ings of EMNLP, pages 551?560.M.
Volk.
2001.
Exploiting the WWW as a corpus toresolve PP attachment ambiguities.
In Proceedings ofCorpus Linguistics.G.
Zhou, J. Zhao, K. Liu, and L. Cai.
2011.
Exploitingweb-derived selectional preference to improve statisti-cal dependency parsing.
In Proceedings of HLT-ACL,pages 1556?1565.785
