Parsing Arguments of Nominalizations in English and Chinese?Sameer Pradhan, Honglin Sun,Wayne Ward, James H. MartinCenter for Spoken Language Research,University of Colorado, Boulder, CO 80303{spradhan,sunh,whw,martin}@cslr.colorado.eduDan JurafskyDepartment of LinguisticsStanford UniversityStanford, CA 94305jurafsky@stanford.eduAbstractIn this paper, we use a machine learning frame-work for semantic argument parsing, and applyit to the task of parsing arguments of eventivenominalizations in the FrameNet database.
Wecreate a baseline system using a subset of fea-tures introduced by Gildea and Jurafsky (2002),which are directly applicable to nominal pred-icates.
We then investigate new features whichare designed to capture the novelties in nom-inal argument structure and show a significantperformance improvement using these new fea-tures.
We also investigate the parsing perfor-mance of nominalizations in Chinese and com-pare the salience of the features for the two lan-guages.1 IntroductionThe field of NLP had seen a resurgence of research inshallow semantic analysis.
The bulk of this recent workviews semantic analysis as a tagging, or labeling prob-lem, and has applied various supervised machine learn-ing techniques to it (Gildea and Jurafsky (2000, 2002);Gildea and Palmer (2002); Surdeanu et al (2003); Ha-cioglu and Ward (2003); Thompson et al (2003); Prad-han et al (2003)).
Note that, while all of these systemsare limited to the analysis of verbal predicates, many un-derlying semantic relations are expressed via nouns, ad-jectives, and prepositions.
This paper presents a prelimi-nary investigation into the semantic parsing of eventivenominalizations (Grimshaw, 1990) in English and Chi-nese.2 Semantic Annotation and CorporaFor our experiments, we use the FrameNet database(Baker et al, 1998) which contains frame-specific se-?This research was partially supported by the ARDAAQUAINT program via contract OCG4423B and by the NSFvia grant IS-9978025mantic annotation of a number of predicates in English.Predicates are grouped by the semantic frame that theyinstantiate, depending on the sense of their usage, andtheir arguments assume one of the frame elements orroles specific to that frame.
The predicate can be a verb,noun, adjective, prepositional phrase, etc.
FrameNetcontains about 500 different frame types and about 700distinct frame elements.
The following example illus-trates the general idea.
Here, the predicate ?complain?instantiates a ?Statement?
frame once as a nominalpredicate and once as a verbal predicate.Did [Speaker she] make an official [Predicate:nominal com-plaint] [Addressee to you] [Topic about the attack.
][Message?Justice has not been done?]
[Speaker he][Predicate:verbal complained.
]Nominal predicates in FrameNet include ultra-nominals(Barker and Dowty, 1992), nominals and nominal-izations.
For the purposes of this study, a human analystwent through the nominal predicates in FrameNet andselected those that were identified as nominalizationsin NOMLEX (Macleod et al, 1998).
Out of those,the analyst then selected ones that were eventivenominalizations.These data comprise 7,333 annotated sentences, with11,284 roles.
There are 105 frames with about 190 dis-tinct frame role1 types.
A stratified sampling over predi-cates was performed to select 80% of this data for train-ing, 10% for development and another 10% for testing.For the Chinese semantic parsing experiments, we se-lected 22 nominalizations from the Penn Chinese Tree-bank and tagged all the sentences containing these predi-cates with PropBank (Kingsbury and Palmer, 2002) stylearguments ?
ARG0, ARG1, etc.
These consisted of 630sentences.
These are then split into two parts: 503 (80%)for training and 127 (20%) for testing.1We will use the terms role and arguments interchangeably3 Baseline SystemThe primary assumption in our system is that a seman-tic argument aligns with some syntactic constituent.
Thegoal is to identify and label constituents in a syntactictree that represent valid semantic arguments of a givenpredicate.
Unlike PropBank, there are no hand-correctedparses available for the sentences in FrameNet, so wecannot quantify the possible mis-alignment of the nomi-nal arguments with syntactic constituents.
The argumentsthat do not align with any constituent are simply missedby the current system.3.1 Features We created a baseline system usingall and only those features introduced by Gildea andJurafsky that are directly applicable to nominal pred-icates.
Most of the features are extracted from thesyntactic parse of a sentence.
We used the Charniakparser (Chaniak, 2001) to parse the sentences in order toperform feature extraction.
The features are listed below:Predicate ?
The predicate lemma is used as a feature.Path ?
The syntactic path through the parse tree from theparse constituent being classified to the predicate.Constituent type ?
This is the syntactic category (NP, PP,S, etc.)
of the constituent corresponding to the semanticargument.Position ?
This is a binary feature identifying whetherthe constituent is before or after the predicate.Head word ?
The syntactic head of the constituent.3.2 Classifier and Implementation We formulate theparsing problem as a multi-class classification problemand use a Support Vector Machine (SVM) classifier in theONE vs ALL (OVA) formalism, which involves trainingn classifiers for a n-class problem ?
including the NULLclass.
We use TinySVM2 along with YamCha3 (Kudoand Matsumoto (2000, 2001)) as the SVM training andtest software.3.3 Performance We evaluate our system on threetasks: i) Argument Identification: Identifying parse con-stituents that represent arguments of a given predicate, ii)Argument Classification: Labeling the constituents thatare known to represent arguments with the most likelyroles, and iii) Argument Identification and Classification:Finding constituents that represent arguments of a pred-icate, and labeling them with the most likely roles.
Thebaseline performance on the three tasks is shown in Ta-ble 1.4 New FeaturesTo improve the baseline performance we investigated ad-ditional features that would provide useful information inidentifying arguments of nominalizations.
Following is a2http://cl.aist-nara.ac.jp/?talus-Au/software/TinySVM/3http://cl.aist-nara.ac.jp/?taku-Au/software/yamcha/Task P R F?=1 A(%) (%) (%)Id.
81.7 65.7 72.8Classification - - - 70.9Id.
+ Classification 65.7 42.1 51.4Table 1: Baseline performance on all three tasks.description of each feature along with an intuitive justifi-cation.
Some of these features are not instantiated for aparticular constituent.
In those cases, the respective fea-ture values are set to ?UNK?.1.
Frame ?
The frame instantiated by the particular senseof the predicate in a sentence.
This is an oracle feature.2.
Selected words/POS in constituent ?
Nominal predi-cates tend to assign arguments, most commonly throughpostnominal of-complements, possessive prenominalmodifiers, etc.
We added the values of the first and lastword in the constituent as two separate features.
Anothertwo features represent the part of speech of these words.3.
Ordinal constituent position ?
Arguments of nounstend to be located closer to the predicate than thosefor verbs.
This feature captures the ordinal positionof a particular constituent to the left or right of thepredicate on a left or right tree traversal, eg., first PPfrom the predicate, second NP from the predicate, etc.This feature along with the position will encode thebefore/after information for the constituent.4.
Constituent tree distance ?
Another way of quan-tifying the position of the constituent is to identify itsindex in the list of constituents that are encounteredduring linear traversal of the tree from the predicate tothe constituent.5.
Intervening verb features ?
Support verbs play animportant role in realizing the arguments of nominalpredicates.
We use three classes of intervening verbs:i) auxiliary verbs ?
ones with part of speech AUX, ii)light verbs ?
a small set of known light verbs: took, take,make, made, give, gave, went and go, and iii) other verbs?
with part of speech VBx.
We added three features foreach: i) a binary feature indicating the presence of theverb in between the predicate and the constituent ii) theactual word as a feature, and iii) the path through thetree from the constituent to the verb, as the subject ofintervening verbs sometimes tend to be arguments ofnominalizations.
The following example could explainthe intuition behind this feature:[Speaker Leapor] makes general [Predicate assertions] [Topicabout marriage]6.
Predicate NP expansion rule ?
This is the nounequivalent of the verb sub-categorization feature used byGildea and Jurafsky (2002).
This is the expansion ruleinstantiated by the parser, for the lowermost NP in thetree, encompassing the predicate.
This would tend tocluster NPs with a similar internal structure and wouldthus help finding argumentive modifiers.7.
Noun head of prepositional phrase constituents?
Instead of using the standard head word rule forprepositional phrases, we use the head word of the firstNP inside the PP as the head of the PP and replace theconstituent type PP with PP-<preposition>.8.
Constituent sibling features ?
These are six featuresrepresenting the constituent type, head word and part ofspeech of the head word of the left and right siblingsof the constituent in consideration.
These are usedto capture arguments represented by the modifiers ofnominalizations.9.
Partial-path from constituent to predicate ?
Thisis the path from the constituent to the lowest commonparent of the constituent and the predicate.
This is usedto generalize the path statistics.10.
Is predicate plural ?
A binary feature indicatingwhether the predicate is singular or plural as they tend tohave different argument selection properties.11.
Genitives in constituent ?
This is a binary featurewhich is true if there is a genitive word (one with the partof speech POS, PRP, PRP$ or WP$) in the constituent,as these tend to be markers for nominal arguments as in[Speaker Burma ?s] [Phenomenon oil] [Predicate search] hitsvirgin forests12.
Constituent parent features ?
Same as the siblingfeatures, except that that these are extracted from theconstituent?s parent.13.
Verb dominating predicate ?
The head word of thefirst VP ancestor of the predicate.14.
Named Entities in Constituent ?
As in Surdeanuet al (2003), this is represented as seven binary fea-tures extracted after tagging the sentence with BBN?sIdentiFinder (Bikel et al, 1999) named entity tagger.5 Feature Analysis and Best SystemPerformance5.1 English For the task of argument identification,features 2, 3, 4, 5 (the verb itself, path to light-verb andpresence of a light verb), 6, 7, 9, 10 an 13 contributed pos-itively to the performance.
The Frame feature degradesperformance significantly.
This could be just an artifactof the data sparsity.
We trained a new classifier using allthe features that contributed positively to the performanceand the F?=1 score increased from the baseline of 72.8%to 76.3% (?2; p < 0.05).For the task of argument classification, adding theFrame feature to the baseline features, provided the mostsignificant improvement, increasing the classificationaccuracy from 70.9% to 79.0% (?2; p < 0.05).
Allother features added one-by-one to the baseline didnot bring any significant improvement to the baseline,which might again be owing to the comparatively smalltraining and test data sizes.
All the features togetherproduced a classification accuracy of 80.9%.
Since theFrame feature is an oracle, we were interested in findingout what all the other features combined contributed.We ran an experiment with all features, except Frame,added to the baseline, and this produced an accuracy of73.1%, which however, is not a statistically significantimprovement over the baseline of 70.9%.For the task of argument identification and classifi-cation, features 8 and 11 (right sibling head word partof speech) hurt performance.
We trained a classifierusing all the features that contributed positively to theperformance and the resulting system had an improvedF?=1 score of 56.5% compared to the baseline of 51.4%(?2; p < 0.05).We found that a significant subset of features that con-tribute marginally to the classification performance, hurtthe identification task.
Therefore, we decided to performa two-step process in which we use the set of features thatgave optimum performance for the argument identifica-tion task and identify all likely argument nodes.
Then, forthose nodes, we use all the available features and classifythem into one of the possible classes.
This ?two-pass?system performs slightly better than the ?one-pass?
men-tioned earlier.
Again, we performed the second pass ofclassification with and without the Frame feature.Table 2 shows the improved performance numbers.Task P R F?=1 A(%) (%) (%)Id.
83.8 70.0 76.3Classification (w/o Frame) - - - 73.1Classification (with Frame) - - - 80.9Id.
+ Classification 69.4 47.6 56.5(one-pass, w/o Frame)Id.
+ Classification 62.2 53.1 57.3(two-pass, w/o Frame)Id.
+ Classification 69.4 59.2 63.9(two-pass, with Frame)Table 2: Best performance on all three tasks.5.2 Chinese For the Chinese task, we use the one-passalgorithm as used for English.
A baseline system wascreated using the same features as used for English (Sec-tion 3).
We evaluate this system on just the combined taskof argument identification and classification.
The base-line performance is shown in Table 3.To improve the system?s performance over the base-line, we added all the features discussed in Section 4, ex-cept features Frame ?
as the data was labeled in a Prop-Bank fashion, there are no frames involved as in Frame-Net; Plurals and Genitives ?
as they are not realized thesame way morphologically in Chinese, and Named En-tities ?
owing to the unavailability of a Chinese NamedEntity tagger.
We found that of these features, 2, 3, 4, 6, 7and 13 hurt the performance when added to the baseline,but the other features helped to some degree, althoughnot significantly.
The improved performance is shown inTable 3Features P R F?=1(%) (%)Baseline 86.2 32.2 46.9Baseline 83.9 44.1 57.8+ more featuresTable 3: Parsing performance for Chinese on the com-bined task of identifying and classifying semantic argu-ments.An interesting linguistic phenomenon was observedwhich explains part of the reason why recall for Chineseargument parsing is so low.
In Chinese, argumentswhich are internal to the NP which encompasses thenominalized predicate, tend to be multi-word, and arenot associated with any node in the parse tree.
Theseviolates our basic assumption of the arguments aligningwith parse tree constituents, and are guaranteed to bemissed.
In the case of English however, these tend to besingle word arguments which are represented by a leafin the parse tree and stand a chance of getting classifiedcorrectly.6 ConclusionIn this paper we investigated the task of identifying andclassifying arguments of eventive nominalizations inFrameNet.
The best system generates an F1 score of57.3% on the combined task of argument identificationand classification using automatically extracted featureson a test set of about 700 sentences using a classifiertrained on about 6,000 sentences.As noted earlier, the bulk of past research in this areahas focused on verbal predicates.
Two notable exceptionsto this include the work of (Hull and Gomez, 1996) ?
arule based system for identifying the semantic argumentsof nominal predicates, and the work of (Lapata, 2002)on interpreting the relation between the head of a nom-inalized compound and its modifier noun.
Unfortunately,meaningful comparisons to these efforts are difficult dueto differing evaluation metrics.We would like to thank Ralph Weischedel and Scott Miller ofBBN Inc. for letting us use BBN?s named entity tagger ?
Iden-tiFinder; Ashley Thornton for identifying the sentences fromFrameNet with predicates that are eventive nominalizations.References[Baker et al1998] Collin F. Baker, Charles J. Fillmore, andJohn B. Lowe.
1998.
The Berkeley FrameNet project.
InCOLING/ACL-98, pages 86?90, Montreal.
[Barker and Dowty1992] Chris Barker and David Dowty.
1992.Non-verbal thematic proto-roles.
In NELS-23, Amy Schafer,ed., GSLA, Amherst, pages 49?62.
[Bikel et al1999] Daniel M. Bikel, Richard Schwartz, andRalph M. Weischedel.
1999.
An algorithm that learns what?sin a name.
Machine Learning, 34:211?231.
[Chaniak2001] Eugene Chaniak.
2001.
Immediate-head pars-ing for language models.
In ACL, Toulouse, France.
[Gildea and Jurafsky2000] Daniel Gildea and Daniel Jurafsky.2000.
Automatic labeling of semantic roles.
In ACL, pages512?520, Hong Kong, October.
[Gildea and Jurafsky2002] Daniel Gildea and Daniel Jurafsky.2002.
Automatic labeling of semantic roles.
ComputationalLinguistics, 28(3):245?288.
[Gildea and Palmer2002] Daniel Gildea and Martha Palmer.2002.
The necessity of syntactic parsing for predicate ar-gument recognition.
In ACL, PA.[Grimshaw1990] Jane Grimshaw.
1990.
Argument Structure.The MIT Press, US.
[Hacioglu and Ward2003] Kadri Hacioglu and Wayne Ward.2003.
Target word detection and semantic role chunking us-ing support vector machines.
In HLT, Edmonton, Canada.
[Hull and Gomez1996] Richard D. Hull and Fernando Gomez.1996.
Semantic interpretation of nominalizations.
In AAAIConference, Oregon, pages 1062?1068.
[Kingsbury and Palmer2002] Paul Kingsbury and Martha Pal-mer.
2002.
From Treebank to PropBank.
In LREC-2002,Las Palmas, Canary Islands, Spain.
[Kudo and Matsumoto2000] Taku Kudo and Yuji Matsumoto.2000.
Use of support vector learning for chunk identifica-tion.
In CoNLL-2000, pages 142?144.
[Kudo and Matsumoto2001] Taku Kudo and Yuji Matsumoto.2001.
Chunking with support vector machines.
In NAACL.
[Lapata2002] Maria Lapata.
2002.
The disambiguation of nom-inalizations.
Computational Linguistics, 28(3):357?388.
[Macleod et al1998] C. Macleod, R. Grishman, A. Meyers,L.
Barrett, and R. Reeves.
1998.
Nomlex: A lexicon ofnominalizations.
[Pradhan et al2003] Sameer Pradhan, Kadri Hacioglu, WayneWard, James Martin, and Dan Jurafsky.
2003.
Semantic roleparsing: Adding semantic structure to unstructured text.
InICDM, Melbourne, Florida.
[Surdeanu et al2003] Mihai Surdeanu, Sanda Harabagiu, JohnWilliams, and Paul Aarseth.
2003.
Using predicate-argument structures for information extraction.
In ACL, Sap-poro, Japan.
[Thompson et al2003] Cynthia A. Thompson, Roger Levy, andChristopher D. Manning.
2003.
A generative model for se-mantic role labeling.
In ECML.
