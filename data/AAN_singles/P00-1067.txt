PENS: A Machine-aided English Writing Systemfor Chinese UsersTing Liu1 Ming Zhou Jianfeng Gao Endong Xun Changning HuangNatural Language Computing Group, Microsoft Research China, Microsoft Corporation5F, Beijing Sigma Center100080 Beijing, P.R.C.
{ i-liutin, mingzhou, jfgao, i-edxun, cnhuang@microsoft.com }AbstractWriting English is a big barrier for mostChinese users.
To build a computer-aided systemthat helps Chinese users not only on spellingchecking and grammar checking but also onwriting in the way of native-English is achallenging task.
Although machine translation iswidely used for this purpose, how to find anefficient way in which human collaborates withcomputers remains an open issue.
In this paper,based on the comprehensive study of Chineseusers requirements, we propose an approach tomachine aided English writing system, whichconsists of two components: 1) a statisticalapproach to word spelling help, and 2) aninformation retrieval based approach tointelligent recommendation by providingsuggestive example sentences.
Both componentswork together in a unified way, and highlyimprove the productivity of English writing.
Wealso developed a pilot system, namely PENS(Perfect ENglish System).
Preliminaryexperiments show very promising results.IntroductionWith the rapid development of the Internet,writing English becomes daily work forcomputer users all over the world.
However, forChinese users who have significantly differentculture and writing style, English writing is a bigbarrier.
Therefore, building a machine-aidedEnglish writing system, which helps Chineseusers not only on spelling checking and grammarchecking but also on writing in the way ofnative-English, is a very promising task.Statistics shows that almost all Chineseusers who need to write in English1 have enoughknowledge of English that they can easily tell thedifference between two sentences written inChinese-English and native-English, respectively.Thus, the machine-aided English writing systemshould act as a consultant that provide variouskinds of help whenever necessary, and let usersplay the major role during writing.
These helpsinclude:1) Spelling help: help users input hard-to-spellwords, and check the usage in a certaincontext simultaneously;2) Example sentence help: help users refine thewriting by providing perfect examplesentences.Several machine-aided approaches havebeen proposed recently.
They basically fall intotwo categories, 1) automatic translation, and 2)translation memory.
Both work at the sentencelevel.
While in the former, the translation is notreadable even after a lot of manually editing.
Thelatter works like a case-based system, in that,given a sentence, the system retrieve similarsentences from translation example database, theuser then translates his sentences by analogy.
Tobuild a computer-aided English writing systemthat helps Chinese users on writing in the way ofnative-English is a challenging task.
Machinetranslation is widely used for this purpose, buthow to find an efficient way in which humancollaborates well with computers remains anopen issue.
Although the quality of fullyautomatic machine translation at the sentencelevel is by no means satisfied, it is hopeful to1 Now Ting Liu is an associate professor in HarbinInstitute of Technology, P.R.C.provide relatively acceptable quality translationsat the word or short phrase level.
Therefore, wecan expect that combining word/phrase levelautomatic translation with translation memorywill achieve a better solution to machine-aidedEnglish writing system [Zhou, 95].In this paper, we propose an approach tomachine aided English writing system, whichconsists of two components: 1) a statisticalapproach to word spelling help, and 2) aninformation retrieval based approach tointelligent recommendation by providingsuggestive example sentences.
Both componentswork together in a unified way, and highlyimprove the productivity of English writing.
Wealso develop a pilot system, namely PENS.Preliminary experiments show very promisingresults.The rest of this paper is structured as follows.In section 2 we give an overview of the system,introduce the components of the system, anddescribe the resources needed.
In section 3, wediscuss the word spelling help, and focus thediscussion on Chinese pinyin to English wordtranslation.
In addition, we describe variouskinds of word level help functions, such asautomatic translation of Chinese word in the formof either pinyin or Chinese characters, andsynonym suggestion, etc.
We also describe theuser interface briefly.
In section 4, an effectiveretrieval algorithm is proposed to implement theso-called intelligent recommendation function.
Insection 5, we present preliminary experimentalresults.
Finally, concluding remarks is given insection 6.1 System Overview1.1 System ArchitectureFigure 1 System ArchitectureThere are two modules in PENS.
The first iscalled the spelling help.
Given an English word,the spelling help performs two functions, 1)retrieving its synonym, antonym, and thesaurus;or 2) automatically giving the correspondingtranslation of Chinese words in the form ofChinese characters or pinyin.
Statistical machinetranslation techniques are used for this translation,and therefore a Chinese-English bilingualdictionary (MRD), an English language model,and an English-Chinese word- translation model(TM) are needed.
The English language model isa word trigram model, which consists of247,238,396 trigrams, and the vocabulary usedcontains 58541 words.
The MRD dictionarycontains 115,200 Chinese entries as well as theircorresponding English translations, and otherinformation, such as part-of-speech, semanticclassification, etc.
The TM is trained from aword-aligned bilingual corpus, which occupiesapproximately 96,362 bilingual sentence pairs.The second module is an intelligentrecommendation system.
It employs an effectivesentence retrieval algorithm on a large bilingualcorpus.
The input is a sequence of keywords or ashort phrase given by users, and the output islimited pairs bilingual sentences expressingrelevant meaning with users?
query, or just a fewpairs of bilingual sentences with syntacticalrelevance.1.2 Bilingual Corpus ConstructionWe have collected bilingual texts extractedfrom World Wide Web bilingual sites,dictionaries, books, bilingual news andmagazines, and product manuals.
The size of thecorpus is 96,362 sentence pairs.
The corpus isused in the following three cases:1) Act as translation memory to support theIntelligent Recommendation Function;2) To be used to acquire English-Chinesetranslation model to support translation atword and phrase level;3) To be used to extract bilingual terms to enrichthe Chinese-English MRD;To construct a sentence aligned bilingualcorpus, we first use an alignment algorithm doingthe automatic alignment and then the alignmentresult are corrected.There have been quite a number of recentpapers on parallel text alignment.
Lexically basedtechniques use extensive online bilinguallexicons to match sentences [Chen 93].
Incontrast, statistical techniques require almost noprior knowledge and are based solely on thelengths of sentences, i.e.
length-based alignmentmethod.
We use a novel method to incorporateboth approaches [Liu, 95].
First, the rough resultis obtained by using the length-based method.Then anchors are identified in the text to reducethe complexity.
An anchor is defined as a blockthat consists of n successive sentences.
Ourexperiments show best performance when n=3.Finally, a small, restricted set of lexical cues isapplied to obtain for further improvement.1.3 Translation Model TrainingChinese sentences must be segmentedbefore word translation training, because writtenChinese consists of a character stream withoutspace between words.
Therefore, we use awordlist, which consists of 65502 words, inconjunction with an optimization proceduredescribed in [Gao, 2000].
The bilingual trainingprocess employs a variant of the model in [Brown,1993] and as such is based on an iterative EM(expectation-maximization) procedure formaximizing the likelihood of generating theEnglish given the Chinese portion.
The output ofthe training process is a set of potential Englishtranslations for each Chinese word, together withthe probability estimate for each translation.1.4 Extraction of BilingualDomain-specific TermsA domain-specific term is defined as a stringthat consists of more than one successive wordand has certain occurrences in a text collectionwithin a specific domain.
Such a string has acomplete meaning and lexical boundaries insemantics; it might be a compound word, phraseor linguistic template.
We use two steps to extractbilingual terms from sentence aligned corpus.First we extract Chinese monolingual terms fromChinese part of the corpus by a similar methoddescribed in [Chien, 1998], then we extract theEnglish corresponding part by using the wordalignment information.
A candidate list of theChinese-English bilingual terms can be obtainedas the result.
Then we will check the list and addthe terms into the dictionary.2 Spelling HelpThe spelling help works on the word orphrase level.
Given an English word or phrase, itperforms two functions, 1) retrievingcorresponding synonyms, antonyms, andthesaurus; and 2) automatically giving thecorresponding translation of Chinese words inthe form of Chinese characters or pinyin.
We willfocus our discussion on the latter function in thesection.To use the latter function, the user may inputChinese characters or just input pinyin.
It is notvery convenient for Chinese users to inputChinese characters by an English keyboard.Furthermore the user must switch betweenEnglish input model and Chinese input modeltime and again.
These operations will interrupthis train of thought.
To avoid this shortcoming,our system allows the user to input pinyin insteadof Chinese characters.
The pinyin can betranslated into English word directly.Let us take a user scenario for an example toshow how the spelling help works.
Suppose that auser input a Chinese word ??
in the form ofpinyin, say ?wancheng?, as shown in figure1-1.PENS is able to detect whether a string is apinyin string or an English string automatically.For a pinyin string, PENS tries to translate it intothe corresponding English word or phrasedirectly.
The mapping from pinyin to Chineseword is one-to-many, so does the mapping fromChinese word to English words.
Therefore, foreach pinyin string, there are alternativetranslations.
PENS employs a statistical approachto determine the correct translation.
PENS alsodisplays the corresponding Chinese word orphrase for confirmation, as shown in figure 1-2.Figure 1-1Figure 1-2If the user is not satisfied with the Englishword determined by PENS, he can browse othercandidates as well as their bilingual examplesentences, and select a better one, as shown infigure 1-3.Figure 1-32.1 Word Translation Algorithmbased on Statistical LM and TMSuppose that a user input two English words,say EW1 and EW2, and then a pinyin string, sayPY.
For PY, all candidate Chinese words aredetermined by looking up a Pinyin-Chinesedictionary.
Then, a list of candidate Englishtranslations is obtained according to a MRD.These English translations are English words oftheir original form, while they should be ofdifferent forms in different contexts.
We exploitmorphology for this purpose, and expand eachword to all possible forms.
For instance,inflections of ?go?
may be ?went?, and ?gone?.In what follows, we will describe how todetermine the proper translation among thecandidate list.Figure 2-1: Word-level Pinyin-EnglishTranslationAs shown in Figure 2-1, we assume that themost proper translation of PY is the English wordwith the highest conditional probability amongall leaf nodes, that isAccording to Bayes?
law, the conditionalprobability is estimated by),|(),|(),,|(),,|(21212121EWEWPYPEWEWEWPEWEWEWPYPEWEWPYEWPijijij?= (2-1)Since the denominator is independent of EWij, werewrite (2-1) as),|(),,|(),,|(212121EWEWEWPEWEWEWPYPEWEWPYEWPijijij??
(2-2)Since CWi is a bridge which connect the pinyinand the English translation, we introduce Chineseword CWi intoWe get),,,|(),,,|(),,|(),,|(21212121EWEWEWPYCWPEWEWEWCWPYPEWEWEWCWPEWEWEWPYPijiijiijiij?=(2-3)For simplicity, we assume that a Chinese worddoesn?t depends on the translation context, so wecan get the following approximate equation:)|(),,|( 21 ijiiji EWCWPEWEWEWCWP ?We can also assume that the pinyin of a Chineseword is not concerned in the correspondingEnglish translation, namely:)|(),,,|( 21 iiji CWPYPEWEWEWCWPYP ?It is almost impossible that two Chinese wordscorrespond to the same pinyin and the sameEnglish translation, so we can suppose that:1),,,|( 21 ?EWEWEWPYCWP ijiTherefore, we get the approximation of (2-3) asfollows:)|()|(),,|( 21iijiijCWPYPEWCWPEWEWEWPYP?= (2-4)According to formula (2-2) and (2-4), we get:),|()|()|(),,|(2121EWEWEWPCWPYPEWCWPEWEWPYEWPijiijiij?
?= (2-5)where P(CWi |EWij) is the translation model, andcan be got from bilingual corpus, and P(PY | CWi)),,|( 21 EWEWEWPYP ijis the polyphone model, here we supposeP(PY|CWi) = 1, and P(EWij | EW1, EW2) is theEnglish trigram language model.To sum up, as indicated in (2-6), the spelling helpfind the most proper translation of PY byretrieving the English word with the highestconditional probability.
),|()|(maxarg),,|(maxarg2121EWEWEWPEWCWPEWEWPYEWPijijiEWEWijij?=(2-6)3 Intelligent RecommendationThe intelligent recommendation works onthe sentence level.
When a user input a sequenceof Chinese characters, the character string will befirstly segmented into one or more words.
Thesegmented word string acts as the user query inIR.
After query expansion, the intelligentrecommendation employs an effective sentenceretrieval algorithm on a large bilingual corpus,and retrieves a pair (or a set of pairs) of bilingualsentences related to the query.
All the retrievedsentence pairs are ranked based on a scoringstrategy.3.1 Query ExpansionSuppose that a user query is of the form CW1,CW2, ?
, CWm.
We then list all synonyms foreach word of the queries based on a Chinesethesaurus, as shown below.mmnnnmmCWCWCWCWCWCWCWCWCW????????????????????
?21 212221212111We can obtain an expanded query bysubstituting a word in the query with its synonym.To avoid over-generation, we restrict that onlyone word is substituted at each time.Let us take the query ??
for an example.The synonyms list is as follows: =	?
? =?
?.The query consists of two words.
By substitutingthe first word, we get expanded queries, such as??????, etc, and bysubstituting the second word, we get otherexpanded queries, such as ??????, etc.Then we select the expanded query, which isused for retrieving example sentence pairs, byestimating the mutual information of words withthe query.
It is indicated as follows?
?=mikkijkjiCWCWMI1,),(maxargwhere CWk is a the kth Chinese word in the query,and CWij is the jth synonym of the i-th Chineseword.
In the above example, ? ?
isselected.
The selection well meets the commonsense.
Therefore, bilingual example sentencescontaining ??
will be retrieved as well.3.2 Ranking AlgorithmThe input of the ranking algorithm is aquery Q, as described above, Q is a Chineseword string, as shown belowQ= T1,T2,T3,?TkThe output is a set of relevant bilingualexample sentence pairs in the form of,S={(Chinsent, Engsent) | Relevance(Q,Chinsent)>Relevance(Q,Engsent) >where Chinsent is a Chinese sentence, andEngsent is an English sentence, and For each sentence, the relevance score iscomputed in two parts, 1) the bonus whichrepresents the similarity of input query and thetarget sentence, and 2) the penalty, whichrepresents the dissimilarity of input query and thetarget sentence.The bonus is computed by the following formula:WhereWj is the weight of the jth word in query Q, whichwill be described later, tfij is the number of the jthword occurring in sentence i, n is the number ofthe sentences in corpus, dfj is the number ofij Ljdfnmj ijtfWiBonus /)/log()1log( ?
?=?=sentence which contains Wj, and Li is the numberof word in the ith sentence.The above formula contains only thealgebraic similarities.
To take the geometrysimilarity into consideration, we designed apenalty formula.
The idea is that we use theediting distance to compute that geometrysimilarity.iii PenaltyBonusR ?=Suppose the matched word list between query Qand a sentence are represented as A and BrespectivelyA1, A2, A3, ?
, AlB1, B2, B3, ?
, BmThe editing distance is defined as thenumber of editing operation to revise B to A. Thepenalty will increase for each editing operation,but the score is different for different wordcategory.
For example, the penalty will be seriouswhen operating a verb than operating a nounwhereWj?
is the penalty of the jth wordEj the editing distanceWe define the score and penalty for each kind ofpart-or-speechPOS Score PenaltyNoun 6 6Verb 10 10Adjective 8 8Adverb 8 8Preposition 8 8Conjuction 4 4Digit 4 4Digit-classifer 4 4Classifer 4 4Exclamation 4 4Pronoun 4 4Auxilary 6 6Post-preposition 6 6Idioms 6 6We then select the first   4 Experimental Results &EvaluationIn this section, we will report the primaryexperimental results on 1) word-levelpinyin-English translation, and 2) examplesentences retrieval.4.1 Word-level Pinyin-EnglishTranslationFirstly, we built a testing set based on theword aligned bilingual corpus automatically.Suppose that there is a word-aligned bilingualsentence pair, and every Chinese word is labelledwith Pinyin.
See Figure 4-1.Figure 5-1: An example of aligned bilingualsentenceIf we substitute an English word with the pinyFigure 4-1: An example of aligned bilingualsentenceIf we substitute an English word with thepinyin of the Chinese word which the Englishword is aligned to, we can get a testing examplefor word-level Pinyin-English translation.
Sincethe user only cares about how to write contentwords, rather than function words, we shouldskip function words in the English sentence.
Inthis example, suppose EW1 is a function word,EW2 and EW3 are content words, thus theextracted testing examples are:EW1 PY2 (CW2, EW2)EW1 EW2 PY4 (CW4, EW3)The Chinese words and English words inbrackets are standard answers to the pinyin.
Wecan get the precision of translation by comparingthe standard answers with the answers obtainedby the Pinyin-English translation module.ijj LjdfnEhjWiPenalty /)/log()1log( ' ??
?==The standard testing set includes 1198 testingsentences, and all the pinyins are polysyllabic.The experimental result is shown in Figure 4-2.Shoot RateChinese Word 0.964942English Top 1 0.794658English Top 5 0.932387English Top 1(Consideringmorphology)0.606845English Top 5(Consideringmorphology)0.834725Figure 4-2: Testing of Pinyin-English Word-levelTranslation4.2 Example Sentence RetrievalWe built a standard example sentences setwhich consists of 964 bilingual example sentencepairs.
We also created 50 Chinese-phrase queriesmanually based on the set.
Then we labelledevery sentence with the 50 queries.
For instance,let?s say that the example sentence is!
"#$%&'()*+,-./(He drewthe conclusion by building on his owninvestigation.
)After labelling, the corresponding queries are ?
'( )*?, and ?+ -.
?, that is, when a userinput these queries, the above example sentenceshould be picked out.After we labelled all 964 sentences, weperformed the sentence retrieval module on thesentence set, that is, PENS retrieved examplesentences for each of the 50 queries.
Therefore,for each query, we compared the sentence setretrieved by PENS with the sentence labelledmanually, and evaluate the performance byestimating the precision and the recall.Let A denotes the number of sentences which isselected by both human and the machine, Bdenotes the number of sentences which isselected only by the machine, and C denotes thenumber of sentences which is selected only byhuman.The precision of the retrieval to query i, sayPi, is estimated by Pi = A / B and the recall Ri, isestimated by Ri = A/C.
The average precisionis50501?==iiPP , and the average recall is50501?==iiRR .The experimental results are P = 83.3%, andR = 55.7%.
The user only cares if he could obtaina useful example sentence, and it is unnecessaryfor the system to find out all the relevantsentences in the bilingual sentence corpus.Therefore, example sentence retrieval in PENS isdifferent from conventional text retrieval at thispoint.ConclusionIn this paper, based on the comprehensivestudy of Chinese users requirements, we proposea unified approach to machine aided Englishwriting system, which consists of twocomponents: 1) a statistical approach to wordspelling help, and 2) an information retrievalbased approach to intelligent recommendation byproviding suggestive example sentences.
Whilethe former works at the word or phrase level, thelatter works at the sentence level.
Bothcomponents work together in a unified way, andhighly improve the productivity of Englishwriting.We also develop a pilot system, namelyPENS, where we try to find an efficient way inwhich human collaborate with computers.Although many components of PENS are underdevelopment, primary experiments on twostandard testing sets have already shown verypromising results.ReferencesMing Zhou, Sheng Li, Tiejun Zhao, Min Zhang,Xiaohu Liu, Meng Cai  1995 .
DEAR: Atranslator?s workstation.
In Proceedings ofNLPRS?95, Dec. 5-7, Seoul.Xin Liu, Ming Zhou, Shenghuo Zhu, ChangningHuang (1998), Aligning sentences in parallelcorpora using self-extracted lexical information,Chinese Journal of Computers (in Chinese), 1998,Vol.
21 (Supplement):151-158.Chen, Stanley F.(1993).
Aligning sentences inbilingual corpora using lexical infromation.
InProceedings of the 31st Annual Conference of theAssociation for Computational Linguistics, 9-16,Columbus, OH.Brown.
P.F., Jennifer C. Lai, and R.L.
Merce.
(1991).Aligning sentences in parallel corpora.InProceedings of the 29th Annual Conference of theAssociation for Computational Linguistics,169-176,Berkeley.Dekai Wu, Xuanyin Xia (1995).
Large-scaleautomatic extraction of an English-Chinesetranslation lexicon.
Machine Translation, 9:3-4,285-313 (1995)Church, K.W.
(1993), Char-align.
A program foraligning parallel texts at the character level.
InProceedings of the 31st Annual Conference of theAssociation for Computational Linguistics, 1-8,Columbus, OH.Dagan, I., K.W.
Church, and W.A.
Gale (1993)Robust bilingual word alignment for machine aidedtranslation.
In Proceedings of the workshop on VeryLarge Corpora, 69-85, Kyoto, Auguest.Jianfeng Gao, Han-Feng Wang, Mingjing Li, andKai-Fu Lee, 2000.
A Unified Approach to StatisticalLanguage Modeling for Chinese.
In IEEE,ICASPP2000.Brown, P. F., S. A. DellaPietra, V.J.
Dellapietra, andR.L.Mercer.
1993.
The Mathematics of StatisticalMachine Translation: Parameter Estimation.Computational Linguistics, 19(2): 263-311Lee-Feng Chien, 1998.
PAT-Tree-Based AdaptiveKeyphrase Extraction for Intelligent ChineseInformation Retrieval.
Special issue on?Information Retrieval with Asian Language?Information Processing and Management, 1998.
