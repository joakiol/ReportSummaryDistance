Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 492?501,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsA Multi-Pass Sieve for Coreference ResolutionKarthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers,Mihai Surdeanu, Dan Jurafsky, Christopher ManningComputer Science DepartmentStanford University, Stanford, CA 94305{kr,heeyoung,sudarshn,natec,mihais,jurafsky,manning}@stanford.eduAbstractMost coreference resolution models determineif two mentions are coreferent using a singlefunction over a set of constraints or features.This approach can lead to incorrect decisionsas lower precision features often overwhelmthe smaller number of high precision ones.
Toovercome this problem, we propose a simplecoreference architecture based on a sieve thatapplies tiers of deterministic coreference mod-els one at a time from highest to lowest preci-sion.
Each tier builds on the previous tier?sentity cluster output.
Further, our model prop-agates global information by sharing attributes(e.g., gender and number) across mentions inthe same cluster.
This cautious sieve guar-antees that stronger features are given prece-dence over weaker ones and that each deci-sion is made using all of the information avail-able at the time.
The framework is highlymodular: new coreference modules can beplugged in without any change to the othermodules.
In spite of its simplicity, our ap-proach outperforms many state-of-the-art su-pervised and unsupervised models on severalstandard corpora.
This suggests that sieve-based approaches could be applied to otherNLP tasks.1 IntroductionRecent work on coreference resolution has shownthat a rich feature space that models lexical, syn-tactic, semantic, and discourse phenomena is cru-cial to successfully address the task (Bengston andRoth, 2008; Haghighi and Klein, 2009; Haghighiand Klein, 2010).
When such a rich representationis available, even a simple deterministic model canachieve state-of-the-art performance (Haghighi andKlein, 2009).By and large most approaches decide if two men-tions are coreferent using a single function over allthese features and information local to the two men-tions.1 This is problematic for two reasons: (1)lower precision features may overwhelm the smallernumber of high precision ones, and (2) local infor-mation is often insufficient to make an informed de-cision.
Consider this example:The second attack occurred after some rocket firingsaimed, apparently, toward [the israelis], apparently inretaliation.
[we]?re checking our facts on that one.
...the president, quoted by ari fleischer, his spokesman, issaying he?s concerned the strike will undermine effortsby palestinian authorities to bring an end to terrorist at-tacks and does not contribute to the security of [israel].Most state-of-the-art models will incorrectly linkwe to the israelis because of their proximity andcompatibility of attributes (both we and the israelisare plural).
In contrast, a more cautious approach isto first cluster the israelis with israel because the de-monymy relation is highly precise.
This initial clus-tering step will assign the correct animacy attribute(inanimate) to the corresponding geo-politicalentity, which will prevent the incorrect merging withthe mention we (animate) in later steps.We propose an unsupervised sieve-like approachto coreference resolution that addresses these is-1As we will discuss below, some approaches use an addi-tional component to infer the overall best mention clusters for adocument, but this is still based on confidence scores assignedusing local information.492sues.
The approach applies tiers of coreferencemodels one at a time from highest to lowest pre-cision.
Each tier builds on the entity clusters con-structed by previous models in the sieve, guarantee-ing that stronger features are given precedence overweaker ones.
Furthermore, each model?s decisionsare richly informed by sharing attributes across thementions clustered in earlier tiers.
This ensures thateach decision uses all of the information availableat the time.
We implemented all components in ourapproach using only deterministic models.
All ourcomponents are unsupervised, in the sense that theydo not require training on gold coreference links.The contributions of this work are the following:?
We show that a simple scaffolding frameworkthat deploys strong features through tiers ofmodels performs significantly better than asingle-pass model.
Additionally, we proposeseveral simple, yet powerful, new features.?
We demonstrate how far one can get with sim-ple, deterministic coreference systems that donot require machine learning or detailed se-mantic information.
Our approach outperformsmost other unsupervised coreference modelsand several supervised ones on several datasets.?
Our modular framework can be easily extendedwith arbitrary models, including statistical orsupervised models.
We believe that our ap-proach also serves as an ideal platform for thedevelopment of future coreference systems.2 Related WorkThis work builds upon the recent observation thatstrong features outweigh complex models for coref-erence resolution, in both supervised and unsuper-vised learning setups (Bengston and Roth, 2008;Haghighi and Klein, 2009).
Our work reinforces thisobservation, and extends it by proposing a novel ar-chitecture that: (a) allows easy deployment of suchfeatures, and (b) infuses global information that canbe readily exploited by these features or constraints.Most coreference resolution approaches performthe task by aggregating local decisions about pairsof mentions (Bengston and Roth, 2008; Finkel andManning, 2008; Haghighi and Klein, 2009; Stoy-anov, 2010).
Two recent works that diverge fromthis pattern are Culotta et al (2007) and Poon andDomingos (2008).
They perform coreference reso-lution jointly for all mentions in a document, usingfirst-order probabilistic models in either supervisedor unsupervised settings.
Haghighi and Klein (2010)propose a generative approach that models entityclusters explicitly using a mostly-unsupervised gen-erative model.
As previously mentioned, our workis not constrained by first-order or Bayesian for-malisms in how it uses cluster information.
Ad-ditionally, the deterministic models in our tieredmodel are significantly simpler, yet perform gener-ally better than the complex inference models pro-posed in these works.From a high level perspective, this work falls un-der the theory of shaping, defined as a ?method ofsuccessive approximations?
for learning (Skinner,1938).
This theory is known by different names inmany NLP applications: Brown et al (1993) usedsimple models as ?stepping stones?
for more com-plex word alignment models; Collins (1999) used?cautious?
decision list learning for named entityclassification; Spitkovsky et al (2010) used ?babysteps?
for unsupervised dependency parsing, etc.
Tothe best of our knowledge, we are the first to applythis theory to coreference resolution.3 Description of the TaskIntra-document coreference resolution clusters to-gether textual mentions within a single documentbased on the underlying referent entity.
Mentionsare usually noun phrases (NPs) headed by nominalor pronominal terminals.
To facilitate comparisonwith most of the recent previous work, we report re-sults using gold mention boundaries.
However, ourapproach does not make any assumptions about theunderlying mentions, so it is trivial to adapt it to pre-dicted mention boundaries (e.g., see Haghighi andKlein (2010) for a simple mention detection model).3.1 CorporaWe used the following corpora for development andevaluation:?
ACE2004-ROTH-DEV2 ?
development splitof Bengston and Roth (2008), from the corpusused in the 2004 Automatic Content Extraction(ACE) evaluation.
It contains 68 documentsand 4,536 mentions.2We use the same corpus names as (Haghighi and Klein,2009) to facilitate comparison with previous work.493?
ACE2004-CULOTTA-TEST ?
partition ofACE 2004 corpus reserved for testing by sev-eral previous works (Culotta et al, 2007;Bengston and Roth, 2008; Haghighi and Klein,2009).
It consists of 107 documents and 5,469mentions.?
ACE2004-NWIRE ?
the newswire subset ofthe ACE 2004 corpus, utilized by Poon andDomingos (2008) and Haghighi and Klein(2009) for testing.
It contains 128 documentsand 11,413 mentions.?
MUC6-TEST ?
test corpus from the sixthMessage Understanding Conference (MUC-6)evaluation.
It contains 30 documents and 2,068mentions.We used the first corpus (ACE2004-ROTH-DEV)for development.
The other corpora are reserved fortesting.
We parse all documents using the Stanfordparser (Klein and Manning, 2003).
The syntactic in-formation is used to identify the mention head wordsand to define the ordering of mentions in a givensentence (detailed in the next section).
For a faircomparison with previous work, we do not use goldnamed entity labels or mention types but, instead,take the labels provided by the Stanford named en-tity recognizer (NER) (Finkel et al, 2005).3.2 Evaluation MetricsWe use three evaluation metrics widely used in theliterature: (a) pairwise F1 (Ghosh, 2003) ?
com-puted over mention pairs in the same entity clus-ter; (b) MUC (Vilain et al, 1995) ?
which measureshow many predicted clusters need to be merged tocover the gold clusters; and (c) B3 (Amit and Bald-win, 1998) ?
which uses the intersection betweenpredicted and gold clusters for a given mention tomark correct mentions and the sizes of the the pre-dicted and gold clusters as denominators for preci-sion and recall, respectively.
We refer the interestedreader to (X. Luo, 2005; Finkel and Manning, 2008)for an analysis of these metrics.4 Description of the Multi-Pass SieveOur sieve framework is implemented as a succes-sion of independent coreference models.
We first de-scribe how each model selects candidate mentions,and then describe the models themselves.4.1 Mention ProcessingGiven a mention mi, each model may either declineto propose a solution (in the hope that one of thesubsequent models will solve it) or deterministicallyselect a single best antecedent from a list of pre-vious mentions m1, .
.
.
, mi?1.
We sort candidateantecedents using syntactic information provided bythe Stanford parser, as follows:Same Sentence ?
Candidates in the same sentenceare sorted using left-to-right breadth-first traversalof syntactic trees (Hobbs, 1977).
Figure 1 shows anexample of candidate ordering based on this traver-sal.
The left-to-right ordering favors subjects, whichtend to appear closer to the beginning of the sentenceand are more probable antecedents.
The breadth-first traversal promotes syntactic salience by rank-ing higher noun phrases that are closer to the top ofthe parse tree (Haghighi and Klein, 2009).
If thesentence containing the anaphoric mention containsmultiple clauses, we repeat the above heuristic sep-arately in each S* constituent, starting with the onecontaining the mention.Previous Sentence ?
For all nominal mentions wesort candidates in the previous sentences using right-to-left breadth-first traversal.
This guarantees syn-tactic salience and also favors document proximity.For pronominal mentions, we sort candidates in pre-vious sentences using left-to-right traversal in or-der to favor subjects.
Subjects are more probableantecedents for pronouns (Kertz et al, 2006).
Forexample, this ordering favors the correct candidate(pepsi) for the mention they:[pepsi] says it expects to double [quaker]?ssnack food growth rate.
after a month-longcourtship, [they] agreed to buy quaker oats.
.
.In a significant departure from previous work,each model in our framework gets (possibly incom-plete) clustering information for each mention fromthe earlier coreference models in the multi-pass sys-tem.
In other words, each mention mi may alreadybe assigned to a cluster Cj containing a set of men-tions: Cj = {mj1, .
.
.
,mjk}; mi ?
Cj .
Unassignedmentions are unique members of their own cluster.We use this information in several ways:Attribute sharing ?
Pronominal coreference reso-lution (discussed later in this section) is severely af-494S?of?will?head?NP?Richard?Levin?the?Globaliza?on?Studies?Center?NP?NP?the?Chancelor?NP?,?VP?NP?PP?this?pres?gious?university?NP?VP?#1?#2?#3?#4?Figure 1: Example of left-to-right breadth-first treetraversal.
The numbers indicate the order in which theNPs are visited.fected by missing attributes (which introduce pre-cision errors because incorrect antecedents are se-lected due to missing information) and incorrect at-tributes (which introduce recall errors because cor-rect links are not generated due to attribute mismatchbetween mention and antecedent).
To address thisissue, we perform a union of all mention attributes(e.g., number, gender, animacy) in a given clusterand share the result with all cluster mentions.
Ifattributes from different mentions contradict eachother we maintain all variants.
For example, ournaive number detection assigns singular to themention a group of students and plural to five stu-dents.
When these mentions end up in the same clus-ter, the resulting number attributes becomes the set{singular, plural}.
Thus this cluster can laterbe merged with both singular and plural pronouns.Mention selection ?
Traditionally, a coreferencemodel attempts to resolve every mention in the text,which increases the likelihood of errors.
Instead, ineach of our models, we exploit the cluster informa-tion received from the previous stages by resolvingonly mentions that are currently first in textual orderin their cluster.
For example, given the following or-dered list of mentions, {m11, m22, m23, m34, m15, m26},where the superscript indicates cluster id, our modelwill attempt to resolve only m22 and m34.
These twoare the only mentions that have potential antecedentsand are currently marked as the first mentions intheir clusters.
The intuition behind this heuristicis two-fold.
First, early cluster mentions are usu-ally better defined than subsequent ones, which arelikely to have fewer modifiers or are pronouns (Fox,1993).
Several of our models use this modifier infor-mation.
Second, by definition, first mentions appearcloser to the beginning of the document, hence thereare fewer antecedent candidates to select from, andfewer opportunities to make a mistake.Search Pruning ?
Finally, we prune the searchspace using discourse salience.
We disable coref-erence for first cluster mentions that: (a) are or startwith indefinite pronouns (e.g., some, other), or (b)start with indefinite articles (e.g., a, an).
One excep-tion to this rule is the model deployed in the firstpass; it only links mentions if their entire extentsmatch exactly.
This model is triggered for all nom-inal mentions regardless of discourse salience, be-cause it is possible that indefinite mentions are re-peated in a document when concepts are discussedbut not instantiated, e.g., a sports bar below:Hanlon, a longtime Broncos fan, thinks it is the perfectplace for [a sports bar] and has put up a blue-and-orangesign reading, ?Wanted Broncos Sports Bar On This Site.?.
.
.
In a Nov. 28 letter, Proper states ?while we have noobjection to your advertising the property as a locationfor [a sports bar], using the Broncos?
name and colorsgives the false impression that the bar is or can be affili-ated with the Broncos.
?4.2 The Modules of the Multi-Pass SieveWe now describe the coreference models imple-mented in the sieve.
For clarity, we summarize themin Table 1 and show the cumulative performance asthey are added to the sieve in Table 2.4.2.1 Pass 1 - Exact MatchThis model links two mentions only if they con-tain exactly the same extent text, including modifiersand determiners, e.g., the Shahab 3 ground-groundmissile.
As expected, this model is extremely pre-cise, with a pairwise precision over 96%.4.2.2 Pass 2 - Precise ConstructsThis model links two mentions if any of the con-ditions below are satisfied:Appositive ?
the two nominal mentions are in anappositive construction, e.g., [Israel?s Deputy De-fense Minister], [Ephraim Sneh] , said .
.
.
Weuse the same syntactic rules to detect appositions asHaghighi and Klein (2009).495Pass Type Features1 N exact extent match2 N,P appositive | predicate nominative | role appositive | relative pronoun | acronym | demonym3 N cluster head match & word inclusion & compatible modifiers only & not i-within-i4 N cluster head match & word inclusion & not i-within-i5 N cluster head match & compatible modifiers only & not i-within-i6 N relaxed cluster head match & word inclusion & not i-within-i7 P pronoun matchTable 1: Summary of passes implemented in the sieve.
The Type column indicates the type of coreference in eachpass: N ?
nominal or P ?
pronominal.
& and | indicate conjunction and disjunction of features, respectively.Predicate nominative ?
the two mentions (nominalor pronominal) are in a copulative subject-object re-lation, e.g., [The New York-based College Board] is[a nonprofit organization that administers the SATsand promotes higher education] (Poon and Domin-gos, 2008).Role appositive ?
the candidate antecedent isheaded by a noun and appears as a modifier in anNP whose head is the current mention, e.g., [[ac-tress] Rebecca Schaeffer].
This feature is inspiredby Haghighi and Klein (2009), who triggered it onlyif the mention is labeled as a person by the NER.We constrain this heuristic more in our work: weallow this feature to match only if: (a) the mentionis labeled as a person, (b) the antecedent is animate(we detail animacy detection in Pass 7), and (c) theantecedent?s gender is not neutral.Relative pronoun ?
the mention is a relative pro-noun that modifies the head of the antecedent NP,e.g., [the finance street [which] has already formedin the Waitan district].Acronym ?
both mentions are tagged as NNP andone of them is an acronym of the other, e.g., [AgenceFrance Presse] .
.
.
[AFP].
We use a simple acronymdetection algorithm, which marks a mention as anacronym of another if its text equals the sequenceof upper case characters in the other mention.
Wewill adopt better solutions for acronym detection infuture work (Schwartz, 2003).Demonym ?
one of the mentions is a demonym ofthe other, e.g., [Israel] .
.
.
[Israeli].
For demonymdetection we use a static list of countries and theirgentilic forms from Wikipedia.3All the above features are extremely precise.
Asshown in Table 2 the pairwise precision of the sieve3http://en.wikipedia.org/wiki/List_of_adjectival_and_demonymic_forms_of_place_namesafter adding these features is over 95% and recallincreases 5 points.4.2.3 Pass 3 - Strict Head MatchingLinking a mention to an antecedent based on thenaive matching of their head words generates a lotof spurious links because it completely ignores pos-sibly incompatible modifiers (Elsner and Charniak,2010).
For example, Yale University and HarvardUniversity have similar head words, but they are ob-viously different entities.
To address this issue, thispass implements several features that must all bematched in order to yield a link:Cluster head match ?
the mention head wordmatches any head word in the antecedent clus-ter.
Note that this feature is actually more relaxedthan naive head matching between mention and an-tecedent candidate because it is satisfied when themention?s head matches the head of any entity in thecandidate?s cluster.
We constrain this feature by en-forcing a conjunction with the features below.Word inclusion ?
all the non-stop4 words in themention cluster are included in the set of non-stopwords in the cluster of the antecedent candidate.This heuristic exploits the property of discourse thatit is uncommon to introduce novel information inlater mentions (Fox, 1993).
Typically, mentionsof the same entity become shorter and less infor-mative as the narrative progresses.
For example,the two mentions in .
.
.
intervene in the [FloridaSupreme Court]?s move .
.
.
does look like very dra-matic change made by [the Florida court] point tothe same entity, but the two mentions in the text be-low belong to different clusters:The pilot had confirmed .
.
.
he had turned onto4Our stop word list includes person titles as well.496MUC B3 PairwisePasses P R F1 P R F1 P R F1{1} 95.9 31.8 47.8 99.1 53.4 69.4 96.9 15.4 26.6{1,2} 95.4 43.7 59.9 98.5 58.4 73.3 95.7 20.6 33.8{1,2,3} 92.1 51.3 65.9 96.7 62.9 76.3 91.5 26.8 41.5{1,2,3,4} 91.7 51.9 66.3 96.5 63.5 76.6 91.4 27.8 42.7{1,2,3,4,5} 91.1 52.6 66.7 96.1 63.9 76.7 90.3 28.4 43.2{1,2,3,4,5,6} 89.5 53.6 67.1 95.3 64.5 76.9 88.8 29.2 43.9{1,2,3,4,5,6,7} 83.7 74.1 78.6 88.1 74.2 80.5 80.1 51.0 62.3Table 2: Cumulative performance on development (ACE2004-ROTH-DEV) as passes are added to the sieve.
[the correct runway] but pilots behind him sayhe turned onto [the wrong runway].Compatible modifiers only ?
the mention?s mod-ifiers are all included in the modifiers of the an-tecedent candidate.
This feature models the samediscourse property as the previous feature, but it fo-cuses on the two individual mentions to be linked,rather than their entire clusters.
For this feature weonly use modifiers that are nouns or adjectives.Not i-within-i ?
the two mentions are not in an i-within-i construct, i.e., one cannot be a child NPin the other?s NP constituent (Haghighi and Klein,2009).This pass continues to maintain high precision(91% pairwise) while improving recall significantly(over 6 points pairwise and almost 8 points MUC).4.2.4 Passes 4 and 5 - Variants of Strict HeadPasses 4 and 5 are different relaxations of thefeature conjunction introduced in Pass 3, i.e.,Pass 4 removes the compatible modifiersonly feature, while Pass 5 removes the wordinclusion constraint.
All in all, these two passesyield an improvement of 1.7 pairwise F1 points,due to recall improvements.
Table 2 shows that theword inclusion feature is more precise thancompatible modifiers only, but the latterhas better recall.4.2.5 Pass 6 - Relaxed Head MatchingThis pass relaxes the cluster head match heuris-tic by allowing the mention head to match any wordin the cluster of the candidate antecedent.
For ex-ample, this heuristic matches the mention Sandersto a cluster containing the mentions {Sauls, thejudge, Circuit Judge N. Sanders Sauls}.
To maintainhigh precision, this pass requires that both mentionand antecedent be labeled as named entities and thetypes coincide.
Furthermore, this pass implementsa conjunction of the above features with wordinclusion and not i-within-i.
This passyields less than 1 point improvement in most met-rics.4.2.6 Pass 7 - PronounsWith one exception (Pass 2), all the previouscoreference models focus on nominal coreferenceresolution.
However, it would be incorrect to saythat our framework ignores pronominal coreferencein the first six passes.
In fact, the previous mod-els prepare the stage for pronominal coreference byconstructing precise clusters with shared mention at-tributes.
These are crucial factors for pronominalcoreference.Like previous work, we implement pronominalcoreference resolution by enforcing agreement con-straints between the coreferent mentions.
We use thefollowing attributes for these constraints:Number ?
we assign number attributes based on:(a) a static list for pronouns; (b) NER labels: men-tions marked as a named entity are considered sin-gular with the exception of organizations, which canbe both singular or plural; (c) part of speech tags:NN*S tags are plural and all other NN* tags are sin-gular; and (d) a static dictionary from (Bergsma andLin, 2006).Gender ?
we assign gender attributes from staticlexicons from (Bergsma and Lin, 2006; Ji and Lin,2009).Person ?
we assign person attributes only to pro-nouns.
However, we do not enforce this constraintwhen linking two pronouns if one appears withinquotes.
This is a simple heuristic for speaker de-tection, e.g., I and she point to the same person in497?
[I] voted my conscience,?
[she] said.Animacy ?
we set animacy attributes using: (a)a static list for pronouns; (b) NER labels, e.g.,PERSON is animate whereas LOCATION is not; and(c) a dictionary boostrapped from the web (Ji andLin, 2009).NER label ?
from the Stanford NER.If we cannot detect a value, we set attributes tounknown and treat them as wildcards, i.e., they canmatch any other value.This final model raises the pairwise recall of oursystem almost 22 percentage points, with only an 8point drop in pairwise precision.
Table 2 shows thatsimilar behavior is measured for all other metrics.After all passes have run, we take the transitive clo-sure of the generated clusters as the system output.5 Experimental ResultsWe present the results of our approach and other rel-evant prior work in Table 3.
We include in the ta-ble all recent systems that report results under thesame conditions as our experimental setup (i.e., us-ing gold mentions) and use the same corpora.
Weexclude from this analysis two notable works thatreport results only on a version of the task that in-cludes finding mentions (Haghighi and Klein, 2010;Stoyanov, 2010).
The Haghighi and Klein (2009)numbers have two variants: with semantics (+S)and without (?S).
To measure the contribution ofour multi-pass system, we also present results from asingle-pass variant of our system that uses all appli-cable features from the multi-pass system (markedas ?single pass?
in the table).Our sieve model outperforms all systems ontwo out of the four evaluation corpora (ACE2004-ROTH-DEV and ACE2004-NWIRE), on all met-rics.
On the corpora where our model is not best,it ranks a close second.
For example, in ACE2004-CULOTTA-TEST our system has a B3 F1 scoreonly .4 points lower than Bengston and Roth (2008)and it outperforms all unsupervised approaches.
InMUC6-TEST, our sieve?s B3 F1 score is 1.8 pointslower than Haghighi and Klein (2009) +S, but it out-performs a supervised system that used gold namedentity labels.
Finally, the multi-pass architecture al-ways beats the equivalent single-pass system withits contribution ranging between 1 and 4 F1 pointsdepending on the corpus and evaluation metric.Our approach has the highest precision on all cor-pora, regardless of evaluation metric.
We believethis is particularly useful for large-scale NLP appli-cations that use coreference resolution components,e.g., question answering or information extraction.These applications can generally function withoutcoreference information so it is beneficial to providesuch information only when it is highly precise.6 Discussion6.1 Comparison to Previous WorkThe sieve model outperforms all other systems onat least two test sets, even though most of the othermodels are significantly richer.
Amongst the com-parisons, several are supervised (Bengston and Roth,2008; Finkel and Manning, 2008; Culotta et al,2007).
The system of Haghighi and Klein (2009)+S uses a lexicon of semantically-compatible nounpairs acquired transductively, i.e., with knowledgeof the mentions in the test set.
Our system doesnot rely on labeled corpora for training (like super-vised approaches) nor access to corpora during test-ing (like Haghighi and Klein (2009)).The system that is closest to ours is Haghighi andKlein (2009) ?S.
Like us, they use a rich set of fea-tures and deterministic decisions.
However, theirsis a single-pass model with a smaller feature set(no cluster-level, acronym, demonym, or animacyinformation).
Table 3 shows that on the two cor-pora where results for this system are available, weoutperform it considerably on all metrics.
To un-derstand if the difference is due to the multi-passarchitecture or the richer feature set we compared(Haghighi and Klein, 2009) ?S against both ourmulti-pass system and its single-pass variant.
Thecomparison indicates that both these contributionshelp: our single-pass system outperforms Haghighiand Klein (2009) consistently, and the multi-pass ar-chitecture further improves the performance of oursingle-pass system between 1 and 4 F1 points, de-pending on the corpus and evaluation metric.6.2 Semantic Head MatchingRecent unsupervised coreference work fromHaghighi and Klein (2009) included a novelsemantic component that matched related headwords (e.g., AOL is a company) learned from select498MUC B3 PairwiseP R F1 P R F1 P R F1ACE2004-ROTH-DEVThis work (sieve) 83.7 74.1 78.6 88.1 74.2 80.5 80.1 51.0 62.3This work (single pass) 82.2 72.6 77.1 86.8 72.6 79.1 76.0 47.6 58.5Haghighi and Klein (2009) ?S 78.3 70.5 74.2 84.0 71.0 76.9 71.3 45.4 55.5Haghighi and Klein (2009) +S 77.9 74.1 75.9 81.8 74.3 77.9 68.2 51.2 58.5ACE2004-CULOTTA-TESTThis work (sieve) 80.4 71.8 75.8 86.3 75.4 80.4 71.6 46.2 56.1This work (single pass) 78.4 69.2 73.5 85.1 73.9 79.1 69.5 44.1 53.9Haghighi and Klein (2009) ?S 74.3 66.4 70.2 83.6 71.0 76.8 66.4 38.0 48.3Haghighi and Klein (2009) +S 74.8 77.7 79.6 79.6 78.5 79.0 57.5 57.6 57.5Culotta et al (2007) ?
?
?
86.7 73.2 79.3 ?
?
?Bengston and Roth (2008) 82.7 69.9 75.8 88.3 74.5 80.8 55.4 63.7 59.2MUC6-TESTThis work (sieve) 90.5 68.0 77.7 91.2 61.2 73.2 90.3 53.3 67.1This work (single pass) 89.3 65.9 75.8 90.2 58.8 71.1 89.5 50.6 64.7Haghighi and Klein (2009) +S 87.2 77.3 81.9 84.7 67.3 75.0 80.5 57.8 67.3Poon and Domingos (2008) 83.0 75.8 79.2 ?
?
?
63.0 57.0 60.0Finkel and Manning (2008) +G 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5ACE2004-NWIREThis work (sieve) 83.8 73.2 78.1 87.5 71.9 78.9 79.6 46.2 58.4This work (single pass) 82.2 71.5 76.5 86.2 70.0 77.3 76.9 41.9 54.2Haghighi and Klein (2009) +S 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7Poon and Domingos (2008) 71.3 70.5 70.9 ?
?
?
62.6 38.9 48.0Finkel and Manning (2008) +G 78.7 58.5 67.1 86.8 65.2 74.5 76.1 44.2 55.9Table 3: Results using gold mention boundaries.
Where available, we show results for a given corpus grouped intwo blocks: the top block shows results of unsupervised systems and the bottom block contains supervised systems.Bold numbers indicate best results in a given block.
+/-S indicates if the (Haghighi and Klein, 2009) system in-cludes/excludes their semantic component.
+G marks systems that used gold NER labels.wikipedia articles.
They first identified articlesrelevant to the entity mentions in the test set, andthen bootstrapped from known syntactic patternsfor apposition and predicate-nominatives in order tolearn a database of related head pairs.
They showimpressive gains by using these learned pairs incoreference decisions.
This type of learning usingtest set mentions is often described as transductive.Our work instead focuses on an approach thatdoes not require access to the dataset beforehand.We thus did not include a similar semantic compo-nent in our system, given that running a bootstrap-ping learner whenever a new data set is encounteredis not practical and, ultimately, reduces the usabilityof this NLP component.
However, our results showthat our sieve algorithm with minimal semantic in-formation still performs as well as the Haghighi andKlein (2009) system with semantics.6.3 Flexible ArchitectureThe sieve architecture offers benefits beyond im-proved accuracy.
Its modular design provides a flex-ibility for features that is not available in most su-pervised or unsupervised systems.
The sieve al-lows new features to be seamlessly inserted with-out affecting (or even understanding) the other com-ponents.
For instance, once a new high precisionfeature (or group of features) is inserted as its ownstage, it will benefit later stages with more preciseclusters, but it will not interfere with their particu-499lar algorithmic decisions.
This flexibility is in sharpcontrast to supervised classifiers that require theirmodels to be retrained on labeled data, and unsu-pervised systems that do not offer a clear insertionpoint for new features.
It can be difficult to fullyunderstand how a system makes a single decision,but the sieve allows for flexible usage with minimaleffort.6.4 Error AnalysisPronominal Nominal Proper TotalPronominal 49 / 237 116 / 317 104 / 595 269 / 1149Nominal 79 / 351 129 / 913 61 / 986 269 / 2250Proper 51 / 518 15 / 730 38 / 595 104 / 1843Total 179 / 1106 260 / 1960 203 / 2176 642 / 5242Table 4: Number of pair-wise errors produced by thesieve after transitive closure in the MUC6-TEST corpus.Rows indicate mention types; columns are types of an-tecedent.
Each cell shows the number of precision/recallerrors for that configuration.
The total number of goldlinks in MUC6-TEST is 11,236.Table 4 shows the number of incorrect pair-wiselinks generated by our system on the MUC6-TESTcorpus.
The table indicates that most of our er-rors are for nominal mentions.
For example, thecombined (precision plus recall) number of errorsfor proper or common noun mentions is three timeslarger than the number of errors made for pronom-inal mentions.
The table also highlights that mostof our errors are recall errors.
There are eight timesmore recall errors than precision errors in our output.This is a consequence of our decision to prioritizehighly precise features in the sieve.The above analysis illustrates that our next effortshould focus on improving recall.
In order to under-stand the limitations of our current system, we ran-domly selected 60 recall errors (20 for each mentiontype) and investigated their causes.
Not surprisingly,the causes are unique to each type.For proper nouns, 50% of recall errors are due tomention lengthening, mentions that are longer thantheir earlier mentions.
For example, Washington-based USAir appears after USAir in the text, so ourhead matching components skip it because their highprecision depends on disallowing new modifiers asthe discourse proceeds.
When the mentions were re-versed (as is the usual case), they match.The common noun recall errors are very differ-ent from proper nouns: 17 of the 20 random exam-ples can be classified as semantic knowledge.
Theseerrors are roughly evenly split between recognizingcategories of names (e.g., Gitano is an organizationname hence it should match the nominal antecedentthe company), and understanding hypernym rela-tions like settlements and agreements.Pronoun errors come in two forms.
Roughly 40%of these errors are attribute mismatches involvingsometimes ambiguous uses of gender and number(e.g., she with Pat Carney).
Another 40% are not se-mantic or attribute-based, but rather simply arise dueto the order in which we check potential antecedents.In all these situations, the correct links are missedbecause the system chooses a closer (incorrect) an-tecedent.These four highlighted errors (lengthening, se-mantics, attributes, ordering) add up to 77% of allrecall errors in the selected set.
In general, eacherror type is particular to a specific mention type.This suggests that recall improvements can be madeby focusing on one mention type without averselyaffecting the others.
Our sieve-based approach tocoreference uniquely allows for such new models tobe seamlessly inserted.7 ConclusionWe presented a simple deterministic approach tocoreference resolution that incorporates document-level information, which is typically exploited onlyby more complex, joint learning models.
Our sievearchitecture applies a battery of deterministic coref-erence models one at a time from highest to low-est precision, where each model builds on the pre-vious model?s cluster output.
Despite its simplicity,our approach outperforms or performs comparablyto the state of the art on several corpora.An additional benefit of the sieve framework is itsmodularity: new features or models can be insertedin the system with limited understanding of the otherfeatures already deployed.
Our code is publicly re-leased5 and can be used both as a stand-alone coref-erence system and as a platform for the developmentof future systems.5http://nlp.stanford.edu/software/dcoref.shtml500The strong performance of our system suggeststhe use of sieves in other NLP tasks for which a va-riety of very high-precision features can be designedand non-local features can be shared; likely candi-dates include relation and event extraction, templateslot filling, and author name deduplication.AcknowledgmentsWe gratefully acknowledge the support of theDefense Advanced Research Projects Agency(DARPA) Machine Reading Program under AirForce Research Laboratory (AFRL) prime contractno.
FA8750-09-C-0181.
Any opinions, findings,and conclusion or recommendations expressed inthis material are those of the author(s) and do notnecessarily reflect the view of DARPA, AFRL, orthe US government.Many thanks to Jenny Finkel for writing a reim-plementation of much of Haghighi and Klein (2009),which served as the starting point for the work re-ported here.
We also thank Nicholas Rizzolo andDan Roth for helping us replicate their experimen-tal setup, and Heng Ji and Dekang Lin for providingtheir gender lexicon.ReferencesB.
Amit and B. Baldwin.
1998.
Algorithms for scoringcoreference chains.
In MUC-7.E.
Bengston and D. Roth.
2008.
Understanding the valueof features for coreference resolution.
In EMNLP.S.
Bergsma and D. Lin.
2006.
Bootstrapping Path-BasedPronoun Resolution.
In ACL-COLING.P.F.
Brown, V.J.
Della Pietra, S.A. Della Pietra, and R.L.Mercer.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
Computational Lin-guistics, 19.M.
Collins and Y.
Singer.
1999.
Unsupervised modelsfor named entity classification.
In EMNLP-VLC.A.
Culotta, M. Wick, R. Hall, and A. McCallum.
2007.First-order probabilistic models for coreference reso-lution.
In NAACL-HLT.M.
Elsner and E. Charniak.
2010.
The same-head heuris-tic for coreference.
In ACL.J.
Finkel, T. Grenager, and C. Manning.
2005.
Incorpo-rating non-local information into information extrac-tion systems by Gibbs sampling.
In ACL.J.
Finkel and C. Manning.
2008.
Enforcing transitivityin coreference resolution.
In ACL.B.
A.
Fox 1993.
Discourse structure and anaphora:written and conversational English.
Cambridge Uni-versity Press.J.
Ghosh.
2003.
Scalable clustering methods for datamining.
Handbook of Data Mining, chapter 10, pages247?277.A.
Haghighi and D. Klein.
2009.
Simple coreferenceresolution with rich syntactic and semantic features.In EMNLP.A.
Haghighi and D. Klein.
2010.
Coreference resolutionin a modular, entity-centered model.
In HLT-NAACL.J.R.
Hobbs.
1977.
Resolving pronoun references.
Lin-gua.H.
Ji and D. Lin.
2009.
Gender and animacy knowl-edge discovery from web-scale n-grams for unsuper-vised person mention detection.
In PACLIC.L.
Kertz, A. Kehler, and J. Elman.
2006.
Grammaticaland Coherence-Based Factors in Pronoun Interpreta-tion.
In Proceedings of the 28th Annual Conference ofthe Cognitive Science Society.D.
Klein and C. Manning.
2003.
Accurate unlexicalizedparsing.
In ACL.X.
Luo.
2005.
On coreference resolution performancemetrics.
In HTL-EMNLP.H.
Poon and P. Domingos.
2008.
Joint unsuper-vised coreference resolution with Markov Logic.
InEMNLP.A.S.
Schwartz and M.A.
Hearst.
2003.
A simplealgorithm for identifying abbrevation definitions inbiomedical text.
In Pacific Symposium on Biocomput-ing.B.F.
Skinner.
1938.
The behavior of organisms: An ex-perimental analysis.
Appleton-Century-Crofts.V.I.
Spitkovsky, H. Alshawi, and D. Jurafsky.
2010.From baby steps to leapfrog: How ?less is more?
inunsupervised dependency parsing.
In NAACL.V.
Stoyanov, N. Gilbert, C. Cardie, and E. Riloff.
2010.Conundrums in noun phrase coreference resolution:making sense of the state-of-the-art.
In ACL-IJCNLP.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, and L.Hirschman.
1995.
A model-theoretic coreferencescoring scheme.
In MUC-6.501
