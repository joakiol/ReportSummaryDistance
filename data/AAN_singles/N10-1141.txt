Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 975?983,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsModel Combination for Machine TranslationJohn DeNero, Shankar Kumar, Ciprian Chelba, and Franz OchUC Berkeley Google, Inc.denero@berkeley.edu {shankarkumar,ciprianchelba,och}@google.comAbstractMachine translation benefits from two typesof decoding techniques: consensus decodingover multiple hypotheses under a single modeland system combination over hypotheses fromdifferent models.
We present model combina-tion, a method that integrates consensus de-coding and system combination into a uni-fied, forest-based technique.
Our approachmakes few assumptions about the underly-ing component models, enabling us to com-bine systems with heterogenous structure.
Un-like most system combination techniques, wereuse the search space of component models,which entirely avoids the need to align trans-lation hypotheses.
Despite its relative sim-plicity, model combination improves trans-lation quality over a pipelined approach offirst applying consensus decoding to individ-ual systems, and then applying system combi-nation to their output.
We demonstrate BLEUimprovements across data sets and languagepairs in large-scale experiments.1 IntroductionOnce statistical translation models are trained, a de-coding approach determines what translations are fi-nally selected.
Two parallel lines of research haveshown consistent improvements over the standardmax-derivation decoding objective, which selectsthe highest probability derivation.
Consensus de-coding procedures select translations for a singlesystem by optimizing for model predictions aboutn-grams, motivated either as minimizing Bayes risk(Kumar and Byrne, 2004), maximizing sentencesimilarity (DeNero et al, 2009), or approximating amax-translation objective (Li et al, 2009b).
Systemcombination procedures, on the other hand, generatetranslations from the output of multiple componentsystems (Frederking and Nirenburg, 1994).
In thispaper, we present model combination, a techniquethat unifies these two approaches by learning a con-sensus model over the n-gram features of multipleunderlying component models.Model combination operates over the compo-nent models?
posterior distributions over translationderivations, encoded as a forest of derivations.1 Wecombine these components by constructing a linearconsensus model that includes features from eachcomponent.
We then optimize this consensus modelover the space of all translation derivations in thesupport of all component models?
posterior distribu-tions.
By reusing the components?
search spaces,we entirely avoid the hypothesis alignment problemthat is central to standard system combination ap-proaches (Rosti et al, 2007).Forest-based consensus decoding techniques dif-fer in whether they capture model predictionsthrough n-gram posteriors (Tromble et al, 2008;Kumar et al, 2009) or expected n-gram counts(DeNero et al, 2009; Li et al, 2009b).
We evaluateboth in controlled experiments, demonstrating theirempirical similarity.
We also describe algorithms forexpanding translation forests to ensure that n-gramsare local to a forest?s hyperedges, and for exactlycomputing n-gram posteriors efficiently.Model combination assumes only that each trans-lation model can produce expectations of n-gramfeatures; the latent derivation structures of compo-nent systems can differ arbitrarily.
This flexibilityallows us to combine phrase-based, hierarchical, andsyntax-augmented translation models.
We evaluateby combining three large-scale systems on Chinese-English and Arabic-English NIST data sets, demon-strating improvements of up to 1.4 BLEU over the1In this paper, we use the terms translation forest and hyper-graph interchangeably.975I ... telescopeYo vi al hombre con el telescopioI ... saw the ... man with ... telescopethe ... telescope0.4?saw the?
?man with?0.6?saw the?1.0?man with?Step 1: Compute Single-Model N-gram FeaturesStep 2: Construct a Search SpaceStep 3: Add Features for the Combination ModelStep 4: Model Training and InferencePhrase-based system Hierarchical system......RRpbRhRRpbRh?saw the?
:[v2pb = 0.7, v2h = 1.0][?pb = 1] [?h = 1]v2pb(?saw the?)
= 0.7green witchwas hereblue witchgreen witch was hereblue witch was herewas heregreen witch was hereblue witch was heregreen witch blue witchv2h(?saw the?)
= 1.0w = arg maxwBLEU({arg maxd?D(f)sw(d)}; e)d?
= arg maxd?Dsw(d)I ... telescopeYo vi al hombre con el telescopioI ... saw the ... man with ... telescopethe ... telescope0.6?saw the?1.0?man with?I ... man0.4?telescope the?0.3?saw with?I ... telescope?I saw the man with the telescope?
?I saw with the telescope the man?Step 1: Compute Combination FeaturesStep 2: Construct a Search SpaceStep 3: Add Features for the Combination ModelStep 4: Model Training and InferencePhrase-based model Hierarchical model......RRpbRh[?pb = 1] [?h = 1]w = arg maxwBLEU({arg maxd?D(f)sw(d)}; e)d?
= arg maxd?Dsw(d)v2h(?saw the?)
= 0.7v2pb(?saw the?)
= 0.9?saw the?
:[v2pb = 0.9, v2h = 0.7]applied rulerule rootrule leavesn !?
P (n)Figure 1: An example translation forest encoding twosynchronous derivations for a Spanish sentence: one solidand one dotted.
Nodes are annotated with their left andright unigram contexts, and hyperedges are annotatedwith scores ?
?
?
(r) and the bigrams they introduce.best single systemmax-derivation baseline, and con-sistent improvements over a more complex multi-system pipeline that includes independent consensusdecoding and system combination.2 Model CombinationModel combination is a model-based approach to se-lecting translations using information from multiplecomponent systems.
Each system provides its poste-rior distributions over derivations Pi(d|f), encodedas a weighted translation forest (i.e., translation hy-pergraph) in which hyperedges correspond to trans-lation rule applications r.2 The conditional distribu-tion over derivations takes the form:Pi(d|f) =exp[?r?d ?i ?
?i(r)]?d?
?D(f) exp[?r?d?
?i ?
?i(r)]whereD(f) is the set of synchronous derivations en-coded in the forest, r iterates over rule applicationsin d, and ?i is the parameter vector for system i. Thefeature vector ?i is system specific and includes bothtranslation model and language model features.
Fig-ure 1 depicts an example forest.Model combination includes four steps, describedbelow.
The entire sequence is illustrated in Figure 2.2Phrase-based systems produce phrase lattices, which are in-stances of forests with arity 1.2.1 Computing Combination FeaturesThe first step in model combination is to com-pute n-gram expectations from component systemposteriors?the same quantities found in MBR, con-sensus, and variational decoding techniques.
For ann-gram g and system i, the expectationvni (g) = EPi(d|f) [h(d, g)]can be either an n-gram expected count, if h(d, g)is the count of g in d, or the posterior probabilitythat d contains g, if h(d, g) is an indicator function.Section 3 describes how to compute these featuresefficiently.2.2 Constructing a Search SpaceThe second step in model combination constructs ahypothesis space of translation derivations, whichincludes all derivations present in the forests con-tributed by each component system.
This searchspace D is also a translation forest, and consists ofthe conjoined union of the component forests.
LetRi be the root node of component hypergraph Di.For all i, we include all of Di in D, along with anedge from Ri to R, the root of D. D may containderivations from different types of translation sys-tems.
However, D only contains derivations (andtherefore translations) that appeared in the hypothe-sis space of some component system.
We do not in-termingle the component search spaces in any way.2.3 Features for the Combination ModelThe third step defines a new combination model overall of the derivations in the search space D, and thenannotates D with features that allow for efficientmodel inference.
We use a linear model over fourtypes of feature functions of a derivation:1.
Combination feature functions on n-gramsvni (d) =?g?Ngrams(d) vni (g) score a deriva-tion according to the n-grams it contains.2.
Model score feature function b gives the modelscore ?i ?
?i(d) of a derivation d under the sys-tem i that d is from.3.
A length feature ` computes the word length ofthe target-side yield of a derivation.4.
A system indicator feature ?i is 1 if the deriva-tion came from system i, and 0 otherwise.976All of these features are local to rule applications(hyperedges) in D. The combination features pro-vide information sharing across the derivations ofdifferent systems, but are functions of n-grams, andso can be scored on any translation forest.
Modelscore features are already local to rule applications.The length feature is scored in the standard way.System indicator features are scored only on the hy-peredges fromRi toR that link each component for-est to the common root.Scoring the joint search space D with these fea-tures involves annotating each rule application r (i.e.hyperedge) with the value of each feature.2.4 Model Training and InferenceWe have defined the following combination modelsw(d) with weights w over derivations d from I dif-ferent component models:I?i=1[4?n=1wni vni (d) + w?i ?i(d)]+wb?b(d)+w`?`(d)Because we have assessed all of these features onlocal rule applications, we can find the highest scor-ing derivation d?
= arg maxd?Dsw(d) using standardmax-sum (Viterbi) inference over D.We learn the weights of this consensus model us-ing hypergraph-based minimum-error-rate training(Kumar et al, 2009).
This procedure maximizes thetranslation quality of d?
on a held-out set, accordingto a corpus-level evaluation metric B(?
; e) that com-pares to a reference set e. We used BLEU, choosingw to maximize the BLEU score of the set of transla-tions predicted by the combination model.3 Computing Combination FeaturesThe combination features vni (d) score derivationsfrom each model with the n-gram predictions of theothers.
These predictions sum over all derivationsunder a single component model to compute a pos-terior belief about each n-gram.
In this paper, wecompare two kinds of combination features, poste-rior probabilities and expected counts.33The model combination framework could incorporate ar-bitrary features on the common output space of the models, butwe focus on features that have previously proven useful for con-sensus decoding.I ... telescopeYo vi al hombre con el telescopioI ... saw the ... man with ... telescopethe ... telescope0.4?saw the?
?man with?0.6?saw the?1.0?man with?Step 1: Compute Singl -Model N-gram FeaturesStep 2: Construct a Search SpaceStep 3: Add Features for the Combination ModelStep 4: Model Training and InferencePhrase-based system Hierarchical system......RRpbRhRRpbRh?saw the?
:[v2pb = 0.7, v2h = 1.0][?pb = 1] [?h = 1]v2pb(?saw the?)
= 0.7green witchwas hereblue witchgreen witch was hereblue witch was herewas heregreen witch was hereblue witch was heregreen witch blue witchv2h(?saw the?)
= 1.0w = arg maxwBLEU({arg maxd?D(f)sw(d)}; e)d?
= arg maxd?Dsw(d)I ... telescopeYo vi al hombre con el telescopioI ... saw the ... man with ... telescopethe ... telescope0.6?saw the?1.0?man with?I ... man0.4?telescope the?0.3?saw with?I ... telescope?I saw the man with the telescope?
?I saw with the telescope the man?Step 1: Compute Combination FeaturesStep 2: Construct a Search SpaceStep 3: Add Features for the Combination ModelStep 4: Model Training and InferencePhrase-based model Hierarchical model......RRpbRh[?pb = 1] [?h = 1]w = arg maxwBLEU({arg maxd?D(f)sw(d)}; e)d?
= arg maxd?Dsw(d)v2h(?saw the?)
= 0.7v2pb(?saw the?)
= 0.9?saw the?
:[v2pb = 0.9, v2h = 0.7]applied rulerule rootrule leavesn !?
P (n)Figure 2: Model combination applied to a phrase-based(pb) and a hierarchical model (h) includes four steps.
(1)shows an excerpt of the bigram feature function for eachcomponent, (2) depicts the result of conjoining a phraselattice with a hierarchical forest, (3) shows example hy-peredge features of the combination model, including bi-gram features vni and system indicators ?i, and (4) givestraining and decoding objectives.Posterior probabilities represent a model?s be-lief that the translation will contain a particular n-gram at least once.
They can be expressed asEP (d|f) [?
(d, g)] for an indicator function ?
(d, g)that is 1 if n-gram g appears in derivation d. Thesequantities arise in approximating BLEU for lattice-based and hypergraph-based minimum Bayes riskdecoding (Tromble et al, 2008; Kumar et al, 2009).Expected n-gram counts EP (d|f) [c(d, g)] representthe model?s belief of how many times an n-gram gwill appear in the translation.
These quantities ap-pear in forest-based consensus decoding (DeNero etal., 2009) and variational decoding (Li et al, 2009b).977Methods for computing both of these quantities ap-pear in the literature.
However, we address two out-standing issues below.
In Section 5, we also com-pare the two quantities experimentally.3.1 Computing N -gram Posteriors ExactlyKumar et al (2009) describes an efficient approx-imate algorithm for computing n-gram posteriorprobabilities.
Algorithm 1 is an exact algorithm thatcomputes all n-gram posteriors from a forest in asingle inside pass.
The algorithm tracks two quanti-ties at each node n: regular inside scores ?
(n) andn-gram inside scores ??
(n, g) that sum the scores ofall derivations rooted at n that contain n-gram g.For each hyperedge, we compute b?
(g), the sum ofscores for derivations that do not contain g (Lines 8-11).
We then use that quantity to compute the scoreof derivations that do contain g (Line 17).Algorithm 1 Computing n-gram posteriors1: for n ?
N in topological order do2: ?(n)?
03: ??
(n, g)?
0, ?g ?
Ngrams(n)4: for r ?
Rules(n) do5: w ?
exp [?
?
?
(r)]6: b?
w7: b?(g)?
w, ?g ?
Ngrams(n)8: for ` ?
Leaves(r) do9: b?
b?
?
(`)10: for g ?
Ngrams(n) do11: b?(g)?
b?(g)?(?(`)?
??
(`, g))12: ?(n)?
?
(n) + b13: for g ?
Ngrams(n) do14: if g ?
Ngrams(r) then15: ??
(n, g)?
??
(n, g)+b16: else17: ??
(n, g)?
??
(n, g)+b?
b?
(g)18: for g ?
Ngrams(root) (all g in the HG) do19: P (g|f)?
??(root,g)?
(root)This algorithm can in principle compute the pos-terior probability of any indicator function on localfeatures of a derivation.
More generally, this algo-rithm demonstrates how vector-backed inside passescan compute quantities beyond expectations of localfeatures (Li and Eisner, 2009).4 Chelba and Maha-jan (2009) developed a similar algorithm for lattices.4Indicator functions on derivations are not locally additive3.2 Ensuring N -gram LocalityDeNero et al (2009) describes an efficient algorithmfor computing n-gram expected counts from a trans-lation forest.
This method assumes n-gram local-ity of the forest, the property that any n-gram intro-duced by a hyperedge appears in all derivations thatinclude the hyperedge.
However, decoders may re-combine forest nodes whenever the language modeldoes not distinguish between n-grams due to back-off (Li and Khudanpur, 2008).
In this case, a forestencoding of a posterior distribution may not exhibitn-gram locality in all regions of the search space.Figure 3 shows a hypergraph which contains non-local trigrams, along with its local expansion.Algorithm 2 expands a forest to ensure n-gram lo-cality while preserving the encoded distribution overderivations.
Let a forest (N,R) consist of nodes Nand hyperedges R, which correspond to rule appli-cations.
Let Rules(n) be the subset of R rooted byn, and Leaves(r) be the leaf nodes of rule applica-tion r. The expanded forest (Ne, Re) is constructedby a function Reapply(r, L) that applies the rule of rto a new set of leavesL ?
Ne, forming a pair (r?, n?
)consisting of a new rule application r?
rooted by n?.P is a map from nodes in N to subsets of Ne whichtracks how N projects to Ne.
Two nodes in Ne areidentical if they have the same (n?1)-gram left andright contexts and are projections of the same nodein N .
The symbol?denotes a set cross-product.Algorithm 2 Expanding for n-gram locality1: Ne ?
{}; Re ?
{}2: for n ?
N in topological order do3: P (n)?
{}4: for r ?
Rules(n) do5: for L ?
?`?Leaves(r) [P (`)] do6: r?, n?
?
Reapply(r, L)7: P (n)?
P (n) ?
{n?
}8: Ne ?
Ne ?
{n?
}9: Re ?
Re ?
{r?
}This transformation preserves the original distri-bution over derivations by splitting states, but main-taining continuations from those split states by du-plicating rule applications.
The process is analogousover the rules of a derivation, even if the features they indicateare local.
Therefore, Algorithm 1 is not an instance of an ex-pectation semiring computation.978I ... telescopeYo vi al hombre con el telescopioI ... saw the ... man with ... telescopethe ... telescope0.4?saw the?
?man with?0.6?saw the?1.0?man with?Step 1: Compute Single-Model N-gram FeaturesStep 2: Construct a Search SpaceStep 3: Add Features for the Combination ModelStep 4: Model Training and InferencePhrase-based system Hierarchical system......RRpbRhRRpbRh?saw the?
: [v2pb = 0.7, v2h = 1.0][?pb = 1] [?h = 1]v2pb(?saw the?)
= 0.7green witchwas hereblue witchgreen witch was hereblue witch was herewas heregreen witch was hereblue witch was heregreen witch blue witchv2h(?saw the?)
= 1.0w = arg maxwBLEU({arg maxd?D(f)sw(d)}; e)d?
= arg maxd?Dsw(d)I ... telescopeYo vi al hombre con el telescopioI ... saw the ... man with ... telescopethe ... telescope0.6?saw the?1.0?man with?I ... man0.4?telescope the?0.3?saw with?I ... telescope?I saw the man with the telescope?
?I saw with the telescope the man?Step 1: Compute Combination FeaturesStep 2: Construct a Search SpaceStep 3: Add Features for the Combination ModelStep 4: Model Training and InferencePhrase-based model Hierarchical model......RRpbRh[?pb = 1] [?h = 1]w = arg maxwBLEU({arg maxd?D(f)sw(d)}; e)d?
= arg maxd?Dsw(d)v2h(?saw the?)
= 0.7v2pb(?saw the?)
= 0.9?saw the?
: [v2pb = 0.9, v2h = 0.7]applied rulerule rootrule leavesn !?
P (n)Figure 3: Hypergraph expansion ensures n-gram localitywithout affecting the distribution over derivations.
In theleft example, trigrams ?green witch was?
and ?blue witchwas?
are non-local due to language model back-off.
Onthe right, states are split to enforce trigram locality.to expanding bigram lattices to e code a trigram his-tory at each lattice node (Weng et al, 1998).4 Relationship to Prior WorkModel combination is a multi-system generaliza-tion of consensus or minimum Bayes risk decod-ing.
When only one component system is included,model combination is identical to minimum Bayesrisk decoding over hypergraphs, as described in Ku-mar et al (2009).54.1 System CombinationSystem combination techniques in machine trans-lat on take as input the outputs {e1, ?
?
?
, ek} of ktranslation systems, where ei is a structured transla-tion object (or k-best lists thereof), typically viewedas a sequence of words.
The dominant approach inthe field chooses a primary translation ep as a back-bone, then finds an alignment ai to the backbone foreach ei.
A new search space is constructed fromthese backbone-aligned outputs, and then a votingprocedure or feature-based model predicts a finalconsensus translation (Rosti et al, 2007).
Modelcombination entirely avoids this alignment problemby viewing hypotheses as n-gram occurrence vec-tors rather than word sequences.Model combination also requires less total com-putation than applying system combination to5We do not refer to model combination as a minimum Bayesrisk decoding procedure despite this similarity because risk im-plies a belief distribution over outputs, and we now have mul-tiple output distributions that are not necessarily calibrated.Moreover, our generalized, multi-model objective (Section 2.4)is motivated by BLEU, but not a direct approximation to it.consensus-decoded outputs.
The best consensus de-coding methods for individual systems already re-quire the computation-intensive steps of model com-bination: producing lattices or forests, computing n-gram feature expectations, and re-decoding to max-imize a secondary consensus objective.
Hence, tomaximize the performance of system combination,these steps must be performed for each system,whereas model combination requires only one for-est rescoring pass over all systems.Model combination also leverages aggregatestatistics from the components?
posteriors, whereassystem combiners typically do not.
Zhao and He(2009) showed that n-gram posterior features areuseful in the context of a system combination model,even when computed from k-best lists.Despite these advantages, system combinationmay be more appropriate in some settings.
In par-ticular, model combination is designed primarily forst tistical systems that generate hypergraph outputs.Model combination can in principle integrate a non-statisti al system that generates either a single hy-pothesis or an unweighted forest.6 Likewise, the pro-cedure c uld be applied to statistical systems thatonly generate k-best lists.
However, we would notexpect the same strong performance from modelcombination in these constrained settings.4.2 Joint Decoding and Collaborative DecodingLiu et al (2009) describes two techniques for com-bining multiple synchronous grammars, which theauthors characterize as joint decoding.
Joint de-coding does not involve a consensus or minimum-Bayes-risk decoding objective; indeed, their bestresults come from standard max-derivation decod-ing (with a multi-system grammar).
More impor-tantly, their computations rely on a correspondencebetween nodes in the hypergraph outputs of differ-ent systems, and so they can only joint decode overmodels with similar search strategies.
We combine aphrase-based model that uses left-to-right decodingwith two hierarchical systems that use bottom-up de-coding ?
a scenario to which joint decoding is notapplicable.
Though Liu et al (2009) rightly pointout that most models can be decoded either left-to-6A single hypothesis can be represented as a forest, while anunweighted forest could be assigned a uniform distribution.979right or bottom-up, such changes can have substan-tial implications for search efficiency and search er-ror.
We prefer to maintain the flexibility of using dif-ferent search strategies in each component system.Li et al (2009a) is another related technique forcombining translation systems by leveraging modelpredictions of n-gram features.
K-best lists of par-tial translations are iteratively reranked using n-gram features from the predictions of other mod-els (which are also iteratively updated).
Our tech-nique differs in that we use no k-best approxima-tions, have fewer parameters to learn (one consensusweight vector rather than one for each collaboratingdecoder) and produce only one output, avoiding anadditional system combination step at the end.5 ExperimentsWe report results on the constrained data track of theNIST 2008 Arabic-to-English (ar-en) and Chinese-to-English (zh-en) translation tasks.7 We train on allparallel and monolingual data allowed in the track.We use the NIST 2004 eval set (dev) for optimiz-ing parameters in model combination and test onthe NIST 2008 evaluation set.
We report resultsusing the IBM implementation of the BLEU scorewhich computes the brevity penalty using the clos-est reference translation for each segment (Papineniet al, 2002).
We measure statistical significance us-ing 95% confidence intervals computed using pairedbootstrap resampling.
In all table cells (except forTable 3) systems without statistically significant dif-ferences are marked with the same superscript.5.1 Base SystemsWe combine outputs from three systems.
Ourphrase-based system is similar to the alignment tem-plate system described by Och and Ney (2004).Translation is performed using a standard left-to-right beam-search decoder.
Our hierarchicalsystems consist of a syntax-augmented system(SAMT) that includes target-language syntactic cat-egories (Zollmann and Venugopal, 2006) and aHiero-style system with a single non-terminal (Chi-ang, 2007).
Each base system yields state-of-the-arttranslation performance, summarized in Table 1.7http://www.nist.gov/speech/tests/mtBLEU (%)ar-en zh-enSys Base dev nist08 dev nist08PB MAX 51.6 43.9 37.7 25.4PB MBR 52.4?
44.6?
38.6?
27.3?PB CON 52.4?
44.6?
38.7?
27.2?Hiero MAX 50.9 43.3 40.0 27.2Hiero MBR 51.4?
43.8?
40.6?
27.8Hiero CON 51.5?
43.8?
40.5?
28.2SAMT MAX 51.7 43.8 40.8?
28.4SAMT MBR 52.7?
44.5?
41.1?
28.8?SAMT CON 52.6?
44.4?
41.1?
28.7?Table 1: Performance of baseline systems.BLEU (%)ar-en zh-enApproach dev nist08 dev nist08Best MAX system 51.7 43.9 40.8 28.4Best MBR system 52.7 44.5 41.1 28.8?MC Conjoin/SI 53.5 45.3 41.6 29.0?Table 2: Performance from the best single system foreach language pair without consensus decoding (BestMAX system), the best system with minimum Bayes riskdecoding (Best MBR system), and model combinationacross three systems.For each system, we report the performance ofmax-derivation decoding (MAX), hypergraph-basedMBR (Kumar et al, 2009), and a linear version offorest-based consensus decoding (CON) (DeNero etal., 2009).
MBR and CON differ only in that the firstuses n-gram posteriors, while the second uses ex-pected n-gram counts.
The two consensus decodingapproaches yield comparable performance.
Hence,we report performance for hypergraph-based MBRin our comparison to model combination below.5.2 Experimental ResultsTable 2 compares model combination (MC) to thebest MAX and MBR systems.
Model combinationuses a conjoined search space wherein each hyper-edge is annotated with 21 features: 12 n-gram poste-rior features vni computed from the PB/Hiero/SAMTforests for n ?
4; 4 n-gram posterior features vncomputed from the conjoined forest; 1 length fea-ture `; 1 feature b for the score assigned by the basemodel; and 3 system indicator (SI) features ?i thatselect which base system a derivation came from.We refer to this model combination approach as MC980BLEU (%)ar-en zh-enStrategy dev nist08 dev nist08Best MBR system 52.7 44.5 41.1 28.8MBR Conjoin 52.3 44.5 40.5 28.3MBR Conjoin/feats-best 52.7 44.9 41.2 28.8MBR Conjoin/SI 53.1 44.9 41.2 28.9MC 1-best HG 52.7 44.6 41.1 28.7MC Conjoin 52.9 44.6 40.3 28.1MC Conjoin/base/SI 53.5 45.1 41.2 28.9MC Conjoin/SI 53.5 45.3 41.6 29.0Table 3: Model Combination experiments.Conjoin/SI.
Model combination improves over thesingle best MAX system by 1.4 BLEU in ar-en and0.6 BLEU in zh-en, and always improves over MBR.This improvement could arise due to multiple rea-sons: a bigger search space, the consensus featuresfrom constituent systems, or the system indicatorfeatures.
Table 3 teases apart these contributions.We first perform MBR on the conjoined hyper-graph (MBR-Conjoin).
In this case, each edge istagged with 4 conjoined n-gram features vn, alongwith length and base model features.
MBR-Conjoinis worse than MBR on the hypergraph from thesingle best system.
This could imply that eitherthe larger search space introduces poor hypothesesor that the n-gram posteriors obtained are weaker.When we now restrict the n-gram features to thosefrom the best system (MBR Conjoin/feats-best),BLEU scores increase relative to MBR-Conjoin.This implies that the n-gram features computed overthe conjoined hypergraph are weaker than the corre-sponding features from the best system.Adding system indicator features (MBR Con-join+SI) helps the MBR-Conjoin system consider-ably; the resulting system is better than the bestMBR system.
This could mean that the SI featuresguide search towards stronger parts of the largersearch space.
In addition, these features provide anormalization of scores across systems.We next do several model-combination experi-ments.
We perform model combination using thesearch space of only the best MBR system (MC1best HG).
Here, the hypergraph is annotated withn-gram features from the 3 base systems, as well aslength and base model features.
A total of 3 ?
4 +1 + 1 = 14 features are added to each edge.
Sur-BLEU (%)ar-en zh-enApproach Base dev nist08 dev nist08Sent-level MAX 51.8?
44.4?
40.8?
28.2?Word-level MAX 52.0?
44.4?
40.8?
28.1?Sent-level MBR 52.7+ 44.6?
41.2 28.8+Word-level MBR 52.5+ 44.7?
40.9 28.8+MC-conjoin-SI 53.5 45.3 41.6 29.0+Table 4: BLEU performance for different system andmodel combination approaches.
Sentence-level andword-level system combination operate over the sentenceoutput of the base systems, which are either decoded tomaximize derivation score (MAX) or to minimize Bayesrisk (MBR).prisingly, n-gram features from the additional sys-tems did not help select a better hypothesis withinthe search space of a single system.When we expand the search space to the con-joined hypergraph (MC Conjoin), it performs worserelative to MC 1-best.
Since these two systems areidentical in their feature set, we hypothesize thatthe larger search space has introduced erroneous hy-potheses.
This is similar to the scenario where MBRConjoin is worse than MBR 1-best.
As in the MBRcase, adding system indicator features helps (MCConjoin/base/SI).
The result is comparable to MBRon the conjoined hypergraph with SI features.We finally add extra n-gram features which arecomputed from the conjoined hypergraph (MC Con-join + SI).
This gives the best performance althoughthe gains over MC Conjoin/base/SI are quite small.Note that these added features are the same n-gramfeatures used in MBR Conjoin.
Although they arenot strong by themselves, they provide additionaldiscriminative power by providing a consensus scoreacross all 3 base systems.5.3 Comparison to System CombinationTable 4 compares model combination to two sys-tem combination algorithms.
The first, which wecall sentence-level combination, chooses among thebase systems?
three translations the sentence thathas the highest consensus score.
The second, word-level combination, builds a ?word sausage?
fromthe outputs of the three systems and chooses a paththrough the sausage with the highest score undera similar model (Macherey and Och, 2007).
Nei-981BLEU (%)ar-en zh-enApproach dev nist08 dev nist08HG-expand 52.7?
44.5?
41.1?
28.8?HG-noexpand 52.7?
44.5?
41.1?
28.8?Table 5: MBR decoding on the syntax augmented system,with and without hypergraph expansion.ther system combination technique provides muchbenefit, presumably because the underlying systemsall share the same data, pre-processing, languagemodel, alignments, and code base.Comparing system combination when no consen-sus (i.e., minimum Bayes risk) decoding is utilizedat all, we find that model combination improvesupon the result by up to 1.1 BLEU points.
Modelcombination also performs slightly better relative tosystem combination over MBR-decoded systems.
Inthe latter case, system combination actually requiresmore computation compared to model combination;consensus decoding is performed for each systemrather than only once for model combination.
Thisexperiment validates our approach.
Model combina-tion outperforms system combination while avoid-ing the challenge of aligning translation hypotheses.5.4 Algorithmic ImprovementsSection 3 describes two improvements to comput-ing n-gram posteriors: hypergraph expansion for n-gram locality and exact posterior computation.
Ta-ble 5 shows MBR decoding with and without expan-sion (Algorithm 2) in a decoder that collapses nodesdue to language model back-off.
These results showthat while expansion is necessary for correctness, itdoes not affect performance.Table 6 compares exact n-gram posterior compu-tation (Algorithm 1) to the approximation describedby Kumar et al (2009).
Both methods yield identicalresults.
Again, while the exact method guaranteescorrectness of the computation, the approximationsuffices in practice.6 ConclusionModel combination is a consensus decoding strat-egy over a collection of forests produced by multi-ple machine translation systems.
These systems canBLEU (%)ar-en zh-enPosteriors dev nist08 dev nist08Exact 52.4?
44.6?
38.6?
27.3?Approximate 52.5?
44.6?
38.6?
27.2?Table 6: MBR decoding on the phrase-based system witheither exact or approximate posteriors.have varied decoding strategies; we only require thateach system produce a forest (or a lattice) of trans-lations.
This flexibility allows the technique to beapplied quite broadly.
For instance, de Gispert et al(2009) describe combining systems based on mul-tiple source representations using minimum Bayesrisk decoding?likewise, they could be combinedvia model combination.Model combination has two significant advan-tages over current approaches to system combina-tion.
First, it does not rely on hypothesis alignmentbetween outputs of individual systems.
Aligningtranslation hypotheses accurately can be challeng-ing, and has a substantial effect on combination per-formance (He et al, 2008).
Instead of aligning hy-potheses, we compute expectations of local featuresof n-grams.
This is analogous to how BLEU score iscomputed, which also views sentences as vectors ofn-gram counts (Papineni et al, 2002) .
Second, wedo not need to pick a backbone system for combina-tion.
Choosing a backbone system can also be chal-lenging, and also affects system combination perfor-mance (He and Toutanova, 2009).
Model combina-tion sidesteps this issue by working with the con-joined forest produced by the union of the compo-nent forests, and allows the consensus model to ex-press system preferences via weights on system in-dicator features.Despite its simplicity, model combination pro-vides strong performance by leveraging existingconsensus, search, and training techniques.
Thetechnique outperforms MBR and consensus decod-ing on each of the component systems.
In addition,it performs better than standard sentence-based orword-based system combination techniques appliedto either max-derivation or MBR outputs of the indi-vidual systems.
In sum, it is a natural and effectivemodel-based approach to multi-system decoding.982ReferencesCiprian Chelba and M. Mahajan.
2009.
A dynamicprogramming algorithm for computing the posteriorprobability of n-gram occurrences in automatic speechrecognition lattices.
Personal communication.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics.A.
de Gispert, S. Virpioja, M. Kurimo, and W. Byrne.2009.
Minimum bayes risk combination of translationhypotheses from alternative morphological decompo-sitions.
In Proceedings of the North American Chapterof the Association for Computational Linguistics.John DeNero, David Chiang, and Kevin Knight.
2009.Fast consensus decoding over translation forests.
InProceedings of the Association for Computational Lin-guistics and IJCNLP.Robert Frederking and Sergei Nirenburg.
1994.
Threeheads are better than one.
In Proceedings of the Con-ference on Applied Natural Language Processing.Xiaodong He and Kristina Toutanova.
2009.
Joint opti-mization for machine translation system combination.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing.Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,and Robert Moore.
2008.
Indirect-hmm-based hy-pothesis alignment for combining outputs from ma-chine translation systems.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing.Shankar Kumar and William Byrne.
2004.
MinimumBayes-risk decoding for statistical machine transla-tion.
In Proceedings of the North American Chapterof the Association for Computational Linguistics.Shankar Kumar, Wolfgang Macherey, Chris Dyer, andFranz Och.
2009.
Efficient minimum error rate train-ing and minimum bayes-risk decoding for translationhypergraphs and lattices.
In Proceedings of the Asso-ciation for Computational Linguistics and IJCNLP.Zhifei Li and Jason Eisner.
2009.
First- and second-orderexpectation semirings with applications to minimum-risk training on translation forests.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing.Zhifei Li and Sanjeev Khudanpur.
2008.
A scalabledecoder for parsing-based machine translation withequivalent language model state maintenance.
In ACLWorkshop on Syntax and Structure in Statistical Trans-lation.Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, andMing Zhou.
2009a.
Collaborative decoding: Partialhypothesis re-ranking using translation consensus be-tween decoders.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing.Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b.Variational decoding for statistical machine transla-tion.
In Proceedings of the Association for Compu-tational Linguistics and IJCNLP.Yang Liu, Haitao Mi, Yang Feng, and Qun Liu.
2009.Joint decoding with multiple translation models.
InProceedings of the Association for Computational Lin-guistics and IJCNLP.Wolfgang Macherey and Franz Och.
2007.
An empiricalstudy on computing consensus translations from mul-tiple machine translation systems.
In EMNLP, Prague,Czech Republic.Franz J. Och and Hermann Ney.
2004.
The AlignmentTemplate Approach to Statistical Machine Translation.Computational Linguistics, 30(4):417 ?
449.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automatic eval-uation of machine translation.
In Proceedings of theAssociation for Computational Linguistics.Antti-Veikko I. Rosti, Necip Fazil Ayan, Bing Xiang,Spyros Matsoukas, Richard Schwartz, and Bonnie J.Dorr.
2007.
Combining outputs from multiple ma-chine translation systems.
In Proceedings of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.Roy Tromble, Shankar Kumar, Franz J. Och, and Wolf-gang Macherey.
2008.
Lattice minimum Bayes-riskdecoding for statistical machine translation.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing.Fuliang Weng, Andreas Stolcke, and Ananth Sankar.1998.
Efficient lattice representation and generation.In Intl.
Conf.
on Spoken Language Processing.Yong Zhao and Xiaodong He.
2009.
Using n-gram basedfeatures for machine translation system combination.In Proceedings of the North American Chapter of theAssociation for Computational Linguistics.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of the NAACL 2006 Workshop on statisti-cal machine translation.983
