COMBIN ING KNOWLEDGE SOURCESTO REORDER N-BEST SPEECH HYPOTHESIS  L ISTSManny Raynez a, David Carter 1, Vassilios Digalakis 2, Patti Price 2(1) SRI International, Suite 23, Millers Yard, Cambridge CB2 1RQ, UK(2) SRI International, Speech Technology and Research Laboratory,333 Ravenswood Ave., Menlo Park, CA 94025-3493, USAABSTRACTA simple and general method is described that can combinedifferent knowledge sources to reorder N-best lists of hypothe-ses produced by a speech recognizer.
The method is automat-ically trainable, acquiring information from both positive andnegative xamples.
In experiments, the method was testedon a 1000-utterance sample of unseen ATIS data.1.
INTRODUCTIONDuring the last few years, the previously separate fields ofspeech and natural anguage processing have moved muchcloser together, and it is now common to see integrated sys-tems containing components for both speech recognition andlanguage processing.
An immediate problem is the nature ofthe interface between the two.
A popular solution has beenthe N-best list-for example, \[9\]; for some N, the speech recog-nizer hands the language processor the N utterance hypothe-ses it considers most plausible.
The recognizer chooses thehypotheses on the basis of the acoustic information i  the in-put signal and, usually, a simple language model such as a bi-gram grammar.
The language processor brings more sophis-ticated linguistic knowledge sources to bear, typically someform of syntactic and/or semantic analysis, and uses them tochoose the most plausible member of the N-best list.
We willcall an algorithm that selects a member of the N-best list apreference method.
The most common preference method isto select the highest member of the list that receives a validsemantic analysis.
We will refer to this as the "highest-in-coverage" method.
Intuitively, highest-in-coverage seems apromising idea.
However, practical experience shows that itis surprisingly hard to use it to extract concrete gains.
Forexample, a recent paper \[8\] concluded that the highest-in-coverage candidate was in terms of the word error rate onlyvery marginally better than the one the recognizer consideredbest.
In view of the considerable computational overhead re-quired to perform linguistic analysis on a large number ofspeech hypotheses, its worth is dubious.In this paper, we will describe a general strategy for con-structing a preference method as a near-optimal combinationof a number of different knowledge sources.
By a "knowledgesource", we will mean any well-defined procedure that asso-ciates some potentially meaningful piece of information witha given utterance hypothesis H. Some examples of knowledgesources are?
The plausibility score originally assigned to H by therecognizer?
The sets of surface unigrams, bigrams and trigramspresent in H?
Whether or not H receives a well-formed syntac-tic/semantic analysis?
If so, properties of that analysisThe methods described here were tested on a 1001-utteranceunseen subset of the ATIS corpus; speech recognition wasperformed using SRI's DECIPHER TM recognizer \[7, 5\], andlinguistic analysis by a version of the Core Language En-gine (CLE \[2\]).
For 10-best hypothesis lists, the best methodyielded proportional reductions of 13% in the word error rateand 11% in the sentence rror rate; if sentence rror wasscored in the context of the task, the reduction was about21%.
By contrast, the corresponding figures for the highest-in-coverage method were a 7% reduction in word error rate, a5% reduction in sentence rror rate (strictly measured), anda 12% reduction in the sentence rror rate in the context ofthe task.The rest of the paper is laid out as follows.
In Section 2 wedescribe a method that allows different knowledge sourcesto be merged into a near-optimal combination.
Section 3describes the experimental results in more detail.
Section 4concludes.2.
COMBIN ING KNOWLEDGESOURCESDifferent knowledge sources (KSs) can be combined.
We be-gin by assuming the existence of a training corpus of N-bestlists produced by the recognizer, e~h list tagged with a "ref-erence sentence" that determines which (if any) of the hy-potheses in it was correct.
We analyse each hypothesis Hin the corpus using a set of possible KSs, each of which as-sociates ome form of information with H. Information canbe of two different kinds.
Some KSs may directly produce anumber that can be viewed as a measure of H's plausibility.Typical examples are the score the recognizer assigned to H,and the score for whether or not H received a linguistic anal-ysis (1 or 0, respectively).
More commonly, however, the KSwill produce a list of one or more "linguistic items" associatedwith H, for example surface N-grams in H or the grammarrules occurring in the best linguistic analysis of H, if therewas one.
A given linguistic item L is associated with a nu-merical score through a "discrimination function" (one func-tion for each type of linguistic item), which summarizes therelative frequencies of occurrence of L in correct and incor-rect hypotheses, respectively.
Discrimination functions arediscussed in more detail shortly.
The score assigned to H217by a KS of this kind will be the sum of the discriminationscores for all the linguistic items it finds.
Thus, each KS willeventuMly contribute a numerical score, possibly via a dis-crimination function derived from an analysis of the trainingcorpus.The totM score for each hypothesis i  a weighted sum of thescores contributed by the various KSs.
The final requirementis to use the training corpus a second time to compute optimalweights for the different KSs.
This is an optimization problemthat can be approximately solved using the method escribedin \[3\] 1 .The most interesting role in the above is played by the dis-crimination functions.
The intent is that linguistic items thattend to occur more frequently in correct hypotheses than in-correct ones will get positive scores; those which occur morefrequently in incorrect hypotheses than correct ones will getnegative scores.
To take an example from the ATIS do-main, the trigram a list of is frequently misrecognized byDECIPHER TM as a list the.
Comparing the different hy-potheses for various utterances, we discover that if we havetwo distinct hypotheses for the same utterance, one of whichis correct and the other incorrect, and the hypotheses differby one of them containing a list o\] while the other containsa list the, then the hypothesis containing a list o\] is nearlyalways tile correct one.
This justifies giving the trigram a listo\] a positive score, and the trigram a list the a negative one.We now define formally the discrimination function dT for agiven type T of linguistic item.
We start by defining dT asa function on linguistic items.
As stated above, it is thenextended in a natural way to a function on hypotheses bydefining dT(H) for a hypothesis H to be ~ dT(L), where thesum is over all the linguistic items L of type T associatedwith H.dT(L) for a given linguistic item L is computed as follows.
(This is a sfight generalization of the method given in \[4\].
)The training corpus is analyzed, and each hypothesis istagged with its set of associated linguistic items.
We thenfind all possible 4-tuples (U, H1, H2, L) where?
U is an utterance.
* H1 and H2 are hypotheses for U, exactly one of whichis correct.?
L is a linguistic item of type T that is associated withexactly one of H1 and H2.If L occurs in the correct hypothesis of the pair (Ha, H2), wecall this a "good" occurrence of L; otherwise, it is a "bad"one.
Counting occurrences over the whole set, we let g bethe total number of good occurrences of L, and b be the totalnumber of bad occurrences.
The discrimination score of typeT for L, dT(L), is then defined as a function d(g, b).
It seemssensible to demand that d(g, b) has the following properties:?
d (g ,b )>Oi fg>b?
d(g, b) = -d(b, g) (and hence d(g, b) ---- 0 if g ---- b)1A summary can also be found in \[11\].?
d(gl,b) > d(g2,b) if ga > g2We have experimented with a number of possible such func-tions, the best one appearing to be the following:log2(2(g-b 1)/(g -\[- b--b 2)) i f  g < bd(g,b)= o i l  g=b-log2(2(b.-t-1)/(g-t-b-.I-2)) i f  g >bThis formula is a symmetric, logarithmic transform of thefunction (g + 1)/(g -t- b + 2), which is the expected a pos-teriori probability that a new (U, Ha,H2, L) 4-tuple will bea good occurrence, assuming that, prior to the quantities gand b being known, this probability has a uniform a prioridistribution on the interval \[0,1\].One serious problem with corpus-based measures like dis-crimination functions is data sparseness; for this reason, itwill often be advantageous to replace the raw linguistic itemsL with equivalence classes of such items, to smooth the data.We will discuss this further in Section 3.2.3.
EXPERIMENTSOur experiments tested the general methods that we haveoutlined.3.1.
Experimental Set-upThe experiments were run on the 1001-utterance subset ofthe ATIS corpus used for the December 1993 evaluations,which was previously unseen data for the purposes of the ex-periments.
The corpus, originally supplied as waveforms, wasprocessed into N-best lists by the DECIPHER TM recognizer.The recognizer used a class bigram language model.
Each N-best hypothesis received a numerical plausibility score; onlythe top 10 hypotheses were retained.
The 1-best sentenceerror rate was about 34%, the 5-best error rate (i.e., thefrequency with which the correct hypothesis was not in thetop 5) about 19%, and the 10-best error rate about 16%.Linguistic processing was performed using a version of theCore Language Engine (CLE) customized to the ATIS do-main, which was developed under the SRI-SICS-Telia Re-search Spoken Language Translator project \[1, 11, 12\].
TheCLE normally assigns a hypothesis everal different possiblelinguistic analyses, scoring each one with a plausibility mea-sure.
The plausibility measure is highly optimized \[3\], andfor the ATIS domain has an error rate of about 5%.
Onlythe most plausible linguistic analysis was used.The general CLE grammar was specialized to the domainusing the Explanation-Based Learning (EBL) algorithm \[13\]and the resulting grammar parsed using an LR parser \[14\],giving a decrease in analysis time, compared to the normalCLE left-corner parser, of about an order of magnitude.
Thismade it possible to impose moderately realistic resource lim-its: linguistic analysis was allowed a maximum of 12 CPUseconds per hypothesis, running SICStus Prolog on a SunSPARCstation 10/412.
Analysis that overran the time limitwas cut off, and corresponding data replaced by null val-ues.
Approximately 1.2% of all hypotheses timed out during2All product names mentioned in this paper are the trademarkof their respective holder.218linguistic analysis; the average analysis time required per hy-pothesis was 2.1 seconds.Experiments were carried out by first dividing the corpus intofive approximately equal pools, in such a way that sentencesfrom any given speaker were never assigned to more than onepool 3 .
Each pool was then in turn held out as test data, andthe other four used as training data..
The fact that utter-ances from the same speaker never occurred both as test andtraining data turned out to have an important effect on theresults, and is discussed in more detail later.3.2.
Knowledge Sources UsedThe following knowledge sources were used in the experi-ments:Max.
length (words)Preference method 8 12 16 o?1-best 28.3 30.4 31.9 33.7Highest-in-coverage 26.3 27.4 30.1 32.2.N-gram/highest-in-coverage 26.1 27.1 29.9 131.7Recognizer+N-gram 25.3 27.8 29.7 31.6Recognizer+linguistic KSs 23.3 24.8 27.9 30.0All available KSs 23.5 25.4 28.1 29.9Lowest WE in 10-best 12.6 13.2 14.5 15.8# utterances in 10-best \ [442 710 800 1804031# utterances 506 818 936Table 1: 10-best sentence rror ratesRecognizer  score: The numerical score assigned to eachhypothesis by the DECIPHER TM recognizer.This is typically a large negative integer.In  coverage: Whether or not the CLE assigned the hy-pothesis a linguistic analysis (1 or 0).Un l ike ly  g rammar  const ruct ion :  1 if the most plansi-ble linguistic analysis assigned to the hypothe-sis by the CLE was "unlikely", 0 otherwise.
Inthese experiments, the only analyses tagged as"unlikely" are ones in which the main verb is aform of be, and there is a number mismatch be-tween subject and predicate-for example, "whatis the fares?
".Class N-gram d isc r iminants  (four distinct knowledgesources): Discrimination scores for 1-, 2-, 3- and4-grams of classes of surface linguistic items.
Theclass N-grams are extracted after some surfacewords are grouped into multi-word phrases, andsome common words and groups are replaced withclasses; the dummy words *START* and *END*are also added to the beginning and end of thelist, respectively.
Thus, for example, the utter-ance one way flights to d f w would, after thisphase of processing, be *START* flight_type_adjflights to airport_name *END*.Grammar  ru le d i sc r iminants :  Discrimination scores forthe grammar ules used in the most plausible lin-guistic analysis of the hypothesis, if there was one.Semant ic  t r ip le  d i ser iminants :Discrimination scores for "semantic triples" inthe most plausible linguistic analysis of the hy-pothesis, if there was one.
A semantic triple isof the form (Head1, Rel, Head2), where Head1and Head2 are head-words of phrases, and Rel isa grammatical relation obtaining between them.Typical values for Rel are "subject" or "object",when Head1 is a verb and Head2 the head-wordof one of its arguments; alternatively, Rel can bea preposition, if the relation is a PP modificationof an NP or VP.
There are also some other possi-bilities (cf.
\[3\]).3We would llke to thank Bob Moore for suggesting this idea.The knowledge sources naturally fall into three groups.
Thefirst is the singleton consisting of the "recognizer score" KS;the second contains the four class N-gram discriminant KSs;the third consists of the remMning "linguistic" KSs.
Themethod of \[3\] was used to calculate near-optimal weights forthree combinations of KSs:1.
Recognizer score + class N-gram discriminant KSs2.
Recognizer score + linguistic KSs3.
All available KSsTo facilitate comparison, some other methods were testedas well.
Two variants of the highest-in-coverage methodprovided a lower limit: the "straight" method, and one inwhich the hypotheses were first rescored using the optimizedcombination of recognizer score and N-gram discriminantKSs.
This is marked in the tables as "N-gram/highest-in-coverage", and is roughly the strategy described in \[6\].
Anupper limit was set by a method that selected the hypothesisin the list with the lower number of insertions, deletions andsubstitutions.
This is marked as "lowest WE in 10-best".3.3.
ResultsTable 1 shows the sentence error rates for different preferencemethods and utterance lengths, using 10-best lists; Table 2shows the word error rates for each method on the full set.The absolute decrease in the sentence rror rate between 1-best and optimized 10-best with all KSs is from 33.7% to29.9%, a proportionM decrease of 11%.
This is nearly exactlythe same as the improvement measured when the lists wererescored using a class trigram model, though it should bestressed that the present experiments used far less trainingdata.
The word error rate decreased from 7.5% to 6.4%, a13% proportional decrease.
Here, however, the trigram modelperformed significantly better, and achieved a reduction of22%.It is apparent hat nearly all of the improvement is com-ing from the linguistic KSs; the difference between the lines"recognizer + linguistic KSs" and "all available KSs" is notsignificant.
Closer inspection of the results also shows thatthe improvement, when evaluated in the context of the spo-ken languagetranslation task, is rather greater than Table 1219Preference method Word Error(%)1-best 7.4Highest-in-coverage 6.9Recognizer+N-gram KSs 6.8N-gram/highest-in-coverage 6.7Recognizer+linguistic KSs 6.5All available KSs 6.4Lowest WE in 10-best 3.0Table 2: 10-best word error rateswould appear to indicate.
Since the linguistic KSs only lookat the abstract semantic analyses of the hypotheses, they of-ten tend to pick harmless yntactic variants of the referencesentence; for example a l l  e l  the  can be substituted for al l  theor what  are  .
.
.
for which  are  .
.
.
.
When syntactic variantsof this kind are scored as correct, the figures are as shownin Table 3.
The improvement in sentence rror rate on thismethod of evaluation is from 28.8% to 22.8%, a proportionaldecrease of 21~0.
On either type of evaluation, the differencebetween "all available KSs" and any other method except"recognizer + linguistic KSs" is significant at the 5% levelaccording to the McNemar sign test \[10\].One point of experimental method is interesting enough tobe worth a diversion.
In earlier experiments, reported in thenotebook version of this paper, we had not separated the datain such a way as to ensure that the speakers of the utterancesin the test and training data were always disjoint.
This ledto results that were both better and also qualitatively differ-ent; the N-gram KSs made a much larger contribution, andappeared to dominate the linguistic KSs.
This presumablyshows that there are strong surface uniformities between ut-terances from at least some of the speakers, which the N-gramKSs can capture more easily than the linguistic ones.
It ispossible that the effect is an artifact of the data-collectionmethods, and is wholly or partially caused by users who re-peat queries after system misrecognitions.For a total of 88 utterances, there was some acceptable 10-best hypothesis, but the hypothesis chosen by the methodMax.
length (words)Preference method 8 12 16 co1-best 24.3 26.0 27.5 28.8Highest-in-coverage 20.4 21.5 23.7 25.3 lRecognizer+N-gram KSs 20.4 22.5 23.8 25.2N-gram/highest-in-coverage 19.0 20.5 22.6 24.1Recognizer+linguistic KSs 17.6 19.6 21.7 23.5All available KSs i 17.6 19.6 21.5 22.8Lowest WE in 10-best 11.3 12.0 13.0 14.0# utterances 506 818 936 1001Table 3: 10-best sentence rror rates counting acceptablevariants as successesApparently impossible 14Coverage problems 44Clear preference failure 2iUncertain 9Table 4: Causes of N-best preference failurethat made use of all available KSs was unacceptable.
To get amore detailed picture of where the preference methods mightbe improved, we inspected these utterances and categorizedthem into different apparent causes of failure.
Four mainclasses of failure were considered:Apparent ly  imposs ib le :  There is no apparent reason toprefer the correct hypothesis to the one cho-sen without access to intersentential context orprosody.
There were two main subclasses: eithersome important content word was substituted byan equally plausible alternative (e.g.
"Minneapo-lis" instead of "Indianapolis"), or the utterancewas so badly malformed that none of the alterna-tives seemed plausible.Coverage prob lem:  The correct hypothesis was not inimplemented linguistic overage, but would prob-ably have been chosen if it had been; alternately,the selected hypothesis was incorrectly classed as" being in linguistic coverage, but would probablynot have been chosen if it had been correctly clas-sifted as ungrammatical.C lear preference fai lure: The information needed tomake the correct choice appeared intuitively tobe present, but had not been exploited.Uncer ta in :  Other cases.The results are summarized in Table 4.At present, the best preference method is in effect able toidentify about 40% of the acceptable new hypotheses pro-duced when going from 1-best to 10-best.
(In contrast, the"highest-in-coverage" method finds only about 20%.)
It ap-pears that addressing the problems responsible for the lastthree failure categories could potentially improve the pro-portion to something between 70% and 90%.
Of this in-crease, about two-thirds could probably be achieved by suit-able improvements o linguistic coverage, and the rest byother means.
It seems plausible that a fairly substantial pro-portion of the failures not due to coverage problems can beascribed to the very small quantity of training data used.4.
CONCLUSIONSA simple and uniform architecture combines different knowl-edge sources to create an N-best preference method.
Themethod can easily absorb new knowledge sources as theybecome available, and can be automatically trained.
It iseconomical with regard to training material, since it makesuse of both correct and incorrect recognizer hypotheses.
Itis in fact to be noted that over 80% of the discrimination220scores are negative, deriving from incorrect hypotheses.
Theapparent success of the method can perhaps most simply beexplained by the fact that it attempts directly to model char-acteristic mistakes made by the recognizer.
These are oftenidiosyncratic to a particular ecognizer (or even to a particu-lar version of a recognizer), and will not necessarily be easyto detect using more standard language models based on in-formation derived from correct utterances only.We find the initial results described here encouraging, and inthe next few months intend to extend them by training onlarger amounts of data, refining existing knowledge sources,and adding new ones.
In particular, we plan to investigatethe possibility of improving the linguistic KSs by using partiallinguistic analyses when a full analysis is not available.
Weare also experimenting with applying our methods to N-bestlists that have first been rescored using normal class trigrammodels.
Preliminary results indicate a proportional decreaseof about 7% in the sentence rror rate when syntactic vari-ants of the reference sentence are counted as correct; this issignificant according to the McNemar test.
Only the finguis-tic KSs appear to contribute.
We hope to be able to reportthese results in more detail at a later date.ACKNOWLEDGEMENTThe work we have described was accomplished under contractto Tefia Research.References1.
AgnEs, M-S., Alshawi, H., Bretan, I., Carter, D.M.Ceder, K., Collins, M., Crouch, R., Digalakis, V.,Ekholm, B., Gamb?ck, B., Kaja, J., Karlgren, J., Ly-berg, B., Price, P., Pulman, S., Rayner, M., Samuels-son, C. and Svensson, T., Spoken Language Transla-tor: First Year Report, joint SRI/SICS technical report,1994.2.
Alshawi, H., The Core Language Engine, Cambridge,Massachusetts: The MIT Press, 1992.3.
Alshawi, H. and Carter, D.M., Training and ScalingPreference Functions for Disambiguation, SRI Techni-cal Report, 1993.4.
Collins, M.J.
The Use of Semantic Collocations in Pref-erence Metrics and Word Similarity Measures, M PhilThesis, Cambridge University, Cambridge, England,1993.5.
Digalakis, V. and Murveit, H., "Genones: Optimizingthe Degree of Tying in a Large Vocabulary HMM SpeechRecognizer", Proc.
Inter.
Conf.
on Acoust., Speech andSignal Proc., 1994.6.
Kubala, F., Barry, C., Bates, M., Bobrow, R., Fung, P.,Ingria, R., Makhoul, J., Nguyen, L., Schwartz, R. andStallard, D., "BBN Byblos and Harc February 1992ATIS Benchmark Results", Proe.
DARPA Workshop onSpeech and Natural Language, 199P.7.
Murveit, H., Butzberger, J., Digalakis, V. and Wein-traub, M., "Large Vocabulary Dictation using SRI'sDECIPHER TM Speech Recognition System: Progres-sive Search Techniques", Proc.
Inter.
Conf.
on Acoust.,Speech and Signal Proc., Minneapolis, Minnesota, April1993.8.
Norton, L.M., Dahl, D.A.
and Linebarger, M.C., "Re-cent Improvements and Benchmark Results for the Para-max ATIS System".
Proc.
DARPA Workshop on Speechand Natural Language, 1992.9.
Ostendorf, M., et al, "Integration of Diverse Recog-nition Methodologies Through Reevaluation of N-bestSentence Hypotheses," Proc.
DARPA Workshop onSpeech and Natural Language, 1991.10.
Powell, F.C., Cambridge Mathematical and StatisticalTables, Cambridge University Press, Cambridge, Eng-land, 1976.11.
Rayner, M., Alshawi, H., Bretan, I., Carter, D.M., Di-ga\]akis, V., Gamb?ck, B., Kaja, J., Karlgren, J., Ly-berg, B., Price, P., Pulman, S. and Samuelsson, C., "ASpeech to Speech Translation System Built From Stan-dard Components".
Proc.
ARPA workshop on HumanLanguage Technology, 199312.
Rayner, M., Bretan, I., Carter, D., Collins, M., Di-galakis, V., Gamb?ck, B., Kaja, J., Karlgren, J., Ly-berg, B., Price, P., Pulman S. and Samuelsson, C., "Spo-ken Language Translation with Mid-90's Technology: ACase Study".
Proc.
Eurospeeeh '93, Berlin, 1993.13.
Samuelsson, C. and Rayner, M., "Quantitative Evalu-ation of Explanation-Based Learning as a Tuning Toolfor a Large-Scale Natural Language System".
Proc.
1PthInternational Joint Conference on Artificial Intelligence.Sydney, Australia, 1991.14.
Samuelsson, C., Fast Natural Language Parsing UsingExplanation-Based Learning, PhD thesis, Royal Insti-tute of Technology, Stockholm, Sweden, 1994.221
