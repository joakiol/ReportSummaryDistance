Learning New Compos i t ions  from Given OnesJi DonghongDept.
of Computer ScienceTsinghua UniversityBeijing, 100084P.
R. Chinaj dh?s l000e ,  cs .
t s?nghua,  edu .
cnHe JunDept.
of Computer ScienceHerbin Institute of Technologyh j  ?pact  518.  h i t .
edu .
cnHuang ChangningDept.
of Computer ScienceTsinghua UniversityBeijing, 100084P.
R. Chinahcn@mail.tsinghua.edu.cnAbstractIn this paper, we study the problem of"learning new compositions of words fromgiven ones with a specific syntactic struc-ture, e.g., A-N or V-N structures.
We firstcluster words according to the given com-positions, then construct a cluster-basedcompositional frame for each word cluster,which contains both new and given compo-sitions relevant with the words in the clus-ter.
In contrast o other methods, we don'tpre-define the number of clusters, and for-malize the problem of clustering words asa non-linear optimization one, in which wespecify the environments ofwords based onword clusters to be determined, rather thantheir neighboring words.
To solve the prob-lem, we make use of a kind of cooperativeevolution strategy to design an evolution-ary algorithm.1 IntroductionWord compositions have long been a concern in lex-icography(Benson et al 1986; Miller et al 1995),and now as a specific kind of lexical knowledge,it has been shown that they have an importantrole in many areas in natural language process-ing, e.g., parsing, generation, lexicon building, wordsense disambiguation, and information retrieving,etc.
(e.g., Abney 1989, 1990; Benson et al 1986;Yarowsky 1995; Church and Hanks 1989; Church,Gale, Hans, and Hindle 1989).
But due to thehuge number of words, it is impossible to list allcompositions between words by hand in dictionar-ies.
So an urgent problem occurs: how to auto-matically acquire word compositions?
In general,word compositions fall into two categories: freecompositions and bound compositions, i.e., collo-cations.
Free compositions refer to those in whichwords can be replaced by other similar ones, whilein bound compositions, words cannot be replacedfreely(Benson 1990).
Free compositions are pre-dictable, i.e., their reasonableness can be determinedaccording to the syntactic and semantic propertiesof the words in them.
While bound compositionsare not predictable, i.e., their reasonableness cannotbe derived from the syntactic and semantic prop-erties of the words in them(Smadja 1993).
Nowwith the availability of large-scale corpus, automaticacquisition of word compositions, especially wordcollocations from them have been extensively stud-ied(e.g., Choueka et al 1988; Church and Hanks1989; Smadja 1993).
The key of their methods isto make use of some statistical means, e.g., frequen-cies or mutual information, to quantify the compo-sitional strength between words.
These methods aremore appropriate for retrieving bound compositions,while less appropriate for retrieving free ones.
Thisis because in free compositions, words are relatedwith each other in a more loose way, which mayresult in the invalidity of mutual information andother statistical means in distinguishing reasonablecompositions from unreasonable ones.
In this pa-per, we start from a different point to explore theproblem of automatic acquisition of free composi-tions.
Although we cannot list all free compositions,we can select some typical ones as those specifiedin some dictionaries(e.g., Benson 1986; Zhang et al1994).
According to the properties held by free com-positions, we can reasonably suppose that selectedcompositions can provide strong clues for others.Furthermore we suppose that words can be classi-fied into clusters, with the members in each clustersimilar in their compositional ability, which can becharacterized as the set of the words able to com-bined with them to form meaningful phrases.
Thusany given composition, although specifying the rela-tion between two words literally, suggests the rela-tion between two clusters.
So for each word(or clus-Ji, He and Huang 25 Learning New CompositionsJi Donghong, He Jun and Huang Changning (1997) Learning New Compositions from Given Ones.
In T.M.Ellison (ed.)
CoNLL97: Computational Natural Language Learning, ACL pp 25-32.
(~) 1997 Association for Computational Linguisticster), there exist some word clusters, the word (orthe words in the cluster) can and only can combinewith the words in the clusters to form meaningfulphrases.
We call the set of these clusters composi-tional frame of the word (or the cluster).
A seem-ingly plausible method to determine compositionalframes is to make use of pre-defined semantic lassesin some thesauri(e.g., Miller et al 1993; Mei etal.
1996).
The rationale behind the method is totake such an assumption that if one word can becombined with another one to form a meaningfulphrase, the words similar to them in meaning canalso be combined with each other.
But it has beenshown that the similarity between words in meaningdoesn't correspond to the similarity in compositionalability(Zhu 1982).
So adopting semantic lasses toconstruct compositional frames will result in consid-erable redundancy.
An alternative to semantic lassis word cluster based on distributional environment(Brown et al, 1992), which in general refers to thesurrounding words distributed around certain word(e.g., Hatzivassiloglou et al, 1993; Pereira et al,1993), or the classes of them(Bensch et al, 1995), ormore complex statistical means (Dagan et al, 1993).According to the properties of the clusters in com-positional frames, the clusters should be based onthe environment, which, however, is narrowed in thegiven compositions.
Because the given compositionsare listed by hand, it is impossible to make use ofstatistical means to form the environment, he re-maining choices are surrounding words or classes ofthem.Pereira et a1.
(1993) put forward a method to clus-ter nouns in V-N compositions, taking the verbswhich can combine with a noun as its environment.Although its goal is to deal with the problem ofdata sparseness, it suffers from the problem itself.A strategy to alleviate the effects of the problem isto cluster nouns and verbs simultaneously.
But asa result, the problem of word clustering becomes abootstrapping one, or a non-linear one: the environ-ment is also to be determined.
Bensch et al (1995)proposed a definite method to deal with the general-ized version of the non-linear problem, but it suffersfrom the problem of local optimization.In this paper, we focus on A-N compositions inChinese, and explore the problem of learning newcompositions from given ones.
In order to copy withthe problem of sparseness, we take adjective clustersas nouns' environment, and take noun clusters as ad-jectives' environment.
In order to avoid local opti-mal solutions, we propose a cooperative volutionarystrategy.
The method uses no specific knowledge ofA-N structure, and can be applied to other struc-tures.The remainder of the paper is organized as fol-lows: in section 2, we give a formal description of theproblem.
In section 3, we discuss a kind of coopera-tive evolution strategy to deal with the problem.
Insection 4, we explore the problem of parameter es-timation.
In section 5, we present our experimentsand the results as well as their evaluation.
In section6, we give some conclusions and discuss future work.2 P rob lem Set t ingGiven an adjective set and a noun set, suppose foreach noun, some adjectives are listed as its composi-tional instances.
Our goal is to learn new reasonablecompositions from the instances.
To do so, we clus-ter nouns and adjectives imultaneously and build acompositional frame for each noun.Suppose A is the set of adjectives, N is the set ofnouns, for any a E A, let f(a) C N be the instanceset of a, i.e., the set of nouns in N which can becombined with a, and for any n E N, let g(n) C Abe the instance set of n, i.e., the set of adjectives inA which can be combined with n. We first give someformal definitions in the following:Def in i t ion  1 partitionSuppose U is a non-empty finite set, we call <U1, U2, ..., Uk > a partition of U, if:i) for any Ui, and Uj, i ?
j, Ui M Uj =ii) U = Ul<t<kUlWe call Ui a cluster of U.Suppose U --< A1,A2,...,Ap > is a partitionof A, V ~< N1,N2,...,Nq > is a partition ofN, f and g are defined as above, for any N/, letg(N ) = {& : n # ?
), and forany n, let ,f<U,V>(n ) =l {a : 3At,Al E g(Nk),a EAjl} -g (n )  I, where n E Nk.
Intuitively, 5<Uv>(n)is the number of the new instances relevant with n.We define the general earning amount as the fol-lowing:Def in i t ion  2 learning amounthENBased on the partitions of both nouns and adjec-tives, we can define the distance between nouns andthat between adjectives.Def in i t ion  3 distance between wordsfor anya  EA ,  let fv(a) = {Ni : 1<i  < q, Ni Mf(a) ~ ~}, for any n E N, let g~= {Ai : 1 < i <p, Ai Mg(n) ?
?
), for any two nouns nl and ha, anytwo adjectives al and a2, we define the distancesbetween them respectively as the following:Ji, He and Huang 26 Learning New Compositionsi)ii)dis~(nl, n2) = 1 - gff(nl) V1 gu(n2)gu(nl)  U gu(n2)disv(al, a2) = 1 - f~-(al) N fv(a2)fV(al) t_J fv(a2)According to the distances between words, we candefine the distances between word sets.Def in i t ion  4 distance between word setsGiven any two adjective sets X1, X2 C A, any twonoun sets Y1, Y2 C N, their distances are:i)disv(Zl ,  X2) = max {disv(al, a2)}a lEX i ,a2EX2ii)max dis~r hi, dis (Yl,Y2) = { v( n2)}nl  EYi,n2EY2Intuitively, the distance between word sets referto the biggest distance between words respectivelyin the two sets.We formalize the problem of clustering nouns andadjectives imultaneously as an optimization prob-lem with some constraints.
(1)To determine a partition U =<A1,A2,...,Ap > of A, and a partitionV =< N1,N2,...,Nq > of N, where p,q > O,which satisfies i) and ii), and minimize ~<e,v>"i) for any al, a2 E Ai, 1 < i < p, disg(al,  as) < tl;for Ai and Aj, 1 < i # j < p, disv(Ai,Aj) > tl;ii) for any nl,n2 E Ni,1 < i < q, disg(nl,n2) <t2; for Ni and Ny, 1 _< i ?
j _< p, disg(Ni, Nj) k t2;Intuitively, the conditions i) and ii) make the dis-tances between words within clusters maller,  andthose between different clusters bigger, and to min-imize 6 ~ ~_ means to minimize the distances be-tween the words within clusters.In fact, (U, V) can be seen as an abstraction modelover given compositions, and tl, t2 can be seen as itsabstraction degree.
Consider the two special case:one is tl = t2 = 0, i.e., the abstract degree is thelowest, when the result is that one noun forms a clus-ter and on adjective forms a cluster, which meansthat no new compositions are learned.
The otheris tl = t2 = 1, the abstract degree is the highest,when a possible result is that all nouns form a clus-ter and all adjectives form a cluster, which meansthat all possible compositions, reasonable or unrea-sonable, are learned.
So we need estimate appropri-ate values for the two parameters, in order to makean appropriate abstraction over given compositions,i.e., make the compositional frames contain as manyreasonable compositions as possible, and as few un-reasonable ones as possible.3 Cooperative EvolutionSince the beginning of evolutionary algorithms, theyhave been applied in many areas in AI(Davis etal., 1991; Holland 1994).
Recently, as a new andpowerful learning strategy, cooperative evolution hasgained much attention in solving complex non-linearproblem.
In this section, we discuss how to deal withthe problem (1) based on the strategy.According to the interaction between adjectiveclusters and noun clusters, we adopt such a coop-erative strategy: after establishing the preliminarysolutions, for any preliminary solution, we optimizeN's  partition based on A's partition, then we opti-mize A's partition based on N's  partition, and soon, until the given conditions are satisfied.3.1 P re l iminary  SolutionsWhen determining the preliminary population, wealso cluster nouns and adjectives respectively.
How-ever, we see the environment of a noun as the setof all adjectives which occur with it in given com-positions, and that of an adjective as the set of allthe nouns which occur with it in given compositions.Compared with (1), the problem is a linear cluster-ing one.Suppose al,a2 E A, f is defined as above, we de-fine the linear distance between them as (2):(2) dis(a1 a2) -- 1 - I/(ax)nl(a2)l' \[f(ax)Of(a2)lSimilarly, we can define the linear distance be-tween nouns dis(nl,n2) based on g. In contrast,we call the distances in definition 3 non-linear dis-tances.According to the linear distances between adjec-tives, we can determine a preliminary partition ofN: randomly select an adjective and put it into anempty set X, then scan the other adjectives in A, forany adjective in A - X, if its distances from the ad-jectives in X are all smaller than tl, then put it intoX, finally X forms a preliminary cluster.
Similarly,we can build another preliminary cluster in (A -  X).So on, we can get a set of preliminary clusters, whichis just a partition of A.
According to the differentorder in which we scan the adjectives, we can get dif-ferent preliminary partitions of A.
Similarly, we candetermine the preliminary partitions of N based onthe linear distances between ouns.
A partition of Aand a partition of N forms a preliminary solution of(1), and all possible preliminary solutions forms theJi, He and Huang 27 Learning New Compositionspopulation of preliminary solutions, which we alsocall the population of Oth generation solutions.3.2 Evo lu t ion  Operat ionIn general, evolution operation consists of recom-bination, mutation and selection.
Recombinationmakes two solutions in a generation combine witheach other to form a solution belonging to next gen-eration.
Suppose < U~ i), Vi(')> and < U~ i), V2(') >are two ith generation solutions, where U~ t) and U~ i)are two partitions of A, V?
i) and V2 (i) are two par-titions of N, then < U~ '), V2 (i) > and < U2 (i), V1 (i) >forms two possible (i+l)th generation solutions.Mutation makes a solution in a generation im-prove its fitness, and evolve into a new one belongingto next generation.
Suppose < U (i), U (i) > is a ithgeneration solution, where U (i) =< A1, A2, ..., Ap >,V (i) =< N1,N2,...,Nq > are partitions of A andN respectively, the mutation is aimed at optimizingV(0 into V (t+l) based on U (t), and makes V (t+l) sat-isfy the condition ii) in (1), or optimizing U (t) intoU(t+l) based on V (0, and makes U (l+1) satisfy thecondition i) in (1), then moving words across clustersto minimize d<u,v>"We design three steps for mutation operation:splitting, merging and moving, the former two areintended for the partitions to satisfy the conditionsin (1), and the third intended to minimize (f<U,v > .In the following, we take the evolution of V (t+l) asan example to demonstrate he three steps.Splitting Procedure.
For any Nk, 1 _< k _<, if thereexist hi,n2 ?
Nk, such that disv(,+~)(nl,n2 ) _> t2,then splitting Nk into two subsets X and Y. Theprocedure is given as the following:i) Put nl into X, n2 into Y,ii) Select the noun in (Nk -- (X U Y)) whose dis-tance from nl is the smallest, and put it into X,iii) Select the noun in (Nk -- (X t_J Y)) whose dis-tance from n2 is the smallest, and put it into Y,iv) Repeat ii) and iii), until X t3 Y = Nk.For X (or Y), if there exist nl,n2 ?
X (or Y),disv(o >_ t2, then we can make use of the aboveprocedure to split it into more smaller sets.
Obvi-ously, we can split any Nk in V(0 into several subsetswhich satisfy the condition ii) in (1) by repeating theprocedure.Merging procedure.
If there exist Nj and Nk,where 1 _< j ,k _< q, such that disu(~)(Nt,Nk ) < t2,then merging them into a new cluster.It is easy to prove that U (t) and V(0 will meet thecondition i) and ii) in (1) respectively, after splittingand merging procedure.Moving procedure.
We call moving n from Nj toNk a word move, where 1 < j ?
k < q, denotedas (n, Nj, Nk), if the condition (ii) remains atisfied.The procedure is as the following:i) Select a word move (n, Nj, Na) which minimizes~<U,V> 'ii) Move n from Nj to Nk,iii) Repeat i) and ii) until there are no word moveswhich reduce 6<u,v>"After the three steps, U (i) and V (i) evolve intoU (i+U and V (i+D respectively.Selection operation selects the solutions amongthose in the population of certain generation accord-ing to their fitness.
We define the fitness of a solutionas its learning amount.We use Ji to denote the set of i$h generation so-lutions, H(i, i + 1), as in (3), specifies the similaritybetween ith generation solutions and (i + 1)th gen-eration solutions.
(3)H(i, i + 1) =min{5(u(,+l),v(i+l)) : (U (~+1), V (i+1)) ?
J~+l}min{5(u(,),v(,) ) : (U (~) ,V (i)) E J~)Let t3 be a threshold for H(i, i + 1), the followingis the general evolutionary algorithm:Procedure Clustering(A, N, f, g);begini) Build preliminary solution population I0,ii) Determine 0th generation solution set J0according to their fitness,iii) Determine/i+1 based on Ji:a) Recombination: if (U~ i), Vff)),(U2 ('), V2 (')) E J,, then (U~ '), V2(')), (U (i), V2 (')) EI~+1,b) Mutation: if (U( ~),V (~)) E J~, then(U (i), V(~+I)), (U (~+D, V (~)) E I~+1,iv) Determine J~+l from Ii+1 according totheir fitness,v) If H(i, i + 1) > t3, then exit, otherwisegoto iii),endAfter determining the clusters of adjectives andnouns, we can construct the compositional frame foreach noun cluster or each noun.
In fact, for eachnoun cluster Ni,g(N~) = {Aj : 3n E Ni,Aj Ng(n) 7??)
is just its compositional frame, and for any nounin N/, g(Ni) is also its compositional frame.
Simi-larly, for each adjective (or adjective cluster), we canalso determine its compositional frame.4 Parameter  Es t imat ionThe parameters tl and t2 in (1) are the thresholdsfor the distances between the clusters of A and N re-Ji, He and Huang 28 Learning New Compositionsspectively.
If they are too big, the established framewill contain more unreasonable compositions, on theother hand, if they are too small, many reason-able compositions may not be included in the frame.Thus, we should determine appropriate values for t~and t2, which makes the fame contain as many rea-sonable compositions as possible, meanwhile as fewunreasonable ones as possible.Suppose Fi is the compositional frame of Ni, letF =< F1,F~,...,Fq >, for any F~, let AF~ = {a :3X E F~, a E X}.
Intuitively, AF~ is the set of theadjectives learned as the compositional instances ofthe noun in Ni.
For any n ~ N~, we use An to de-note the set of all the adjectives which in fact canmodify n to form a meaningful phrase, we now de-fine deficiency rate and redundancy rate of F. Forconvenience, we use (iF to represent 5(U, V).Def in i t ion  5 Deficiency rate o~FEl<i<q EneN, \[ A~ - ARe \[Intuitively, aF refers to the ratio between the rea-sonable compositions which are not learned and allthe reasonable ones.Def in i t ion  6 Redundancy rate firfiR ---- El_<i_<q EneNi \] AF~ -- An I5FIntuitively, fie refers to the ratio between unrea-sonable compositions which are learned and all thelearned ones.So the problem of estimating tl and t2 can beformalized as (5):(5) to find tl and t2, which makes av  = 0, andflF=0.But, (5) may exists no solutions, because its con-straints are two strong, on one hand, the sparsenessof instances may cause ~F not to get 0 value, even iftl and t~ close to 1, on the other hand, the differencebetween words may cause fir not to get 0 value, evenif tl and t2 close to 0.
So we need to weaken (5).In fact, both O~F and flF can be seen as thefunctions of tl and t2, denoted as  o~f(tl,t2) andl~F(tl, tu) respectively.
Given some values for tl andt2, we can compute aF and fiR.
Although theremay exist no values (t~,t~) for (tl,t2), such that!
!
aF(t~,t~) = flF(tx,t2) = 0, but with t~ and t2 in-creasing, off tends to decrease, while fiE tends toincrease.
So we can weaken (5) as (6).
(6) to find tl and t2, which maximizes (7).
(7)~(~l,~)~rl(~',,~'~) ~F(tl, t2)Fi (t' 1 , t'2) \[~(ta,t:)eF2(t' 1,t~2) ~F(tl ,  42))I r2(t'l, Iwhere rx(t~,t~) = {(tl,t2) : 0 < t l  _< t~,0 _< t2 _<t~), r2(t~,t~) = {(t l , t2) :  t~ < t l  < 1,t~ < t2 < 1}Intuitively, if we see the area (\[0, 1\]; \[0, 1\]) as asample space for tl and t2, Fl(t~,t~) and F2(t~,t~)are its sub-areas.
So the former part of (7) is the!
!
mean deficiency rate of the points in Fl(t l ,  tz), andthe latter part of (7) is the mean deficiency rate ofthe points in F2(t~,t~).
To maximize (7) means tomaximize its former part, while to minimize its latterpart.
So our weakening (5) into (6) lies in findinga point (t~,t~), such that the mean deficiency rateof the sample points in F2(t~,t~) tends to be verylow, rather than finding a point (t~,t~), such thatits deficiency rate is 0.5 Exper iment  Results andEvaluationWe randomly select 30 nouns and 43 adjectives,and retrieve 164 compositions(see Appendix I) be-tween them from Xiandai Hanyu Cihai (Zhang etal.
1994), a word composition dictionary of Chinese.After checking by hand, we get 342 reasonable com-positions (see Appendix I), among which 177 onesare neglected in the dictionary.
So the sufficiencyrate (denoted as 7) of these given compositions i47.9%.We select 0.95 as the value of t3, and let tl =0.0, 0.1,0.2, ..., 1.0, t2 = 0.0, 0.1, 0.2, ..., 1.0 respec-tively, we get 121 groups of values for O~F and fiR.Fig.1 and Fig.2 demonstrate he distribution of aFand ~3F respectively.dc?ielcneyi!iiiii!iiiiiii!i!iiiii!4o .... "i i iiiiiiiiiiiiiiiiiiiiiiiiiiiii:: .... 3t2t lFigure 1: The distribution of O~FFor any given tl, and t2,we found (7) get itsbiggest value when t I = 0.4 and t2 = 0.4, so we se-Ji, He and Huang 29 Learning New Compositionsrcdundanec?
atc(%) ..... ~:~'..::':ili::iii~i~i~i~ 160-80.~<.v..'.~!!!~i!!!i!!!ii!!!!:':':!:!:!!!!i!!
!i!i!i iiii iii!iii i:i::':':':" D 4.0-50L00:~:~iii::iiiiiii;;ii;iiiiiiii:: ~ ....... i?
:~i ~ .......... \[\]Z0-~  0:iiiiiiiiiiiiiiiii',i',iii',iiiiiii',0 iiiiiiiiiiiiiit2 (L/tO)Figure 2: The distribution of f ir~(%) ~1 ~2 O/F (%) BF(%)32.5 0.5 0.6 13.2 34.547.9 0.4 0.4 15.4 26.458.2 0.4 0.4 10.3 15.472.5 0.3 0.3 9.5 7.6Table 1: The relation between 7,~1,t2, aF  and fiR.~(%) ~F(%) e1(%) BF(%) ~2(%)58.2 11.2 8.3 17.5 10.872.5 7.4 4.1 8.7 5.4Table 2: The relation between 7, mean O~F and meanBF, el and e~ is the mean error.lect 0.4 as the appropriate value for both tl and t2.The result is listed in Appendix II.
From Fig.1 andFig.2, we can see that when tl = 0.4 and t2 = 0.4,both c~F and BF get smaller values.
With the twoparameters increasing, aF  decreases slowly, while BFincreases everely, which demonstrates the fact thatthe learning of new compositions from the given oneshas reached the limit at the point: the other reason-able compositions will be learned at a cost of severelyraising the redundancy rate.From Fig.l, we can see that o~F generally increasesas ~1 and t2 increase, this is because that to in-crease the thresholds of the distances between clus-ters means to raise the abstract degree of the model,then more reasonable compositions will be learned.On the other hand, we can see from Fig.2 that whentl _> 0.4, t2 >_ 0.4, fiR roughly increases as ~1 and ~2increase, but when tz < 0.4, or t2 < 0.4, fir changesin a more confused manner.
This is because thatwhen tl < 0.4, or ~2 < 0.4, it may be the case thatmuch more reasonable compositions and much lessunreasonable ones are learned, with tl and t2 in-creasing, which may result in fiR's reduction, other-wise f ir will increase, but when tz >_ 0.4, t2 > 0.4,most reasonable compositions have been learned, soit tend to be the case that more unreasonable com-positions will be learned as tl and t2 increase, thusf ir increases in a rough way.To explore the relation between % aF and fiE,we reduce or add the given compositions, then es-t imate Q and t2, and compute aRE and fiR.
Theircorrespondence is listed in Table 1.From Table 1, we can see that as 7 increases, theestimated values for tl and t2 will decrease, and BEwill also decrease.
This demonstrates that if givenless compositions, we should select bigger values forthe two parameters in order to learn as many reason-Ji, He and Huangable compositions as possible, however, which willlead to non-expectable increase in fly.
If given morecompositions, we only need to select smaller valuesfor the two parameters to learn as many reasonablecompositions as possible.We select other 10 groups of adjectives and nouns,each group contains 20 adjectives and 20 nouns.Among the 10 groups, 5 groups hold a sufficiencyrate about 58.2%, the other 5 groups a sufficiencyrate about 72.5%.
We let ~1 -~ 0.4 and t2 = 0.4 forthe former 5 groups, and let tl = 0.3 and t2 = 0.3 forthe latter 5 groups respectively to further considerthe relation between 7, o~F and fiR, with the valuesfor the two parameters fixed.Table 2 demonstrates that for any given compo-sitions with fixed sufficiency rate, there exist closevalues for the parameters, which make c~F and firmaintain lower values, and if given enough compo-sitions, the mean errors of O~FF and fie will be lower.So if given a large number of adjectives and nouns tobe clustered, we can extract a small sample to esti-mate the appropriate values for the two parameters,and then apply them into the original tasks.6 Conc lus ions  and  Future  workIn this paper, we study the problem of learning newword compositions from given ones by establishingcompositional frames between words.
Although wefocus on A-N structure of Chinese, the method usesno structure-specific or language-specific knowledge,and can be applied to other syntactic structures, andother languages.There are three points key to our method.
First,we formalize the problem of clustering adjectives andnouns based on their given compositions as a non-linear optimization one, in which we take noun clus-ters as the environment of adjectives, and adjective30 Learning New CompositionsPclusters as the environment of nouns.
Second, wedesign an evolutionary algorithm to determine itsoptimal solutions.
Finally, we don't pre-define thenumber of the clusters, instead it is automaticallydetermined by the algorithm.Although the effects of the sparseness problemcan be alleviated compared with that in traditionalmethods, it is still the main problem to influence thelearning results.
If given enough and typical compo-sitions, the result is very promising.
So importantfuture work is to get as many typical compositionsas possible from dictionaries and corpus as the foun-dation of our algorithms.At present, we focus on the problem of learningcompositional frames from the given compositionswith a single syntactic structure.
In future, we maytake into consideration several structures to clusterwords, and use the clusters to construct more com-plex frames.
For example, we may consider bothA-N and V-N structures in the meantime, and buildthe frames for them simultaneously.Now we make use of sample points to estimateappropriate values for the parameters, which seemsthat we cannot determine very accurate values dueto the computational costs with sample points in-creasing.
Future work includes how to model thesample points and their values using a continuousfunction, and estimate the parameters based on thefunction.ReferencesAbney, S. 1989.
Parsing by Chunks.
In C. Tennyed.
The MIT Parsing Volume, MIT Press.Abney, S. 1990.
Rapid Incremental Parsing withRepair.
in Proceedings of Waterloo Conferenceon Electronic Text Research.Bensch, P.A.
and W. J. Savitch.
1995.
AnOccurrence-Based Model of Word Categoriza-tion, Annals of Mathematics and Artificial In-telligence, 14:1-16.Benson, M., Benson, E., and Ilson, R. 1986.
The lex-icographic Description of English.
John Ben-jamins.Benson, M. 1986.
The BBI Combinatory Dictionaryof English: A Guide to Word Combinations.John Benjamins.Benson.
M. 1990.
Collocations and General - Pur-pose Dictionaries.
International Journal ofLexicography, 3(1): 23-35.Davis, L. et al 1991.
Handbook of Genetic Algo-rithms.
New York: Van Nostrand, Reinhold.Choueka, Y., T. Klein, and E. Neuwitz.
1983.
Au-tomatic Retrieval of Frequent Idiomatic andCollocational Expressions in a Large Corpus.Journal of Literary and Linguistic Computing,4: 34-38.Church, K. and P. Hanks.
1989.
Word AssociationNorms, Mutual Information, and Lexicogra-phy, in Proceedings of 27th Annual Meetingof the Association for Computational Linguis-tics, 76-83.Church, K., W. Gale, P. Hanks, and D. Hindle.1989.
Parsing, Word Associations and Typi-cal Predicate-Argument relations, in Proceed-ings of the International Workshop on Pars-ing Technologies, Carnegie Mellon University,Pittsburgh, PA. 103-112.Holland, J.H.
1992.
Adaption in Natural and Arti-ficial Systems, 2nd edition, Cambridge, Mas-sachusetts, MIT Press.Hatzivassiloglou, V. and K.R.Mckeown.
Towards theAutomatic Identification of Adjectival Scales:Clustering of adjectives According to Meaning.In Proceedings of Annual Meeting of 31st ACL,Columbus, Ohio, USA.Lin, X.G.
et al 1994.
Xiandai Hanyu Cihai.
RenminZhongguo Press(in Chinese).Mei, J.J. et al 1983.
TongYiCi CiLin (A ChineseThesaurus).
Shanghai Cishu press, Shanghai.Miller, G.A., R. Backwith, C. Fellbaum, D. Gross,K.J.
Miller.
1993 Introduction to WordNet:An On-line Lexical Database, InternationalJournal of Lexicography, (Second Edition.Pereira, F., N. Tishby, and L. Lillian.
1993.
Dis-tributional Clustering of English Words, InProceedings of Annual Meeting of 31st ACL,Columbus, Ohio, USA, 1995.Smadia, F. 1993.
Retrieving Collocations from Text:Xtract, Computational Linguistics, 19(1).Yarowsky, D. 1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Pro-ceedings of the 33th Annual Meeting of the As-sociation for Computational Linguistics, Cam-bridge, Massachusetts.Zhu, D.X.
1982.
Lectures in Grammar.
ShanghaiEducation Press(in Chinese).Ji, He and Huang 31 Learning New CompositionsAppendix IIn this ~pendix, we listed the 30 nouns, ~d ~r anyone of the nouns, we also list the a~ectives whichcan combined with it to ~rm a meaning~ phase5.2 ~ : ~ ~ ~ 1 1 ~ ~3 ~ : ~ f f ~ A ~ / / ~4 ~ : ~ .
~ ~ ~ / / ~ $5 ~ : ~ k ~ ~ l l ~6 ~ : ~ $ ~ ~ X ~ / / ~~ ~ .
~ ~ ~ ~ ~7 ~ , : ~ / / ~ ~8 , ~ : ~ , ~ ~ ~ ~ ~~ $ ~ / / ~ ~ @ ~9 ~ : ~ ~ ~ 1 1 ~ ~  ~ ~ ~ ~ ~ ' ~~ 0 ~ : ~ ~ ~ ~ ~~ / / ~ ~ $ ~ ~ ~12 ~:  ~ ~ X ~ / / ~ ~13 ~:  ~ ~  / / ~ ~1 4 ~ : ~ ~ $ ~ ~ / / ~1 5 ~ : ~ .
~ ~ ~ ~ ~l l ~ k ~ ~ ~ ~ ~~ I I ~ $ ~ A  16 :~ j  :17 ~ :I8 ~:19 ~k:20 ~-~ :21 ~,L, :22 {~?~:~ ~ / / $ ~ A ~~ / / ~ $ ~ ~~ _ ~ ~ / / ~ ~ ~2 3 , ~ : ~ ~ ~ / / ~ ~2 4 , 5 ~ : ~ ~ ~ ~ / / ~2 5 , b ~ : ~ ~ / / ~ ~ % % ~~,~26 ~%: ~ / / ~27 ~2:  ~ / / ~ ~28 ~:  ~ ~  / / ~ ~2 9 ~ : ~ ~ ~ / / ~ k ~3 0 ~ : ~ ~ ~ ~ ~Appendix II1) lists noun clusters and their compositionalframes, 2) lists adjective clusters.1).
Noun Clusters and Their CompositionalFrames:N1 )3i~'~.~i~l'i~l~l~: h l  A10N2 ~'l~/~l'i~/: A1 A2 A10 Al l  A13N3 ~ :  A2 A3 A9N4 :~:~ : A12N5 ~ ~ :  A4 A5 A8 A l lN6 \ [ ~ ~ :  A2 A3N7 ~d~: A2 A7.A8 All Ai3 AI4N8 ~I~:  Ai A2 AI4N9 :~k~:  A4 AI3N10 {~,L-{~'~!~-: A2 A6 A10 A14Nl l  ,L-~,L-~,L,~J: A2A8 A9 A10AllrN12 ~)~:  A6N13 ~:  A7N14.~}:  A2 A6 A142).
Adjective Clusters:A1 ~!
,~$:~:~A2 ~~.~-~J~.~A3 ~Y\] ~d ~ ~A4 ~,~ff.
.
.~A5 ~t~t~A6 '~ i~ '~ i~A7 ~ ~ ~A8 ~A9 ~,  ~A10 ~i~An ~g~l~ ~0i ~i~ ~A12 ~ ~A13 ~i~)~A14 ~ff~ 'll~'~ ~1 The compositional instances of the adjectivescan be inferred from those of the nouns.2 Sufficiency rate refers to the ratio between givenreasonable compositions and all reasonable ones.3 On some points, it may be not the case.4 For a variable X, suppose its value are X1, X2,? "
", Xn ,  its mean error refers to.5 The adjectives before " / / "  are those retrievedfrom the word composition dictionary, and thoseafter " / / "  are those added by hand.Ji, He and Huang 32 Learning New Composit ions
