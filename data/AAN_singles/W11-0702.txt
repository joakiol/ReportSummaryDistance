Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 2?11,Portland, Oregon, 23 June 2011. c?2011 Association for Computational LinguisticsHow can you say such things?!?
:Recognizing Disagreement in Informal Political ArgumentRob Abbott, Marilyn Walker, Pranav Anand, Jean E. Fox Tree,Robeson Bowmani, and Joseph KingUniversity of California Santa Cruzabbott|maw@soe.ucsc.edu,panand|foxtree@ucsc.eduAbstractThe recent proliferation of political and so-cial forums has given rise to a wealth of freelyaccessible naturalistic arguments.
People can?talk?
to anyone they want, at any time, inany location, about any topic.
Here we usea Mechanical Turk annotated corpus of forumdiscussions as a gold standard for the recog-nition of disagreement in online ideologicalforums.
We analyze the utility of meta-postfeatures, contextual features, dependency fea-tures and word-based features for signalingthe disagreement relation.
We show that us-ing contextual and dialogic features we canachieve accuracies up to 68% as compared toa unigram baseline of 63%.1 IntroductionThe recent proliferation of political and social fo-rums has given rise to a wealth of freely accessiblenaturalistic arguments.
People can ?talk?
to anyonethey want, at any time, in any location, about anytopic.
Their conversations range from current polit-ical topics such as national health care to religiousquestions such as the meaning of biblical passages.See Figure 1.
We aim to automatically derive rep-resentations of the discourse structure of such argu-ments and to gain a deeper theoretical and empiricalunderstanding of the linguistic reflexes of perlocu-tionary acts such as persuasion (Austin, 1965).The study of the structure of argumentative com-munication has a long lineage in psychology (Cial-dini, 2000) and rhetoric (Hunter, 1987), but the his-torical lack of a large corpus of naturalistic exam-Topic Q-R: PostEvolution Q: How can you say such things?
The Bible says thatGod CREATED over and OVER and OVER again!
Andyou reject that and say that everything came about byevolution?
If you reject the literal account of the Cre-ation in Genesis, you are saying that God is a liar!
If youcannot trust God?s Word from the first verse, how canyou know that the rest of it can be trusted?R: It?s not a literal account unless you interpret it thatway.Gaymar-riageQ: Gavin Newsom- I expected more from him when Isupported him in the 2003 election.
He showed himselfas a family-man/Catholic, but he ended up being the ex-act oppisate, supporting abortion, and giving homosexu-als marriage licenses.
I love San Francisco, but I hate thepeople.
Sometimes, the people make me want to moveto Sacramento or DC to fix things up.R: And what is wrong with giving homosexuals the rightto settle down with the person they love?
What is it toyou if a few limp-wrists get married in San Francisco?Homosexuals are people, too, who take out their garbage,pay their taxes, go to work, take care of their dogs, andwhat they do in their bedroom is none of your business.Abortion Q: Equality is not defined by you or me.
It is defined bythe Creator who created men.R: Actually I think it is defined by the creator who cre-ated all women.
But in reality your opinion is gibberish.Equality is, like every other word, defined by the peoplewho use the language.
Currently it means ?the same?.People aren?t equal because they are not all the same.Any attempt to argue otherwise is a display of gross stu-pidity.Figure 1: Sample Quote/Response Pairsples has limited empirical work to a handful of gen-res (e.g., editorials or simulated negotiations).
Ar-gumentation is above all tactical.
Thus being ableto effectively model it would afford us a glimpseof pragmatics beyond the conversational turn.
Morepractically, an increasing portion of information andopinion exchange online occurs in natural dialogue,in forums, in webpage comments, and in the back2and forth of short messages (e.g., Facebook statusupdates, tweets, etc.)
Effective models of argumen-tative discourse thus have clear applications in auto-matic summarization, information retrieval, or pre-dicting real-world events such as how well a newproduct is being received or the outcome of a popu-lar vote on a topic (Bollen et al, 2011).In this paper, we focus on an important initial taskfor the recognition of argumentative structure: auto-matic identification of agreement and disagreement.We introduce the ARGUE corpus, an annotated col-lection of 109,553 forum posts (11,216 discussionthreads) from the debate website 4forums.com.
On4forums, a person starts a discussion by posting atopic or a question in a particular category, such associety, politics, or religion.
Some example topicscan be seen in Table 1.
Forum participants can thenpost their opinions, choosing whether to respond di-rectly to a previous post or to the top level topic (starta new thread).
These discussions are essentially di-alogic; however the affordances of the forum suchas asynchrony, and the ability to start a new threadrather than continue an existing one, leads to dia-logic structures that are different than other multi-party informal conversations (Fox Tree, 2010).
Anadditional source of dialogic structure in these dis-cussions, above and beyond the thread structure, isthe use of the quote mechanism, in which partici-pants often break a previous post down into the com-ponents of its argument and respond to each compo-nent in turn.
Many posts include quotations of previ-ous posts.
Because we hypothesize that these postsare more targeted at a particular proposition that theposter wants to comment on, than posts and repliesin general, we focus here on understanding the rela-tionship between a quoted text and a response, andthe linguistic reflexes of those relationships.
Exam-ples of quote/response pairs for several of our topicsare provided in Figure 1.The most similar work to our own is that ofWang & Rose (2010) who analyzed Usenet fo-rum quote/response structures.
This work did notdistinguish agreement vs. disagreement acrossquote/response pairs.
Rather they show that they canuse a variant of LSA to improve accuracy for identi-fying a parent post, given a response post, with 70%accuracy.
Other similar work uses Congressionaldebate transcripts or blogs or other social media todevelop methods for distinguishing agreement fromdisagreement or to distinguish rebuttals from out-of-context posts (Thomas et al, 2006; Bansal et al,2008; Awadallah et al, 2010; Walker et al, ; Bur-foot, 2008; Mishne and Glance, 2006; Popescu andPennacchiotti, 2010).
These methods are directlyapplicable, but the genre of the language is so dif-ferent from our informal forums that the results arenot directly comparable.
Work by Somasundaran &Wiebe (2009, 2010) has examined debate websitesand focused on automatically determining the stanceof a debate participant with respect to a particular is-sue.
This work has treated each post as a text to beclassified in terms of stance, for a particular topic,and shown that discourse relations such as conces-sions and the identification of argumentation triggersimproves performance .
Their work, along with oth-ers, also indicates that for such tasks it is difficult tobeat a unigram baseline (Pang and Lee, 2008).
Otherwork has focused on the social network structureof online forums (Murakami and Raymond, 2010;Agrawal et al, 2003).
However, Agarwal?s work as-sumed that adjacent posts always disagree, and didnot use any of the information in the text.
Murakami& Raymond (2010) show that simple rules definedon the textual content of the post can improve overAgarwal?s results.Section 2 discusses our corpus in more detail, de-scribes how we collected annotations using Mechan-ical Turk, and presents results of a corpus analysisof the use of particular discourse cues.
Section 3 de-scribes how we set up classification experiments fordistinguishing agreement from disagreement, andSection 4 presents our results for agreement classifi-cation.
We also characterize the linguistic reflexes ofthis relation.
We analyze the utility of meta-post fea-tures, contextual features, dependency features andword-based features for signaling the disagreementrelation.
We show that using contextual and dia-logic features we can achieve accuracies up to 68%as compared to a unigram baseline of 63%.2 Data and Corpus AnalysisTable 1 provides an overview of some of the charac-teristics of our corpus by topic.
Figure 2 shows thewording of the survey questions that we posted foreach quote/response as Mechanical Turk hits.3Topic Discs Posts NumA P/A A>1P PL Agree Sarcasm Emote Attack Nastyevolution 872 10292 580 17.74 76% 576 10% 6% 16% 13% 9%gun control 825 7968 411 19.39 66% 521 11% 8% 21% 16% 12%abortion 564 7354 574 12.81 69 % 454 9% 6% 31% 16% 12%gay marriage 305 3586 342 10.49 69% 522 13% 9% 23% 12% 8%existence of God 105 1581 258 6.13 66% 569 11 % 7% 26% 14% 10%healthcare 81 702 112 6.27 64% 522 14% 10% 34% 17% 17%communism vs. capitalism 38 585 110 5.32 59% 393 23% 8% 15% 8% 0%death penalty 25 500 138 3.62 62% 466 25% 5% 5% 5% 5%climate change 40 361 116 3.11 55% 375 20% 9% 17% 26% 17%marijuana legalization 13 160 72 2.22 38% 473 5% 2% 20% 5% 5%Table 1: Characteristics of Different Topics.
KEY: Number of discussions and posts on the topic (Discs, Posts).Number of authors (NumA).
Posts per author (P/A).
Authors with more than one post (A > 1P).
Median post Lengthin Characters (PL).
The remainder of the columns are the annotations shown in Figure 2.
Percentage of posts thatagree (Agree%), use sarcasm (Sarcasm%), are emotional (Emote), attack the previous poster (Attack), and arenasty (Nasty).
The scalar values are threshholded at -1,1.Our corpus is derived from a debate oriented in-ternet forum called 4forums.com.
It is a typical in-ternet forum built on the vBulletin software.
Peopleinitiate discussions (threads) and respond to others?posts.
Each thread has a tree-like dialogue structure.Each post has author information and a timestampwith minute resolution.
Many posts include quota-tions of previous posts.
For this work we chose tofocus on quotations because they establish a clear re-lationship between the quoted text and the response.Our corpus consists of 11,216 discussions and109,553 posts by 2764 authors.
We hand annotateddiscussions for topic from a set of previously identi-fied contentious political and social issues.
The web-site is tailored to a US audience and our topics aresomewhat US centric.
Table 1 describes features ofour topics in order of decreasing discussion count.When restricted to these topics, the corpus consistsof 2868 discussions, 33,089 posts, and 1302 authors.Many posts include quotations.
Overall 60,382posts contain one or more quotation.
Within ourtopics of interest, nearly 20,000 posts contain quota-tions.
We defined a quote-response pair (Q-R pair)where the response was the portion of the respond-ing post directly following a quotation but precedingany additional quotations.We selected 10,003 Q-R pairs from the topicsof interest for a Mechanical Turk annotation task.These were biased by cue word to ensure adequatedata for discourse marker analysis (See Section 2.1.For this task we showed annotators seven Q-R pairsand asked them to judge Agreement/Disagreementand a set of other measures as shown in Figure 2.Most of our measures were scalar; we chose to dothis because previous work on estimating the rela-tionship between MTurk annotations and expert an-notations suggest that taking the means of scalarannotations could be a good way to reduce noisein MTurk annotations (Snow et al, 2008).
For allof the measures annotated, the Turkers were notgiven additional definitions of their meaning.
Forexample, we let Turkers to use their native intu-itions about what it means for a post to be sarcas-tic, since previous work suggests that non-specialiststend to collapse all forms of verbal irony under theterm sarcastic (Bryant and Fox Tree, 2002).
Wedid not ask Turkers to distinguish between sarcasmand other forms of verbal irony such as hyperbole,understatement, rhetorical questions and jocularity(Gibbs, 2000).Agreement was a scalar judgment on an 11 pointscale [-5,5] implemented with a slider.
The anno-tators were also able to signal uncertainty with anCAN?T TELL option.
Each of the pairs was anno-tated by 5-7 annotators.
We showed the first 155characters of each quote and each response.
We alsoprovided a SHOW MORE button which expanded thepost to its full length.
After annotation, we removeda number of Q-R pairs in cases where a clear linkbetween the quote and a previous post could not beestablished, e.g.
the source quoted was not anotherpost, but the NY Times.
This left us with 8,242 Q-Rpairs for our final analysis.
Resampling to a naturaldistribution left us with 2,847 pairs which we usedto build our machine learning test set.
We used theremaining annotated and unannotated pairs for de-4velopment.Type ?
Survey QuestionS 0.62 Agree/Disagree: Does the respondent agree or dis-agree with the prior post?S 0.32 Fact/Emotion: Is the respondent attempting tomake a fact based argument or appealing to feel-ings and emotions?S 0.42 Attack/Insult: Is the respondent being support-ive/respectful or are they attacking/insulting intheir writing?B 0.22 Sarcasm: Is the respondent using sarcasm?S 0.46 Nice/Nasty: Is the respondent attempting to be niceor is their attitude fairly nasty?Figure 2: Mechanical Turk Annotations (Binary = B andScalar = S) and level of agreement as Krippendorff?s ?.Figure 3 provides examples from the endpoints and means of the annotations for threeof the questions, Respect/Insult, Sarcasm, andFact/Emotion.
Nice/Nasty and Respect/Insult arestrongly correlated by worker annotations r(54003)= 0.84, p < 2.2e-16 and both weakly corre-lated with Agree/Disagree ratings (r(54003) = 0.32and r(54003)=0.36, respectively; p < 2.2e-16)and Fact/Emotion ratings (r(54003) = 0.32 andr(54003)=0.31, respectively; p < 2.2e-16), whileAgree/Disagree and Fact/Emotion ratings show thesmallest correlation, r(54003)=0.11, p < 2.2e-16.For the linguistic marker correlations discussed be-low we averaged scores across annotators, a processwhich sharpened correlations (e.g., Respect/Insultmeans correlate with Agree/Disagree means morestrongly (r(5393) = 0.51) as well as Nice/Nastymeans (r(5393) = 0.91; Agree/Disagree is far lesscorrelated with Fact/Emotion (r(5393) = 0.07).
In-terannotator agreement was computed using Krip-pendorff?s ?
(due to the variability in number of an-notators that completed each hit), assuming an ordi-nal scale for all measures except sarcasm; see Fig-ure 2.
The low agreement for Sarcasm accords withnative intuition ?
it is the class with the least de-pendence on lexicalization and the most subject tointer-speaker stylistic variation.
The relatively lowresults for Fact/Emotion is perhaps due to the emo-tional charge many ideological arguments engender;informal examination of posts that showed the mostdisagreement in this category often showed a cut-ting comment or a snide remark at the end of a post,which was was ignored by some annotators and ev-idence for others (one Emotional post in Figure 3 isclearly an insult, but was uniformly labeled as -5 byall annotators).2.1 Discourse MarkersBoth psychological research on discourse processes(Fox Tree and Schrock, 1999; Fox Tree and Schrock,2002; Groen et al, 2010) and computational workon agreement (Galley et al, 2004) indicate that dis-course markers are strongly associated with partic-ular pragmatic functions.
Because of their salientposition, we test the role of turn-initial markers inpredicting upcoming content (Fox Tree and Schrock,2002; Groen et al, 2010).
Based on manual inspec-tion of a subset of the corpus, we constructed a list of20 discourse markers; 17 of these occurred at least50 times in a quote response (upper bound of 700samples): actually, and, because, but, I believe, Iknow, I see, I think, just, no, oh, really, so, well,yes, you know, you mean.
All of their occurrencesbecame part of the 10,003 Q-R pairs annotated.The top discourse markers highlighting disagree-ment were really (67% read a response beginningwith this marker as prefacing a disagreement witha prior post), no (66%), actually (60%), but (58%),so (58%), and you mean (57%).
At this point, thenext most disagreeable category was the unmarkedcategory, with about 50% of respondents interpret-ing an unmarked post as disagreeing.
On the otherhand, the most agreeable marker was yes (73% reada response beginning with this marker as prefacingan agreement) followed by I know (64%), I believe(62%), I think (61%), and just (57%).
The othermarkers were close to the unmarked category: and(50%), because (51%), oh (51%), I see (52%), youknow (54%), and well (55%).The overall agreement on sarcasm was low, as inother computational work on recognizing sarcasm(Davidov et al, 2010).
At most, only 31% of re-spondents agreed that the material after a discoursemarker was sarcastic, with the most sarcastic mark-ers being you mean (31%), oh (29%), really (24%),so (22%), and I see (21%).
Only 15% of respon-dents rated the unmarked category as sarcastic (e.g.,fewer than 1 out of 6 respondents).
The cues I think(10%), I believe (9%), and actually (10%) were theleast sarcastic markers.Taken together, these ratings suggest that the cuesreally, you mean, and so can be used to indicate both5Class Very High Degree Neutral Very Low DegreeInsultorAttackWell, you have proven yoruself to be aman with no brain, that is for sure.
Thedefinition that was given was the one thatscientists use, not the layperson.The empire you defend is tyrannical.They are responsible for the death of mil-lions.Very well put.Is that what you said right be-fore they started banning assaultweapons?...Obviously, you?re gullible.Since you?re such a brainiac and all, whydon?t you visit the UN website and seewhat your beloved UN is up to?Bad comparisons.
A fair comparisonwould be comparing the total number ofdefensive gun uses to the total numberof gun crimes (not just limiting it to gunhomicides).In some cases yes, in others no.
If themutation gives a huge advantage, thenthere will be a decline in the size of thegene pool for a while (eg when the Aus-tralian rabbit population...Sarcasm My pursuit of happiness is denied bytrees existing.
Let?s burn them down anddestroy the environment.
It?s much bet-ter than me being unhappy.An interesting analysis of that article youkeep quoting from the World Net Daily[url]I would suggest you look at the faero is-land mouse then.
That is a new species,and it is not man doing it, but rather na-ture itself.Like the crazy idea the Earth goes aroundthe Sun.Indeed there is no diffrence it is still adead baby but throwing a baby in a trashcan and leaving it for dead is far morecruel than abortion.Too late, drug usage has already createdthose epidemics.
Legalizing drugs mayincrease some of them temporarily, butthey already exist.Emotion-basedArgu-mentReally!
You can prove that most pro-lifers don?t care about women?...it is id-iotic thinking like this that makes me re-spect you less and less.Fine by me.
First, I don?t consider hav-ing a marriage recognized by govern-ment to be a ?right?.
Second, I?ve saidmany times I don?t think governmentshould be in the marriage business at all.Sure.
Here is an explanation.
The 14CMethod.
That is from the RadiocarbonWEB info site by the Waikato Radio-carbon Dating Lab of the University ofWaikato (New Zeland).I love Jesus John the Beloved is my mostfavorite writer throughout time If youthink I have a problem with a followerof Jesus your wrong.
I have a problemwith the ChristiansI agree that the will to survive is an amaz-ing phenomenon when put to the test.But I do not agree with your statementof life at *any* cost.
There will alwaysbe a time when the humane/loving thingto do is to let an infant/child/adult go.Heller is about determining the answer toa long standing question on the nature ofthe Second Amendment, and how muchgun control is legally allowed.
Roe v.Wade is about finding legal precedent forthe murder of unborn children.
I see ab-solutely no comparison between the two.Figure 3: Sample Responses for the Insult, Sarcasm, and Fact/Feeling spectrumsdisagreement and sarcasm.
However, but, no, andactually can be used for disagreement, but not sar-casm.
And I know (14% sarcastic, similar to None),I believe, and I think can be used for non-sarcasticagreement.From informal analyses, we hypothesized that re-ally and oh might indicate sarcasm.
While we foundevidence supporting this for really, it was not thecase for oh.
Instead, oh was used to indicate emo-tion; it was the discourse marker with the highestratings of feeling over fact.Despite the fact that it would seem that disagree-ment would be positively correlated with sarcasm,disagreement and sarcasm were not related.
Therewere two tests possible.
One tested the percentage ofpeople who identified an item as disagreeing againstthe percentage of people who identified it as sar-casm, r(16) = -.27, p = .27 (tested on 17 discoursemarkers plus the None category).
The other testedthe degree of disagreement (from -5 to +5) againstthe percentage of people who identified the post assarcastic, r(16) = -.33, p = .18.However, we did observe relationships betweensarcasm and other variables.
Two results support theargument that sarcasm is emotional and personal.The more sarcastic, the nastier (rather than nicer),r(16) = .87, p < .001.
In addition, the more sarcas-tic, the more emotional (over factual) respondentswere judged to be, r(16) = .62, p = .006 Taken to-gether, these analyses suggest that sarcasm is emo-tional and personal, but not necessarily a sign of dis-agreement.3 Machine Learning Experimental SetupFor our experiments we used the Weka machinelearning toolkit.
All results are from 10 fold cross-validation on a balanced test set.
Unless otherwisementioned, we used thresholds of 1 and -1 on themean agreement judgment to determine agreement6and disagreement respectively.
We omitted those Q-R pairs which were judged neutral (mean annotatorjudgment in the (-1,1) range).As described above, from the original 10,003 Q-R pairs we applied certain constraints (notably re-quirement that we be able to identify the originatingpost) which left us with 8,242.
We then resampledto obtain a natural distribution leaving us with 2,847pairs.
Applying the (-1,1) threshold and balancingthe result yielded a test set of 682 Q-R pairs.3.1 ClassifiersOur experiments used two simple classifiers: Naive-Bayes and JRip.
NaiveBayes makes a strict indepen-dence assumption and can be swamped by the sheernumber of features we used, but it is a solid baselineand does a decent job of suggesting which featuresare more powerful.
JRip is a rule based classifierwhich produces a compact model suitable for hu-man consumption and quick application.
JRip is notwithout its own limitations but, for our task, it showsbetter results than NaiveBayes.
The model it buildsuses only a handful of features.3.2 Feature ExtractionOur aim was to develop features for the automaticidentification of agreement and disagreement thatwould do well on the task and provide useful base-lines for comparisons with previous and future work.Features are grouped into sets as shown in Table 2and discussed in more detail below.Set Description/ExamplesMetaPost Non-lexical features.
E.g.
posterid, time be-tween posts, etc.Unigrams,BigramsWord and Word Pair frequenciesCue Words Initial unigram, bigram, and trigramPunctuation Collapsed into one of the following: ?
?, !
!, ?
!LIWC LIWC measures and frequenciesDependencies Dependencies derived from the StanfordParser.Generalized De-pendenciesDependency features generalized with re-spect to POS of the head word and opinionpolarity of both words.Table 2: Feature Sets, Descriptions, and ExamplesUnigrams, Bigrams, Trigrams.
Results of pre-vious work suggest that a unigram baseline can bedifficult to beat for certain types of debates (Walkeret al, ; Somasundaran and Wiebe, 2010).
Thus wederived both unigrams and bigrams as features.
Wecaptured the final token as a feature by padding with-nil- tokens when building the bigrams.
See belowfor comments on initial uni/bi/tri-grams.MetaPost Info.
Previous work suggested thatnon-lexical features like poster ids and the time be-tween posts might contain indicators of disagree-ment.
People on these forums get to know one an-other and often enjoy repeatedly arguing with thesame person.
In addition, we hypothesized that the?heat?
of a particular conversation could be corre-lated with rapid-fire exchanges, as indicated by shorttime periods between posts.Thus these features involve structure outside ofthe quote/response text.
This includes author infor-mation, time between posts, the log10 of the timebetween posts, the number of other quotes in theresponse, whether the quote responds to a post bythe response?s author, the percent of the quoted postwhich is actually quoted, whether the quoted post isby the same author as the response (there were onlyan handful of these), whether the response mentionsthe quote author by name, and whether the responseis longer than the quote.The forum software effectively does this annota-tion for us so there is no reason not to consider it asa clue in our quest to understand and interpret onlinedialogue.Discourse Markers.
Previous work on dialogueanalysis has repeatedly noted the discourse func-tions of particular discourse markers, and our corpusanalysis above also suggests their use in this par-ticular dataset (Hirschberg and Litman, 1993; FoxTree, 2010; Schiffrin, 1987; Di Eugenio et al, 1997;Moser and Moore, 1995).
However, because dis-course markers can be stacked up Oh, so really wedecided to represent this feature as post initial uni-grams, bigrams and trigrams.Repeated Punctuation.
Informal analyses of ourdata suggested that repeated sequential use of partic-ular types of punctuation such as !!
and ??
did notmean the same thing as simple counts or frequen-cies of punctuation across a whole post.
Thus wedeveloped distinct features for a subset of these rep-etitions.LIWC.
We also derived features using the Lin-guistics Inquiry Word Count tool (LIWC-2001)(Pennebaker et al, 2001).
LIWC classifies words7into 69 categories and counts how many words getclassified into each category.
Some LIWC featuresthat we expect to be important are words per sen-tence (WPS), pronominal forms, and positive andnegative emotion words.Dependency and Generalized Dependency.
Weused the Stanford parser to extract dependency fea-tures for each quote and response (De Marneffe etal., 2006; Klein and Manning, 2003).
The depen-dency parse for a given sentence is a set of triples,composed of a grammatical relation and the pairof words for which the grammatical relation holds(reli, wj , wk), where reli is the dependency relationamong words wj and wk.
The word wj is the HEADof the dependency relation.Following (Joshi and Penstein-Rose?, 2009) we ex-tracted generalized dependency features by leavingone dependency element lexicalized and generaliz-ing the other to part of speech.
Joshi & Rose?s re-sults suggested that this approach would work betterthan either fully lexicalized or fully generalized de-pendency features.Opinion Dependencies.
Somasundaran & Wiebe(2009) introduce the concept of features that iden-tify the TARGET of opinion words.
Inspired by thisapproach, we used the MPQA dictionary of opinionwords to select the subset of dependency and gen-eralized dependency features in which those opin-ion words appear.
For these features we replace theopinion words with their positive or negative polar-ity equivalents.Cosine Similarity.
This feature is based on previ-ous work on threading.
We derive cosine-similaritymeasure using tf-idf vectors where the documentfrequency was derived from the entire topic re-stricted corpus.Annotations.
We also add features represent-ing information that we do not currently derive au-tomatically, but which might be automatically de-rived in future work based on annotations in the cor-pus.
These include the topic and Mechanical Turkannotations for Fact/Emotion, Respect/Insult, Sar-casm, and Nasty/Nice, which could reasonably beexpected to be recognized independently of Agree-ment/Disagreement.FeaturetypeSelected FeaturesMeta number-of-other-quotes, percent-quoted, author-quote-USERNAMEInitialn-gramyes, so, I agree, well said, really?, I don?t knowBigram that you, ?
-nil-, you have, evolution isDepend-encydep-nsubj(agree, i), dep-nsubj(think, you), dep-prep-with(agree, you)OpinionDepen-dencydep-opinion-nsubj(negative, you), dep-opinion-dep(proven, negative), dep-opinion-aux(positive,to)Anno-tationstopic-gay marriage, mean-response-nicenasty, mean-unsure-sarcasmTable 3: Some of the more useful features for each cate-gory, using ?2 for feature selection.Figure 4: Sample model learned using JRip.
The num-bers represent (total instances covered by a rule / numberincorrectly labeled).
This particular model was built ondevelopment data.4 ResultsTable 3 shows features which were selected for eachof our feature categories using a ?2 test for fea-ture selection.
These results vindicate our interestin discourse markers as cues to argument structure,as well as the importance of the generalized depen-dency features and opinion target pairs (Wang andRose?, 2010; Somasundaran and Wiebe, 2009).
Fig-ure 4 shows a sample model learned using JRip.We limit our pair-wise comparisons between clas-sifiers and feature sets to those corresponding to par-8Feats NB JRip?2Uni,UniCue 0.578 0.626BOW 0.598 0.654Meta 0.579 0.588Response Local 0.600 0.666Quote Local 0.531 0.588Both Local 0.601 0.682Meta+Local 0.603 0.654All 0.603 0.632Just Annotations 0.765 0.814All+Annotations 0.603 0.795Table 4: Accuracies on a balanced test set (random base-line: 0.5).
NB = NaiveBayes.
JRip?2 = Jripper with ?2feature selection on the training set during cross valida-tion.
BOW = Unigrams, CueWords, Bigrams, Trigrams,LIWC, Repeated Punctuation.
Response/Quote/BothLocal uses only those features which exist in the text ofthe response or quote respectively.
It consists of LIWC,dependencies, generalized dependencies, the various n-grams, and length measures.ticular hypotheses.
We conducted five tests withBonferroni correction to .01 for a .05 level of sig-nificance.While we hypothesized that more sophisticatedlinguistic features would improve over unigram fea-tures alone, a paired t-test using the results in Table 4indicate that there is no statistical difference be-tween the performance of JRip using only responselocal features (JRip,ResponseLocal), as compared tothe Unigram,UniCue features (t(9) = 2.18, p = .06).However, a paired t-test using the results inTable 4 indicate that there is a statistical dif-ference between the performance of JRip usinglocal features from both the quote and the re-sponse, (JRip,BothLocal) as compared to the Uni-gram,UniCue features (t(9) = 3.94, p =.003).
Thisshows that the contextual features do matter, eventhough (JRip,BothLocal) does not provide signifi-cant improvements over (JRip,Response Local) (t(9)= .92, p = .38).In general, examination of the table suggeststhat the JRip classifier performs better than NaiveBayes.
A paired t-test indicates that there is a sta-tistical difference between the performance of JRipusing local features from both the quote and theresponse, (JRip,BothLocal) (JRip,BothLocal) andNaive Bayes using local features from both the quoteand the response, (NB,BothLocal) (t(9) = 3.43, p =.007).In addition, with an eye toward the future, we ex-amined whether automatic recognition of sarcasm,attack/insult, fact/feeling nice/nasty could possiblyimprove results for recognizing disagreement.
Us-ing the human annotations as a proxy for automaticresults, we get classification accuracies of over 81%(JRip,JustAnnotations).
This suggests it might bepossible to improve results over our best current re-sults (JRip,BothLocal) (t(9) = 6.09, p < .001).Another interesting fact, is that despite its use inprevious work for threading, the cosine similaritybetween the quote and response did not improve ac-curacy for the classifiers we tested, over and abovethe use of text-based contextual features.
Furtherinvestigation is required to draw conclusions aboutthis or similar metrics (LSA, PMI, etc.
).5 Discussion and ConclusionIn this paper, we have introduced a new collectionof internet forum posts, the ARGUE corpus, col-lected across a range of ideological topics, and con-taining scalar Agreement/Disagreement annotationsover quote-response pairs within a post.
We havedemonstrated that we can achieve a significant im-provement over a unigram baseline agreement de-tection system using features from both a responseand the quote being responded to.Beyond agreement, the ARGUE corpus containsfiner-grained annotations for degrees of insult, nas-tiness, and emotional appeal, as well as the pres-ence of sarcasm.
We have demonstrated that theseclasses (especially insult and nastiness) correlatewith agreement.
While the utility of these classesas features for agreement detection is dependent onhow easily they are learned, in closing we note thatthey also afford us a richer understanding of how ar-gumentative conversation flows.
In section 2.1.2, weoutlined how they can yield understanding of the po-tential functions of a discourse particle within a par-ticular post.
They may as allow us to understand theextent to which participants react in kind, rewardinginsult with insult or kindness in turn.
In future work,we hope to turn to these conversational dynamics.In future work, it would be useful tobuild a ternary classifier which labels9Agree/Disagree/Neutral, thus reflecting the truedistribution of these dialogue acts in the data.Additionally, the proportion of agreeing utterancesvaries widely across media so it may be desirable toadd an appropriate prior when adapting the modelto a new dataset.AcknowledgmentsThis work was funded by Grant NPS-BAA-03 toUCSC and and through the Intelligence AdvancedResearch Projects Activity (IARPA) through theArmy Research Laboratory.
We?d like to thankCraig Martell for helpful discussions over the courseof this project, and the anonymous reviewers for use-ful feedback.
We would also like to thank MichaelMinor and Jason Aumiller for their contributions toscripting and the database.ReferencesR.
Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu.
2003.Mining newsgroups using networks arising from so-cial behavior.
In Proceedings of the 12th internationalconference on World Wide Web, pages 529?535.
ACM.J.L.
Austin.
1965.
How to do things with words.
OxfordUniversity Press, New York.R.
Awadallah, M. Ramanath, and G. Weikum.
2010.Language-model-based pro/con classification of polit-ical text.
In Proceeding of the 33rd international ACMSIGIR conference on Research and development in in-formation retrieval, pages 747?748.
ACM.M.
Bansal, C. Cardie, and L. Lee.
2008.
The powerof negative thinking: Exploiting label disagreement inthe min-cut classification framework.
Proceedings ofCOLING: Companion volume: Posters, pages 13?16.J.
Bollen, H. Mao, and X. Zeng.
2011.
Twitter moodpredicts the stock market.
Journal of ComputationalScience.G.A.
Bryant and J.E.
Fox Tree.
2002.
Recognizing ver-bal irony in spontaneous speech.
Metaphor and sym-bol, 17(2):99?119.C.
Burfoot.
2008.
Using multiple sources of agree-ment information for sentiment classification of polit-ical transcripts.
In Australasian Language TechnologyAssociation Workshop 2008, volume 6, pages 11?18.Robert B. Cialdini.
2000.
Influence: Science and Prac-tice (4th Edition).
Allyn & Bacon.D.
Davidov, O. Tsur, and A. Rappoport.
2010.
Semi-supervised recognition of sarcastic sentences in twitterand amazon.
In Proceedings of the Fourteenth Confer-ence on Computational Natural Language Learning,pages 107?116.
Association for Computational Lin-guistics.M.C.
De Marneffe, B. MacCartney, and C.D.
Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In Proceedings of LREC, vol-ume 6, pages 449?454.
Citeseer.Barbara Di Eugenio, Johanna D. Moore, and MassimoPaolucci.
1997.
Learning features that predict cueusage.
In Proceedings of the 35th Annual Meet-ing of the Association for Computational Linguistics,ACL/EACL 97, pages 80?87.J.E.
Fox Tree and J.C. Schrock.
1999.
Discourse Mark-ers in Spontaneous Speech: Oh What a Differencean Oh Makes.
Journal of Memory and Language,40(2):280?295.J.E.
Fox Tree and J.C. Schrock.
2002.
Basic mean-ings of you know and I mean.
Journal of Pragmatics,34(6):727?747.J.
E. Fox Tree.
2010.
Discourse markers across speak-ers and settings.
Language and Linguistics Compass,3(1):113.M.
Galley, K. McKeown, J. Hirschberg, and E. Shriberg.2004.
Identifying agreement and disagreement inconversational speech: Use of bayesian networks tomodel pragmatic dependencies.
In Proceedings of the42nd Annual Meeting on Association for Computa-tional Linguistics, pages 669?es.
Association for Com-putational Linguistics.R.W.
Gibbs.
2000.
Irony in talk among friends.Metaphor and Symbol, 15(1):5?27.M.
Groen, J. Noyes, and F. Verstraten.
2010.
The Effectof Substituting Discourse Markers on Their Role inDialogue.
Discourse Processes: A MultidisciplinaryJournal, 47(5):33.Julia Hirschberg and Diane Litman.
1993.
Empiricalstudies on the disambiguation of cue phrases.
Com-putational Linguistics, 19(3):501?530.John E. Hunter.
1987.
A model of compliance-gaining message selection.
Communication Mono-graphs, 54(1):54?63.M.
Joshi and C. Penstein-Rose?.
2009.
Generalizing de-pendency features for opinion mining.
In Proceed-ings of the ACL-IJCNLP 2009 Conference Short Pa-pers, pages 313?316.
Association for ComputationalLinguistics.D.
Klein and C.D.
Manning.
2003.
Accurate unlexical-ized parsing.
In Proceedings of the 41st Annual Meet-ing on Association for Computational Linguistics-Volume 1, pages 423?430.
Association for Computa-tional Linguistics.G.
Mishne and N. Glance.
2006.
Leave a reply: An anal-ysis of weblog comments.
In Third annual workshopon the Weblogging ecosystem.
Citeseer.10Margaret G. Moser and Johanna Moore.
1995.
Inves-tigating cue selection and placement in tutorial dis-course.
In ACL 95, pages 130?137.A.
Murakami and R. Raymond.
2010.
Support orOppose?
Classifying Positions in Online Debatesfrom Reply Activities and Opinion Expressions.
InProceedings of the 23rd International Conference onComputational Linguistics: Posters, pages 869?875.Association for Computational Linguistics.B.
Pang and L. Lee.
2008.
Opinion mining and senti-ment analysis.
Foundations and Trends in InformationRetrieval, 2(1-2):1?135.J.
W. Pennebaker, L. E. Francis, and R. J. Booth, 2001.LIWC: Linguistic Inquiry and Word Count.A.M.
Popescu and M. Pennacchiotti.
2010.
Detectingcontroversial events from twitter.
In Proceedings ofthe 19th ACM international conference on Informationand knowledge management, pages 1873?1876.
ACM.Deborah Schiffrin.
1987.
Discourse Markers.
Cam-bridge University Press, Cambridge, U.K.R.
Snow, B. O?Connor, D. Jurafsky, and A.Y.
Ng.
2008.Cheap and fast?but is it good?
: evaluating non-expertannotations for natural language tasks.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 254?263.
Association forComputational Linguistics.S.
Somasundaran and J. Wiebe.
2009.
Recognizingstances in online debates.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on NaturalLanguage Processing of the AFNLP: Volume 1-Volume1, pages 226?234.
Association for Computational Lin-guistics.S.
Somasundaran and J. Wiebe.
2010.
Recognizingstances in ideological on-line debates.
In Proceedingsof the NAACL HLT 2010 Workshop on ComputationalApproaches to Analysis and Generation of Emotion inText, pages 116?124.
Association for ComputationalLinguistics.M.
Thomas, B. Pang, and L. Lee.
2006.
Get out thevote: Determining support or opposition from Con-gressional floor-debate transcripts.
In Proceedings ofthe 2006 conference on empirical methods in naturallanguage processing, pages 327?335.
Association forComputational Linguistics.Marilyn Walker, Rob Abbott, Pranav Anand, Jean E. FoxTree, Robeson Bowmani, and Michael Minor.
CatsRule and Dogs Drool: Classifying Stance in OnlineDebate.Y.C.
Wang and C.P.
Rose?.
2010.
Making conversationalstructure explicit: identification of initiation-responsepairs within online discussions.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 673?676.
Association forComputational Linguistics.11
