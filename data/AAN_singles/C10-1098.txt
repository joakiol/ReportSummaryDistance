Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 869?876,Beijing, August 2010Text Summarization of Turkish Texts usingLatent Semantic AnalysisMakbule Gulcin OzsoyDept.
of Computer Eng.Middle East Tech.
Univ.e1395383@ceng.metu.edu.trIlyas CicekliDept.
of Computer Eng.Bilkent Universityilyas@cs.bilkent.edu.trFerda Nur AlpaslanDept.
of Computer Eng.Middle East Tech.
Univ.alpaslan@ceng.metu.edu.trAbstractText summarization solves the problemof extracting important information fromhuge amount of text data.
There are vari-ous methods in the literature that aim tofind out well-formed summaries.
One ofthe most commonly used methods is theLatent Semantic Analysis (LSA).
In thispaper, different LSA based summariza-tion algorithms are explained and twonew LSA based summarization algo-rithms are proposed.
The algorithms areevaluated on Turkish documents, andtheir performances are compared usingtheir ROUGE-L scores.
One of our algo-rithms produces the best scores.1 IntroductionThe exponential growth in text documents bringsthe problem of finding out whether a text docu-ment meets the needs of a user or not.
In order tosolve this problem, text summarization systemswhich extract brief information from a given textare created.
By just looking at the summary of adocument, a user can decide whether the docu-ment is of interest to him/her without looking atthe whole document.The aim of a text summarization system is togenerate a summary for a given document suchthat the generated summary contains all neces-sary information in the text, and it does not in-clude redundant information.
Summaries canhave different forms (Hahn and Mani, 2000).Extractive summarization systems collect impor-tant sentences from the input text in order togenerate summaries.
Abstractive summarizationsystems do not collect sentences from the inputtext, but they try to capture the main concepts inthe text, and generate new sentences to representthese main concepts.
Abstractive summarizationapproach is similar to the way that human sum-marizers follow.
Since creating abstractivesummaries is a more complex task, most ofautomatic text summarization systems are ex-tractive summarization systems.Summarization methods can be categorizedaccording to what they generate and how theygenerate it (Hovy and Lin, 1999).
A summarycan be extracted from a single document or frommultiple documents.
If a summary is generatedfrom a single document, it is known as single-document summarization.
On the other hand, if asingle summary is generated from multipledocuments on the same subject, this is known asmulti-document summarization.
Summaries arealso categorized as generic summaries andquery-based summaries.
Generic summarizationsystems generate summaries containing maintopics of documents.
In query-based summariza-tion, the generated summaries contain the sen-tences that are related to the given queries.Extractive summarization systems determinethe important sentences of the text in order toput them into the summary.
The important sen-tences of the text are the sentences that representthe main topics of the text.
Summarization sys-tems use different approaches to determine theimportant sentences (Hahn and Mani, 2000;Hovy and Lin, 1999).
Some of them look surfaceclues such as the position of the sentence and thewords that are contained in the sentence.
Somesummarization systems use more semantic ori-ented analysis such as lexical chains in order todetermine the important sentences.
Lately, analgebraic method known as Latent SemanticAnalysis (LSA) is used in the determination of869the important sentences, and successful resultsare obtained (Gong and Liu, 2001).In this paper, we present a generic extractiveTurkish text summarization system based onLSA.
We applied the known text summarizationapproaches based on LSA in order to extract thesummaries of Turkish texts.
One of the maincontributions of this paper is the introduction oftwo new summarization methods based on LSA.One of our methods produced much better re-sults than the results of the other known methods.The rest of the paper is organized as follows.Section 2 presents the related work in summari-zation.
Section 3 explains the LSA approach indetail.
Then, the existing algorithms that use dif-ferent LSA approaches are presented (Gong andLiu, 2001; Steinberger and Jezek 2004; Murrayet al, 2005), and two new algorithms are pro-posed in Section 4.
Section 5 presents theevaluation results of these algorithms, and Sec-tion 6 presents the concluding remarks.2 Related WorkText summarization is an active research areaof natural language processing.
Its aim is to ex-tract short representative information from inputdocuments.
Since the 1950s, various methodsare proposed and evaluated.
The first studiesconducted on text summaries use simple featureslike terms from keywords/key phrases, termsfrom user queries, frequency of words, and posi-tion of words/sentences (Luhn, 1958).The use of statistical methods is another ap-proach used for summary extraction.
The mostwell known project that uses statistical approachis the SUMMARIST (Hovy and Lin, 1999).
Inthis project, natural language processing meth-ods are used together with the concept relevanceinformation.
The concept relevance informationis extracted from dictionaries and WordNet.Text connectivity is another approach used forsummarization.
The most well-known algorithmthat uses text connectivity is the lexical chainsmethod (Barzilay and Elhadad, 1997; Ercan andCicekli, 2008).
In lexical chains method, Word-Net and dictionaries are used to determine se-mantic relations between words where semanti-cally related words construct lexical chains.Lexical chains are used in the determination ofthe important sentences of the text.TextRank (Mihalcea and Tarau, 2004) is asummarization algorithm which is based ongraphs, where nodes are sentences and edgesrepresent similarity between sentences.
Thesimilarity value is decided by using the overlap-ping terms.
Cluster Lexrank (Qazvinian andRadev, 2008) is another graph-based summariza-tion algorithm, and it tries to find important sen-tences in a graph in which nodes are sentencesand edges are similarities.In recent years, algebraic methods are usedfor text summarization.
Most well-known alge-braic algorithm is Latent Semantic Analysis(LSA) (Landauer et al, 1998).
This algorithmfinds similarity of sentences and similarity ofwords using an algebraic method, namely Singu-lar Value Decomposition (SVD).
Besides textsummarization, the LSA algorithm is also usedfor document clustering and information filter-ing.3 Latent Semantic AnalysisLatent Semantic Analysis (LSA) is an algebraic-statistical method that extracts meaning of wordsand similarity of sentences using the informationabout the usage of the words in the context.
Itkeeps information about which words are usedin a sentence, while preserving information ofcommon words among sentences.
The morecommon words between sentences mean thatthose sentences are more semantically related.LSA method can represent the meaning ofwords and the meaning of sentences simultane-ously.
It averages the meaning of words that asentence contains to find out the meaning of thatsentence.
It represents the meaning of words byaveraging the meaning of sentences that containthis word.LSA method uses Singular Value Decomposi-tion (SVD) for finding out semantically similarwords and sentences.
SVD is a method thatmodels relationships among words and sen-tences.
It has the capability of noise reduction,which leads to an improvement in accuracy.LSA has three main limitations.
The first limi-tation is that it uses only the information in theinput text, and it does not use the information ofworld knowledge.
The second limitation is that itdoes not use the information of word order, syn-tactic relations, or morphologies.
Such informa-tion is used for finding out the meaning of words870and texts.
The third limitation is that the per-formance of the algorithm decreases with largeand inhomogeneous data.
The decrease in per-formance is observed since SVD which is a verycomplex algorithm is used for finding out thesimilarities.All summarization methods based on LSA usethree main steps.
These steps are as follows:1.
Input Matrix Creation: A matrix whichrepresents the input text is created.
The col-umns of the matrix represent the sentences ofthe input text and the rows represent thewords.
The cells are filled out to represent theimportance of words in sentences using dif-ferent approaches, whose details are de-scribed in the rest of this section.
The createdmatrix is sparse.2.
Singular Value Decomposition (SVD): Singu-lar value decomposition is a mathematicalmethod which models the relationshipsamong terms and sentences.
It decomposesthe input matrix into three other matrices asfollows:A = U ?
VTwhere A is the input matrix with dimensionsm x n, U is an m x n matrix which representsthe description of the original rows of the in-put matrix as a vector of extracted concepts,?
is an n x n diagonal matrix containing scal-ing values sorted in descending order, and Vis an m x n matrix which represents the de-scription of the original columns of input ma-trix as a vector of the extracted concepts.3.
Sentence Selection:  Different algorithms areproposed to select sentences from the inputtext for summarization using the results ofSVD.
The details of these algorithms are de-scribed in Section 4.The creation of the input matrix is importantfor summarization, since it affects the resultingmatrices of SVD.
There are some ways to reducethe row size of the input matrix, such as elimi-nating words seen in stop words list, or using theroot words only.
There are also different ap-proaches to fill out the input matrix cell values,and each of them affects the performance of thesummarization system differently.
These ap-proaches are as follows:1.
Number of Occurrence: The cell is filled withthe frequency of the word in the sentence.2.
Binary Representation of Number of Occur-rence: If the word is seen in the sentence, thecell is filled with 1; otherwise it is filled with0.3.
TF-IDF (Term Frequency?Inverse DocumentFrequency): The cell is filled with TF-IDFvalue of the word.
This method evaluates theimportance of words in a sentence.
The im-portance of a word is high if it is frequent inthe sentence, but less frequent in the docu-ment.
TF-IDF is equal to TF*IDF, and TFand IDF are computed as follows:tf (i,j) = n(i,j)  /  ?k n(k,j)where n(i,j) is the number of occurrences ofthe considered word i in sentence j, and    ?kn(k,j) is the sum of number of occurrences ofall words in sentence j.idf (i) = log( |D| / di)where |D| is the total number of sentences inthe input text, and di is the number of sen-tences where the word i appears4.
Log Entropy: The cell is filled with log-entropy value of the word, and it is computedas follows.sum = ?j p(i,j) log2(p(i,j))global(i) = 1 + (sum / log2(n))local(i,j)= log2(1 + f(i,j))log-entropy = global*localwhere p(i,j) is the probability of word i that isappeared in sentence j, f(i,j) is the number oftimes word i appeared in sentence j, and n isthe number of sentences in the document.5.
Root Type: If the root type of the word isnoun, the related cell is filled with the fre-quency of the word in the sentence; otherwisethe cell is filled with 0.6.
Modified TF-IDF: First the matrix is filledwith TF-IDF values.
Then, the average TF-IDF values in each row are calculated.
If thevalue in the cell is less than or equal to theaverage value, the cell value is set to 0.
Thisis our new approach which is proposed toeliminate the noise from the input matrix.8714 Text SummarizationThe algorithms in the literature that use LSA fortext summarization perform the first two steps ofLSA algorithm in the same way.
They differ inthe way they fill out the input matrix cells.4.1 Sentence Selection Algorithms in Lit-erature4.1.1.
Gong & Liu (Gong and Liu, 2001)After performing the first two steps of the LSAalgorithm, Gong & Liu summarization algorithmuses VT matrix for sentence selection.
The col-umns of VT matrix represent the sentences of theinput matrix and the rows of it represent theconcepts that are obtained from SVD method.The most important concept in the text is placedin the first row, and the row order indicates theimportance of concepts.
Cells of this matrix giveinformation about how much the sentence is re-lated to the given concept.
A higher cell valuemeans the sentence is more related to the con-cept.In Gong & Liu summarization algorithm, thefirst concept is chosen, and then the sentencemost related to this concept is chosen as a part ofthe resulting summary.
Then the second conceptis chosen, and the same step is executed.
Thisrepetition of choosing a concept and the sen-tence most related to that concept is continueduntil a predefined number of sentences are ex-tracted as a part of the summary.
In Figure 1, anexample VT matrix is given.
First, the conceptcon0 is chosen, and then the sentence sent1 ischosen, since it has the highest cell value in thatrow.There are some disadvantages of this algo-rithm, which are defined by Steinberger andJezek (2004).
First, the reduced dimension sizehas to be the same as the summary length.
Thisapproach may lead to the extraction of sentencesfrom less significant concepts.
Second, thereexist some sentences that are related to the cho-sen concept somehow, but do not have the high-est cell value in the row of that concept.
Thesekinds of sentences cannot be included in the re-sulting summary by this algorithm.
Third, allchosen concepts are thought to be in the sameimportance level, but some of those conceptsmay not be so important in the input text.sent0 sent1 sent2 sent3 sent4con0 0,557 0,691 0,241 0,110 0,432con1 0,345 0,674 0,742 0,212 0,567con2 0,732 0,232 0,435 0,157 0,246con3 0,628 0,836 0,783 0,265 0,343Figure 1.
Gong & Liu approach: From each rowof VT matrix which represents a concept, the sen-tence with the highest score is selected.
This isrepeated until a predefined number of sentencesare collected.4.1.2.
Steinberger & Jezek (Steinberger andJezek 2004)As in the Gong & Liu summarization algorithm,the first two steps of LSA algorithm are exe-cuted before selecting sentences to be a part ofthe resulting summary.
For sentence selection,both V and ?
matrixes are used.The sentence selection step of this algorithmstarts with the calculation of the length of eachsentence vector which is represented by a row inV matrix.
In order to find the length of a sen-tence vector, only concepts whose indexes areless than or equal to the number of dimension inthe new space is used.
The dimension of a newspace is given as a parameter to the algorithm.The concepts which are highly related to the textare given more importance by using the valuesin ?
matrix as a multiplication parameter.
If thedimension of the new space is n, the length ofthe sentence i is calculated as follows:?=?=njjjjii Vlength1*After the calculation of sentence lengths, thelongest sentences are chosen as a part of the re-sulting summary.
In Figure 2, an example V ma-trix is given, and the dimension of the new spaceis assumed to be 3.
The lengths of the sentencesare calculated using the first three concepts.Since the sentence sent2 has the highest length,it is extracted first as a part of the summary.The aim of this algorithm is to get rid of thedisadvantages of Gong & Liu summarizationalgorithm, by choosing sentences which are re-lated to all important concepts and at the sametime choosing more than one sentence from animportant topic.872con0 con1 con2 con3 lengthsent0 0,846 0,334 0,231 0,210 0,432sent1 0,455 0,235 0,432 0,342 0,543sent2 0,562 0,632 0,735 0,857 0,723sent3 0,378 0,186 0,248 0,545 0,235Figure 2.
Steinberger & Jezek approach: Foreach row of V matrix, the lengths of sentencesusing n concepts are calculated.
The value n isgiven as an input parameter.
?
matrix values arealso used as importance parameters in the lengthcalculations.sent0 sent1 sent2 sent3 sent4con0 0,557 0,691 0,241 0,110 0,432con1 0,345 0,674 0,742 0,212 0,567con2 0,732 0,232 0,435 0,157 0,246con3 0,628 0,836 0,783 0,265 0,343Figure 3.
Murray & Renals & Carletta ap-proach: From each row of VT matrix, concepts,one or more sentences with the higher scores areselected.
The number of sentences to be selectedis decided by using ?
matrix.4.1.3.
Murray & Renals & Carletta (Murrayet al, 2005)The first two steps of the LSA algorithm areexecuted, as in the previous algorithms beforethe construction of the summary.
VT and ?
ma-trices are used for sentence selection.In this approach, one or more sentences arecollected from the topmost concepts in VT ma-trix.
The number of sentences to be selected de-pends on the values in the ?
matrix.
The numberof sentences to be collected for each topic is de-termined by getting the percentage of the relatedsingular value over the sum of all singular val-ues, which are represented in the ?
matrix.
InFigure 3, an example VT matrix is given.
Let?schoose two sentences from con0, and one sen-tence from con1.
Thus, the sentences sent1 andsent0 are selected from con0, and sent2 is se-lected from con1 as a part of the summary.This approach tries to solve the problems ofGong & Liu?s approach.
The reduced dimensionhas not to be same as the number of sentences inthe resulting summary.
Also, more than one sen-tence can be chosen even they do not have thehighest cell value in the row of the related con-cept.4.2 Proposed Sentence Selection Algo-rithmsThe analysis of input documents indicates thatsome sentences, especially the ones in the intro-duction and conclusion parts of the documents,belong to more than one main topic.
In order toobserve whether these sentences are important orthey cause noise in matrices of LSA, we proposea new method, named as Cross.Another concern about matrices in LSA is thatthe concepts that are found after the SVD stepmay represent main topics or subtopics.
So, it isimportant to determine whether the found con-cepts are main topics or subtopics.
This causesthe ambiguity that whether these concepts aresubtopics of another main topic, or all the con-cepts are main topics of the input document.
Wepropose another new method, named as Topic, inorder to distinguish main topics from subtopicsand make sentence selections from main topics.4.2.1.
Cross MethodIn this approach, the first two steps of LSA areexecuted in the same way as the other ap-proaches.
As in the Steinberger and Jezek ap-proach, the VT matrix is used for sentence selec-tion.
The proposed approach, however, preproc-esses the VT matrix before selecting the sen-tences.
First, an average sentence score is calcu-lated for each concept which is represented by arow of VT matrix.
If the value of a cell in thatrow is less than the calculated average score ofthat row, the score in the cell is set to zero.
Themain idea is that there can be sentences such thatthey are not the core sentences representing thetopic, but they are related to the topic in someway.
The preprocessing step removes the overalleffect of such sentences.After preprocessing, the steps of Steinbergerand Jezek approach are followed with a modifi-cation.
In our Cross approach, first the cell val-ues are multiplied with the values in the ?
ma-trix, and the total lengths of sentence vectors,which are represented by the columns of the VTmatrix, are calculated.
Then, the longest sen-tence vectors are collected as a part of the result-ing summary.In Figure 4, an example VT matrix is given.First, the average scores of all concepts are cal-culated, and the cells whose values are less thanthe average value of their row are set to zero.873The boldface numbers are below row averagesin Figure 4, and they are set to zero before thecalculation of the length scores of sentences.Then, the length score of each sentence is calcu-lated by adding up the concept scores of sen-tences in the updated matrix.
In the end, the sen-tence sent1 is chosen for the summary as thefirst sentence, since it has the highest lengthscore.sent0 sent1 sent2 sent3 averagecon0 0,557 0,691 0,241 0,110 0,399con1 0,345 0,674 0,742 0,212 0,493con2 0,732 0,232 0,435 0,157 0,389con3 0,628 0,436 0,783 0,865 0,678con4 0,557 0,691 0,241 0,710 0,549length 1,846 2,056 1,960 1,575Figure 4.
Cross approach: For each row of VTmatrix, the cell values are set to zero if they areless than the row average.
Then, the cell valuesare multiplied with the values in the ?
matrix,and the lengths of sentence vectors are found, bysumming up all concept values in columns of VTmatrix, which represent the sentences.4.2.2.
Topic MethodThe first two steps of LSA algorithm are exe-cuted as in the other approaches.
For sentenceselection, the VT matrix is used.
In the proposedapproach, the main idea is to decide whether theconcepts that are extracted from the matrix VTare really main topics of the input text, or theyare subtopics.
After deciding the main topicswhich may be a group of subtopics, the sen-tences are collected as a part of the summaryfrom the main topics.In the proposed algorithm, a preprocessingstep is executed, as in the Cross approach.
First,for each concept which is represented by a rowof VT matrix, the average sentence score is cal-culated and the values less than this score are setto zero.
So, a sentence that is not highly relatedto a concept is removed from the concept in theVT matrix.
Then, the main topics are found.
Inorder to find out the main topics, a concept xconcept matrix is created by summing up the cellvalues that are common between the concepts.After this step, the strength values of the con-cepts are calculated.
For this calculation, eachconcept is thought as a node, and the similarityvalues in concept x concept matrix are consid-ered as edge scores.
The strength value of eachconcept is calculated by summing up the valuesin each row in concept x concept matrix.
Thetopics with the highest strength values are cho-sen as the main topic of the input text.sent0 sent1 sent2 sent3 averagecon0 0,557 0,691 0,241 0,110 0,399con1 0,345 0,674 0,742 0,212 0,493con2 0,732 0,232 0,435 0,157 0,389con3 0,628 0,436 0,783 0,865 0,678con4 0,557 0,691 0,241 0,710 0,549con0 con1 con2 con3 con4 strengthcon0 1,248 1,365 1,289 0 2,496 6,398con1 1,365 1,416 1,177 1,525 1,365 6,848con2 1,289 1,177 0,732 1,218 1,289 5,705con3 0 1,525 1,218 1,648 1,575 5,966con4 2,496 1,365 1,289 1,575 1,958 8,683sent0 sent1 sent2 sent3con0 0,557 0.691 0 0con1 0 0,674 0,742 0con2 0,732 0 0,435 0con3 0 0 0,783 0,865con4 0,557 0.691 0 0,710Figure 5.
Topic approach: From each row of VTmatrix, concepts, the values are set to zero ifthey are less than the row average.
Then conceptx concept similarity matrix is created, and thestrength values of concepts are calculated, whichshow how strong the concepts are related to theother concepts.
Then the concept whose strengthvalue is highest is chosen, and the sentence withthe highest score from that concept is collected.The sentence selection s repeated until a prede-fined number of sentences is collected.After the above steps, sentence selection isperformed in a similar manner to Gong and Liuapproach.
For each main topic selected, the sen-tence with the highest score is chosen.
This se-lection is done until predefined numbers of sen-tences are collected.In Figure 5, an example VT matrix is given.First, the average scores of each concept is cal-culated and shown in the last column of the ma-874trix.
The cell values that are less than the rowaverage value (boldface numbers in Figure 5)are set to zero.
Then, a concept x concept matrixis created by filling a cell with the summation ofthe cell values that are common between thosetwo concepts.
The strength values of the con-cepts are calculated by summing up the conceptvalues, and the strength values are shown in thelast column of the related matrix.
A higherstrength value indicates that the concept is muchmore related to the other concepts, and it is oneof the main topics of the input text.
After findingout the main topic which is the concept con4 inthis example, the sentence with the highest cellvalue which is sentence sent3 is chosen as a partof the summary.5 EvaluationTwo different sets of scientific articles in Turk-ish are used for the evaluation our summariza-tion approach.
The articles are chosen from dif-ferent areas, such as medicine, sociology, psy-chology, having fifty articles in each set.
Thesecond data set has longer articles than the firstdata set.
The abstracts of these articles, whichare human-generated summaries, are used forcomparison.
The sentences in the abstracts maynot match with the sentences in the input text.The statistics about these data sets are given inTable 1.DS1 DS2Number of documents 50 50Sentences per document 89,7 147,3Words per document 2302,2 3435Words per sentence 25,6 23,3Table 1.
Statistics of datasetsEvaluation of summaries is an active researcharea.
Judgment of human evaluators is a com-mon approach for the evaluation, but it is verytime consuming and may not be objective.
An-other approach that is used for summarizationevaluation is to use the ROUGE evaluation ap-proach (Lin and Hovy, 2003), which is based onn-gram co-occurrence, longest common subse-quence and weighted longest common subse-quence between the ideal summary and the ex-tracted summary.
Although we obtained allROUGE results (ROUGE-1, ROUGE-2,ROUGE-3, ROUGE-W and ROUGE-L) in ourevaluations, we only report ROUGE-L results inthis paper.
The discussions that are made de-pending on our ROUGE-L results are also appli-cable to other ROUGE results.
Different LSAapproaches are executed using different matrixcreation methods.G&L S&J MRC Cross Topicfrequency 0,236 0,250 0,244 0,302 0,244binary 0,272 0,275  0,274  0,313 0,274tf-idf 0,200 0,218 0,213 0,304 0,213logentropy 0,230 0,250 0,235  0,302  0,235root type 0,283 0,282  0,289  0,320  0,289mod.
tf-idf 0,195 0,221  0,223  0,290  0,223Table 2.
ROUGE-L scores for the data set DS1In Table 2, it can be observed that the Crossmethod has the highest ROUGE scores for allmatrix creation techniques.
The Topic methodhas the same results with Murray & Renals &Carletta approach, and it is better than the Gong& Liu approach.Table 2 indicates that all algorithms give theirbest results when the input matrix is created us-ing the root type of words.
Binary and log-entropy approaches also produced good results.Modified tf-idf approach, which is proposed inthis paper, did not work well for this data set.The modified tf-idf approach lacks performancebecause it removes some of the sentences/wordsfrom the input matrix, assuming that they causenoise.
The documents in the data set DS1 areshorter documents, and most of words/sentencesin shorter documents are important and shouldbe kept.Table 3 indicates that the best F-score isachieved for all when the log-entropy method isused for matrix creation.
Modified tf-idf ap-proach is in the third rank for all algorithms.
Wecan also observe that, creating matrix accordingto the root types of words did not work well forthis data set.Given the evaluation results it can be said thatCross method, which is proposed in this paper,is a promising approach.
Also Cross approach isnot affected from the method of matrix creation.It produces good results when it is comparedagainst an abstractive summary which is createdby a human summarizer.875G&L S&J MRC Cross Topicfrequency 0,256 0,251 0,259 0,264 0,259binary 0,191 0,220 0,189 0,274 0,189tf-idf 0,230 0,235 0,227 0,266 0,227logentropy 0,267 0,245 0,268 0,267 0,268root type 0,194 0,222 0,197 0,263 0,197mod.
tf-idf 0,234 0,239 0,232 0,268 0,232Table 3.
ROUGE-L scores for the data set DS26 ConclusionThe growth of text based resources brings theproblem of getting the information matchingneeds of user.
In order to solve this problem, textsummarization methods are proposed and evalu-ated.
The research on summarization startedwith the extraction of simple features and im-proved to use different methods, such as lexicalchains, statistical approaches, graph based ap-proaches, and algebraic solutions.
One of thealgebraic-statistical approaches is Latent Seman-tic Analysis method.In this study, text summarization methodswhich use Latent Semantic Analysis are ex-plained.
Besides well-known Latent SemanticAnalysis approaches of Gong & Liu, Steinberger& Jezek and Murray & Renals & Carletta, twonew approaches, namely Cross and Topic, areproposed.Two approaches explained in this paper areevaluated using two different datasets that are inTurkish.
The comparison of these approaches isdone using the ROUGE-L F-measure score.
Theresults show that the Cross method is better thanall other approaches.
Another important result ofthis approach is that it is not affected by differ-ent input matrix creation methods.In future work, the proposed approaches willbe improved and evaluated in English texts aswell.
Also, ideas that are used in other methods,such as graph based approaches, will be usedtogether with the proposed approaches to im-prove the performance of summarization.AcknowledgmentsThis work is partially supported by The Scien-tific and Technical Council of Turkey Grant?TUBITAK EEEAG-107E151?.ReferencesBarzilay, R. and Elhadad, M. 1997.
Using LexicalChains for Text Summarization.
Proceedings ofthe ACL/EACL'97 Workshop on Intelligent Scal-able Text Summarization, pages 10-17.Ercan G. and Cicekli, I.
2008.
Lexical Cohesionbased Topic Modeling for Summarization.
Pro-ceedings of 9th Int.
Conf.
Intelligent Text Process-ing and Computational Linguistics (CICLing-2008), pages 582-592.Gong, Y. and Liu, X.
2001.
Generic Text Summariza-tion Using Relevance Measure and Latent Seman-tic Analysis.
Proceedings of SIGIR'01.Hahn, U. and Mani, I.
2000.
The challenges of auto-matic summarization.
Computer, 33, 29?36.Hovy, E. and Lin, C-Y.
1999.
Automated Text Sum-marization in SUMMARIST.
I. Mani and M.T.Maybury (eds.
), Advances in Automatic TextSummarization, The MIT Press, pages 81-94.Landauer, T.K., Foltz, P.W.
and Laham, D. 1998.
AnIntroduction to Latent Semantic Analysis.
Dis-course Processes, 25, 259-284.Lin, C.Y.
and Hovy, E.. 2003.
Automatic Evaluationof Summaries Using N-gram Co-occurrence Sta-tistics.
Proceedings of 2003 Conf.
North AmericanChapter of the Association for Computational Lin-guistics on Human Language Technology (HLT-NAACL-2003), pages 71-78.Luhn, H.P.
1958.
The Automatic Creation of Litera-ture Abstracts.
IBM Journal of Research Devel-opment 2(2), 159-165.Mihalcea, R. and Tarau, P. 2004.
Text-rank - bringingorder into texts.
Proceeding of the Conference onEmpirical Methods in Natural Language Process-ing.Murray, G., Renals, S. and Carletta, J.
2005.
Extrac-tive summarization of meeting recordings.
Pro-ceedings of the 9th European Conference onSpeech Communication and Technology.Qazvinian, V. and Radev, D.R.
2008.
Scientific papersummarization using citation summary networks.Proceedings of COLING2008, Manchester, UK,pages 689-696.Steinberger,  J. and Jezek, K. 2004.
Using Latent Se-mantic Analysis in Text Summarization and Sum-mary Evaluation.
Proceedings of ISIM '04, pages93-100.876
