Parallel Distributed Processing and Role Assignment Constraints.lames L. McClel landCarnegie-Mel lon UniversityMy work in natural language processing is based on the premise that it is not in general possible to recover theunderlying representations of sentences wilhout considering semantic onstraints on their possible case structures, itseems clear that people use these constraints o do several things:To assign constituents o the proper ease roles and attach then to the proper other constituents.To assign the appropriate reading to a word or larger constituent when it occurs in context.To assign default values to missing constituents.To instantiate the concepts referenced by the words in a sentence so that they fit the context.I believe that parallel-distributed processing models (i.e., conneelionist models which make use of distributedrepresentations) provide the mechanisms that are needed for these lasks.
Argument altachments and role assign-ments seem to require a consideration of the relative merits of competing possibilities (Marcus, 1980; Bates andMacWhinney, 1987; MaeWhinney, 1987), as deles lexical dlsambigualion.
Conuectionist models provide a verynatural substrate for these kinds of competition processes (Cottrell, 1985; Wallz and Pollack, 1985).The use of distributed representations al o seems well suited to capturing many aspects of the way peopleexploit sen)antic onstraints.
For choosing between two distinct alternalive inlerprelatlons of a constituent, local anddislribnted representations may be approximately equivalent, but distributed represenlalions are much more naturalfrom c~pturing contextual shading of the interpretation f a constituent, hi a dislributed representation the patternof aclivation Ihat is most typically activated by a particular word or phrase can lye sublly shaded hy constralnLsimposed by context; there is no need to limit the choice of alternative shadings to a pre-speeified set of alternativeseach represented by a differnt single unit.
Similarly, filling in missing argumenls i not a mailer of choosing a partic-ular concept, but of fiifirlg in a pattern that specifies what is known about the filler, without necessarily specifying aparticular specific concept.In previous work, Alan Kawamoto and I (McClelland and Kawamoto, 1986) implemented a parallel-distributed processing (PDP) model that can use semantic onstraints to do the fi~ur things listed at the beginning ofthe article, though it was limited to processing only one clause at a lime.
While it would lye possible to use such amechanism clause-by-clause, mantic onstrainLs are often required to decide which of several clauses a phrasebelongs to.
For example, in the sentence:1) John ate the cake that his mother baked at the picnic.we attach "at the picnic" to the main clause (as the place where the cake was ealen), whereas in2) John ate the cake that his mother baked in the over).we attach "in the over)" to the subordir)ate clause (as the place where Ihe cake was baked).
Clearly these attach-ments depend on knowing Ihat baking can take place in ovens, not at picnics, arid ealiug can lake place at picnics,not in ovens; !
would also claim that the relative merits of both atlachments must be laken into account o get theatlaehmenls right.
It seems, lhen, that a mechanism is needed lhat can consider Ihe possihilily of altaching a phraseto more than one possihle clause.This article sketches out a model that aims to achieve mtdli-clause capabilily.
The model has not yet beenfidly implemented, so the paper is quite speculative.
1lowever, I think the model promises to lake us some disl~ancetoward a belter underslanding of the interaction of syntactic and case-role analysis.
In particular, it suggesLs thatwith the right cnr)neclionist archilecture, the four uses of semantic onstraints enumerated above become intrinsiccharacteristics of the language processing machinery.I would like to thank Geoff tlinton, George l,akoff, Brian MacWhinney and Mark St. John for discussions of the topic of this paper and/or forspecific comments on the first draft.
Supported hy ONR contract N00014-82-C-0374, NR 667.483.75Representing structure and content.
To begin, let us consider how to represent the structure of a sentence ina PDP mechanism.
To do this, we make use of the notion that a structural description can be repesented as a set oftriples.
For example the correct role structure of Sentence 2 can be represented with a set of triples such as the fol-lowing:(P1 AGENT BOY) (Pl ACFION ATE) (P1 PATIENT CAKE)(P2 AGENT MOTIIER) (P2 ACFION BAKED) (P2 PATIENT CAKE)(P2 I,OCATION OVEN)An individual triple can be represented in distributed form by dedicating a set of units to each of its parts; thus wecan have one set of units for the head of the triple, one for the relation, and one for the tail or slot-filler.
Each ofthe three parts of a triple can then be represented in distributed form as a pallern of activation over the units.
Theidea of using this kind of three-part distributed representation was inlroduced by l linton (1981) to represent the con-tents of semantic nell; the extension to arbitrary tree structures is due to "Fouretzky and l linton (1985) andTouretzky (1986).For the fillers, or the tail of a triple, the units stand for useful characterizers that serve to distinguish one fillerfrom another, l l inton (1981) used the term "microfeatures" for these units; these features need not correspond inany simple way to verbalizable primitives.
Different slot fillers produce different patterns on these units; and thedifferent possible instanliatlons of a filler are likewise captured by differences in the pattern of activation on theunits.For the relations, fire units stand for characteristics of the relation itself Note that this differs from most otherapproaches in treating each role or relation as a distributed pattern.
This has several virtues.
For one thing, itimmediately eliminates the problem of specifying a small set of case roles, in the face of the fact that there seem tohe a very large number of very subtle differences between roles that are in many ways very similar.
Further, the useof distributed representations allows us to capture both the similarities and differences among case roles.
The ideahas been proposed on independent linguistic grounds, as well.For the head of each triple, the units stand for characteristics of the whole in which the filler plays a part.Thus the pattern that represents P1 is not some arbitrary pointer as it might be in a Lisp-based rcpresentalion, but israther a Reduced Description of the constituent that it stands for (I linton, McClelland, and Rumelhart, 1986; Lakoff,personal communication).
In particular, the pattern representing P1 would capture characteristics of the act of eat-ing and of the participants in the act.
There would he less detail, of course, than in the separate representations ofthese constituents where they occur as separate fillers of the tail slot.,~yntaclic and case-role representations.
Sentences have both art augmented surface structure representationand a case-role representation, in the present model, then, there are two sets of units, one that represents he syn-tactic structure triples, and one that represents the case-structure triples.
I have already described the general formof the case-role triples; the syntactic Iriples would have a similar form, though they would capture primarily syntacticrelations among the constituents.
So, for example, the set of syntactic triples of Senlence 2 would be something like:(S1 SUBJ BOY) (S1 VERB SAW) (S1 DOBJ (SAKE)(CAKE MODIFIER $2)($2 SUBJ MOTIIER) ($2 VERB BAKED) ($2 I)OBJ T=CAKE)(SI LOC-PP OVEN)There are, correspondingly, two main parts to the model, a syntactic processor and a case-fi'ame processor (See Fig-ure !).
In this respect, the model is similar to marly conventional parsing schemes (e.g., Marcus, 1980; Kaplan andBresnan, 19821.
The microstructure is quite different, however.
One of the key I.hings ttlat a PDP microstructurebuys us is the ability to improve the interaction between these two main components.Syntactic processing.
The role of the syntactic processor is to take in words as they are encountered in read-ing or listening and to produce at its outputs a sequence of patterns, with each pattern capturing one syntactic struc-ture triple.
~ In Figure l the syntactic processor is shown in the midst of processing Sentence 2.
It has reached the1.
Note that this means that ~everai words can be packed into the same constituent, and that as the words of a constituent (e.g., "the old grey don-key") are encountered the microfeatures of the constituent wil| he gradually specified.
Thus the representation of  the constituent can gradually buildup at the output of the syntactic processor.76~ase - ~o/e Prac.es_~o~F/4"/5y.i:o .
'fi  Zv-ipkI I0 M,.0,j{., xe_ T i le_ IJ.,'/'xIqiAabl ttFigure I.
A diagram of the model.
See text for explanation.point where it is processing the words "the cake".
The output of at this point should tend to aclivate the patterncorresponding to (S1 I)OBJ CAKE)  over a sef.
of units (the syntactic triple "units) whose role is to display the patternof activation corresponding to the current syntactic triple.
Note that these units also receive feedback from thecase-frame processor; the role of this feedback is to fill in unspecified parts of  Ihe syntactic triple, as shall be dis-cussed below.
The syntactic triple unil.s have connections to units (Ihe case-frame triple units) which serve torepresent the current case-frame triple.The connections between these two sets of units are assumed to be learned through prior pairings of synlactic triplesand case-frame triples, so that they capture the mutual constraints on case and synlactic role assignmenls.
Theinner workings of the syntaclic processor have yet to be fidly worked out, so for now I leave it as a black box.The case-frame processor.
Tire role of the case-frame processor is to produce art aclive representation of thecurrent case-frame cortstituent, based on the pattern represenling Ihe current synlaclic consliluent on the syntactictriple units and on feedback from a set of units called the working memory.
Tlte working memory is Ihe slruclure iHwhich the developing case-frame represenlalion of Ihe sentence is held.
As conslilucnts are parsed, Ihey are loadedinto the working memory, by way of a network called an I/O ncl.
2 Within Ihe working memory, individual unitscorrespond to combinations of units in the current case-role represenlation.
Tbtts, Ihe represenlalion at Ihis level isconjunctive, artd is therefore capable of maintaining information about which combinations of case-role units wereactivated togelher in the same case-role triple when the patterns aclivated by several triples are snperimposed in theworking memory (see l l inton et al1986, for discussion).
Of  course, early in a parse, Ihe loaded constituents willnecessarily be incomplete.Pattern completion.
The working memory provides a persisting representation of the cottstituents alreadyparsed.
This representation persists as a pattern of activation, so that it can bolh constrain and be constrained bynew constituents as they are encountered, through interactions with a final set of units, called tile hidden ease-roleunits.
These units are called "hidden" because their state is not visible to any olher part of the system; instead they2.
The l/O net is equivalent to TouretT.ky and llinton's (1985) "pull-out net".
Its job is to ensure that the characteristics of only a one of the consti-tuents tored in the working memory are interacting with the case-rrame triple unit':,.
See Touretzky and tlinton (19~5) for details.77serve to mediate constraining relations among the units in the working memory.
The process works as follows.Connections from working memory units to hidden units allow the pattern of activation over the working memory toproduce a pattern over the hidden units.
Connections from the hidden units to the working memory units allowthese patterns, in turn, to feed activation back to the working memory.
This feedback allows the network to com-plete and dean-up distorted and incomplete patterns (that is, representations of sentences).
The connections in thenetwork are acquired through training on on a sample of sentences ( ee St. John, 1986, for details).
The connectionstrengths derived from this training experience allow it to sustain and complete the representations of familiar sen-tences; this capability generalizes to novel sentences with similar structure.What this model can do.
The model I have described should be able to do all of the kinds of things listed atthe beginning of the paper.
Consider, for example, the problem of interpreting the sentence "The boy hit the ballwith the bat."
This requires both assigning the appropriate reading (baseball bat) and the appropriate role (instru-ment) to the bat.
The syntactic triple for this constituent (S1 with-PP BAT), would tend to activate a pattern overthe coresponding to a blend of baseball bat and flying bat as the tail of the triple, and a blend of the possible case-roles consistent with "with" as the the pattern representing the relalion portion of the triple.
These in turn wouldtend to activate units representing the various possible filler-role combinations consistent with this syntactic onsti-tuent.
But since the other constituents of the sentence w,~uld already have been stored in the working memory, thecompletion process would tend to support units standing for the baseball-bat s instrument interpretation more thanothers.
Thus, simultaneous role assignment and context sensitive selection of the appropriate reading of an ambigu-ous word would be expected to fall out naturally from the operation of the completion process.Filling in default values for missing arguments and shading or shaping the representations of vaguely describedconstituents i  also a simple by-product of the pattern completion process.
Thus, fi)r example, on encountering"The man stirred the coffee", the completion process will tend to fill in the paltern for the completion that includes aspoon as instrument.
Note that the pattern so filled in need not specify a particular specific concept; thus for a sen-tence like "The boy wrote his name", we would expect a pattern representing a writing inslrument, but not specify-ing if it is a pen or a pencil, to be filled in; unless, of course, the network had had specific experience indicating thatboys always write their names with one particular instrument or another.
A similar process occurs on encounteringthe container in a sentence like "The container held the cola".
In such eases the constraints impo~d by other con-stituents (the cola) would be expected to si~ape the representation f "container", toward a smallish, hand-holdable,non-porous container; Again, this process would not necessarily specify a specific container, just the properties ucha container could be predicted to have.l have not yet said anything about what the model would do with the altachment problem posed by the sen-tence "The boy ate the cake that his mother baked in the oven."
In this case, we would expect that the syntacticprocessor would pass along a constituent like (S?
in-PP OVEN), and that it would be the job of the case-role proces-sor to determine its correct attachment.
Supposing that the experience the network has been exposed to includesmothers (and others) baking cakes (and other things) in ovens, we would expect hat the case-role triple (P2 LOCOVEN) (where P2 stands for the reduced description of "mother-baked-cake") would already be partially active asthe syntactic onstituent became available.
Thus the incoming constituent wm,ld simply reinforce a pattern ofactivation thai.
already reflected the correct attachment of oven.Current staltt~ of  the model.
As I previously staled, the model has not yet been hnplcmented, and so onecan treat the previous ection as describing the performance of a machine made out of hopeware.
Nevertheless Ihave reason to believe it will work.
CMU connectlonists now have considerable experience with representations ofthe kind used in the cnse-fi'ame processor (l'ouretzky & ll inlon, 1985; Tourelzky, 1986; l)erlhick, 1986).
Amechanism quite like the case-frame processor has been implemented by St.. Iohn (1986), and it demonstratesseveral of the uses of semantic onslraints that I have been discussing.Obviously, though, even if the case-frame processor is successfid l here are many more tasks that lie ahead.One crucial one is the development of a cormectionist implemenlation f the synlactic processor.
I helievc that weare now on the verge of understanding sequential processes in connectionist networks (see Jordan, 1986), and thatthis will soon make it possible to describe a complete connectionist mechanism for language processing that capturesboth the strengths and limitations of human language processing capabilities.78ReferencesBates, E., & MaeWhinney, B.
(1987).
Competition, variation and language learning: What is not universal inlanguage acquisition.
In B. MacWhinney (Ed.
), Mechanisms of language acquisition, ttillsdale, N J: Erlbat, m.Cottrell, G. (1985).
A connectionist approach to word sense disambiguation (q'R-154).
Rochester, NY: Universityof Rochester, Department of Computer Science.Derthick, M. (1986).
A connectionist knowledge representation system.
Thesis proposal, Carnegie-Mellon Univer-sity, Department of Computer Science, PitLsburgh, PA.llinton, G. E. (1981).
Implementing semantic networks in parallel hardware.
In G. E. llinton & J.
A.
Anderson(Eds.
), Parallel models of associative memory (pp.
161-188).
llillsdale, N J: Erlbaum.llinton, G. E., McClelland, J.
!.., & Rumelhart, I).
E. (1986).
Distriboted Reprcsentations.
In D. E. Rumelhart,J.
I.. McClelland, & the Pl)P research group (Eds.
), Parallel distributed processing: Explorations in the micros-trueture of cognition.
Volume I. Cambridge, MA: Bradford Books.Jordan, M. I.
(1986).
Serial order: A parallel distributed processing approach (ICS Rep. No.
8604).
University ofCalifornia, San Diego, Institute for Cognitive Science.Kaplan, R., & Bresnan, J.
(1982).
I.exical fimctional grammar: A formal system for grammatical representation.In I. Bresnan (Ed.
), The mental representation ofgrammatical relations.
Cambridge, MA: Mrl" Press.Kawamoto, A. II.
(1985).
Dynamic processes in the (re)solution of lexieal ambiguity.
Unpublished doctoraldissertation, Brown University.MacWhinney,-B.
J.
(1987).
The competition model.
In B. MacWhinney (Ed.
), Mechanisms or language acquisi-tion.
llillsdale, NJ: Erlbaum.Marcus, M. P. (1980).
A theory of.wntactic recognition for natural anguage.
Camhridge, MA: MIT Press.McClelland, J. L. (in press.)
How we use what we know in reading: An interactive activalion approach.
In M.Coltheart (Ed.
), Attention and performance XII: The psychology of reading.
I.ondon: Erlbaum.McClelland, .I.L., & Kawamoto, A. H. (1986).
Mechanisms of sentence processing: Assigning roles to consti-tuents.
In J. L. MeClelland, D. E. Rumelhart, & the PDP research group (Eds.
), Parallel distributed processing:Explorations in the microstrueture of cognition.
Volume 1I.
Cambridge, MA: Bradford Books.St.
John, M. F. (1986).
Reconstructive memory for sentences.
Working paper, Department of Psychology,Carnegie-Mellon University, Pittsburgh, PA.Tourelzky, D. S. (1986).
BoltzCONS: Reconciling conneclionism wilh lhe recnrsive nalnrc of stacks and trees.Proceedings or the Eighth Annual ConFerence of the Cognitive Science Society, Amhersl, MA, 522-530.Touretzky, I)., & l l inlon, G. E. (1985).
Symbols among the neurons: l)elails of'a counectionisl inference architec-Lure.
Proceedings of the Ninth International Johzt Conference on Art~fi?
'ial Intelligence.IWaltz, D. 1.., & Pollack, J.
B.
(1985).
Massively parallel parsing.
Cognitive Science, 9, 51-74.79
