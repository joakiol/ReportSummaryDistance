Exploiting Strong Syntactic Heuristics and Co-Training to Learn SemanticLexiconsWilliam Phillips and Ellen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112 USAfphillips,riloffg@cs.utah.eduAbstractWe present a bootstrapping method thatuses strong syntactic heuristics to learnsemantic lexicons.
The three sourcesof information are appositives, compoundnouns, and ISA clauses.
We apply heuris-tics to these syntactic structures, embedthem in a bootstrapping architecture, andcombine them with co-training.
Resultson WSJ articles and a pharmaceutical cor-pus show that this method obtains highprecision and finds a large number ofterms.1 IntroductionSyntactic structure helps us understand the seman-tic relationships between words.
Given a text cor-pus, we can use knowledge about syntactic struc-tures to obtain semantic knowledge.
For example,Hearst (Hearst, 1992) learned hyponymy relation-ships by collecting words in lexico-syntactic expres-sions, such as ?NP, NP, and other NPs?, and Roarkand Charniak (Roark and Charniak, 1998) gener-ated semantically related words by applying statisti-cal measures to syntactic contexts involving apposi-tives, lists, and conjunctions.Exploiting syntactic structures to learn semanticknowledge holds great promise, but can run intoproblems.
First, lexico-syntactic expressions thatexplicitly indicate semantic relationships (e.g., ?NP,NP, and other NPs?)
are reliable but a lot of semanticinformation occurs outside these expressions.
Sec-ond, general syntactic structures (e.g., lists and con-junctions) capture a wide range of semantic rela-tionships.
For example, conjunctions frequentlyjoin items of the same semantic class (e.g., ?catsand dogs?
), but they can also join different seman-tic classes (e.g., ?fire and ice?).
Some researchers(Roark and Charniak, 1998; Riloff and Shepherd,1997) have applied statistical methods to identifythe strongest semantic associations.
This approachhas produced reasonable results, but the accuracy ofthese techniques still leaves much room for improve-ment.We adopt an intermediate approach that learnssemantic lexicons using strong syntactic heuristics,which are both common and reliable.
We haveidentified certain types of appositives, compoundnouns, and identity (ISA) clauses that indicate spe-cific semantic associations between words.
We em-bed syntactic heuristics in a bootstrapping processand present empirical results demonstrating that thisbootstrapping process produces high-quality seman-tic lexicons.
In another set of experiments, we in-corporate a co-training (Blum and Mitchell, 1998)mechanism to combine the hypotheses generated bydifferent types of syntactic structures.
Co-trainingproduces a synergistic effect across different heuris-tics, substantially increasing the coverage of the lex-icons while maintaining nearly the same level of ac-curacy.2 Semantic Lexicon LearningThe goal of our research is to automatically gener-ate a semantic lexicon.
For our purposes, we de-fine a semantic lexicon to be a list of words withsemantic category labels.
For example, the word?bird?
might be labeled as an ANIMAL and the word?car?
might be labeled as a VEHICLE.
Semanticlexicons have proven to be useful for many lan-Association for Computational Linguistics.Language Processing (EMNLP), Philadelphia, July 2002, pp.
125-132.Proceedings of the Conference on Empirical Methods in Naturalguage processing tasks, including anaphora resolu-tion (Aone and Bennett, 1996; McCarthy and Lehn-ert, 1995), prepositional phrase attachment (Brilland Resnik, 1994), information extraction (Soder-land et al, 1995; Riloff and Schmelzenbach, 1998),and question answering (Harabagiu et al, 2000;Hirschman et al, 1999).Some general-purposes semantic dictionaries al-ready exist, such as WordNet (Miller, 1990).
Word-Net has been used for many applications, but it maynot contain the vocabulary and jargon needed forspecialized domains.
For example, WordNet doesnot contain much of the vocabulary found in medicaltexts.
In previous research on semantic lexicon in-duction, Roark and Charniak (Roark and Charniak,1998) showed that 3 of every 5 words learned bytheir system were not present in WordNet.
Further-more, they used relatively unspecialized text cor-pora: Wall Street Journal articles and terrorism newsstories.
Our goal is to develop techniques for seman-tic lexicon induction that could be used to enhanceexisting resources such as WordNet, or to create dic-tionaries for specialized domains.Several techniques have been developed to gen-erate semantic knowledge using weakly supervisedlearning techniques.
Hearst (Hearst, 1992) ex-tracted information from lexico-syntactic expres-sions that explicitly indicate hyponymic relation-ships.
Hearst?s work is similar in spirit to ourwork in that her system identified reliable syntac-tic structures that explicitly reveal semantic associa-tions.
Meta-bootstrapping (Riloff and Jones, 1999)is a semantic lexicon learning technique very differ-ent from ours which utilizes information extractionpatterns to identify semantically related contexts.Named entity recognizers (e.g., (Bikel et al, 1997;Collins and Singer, 1999; Cucerzan and Yarowsky,1999)) can be trained to recognize proper namesassociated with semantic categories such as PER-SON or ORGANIZATION, but they typically are notaimed at learning common nouns such as ?surgeon?or ?drugmaker?.Several researchers have used some of the samesyntactic structures that we exploit in our research,namely appositives and compound nouns.
For ex-ample, Riloff and Shepherd (Riloff and Shepherd,1997) developed a statistical co-occurrence modelfor semantic lexicon induction that was designedwith these structures in mind.
Roark and Char-niak (Roark and Charniak, 1998) followed up onthis work by using a parser to explicitly capturethese structures.
Caraballo (Caraballo, 1999) alsoexploited these syntactic structures and applied a co-sine vector model to produce semantic groupings.
Inour view, these previous systems used ?weak?
syn-tactic models because the syntactic structures some-times identified desirable semantic associations andsometimes did not.
To compensate, statistical mod-els were used to separate the meaningful semanticassociations from the spurious ones.
In contrast, ourwork aims to identify ?strong?
syntactic heuristicsthat can isolate instances of general structures thatreliably identify the desired semantic relations.3 A Bootstrapping Model that ExploitsStrong Syntactic HeuristicsFor the purposes of this research, we will define twodistinct types of lexicons.
One lexicon will con-sist of proper noun phrases, such as ?Federal Avi-ation Administration?.
We will call this the PNP(proper noun phrase) lexicon.
The second lexiconwill consist of common (non-proper) nouns, such as?airplane?.
We will call this the GN (general noun)lexicon.
The reason for creating these distinct lexi-cons is that our algorithm takes advantage of syntac-tic relationships between proper nouns and generalnouns.3.1 Syntactic HeuristicsOur goal is to build a semantic lexicon of words thatbelong to the same semantic class.
More specifi-cally, we aim to find words that have the same hyper-nym, for example ?dog?
and ?frog?
would both havethe hypernym ANIMAL.1 We will refer to words thathave the same immediate hypernym as semantic sib-lings.We hypothesize that some syntactic structures canbe used to reliably identify semantic siblings.
Wehave identified three candidates: appositives, com-pound nouns, and identity clauses whose main verbis a form of ?to be?
(we will call these ISA clauses).1The appropriate granularity of a set of semantic classes,or the organization of a semantic hierarchy, is always open todebate.
We chose categories that seem to represent importantand relatively general semantic distinctions.While these structures often do capture semantic sib-lings, they frequently capture other types of seman-tic relationships as well.
Therefore we use heuristicsto isolate subsets of these syntactic structures thatconsistently contain semantic siblings.
Our heuris-tics are based on the observation that many of thesestructures contain both a proper noun phrase and ageneral noun phrase which are co-referent and usu-ally belong to the same semantic class.
In the fol-lowing sections, we explain the heuristics that weuse for each syntactic structure, and how those struc-tures are used to learn new lexicon entries.3.1.1 AppositivesAppositives are commonly occurring syntacticstructures that contain pairs of semantically relatednoun phrases.
A simple appositive structure con-sists of a noun phrase (NP), followed by a comma,followed by another NP, where the two NPs are co-referent.
However, appositives often signify hyper-nym relationships (e.g., ?the dog, a carnivorous ani-mal?
).To identify semantic siblings, we only use appos-itives that contain one proper noun phrase and onegeneral noun phrase.
For example, ?George Bush,the president?
or ?the president, George Bush?.
The-oretically, such appositives could also indicate a hy-pernym relationship (e.g., ?George Bush, a mam-mal?
), but we have found that this rarely happensin practice.3.1.2 Compound NounsCompound nouns are extremely common but theycan represent a staggering variety of semantic rela-tionships.
We have found one type of compoundnoun that can be reliably used to harvest seman-tic siblings.
We loosely define these compoundsas ?GN+ PNP?
noun phrases, where the compoundnoun ends with a proper name but is modified withone or more general nouns.
Examples of such com-pounds are ?violinist James Braum?
or ?softwaremaker Microsoft?.
One of the difficulties with rec-ognizing these constructs, however, is resolving theambiguity between adjectives and nouns among themodifiers (e.g., ?violinist?
is a noun).
We only useconstructs in which the GN modifier is unambigu-ously a noun.3.1.3 ISA ClausesCertain ?to be?
clauses can also be harvested toextract semantic siblings.
We define an ISA clauseas an NP followed by a VP that is a form of ?to be?,followed by another NP.
These identity clauses alsoexhibit a wide range of semantic relationships, butharvesting clauses which contain one proper NP andone general NP can reliably identify noun phrasesof the same semantic class.
We found that this struc-ture yields semantic siblings when the subject NP isconstrained to be a proper NP and the object NP isconstrained to be a general NP (e.g., ?Jing Lee is thepresident of the company?
).3.2 The Bootstrapping ModelFigure 1 illustrates the bootstrapping model for eachof the three syntactic structures.
Initially, the lex-icons contain only a few manually defined seedwords: some proper noun phrases and some generalnouns.
The syntactic heuristics are then applied tothe text corpus to collect potentially ?harvestable?structures.
Each heuristic identifies structures withone proper NP and one general NP, where one ofthem is already present in the lexicon as a memberof a desired semantic class.
The other NP is then as-sumed to belong to the same semantic class and isadded to a prospective word list.
Finally, statisticalfiltering is used to divide the prospective word listsinto exclusive and non-exclusive subsets.
We willdescribe the motivation for this in Section 3.2.2.
Theexclusive words are added to the lexicon, and thebootstrapping process repeats.
In the remainder ofthis section, we explain how the bootstrapping pro-cess works in more detail.3.2.1 Bootstrapping ProcedureThe input to our system is a small set of seedwords for the semantic categories of interest.
Toidentify good seed words, we sorted all nouns inthe corpus by frequency and manually identified themost frequent nouns that belong to each targeted se-mantic category.Each bootstrapping iteration alternates betweenusing either the PNP lexicon or the GN lexicon togrow the lexicons.
As a motivating example, as-sume that (1) appositives are the targeted syntacticstructure (2) bootstrapping begins by using the PNPlexicon, and (3) PEOPLE is the semantic category ofProper NPLexiconSyntactic Heuristics Text CorpusProspectiveProper NPsSeed WordsExclusiveNon?ExclusiveNon?ExclusiveExclusiveLexiconGeneral NounProspectiveGeneral NounsFigure 1: Bootstrapping Modelinterest.
The system will then collect all appositivesthat contain a proper noun phrase known to be a per-son.
So if ?Mary Smith?
belongs to the PNP lexiconand the appositive ?Mary Smith, the analyst?
is en-countered, the head noun ?analyst?
will be learnedas a person.The next bootstrapping iteration uses the GN lex-icon, so the system will collect all appositives thatcontain a general noun phrase known to be a person.If the appositive ?John Seng, the financial analyst?is encountered, then ?John Seng?
will be learned asa person because the word ?analyst?
is known tobe a person from the previous iteration.
The boot-strapping process will continue, alternately using thePNP lexicon and the GN lexicon, until no new wordscan be learned.We treat proper noun phrases and general nounphrases differently during learning.
When a propernoun phrase is learned, the full noun phrase is addedto the lexicon.
But when a general noun phrase islearned, only the head noun is added to the lexi-con.
This approach gives us generality because headnouns are usually (though not always) sufficient toassociate a common noun phrase with a semanticclass.
Proper names, however, often do not exhibitthis generality (e.g., ?Saint Louis?
is a location but?Louis?
is not).However, using full proper noun phrases can limitthe ability of the bootstrapping process to acquirenew terms because exact matches are relatively rare.To compensate, head nouns and modifying nouns ofproper NPs are used as predictor terms to recognizenew proper NPs that belong to the same semanticclass.
We identify reliable predictor terms using theevidence and exclusivity measures that we will de-fine in the next section.
For example, the word ?Mr.
?is learned as a good predictor term for the person cat-egory.
These predictor terms are only used to clas-sify noun phrases during bootstrapping and are notthemselves added to the lexicon.3.2.2 Exclusivity FilteringOur syntactic heuristics were designed to reliablyidentify words belonging to the same semantic class,but some erroneous terms still slip through for vari-ous reasons, such as parser errors and idiomatic ex-pressions.
Perhaps the biggest problem comes fromambiguous terms that can belong to several seman-tic classes.
For instance, in the financial domain?leader?
can refer to both people and corporations.If ?leader?
is added to the person lexicon, then itwill pull corporation terms into the lexicon duringsubsequent bootstrapping iterations and the personlexicon will be compromised.To address this problem, we classify all candidatewords as being exclusive to the semantic category ornon-exclusive.
For example, the word ?president?nearly always refers to a person so it is exclusive tothe person category, but the word ?leader?
is non-exclusive.
Only the exclusive terms are added tothe semantic lexicon during bootstrapping to keepthe lexicon as pure (unambiguous) as possible.
Thenon-exclusive terms can be added to the final lexiconwhen bootstrapping is finished if polysemous termsare acceptable to have in the dictionary.Exclusivity filtering is the only step that usesstatistics.
Two measures determine whether a wordis exclusive to a semantic category.
First, we use anevidence measure:Evidence(w; c) =Sw;cSwwhere Swis the number of times word w was foundin the syntactic structure, and Sw;cis the number oftimes word w was found in the syntactic structurecollocated with a member of category c. The evi-dence measure is the maximum likelihood estimatethat a word belongs to a semantic category given thatit appears in the targeted syntactic structure (a wordis assumed to belong to the category if it is collo-cated with another category member).
Since fewwords are known category members initially, we usea low threshold value (.25) which simply ensuresthat a non-trivial proportion of instances are collo-cated with category members.The second measure that we use, exclusivity, isthe number of occurrences found in the given cate-gory?s prospective list divided by the number of oc-currences found in all other categories?
prospectivelists.Exclusivity(w; c) =Sw;cSw;:cwhere Sw;cis the number of times word w was foundin the syntactic structure collocated with a memberof category c, and Sw;:cis the number of times wordw was found in the syntactic structure collocatedwith a member of a different semantic class.
Weapply a threshold to this ratio to ensure that the termis exclusive to the targeted semantic category.3.3 Experimental ResultsWe evaluated our system on several semantic cate-gories in two domains.
In one set of experiments,we generated lexicons for PEOPLE and ORGANIZA-TIONS using 2500 Wall Street Journal articles fromthe Penn Treebank (Marcus et al, 1993).
In the sec-ond set of experiments, we generated lexicons forPEOPLE, ORGANIZATIONS, and PRODUCTS usingapproximately 1350 press releases from pharmaceu-tical companies.2Our seeding consisted of 5 proper nouns and 5general nouns for each semantic category.
We used athreshold of 25% for the evidence measure and 5 forthe exclusivity ratio.
We ran the bootstrapping pro-cess until no new words were learned, which rangedfrom 6-14 iterations depending on the category andsyntactic structure.Table 1 shows 10 examples of words learned foreach semantic category in each domain.
The peopleand organization lists illustrate (1) how dramaticallythe vocabulary can differ across domains, and (2)that the lexicons may include domain-specific wordmeanings that are not the most common meaning2We found these texts using Yahoo?s financial industry pagesat http://biz.yahoo.com/news/medical.html.People (WSJ): adman, co-chairman, head,economist, shareholder, AMR Chairman RobertCrandall, Assistant Secretary David Mullins,Deng Xiaoping, Abby Joseph Cohen, C. EverettKoopOrganization (WSJ): parent, subsidiary, dis-tiller, arm, suitor, AMR Corp., ABB ASEABrown Boveri, J.P. Morgan, James River, FederalReserve BoardPeople (Pharm): surgeon, executive, recipient,co-author, pioneer, Amgen Chief Executive Of-ficer, Barbara Ryan, Chief Scientific Officer Nor-bert Riedel, Dr. Cole, Analyst Mark AugustineOrganization (Pharm): device-maker, drug-maker, licensee, organization, venture, ALRTechnologies, Aventis Pharmaceuticals, BayerAG, FDA Advisory Panel, Hadassah UniversityHospitalProduct (Pharm): compound, stent, platform,blocker, antibiotic, Bexxar, Viratrol, MBX-102,Apothesys Decision Support System, AERx PainManagement SystemTable 1: Examples of Learned Wordsof a word in general.
For example, the word ?par-ent?
generally refers to a person, but in a financialdomain it nearly always refers to an organization.The pharmaceutical product category contains manynouns (e.g., drug names) that may not be in a generalpurpose lexicon such as WordNet.Tables 2 and 3 show the results of our evaluation.We ran the bootstrapping algorithm on each type ofsyntactic structure independently.
The Total columnshows the total number of lexicon entries generatedby each syntactic structure.
The Correct columncontains two accuracy numbers: X/Y.
The first value(X) is the percentage of entries that were judged tobe correct, and the second value (Y) is the accuracyafter removing entries resulting from parser errors.3The PNP lexicons were substantially larger thanthe GN lexicons, in part because we saved full noun3For example, our parser frequently mistags adjectives asnouns, so many adjectives were hypothesized to be people.
Ifthe parser had tagged them correctly, they would not have beenallowed in the lexicon.Appositives Compounds ISA UnionCategory Total Correct Total Correct Total Correct Total CorrectPeople (WSJ) 1826 .97/.97 2026 .99/.99 113 .94/.94 3543 1.0/1.0Orgs (WSJ) 674 .87/.94 3770 .77/.78 54 .93/.96 4191 .79/.79People (Pharm) 280 .86/.87 1723 .87/.88 39 1.0/1.0 1872 .85/.91Orgs (Pharm) 205 .85/.88 1128 .85/.92 248 .85/.91 1399 .78/.84Products (Pharm) 64 .94/.95 223 .77/.79 64 .84/.84 330 .83/.85Table 2: Proper Noun Phrase Lexicon ResultsAppositives Compounds ISA UnionCategory Total Correct Total Correct Total Correct Total CorrectPeople (WSJ) 159 .91/.95 60 .30/.56 41 .85/.97 229 .73/.88Orgs (WSJ) 84 .69/.75 54 .26/.47 6 1.0/1.0 134 .52/.66People (Pharm) 34 .91/.91 32 .66/.75 18 1.0/1.0 66 .79/.84Orgs (Pharm) 36 .58/.60 29 .35/.46 41 .51/.66 95 .45/.54Products (Pharm) 8 .75/1.0 11 .09/.33 13 .54/1.0 32 .50/.89Table 3: General Noun Lexicon Resultsphrases in the PNP lexicon but only head nouns inthe GN lexicon.
Probably the main reason, however,is that there are many more proper names associatedwith most semantic categories than there are generalnouns.
Consequently, we evaluated the PNP and GNlexicons differently.
For the GN lexicons, a volun-teer (not one of the authors) labeled every word ascorrect or incorrect.
Due to the large size of the PNPlexicons, we randomly sampled 100 words for eachsyntactic structure and semantic category and askedvolunteers to label these samples.
Consequently, thePNP evaluation numbers are estimates of the true ac-curacy.The Union column tabulates the results obtainedfrom unioning the lexicons produced by the threesyntactic structures independently.4 Although thereis some overlap in their lexicons, we found thatmany different words are being learned.
This indi-cates that the three syntactic structures are tappinginto different parts of the search space, which sug-gests that combining them in a co-training modelcould be beneficial.4Since the number of words contributed by each syntac-tic structure varied greatly, we evaluated the Union results forthe PNP lexicon by randomly sampling 100 words from theunioned lexicons regardless of which structure generated them.This maintained the same distribution in our evaluation set asexists in the lexicon as a whole.
However, this sampling strat-egy means that the evaluation results in the Union column arenot simply the sum of the results in the preceding columns.Seed WordsSyntactic StructuresLexicons for All 3AppositiveBootstrappingProcessCompound NounBootstrappingProcessISA ClauseBootstrappingProcessFigure 2: Co-Training Model3.4 Co-TrainingCo-training (Blum and Mitchell, 1998) is a learn-ing technique which combines classifiers that sup-port different views of the data in a single learningmechanism.
The co-training model allows exampleslearned by one classifier to be used by the other clas-sifiers, producing a synergistic effect.
The three syn-tactic structures that we have discussed provide threedifferent ways to harvest semantically related nounphrases.Figure 2 shows our co-training model, with eachsyntactic structure serving as an independent classi-fier.
The words hypothesized by each classifier areput into a single PNP lexicon and a single GN lex-icon, which are shared by all three classifiers.
Weused an aggressive form of co-training, where allterms hypothesized by a syntactic structure with fre-quency   are added to the shared lexicon.
Thethreshold ensures some confidence in a term beforeit is allowed to be used by the other learners.
Weused a threshold of =3 for the WSJ corpus and=2 for the pharmaceutical corpus since it is sub-stantially smaller.
We ran the bootstrapping processuntil no new words were learned, which was 12 it-erations for the WSJ corpus and 10 iterations for thepharmaceutical corpus.5PNP PNP GN GNCategory cotrn w/o cotrn w/oPeople (WSJ) 5414 3543 347 229Orgs (WSJ) 4227 4191 213 134People (Pharm) 2217 1872 84 66Orgs (Pharm) 4068 1399 196 95Products (Pharm) 309 330 38 32Table 4: Lexicon sizes with and w/o co-trainingTable 4 shows the size of the learned lexicons withco-training and without co-training (i.e., running theclassifiers separately).
In almost all cases, many ad-ditional words were learned using the co-trainingmodel.
Tables 5 and 6 show the evaluation resultsfor the lexicons produced by co-training.
The co-training model produced substantially better cover-age, while achieving nearly the same accuracy.
Oneexception was organizations in the pharmaceuticaldomain, which suffered a sizeable loss in precision.This is most likely due to the co-training loop be-ing too aggressive.
If one classifier produces a lotof mistakes (in this case, the compound noun classi-fier), then those mistakes can drag down the overallaccuracy of the lexicon.4 ConclusionsWe have presented a method for learning seman-tic lexicons that uses strong syntactic heuristics ina bootstrapping algorithm.
We exploited three typesof syntactic structures (appositives, compound NPs,5After co-training finished, we also added terms to the lexi-con that were hypothesized by an individual classifier with fre-quency <  if they had not previously been labeled.and ISA clauses) in combination with heuristics toidentify instances of these structures that containboth a proper and general noun phrase.
Each syntac-tic structure generated many lexicon entries, in mostcases with high accuracy.
We also combined thethree classifiers using co-training.
The co-trainingmodel increased the number of learned lexicon en-tries, while maintaining nearly the same level of ac-curacy.
One limitation of this work is that it can onlylearn semantic categories that are commonly foundas proper nouns and general nouns.This research illustrates that common syntacticstructures can be combined with heuristics to iden-tify specific semantic relationships.
So far we haveexperimented with three structures and one type ofheuristic (proper NP/general NP collocations), butwe believe that this approach holds promise for othersemantic learning tasks as well.
In future work, wehope to investigate other types of syntactic struc-tures that may be used to identify semantically re-lated terms, and other types of heuristics that canreveal specific semantic relationships.5 AcknowledgementsThis research was supported by the National ScienceFoundation under award IRI-9704240.
Thanks toErin Davies, Brijesh Garabadu, Dominic Jones, andHenry Longmore for labeling data.ReferencesC.
Aone and S. W. Bennett.
1996.
Applying machine learn-ing to anaphora resolution.
In Stefan Wermter, Ellen Riloff,and Gabriele Scheler, editors, Connectionist, Statistical, andSymbolic Approaches to Learning for Natural LanguageProcessing, pages 302?314.
Springer-Verlag, Berlin.Daniel M. Bikel, Scott Miller, Richard Schwartz, and RalphWeischedel.
1997.
Nymble: a high-performance learningname-finder.
In Proceedings of ANLP-97, pages 194?201.A.
Blum and T. Mitchell.
1998.
Combining Labeled and Unla-beled Data with Co-Training.
In Proceedings of the 11th An-nual Conference on Computational Learning Theory (COLT-98).E.
Brill and P. Resnik.
1994.
A Transformation-based Ap-proach to Prepositional Phrase Attachment Disambiguation.In Proceedings of the Fifteenth International Conference onComputational Linguistics (COLING-94).S.
Caraballo.
1999.
Automatic Acquisition of a Hypernym-Labeled Noun Hierarchy from Text.
In Proceedings of the37th Annual Meeting of the Association for ComputationalLinguistics, pages 120?126.Appositives Compounds ISA UnionCategory Total Correct Total Correct Total Correct Total CorrectPeople (WSJ) 1890 .98/.98 4979 .99/.99 143 .90/.90 5414 .99/.99Orgs (WSJ) 744 .83/.88 3791 .77/.77 115 .76/.78 4227 .78/.78People (Pharm) 292 .87/.88 2132 .82/.90 56 .80/.82 2217 .81/.90Orgs (Pharm) 281 .79/.80 3872 .53/.59 305 .77/.82 4068 .49/.50Products (Pharm) 65 .94/.95 225 .78/.80 69 .84/.84 309 .83/.86Table 5: Proper Noun Phrase Lexicon Results after Co-TrainingAppositives Compounds ISA UnionCategory Total Correct Total Correct Total Correct Total CorrectPeople (WSJ) 200 .89/.93 160 .58/.78 73 .69/.81 347 .69/.83Orgs (WSJ) 125 .66/.71 66 .26/.46 55 .46/.60 213 .47/.60People (Pharm) 44 .86/.86 38 .66/.74 30 .90/.93 84 .75/.80Orgs (Pharm) 73 .56/.59 90 .23/.35 70 .49/.61 196 .36/.47Products (Pharm) 9 .78/1.0 17 .24/.67 17 .65/1.0 38 .50/.91Table 6: General Noun Lexicon Results after Co-TrainingM.
Collins and Y.
Singer.
1999.
Unsupervised Models forNamed Entity Classification.
In Proceedings of the JointSIGDAT Conference on Empirical Methods in Natural Lan-guage Processing and Very Large Corpora (EMNLP/VLC-99).S.
Cucerzan and D. Yarowsky.
1999.
Language IndependentNamed Entity Recognition Combining Morphologi cal andContextual Evidence.
In Proceedings of the Joint SIGDATConference on Empirical Methods in Natural Language Pro-cessing and Very Large Corpora (EMNLP/VLC-99).S.
Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, SurdeanuM., R. Bunescu, R. Girju, V. Rus, and P. Morarescu.
2000.FALCON: Boosting Knowledge for Answer Engines.
InProceedings of the Ninth Text Retrieval Conference (TREC-9).M.
Hearst.
1992.
Automatic Acquisition of Hyponymsfrom Large Text Corpora.
In Proceedings of the Four-teenth International Conference on Computational Linguis-tics (COLING-92).Lynette Hirschman, Marc Light, Eric Breck, and John D.Burger.
1999.
Deep Read: A reading comprehension sys-tem.
In Proceedings of the 37th Annual Meeting of the As-sociation for Computational Linguistics.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.
Build-ing a Large Annotated Corpus of English: The Penn Tree-bank.
Computational Linguistics, 19(2):313?330.Joseph F. McCarthy and Wendy G. Lehnert.
1995.
Using De-cision Trees for Coreference Resolution.
In Proceedings ofthe Fourteenth International Joint Conference on ArtificialIntelligence, pages 1050?1055.G.
Miller.
1990.
Wordnet: An On-line Lexical Database.
In-ternational Journal of Lexicography, 3(4).E.
Riloff and R. Jones.
1999.
Learning Dictionaries for In-formation Extraction by Multi-Level Bootstrapping.
In Pro-ceedings of the Sixteenth National Conference on ArtificialIntelligence.E.
Riloff and M. Schmelzenbach.
1998.
An Empirical Ap-proach to Conceptual Case Frame Acquisition.
In Proceed-ings of the Sixth Workshop on Very Large Corpora, pages49?56.E.
Riloff and J. Shepherd.
1997.
A Corpus-Based Approachfor Building Semantic Lexicons.
In Proceedings of the Sec-ond Conference on Empirical Methods in Natural LanguageProcessing, pages 117?124.B.
Roark and E. Charniak.
1998.
Noun-phrase Co-occurrenceStatistics for Semi-automatic Semantic Lexicon Construc-tion.
In Proceedings of the 36th Annual Meeting of the As-sociation for Computational Linguistics, pages 1110?1116.S.
Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995.CRYSTAL: Inducing a conceptual dictionary.
In Proceed-ings of the Fourteenth International Joint Conference on Ar-tificial Intelligence, pages 1314?1319.
