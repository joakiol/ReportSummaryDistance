Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 34?44,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsTurbo Parsers: Dependency Parsing by Approximate Variational InferenceAndre?
F. T.
Martins??
Noah A. Smith?
Eric P.
Xing?
?School of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{afm,nasmith,epxing}@cs.cmu.eduPedro M. Q.
Aguiar?
?Instituto de Sistemas e Robo?ticaInstituto Superior Te?cnicoLisboa, Portugalaguiar@isr.ist.utl.ptMa?rio A. T.
Figueiredo?
?Instituto de Telecomunicac?o?esInstituto Superior Te?cnicoLisboa, Portugalmtf@lx.it.ptAbstractWe present a unified view of two state-of-the-art non-projective dependency parsers, bothapproximate: the loopy belief propagationparser of Smith and Eisner (2008) and the re-laxed linear program of Martins et al (2009).By representing the model assumptions witha factor graph, we shed light on the optimiza-tion problems tackled in each method.
We alsopropose a new aggressive online algorithm tolearn the model parameters, which makes useof the underlying variational representation.The algorithm does not require a learning rateparameter and provides a single framework fora wide family of convex loss functions, includ-ing CRFs and structured SVMs.
Experimentsshow state-of-the-art performance for 14 lan-guages.1 IntroductionFeature-rich discriminative models that break local-ity/independence assumptions can boost a parser?sperformance (McDonald et al, 2006; Huang, 2008;Finkel et al, 2008; Smith and Eisner, 2008; Martinset al, 2009; Koo and Collins, 2010).
Often, infer-ence with such models becomes computationally in-tractable, causing a demand for understanding andimproving approximate parsing algorithms.In this paper, we show a formal connection be-tween two recently-proposed approximate inferencetechniques for non-projective dependency parsing:loopy belief propagation (Smith and Eisner, 2008)and linear programming relaxation (Martins et al,2009).
While those two parsers are differently moti-vated, we show that both correspond to inference ina factor graph, and both optimize objective functionsover local approximations of the marginal polytope.The connection is made clear by writing the explicitdeclarative optimization problem underlying Smithand Eisner (2008) and by showing the factor graphunderlying Martins et al (2009).
The success ofboth approaches parallels similar approximations inother fields, such as statistical image processing anderror-correcting coding.
Throughtout, we call theseturbo parsers.1Our contributions are not limited to dependencyparsing: we present a general method for inferencein factor graphs with hard constraints (?2), whichextends some combinatorial factors considered bySmith and Eisner (2008).
After presenting a geo-metric view of the variational approximations un-derlying message-passing algorithms (?3), and clos-ing the gap between the two aforementioned parsers(?4), we consider the problem of learning the modelparameters (?5).
To this end, we propose an ag-gressive online algorithm that generalizes MIRA(Crammer et al, 2006) to arbitrary loss functions.We adopt a family of losses subsuming CRFs (Laf-ferty et al, 2001) and structured SVMs (Taskar etal., 2003; Tsochantaridis et al, 2004).
Finally, wepresent a technique for including features not at-tested in the training data, allowing for richer mod-els without substantial runtime costs.
Our experi-ments (?6) show state-of-the-art performance on de-pendency parsing benchmarks.1The name stems from ?turbo codes,?
a class of high-performance error-correcting codes introduced by Berrou et al(1993) for which decoding algorithms are equivalent to runningbelief propagation in a graph with loops (McEliece et al, 1998).342 Structured Inference and Factor GraphsDenote by X a set of input objects from which wewant to infer some hidden structure conveyed in anoutput set Y.
Each input x ?
X (e.g., a sentence)is associated with a set of candidate outputs Y(x) ?Y (e.g., parse trees); we are interested in the casewhere Y(x) is a large structured set.Choices about the representation of elements ofY(x) play a major role in algorithm design.
Inmany problems, the elements of Y(x) can be rep-resented as discrete-valued vectors of the form y =?y1, .
.
.
, yI?, each yi taking values in a label set Yi.For example, in unlabeled dependency parsing, I isthe number of candidate dependency arcs (quadraticin the sentence length), and each Yi = {0, 1}.
Ofcourse, the yi are highly interdependent.Factor Graphs.
Probabilistic models like CRFs(Lafferty et al, 2001) assume a factorization of theconditional distribution of Y ,Pr(Y = y | X = x) ?
?C?C ?C(x,yC), (1)where each C ?
{1, .
.
.
, I} is a factor, C is the setof factors, each yC , ?yi?i?C denotes a partial out-put assignment, and each ?C is a nonnegative po-tential function that depends on the output only viaits restriction to C. A factor graph (Kschischanget al, 2001) is a convenient representation for thefactorization in Eq.
1: it is a bipartite graph Gx com-prised of variable nodes {1, .
.
.
, I} and factor nodesC ?
C, with an edge connecting the ith variablenode and a factor node C iff i ?
C. Hence, the fac-tor graph Gx makes explicit the direct dependenciesamong the variables {y1, .
.
.
, yI}.Factor graphs have been used for several NLPtasks, such as dependency parsing, segmentation,and co-reference resolution (Sutton et al, 2007;Smith and Eisner, 2008; McCallum et al, 2009).Hard and Soft Constraint Factors.
It may bethe case that valid outputs are a proper subset ofY1 ?
?
?
?
?
YI?for example, in dependency pars-ing, the entries of the output vector y must jointlydefine a spanning tree.
This requires hard constraintfactors that rule out forbidden partial assignmentsby mapping them to zero potential values.
See Ta-ble 1 for an inventory of hard constraint factors usedin this paper.
Factors that are not of this special kindare called soft factors, and have strictly positive po-tentials.
We thus have a partition C = Chard ?
Csoft.We let the soft factor potentials take the form?C(x,yC) , exp(?>?C(x,yC)), where ?
?
Rdis a vector of parameters (shared across factors) and?C(x,yC) is a local feature vector.
The conditionaldistribution of Y (Eq.
1) thus becomes log-linear:Pr?
(y|x) = Zx(?
)?1 exp(?>?
(x,y)), (2)where Zx(?)
,?y?
?Y(x) exp(?>?(x,y?))
is thepartition function, and the features decompose as:?
(x,y) ,?C?Csoft?C(x,yC).
(3)Dependency Parsing.
Smith and Eisner (2008)proposed a factor graph representation for depen-dency parsing (Fig.
1).
The graph has O(n2) vari-able nodes (n is the sentence length), one per candi-date arc a , ?h,m?
linking a head h and modifierm.
Outputs are binary, with ya = 1 iff arc a belongsto the dependency tree.
There is a hard factor TREEconnected to all variables, that constrains the overallarc configurations to form a spanning tree.
There is aunary soft factor per arc, whose log-potential reflectsthe score of that arc.
There are also O(n3) pair-wise factors; their log-potentials reflect the scoresof sibling and grandparent arcs.
These factors cre-ate loops, thus calling for approximate inference.Without them, the model is arc-factored, and ex-act inference in it is well studied: finding the mostprobable parse tree takes O(n3) time with the Chu-Liu-Edmonds algorithm (McDonald et al, 2005),2and computing posterior marginals for all arcs takesO(n3) time via the matrix-tree theorem (Smith andSmith, 2007; Koo et al, 2007).Message-passing algorithms.
In generalfactor graphs, both inference problems?obtaining the most probable output (the MAP)argmaxy?Y(x) Pr?
(y|x), and computing themarginals Pr?
(Yi = yi|x)?can be addressedwith the belief propagation (BP) algorithm (Pearl,1988), which iteratively passes messages betweenvariables and factors reflecting their local ?beliefs.
?2There is a faster but more involvedO(n2) algorithm due toTarjan (1977).35A general binary factor: ?C(v1, .
.
.
, vn) =?1 v1, .
.
.
, vn ?
SC0 otherwise,where SC ?
{0, 1}n.?Message-induced distribution: ?
, ?mj?C?j=1,...,n ?
Partition function: ZC(?)
,P?v1,...,vn?
?SCQni=1mvii?C?Marginals: MARGi(?)
, Pr?
{Vi = 1|?V1, .
.
.
, Vn?
?
SC} ?Max-marginals: MAX-MARGi,b(?)
, maxv?SC Pr?
(v|vi = b)?
Sum-prod.
: mC?i = m?1i?C ?
MARGi(?)/(1?
MARGi(?))
?Max-prod.
: mC?i = m?1i?C ?
MAX-MARGi,1(?)/MAX-MARGi,0(?)?
Local agreem.
constr.
: z ?
conv SC , where z = ?
?i(1)?ni=1 ?
Entropy: HC = logZC(?
)?Pni=1 MARGi(?)
logmi?CTREE ?TREE(?ya?a?A) =?1 y ?
Ytree (i.e., {a ?
A | ya = 1} is a directed spanning tree)0 otherwise,where A is the set of candidate arcs.?
Partition function Ztree(?)
and marginals ?MARGa(?
)?a?A computed via the matrix-tree theorem, with ?
, ?ma?TREE?a?A?
Sum-prod.
: mTREE?a = m?1a?TREE ?
MARGa(?)/(1?
MARGa(?))?Max-prod.
: mTREE?a = m?1a?TREE ?
MAX-MARGa,1(?)/MAX-MARGa,0(?
), where MAX-MARGa,b(?)
, maxy?Ytree Pr?
(y|ya = b)?
Local agreem.
constr.
: z ?
Ztree, where Ztree , convYtree is the arborescence polytope?
Entropy: Htree = logZtree(?
)?Pa?A MARGa(?)
logma?TREEXOR (?one-hot?)
?XOR(v1, .
.
.
, vn) =?1Pni=1 vi = 10 otherwise.?
Sum-prod.
: mXOR?i =?Pj 6=imj?XOR??1?Max-prod.
: mXOR?i =`maxj 6=imj?XOR??1?
Local agreem.
constr.
:Pi zi = 1, zi ?
[0, 1],?i ?HXOR = ?Pi(mi?XOR/Pj mj?XOR) log(mi?XOR/Pj mj?XOR)OR ?OR(v1, .
.
.
, vn) =?1Pni=1 vi ?
10 otherwise.?
Sum-prod.
: mOR?i =?1?Qj 6=i(1 +mj?OR)?1??1?Max-prod.
: mOR?i = max{1,minj 6=im?1j?OR}?
Local agreem.
constr.
:Pi zi ?
1, zi ?
[0, 1],?iOR-WITH-OUTPUT ?OR-OUT(v1, .
.
.
, vn) =?1 vn =Wn?1i=1 vi0 otherwise.?
Sum-prod.
: mOR-OUT?i =( ?1?
(1?m?1n?OR-OUT)Qj 6=i,n(1 +mj?OR-OUT)?1?
?1i < nQj 6=n(1 +mj?OR-OUT)?
1 i = n.?Max-prod.
: mOR-OUT?i =(min mn?OR-OUTQj 6=i,n max{1,mj?OR-OUT},max{1,minj 6=i,nm?1j?OR-OUT}oi < nQj 6=n max{1,mj?OR-OUT}min{1,maxj 6=nmj?OR-OUT} i = n.Table 1: Hard constraint factors, their potentials, messages, and entropies.
The top row shows expressions for ageneral binary factor: each outgoing message is computed from incoming marginals (in the sum-product case), ormax-marginals (in the max-product case); the entropy of the factor (see ?3) is computed from these marginals and thepartition function; the local agreement constraints (?4) involve the convex hull of the set SC of allowed configurations(see footnote 5).
The TREE, XOR, OR and OR-WITH-OUTPUT factors allow tractable computation of all these quantities(rows 2?5).
Two of these factors (TREE and XOR) had been proposed by Smith and Eisner (2008); we provide furtherinformation (max-product messages, entropies, and local agreement constraints).
Factors OR and OR-WITH-OUTPUTare novel to the best of our knowledge.
This inventory covers many cases, since the above formulae can be extendedto the case where some inputs are negated: just replace the corresponding messages by their reciprocal, vi by 1?
vi,etc.
This allows building factors NAND (an OR factor with negated inputs), IMPLY (a 2-input OR with the first inputnegated), and XOR-WITH-OUTPUT (an XOR factor with the last input negated).In sum-product BP, the messages take the form:3Mi?C(yi) ?
?D 6=CMD?i(yi) (4)MC?i(yi) ?
?yC?yi?C(yC)?j 6=iMj?C(yj).
(5)In max-product BP, the summation in Eq.
5 is re-placed by a maximization.
Upon convergence, vari-able and factor beliefs are computed as:?i(yi) ?
?CMC?i(yi) (6)?C(yC) ?
?C(yC)?iMi?C(yi).
(7)BP is exact when the factor graph is a tree: in thesum-product case, the beliefs in Eqs.
6?7 correspond3We employ the standard ?
notation, where a summa-tion/maximization indexed by yC ?
yi means that it is overall yC with the i-th component held fixed and set to yi.to the true marginals, and in the max-product case,maximizing each ?i(yi) yields the MAP output.
Ingraphs with loops, BP is an approximate method, notguaranteed to converge, nicknamed loopy BP.
Wehighlight a variational perspective of loopy BP in ?3;for now we consider algorithmic issues.
Note thatcomputing the factor-to-variable messages for eachfactorC (Eq.
5) requires a summation/maximizationover exponentially many configurations.
Fortu-nately, for all the hard constraint factors in rows 3?5of Table 1, this computation can be done in lineartime (and polynomial for the TREE factor)?this ex-tends results presented in Smith and Eisner (2008).44The insight behind these speed-ups is that messages onbinary-valued potentials can be expressed as MC?i(yi) ?36TREE1ARC(T,RE)SIB(T1RE1RE)1 2SIB(T1RE1RE)1 3SIB(T1RE1RE)2 3GRAND(A,T1RE)12ARC(T,RE) 3ARC(T,RE)ARC(A,T)Figure 1: Factor graph corresponding to the dependencyparsing model of Smith and Eisner (2008) with siblingand grandparent features.
Circles denote variable nodes,and squares denote factor nodes.
Note the loops createdby the inclusion of pairwise factors (GRAND and SIB).In Table 1 we present closed-form expressionsfor the factor-to-variable message ratios mC?i ,MC?i(1)/MC?i(0) in terms of their variable-to-factor counterparts mi?C , Mi?C(1)/Mi?C(0);these ratios are all that is necessary when the vari-ables are binary.
Detailed derivations are presentedin an extended version of this paper (Martins et al,2010b).3 Variational RepresentationsLet Px , {Pr?
(.|x) | ?
?
Rd} be the family of alldistributions of the form in Eq.
2.
We next presentan alternative parametrization for the distributions inPx in terms of factor marginals.
We will see thateach distribution can be seen as a point in the so-called marginal polytope (Wainwright and Jordan,2008); this will pave the way for the variational rep-resentations to be derived next.Parts and Output Indicators.
A part is a pair?C,yC?, where C is a soft factor and yC a partialoutput assignment.
We let R = {?C,yC?
| C ?Csoft,yC ?
?i?C Yi} be the set of all parts.
Givenan output y?
?
Y(x), a part ?C,yC?
is said to be ac-tive if it locally matches the output, i.e., if yC = y?C .Any output y?
?
Y(x) can be mapped to a |R|-dimensional binary vector ?(y?)
indicating whichparts are active, i.e., [?(y?)]?C,yC?
= 1 if yC = y?CPr{?C(YC) = 1|Yi = yi} and MC?i(yi) ?max?C(yC)=1 Pr{YC = yC |Yi = yi}, respectively for thesum-product and max-product cases; these probabilities are in-duced by the messages in Eq.
4: for an event A ?Qi?C Yi,Pr{YC ?
A} ,PyCI(yC ?
A)Qi?CMi?C(yi).and 0 otherwise; ?(y?)
is called the output indicatorvector.
This mapping allows decoupling the featurevector in Eq.
3 as the product of an input matrix andan output vector:?
(x,y) =?C?Csoft?C(x,yC) = F(x)?
(y), (8)where F(x) is a d-by-|R| matrix whose columnscontain the part-local feature vectors ?C(x,yC).Observe, however, that not every vector in {0, 1}|R|corresponds necessarily to a valid output in Y(x).Marginal Polytope.
Moving to vector representa-tions of outputs leads naturally to a geometric viewof the problem.
The marginal polytope is the convexhull5 of all the ?valid?
output indicator vectors:M(Gx) , conv{?
(y) | y ?
Y(x)}.Note that M(Gx) only depends on the factor graphGx and the hard constraints (i.e., it is independent ofthe parameters ?).
The importance of the marginalpolytope stems from two facts: (i) each vertex ofM(Gx) corresponds to an output in Y(x); (ii) eachpoint in M(Gx) corresponds to a vector of marginalprobabilities that is realizable by some distribution(not necessarily in Px) that factors according to Gx.Variational Representations.
We now describeformally how the points in M(Gx) are linked to thedistributions in Px.
We extend the ?canonical over-complete parametrization?
case, studied by Wain-wright and Jordan (2008), to our scenario (commonin NLP), where arbitrary features are allowed andthe parameters are tied (shared by all factors).
LetH(Pr?
(.|x)) , ?
?y?Y(x) Pr?
(y|x) log Pr?
(y|x)denote the entropy of Pr?
(.|x), and E?[.]
the ex-pectation under Pr?(.|x).
The component of ?
?M(Gx) indexed by part ?C,yC?
is denoted ?C(yC).Proposition 1.
There is a map coupling each distri-bution Pr?
(.|x) ?
Px to a unique ?
?
M(Gx) suchthat E?[?
(Y )] = ?.
Define H(?)
, H(Pr?
(.|x))if some Pr?
(.|x) is coupled to ?, and H(?)
= ?
?if no such Pr?
(.|x) exists.
Then:1.
The following variational representation for thelog-partition function (mentioned in Eq.
2) holds:logZx(?)
= max??M(Gx)?>F(x)?
+H(?).
(9)5The convex hull of {z1, .
.
.
, zk} is the set of points that canbe written asPki=1 ?izi, wherePki=1 ?i = 1 and each ?i ?
0.37Parameter?space Factor?log-potentials?space??????
?Marginal?polytope?Figure 2: Dual parametrization of the distributions inPx.
Our parameter space (left) is first linearly mapped tothe space of factor log-potentials (middle).
The latter ismapped to the marginal polytope M(Gx) (right).
In gen-eral only a subset of M(Gx) is reachable from our param-eter space.
Any distribution in Px can be parametrized bya vector ?
?
Rd or by a point ?
?M(Gx).2.
The problem in Eq.
9 is convex and its solutionis attained at the factor marginals, i.e., there is amaximizer ??
s.t.
?
?C(yC) = Pr?
(YC = yC |x)for each C ?
C. The gradient of the log-partitionfunction is?
logZx(?)
= F(x)??.3.
The MAP y?
, argmaxy?Y(x) Pr?
(y|x) can beobtained by solving the linear program??
, ?(y?)
= argmax??M(Gx)?>F(x)?.
(10)A proof of this proposition can be found in Mar-tins et al (2010a).
Fig.
2 provides an illustration ofthe dual parametrization implied by Prop.
1.4 Approximate Inference & Turbo ParsingWe now show how the variational machinery justdescribed relates to message-passing algorithms andprovides a common framework for analyzing two re-cent dependency parsers.
Later (?5), Prop.
1 is usedconstructively for learning the model parameters.4.1 Loopy BP as a Variational ApproximationFor general factor graphs with loops, the marginalpolytope M(Gx) cannot be compactly specified andthe entropy term H(?)
lacks a closed form, render-ing exact optimizations in Eqs.
9?10 intractable.
Apopular approximate algorithm for marginal infer-ence is sum-product loopy BP, which passes mes-sages as described in ?2 and, upon convergence,computes beliefs via Eqs.
6?7.
Were loopy BP exact,these beliefs would be the true marginals and hencea point in the marginal polytope M(Gx).
However,this need not be the case, as elucidated by Yedidia etal.
(2001) and others, who first analyzed loopy BPfrom a variational perspective.
The following twoapproximations underlie loopy BP:?
The marginal polytope M(Gx) is approximated bythe local polytope L(Gx).
This is an outer bound;its name derives from the fact that it only imposeslocal agreement constraints ?i, yi ?
Yi, C ?
C:?yi?i(yi) = 1,?yC?yi?C(yC) = ?i(yi).
(11)Namely, it is characterized by L(Gx) , {?
?R|R|+ | Eq.
11 holds ?i, yi ?
Yi, C ?
C}.
Theelements of L(Gx) are called pseudo-marginals.Clearly, the true marginals satisfy Eq.
11, andtherefore M(Gx) ?
L(Gx).?
The entropy H is replaced by its Bethe approx-imation HBethe(? )
,?Ii=1(1 ?
di)H(?
i) +?C?CH(?C), where di = |{C | i ?
C}| is thenumber of factors connected to the ith variable,H(?
i) , ?
?yi?i(yi) log ?i(yi) and H(?C) ,?
?yC?C(yC) log ?C(yC).Any stationary point of sum-product BP is a lo-cal optimum of the variational problem in Eq.
9with M(Gx) replaced by L(Gx) and H replaced byHBethe (Yedidia et al, 2001).
Note however thatmultiple optima may exist, since HBethe is not nec-essarily concave, and that BP may not converge.Table 1 shows closed form expressions for thelocal agreement constraints and entropies of somehard-constraint factors, obtained by invoking Eq.
7and observing that ?C(yC) must be zero if configu-ration yC is forbidden.
See Martins et al (2010b).4.2 Two Dependency Turbo ParsersWe next present our main contribution: a formalconnection between two recent approximate depen-dency parsers, which at first sight appear unrelated.Recall that (i) Smith and Eisner (2008) proposed afactor graph (Fig.
1) in which they run loopy BP,and that (ii) Martins et al (2009) approximate pars-ing as the solution of a linear program.
Here, wefill the blanks in the two approaches: we derive ex-plicitly the variational problem addressed in (i) andwe provide the underlying factor graph in (ii).
Thisputs the two approaches side-by-side as approximatemethods for marginal and MAP inference.
Sinceboth rely on ?local?
approximations (in the sense38of Eq.
11) that ignore the loops in their graphicalmodels, we dub them turbo parsers by analogy witherror-correcting turbo decoders (see footnote 1).Turbo Parser #1: Sum-Product Loopy BP.
Thefactor graph depicted in Fig.
1?call it Gx?includespairwise soft factors connecting sibling and grand-parent arcs.6 We next characterize the local polytopeL(Gx) and the Bethe approximationHBethe inherentin Smith and Eisner?s loopy BP algorithm.Let A be the set of candidate arcs, and P ?A2 the set of pairs of arcs that have factors.
Let?
= ?
?A, ?P ?
with ?A = ?
?a?a?A and ?P =??ab??a,b?
?P .
Since all variables are binary, we maywrite, for each a ?
A, ?a(1) = za and ?a(0) =1 ?
za, where za is a variable constrained to [0, 1].Let zA , ?za?a?A; the local agreement constraintsat the TREE factor (see Table 1) are written as zA ?Ztree(x), where Ztree(x) is the arborescence poly-tope, i.e., the convex hull of all incidence vectorsof dependency trees (Martins et al, 2009).
It isstraightforward to write a contingency table and ob-tain the following local agreement constraints at thepairwise factors:?ab(1, 1) = zab, ?ab(0, 0) = 1?
za ?
zb + zab?ab(1, 0) = za ?
zab, ?ab(0, 1) = zb ?
zab.Noting that all these pseudo-marginals are con-strained to the unit interval, one can get rid of allvariables ?ab and write everything asza ?
[0, 1], zb ?
[0, 1], zab ?
[0, 1],zab ?
za, zab ?
zb, zab ?
za + zb ?
1,(12)inequalities which, along with zA ?
Ztree(x), de-fine the local polytope L(Gx).
As for the factor en-tropies, start by noting that the TREE-factor entropyHtree can be obtained in closed form by computingthe marginals z?A and the partition function Zx(?
)(via the matrix-tree theorem) and recalling the vari-ational representation in Eq.
9, yielding Htree =logZx(?)?
?>F(x)z?A.
Some algebra allows writ-ing the overall Bethe entropy approximation as:HBethe(? )
= Htree(zA)???a,b?
?PIa;b(za, zb, zab), (13)where we introduced the mutual information asso-ciated with each pairwise factor, Ia;b(za, zb, zab) =6Smith and Eisner (2008) also proposed other variants withmore factors, which we omit for brevity.TRE1ACTRTE(TRE1A1TRTE(,)SIARTE(,)SIB23GRNDARTE(TRE1AATTE( TRE1AAT1TE(,)SIAATE(TRE1BNDRS)AATE()AATR(TRE1AATRTE(TRE1BG,RGDB)AATRTE()ACTR()A1TR(GRDB,)DSAR(EEEFigure 3: Details of the factor graph underlying the parserof Martins et al (2009).
Dashed circles represent auxil-iary variables.
See text and Table 1.?ya,yb?ab(ya, yb) log?ab(ya,yb)?a(ya)?b(yb).
The approximatevariational expression becomes logZx(?)
?maxz ?>F(x)z +Htree(zA)???a,b?
?PIa;b(za, zb, zab)s.t.
zab ?
za, zab ?
zb,zab ?
za + zb ?
1, ?
?a, b?
?
P,zA ?
Ztree,(14)whose maximizer corresponds to the beliefs re-turned by the Smith and Eisner?s loopy BP algorithm(if it converges).Turbo Parser #2: LP-Relaxed MAP.
We nowturn to the concise integer LP formulation of Mar-tins et al (2009).
The formulation is exact but NP-hard, and so an LP relaxation is made there by drop-ping the integer constraints.
We next construct a fac-tor graph G?x and show that the LP relaxation corre-sponds to an optimization of the form in Eq.
10, withthe marginal polytope M(G?x) replaced by L(G?x).G?x includes the following auxiliary variablenodes: path variables ?pij?i=0,...,n,j=1,...,n, whichindicate whether word j descends from i in the de-pendency tree, and flow variables ?fka ?a?A,k=1,...,n,which evaluate to 1 iff arc a ?carries flow?
to k,i.e., iff there is a path from the root to k that passesthrough a.
We need to seed these variables imposingp0k = pkk = 1,?k, fh?h,m?
= 0, ?h,m; (15)i.e., any word descends from the root and from it-self, and arcs leaving a word carry no flow to that39word.
This can be done with unary hard constraintfactors.
We then replace the TREE factor in Fig.
1 bythe factors shown in Fig.
3:?
O(n) XOR factors, each connecting all arc vari-ables of the form {?h,m?}h=0,...,n.
These ensurethat each word has exactly one parent.
Each factoryields a local agreement constraint (see Table 1):?nh=0 z?h,m?
= 1, m ?
{1, .
.
.
, n} (16)?
O(n3) IMPLY factors, each expressing that if anarc carries flow, then that arc must be active.
Suchfactors are OR factors with the first input negated,hence, the local agreement constraints are:fka ?
za, a ?
A, k ?
{1, .
.
.
, n}.
(17)?
O(n2) XOR-WITH-OUTPUT factors, which im-pose the constraint that each path variable pmk isactive if and only if exactly one incoming arc in{?h,m?
}h=0,...,n carries flow to k. Such factorsare XOR factors with the last input negated, andhence their local constraints are:pmk =?nh=0 fk?h,m?, m, k ?
{1, .
.
.
, n} (18)?
O(n2) XOR-WITH-OUTPUT factors to impose theconstraint that words don?t consume other words?commodities; i.e., if h 6= k and k 6= 0, then thereis a path from h to k iff exactly one outgoing arcin {?h,m?
}m=1,...,n carries flow to k:phk =?nm=1 fk?h,m?, h, k ?
{0, .
.
.
, n}, k /?
{0, h}.
(19)L(G?x) is thus defined by the constraints in Eq.
12and 15?19.
The approximate MAP problem, thatreplaces M(G?x) by L(G?x) in Eq.
10, thus becomes:maxz,f ,p ?>F(x)zs.t.
Eqs.
12 and 15?19 are satisfied.
(20)This is exactly the LP relaxation considered by Mar-tins et al (2009) in their multi-commodity flowmodel, for the configuration with siblings and grand-parent features.7 They also considered a config-uration with non-projectivity features?which fireif an arc is non-projective.8 That configurationcan also be obtained here if variables {n?h,m?}
are7To be precise, the constraints of Martins et al (2009) arerecovered after eliminating the path variables, via Eqs.
18?19.8An arc ?h,m?
is non-projective if there is some word in itsspan not descending from h (Kahane et al, 1998).added to indicate non-projective arcs and OR-WITH-OUTPUT hard constraint factors are inserted to en-force n?h,m?
= z?h,m??
?min(h,m)<j<min(h,m) ?phj .Details are omitted for space.In sum, although the approaches of Smith and Eis-ner (2008) and Martins et al (2009) look very dif-ferent, in reality both are variational approximationsemanating from Prop.
1, respectively for marginaland MAP inference.
However, they operate on dis-tinct factor graphs, respectively Figs.
1 and 3.95 Online LearningOur learning algorithm is presented in Alg.
1.
It is ageneralized online learner that tackles `2-regularizedempirical risk minimization of the formmin??Rd?2??
?2 + 1m?mi=1 L(?
;xi,yi), (21)where each ?xi,yi?
is a training example, ?
?
0 isthe regularization constant, and L(?
;x,y) is a non-negative convex loss.
Examples include the logisticloss used in CRFs (?
log Pr?
(y|x)) and the hingeloss of structured SVMs (maxy?
?Y(x) ?>(?(x,y?)??
(x,y)) + `(y?,y) for some cost function `).
Theseare both special cases of the family defined in Fig.
4,which also includes the structured perceptron?s loss(?
?
?, ?
= 0) and the softmax-margin loss ofGimpel and Smith (2010; ?
= ?
= 1).Alg.
1 is closely related to stochastic or onlinegradient descent methods, but with the key advan-tage of not needing a learning rate hyperparameter.We sketch the derivation of Alg.
1; full details canbe found in Martins et al (2010a).
On the tth round,one example ?xt,yt?
is considered.
We seek to solvemin?,?
?m2 ??
?
?t?2 + ?s.t.
L(?
;xt,yt) ?
?, ?
?
0,(23)9Given what was just exposed, it seems appealing to trymax-product loopy BP on the factor graph of Fig.
1, or sum-product loopy BP on the one in Fig.
3.
Both attempts present se-rious challenges: the former requires computing messages sentby the tree factor, which requires O(n2) calls to the Chu-Liu-Edmonds algorithm and hence O(n5) time.
No obvious strat-egy seems to exist for simultaneous computation of all mes-sages, unlike in the sum-product case.
The latter is even morechallenging, as standard sum-product loopy BP has serious is-sues in the factor graph of Fig.
3; we construct in Martins et al(2010b) a simple example with a very poor Bethe approxima-tion.
This might be fixed by using other variants of sum-productBP, e.g., ones in which the entropy approximation is concave.40L?,?(?
;x,y) , 1?
log?y?
?Y(x) exp[?(?>(?(x,y?)?
?
(x,y))+ ?`(y?,y))](22)Figure 4: A family of loss functions including as particular cases the ones used in CRFs, structured SVMs, and thestructured perceptron.
The hyperparameter ?
is the analogue of the inverse temperature in a Gibbs distribution, while?
scales the cost.
For any choice of ?
> 0 and ?
?
0, the resulting loss function is convex in ?, since, up to a scalefactor, it is the composition of the (convex) log-sum-exp function with an affine map.Algorithm 1 Aggressive Online Learning1: Input: {?xi,yi?
}mi=1, ?, number of epochs K2: Initialize ?1 ?
0; set T = mK3: for t = 1 to T do4: Receive instance ?xt, yt?
and set ?t = ?
(yt)5: Solve Eq.
24 to obtain ?
?t and L?,?
(?t, xt,yt)6: Compute?L?,?
(?t, xt,yt)=F(xt)(??t?
?t)7: Compute ?t = min{1?m ,L?,?(?t;xt,yt)??L?,?
(?t;xt,yt)?2}8: Return ?t+1 = ?t ?
?t?L?,?
(?t;xt,yt)9: end for10: Return the averaged model ??
?
1T?Tt=1 ?t.which trades off conservativeness (stay close to themost recent solution ?t) and correctness (keep theloss small).
Alg.
1?s lines 7?8 are the result of tak-ing the first-order Taylor approximation of L around?t, which yields the lower bound L(?
;xt,yt) ?L(?t;xt,yt) + (?
?
?t)>?L(?t;xt,yt), and plug-ging that linear approximation into the constraint ofEq.
23, which gives a simple Euclidean projectionproblem (with slack) with a closed-form solution.The online updating requires evaluating the lossand computing its gradient.
Both quantities canbe computed using the variational expression inProp.
1, for any loss L?,?(?
;x,y) in Fig.
4.10 Ouronly assumption is that the cost function `(y?,y)can be written as a sum over factor-local costs; let-ting ?
= ?
(y) and ??
= ?(y?
), this implies`(y?,y) = p>??
+ q for some p and q which areconstant with respect to ?
?.11 Under this assump-tion, L?,?(?
;x,y) becomes expressible in terms ofthe log-partition function of a distribution whoselog-potentials are set to ?(F(x)>?
+ ?p).
FromEq.
9 and after some algebra, we finally obtainL?,?(?
;x,y) =10Our description also applies to the (non-differentiable)hinge loss case, when ?
?
?, if we replace all instances of?the gradient?
in the text by ?a subgradient.
?11For the Hamming cost, this holds with p = 1 ?
2?
andq = 1>?.
See Taskar et al (2006) for other examples.max???M(Gx)?>F(x)(????)+1?H(??)+?(p>??+q).
(24)Let ??
be a maximizer in Eq.
24; from the secondstatement of Prop.
1 we obtain ?L?,?(?
;x,y) =F(x)(????).
When the inference problem in Eq.
24is intractable, approximate message-passing algo-rithms like loopy BP still allow us to obtain approx-imations of the loss L?,?
and its gradient.For the hinge loss, we arrive precisely at the max-loss variant of 1-best MIRA (Crammer et al, 2006).For the logistic loss, we arrive at a new online learn-ing algorithm for CRFs that resembles stochasticgradient descent but with an automatic step size thatfollows from our variational representation.Unsupported Features.
As datasets grow, so dothe sets of features, creating further computationalchallenges.
Often only ?supported?
features?thoseobserved in the training data?are included, andeven those are commonly eliminated when their fre-quencies fall below a threshold.
Important infor-mation may be lost as a result of these expedi-ent choices.
Formally, the supported feature setis Fsupp ,?mi=1 supp?
(xi,yi), where suppu ,{j |uj 6= 0} denotes the support of vector u. Fsuppis a subset of the complete feature set, comprised ofthose features that occur in some candidate output,Fcomp ,?mi=1?y?i?Y(xi)supp?(xi,y?i).
Featuresin Fcomp\Fsupp are called unsupported.Sha and Pereira (2003) have shown that training aCRF-based shallow parser with the complete featureset may improve performance (over the supportedone), at the cost of 4.6 times more features.
De-pendency parsing has a much higher ratio (around20 for bilexical word-word features, as estimated inthe Penn Treebank), due to the quadratic or fastergrowth of the number of parts, of which only a feware active in a legal output.
We propose a simplestrategy for handling Fcomp efficiently, which canbe applied for those losses in Fig.
4 where ?
= ?.
(e.g., the structured SVM and perceptron).
Our pro-cedure is the following: keep an active set F contain-41CRF (TURBO PARS.
#1) SVM (TURBO PARS.
#2) SVM (TURBO #2)ARC-FACT.
SEC.
ORD.
ARC-FACT.
SEC.
ORD.
|F| |F||Fsupp| +NONPROJ., COMPL.ARABIC 78.28 79.12 79.04 79.42 6,643,191 2.8 80.02 (-0.14)BULGARIAN 91.02 91.78 90.84 92.30 13,018,431 2.1 92.88 (+0.34) (?
)CHINESE 90.58 90.87 91.09 91.77 28,271,086 2.1 91.89 (+0.26)CZECH 86.18 87.72 86.78 88.52 83,264,645 2.3 88.78 (+0.44) (?
)DANISH 89.58 90.08 89.78 90.78 7,900,061 2.3 91.50 (+0.68)DUTCH 82.91 84.31 82.73 84.17 15,652,800 2.1 84.91 (-0.08)GERMAN 89.34 90.58 89.04 91.19 49,934,403 2.5 91.49 (+0.32) (?
)JAPANESE 92.90 93.22 93.18 93.38 4,256,857 2.2 93.42 (+0.32)PORTUGUESE 90.64 91.00 90.56 91.50 16,067,150 2.1 91.87 (-0.04)SLOVENE 83.03 83.17 83.49 84.35 4,603,295 2.7 85.53 (+0.80)SPANISH 83.83 85.07 84.19 85.95 11,629,964 2.6 87.04 (+0.50) (?
)SWEDISH 87.81 89.01 88.55 88.99 18,374,160 2.8 89.80 (+0.42)TURKISH 76.86 76.28 74.79 76.10 6,688,373 2.2 76.62 (+0.62)ENGLISH NON-PROJ.
90.15 91.08 90.66 91.79 57,615,709 2.5 92.13 (+0.12)ENGLISH PROJ.
91.23 91.94 91.65 92.91 55,247,093 2.4 93.26 (+0.41) (?
)Table 2: Unlabeled attachment scores, ignoring punctuation.
The leftmost columns show the performance of arc-factored and second-order models for the CRF and SVM losses, after 10 epochs with 1/(?m) = 0.001 (tuned on theEnglish Non-Proj.
dev.-set).
The rightmost columns refer to a model to which non-projectivity features were added,trained under the SVM loss, that handles the complete feature set.
Shown is the total number of features instantiated,the multiplicative factor w.r.t.
the number of supported features, and the accuracies (in parenthesis, we display thedifference w.r.t.
a model trained with the supported features only).
Entries marked with ?
are the highest reported inthe literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008),Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), whichachieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different).ing all features that have been instantiated in Alg.
1.At each round, run lines 4?5 as usual, using onlyfeatures in F. Since the other features have not beenused before, they have a zero weight, hence can beignored.
When ?
= ?, the variational problem inEq.
24 consists of a MAP computation and the solu-tion corresponds to one output y?t ?
Y(xt).
Only theparts that are active in y?t but not in yt, or vice-versa,will have features that might receive a nonzero up-date.
Those parts are reexamined for new featuresand the active set F is updated accordingly.6 ExperimentsWe trained non-projective dependency parsers for14 languages, using datasets from the CoNLL-Xshared task (Buchholz and Marsi, 2006) and twodatasets for English: one from the CoNLL-2008shared task (Surdeanu et al, 2008), which containsnon-projective arcs, and another derived from thePenn Treebank applying the standard head rules ofYamada and Matsumoto (2003), in which all parsetrees are projective.12 We implemented Alg.
1,12We used the provided train/test splits for all datasets.
ForEnglish, we used the standard test partitions (section 23 of theWall Street Journal).
We did not exploit the fact that somedatasets only contain projective trees and have unique roots.which handles any loss function L?,?
.13 When ?
<?, Turbo Parser #1 and the loopy BP algorithm ofSmith and Eisner (2008) is used; otherwise, TurboParser #2 is used and the LP relaxation is solved withCPLEX.
In both cases, we employed the same prun-ing strategy as Martins et al (2009).Two different feature configurations were firsttried: an arc-factored model and a model withsecond-order features (siblings and grandparents).We used the same arc-factored features as McDon-ald et al (2005) and second-order features that con-join words and lemmas (at most two), parts-of-speech tags, and (if available) morphological infor-mation; this was the same set of features as in Mar-tins et al (2009).
Table 2 shows the results obtainedin both configurations, for CRF and SVM loss func-tions.
While in the arc-factored case performance issimilar, in second-order models there seems to be aconsistent gain when the SVM loss is used.
Thereare two possible reasons: first, SVMs take the costfunction into consideration; second, Turbo Parser #2is less approximate than Turbo Parser #1, since onlythe marginal polytope is approximated (the entropyfunction is not involved).13The code is available at http://www.ark.cs.cmu.edu/TurboParser.42?
1 1 1 1 3 5 ??
0 (CRF) 1 3 5 1 1 1 (SVM)ARC-F. 90.15 90.41 90.38 90.53 90.80 90.83 90.662 ORD.
91.08 91.85 91.89 91.51 92.04 91.98 91.79Table 3: Varying ?
and ?
: neither the CRF nor theSVM is optimal.
Results are UAS on the English Non-Projective dataset, with ?
tuned with dev.-set validation.The loopy BP algorithm managed to converge fornearly all sentences (with message damping).
Thelast three columns show the beneficial effect of un-supported features for the SVM case (with a morepowerful model with non-projectivity features).
Formost languages, unsupported features convey help-ful information, which can be used with little extracost (on average, 2.5 times more features are instan-tiated).
A combination of the techniques discussedhere yields parsers that are in line with very strongcompetitors?for example, the parser of Koo andCollins (2010), which is exact, third-order, and con-strains the outputs to be projective, does not outper-form ours on the projective English dataset.14Finally, Table 3 shows results obtained for differ-ent settings of ?
and ?.
Interestingly, we observethat higher scores are obtained for loss functions thatare ?between?
SVMs and CRFs.7 Related WorkThere has been recent work studying efficient com-putation of messages in combinatorial factors: bi-partite matchings (Duchi et al, 2007), projectiveand non-projective arborescences (Smith and Eis-ner, 2008), as well as high order factors with count-based potentials (Tarlow et al, 2010), among others.Some of our combinatorial factors (OR, OR-WITH-OUTPUT) and the analogous entropy computationswere never considered, to the best of our knowledge.Prop.
1 appears in Wainwright and Jordan (2008)for canonical overcomplete models; we adapt it herefor models with shared features.
We rely on the vari-ational interpretation of loopy BP, due to Yedidia etal.
(2001), to derive the objective being optimizedby Smith and Eisner?s loopy BP parser.Independently of our work, Koo et al (2010)14This might be due to the fact that Koo and Collins (2010)trained with the perceptron algorithm and did not use unsup-ported features.
Experiments plugging the perceptron loss(?
?
?, ?
?
0) into Alg.
1 yielded worse performance thanwith the hinge loss.recently proposed an efficient dual decompositionmethod to solve an LP problem similar (but notequal) to the one in Eq.
20,15 with excellent pars-ing performance.
Their parser is also an instanceof a turbo parser since it relies on a local approxi-mation of a marginal polytope.
While one can alsouse dual decomposition to address our MAP prob-lem, the fact that our model does not decompose asnicely as the one in Koo et al (2010) would likelyresult in slower convergence.8 ConclusionWe presented a unified view of two recent approxi-mate dependency parsers, by stating their underlyingfactor graphs and by deriving the variational prob-lems that they address.
We introduced new hard con-straint factors, along with formulae for their mes-sages, local belief constraints, and entropies.
Weprovided an aggressive online algorithm for trainingthe models with a broad family of losses.There are several possible directions for futurework.
Recent progress in message-passing algo-rithms yield ?convexified?
Bethe approximationsthat can be used for marginal inference (Wainwrightet al, 2005), and provably convergent max-productvariants that solve the relaxed LP (Globerson andJaakkola, 2008).
Other parsing formalisms can behandled with the inventory of factors shown here?among them, phrase-structure parsing.AcknowledgmentsThe authors would like to thank the reviewers for theircomments, and Kevin Gimpel, David Smith, David Son-tag, and Terry Koo for helpful discussions.
A. M. wassupported by a grant from FCT/ICTI through the CMU-Portugal Program, and also by Priberam Informa?tica.N.
S. was supported in part by Qatar NRF NPRP-08-485-1-083.
E. X. was supported by AFOSR FA9550010247,ONR N000140910758, NSF CAREER DBI-0546594,NSF IIS-0713379, and an Alfred P. Sloan Fellowship.M.
F. and P. A. were supported by the FET programme(EU FP7), under the SIMBAD project (contract 213250).15The difference is that the model of Koo et al (2010)includes features that depend on consecutive siblings?making it decompose into subproblems amenable to dynamicprogramming?while we have factors for all pairs of siblings.43ReferencesC.
Berrou, A. Glavieux, and P. Thitimajshima.
1993.Near Shannon limit error-correcting coding and decod-ing.
In Proc.
of ICC, volume 93, pages 1064?1070.S.
Buchholz and E. Marsi.
2006.
CoNLL-X shared taskon multilingual dependency parsing.
In CoNLL.K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,and Y.
Singer.
2006.
Online passive-aggressive al-gorithms.
JMLR, 7:551?585.J.
Duchi, D. Tarlow, G. Elidan, and D. Koller.
2007.Using combinatorial optimization within max-productbelief propagation.
NIPS, 19.J.
R. Finkel, A. Kleeman, and C. D. Manning.
2008.
Effi-cient, feature-based, conditional random field parsing.Proc.
of ACL.K.
Gimpel and N. A. Smith.
2010.
Softmax-margincrfs: Training log-linear models with loss functions.In Proc.
of NAACL.A.
Globerson and T. Jaakkola.
2008.
Fixing max-product: Convergent message passing algorithms forMAP LP-relaxations.
NIPS, 20.L.
Huang.
2008.
Forest reranking: Discriminative pars-ing with non-local features.
In Proc.
of ACL.S.
Kahane, A. Nasr, and O. Rambow.
1998.
Pseudo-projectivity: a polynomially parsable non-projectivedependency grammar.
In Proc.
of COLING.T.
Koo and M. Collins.
2010.
Efficient third-order de-pendency parsers.
In Proc.
of ACL.T.
Koo, A. Globerson, X. Carreras, and M. Collins.
2007.Structured prediction models via the matrix-tree theo-rem.
In Proc.
of EMNLP.T.
Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-tag.
2010.
Dual decomposition for parsing with non-projective head automata.
In Proc.
of EMNLP.F.
R. Kschischang, B. J. Frey, and H. A. Loeliger.
2001.Factor graphs and the sum-product algorithm.
IEEETrans.
Inf.
Theory, 47(2):498?519.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
of ICML.A.
F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.2008.
Stacking dependency parsers.
In EMNLP.A.
F. T. Martins, N. A. Smith, and E. P. Xing.
2009.Concise integer linear programming formulations fordependency parsing.
In Proc.
of ACL-IJCNLP.A.
F. T. Martins, K. Gimpel, N. A. Smith, E. P. Xing,P.
M. Q. Aguiar, and M. A. T. Figueiredo.
2010a.Learning structured classifiers with dual coordinatedescent.
Technical Report CMU-ML-10-109.A.
F. T. Martins, N. A. Smith, E. P. Xing, P. M. Q. Aguiar,and M. A. T. Figueiredo.
2010b.
Turbo parsers:Dependency parsing by approximate variational infer-ence (extended version).A.
McCallum, K. Schultz, and S. Singh.
2009.
Fac-torie: Probabilistic programming via imperatively de-fined factor graphs.
In NIPS.R.
T. McDonald, F. Pereira, K. Ribarov, and J. Hajic.2005.
Non-projective dependency parsing using span-ning tree algorithms.
In Proc.
of HLT-EMNLP.R.
McDonald, K. Lerman, and F. Pereira.
2006.
Multi-lingual dependency analysis with a two-stage discrim-inative parser.
In Proc.
of CoNLL.R.
J. McEliece, D. J. C. MacKay, and J. F. Cheng.
1998.Turbo decoding as an instance of Pearl?s ?belief prop-agation?
algorithm.
IEEE Journal on Selected Areasin Communications, 16(2).J.
Pearl.
1988.
Probabilistic Reasoning in IntelligentSystems: Networks of Plausible Inference.
MorganKaufmann.F.
Sha and F. Pereira.
2003.
Shallow parsing with condi-tional random fields.
In Proc.
of HLT-NAACL.D.
A. Smith and J. Eisner.
2008.
Dependency parsing bybelief propagation.
In Proc.
of EMNLP.D.
A. Smith and N. A. Smith.
2007.
Probabilistic modelsof nonprojective dependency trees.
In EMNLP.M.
Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, andJ.
Nivre.
2008.
The CoNLL-2008 shared task onjoint parsing of syntactic and semantic dependencies.CoNLL.C.
Sutton, A. McCallum, and K. Rohanimanesh.
2007.Dynamic conditional random fields: Factorized prob-abilistic models for labeling and segmenting sequencedata.
JMLR, 8:693?723.R.
E. Tarjan.
1977.
Finding optimum branchings.
Net-works, 7(1):25?36.D.
Tarlow, I. E. Givoni, and R. S. Zemel.
2010.
HOP-MAP: Efficient message passing with high order po-tentials.
In Proc.
of AISTATS.B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-marginMarkov networks.
In NIPS.B.
Taskar, S. Lacoste-Julien, and M. I. Jordan.
2006.Structured prediction, dual extragradient and Bregmanprojections.
JMLR, 7:1627?1653.I.
Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.2004.
Support vector machine learning for interdepen-dent and structured output spaces.
In Proc.
of ICML.M.
J. Wainwright and M. I. Jordan.
2008.
GraphicalModels, Exponential Families, and Variational Infer-ence.
Now Publishers.M.
J. Wainwright, T.S.
Jaakkola, and A.S. Willsky.
2005.A new class of upper bounds on the log partition func-tion.
IEEE Trans.
Inf.
Theory, 51(7):2313?2335.H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis with support vector machines.
In Proc.of IWPT.J.
S. Yedidia, W. T. Freeman, and Y. Weiss.
2001.
Gen-eralized belief propagation.
In NIPS.44
