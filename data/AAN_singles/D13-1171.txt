Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1643?1654,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsOpen Domain Targeted SentimentMargaret Mitchell Jacqueline Aguilar Theresa Wilson Benjamin Van DurmeHuman Language Technology Center of ExcellenceJohns Hopkins UniversityBaltimore, MD 21218, USA{m.mitchell,jacqui.aguilar}@jhu.edu, Theresa.Wilson@oberlin.edu, vandurme@cs.jhu.eduAbstractWe propose a novel approach to sentimentanalysis for a low resource setting.
The in-tuition behind this work is that sentimentexpressed towards an entity, targeted senti-ment, may be viewed as a span of sentimentexpressed across the entity.
This represen-tation allows us to model sentiment detec-tion as a sequence tagging problem, jointlydiscovering people and organizations alongwith whether there is sentiment directed to-wards them.
We compare performance inboth Spanish and English on microblog data,using only a sentiment lexicon as an exter-nal resource.
By leveraging linguistically-informed features within conditional randomfields (CRFs) trained to minimize empiri-cal risk, our best models in Spanish signifi-cantly outperform a strong baseline, and reacharound 90% accuracy on the combined task ofnamed entity recognition and sentiment pre-diction.
Our models in English, trained on amuch smaller dataset, are not yet statisticallysignificant against their baselines.1 IntroductionSentiment analysis is a multi-faceted problem.
De-termining when a positive or negative sentiment isbeing expressed is a large part of the challenge, butidentifying other attributes, such as the target of thesentiment, is also crucial if the ultimate goal is topinpoint and extract opinions.
Consider the exam-ples below, all of which contain a positive sentiment:(1) So happy that Kentucky lost to Tennessee!
(2) Kentucky versus Kansas I can hardly wait...(3) Kentucky is the best alley-oop throwing teamsince Sherman Douglas?
Syracuse squads!
!The entities in these examples are college basket-ball teams, and the events referred to are games.
In(1), although there is a positive sentiment, the tar-get of the sentiment is an event (Kentucky losing toTennessee).
However, from the positive sentimenttoward this event, we can infer that the speaker hasa negative sentiment toward Kentucky and a positivesentiment toward Tennessee.
In (2), the positive sen-timent is toward a future event, but we are not givenenough information to infer a sentiment toward thementioned entities.
In (3), Kentucky is the directtarget of the positive sentiment.
We can also in-fer a positive sentiment toward Douglas?s Syracuseteams, and even toward Douglas himself.These examples illustrate the importance of thetarget when interpreting sentiment in context.
If weare looking for sentiments toward Kentucky, for ex-ample, we would want to identify (1) as negative, (2)as neutral (no sentiment) and (3) as positive.
How-ever, if we are looking for sentiment toward Ten-nessee, we would want to identify (1) as positive,and (2) and (3) as neutral.The expression of these and other kinds of sen-timent can be understood as involving three items:(1) An experiencer(2) An attitude(3) A target (optionally)Research in sentiment analysis often focuses on (2),predicting overall sentiment polarity (Agarwal et al2011; Bora, 2012).
Recent work has begun to com-bine (2) with (3), examining how to automaticallypredict the sentiment polarity expressed towards atarget entity (Jiang et al 2011; Chen et al 2012)for a fixed set of targets.
This topic-dependent sen-timent classification requires that the target entity be1643Figure 1: Sentiment expressed across an entity.given, and returns statements expressing sentimenttowards the given entity.In this paper, we take a step towards open-domain,targeted sentiment analysis by investigating how todetect both the named entity and the sentiment ex-pressed toward it.
We observe that sentiment ex-pressed towards a target entity may be possible tolearn in a graphical model along the span of the en-tity itself: Similar to how named entity recognition(NER) learns labels along the span of each word inan entity name, sentiment may be expressed alongthe entity as well.
A small example is shown in Fig-ure 1.
We focus on people and organizations (voli-tional named entities), which are the primary targetsof sentiment in our microblog data (see Table 1).Both NER and opinion expression extraction haveachieved impressive results using conditional ran-dom fields (CRFs) (Lafferty et al 2001) to definethe conditional probability of entity categories (Mc-Callum and Li, 2003; Choi et al 2006; Yang andCardie, 2013).
We develop such models to jointlypredict the NE and the sentiment expressed towardsit using minimum risk training (Stoyanov and Eis-ner, 2012).
We learn our models on informal Span-ish and English language taken from the social net-work Twitter,1 where the language variety makesNLP particularly challenging (see Figure 2).Our ultimate goal is to develop models that willbe useful for low resource languages, where a sen-timent lexicon may be known or bootstrapped, butmore sophisticated linguistic tools may not be read-ily available.
We therefore do not rely on an externalpart-of-speech tagger or parser, which are often usedfor features in fine-grained sentiment analysis; suchtools are not available in many languages, and if theyare, are not usually adapted for noisy social media.Instead, we use information from sentiment lex-icons and some simple hand-written features, andotherwise use only features of the word that can be1www.twitter.com@[user] le dijo erralo muy por lo bajo jaja un grandejuancito grandes amigos mios@[user] he told him it was very on the dl haha a greatjuancito great friends of mine@[user] buenos d?
?as Profe!!
Nos quedamos acciden-tados otra vez en la carretera vieja guarenas echandogasoil, estamos a la interperie@[user] good morning, Prof!!
We were wrecked againon the old guarenas highway while getting diesel, we?reout in the openSin a?nimo de ofender a los Militares, que realmentese merecen ese aumento y ma?s.
Pero, do?nde queda lamisma recompensa para Me?dicos.I do not intend to offend the military in the slightest,they truly deserve the raise and more.
However, I?mwondering whether doctors will ever receive a similarcompensation.Figure 2: Messages on Twitter use a wide range offormality, style, and errors, which makes extracting in-formation particularly difficult.
Examples from Spanish(screen names anonymized), with approximate transla-tions in English.extracted without supervision.
These include fea-tures based on unsupervised word tags (Brown clus-ters) and a method that automatically syllabifies aword based on the orthography of the language.
Alltools and code used for this research are releasedwith this paper.22 Related WorkAs the scale of social media has grown, usingsources such as Twitter to mine public sentimenthas become increasingly promising.
Commer-cial systems include Sentiment1403 (products andbrands) and tweetfeel4 (suggests searching for pop-ular movies, celebrities and companies).The majority of academic research has focused onsupervised classification of message sentiment irre-spective of target (Barbosa and Feng, 2010; Pak andParoubek, 2010; Bifet and Frank, 2010; Davidov etal., 2010; Kouloumpis et al 2011; Agarwal et al2011).
Large datasets are collected for this work byleveraging the sentiment inherent in emoticons (e.g.,smilies and frownies) and/or select Twitter hashtags(e.g., #bestdayever, #fail), resulting in noisy collec-2www.m-mitchell.com/code3www.sentiment140.com4www.tweetfeel.com1644tions appropriate for initial exploration.
Prior workincludes: the use of a social network (Speriosu etal., 2011; Tan et al 2011; Calais Guerra et al2011; Jiang et al 2011; Li et al 2012; Hu etal., 2013); user-adapted models based on collabo-rative online-learning (Li et al 2010b); unsuper-vised, joint sentiment-topic modeling (Saif et al2012); tracking changing sentiment during debates(Diakopoulos and Shamma, 2010); and how ortho-graphic conventions such as word-lengthening canbe used to adapt a Twitter-specific sentiment lexicon(Brody and Diakopoulos, 2011).Efforts in targeted sentiment (Bermingham andSmeaton, 2010; Jin and Ho, 2009; Li et al 2010a;Jiang et al 2011; Tan et al 2011; Wang et al2011; Li et al 2012; Chen et al 2012), have mostlyfocused on topic-dependent analysis.
In these ap-proaches, messages are collected on a fixed set oftopics/targets, such as products or sports teams, andsentiment is learned for the given set.
In contrast,we aim to predict sentiment in tweets for any namedperson or organization.
We refer to this task as opendomain targeted sentiment analysis.Within topic-dependent sentiment analysis, sev-eral approaches have explored applying CRFs orHMMs to extract sentiment and target words fromtext (Jin and Ho, 2009; Li et al 2010a).
In theseapproaches, opinion expressions are extracted, andpolarity is annotated across the opinion expression.However, as noted by many researchers in senti-ment, opinion orientation towards a specific targetis often not equal to the orientation of a neighbor-ing opinion expression; and opinion expressions inone context may not be opinion expressions in an-other (Kim and Hovy, 2006), making open domainapproaches particularly challenging.The above work by Jiang et al(2011) is mostsimilar to our own.
They do not use joint learning,but they do incorporate a number of parse-based fea-tures designed to capture relationships between sen-timent terms and topic references.
In our work theserelationships are captured by the CRF model, andwe compare against their approach in Section 6.Recent work by Yang and Cardie (2013) is sim-ilar in spirit to our own, where the identificationof opinion holders, opinion targets, and opinion ex-pressions is modeled as a sequence tagging problemusing a CRF.
However, similar to previous work ap-plying CRFs to extract sentiment, Yang and Cardieuse syntactic relations to connect an opinion targetto an opinion expression.
In contrast, we modelthe expression of sentiment polarity across the senti-ment target itself, extracting both the sentiment tar-get and the sentiment expressed towards it within thesame span of words.
This allows us to use surround-ing context to determine sentiment polarity withoutidentifying explicit opinion expressions or relyingon a parser to help link expression to target.Most work in targeted sentiment outside the mi-croblogging domain has been in relation to prod-uct review mining (e.g., Yi et al(2003), Hu andLiu (2004), Popescu and Etzioni (2005), Qiu et al(2011)).
Rather than identify named entities (NEs),this work seeks to identify products and their fea-tures mentioned in reviews, and classify these forsentiment.
Recent work by Qui et aljointly learnstargets and opinion words, and Jakob and Gurevych(2010) use CRFs to extract the targets of opinions,but do not attempt to classify the sentiment towardthese targets.
To the best of our knowledge, this isthe first work to approach targeted sentiment in a lowresource setting and to jointly predict NEs and tar-geted sentiment.3 DataTwitter Collection We use the Spanish/EnglishTwitter dataset of Etter et al(2013) to train and testour models.
Approximately 30,000 Spanish tweetsand 10,000 English were labeled for named entitiesin BIO encoding: The start of an NE is labeled B-{NE} and the rest of the NE is labeled I-{NE}.
TheNE COUNT NEUTRAL POS NEGPERSON 5462 80% 20% 0%ORGANIZATION 4408 80% 20% 0%LOCATION 1405 100% 0% 0%URL 1030 100% 0% 0%TIME 535 70% 10% 20%DATE 222 100% 0% 0%MONEY 95 90% 0% 10%PERCENT 81 80% 20% 0%TELEPHONE 23 100% 0% 0%EMAIL 8 100% 0% 0%Table 1: Distribution of named entities in our SpanishTwitter corpus.
Targeted sentiment percentages are basedon expert annotations from a random sample of 10 (orall) of of each entity.
Most entities are not sentiment tar-gets (NEUTRAL).
PERSON and ORGANIZATION are mostfrequent, and among the top recipients of sentiment.1645full set of NE categories are shown in Table 1.
Forexample, the sequence ?Mark Twain?
would be la-beled B-PERSON, I-PERSON.
We are interested in bothPERSON and ORGANIZATION entities, which makeup the majority of named entities in this data, and weevaluate these using the more general entity categoryVOLITIONAL.
Removing retweets, 7,105 Spanishtweets contained a total of 9,870 volitional entitiesand 2,350 English tweets contained a total of 3,577volitional entities.Sentiment Lexicons We use two sentiment lex-icon sources in each language.
For English, weuse the MPQA lexicon (Wilson et al 2005), whichidentifies 12,296 manually and semi-automaticallyproduced subjective terms along with their polarity.For the second lexicon, we use SentiWordNet 3.0(Baccianella et al 2010), which assigns positive andnegative polarity scores to WordNet synsets.
We usethe majority polarity of all words with a subjectivityscore above 0.5.For Spanish, the first lexicon is obtained fromVolkova et al(2013), who automatically trans-lated strongly subjective terms from the MPQA lex-icon (Wilson et al 2005) into Spanish.
The re-sulting Spanish lexicon contains about 65K words.The second lexicon is available from Perez-Rosaset al(2012).
This contains approximately 1000sentiment-bearing words collected leveraging man-ual resources and 2000 collected leveraging auto-matic resources.Annotation To collect sentiment labels, weuse crowdsourcing through Amazon?s MechanicalTurk.5 Annotators (?Turkers?)
were shown sixtweets at a time, each with a single highlightednamed entity.
Turkers were instructed to (1) se-lect the sentiment being expressed towards the en-tity (positive, negative, or no sentiment); and (2)rate their level of confidence in their selection.
Fol-lowing best practices on collecting language datawith Mechanical Turk (Callison-Burch and Dredze,2010), two controls were placed among each set ofsix tweets to screen out unreliable judgments.
Anexample prompt is shown in Figure 3.Each ?tweet, NE?
pair was shown to three Turk-ers, and those with majority consensus on sentimentpolarity were extracted.
Tweets without sentiment5www.mturk.com/mturkORGANIZATION PERSONNamed EntityFrequency in Tweets05001000150020002500PositiveNegativeNeutralFigure 4: Targeted sentiment annotated for Spanish.MajorityPOS NEUTRAL NEGMinority POS 757 1249 130NEUTRAL 707 2151 473NEG 129 726 452Table 2: Number of targeted sentiment instances whereat least two of the three annotators (Majority) agreed.Common disagreements with a third annotator (Minority)were over whether no sentiment or positive sentiment wasexpressed, and whether no sentiment or negative sent-ment was expressed.consensus on all NEs were removed.
In Spanish, thisyielded 6,658 unique ?tweet, NE?
pairs.
In English,which is a smaller data set, this yielded 3,288 uniquepairs.
We split the data into folds for 10-fold cross-validation, developing on the data from one fold andreporting results for the remaining nine.The distribution of sentiment for the named en-tities annotated by Turkers is shown in Figure 4.Neutral (no targeted sentiment) dominates, followedby positive sentiment for both organizations andpeople.
As shown in Table 2, common disagree-ments were over whether or not there was targetedpositive sentiment, and whether or not there wastargeted negative sentiment.
This is in line withprevious research showing that distinguishing pos-itive sentiment from no sentiment (and distinguish-ing negative sentiment from no sentiment) is oftenmore challenging than distinguishing between pos-itive and negative sentiment (Wilson et al 2009).Indeed, we see that it was more common for annota-tors to disagree than to agree on targeted sentiment,particularly for negative targeted sentiment, wheremore instances had NEUTRAL/NEGATIVE disagree-ment than NEGATIVE three-way agreement.1646Figure 3: Example Tweet shown to Turkers.Variable Possible valuesSentiment (s) NOT-TARG, SENT-TARG(PIPE & JOINT models)Named Entity (l) O, B-VOLITIONAL, I-VOLITIONAL(PIPE & JOINT models)Combined Sent/NE (y) O, B+NOT-TARG, I+NOT-TARG(COLL models) B+SENT-TARG, I+SENT-TARGTable 3: Possible values for random variables, targetedsubjectivity (is/is not sentiment target).
COLL modelscollapse targeted subjectivity and NE label into one node.Variable Possible valuesSentiment (s) NOT-TARG, POS, NEG(PIPE & JOINT models)Named Entity (l) O, B-VOLITIONAL, I-VOLITIONAL(PIPE & JOINT models)Combined Sent/NE (y) O, B+NOT-TARG, I+NOT-TARG(COLL models) B+POS, I+POSB+NEG, I+NEGTable 4: Possible values for random variables, targetedsentiment.
The COLL models collapse both targeted sen-timent and NE label into one node.4 Targeted Subjectivity and SentimentFormally, we define the problem as follows: Givenan observed message w = (w1 .
.
.
wn), where n isthe number of words in the message and wj(1 ?j ?
n) is a word, we learn the probability of alabel sequence l = (l1 .
.
.
ln), where li ?
the setof named entity values; and a sentiment sequences = (s1 .
.
.
sn), where si ?
the set of sentiment val-ues.
We additionally explore simpler linear-chainmodels that learn the probability of a single labelsequence y = (y1 .
.
.
yn), where yi ?
the set of con-joined entity+sentiment values (Tables 3 and 4).Our basic model is a linear conditional randomfield, an undirected graph that represents the con-ditional distribution p(l, s|w).6 Sentiment towardsa named entity may be modeled in a CRF as a se-6For the COLL models, this is instead the conditional distri-bution p(y|w), where entity and sentiment labels are conjoinedin one sequence assignment y.quence of random variables for sentiment s con-nected to named entities l. In all models, entity vari-ables are connected by a factor to their neighborsin sequence, and we include skip-chains (Finkel andManning, 2010) connecting identical words whereat least one is capitalized.
Our model strategies in-clude: a pipeline that first learns volitional entitiesthen sentiment directed towards them (PIPE); onethat jointly learns volitional entities along with sen-timent directed towards them (JOINT); and one thatlearns volitional entities and targeted sentiment withcombined labels (COLL) (Figure 5).Using these models, we explore two primarytasks: (1) the task of detecting whether sentimentis targeted at an entity, which we refer to as targetedsubjectivity; and (2) the task of detecting whetherpositive, negative, or neutral sentiment (no senti-ment) is targeted at an entity, which we refer to astargeted sentiment.
Moving from targeted subjectiv-ity prediction to targeted sentiment prediction is pos-sible by changing the sentiment target (SENT-TARG)variable into two variables, one for positive targetedsentiment (POS) and one for negative (NEG).
Possi-ble values for targeted subjectivity are shown in Ta-ble 3, and possible values for targeted sentiment areshown in Table 4.In the pipeline models (PIPE), we first build aCRF where each word is connected by a factor toan entity label li ?
l. In a second model, every ob-served volitional entity node is connected by a factorto a sentiment label si ?
s. An example is shown inFigure 5 (1).In the joint models (JOINT), each si ?
s is con-nected by a factor to the corresponding entity labelin the sequence, li ?
l. Sentiment in this modelis partially observed: All sentiment variables aretreated as latent except for the sentiment connectedto the volitional entity.
An example is shown in Fig-ure 5 (2).1647In the collapsed models (COLL), we combine sen-timent and named entity into one label sequence(e.g., O, B+SENT-TARG, I+SENT-TARG).
An exampleis shown in Figure 5 (3).
The JOINT and PIPE mod-els therefore predict named entity sequences, theircategory labels, and the sentiment expressed towardsvolitional named entities.7 The collapsed modelspredict volitional labels and targeted sentiment ascombined categories.
The COLL and PIPE modelsare considerably faster than JOINT models, whereexact inference is intractable.1.
PIPELINE MODEL (PIPE)Step 1: Volitional Named Step 2: SentimentEntity Recognition2.
JOINT MODEL 3.
COLLAPSED MODEL(JOINT) (COLL)Figure 5: Example CRFs for targeted subjectivity withobserved variables (dark nodes), predicted variables(white nodes) and hidden variables (light grey nodes).5 TrainingMinimum-Risk CRF Training We use theERMA system (Stoyanov et al 2011) to learn ourmodels.8 ERMA (Empirical Risk Minimization un-der Approximations) learns parameters to minimizeloss on the training data.
Predicting NE labels usinga linear-chain CRF trained with empirical risk mini-mization has been shown to result in a statisticallysignificant improvement over the common approachof maximum likelihood estimation (Stoyanov andEisner, 2012).
All models are trained to optimize7We found that learning the VOLITIONAL categories dur-ing training rather than maintaining beliefs about separatenamed entities during inference (ORGANIZATION, PERSON)and then post-processing to VOLITIONAL leads to slightly bet-ter accuracy.8sites.google.com/site/ermasoftwarelog likelihood using 20 iterations of stochasticgradient descent, and a maximum of 100 iterationsof belief propagation to compute the marginals foreach example.Features Features of the models are shown in Ta-ble 5.
For an observed word, features are extractedfor the word itself as well as within a context win-dow of three words in either direction.
Words seenonly once are treated as out-of-vocabulary.
Surfacefeatures and linguistic features are concatenated ingroups of two and three to create further features.All algorithms and code that we have developed forfeature extraction are available online.9Because we aim to develop models that do notheavily rely on language-specific resources, we areinterested in exploring unsupervised and lightlysupervised methods for learning relevant features.Rather than use part-of-speech tags, we thereforeuse Brown cluster labels as unsupervised word tags(Brown et al 1992; Koo et al 2008).
Brownclustering is a distributional similarity method thatmerges pairs of word clusters in the training data10to create the smallest decrease in corpus likelihood,using a bigram language model on the clusters.
Forour task, we cut clusters at length 3 and length 5,and these serve as rough part-of-speech tags withoutthe need to train additional models.
For example,the word hello is tagged as belonging to cluster 011(length 3) and 01111 (length 5).During development, we found that being ableto syllabify the word (break the word into sylla-bles) was a positive indicator of people names, buta negative indicator of organization names.
Thisobservation can be approximated automatically us-ing constraints from the sonority sequencing princi-ple (Hooper, 1976; Clements, 1990; Blevins, 1996;Morelli, 2003) on a language?s orthography.
Thisis a phonotactic principle that states that syllableswill tend to have a sonority peak, usually a vowel,in the center of the syllable, followed on either sideby consonants with decreasing sonority.
Althoughlanguages may violate this principle, the core ideathat a vowel forms the nucleus of a syllable with op-9www.m-mitchell.com/code10For Spanish, we train on a sample of ?7 million Spanishtweets.
For English, we train on the essays (Pennebaker et al2007) and Facebook data (Kosinskia et al 2013) available fromICWSM 2013.1648tional consonants before (the onset) and after (thecoda) can be used to begin to automatically learnsyllable structure.11 We learn this in an unsuper-vised way, using the most frequent (seen more than1,000 times) word-initial non-vowel sequences fromthe Brown cluster data as allowable syllable onsetconsonants.
Similarly, the most frequent word-finalnon-vowel sequences are learned as possible sylla-ble codas.
For each word, we then attempt to seg-ment syllables using the learned onsets and codasaround each vowel.
If a word cannot be syllabified,it is often an initialism (e.g., CND, lsat).We follow the approach from the out-of-vocabulary assignment in the Berkeley parser(Petrov et al 2006) to encode common surfacepatterns such as capitalization and lexical patternssuch as verb endings as a single feature for wordswe have seen once or less.
We also use the Jer-boa toolkit (Van Durme, 2012) to extract furtherlanguage-independent features from the data, suchas features for emoticons and binning for repeatedcharacters (like !!!).
In addition, we include featuresfor whether the word is three or four letters, whichis often used for acronyms and initialisms in severallanguages (including Spanish and English); whetherthe word is neighbored by a punctuation mark; wordidentity; word length; message length; and positionin the sentence.We utilize a speaker of each language to simplylist word forms for sentiment features that may beindicative of sentiment, totaling less than two hoursof annotation time.
This set includes intensifiers(e.g., hella, freakin?
in English; e.g., muy, suma-mente in Spanish), positive/negative abbreviations(WTF, pso), positive/negative slang words, and pos-itive/negative prefix and suffixes (e.g., anti- in En-glish and Spanish, -ito in Spanish).6 ExperimentsWe are interested in both PERSON and ORGANIZA-TION entities, and evaluate these in the collapsedcategory VOLITIONAL.
This suggests that the datamay be pre-processed to label all volitional entitiesas VOLITIONAL NEs, or the models may be learnedwith the traditional named entities in place, and post-11Further development is necessary to extend a similar ideato languages that do not ordinarily mark all vowels in their or-thography, such as Hebrew and Arabic.SURFACE FEATURESbinned word length, message length, and sen-tence position; Jerboa features; word identity; wordlengthening; punctuation characters, has digit; hasdash; is lower case; is 3 or 4 letters; first letter capi-talized; more than one letter capitalized, etc.LINGUISTIC FEATURESfunction words; can syllabify; curse words; laughwords; words for good, bad, no, my; slang words; ab-breviations; intensifiers; subjective suffixes and pre-fixes (such as diminutive forms); common verb end-ings; common noun endingsBROWN CLUSTERING FEATUREScluster at length 3; cluster at length 5SENTIMENT FEATURESis sentiment-bearing word; prior sentiment polarityTable 5: Features used in model.processed to identify those that are VOLITIONAL.We explored results using both methods, and foundthat training models on VOLITIONAL tags yieldedthe best performance overall; we report numbers forthis approach below.We compare against a baseline (BASE-NS) wherewe use our volitional entity labels and assign nosentiment directed towards the entity (the majoritycase).
This is a strong baseline to isolate how ourmethods perform specifically for the task of identi-fying sentiment targeted at an entity.We report on precision, recall, and sensitivity forthe tasks of NER and targeted subjectivity/sentimentprediction in isolation; and we report on accuracyfor the targeted subjectivity and targeted sentimentmodels.
For sentiment, a true positive is an instancewhere the label has sentiment, and a true negative isan instance where the label has no sentiment (neu-tral).
For NER, a true positive is an instance wherethe label is a B- or I- label; a true negative is aninstance where the label is O.
The three systemsare evaluated against one another for NER, subjec-tivity (entity has/does not have sentiment expressedtowards it), and sentiment (positive/negative/no sen-timent) using paired t-tests across folds, with a Bon-ferroni correction to set ?
to 0.02.NER We include results for the isolated task of vo-litional named entity recognition in Table 6.
In bothSpanish and English, all three models are roughlycomparable for precision, recall, and specificity.
Thetask of finding O tags ?
spans that are not named en-tities ?
works especially well (NE spec).
Common1649Spanish EnglishModel Joint Pipe Coll Joint Pipe CollNE prec 65.2 64.3 65.1 59.8 62.3 60.5NE rec 65.8 64.7 61.2 60.2 57.2 56.5NE spec 95.4 95.2 95.6 94.3 95.1 94.7Table 6: Average precision, recall, and specificity for vo-litional entity NER (in %).mistakes include confusing B- labels with I- labels.Subjectivity and Sentiment Table 7 shows resultsfor the isolated task of predicting the presence ofsentiment about a volitional entity.
In Spanish, thepipeline models (PIPE) perform optimally for sub-jectivity recall (Subj rec), and significantly abovethe COLL models (p<.001).
Precision and speci-ficity are comparable across models.
In English asin Spanish, the collapsed model is particularly poorat subjectivity recall.As discussed in Section 2, the subtask of predict-ing whether subjectivity is expressed towards an en-tity is comparable to the main task of Jiang et al(2011), and so we compare our approach here.
TheJiang et alstudy is similar to the current study in thatthey aim to detect targeted sentiment, but it differsfrom the current study in that they focus exclusivelyon subjectivity towards five manually selected enti-ties: {Obama, Google, iPad, Lakers, Lady Gaga}.They also evaluate on artificially balanced evalu-ation data, and evaluate sentiment polarity (posi-tive/negative) separately from subjectivity (has/doesnot have sentiment).Our dataset includes any entity labeled as PERSONor ORGANIZATION, and is not balanced (most tar-gets have no sentiment expressed towards them; seeTable 1), thus we can only roughly compare againsttheir approach.
Lakers and Lady Gaga are rare inour collection (appearing less than 3 times), and sowe updated the comparison set prior to evaluation to:{Obama, Google, iPad, BBC, Tebow}.
On this set, abaseline that always guesses no sentiment reaches anaccuracy of 66.9%, compared to Jiang et als 65.5%accuracy on a balanced set (not strictly compara-ble, but provided for reference).
The JOINT mod-els reach an accuracy of 71.04% on this set, demon-strating this approach as potentially useful for topic-dependent targeted sentiment.Table 8 shows results for the task of predictingthe polarity of the sentiment expressed about an en-tity.
In Spanish, the PIPE models significantly out-Spanish EnglishModel Joint Pipe Coll Joint Pipe CollSubj prec 58.3 58.8 58.9 46.6 52.2 45.9Subj rec 40.1 50.9 19.1 44.5 48.5 16.4Subj spec 79.6 77.5 77.8 77.6 80.8 74.0Table 7: Average precision, recall, and specificity (in %)for subjectivity prediction (has/does not have sentiment)along the target entity.Spanish EnglishModel Joint Pipe Coll Joint Pipe CollSent prec 36.6 45.8 42.5 31.6 42.9 38.5Sent rec 38.0 40.6 15.5 36.6 34.8 9.7Sent spec 67.1 75.2 73.3 72.3 82.0 78.1Table 8: Average precision, recall, and specificity (in %)for sentiment prediction (positive/negative/no sentiment)along the target entity.perform the COLL models on sentiment recall, andthe JOINT models on sentiment precision (p<.01).In English, PIPE significantly outperforms JOINT onprecision (p<.001).Targeted Subjectivity and Targeted SentimentThe JOINT and PIPE models work reasonablywell for the isolated tasks of NER and subjectiv-ity/sentiment prediction.
We now examine resultsfor targeted subjectivity ?
labeling an entity and pre-dicting whether there is sentiment directed towardsit ?
in Table 9; and targeted sentiment ?
labeling anentity and predicting what the sentiment directed to-wards it is ?
in Table 10.We evaluate using two accuracy metrics: Acc-all,which measures the accuracy of the entire named en-tity span along with the sentiment span; and Acc-Bsent, which measures the accuracy of identifyingthe start of a named entity (B- labels) along withthe sentiment expressed towards it.
Acc-all primar-ily measures the correctness of O labels, while Acc-Bsent focuses on the beginning of named entities.For the targeted subjectivity task, our JOINT mod-els perform optimally in Spanish, and significantlyabove their baselines.
For the Acc-Bsent task, JOINTmodels perform best, significantly outperformingtheir baseline for subjectivity prediction.
In English,where our data is half the size, we do not see a statis-tically significant difference between the predictivemodels and the no sentiment baselines.For the targeted sentiment task, the JOINT mod-els again perform relatively well in Spanish (Table10), labeling volitional entities, predicting whetheror not there is sentiment targeted towards them, and1650Model Joint JointBasePipe PipeBaseColl CollBaseSpa Acc-all 89.5* 89.3 89.3** 89.1 89.5* 89.3Acc-Bsent 32.1*** 29.5 30.9*** 28.3 30.1** 28.1Eng Acc-all 88.0 88.1 88.6 88.6 87.9 88.1Acc-Bsent 30.4 30.8 30.7 30.3 28.1 29.2***p<.001 **p<.01 *p<.05Table 9: Average accuracy on Targeted Subjectivity Pre-diction: Identifying volitional entities and whether theyare a sentiment target.
In the core task, Acc-Bsent, thebest model in Spanish is JOINT, significantly outperform-ing the baseline.
In English, the best model (PIPE) doesnot significantly improve over its baseline.Model Joint JointBasePipe PipeBaseColl CollBaseSpa Acc-all 89.4 89.4 89.0 89.0 89.2 89.3Acc-Bsent 29.7* 29.0 30.0 29.2 28.9 29.0Eng Acc-all 88.0 88.1 88.2 88.4 87.7 88.1Acc-Bsent 30.4 30.6 30.5 30.8 27.9 29.8*p<.05Table 10: Average accuracy on Targeted Sentiment Pre-diction: Identifying volitional entities and the polarityof the sentiment expressed towards them.
The SpanishJOINT models significantly improve over their baselinefor the core task.
In English, no models outperform theirbaseline.the sentiment polarity above their no sentiment base-lines.
We find this to be the most difficult task: Itmay be clear that sentiment is being expressed to-wards an entity, but it is not always clear what thepolarity of that sentiment is.
Error analysis is givenbelow in this section.
In the smaller English set, themodels do not outperform the no sentiment baseline.7 DiscussionFeature Analysis Examples of some of the top-weighted features in the Spanish models are shownin Table 11.
In addition to lexical identity and Browncluster, we find that positive indicators include pos-itive suffixes such as diminutive forms, whether theword can be syllabized (Section 5), and whether it isthree or four letters.Error Analysis Because it is relatively commonfor there not to be sentiment targeted at a named en-tity, it is difficult to tease out the polarity in instanceswhere there is targeted sentiment.
Similarly, our pre-dictions are most reliable for detecting the absenceof a named entity (O labels).Label confusions are shown in Table 12.
Mistakesare often made by confusing B- labels (the start ofB-VOLITIONAL FEATURESNegative is a function word; jerboa tags; followed by a wordwith 3 or 4 letters that cannot be syllabifiedPositive ends in -a, -o, or -s; is capitalized; has one non-initial capital letter; is 3 or 4 lettersB-VOLITIONAL, POS FEATURESNegative preceded by a curse word; followed by a wordwith a positive suffix; immediately preceded by aword with a negative prefixPositive not in a sentiment lexicon; preceded by a happyemoticon; followed by an exclamation or a ?my?word; immediately preceded by a laugh; has twoor more sentiment-bearing words in the sentenceB-VOLITIONAL, NEG FEATURESNegative is immediately followed by a question mark orpositive abbreviation wordPositive preceded by a ?bad?
word or curse word; has fouror more sentiment lexicon itemsB-VOLITIONAL, NOT-TARG FEATURESNegative immediately followed by a ?no?
word or word witha negative prefix; is preceded by a question mark;is immediately preceded by a curse word or laugh;is followed by an exclamation markPositive not followed by sentiment lexicon wordTable 11: Example strongly weighted features for aSpanish joint sentiment model.
In addition to lexicalidentity, we find that curse words and positive and neg-ative prefixes are used to detect volitional entities and thesentiment directed towards them.an entity) with I- labels (inside an entity); and bypredicting sentiment polarity when the gold annota-tions say there is not sentiment targeted at the entity.Some example errors are shown in Figure 13.
In(1), ?CANSADO?
(?TIRED?)
was predicted to bevolitional, while ?Matthew?
was not.
In (2), ?Ma-tias del r??o?
was not predicted to be an entity, likelydue to the fact that the capitalization patterns we seein this sentence are indicative of the start of a sen-tence rather than a proper name (similar to 1).
In (3),a.ObservedB I OPredicted B 423 21 186I 36 236 135O 197 90 7168b.ObservedPOS NEG NEUTPOS 68 24 42NEG 58 65 102NEUT 115 61 468Table 12: Predicted vs. observed values for a joint model.
(a) For named entities, most common confusions werebetween B-VOLITIONAL and O labels.
(b) For sentiment,most common mistakes were to predict that a positivesentiment was neutral (no sentiment), and that a neutralsentiment was negative.1651NE prediction errors1.Spanish: Cuando estoy CANSADO , e?l es mi DESCANSO .
Mateo .
11 : 29 .Predicted: O O B-VOLITIONAL O O O O O O O O O O O OGold: O O O O O O O O O B-VOLITIONAL O O O O OEnglish: When I?m TIRED , he is my REST .
Matthew .
11 : 29 .2.Spanish: Matias del r?
?o fue una lata .
.
.Predicted: O O O O O O .
.
.Gold: B-VOLITIONAL I-VOLITIONAL I-VOLITIONAL O O O .
.
.English: Matias del r?
?o was a drag .
.
.Sentiment prediction errors3.Spanish: Mario que dio este contigoPredicted: NOT-TARG - - - -Gold: POSITIVE - - - -English: Mario may God be with you4.Spanish: .
.
.
si de verdad estas en cielo , ayudame Superman !!
!Predicted: - - - - - - - - POSITIVE -Gold: - - - - - - - - NOT-TARG -English: .
.
.
if you really are in the skies , help me Superman !!
!Sentiment and NE prediction errors5.Spanish: Salen del gobierno de Humala dos connotados izquierdistas, Giesecke y EiguigurenPredicted:O O O O B-VOLITIONAL I-VOLITIONAL O O O B-VOLITIONAL O B-VOLITIONAL- - - - NOT-TARG NOT-TARG - - - NOT-TARG - NOT-TARGGold:O O O O B-VOLITIONAL O O O O B-VOLITIONAL O B-VOLITIONAL- - - - NOT-TARG - - - - NEGATIVE - NOT-TARGEnglish: Leaving the Humala government are two notorious leftists , Giesecke and EiguigurenTable 13: Example errors made by joint models.sentiment may not be clear without spelling correc-tion: ?dio?
should be ?dios?, meaning ?God?
; other-wise, ?dio?
is the word for ?gave?.
Humans can eas-ily fix the spelling error, which changes the overallreading of the expression.
In (4), the positive polar-ity item ?verdad?
(?believe?)
and the exclamationmarks (!!!)
were likely used as indicators of posi-tive sentiment; however, in this case the annotatorsmarked the targeted sentiment as neutral.
In (5), the?Humala?
entity was predicted to be longer than it is(?Hamala dos?
or ?Hamala two?).
It was also pre-dicted that both ?Giesecke?
and ?Eiguiguren?
hadno sentiment expressed towards them; annotatorsdisagreed, with the majority of those who annotated?Giesecke?
marking negative sentiment, and the ma-jority of those who annotated ?Eiguiguren?
mark-ing no sentiment.
This highlights some of the diffi-culty in predicting sentiment discussed in Section 3,where annotators will often disagree as to whetherthere is no sentiment or positive/negative sentiment.During development, we found that the collapsedmodel (COLL) performed best on small amounts ofdata.
However, as we scaled up the amount of datawe trained on, the PIPE and JOINT models signif-icantly improved, while the COLL models did nothave significant performance gains.8 ConclusionWe have introduced the task of open domain targetedsentiment: predicting sentiment directed towards anentity along with discovering the entity itself.
Ourapproach is developed to find targeted sentiment to-wards both person and organization named entitiesby modeling sentiment as a span along the entity.We find that by modeling targeted sentiment inthis way, we can reliably detect entities and whetheror not they are sentiment targets above a no senti-ment baseline.
How best to determine the polarityof the sentiment expressed towards the entity, how-ever, is still an open issue.
Our data suggests thatit is usually not clear-cut whether sentiment is beingexpressed or not; the strong disagreement betweenannotators suggests that detecting sentiment polar-ity in microblogs is difficult even for humans.In future work, we hope to explore further meth-ods for teasing apart sentiment polarity expressed to-wards a target.
This research has achieved promis-ing results for detecting sentiment targets without re-lying on external supervised models, and we hopethat the features and approaches developed here canaid in sentiment analysis in noisy text and languageswithout rich linguistic resources.1652ReferencesA.
Agarwal, B. Xie, I. Vovsha, O. Rambow, and R. Pas-sonneau.
2011.
Sentiment analysis of twitter data.
InProceedings of the Workshop on Language in SocialMedia.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.
InProceedings of the Seventh International Conferenceon Language Resources and Evaluation (LREC?10).Luciano Barbosa and Junlan Feng.
2010.
Robust senti-ment detection on Twitter from biased and noisy data.In Proceedings of Coling: Posters.Adam Bermingham and Alan F Smeaton.
2010.
Clas-sifying sentiment in microblogs: Is brevity an advan-tage?
In Proceedings of CIKM-2010.Albert Bifet and Eibe Frank.
2010.
Sentiment knowl-edge discovery in Twitter streaming data.
In Proceed-ings of the International Conference on Discovery Sci-ence (DS-2010).Juliette Blevins.
1996.
The syllable in phonological the-ory.
In John A. Goldswmith, editor, The Handbookof Phonological Theory.
Blackwell Publishing, Black-well Reference Online.N.
N. Bora.
2012.
Summarizing public opinions intweets.
In Proceedings of CICLing-2012.Samuel Brody and Nicholas Diakopoulos.
2011.Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!
: usingword lengthening to detect sentiment in microblogs.In Proceedings of EMNLP-2011.P.
F. Brown, V. J. Della Pietra, P. V. deSouza, J.C. Lai,and R.L.
Mercer.
1992.
Class-based n-gram mod-els of natural language.
Computational Linguistics,18(4):467?479.Pedro Henrique Calais Guerra, Adriano Veloso, WagnerMeira Jr, and Virg?
?lio Almeida.
2011.
From bias toopinion: a transfer-learning approach to real-time sen-timent analysis.
In Proceedings of the KDD-2011.Chris Callison-Burch and Mark Dredze.
2010.
Creatingspeech and language data with amazon?s mechanicalturk.
In Proceedings of the NAACL:HLT Workshop onCreating Speech and Language Data with Amazon?sMechanical Turk.Lu Chen, Wenbo Wang, Meenakshi Nagarajan, ShaojunWang, and Amit P. Sheth.
2012.
Extracting diversesentiment expressions with target-dependent polarityfrom twitter.
In Proceedings of ICWSM-2012.Yejin Choi, Eric Breck, and Claire Cardie.
2006.
Jointextraction of entities and relations for opinion recog-nition.
Proceedings of EMNLP 2006.G.
N. Clements.
1990.
The role of the sonority cycle incore syllabification.
In J. Kingston and M. Beckman,editors, Papers in Laboratory Phonology, pages 283?333.
CUP, Cambridge.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using Twitter hashtagsand smileys.
In Proceedings of Coling: Posters.Nicholas A Diakopoulos and David A Shamma.
2010.Characterizing debate performance via aggregatedtwitter sentiment.
In Proceedings of CHI-2010.David Etter, Francis Ferraro, Ryan Cotterell, OliviaBuzek, and Benjamin Van Durme.
2013.
Nerit:Named entity recognition for informal text.
Techni-cal Report 11, Human Language Technology Centerof Excellence, Johns Hopkins University, July.Jenny Rose Finkel and Christopher D. Manning.
2010.Hierarchical joint learning: Improving joint parsingand named entity recognition with non-jointly labeleddata.
In Proceedings of ACL-2010.Joan B. Hooper.
1976.
The syllable in phonological the-ory.
Language, 48(3):525?540.Minqing Hu and Bing Liu.
2004.
Mining and summariz-ing customer reviews.
In Proceedings of KDD.Xia Hu, Lei Tang, Jiliang Tang, and Huan Liu.
2013.
Ex-ploiting social relations for sentiment analysis in mi-croblogging.
In Proceedings of the 6th ACM Inter-national Conference on Web Search and Data Mining(WSDM-2013).Niklas Jakob and Iryna Gurevych.
2010.
Extractingopinion targets in a single-and cross-domain settingwith conditional random fields.
In Proceedings ofEMNLP.Long Jiang, Mo Yu, Xiaohua Liu, and Tiejun Zhao.
2011.Target-dependent twitter sentiment classification.
InProceedings of ACL-2011.Wei Jin and Hung Hay Ho.
2009.
A novel lexicalizedhmm-based learning framework for web opinion min-ing.
Proceedings of ICML 2009.Soo-Min Kim and Eduard Hovy.
2006.
Identifying andanalyzing judgment opinions.
Proceedings of NAACL2006.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Pro-ceedings of ACL/HLT.Michal Kosinskia, David Stillwell, and Thore Graepel.2013.
Private trains and attributes are predictable fromdigital records of human behavior.
Proc.
of the Na-tional Academy of Sciences of the USA, 110(5).Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter sentiment analysis: The goodthe bad and the OMG!
In Proceedings of ICWSM-2011.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedingsof ICML-2001.1653Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,Ying-Ju Xia, Shu Zhang, and Hao Yu.
2010a.Structure-aware review mining and summarization.Proceedings of Coling 2010.Guangxia Li, Steven CH Hoi, Kuiyu Chang, and RameshJain.
2010b.
Micro-blogging sentiment detectionby collaborative online learning.
In Proceedings ofICDM-2010.Hao Li, Yu Chen, Heng Ji, Smaranda Muresan, and De-quan Zheng.
2012.
Combining social cognitive theo-ries with linguistic features for multi-genre sentimentanalysis.
In Proceedings of the Pacific Asia Con-ference on Language, Information and Computation(PACLIC-2012).Andrew McCallum and Wei Li.
2003.
Early resultsfor named entity recognition with conditional randomfields, feature induction, and web-enhanced lexicons.In Proceedings of CoNLL-2003.Frida Morelli.
2003.
The relative harmony of /s+stop/onsets: Obstruent clusters and the sonority sequenc-ing principle.
In C. Fery and R. van de Vijver, edi-tors, The syllable in optimality theory, pages 356?371.CUP, New York.Alexander Pak and Patrick Paroubek.
2010.
Twitter as acorpus for sentiment analysis and opinion mining.
InProceedings of LREC-2010.James W. Pennebaker, Roger J. Booth, and Martha E.Francis.
2007.
Linguistic inquiry and word count:Liwc2007, operator?s manual.Veronica Perez-Rosas, Carmen Banea, and Rada Mihal-cea.
2012.
Learning sentiment lexicons in span-ish.
Proceedings of the Conference on Language Re-sources and Evaluations (LREC 2012).Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofColing:ACL-2006.Ana-Maria Popescu and Oren Etzioni.
2005.
Extractingproduct features and opinions from reviews.
In Pro-ceedings of HLT:EMNLP-2005.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.2011.
Opinion word expansion and target extractionthrough double propagation.
Computational Linguis-tics, 37(1).Hassan Saif, Yulan He, and Harith Alani.
2012.
Allevi-ating data sparsity for twitter sentiment analysis.
Pro-ceedings of the WWW Workshop on Making Sense ofMicroposts (# MSM2012).Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-son Baldridge.
2011.
Twitter polarity classificationwith label propagation over lexical links and the fol-lower graph.
In Proceedings of the EMNLP-2011Workshop on Unsupervised Learning in NLP.Veselin Stoyanov and Jason Eisner.
2012.
Minimum-risk training of approximate crf-based nlp systems.
InProceedings of NAACL:HLT-2012.Veselin Stoyanov, Alexander Ropson, and Jason Eis-ner.
2011.
Empirical risk minimization of graphi-cal model parameters given approximate inference, de-coding, and model structure.
In AIStats.Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, MingZhou, and Ping Li.
2011.
User-level sentiment anal-ysis incorporating social networks.
In Proceedings ofthe KDD-2011.Benjamin Van Durme.
2012.
Jerboa: A toolkit for ran-domized and streaming algorithms.
Technical report,Human Language Technology Center of Excellence,Johns Hopkins University.Svitlana Volkova, Theresa Wilson, and David Yarowsky.2013.
Exploring sentiment in social media: Boot-strapping subjectivity clues from multilingual twitterstreams.
In Association for Computational Linguistics(ACL).Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou, andMing Zhang.
2011.
Topic sentiment analysis in Twit-ter: A graph-based hashtag sentiment classification ap-proach.
In Proceedings of CIKM-2011.T.
Wilson, J. Wiebe, and P. Hoffmann.
2005.
Recogniz-ing contextual polarity in phrase-level sentiment anal-ysis.
In Proceedings of HLT-EMNLP.Theresa Wilson, Janyce Wiebe, and Paul Hoffman.
2009.Recognizing contextual polarity: An exploration offeatures for phrase-level sentiment analysis.
Compu-tational Linguistics, 35(3).Bishan Yang and Claire Cardie.
2013.
Joint inference forfine-grained opinion extraction.
Proceedings of ACL2013.Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, andWayne Niblack.
2003.
Sentiment analyzer: Ex-tracting sentiments about a given topic using naturallanguage processing techniques.
In Proceedings ofICDM-2003.1654
