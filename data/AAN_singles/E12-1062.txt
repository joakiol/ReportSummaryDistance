Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 603?612,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsLearning for Microblogs with Distant Supervision:Political Forecasting with TwitterMicol Marchetti-BowickMicrosoft Corporation475 Brannan StreetSan Francisco, CA 94122micolmb@microsoft.comNathanael ChambersDepartment of Computer ScienceUnited States Naval AcademyAnnapolis, MD 21409nchamber@usna.eduAbstractMicroblogging websites such as Twitteroffer a wealth of insight into a popu-lation?s current mood.
Automated ap-proaches to identify general sentiment to-ward a particular topic often perform twosteps: Topic Identification and SentimentAnalysis.
Topic Identification first identi-fies tweets that are relevant to a desiredtopic (e.g., a politician or event), and Sen-timent Analysis extracts each tweet?s atti-tude toward the topic.
Many techniques forTopic Identification simply involve select-ing tweets using a keyword search.
Here,we present an approach that instead usesdistant supervision to train a classifier onthe tweets returned by the search.
We showthat distant supervision leads to improvedperformance in the Topic Identification taskas well in the downstream Sentiment Anal-ysis stage.
We then use a system that incor-porates distant supervision into both stagesto analyze the sentiment toward PresidentObama expressed in a dataset of tweets.Our results better correlate with Gallup?sPresidential Job Approval polls than pre-vious work.
Finally, we discover a sur-prising baseline that outperforms previouswork without a Topic Identification stage.1 IntroductionSocial networks and blogs contain a wealth ofdata about how the general public views products,campaigns, events, and people.
Automated algo-rithms can use this data to provide instant feed-back on what people are saying about a topic.Two challenges in building such algorithms are(1) identifying topic-relevant posts, and (2) iden-tifying the attitude of each post toward the topic.This paper studies distant supervision (Mintz etal., 2009) as a solution to both challenges.
Weapply our approach to the problem of predictingPresidential Job Approval polls from Twitter data,and we present results that improve on previouswork in this area.
We also present a novel base-line that performs remarkably well without usingtopic identification.Topic identification is the task of identifyingtext that discusses a topic of interest.
Most pre-vious work on microblogs uses simple keywordsearches to find topic-relevant tweets on the as-sumption that short tweets do not need more so-phisticated processing.
For instance, searches forthe name ?Obama?
have been assumed to returna representative set of tweets about the U.S. Pres-ident (O?Connor et al 2010).
One of the maincontributions of this paper is to show that keywordsearch can lead to noisy results, and that the samekeywords can instead be used in a distantly super-vised framework to yield improved performance.Distant supervision uses noisy signals in textas positive labels to train classifiers.
For in-stance, the token ?Obama?
can be used to iden-tify a series of tweets that discuss U.S. PresidentBarack Obama.
Although searching for tokenmatches can return false positives, using the re-sulting tweets as positive training examples pro-vides supervision from a distance.
This paper ex-periments with several diverse sets of keywordsto train distantly supervised classifiers for topicidentification.
We evaluate each classifier on ahand-labeled dataset of political and apoliticaltweets, and demonstrate an improvement in F1score over simple keyword search (.39 to .90 inthe best case).
We also make available the first la-beled dataset for topic identification in politics toencourage future work.Sentiment analysis encompasses a broad fieldof research, but most microblog work focuseson two moods: positive and negative sentiment.603Algorithms to identify these moods range frommatching words in a sentiment lexicon to trainingclassifiers with a hand-labeled corpus.
Since la-beling corpora is expensive, recent work on Twit-ter uses emoticons (i.e., ASCII smiley faces suchas :-( and :-)) as noisy labels in tweets for distantsupervision (Pak and Paroubek, 2010; Davidov etal., 2010; Kouloumpis et al 2011).
This paperpresents new analysis of the downstream effectsof topic identification on sentiment classifiers andtheir application to political forecasting.Interest in measuring the political mood ofa country has recently grown (O?Connor et al2010; Tumasjan et al 2010; Gonzalez-Bailon etal., 2010; Carvalho et al 2011; Tan et al 2011).Here we compare our sentiment results to Presi-dential Job Approval polls and show that the sen-timent scores produced by our system are posi-tively correlated with both the Approval and Dis-approval job ratings.In this paper we present a method for cou-pling two distantly supervised algorithms fortopic identification and sentiment classification onTwitter.
In Section 4, we describe our approach totopic identification and present a new annotatedcorpus of political tweets for future study.
In Sec-tion 5, we apply distant supervision to sentimentanalysis.
Finally, Section 6 discusses our sys-tem?s performance on modeling Presidential JobApproval ratings from Twitter data.2 Previous WorkThe past several years have seen sentiment anal-ysis grow into a diverse research area.
The ideaof sentiment applied to microblogging domains isrelatively new, but there are numerous recent pub-lications on the subject.
Since this paper focuseson the microblog setting, we concentrate on thesecontributions here.The most straightforward approach to senti-ment analysis is using a sentiment lexicon to la-bel tweets based on how many sentiment wordsappear.
This approach tends to be used by appli-cations that measure the general mood of a popu-lation.
O?Connor et al(2010) use a ratio of posi-tive and negative word counts on Twitter, Kramer(2010) counts lexicon words on Facebook, andThelwall (2011) uses the publicly available Sen-tiStrength algorithm to make weighted counts ofkeywords based on predefined polarity strengths.In contrast to lexicons, many approaches in-stead focus on ways to train supervised classi-fiers.
However, labeled data is expensive to cre-ate, and examples of Twitter classifiers trained onhand-labeled data are few (Jiang et al 2011).
In-stead, distant supervision has grown in popular-ity.
These algorithms use emoticons to serve assemantic indicators for sentiment.
For instance,a sad face (e.g., :-() serves as a noisy label for anegative mood.
Read (2005) was the first to sug-gest emoticons for UseNet data, followed by Goet al(Go et al 2009) on Twitter, and many otherssince (Bifet and Frank, 2010; Pak and Paroubek,2010; Davidov et al 2010; Kouloumpis et al2011).
Hashtags (e.g., #cool and #happy) havealso been used as noisy sentiment labels (Davi-dov et al 2010; Kouloumpis et al 2011).
Fi-nally, multiple models can be blended into a sin-gle classifier (Barbosa and Feng, 2010).
Here, weadopt the emoticon algorithm for sentiment analy-sis, and evaluate it on a specific domain (politics).Topic identification in Twitter has receivedmuch less attention than sentiment analysis.
Themajority of approaches simply select a singlekeyword (e.g., ?Obama?)
to represent their topic(e.g., ?US President?)
and retrieve all tweets thatcontain the word (O?Connor et al 2010; Tumas-jan et al 2010; Tan et al 2011).
The underlyingassumption is that the keyword is precise, and dueto the vast number of tweets, the search will re-turn a large enough dataset to measure sentimenttoward that topic.
In this work, we instead usea distantly supervised system similar in spirit tothose recently applied to sentiment analysis.Finally, we evaluate the approaches presentedin this paper on the domain of politics.
Tumasjanet al(2010) showed that the results of a recentGerman election could be predicted through fre-quency counts with remarkable accuracy.
Mostsimilar to this paper is that of O?Connor et al(2010), in which tweets relating to PresidentObama are retrieved with a keyword search anda sentiment lexicon is used to measure overallapproval.
This extracted approval ratio is thencompared to Gallup?s Presidential Job Approvalpolling data.
We directly compare their resultswith various distantly supervised approaches.3 DatasetsThe experiments in this paper use seven months oftweets from Twitter (www.twitter.com) collected604between June 1, 2009 and December 31, 2009.The corpus contains over 476 million tweets la-beled with usernames and timestamps, collectedthrough Twitter?s ?spritzer?
API without keywordfiltering.
Tweets are aligned with polling data inSection 6 using their timestamps.The full system is evaluated against the pub-licly available daily Presidential Job Approvalpolling data from Gallup1.
Every day, Gallup asks1,500 adults in the United States about whetherthey approve or disapprove of ?the job Presi-dent Obama is doing as president.?
The resultsare compiled into two trend lines for Approvaland Disapproval ratings, as shown in Figure 1.We compare our positive and negative sentimentscores against these two trends.4 Topic IdentificationThis section addresses the task of Topic Identi-fication in the context of microblogs.
While thegeneral field of topic identification is broad, itsuse on microblogs has been somewhat limited.Previous work on the political domain simply useskeywords to identify topic-specific tweets (e.g.,O?Connor et al(2010) use ?Obama?
to find pres-idential tweets).
This section shows that distantsupervision can use the same keywords to build aclassifier that is much more robust to noise thanapproaches that use pure keyword search.4.1 Distant SupervisionDistant supervision uses noisy signals to identifypositive examples of a topic in the face of unla-beled data.
As described in Section 2, recent sen-timent analysis work has applied distant supervi-sion using emoticons as the signals.
The approachextracts tweets with ASCII smiley faces (e.g., :)and ;)) and builds classifiers trained on these pos-itive examples.
We apply distant supervision totopic identification and evaluate its effectivenesson this subtask.As with sentiment analysis, we need to collectpositive and negative examples of tweets aboutthe target topic.
Instead of emoticons, we extractpositive tweets containing one or more predefinedkeywords.
Negative tweets are randomly chosenfrom the corpus.
Examples of positive and neg-ative tweets that can be used to train a classifierbased on the keyword ?Obama?
are given here:1http://gallup.com/poll/113980/gallup-daily-obama-job-approval.aspxID Type KeywordsPC-1 Obama obamaPC-2 General republican, democrat, senate,congress, governmentPC-3 Topic health care, economy, tax cuts,tea party, bailout, sotomayorPC-4 Politician obama, biden, mccain, reed,pelosi, clinton, palinPC-5 Ideology liberal, conservative, progres-sive, socialist, capitalistTable 1: The keywords used to select positive trainingsets for each political classifier (a subset of all PC-3and PC-5 keywords are shown to conserve space).positive: LOL, obama made a bears refer-ence in green bay.
uh oh.negative: New blog up!
It regards the newiPhone 3G S: <URL>We then use these automatically extracteddatasets to train a multinomial Naive Bayes classi-fier.
Before feature collection, the text is normal-ized as follows: (a) all links to photos (twitpics)are replaced with a single generic token, (b) allnon-twitpic URLs are replaced with a token, (c)all user references (e.g., @MyFriendBob) are col-lapsed, (d) all numbers are collapsed to INT, (e)tokens containing the same letter twice or morein a row are condensed to a two-letter string (e.g.the word ahhhhh becomes ahh), (f) lowercase thetext and insert spaces between words and punctu-ation.
The text of each tweet is then tokenized,and the tokens are used to collect unigram and bi-gram features.
All features that occur fewer than10 times in the training corpus are ignored.Finally, after training a classifier on this dataset,every tweet in the corpus is classified as eitherpositive (i.e., relevant to the topic) or negative(i.e., irrelevant).
The positive tweets are then sentto the second sentiment analysis stage.4.2 Keyword SelectionKeywords are the input to our proposed distantlysupervised system, and of course, the input to pre-vious work that relies on keyword search.
Weevaluate classifiers based on different keywords tomeasure the effects of keyword selection.O?Connor et al(2010) used the keywords?Obama?
and ?McCain?, and Tumasjan et al(2010) simply extracted tweets containing Ger-many?s political party names.
Both approachesextracted matching tweets, considered them rele-605Gallup Daily Obama Job Approval RatingsFigure 1: Gallup presidential job Approval and Disapproval ratings measured between June and Dec 2009.vant (correctly, in many cases), and applied sen-timent analysis.
However, different keywordsmay result in very different extractions.
We in-stead attempted to build a generic ?political?
topicclassifier.
To do this, we experimented with thefive different sets of keywords shown in Table 1.For each set, we extracted all tweets matchingone or more keywords, and created a balancedpositive/negative training set by then selectingnegative examples randomly from non-matchingtweets.
A couple examples of ideology (PC-5) ex-tractions are shown here:You often hear of deontologist libertariansand utilitarian liberals but are there anyAristotelian socialists?<url> - Then, slather on a liberal amountof plaster, sand down smooth, and painthowever you want.
I hope this helps!The second tweet is an example of the noisynature of keyword extraction.
Most extractionsare accurate, but different keywords retrieve verydifferent sets of tweets.
Examples for the politicaltopics (PC-3) are shown here:RT @PoliticalMath: hope the president?shealth care predictions <url> are betterthan his stimulus predictions <url>@adamjschmidt You mean we could havechosen health care for every man womanand child in America or the Iraq war?Each keyword set builds a classifier using the ap-proach described in Section 4.1.4.3 Labeled DatasetsIn order to evaluate distant supervision againstkeyword search, we created two new labeleddatasets of political and apolitical tweets.The Political Dataset is an amalgamation of allfour keyword extractions (PC-1 is a subset of PC-4) listed in Table 1.
It consists of 2,000 tweets ran-domly chosen from the keyword searches of PC-2, PC-3, PC-4, and PC-5 with 500 tweets fromeach.
This combined dataset enables an evalua-tion of how well each classifier can identify tweetsfrom other classifiers.
The General Dataset con-tains 2,000 random tweets from the entire corpus.This dataset alws us to evaluate how well clas-sifiers identify political tweets in the wild.This paper?s authors initially annotated thesame 200 tweets in the General Dataset to com-pute inter-annotator agreement.
The Kappa was0.66, which is typically considered good agree-ment.
Most disagreements occurred over tweetsabout money and the economy.
We then split theremaining portions of the two datasets betweenthe two annotators.
The Political Dataset con-tains 1,691 political and 309 apolitical tweets, andthe General Dataset contains 28 political tweetsand 1,978 apolitical tweets.
These two datasets of2000 tweets each are publicly available for futureevaluation and comparison to this work2.4.4 ExperimentsOur first experiment addresses the question ofkeyword variance.
We measure performance onthe Political Dataset, a combination of all of ourproposed political keywords.
Each keyword setcontributed to 25% of the dataset, so the eval-uation measures the extent to which a classifieridentifies other keyword tweets.
We classifiedthe 2000 tweets with the five distantly supervisedclassifiers and the one ?Obama?
keyword extrac-tor from O?Connor et al(2010).Results are shown on the left side of Figure 2.Precision and recall calculate correct identifica-tion of the political label.
The five distantly super-vised approaches perform similarly, and show re-markable robustness despite their different train-ing sets.
In contrast, the keyword extractor only2http://www.usna.edu/cs/nchamber/data/twitter606Figure 2: Five distantly supervised classifiers and the Obama keyword classifier.
Left panel: the Political Datasetof political tweets.
Right panel: the General Dataset representative of Twitter as a whole.captures about a quarter of the political tweets.PC-1 is the distantly supervised analog to theObama keyword extractor, and we see that dis-tant supervision increases its F1 score dramati-cally from 0.39 to 0.90.The second evaluation addresses the questionof classifier performance on Twitter as a whole,not just on a political dataset.
We evaluate on theGeneral Dataset just as on the Political Dataset.Results are shown on the right side of Figure 2.Most tweets posted to Twitter are not about pol-itics, so the apolitical label dominates this morerepresentative dataset.
Again, the five distantsupervision classifiers have similar results.
TheObama keyword search has the highest precision,but drastically sacrifices recall.
Four of the fiveclassifiers outperform keyword search in F1 score.4.5 DiscussionThe Political Dataset results show that distant su-pervision adds robustness to a keyword search.The distantly supervised ?Obama?
classifier (PC-1) improved the basic ?Obama?
keyword searchby 0.51 absolute F1 points.
Furthermore, dis-tant supervision doesn?t require additional humaninput, but simply adds a trained classifier.
Twoexample tweets that an Obama keyword searchmisses but that its distantly supervised analogcaptures are shown here:Why does Congress get to opt out of theObummercare and we can?t.
A companygets fined if they don?t comply.
Kiss free-dom goodbye.I agree with the lady from california, I amsixty six years old and for the first time inmy life I am ashamed of our government.These results also illustrate that distant supervi-sion allows for flexibility in construction of theclassifier.
Different keywords show little changein classifier performance.The General Dataset experiment evaluates clas-sifier performance in the wild.
The keyword ap-proach again scores below those trained on noisylabels.
It classifies most tweets as apolitical andthus achieves very low recall for tweets that areactually about politics.
On the other hand, distantsupervision creates classifiers that over-extractpolitical tweets.
This is a result of using balanceddatasets in training; such effects can be mitigatedby changing the training balance.
Even so, fourof the five distantly trained classifiers score higherthan the raw keyword approach.
The only under-performer was PC-1, which suggests that whenbuilding a classifier for a relatively broad topiclike politics, a variety of keywords is important.The next section takes the output from our clas-sifiers (i.e., our topic-relevant tweets) and eval-uates a fully automated sentiment analysis algo-rithm against real-world polling data.5 Targeted Sentiment AnalysisThe previous section evaluated algorithms thatextract topic-relevant tweets.
We now evaluatemethods to distill the overall sentiment that theyexpress.
This section compares two common ap-proaches to sentiment analysis.We first replicated the technique used inO?Connor et al(2010), in which a lexicon of pos-itive and negative sentiment words called Opin-607ionFinder (Wilson and Hoffmann, 2005) is usedto evaluate the sentiment of each tweet (othershave used similar lexicons (Kramer, 2010; Thel-wall et al 2010)).
We evaluate our full distantlysupervised approach to theirs.
We also experi-mented with SentiStrength, a lexicon-based pro-gram built to identify sentiment in online com-ments of the social media website, MySpace.Though MySpace is close in genre to Twitter, wedid not observe a performance gain.
All reportedresults thus use OpinionFinder to facilitate a moreaccurate comparison with previous work.Second, we built a distantly supervised systemusing tweets containing emoticons as done in pre-vious work (Read, 2005; Go et al 2009; Bifet andFrank, 2010; Pak and Paroubek, 2010; Davidovet al 2010; Kouloumpis et al 2011).
Althoughdistant supervision has previously been shown tooutperform sentiment lexicons, these evaluationsdo not consider the extra topic identification step.5.1 Sentiment LexiconThe OpinionFinder lexicon is a list of 2,304 pos-itive and 4,151 negative sentiment terms (Wilsonand Hoffmann, 2005).
We ignore neutral wordsin the lexicon and we do not differentiate betweenweak and strong sentiment words.
A tweet is la-beled positive if it contains any positive terms, andnegative if it contains any negative terms.
A tweetcan be marked as both positive and negative, andif a tweet contains words in neither category, itis marked neutral.
This procedure is the same asused by O?Connor et al(2010).
The sentimentscores Spos and Sneg for a given set of N tweetsare calculated as follows:Spos =?x 1{xlabel = positive}N(1)Spos =?x 1{xlabel = negative}N(2)where 1{xlabel = positive} is 1 if the tweet x islabeled positive, and N is the number of tweets inthe corpus.
For the sake of comparison, we alsocalculate a sentiment ratio as done in O?Connoret al(2010):Sratio =?x 1{xlabel = positive}?x 1{xlabel = negative}(3)5.2 Distant SupervisionTo build a trained classifier, we automatically gen-erated a positive training set by searching fortweets that contain at least one positive emoti-con and no negative emoticons.
We generated anegative training set using an analogous process.The emoticon symbols used for positive sentimentwere :) =) :-) :] =] :-] :} :o) :D =D :-D :P =P:-P C:.
Negative emoticons were :( =( :-( :[ =[:-[ :{ :-c :c} D: D= :S :/ =/ :-/ :?
( : (.
Using thisdata, we train a multinomial Naive Bayes classi-fier using the same method used for the politicalclassifiers described in Section 4.1.
This classifieris then used to label topic-specific tweets as ex-pressing positive or negative sentiment.
Finally,the three overall sentiment scores Spos, Sneg, andSratio are calculated from the results.6 Predicting Approval PollsThis section uses the two-stage Targeted Senti-ment Analysis system described above in a real-world setting.
We analyze the sentiment of Twit-ter users toward U.S. President Barack Obama.This allows us to both evaluate distant supervisionagainst previous work on the topic, and demon-strate a practical application of the approach.6.1 Experiment SetupThe following experiments combine both topicidentification and sentiment analysis.
The previ-ous sections described six topic identification ap-proaches, and two sentiment analysis approaches.We evaluate all combinations of these systems,and compare their final sentiment scores for eachday in the nearly seven-month period over whichour dataset spans.Gallup?s Daily Job Approval reports two num-bers: Approval and Disapproval.
We calculate in-dividual sentiment scores Spos and Sneg for eachday, and compare the two sets of trends usingPearson?s correlation coefficient.
O?Connor et aldo not explicitly evaluate these two, but insteaduse the ratio Sratio.
We also calculate this dailyratio from Gallup for comparison purposes by di-viding the Approval by the Disapproval.6.2 Results and DiscussionThe first set of results uses the lexicon-based clas-sifier for sentiment analysis and compares the dif-ferent topic identification approaches.
The firsttable in Table 2 reports Pearson?s correlation co-efficient with Gallup?s Approval and Disapprovalratings.
Regardless of the Topic classifier, all608Sentiment LexiconTopic Classifier Approval Disapprovalkeyword -0.22 0.42PC-1 -0.65 0.71PC-2 -0.61 0.71PC-3 -0.51 0.65PC-4 -0.49 0.60PC-5 -0.65 0.74Distantly Supervised SentimentTopic Classifier Approval Disapprovalkeyword 0.27 0.38PC-1 0.71 0.73PC-2 0.33 0.46PC-3 0.05 0.31PC-4 0.08 0.26PC-5 0.54 0.62Table 2: Correlation between Gallup polling data andthe extracted sentiment with a lexicon (trends shownin Figure 3) and distant supervision (Figure 4).Sentiment Lexiconkeyword PC-1 PC-2 PC-3 PC-4 PC-5.22 .63 .46 .33 .27 .61Distantly Supervised Sentimentkeyword PC-1 PC-2 PC-3 PC-4 PC-5.40 .64 .46 .30 .28 .60Table 3: Correlation between Gallup Approval / Dis-approval ratio and extracted sentiment ratio scores.systems inversely correlate with Presidential Ap-proval.
However, they correlate well with Dis-approval.
Figure 3 graphically shows the trendlines for the keyword and the distantly supervisedsystem PC-1.
The visualization illustrates howthe keyword-based approach is highly influencedby day-by-day changes, whereas PC-1 displays amuch smoother trend.The second set of results uses distant supervi-sion for sentiment analysis and again varies thetopic identification approach.
The second tablein Table 2 gives the correlation numbers and Fig-ure 4 shows the keyword and PC-1 trend lines.Theresults are widely better than when a lexicon isused for sentiment analysis.
Approval is no longerinversely correlated, and two of the distantly su-pervised systems strongly correlate (PC-1, PC-5).The best performing system (PC-1) used dis-tant supervision for both topic identification andsentiment analysis.
Pearson?s correlation coeffi-cient for this approach is 0.71 with Approval and0.73 with Disapproval.Finally, we compute the ratio Sratio betweenthe positive and negative sentiment scores (Equa-tion 3) to compare to O?Connor et al(2010).
Ta-ble 3 shows the results.
The distantly supervisedtopic identification algorithms show little changebetween a sentiment lexicon or a classifier.
How-ever, O?Connor et als keyword approach im-proves when used with a distantly supervised sen-timent classifier (.22 to .40).
Merging Approvaland Disapproval into one ratio appears to maskthe sentiment lexicon?s poor correlation with Ap-proval.
The ratio may not be an ideal evalua-tion metric for this reason.
Real-world interest inPresidential Approval ratings desire separate Ap-proval and Disapproval scores, as Gallup reports.Our results (Table 2) show that distant supervi-sion avoids a negative correlation with Approval,but the ratio hides this important advantage.One reason the ratio may mask the negativeApproval correlation is because tweets are oftenclassified as both positive and negative by a lexi-con (Section 5.1).
This could explain the behav-ior seen in Figure 3 in which both the positive andnegative sentiment scores rise over time.
How-ever, further experimentation did not rectify thispattern.
We revised Spos and Sneg to make binarydecisions for a lexicon: a tweet is labeled posi-tive if it strictly contains more positive words thannegative (and vice versa).
Correlation showed lit-tle change.
Approval was still negatively corre-lated, Disapproval positive (although less so inboth), and the ratio scores actually dropped fur-ther.
The sentiment ratio continued to hide thepoor Approval performance by a lexicon.6.3 New Baseline: Topic-Neutral SentimentDistant supervision for sentiment analysis outper-forms that with a sentiment lexicon (Table 2).Distant supervision for topic identification furtherimproves the results (PC-1 v. keyword).
Thebest system uses distant supervision in both stages(PC-1 with distantly supervised sentiment), out-performing the purely keyword-based algorithmof O?Connor et al(2010).
However, the questionof how important topic identification is has not yetbeen addressed here or in the literature.Both O?Connor et al(2010) and Tumasjan etal.
(2010) created joint systems with two topicidentification and sentiment analysis stages.
But609Sentiment LexiconFigure 3: Presidential job approval and disapproval calculated using two different topic identification techniques,and using a sentiment lexicon for sentiment analysis.
Gallup polling results are shown in black.Distantly Supervised SentimentFigure 4: Presidential job approval sentiment scores calculated using two different topic identification techniques,and using the emoticon classifier for sentiment analysis.
Gallup polling results are shown in black.Topic-Neutral SentimentFigure 5: Presidential job approval sentiment scores calculated using the entire twitter corpus, with two differenttechniques for sentiment analysis.
Gallup polling results are shown in black for comparison.610Topic-Neutral SentimentAlgorithm Approval DisapprovalDistant Sup.
0.69 0.74Keyword Lexicon -0.63 0.69Table 4: Pearson?s correlation coefficient of SentimentAnalysis without Topic Identification.what if the topic identification step were removedand sentiment analysis instead run on the entireTwitter corpus?
To answer this question, weran the distantly supervised emoticon classifier toclassify all tweets in the 7 months of Twitter data.For each day, we computed the positive and neg-ative sentiment scores as above.
The evaluation isidentical, except for the removal of topic identifi-cation.
Correlation results are shown in Table 4.This baseline parallels the results seen whentopic identification is used: the sentiment lexi-con is again inversely correlated with Approval,and distant supervision outperforms the lexiconapproach in both ratings.
This is not surpris-ing given previous distantly supervised work onsentiment analysis (Go et al 2009; Davidov etal., 2010; Kouloumpis et al 2011).
However,our distant supervision also performs as well asthe best performing topic-specific system.
Thebest performing topic classifier, PC-1, correlatedwith Approval with r=0.71 (0.69 here) and Dis-approval with r=0.73 (0.74 here).
Computingoverall sentiment on Twitter performs as well aspolitical-specific sentiment.
This unintuitive re-sult suggests a new baseline that all topic-basedsystems should compute.7 DiscussionThis paper introduces a new methodology forgleaning topic-specific sentiment information.We highlight four main contributions here.First, this work is one of the first to evaluatedistant supervision for topic identification.
Allfive political classifiers outperformed the lexicon-driven keyword equivalent that has been widelyused in the past.
Our model achieved .90 F1 com-pared to the keyword .39 F1 on our political tweetdataset.
On twitter as a whole, distant supervisionincreased F1 by over 100%.
The results also sug-gest that performance is relatively insensitive tothe specific choice of seed keywords that are usedto select the training set for the political classifier.Second, the sentiment analysis experimentsbuild upon what has recently been shown in theliterature: distant supervision with emoticons isa valuable methodology.
We also expand uponprior work by discovering drastic performancedifferences between positive and negative lexi-con words.
The OpinionFinder lexicon failedto correlate (inversely) with Gallup?s Approvalpolls, whereas a distantly trained classifier cor-related strongly with both Approval and Disap-proval (Pearson?s .71 and .73).
We only testedOpinionFinder and SentiStrength, so it is possiblethat another lexicon might perform better.
How-ever, our results suggest that lexicons vary in theirquality across sentiment, and distant supervisionmay provide more robustness.Third, our results outperform previous work onPresidential Job Approval prediction (O?Connoret al 2010).
We presented two novel approachesto the domain: a coupled distantly supervised sys-tem, and a topic-neutral baseline, both of whichoutperform previous results.
In fact, the baselinesurprisingly matches or outperforms the more so-phisticated approaches that use topic identifica-tion.
The baseline correlates .69 with Approvaland .74 with Disapproval.
This suggests a newbaseline that should be used in all topic-specificsentiment applications.Fourth, we described and made available twonew annotated datasets of political tweets to facil-itate future work in this area.Finally, Twitter users are not a representativesample of the U.S. population, yet the high corre-lation between political sentiment on Twitter andGallup ratings makes these results all the moreintriguing for polling methodologies.
Our spe-cific 7-month period of time differs from previouswork, and thus we hesitate to draw strong con-clusions from our comparisons or to extend im-plications to non-political domains.
Future workshould further investigate distant supervision as atool to assist topic detection in microblogs.AcknowledgmentsWe thank Jure Leskovec for the Twitter data,Brendan O?Connor for open and frank correspon-dence, and the reviewers for helpful suggestions.611ReferencesLuciano Barbosa and Junlan Feng.
2010.
Robust sen-timent detection on twitter from biased and noisydata.
In Proceedings of the 23rd InternationalConference on Computational Linguistics (COL-ING 2010).Albert Bifet and Eibe Frank.
2010.
Sentiment knowl-edge discovery in twitter streaming data.
In LectureNotes in Computer Science, volume 6332, pages 1?15.Paula Carvalho, Luis Sarmento, Jorge Teixeira, andMario J. Silva.
2011.
Liars and saviors in a senti-ment annotated corpus of comments to political de-bates.
In Proceedings of the Association for Com-putational Linguistics (ACL-2011), pages 564?568.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using twitter hashtagsand smileys.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics(COLING 2010).Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervi-sion.
Technical report.Sandra Gonzalez-Bailon, Rafael E. Banchs, and An-dreas Kaltenbrunner.
2010.
Emotional reactionsand the pulse of public opinion: Measuring the im-pact of political events on the sentiment of onlinediscussions.
Technical report.Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, andTiejun Zhao.
2011.
Target-dependent twitter sen-timent classification.
In Proceedings of the Associ-ation for Computational Linguistics (ACL-2011).Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter sentiment analysis: The goodthe bad and the omg!
In Proceedings of the FifthInternational AAAI Conference on Weblogs and So-cial Media.Adam D. I. Kramer.
2010.
An unobtrusive behavioralmodel of ?gross national happiness?.
In Proceed-ings of the 28th International Conference on HumanFactors in Computing Systems (CHI 2010).Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, ACL?09, pages 1003?1011.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.From tweets to polls: Linking text sentiment topublic opinion time series.
In Proceedings of theAAAI Conference on Weblogs and Social Media.Alexander Pak and Patrick Paroubek.
2010.
Twitteras a corpus for sentiment analysis and opinion min-ing.
In Proceedings of the Seventh InternationalConference On Language Resources and Evalua-tion (LREC).Jonathon Read.
2005.
Using emoticons to reduce de-pendency in machine learning techniques for senti-ment classification.
In Proceedings of the ACL Stu-dent Research Workshop (ACL-2005).Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, MingZhou, and Ping Li.
2011.
User-level sentimentanalysis incorporating social networks.
In Pro-ceedings of the 17th ACM SIGKDD Conference onKnowledge Discovery and Data Mining.Mike Thelwall, Kevan Buckley, Georgios Paltoglou,Di Cai, and Arvid Kappas.
2010.
Sentimentstrength detection in short informal text.
Journal ofthe American Society for Information Science andTechnology, 61(12):2544?2558.Mike Thelwall, Kevan Buckley, and Georgios Pal-toglou.
2011.
Sentiment in twitter events.
Jour-nal of the American Society for Information Scienceand Technology, 62(2):406?418.Andranik Tumasjan, Timm O. Sprenger, Philipp G.Sandner, and Isabell M. Welpe.
2010.
Electionforecasts with twitter: How 140 characters reflectthe political landscape.
Social Science ComputerReview.J.
; Wilson, T.; Wiebe and P. Hoffmann.
2005.
Recog-nizing contextual polarity in phrase-level sentimentanalysis.
In Proceedings of the Conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing.612
