DESCRIPTION OF THE NTU SYSTEM USED FOR MET2Hsin-Hsi Chen, Yung-Wei Ding, Shih-Chung Tsai and Guo-Wei BianNatural Language Processing LaboratoryDepartment of Computer Science and Information EngineeringNational Taiwan UniversityTaipei, TAIWANE-mail: hh_chen@csie.ntu.edu.twFax: +886-2-23628167ABSTRACTNamed entities form the major components in a document.
When we catch the fundamental entities, wecan understand a document to some degree.
This paper employs different types of information from differentlevels of text to extract named entities, including character conditions, statistic information, titles, punctuationmarks, organization and location keywords, speech-act and locative verbs, cache and n-gram model.
In theformal run of MET-2, the F-measures P&R, 2P&R and P&2R are 79.61%, 77.88% and 81.42%, respectively.INTRODUCTIONPeople, affairs, time, places and things are five basic entities in a document.
When we catch thefundamental entities, we can understand a document to some degree.
Natural Language Processing Laboratory(NLPL) in Department of Computer Science and Information Engineering (CSIE), National Taiwan University(NTU) starts to study named entity extraction problem in 1993.
At first, we focus on the extraction of Chineseperson names, transliterated person names [1] and organization names [2].
The training data and the testingdata in these experiments are selected from three Taiwan newspaper corpora (China Times, Liberty Times Newsand United Daily News).
Chen and Lee [3] reported the precision rate and the recall rate for the extraction ofChinese person names, transliterated person names and organization names are (88.04%, 92.56%), (50.62%,71.93%) and (61.79%, 54.50%), respectively in the 16th International Conference on Computational Linguistics.We employ these results to several applications.
Chen and Wu [4] considered person names as one of clues insentence alignment.
Chen and Lee [3] show its application to anaphora resolution.
Chen and Bian [5]proposed a method to construct white pages for Internet/Intranet users automatically.
We extract informationfrom World Wide Web documents, including proper nouns, E-mail addresses and home page URLs, and findthe relationship among these data.
Chen, Ding and Tsai [6,7] dealt with proper noun extraction for informationretrieval.In MUC-7 and MET-2, we attend named entity extraction tasks for both English and Chinese.
Weextend our previous work on this problem to cover more named entity types such as locations, date/timeexpressions and monetary and percentage expressions.
Several issues have to be addressed during extension.One of the major differences between Chinese and English language processing is that segmentation is requiredfor Chinese.
That is, we have to identify word boundary in Chinese sentences beforehand.
That makesChinese named entity extraction tasks more changeable.
Besides, the vocabulary set and the Chinese codingset used in Taiwan and in China are not the same.
The documents adopted in MET-2 are selected fromnewspapers in China, thus we have to transform simplified Chinese characters in GB coding set to traditionalChinese characters in Big-5 coding set before testing.
A word that is known may become unknown due totransformation.
For example, the character "?"
in "??"
(early morning) is used in traditional Chinesecharacters.
However, "D" is used in simplified Chinese characters and it is also a legal traditional Chinesecharacter that denotes another meaning.
In other words, the mapping from GB to Big5 is "?D", which is anunknown word based on our dictionary.
The different vocabulary set between China and Taiwan results indifferent segmentation.This paper is organized as follows.
Section 2 illustrates the flow of named entity extraction and thesummary scores of our team in MET-2 formal run.
Sections 3, 4 and 5 propose methods to extract namedpeople, organizations and locations.
Section 6 deals with the rest of named entities, i.e., date/time expressionsand monetary and percentage expressions.
After each section, we discuss the sources of errors in the formalrun.
Section 7 concludes the remarks.FLOW OF NAMED ENTITY EXTRACTIONThe following shows the flow of named entity extraction in MET-2 formal run.
(1) Transform Chinese texts in GB codes into texts in Big-5 codes.
(2) Segment Chinese texts into a sequence of tokens.2(3) Identify named people.
(4) Identify named organizations.
(5) Identify named locations.
(6) Use n-gram model to identify named organizations/locations.
(7) Identify the rest of named expressions.
(8) Transform the results in Big-5 codes into the results in GB codes.Steps (1) and (2) form the preprocessing of the named entity extraction tasks.
As mentioned in Section 1,Big-5 traditional character set and GB simplified character set are adopted in Taiwan and in China, respectively.Our system is developed on the basis of Big-5 codes, so that the transformation of the official documents inMET-2 into the documents in terms of Big-5 codes is necessary.
Characters used both in simplified characterset and tradition character set alays result in error mapping.
For example,f vs.?, K?
vs. Kg,L vs.
?, N- vs.
N?, ??
vs.
?
?, W?y vs.
?
?y, j??
vs.
??
?, ?
?0vs.
?
?Q, ??
vs.
?
?, ?$ vs.
?
?, and so on.A Chinese sentence is composed of a sequence of characters without any word boundary.
Step (2) triesto identify words on the basis of a dictionary and segmentation strategies.
We list all the possible words bydictionary look-up, and then resolve ambiguities by segmentation strategies.
Our dictionary is trained fromCKIP corpus [8], of which articles are collected from Taiwan newspapers, magazines, and so on.
Thevocabulary used in MET-2 documents may be different from the vocabulary trained from Taiwan corpora, sothat more unknown words are introduced.
For example, "??
?t" vs.
"????
", "?F" vs.
"??
", "?B?"
vs.
"\B?
", "?\" vs. "?d\", and so on.
That will interfere with the named entity extractionbecause named entities are often unknown words too.Table 1 summarizes the results of MET-2 formal run of our team.
The F-measures in terms of P&R,2P&R, and P&2R are 79.61%, 77.88% and 81.42%, respectively.
The recall rate and the precision rate (objectscores) for the extraction of name, time and number expressions are (85%, 79%), (91%, 98%) and (95%, 85%),respectively.
We will discuss the major errors for each type of named entities.NAMED PEOPLE EXTRACTIONThe naming methods are totally different for Chinese person names and transliterated person names.
Thefollowing two subsections deal with each of them.Identification of Chinese Person NamesChinese person names are composed of surnames and names.
Most Chinese surnames are singlecharacter and some rare ones are two characters.
The following shows three different types:(1) Single character like '?
', ?
', '?'
and '?'.
(2) Two characters like '??'
and '??'.
(3) Two surnames together like '&?
'.Most names are two characters and some rare ones are single characters.
Theoretically, every character can beconsidered as names rather than a fixed set.
Thus the length of Chinese person names ranges from 2 to 6characters.Three kinds of recognition strategies are adopted:(1) name-formulation rules(2) context clues, e.g., titles, positions, speech-act verbs, and so on(3) cacheName-formulation rules form the baseline model.
It proposes possible candidates.
The context clues addextra scores to the candidates.
Cache records the occurrences of all the possible candidates in a paragraph.
Ifa candidate appears more than once, it has high tendency to be a person name.
The following illustrates eachstrategy in details.Name-formulation rules are trained from a person name corpus in Taiwan [9].
It contains 1 millionChinese person names.
Each contains surname, name and sex.
During training, we divide the corpus intotwo partitions according to sex of persons.
In our method, we postulate that the formulation of names isdifferent for male and female.
At first, we get 598 surnames from this 1M person name corpus, and thencompute the probabilities of these characters to be surnames.
Of these, surnames of very low frequency like "y", "N", etc., are removed from this set to avoid too much false alarms.
Only 541 surnames are left, and areused to trigger the person name identification system.
Next, the probability of a Chinese character to be thefirst character (the second character) of a name is computed for male and female, separately.3POS ACT COR PAR INC MIS SPU NON REC PRE UND OVG SUB ERRSUB TASK SCOREenamexorganization 377 344 293 0 7 77 44 0 78 85 20 13 2 30person 174 215 159 0 0 15 56 0 91 74 9 26 0 31location 750 842 583 0 65 102 194 0 78 69 14 23 10 38other 0 0 0 0 0 0 0 0 0 0 0 0 0 0timexdate 380 410 359 0 0 21 51 0 94 88 6 12 0 17time 43 60 42 0 0 1 18 0 98 70 2 30 0 31other 0 0 0 0 0 0 0 0 0 0 0 0 0 0numexmoney 52 52 51 0 0 1 1 0 98 98 2 2 0 4percent 47 40 39 0 0 8 1 0 83 98 17 3 0 19other 0 0 0 0 0 0 0 0 0 0 0 0 0 0SECT SCOREDOC 0 0 0 0 0 0 0 0 0 0 0 0 0 0AU 0 0 0 0 0 0 0 0 0 0 0 0 0 0AUTHOR 0 0 0 0 0 0 0 0 0 0 0 0 0 0CAT 0 0 0 0 0 0 0 0 0 0 0 0 0 0DATE 0 0 0 0 0 0 0 0 0 0 0 0 0 0HEADLINE 144 138 105 0 7 32 26 0 73 76 22 19 6 38HL 50 54 38 0 4 8 12 0 76 70 16 22 10 39ID 0 0 0 0 0 0 0 0 0 0 0 0 0 0NUM 0 0 0 0 0 0 0 0 0 0 0 0 0 0TEXT 3452 3734 2871 0 171 410 692 0 83 77 12 19 6 31OBJ SCOREenamex 1301 1401 1107 0 0 194 294 0 85 79 15 21 0 31numex 99 92 90 0 0 9 2 0 91 98 9 2 0 11timex 423 470 401 0 0 22 69 0 95 85 5 15 0 18SLOT SCOREenamextext 1301 1401 1039 0 68 194 294 0 80 74 15 21 6 35type 1301 1401 1035 0 72 194 294 0 80 74 15 21 7 35numextext 99 92 88 0 2 9 2 0 89 96 9 2 2 13type 99 92 90 0 0 9 2 0 91 98 9 2 0 11timextext 423 470 361 0 40 22 69 0 85 77 5 15 10 27type 423 470 401 0 0 22 69 0 95 85 5 15 0 18ALL SLOTS3646 3926 3014 0 182 450 730 0 83 77 12 19 6 31F-MEASURES P&R 2P&R P&2R79.61 77.88 81.42Table 1: Summary Scores of NTUNLPLThe following models are adopted to select the possible candidates.
We consider the above three typesof surnames.Model 1.
Single character4(i) P(C1)*P(C2)*P(C3) using male training table > Threshold1 andP(C2)*P(C3) using male training table > Threshold2, or(ii)P(C1)*P(C2)*P(C3) using female training table > Threshold3 andP(C2)*P(C3) using female training table > Threshold4Model 2.
Two characters(i)P(C2)*P(C3) using male training table > Threshold2, or(ii)P(C2)*P(C3) using female training table > Threshold4Model 3.
Two surnames togetherP(C12)*P(C2)*P(C3) using female training table > Threshold3,P(C2)*P(C3) using female training table > Threshold4 andP(C12)*P(C2)*P(C3) using female training table >P(C12)*P(C2)*P(C3) using male training tablewhere C1, C2 and C3 are a continuous sequence of characters in a sentence, and they denotesurname and names, respectively,C11 and C12 denote the first and the second surnames,P(Ci) is the probability of Ci to be a surname or a name.For different types of surnames, different models are adopted.
Because the surnames with two characters arealways surnames, Model 2 neglects the score of surname part.
Both Models 1 and 3 consider the score ofsurname.
We compute the probabilities using female and male training tables, respectively.
In Models (1)and (2), either male score or female score must be greater than thresholds.
In Model (3), the person namesmust denote a female.
In this case, the probability to be female must be greater than the probability to be male.The above three models can be extended to the single-character names.
When a candidate cannot pass thethresholds, its last character is cut off and the remaining string is tried again.
Thresholds are trained from the1-million person name corpus.
We let 99% of the training data pass the thresholds.Besides the baseline model, titles, positions and special verbs are important local clues.
When a titlesuch as 'k?'
(President) appears before (after) a string, it is probably a person name.
There are 476 titles inour database.
Person names usually appear at the head or the tail of a sentence.
Persons may be accompaniedwith speech-act verbs like "9", "?
", "?7", etc.
For these cases, extra scores are added to help stringspass the thresholds.Finally, we present a global clue.
A person name may appear more than once in a document.
We usecache to store the identified candidates and reset cache when next document is considered.
There are fourcases shown below when cache is used:(1) C1C2C3 and C1C2C4 are in the cache, and C1C2 is correct.
(2) C1C2C3 and C1C2C4 are in the cache, and both are correct.
(3) C1C2C3 and C1C2 are in the cache, and C1C2C3 is correct.
(4) C1C2C3 and C1C2 are in the cache, and C1C2 is correct.Cases (1) and (2) (cases (3) and (4)) are contradictory.
In our treatment, a weight is assigned to each entry inthe cache.
The entry that has clear right boundary has a high weight.
Titles, positions, and special verbs areclues for boundary.
For those similar pairs that have different weights, the entry having high weight is selected.If both have high weights, both are chosen.
When both have low weights, the score of the second character ofa name part is critical.
It determines if the character is kept or deleted.Identification of Transliterated Person NamesTransliterated person names denote foreigners.
Compared with Chinese person names, the length oftransliterated names is not restricted to 2 to 6 characters.
The following strategies are adopted to recognizetransliterated names:(1) transliterated name setThe transliterated names trained from MET data are regarded as a built-in name set.
(2) character conditionTwo special character sets are retrieved from MET training data, Hornby [10] and Huang [11].The first character of transliterated names must belong to a 280-character set, and the remainingcharacters must appear in a 411-character set.
The character condition is a loose restriction.
Thestring that satisfies the character condition may denote a location, a building, an address, etc.
Itshould be employed with other clues (refer to (3)-(5)).
(3) titlesTitles used in Chinese person names are also applicable to transliterated person names.
Thus, ?Z ?
will not be recognized as a transliterated person name.
(4) name introducersSome words like "R", "RY", "R1", "?R", and "^w" can introduce transliterated names when5they are used at the first time.
(5) special verbsPersons always appear with some special verbs like "?
", "]?
", and so on.
Thus the same setof verbs used in Chinese person names are also used for transliterated person names.Besides the above strategies, a complete transliterated person name is composed of first name, middle name andlast name.
For example, ???C??T??Q??(B.
The first, middle and last names areconnected by a dot.Cache mechanism is also helpful in the identification of transliterated names.
A candidate that satisfiesthe character condition and one of the clues will be placed in the cache.
At the second time, the clues maydisappear, but we can recover the transliterated person name by checking cache.
The following shows anexample:?
&?V[?
???
&?V??
?Title does not show up, when the name is mentioned again.DiscussionThe summary report in Table 1 shows the recall rate and the precision for person names are 91% and 74%,respectively.
The major errors are listed below:(1) segmentationIn our treatment, segmentation is done before named entity extraction.
Part of person names maybe regarded as words during segmentation.
The following show some examples.
The characters"??
", "&?
", and "g" are common content words.??
[ -> ??
[?&?
-> ?
&?g3 -> g 3In this case, the person name is missed.
(2) surname set and character setThose characters not listed in surname set are not considered as surnames, so that they cannottrigger our identification system.
The characters "," and "" in person names ",<" and "?"
are typical examples.
Similarly, if the character of a transliterated person name does notbelong to the predefined character set, the character will be neglected.
For example, "?"
in "C??"
is not listed in the character set, and the scope error happens.
(3) blanksBlank may appear between surname and name in the original MET-2 documents, e.g., "?
?
".After segmentation, blanks are also inserted between words.
We cannot tell if the blanks exist inthe original documents or are inserted by our segmentation system.
(4) boundary errorsSome Chinese person names are mis-regarded as transliterated names, e.g.,?c?
-> ?cOBj -> OB(5) titlesTitles are important clues for the identification of transliterated person names.
Even if atransliterated name satisfies the character condition, it is not identified without title.
The name "C?"
in the string "k?C?"
is missed because "k?"
is not listed in our title set.
(6) Japanese namesThe current version cannot deal with Japanese names like "??v??
".NAMED ORGANIZATION EXTRACTIONExtraction AlgorithmThe structure of organization names is more complex than that of person names.
Basically, a completeorganization name can be divided into two parts, i.e., name and keyword.
The following specifies the rules weadopted to formulate its structure.OrganizationName ?
OrganizationName OrganizationNameKeyworde.g., {?f ?
?OrganizationName ?
CountryName OrganizationNameKeyworde.g., f ?c?OrganizationName ?
PersonName OrganizationNameKeyworde.g., ?t?
n?e6OrganizationName ?
CountryName {D|DD} OrganizationNameKeywordwhere D is a content word.e.g., ?f f m?
?WOrganizationName ?
PersonName {D|D} OrganizationNameKeyworde.g., ?t?
?
n?eOrganizationName ?
LocationName {D|D} OrganizationNameKeyworde.g., W> f m?
?WOrganizationName ?
CountryName OrganizationNamee.g., f fR?OrganizationName ?
LocationName OrgnizationNamee.g., ?j??
?
?In current version, we collect 776 organization names and 1059 organization name keywords.Transliterated person names and location names in the above rules still have to satisfy the charactercondition mentioned in last section.
However, the character set is trained from transliterated person namecorpus.
It may not be suitable for location names.
Consider an example "????fB.?".
"????
", which is a lake in China, is not a transliterated name.
The characters "?"
and "?"
do not belong to thecharacter set.
Here, we utilize the feature of multiple occurrences of organization names in a document andpropose n-gram model to deal with this problem.
Although cache mechanism and n-gram use the same feature,i.e., multiple occurrences, their concepts are totally different.
For organization names, we are not sure when apattern should be put into cache because its left boundary is hard to decide.
In our n-gram model, we selectthose patterns that meet the following criteria:(1) It must consist of a name and an organization name keyword.
(2) Its length must be greater than 2 words.
(3) It does not cross sentence boundary and any punctuation marks.
(4) It must occur at lease two times.DiscussionTable 1 shows the recall rate and the precision rate for the extraction of organization names are 78% and85%, respectively.
The following shows the error analysis.
(1) more than two content words between name and keywordIn current version, we accept only two interference words.
Thus, the string "?f 8z ?
-R ?P" is not recognized.
(2) absent of keywordsKeywords are important indicators for right boundary.
The string "?5[?J/" is lack ofkeyword, so it is missed.
(3) absent of name partName part serves as an indicator of left boundary.
In the string "\z?P", we cannot find aname.
(4) n-gram errorsN-gram employs multiple occurrences to find a pattern.
It is easy to propose false alarms, e.g., "?
?@" and "??M6??
?n?
".NAMED LOCATION EXTRACTIONExtraction AlgorithmThe structure of location names is similar to that of organization names.
A complete location name iscomposed of name part and keyword part.
We use the following rule to formulate this structure.LocationName ?
PersonName LocationNameKeywordLocationName ?
LocationName LocationNameKeywordCurrently, we have 45 location keywords.
The following shows some examples:'?
', '??
', '?X', ' >', ' B', ' 6', ' ?
', 'A?
', 'AQ', 'h', 'h??
', etc.There are 16,442 built-in location names in current versions.
For the treatment of location names withoutkeywords, we also introduce some locative verbs like 'g5',  '??
', and so on.
The objects following thiskind of verbs may be location names.
For example, in the string "]??X'?
", "?X'?"
will beidentified.
Cache is also useful.
For example, assume '??
?h' is recognized as a location name andplaced in cache.
When '???'
appears, it will be identified as a location name even if the location namekeyword is omitted.
N-gram model is also employed to recover those names that do not meet the charactercondition.7DiscussionTable 1 shows the recall rate and the precision rate in this part are 78% and 69%, respectively.
Theperformance is worse than that of named people and named organization.
The major types of errors are shownbelow.
(1) character setThe characters "" and "?"
in the string "t?S" do not belong to our transliterated characterset.
Actually, it denotes a Japanese location name.
(2) wrong keywordThe character "?"
is an organization keyword.
Thus the string "nJ?$?"
is misregarded asan organization name.
(3) common content wordsThe words such as "??
", "?z", etc., are common content words.
We do not give them specialtags.
(4) single-character locationsThe single-character locations such as "?
", "	", and so on, are missed during recognition.
Total57 errors are of this type.
(5) interference words between name part and keywordsThere are words between name part and keywords.
For example, "?dD????"
and "?T#?
?o?H".
Here the words "??"
and "?o" are common words in China newspaper, butseldom used in Taiwan.OTHER ENTITY EXTRACTIONExtraction AlgorithmWe use grammar rules to capture the remaining entities, including date/time expressions and monetaryand percentage expressions.
The following shows the specification of each type of expressions.
Each rule isaccompanied with an example.
(1) date expressionsDATE ?
NUMBER YEAR (?
?
)DATE ?
NUMBER MTHUNIT (?
)DATE ?
NUMBER DUNIT (?
)DATE ?
REGINC (?v)DATE ?
FSTATE DATE (??
?
)DATE ?
COMMON DATE (?
v?
)DATE ?
REGINE DATE (~f ????
)DATE ?
DATE DMONTH (??
?
)DATE ?
DATE BSTATE (G?
?
)DATE ?
FSTATEDATE DATE (??
?
?
)DATE ?
FSTATEDATE DMONTH (??
?
)DATE ?
FSTATEDATE FSTATEDATE (+?
??
)DATE ?
DATE YXY DATE (G??
6 ???
)(2) time expressionsTIME ?
NUMBER HUNIT (?
)TIME ?
NUMBER MUNIT (??
?
)TIME ?
NUMBER SUNIT (?
?
)TIME ?
FSTAETIME TIMETIME ?
FSTATE TIMETIME ?
TIME BSTATETIME ?
MORN BSTATETIME ?
TIME TIMETIME ?
TIME YXY TIME (??
?
+?
)TIME ?
NUMBER COLON NUMBER (03 ?
45)(3) monetary expressionsDMONEY ?
MOUNIT NUMBER MOUNIT (?
?
?
)DMONEY ?
NUMBER MOUNIT MOUNIT (?
?
?
)DMONEY ?
NUMBER MOUNIT  (?
?
)8DMONEY ?
MOUNIT MOUNIT NUMBER (?
$ 5 )DMONEY ?
MOUNIT NUMBER ($ 5)DMONEY ?
NUMBER YXY DMONEY (?
6 ??
)DMONEY ?
DMONEY YXY DMONEY (??
6 ??
)DMONEY ?
DMONEY YXY NUMBER ($200 - 500)(4) percentage expressionsDPERCENT ?
PERCENT NUMBER ($??
?
)DPERCENT ?
NUMBER PERCENT (3 %)DPERCENT ?
DPERCENT YXY DPERCENT (5% ?
8%)DPERCENT ?
DPERCENT YXY NUMBE ($???
?
?
)DPERCENT ?
NUMBER YXY DPERCENT (?
?
?$??
)Rule-based approach is simple.
We can add, delete and modify rules quickly without modifying theidentification programs.
However, the above rules cannot capture ambiguous cases.
For example, "" maymean the address number (e.g., ??X?)
or the date number (e.g., ?
?
).
Augmented grammarrules are needed to introduce constraints to check if the extracted entity can fit into the context.DiscussionThe summary report in Table 1 shows that the recall rate and the precision rate for date expression, timeexpression, monetary expression and percentage expression are (94%, 88%), (98%, 70%), (98%, 98%) and(83%, 98%), respectively.
The major errors are shown as follows:(1) propagation errorsBecause we employ a segmentation system to identify basic tokens before entity extraction, somewords like "H?
", "?L", etc., are regarded as terms.
In this way, "?"
is always missed.Similarly, named people are extracted before date expressions.
The errors resulting from theprevious steps propagate to the next steps.
Consider the following example.5 1998??4??
?The named people extraction procedure regards "??4??"
as a transliterated name.
After that,"1998?"
is missed because the date unit is absent.
(2) absent date unitsIn some sentences, the date unit "?"
does not appear, so that "????"
is missed.
In someexamples like "?
?
", the date unit should appear but it is absent.
Thus it is also not captured.
(3) absent keywordsSome keywords are not listed.
For example, "???
?
", "6?
", and so on.
Thus, for "?????
?
8?
58?"
and "1960?6?
", only some fragments, e.g., "??
", "8?
58?
", and"1960?"
are identified.
(4) rule coveragePatterns like "??+v?"
are not considered in this version, thus they are missed.
Similarly, thepercentage expressions like "2/3", "?????
", "$????
", and so on, are not represented inour grammar.
(5) ambiguitySome characters like "?"
can be used in time and monetary expressions.
Expression "?????0?"
is divided into two parts: "???"
and "??0?".
Similarly, the strings "??
"and "?"
are words.
In our pipelined model, "????"
and "???"
will be missed.CONCLUDING REMARKSThis paper proposes a pipeline model to extract named entities from Chinese documents.
Different typesof information from different levels of text are employed, including character conditions, statistic information,titles, punctuation marks, organization and location keywords, speech-act and locative verbs, cache and n-grammodel.
The context ranges from very short to very long.
The recall rate (83%) and the precision rate (77%)are achieved.
The major errors result from propagation errors, keyword sets, character sets, rule coverage, andso on.
How to integrate different modules (including segmentation and recognition) in an interleaving way,and how to learn grammar rules, keyword sets and character sets automatically have to be studied furthermore.REFERENCES[ 1 ] Jen-Chang Lee, Yue-Shi Lee and Hsin-Hsi Chen, ?Identification of Personal Names in Chinese Texts,?Proceedings of ROCLING VII, Taiwan, August 12-13 1994, pp.
203-222 (in Chinese).
[ 2 ]Hsin-Hsi Chen Jen-Chang and Lee, ?The Identification of Organization Names in Chinese Texts,?9Communication of Chinese and Oriental Languages Information Processing Society, 4(2), Singapore, 1994,pp.
131-142 (in Chinese).
[ 3 ]Hsin-Hsi Chen and Jen-Chang Lee, ?Identification and Classification of Proper Nouns in Chinese Texts,?Proceedings of 16th International Conference on Computational Linguistics, Copenhagen, Denmark,August 5-9 1996, pp.
222-229.
[ 4 ]Hsin-Hsi Chen and Yeong-Yui Wu, ?Aligning Parallel Chinese-English Texts Using Multiple Clues,?Proceedings of 2nd Pacific Association for Computational Linguistic Conference, Queensland, Australia,April 19-22 1995, pp.
39-48.
[ 5 ]Hsin-Hsi Chen and Guo-Wei Bian, ?Proper Name Extraction from Web Pages for Finding People inInternet,?
Proceedings of ROCLING X, Taipei, Taiwan, August 22-24, 1997, pp.
143-158.
[ 6 ]Hsin-Hsi Chen, Yung-Wei Ding and Shih-Chung Tsai, "Proper Noun Extraction for Information Retrieval,"International Journal on Computer Processing of Oriental Languages, Special Issue on InformationRetrieval on Oriental Languages, 1998.
[ 7 ]Hsin-Hsi Chen, Sheng-Jie Huang, Yung-Wei Ding and Shih-Chung Tsai, "Proper Name Translation inCross-Language Information Retrieval," Proceedings of COLING-ACL98, Montreal, August 10-14, 1998.
[ 8 ]Chu-Ren Huang, et al, "Introduction to CKIP Balanced Corpus," Proceedings of ROCLING VIII, Taiwan,August 18-19 1995, pp.
81-99 (in Chinese).
[ 9 ]ROCLING, ROCLING Text Corpus Exchange, Association of Computational Linguistics and ChineseLanguage Processing, 1993.[10]A.S.
Hornby, Oxford Advanced Learner?s Dictionary of Current English, Revised Third Edition, 1984.[11]Y.J.
Huang, English Names for You, Learning Publishing Company, Taiwan, 1992.
