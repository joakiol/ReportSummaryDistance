Proceedings of the 12th Conference of the European Chapter of the ACL, pages 220?228,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsSemi-Supervised Semantic Role LabelingHagen Fu?rstenauDept.
of Computational LinguisticsSaarland UniversitySaarbru?cken, Germanyhagenf@coli.uni-saarland.deMirella LapataSchool of InformaticsUniversity of EdinburghEdinburgh, UKmlap@inf.ed.ac.ukAbstractLarge scale annotated corpora are pre-requisite to developing high-performancesemantic role labeling systems.
Unfor-tunately, such corpora are expensive toproduce, limited in size, and may not berepresentative.
Our work aims to reducethe annotation effort involved in creat-ing resources for semantic role labelingvia semi-supervised learning.
Our algo-rithm augments a small number of man-ually labeled instances with unlabeled ex-amples whose roles are inferred automat-ically via annotation projection.
We for-mulate the projection task as a generaliza-tion of the linear assignment problem.
Weseek to find a role assignment in the un-labeled data such that the argument sim-ilarity between the labeled and unlabeledinstances is maximized.
Experimental re-sults on semantic role labeling show thatthe automatic annotations produced by ourmethod improve performance over usinghand-labeled instances alone.1 IntroductionRecent years have seen a growing interest in thetask of automatically identifying and labeling thesemantic roles conveyed by sentential constituents(Gildea and Jurafsky, 2002).
This is partly due toits relevance for applications ranging from infor-mation extraction (Surdeanu et al, 2003; Mos-chitti et al, 2003) to question answering (Shen andLapata, 2007), paraphrase identification (Pado?
andErk, 2005), and the modeling of textual entailmentrelations (Tatu and Moldovan, 2005).
Resourceslike FrameNet (Fillmore et al, 2003) and Prop-Bank (Palmer et al, 2005) have also facilitated thedevelopment of semantic role labeling methods byproviding high-quality annotations for use in train-ing.
Semantic role labelers are commonly devel-oped using a supervised learning paradigm1 wherea classifier learns to predict role labels based onfeatures extracted from annotated training data.Examples of the annotations provided inFrameNet are given in (1).
Here, the meaning ofpredicates (usually verbs, nouns, or adjectives) isconveyed by frames, schematic representations ofsituations.
Semantic roles (or frame elements) aredefined for each frame and correspond to saliententities present in the situation evoked by the pred-icate (or frame evoking element).
Predicates withsimilar semantics instantiate the same frame andare attested with the same roles.
In our exam-ple, the frameCause harm has three core semanticroles, Agent, Victim, and Body part and can be in-stantiated with verbs such as punch, crush, slap,and injure.
The frame may also be attested withnon-core (peripheral) roles that are more genericand often shared across frames (see the roles De-gree, Reason, and Means, in (1c) and (1d)).
(1) a.
[Lee]Agent punched [John]Victim[in the eye]Body part.b.
[A falling rock]Cause crushed [myankle]Body part.c.
[She]Agent slapped [him]Victim[hard]Degree [for his change ofmood]Reason.d.
[Rachel]Agent injured [herfriend]Victim [by closing the cardoor on his left hand]Means.The English FrameNet (version 1.3) contains502 frames covering 5,866 lexical entries.
It alsocomes with a set of manually annotated exam-ple sentences, taken mostly from the British Na-tional Corpus.
These annotations are often used1The approaches are too numerous to list; we refer theinterested reader to the proceedings of the SemEval-2007shared task (Baker et al, 2007) for an overview of the state-of-the-art.220as training data for semantic role labeling sys-tems.
However, the applicability of these sys-tems is limited to those words for which labeleddata exists, and their accuracy is strongly corre-lated with the amount of labeled data available.Despite the substantial annotation effort involvedin the creation of FrameNet (spanning approxi-mately twelve years), the number of annotated in-stances varies greatly across lexical items.
For in-stance, FrameNet contains annotations for 2,113verbs; of these 12.3% have five or less annotatedexamples.
The average number of annotations perverb is 29.2.
Labeled data is thus scarce for indi-vidual predicates within FrameNet?s target domainand would presumably be even scarcer across do-mains.
The problem is more severe for languagesother than English, where training data on thescale of FrameNet is virtually non-existent.
Al-though FrameNets are being constructed for Ger-man, Spanish, and Japanese, these resources aresubstantially smaller than their English counter-part and of limited value for modeling purposes.One simple solution, albeit expensive and time-consuming, is to manually create more annota-tions.
A better alternative may be to begin withan initial small set of labeled examples and aug-ment it with unlabeled data sufficiently similar tothe original labeled set.
Suppose we have man-ual annotations for sentence (1a).
We shall try andfind in an unlabeled corpus other sentences thatare both structurally and semantically similar.
Forinstance, we may think that Bill will punch me inthe face and I punched her hard in the head re-semble our initial sentence and are thus good ex-amples to add to our database.
Now, in order touse these new sentences as training data we mustsomehow infer their semantic roles.
We can prob-ably guess that constituents in the same syntacticposition must have the same semantic role, espe-cially if they refer to the same concept (e.g., ?bodyparts?)
and thus label in the face and in the headwith the role Body part.
Analogously, Bill andI would be labeled as Agent and me and her asVictim.In this paper we formalize the method sketchedabove in order to expand a small number ofFrameNet-style semantic role annotations withlarge amounts of unlabeled data.
We adopt a learn-ing strategy where annotations are projected fromlabeled onto unlabeled instances via maximizinga similarity function measuring syntactic and se-mantic compatibility.
We formalize the annotationprojection problem as a generalization of the linearassignment problem and solve it efficiently usingthe simplex algorithm.
We evaluate our algorithmby comparing the performance of a semantic rolelabeler trained on the annotations produced by ourmethod and on a smaller dataset consisting solelyof hand-labeled instances.
Results in several ex-perimental settings show that the automatic anno-tations, despite being noisy, bring significant per-formance improvements.2 Related WorkThe lack of annotated data presents an obstacleto developing many natural language applications,especially when these are not in English.
It istherefore not surprising that previous efforts to re-duce the need for semantic role annotation havefocused primarily on non-English languages.Annotation projection is a popular frameworkfor transferring frame semantic annotations fromone language to another by exploiting the transla-tional and structural equivalences present in par-allel corpora.
The idea here is to leverage the ex-isting English FrameNet and rely on word or con-stituent alignments to automatically create an an-notated corpus in a new language.
Pado?
and Lap-ata (2006) transfer semantic role annotations fromEnglish onto German and Johansson and Nugues(2006) from English onto Swedish.
A differentstrategy is presented in Fung and Chen (2004),where English FrameNet entries are mapped toconcepts listed in HowNet, an on-line ontologyfor Chinese, without consulting a parallel corpus.Then, Chinese sentences with predicates instan-tiating these concepts are found in a monolin-gual corpus and their arguments are labeled withFrameNet roles.Other work attempts to alleviate the data re-quirements for semantic role labeling either by re-lying on unsupervised learning or by extending ex-isting resources through the use of unlabeled data.Swier and Stevenson (2004) present an unsuper-vised method for labeling the arguments of verbswith their semantic roles.
Given a verb instance,their method first selects a frame from VerbNet, asemantic role resource akin to FrameNet and Prop-Bank, and labels each argument slot with sets ofpossible roles.
The algorithm proceeds iterativelyby first making initial unambiguous role assign-ments, and then successively updating a probabil-221ity model on which future assignments are based.Being unsupervised, their approach requires nomanual effort other than creating the frame dic-tionary.
Unfortunately, existing resources do nothave exhaustive coverage and a large number ofverbs may be assigned no semantic role informa-tion since they are not in the dictionary in thefirst place.
Pennacchiotti et al (2008) addressprecisely this problem by augmenting FrameNetwith new lexical units if they are similar to an ex-isting frame (their notion of similarity combinesdistributional and WordNet-based measures).
Ina similar vein, Gordon and Swanson (2007) at-tempt to increase the coverage of PropBank.
Theirapproach leverages existing annotations to handlenovel verbs.
Rather than annotating new sentencesthat contain novel verbs, they find syntacticallysimilar verbs and use their annotations as surro-gate training data.Our own work aims to reduce but not entirelyeliminate the annotation effort involved in creatingtraining data for semantic role labeling.
We thusassume that a small number of manual annotationsis initially available.
Our algorithm augmentsthese with unlabeled examples whose roles are in-ferred automatically.
We apply our method in amonolingual setting, and thus do not project an-notations between languages but within the samelanguage.
In contrast to Pennacchiotti et al (2008)and Gordon and Swanson (2007), we do not aimto handle novel verbs, although this would be anatural extension of our method.
Given a verband a few labeled instances exemplifying its roles,we wish to find more instances of the same verbin an unlabeled corpus so as to improve the per-formance of a hypothetical semantic role labelerwithout having to annotate more data manually.Although the use of semi-supervised learning iswidespread in many natural language tasks, rang-ing from parsing to word sense disambiguation, itsapplication to FrameNet-style semantic role label-ing is, to our knowledge, novel.3 Semi-Supervised Learning MethodOur method assumes that we have access to asmall seed corpus that has been manually anno-tated.
This represents a relatively typical situationwhere some annotation has taken place but not ona scale that is sufficient for high-performance su-pervised learning.
For each sentence in the seedcorpus we select a number of similar sentencesFluidic motionFEE~~}}}}}}}}}}Fluidhknqty}Path||~zfeelSUBJuukkkkkkkkkkkkkkkkAUX{{vvvvvvvvvDOBJXCOMP%%KKKKKKKKKKwe can courseSUBJyyssssssssssIOBJMOD((PPPPPPPPPPPPbloodDETthroughDOBJagainthe veinDETourFigure 1: Labeled dependency graph with seman-tic role annotations for the frame evoking ele-ment (FEE) course in the sentence We can feel theblood coursing through our veins again.
The frameis Fluidic motion, and its roles are Fluid and Path.Directed edges (without dashes) represent depen-dency relations between words, edge labels denotetypes of grammatical relations (e.g., SUBJ, AUX).from an unlabeled expansion corpus.
These areautomatically annotated by projecting relevant se-mantic role information from the labeled sentence.The similarity between two sentences is opera-tionalized by measuring whether their argumentshave a similar structure and whether they expressrelated meanings.
The seed corpus is then en-larged with the k most similar unlabeled sentencesto form the expanded corpus.
In what follows wedescribe in more detail how we measure similarityand project annotations.3.1 Extracting Predicate-ArgumentStructuresOur method operates over labeled dependencygraphs.
We show an example in Figure 1 forthe sentence We can feel the blood coursingthrough our veins again.
We represent verbs(i.e., frame evoking elements) in the seed andunlabeled corpora by their predicate-argumentstructure.
Specifically, we record the direct de-pendents of the predicate course (e.g., bloodor again in Figure 1) and their grammaticalroles (e.g., SUBJ, MOD).
Prepositional nodesare collapsed, i.e., we record the preposition?sobject and a composite grammatical role (likeIOBJ THROUGH, where IOBJ stands for ?preposi-tional object?
and THROUGH for the prepositionitself).
In addition to direct dependents, we also222Lemma GramRole SemRoleblood SUBJ Fluidvein IOBJ THROUGH Pathagain MOD ?Table 1: Predicate-argument structure for the verbcourse in Figure 1.consider nodes coordinated with the predicate asarguments.
Finally, for each argument node werecord the semantic roles it carries, if any.
All sur-face word forms are lemmatized.
An example ofthe argument structure information we obtain forthe predicate course (see Figure 1) is shown in Ta-ble 1.We obtain information about grammatical rolesfrom the output of RASP (Briscoe et al, 2006),a broad-coverage dependency parser.
However,there is nothing inherent in our method that re-stricts us to this particular parser.
Any otherparser with broadly similar dependency outputcould serve our purposes.3.2 Measuring SimilarityFor each frame evoking verb in the seed corpus ourmethod creates a labeled predicate-argument re-presentation.
It also extracts all sentences from theunlabeled corpus containing the same verb.
Notall of these sentences will be suitable instancesfor adding to our training data.
For example, thesame verb may evoke a different frame with dif-ferent roles and argument structure.
We thereforemust select sentences which resemble the seed an-notations.
Our hypothesis is that verbs appearingin similar syntactic and semantic contexts will be-have similarly in the way they relate to their argu-ments.Estimating the similarity between two predi-cate argument structures amounts to finding thehighest-scoring alignment between them.
Moreformally, given a labeled predicate-argumentstructure pl with m arguments and an unla-beled predicate-argument structure pu with n ar-guments, we consider (and score) all possiblealignments between these arguments.
A (partial)alignment can be viewed as an injective function?
: M?
?
{1, .
.
.
, n} where M?
?
{1, .
.
.
,m}.In other words, an argument i of pl is aligned toargument ?
(i) of pu if i ?
M?.
Note that this al-lows for unaligned arguments on both sides.We score each alignment ?
using a similarityfunction sim(?)
defined as:?i?M?
(A ?
syn(gli, gu?
(i)) + sem(wli, wu?
(i))?B)where syn(gli, gu?
(i)) denotes the syntactic similar-ity between grammatical roles gli and gu?
(i) andsem(wli, wu?
(i)) the semantic similarity betweenhead words wli and wu?
(i).Our goal is to find an alignment suchthat the similarity function is maximized:??
:= argmax?sim(?).
This optimizationproblem is a generalized version of the linearassignment problem (Dantzig, 1963).
It can bestraightforwardly expressed as a linear program-ming problem by associating each alignment ?with a set of binary indicator variables xij :xij :={1 if i ?
M?
?
?
(i) = j0 otherwiseThe similarity objective function then becomes:m?i=1n?j=1(A ?
syn(gli, guj ) + sem(wli, wuj )?B)xijsubject to the following constraints ensuring that ?is an injective function on some M?
:n?j=1xij ?
1 for all i = 1, .
.
.
,mm?i=1xij ?
1 for all j = 1, .
.
.
, nFigure 2 graphically illustrates the alignmentprojection problem.
Here, we wish to projectsemantic role information from the seed bloodcoursing through our veins again onto the un-labeled sentence Adrenalin was still coursingthrough her veins.
The predicate course has threearguments in the labeled sentence and four in theunlabeled sentence (represented as rectangles inthe figure).
There are 73 possible alignments inthis example.
In general, for any m and n argu-ments, where m ?
n, the number of alignmentsis?mk=0m!n!(m?k)!(n?k)!k!
.
Each alignment is scoredby taking the sum of the similarity scores of the in-dividual alignment pairs (e.g., between blood andbe, vein and still ).
In this example, the highestscoring alignment is between blood and adrenalin,vein and vein, and again and still, whereas be is223left unaligned (see the non-dotted edges in Fig-ure 2).
Note that only vein and blood carry seman-tic roles (i.e., Fluid and Path) which are projectedonto adrenalin and vein, respectively.Finding the best alignment crucially dependson estimating the syntactic and semantic similar-ity between arguments.
We define the syntacticmeasure on the grammatical relations producedby RASP.
Specifically, we set syn(gli, gu?
(i)) to 1if the relations are identical, to a ?
1 if the rela-tions are of the same type but different subtype2and to 0 otherwise.
To avoid systematic errors,syntactic similarity is also set to 0 if the predicatesdiffer in voice.
We measure the semantic similar-ity sem(wli, wu?
(i)) with a semantic space model.The meaning of each word is represented by a vec-tor of its co-occurrences with neighboring words.The cosine of the angle of the vectors represent-ingwl andwu quantifies their similarity (Section 4describes the specific model we used in our exper-iments in more detail).The parameter A counterbalances the impor-tance of syntactic and semantic information, whilethe parameter B can be interpreted as the lowestsimilarity value for which an alignment betweentwo arguments is possible.
An optimal align-ment ??
cannot link arguments i0 of pl and j0of pu, if A ?
syn(gli0 , guj0) + sem(wli0 , wuj0) < B(i.e., either i0 /?
M??
or ??
(i0) 6= j0).
Thisis because for an alignment ?
with ?
(i0) = j0we can construct a better alignment ?0, which isidentical to ?
on all i 6= i0, but leaves i0 un-aligned (i.e., i0 /?
M?0).
By eliminating a neg-ative term from the scoring function, it followsthat sim(?0) > sim(?).
Therefore, an alignment ?satisfying ?
(i0) = j0 cannot be optimal and con-versely the optimal alignment ??
can never linktwo arguments with each other if the sum of theirweighted syntactic and semantic similarity scoresis below B.3.3 Projecting AnnotationsOnce we obtain the best alignment ??
between pland pu, we can simply transfer the role of eachrole-bearing argument i of pl to the aligned argu-ment ??
(i) of pu, resulting in a labeling of pu.To increase the accuracy of our method we dis-card projections if they fail to transfer all rolesof the labeled to the unlabeled dependency graph.2This concerns fine-grained distinctions made by theparser, e.g., the underlying grammatical roles in passive con-structions.Fluid //___ bloodSUBJ//!
!adrenalinSUBJPath //___ veinIOBJ THROUGH==//!
!11111111111111111111111 beAUXagainMODFF==//!
!stillMODveinIOBJ THROUGHFigure 2: Alignments between the argumentstructures representing the clauses blood coursingthrough our veins again and Adrenalin was stillcoursing through her veins; non-dotted lines illus-trate the highest scoring alignment.This can either be the case if pl does not cover allroles annotated on the graph (i.e., there are role-bearing nodes which we do not recognize as argu-ments of the frame evoking verb) or if there areunaligned role-bearing arguments (i.e., i /?
M?
?for a role-bearing argument i of pl).The remaining projections form our expan-sion corpus.
For each seed instance we selectthe k most similar neighbors to add to our trainingdata.
The parameter k controls the trade-off be-tween annotation confidence and expansion size.4 Experimental SetupIn this section we discuss our experimental setupfor assessing the usefulness of the method pre-sented above.
We give details on our training pro-cedure and parameter estimation, describe the se-mantic labeler we used in our experiments and ex-plain how its output was evaluated.Corpora Our seed corpus was taken fromFrameNet.
The latter contains approximately2,000 verb entries out of which we randomly se-lected a sample of 100.
We next extracted all an-notated sentences for each of these verbs.
Thesesentences formed our gold standard corpus, 20%of which was reserved as test data.
We usedthe remaining 80% as seeds for training purposes.We generated seed corpora of various sizes byrandomly reducing the number of annotation in-stances per verb to a maximum of n. An addi-tional (non-overlapping) random sample of 100verbs was used as development set for tuning theparameters for our method.
We gathered unla-beled sentences from the BNC.224The seed and unlabeled corpora were parsedwith RASP (Briscoe et al, 2006).
The FrameNetannotations in the seed corpus were convertedinto dependency graphs (see Figure 1) using themethod described in Fu?rstenau (2008).
Briefly,the method works by matching nodes in the de-pendency graph with role bearing substrings inFrameNet.
It first finds the node in the graphwhich most closely matches the frame evokingelement in FrameNet.
Next, individual graphnodes are compared against labeled substrings inFrameNet to transfer all roles onto their closestmatching graph nodes.Parameter Estimation The similarity functiondescribed in Section 3.2 has three free parameters.These are the weight A which determines the rel-ative importance of syntactic and semantic infor-mation, the parameter B which determines whentwo arguments cannot be aligned and the syntacticscore a for almost identical grammatical roles.
Weoptimized these parameters on the developmentset using Powell?s direction set method (Brent,1973) with F1 as our loss function.
The optimalvalues for A, B and a were 1.76, 0.41 and 0.67,respectively.Our similarity function is further parametrizedin using a semantic space model to compute thesimilarity between two words.
Considerable lat-itude is allowed in specifying the parameters ofvector-based models.
These involve the defi-nition of the linguistic context over which co-occurrences are collected, the number of com-ponents used (e.g., the k most frequent wordsin a corpus), and their values (e.g., as raw co-occurrence frequencies or ratios of probabilities).We created a vector-based model from a lem-matized version of the BNC.
Following previ-ous work (Bullinaria and Levy, 2007), we opti-mized the parameters of our model on a word-based semantic similarity task.
The task involvesexamining the degree of linear relationship be-tween the human judgments for two individualwords and vector-based similarity values.
We ex-perimented with a variety of dimensions (rangingfrom 50 to 500,000), vector component definitions(e.g., pointwise mutual information or log likeli-hood ratio) and similarity measures (e.g., cosine orconfusion probability).
We used WordSim353, abenchmark dataset (Finkelstein et al, 2002), con-sisting of relatedness judgments (on a scale of 0to 10) for 353 word pairs.We obtained best results with a model using acontext window of five words on either side of thetarget word, the cosine measure, and 2,000 vec-tor dimensions.
The latter were the most com-mon context words (excluding a stop list of func-tion words).
Their values were set to the ratio ofthe probability of the context word given the tar-get word to the probability of the context wordoverall.
This configuration gave high correlationswith the WordSim353 similarity judgments usingthe cosine measure.Solving the Linear Program A variety of algo-rithms have been developed for solving the linearassignment problem efficiently.
In our study, weused the simplex algorithm (Dantzig, 1963).
Wegenerate and solve an LP of every unlabeled sen-tence we wish to annotate.Semantic role labeler We evaluated our methodon a semantic role labeling task.
Specifically, wecompared the performance of a generic seman-tic role labeler trained on the seed corpus anda larger corpus expanded with annotations pro-duced by our method.
Our semantic role labelerfollowed closely the implementation of Johans-son and Nugues (2008).
We extracted featuresfrom dependency parses corresponding to thoseroutinely used in the semantic role labeling liter-ature (see Baker et al (2007) for an overview).SVM classifiers were trained to identify the argu-ments and label them with appropriate roles.
Forthe latter we performed multi-class classificationfollowing the one-versus-one method3 (Friedman,1996).
For the experiments reported in this paperwe used the LIBLINEAR library (Fan et al, 2008).The misclassification penalty C was set to 0.1.To evaluate against the test set, we linearizedthe resulting dependency graphs in order to obtainlabeled role bracketings like those in example (1)and measured labeled precision, labeled recall andlabeled F1.
(Since our focus is on role labeling andnot frame prediction, we let our role labeler makeuse of gold standard frame annotations, i.e., label-ing of frame evoking elements with frame names.
)5 ResultsThe evaluation of our method was motivated bythree questions: (1) How do different training setsizes affect semantic role labeling performance?3Given n classes the one-versus-one method buildsn(n?
1)/2 classifiers.225TrainSet Size Prec (%) Rec (%) F1 (%)0-NN 849 35.5 42.0 38.51-NN 1205 36.4 43.3 39.52-NN 1549 38.1 44.1 40.9?3-NN 1883 37.9 43.7 40.6?4-NN 2204 38.0 43.9 40.7?5-NN 2514 37.4 43.9 40.4?self train 1609 34.0 41.0 37.1Table 2: Semantic role labeling performance usingdifferent amounts of training data; the seeds areexpanded with their k nearest neighbors; ?
: F1 issignificantly different from 0-NN (p < 0.05).Training size varies depending on the number ofunlabeled sentences added to the seed corpus.
Thequality of these sentences also varies dependingon their similarity to the seed sentences.
So,we would like to assess whether there is a trade-off between annotation quality and training size.
(2) How does the size of the seed corpus influencerole labeling performance?
Here, we are interestedto find out what is the least amount of manualannotation possible for our method to have somepositive impact.
(3) And finally, what are the an-notation savings our method brings?Table 2 shows the performance of our semanticrole labeler when trained on corpora of differentsizes.
The seed corpus was reduced to at most 10instances per verb.
Each row in the table corre-sponds to adding the k nearest neighbors of theseinstances to the training data.
When trained solelyon the seed corpus the semantic role labeler yieldsa (labeled) F1 of 38.5%, (labeled) recall is 42.0%and (labeled) precision is 35.5% (see row 0-NNin the table).
All subsequent expansions yieldimproved precision and recall.
In all cases ex-cept k = 1 the improvement is statistically signif-icant (p < 0.05).
We performed significance test-ing onF1 using stratified shuffling (Noreen, 1989),an instance of assumption-free approximative ran-domization testing.
As can be seen, the optimaltrade-off between the size of the training corpusand annotation quality is reached with two nearestneighbors.
This corresponds roughly to doublingthe number of training instances.
(Due to the re-strictions mentioned in Section 3.3 a 2-NN expan-sion does not triple the number of instances.
)We also compared our results against a self-training procedure (see last row in Table 2).
Here,we randomly selected unlabeled sentences corre-sponding in number to a 2-NN expansion, labeledthem with our role labeler, added them to the train-ing set, and retrained.
Self-training resulted in per-formance inferior to the baseline of adding no un-labeled data at all (see the first row in Table 2).Performance decreased even more with the addi-tion of more self-labeled instances.
These resultsindicate that the similarity function is crucial to thesuccess of our method.An example of the annotations our method pro-duces is given below.
Sentence (2a) is the seed.Sentences (2b)?
(2e) are its most similar neighbors.The sentences are presented in decreasing order ofsimilarity.
(2) a.
[He]Theme stared and came[slowly]Manner [towards me]Goal.b.
[He]Theme had heard the shootingand come [rapidly]Manner [back to-wards the house]Goal.c.
Without answering, [she]Theme leftthe room and came [slowly]Manner[down the stairs]Goal.d.
[Then]Manner [he]Theme won?t come[to Salisbury]Goal.e.
Does [he]Theme always come round[in the morning]Goal [then]Manner?As we can see, sentences (2b) and (2c) accu-rately identify the semantic roles of the verb comeevoking the frame Arriving.
In (2b) He is la-beled as Theme, rapidly as Manner, and towardsthe house as Goal.
Analogously, in (2c) she isthe Theme, slowly is Manner and down the stairsis Goal.
The quality of the annotations decreaseswith less similar instances.
In (2d) then is markederroneously as Manner, whereas in (2e) only theTheme role is identified correctly.To answer our second question, we varied thesize of the training corpus by varying the num-ber of seeds per verb.
For these experiments wefixed k = 2.
Table 3 shows the performance of thesemantic role labeler when the seed corpus has oneannotation per verb, five annotations per verb, andso on.
(The results for 10 annotations are repeatedfrom Table 2).
With 1, 5 or 10 instances per verbour method significantly improves labeling perfor-mance.
We observe improvements in F1 of 1.5%,2.1%, and 2.4% respectively when adding the 2most similar neighbors to these training corpora.Our method also improves F1 when a 20 seeds226TrainSet Size Prec (%) Rec (%) F1 (%)?
1 seed 95 24.9 31.3 27.7+ 2-NN 170 26.4 32.6 29.2??
5 seeds 450 29.7 38.4 33.5+ 2-NN 844 31.8 40.4 35.6??
10 seeds 849 35.5 42.0 38.5+ 2-NN 1549 38.1 44.1 40.9??
20 seeds 1414 38.7 46.1 42.1+ 2-NN 2600 40.5 46.7 43.4all seeds 2323 38.3 47.0 42.2+ 2-NN 4387 39.5 46.7 42.8Table 3: Semantic role labeling performance us-ing different numbers of seed instances per verb inthe training corpus; the seeds are expanded withtheir k = 2 nearest neighbors; ?
: F1 is signifi-cantly different from seed corpus (p < 0.05).corpus or all available seeds are used, however thedifference is not statistically significant.The results in Table 3 also allow us to drawsome conclusions regarding the relative qualityof manual and automatic annotation.
Expand-ing a seed corpus with 10 instances per verb im-proves F1 from 38.5% to 40.9%.
We can com-pare this to the labeler?s performance when trainedsolely on the 20 seeds corpus (without any ex-pansion).
The latter has approximately the samesize as the expanded 10 seeds corpus.
Interest-ingly, F1 on this exclusively hand-annotated cor-pus is only 1.2% better than on the expanded cor-pus.
So, using our expansion method on a 10 seedscorpus performs almost as well as using twice asmany manual annotations.
Even in the case of the5 seeds corpus, where there is limited informa-tion for our method to expand from, we achievean improvement from 33.5% to 35.6%, comparedto 38.5% for manual annotation of about the samenumber of instances.
In sum, while additionalmanual annotation is naturally more effective forimproving the quality of the training data, we canachieve substantial proportions of these improve-ments by automatic expansion alone.
This is apromising result suggesting that it is possible toreduce annotation costs without drastically sacri-ficing quality.6 ConclusionsThis paper presents a novel method for reducingthe annotation effort involved in creating resourcesfor semantic role labeling.
Our strategy is to ex-pand a manually annotated corpus by projectingsemantic role information from labeled onto un-labeled instances.
We formulate the projectionproblem as an instance of the linear assignmentproblem.
We seek to find role assignments thatmaximize the similarity between labeled and un-labeled instances.
Similarity is measured in termsof structural and semantic compatibility betweenargument structures.Our method improves semantic role labelingperformance in several experimental conditions.
Itis especially effective when a small number of an-notations is available for each verb.
This is typi-cally the case when creating frame semantic cor-pora for new languages or new domains.
Our ex-periments show that expanding such corpora withour method can yield almost the same relative im-provement as using exclusively manual annota-tion.In the future we plan to extend our methodin order to handle novel verbs that are not at-tested in the seed corpus.
Another direction con-cerns the systematic modeling of diathesis alter-nations (Levin, 1993).
These are currently onlycaptured implicitly by our method (when the se-mantic similarity overrides syntactic dissimilar-ity).
Ideally, we would like to be able to system-atically identify changes in the realization of theargument structure of a given predicate.
Althoughour study focused solely on FrameNet annotations,we believe it can be adapted to related annotationschemes, such as PropBank.
An interesting ques-tion is whether the improvements obtained by ourmethod carry over to other role labeling frame-works.Acknowledgments The authors acknowledgethe support of DFG (IRTG 715) and EPSRC(grant GR/T04540/01).
We are grateful toRichard Johansson for his help with the re-implementation of his semantic role labeler.ReferencesCollin F. Baker, Michael Ellsworth, and Katrin Erk.2007.
SemEval-2007 Task 19: Frame SemanticStructure Extraction.
In Proceedings of the 4thInternational Workshop on Semantic Evaluations,pages 99?104, Prague, Czech Republic.R.
P. Brent.
1973.
Algorithms for Minimization with-out Derivatives.
Prentice-Hall, Englewood Cliffs,NJ.227Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The Second Release of the RASP System.
In Pro-ceedings of the COLING/ACL 2006 Interactive Pre-sentation Sessions, pages 77?80, Sydney, Australia.J.
A. Bullinaria and J. P. Levy.
2007.
Extractingsemantic representations from word co-occurrencestatistics: A computational study.
Behavior Re-search Methods, 39:510?526.George B. Dantzig.
1963.
Linear Programming andExtensions.
Princeton University Press, Princeton,NJ, USA.R.-E.
Fan, K.-W. Chang, C.-J.
Hsieh, X.-R. Wang, andC.-J.
Lin.
2008.
LIBLINEAR: A library for largelinear classification.
Journal of Machine LearningResearch, 9:1871?1874.Charles J. Fillmore, Christopher R. Johnson, andMiriam R. L. Petruck.
2003.
Background toFrameNet.
International Journal of Lexicography,16:235?250.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2002.
Placing search in context: Theconcept revisited.
ACM Transactions on Informa-tion Systems, 20(1):116?131.Jerome H. Friedman.
1996.
Another approach to poly-chotomous classification.
Technical report, Depart-ment of Statistics, Stanford University.Pascale Fung and Benfeng Chen.
2004.
BiFrameNet:Bilingual frame semantics resources constructionby cross-lingual induction.
In Proceedings of the20th International Conference on ComputationalLinguistics, pages 931?935, Geneva, Switzerland.Hagen Fu?rstenau.
2008.
Enriching frame semantic re-sources with dependency graphs.
In Proceedings ofthe 6th Language Resources and Evaluation Confer-ence, Marrakech, Morocco.Daniel Gildea and Dan Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguis-tics, 28:3:245?288.Andrew Gordon and Reid Swanson.
2007.
General-izing semantic role annotations across syntacticallysimilar verbs.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Lin-guistics, pages 192?199, Prague, Czech Republic.Richard Johansson and Pierre Nugues.
2006.
AFrameNet-based semantic role labeler for Swedish.In Proceedings of the COLING/ACL 2006 MainConference Poster Sessions, pages 436?443, Syd-ney, Australia.Richard Johansson and Pierre Nugues.
2008.
The ef-fect of syntactic representation on semantic role la-beling.
In Proceedings of the 22nd InternationalConference on Computational Linguistics, pages393?400, Manchester, UK.Beth Levin.
1993.
English Verb Classes and Alter-nations: A Preliminary Investigation.
University ofChicago Press.Alessandro Moschitti, Paul Morarescu, and SandaHarabagiu.
2003.
Open-domain information extrac-tion via automatic semantic labeling.
In Proceed-ings of FLAIRS 2003, pages 397?401, St. Augustine,FL.E.
Noreen.
1989.
Computer-intensive Methods forTesting Hypotheses: An Introduction.
John Wileyand Sons Inc.Sebastian Pado?
and Katrin Erk.
2005.
To causeor not to cause: Cross-lingual semantic matchingfor paraphrase modelling.
In Proceedings of theEUROLAN Workshop on Cross-Linguistic Knowl-edge Induction, pages 23?30, Cluj-Napoca, Roma-nia.Sebastian Pado?
and Mirella Lapata.
2006.
Optimalconstituent alignment with edge covers for seman-tic projection.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Com-putational Linguistics, pages 1161?1168, Sydney,Australia.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The Proposition Bank: An annotated corpus of se-mantic roles.
Computational Linguistics, 31(1):71?106.Marco Pennacchiotti, Diego De Cao, Roberto Basili,Danilo Croce, and Michael Roth.
2008.
Automaticinduction of FrameNet lexical units.
In Proceedingsof the Conference on Empirical Methods in Natu-ral Language Processing, pages 457?465, Honolulu,Hawaii.Dan Shen and Mirella Lapata.
2007.
Using semanticroles to improve question answering.
In Proceed-ings of the joint Conference on Empirical Methodsin Natural Language Processing and Conference onComputational Natural Language Learning, pages12?21, Prague, Czech Republic.Mihai Surdeanu, Sanda Harabagiu, John Williams, andPaul Aarseth.
2003.
Using predicate-argumentstructures for information extraction.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 8?15, Sap-poro, Japan.Robert S. Swier and Suzanne Stevenson.
2004.
Un-supervised semantic role labelling.
In Proceedingsof the Conference on Empirical Methods in Natu-ral Language Processing, pages 95?102.
Bacelona,Spain.Marta Tatu and Dan Moldovan.
2005.
A semantic ap-proach to recognizing textual entailment.
In Pro-ceedings of the joint Human Language TechnologyConference and Conference on Empirical Methodsin Natural Language Processing, pages 371?378,Vancouver, BC.228
