Automatic generation of paraphrasesto be used as translation referencesin objective evaluation measures of machine translationYves Lepage Etienne DenoualATR ?
Spoken language communication research labsKeihanna gakken tosi, 619-0288 Kyoto, Japan{yves.lepage, etienne.denoual}@atr.jpAbstractWe propose a method that automat-ically generates paraphrase sets fromseed sentences to be used as refer-ence sets in objective machine trans-lation evaluation measures like BLEUand NIST.
We measured the qualityof the paraphrases produced in an ex-periment, i.e., (i) their grammatical-ity: at least 99% correct sentences;(ii) their equivalence in meaning: atleast 96% correct paraphrases eitherby meaning equivalence or entailment;and, (iii) the amount of internal lexi-cal and syntactical variation in a set ofparaphrases: slightly superior to thatof hand-produced sets.
The paraphrasesets produced by this method thus seemadequate as reference sets to be used forMT evaluation.1 IntroductionWe present and evaluate a method to automati-cally produce paraphrases from seed sentences,from a given linguistic resource.
Lexical and syn-tactical variation among paraphrases is handledthrough commutations exhibited in proportionalanalogies, while well-formedness is enforced byfiltering with sequences of characters of a cer-tain length.
In an experiment, the quality of theparaphrases produced, i.e., (i) their grammatical-ity, (ii) their equivalence in meaning with the seedsentence, and, (iii) the internal lexical and syn-tactical variation in a set of paraphrases, was as-sessed by sampling and objective measures.2 MotivationParaphrases are an important element in the eval-uation of many natural language processing tasks.Specifically, in the automatic evaluation of ma-chine translation systems, the quality of transla-tion candidates is judged against reference trans-lations that are paraphrases in the target language.Automatic measures like BLEU (PAPINENI et al,2001) or NIST (DODDINGTON, 2002) do so bycounting sequences of words in such paraphrases.It is expected that such reference sets containsynonymous sentences (i.e., paraphrases) that ex-plicit possible lexical and syntactical variations inorder to cope with translation variations in termsand structures (BABYCH and HARTLEY, 2004).In order to produce such reference sets, we pro-pose a method to generate paraphrases from aseed sentence where lexical and syntactical vari-ations are handled by the use of commutations ascaptured by proportional analogies whereas N -sequences are used to enforce fluency of expres-sion and adequacy of meaning.3 The linguistic resource usedThe linguistic resource used in the experimentpresented in this paper relies on the C-STAR col-lection of utterances called Basic Traveler?s Ex-pressions1.
This is a multilingual resource of ex-pressions from the travel and tourism domain thatcontains 162,318 aligned translations in severallanguages, among which English.
The items arequite short as the following examples show (oneline is one item in the corpus), and as the figuresin Table 1 show.1http://www.c-star.org/.57Number of Avg.
size ?
std.
dev.6= sentences in characters in words97,769 35.14 ?
18.81 6.86 ?
3.57Table 1: Some statistics about the linguistic resourceNumber of Avg.
size ?
std.
dev.6= sentences in characters in words42,249 33.15 ?
9.31 6.44 ?
1.90Table 2: Some statistics about the paraphrases producedThank you so much.
Keep the change.Bring plenty of lemon, please.Please tell me about some interesting places[near here.Thank you.
Please sign here.How do you spell your name?The quality of this resource is of at least 99%correct sentences (p-value = 1.92%).
The few in-correct sentences contain spelling errors or slightsyntactical mistakes.4 Our paraphrasing methodology4.1 Our algorithmThe proposed method consists in two phases:firstly, paraphrase detection through equality oftranslation and secondly, paraphrase generationthrough linguistic commutations based on thedata produced in the first phase:?
Detection: find sentences which share asame translation in the multilingual resource(4.2);?
Generation: produce new sentences by ex-ploiting commutations (4.3); limit combina-torics by contiguity constraints (4.4).Each of the steps of the previous algorithm isexplained in details in the following sections.4.2 Initialisation by paraphrase detectionIn a first phase we initialise our data by para-phrase detection.
By definition, paraphrase isan equivalence in meaning, thus, different sen-tences having the same translation ought to beconsidered equivalent in meaning, i.e., they areparaphrases2.
As the linguistic resource used2This is basically the same approach as (OHTAKE andYAMAMOTO, 2003, p. 3 and 4).in the present experiment is a multilingual cor-pus, we have at our disposal the correspondingtranslations in different languages for each of itssentences.
For instance, the following Englishsentences share a common Japanese translationshown in bold face below.
Therefore, they areparaphrases.A beer, please.
??????????????????????????
?Beer, please.
?????????????????????????????????????????
?Can I have a beer?
????????
?Give me a beer, please.
????????
?I would like beer.
????????
?I?d like a beer, please.
????????
?4.3 Commutation in proportional analogiesfor paraphrase generationIn a second phase, we implement paraphrase gen-eration.
Any given sentence may share commu-tations with other sentences of the corpus.
Suchcommutations are best seen in analogical relationsthat explicit syntagmatic and paradigmatic varia-tions (de SAUSSURE, 1995, part 3, chap 4).
Forinstance, the seed sentenceA slice of pizza, please.58I?d like a beer,please.
: A beer, please.
::I?d like a slice ofpizza, please.
:A slice of pizza,please.I?d like a twin,please.
: A twin, please.
::I?d like a slice ofpizza, please.
:A slice of pizza,please.I?d like a bottle ofred wine, please.
:A bottle of redwine, please.
::I?d like a slice ofpizza, please.
:A slice of pizza,please.Table 3: Some analogies formed with sentences of the linguistic resource that show commutations withthe sentence A slice of pizza, please.
(i) I?d like a beer,please.
: A beer, please.
::I?d like a slice ofpizza, please.
:A slice of pizza,please.
(ii) I?d like a beer,please.
: Can I have a beer?
::I?d like a slice ofpizza, please.
: x(iii) I?d like a beer,please.
: Can I have a beer?
::I?d like a slice ofpizza, please.
:Can I have a sliceof pizza?Table 4: Generating a paraphrase for the seed sentence A slice of pizza, please.
using proportionalanalogies.
(i) The original proportional analogy taken from Table 3.
(ii) Replacing the sentence Abeer, please.
with one of its paraphrases acquired during the detection phase: Can I have a beer?
Thelast sentence of the proportional analogy becomes unknown.
(iii) Solving the analogical equation, i.e.,generating a paraphrase of A slice of pizza, please.enters in the analogies of Table 3.
The replace-ment of some sentences with known paraphrasesin such analogies allows us to produce new sen-tences.
This explains why we needed some para-phrases to start with.
For instance, by replacingthe sentence:A beer, please.with the sentence:Can I have a beer?in the first analogy of Table 3, one gets the fol-lowing analogical equation, that is solved as indi-cated.I?d like a beer, please.
: Can I have a beer?
::I?d like a slice of pizza, please.
: x?
x = Can I have a slice of pizza?It is then legitimate to say that the produced sen-tence:Can I have a slice of pizza?is a paraphrase of the seed sentence (see Table 4).Such a method alleviates the problem of cre-ating templates from examples which would beused in an ulterior phase of generation (BARZI-LAY and LEE, 2003).
Here, all examples in thecorpus are potential templates in their actual rawform, with the advantage that the choice of theplaces where commutations may occur is left toproportional analogy.4.4 Limitation of combinatorics bycontiguity constraintsDuring paraphrase generation, spurious sentencesmay be produced.
For instance, the replacementin the previous analogy, of the sentence:A beer, please.by the following paraphrase detected during thefirst phase:A bottle of beer, please.produces the unfortunate sentence:?A bottle of slice of pizza, please.Moreover, as no complete and valid formalisationof linguistic analogies has yet been proposed, thealgorithm used (LEPAGE, 1998) may deliver suchunacceptable strings as:5943 Could we have a table in the corner?43 I?d like a table in the corner.43 We would like a table in the corner.28 Can we have a table in the corner?5 Can I get a table in the corner?5 In the corner, please.4 We?d like to sit in the corner.2 I?d like to sit in the corner.2 I would like a table in the corner.2 We?d like a table in the corner.1 I?d prefer a table in the corner.1 I prefer a table in the corner.Table 5: Paraphrase candidates for Can we have a table in the corner?
Candidates filtered out by unseenN -sequences (N = 20) are struck out.
Notice that the seed sentence itself has been generated againby the method (4th sentence from the top).
The figures on the left are the frequencies with which thesentence has been generated.
?A slice of pizzthe, pleaset for tha, please.In order to ensure a very high rate of well-formedness among the sentences produced, werequire a method that extracts well-formed sen-tences from the set of generated sentences with avery high precision (to the possible prejudice ofthe recall).To this end, we eliminate all sentences contain-ing sequences of characters of a given length un-seen in the original data3.
It is clear that, by ad-equately tuning the given length, such a methodwill be able to retain a satisfactory number of sen-tences that will be undoubtedly correct, at least inthe sense of the linguistic resource.5 ExperimentsDuring the first phase of paraphrase detection,26, 079 sentences (out of 97, 769) got at least onepossibly incorrect paraphrase candidate with anaverage of 5.35 paraphrases by sentence.
How-ever, the distribution is not uniform: 60 sentencesget more than 100 paraphrases.The maximum is reached with 529 paraphrasesfor the sentence Sure.
Such a sentence has a vari-ety of meanings depending on the context, whichexplains the high number of its possible para-phrases as illustrated below.3This is conform to the trend of using N -sequences to as-sess the quality of outputs of various NLP systems like (LINand HOVY, 2003) for summary generation, (DODDINGTON,2002) for machine translation, etc..Sure.
Here you are.Sure.
This way, please.Certainly, go ahead, please.I?m sure I will.No, I don?t mind a bit.Okay.
I understand quite well, thank you.Sounds fine to me.Yes, I do.. .
.However, such an example shows also that themore the paraphrases obtained by this method, theless reliable their quality.During the second phase of paraphrase gener-ation, the method generated 4, 495, 266 Englishsentences on our linguistic resource.
An inspec-tion of a sample of 400 sentences shows that thequality lies around 23.6% of correct sentences (p-value = 1.19%) in syntax and meaning.
The setof paraphrase candidates obtained on an examplesentence are shown in Table 5.To ensure fluency of expression and adequacyof meaning, the method then filtered out any sen-tence containing an N -sequence unseen in thecorpus (see Section 4.4).
The best value for Nthat allowed us to obtain a quality rate at the samelevel to that of the original linguistic resource was20.As a final result, the number of seed sentencesfor which we obtained at least one paraphrase is16, 153.
With a total number of 147, 708 para-60phrases generated4, the average number of para-phrases per sentence is 8.65 with a standard de-viation of 16.98 which means that the distribu-tion is unbalanced.
The graph on the left of Fig-ure 1 shows the number of seed sentences with thesame number of paraphrases.
while the graph onthe right shows the number of paraphrases againstthe length of the seed sentence in words.6 Quality of the generated paraphrases6.1 Well-formedness of the generatedparaphrasesThe grammatical quality of the paraphrase candi-dates obtained was evaluated on a sample of 400sentences: at least 99% of the paraphrases maybe considered grammatically correct (p-value =2.22%).
This quality is approximately the sameas that of the original resource: at least 99% (p-value = 1.92%).An overview of the errors in the generated para-phrases suggests that they do not differ from theones in the original data.
For instance, one notesthat an article is lacking before the noun phrasetourist area in the following sentence:Where is tourist area?Although we are not able to trace the error backto its origin, such a mistake is certainly due to acommutation with a sentence like:Where is information office?that contains a similar mistake and that is foundin the original linguistic resource.6.2 Equivalence in content betweengenerated paraphases and seed sentenceThe semantic quality of the paraphrases producedwas also checked by hand on a sample of 470paraphrases that were compared with their cor-responding seed sentence.
We not only checkedfor strict equivalence, but also for meaning entail-ment5.4The same sentence may have been generated severaltimes for different seed sentences.
Overall there were42, 249 different sentences generated.
Their lengths in char-acters and words are given in Table 2.5Bill Dolan, Chris Brockett, and Chris Quirk,Microsoft Research Paraphrase Corpus, http://-research.microsoft.com/research/nlp-/msr paraphrase.htm.The following three paraphrases on the leftwith their corresponding seed sentences on theright are examples that were judged to be strictequivalences.Can I see some ID?
Could you show mesome ID?Please exchange this.
Could you exchangethis, please.Please send it toJapan.Send it to Japan,please.The following are examples in which there is alack of information either in the paraphrase pro-duced or in the seed sentence.
This is preciselywhat entailment is.Coke, please.
Miss, could I have acoke?I want to changemoney.
Please exchange this.Sunny-side up, please.
Fried eggs, sunny-sideup, please.The result of the sampling is that the paraphrasecandidates can be considered valid paraphrases inat least 94% of the cases either by equivalenceor entailment (p-value = 3.05%).
The followingsentences exemplify the remaining cases wheretwo sentences were not judged valid paraphrasesof one another.Do you charge extra ifI drop it off?There will be a dropoff charge.Here?s one for you,sir.You can get one here.There it is.
Yes, please sit down.Table 6 summarises the distribution of para-phrase candidates according to the abovemen-tionned classification.610501001502002503003501  10  100  1000  10000number of paraphrasesnumber of seed sentences (log scale)0501001502002503003500  2  4  6  8  10  12  14  16  18number of paraphraseslength of seed sentences in wordsFigure 1: Number of seed sentences with the same number of paraphrases (on the left).
Number ofparaphrases by length of seed sentence in words (on the right).00.20.40.60.810  2  4  6  8  10  12  14averageBLEUscoreslength of seed sentence in words00.20.40.60.810  2  4  6  8  10  12  14averageNISTscoreslength of seed sentence in words00.20.40.60.810  20  40  60  80  100  120averageBLEUscoresnumber of paraphrases / seed sentence00.20.40.60.810  20  40  60  80  100  120averageNISTscoresnumber of paraphrases / seed sentenceFigure 2: BLEU and NIST scores by length of seed sentence (upper graphs) and by number of para-phrases per seed sentence (lower graphs).
In these graphs, each point is the score of a set of paraphrasesagainst the seed sentence they were produced for.
Lower scores indicate a greater lexical and syntacticalvariation in paraphrases.
The connected points show mean values along the axis of abscissae.62Paraphrase Not a paraphraseEquivalence Entailment346 104 20Table 6: Equivalence or entailment in meaning of the paraphrases produced, on a sample of 470 para-phrases from various seed sentences.7 Measure of lexical and syntacticalvariation in paraphrases7.1 Objective measuresWe assessed the lexical and syntactical variationof our paraphrases on a sample of 400 seed sen-tences using BLEU and NIST.
On the contrary toevaluation of machine translation where the goalis to obtain high scores in BLEU and NIST, ourgoal here, when comparing a paraphrase to theseed sentence it has been produced for, is to getlow scores.
Indeed, high scores reflect some highcorrelation with translation references that is alesser variation.
As our goal is precisely to pre-pare data for evaluation with BLEU and NIST, itis thus to generate sets of paraphrases that wouldcontain as much variation as possible to expressthe same meaning as the seed sentences, i.e.
welook for low scores in BLEU and NIST.Again, all this can be done safely as long asone is sure that the sentences compared are validsentences and valid paraphrases.
This is the caseof our data, as we have already shown that theparaphrases produced are 99% grammatically andsemantically correct sentences and that they areparaphrases of their corresponding seed sentencesin 94% of the cases.As for the meaning of BLEU and NIST, theyare supposed to measure complementary charac-teristics of translations: namely fluency and ade-quacy (AKIBA et al, 2004, p. 7).
BLEU tendsto measure the quality in form of expression (flu-ency), while NIST6 tends to measure quality inmeaning (adequacy).7.2 ResultsThe scores in BLEU and NIST (both on a scalefrom 0 to 1) shown in Figure 2 are interpreted6Formally, NIST is an open scale.
Hence, scores cannotbe directly compared for different seed sentences.
We thusnormalised them by the score of the seed sentence againstitself.
In this way, NIST scores become comparable for dif-ferent seed sentences.as a measure of the lexical and syntactical vari-ation among paraphrases.
The lower they are, thegreater the variation.
The upper graphs show thatthis variation depends clearly on the lengths of theseed sentences.
The shorter the seed sentence,the greater the variation among the paraphrasesproduced by this method.
This is no surprise asthe detection phase introduces a bias as was men-tionned in Section 5 with the example sentenceSure.The lower graphs show that the variation doesnot depend on the number of paraphrases per seedsentence.
Hence, on the contrary to a methodthat would produce more variations as more para-phrases are generated, in our method, the varia-tion is not expected to change when one producesmore and more paraphrases (however, the gram-matical quality or the paraphrasing quality couldchange).
In this sense, the method is scalable, i.e.,one could tune the number of paraphrases wishedwithout considerably altering the lexical and syn-tactical variation.7.3 Comparison with reference setsproduced by handWe compared the lexical and syntactical variationof our paraphrases with paraphrases created byhand for a past MT evaluation campaign (AKIBAet al, 2004) in two language pairs: Japanese toEnglish and Chinese to English.For every reference set, we evaluated each sen-tence against one chosen at random and left out.The mean of all these evaluation scores gives anindication on the overall internal lexical and syn-tactical variation inside the reference sets.
Thelower the scores, the better the lexical and syn-tactical variation.
This scheme was applied toboth reference sets created by hand, and to theone automatically produced by our method.
Thescores obtained are shown on Figure 7.
WhereasBLEU scores are comparable for all referencesets, which indicates no notable difference in flu-63Average AverageBLEU NISTAutomatically produced set 0.11 0.39Hand-produced set 1 0.10 0.49Hand-produced set 2 0.11 0.49Table 7: Measure of the lexical and syntactical variation of various reference sets produced by handand automatically produced by our method.
The lower the scores, the better the lexical and syntacticalvariation.ency, NIST scores are definitely better for the au-tomatically produced reference set: this hints at apossibly richer lexical variation.8 ConclusionWe reported a technique to generate paraphrasesin the view of constituting reference sets for ma-chine translation evaluation measures like BLEUand NIST.
In an experiment with a linguistic re-source of 97, 769 sentences we generated 8, 65paraphrases in average for 16, 153 seed sentences.The grammaticality was evaluated by samplingand was shown to be of at least 99% grammati-cally and semantically correct sentences (p-value= 2.22%), a quality comparable to that of the orig-inal linguistic resource.
In addition, at least 96%of the candidates (p-value = 1.92%) were correctparaphrases either by meaning equivalence or en-tailment.Finally, the lexical and syntactical variationwithin each paraphrase set was assessed usingBLEU and NIST against the seed sentence.
It wasfound that the lexical and syntactical variation didnot depend upon the number of paraphrases gen-erated, but on the length of the seed sentence.Going back to the view of constituting refer-ence sets for machine translation evaluation, notonly are the paraphrase sets produced by thismethod correct sentences and valid paraphrases,but they also exhibit an internal lexical and syn-tactical variation which was shown to be slightlysuperior to that of two evaluation campaign setsof paraphrases produced by hand.AcknowledgementsThis research was supported in part by the Na-tional Institute of Information and Communica-tions Technology.ReferencesYasuhiro AKIBA, Marcello FEDERICO, NorikoKANDO, Hiromi NAKAIWA, Michael PAUL, andJun?ichi TSUJII.
2004.
Overview of the IWSLT04evaluation campaign.
In Proc.
of the InternationalWorkshop on Spoken Language Translation, pages1?12, Kyoto, Japan.Bogdan BABYCH and Anthony HARTLEY.
2004.
Ex-tending the BLEU MT evaluation method with fre-quency weighting.
In Proceedings of ACL 2004,pages 621?628, Barcelone, July.Regina BARZILAY and Lillian LEE.
2003.
Learn-ing to paraphrase: An unsupervised approach us-ing multiple-sequence alignment.
In HLT-NAACL2003: Main Proceedings, pages 16?23.Ferdinand de SAUSSURE.
1995.
Cours de linguis-tique ge?ne?rale.
Payot, Lausanne et Paris.
[1ee?d.
1916].George DODDINGTON.
2002.
Automatic evalua-tion of machine translation quality using N-gramco-occurrence statistics.
In Proceedings of HumanLanguage Technology, pages 128?132, San Diego,March.Yves LEPAGE.
1998.
Solving analogies on words:an algorithm.
In Proceedings of COLING-ACL?98,volume I, pages 728?735, Montre?al, August.Chin-Yew LIN and Eduard HOVY.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of HLT-NAACL 2003, pages 71?78, Edmonton, May.Kiyonori OHTAKE and Kazuhide YAMAMOTO.
2003.Applicability analysis of corpus-derived para-phrases toward example-based paraphrasing.
InLanguage, Information and Computation, Proceed-ings of 17th Pacific Asia Conference, pages 380?391.Kishore PAPINENI, Salim ROUKOS, Todd WARD, andWei-Jing ZHU.
2001.
Bleu: a method for automaticevaluation of machine translation.
Research reportRC22176, IBM, September.64
