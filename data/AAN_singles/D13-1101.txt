Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 989?999,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsAutomatically Detecting and Attributing Indirect QuotationsSilvia Pareti?
Tim O?Keefe??
Ioannis Konstas James R. Curran?
Irena Koprinska?ILCC, School of Informatics ?
e-lab, School of ITUniversity of Edinburgh University of SydneyUnited Kingdom NSW 2006, Australia{s.pareti,i.konstas}@sms.ed.ac.uk {tokeefe,james,irena}@it.usyd.edu.auAbstractDirect quotations are used for opinion min-ing and information extraction as they have aneasy to extract span and they can be attributedto a speaker with high accuracy.
However,simply focusing on direct quotations ignoresaround half of all reported speech, which isin the form of indirect or mixed speech.
Thiswork presents the first large-scale experimentsin indirect and mixed quotation extraction andattribution.
We propose two methods of ex-tracting all quote types from news articles andevaluate them on two large annotated corpora,one of which is a contribution of this work.We further show that direct quotation attribu-tion methods can be successfully applied to in-direct and mixed quotation attribution.1 IntroductionQuotations are crucial carriers of information, par-ticularly in news texts, with up to 90% of sentencesin some articles being reported speech (Bergleret al 2004).
Reported speech is a carrier of evi-dence and factuality (Bergler, 1992; Saur??
and Puste-jovsky, 2009), and as such, text mining applicationsuse quotations to summarise, organise and validateinformation.
Extraction of quotations is also rele-vant to researchers interested in media monitoring.Most quotation attribution studies (Pouliquenet al 2007; Glass and Bangay, 2007; Elson andMcKeown, 2010) thus far have limited their scopeto direct quotations (Ex.1a), as they are delimited?
*These authors contributed equally to this work.by quotation marks, which makes them easy to ex-tract.
However, annotated resources suggest that di-rect quotations represent only a limited portion of allquotations, i.e., around 30% in the Penn AttributionRelation Corpus (PARC), which covers Wall StreetJournal articles, and 52% in the Sydney MorningHerald Corpus (SMHC), with the remainder being in-direct (Ex.1c) or mixed (Ex.1b) quotations.
Retriev-ing only direct quotations can miss key content thatcan change the interpretation of the quotation (Ex.1b) and will entirely miss indirect quotations.
(1) a.
?For 10 million, you can move $100 mil-lion of stocks,?
a specialist on the Big Boardgripes.
?That gives futures traders a lotmore power.?b.
Police would only apply for the restrictionswhen ?we have a lot of evidence that late-night noise.
.
.
is disturbing the residents ofthat neighbourhood?, Superintendent TonyCooke said.c.
Mr Walsh said Rio was continuing to holddiscussions with its customers to arrive at amutually agreed price.Previous work on extracting indirect and mixedquotations has suffered from a lack of large-scaledata, and has instead used hand-crafted lexica of re-porting verbs with rule-based approaches.
The lackof data has also made comparing the relative meritof these approaches difficult, as existing evaluationsare small-scale and do not compare multiple meth-ods on the same data.In this work we address this lack of clear, com-parable results by evaluating two baseline meth-989Method Language Test Size Results(quotations) P RKrestel et al(2008) hand-built grammar English 133 74% 99%Sarmento and Nunes (2009) patterns over text Portuguese 570 88% 5%1Fernandes et al(2011) ML and regex Portuguese 205 64%2 67%2de La Clergerie et al(2011) patterns over parse French 40 87% 70%Schneider et al(2010) hand-built grammar English N/D 56%2 52%2Table 1: Related work on direct, indirect and mixed quotation extraction.
Note that they are not directly comparableas they apply to different languages and greatly differ in evaluation style and size of test set.
1 Figure estimated by theauthors for extracting 570 quotations from 26k articles.
2 Results are for quotation extraction and attribution jointly.ods against both a token-based approach that uses aConditional Random Field (CRF) to predict IOB la-bels, and a maximum entropy classifier that predictswhether parse nodes are quotations or not.
We eval-uate these approaches on two large-scale corporafrom the news domain that together include over18,000 quotations.
One of these corpora (SMHC) is aa contribution of this work, while our results are thefirst presented on the other corpus (PARC).
Insteadof relying on a lexicon of reporting verbs, we de-velop a classifier to detect verbs introducing a quo-tation.
To inform future research we present resultsfor direct, indirect, and mixed quotations, as well asoverall results.Finally, we use the direct quotation attributionmethods described in O?Keefe et al(2012) andshow that they can be successfully applied to indi-rect and mixed quotations, albeit with lower accu-racy.
This leads us to conclude that attributing indi-rect and mixed quotations to speakers is harder thanattributing direct quotations.With this work, we set a new state of the art inquotation extraction.
We expect that the main con-tribution of this work will be that future methods canbe evaluated in a comparable way, so that the relativemerit of various approaches can be determined.2 BackgroundPareti (2012) defines an attribution as having asource span, a cue span, and a content span:Source is the span of text that indicates who thecontent is attributed to, e.g.
?president Obama?,?analysts?, ?China?, ?she?.Cue is the lexical anchor of the attribution relation,usually a verb, e.g.
?say?, ?add?, ?quip?.Content is the span of text that is attributed.Based on the type of attitude the source expressestowards a proposition or eventuality, attributions aresubcategorised (Prasad et al 2006) into assertions(Ex.2a) and beliefs (Ex.2b), which imply differentdegrees of commitment, facts (Ex.2c), expressingevaluation or knowledge, and eventualities (Ex.2d),expressing intention or attitude.
(2) a. Mr Abbott said that he will win the election.b.
Mr Abbott thinks he will win the election.c.
Mr Abbott knew that Gillard was in Sydney.d.
Mr Abbott agreed to the public sector cuts.Only assertion attributions necessarily imply aspeech act.
Their content corresponds to a quotationspan and their source is generally referred to in theliterature as the speaker.
Direct, indirect and mixedquotations differ in the degree of factuality they en-tail, since the former are by convention interpretedas a verbatim transcription of an utterance whereasindirect and the non-quoted portion of mixed quota-tions can be paraphrased forms of the original word-ing, and are thus filtered by the writer?s perspective.The first speaker attribution systems (Zhang et al2003; Mamede and Chaleira, 2004; Glass and Ban-gay, 2007) originate from the narrative domain andwere concerned with the identification of differentcharacters for speech synthesis applications.
Directquotation attribution, with direct quotations beinggiven or extracted heuristically, has been the focusof further studies in both the narrative (Elson andMcKeown, 2010) and news (Pouliquen et al 2007;Liang et al 2010) domains.
The few studies that990have addressed the extraction and attribution of in-direct and mixed quotations are discussed below.Krestel et al(2008) developed a quotation ex-traction and attribution system that combines a lexi-con of 53 common reporting verbs and a hand-builtgrammar to detect constructions that match 6 gen-eral lexical patterns.
They evaluate their work on 7articles from the Wall Street Journal, which contain133 quotations, achieving macro-averaged Precision(P ) of 99% and Recall (R) of 74% for quotationspan detection.
PICTOR (Schneider et al 2010) re-lies instead on a context-free grammar for the extrac-tion and attribution of quotations.
PICTOR yielded75% P and 86% R in terms of words correctly as-cribed to a quotation or speaker, while it achieved56% P and 52% R when measured in terms of com-pletely correct quotation-speaker pairs.SAPIENS (de La Clergerie et al 2011) extractsquotations from French news, by using a lexiconof reporting verbs and syntactic patterns to extractthe complement of a reporting verb as the quota-tion span and its subject as the source.
They eval-uated 40 randomly sampled quotations and foundthat their system made 32 predictions and correctlyidentified the span in 28 of the 40 cases.
Verba-tim (Sarmento and Nunes, 2009) extracts quotationsfrom Portuguese news feeds by first finding one of35 speech verbs and then matching the sentence toone of 19 patterns.
Their manual evaluation showsthat 11.9% of the quotations Verbatim finds are er-rors and that the system identifies approximately onedistinct quotation for every 46 news articles.The system presented by Fernandes et al(2011)also works over Portuguese news.
Their work is theclosest to ours as they partially apply supervised ma-chine learning to quotation extraction.
Their workintroduces GloboQuotes, a corpus of 685 news itemscontaining 1,007 quotations of which 802 were usedto train an Entropy Guided Transformation Learn-ing (ETL) algorithm (dos Santos and Milidiu?, 2009).They treat quotation extraction as an IOB labellingtask, where they use ETL with POS and NE featuresto identify the beginning of a quotation, while theinside and outside labels are found using regular ex-pressions.
Finally they use ETL to attribute quota-tions to their source.
The overall system achieves64% P and 67% R.We have summarised these approaches in Table 1,SMHC PARCCorpus Doc Corpus DocDocs 965 - 2,280 -Tokens 601k 623.3 1,139k 499.9Quotations 7,991 8.3 10,526 4.6Direct 4,204 4.4 3,262 1.4Indirect 2,930 3.0 5,715 2.5Mixed 857 0.9 1,549 0.6Table 2: Comparison of the SMHC and PARC corpora, re-porting their document and token size and per-type occur-rence of quotations overall and per document (average).which shows that the majority of evaluations thus farhave been small-scale.
Furthermore, the publishedresults do not include any comparisons with previ-ous work, which prevents a quantitative comparisonof the approaches, and they do not include resultsbroken down by whether the quotation is direct, in-direct, or mixed.
It is these issues that motivate ourwork.3 CorporaWe perform our experiments over two large corporafrom the news domain.3.1 Penn Attribution Relations Corpus (PARC)Our first corpus (Pareti, 2012), which we will re-fer to as PARC, is a semi-automatically built ex-tension to the attribution annotations included inthe PDTB (Prasad et al 2008).
The corpus covers2,280 Wall Street Journal articles and contains an-notations of assertions, beliefs, facts, and eventual-ities, which are altogether referred to as attributionrelations (ARs).
For this work we use only the asser-tions, as they correspond to quotations (direct, indi-rect and mixed).
The drawback of this corpus is thatit is not yet fully annotated, i.e., it comprises positiveand unlabelled data.The corpus includes a test set of 14 articles thatare fully annotated, which enables us to properlyevaluate our work and estimate that a proportion of30-50% of ARs are unlabelled in the rest of the cor-pus.
The test set was manually annotated by two ex-pert annotators.
The annotators identified 491 ARs,of which 22% were nested within another AR, with991an agreement score of 87%1.
The agreement for theselection of the content and source spans of com-monly annotated ARs was 95% and 94% respec-tively.
In this work we address only non-embeddedassertions, so the final test-set includes 267 quotes,totalling 321 non-discontinuous gold spans.3.2 Sydney Morning Herald Corpus (SMHC)We based our second corpus on the existing anno-tations of direct quotations within Sydney MorningHerald articles presented in O?Keefe et al(2012).In that work we defined direct quotations as anytext between quotation marks, which included thedirectly-quoted portion of mixed quotations, as wellas scare quotes.
Under that definition direct quo-tations could be automatically extracted with veryhigh accuracy, so annotations in that work wereover the automatically extracted direct quotations.As part of this work one annotator removed scarequotes, updated mixed quotations to include boththe directly and indirectly quoted portions, andadded whole new indirect quotations.
The anno-tation scheme was developed to be comparable tothe scheme used in the PARC corpus (Pareti, 2012),although the SMHC corpus only includes assertionsand does not annotate the lexical cue.The resulting corpus contains 7,991 quotationstaken from 965 articles from the 2009 Sydney Morn-ing Herald (we refer to this corpus as SMHC).
Theannotations in this corpus also include the speakersof the quotations, as well as gold standard NamedEntities (NEs).
We use 60% of this corpus as train-ing data (4,872 quotations), 10% as developmentdata (759 quotations), and 30% as test data (2,360quotations).
Early experiments were conducted overthe development data, while the final results weretrained on both the training and development setsand were tested on the unseen test data.3.3 ComparisonTable 2 shows a comparison of the two corpora andthe quotations annotated within them.
SMHC has ahigher density of quotations per document, 8.3 vs.4.6 in PARC, since articles are fully annotated and1The agreement was calculated using the agr metric de-scribed in Wiebe and Riloff (2005) as the proportion of com-monly annotated ARs with respect to the ARs identified overallby Annotator A and Annotator B respectivelyP R FBsay 94.4 43.5 59.5Blist 75.4 71.1 73.2k-NN 88.9 72.6 79.9Table 3: Results for the k-NN verb-cue classifier.
Bsayclassifies as verb-cue all instances of say while Blistmarks as verb-cues all verbs from a pre-compiled list inKrestel et al(2008).were selected to contain at least one quotation.
PARCis instead only partially annotated and comprises ar-ticles with no quotations.
Excluding null-quotationarticles from PARC, the average incidence of anno-tated quotations per article raises to 7.1.
The corporaalso differ in quotation type distribution, with di-rect quotations being largely predominant in SMHCwhile indirect are more common in PARC.4 Experimental Setup4.1 Quotation ExtractionQuotation extraction is the task of extracting thecontent span of all of the direct, indirect, and mixedquotations within a given document.
More pre-cisely, we consider quotations to be acts of com-munication, which correspond to assertions in Pareti(2012).
Some quotations have content spans that aresplit into separate, non-adjacent spans, as in exam-ple (1a).
Ideally the latter span should be marked asa continuation of a quotation, however we considerthis to be out of scope for this work, so we treat eachspan as a separate quotation.4.2 PreprocessingAs a pre-processing step, both corpora were to-kenised and POS tagged, and the potential speak-ers anonymised to prevent over-fitting.
We used theStanford factored parser (Klein and Manning, 2002)to retrieve both the Stanford dependencies and thephrase structure parse.
Quotation marks were nor-malised to a single character, as the quotation di-rection is often incorrect for multi-paragraph quo-tations.4.3 Verb-cue ClassifierVerbs are by far the most common introducer of aquotation.
In PARC verbs account for 96% of all992cues, the prepositional phrase according to for 3%,with the remaining 1% being nouns, adverbials andprepositional groups.
Attributional verbs are not aclosed set, they can vary across styles and genres,and their attributional use is highly dependent on thecontext in which they occur.
It is therefore not possi-ble to simply rely on a pre-compiled list of commonspeech verbs.
Quotations in PARC are introduced by232 verb types, 87 of which are unique occurrences.Not all of the verbs are speech verbs, for exampleadd, which is the second most frequent after say, orthe manner verb gripe (Ex.1a).We used the attributional cues in the PARC cor-pus to develop a separate component of our systemto identify attribution verb-cues.
The classifier pre-dicts whether the head of each verb group is a verb-cue using the k-nearest neighbour (k-NN) algorithm,with k equal to 3.
The classifier uses 20 featuretypes, including:?
Lexical (e.g.
token, lemma, adjacent tokens)?
VerbNet classes membership?
Syntactic (e.g.
node-depth in the sentence, par-ent and sibling nodes)?
Sentence features (e.g.
distance from sentencestart/end, within quotation markers).We compared the system to one baseline, Bsay,that marks every instance of say as a verb-cue, andanother, Blist, that marks every instance of a verbthat is on the list of 53 verbs presented in Krestelet al(2008).
We tested the system on the test set forPARC, which contains 1809 potential verb-cues, ofwhich 354 are positive and 1455 are negative.The results in Table 3 show that the verb-cueclassifier can outperform expert-derived knowledge.The classifier was able to identify verb-cues with Pof 88.9% and R of 72.6%.
While frequently oc-curring verbs are highly predictive, the inclusion ofVerbNet classes (Schuler, 2005) and contextual fea-tures allows for a more accurate classification of pol-ysemous and unseen verbs.Since PARC contains labelled and unlabelled attri-butions, which is detrimental for training, we usedthe verb-cue classifier to identify in the corpus sen-tences that we suspected contained an unlabelled at-tribution.
Sentences containing a verb classified as acue that do not contain a quotation were removedfrom the training set for the quotation extractionmodel.4.4 EvaluationWe use two metrics, listed below, for evaluating thequotation spans predicted by our model against thegold spans from the annotation.Strict The first is a strict metric where a predictedspan is only considered to be correct if it exactlymatches a span from the gold standard.
The stan-dard precision, recall, and F -score can be calculatedusing this definition of correctness.
The drawback ofthis strict score is that if a prediction is incorrect byas little as one token it will be considered completelyincorrect.Partial We also consider an overlap metric(Hollingsworth and Teufel, 2005), which allowspartially correct predictions to be proportionallycounted.
Precision (P ), recall (R), and F -score forthis method are:P =?g?gold?p?pred overlap(g, p)|pred|(1)R =?g?gold?p?pred overlap(p, g)|gold|(2)F =2PR(P + R)(3)Where overlap(x, y) returns the proportion of to-kens of y that are overlapped by x.
For each of thesemetrics we report the micro-average, as the numberof quotations in each document varies significantly.When reporting P for the typewise results we re-strict the set of predicted quotations to only thosewith the requisite type, while still considering thefull set of gold quotations.
Similarly, when calculat-ing R we restrict the set of gold quotations to onlythose with the required type.4.5 BaselinesWe have developed two baselines inspired by thecurrent lexical/syntactic pattern-based approachesin the literature, which combine speech verbs andhand-crafted rules.993Blex Lexical: cue verb + the longest of the spans be-fore or after it until the sentence boundary.Bsyn Syntactic: cue verb + verb syntactic object.Bsyn is close to the model in de La Clergerieet al(2011).Instead of relying on a lexicon of verbs, our base-lines use those identified by the verb-cue classifier.As direct quotations are not always explicitly intro-duced by a cue-verb, we defined a separate baselinewith a rule-based approach (Brule) that returns textbetween quotation marks that has at least 3 tokens,and where the non-stopword and non-proper nountokens are not all title cased.
In our full results weapply each method along with Brule and greedilytake the longest predicted spans that do not overlap.5 Supervised ApproachesWe present two supervised approaches to quotationextraction, which operate over the tokens and thephrase-structure parse nodes respectively.
Despitethe difference in the item being classified, these ap-proaches have some common features:Lexical: unigram and bigram versions of the token,lemma, and POS tags within a window of 5 to-kens either side of the target, all indexed by po-sition.Sentence: features indicating whether the sentencecontains a quotation mark, a NE, a verb-cue, apronoun, or any combination of these.
There isalso a sentence length feature.Dependency: relation with parent, relations withany dependants, as well as versions of thesethat include the head and dependent tokens.External knowledge: position-indexed features forwhether any of the tokens in the sentence matcha known role, organisation, or title.
The titlescome from a small hand-built list, while therole and organisation lists were built by recur-sively following the WordNet (Fellbaum, 1998)hyponyms of person and organization respec-tively.Other: features for whether the target is within quo-tation marks, and whether there is a verb-cuenear the end of the sentence.Strict PartialP R F P R FPARC Brule 75 94 83 96 94 95Token 97 91 94 98 97 97SMHC Brule 87 93 90 98 94 96Token 94 90 92 99 97 98Table 4: PARC and SMHC results on direct quotations.The token based approach is trained and tested on all quo-tations.5.1 Token-based ApproachThe token-based approach treats quotation extrac-tion as analogous to NE tagging, where there are asequence of tokens that need to be individually la-belled.
Each token is given either an I, an O, or a Blabel, where B denotes the first token in a quotation,I denotes the token is inside a quotation, and O indi-cates that the token is not part of a quotation.
For NEtagging it is common to use a sentence as a singlesequence, as NEs do not cross sentence boundaries.This does not work for quotations, as they can crosssentence and even paragraph boundaries.
As such,we treat the entire document as a single sequence,which allows the predicted quotations to span bothsentence and paragraph bounds.We use a linear chain Conditional Random Field(CRF)2 as the learning algorithm, with the commonfeatures listed above, as well as the following fea-tures:Verb: features indicating whether the current tokenis a (possibly indirect) dependent of a verb-cue,and another for whether the token is at the startof a constituent that is a dependent of a verb-cue.Ancestor: the labels of all constituents that containthe current token in their span, indexed by theirdepth in the parse tree.Syntactic: the label, depth, and token span size ofthe highest constituent where the current tokenis the left-most token in the constituent, as wellas its parent, and whether either of those con-tains a verb-cue.2http://www.chokkan.org/software/crfsuite/994Indirect Mixed All1Strict P R F P R F P R FBlex 34 32 33 17 26 20 46 44 45Bsyn 78 46 58 61 40 49 80 63 70Token 66 54 59 55 58 56 76 70 73Constituent 61 50 55 50 38 43 70 64 67ConstituentG 66 42 51 68 49 57 76 62 68Partial P R F P R F P R FBlex 56 66 61 78 79 78 73 79 76Bsyn 89 58 70 88 75 81 92 74 82Token 79 74 76 85 90 87 87 86 87Constituent 78 67 72 84 82 83 86 80 83ConstituentG 80 54 65 90 80 85 90 74 81Table 5: Results on PARC.
1All reports the results over all quotations (direct, indirect and mixed).
For the baselines,this is a combination of the strategy in Blex or Bsyn with the rules for direct quotations.
ConstituentG shows theresults for the constituent model using the gold parse.5.2 Constituent-based ApproachThe constituent approach classifies whole phrasestructure nodes as either quotation or not a quota-tion.
Ideally each quotation would match exactlyone constituent, however this is not always the casein our data.
In cases without an exact match we la-bel every constituent that is a subspan of the quo-tation as a quotation as long as it has a parent thatis not a subspan of the quotation.
In these casesmultiple nodes will be labelled quotation, so a post-processing step is introduced that rebuilds quota-tions by merging predicted spans that are adjacent oroverlapping within a sentence.
Restricting the merg-ing process this way loses the ability to predict quo-tations that cover more than a sentence, but withoutthis restriction too many predicted quotations are er-roneously merged.This approach uses a maximum entropy classi-fier3 with L1 regularisation.
In early experimentswe found that the constituent-based approach per-formed poorly when trained on all quotations, so forthese experiments the constituent classifier is trainedonly on indirect and mixed quotations.
The classifieruses the common features listed above as well as thefollowing features:Span: length of the span, features for whether thereis a verb or a NE.3http://scikit-learn.org/Node: the label, number of descendants, number ofancestors, and number of children of the target.Context: dependency, node, and span features forthe parent and siblings of the target.In addition the lexical features described earlierare applied to both the start and end tokens of thenode?s span, as well as the highest token in the de-pendency parse that is within the span.6 Results6.1 Direct QuotationsTable 4 shows the results for predicting direct quota-tions on PARC and SMHC.
In both corpora and withboth metrics the token-based approach outperformsBrule.
Although direct quotations should be trivialto extract, and a simple system that returns the con-tent between quotation marks should be hard to beat,there are two main factors that confound the rule-based system.The first is the presence of mixed quotations,which is most clearly demonstrated in the differencebetween the strict precision scores and the partialprecision scores for Brule.
Brule will find all ofthe directly-quoted portions of mixed quotes, whichdo not exactly match a quotation, and so will re-ceive a low precision score with the strict metric.However the partial overlap score will reward these995Indirect Mixed All1Strict P R F P R F P R FBlex 37 42 40 15 36 21 50 50 50Bsyn 63 49 55 67 36 47 82 72 76Token 69 53 60 80 91 85 82 75 78Constituent 54 49 51 64 42 51 77 72 75Partial P R F P R F P R FBlex 52 68 59 87 77 82 77 84 81Bsyn 75 59 66 89 66 76 91 80 85Token 82 67 74 88 84 86 92 86 89Constituent 77 63 69 91 75 82 91 82 86Table 6: Results on SMHC.
1All reports the results over all quotations (direct, indirect and mixed).
For the baselines,this is a combination of the strategy in Blex or Bsyn with the rules for direct quotations.predictions, as they do partially match a quote, sothere is a large difference in those scores.
Note thatthe reduced strict score does not occur for the tokenmethod, which correctly identifies mixed quotations.The other main issue is the presence of quotationmarks around items such as book titles and scarequotes (i.e.
text that is in quotation marks to distancethe author from a particular wording or claim).
InSection 4.5 we described the methods that we use toavoid scare quotes and titles, which are rule-basedand imperfect.
While these methods increase theoverall F -score of Brule, they do have a negativeimpact on recall, which is why the recall is lowerthan might be expected.
These results demonstratethat although direct quotations can be accurately ex-tracted with rules, the accuracy will be lower thanmight be anticipated and the returned spans will in-clude a number of mixed quotations, which will bemissing some content.6.2 Indirect and Mixed QuotationsThe token approach was also the most effectivemethod for extracting indirect and mixed quotationsas Tables 5 and 6 show.
Indirect quotations wereextracted with strict F -scores of 59% and 60% andpartial F -scores of 76% and 74% in PARC and SMHCrespectively, while mixed quotes were found withstrict F -scores of 56% and 85% and partial F -scoresof 87% and 86%.Although there is a strong interconnection be-tween syntax and attribution, results for Bsyn showthat merely considering attribution as a syntactic re-lation (Skadhauge and Hardt, 2005) has a large im-pact on recall: only a subset of inter-sentential quo-tations can be effectively matched by verb comple-ment boundaries.The constituent model yielded lower results thanthe token one, and in particular it greatly loweredthe recall of mixed quotations in both corpora.
Sincethe model heavily relies on syntax, it is particularlyaffected by errors made by the parser.
The conjunc-tion and in Example 3 is incorrectly attached by theparser to the cue said, leading the classifier to iden-tify two separate spans.
In order to verify the impactof incorrect parsing on the model, we ran the con-stituent model using gold standard parses for PARC.This resulted in an increase in strict P and increasedthe F -score for mixed quotations to 57%, similarlyto the score achieved by the token model.
However,it surprisingly negatively affected R for indirect quo-tations.
(3) Graeme Hugo, said strong links between Aus-tralia?s 700,000 ethnic Chinese and Chinacould benefit both countries and were unlikelyto pose a threat.The tables also report results for the extraction ofall quotations, irrespective of their type.
For thisscore, the baseline models for indirect and mixedquotations are combined with Brule for direct quo-tations.6.3 Model ComparisonWe designed the features for the token and con-stituent models to be largely similar.
This al-996lows us to conclude that the difference in perfor-mance between the token and constituent modelsis largely driven by the class labelling and learn-ing method.
Overall, the token-based approach out-performed both the baselines and the constituentmethod.
Qualitatively we found that the token-basedapproach was making reasonable predictions mostof the time, but would often fail when a quotationwas attributed to a speaker through a parentheticalclause, as in Example 4.
(4) Finding lunar ice, said Tidbinbilla?sspokesman, Glen Nagle, would give amajor boost to NASA?s hopes of returninghumans to the moon by 2020.The token-based approach has a reasonable bal-ance of the various label types, and benefits from adecoding step that allows it to make trade-offs be-tween good local decisions and a good overall so-lution.
By comparison, the constituent-based ap-proach has a large class imbalance, as there are manymore negative (i.e.
not quotation) parse nodes thanthere are positive, which makes finding a good deci-sion boundary difficult.
We experimented with re-ducing the number of negative nodes to consider,but found that the overall F -score was equivalent orworse, largely driven by a drop in recall.
We alsofound that in many cases the constituent-approachpredicted quotes that were too short, or that wereonly the second half of a conjunction, without thefirst half being labelled.
We expect that these issueswould be corrected with the addition of a decodingstep, that forces the classifier to make a good globaldecision.7 Speaker AttributionWhile the focus of this paper is on extracting quota-tions, we also present results on finding the speakerof each quotation.
As discussed in Section 2, quo-tation attribution has been addressed in the litera-ture before, including some work that includes large-scale data (Elson and McKeown, 2010).
However,the large-scale evaluations that exist cover only di-rect quotations, whereas we present results for di-rect, indirect, and mixed quotations.For this evaluation we use four of the methods thatwere introduced in O?Keefe et al(2012).
The firstis a simple rule-based approach (Rule) that returnsthe entity closest to the speech verb nearest the quo-tation, or if there is no such speech verb then theentity nearest the end of the quotation.
The secondmethod uses a CRF which is able to choose betweenup to 15 entities that are in the paragraph containingthe quotation or any preceding it.
The third method(No seq.)
is a binary MaxEnt classifier that predictswhether each entity is the speaker or not the speaker,with the entity achieving the highest speaker proba-bility predicted.
In O?Keefe et al(2012) this modelachieved the best results on the direct quotations inSMHC, despite not using the sequence features or de-coding methods that were available to other models.The final method that we evaluate (Gold) is the ap-proach that uses sequence features that use the gold-standard labels from previous decisions.
As notedby O?Keefe et al this method is not realisable inpractise, however we include these results so thatwe can reassess the claims of O?Keefe et alwhendirect, indirect, and mixed quotations are included.For our results to be comparable we use the list ofspeech verbs that was presented in Elson and McK-eown (2010) and used in O?Keefe et al(2012).Table 7 shows the accuracy of the two meth-ods on both PARC and SMHC, broken down by thetype of the quotation.
The first observation thatwe make about these results in comparison to theO?Keefe et alresults, is that the accuracy is gener-ally lower, even for direct quotations.
This discrep-ancy is caused by differences in our data comparedto theirs, notably that the sequence of quotations isaltered in ours by the introduction of indirect quota-tions, and that some of the direct quotations that theyevaluated would be considered mixed quotations inour corpora.
The rule based method performs par-ticularly poorly on PARC, which is likely caused bythe relative scarcity of direct quotations and the factthat it was designed for direct quotations only.
Di-rect quotations are much more frequent in SMHC, sothe rules that rely on the sequence of speakers wouldlikely perform relatively better than on PARC.While the approach using gold-standard sequencefeatures unsurprisingly performed the best, the moststraightforward learned model (No seq.
), trainedwithout any sequence information, equalled or out-performed the two other non-gold approaches for allquotation types on both corpora.
This indicates thatthe CRF model evaluated here was not able to effec-997Corpus Method Dir.
Ind.
Mix.
AllPARC Rule 70 60 47 62CRF 82 68 65 73No seq.
85 74 65 77Gold 88 79 74 82SMHC Rule 89 76 78 84CRF 83 72 71 78No seq.
91 79 81 87Gold 93 81 83 89Table 7: Speaker attribution accuracy results for both cor-pora over gold standard quotations.tively use the sequence information that is present.8 ConclusionIn this work we have presented the first large-scaleexperiments on the entire quotation extraction andattribution task: evaluating the extraction and at-tribution of direct, indirect and mixed quotationsover two large news corpora.
One of these corpora(SMHC) is a novel contribution of this work, whileour results are the first presented for the other cor-pus (PARC).
This work has shown that while rule-based approaches that return the object of a speechverb are indeed effective, they are outperformed bysupervised systems that can take advantage of addi-tional evidence.
We also show that state-of-the-artquotation attribution methods are less accurate onindirect and mixed quotations than they are on di-rect quotations.Future work will include extending these methodsto extract all attributions, i.e.
beliefs, eventualities,and facts, as well as the source spans.
We will alsoevaluate the effect of adding a decoding step to theconstituent approach.
This work provides an accu-rate and complete quotation extraction and attribu-tion system that can be used for a wide range of tasksin information extraction and opinion mining.AcknowledgementsWe would like to thank Bonnie Webber for her feed-back and assistance.
Pareti has been supported by aScottish Informatics & Computer Science Alliance(SICSA) studentship; O?Keefe has been supportedby a University of Sydney Merit scholarship anda Capital Markets CRC top-up scholarship.
Thiswork has been supported by ARC Discovery grantDP1097291 and the Capital Markets CRC Com-putable News project.ReferencesSabine Bergler.
1992.
Evidential analysis of re-ported speech.
Ph.D. thesis, Brandeis University.Sabine Bergler, Monia Doandes, Christine Gerard,and Rene?
Witte.
2004.
Attributions.
In ExploringAttitude and Affect in Text: Theories and Applica-tions, Technical Report SS-04-07, pages 16?19.Papers from the 2004 AAAI Spring Symposium.Eric de La Clergerie, Benoit Sagot, Rosa Stern, Pas-cal Denis, Gaelle Recource, and Victor Mignot.2011.
Extracting and visualizing quotations fromnews wires.
Human Language Technology.
Chal-lenges for Computer Science and Linguistics,pages 522?532.C?
?cero Nogueira dos Santos and Ruy Luiz Milidiu?.2009.
Entropy guided transformation learning.
InFoundations of Computational, Intelligence Vol-ume 1, Studies in Computational Intelligence,pages 159?184.
Springer.David K. Elson and Kathleen R. McKeown.
2010.Automatic attribution of quoted speech in literarynarrative.
In Proceedings of the Twenty-FourthConference of the Association for the Advance-ment of Artificial Intelligence, pages 1013?1019.Christine Fellbaum.
1998.
WordNet: An electroniclexical database.
MIT press Cambridge, MA.William Paulo Ducca Fernandes, Eduardo Motta,and Ruy Luiz Milidiu?.
2011.
Quotation extractionfor portuguese.
In Proceedings of the 8th Brazil-ian Symposium in Information and Human Lan-guage Technology (STIL 2011), pages 204?208.Kevin Glass and Shaun Bangay.
2007.
A naivesalience-based method for speaker identificationin fiction books.
In Proceedings of the 18th An-nual Symposium of the Pattern Recognition Asso-ciation of South Africa (PRASA07), pages 1?6.Bill Hollingsworth and Simone Teufel.
2005.
Hu-man annotation of lexical chains: Coverage andagreement measures.
In ELECTRA Workshop onMethodologies and Evaluation of Lexical Cohe-sion Techniques in Real-world Applications (Be-yond Bag of Words), page 26.998Dan Klein and Christopher D Manning.
2002.
Fastexact inference with a factored model for naturallanguage parsing.
In Advances in neural informa-tion processing systems, pages 3?10.Ralf Krestel, Sabine Bergler, and Rene?
Witte.
2008.Minding the source: Automatic tagging of re-ported speech in newspaper articles.
In Pro-ceedings of the Sixth International Language Re-sources and Evaluation (LREC?08).Jisheng Liang, Navdeep Dhillon, and KrzysztofKoperski.
2010.
A large-scale system for an-notating and querying quotations in news feeds.In Proceedings of the 3rd International SemanticSearch Workshop, pages 1?5.Nuno Mamede and Pedro Chaleira.
2004.
Charac-ter identification in children stories.
Advances inNatural Language Processing, pages 82?90.Tim O?Keefe, Silvia Pareti, James R. Curran, IrenaKoprinska, and Matthew Honnibal.
2012.
A se-quence labelling approach to quote attribution.
InProceedings of the 2012 Joint Conference on Em-pirical Methods in Natural Language Processingand Computational Natural Language Learning,pages 790?799.Silvia Pareti.
2012.
A database of attribution rela-tions.
In Proceedings of the Eight InternationalConference on Language Resources and Evalua-tion, pages 3213?3217.Bruno Pouliquen, Ralf Steinberger, and Clive Best.2007.
Automatic detection of quotations in multi-lingual news.
In Proceedings of Recent Advancesin Natural Language Processing, pages 487?492.Rashmi Prasad, Nikhil Dinesh, Alan Lee, AravindJoshi, and Bonnie Webber.
2006.
Annotating at-tribution in the Penn Discourse TreeBank.
In Pro-ceedings of the Workshop on Sentiment and Sub-jectivity in Text, pages 31?38.Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh,Alan Lee, Aravind Joshi, Livio Robaldo, andBonnie Webber.
2008.
The Penn Discourse Tree-Bank 2.0 annotation manual.
In Technical report,University of Pennsylvania: Institute for Researchin Cognitive Science.Luis Sarmento and Sergio Nunes.
2009.
Automaticextraction of quotes and topics from news feeds.In 4th Doctoral Symposium on Informatics Engi-neering.Roser Saur??
and James Pustejovsky.
2009.
Factbank:A corpus annotated with event factuality.
In Lan-guage Resources and Evaluation, pages 227?268.Nathan Schneider, Rebecca Hwa, Philip Gianfor-toni, Dipanjan Das, Michael Heilman, Alan W.Black, Frederik L. Crabbe, and Noah A. Smith.2010.
Visualizing topical quotations over timeto understand news discourse.
Technical report,Carnegie Mellon University.Karin K. Schuler.
2005.
Verbnet: A Broad-Coverage, Comprehensive Verb Lexicon.
Ph.D.thesis, Faculties of Computer and InformationScience of the University of Pennsylvania.Peter R. Skadhauge and Daniel Hardt.
2005.
Syn-tactic identification of attribution in the RST tree-bank.
In Proceedings of the Sixth InternationalWorkshop on Linguistically Interpreted Corpora.Janyce Wiebe and Ellen Riloff.
2005.
Creatingsubjective and objective sentence classifiers fromunannotated texts.
In Computational Linguisticsand Intelligent Text Processing, pages 486?497.Springer.Jason Zhang, Alan Black, and Richard Sproat.2003.
Identifying speakers in children?s storiesfor speech synthesis.
In Proceedings of EU-ROSPEECH, pages 2041?2044.999
