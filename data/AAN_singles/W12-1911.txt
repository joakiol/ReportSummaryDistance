NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 84?89,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsUnsupervised Dependency Parsingusing Reducibility and Fertility features?David Marec?ek and Zdene?k Z?abokrtsky?Charles University in Prague, Faculty of Mathematics and PhysicsInstitute of Formal and Applied LinguisticsMalostranske?
na?me?st??
25, Prague{marecek,zabokrtsky}@ufal.mff.cuni.czAbstractThis paper describes a system for unsuper-vised dependency parsing based on Gibbssampling algorithm.
The novel approach in-troduces a fertility model and reducibilitymodel, which assumes that dependent wordscan be removed from a sentence without vio-lating its syntactic correctness.1 IntroductionOne of the traditional linguistic criteria for recog-nizing dependency relations (including their head-dependent orientation) is that stepwise deletion ofdependent elements within a sentence preservesits syntactic correctness (Lopatkova?
et al, 2005;Ku?bler et al, 2009; Gerdes and Kahane, 2011).1 If aword can be removed from a sentence without dam-aging it, then it is likely to be dependent on someother (still present) word.Our approach allows to utilize information fromvery large corpora.
While the computationally de-manding sampling procedure can be applied onlyon limited data, the unrepeated precomputation of?
This research was supported by the grantsGA201/09/H057 (Res Informatica), MSM0021620838,GAUK 116310, and by the European Commission?s 7thFramework Program (FP7) under grant agreement n?
247762(FAUST).1Of course, all the above works had to respond to the noto-rious fact that there are many language phenomena precludingthe ideal (word by word) sentence reducibility (e.g.
in the caseof prepositional groups, or in the case of subjects in English fi-nite clauses).
However, we borrow only the very core of thereducibility idea.statistics for reducibility estimates can easily exploitmuch larger data.2 Precomputing PoS tag reducibility scoresWe call a word (or a sequence of words) in a sen-tence reducible, if the sentence after removing theword remains grammatically correct.
Although wecannot automatically recognize grammaticality ofsuch newly created sentence, we can search for itin a large corpus.
If we find it, we assume the wordwas reducible in the original sentence.
It is obvi-ous that the number of such reducible sequences ofwords found in a corpus is relatively low.
However,it is sufficient for determining reducibility scores atleast for individual types of words (part-of-speechtags).2Assume a PoS n-gram g = [t1, .
.
.
, tn].
We gothrough the corpus and search for all its occurrences.For each such occurrence, we remove the respec-tive words from the current sentence and check inthe corpus whether the rest of the sentence occurs atleast once elsewhere in the corpus.3 If so, then suchoccurrence of PoS n-gram is reducible, otherwise itis not.
We denote the number of such reducible oc-currences of PoS n-gram g by r(g).
The number ofall its occurrences is c(g).
The relative reducibility2Although we search for reducible sequences of word formsin the corpus, we compute reducibility scores for sequences ofpart-of-speech tags.
This requires to have the corpus morpho-logically disambiguated.3We do not take into account sentences with less then 10words, because they could be nominal (without any verb) andmight influence the reducibility scores of verbs.84unigrams R bigrams R trigrams RVB 0.04 VBN IN 0.00 IN DT JJ 0.00TO 0.07 IN DT 0.02 JJ NN IN 0.00IN 0.11 NN IN 0.04 NN IN NNP 0.00VBD 0.12 NNS IN 0.05 VBN IN DT 0.00CC 0.13 JJ NNS 0.07 JJ NN .
0.00VBZ 0.16 NN .
0.08 DT JJ NN 0.04NN 0.22 DT NNP 0.09 DT NNP NNP 0.05VBN 0.24 DT NN 0.09 NNS IN DT 0.14.
0.32 NN , 0.11 NNP NNP .
0.15NNS 0.38 DT JJ 0.13 NN IN DT 0.23DT 0.43 JJ NN 0.14 NNP NNP , 0.46NNP 0.78 NNP .
0.15 IN DT NNP 0.55JJ 0.84 NN NN 0.22 DT NN IN 0.59RB 2.07 IN NN 0.67 NNP NNP NNP 0.64, 3.77 NNP NNP 0.76 IN DT NN 0.80CD 55.6 IN NNP 1.81 IN NNP NNP 4.27Table 1: Reducibility scores of the most frequent PoS tagEnglish n-grams.R(g) of a PoS n-gram g is then computed asR(g) =1Nr(g) + ?1c(g) + ?2, (1)where the normalization constant N , which ex-presses relative reducibility over all the PoS n-grams(denoted by G), causes the scores are concentratedaround the value 1.N =?g?G(r(g) + ?1)?g?G(c(g) + ?2)(2)Smoothing constants ?1 and ?2, which prevent re-ducibility scores from being equal to zero, are set asfollows: 4?1 =?g?G r(g)?g?G c(g), ?2 = 1 (3)Table 1 shows reducibility scores of the most fre-quent English PoS n-grams.
If we consider only uni-grams, we can see that the scores for verbs are oftenamong the lowest.
Verbs are followed by preposi-tions and nouns, and the scores for adjectives and ad-verbs are very high for all three examined languages.That is desired, because the reducible unigrams willmore likely become leaves in dependency trees.3 Dependency ModelsWe introduce a new generative model that is differ-ent from the widely used Dependency Model with4This setting causes that even if a given PoS n-gram isnot reducible anywhere in the corpus, its reducibility score is1/(c(g) + 1).Valence (DMV).5 Our generative model introducesfertility of a node.
For a given head, we first gener-ate the number of its left and right children (fertilitymodel) and then we fill these positions by generat-ing its individual dependents (edge model).
If a zerofertility is generated, the head becomes a leaf.Besides the fertility model and the edge model,we use two more models (subtree model and dis-tance model), which force the generated trees tohave more desired shape.3.1 Fertility modelWe express a fertility of a node by a pair of num-bers: the number of its left dependents and the num-ber of its right dependents.6 Fertility is conditionedby part-of-speech tag of the node and it is computedfollowing the Chinese restaurant process.7 The for-mula for computing probability of fertility fi of aword on the position i in the corpus is as follows:Pf (fi|ti) =c?i(?ti, fi?)
+ ?P0(fi)c?i(?ti?)
+ ?, (4)where ti is part-of-speech tag of the word on the po-sition i, c?i(?ti, fi?)
stands for the count of wordswith PoS tag ti and fertility fi in the history, andP0 is a prior probability for the given fertility whichdepends on the total number of node dependents de-noted by |fi| (the sum of numbers of left and rightdependents):P0(fi) =12|fi|+1(5)This prior probability has a nice property: for agiven number of nodes, the product of fertility prob-abilities over all the nodes is equal for all possi-ble dependency trees.
This ensures balance of thismodel during inference.5In DMV (Klein and Manning, 2004) and in the extendedmodel EVG (Headden III et al, 2009), there is a STOP sign in-dicating that no more dependents in a given direction will begenerated.
Given a certain head, all its dependents in left di-rection are generated first, then the STOP sign in that direction,then all its right dependents and then STOP in the other direc-tion.
This process continues recursively for all generated de-pendents.6For example, fertility ?1-3?
means that the node has oneleft and three right dependents, fertility ?0-0?
indicates that it isa leaf.7If a specific fertility has been frequent for a given PoS tagin the past, it is more likely to be generated again.85Besides the basic fertility model, we introducealso an extended fertility model, which uses fre-quency of a given word form for generating numberof children.
We assume that the most frequent wordsare mostly function words (e.g.
determiners, prepo-sitions, auxiliary verbs, conjunctions).
Such wordstend to have a stable number of children, for exam-ple (i) some function words are exclusively leaves,(ii) prepositions have just one child, and (iii) attach-ment of auxiliary verbs depends on the annotationstyle, but number of their children is also not veryvariable.
The higher the frequency of a word form,the higher probability mass is concentrated on onespecific number of children and the lower Dirichlethyperparameter ?
in Equation 4 is needed.
The ex-tended fertility is described by equationP ?f (fi|ti, wi) =c?i(?ti, fi?)
+ ?eF (wi)P0(fi)c?i(?ti?)
+ ?eF (wi), (6)where F (wi) is a frequency of the word wi, whichis computed as a number of words wi in our corpusdivided by number of all words.3.2 Edge modelAfter the fertility (number of left and right depen-dents) is generated, the individual slots are filled us-ing the edge model.
A part-of-speech tag of each de-pendent is conditioned by part-of-speech tag of thehead and the edge direction (position of the depen-dent related to the head).8Similarly as for the fertility model, we employChinese restaurant process to assign probabilities ofindividual dependent.Pe(tj |ti, dj) =c?i(?ti, tj , dj?)
+ ?c?i(?ti, dj?)
+ ?|T |, (7)where ti and tj are the part-of-speech tags of thehead and the generated dependent respectively; dj isa direction of edge between the words i and j, whichcan have two values: left and right.
c?i(?ti, tj , dj?
)stands for the count of edges ti ?
tj with the direc-tion dj in the history, |T | is a number of unique tagsin the corpus and ?
is a Dirichlet hyperparameter.8For the edge model purposes, the PoS tag of the technicalroot is set to ?<root>?
and it is in the zero-th position in thesentence, so the head word of the sentence is always its rightdependent.3.3 Distance modelDistance model is an auxiliary model that preventsthe resulting trees from being too flat.9 This sim-ple model says that shorter edges are more probablethan longer ones.
We define probability of a distancebetween a word and its parent as its inverse value,10which is then normalized by the normalization con-stant d.Pd(i, j) =1d(1|i?
j|)?
(8)The hyperparameter ?
determines the weight of thismodel.3.4 Subtree modelThe subtree model uses the reducibility measure.
Itplays an important role since it forces the reduciblewords to be leaves and reducible n-grams to be sub-trees.
Words with low reducibility are forced to-wards the root of the tree.
We define desc(i) as asequence of tags [tl, .
.
.
, tr] that corresponds to allthe descendants of the word wi including wi, i.e.
thewhole subtree of wi.
The probability of such sub-tree is proportional to its reducibility R(desc(i)).The hyperparameter ?
determines the weight of themodel; s is a normalization constant.Ps(i) =1sR(desc(i))?
(9)3.5 Probability of the whole treebankWe want to maximize the probability of the wholegenerated treebank, which is computed as follows:Ptreebank =n?i=1(P ?f (fi|ti, wi) (10)Pe(ti|tpi(i), di) (11)Pd(i, pi(i)) (12)Ps(i)), (13)where pi(i) denotes the parent of the word on theposition i.
We multiply the probabilities of fertil-ity, edge, distance from parent, and subtree over all9Ideally, the distance model would not be needed, but exper-iments showed that it helps to infer better trees.10Distance between any word and the technical root of thedependency tree was set to 10.
Since each technical root hasonly one dependent, this value does not affect the model.86The   dog   was   in   the   park  .
(((The) dog) was (in ((the) park)) (.
))Figure 1: Arrow and bracketing notation of a projectivedependency tree.words (nodes) in the corpus.
The extended fertilitymodel P ?f can be substituted by its basic variant Pf .4 Sampling algorithmFor stochastic searching for the most probable de-pendency trees, we employ Gibbs sampling, a stan-dard Markov Chain Monte Carlo technique (Gilks etal., 1996).
In each iteration, we loop over all wordsin the corpus in a random order and change the de-pendencies in their neighborhood (a small changedescribed in Section 4.2).
In the end, ?average?
treesbased on the whole sampling are built.4.1 InitializationBefore the sampling starts, we initialize the projec-tive trees randomly in a way that for each sentence,we choose randomly one word as the head and attachall other words to it.114.2 Small Change OperatorWe use the bracketing notation for illustrating thesmall change operator.
Each projective dependencytree consisting of n words can be expressed by npairs of brackets.
Each bracket pair belongs to onenode and delimits its descendants from the rest ofthe sentence.
Furthermore, each bracketed segmentcontains just one word that is not embedded deeper;this node is the segment head.
An example of thisnotation is in Figure 1.The small change is then very simple.
We removeone pair of brackets and add another, so that the con-ditions defined above are not violated.
An exampleof such change is in Figure 2.From the perspective of dependency structures,the small change can be described as follows:11More elaborated methods for generating random trees con-verges to similar results.
Therefore we conclude that the choiceof the initialization mechanism is not so important here.TTThe dogwadosinootpoTTre doki.
(dooT)ddTTThe dogwadosinootpoTTre doki.
(dooT)ddTTThe dogwadosinootpoTTre doki.
(dooT)ddTTThe dogwadosinootpoTTre doki.
(dooT)ddTTd dTTThe dogwadosinootpoTTre doki.
(dooT)ddTTThe dogwadosinootpoTTre doki.
(dooT)ddT Td dFigure 2: An example of small change in a projectivetree.
The bracket (in the park) is removed and there arefive possibilities how to replace it.1.
Pick a random non-root word w (the word in inour example) and find its parent p (the word was).2.
Find all other children of w and p (the wordsdog, park, and .)
and denote this set by C.3.
Choose the new head out of w and p. Mark thenew head as g and the second candidate as d. Attachd to g.4.
Select a neighborhood D adjacent to the wordd as a continuous subset of C and attach all wordsfrom D to d. D may be also empty.5.
Attach the remaining words from C that werenot in D to the new head g.4.3 Building ?average?
treesThe ?burn-in?
period is set to 10 iterations.
Afterthis period, we begin to count how many times anedge occurs at a particular location in the corpus.This counts are collected over the whole corpus withthe collection-rate 0.01.12When the sampling is finished, the final depen-dency trees are built using such edges that belongedto the most frequent ones during the sampling.
Weemploy the maximum spanning tree (MST) algo-rithm (Chu and Liu, 1965) to find them.13 Tree pro-jectivity is not guaranteed by the MST algorithm.5 ExperimentsWe evaluated our parser on 10 treebanks included inthe WILS shared-task data.
Similarly to some previ-ous papers on unsupervised parsing (Gillenwater etal., 2011; Spitkovsky et al, 2011), the tuning exper-iments were performed on English only.
We used12After each small change is made, the edges from the wholecorpus are collected with a probability 0.01.13The weights of edges needed in MST algorithm correspondto the number of times they were present during the sampling.87language tokens (mil.)
language tokens (mil.
)Arabic 19.7 English 85.0Basque 14.1 Portuguese 31.7Czech 20.3 Slovenian 13.7Danish 15.9 Swedish 19.2Dutch 27.1Table 2: Wikipedia texts statisticsEnglish development data for checking functional-ity of the individual models and for optimizing hy-perparameter values.
The best configuration of theparser achieved on English was then used for pars-ing all other languages.
This simulates the situationin which we have only one treebank (English) onwhich we can tune our parser and we want to parseother languages for which we have no manually an-notated treebanks.5.1 DataFor each experiment, we need two kinds of data: asmaller treebank, which is used for sampling andfor evaluation, and a large corpus, from which wecompute n-gram reducibility scores.
The inductionof dependency trees and evaluation is done only onWILS testing data.For obtaining reducibility scores, we usedWikipedia articles downloaded by Majlis?
andZ?abokrtsky?
(2012).
Their statistics across languagesare showed in Table 2.
To make them useful, thenecessary preprocessing steps must have been done.The texts were first automatically segmented and to-kenized14 and then they were part-of-speech taggedby TnT tagger (Brants, 2000), which was trained onthe respective WILS training data.
The quality ofsuch tagging is not very high, since we do not useany lexicons15 or pretrained models.
However, it issufficient for obtaining good reducibility scores.5.2 Setting the hyperparametersThe applicability of individual models and their pa-rameters were tested on English development data14The segmentation to sentences and tokenization was per-formed using the TectoMT framework (Popel and Z?abokrtsky?,2010).15Using lexicons or another pretrained models for taggingmeans using other sources of human annotated data, which isnot allowed if we want to compare our results with others.and full part-of-speech tags.
The four hyperparame-ters ?
(fertility model), ?
(edge model), ?
(distancemodel), and ?
(subtree model), were set by a gridsearch algorithm, which found the following opti-mal values:?e = 0.01, ?
= 1, ?
= 1.5, ?
= 15.3 ResultsThe best setting from the experiments on Englishis now used for evaluating our parser across allWILS treebanks.
Besides using the standard part-of-speech tags (POS), the experiments were done alsoon coarse tags (CPOS) and universal tags (UPOS).From results presented in Table 3, it is obvious thatour systems outperforms all the given baselines con-sidering the average scores across all the testing lan-guages.
However, it is important to note that weused an additional source of information, namelylarge unannotated corpora for computing reducibil-ity scores, while the baseline system (Gillenwater etal., 2011) use only the WILS datasets.6 Conclusions and Future WorkWe have described a novel unsupervised depen-dency parsing system employing new features, suchas reducibility or fertility.
The reducibility scores areextracted from a large corpus, and the computation-ally demanding inference is then done on smallerdata.In future work, we would like to estimate thehyperparameters automatically, for example by theMetropolis-Hastings technique (Gilks et al, 1996).Furthermore, we would like to add lexicalized mod-els and automatically induced word classes insteadof the PoS tags designed by humans.
Finally, wewould like to move towards deeper syntactic struc-tures, where the tree would be formed only by con-tent words and the function words would be treatedin a different way.ReferencesThorsten Brants.
2000.
TnT - A Statistical Part-of-Speech Tagger.
Proceedings of the sixth conferenceon Applied natural language processing, page 8.Y.
J. Chu and T. H. Liu.
1965.
On the Shortest Arbores-cence of a Directed Graph.
Science Sinica, 14:1396?1400.88baselines Gillenwater et al (2011) our approachTreebank random left br.
right br.
CPOS POS UPOS CPOS POS UPOSArabic PADT 37.0 5.2 58.7 14.9 14.5 23.4 12.7 57.2 52.0Basque 3LB 9.8 32.4 22.5 48.9 41.8 30.0 21.0 25.5 22.3Czech PDT 10.8 23.9 29.1 25.6 31.6 27.5 49.1 42.9 44.1Danish CDT 24.7 13.3 47.4 31.8 27.2 26.3 48.4 41.4 49.7Dutch Alpino 17.1 27.9 24.6 23.3 21.2 23.5 28.3 44.2 29.9English Childes 12.8 34.8 22.2 48.8 50.4 36.2 54.2 44.2 49.3English PTB 13.2 31.4 20.4 30.9 34.4 25.9 41.0 50.3 37.5Portuguese Floresta 33.1 25.9 31.1 26.0 31.1 25.5 50.2 49.5 29.3Slovene JOS 9.5 31.1 10.8 36.6 53.3 36.6 30.4 40.8 26.7Swedish Talbanken 23.9 25.8 28.0 27.2 27.2 27.1 48.4 50.6 52.6Average: 19.2 25.2 29.5 31.4 33.3 28.2 38.4 44.7 39.3Table 3: Directed attachment scores on the WILS testing data (all sentences).
Punctuation was excluded.
Comparisonwith simple baselines and the given baseline system (Gillenwater et al, 2011).Kim Gerdes and Sylvain Kahane.
2011.
Defining depen-dencies (and constituents).
In Proceedings of Depen-dency Linguistics 2011, Barcelona.W.
R. Gilks, S. Richardson, and D. J. Spiegelhalter.
1996.Markov chain Monte Carlo in practice.
Interdisci-plinary statistics.
Chapman & Hall.Jennifer Gillenwater, Kuzman Ganchev, Joa?o Grac?a, Fer-nando Pereira, and Ben Taskar.
2011.
PosteriorSparsity in Unsupervised Dependency Parsing.
TheJournal of Machine Learning Research, 12:455?490,February.William P. Headden III, Mark Johnson, and David Mc-Closky.
2009.
Improving unsupervised dependencyparsing with richer contexts and smoothing.
In Pro-ceedings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,NAACL ?09, pages 101?109, Stroudsburg, PA, USA.Association for Computational Linguistics.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: models of de-pendency and constituency.
In Proceedings of the42nd Annual Meeting on Association for Computa-tional Linguistics, ACL ?04, Stroudsburg, PA, USA.Association for Computational Linguistics.Sandra Ku?bler, Ryan T. McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Synthesis Lectures onHuman Language Technologies.
Morgan & ClaypoolPublishers.Marke?ta Lopatkova?, Martin Pla?tek, and Vladislav Kubon?.2005.
Modeling syntax of free word-order languages:Dependency analysis by reduction.
In Va?clav Ma-tous?ek, Pavel Mautner, and Toma?s?
Pavelka, editors,Lecture Notes in Artificial Intelligence, Proceedings ofthe 8th International Conference, TSD 2005, volume3658 of Lecture Notes in Computer Science, pages140?147, Berlin / Heidelberg.
Springer.Martin Majlis?
and Zdene?k Z?abokrtsky?.
2012.
Languagerichness of the web.
In Proceedings of LREC2012, Is-tanbul, Turkey, May.
ELRA, European Language Re-sources Association.
In print.Martin Popel and Zdene?k Z?abokrtsky?.
2010.
TectoMT:modular NLP framework.
In Proceedings of the 7thinternational conference on Advances in natural lan-guage processing, IceTAL?10, pages 293?304, Berlin,Heidelberg.
Springer-Verlag.Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-sky.
2011.
Lateen EM: Unsupervised training withmultiple objectives, applied to dependency grammarinduction.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing(EMNLP 2011).89
