A Machine Learning Approach to Answering Questionsfor :Reading Comprehension TestsHwee Ton NgLeong Hwee Teo*Jenn i fe r  Lai  Pheng KwanDSO National Laboratories20 Science Park DriveSingapore 118230{nhweetou, tleonghw, klaiphen}~dso.org.sgAbst rac tIn this paper, we report results on answeringquestions for the reading comprehension task,using a machine learning approach.
We eval-uated our approach on the Remedia data set,a common data set used in several recent pa-pers on the reading comprehension task.
Ourlearning approach achieves accuracy competi-tive to previous approaches that rely on hand-crafted, deterministic rules and algorithms.To the best of our knowledge, this is the firstwork that reports that the use of a machinelearning approach achieves competitive r sultson answering questions for reading compre-hension tests.1 In t roduct ionThe advent of the Internet has resulted in amassive information explosion.
We need tohave an effective and efficient means of locat-ing just the desired information.
The fieldof information retrieval (IR) is the traditionaldiscipline that addresses this problem.However, most of the prior work in IR dealmore with document retrieval rather than "in-formation" retrieval.
This also applies tosearch engines on the Internet.
Current searchengines take a list of input words and return aranked list of web pages that contain (or notcontain) the given words.
It is then left tothe user to search through the returned list ofweb pages for the information that he needs.While finding the web pages that contain thedesired information is an important first step,what an information seeker needs is often ananswer to a question.
That is, given a ques-tion, we want a system to return the exactanswers to the question, and not just the doc-uments to allow us to further search for the* Leong Hwee Teo's current a~at ion:  Defence Med-ical Research Institute, Defence Science and Technol-ogy Agency, 1 Depot Road, Defence Technology TowerA, ~19-05, Singapore 109679 l:leonghw~dsl:a.gov, sganswers .The need for question answering (QA) sys-tems has prompted the initiation of the ques-tion answering track in TREC-8 (Voorheesand Tice, 2000) to address this problem.
Inthe QA track, each participant is given a listof 200 questions, and the goal is to locateanswers to these questions from a documentdatabase consisting of hundreds of thousandsof documents (about two gigabytes of text).Each participant is to return a ranked list ofthe five best answer strings for each question,where ach answer string is a string of 50 bytes(or 250 bytes) that contains an answer to thequestion.
What, when, where, and who ques-tions that have explicit answers given in somedocument inthe database are emphasized, butnot why questions.In a related but independent effort, a groupat MITRE has investigated question answer-ing in the context of the reading comprehen-sion task (Hirschman et al, 1999).
The docu-ments in this task axe 115 children stories atgrade two to five from Remedia Publications,and the task involves answering five questions(who, what, when, where, and why question)per story, as a measure of how well a sys-tem has understood the story.
Each story hasan average of 20 sentences, and the questionanswering task as formulated for a computerprogram is to select a sentence in the storythat answers to a question.
For about 10%of the questions, there is not a single sen-tence in the story that is judged to answerthe question.
Conversely, a question can havemultiple correct answers, where each of sev-eral individual sentences i a correct answer.An example story from the Remedia corpusand its five accompanying questions axe givenin Figure 1.
Each story has a title (such as"Storybook Person Found Alive!")
and date-line (such as "ENGLAND, June, 1989") in theRemedia corpus.124Storybook Person Found Alive!
(ENGLAND, June, 1989) - Christopher Robin is alive and well.
He livesin England.
He is the same person that you read about in the book,Winnie the Pooh.As a boy, Chris l i ved  in a pret ty  home ca l led  Cotchf ie ld  Farm.
WhenChris was three years old,  h is  fa ther  wrote a poem about him.
The poemwas printed in a magazine for others to read.Mr.
Robin then wrote a book.
He made up a fairy tale land where Chrislived.
His friends were animals.
There was a bear called W~nnie thePooh.
There was also an owl and a young pig, called a piglet.
All theanimals were stuffed toys that Chris owned.
Mr. Robin made them cometo life with his words.
The places in the story were all nearCotchfield Farm.Winnie the Pooh was written in 1925.
Children still love to read aboutChristopher Robin and his animal friends.
Most people don't know he isa real person who is grown now.
He has written two books of hisown.
They tell what it is like to be famous.i.
Who is Christopher Robin?2.
What did Mr. Robin do when Chris was three years old?3.
When was Winuie the Pooh written?4.
Where did young Chris live?5.
Why did Chris write two books of his own?Figure h A sample story and its five questionsAlthough both the TREC-8 QA task andthe reading comprehension QA task are aboutquestion answering, there are a few differencesin the two tasks.
In TREC,-8, the answer toa question can be from any of the hundredsof thousands of documents in the database,whereas for the reading comprehension task,the answer only comes from the short storyassociated with the question.
Thus, while theTREC-8 QA task requires efficient indexingand retrieval techniques to narrow down tothe documents hat contain the answers, thisstep is largely not needed for the reading com-prehension task.
Also, an answer as definedin the TREC-8 QA task is a 50-byte or 250-byte answer string, whereas an answer is acomplete sentence in the reading comprehen-sion task.
Another perhaps more subtle differ-ence is that the questions formulated in bothtasks have different motivation: for TI:tEC-8,it is for the purpose of information-seeking,whereas for reading comprehension, it is fortesting whether a system has "understood"the story.
Hence, it may well be that thequestions in TREC-8 can be expected to bemore "cooperative", while those for readingcomprehension can be uncooperative or even"tricky" in nature.In this paper, we only address the ques-tion answering task in the context of read-ing comprehension, although we expect hatthe techniques we developed in this paper willbe equally applicable to answering questionsin an information-seeking context like that ofTREC-8.1252 Rela ted  WorkThe early work on questiion answering by(Lehnert, 1978) focused more on knowledgerepresentation and inference issues, and thework was not targeted at aal~wering questionsfrom unrestricted text.
Prior to 1999, theother notable research work on question an-swering that is designed to work on unre-stricted text (from encyclopedia) is (Kupiec,1993).
However, no large scale evaluation wasattempted, and the work was not based on amachine learning approach.Fueled by the question answering track ini-tiated in TREC-8 (Voorbees and Tice, 2000),there is a recent surge of research activities onthe topic of question answering.
Among theparticipants who returned the best scores atthe TREC-8 QA track (Srihari and Li, 2000;Moldovan et al, 2000; Singhal et al, 2000),none of them uses a machine learning ap-proach.
One exception is the work of (Radevet al, 2000) at the TREC-8 QA track, whichuses logistic regression to rank potential an-swers using a training set with seven features.However, their features are meant for the taskof selecting more specific answer spans, andare different from the features we use in thispaper.
The TREC-8 QA test scores of (Radevet al, 2000) were also considerably ower thanbest QA test scores.Because of the huge number of documentsused in the TRECC-8 QA track, the partici-pants have to perform efficient document in-dexing and retrieval in order to tackle thecomplete QA task.
It has been found thatboth the shallow processing techniques of IR,as well as the more linguistic-oriented naturallanguage processing techniques are needed toperform well on the TREC-8 QA track.
Incontrast, for our current QA work on readingcomprehension, because the answer for eachquestion comes from the associated story, nosophisticated IR indexing and retrieval tech-niques are used.Naturally, our current work on question an-swering for the reading comprehension taskis most related to those of (Hirschman et al,1999; Charniak et al, 2000; Riloffand Thelen,2000; Wang et al, 2000).
In fact, all of thisbody of work as well as ours are evaluated onthe same set of test stories, and are developed(or trained) on the same development set ofstories.
The work of (Hirschman et al, 1999)initiated this series of work, and it reported anaccuracy of 36.3% on answering the questionsin the test stories.
Subsequently, the work of(Riloffand Thelen, 2000) and (Chaxniak et al,2000) improved the accuracy further to 39.7%and 41%, respectively.
However, all of thesethree systems used handcrafted, eterministicrules and algorithms.
In contrast, we adopt amachine learning approach in this paper.The one notable exception is the work of(Wang et al, 2000), which attempted a ma-chine learning approach to question answeringfor the same reading comprehension task.
Un-fortunately, out of the several machine learn-ing algorithms they tried, the best approach(using neural network learning) only managedto obtain an accuracy of 14%.
This workcasts doubt on whether a machine learningapproach to question answering can achieveaccuracy competitive to the handcrafted rule-based approach.
Our current work attemptsto address exactly this issue.3 A Mach ine  Learn ing  ApproachIn this section, we present our machine learn-ing approach to question answering.
We havesuccessfully implemented a question answer-ing system based on this approach.
Our sys-tem is named AQUAREAS (Automated QUes-tion Answering upon REAding Stories).
Theadvantage of a machine learning approach isthat it is more adaptable, robust, flexible, andmaintainable.
There is no need for a human tomanually engineer a set of handcrafted rulesand continuously improve or maintain the setof rules.For every question, our QA task requiresthe computer program to pick a sentence inthe associated story as the answer to thatquestion.
In our approach, we represent eachquestion-sentence pair as a feature vector.Our goal is to design a feature vector epresen-tation such that it provides useful informationto a learning algorithm to automatically buildfive classifiers, one for each question type.
Inprior work (Hirschman et al, 1999; Charniaket al, 2000; Riloffand Thelen, 2000) the num-ber and type of information sources used forcomputation is specific to and rlifFerent foreach question type.
In AQUAREAS, we usethe same set of features for all five questiontypes, leaving it to the learning algorithm toidentify which are the useful features to testfor in each question type.The machine learning approach comprisestwo steps.
First, we design a set of features tocapture the information that helps to distin-126guish answer sentences from non-answer sen-tences.
Next, we use a learning algorithmto generate a classifier for each question typefrom the training examples.Each training example or test example isa feature vector representing one question-sentence pair.
Given a question q in a story,one positive example is generated from eachsentence s that is marked (by the MITREgroup) as an answer to q, and the ques-tion q itself.
For negative training exam-ples, all other sentences that are not markedas answers to a question q can be used.
InAQUAREAS, we use all other sentences thatare marked as answers to the questions otherthan q in the same story to generate the neg-ative examples for question q.
This also helpsto keep the ratio of negative xamples to pos-itive examples from becoming too high.3.1 Feature  Representat ionOur feature representation was designed tocapture the information sources that priorwork (Hirschman et al, 1999; Cha_niak et al,2000; Riloff and Thelen, 2000) used in theircomputations or rules.
We hypothesize thatgiven equivalent information sources, a ma-chine learning approach can do as well as asystem built using handcrafted rules.
Our fea-ture vector consists of 20 features.?
Diff- f rom-  Max-Word-  Match(DMWM)The possible values for this feature are 0,1, 2, 3, .
.
.
.
For a given question q anda sentence s, the value for this featureis computed by first counting the num-ber of matching words present in q and s,where two words match if they have thesame morphological root.
This gives theraw word match score m for the question-sentence pair q and s. Next, we find thehighest raw word match score M over allsentences si in the story and q.
The valueof this feature DMWM for the question-sentence pair q and s is M - rn.
11In an earlier version of AQUAREAS, we simply usedthe raw word match score m as the feature.
However,the learned classifiers did not perform well.
We suspectthat the absolute raw word match score m may notmatter as much as whether asentence has the highestraw word match score M in a story (relative to othersentences in the same story).
We address this deft-ciency in our reformulated ifference-from-maximumcomputation.Intuitively, a feature value of 0 is the best,indicating that for that question-sentencepair q and s, they have the most num-ber of matching words in the story, whencomparing q with all sentences z in thesame story.In the case where there are zero match-ing words between a question q and allsentences sz in a story (i.e., M = 0), thenthis DMWM feature will be assigned 0 forall question-sentence pairs q and si in thestory.
To avoid such a situation where avalue of 0 is also assigned for this featureeven when there are no matching words,we instead assign a very large value (200)to this feature for such cases.D \[if- f rom-Max-Verb -Mat  ch(DMVM)The possible values for this feature are 0,1, 2, 3, .
.
.
.
The value for this featureis computed in exactly the same way asDMWM, except hat we only count mainverb matches between a question and asentence, excluding verbs whose morpho-logical roots are "be", "do" or "have".
(Such verbs tend not to carry as much"semantic" information.
)DMWM-Prev ,  DMVM-Prev ,DMWM-Next ,  DMVM-NextThe possible values for each of these fea-tures are 0, 1, 2, 3, .
.
.
.
Their compu-tation is similar to DMWM and DMVM,except that they are computed from thequestion q and the sentence s-1 (the sen-tence preceding the current sentence s inconsideration), in the case of DMWM-Prey and DMVM-Prev, and the questionq and the sentence s+l (the sentence fol-lowing s) in the case of DMWM-Next andDMVM-Next.
For the title and datelineof a story, we take them as having no pre-vious or next sentences.
For the first sen-tence in the body of a story, there is noprevious entence and likewise for the lastsentence in the body of a story, there isno next sentence.
For all such cases, wegive a raw word/verb match score rn of 0in the computation.We designed these 4 features to captureinformation that will be helpful to thewhy questions, since it has been observedin prior work (Charniak et al, 2000;Riloff and Thelen, 2000) that the answer127sentence to a why question tends to follow(or precede) the sentence in the story thathas the most number of word matcheswith the question.?
Sentence-cont ains=Person,Sentence=cont ains- Organizat ion)Sentence=cont ains-Location,Sentence=conta ins=Date ,  Sentence-conta ins -T imeThe possible values for each of these fea-tures are true or false.
To compute thesefeature values for a sentence, we usedthe Remedia corpus provided by MITREwhich has been hand-tagged with namedentities.
If a sentence contains at leastone word tagged with the named en-tity person, then the feature Sentence-contalns-Person will be assigned the valuetrue.
Its value is false otherwise.
Sim-ilarly for the other four named entitiesorganization, location, date, and time.?
Coreference i n fo rmat ionCoreferenee information does not con-tribute any new features, but rather itmay change the values assigned to thefive features Sentence-contains-Person,Sentence-contains-Organization, .
.
.
.
Byusing the Remedia corpus provided byMITRE which has been hand-taggedwith coreference hains of noun phrases,we can propagate a named entity tagacross all noun phrases in the same coref-erence chain.
We then utilize the revisedpropagated named entities to assign val-ues to the five named entity features for asentence.
The effect of using coreferenceinformation is that for some sentence, anamed entity feature may have its valuechanged from false to true.
This oc-curs when, for instance, a pronoun "he"in a sentence corefers to a noun phrase"Mr. Robin" and inherits the named en-tity tag person from "Mr. Robin" in thesame coreference hain.?
Sentence=is -T i t le ,  Sentence=is -Date l ineThe possible values for each of these fea-tures are true or false.
If a sentence is thetitle of a story, then the feature Sentence-is-Title will be assigned the value true.Its value is false otherwise.
Similarly, thefeature Sentence-is-Dateline applies to asentence which is the dateline of a story.It has been observed in prior work (Char-niak et al, 2000; Riloff and Thelen, 2000)that such sentences may be more likely tobe the answer sentences to some questiontype (for example, dateline can answer towhen questions).keywords in sentencesThe idea behind the use of this group offeatures is that certain words in a sen-tence may provide strong clues to the sen-tence being the answer to some questiontype.
For instance, the preposition "in"(such as "... in the United States, .
.
. "
)may be a strong clue that the sentence isan answer to a where question.We devised an automated procedure tofind such words.
For each of the five ques-tion types, we collect all the sentencesin the training stories that answer to thequestion type.
Any word (in its morpho-logical root form) that occurs at least 3times in this set of sentences i a possi-ble candidate word.
For each candidateword, we compute the following correla-tion metric C (Ng et al, 1997):C= (Nr+N~,- - N,_ N,~+)v~x/(lv.+ + N~-)CN,+  N,_)(N.+ + N,+)0V,- + g, - )where Nr+ (Nn+) is the number of train-ing story sentences that answer (do notanswer) to the question type and in whichthe word w occurs, and Nr_ (Nn_) is thenumber of training story sentences thatanswer (do not answer) to the questiontype and in which the word w does notoccur.
N = Nr+ + N r_ + Nn+ + Nn-.Note that the correlation metric C is thesquare root of the X 2 metric.
A candidateword that has high positive C value is agood clue word.
If such a word occurs ina sentence, then the sentence is likely toanswer to the question type.For each question type, we find one wordthat has the highest positive C value forthat question type.
The following fivewords ("name", "call", "year", "in", and"to") are found automatically in this wayfor the five question types who, what,when, where, and why, respectively.
Onefeature is then formed for each word:whether a sentence contains the word"name", whether a sentence contains the128word "call", etc.
The possible values forthese features are true or false.Note that this list of keywords is deter-mined automatically from the trainingstories only, without looking at the teststories.?
keywords in questionsIt has been observed in the work of (Riloffand Thelen, 2000) that certain words in awhen or where question tend to indicatethat the dateline is an ~n~wer sentence tothe question.
The words used in (Riloffand Thelen, 2000) are "happen", "takeplace" "this", "story".In our work, we attempted to discoverthese words automatically, using the cor-relation metric.
The method is the sameas what we used to discover the keywordsin sentences, except that Nr+ (Nn+) isthe number of training story questionsthat have (do not have) dateline as ananswer to the question, and in which theword w occurs, and Nr- (Nn-) is thenumber of training story questions thathave (do not have) dateline as an answerto the question and in which the word wdoes not occur.We again picked the word with the high-est positive C value for each questiontype.
Only two words ("story and "this")are found, for the when and where ques-tion, respectively.
For the other questiontypes, either the dateline was never ananswer sentence to the question type, orthat no candidate words occur at leastthree times in the training story ques-tions.We then form one feature for each word:whether a question contains the word%tory', and whether a question containsthe word "this".
The possible values forthese features are true or false.
Again,these two keywords are determined fromthe training stories only, without lookingat the test stories.
It is interesting tonotethat the words automatically determinedby out procedure are also part of thosewords found manually in the prior workof (l:tiloff and Thelen, 2000).3.2 Building ClassifiersThe next step is to use a machine learning al-gorithm to learn five classifiers from the train-ing examples, one classifier per question type.The learning algorithm we used in AQUAREASis C5, a more recent version of C4.5 (Quinlan,1993).For each test example, the classifier will de-cide if it is positive (an answer) or negative(not an answer) with a confidence value.
Wepick as the answer to the question the sen-tence whose feature vector was classified posi-tive with the highest confidence, or in the ab-sence of such, the sentence classified negativewith the lowest confidence.
AQUAREAS breaksties in favor of the sentence appearing earlierin the story.C5 accepts parameters that affect its learn-ing algorithm.
The following three parame-ters were used in AQUAB.EAS to achieve betterperformance, m avoids over-fitting the train-ing data by specifying that a minimum num-ber of rn examples must follow a decision treebranch, t specifies the maximum number ofdecision trees used in adaptive boosting to de-termine the final decision through voting, nipcost influences C5 to avoid false negatives (orfalse positives).4 Eva luat ionTo evaluate our learning approach, we trainedAQUAREA$ on the same development set ofstories and tested it on the same test set ofstories as those used in all past work on thereading comprehension task (Hirschman et al,1999; Charniak et al, 2000; Riloffand Thelen,2000; Wang et al, 2000).
Specifically, the setof stories used are published by Remedia Pub-licatious.
We used the same softcopy versioncreated by the MITRE group, and the ma-terial includes manual annotations of namedentities and coreference balns as done by theMITRE group.The training set consists of 28 stories fromgrade 2 and 27 stories from grade 5.
The testset consists of 30 stories from grade 3 and 30stories from grade 4.
Within the 60 test sto-ries, there are 59 who questions, 61 what ques-tions, 60 when questions, 60 where questions,and 60 why questions, for a total of 300 testquestions.The scoring metric that we used for evalu-ation is HumSent, which is the percentage oftest questions for which AQUAREAS has cho-sen a correct sentence as the answer.
Thismetric is originally proposed by (Hirschmanet al, 1999).
The correct answer sentencesare chosen manually by the MITRE group.Although there were a few other scoring met-129rics originally proposed in (Hirschman et al,1999), all the metrics were found to correlatewell with one another.
As such, all subsequentwork (Charniak et al, 2000; Riloff and The-len, 2000; Wang et al, 2000) uses HumSentas the main scoring metric, and it is also thescoring metric that we adopted in this paper.Based on the complete set of 20 features de-scribed in the previous ection, we trained oneclassifier per question type.
For each questiontype, we uniformly use the same, identical setof features.
The following learning parameterswere found to give the best HuinSent accuracyand were uniformly used in generating all thedecision tree classifiers for all question typesreported in this paper: m = 37, t = 7, andnip cost = 1.2.
Using a large rn results insimpler decision trees, t = 7 results in the useof boosting with multiple decision trees.
Sincethere are a lot more negative training exam-ples compared to positive training examples(ratio of approximately 4:1), there is a ten-dency to generate a default ree classifying alltraining examples as negative (since the ac-curacy of such a tree is already quite good -about 80% on our skewed istribution of train-ing examples).
Setting nip cost at 1.2 willmake it more costly to misclassify a positivetraining example as negative, and thus morecostly to generate the default ree, resultingin better accuracy.We achieved an overall HumSent accuracyof 39.3% on the 300 test questions.
Thebreakdown into the number of questions an-swered correctly per question type is shownin the first row of Table 1.
Our results indi-cate that our machine learning approach canachieve accuracy comparable with other ap-proaches that rely on handcrafted, etermin-istic rules and algorithms.
For comparison,the HumSent scores reported in the work of(Hirschm~.n et al, 1999), (Charniak et al,2000), (Riloff and Thelen, 2000), and (Wanget al, 2000) are 36.3%, 41%, 39.7%, and 14%,respectively.Figure 2 shows the sequence of decisiontrees generated via boosting for the whenquestion type.
All the trees look reasonableand intuitive.
The first tree states that ifthe Diff-from-Max-Word-Match is zero (i.e.,the sentence has the highest number of wordmatch to the question), then the sentence isan answer.
Otherwise, the classifier tests forwhether the sentence contains a date.
If itdoes, then the sentence is an answer, else it isnot an answer.
The second tree is a defaulttree that just classifies any sentence as not ananswer.
The rest of the trees similarly teston features that we intuitively feel are indica-tive of whether a sentence answers to a whenquestion.To investigate the relative importance ofeach type of features, we remove one type offeatures at a time and observe its impact onHuinSent accuracy.
The resulting drop in ac-curacy is tabulated in the remaining rows ofTable 1.
The rows are ordered in decreasingoverall HumSent accuracy.As expected, removing the word match fea-ture causes the largest drop in overall accu-racy, and the accuracy decline affects all ques-tion types.
Removing the five named entityfeatures also causes a large decline, affect-ing mainly the who, when, and where ques-tions.
Named entities are useful for answer-ing these question types, since who typicallyasks for a person (or organization), when asksfor date or time, and where asks for location.What is perhaps a little surprising is that theseven automatically discovered keywords arealso found to be very important, and removingthese seven features causes the second largestdecline in overall HumSent accuracy.Coreference is found to affect the who,when, and where questions, as expected.
Theprevious and next word/verb matches causethe largest decline for why questions, droppingthe number of correctly answered why ques-tions to 3.
Removing verb match also causesa 3% drop in overall accuracy, while datelineand title only affect he when questions.In our future work, we plan to investigateother potential knowledge sources that mayfurther improve accuracy.
We also plan to in-vestigate the use of other supervised machinelearning algorithms for this problem.5 Conc lus ionIll 811mrnary, we reported in this paper the re-sults on answering questions for the readingcomprehension task, using a machine learningapproach.
We evaluated our approach on theRemedia data set, a common data set used inseveral recent papers on the reading compre-hension task.
Our learning approach achievesaccuracy competitive to previous approachesthat rely on handcrafted, eterministic rulesand algorithm~.
To the best of our knowledge,this is the first work that reports that theuse of a machine learning approach achieves130Features Who What When WhereAll 31 21 27 31- title & dateline 31 21 19 31- DMVM 24 21 31 25-p rev  & next 27 21 26 25- coreference 27 21 24 22- named entities 24 21 21 23- keywords 27 21 19 17- DMWM 29 6 20 24Table 1: Accuracy using different setDMVM<=0~0+ Sent-contains-Datet rue~lse+Why All8 118 (39.3%)8 II0 (36.7%)8 109 (36.3%)3 102 (34.0%)7 101 (33.7%)6 95 (31.7%)9 93 (31.0%)5 84 (28.0%)of featuresDMWM<- -0~0+ Sent-is-Datelinet ru~lse?Tree #1 Tree #2 Tree #3Year-keyword-m-Sent Sent-contains-Datet rue~lse  t rue~lse+ DMVM DMWM -+ .
?Tree #4 Tree #5Figure 2: The classifier learned for the when question typecompetitive r sults on answering questions forreading comprehension tests.6 AcknowledgementsThanks to Marc Light and Eric Breck ofMITRE Corporation for assistance in provid-ing the annotated Remedia corpus.ReferencesEugene Chamiak, Yasemin Altun, Rodrigo deSalvo Braz, Benjxmlu Garrett, Margaret Kos-mala, Tomer Moscovich, Lixin Pang, ChangheePyo, Ye Sun, Wei Wy, Zhongfa Yang,Shawn Zeller, and Lisa Zorn.
2000.
Read-ing comprehension programs in a statistical-language-processing class.
In Proceedings ofthe ANLP/NAACL ~000 Workshop on Read-ing Comprehension Tests as Evaluation forComputer-Based Language Understanding Sys-tems, pages 1-5, Seattle, Washington.Lynette Hirschman, Marc Light, Eric Breck, andJohn D. Burger.
1999.
Deep Read: a read-hag comprehension system.
In Proceedings ofthe 37th Annual Meeting of the Association forComputational Linguistics, pages 325-332, Col-lege Park, Maryland.Julian Kupiee.
1993.
MURAX: a robust linguisticapproach for question answering using an on-line encyclopedia.
In Proceedings of the I6thAnnual International ACM 8IGIR Conferenceon Research and Development in InformationRetrieval, pages 181-190, Pittsburgh, Pennsyl-vania.Wendy Lehnert.
1978.
The Process of Ques-tion Answering.
Lawrence Erlbaum Associates,Hillsdale, NJ.Dan Moldovan, Sanda Harabagiu, Marius Pasea,Rada Mihaleea, Richard Goodrum, RoxanaGirju, and Vasile Rus.
2000.
LASSO: a toolfor surfing the answer net.
In Proceedings ofthe Eighth Text REtrieval Conference (TREC-1318), Gaithersburg, Maryland.Hwee Tou Ng, Wei Boon Goh, and Kok LeongLow.
1997.
Feature selection, perceptron learn-ing, and a usability case study for text cat-egorization.
In Proceedings of the 20th An-nual International A CM SIGIR Conference onResearch and Development in Information Re-trieval, pages 67-73, Philadelphia, Pennsylva-nia.John Ross Quinlan.
1993.
C4.5: Programs forMachine Learning.
Morgan Kaufmann, SanFrancisco, California.Dragomir R. Radev, John Prager, and ValerieSaran.
2000.
Ranking suspected answers to nat-ural language questions using predictive annota-tion.
In Proceedings of the Sixth Applied Natu-ral Language Processing Conference, pages 150--157, Seattle, Washington.Ellen Riloff and Michael Thelen.
2000.
A rule-based question answering system for read-ing comprehension tests.
In Proceedings ofthe ANLP/NAACL ~000 Workshop on Read-ing Comprehension Tests as Evaluation forComputer-Based Language Understanding Sys-tera~, pages 13-19, Seattle, Washington.Amit Singhal, Steve Abney, Michiel Bacchiani,Michael Collins, Donald Hindle, and FernandoPereira.
2000.
AT&T at TREC-8.
In Proceed-ings of the Eighth Text REtrieval Conference(TREC-8), Gaithersburg, Maryland.Rohini Srihari and Wei Li.
2000.
Information ex-traction supported question answering.
In Pro-ceedings of the Ezghth Text REtrieval Confer-ence (TREC-8), Gaithersburg, Maryland.Ellen M. Voorhees and Dawn M. Tice.
2000.
TheTREC-8 question answering track evaluation.In Proceedings of the Eighth Text REtrievalConference (TREC-8), Gaithersburg, Mary-land.W.
Wang, J. Auer, R. Parasuraman, I. Zubarev,D.
Brandyberry, and M. P. Harper.
2000.A question answering system developed as aproject in a natural language processing course.In Proceedings of the ANLP/NAACL $000Workshop on Reading Comprehension Testsas Evaluation for Computer-Based LanguageUnderstanding Systems, pages 28-35, Seattle,Washington.132
