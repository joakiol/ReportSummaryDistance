Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1169?1179,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsTransition-based Neural Constituent ParsingTaro Watanabe?and Eiichiro SumitaNational Institute of Information and Communications Technology3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289 JAPANtarow@google.com, eiichiro.sumita@nict.go.jpAbstractConstituent parsing is typically modeledby a chart-based algorithm under prob-abilistic context-free grammars or by atransition-based algorithm with rich fea-tures.
Previous models rely heavily onricher syntactic information through lex-icalizing rules, splitting categories, ormemorizing long histories.
However en-riched models incur numerous parametersand sparsity issues, and are insufficient forcapturing various syntactic phenomena.We propose a neural network structure thatexplicitly models the unbounded history ofactions performed on the stack and queueemployed in transition-based parsing, inaddition to the representations of partiallyparsed tree structure.
Our transition-basedneural constituent parsing achieves perfor-mance comparable to the state-of-the-artparsers, demonstrating F1 score of 90.68%for English and 84.33% for Chinese, with-out reranking, feature templates or addi-tional data to train model parameters.1 IntroductionA popular parsing algorithm is a cubic time chart-based dynamic programming algorithm that usesprobabilistic context-free grammars (PCFGs).However, PCFGs learned from treebanks are toocoarse to represent the syntactic structures of texts.To address this problem, various contexts are in-corporated into the grammars through lexicaliza-tion (Collins, 2003; Charniak, 2000) or cate-gory splitting either manually (Klein and Man-ning, 2003) or automatically (Matsuzaki et al,2005; Petrov et al, 2006).
Recently a rich featureset was introduced to capture the lexical contexts?The first author is now affiliated with Google, Japan.in each span without extra annotations in gram-mars (Hall et al, 2014).Alternatively, transition-based algorithms run inlinear time by taking a series of shift-reduce ac-tions with richer lexicalized features consideringhistories; however, the accuracies did not matchwith the state-of-the-art methods until recently(Sagae and Lavie, 2005; Zhang and Clark, 2009).Zhu et al (2013) show that the use of better transi-tion actions considering unaries and a set of non-local features can compete with the accuracies ofchart-based parsing.
The features employed in atransition-based algorithm usually require part ofspeech (POS) annotation in the input, but the de-layed feature technique allows joint POS inference(Wang and Xue, 2014).In both frameworks, the richer models requirethat more parameters be estimated during train-ing which can easily result in the data sparsenessproblems.
Furthermore, the enriched models arestill insufficient to capture various syntactic rela-tions in texts due to the limited contexts repre-sented in latent annotations or non-local features.Recently Socher et al (2013) introduced composi-tional vector grammar (CVG) to address the abovelimitations.
However, they employ reranking overa forest generated by a baseline parser for efficientsearch, because CVG is built on cubic time chart-based parsing.In this paper, we propose a neural network-based parser ?
transition-based neural con-stituent parsing (TNCP) ?
which can guaranteeefficient search naturally.
TNCP explicitly modelsthe actions performed on the stack and queue em-ployed in transition-based parsing.
More specif-ically, the queue is modeled by recurrent neuralnetwork (RNN) or Elman network (Elman, 1990)in backward direction (Henderson, 2004).
Thestack structure is also modeled similarly to RNNs,and its top item is updated using the previouslyconstructed hidden representations saved in the1169stack.
The representations from both the stack andqueue are combined with the representations prop-agated from the partially parsed tree structure in-spired by the recursive neural networks of CVGs.Parameters are estimated efficiently by a variantof max-violation (Huang et al, 2012) which con-siders the worst mistakes found during search andupdates parameters based on the expected mistake.Under similar settings, TCNP performs compa-rably to state-of-the-art parsers.
Experimental re-sults obtained using the Wall Street Journal corpusof the English Penn Treebank achieved a labeledF1 score of 90.68%, and the result for the PennChinese Treebank was 84.33%.
Our parser per-forms no reranking with computationally expen-sive models, employs no templates for feature en-gineering, and requires no additional monolingualdata for reliable parameter estimation.
The sourcecode and models will be made public1.2 Related WorkOur study is largely inspired by recursive neuralnetworks for parsing, first pioneered by Costa etal.
(2003), in which parsing is treated as a rankingproblem of finding phrasal attachment.
Such net-work structures have been used successfully as areranker for k-best parses from a baseline parser(Menchetti et al, 2005) or parse forests (Socher etal., 2013), and have achieved gains on large data.Stenetorp (2013) showed that the recursive neu-ral networks are comparable to the state-of-the-artsystem with a rich feature set under dependencyparsing.
Our model is not a reranking model, buta discriminative parsing model, which incorpo-rates the representations of stacks and queues em-ployed in the transition-based parsing framework,in addition to the representations of the tree struc-tures.
The use of representations outside of thepartial parsed trees is very similar to the recentlyproposed inside-outside recursive neural networks(Le and Zuidema, 2014) which can assign proba-bilities in a top-down manner, in the same way asPCFGs.Henderson (2003) was the first to demonstratethe successful use of neural networks to representderivation histories under large-scale parsing ex-periments.
He employed synchrony networks, i.e.,feed-forward style networks, to assign a probabil-ity for each step in the left-corner parsing condi-tioning on all parsing steps.
Henderson (2004)1http://github.com/tarowatanabe/trancelater employed a discriminative model and showedfurther gains by conditioning on the representa-tion of the future input in addition to the historyof parsing steps.
Similar feed-forward style net-works are successfully applied for transition-baseddependency parsing in which limited contexts areconsidered in the feature representation (Chen andManning, 2014).
Our model is very similar in thatthe score of each action is computed by condition-ing on all previous actions and future input in thequeue.The use of neural networks for transition-basedshift-reduce parsing was first presented by May-berry and Miikkulainen (1999) in which the stackrepresentation was treated as a hidden state of anRNN.
In their study, the hidden state is updatedrecurrently by either a shift or reduce action, andits corresponding parse tree is decoded recursivelyfrom the hidden state (Berg, 1992) using recursiveauto-associative memories (Pollack, 1990).
Weapply the idea of representing a stack in a contin-uous vector; however, our method differs in thatit memorizes all hidden states pushed to the stackand performs push/pop operations.
In this man-ner, we can represent the local contexts saved inthe stack explicitly and use them to construct newhidden states.3 Transition-based Constituent ParsingOur transition-based parser is based on a study byZhu et al (2013), which adopts the shift-reduceparsing of Sagae and Lavie (2005) and Zhang andClark (2009).
However, our parser differs in thatwe do not differentiate left or right head words.In addition, POS tags are jointly induced duringparsing in the same manner as Wang and Xue(2014).
Given an input sentence w0, ?
?
?
, wn?1,the transition-based parser employs a stack of par-tially constructed constituent tree structures and aqueue of input words.
In each step, a transitionaction is applied to a state ?i, f, S?, where i is thenext input word position in the queuewi, f is a flagindicating the completion of parsing, i.e., whetherthe ROOT of a constituent tree covering all theinput words is reached, and S represents a stack oftree elements, s0, s1, ?
?
?
.The parser consists of five actions:shift-X consumes the next input word, wi, fromthe queue and pushes a non-terminal symbol(or a POS label) as a tree of X ?
wi.1170axiom 0 : ?0, false, ?eps??
: 0goal (2 + u)n : ?n, true, S?
: ?shift-Xj : ?i, false, S?
: ?j + 1 : ?i+ 1, false, S|X?
: ?+ ?shreduce-Xj : ?i, false, S|s1|s0?
: ?j + 1 : ?i, false, S|X?
: ?+ ?reunary-Xj : ?i, false, S|s0?
: ?j + 1 : ?i, false, S|X?
: ?+ ?unfinishj : ?n, false, S?
: ?j + 1 : ?n, true, S?
: ?+ ?fiidlej : ?n, true, S?
: ?j + 1 : ?n, true, S?
: ?+ ?idFigure 1: Deduction system for shift-reduce pars-ing, where j is a step size and ?
is a score.reduce-X pops the top two items s0and s1out ofthe stack and combines them as a partial treewith the constituent label X as its root, andwith s0and s1as right and left antecedents,respectively (X ?
s1s0).
The newly createdtree is then pushed into the stack.unary-X is similar to reduce-X; however, it con-sumes only the top most item s0from thestack and pushes a new tree of X ?
s0.finish indicates the completion of parsing, i.e.,reaching the ROOT .idle preserves completion until the goal isreached.The whole procedure is summarized as a deduc-tion system in Figure 1.
We employ beam searchwhich starts from an axiom consisting of a stackwith a special symbol ?eps?, and ends when wereach a goal item (Zhang and Clark, 2009).
A setof agenda B = B0, B1, ?
?
?
maintains the k-beststates for each step j at Bj, which is first initial-ized by inserting the axiom in B0.
Then, at eachstep j = 0, 1, ?
?
?
, every state in the agenda Bjisextended by applying one of the actions and thenew states are inserted into the agenda Bj+1forthe next step, which retains only the k-best states.We limit the maximum number of consecutiveunary actions to u (Sagae and Lavie, 2005; Zhangand Clark, 2009) and the maximum number ofunary actions in a single derivation to u?n.
Thus,the process is repeated until we reach the final stepof (2+u)n, which keeps the completed states.
Theidle action is inspired by the padding method ofZhu et al (2013), such that the states in an agendaare comparable in terms of score even if differ-ences exist in the number of unary actions.
Un-like Zhu et al (2013) we do not terminate parsingeven if all the states in an agenda are completed(f = true).The score of a state is computed by summingthe scores of all the actions leading to the state.
InFigure 1, ?sh, ?re, ?un, ?fiand ?idare the scoresof shift-X , reduce-X , unary-X , finish and idle ac-tions, respectively, which are computed on the ba-sis of the history of actions.4 Neural Constituent ParsingThe score of a state is defined formally as the totalscore of transition actions, or a (partial) derivationd = d0, d1, ?
?
?
leading to the state as follows:?
(d) =|d|?1?j=0?(dj|dj?10).
(1)Note that the score of each action is dependent onall previous actions.
In previous studies, the scoreis computed by a linear model, i.e., a weightedsum of feature values derived from limited histo-ries, such as those that consider two adjacent con-stituent trees in a stack (Sagae and Lavie, 2005;Zhang and Clark, 2009; Zhu et al, 2013).
Ourmethod employs an RNN or Elman network (El-man, 1990) to represent an unlimited stack andqueue history.Formally, we use an m-dimensional vector foreach hidden state unless otherwise stated.
Here, letxi?
Rm?
?1be an m?-dimensional vector repre-senting the input word wiand the dimension maynot match with the hidden state sizem.
qi?
Rm?1denotes the hidden state for the input word wiin aqueue.
Following the RNN in backward direction(Henderson, 2004), the hidden state for each wordwiis computed right-to-left, qn?1to q0, beginningfrom a constant qn:qi= ?
(Hquqi+1+Wquxi+ bqu) , (2)where Hqu?
Rm?m, Wqu?
Rm?m?, bqu?Rm?1and ?
(x) is hard-tanh applied element-wise2.2?
(x) = ?1 for x < 1, 1 for x > 1 otherwise x.1171h1jh0jxixi+1qiqi+1h2j+1h1j+1h0j+1current stacknext stackqueueinputHshQshWsh(a) shift-X actionh3jh2jh1jh0jqiqi+1h2j+1h1j+1h0j+1current stacknext stackqueueWreQreHre(b) reduce-X actionh2jh1jh0jqiqi+1h2j+1h1j+1h0j+1current stacknext stackqueueWunHunQun(c) unary-X actionFigure 2: Example neural network for constituentparsing.
The thick arrows indicate the context oftree structures, and the gray arrows represent in-teractions from the stack and queue.
The dottedarrows denote popped states.Shift: Now, let hlj?
Rm?1represent a hiddenstate associated with the lth stack item for the jthaction.
We define the score of a shift action:h0j+1= ?
(HXshh0j+QXshqi+WXshxi+ bXsh)(3)?
(dj= shift-X|dj?10) = VXshh0j+1+ vXsh(4)where HXsh?
Rm?m, QXsh?
Rm?m, WXsh?Rm?m?and bXsh?
Rm?1.
Figure 2(a) shows thenetwork structure for Equation 3.
HXshrepresentsan RNN-style architecture that propagates the pre-vious context in the stack.
QXshcan reflect thequeue context qi, or the future input sequence fromwithrough wn?1, while WXshdirectly expressesthe leaf of a tree structure using the shifted inputword representation xifor wi.
The hidden stateh0j+1is used to compute the score of a derivation?
(dj|dj?10) in Equation 4, which is based on thematrix VXsh?
R1?mand the bias term vXsh?
R.Note that hlj+1= hl?1jfor l = 1, 2, ?
?
?
becausethe stack is updated by the newly created partialtree label X associated with the new hidden stateh0j+1.Inspired by CVG (Socher et al, 2013), we dif-ferentiate the matrices for each non-terminal (orPOS) labelX rather than using shared parameters.However, our model differs in that the parametersare untied on the basis of the left hand side of arule, rather than the right hand side, because ourmodel assigns a score discriminatively for each ac-tion with the left hand side label X unlike a gen-erative model derived from PCFGs.Reduce: Similarly, the score for a reduce actionis obtained as follows:h0j+1= ?
(HXreh2j+QXreqi+WXreh[0:1]j+ bXre)(5)?
(dj= reduce-X|dj?10) = VXreh0j+1+ vXre, (6)where HXre?
Rm?m, QXre?
Rm?m, WXre?Rm?2m, bXre?
Rm?1, and h[l:l?
]denotes the verti-cal matrix concatenation of hidden states from hlto hl?.Note that the reduce-X action pops top twoitems in the stack that correspond to the two hid-den states of h[0:1]jas represented by Figure 2(b).By pushing a newly created tree with the con-stituent X , its corresponding hidden state h0j+1ispushed to the stack with each remaining hiddenstate hlj+1= hl+1jfor l = 1, 2, ?
?
?
.
The hid-den state of the top stack item h0jis a represen-tation of the right antecedent of a newly createdbinary tree with h0j+1as a root, while the hiddenstate of the next top stack item h1jcorrespondsto the left antecedent of the binary tree.
Thus,the two hidden states capture the recursive neuralnetwork-like structure (Costa et al, 2003), whileh2j= h1j+1represents the RNN-like linear historyin the stack.Unary: In the same manner as the reduce action,the unary action is defined by simply reducing asingle item from a stack and by pushing a new item(Figure 2(c)):h0j+1= ?
(HXunh1j+QXunqi+WXunh0j+ bXun)(7)?
(dj= unary-X|dj?10) = VXunh0j+1+ vXun, (8)where HXun?
Rm?m, QXun?
Rm?m, WXun?Rm?mand bXun?
Rm?1.
Note that hlj+1= hljfor l = 1, 2, ?
?
?
, because only the top item is up-dated in the stack by creating a partial tree with h0jtogether with the stack history h1j.In summary, the number of model parametersfor the three actions is 9?m2+m?m?+6?m+3for each non-terminal label X .
The scores for a1172finish action and an idle action are defined analo-gous to the unary-X action with special labels forX , ?finish?
and ?idle?, respectively3.5 Parameter EstimationLet ?
={HXsh, QXsh, ?
?
?}?
RMbe an M -dimensional vector of all model parameters.
Theparameters are initialized randomly by followingGlorot and Bengio (2010), in which the randomvalue range is determined by the size of the in-put/output layers.
The bias parameters are initial-ized to zeros.We employ a variant of max-violation (Huanget al, 2012) as our training objective, in which pa-rameters are updated based on the worst mistakefound during search, rather than the first mistakeas performed in the early update perceptron al-gorithm (Collins and Roark, 2004).
Specifically,given a training instance (w,y) where w is an in-put sentence and y is its gold derivation, i.e., a se-quence of actions representing the gold parse treeforw, we seek for the step j?where the differenceof the scores is the largest:j?= argminj{??(yj0)?
maxd?Bj??(d)}.
(9)Then, we define the following hinge-loss function:L(w,y;B,?)
= max{0, 1?
??
(yj?0) + E?Bj?[??
]},(10)wherein we consider the subset of sub-derivations?Bj??
Bj?consisting of those scored higher than??
(yj?0):?Bj?={d ?
Bj?????
(d) > ??(yj?0)}(11)p?
(d) =exp(??(d))?d??
?Bj?exp(??(d?))(12)E?Bj?[??]
=?d?
?Bj?p?(d)??(d).
(13)Unlike Huang et al (2012) and inspired by Tamuraet al (2014), we consider all incorrect sub-derivations found in?Bj?through the expectedscore E?Bj?[??]4.
The loss function in Equation3Since h1jand qnare constants for the finish and idle ac-tions, we enforce HXun= 0 and QXun= 0 for those specialactions.4We can use all the sub-derivations in Bj?
; however, ourpreliminary studies indicated that the use of?Bj?was better.10 can be intuitively considered an expected mis-take suffered at the maximum violated step j?,which is measured by the Viterbi violation inEquation 9.
Note that if we replace E?Bj?[??]
withmaxd?Bj???
(d) in Equation 10, it is exactly thesame as the max-violation objective (Huang et al,2012)5.To minimize the loss function, we use a di-agonal version of AdaDec (Senior et al, 2013)?
a variant of diagonal AdaGrad (Duchi et al,2011) ?
under mini-batch settings.
Given thesub-gradient gt?
RMof Equation 10 at time tcomputed by the back-propagation through struc-ture (Goller and K?uchler, 1996), we maintain ad-ditional parametersGt?
RM:Gt?
?Gt?1+ gtgt, (14)where is the Hadamard product (or the element-wise product).
?t?1is updated using the elementspecific learning rate ?t?
RMderived from Gtand a constant ?0> 0:?t?
?0(Gt+ )?12(15)?t?12?
?t?1?
?tgt(16)?t?
argmin?12??
?
?t?12?22+ ??>tabs(?).
(17)Compared with AdaGrad, the squared sum of thesub-gradients decays over time using a constant0 < ?
?
1 in Equation 14.
The learningrate in Equation 15 is computed element-wise andbounded by a constant  ?
0, and if we set  ?
?20,it is always decayed6.
In our preliminary stud-ies, AdaGrad eventually becomes very conserva-tive to update parameters when training longer it-erations.
AdaDec fixes the problem by ignoringolder histories of sub-gradients in G, which is re-flected in the learning rate ?.
In each update, weemploy `1regularization through FOBOS (Duchiand Singer, 2009) using a hyperparameter ?
?
0to control the fitness in Equation 16 and 17.
Fortesting, we found that taking the average of the pa-rameters over period1T+1?Tt=0?tunder trainingiterations T was very effective as demonstrated byHashimoto et al (2013).Parameter estimation is performed in parallelby distributing training instances asynchronously5Or, setting p?(d?)
= 1 for the Viterbi derivation d?
=argmaxd?Bj???
(d) and zero otherwise.6Note that AdaGrad is a special case of AdaDec with ?
=1 and  = 0.1173in each shard and by updating locally copied pa-rameters using the sub-gradients computed fromthe distributed mini-batches (Dean et al, 2012).The sub-gradients are broadcast asynchronouslyto other shards to reflect the updates in one shard.Unlike Dean et al (2012), we do not keep a cen-tral storage for model parameters; the replicatedparameters are synchronized in each iteration bychoosing the model parameters from one of theshards with respect to the minimum of `1norm7.Note that we synchronize ?, but G is maintainedas shard local parameters.6 Experiments6.1 SettingsWe conducted experiments for transition-basedneural constituent parsing (TNCP) for two lan-guages ?
English and Chinese.
English data werederived from the Wall Street Journal (WSJ) of thePenn Treebank (Marcus et al, 1993), from whichsections 2-21 were used for training, 22 for de-velopment and 23 for testing.
Chinese data wereextracted from the Penn Chinese Treebank (CTB)(Xue et al, 2005); articles 001-270 and 440-1151 were used for training, 301-325 for develop-ment, and 271-300 for testing.
Inspired by jack-knifing (Collins and Koo, 2005), we reassignedPOS tags for training data using the Stanford tag-ger (Toutanova et al, 2003)8.
The treebank treeswere normalized by removing empty nodes andunary rules with X over X (or X ?
X), thenbinarized in a left-branched manner.The possible actions taken for our shift-reduceparsing, e.g., X ?
w in shift-X , were learnedfrom the normalized treebank trees.
The wordsthat occurred twice or less were handled differ-ently in order to consider OOVs for testing: Theywere simply mapped to a special token ?unk?whenlooking up their corresponding word representa-tion vector.
Similarly, when assigning possiblePOS tags in shift actions, they fell back to theircorresponding ?word signature?
in the same man-ner as the Berkeley parser9.
A maximum numberof consecutive unary actions was set to u = 3 forWSJ and u = 4 for CTB, as determined by the7We also tried averaging among shards.
However we ob-served no gains likely because we performed averaging fortesting.8http://nlp.stanford.edu/software/tagger.shtml9https://code.google.com/p/berkeleyparser/rep.
size 32 64 128 256 512 1024devWSJ-32 89.91 90.15 90.48 90.70 90.75 90.8764 90.37 90.73 90.81 90.62 90.71 91.11CTB-32 79.25 81.59 82.80 82.68 84.17 85.1264 84.04 83.29 82.92 85.12 85.24 85.77testWSJ-32 89.03 89.49 89.75 90.45 90.37 90.0164 89.74 90.16 90.48 90.06 89.91 90.68CTB-32 75.19 78.29 80.46 81.87 83.16 82.6464 80.11 81.35 81.67 82.91 83.76 84.33Table 1: Comparison of various state/word rep-resentation dimension size measured by labeledF1(%).
?-32?
denotes the hidden state size m =32.
The numbers in bold indicate the best resultsfor each hidden state dimension.treebanks.Parameter estimation was performed on 16cores of a Xeon E5-2680 2.7GHz CPU.
It tookapproximately one day for 100 training iterationswith m = 32 and m?= 128 under a mini-batch size of 4 and a beam size of 32.
Dou-bling either one of m or m?incurred approxi-mately double training time.
We chose the fol-lowing hyperparameters by tuning toward the de-velopment data in our preliminary experiments10:?0= 10?2, ?
= 0.9,  = 1.
The choice of ?
from{10?5, 10?6, 10?7} and the number of training it-erations were very important for different trainingobjectives and models in order to avoid overfitting.Thus, they were determined by the performanceon the development data for each different train-ing objective and/or network configuration, e.g.,the dimension for a hidden state.
The word rep-resentations were initialized by a tool developedin-house for an RNN language model (Mikolov etal., 2010) trained by noise contrastive estimation(Mnih and Teh, 2012).
Note that the word repre-sentations for initialization were learned from thegiven training data, not from additional unanno-tated data as done by Chen and Manning (2014).Testing was performed using a beam size of 64with a Xeon X5550 2.67GHz CPU.
All resultswere measured by the labeled bracketing metricPARSEVAL (Black et al, 1991) using EVALB11after debinarization.6.2 ResultsTable 1 shows the impact of dimensions onthe parsing performance.
We varied the hid-10We confirmed that this hyperparameter setting was ap-propriate for different models experimented in Section 6.2through our preliminary studies.11http://nlp.cs.nyu.edu/evalb/1174model tree +stack +queuedevWSJ 77.70 90.54 91.11CTB 69.74 84.70 85.77testWSJ 76.48 90.00 90.68CTB 66.03 82.85 84.33Table 2: Comparison of network structures mea-sured by labeled F1(%).den vector size m = {32, 64} and the wordrepresentation (embedding) vector size m?={32, 64, 128, 256, 512, 1024}12.
As can be seen,the greater word representation dimensions aregenerally helpful for both WSJ and CTB on theclosed development data (dev), which may matchwith our intuition that the richer syntactic and se-mantic knowledge representation for each word isrequired for parsing.
However, overfitting was ob-served when using a 32-dimension hidden vectorin both tasks, i.e., drops of performance on theopen test data (test) when m?= 1024, probablycaused by the limited generalization capability inthe smaller hidden state size.
In the rest of this pa-per, we show the results with m = 64 and m?=1024 as determined by the performance on thedevelopment data, wherein we achieved 91.11%and 85.77% labeled F1 for WSJ and CTB, respec-tively.
The total number of parameters were ap-proximately 28.3M and 22.0M for WSJ and CTB,respectively, among which 17.8M and 13.4M wereoccupied for word representations, respectively.Table 2 differentiated the network structure.The tree model computes the new hidden stateh0j+1using only the recursively constructed net-work by ignoring parameters from the stack andqueue, e.g., by enforcing HXsh= 0 and QXsh= 0in Equation 3, which is essentially similar to theCVG approach (Socher et al, 2013).
Adding thecontext from the stack in +stack boosts the per-formance significantly.
Further gains are observedwhen the queue context +queue is incorporated inthe model.
These results clearly indicate that ex-plicit representations of the stack and queue arevery important when applying a recursive neuralnetwork model for transition-based parsing.We then compared the expected mistake withthe Viterbi mistake (Huang et al, 2012) as ourtraining objective by replacing E?Bj?[??]
withmaxd?Bj???
(d) in Equation 10.
Table 3 showsthat the use of the expected mistake (expected)as a loss function is significantly better than that12We experimented larger dimensions in Appendix A.loss Viterbi expecteddevWSJ 90.89 91.11CTB 84.94 85.77testWSJ 90.21 90.68CTB 82.62 84.33Table 3: Comparison of loss functions measuredby labeled F1(%).657075808590951000  10  20  30  40  50  60  70  80  90  100F1(%)iterationsexpected (train)Viterbi (train)expected (dev)Viterbi (dev)Figure 3: Plots for training iterations and labeledF1(%) on WSJ.of the Viterbi mistake (Viterbi) by considering allthe incorrect sub-derivations at maximum violatedsteps during search.
Figure 3 and 4 plot the train-ing curves for WSJ and CTB, respectively.
Theplots clearly demonstrate that the use of the ex-pected mistake is faster in convergence and stablerin learning when compared with that of the Viterbimistake13.Next, we compare our parser, TNCP, with otherparsers listed in Table 4 for WSJ and Table 5 forCTB on the test data.
The Collins parser (Collins,1997) and the Berkeley parser (Petrov and Klein,2007) are chart-based parsers with rich states, ei-ther through lexicalization or latent annotation.SSN is a left-corner parser (Henderson, 2004), andCVG is a compositional vector grammar-basedparser (Socher et al, 2013)14.
Both parsers rely onneural networks to represent rich contexts, similarto our work; however they differ in that they es-sentially perform reranking from either the k-bestparses or parse forests15.
The word representa-13The labeled F1 on those plots are slightly different fromEVALB in that all the syntactic labels are considered whencomputing bracket matching.
Further, the scores on the train-ing data are approximation since they were obtained as a by-product of online learning.14http://nlp.stanford.edu/software/lex-parser.shtml15Strictly speaking, SSN can work as a standalone parser;Table 4 shows the result after reranking (Henderson, 2004).1175657075808590951000  10  20  30  40  50  60  70  80  90  100F1(%)iterationsexpected (train)Viterbi (train)expected (dev)Viterbi (dev)Figure 4: Plots for training iterations and labeledF1(%) on CTB.parser testCollins (Collins, 1997) 87.8Berkeley (Petrov and Klein, 2007) 90.1SSN (Henderson, 2004) 90.1ZPar (Zhu et al, 2013) 90.4CVG (Socher et al, 2013) 90.4Charniak-R (Charniak and Johnson, 2005) 91.0This work: TNCP 90.7Table 4: Comparison of different parsers on theWSJ test data measured by labeled F1(%).tion in CVG was learned from large monolingualdata (Turian et al, 2010), but our parser learnsword representation from only the provided train-ing data.
Charniak-R is a discriminative rerank-ing parser with non-local features (Charniak andJohnson, 2005).
ZPar is a transition-based shift-reduce parser (Zhu et al, 2013)16that influencesthe deduction system in Figure 1, but differs in thatscores are computed by a large number of featuresand POS tagging is performed separately.
The re-sults shown in Table 4 and 5 come from the featureset without extra data, i.e., semi-supervised fea-tures.
Joint is the joint POS tagging and transition-based parsing with non-local features (Wang andXue, 2014).
Similar to ZPar, we present the resultwithout cluster features learned from extra unan-notated data.Finally, we measured the speed for parsing byvarying beam size and hidden dimension (Table6).
When testing, we applied a pre-computationtechnique for layers involving word representationvectors (Devlin et al, 2014), i.e., Wquin Equation2 and WXshin Equation 3.
Thus, the parsing speedwas influenced by only the hidden state size m. Itis clear that the enlarged beam size improves per-16http://sourceforge.net/projects/zpar/parser testZPar (Zhu et al, 2013) 83.2Berkeley (Petrov and Klein, 2007) 83.3Joint (Wang and Xue, 2014) 84.9This work: TNCP 84.3Table 5: Comparison of different parsers on theCTB test data measured by labeled F1(%).beam 32 64 128WSJ-32 15.42/89.95 7.90/90.01 3.97/90.0464 7.31/90.56 3.56/90.68 1.76/90.73CTB-32 13.67/82.35 6.95/82.64 3.68/82.8464 6.15/84.12 3.11/84.33 1.53/83.83Table 6: Comparison of parsing speed by varyingbeam size and hidden dimension; each cell showsthe number of sentences per second/labeled F1(%)measured on the test data.formance by trading off run time in most cases.Note that Berkeley, CVG and ZPar took 4.74, 1.54and 37.92 sentences/sec, respectively, with WSJ.Although it is more difficult to compare with otherparsers, our parser implemented in C++ is on parwith Java implementations of Berkeley and CVG.The large run time difference with the C++ imple-mented ZPar may come from the network compu-tation and joint POS inference in our model whichimpact parsing speed significantly.6.3 Error AnalysisTo assess parser error types, we used the tool pro-posed by Kummerfeld et al (2012)17.
The averagenumber of errors per sentence is listed in Table 7for each error type on the WSJ test data.
Gener-ally, our parser results in errors that are compara-ble to the state-of-the-art parsers; however, greaterreductions are observed for various attachmentserrors.
One of the largest gains comes from theclause attachment, i.e., 0.12 reduction in averageerrors from Berkeley and 0.05 from CVG.
The av-erage number of errors is also reduced by 0.09from Berkeley and 0.06 from CVG for the PP at-tachment.
We also observed large reductions incoordination and unary rule errors.7 ConclusionWe have introduced transition-based neural con-stituent parsing ?
a neural network architecturethat encodes each state explicitly ?
as a con-tinuous vector by considering the recurrent se-17https://code.google.com/p/berkeley-parser-analyser/1176error type Berkeley CVG TNCPPP Attach 0.82 0.79 0.73Clause Attach 0.50 0.43 0.38Diff Label 0.29 0.29 0.29Mod Attach 0.27 0.27 0.27NP Attach 0.37 0.31 0.32Co-ord 0.38 0.32 0.291-Word Span 0.28 0.31 0.30Unary 0.24 0.22 0.18NP Int 0.18 0.19 0.20Other 0.41 0.41 0.45Table 7: Comparison of different parsers on theWSJ test data measured by average number of er-rors per sentence; the numbers in bold indicate theleast errors in each error type.quences of the stack and queue in the transition-based parsing framework in addition to recursivelyconstructed partial trees.
Our parser works ina standalone fashion without reranking and doesnot rely on an external POS tagger or additionalmonolingual data for reliable estimates of syntac-tic and/or semantic representations of words.
Theparser achieves performance that is comparable tostate-of-the-art systems.In the future, we plan to apply our neural net-work structure to dependency parsing.
We are alsointerested in using long short-term memory neu-ral networks (Hochreiter and Schmidhuber, 1997)to better model the locality of propagated infor-mation from the stack and queue.
The parameterestimation under semi-supervised setting will beinvestigated further.AcknowledgmentsWe would like to thank Lemao Liu for suggestionswhile drafting this paper.
We are also grateful forvarious comments from anonymous reviewers.ReferencesGeorge Berg.
1992.
A connectionist parser with recur-sive sentence structure and lexical disambiguation.In Proc.
of AAAI ?92, pages 32?37.Ezra Black, Steve Abney, Dan Flickinger, ClaudiaGdaniec, Ralph Grishman, Phil Harrison, Don Hin-dle, Robert Ingria, Fred Jelinek, Judith Klavans,Mark Liberman, Mitchell Marcus, Salim Roukos,Beatrice Santorini, and Tomek Strzalkowski.
1991.Procedure for quantitatively comparing the syntac-tic coverage of english grammars.
In Proc.
of theWorkshop on Speech and Natural Language, pages306?311, Stroudsburg, PA, USA.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proc.
of ACL 2005, pages 173?180,Ann Arbor, Michigan, June.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proc.
of NAACL 2000, pages132?139, Stroudsburg, PA, USA.Danqi Chen and Christopher Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Proc.
of EMNLP 2014, pages 740?750,Doha, Qatar, October.Michael Collins and Terry Koo.
2005.
Discriminativereranking for natural language parsing.
Computa-tional Linguistics, 31(1):25?70, March.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proc.
ofACL 2004, pages 111?118, Barcelona, Spain, July.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proc.
of ACL ?97,pages 16?23, Madrid, Spain, July.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Lin-guistics, 29(4):589?637, December.Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo,and Giovanni Soda.
2003.
Towards incrementalparsing of natural language using recursive neuralnetworks.
Applied Intelligence, 19(1-2):9?25, May.Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen,Matthieu Devin, Mark Mao, Marc?aurelio Ranzato,Andrew Senior, Paul Tucker, Ke Yang, Quoc V. Le,and Andrew Y. Ng.
2012.
Large scale distributeddeep networks.
In Advances in Neural InformationProcessing Systems 25, pages 1223?1231.
CurranAssociates, Inc.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and robust neural network joint models for sta-tistical machine translation.
In Proc.
of ACL 2014,pages 1370?1380, Baltimore, Maryland, June.John Duchi and Yoram Singer.
2009.
Efficient onlineand batch learning using forward backward splitting.Journal of Machine Learning Research, 10:2899?2934, December.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
Journal of MachineLearning Research, 12:2121?2159, July.Jeffrey L. Elman.
1990.
Finding structure in time.Cognitive Science, 14(2):179?211.Xavier Glorot and Yoshua Bengio.
2010.
Understand-ing the difficulty of training deep feedforward neuralnetworks.
In Proc.
of the Thirteenth InternationalConference on Artificial Intelligence and Statistics(AISTATS-10), volume 9, pages 249?256.1177Christoph Goller and Andreas K?uchler.
1996.
Learn-ing task-dependent distributed representations bybackpropagation through structure.
In Proc.
ofIEEE International Conference on Neural Networks,1996, volume 1, pages 347?352 vol.1, Jun.David Hall, Greg Durrett, and Dan Klein.
2014.
Lessgrammar, more features.
In Proc.
of ACL 2014,pages 228?237, Baltimore, Maryland, June.Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsu-ruoka, and Takashi Chikayama.
2013.
Simple cus-tomization of recursive neural networks for seman-tic relation classification.
In Proc.
of EMNLP 2013,pages 1372?1376, Seattle, Washington, USA, Octo-ber.James Henderson.
2003.
Inducing history represen-tations for broad coverage statistical parsing.
InProc.
of HLT-NAACL 2003, pages 24?31, Strouds-burg, PA, USA.James Henderson.
2004.
Discriminative training of aneural network statistical parser.
In Proc.
of ACL2004, pages 95?102, Barcelona, Spain, July.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural Computation,9(8):1735?1780, November.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Proc.of NAACL-HLT 2012, pages 142?151, Montr?eal,Canada, June.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proc.
of ACL 2003,pages 423?430, Sapporo, Japan, July.Jonathan K. Kummerfeld, David Hall, James R. Cur-ran, and Dan Klein.
2012.
Parser showdown at thewall street corral: An empirical investigation of errortypes in parser output.
In Proc.
of EMNLP-CoNLL2012, pages 1048?1059, Jeju Island, Korea, July.Phong Le and Willem Zuidema.
2014.
The inside-outside recursive neural network model for depen-dency parsing.
In Proc.
of EMNLP 2014, pages729?739, Doha, Qatar, October.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The penn treebank.
Compu-tational Linguistics, 19(2):313?330, June.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InProc.
of ACL 2005, pages 75?82, Ann Arbor, Michi-gan, June.Marshall R. Mayberry and Risto Miikkulainen.
1999.Sardsrn: A neural network shift-reduce parser.
InProc.
of IJCAI ?99, pages 820?827, San Francisco,CA, USA.Sauro Menchetti, Fabrizio Costa, Paolo Frasconi, andMassimiliano Pontil.
2005.
Wide coverage naturallanguage processing using kernel methods and neu-ral networks for structured data.
Pattern Recogni-tion Letters, 26(12):1896?1906, September.Tom?a?s Mikolov, Martin Karafi?at, Luk?a?s Burget, Jan?Cernock?y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In Proc.of INTERSPEECH 2010, pages 1045?1048.Andriy Mnih and Yee W. Teh.
2012.
A fast and simplealgorithm for training neural probabilistic languagemodels.
In John Langford and Joelle Pineau, edi-tors, Proc.
of ICML-2012, pages 1751?1758, NewYork, NY, USA.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proc.
of NAACL-HLT2007, pages 404?411, Rochester, New York, April.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc.
of COLING-ACL2006, pages 433?440, Sydney, Australia, July.Jordan B. Pollack.
1990.
Recursive distributed repre-sentations.
Artificial Intelligence, 46(1-2):77?105,November.Kenji Sagae and Alon Lavie.
2005.
A classifier-based parser with linear run-time complexity.
InProc.
of the Ninth International Workshop on Pars-ing Technology, pages 125?132, Vancouver, BritishColumbia, October.Andrew Senior, Georg Heigold, Marc?Aurelio Ran-zato, and Ke Yang.
2013.
An empirical study oflearning rates in deep neural networks for speechrecognition.
In Proc.
of ICASSP 2013, pages 6724?6728, May.Richard Socher, John Bauer, Christopher D. Manning,and Ng Andrew Y.
2013.
Parsing with compo-sitional vector grammars.
In Proc.
of ACL 2013,pages 455?465, Sofia, Bulgaria, August.Pontus Stenetorp.
2013.
Transition-based dependencyparsing using recursive neural networks.
In Proc.of Deep Learning Workshop at the 2013 Conferenceon Neural Information Processing Systems (NIPS),Lake Tahoe, Nevada, USA, December.Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.2014.
Recurrent neural networks for word align-ment model.
In Proc.
of ACL 2014, pages 1470?1480, Baltimore, Maryland, June.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency net-work.
In Proc.
of HLT-NAACL 2003, pages 173?180, Stroudsburg, PA, USA.1178rep.
size 32 64 128 256 512 1024 2048 4096devWSJ-32 89.91 90.15 90.48 90.70 90.75 90.87 91.11 91.0464 90.37 90.73 90.81 90.62 90.71 91.11 91.34 91.36CTB-32 79.25 81.59 82.80 82.68 84.17 85.12 85.61 85.7664 84.04 83.29 82.92 85.12 85.24 85.77 86.28 86.94testWSJ-32 89.03 89.49 89.75 90.45 90.37 90.01 90.33 90.4064 89.74 90.16 90.48 90.06 89.91 90.68 91.05 90.94CTB-32 75.19 78.29 80.46 81.87 83.16 82.64 83.13 83.6764 80.11 81.35 81.67 82.91 83.76 84.33 83.76 84.38Table 8: Comparison of various state/word representation dimension size measured by labeled F1(%).?-32?
denotes the hidden state sizem = 32.
The numbers in bold indicate the best results for each hiddenstate dimension.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proc.
ofACL 2010, pages 384?394, Uppsala, Sweden, July.Zhiguo Wang and Nianwen Xue.
2014.
Joint pos tag-ging and transition-based constituent parsing in chi-nese with non-local features.
In Proc.
of ACL 2014,pages 733?742, Baltimore, Maryland, June.Naiwen Xue, Fei Xia, Fu-dong Chiou, and MartaPalmer.
2005.
The penn chinese treebank: Phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, 11(2):207?238, June.Yue Zhang and Stephen Clark.
2009.
Transition-basedparsing of the chinese treebank using a global dis-criminative model.
In Proc.
of the 11th InternationalConference on Parsing Technologies (IWPT?09),pages 162?171, Paris, France, October.Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,and Jingbo Zhu.
2013.
Fast and accurate shift-reduce constituent parsing.
In Proc.
of ACL 2013,pages 434?443, Sofia, Bulgaria, August.A Additional ResultsWe conducted additional experiments by enlarg-ing the word representation vector size m?in Ta-ble 8.
In general, we observed further gains withricher word representation, but suffered overfit-ting effects when setting m?= 4096.
The re-sults with m = 64 and m?= 4096 achievedthe best performance on the development data,91.36% and 86.94% labeled F1 for WSJ and CTB,respectively, wherein we observed the accuraciesof 90.94% and 84.38% on the test data, respec-tively.
Note that it took approximately one weekto train the model when m?= 4096 under WSJ,which was impractical to analyze the results fur-ther, e.g.
comparison with other training objec-tives.1179
