Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 753?761, Dublin, Ireland, August 23-29 2014.Unsupervised Multiword Segmentation of Large Corpora usingPrediction-Driven Decomposition of n-gramsJulian Brooke*?Vivian Tsang?Graeme Hirst*Fraser Shein*?
*Department of Computer ScienceUniversity of Torontojbrooke@cs.toronto.edugh@cs.toronto.edu?Quillsoft Ltd.Toronto, Canadavtsang@quillsoft.cafshein@quillsoft.caAbstractWe present a new, efficient unsupervised approach to the segmentation of corpora into multiwordunits.
Our method involves initial decomposition of common n-grams into segments which max-imize within-segment predictability of words, and then further refinement of these segments intoa multiword lexicon.
Evaluating in four large, distinct corpora, we show that this method cre-ates segments which correspond well to known multiword expressions; our model is particularlystrong with regards to longer (3+ word) multiword units, which are often ignored or minimizedin relevant work.1 IntroductionIdentification of multiword units in language is an active but increasingly fragmented area of research, aproblem which can limit the ability of others to make use of units beyond the level of the word as inputto other applications.
General research on word association metrics (Church and Hanks, 1990; Smadja,1993; Schone and Jurafsky, 2001; Evert, 2004; Pecina, 2010), though increasingly comprehensive inits scope, has mostly failed to identify a single best choice, leading some to argue that the variety ofmultiword phenomena must be tackled individually.
For instance, there is a body of research focusingspecifically on collocations that are (to some degree) non-compositional, i.e.
multiword expressions (Saget al., 2002; Baldwin and Kim, 2010), with individual projects often limited to a particular set of syntacticpatterns, e.g.
verb-noun combinations (Fazly et al., 2009).
A major issue with approaches involvingstatistical association is that they rarely address expressions larger than 2 words (Heid, 2007); in corpuslinguistics, larger sequences referred to as lexical bundles are extracted using an n-gram frequency cutoff(Biber et al., 2004), but the frequency threshold is typically set very high so that only a very limited setis extracted.
Another drawback, common to almost all these methods, is that they rarely offer an explicitsegmentation of a text into multiword units, which would be preferable for downstream uses such asprobabilistic distributional semantics.
An exception is the Bayesian approach of Newman et al.
(2012),but their method does not scale well (see Section 2).
Our own long-term motivation is to identify a widevariety of multiword units for assisting language learning, since correct use of collocations is known topose a particular challenge to learners (Chen and Baker, 2010).Here, we present a multiword unit segmenter1with the following key features:?
It is entirely unsupervised.?
It offers both segmentation of the input corpus and a lexicon which can be used to segment newcorpora.?
It is scalable to very large corpora, and works for a variety of corpora.?
It is language independent.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1The software is available at http:/www.cs.toronto.edu/?jbrooke/ngram decomp seg.py .753?
It does not inherently limit possible units with respect to part-of-speech or length.?
It has a bare minimum of parameters, and can be used off-the-shelf: in particular, it does not requirethe choice of an arbitrary cutoff for some uninterpretable statistical metric.?
It does, however, include a parameter fixing the minimum number of times that a valid multiwordunit will appear in the corpus, which ensures sufficient usage examples for relevant applications.Our method involves three major steps: extraction of common n-grams, initial segmentation of thecorpus, and a refinement of the resulting lexicon (and, by extension, the initial segmentation).
Thelatter two steps are carried out using a simple but novel heuristic based on maximizing word predictionwithin multiword segments.
Importantly, our method requires just a few iterations through the corpus,and in practice these iterations can be parallelized.
Evaluating with an existing set of multiword unitsfrom WordNet in four large corpora from distinct genres, we show that our initial segmentation offersextremely good subsumption of known collocations, and after lexicon refinement the model offers agood trade-off between subsumption and exact matches.
We also evaluate a sample of our multiwordvocabulary using crowdsourcing, and offer a qualitative analysis.2 Related WorkIn computational linguistics, there is a large body of research that proposes and/or evaluates lexical as-sociation measures for the creation of multiword lexicons (Church and Hanks, 1990; Smadja, 1993;Schone and Jurafsky, 2001; Evert, 2004): there are many more measures than can be addressed here?work by Pecina (2010) considered 82 variations?but popular choices include the t-test, log likelihood,and pointwise mutual information (PMI).
In order to build lexicons using these methods, particular syn-tactic patterns and thresholds for the metrics are typically chosen.
Many of the statistical metrics do notgeneralize at all beyond two words, but PMI (Church and Hanks, 1990), the log ratio of the joint proba-bility to the product of the marginal probabilities, is a prominent exception.
Other measures specificallydesigned to address collocations of larger than two words include the c-value (Frantzi et al., 2000), ametric designed for term extraction which weights term frequency by the log length of the n-gram whilepenalizing n-grams that appear in frequent larger ones, and mutual expectation (Dias et al., 1999), whichproduces a normalized statistic that reflects how much a candidate phrase resists the omission of anyparticular word.
Another approach is to simply to combine known n?
1 collocations to form n-lengthcollocations (Seretan, 2011), but this is based on the assumption that all longer collocations are built upfrom shorter ones?idioms, for instance, do not usually work in that way.An approach used in corpus linguistics which does handle naturally longer sequences is the studyof lexical bundles (Biber et al., 2004), which are simply n-grams that occur above a certain frequencythreshold.
This includes larger phrasal chunks that would be missed by traditional collocation extraction,and so research in this area has tended to focus on how particular phrases (e.g.
if you look at) are indi-ciative of particular genres (e.g.
university lectures).
In order to get very reliable phrases, the thresholdis typically set high enough (Biber et al.
use 40 occurrences in 1 million words) to filter out the vastmajority of expressions in the process.With respect to the features of our model, the work closest to ours is probably that of Newman et al.(2012).
Like us, they offer an unsupervised solution, in their case a generative Dirichlet Process modelwhich jointly creates a segmentation of the corpus and a multiword term vocabulary.
Their method,however, requires full Gibbs sampling with thousands of iterations through the corpus (Newman et al.report using 5000), an approach which is simply not tractable for the large corpora that we address inthis paper (which are roughly 1000 times larger than theirs).
Though the model is general, their focus islimited to term extraction, and for larger terms they compare only with the c-value approach of Frantziet al.
(2000).
Other closely related work includes general tools available for creating multiword lexiconsusing association measures or otherwise exploring the collocational behavior of words (Kilgarriff andTugwell, 2001; Araujo et al., 2011; Kulkarni and Finlayson, 2011; Pedersen et al., 2011).
Other relatedbut distinct tasks include syntactic chunking (Abney, 1991) and word segmentation for Asian languages,in particular Chinese (Emerson, 2005).7543 Method3.1 Prediction-based segmentationOur full method consists of multiple independent steps, but it is based on one central and relatively simpleidea that we will introduce first.
Given a sequence of words, w1.
.
.wn, and statistics (i.e.
n-gram counts)about the use of these words in a corpus, we first define p(wi|wj,k) as the conditional probability of someword wiappearing with some contextual subsequence wj.
.
.wi?1,wi+1.
.
.wk,1 ?
j ?
i ?
k ?
n. In thecase i = j = k, this is simply the marginal probability, p(wi).
We then define the word predictabilityof some wiin the context w1,nas the log of the maximal conditional probability of the word across allpossible choices of j and k:pred(wi,w1,n) = maxj,klog p(wi|wj,k)We can define predictability for the entire sequence then as:pred(w1,n) =n?i=1pred(wi,w1,n)Now we consider the case where we have a set of possible segmentations S of the sequence, whereeach segmentation s ?
S can be viewed as a (possibly empty) set of segment boundaries ?s0,s1, .
.
.
,sm?.Among the available options, our optimal segmentation is:argmaxs?Sm?1?i=0pred(wsi,si+1?1)That is, we will prefer the segmentation which maximizes the overall predictability of each word in thesequence, under the restriction that we only predict words using the context within their segments.
Thisreflects our basic assumption that words within a good segment, i.e.
a multiword unit, are (much) morepredictive of each other than words outside a unit.
Note that if our probabilities are calculated from thefull set of n-gram counts for the corpus being segmented and the set of possible segmentations S is notconstrained, a segmentation with a smaller number of breaks will generally be preferred over one withmore breaks.
However, in practice we will be greatly constraining S and also using probabilities basedon only a subset of all the information in the corpus.3.2 Extraction of n-gramsIn order to carry out a segmentation of the corpus using this method, we first need to extract statisticsin the form of n-gram counts.
Given a minimum occurrence threshold, this can be done efficiently evenfor large corpora in an iterative fashion until all n-grams have been extracted.
For all our experimentshere, we limit ourselves to n-grams that appear at least once in 10 million tokens, and we did not collectn-grams for n > 10 (which are almost always the result of duplication of texts in the corpus).
For the pur-poses of calculating conditional probabilities given surrounding context in our predictive segmentation,we collected both standard n-grams as well as (for n?
3) skip n-grams with a missing word (e.g.
basic *processes where the asterisk indicates that any word could appear in that slot).
Here we use lower-casedunlemmatized tokens, excluding punctuation, though for languages with more inflectional morphologythan English, lemmatization would be advised.3.3 Initial segmentationGiven these n-gram statistics, our initial segmentation proceeds as follows: For each sentence in thecorpus, we identify all maximum length n-grams in the sentence, i.e.
all those n-grams for n ?
2 wherethere is no larger n-gram which contains them while still being above our threshold of occurrence.
Thesen-grams represent the upper bound of our segmentation: we will never break into segments larger thanthese.
However, there are many overlaps among these n-grams (in fact, with a low threshold the vastmajority of n-grams overlap with at least one other), and for proper segmentation we need to resolve755Figure 1: Three-step procedure for n-gram decomposition into multiword units.
a) shows the maximaln-grams identified in the sentence, b) is the segmentation after the initial pass of the corpora, and c)shows further decomposition of segments after a pass through the lexicon resulting from b).all overlaps between these maximal n-grams by inserting at least one break.
For this we apply ourprediction-based decomposition technique.
In our discussion in Section 3.1, we did not consider howthe possible segmentations were selected, but now we can be explicit: the set S consists of all possiblesegmentations which minimally resolve all n-gram overlaps.
By minimally resolve, we mean that theremoval of any breakpoint from our set would result in an unresolved overlap: in short, there are noextraneous breaks, and therefore no cases where a possible set of breaks is a subset of another possibleset.
Figure 1a shows a real example: if we just consider the last three maximal n-grams, there are twopossible minimal breaks: a single break between in and basic or two breaks, one between roles and inand one between basic and cellular.Rather than optimizing over all possible breaks over the whole sentence, which is computationallyproblematic, we simplify the algorithm somewhat by moving sequentially through each n-gram over-lap in the sentence, taking any previous breaks as given while considering only the minimum breaksnecessary to resolve any overlaps that directly influence the segmentation of the two overlapping spansunder consideration, which is to say any other overlapping spans which contain at least one word alsocontained in at least one of overlapping spans under consideration.
For example, in Figure 1a we firstdeal independently with each of the first two overlaps (the spans modified with glucose and are enrichedin lipid rafts, and then we consider the final two overlaps together: The result is shown in Figure 1b.
Indevelopment, we tested including more context (i.e.
considering second-order influence) and found nobenefit.
Since we do not consider breaks other than those required to resolve overlapping n-grams, thesesegments tend to be long.
This is by design; our intention is that these segments will subsume as manymultiword units as possible, and therefore will be amenable to refinement by further decomposition inthe next step.3.4 Lexicon decompositionBased on the initial segmentation of the entire corpus, we extract a tentative lexicon, with correspondingcounts.
Then, in order from longest to shortest, we consider decomposition of each entry.
First, usingour prediction-based decomposition method, we find the best decomposition of the entry into two parts;note that we only need to consider one break per lexicon entry, since breaks in the (smaller) parts willbe considered independently later in the process.
If the count in our lexicon is below the occurrencethreshold, we always carry out this split, which means we remove the entry from the lexicon and (afterall n-grams of that length have been processed, so as to avoid ordering effects) add its counts to thecounts of n-grams of its best decomposition.
If the count is above the threshold, we preserve the fullentry (for entries of length 3 or greater) only if the following inequality is true for each subsegment wj,kin the full entry w1,n:k?i= jpred(wi,w1,n)?
pred(wj,k) > log p(wj,k)?
log p(w1,n)756That is, the ratio (expressed as a difference of logarithms) between the count of the segment and the fullunsegmented entry (in our preliminary lexicon) is lower than the ratio of the predictability (as definedin our discussion of prediction-based decomposition) of the words in the segment with the context ofthe full entry to the predictability of words with only the context included within the segment (whichis just pred(wj,k)).
In other words, we preserve only longer multiword sequences in our lexicon whenany decrease in the probability of the full entry relative to its smaller components2is fully offset by anincrease in the conditional probability of the individual words of that segment when the larger contextfrom the full segment is available.
For example, after we have decided on a potential break in the phrasebasic | cellular process from our example in Figure 1, we compare the (marginal) probability ?lost?
byincluding basic in a larger phrase, i.e.
the ratio of counts of basic to basic cellular process in our lexicon),to the (conditional) probability ?gained?
by how much more predictable the segment is in this context;when the segment in question is a single word, as in this case, this is simply p(basic|cellular process)/p(basic), and we break only when there is more gain than loss.
This restriction could be parameterizedfor more fine-grained control of the trade-off between larger and smaller segments in specific cases, butin the interest of avoiding nuisance parameters we just use it directly.
Once we have decomposed alleligible entries to create a final lexicon, we apply these same decompositions to the segments in ourinitial segmentation to produce a final segmentation (see Figure 1c).4 EvaluationMultiword lexicons are typically evaluated in one of two ways: direct comparison to an existing lexicon,or precision of the top n candidates offered by the model.
There are problems with both these meth-ods, since there are no resources that offer a truly comprehensive treatment of multiword units, definedbroadly, and the top n candidates from a model for small n may not be a particularly representative sam-ple: in particular, they might not include more common terms, which should be given more weight whenone is considering downstream applications.
Given the dual output of our model, evaluation using seg-mentation is another option, except that creating full gold standard segmentations would be a particularlydifficult annotation task, since our notion of multiword unit is a broad one.In light of this, we evaluate by taking the best from these various approaches.
Given an existing mul-tiword lexicon, we can evaluate not by comparing our lexicon to it directly, but rather by looking at theextent to which our segmentation preserves these known multiword units.
There are several major ad-vantages to this approach: first, it does not require a full lexicon or gold standard segmentation; second,common units are automatically given more weight in the evaluation; third, we can use it for evaluationin very large corpora.
Our two main metrics are subsumption (Sub), namely the percentage of multiwordtokens that are fully contained with a segment, and exact matches (Exact), the percentage of multiwordtokens which correspond exactly to a segment.
Exact matches would seem to be preferable to subsump-tion, but in practice this is not necessarily the case, since our method often identifies valid compoundterms and larger constructions than our reference lexicon contains; for example, WordNet only containsthe expression a lot, but when appearing as part of a noun phrase our model typically segments this to alot of, which, in our opinion, is a preferable segmentation.
To quantify overall performance, we calculatea harmonic mean (Mean) of the two metrics.
We also looked specifically at performance for terms of 3or more words (Mean 3+), which are less studied and more relevant to our interests.Our second evaluation focuses on the quality of these longer terms with a post hoc annotation ofoutput from our model and the best alternatives.
We randomly extracted pairs of segments of three wordsor more where our model mostly but not entirely overlapped with an alternative model (750 examplesper corpus per method), and asked CrowdFlower workers to choose which output seemed to be a bettermultiword unit in the context; they were shown the entire sentence with the relevant span underlined,and then the two individual chunks separately.
To ensure quality, we used our multiword lexicon to2This probability is based on the respective counts in our preliminary lexicon at this step in the process, not the original n-gram probability.
One key advantage to doing the initial segmentation first is that words that appear consistently in larger units,an extreme example is the bigram vector machine in the term support vector machine, already have low or zero probability, andwill not appear in the lexicon or be good candidate segments for decomposition.
This rather intuitively accomplishes what thec-value metric is modeling by applying negative weights to candidates appearing in larger n-grams.757create gold standard examples (comparing known multiword units to purposely bad segmentations whichoverlapped with them), and used them to test and filter out unreliable workers: for inclusion in our finalset, we required a minimum 90% performance on the test questions.
We also limited each contributor toonly 250 judgments, so that our results reflected a variety of opinions.We considered a number of alternatives to our approach, though we limited the comparison to methodswhich could predict segments greater than 2 words, those that were computationally feasible for largecorpora, and those which segment into single words only as a last resort: approaches which prefer singlewords cannot do well under our evaluation because we have no negative examples, only positive ones.The majority of our alternatives involve ranking all potential n-grams (not just the maximal) with n?
2and then greedily segmenting them: big-n prefers longer n-grams (with a backoff to counts); c-value isused for term extraction (Frantzi et al., 2000) and was also compared to by Newman et al.
(2012); MErefers to the Mutual Expectation metric (Dias et al., 1999); and PMI uses a standard extension of PMI tomore than 2 words.
We also tested standard (pairwise) PMI as a metric for recursively joining contiguousunits (starting with words) into larger units until no larger units can be formed (PMI join), and a versionof our decomposition algorithm which selects the minimal breaks which maximize total word countacross segments rather than total word predictability (count decomp); the fact that traditional associationmetrics are not defined for single words prevents us from using them as alternatives to predictability inour decomposition approach.
Finally, we also include an oracle which chooses the correct n-grams whenthey are available for segmentation, but which still fails for units that are below our threshold.We evaluated our model in four large English corpora: news articles from the Gigaword corpus (Graffand Cieri, 2003) (4.9 billion tokens), out-of-copyright texts from the Gutenberg Project3(1.7 billiontokens), a collection of abstracts from PubMed (2.2 billion tokens)4, and blogs from the ICWSM 2009social media corpus (Burton et al., 2009) (1.1 million tokens).
Our main comparison lexicon is WordNet3.0, which contains a good variety of expressions appropriate to the various genres, but we also includedmultiword terms from the Specialist Lexicon5for better coverage of the biomedical domain.
One issuewith our evaluation is that it assumes all tokens are true instances of the multiword unit in question; wecarried out a manual inspection of multiword tokens identified by string match in our development sets(5000 sentences set aside from each of the abstract and blog corpora), and excluded from the evaluationa small set of idiomatic expressions (e.g.
on it, do in) whose literal, non-MWE usage is too common forthe expression to be used reliably for evaluation; otherwise, we were satisfied that the vast majority ofmultiword tokens were true matches.
When one multiword token appeared within another, we ignoredthe smaller of the two; when two overlapped in the text, we ignored both.5 ResultsAll the results for the main evaluation are shown in Table 1.
First, we observe that our initial segmentationalways provides the highest subsumption, and our final lexicon always provides the highest harmonicmean, with a modest drop in subsumption but a huge increase in exact matches.
The alternative modelsfall roughly into two categories: those which have reasonably high subsumption, but few exact matches(PMI rank seems to be the best of these) and those that have many exact matches (sometimes betterthan either of our models) but are almost completely ineffective for identifying multiword units of lengthgreater than 2 (ME rank and c-value, with ME offering more exact matches): the latter phenomenon isattributable to the predominance of two-word multiword tokens in our evaluation, which means a modelcan do reasonably well by guessing mostly two-word units.
For the corpora with more multiword unitsof greater length, i.e.
the PubMed abstracts and the Gutenberg corpus, our method also provides the mostexact matches.
Our best results come in the PubMed corpus, probably because the texts are the mostuniform, though results are satisfactory in all four corpora tested here, which represent a considerablerange of genres.3http://www.gutenberg.org .
Here we use the English texts from the 2010 image, with headers and footers filtered out usingsome simple heuristics.4http://www.ncbi.nlm.nih.gov/pubmed/5http://www.nlm.nih.gov/research/umls/new users/online learning/LEX 001.htm758Table 1: Performance in segmenting multiword units of various segmentation methods in 4 large corpora.Sub.
= Subsumption (%); Exact = Exact Match (%); Mean = Harmonic mean of Sub and Exact; Mean 3+= Harmonic mean of Sub and Exact for multiword tokens of at length 3 or more.
Bold is best in columnfor corpus, excluding the oracle.MethodGigaword news articles Gutenberg textsSub Exact Mean Mean 3+ Sub Exact Mean Mean 3+Oracle 97.1 97.1 97.1 95.5 97.0 97.0 97.0 97.8big-n rank 88.7 28.8 43.5 51.4 84.9 30.1 44.4 57.5c-value rank 69.1 66.1 67.6 23.3 58.6 57.7 58.2 12.6ME rank 75.3 70.0 72.6 14.4 63.2 61.0 62.1 10.9PMI rank 90.8 30.0 45.1 53.5 86.9 32.8 47.7 61.2PMI join 83.1 32.8 47.0 43.7 77.7 32.6 46.0 45.5Count decomp 75.9 31.3 44.3 47.1 69.2 31.5 43.3 54.2Prediction decomp, initial 92.2 36.4 52.2 64.4 89.3 38.7 54.0 71.6Prediction decomp, final 85.6 66.4 75.2 63.8 78.9 62.8 70.0 61.6MethodPubMed abstracts ICWSM blogsSub Exact Mean Mean 3+ Sub Exact Mean Mean 3+Oracle 91.9 91.9 91.9 84.0 96.5 96.5 96.5 99.4big-n rank 82.2 40.1 53.9 55.5 86.1 33.3 48.0 60.8c-value rank 63.2 62.3 62.7 21.7 64.3 62.4 63.3 14.6ME rank 68.5 65.8 67.1 9.1 69.7 66.2 67.9 11.7PMI rank 87.0 41.4 56.1 58.3 88.4 35.7 50.8 63.4PMI join 79.8 39.7 53.0 46.8 80.3 35.4 49.1 47.0Count decomp 71.0 38.4 49.9 50.4 71.5 33.5 45.6 53.9Prediction decomp, initial 88.6 50.3 64.1 67.2 90.5 40.3 55.8 70.9Prediction decomp, final 85.2 73.4 78.8 69.5 83.2 64.9 72.9 66.9Table 2: CrowdFlower pairwise preference evaluation, our full model versus a selection of alternativesComparison Preference for Prediction decomp, finalPrediction decomp, final vs. ME 57.9%Prediction decomp, final vs. Multi PMI 71.0%Prediction decomp, final vs.
Prediction decomp, initial 70.5%For our crowdsourced evaluation, we compared our final model to the best models of each of the twomajor types from the first round, namely Mutual Expectation and PMI rank, as well as our initial seg-mentation.
The results are given in Table 2.
Our full model is consistently preferred over the alternatives.This is not surprising in the case of the high-subsumption, low-accuracy models, since the resulting seg-ments often have extraneous words included: an example is in spite of my, which our model correctlysegmented to just in spite of.
Given that the ME ranking rarely produces units larger than 2 words, how-ever, we might have predicted that when it does it would be more precise than our model, but in factour model was somewhat preferred (a chi-square test confirmed that this result was statistically differentfrom chance, p < 0.001).
An example of an instance where our model offered a better segmentation iscall for an end to as compared to for an end to from the ME model, though there are also many instanceswhere the ME segmentation is more sensible, e.g.
what difference does it make as compared to differencedoes it make from our model.Looking closer at the output and vocabulary of our model across the various genres, we see a widerange of multiword phenomena: in the medical abstracts, for instance, there is a lot of medical jargon (e.g.daily caloric intake) but also other larger connective phrases and formulaic language (e.g.
an alternativeexplanation for, readily distinguished from).
The blogs also have (very different) formulaic language of759the sort studied using lexical bundles (e.g.
all I can say is that, where else can you) and lots of idiomaticlanguage (e.g.
reinventing the wheel, look on the bright side).
The idioms from the Gutenberg, notsurprisingly, tend to be less clich?ed and more evocative (e.g.
ghost of a smile); there are rather stodgyexpressions like far be it from me and conjunctions we would not see in the other corpora (e.g.
rocksand shoals, masters and mistresses).
By contrast, many of the larger expressions in the news articles arefrom sports and finance (e.g.
investor demand for, tied the game with), with many that would be filteredout using the simple grammatical filters often applied in this space.
However, for bigrams in particular,some additional syntactic filtering is clearly warranted.6 ConclusionWe have presented an efficient but effective method for segmenting a corpus into multiword collocationalunits, with a particular focus on units of length greater than two.
Our evaluation indicates that thismethod results in high-quality segments that capture a variety of multiword phenomena, and is betterin this regard than alternatives based on relevant association measures.
This result is consistent acrosscorpora, though we do particularly well with highly stereotyped language such as seen in the biomedicaldomain.Future work on improving the model will likely focus on extensions related to syntax, for instancebootstrapped POS filtering and discounting of predictability that can be attributed solely to syntacticpatterns.
Our method could also be adapted to decompose full syntactic trees rather than sequences ofwords, offering tractable alternatives to Bayesian approaches that identify recurring tree fragments (Cohnet al., 2009); this would allow us, for instance, to correctly identify constructions with long-distancedependencies or other kinds of variation where relying on the surface form is insufficient (Seretan, 2011).With regards to applications, we will be investigating how to help learners notice these chunks whenreading and then use them appropriately in their own writing; this work will eventually intersect withthe well-established areas of grammatical error correction (Leacock et al., 2014) and automated essayscoring (Shermis and Burstein, 2003).
As part of this, we will be building distributional lexical repre-sentations of these multiword units, which is why our emphasis here was on a highly scalable method.Part of our interest is of course in capturing the semantics of idiomatic phrases, but we note that evenin the case when a multiword unit is semantically compositional, it might provide de facto word sensedisambiguation or be stylistically distinct from its components, i.e.
be very specific to a particular genreor sub-genre.
Therefore, provided we have enough examples to get reliable distributional statistics, theselarger units are likely to provide useful information for various downstream applications.AcknowledgmentsThis work was supported by the Natural Sciences and Engineering Research Council of Canada and theMITACS Elevate program.
Thanks to our reviewers and also Tong Wang and David Jacob for their input.ReferencesSteven Abney.
1991.
Parsing by chunks.
In Robert Berwick, Steven Abney, and Carol Tenny, editors, Principle-Based Parsing, pages 257?278.
Kluwer Academic Publishers.Vitor De Araujo, Carlos Ramisch, and Aline Villavicencio.
2011.
Fast and flexible MWE candidate generationwith the mwetoolkit.
In Proceedings of the Multiword Expression Workshop at ACL 2011 (MWE 2011).Timothy Baldwin and Su Nam Kim.
2010.
Multiword expressions.
In Nitin Indurkhya and Fred J. Damerau,editors, Handbook of Natural Language Processing, Second Edition.
CRC Press, Taylor and Francis Group,Boca Raton, FL.Douglas Biber, Susan Conrad, and Viviana Cortes.
2004.
If you look at.
.
.
: Lexical bundles in university teachingand textbooks.
Applied Linguistics, 25:371?405.Kevin Burton, Akshay Java, and Ian Soboroff.
2009.
The ICWSM 2009 Spinn3r Dataset.
In Proceedings of theThird Annual Conference on Weblogs and Social Media (ICWSM ?09), San Jose, CA.760Yu-Hua Chen and Paul Baker.
2010.
Lexical bundles in L1 and L2 academic writing.
Language Learning &Technology, 14(2):30?49.Kenneth Ward Church and Patrick Hanks.
1990.
Word association norms, mutual information, and lexicography.Computational Linguistics, 16(1):22?29.Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009.
Inducing compact but accurate tree-substitution gram-mars.
In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Linguistics (NAACL ?09).Ga?el Dias, Sylvie Guillor?e, and Jos?e Gabriel Pereira Lopes.
1999.
Language independent automatic acquisitionof rigid multiword units from unrestricted text corpora.
In Proceedings of Conf?erence Traitement Automatiquedes Langues Naturelles (TALN) 1999.Thomas Emerson.
2005.
The second international Chinese word segmentation bakeoff.
In Proceedings of theFourth SIGHAN Workshop on Chinese Language Processing.Stefan Evert.
2004.
The statistics of word cooccurences?word pairs and collocatoins.
Ph.D. thesis, University ofStuttgart.Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009.
Unsupervised type and token identification of idiomaticexpressions.
Computational Linguistics, 35(1):61?103.Katerina Frantzi, Sophia Ananiadou, and Hideki Mima.
2000.
Automatic recognition of multi-word terms: thec-value/nc-value method.
International Journal on Digital Libraries, 3:115?130.David Graff and Christopher Cieri.
2003.
English Gigaword.
Linguistic Data Consortium, Philadelphia, PA.Ulrich Heid.
2007.
Compuational linguistic aspects of phraseology.
In Harald Burger, Dmitrij Dobrovol?skij,Peter K?uhn, and Neal R. Norrick, editors, Phraseology.
An international handbook.
Mouton de Gruyter, Berlin.Adam Kilgarriff and David Tugwell.
2001.
Word sketch: Extraction and display of significant collocations forlexicography.
In Proceedings of the ACL Workshop on Collocation: Computational Extraction, Analysis andExploitation.Nidhi Kulkarni and Mark Finlayson.
2011. jMWE: A Java toolkit for detecting multi-word expressions.
InProceedings of the Multiword Expression Workshop at ACL 2011 (MWE 2011).Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault.
2014.
Automated Grammatical ErrorDetection for Language Learners (2nd Edition).
Morgan & Claypool.David Newman, Nagendra Koilada, Jey Han Lau, and Timothy Baldwin.
2012.
Bayesian text segmentation forindex term identification and keyphrase extraction.
In Proceedings of the 24th International Conference onComputational Linguistics (COLING ?12).Pavel Pecina.
2010.
Lexical association measures and collocation extraction.
Language Resources and Evalua-tion, 44:137?158.Ted Pedersen, Satanjeev Banerjee, Bridget McInnes, Saiyam Kohli, Mahesh Joshi, and Ying Liu.
2011.
TheNgram statistics package (text::nsp) : A flexible tool for identifying ngrams, collocations, and word associations.In Proceedings of the Multiword Expression Workshop at ACL 2011 (MWE 2011).Ivan A.
Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger.
2002.
Multiword expressions:A pain in the neck for NLP.
In Proceedings of the 3rd International Conference on Intelligent Text Processingand Computational Linguistics (CICLing ?02).Patrick Schone and Dan Jurafsky.
2001.
Is knowledge-free induction of multiword unit dictionary headwords asolved problem?
In Proceedings of Empirical Methods in Natural Language Processing (EMNLP ?01).Violeta Seretan.
2011.
Syntax-Based Collocation Extraction.
Springer.Mark D. Shermis and Jill Burstein, editors.
2003.
Automated Essay Scoring: A Cross-Disciplinary Approach.Lawrence Erlbaum Associates, Mahwah, NJ.Frank Smadja.
1993.
Retrieving collocations from text: Xtract.
Computational Linguistics, pages 143?177.761
