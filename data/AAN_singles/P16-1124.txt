Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1308?1318,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsKnowledge Base Completion via Coupled Path RankingQuan Wang?, Jing Liu?, Yuanfei Luo?, Bin Wang?, Chin-Yew Lin?
?Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100093, China{wangquan,luoyuanfei,wangbin}@iie.ac.cn?Microsoft Research, Beijing 100080, China{liudani,cyl}@microsoft.comAbstractKnowledge bases (KBs) are often greatlyincomplete, necessitating a demand for K-B completion.
The path ranking algorith-m (PRA) is one of the most promising ap-proaches to this task.
Previous work onPRA usually follows a single-task learn-ing paradigm, building a prediction mod-el for each relation independently with itsown training data.
It ignores meaningfulassociations among certain relations, andmight not get enough training data for lessfrequent relations.
This paper proposes anovel multi-task learning framework forPRA, referred to as coupled PRA (CPRA).It first devises an agglomerative clusteringstrategy to automatically discover relation-s that are highly correlated to each other,and then employs a multi-task learning s-trategy to effectively couple the predictionof such relations.
As such, CPRA takes in-to account relation association and enablesimplicit data sharing among them.
Weempirically evaluate CPRA on benchmarkdata created from Freebase.
Experimen-tal results show that CPRA can effective-ly identify coherent clusters in which rela-tions are highly correlated.
By further cou-pling such relations, CPRA significantlyoutperforms PRA, in terms of both predic-tive accuracy and model interpretability.1 IntroductionKnowledge bases (KBs) like Freebase (Bollack-er et al, 2008), DBpedia (Lehmann et al, 2014),and NELL (Carlson et al, 2010) are extremelyuseful resources for many NLP tasks (Cucerzan,2007; Schuhmacher and Ponzetto, 2014).
Theyprovide large collections of facts about entities andtheir relations, typically stored as (head entity, re-lation, tail entity) triples, e.g., (Paris, capitalOf,France).
Although such KBs can be impressivelylarge, they are still quite incomplete and missingcrucial facts, which may reduce their usefulness indownstream tasks (West et al, 2014; Choi et al,2015).
KB completion, i.e., automatically infer-ring missing facts by examining existing ones, hasthus attracted increasing attention.
Approaches tothis task roughly fall into three categories: (i) pathranking algorithms (PRA) (Lao et al, 2011); (ii)embedding techniques (Bordes et al, 2013; Guoet al, 2015); and (iii) graphical models such asMarkov logic networks (MLN) (Richardson andDomingos, 2006).
This paper focuses on PRA,which is easily interpretable (as opposed to em-bedding techniques) and requires no external logicrules (as opposed to MLN).The key idea of PRA is to explicitly use pathsconnecting two entities to predict potential rela-tions between them.
In PRA, a KB is encoded asa graph which consists of a set of heterogeneousedges.
Each edge is labeled with a relation typethat exists between two entities.
Given a specificrelation, random walks are first employed to findpaths between two entities that have the given rela-tion.
Here a path is a sequence of relations linkingtwo entities, e.g., hbornIn??????
ecapitalOf?????????
t.These paths are then used as features in a bina-ry classifier to predict if new instances (i.e., entitypairs) have the given relation.While KBs are naturally composed of multiplerelations, PRA models these relations separatelyduring the inference phase, by learning an individ-ual classifier for each relation.
We argue, however,that it will be beneficial for PRA to model certainrelations in a collective way, particularly when therelations are closely related to each other.
For ex-ample, given two relations bornIn and livedIn,1308there must be a lot of paths (features) that are pre-dictive for both relations, e.g., hnationality???????????
ehasCapital??????????
t. These features make the cor-responding relation classification tasks highly re-lated.
Numerous studies have shown that learn-ing multiple related tasks simultaneously (a.k.a.multi-task learning) usually leads to better predic-tive performance, profiting from the relevant infor-mation available in different tasks (Carlson et al,2010; Chapelle et al, 2010).This paper proposes a novel multi-task learningframework that couples the path ranking of multi-ple relations, referred to as coupled PRA (CPRA).The new model needs to answer two critical ques-tions: (i) which relations should be coupled, and(ii) in what manner they should be coupled.As to the first question, it is obvious that not allrelations are suitable to be learned together.
Forinstance, modeling bornIn together with hasWifemight not bring any real benefits, since there arefew common paths between these two relations.CPRA introduces a common-path based similaritymeasure, and accordingly devises an agglomera-tive clustering strategy to group relations.
Only re-lations that are grouped into the same cluster willbe coupled afterwards.As to the second question, CPRA follows thecommon practice of multi-task learning (Evgeniouand Pontil, 2004), and couples relations by usingclassifiers with partially shared parameters.
Givena cluster of relations, CPRA builds the classifier-s upon (i) relation-specific parameters to addressthe specifics of individual relations, and (ii) sharedparameters to model the commonalities among d-ifferent relations.
These two types of parametersare balanced by a coupling coefficient, and learnedjointly for all relations.
In this way CPRA couplesthe classification tasks of multiple relations, andenables implicit data sharing and regularization.The major contributions of this paper are as fol-lows.
(i) We design a novel framework for multi-task learning with PRA, i.e., CPRA.
To the best ofour knowledge, this is the first study on multi-taskPRA.
(ii) We empirically verify the effectivenessof CPRA on a real-world, large-scale KB.
Specifi-cally, we evaluate CPRA on benchmark data creat-ed from Freebase.
Experimental results show thatCPRA can effectively identify coherent clustersin which relations are highly correlated.
By fur-ther coupling such relations, CPRA substantiallyoutperforms PRA, in terms of not only predictiveaccuracy but also model interpretability.
(iii) Wecompare CPRA and PRA to the embedding-basedTransE model (Bordes et al, 2013), and demon-strate their superiority over TransE.
As far as weknow, this is the first work that formally comparesPRA-style approaches to embedding-based ones,on publicly available Freebase data.In the remainder of this paper, we first reviewrelated work in Section 2, and formally introducePRA in Section 3.
We then detail the proposedCPRA framework in Section 4.
Experiments andresults are reported in Section 5, followed by theconclusion and future work in Section 6.2 Related WorkWe first review three lines of related work: (i) KBcompletion, (ii) PRA and its extensions, and (iii)multi-task learning, and then discuss the connec-tion between CPRA and previous approaches.KB completion.
This task is to automaticallyinfer missing facts from existing ones.
Prior workroughly falls into three categories: (i) path rankingalgorithms (PRA) which use paths that connect t-wo entities to predict potential relations betweenthem (Lao et al, 2011; Lao and Cohen, 2010); (i-i) embedding-based models which embed entitiesand relations into a latent vector space and makeinferences in that space (Nickel et al, 2011; Bor-des et al, 2013); (iii) probabilistic graphical mod-els such as the Markov logic network (MLN) andits variants (Pujara et al, 2013; Jiang et al, 2012).This paper focuses on PRA, since it is easily inter-pretable (as opposed to embedding-based models)and requires no external logic rules (as opposed toMLN and its variants).PRA and its extensions.
PRA is a randomwalkinference technique designed for predicting newrelation instances in KBs, first proposed by Laoand Cohen (2010).
Recently various extension-s have been explored, ranging from incorporatinga text corpus as additional evidence during infer-ence (Gardner et al, 2013; Gardner et al, 2014),to introducing better schemes to generate morepredictive paths (Gardner and Mitchell, 2015; Shiand Weninger, 2015), or using PRA in a broadercontext such as Google?s Knowledge Vault (Donget al, 2014).
All these approaches are based onsome single-task version of PRA, while our workexplores multi-task learning for it.Multi-task learning.
Numerous studies haveshown that learning multiple related tasks simulta-1309neously can provide significant benefits relative tolearning them independently (Caruana, 1997).
Akey ingredient of multi-task learning is to modelthe notion of task relatedness, through either pa-rameter sharing (Evgeniou and Pontil, 2004; An-do and Zhang, 2005) or feature sharing (Argyri-ou et al, 2007; He et al, 2014).
In recent years,there has been increasing work showing the ben-efits of multi-task learning in NLP-related tasks,such as relation extraction (Jiang, 2009; Carlson etal., 2010) and machine translation (Sennrich et al,2013; Cui et al, 2013; Dong et al, 2015).
This pa-per investigates the possibility of multi-task learn-ing with PRA, in a parameter sharing manner.Connection with previous methods.
Actually,modeling multiple relations collectively is a com-mon practice in embedding-based approaches.
Insuch a method, embeddings are learned jointly forall relations, over a set of shared latent features(entity embeddings), and hence can capture mean-ingful associations among different relations.
Asshown by (Toutanova and Chen, 2015), observedfeatures such as PRA paths usually perform bet-ter than latent features for KB completion.
In thiscontext, CPRA is designed in a way that gets themulti-relational benefit of embedding techniqueswhile keeping PRA-style path features.
Nickel etal.
(2014) and Neelakantan et al (2015) have triedsimilar ideas.
However, their work focuses on im-proving embedding techniques with observed fea-tures, while our approach aims at improving PRAwith multi-task learning.3 Path Ranking AlgorithmPRA was first proposed by Lao and Cohen (2010),and later slightly modified in various ways (Gard-ner et al, 2014; Gardner and Mitchell, 2015).
Thekey idea of PRA is to explicitly use paths that con-nect two entities as features to predict potential re-lations between them.
Here a path is a sequenceof relations ?r1, r2, ?
?
?
, r??
that link two entities.For example, ?bornIn, capitalOf?
is a path link-ing SophieMarceau to France, through an inter-mediate node Paris.
Such paths are then usedas features to predict the presence of specific re-lations, e.g., nationality.
A typical PRA modelconsists of three steps: feature extraction, featurecomputation, and relation-specific classification.Feature extraction.
The first step is to generateand select path features that are potentially usefulfor predicting new relation instances.
To this end,PRA first encodes a KB as a multi-relation graph.Given a pair of entities (h, t), PRA then finds thepaths by performing random walks over the graph,recording those starting from h and ending at twith bounded lengths.
More exhaustive strategieslike breadth-first (Gardner and Mitchell, 2015) ordepth-first (Shi and Weninger, 2015) search couldalso be used to enumerate the paths.
After that aset of paths are selected as features, according tosome precision-recall measure (Lao et al, 2011),or simply frequency (Gardner et al, 2014).Feature computation.
Once path features areselected, the next step is to compute their values.Given an entity pair (h, t) and a path ?, PRA com-putes the feature value as a random walk proba-bility p(t|h, ?
), i.e., the probability of arriving at tgiven a randomwalk starting from h and followingexactly all relations in ?.
Computing these ran-dom walk probabilities could be at great expense.Gardner and Mitchell (2015) recently showed thatsuch probabilities offer no discernible benefits.
Sothey just used a binary value to indicate the pres-ence or absence of each path.
Similarly, Shi andWeninger (2015) used the frequency of a path as it-s feature value.
Besides paths, other features suchas path bigrams and vector space similarities couldalso be incorporated (Gardner et al, 2014).Relation-specific classification.
The last stepof PRA is to train an individual classifier for eachrelation, so as to judge whether two entities shouldbe linked by that relation.
Given a relation and aset of training instances (i.e., pairs of entities thatare linked by the relation or not, with features s-elected and computed as above), one can use anykind of classifier to train a model.
Most previouswork simply chooses logistic regression.4 Coupled Path Ranking AlgorithmAs we can see, PRA (as well as its variants) fol-lows a single-task learning paradigm, which buildsa classifier for each relation independently with itsown training data.
We argue that such a single-taskstrategy might not be optimal for KB completion:(i) by learning the classifiers independently, it fail-s to discover and leverage meaningful associationsamong different relations; (ii) it might not perfor-m well on less frequent relations for which only afew training instances are available.
This sectionpresents coupled PRA (CPRA), a novel multi-tasklearning framework that couples the path rankingof multiple relations.
Through a multi-task strat-1310egy, CPRA takes into account relation associationand enables implicit data sharing among them.4.1 Problem FormulationSuppose we are given a KB containing a collectionof triplesO = {(h, r, t)}.
Each triple is composedof two entities h, t ?
E and their relation r ?
R,where E is the entity set and R the relation set.The KB is then encoded as a graph G, with entitiesrepresented as nodes, and triple (h, r, t) a directededge from node h to node t. We formally defineKB completion as a binary classification problem.That is, given a particular relation r, for any entitypair (h, t) such that (h, r, t) /?
O, we would liketo judge whether h and t should be linked by r, byexploiting the graph structure of G. Let R ?
Rdenote a set of relations to be predicted.Each relation r ?
R is associated with a set oftraining instances.
Here a training instance is anentity pair (h, t), with a positive label if (h, r, t) ?O or a negative label otherwise.1For each of theentity pairs, path features could be extracted andcomputed using techniques described in Section 3.We denote by?rthe set of path features extractedfor relation r, and define its training set as Tr={(xir, yir)}.
Here xiris the feature vector for anentity pair, with each dimension corresponding toa path ?
?
?r, and yir= ?1 is the label.
Notethat our primary goal is to verify the possibilityof multi-task learning with PRA.
It is beyond thescope of this paper to further explore better featureextraction or computation.Given the relations and their training instances,CPRA performs KB completion using a multi-tasklearning strategy.
It consists of two components:relation clustering and relation coupling.
The for-mer automatically discovers highly correlated re-lations, and the latter further couples the learningof these relations, described in detail as follows.4.2 Relation ClusteringIt is obvious that not all relations are suitable tobe coupled.
We propose an agglomerative cluster-ing algorithm to automatically discover relationsthat are highly correlated and should be learned to-gether.
Our intuition is that relations sharing morecommon paths (features) are probably more simi-lar in classification, and hence should be coupled.Specifically, we start with |R| clusters and eachcluster contains a single relation r ?
R. Here |?| is1Wewill introduce the details of generating negative train-ing instances in Section 5.1.the cardinality of a set.
Then we iteratively mergethe most similar clusters, say Cmand Cn, into anew cluster C. The similarity between two clustersis defined as:Sim(Ci, Cj) =|?Ci?
?Cj|min(|?Ci|, |?Cj|), (1)where ?Ciis the feature set associated with clus-ter Ci(if Cicontains a single relation, ?Cithe fea-ture set associated with that relation).
It essential-ly measures the overlap between two feature sets.The larger the overlap is, the higher the similari-ty will be.
Once two clusters are merged, we up-date the feature set associated with the new cluster:?C= ?Cm??Cn.
The algorithm stops when thehighest cluster similarity is below some predefinedthreshold ?.
This paper empirically sets ?
= 0.5.As such, relations sharing a substantial number ofcommon paths are grouped into the same cluster.4.3 Relation CouplingAfter clustering, the next step of CPRA is to cou-ple the path ranking of different relations withineach cluster, i.e., to learn the classification tasksfor these relations simultaneously.
We employ amulti-task classification algorithm similar to (Ev-geniou and Pontil, 2004), and learn the classifiersjointly in a parameter sharing manner.Consider a cluster containing K relations C ={r1, r2, ?
?
?
, rK}.
Recall that during the clusteringphase a shared feature set has been generated forthat cluster, i.e., ?C= ?r1?
?
?
?
??rK.
We firstreform the training instances for the K relation-s using this shared feature set, so that all trainingdata is represented in the same space.2We denoteby Tk= {(xik, yik)}Nki=1the reformed training da-ta associated with the k-th relation.
Then our goalis to jointly learnK classifiers f1, f2, ?
?
?
, fKsuchthat fk(xik) ?
yik.We first assume that the classifier for each rela-tion has a linear form fk(x)=wk?
x+ bk, wherewk?
Rdis the weight vector and bkthe bias.
Tomodel associations among different relations, wefurther assume that all wkand bkcan be written,for every k ?
{1, ?
?
?
,K}, as:wk= w0+ vkand bk= b0.
(2)Here the sharedw0is used to model the common-alities among different relations, and the relation-specific vkto address the specifics of individual2Note that ?rk?
?C.
We just assign zero values tofeatures that are contained in ?Cbut not in ?rk.1311relations.
If the relations are closely related (vk?0), they will have similar weights (wt?
w0) onthe common paths.
We use the same bias b0for allthe relations.3We estimate vk,w0, and b0simultaneously in ajoint optimization problem, defined as follows.Problem 1 CPRA amounts to solving the generaloptimization problem:min{vk},w0,b0K?k=1Nk?i=1?
(xik, yik) +?1KK?k=1?vk?22+ ?2?w0?22,where ?
(xik, yik) is the loss on a training instance.It can be instantiated into a logistic regression (L-R) or support vector machine (SVM) version, byrespectively defining the loss ?
(xik, yik) as:?
(xik, yik) = log (1 + exp (?yikfk(xik))) ,?
(xik, yik) = [1?
yikfk(xik)]+,where fk(xik) = (w0+ vk) ?
xik+ b0.
We call themCPRA-LR and CPRA-SVM respectively.In this problem, ?1and ?2are regularization pa-rameters.
By adjusting their values, we control thedegree of parameter sharing among different rela-tions.
The larger the ratio?1?2is, the more we be-lieve that all wtshould conform to the commonmodel w0, and the smaller the relation-specificweight vtwill be.The multi-task learning problem can be directlylinked to a standard single-task learning one, builton all training data from different relations.Proposition 1 Suppose the training data associ-ated with the k-th relation, for every k=1, ?
?
?
,K,is transformed into:?xik= [xik?
?K,0, ?
?
?
,0?
??
?k?1,xik,0, ?
?
?
,0?
??
?K?k],where 0 ?
Rdis a vector whose coordinates are allzero, and ?
=?2?1a coupling coefficient.
Consider alinear classifier for the transformed data?f(?x) =?w ?
?x+?b, with?w and?b constructed as:?w = [?
?Kw0,v1, ?
?
?
,vK] and?b = b0.Then the objective function of Problem 1 is equiv-alent to:L =K?k=1Nk?i=1??
(?xik, yik) +??
?
?w?22,3It implicitly assumes that all the relations have the sameproportion of positive instances.
This assumption actuallyholds since given any relation we can always generate thesame number of negative instances for each positive one.
Weset this number to 4 in our experiments.where?
?= log(1 + exp(?yik?f(?xik))) is a logistic lossfor CPRA-LR, and?
?=[1 ?
yik?f(?xik)]+a hinge lossfor CPRA-SVM; and??
=?1K.That means, after transforming data from differ-ent relations into a unified representation, Prob-lem 1 is equivalent to a standard single-task learn-ing problem, built on the transformed data from allthe relations.
So it can easily be solved by existingtools such as LR or SVM.5 ExperimentsIn this section we present empirical evaluation ofCPRA in the KB completion task.5.1 Experimental SetupsWe create our data on the basis of FB15K (Bor-des et al, 2011)4, a relatively dense subgraph ofFreebase containing 1,345 relations and the corre-sponding triples.KB graph construction.
We notice that in mostcases FB15K encodes a relation and its reverse re-lation at the same time.
That is, once a new fact isobserved, FB15K creates two triples for it, e.g., (x,film/edited-by, y) and (y, editor/film, x).
Re-verse relations provide no additional knowledge.They may even hurt the performance of PRA-stylemethods.
Actually, to enhance graph connectivity,PRA-style methods usually automatically add aninverse version for each relation in a KB (Lao andCohen, 2010; Lao et al, 2011).
That is, for eachobserved triple (h, r, t), another triple (t, r?1, h)is constructed and added to the KB.
Consider theprediction of a relation, say film/edited-by.
Inthe training phase, we could probably find that ev-ery two entities connected by this relation are alsoconnected by the path editor/film?1, and henceassign an extremely high weight to it.5However,in the testing phase, for any entity pair (x, y) suchthat (y, editor/film, x) has not been encoded, wemight not even find that path and hence could al-ways make a negative prediction.6For this reason, we remove reverse relations inFB15K.
Specifically, we regard r2to be a reverserelation of r1if the triple (t, r2, h) holds whenev-er (h, r1, t) is observed, and we randomly discard4https://everest.hds.utc.fr/doku.php?id=en:smemlj125For every observed triple (x, film/edited-by, y),FB15K also encodes (y, editor/film, x), for which (x,editor/film?1, y) is further constructed.6Note that such test cases are generally more meaningful:if we already know (y, editor/film, x), predicting (x,film/edited-by, y) could be trivial.1312one of the two relations.7As such, we keep 774out of 1,345 relations in FB15K, covering 14,951entities and 327,783 triples.
Then we build a graphbased on this data and use it as input to CPRA (andour baseline methods).Labeled instance generation.
We select 171relations to test our methods.
To do so, we pick10 popular domains, including award, education,film, government, location, music, olympics, or-ganization, people, and tv.
Relations in these do-mains with at least 50 triples observed for them areselected.
For each of the 171 relations, we split theassociated triples into roughly 80% training, 10%validation, and 10% testing.
Since the triple num-ber varies significantly among the relations, we al-low at most 200 validation/testing triples for eachrelation, so as to make the test cases as balancedas possible.
Note that validation and testing triplesare not used for constructing the graph.We generate positive instances for each relationdirectly from these triples.
Given a relation r and atriple (h, r, t) observed for it (training, validation,or testing), we take the pair of entities (h, t) as apositive instance for that relation.
Then we follow(Shi and Weninger, 2015; Krompa?
et al, 2015) togenerate negative instances.
Given each positiveinstance (h, t)we generate four negative ones, twoby randomly corrupting the head h, and the othertwo the tail t. To make the negative instances as d-ifficult as possible, we corrupt a position using on-ly entities that have appeared in that position.
Thatmeans, given the relation capitalOf and the pos-itive instance (Paris, France), we could generatea negative instance (Paris, UK) but never (Paris,NBA), since NBA never appears as a tail entity ofthe relation.
We further ensure that the negativeinstances do not overlap with the positive ones.Feature extraction and computation.
Giventhe labeled instances, we extract path features forthem using the code provided by Shi andWeninger(2015)8.
It is a depth-first search strategy that enu-merates all paths between two entities.
We set themaximum path length to be ?
= 3.
There are about8.2% of the labeled instances for which no pathcould be extracted.
We remove such cases, givingon average about 5,250 training, 323 validation,and 331 testing instances per relation.
Then we re-move paths that appear only once in each relation,getting 5,515 features on average per relation.
We7We still add an inverse version for the relation kept dur-ing path extraction.8https://github.com/nddsg/KGMiner# Relations 774# Entities 14,951# Triples 327,783# Relations tested 171# Avg.
training instances/relation 5,250# Avg.
validation instances/relation 323# Avg.
testing instances/relation 331# Avg.
features/relation 5,515Table 1: Statistics of the data.simply compute the value of each feature as its fre-quency in an instance.
Table 1 lists the statistics ofthe data used in our experiments.Evaluation metrics.
As evaluation metrics, weuse mean average precision (MAP) and mean re-ciprocal rank (MRR), following recent work eval-uating KB completion performance (West et al,2014; Gardner and Mitchell, 2015).
Both metricsevaluate some ranking process: if a method ranksthe positive instances before the negative ones foreach relation, it will get a high MAP or MRR.Baseline methods.
We compare CPRA to tra-ditional single-task PRA.
CPRA first groups the171 relations into clusters, and then learns classi-fiers jointly for relations within the same cluster.We implement two versions of it: CPRA-LR andCPRA-SVM.
As we have shown in Proposition 1,both of them could be solved by standard classi-fication tools.
PRA learns an individual classifierfor each of the relations, using LR or SVM classi-fication techniques, denoted by PRA-LR or PRA-SVM.
We use LIBLINEAR (Fan et al, 2008)9tosolve the LR and SVM classification problems.For all these methods, we tune the cost c in therange of {2?5, 2?4, ?
?
?
, 24, 25}.
And we set thecoupling coefficient ?
=?2?1in CPRA in the rangeof {0.1, 0.2, 0.5, 1, 2, 5, 10}.We further compare CPRA to TransE, a widelyadopted embedding-based method (Bordes et al,2013).
TransE learns vector representations forentities and relations (i.e., embeddings), and usesthe learned embeddings to determine the plausibil-ity of missing facts.
Such plausibility can then beused to rank the labeled instances.
We implementTransE using the code provided by Bordes et al(2013)10.
To learn embeddings, we take as inputthe triples used to construct the graph (from whichCPRA and PRA extract their paths).
We tune theembedding dimension in {20, 50, 100}, the mar-gin in {0.1, 0.2, 0.5, 1, 2, 5}, and the learning rate9http://www.csie.ntu.edu.tw/ cjlin/liblinear10https://github.com/glorotxa/SME1313film/casting-director gov-jurisdiction/dist-representfilm/cinematography location/containfilm/costume-design-by location/adjoinfilm/art-direction-by us-county/county-seatfilm/crewmember county-place/countyfilm/set-decoration-by location/partially-containfilm/production-design-by region/place-exportfilm/edited-byfilm/written-byfilm/story-byorg/place-founded country/divisionsorg/headquarter-city country/capitalorg/headquarter-state country/fst-level-divisionsorg/geographic-scope country/snd-level-divisionsorg/headquarter-country admin-division/capitalorg/service-locationtv/tv-producer music-group-member/instrumenttv/recurring-writer music-artist/recording-roletv/program-creator music-artist/track-roletv/regular-appear-person music-group-member/roletv/tv-actorTable 2: Six largest clusters of relations (with thestopping criterion ?
= 0.5).in {10?4, 10?3, 10?2, 10?1, 1}.
For details pleaserefer to (Bordes et al, 2013).
For each of thesemethods, we select the optimal configuration thatleads to the highest MAP on the validation set andreport its performance on the test set.5.2 Relation Clustering ResultsWe first test the effectiveness of our agglomerativestrategy (Section 4.2) in relation clustering.
Withthe stopping criterion ?
= 0.5, 96 out of the 171relations are grouped into clusters which contain atleast two relations.
Each of these 96 relations willlater be learned jointly with some other relations.The other 75 relations cannot be merged, and willstill be learned individually.
Table 2 shows the sixlargest clusters discovered by our algorithm.
Rela-tions in each cluster are arranged in the order theywere merged.
The results indicate that our algo-rithm can effectively identify coherent clusters inwhich relations are highly correlated to each other.For example, the top left cluster describes relationsbetween a film and its crew members, and the mid-dle left between an organization and a location.During clustering we might obtain clusters thatcontain too many relations and hence too manytraining instances for our CPRA model to learn ef-ficiently.
We split such clusters into sub-clusters,either according to the domain (e.g., the film clus-ter and tv cluster) or randomly (e.g., the two loca-tion clusters on the top right).5.3 KB Completion ResultsWe further test the effectiveness of our multi-tasklearning strategy (Section 4.3) in KB completion.Table 3 gives the results on the 96 relations that areactually involved in multi-tasking learning (i.e.,grouped into clusters with size larger than one).11The 96 relations are grouped into 29 clusters, andrelations within the same cluster are learned joint-ly.
Table 3 reports (i) MAP and MRR within eachcluster and (ii) overall MAP and MRR on the 96relations.
Numbers marked in bold type indicatethat CPRA-LR/SVM outperforms PRA-LR/SVM,within a cluster (with its ID listed in the first col-umn) or on all the 96 relations (ALL).
We judge s-tatistical significance of the overall improvementsachieved by CPRA-LR/SVM over PRA-LR/SVMand TransE, using a paired t-test.
The average pre-cision (or reciprocal rank) on each relation is usedas paired data.
The symbol ????
indicates a signif-icance level of p < 0.0001, and ???
a significancelevel of p < 0.05.From the results, we can see that (i) CPRAoutperforms PRA (using either LR or SVM) andTransE on the 96 relations (ALL) in both metrics.All the improvements are statistically significant,with a significance level of p < 0.0001 for MAPand a significance level of p < 0.05 for MRR.
(i-i) CPRA-LR/SVM outperforms PRA-LR/SVM in22/24 out of the 29 clusters in terms of MAP.
Mostof the improvements are quite substantial.
(iii) Im-proving PRA-LR and PRA-SVM in terms of MRRcould be hard, since they already get the best per-formance (MRR = 1) in 19 out of the 29 clusters.But even so, CPRA-LR/SVM still improves 7/8out of the remaining 10 clusters.
(iv) The PRA-style methods perform substantially better than theembedding-based TransE model in most of the 29clusters and on all the 96 relations.
This observa-tion demonstrates the superiority of observed fea-tures (i.e., PRA paths) over latent features.Table 4 further shows the top 5 most discrimina-tive paths (i.e., features with the highest weights)discovered by PRA-SVM (left) and CPRA-SVM(right) for each relation in the 6th cluster.12Theaverage precision on each relation is also provid-11The other 75 relations are still learned individually.
SoCPRA and PRA perform the same on these relations.
TheMAP values on these 75 relations are 0.6360, 0.6558, 0.6543for TransE, PRA-LR, and PRA-SVM respectively, and theMRR values are 0.9049, 0.9033, and 0.9013 respectively.12This is one of the largest clusters on which CPRA-SVMimproves PRA-SVM substantially.1314MAP MRRTransE PRA-LR CPRA-LR PRA-SVM CPRA-SVM TransE PRA-LR CPRA-LR PRA-SVM CPRA-SVM1 0.5419 0.5160 0.5408 0.4687 0.5204 0.7500 0.8333 1.0000 0.7778 0.83332 0.7480 0.7888 0.7807 0.8010 0.8092 1.0000 1.0000 1.0000 1.0000 1.00003 0.4624 0.4625 0.4788 0.4634 0.4560 0.8333 1.0000 1.0000 1.0000 0.83334 0.5495 0.5378 0.5423 0.5385 0.5460 0.7667 0.6400 0.7000 0.7167 0.70005 0.5164 0.5789 0.6030 0.5891 0.6072 0.8333 0.6667 1.0000 0.8333 1.00006 0.6918 0.7733 0.7950 0.7369 0.8084 1.0000 0.8333 0.8333 0.8056 0.91677 0.7381 0.7531 0.7754 0.7456 0.7414 0.7500 1.0000 1.0000 1.0000 1.00008 0.4258 0.5180 0.5446 0.3162 0.4606 1.0000 1.0000 1.0000 0.3056 0.75009 0.6353 0.7879 0.7708 0.7680 0.7685 0.7500 1.0000 1.0000 1.0000 1.000010 0.8615 0.7773 0.7738 0.7618 0.7507 1.0000 1.0000 1.0000 1.0000 1.000011 0.4549 0.5814 0.6014 0.5717 0.5896 0.8333 1.0000 1.0000 0.8750 1.000012 0.6202 0.7187 0.7479 0.7455 0.7457 0.7500 0.5833 1.0000 1.0000 1.000013 0.5530 0.6681 0.6716 0.6373 0.6502 0.6667 1.0000 1.0000 1.0000 1.000014 0.5082 0.4360 0.5280 0.4715 0.5806 0.3750 0.6667 0.6250 1.0000 1.000015 0.9881 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.000016 0.5324 0.6818 0.6863 0.6522 0.6705 0.8750 1.0000 1.0000 1.0000 1.000017 0.3759 0.3351 0.3593 0.3273 0.3219 0.6111 0.5667 0.7778 0.5111 0.666718 0.9423 0.9968 1.0000 0.9947 0.9975 1.0000 1.0000 1.0000 1.0000 1.000019 0.7903 0.8376 0.8310 0.8296 0.8328 0.8714 0.9286 0.8571 0.8571 0.857120 0.7920 0.8285 0.8746 0.8491 0.8754 1.0000 1.0000 1.0000 1.0000 1.000021 0.4885 0.5869 0.5799 0.5554 0.5952 0.6250 1.0000 1.0000 1.0000 1.000022 0.7894 0.8371 0.8486 0.8371 0.8374 1.0000 1.0000 1.0000 1.0000 1.000023 0.7123 0.7848 0.8191 0.7811 0.7957 0.9500 1.0000 1.0000 1.0000 1.000024 0.5982 0.7923 0.8048 0.8204 0.8220 1.0000 1.0000 1.0000 1.0000 1.000025 0.6223 0.8723 0.8723 0.7785 0.8109 0.7500 1.0000 1.0000 1.0000 1.000026 0.5253 0.5377 0.5685 0.5337 0.5447 0.8750 0.8125 0.8750 0.7083 0.833327 0.8763 0.6890 0.8124 0.7014 0.8016 1.0000 0.6667 1.0000 0.7500 1.000028 0.7588 0.8131 0.8154 0.8130 0.8146 1.0000 1.0000 1.0000 1.0000 1.000029 0.4894 0.5921 0.6543 0.6093 0.6566 0.7500 1.0000 1.0000 1.0000 1.0000ALL 0.6540 0.7058 0.7254?
?0.6943 0.7162?
?0.8682 0.9061 0.9436?0.8982 0.9358?Table 3: KB completion results on the 96 relations that have been grouped into clusters with size largerthan one (with the stopping criterion ?
= 0.5), and hence involved in multi-tasking learning.ed.
We can observe that (i) CPRA generally dis-covers more predictive paths than PRA.
Almost allthe top paths discovered by CPRA are easily inter-pretable and provide sensible reasons for the finalprediction, while some of the top paths discoveredby PRA are hard to interpret and less predictive.Take org/place-founded as an example.
All the5 CPRA paths are useful to predict the place wherean organization was founded, e.g., the 3rd one tellsthat ?the organization headquarter in a city whichis located in that place?.
However, the PRA path?common/class?
common/class?1?
film/debut-venue?
is hard to interpret and less predictive.
(ii)For the 1st/4th/6th relation on which PRA gets alow average precision, CPRA learns almost com-pletely different top paths and gets a substantiallyhigher average precision.
While for the other re-lations (2nd/3rd/5th) on which PRA already per-forms well enough, CPRA learns similar top pathsand gets a comparable average precision.
We haveconducted the same analyses with CPRA-LR andPRA-LR, and observed similar phenomena.
Allthese observations demonstrate the superiority ofCPRA, in terms of not only predictive accuracybut also model interpretability.6 ConclusionIn this paper we have studied the path ranking al-gorithm (PRA) from the viewpoint of multi-tasklearning.
We have designed a novel multi-tasklearning framework for PRA, called coupled PRA(CPRA).
The key idea of CPRA is to (i) automat-ically discover relations highly correlated to eachother through agglomerative clustering, and (ii) ef-fectively couple the prediction of such relationsthrough multi-task learning.
By coupling differentrelations, CPRA takes into account relation asso-ciations and enables implicit data sharing amongthem.
We have tested CPRA on benchmark datacreated from Freebase.
Experimental results showthat CPRA can effectively identify coherent clus-ters in which relations are highly correlated.
Byfurther coupling such relations, CPRA significant-ly outperforms PRA, in terms of both predictive1315org/place-founded (0.4920 vs. 0.6750)org/headquarter-city location/contain?1common/class?common/class?1?film/debut-venue org/headquarter-citycommon/class?common/class?1?sports-team/location org/headquarter-city?location/contain?1employer/job-title?employer/job-title?1?location/contain org/headquarter-state?location/containmusic-artist/label?1?person/place-of-birth org/headquarter-city?bibs-location/stateorg/headquarter-city (0.9014 vs. 0.9141)location/contain?1location/contain?1org/place-founded org/headquarter-state?location/containorg/headquarter-state?location/contain org/place-foundedorg/child?1?org/child?org/place-founded org/child?1?org/child?org/place-foundedsports-team/location industry/company?1?industry/company?org/place-foundedorg/headquarter-state (0.9522 vs. 0.9558)location/contain?1location/contain?1org/headquarter-city?location/contain?1org/headquarter-city?location/contain?1org/headquarter-city?bibs-location/state org/headquarter-city?bibs-location/stateorg/headquarter-city?county-place/county?location/contain?1org/headquarter-cityorg/headquarter-city?location/contain?1?location/contain?1org/place-foundedorg/geographic-scope (0.5252 vs. 0.6075)common/class?common/class?1?location/vacationer?1location/contain?1common/class?common/class?1?country/languages?1org/headquarter-city?location/contain?1common/class?common/class?1?gov-jurisdiction/gov-body?1location/contain?1?location/contain?1common/class?common/class?1?region/currency-of-gdp?1org/place-founded?location/contain?1politician/party?1?person/nationality?location/adjoins org/headquarter-city?location/contain?1?location/contain?1org/headquarter-country (0.9859 vs. 0.9938)org/headquarter-city?airline/city-served?1?org/service-location location/contain?1org/headquarter-city?admin-area/child?1?region/place-export org/headquarter-city?location/contain?1org/headquarter-city?country/divisions?1?region/place-export org/headquarter-city?county-place/county?location/contain?1org/headquarter-city?film/feat-location?1?film/feat-location location/contain?1?location/contain?1org/headquarter-city?gov-jurisdiction/title?employer/job-title?1org/place-founded?location/contain?1org/service-location (0.5644 vs. 0.7044)org/headquarter-city?country/divisions?1org/headquarter-city?location/contain?1org-extra/service-location org/headquarter-city?county-place/county?location/contain?1film/production-company?1?film/subjects?admin-area/child?1location/contain?1?location/contain?1org/legal-structure?entry/taxonomy?entry/taxonomy?1org/place-founded?location/contain?1airline/city-served?region/currency?region/currency-of-gdp?1org-extra/service-locationTable 4: Top paths given by PRA-SVM (left) and CPRA-SVM (right) for each relation in the 6th cluster.accuracy and model interpretability.This is the first work that investigates the pos-sibility of multi-task learning with PRA, and wejust provide a very simple solution.
There are stillmany interesting topics to study.
For instance, theagglomerative clustering strategy can only identi-fy highly correlated relations, i.e., those sharinga lot of common paths.
Relations that are onlyloosely correlated, e.g., those sharing no commonpaths but a lot of sub-paths, will not be identified.We would like to design new mechanisms to dis-cover loosely correlated relations, and investigatewhether coupling such relations still provides ben-efits.
Another example is that the current methodis a two-step approach, performing relation clus-tering first and then relation coupling.
It will be in-teresting to study whether one can merge the clus-tering step and the coupling step so as to have aricher inter-task dependent structure.
We will in-vestigate such topics in our future work.AcknowledgmentsWe would like to thank Baoxu Shi for providingthe code for path extraction.
We would also like tothank the anonymous reviewers for their valuablecomments and suggestions.
This work is support-ed by the National Natural Science Foundation ofChina (grant No.
61402465), the Strategic Priori-ty Research Program of the Chinese Academy ofSciences (grant No.
XDA06030200), and the Mi-crosoft Research Asia StarTrack Program.
Thiswork was done when Quan Wang was a visitingresearcher at Microsoft Research Asia.1316ReferencesRie Kubota Ando and Tong Zhang.
2005.
A frame-work for learning predictive structures from multi-ple tasks and unlabeled data.
Journal of MachineLearning Reseach, 6:1817?1853.Andreas Argyriou, Theodoros Evgeniou, and Massim-iliano Pontil.
2007.
Multi-task feature learning.
InAdvances in Neural Information Processing System-s, pages 41?48.Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim S-turge, and Jamie Taylor.
2008.
Freebase: A collab-oratively created graph database for structuring hu-man knowledge.
In Proceedings of the 2008 ACMSIGMOD International Conference on Managementof Data, pages 1247?1250.Antoine Bordes, Jason Weston, Ronan Collobert, andYoshua Bengio.
2011.
Learning structured em-beddings of knowledge bases.
In Proceedings ofthe 25th AAAI Conference on Artificial Intelligence,pages 301?306.Antoine Bordes, Nicolas Usunier, Alberto Garcia-Dur?an, Jason Weston, and Oksana Yakhnenko.2013.
Translating embeddings for modeling multi-relational data.
In Advances in Neural InformationProcessing Systems, pages 2787?2795.Andrew Carlson, Justin Betteridge, Bryan Kisiel, Bur-r Settles, Estevam R. Hruschka Jr, and Tom M.Mitchell.
2010.
Toward an architecture for never-ending language learning.
In Proceedings of the24th AAAI Conference on Artificial Intelligence,pages 1306?1313.Rich Caruana.
1997.
Multitask learning.
MachineLearning, 28(1):41?75.Olivier Chapelle, Pannagadatta Shivaswamy, SrinivasVadrevu, Kilian Weinberger, Ya Zhang, and BelleTseng.
2010.
Multi-task learning for boosting withapplication to web search ranking.
In Proceedingsof the 16th ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining, pages1189?1198.Eunsol Choi, Tom Kwiatkowski, and Luke Zettlemoy-er.
2015.
Scalable semantic parsing with partial on-tologies.
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Linguisticsand the 7th International Joint Conference on Natu-ral Language Processing, pages 1311?1320.Silviu Cucerzan.
2007.
Large-scale named entitydisambiguation based on Wikipedia data.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 708?716.Lei Cui, Xilun Chen, Dongdong Zhang, Shujie Liu,Mu Li, and Ming Zhou.
2013.
Multi-domain adap-tation for SMT using multi-task learning.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1055?1065.Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, WilkoHorn, Ni Lao, Kevin Murphy, Thomas Strohman-n, Shaohua Sun, and Wei Zhang.
2014.
Knowl-edge vault: A web-scale approach to probabilisticknowledge fusion.
In Proceedings of the 20th ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, pages 601?610.Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, andHaifeng Wang.
2015.
Multi-task learning for mul-tiple language translation.
In Proceedings of the53rd Annual Meeting of the Association for Compu-tational Linguistics and the 7th International JointConference on Natural Language Processing, pages1723?1732.Theodoros Evgeniou and Massimiliano Pontil.
2004.Regularized multi-task learning.
In Proceedings ofthe 10th ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, pages 109?117.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Matt Gardner and Tom Mitchell.
2015.
Efficient andexpressive knowledge base completion using sub-graph feature extraction.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing, pages 1488?1498.Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,and Tom Mitchell.
2013.
Improving learning andinference in a large knowledge-base using laten-t syntactic cues.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 833?838.Matt Gardner, Partha Talukdar, Jayant Krishnamurthy,and TomMitchell.
2014.
Incorporating vector spacesimilarity in random walk inference over knowledgebases.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Process-ing, pages 397?406.Shu Guo, Quan Wang, Lihong Wang, Bin Wang, andLi Guo.
2015.
Semantically smooth knowledgegraph embedding.
In Proceedings of the 53rd An-nual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Confer-ence on Natural Language Processing, pages 84?94.Jingrui He, Yan Liu, and Qiang Yang.
2014.
Linkingheterogeneous input spaces with pivots for multi-task learning.
In Proceedings of the 2014 SIAM In-ternational Conference on Data Mining, pages 181?189.1317Shangpu Jiang, Daniel Lowd, and Dejing Dou.
2012.Learning to refine an automatically extracted knowl-edge base using markov logic.
In Proceedings of the2012 IEEE International Conference on Data Min-ing, pages 912?917.Jing Jiang.
2009.
Multi-task transfer learning forweakly-supervised relation extraction.
In Proceed-ings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP, pages 1012?1020.Denis Krompa?, Stephan Baier, and Volker Tresp.2015.
Type-constrained representation learning inknowledge graphs.
In Proceedings of the 13th Inter-national Semantic Web Conference, pages 640?655.Ni Lao and William W. Cohen.
2010.
Relational re-trieval using a combination of path-constrained ran-dom walks.
Machine Learning, 81(1):53?67.Ni Lao, Tom Mitchell, and William W. Cohen.
2011.Random walk inference and learning in a large scaleknowledge base.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 529?539.Jens Lehmann, Robert Isele, Max Jakob, AnjaJentzsch, Dimitris Kontokostas, Pablo N. Mendes,Sebastian Hellmann, Mohamed Morsey, Patrick vanKleef, S?oren Auer, et al 2014.
Dbpedia: A large-scale, multilingual knowledge base extracted fromwikipedia.
Semantic Web Journal.Arvind Neelakantan, Benjamin Roth, and Andrew M-cCallum.
2015.
Compositional vector space mod-els for knowledge base completion.
In Proceedingsof the 53rd Annual Meeting of the Association forComputational Linguistics and the 7th InternationalJoint Conference on Natural Language Processing,pages 156?166.Maximilian Nickel, Volker Tresp, and Hans-PeterKriegel.
2011.
A three-way model for collectivelearning on multi-relational data.
In Proceedingsof the 28th International Conference on MachineLearning, pages 809?816.Maximilian Nickel, Xueyan Jiang, and Volker Tresp.2014.
Reducing the rank in relational factorizationmodels by including observable patterns.
In Ad-vances in Neural Information Processing Systems,pages 1179?1187.Jay Pujara, Hui Miao, Lise Getoor, andWilliam Cohen.2013.
Knowledge graph identification.
In Proceed-ings of the 11th International Semantic Web Confer-ence, pages 542?557.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine Learning, 62(1-2):107?136.Michael Schuhmacher and Simone Paolo Ponzetto.2014.
Knowledge-based graph document modeling.In Proceedings of the 7th ACM International Con-ference onWeb Search and Data Mining, pages 543?552.Rico Sennrich, Holger Schwenk, and Walid Aransa.2013.
A multi-domain translation model frameworkfor statistical machine translation.
In Proceedings ofthe 51st Annual Meeting of the Association for Com-putational Linguistics, pages 832?840.Baoxu Shi and Tim Weninger.
2015.
Fact checkingin large knowledge graphs: A discriminative predictpath mining approach.
In arXiv:1510.05911.Kristina Toutanova and Danqi Chen.
2015.
Observedversus latent features for knowledge base and tex-t inference.
In Proceedings of the 3rd Workshop onContinuous Vector Space Models and Their Compo-sitionality.Robert West, Evgeniy Gabrilovich, Kevin Murphy,Shaohua Sun, Rahul Gupta, and Dekang Lin.
2014.Knowledge base completion via search-based ques-tion answering.
In Proceedings of the 23rd Interna-tional Conference on World Wide Web, pages 515?526.1318
