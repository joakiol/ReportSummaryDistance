A Parser That Doesn'tS.
G. Pu lmanUniversity of Cambr idgeComputer  LaboratoryCorn Exchange StreetCambr idge CB2 3QG, UK,Abst ractThis paper describes an implemented parser-interpreter whichis intended as an abstract formal model of part of the process ofsentence comprehension.
It is illustrated here for Phrase StructureGrammars with a translation into a familiar type of logical form,although the general principles are intended to apply to any gram-matical theory sharing certain basic assumptions, which are dis-cussed in the paper.
The procedure allows for incremental seman-tic interpretation as a sentence is parsed, and provides a principledexplanation for some familiar observations concerning properties ofdeeply recursive constructions.BackgroundThe starting point for the present work is a set of familiarand, for the most part, uncontroversial c|~Lm~ s about the nature ofgrammatical description and of human parsing of natural language.These claims and assumptions can be briefly summarised asfollows:A Hierarchical StructureLinguists assign constituent structures to sentences on the ba-sis ~f distributional tests of various kinds.
On the basis of thesetests, the 'correct' structures are always hierarchical and oftendeeply nested.
The tree representing a sentence may impose agreat deal of structure on it, with string-adjacent items often ap-pearing at very different levels in the tree.
In general, shallow,'flat' structures are not generated by grammars, nor warranted ondistributional grounds.
However, as we shall see, it is likely thatthese deeply nested structures may be somewhat remote from anythat are actually computed during parsing.B Semantics is (1) composit ional and (ll) syntax-drlven.Both of these claims can be made in a variety of versionsof different strengths, from the trivially true to the fairly clearlyfalse.
What is intended here is the assumption sometimes calledthe 'rule to rule' hypothesis, hared by almost all current grammat-ical frameworks, that to each syntactic rule of a grammar (or foreach subrree induced by such a rule) there is an associated seman-tic rule, either producing an interpretation directly, or translatinginto some formal anguage.
Interpretations for whole sentences arebuilt up from the constituent parts in ways specified by these rules,in a fashion which mimics and uses the syntactic structure of thesentence.C Incremental interpretationAs a sentence is parsed, its interpretation is built up word byword: there is little or no delay in interpreting it.
In particular,we do not wait until all syntactic constituents have been completedbefore beginning to integrate then into some non-syntactic repre-sentation.
Ample intuitive and experimental evidence supports thisuncontroversial observation.D Limited recurslon.One of the most firmly established facts about human syntac-tic processing is that constructions which are ineliminably deeplyrecursive (such as central self-embeddings) are difficult or impossi-ble to parse.
A sentence like:I The boy who the girl that the dog bit liked ran awayis clumsy at best, and one like:2 The boy the girl the dog the cat scratched bit saw leftis utterly unmanageable under normal circumstances.Under the further assumption, recently more controversial (Katz1981), that grammars have some kind of mental reality as repre-sentations of linguistic knowledge, it is clear that A to D, althoughsimple and generally agreed upon observations, by no means obvi-ously consistent with each other.
Consider, for example, the nat-ural way in which one might set about implementing a systemwhich observed B, a principle which, in itself, is a computation-ally natural principle.
Such a system might first parse a sentence,annotating nodes in the resulting tree with an indication of the syn-tactic rules used.
This annotated tree would then be passed to aninterpretation routine which applied the appropriate semantic op-eration to the topmost node (guided by the syntactic informationfound there, in particular a pointer to the semantic informationnecessary), calling itself recursively on each subtree to build up thecomplete interpretation.
(Systems operating in more or less thismanner are described in Rosenschein and Shieber 1982, Gawron etal.
1982 and Schubert and Pelletier 1982.
They are not intended aspsychological models in any but the most abstract sense, of course.
)Such a system would, in observing B, also naturally be con*sistent with A.
Obviously, though, this type of system requires acomplete syntactic analysis to be available before it can even be-gin the process of interpretation, thus conflicting straightforwardlywith C.Consider next A and D. The structures which linguists postu-late in accordance with A are often recursive, and it is in the natureof hierarchical structures that this should be a possibility.
This israther puzzling in the light of D, for if D is correct, it seems to showthat a class of structures which are natural from one point of view(i.e.
centre embeddings) are extremely unnatural from another.It is not necessarily to be expected that human linguistic abili-ties have evoh'ed in a harmonious and homogeneous manner, butother things being equal, we would not expect o find two appar-ently co-operating modules o ill-suited to each other.
Why shouldgrammars be able to generate things that parsers can't parse?When we consider left and right recursions, ome further ten-sion between Ao B and D emerges.
Multiple left recursions in En-glish are most clearly illustrated by possessive determiner phrases,which are generally assumed to have a structure something like 3:1283SNP VP~PDecNPDetHP ports:9*t  a :~rP poss  : :N : : :Jane ?
mothar ?Nposn  :N : :: : :: : :ta t  i fa rand multiple right recursions by a variety of structures, forexample, relative clauses:4 That's \[the company that manufactures \[the drugs that have\[the side effects that made her come out in n rash\]\]\]There are several facts which suggest that the structures as-signed to these examples by a grammar in accordance with A can-not realistically be assumed to play any very direct role in theactual processing of them in performance.
Firstly, there is the fa*miliar .bservatioa (Chomsky 1965: 13.-14, Langeadoen 1975: 544),that examples like 3 and 4 do not have the intonation contoursthat would be predicted for them on the basis of the constituents~rucrures assigned to them by a grammar.
For example, in 3, theintonation of the sequence of possessives i  not defined over thewhole constituent, as might be expected, but is more like a 'list'intonation.
In sentences like 4, the intonation contour somethnesbreaks the sentence up in the way indicated informally here:5 \[That's the company\[ that manufactures the drugs\] \[thathave the side effects\] [that made her come out in a rash\]This chunking of the sentence does not respect its syntacticstructure, splitting the head NP of the relative clause from itsmodifier and grouping it with the main clause instead.
The condi-tions under which this happens are clearly connected with mattersof length and so on, so the actual examples here are also capable ofreceiving the 'correct' contour, bur the effect is clearly to be seen inlonger and more complex sequences.
This observation is generallytaken to indicate that, whatever else is happening in the produc-tion and comprehension f such examples, it is not the case thatcomplete syntactic structures of the type assigned by a grammarare being computed.A filrther argument that this is so derives from the fact thatalthough 4 was displayed as a right branching structure, it wouldalso receive a left branching analysis, and if sufficiently complex.all possible combinations of the two.
This means that the numberof parses such a structure would receive goes up massively withthe number of clauses invoh, ed (see Church and Patil 1982 for dis-cussion of this.
Analogous comments hold for PP modifiers andconjunctions on most analyses).
It is clearly stretching credibilityto assume that a parsing procedure follows very faithfully what agrammar says about such cases.While ditficult to reconcile with A (and hence B) these obser-vations are consistent with D. This perhaps needs ome elaboration:it is a reasonable conjecture, given what we know about short termlinguistic memory, that the human parsing mechanism operates ina way which has the formal properties of a finite state device (seee.g.
Chomsky 1963, Chomsky and Miller 1963, or, more recently,Langendoeu and Langsam 1984).
The fact that unlimited right orleft recursiou can be recognised, whereas centre recursion cannot,is consistent with this, for any (CF) language with bounded centreembedding isalso a finite state language.
However, when we turn tofull parsing, as opposed to recognition, it turns out that the properanalysis even of left and fight recursion demands non-finite-stateresources (Langendoen 1975).
Intuitively, this is easily seen: pars-ing a language can be regarded, abstractly, as a traasduction fromstrings of terminal items to labelled bracketings representing struc-tural descriptions.
For the labelled bracketings to be well formed,left and right brackets bearing the same label must be paired upcorrectly.
In the case of recursion, this means that the bracket lan-guage contains cases where some number of left brackets of type Xmust be paired up with the same number of right brackets of typeX, for any number.
This is a classic non-finite state language, andthus even if the input to the transducer is finite state, the overalltransduction must be at least of context-free power, given no finitebound on recursion.
Full parsing, therefore, of strnctures like 3 and4, will demand resources of at least this power.Let us now assume that D should be taken to apply, not justto cases of centre embedding, but to all types of recursion (as inMiller and Isard's original {1963) discussion of centre embedding).This is, in effect, a conjecture that the human parsing mechanismis forced to operate with no more than finite state resources, eventhough the class of languages generated by the grammars foundnatural by human beings might lie far outside the finite state class.Under such circumstances it would be expected that in the left andright recursive cases, a full parsing would not always be available,an expectation that we may take to be supported by the intona-tional evidence, and by the combinatorial explosion considerationsalluded to above.If this is a plausible line of reasoning, it nevertheless presentsus with a further difficulty in the fight of observation B.
For if se-mantics is driven by syntax, it would seem to follow that structureswhich are not properly parsed should not be fully interpretable ei-ther.
While this is clearly the case for centre embeddings, it isnot the case for either left or right recursion: semantically speak-ing they are completely unproblematic.
This is a further conflictwhich our model of parsing will have to resolve.129An Incrementa l  Parser - In terpreterMy aim was to develop a parser and interpreter which wascompatible with A to D, resolving the apparent conflicts betweenthem, and which also incorporated in a fairly concrete form theassumption that grammars have some status, independently ofparsers, as mental objects.
That is to say, it was assumed thatwhat linguists say about natural anguage in the form of a gram-mar {including semantic interpretation rules} is available to theparser-interpreter as some kind of data structure having roughlythe form that the linguist's pencil and paper description wouldsuggest.
The aim was also to demonstrate a serious commitmentto C by getting the parser to build up explicit representations of themeaning of a sentence piece by piece during the course of a parse.To my knowledge, the only other work which takes this commit-meat seriously at the appropriate level of formal detail {there isno shortage of well intentioned hand-waving} is that of Ades andSteedmau (1982).
In Pulman (forthcoming), I discuss ome of thesimilarities and differences between these two approaches.For purposes of illustration, I will assume that the underly-ing grammatical theory involved is some form of Phrase StructureGrammar, where semantic interpretation consists of translationinto a simple form of higher order logic.
Neither of these assump-tions is crucial: the parsing procedure can be adapted to certaintypes of transformational grammar, and the associated process ofsemantic interpretation requires only that the semantic theory canh,, driven hy syntactic structures, and that there is some way ofd,,ing function application and composition.
It is unlikely that thisthis rules ,,at any candidates at all.The pr,,cedure is best thought of as a type of stack-basedghift-r,,duee algorithm, though with the ability to deal with in-complete constituents.
In the current implementation it operatesaon-deterministicalb': I (and others) have argued elsewhere (Pubnian.
f.rthcoming) that there is no good reason to suppose thatparsing {as opposed to a more global process of comprehension) isdeterministic.
(Contra Marcus 1980, Berwick and Weinberg 1984.See al~- Crain and Steedman, forthcoming;, Briscoe 1984).The driving mechanism of the parser-interpreter maintains anagenda of configurations, each representing a particular state of apars,,.
A configuration is a pair consisting of a representation fthe state of the stack, and the current position in the input string.The stack is a list of entries, of which (usually} only the top twoare accessible to the basic operations of the parser.
Each entryrepr,,sonts a wholly or partially recognised constituent, along withits interpretation i  terms of a translation into a logical expres-sion.
An entry is a triple, consisting of a category label, indicatingwhat type of constituent is being recognised, a 'needed' list of con-stituents which must be found before the category is complete, andthe interpretation so far.
The parser starts with an initial configu-ration and proceeds by trying to produce new ones from that untileither m~ more alternatives a,.e left, and the parse has failed, or oneor more complete parses are produced.There are four basic operations which produce a new config-uration from an old one.
Which one is performed epends on thestate of the stack.
If there is a choice between two, both are per-formed, producing two new configurations.SHIFT: takes the next word from the input and creates a newstack entry for it (for each lexical entry it has in the dictionary).For example, given a lexicai entry like{every, Det, A P A Q A.x Px - -  Qx}Shift produces a stack entry like:{Det.
nil, A P A Q Ax Px - -  Qx}The interpretation of non-logical words is assumed to be theassociated constant, as is customary.
Since lexical categories arealways complete the second 'needed' element in a stack entry willalways be empty.
Having created a new stack entry, Shift recordsa new configuration with that entry on top of the stack, and anupdated input pointer.INVOKE-RULE: applies when there is a completed entry ontop of the stack.
Essentially, it checks the rules in the grammarto see whether the category represented by that entry could beginsome higher level constituent.
Although this is not strictly neons-saD', a one-word 1oo "l'l'l'lmhead is incorporated for efficiency.If Invoke-rule succeeds in matching a category of an entry withthe first member of the right hand side of a rule, it creates a new en-tr)" from them.
Logically speaking, this process happens as follows:assume, for illustration, an entry of the form{Det, nil.
every}(where the interpretation of 'every' might actually be as above)and a example rule of the form:NP - -  Det N ; Det' (N'}where the part offer the semi-colon is the semantic omponent.The entry matches the beginning of the right hand side of the ruleand so could begin an NP constituent.
Now assume a function, callit Abstract, which when applied to a rule of this form produces fromits right hand side and semantic omponent the result of lambdaabstracting over all the right hand side symbols (in the order spec-ified in the rule) which appear in the semantic omponent.
ThusAbstract applied to the rub above would produceA det A n { det In)}If applied to a rule likeS - -  NP VP ; VP' (NP')it would producenp ~ vp { vp {np)}This is simply a more literal rendering of what the rule actuallysays, in fact: making explicit the fact that the items occurring inthe semantic part of the rule are to be interpreted as variables.When Invoke-rule has matched an entry to a rule it producesa new entry where the category is the left hand side of the rule,the 'needed' list is all but the first of the right hand side, and theinterpretation is the result of applying Abstract to the rule andthen applying that to the interpretation of the original entry.
Inthe example above the result of all this would be:{NP, N, A n { every (n)} }In other words, the interpretation is simply that of the wholerule with that of the existing entry put in the appropriate place: asemantic equivalent of the 'needed' field.
In general, the interpreta-tion of an incomplete constituent is that it is a function expectingto find the needed items as arguments.COMBINE: combines a complete ntry on top of the stackwith an incomplete one below it, if the category label of the for-mer matches the first 'needed' item of the latter.
For example, ifthe stack contained an entry like the one just described, with acomplete ntry on top:{N, nil, man}{NP, N, A u { every (u)} }then Combine would produce a new entry with the category ofthe incomplete one, the remainder, if any, of the needed list, and aninterpretation which is the result of applying that of the incompleteentry to tllat of the complete one.
Here the result would be:{NP.
nil, ever)" {man) }130when beta reduction of the lambda expressions has taken place,which is a complete constituent, in this instance, although this neednot be the c,'~se.
If the needed field is not nil, the interpretationwill always reflect this.These three operations are in fact sufi|cient to allow the parserto operate.
However, a further operation is also necessary" if we areto maintain consistency with our original assumptions.CLEAR: Clear is intended to correspond to the intuition that.nee a c.mplete or completable representation of a proposition hasl..en built up.
the syntactic information needed to do this is nohm~cr required, under normal circumstances.
The conditions underwhich Clear operates in the present implementation ensures thatdlis type of syntactic information is discarded as soon as possible:aldvmgh this is probably not a realistic claim about human parsing.Clear operates when:(i) there are only two items on the stack (in a less enthusiasticversion.
Clear would be constrained to operate only on the bottomtwo items on the stack)(ii) the topmost one potentially contains everything needed tocomplete 'the bottom one(iii) the topmost one is a VP  or SThe first two conditions correspond to the obvious truth thatyou can only get rid of syntactic information when it is safe todo so, and that 'selective forgetting' is not possible: either all thesyntactic information relevant to the earlier portion of the sentenceis discarded, or none of it is.
Otherwise, the claim, and the laterexplanations which depend on it, would be vacuous.
The third isintended to capture the intuition that it is the main predicate of ssentence which when encountered provides enough information tobe able to continue parsing safely after that point with no referenceto anything before.
For example, when a verb is encountered, thenumber and type of (obligatory} arguments will be known.When the conditions for Clear are met, the effect is that theinterpretation of the bottommost entry is composed with that ofthe topmost, the bottom one then being erased.
For example, in asituation like:{VP, NP, A np {likes (np)} }{S, VP, A vp {vp {some (man})} }where the topmost entry is of the type that the one underneathis looking for, the result of Clear is that the stack will contain just:{VP, NP, A x {A vp {vp (some (man))} {A up {likes (up)}(.~)}}}When this VP finds the NP it is looking for, the interpretationwill reduce to what we would have had more directly if Clear hadnot operated.Here is a trace of the parser to show how all these operationswork together.
The meanings of the individual exical items havebeen suppressed in the interests of readability.S - -NP  VP ;  VP (NP}VP --  VNP ;V  (NP)NP --  Det N ; Det (N)Input: The farmer killed the ducklingShift:{Det, nil, the}Invoke:{NP, N, A n {the (n)} }Shift:{N, nil, farmer}{NP, N, A n {the (n)} }Combine:{,NT, N, the (farmer) }Invoke:{S. VP, A vp { vp (the (farmer))} }Shift:{V, nil, killed}{S, VP, A vp { vp (the (farmer))} }Invoke:{VP, NP, A np { killed (rip}} }{S, VP, A vp { vp (the (farmer))} }Clear:.
{VP, NP, A x {A vp {vp (the (farmer))} {A np {killed (rip)} (x)}}}Shift:{Dec, nil, the}{VP, NP, A x {A vp {vp (the (farmer))} {A np {killed (np)} (x)}}}Invoke:{NP, N, A n {the (n)} }{VP, NP, ,X x (A vp {vp (the (farmer))} {A np {kilted (up}} {x)}}}Shift:{N, nil, duckling}{NP, N, A n {the (n)} }{VP, NP, A x {A vp {vp (the (farmer))} {A np {killed (up)} (x)}}}Combine:{NP, nil, the {duckling)}{VP, NP, A x {A vp {vp (the (farmer))} {A np {killed (np)} (x)}}}Combine: {VP, nil, A x {k vp {vp (the (farmer))} {k np {killed(np)} (x) }} (the (duckling)) }At this point the parse is complete, and the complex interpre-tation beta-reduces to:{killed (the (duckl!ng))} (the (farmer))The resulting interpretation is exactly what would have beenobtained by a 'classical' system operating as described earlier.Model l ing  Incrementa l  In terpretat ionHow does the parsing procedure manage to remain faithful toA to D simultaneously?
Let us begin with B: the compositional,syntax-driven ature of semantics.
The parser assumes that se-mantic information can be associated with syntactic rules in someway (though it is not ruled out - in fact, it is assumed - that someextra aspects of interpretation may need to be computed by sep-arate procedures: for example, identification of variables for thepurposes of indicating coreference; cases of wide scope of quantifierphrases in syntactic narrow scope positions, etc.).
Once the rule inquestion has been identified by Invoke-rule, the semantic informa-tion involved is extracted and used to form the next stack entry.The syntactic information is also used to form expectations aboutwhat constituents must come next, although it is conceivable thatif semantic type is entirely predictable from syntactic ategory andvice versa this information is actually redundant.
No other mech-anisms for linking syntax with semantics are required.
Hence theparser obeys condition B absolutely literally and faithfully.131The important thing to notice is that this is achieved withoutbuilding any explicit syntax trees during the course of parsing asentence.
Syntactic information is used to build up the interpreta-tion and to guide the parse, but does not result in the constructionof an independent level of representation.
As the title of the paperindicates, there is no parse tree built for a sentence at all, Whileit is tr~w that in some sense trees are implicit in the sequence ofoperations of the parser, this is an inevitable consequence of thefact that the rules used themselves define trees, and as we shallsee, even in this weak sense the tree structures implicit for certaintypes of recursive construction are not isomorphic to those whichwould be defined by the grammar.I like to think of this aspect of the operation of the parser asembodying the intuition often expressed (most often in the oraltraditiou, though explicit in Isard 1974), that syntax is a 'controlstructure' for semantics.
It also has the merit of being consistentboth with the widespread agreement among linguists that syntaxplay's a central role in language understanding, and with the ap-parently equally widespread failure of psycholinguists to llnd anyevidence that purely syntactic representations are computed at anystage during normal comprehension.Turuing now to C, the observation that sentences are under-stood on {at least) a word by word basis on a pass through fromleft to right, it should be clear that our procedure provides a directmodel of this process, on the assumption that at least a centralpart of the meaning of a sentence is given by a translation intoa logical form of this kind.
As soon as a word is encountered, itis integrated into the logical form being built up.
At every stage,this logical form, though possibly not yet complete, is a perfectlymeaningful object (within the higher order logic assumed here it isjust a term like any other}: it can be used to perform inferences,be the antecedent for anaphora or ellipsis, be integrated with thecontext so as to assess and discard alternative interpretations cor-responding to different parsings, and in general perform any of thefunctions we expect the meaning of a sentence or sentence fragmentto be able to do.The satisfying of A is in a sense automatic but trivial, giventhat the parser uses ordinary grammatical rules, rather than somepreprocessed version altering the output of the rules to producefiat structures (as, for example, in Langendoen 1975, Langendoenand Langsam 1984, and also - wrongly, on the present approach -in Pulman 1983}.
More interesting is the way the parser producesa similar effect to that achieved with these preprocessings, withoutaltering the rules themselves, as a side effect of its observance ofD- the limitation on recursion.Reeurslon L imitat ionsI have argued eLsewhere (Puiman, forthcoming) that attemptsto explain the difficulty of centre embedded sentences as a conse-quence of parsing strategies axe unsuccessful, and that the simplestexplanation is the original one (Miller and Isard 1963): that thehuman parsing mechanism is fundamentally incapable of operatingrecursively.
To be more precise: if (in the worst case} the parserencounters an instance of a construction in the course of trying toparse an earlier instance of it, the record of the earlier instance willbe erased and 'forgotten', causing confusion in those cases wherethe information is needed to complete a parse successfully, as inthe centre embedding cases.
Clearly this is not absolute: some ia-stances of centre embedding can be found to a depth of 4 or 5, butfor simplicity we will assume that there is some small fixed limit,L.The present procedure implements this restriction quite lit-erally: if Invoke-rule attempts to put on the stack an incompleteconstituent of category X, when there are already L instances ofsuch incomplete Xs on the stack, then the earliest instance is erasedbefore Invoke-rule can succeed.
The interesting and striking thin#about this restriction is that as stated, it applies to all types ofrecursion, and thus might be expected to result in parsing failuresnot just for centre embedded e.xamples of a depth greater than L,but for left and right recursions deeper than L too.
However, thisdoes not happen: the basic operations of the parser in fact conspireto bring it about that both left and right recursions can be parsed,the former fully, and the latter to just the extent, apparently, thatis needed to be able to provide them with an appropriate inter-pretation.
Thus a perfectly general and simple restriction can bein, posed, rather than some version (implausibly) qualified so as todistinguish between different ypes of recursion.The simplest case is that of left recursiou, which we will illus-trate with an artifical example grammar:A - - *Aa :A(a)A~a;aWhen processing a string 'aaa...', the parser operates as in thefollowing trace ('b' is the interpretation of 'a'):Shift:{~, nil, b}Invoke:{A, ~,  b}Invoke: {A, a, Aa {b (a)}}Shift:{a, nil, b}{.
'., ,~ ~a {b (a)}}Combine:{A, nil, b (b ) )Invoke:{A, a, Aa {b (b (a))})At this point the cycle of operations has become vident: atno point is there ever more than one occurrence of an incompleteA constituent on the stack, and so there is never any situation inwhich the recursion limitation would come into effect.
In otherwords, like any shift-reduce mechanism, this parser can processunbounded left recursion without the stack growing beyond a con-stant depth.Centre embeddings ofa depth greater than L will not be parsedcorrectly.
To see how this might work out in detail we will assumesome simple rules for relatives:NP -.
NP  R.EL : REL'(NP')REL --, NP VP : NP'(VP')and we will ignore the question of how wh words are linkedwith gaps appropriately, other than the assumption that this infor-mation is contained somewhere in the trees defined by these rules.Notice that we are assuming for simplicity that relative clauses area distinct constituent from S, and also, that no recursion at all isallowed.
For clarity, rather than build the incremental semantic in-terpretations yielded by the parser we will display the partial treethat a more conventional parser might build.For a sentence like:7 The woman the boy the child knew waved to laughedwe ought to build a tree like:132ggLl{P: : IfP ~rP: : : : V: : : : :VPVVPVthe wcaan the hey the child knew waved to laughedThings proceed as follows, ignoring some obvious steps:{i) { NP, nil, {NP the woman}}(ii) { NP, REL, {NP {NP the woman}{REL  ...}}}(iii) {N'P, nil, {NP the boy}}{ NP, REL, {NP {NP the woman}{REL  ...}}}At this point, if we are to lind the correct interpretation or buildthe appropriate parse tree Invoke must recognise the NP 'the girl'as the beginning of another elative clause, and place on the stackan entry like:{NP, REL, {NP{NP the girl}{IZEL ...}}}But of course this will violate the recursion restriction, forthere is already an {NP, REL...} on the stack.
Let as assume thatthis earlier one is thus 'forgotten', or at least rendered inaccessibleto the parsing procedure in some way.
Things now proceed - againignoring obvious details - until we have recognised the sentence asfar as the word 'knew':(iv) {NP, nil, {NP {NP the girl}{RgL {NP the boy}{VPknew}}}}At this point the procedure runs into trouble.
If the parsermerely continues with 'waved to' it will be stuck: 'waved to' in itsown is not a complete VP, for it is missing an object.
So a possibleparse in which what is on the stack is the subject of'waved to' willfail.
But there is no other option available for it.
In order to treat'waved to' correctly, the parser needs to know that it is part ofa relative clause and thus can legitimately have a missing object.But this of course is precisely the information that is no longeravail~ble to it, for the REL entry which would have signalled thishas been erased.
So the parser cannot proceed beyond this pointcoherently.
It is reassuring that this is exactly the point - after thefirst verb ,,f the sequence stacked up - where both intuitive andexp*,rimental evidence {Miller and Isard 1963} suggest the onset ofdifficulty with these constructions.
Our parsing procedure seemsto get stuck at exactly the same point people do in these centreembedded constructions.With right recursion there are two cases of interest.
Withu|ultilde sentential complementation like9 .I.e th.,tght hat Bill expected that Mary knew ....then the c,peration of Clear means that the recursion limit willnever be exceeded.
Whenever we have a stack of the form:{VP.
S, beta}{S, VP, alpha }Clear will erase the bottom entry leaving:.
{VP, S, Ax {alpha {beta (x)}} }Whenever there is a stack of the form:{S, VP, beta}{VP, S. alpha}Clear will likewise produce:{S, VP, Ax {alpha {beta (x)}} }Thus neither recursive category will ever have more than oneinstance on the stack at a time.
As in the earlier illustrative ex-amples, the process of function composition means that, when thefinal constituent is encountered, the whole complex logical expres-sion reduces down to exactly what we would have had under the'classical' view: the difference here is that we do not depend on thewhole syntactic tree being explicitly constructed first in order toget the correct results.While the general idea here seems correct, the details arenot entirely satisfactory, however.
In the current implementation,Clear operates whenever it can, which, as remarked above, doesnot seem very plausible.
Since the motivation for Clear is partlyvia considerations of short term memory load, in a more realisticmodel some extra parameter to reflect this transient load shouldclearly be involved, such that Clear only operates when a certainthreshold is exceeded.
This would mean that there was room forsome decoupling of the recursion limitation from the conditions onClear: at present, with a recursion limit of 1, even a sentence like10 John expected that Bill would leavecould not be parsed unless Clear had operated.
But it seemsunlikely that such a short sentence imposes any very great strainon syntactic short term memory.
Furthermore, in the present im-plementation, Clear will prevent sententinl conjunctions from beingparsed at all, for by the time the conjunction is reached, the onlyconstituent left on the stack is labelled as a VP, not an S, andso Invoke-rule cannot find an appropriate candidate to continue.Fortunately, both of these wrinkles are easily amended by mak-ing Clear more conservative in its operation, while preserving thepresent ype of explanation for why this type of right recursiveconstruction can still be parsed with little apparent effort.Not all cases of right recursion need be 'rescued' by Clear,however.
Given nmltiple PP modifiers, introduced by a rule:NP -- NP PPwe have the potential for the type of situation described earlier,where there may be many distinct parse trees, only one of whichmay aecurat*.ly reflect the actual pattern of attachment of PPs tothe NP tit,')" modify.I IThe house in the woods by the riverThe book on rock climbing by the writer fromScotlandThe bird in the tree near the flowerbed with a redbeakAssuming a recursion limit of 1. there is only one 'parse' ofsuch strucrltres that will succeed, since Clear- applying only toprojeeti(ms of +V, recall - cannot be involved.
The parsing proce-dure witl process these cases in a way which corresponds to a leftbranching or stacked analysis:133~PllP PP~IP PP :I~P pp : :This might seem to be a serious disadvantage, for there areclearly readings of the above examples which appear not to bethose suggested by such a 'parse'.
However, it is actually a goodresult: when there is more than one parse of a sequence like this, the'correct' one - i.e.
that consistent with the preferred attachments -must be decided on by a mLxture of semantic and contextual con-straints on what can modify what.
A full and exhaustive parse isthus still not sufficient o arrive at a unique interpretation.
But ifthe real work of deciding what attachments are to be made is doneby these non-syntactic procedures, then all but the lowest level ofsyntactic analysis, (into non-recursive NIP and PP constituents},is entirely redundant.
All but one of the more complex analyseswill be thrc,wn away, and all of the semantic information to begained from that analysis has already been computed in the courseof deciding that it is the 'correct' one.
(As everyone who has everwritten a practical parser has discovered, this is in any case anextremely silly way to do things).
Thus an exhaustive syntacticanal.vsis is neither necessary nor sufficient for the correct handlingof these sequences.
All that is required is that the low level con-stituent structure be recogaised: thereafter, the meaning of a mod-ifier can be assumed to be a function which seeks an appropriateargatment to modify, and is thus just applied to the representationof the meaning of the sentence that has already been built up.
Inocid,-~tall.v.n.dco that this latter assumption is almost forced onus indq~,,mleutly by the existence of rightward extraposed nomi-nal nxmlili,,rs which may be encountered without warning after anappar,utly e,mxplete sentence meaning has been assembled:12 1 gave the book back to the girl in the library that youasked me to photocopyThe level of analysis provided by our treatment appears to beexa,'tl.v what is needed for the attachment of these modifiers to beace,mmmdated appropriately.S~.queuces of ordinary relative clauses, and multiple conjunc-ti,,ns will he treated in a similar way, and similar arguments applyto them.
In the case of conjunctions, of course, the fact that noinformati.n is lost hy not computing massive parse trees is evenmore  ~\ ]o .
, im ls .It is int~'re~ting to note, in connection with sequences of rela-tives, that the stacked 'parse' which the operation of the proceduremimics is actually the one which corresponds almost exactly to theunexpected intonation patterns noted by Chomsky and Langen-doen:13 {That's the company} {that manufactures the drugs} {thathave the side effects} {that made her come out in a rash}~P~P RZL?
P ~ :l~P I F  J ,  : :: : : : :".
: : ."
:the  company that .
.
,  that .
.
,  that .
.
,In general, then, the recursion limitation and the basic opera-tions of the parser-interpreter seem to combine to provide a fairlysatisfactory model of the parsing and understanding of these dif-ferent types of recnrsive constructions.SummaryI have presented an algorithm for parsing and interpretinggrammars and semantic descriptions of a certain formal type, whichis consistent with a set of clear and uncontroversial facts abouthuman linguistic performance.
In particular, I hope to have showthat a (partial) theory of competence can be literary embeddedwithin a model of performance, in such a way that simple principlesbelonging to the latter (recursion limitations) explain phenomenathat have sometimes bee,, taken to pertain to the former.There are some further practical consequences arising fromthis work: there is not space to go into the details here, but thereis an interpretation of the parsing algorithm above - as one mightsuspect, given its formal properties - as a finite state transducermapping strings of (labelled} terminal items directly into logicalforms.
While the construction of such a device from a grammar ofthe original type is rather complex, the result would be a 'linguisticengine' {sentences in, logical forms out) of formidable fficency.FootnoteThe parser-interpreter is written in Franz Lisp under 4.2 Unixon a Sun workstation.
The current grammar provides syntacticand semantic overage for simple complement types, phrasal andsentential conjunction, relative clauses, and questions.134ReferencesAdes, A. E. and Steedman, M. J.
(1982)On the Order of Words.
Linguistics and Phi-losophy 4, 517-558Berwick, R. C. and Weinberg, A. S. (1984)The Grammat ica l  Basis of Llnguistlc PePformance: Language Use and Acquisition.Cambridge, Mass: MIT Press.Briscoe, E. J.
(1984)Towards an Understanding of Spoken SpeechComprehension: the Interactive Determln-ism Hypothesis.
Ph .D.
Diss., Dept.
of Lin-guistics, Univ.
of Cambridge.Chomsky, N. (1963)Formal Propert ies of Grammars .
In R.Luce, R. Bush and E. Galanter (eds) Handbookof Mathematical Psychology Vol II, New York:John Wiley.Chomsky, N. (1965)Aspects of the Theory of Syntax.
Cam-bridge, Mass: M\]T Press.Chomsky, N. and Miller, G. {1963)Flnltary Models of Language Users.
In R.Lute, R. Bush and E. Galanter (eds) Handbookof Mathematical Psychology Vol 11, New York:John Wiley.Church, K. W. and Patil, R. (1982)Coping wlth Syntactic Amblgultyt AmericanJournal of Computational Linguistics, 8, 139-149Crain.
S. and Steedman, M. J.
{forthcoming)On Not Being Led Up The Garden PathzThe Use of Context by the PsychologicalParser.
In A. Zwicky, L. Kartunnen, andD.
Dowty (eds) Natural Language Parsing:.
Psy-choliuguistic, Theoretical, and Computational Per-spectives, Cambridge: Cambridge Univ.
Press.Gawron, J. M. et al(1082)The GPSG Linguistics System, Proceedingsof the 20th annual meeting, Association for Com-putational Linguistics.Isard, S. (1974)What  would you have done If...~ TheoreticalLinguistics, Vol I, 233-256Katz, J. J, (1981)Language and Other Abstract Objects Ox-ford: Basil Blackwell.Laugendoen, D. T. (1975)Finite State Parsing of Phrase StructureLanguages and the Status of Readjustment Rulesin Grammar.
Linguistic Inquiry 6, 533-554.Langendoen, D. T. and Langsam, Y.
(1984)The Representat ion of Const i tuent Struc-tures for Finite State Parsing in Proceedingsof Coling 84, Association for Computational Lin-guistics.Miller, G. and Isard, S. D. (1964)Free Recall of Self Embedded English Sen-tences.
Information and Control 7, 292-303.Pulman, S. G. (1983)General ised Phrase Structure Grammart  Ear-ley's Algor i thm, and the Mlnlmlsat lon ofReeurslon in Sparck Jones and Wilks eds.Pulman, S. G. (forthcoming)Computat ional  Models of Parsing in A. Ellis(ed) Progress in the Psychology of Language, Vol2, Lawrence Erlbaum Associates Ltd.Rosenschein, S. J. and Shieber, S. M. (1982)Translat ing English Into Logical Form Pro-ceedings of the 20?h annual meeting, Asssociadoufor Computational Linguistics.Schubert, L. K. and Pelletier, F. J.
(1982)From English to Logic: Context Free Com-putat ion of 'Convent ional '  Logical Trans-lation, American Journal of Computational Lin-guistics, 8, 27-44.Sparck Jones, K., and Wilks, Y.
(eds) (1983)Automatic  Natural  Language Parsing, Chich-ester: EllLs Horwood Ltd.135
