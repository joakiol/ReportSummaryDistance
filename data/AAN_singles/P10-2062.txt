Proceedings of the ACL 2010 Conference Short Papers, pages 336?341,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsAutomatically generating annotator rationalesto improve sentiment classificationAinur Yessenalina Yejin Choi Claire CardieDepartment of Computer Science, Cornell University, Ithaca NY, 14853 USA{ainur, ychoi, cardie}@cs.cornell.eduAbstractOne of the central challenges in sentiment-based text categorization is that not ev-ery portion of a document is equally in-formative for inferring the overall senti-ment of the document.
Previous researchhas shown that enriching the sentiment la-bels with human annotators?
?rationales?can produce substantial improvements incategorization performance (Zaidan et al,2007).
We explore methods to auto-matically generate annotator rationales fordocument-level sentiment classification.Rather unexpectedly, we find the automat-ically generated rationales just as helpfulas human rationales.1 IntroductionOne of the central challenges in sentiment-basedtext categorization is that not every portion ofa given document is equally informative for in-ferring its overall sentiment (e.g., Pang and Lee(2004)).
Zaidan et al (2007) address this prob-lem by asking human annotators to mark (at leastsome of) the relevant text spans that support eachdocument-level sentiment decision.
The text spansof these ?rationales?
are then used to construct ad-ditional training examples that can guide the learn-ing algorithm toward better categorizationmodels.But could we perhaps enjoy the performancegains of rationale-enhanced learningmodels with-out any additional human effort whatsoever (be-yond the document-level sentiment label)?
We hy-pothesize that in the area of sentiment analysis,where there has been a great deal of recent re-search attentiongiven to various aspects of the task(Pang and Lee, 2008), this might be possible: us-ing existing resources for sentiment analysis, wemight be able to construct annotator rationales au-tomatically.In this paper, we explore a number of methodsto automatically generate rationales for document-level sentiment classification.
In particular, we in-vestigate the use of off-the-shelf sentiment analy-sis components and lexicons for this purpose.
Ourapproaches for generating annotator rationales canbe viewed as mostly unsupervised in that we do notrequire manually annotated rationales for training.Rather unexpectedly, our empirical results showthat automatically generated rationales (91.78%)are just as good as human rationales (91.61%) fordocument-level sentiment classification of moviereviews.
In addition, complementing the hu-man annotator rationales with automatic rationalesboosts the performance even further for this do-main, achieving 92.5% accuracy.
We further eval-uate our rationale-generation approaches on prod-uct review data for which human rationales are notavailable: here we find that even randomly gener-ated rationales can improve the classification accu-racy although rationales generated from sentimentresources are not as effective as for movie reviews.The rest of the paper is organized as follows.We first briefly summarize the SVM-based learn-ing approach of Zaidan et al (2007) that allows theincorporation of rationales (Section 2).
We nextintroduce three methods for the automatic gener-ation of rationales (Section 3).
The experimentalresults are presented in Section 4, followed by re-lated work (Section 5) and conclusions (Section6).2 Contrastive Learning with SVMsZaidan et al (2007) first introduced the notion ofannotator rationales ?
text spans highlighted byhuman annotators as support or evidence for eachdocument-level sentiment decision.
These ratio-nales, of course, are only useful if the sentimentcategorization algorithm can be extended to ex-ploit the rationales effectively.
With this in mind,Zaidan et al (2007) propose the following con-336trastive learning extension to the standard SVMlearning algorithm.Let ~xi be movie review i, and let {~rij} be theset of annotator rationales that support the posi-tive or negative sentiment decision for ~xi.
For eachsuch rationale~rij in the set, construct a contrastivetraining example ~vij , by removing the text spanassociated with the rationale ~rij from the originalreview ~xi.
Intuitively, the contrastive example ~vijshould not be as informative to the learning algo-rithm as the original review ~xi, since one of thesupporting regions identified by the human anno-tator has been deleted.
That is, the correct learnedmodel should be less confident of its classifica-tion of a contrastive example vs. the correspondingoriginal example, and the classification boundaryof the model should be modified accordingly.Zaidan et al (2007) formulate exactly this intu-ition as SVM constraints as follows:(?i, j) : yi (~w~xi ?
~w~vij) ?
?
(1 ?
?ij)where yi ?
{?1,+1} is the negative/positive sen-timent label of document i, ~w is the weight vector,?
?
0 controls the size of the margin between theoriginal examples and the contrastive examples,and ?ij are the associated slack variables.
Aftersome re-writing of the equations, the resulting ob-jective function and constraints for the SVM are asfollows:12 ||~w||2 + C?i?i + Ccontrast?ij?ij (1)subject to constraints:(?i) : yi ~w ?
~xi ?
1 ?
?i, ?i ?
0(?i, j) : yi ~w ?
~xij ?
1 ?
?ij ?ij ?
0where ?i and ?ij are the slack variables for ~xi(the original examples) and ~xij (~xij are named aspseudo examples and defined as ~xij = ~xi?~vij?
), re-spectively.
Intuitively, the pseudo examples (~xij)represent the difference between the original ex-amples (~xi) and the contrastive examples (~vij),weighted by a parameter ?.
C and Ccontrast areparameters to control the trade-offs between train-ing errors and margins for the original examples ~xiand pseudo examples ~xij respectively.
As noted inZaidan et al (2007),Ccontrast values are generallysmaller than C for noisy rationales.In the work described below, we similarly em-ploy Zaidan et al?s (2007) contrastive learningmethod to incorporate rationales for document-level sentiment categorization.3 Automatically Generating RationalesOur goal in the current work, is to generate anno-tator rationales automatically.
For this, we rely onthe following two assumptions:(1) Regions marked as annotator rationales aremore subjective than unmarked regions.
(2) The sentiment of each annotator rationale co-incides with the document-level sentiment.Note that assumption 1 was not observed in theZaidan et al (2007) work: annotators were askedonly to mark a few rationales, leaving other (alsosubjective) rationale sections unmarked.And at first glance, assumption (2) might seemtoo obvious.
But it is important to include as therecan be subjective regions with seemingly conflict-ing sentiment in the same document (Pang et al,2002).
For instance, an author for a movie re-view might express a positive sentiment towardthe movie, while also discussing a negative sen-timent toward one of the fictional characters ap-pearing in the movie.
This implies that not all sub-jective regions will be relevant for the document-level sentiment classification ?
rather only thoseregions whose polarity matches that of the docu-ment should be considered.In order to extract regions that satisfy the aboveassumptions, we first look for subjective regionsin each document, then filter out those regions thatexhibit a sentiment value (i.e., polarity) that con-flicts with polarity of the document.
Assumption2 is important as there can be subjective regionswith seemingly conflicting sentiment in the samedocument (Pang et al, 2002).Because our ultimate goal is to reduce humanannotation effort as much as possible, we do notemploy supervised learning methods to directlylearn to identify good rationales from human-annotated rationales.
Instead, we opt for methodsthat make use of only the document-level senti-ment and off-the-shelf utilities that were trainedfor slightly different sentiment classification tasksusing a corpus from a different domain and of adifferent genre.
Although such utilities might notbe optimal for our task, we hoped that these ba-sic resources from the research community wouldconstitute an adequate source of sentiment infor-mation for our purposes.We next describe three methods for the auto-matic acquisition of rationales.3373.1 Contextual Polarity ClassificationThe first approach employs OpinionFinder (Wil-son et al, 2005a), an off-the-shelf opinion anal-ysis utility.1 In particular, OpinionFinder identi-fies phrases expressing positive or negative opin-ions.
Because OpinionFinder models the task asa word-based classification problem rather than asequence tagging task, most of the identified opin-ion phrases consist of a single word.
In general,such short text spans cannot fully incorporate thecontextual information relevant to the detection ofsubjective language (Wilson et al, 2005a).
There-fore, we conjecture that good rationales should ex-tend beyond short phrases.2 For simplicity, wechoose to extend OpinionFinder phrases to sen-tence boundaries.In addition, to be consistentwith our second op-erating assumption, we keep only those sentenceswhose polarity coincides with the document-levelpolarity.
In sentences where OpinionFindermarksmultiple opinion words with opposite polaritieswe perform a simple voting ?
if words with pos-itive (or negative) polarity dominate, then we con-sider the entire sentence as positive (or negative).We ignore sentences with a tie.
Each selected sen-tence is considered as a separate rationale.3.2 Polarity LexiconsUnfortunately, domain shift as well as task mis-match could be a problem with any opinion util-ity based on supervised learning.3 Therefore, wenext consider an approach that does not rely on su-pervised learning techniques but instead exploresthe use of a manually constructed polarity lexicon.In particular, we use the lexicon constructed forWilson et al (2005b), which contains about 8000words.
Each entry is assigned one of three polarityvalues: positive, negative, neutral.
We constructrationales from the polarity lexicon for every in-stance of positive and negative words in the lexi-con that appear in the training corpus.As in the OpinionFinder rationales, we extendthe words found by the PolarityLexicon approachto sentence boundaries to incorporate potentially1Available at www.cs.pitt.edu/mpqa/opinionfinderrelease/.2This conjecture is indirectly confirmed by the fact thathuman-annotated rationales are rarely a single word.3It is worthwhile to note that OpinionFinder is trained on anewswire corpus whose prevailing sentiment is known to benegative (Wiebe et al, 2005).
Furthermore, OpinionFinderis trained for a task (word-level sentiment classification) thatis different from marking annotator rationales (sequence tag-ging or text segmentation).relevant contextual information.
We retain as ra-tionales only those sentences whose polarity co-incides with the document-level polarity as deter-mined via the voting scheme of Section 3.1.3.3 Random SelectionFinally, we generate annotator rationales ran-domly, selecting 25% of the sentences from eachdocument4 and treating each as a separate ratio-nale.3.4 Comparison of Automatic vs.Human-annotated RationalesBefore evaluating the performance of the au-tomatically generated rationales, we summarizein Table 1 the differences between automaticvs.
human-generated rationales.
All computa-tions were performed on the same movie reviewdataset of Pang and Lee (2004) used in Zaidan etal.
(2007).
Note, that the Zaidan et al (2007) an-notation guidelines did not insist that annotatorsmark all rationales, only that some were markedfor each document.
Nevertheless, we report pre-cision, recall, and F-score based on overlap withthe human-annotated rationales of Zaidan et al(2007), so as to demonstrate the degree to whichthe proposed approaches align with human intu-ition.
Overlap measures were also employed byZaidan et al (2007).As shown in Table 1, the annotator rationalesfound by OpinionFinder (F-score 49.5%) and thePolarityLexicon approach (F-score 52.6%) matchthe human rationales much better than those foundby random selection (F-score 27.3%).As expected, OpinionFinder?s positive ratio-nales match the human rationales at a significantlylower level (F-score 31.9%) than negative ratio-nales (59.5%).
This is due to the fact that Opinion-Finder is trained on a dataset biased toward nega-tive sentiment (see Section 3.1 - 3.2).
In contrast,all other approaches show a balanced performancefor positive and negative rationales vs. human ra-tionales.4 ExperimentsFor our contrastive learning experiments we useSVM light (Joachims, 1999).
We evaluate the use-fulness of automatically generated rationales on4We chose the value of 25% to match the percentage ofsentences per document, on average, that contain human-annotated rationales in our dataset (24.7%).338% of sentences Precision Recall F-ScoreMethod selected ALL POS NEG ALL POS NEG ALL POS NEGOPINIONFINDER 22.8% 54.9 56.1 54.6 45.1 22.3 65.3 49.5 31.9 59.5POLARITYLEXICON 38.7% 45.2 42.7 48.5 63.0 71.8 55.0 52.6 53.5 51.6RANDOM 25.0% 28.9 26.0 31.8 25.9 24.9 26.7 27.3 25.5 29.0Table 1: Comparison of Automatic vs. Human-annotated Rationales.five different datasets.
The first is the movie re-view data of Pang and Lee (2004), which wasmanually annotated with rationales by Zaidan etal.
(2007)5; the remaining are four product re-view datasets from Blitzer et al (2007).6 Onlythe movie review dataset contains human annota-tor rationales.
We replicate the same feature setand experimental set-up as in Zaidan et al (2007)to facilitate comparison with their work.7The contrastive learning method introduced inZaidan et al (2007) requires three parameters: (C,?, Ccontrast).
To set the parameters, we use a gridsearch with step 0.1 for the range of values of eachparameter around the point (1,1,1).
In total, we tryaround 3000 different parameter triplets for eachtype of rationales.4.1 Experiments with the Movie Review DataWe follow Zaidan et al (2007) for the training/testdata splits.
The top half of Table 2 shows theperformance of a system trained with no anno-tator rationales vs. two variations of human an-notator rationales.
HUMANR treats each rationalein the same way as Zaidan et al (2007).
HU-MANR@SENTENCE extends the human annotatorrationales to sentence boundaries, and then treatseach such sentence as a separate rationale.
Asshown in Table 2, we get alost the same per-formance from these two variations (91.33% and91.61%).8 This result demonstrates that lockingrationales to sentence boundaries was a reasonable5Available at http://www.cs.jhu.edu/?ozaidan/rationales/.6http://www.cs.jhu.edu/?mdredze/datasets/sentiment/.7We use binary unigram features corresponding to the un-stemmed words or punctuation marks with count greater orequal to 4 in the full 2000 documents, then we normalize theexamples to the unit length.
When computing the pseudo ex-amples ~xij = ~xi?~vij?
we first compute (~xi ?
~vij) using thebinary representation.
As a result, features (unigrams) thatappeared in both vectors will be zeroed out in the resultingvector.
We then normalize the resulting vector to a unit vec-tor.8The performance of HUMANR reported by Zaidan et al(2007) is 92.2% which lies between the performance we get(91.61%) and the oracle accuracy we get if we knew the bestparameters for the test set (92.67%).Method AccuracyNORATIONALES 88.56HUMANR 91.61?HUMANR@SENTENCE 91.33?
?OPINIONFINDER 91.78?
?POLARITYLEXICON 91.39?
?RANDOM 90.00?OPINIONFINDER+HUMANR@SENTENCE 92.50?
4Table 2: Experimental results for the moviereview data.?
The numbers marked with ?
(or ?)
are statisticallysignificantly better than NORATIONALES according to apaired t-test with p < 0.001 (or p < 0.01).?
The numbers marked with 4 are statistically significantlybetter than HUMANR according to a paired t-test withp < 0.01.?
The numbers marked with ?
are not statistically signifi-cantly worse than HUMANR according to a paired t-test withp > 0.1.choice.Among the approaches that make use of onlyautomatic rationales (bottom half of Table 2), thebest is OPINIONFINDER, reaching 91.78% accu-racy.
This result is slightly better than resultsexploiting human rationales (91.33-91.61%), al-though the difference is not statistically signifi-cant.
This result demonstrates that automaticallygenerated rationales are just as good as humanrationales in improving document-level sentimentclassification.
Similarly strong results are ob-tained from the POLARITYLEXICON as well.Rather unexpectedly, RANDOM also achievesstatistically significant improvement over NORA-TIONALES (90.0% vs. 88.56%).
However, noticethat the performance of RANDOM is statisticallysignificantly lower than those based on human ra-tionales (91.33-91.61%).In our experiments so far, we observed thatsome of the automatic rationales are just asgood as human rationales in improving thedocument-level sentiment classification.
Couldwe perhaps achieve an even better result if wecombine the automatic rationales with human339rationales?
The answer is yes!
The accuracyof OPINIONFINDER+HUMANR@SENTENCEreaches 92.50%, which is statistically signifi-cantly better than HUMANR (91.61%).
In otherwords, not only can our automatically generatedrationales replace human rationales, but they canalso improve upon human rationales when theyare available.4.2 Experiments with the Product ReviewsWe next evaluate our approaches on datasets forwhich human annotator rationales do not exist.For this, we use some of the product review datafrom Blitzer et al (2007): reviews for Books,DVDs, Videos and Kitchen appliances.
Eachdataset contains 1000 positive and 1000 negativereviews.
The reviews, however, are substantiallyshorter than those in the movie review dataset:the average number of sentences in each reviewis 9.20/9.13/8.12/6.37 respectively vs. 30.86 forthe movie reviews.
We perform 10-fold cross-validation, where 8 folds are used for training, 1fold for tuning parameters, and 1 fold for testing.Table 3 shows the results.
Rationale-basedmethods perform statistically significantly bet-ter than NORATIONALES for all but the Kitchendataset.
An interesting trend in product re-view datasets is that RANDOM rationales are justas good as other more sophisticated rationales.We suspect that this is because product reviewsare generally shorter and more focused than themovie reviews, thereby any randomly selectedsentence is likely to be a good rationale.
Quantita-tively, subjective sentences in the product reviewsamount to 78% (McDonald et al, 2007), whilesubjective sentences in the movie review datasetare only about 25% (Mao and Lebanon, 2006).4.3 Examples of Annotator RationalesIn this section, we examine an example to com-pare the automatically generated rationales (usingOPINIONFINDER) with human annotator ratio-nales for the movie review data.
In the followingpositive document snippet, automatic rationalesare underlined, while human-annotated ratio-nales are in bold face....But a little niceness goes a long way these days, andthere?s no denying the entertainment value of that thingyou do!
It?s just about impossible to hate.
It?s aninoffensive, enjoyable piece ofnostalgia that is sure to leaveaudiences smiling and humming, if not singing, ?that thingyou do!?
?quite possibly for days...Method Books DVDs Videos KitchenNORATIONALES 80.20 80.95 82.40 87.40OPINIONFINDER 81.65?
82.35?
84.00?
88.40POLARITYLEXICON 82.75?
82.85?
84.55?
87.90RANDOM 82.05?
82.10?
84.15?
88.00Table 3: Experimental results for subset ofProduct Review data?
The numbers marked with ?
(or ?)
are statisticallysignificantly better than NORATIONALES according to apaired t-test with p < 0.05 (or p < 0.08).Notice that, although OPINIONFINDER missessome human rationales, it avoids the inclusion of?impossible to hate?, which contains only negativeterms and is likely to be confusing for the con-trastive learner.5 Related WorkIn broad terms, constructing annotator rationalesautomatically and using them to formulate con-trastive examples can be viewed as learning withprior knowledge (e.g., Schapire et al (2002), Wuand Srihari (2004)).
In our task, the prior knowl-edge corresponds to our operating assumptionsgiven in Section 3.
Those assumptions can beloosely connected to recognizing and exploitingdiscourse structure (e.g., Pang and Lee (2004),Taboada et al (2009)).
Our automatically gener-ated rationales can be potentially combined withother learning frameworks that can exploit anno-tator rationales, such as Zaidan and Eisner (2008).6 ConclusionsIn this paper, we explore methods to automaticallygenerate annotator rationales for document-levelsentiment classification.
Our study is motivatedby the desire to retain the performance gains ofrationale-enhanced learning models while elimi-nating the need for additional human annotationeffort.
By employing existing resources for sen-timent analysis, we can create automatic annota-tor rationales that are as good as human annotatorrationales in improving document-level sentimentclassification.AcknowledgmentsWe thank anonymous reviewers for their comments.
Thiswork was supported in part by National Science Founda-tion Grants BCS-0904822, BCS-0624277, IIS-0535099 andby the Department of Homeland Security under ONR GrantN0014-07-1-0152.340ReferencesJohn Blitzer, Mark Dredze, and Fernando Pereira.
2007.
Bi-ographies, bollywood, boom-boxes and blenders: Domainadaptation for sentiment classification.
In Proceedings ofthe 45th Annual Meeting of the Association of Computa-tional Linguistics, pages 440?447, Prague, Czech Repub-lic, June.
Association for Computational Linguistics.Thorsten Joachims.
1999.
Making large-scale support vectormachine learning practical.
pages 169?184.Yi Mao and Guy Lebanon.
2006.
Sequential models for sen-timent prediction.
In Proceedings of the ICML Workshop:Learning in Structured Output Spaces Open Problems inStatistical Relational Learning Statistical Network Analy-sis: Models, Issues and New Directions.Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike Wells,and Jeff Reynar.
2007.
Structured models for fine-to-coarse sentiment analysis.
In Proceedings of the 45thAnnual Meeting of the Association of Computational Lin-guistics, pages 432?439, Prague, Czech Republic, June.Association for Computational Linguistics.Bo Pang and Lillian Lee.
2004.
A sentimental education:sentiment analysis using subjectivity summarization basedon minimum cuts.
In ACL ?04: Proceedings of the 42ndAnnual Meeting on Association for Computational Lin-guistics, page 271, Morristown, NJ, USA.
Association forComputational Linguistics.Bo Pang and Lillian Lee.
2008.
Opinion mining and senti-ment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002.Thumbs up?
: sentiment classification using machinelearning techniques.
In EMNLP ?02: Proceedings of theACL-02 conference on Empirical methods in natural lan-guage processing, pages 79?86, Morristown, NJ, USA.Association for Computational Linguistics.Robert E. Schapire, Marie Rochery, Mazin G. Rahim, andNarendra Gupta.
2002.
Incorporating prior knowledgeinto boosting.
In ICML ?02: Proceedings of the Nine-teenth International Conference on Machine Learning,pages 538?545, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.Maite Taboada, Julian Brooke, and Manfred Stede.
2009.Genre-based paragraph classification for sentiment anal-ysis.
In Proceedings of the SIGDIAL 2009 Conference,pages 62?70, London, UK, September.
Association forComputational Linguistics.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions in lan-guage.
Language Resources and Evaluation, 1(2):0.Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Ja-son Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie,Ellen Riloff, and Siddharth Patwardhan.
2005a.
Opinion-finder: a system for subjectivity analysis.
In Proceedingsof HLT/EMNLP on Interactive Demonstrations, pages 34?35, Morristown, NJ, USA.
Association for ComputationalLinguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b.Recognizing contextual polarity in phrase-level sentimentanalysis.
In HLT-EMNLP ?05: Proceedings of the con-ference on Human Language Technology and EmpiricalMethods in Natural Language Processing, pages 347?354, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Xiaoyun Wu and Rohini Srihari.
2004.
Incorporatingprior knowledgewith weighted margin support vector ma-chines.
In KDD ?04: Proceedings of the tenth ACMSIGKDD international conference on Knowledge discov-ery and data mining, pages 326?333, New York, NY,USA.
ACM.Omar F. Zaidan and Jason Eisner.
2008.
Modeling anno-tators: a generative approach to learning from annotatorrationales.
In EMNLP ?08: Proceedings of the Confer-ence on Empirical Methods in Natural LanguageProcess-ing, pages 31?40, Morristown, NJ, USA.
Association forComputational Linguistics.Omar F. Zaidan, Jason Eisner, and Christine Piatko.
2007.Using ?annotator rationales?
to improve machine learningfor text categorization.
In NAACLHLT 2007; Proceedingsof the Main Conference, pages 260?267, April.341
