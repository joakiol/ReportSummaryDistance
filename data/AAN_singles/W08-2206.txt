How Well Do SemanticRelatedness Measures Perform?A Meta-StudyIrene CramerDortmund University of Technology (Germany)email: irene.cramer@udo.eduAbstractVarious semantic relatedness, similarity, and distance measures have beenproposed in the past decade and many NLP-applications strongly rely onthese semantic measures.
Researchers compete for better algorithms andnormally only few percentage points seem to suffice in order to prove anew measure outperforms an older one.
In this paper we present a meta-study comparing various semantic measures and their correlation withhuman judgments.
We show that the results are rather inconsistent andask for detailed analyses as well as clarification.
We argue that the defini-tion of a shared task might bring us considerably closer to understandingthe concept of semantic relatedness.5960 Cramer1 IntroductionVarious applications in Natural Language Processing, such as Question Answering(Novischi and Moldovan, 2006), Topic Detection (Carthy, 2004), and Text Summa-rization (Barzilay and Elhadad, 1997), rely on semantic relatedness (similarity or dis-tance)1 measures either based on word nets and/or corpus statistics as a resource.
Inthe HyTex project, funded by the German Research Foundation, we develop strategiesfor the text-to-hypertext conversion using text-grammatical features.
One strand ofresearch in this project consists of topic-based linking methods using lexical chain-ing as a resource (Cramer and Finthammer, 2008).
Lexical chaining is a well-knownmethod to calculate partial text representations; it relies on semantic relatedness val-ues as basic input.
We therefore implemented2 eight semantic relatedness measures?
(Hirst and St-Onge, 1998; Jiang and Conrath, 1997; Leacock and Chodorow, 1998;Lin, 1998; Resnik, 1995; Wu and Palmer, 1994) ?
based on GermaNet3 Lemnitzerand Kunze (2002) and three based on Google co-occurrence counts (Cilibrasi and Vi-tanyi, 2007).
In order to evaluate the performance of these measures we conductedtwo human judgment experiments and computed the correlation between the humanjudgment and the values of the eleven semantic measures.
We also compared ourresults with those reported in the literature and found that the correlations betweenhuman judgments and semantic measures are extremely scattered.
In this paper wecompare the correlation of our own human judgment experiments and the results ofthree similar studies.
In our opinion this comparison points to the necessity of a thor-ough analysis of the methods used in these experiments.
We argue that this analysisshould aim at answering the following questions:?
How does the setting of the human judgment experiment influence the results??
How does the selection of the word-pairs influence the results??
Which aspects of semantic relatedness are included in human judgments?
Thus,what do these experiments actually measure??
Are the semantic relatedness measures proposed in the literature able to captureall of these aspects?In this paper we intend to open the above mentioned analysis and therefore assembleda set of aspects which we consider to be important in order to answer these questions.Consequently, the remainder of this paper is structured as follows: In Section 2 we1The notions of semantic relatedness, similarity, and distance measure are controversially discussed inthe literature, e.g.
Budanitsky and Hirst (2006).
However, semantic similarity and relatedness seem to bethe predominant terms in this context.
Budanitsky and Hirst (2006) define them as follows: word-pairs areconsidered to be semantically similar if a synonymy or hypernymy relation holds.
In contrast, word-pairsare considered to be semantically related if a systematic relation, such as synonymy, antonymy, hypernymy,holonymy, or an unsystematic relation holds.
Thus relatedness is the more general (broader) concept sinceit includes intuitive associations as well as linguistically formalized relations between words (or concepts).The focus of this paper is on relatedness.2Since GermaNet ?
e.g.
in terms of internal structure ?
slightly differs from Princeton WordNet wecould not simply use the measure implementations of the latter and therefore had to reimplement and adaptthem for GermaNet.3GermaNet is the German counterpart of WordNet (Fellbaum, 1998).How Well Do Semantic Relatedness Measures Perform?
A Meta-Study 61present our own human judgment experiments.
In Section 3 we describe three simi-lar studies, two conducted with English data and one with German.
In Section 4 wecompare the results of the four studies and discuss (with respect to the experimentalsetting and goals) potential differences and possible causes for the observed inconsis-tency of the results.
Finally, we summarize our work and outline some ideas for futureresearch.2 Our Human Judgement ExperimentsIn order to evaluate the quality of a semantic measure, a set of pre-classified (i.e.judged with respect to their semantic relatedness by subjects) word-pairs is neces-sary.
In previous work for English data, most researchers used the word-pair list byRubenstein and Goodenough (1965) as well as the list by Miller and Charles (1991)as an evaluation resource.
For German there are ?
to our knowledge ?
two researchgroups, which compiled lists of word-pairs with respective human judgment:?
Gurevych et al constructed three lists (a translation of Rubenstein and Good-enough?s list (Gurevych, 2005), a manually generated set of word-pairs, and asemi-automatically generated one (Zesch and Gurevych, 2006)).?
While investigating lexical chaining for German corpora, we additionally com-piled a total of six lists, each of which consists of 100 word-pairs with respectivehuman judgments.The goal of our experiments was to cover a wide range of relatedness types, i.e.
sys-tematic and unsystematic relations, and relatedness levels, i.e.
various degrees of re-lation strength.
However, we only included nouns in the construction of our setsof word-pairs, since we consider cross-part-of-speech (cross-POS) relations to be anadditional challenge4, which we intend to address in a continuative experiment.
Fur-thermore, in order to identify a potential bias of the lists and the impact of this bias onthe results, we applied two different methods for the compilation of word-pairs.For our first human judgment experiment (Cramer and Finthammer, 2008) we col-lected nouns (analytically)5 of diverse semantic classes, e.g.
abstract nouns, such asdas Wissen (Engl.
knowledge), and concrete nouns, such as das B?geleisen (Engl.flat-iron).
By this means, we constructed a list of approximately 300 word-pairs.
Wepicked approximately 75 and randomized them.
For the remaining 25 word-pairs, weselected five words and constructedword-pairs such as Sonne-Wind (Engl.
sun-wind),Sonne-W?rme (Engl.
sun-warmth), Sonne-Wetter (Engl.
sun-weather) etc.
We ar-ranged these 25 pairs into sequences in order to focus our subjects?
attention on smallsemantic relatedness distinctions.4Since in most word nets cross-POS relations are very sparse, researchers currently investigate relationtypes able to connect the noun, verb, and adjective sub-graphs (e.g.
Marrafa and Mendes (2006) or Lem-nitzer et al (2008)).
However, these new relations are not yet integrated on a large scale and thereforeshould not (or even cannot) be used in semantic relatedness measures.
Furthermore, calculating seman-tic relatedness between words with different POS might introduce additional challenges potentially as yetunidentified, which calls for a careful exploration.5In this paper and in most comparable studies, the term analytical means that the word-pairs are hand-picked.
Obviously, the disadvantage of this approach is its sensibility to idiosyncrasies, which might ex-tremely bias the outcome of the experiments.62 CramerFor the five remaining lists (WP2-WP6), we applied a different method: firstly,we again analytically collected word-pairs which are part of collocations, i.e.
thetwo nouns Rat and Tat (mit Rat und Tat helfen, Engl.
to help with words anddeeds) or Qual and Wahl (die Qual der Wahl haben, Engl.
to be spoilt for choice).Secondly, we assembled word-pairs which feature association relations, i.e.
Afrika(Engl.
Africa) and Tiger (Engl.
tiger) or Weihnachten (Engl.
Christmas) and Zimt(Engl.
cinnamon).
Thirdly, we automatically constructed a list of random word-pairsusing the Wacky corpus (Baroni and Bernardini, 2006) as a resource and manuallyexcluded ad-hoc-constructions.
Finally, out of these three resources we compiled fivesets of 100 randomized word-pairs with no more than 20% of the collocation andassociation word-pairs.We asked subjects to rate the word-pairs on a 5-level scale (0 = not related to 4= strongly related).
The subjects were instructed to base the rating on their intuitionabout any kind of conceivable relation between the two words.
WP1 was rated by 35subjects and WP2 to WP6 were each rated by 15 subjects.
We then calculated theaverage judgment per word-pair and ranked the word-pairs accordingly.The correlation between the human judgments and the eleven semantic measuresis shown in Table 1.
The difference between the correlation coefficients of WP1 andWP2-WP6 suggests that the method of construction might have an impact on the re-sults of the experiments.
The manual compilation of word-pairs seems to lead tobetter correlation coefficients and might therefore cause an overestimation of the per-formance of the semantic measures.
Furthermore, with respect to the list constructionmethods, the two resources and respective measures, namely GermaNet (TreePath?Lin) and Google (GoogleQ?GooglePMI), seem to respond differently: whereas thecorrelation coefficients of the eight GermaNet based measures drop to a greater orlesser extend (Table 1: r for WP1 and r for WP2-WP6), the correlation coefficients ofthe three Google based measures approximately level off.Table 1: Our Correlation Coefficients: Correlation between AverageHuman Judgmentand Semantic Measure Valuesr Tree Graph Wu- Leac.- Hirst- Resnik Jiang- Lin Google Google GooglePath Path Palm.
Chod.
St-O.
Conr.
Norm.
Quot.
PMIWP1 0.41 0.42 0.36 0.48 0.47 0.44 0.45 0.48 0.27 0.37 0.37WP2 0.09 0.31 0.33 0.16 0.26 0.37 0.18 0.36 0.24 0.29 0.27WP3 0.03 0.22 0.24 0.11 0.28 0.19 0.15 0.26 0.46 0.45 0.40WP4 0.07 0.39 0.11 0.11 0.31 0.11 0.25 0.16 0.34 0.38 0.34WP5 0.27 0.39 0.26 0.32 0.38 0.31 0.41 0.34 0.19 0.32 0.28WP6 0.09 0.27 0.15 0.17 0.39 0.24 0.29 0.25 0.26 0.38 0.43mean 0.16 0.33 0.24 0.23 0.35 0.28 0.29 0.31 0.29 0.36 0.35In any case, since the correlation coefficients are rather low, there is much room forimprovement.
However, as all measures scatter in the same range ?
independentlyof the precise algorithm or resource used, as it seems ?
we argue that the reason forthis critical performance might be one of the following two aspects (most probably acombination of both):How Well Do Semantic Relatedness Measures Perform?
A Meta-Study 63?
Word nets (and/or corpora) do not cover the (all) types of semantic informationrequired.?
Human judgment experiments are (without clear and standardized specificationof the experimental setup) an inappropriate way to evaluate semantic measures.Both aspects are discussed in Section 4.
However, we first should have a look at threesimilar studies, two for English and one for German.3 Three Similar StudiesAs mentioned above various researchers rely on human judgment experiments as anevaluation resource for semantic relatedness measures.
In this section, three suchstudies are summarized in order to identify differences with respect to the methodsadopted and results obtained.63.1 Budanitsky and HirstBudanitsky and Hirst (2006) specify the purpose of their paper Evaluating WordNet-based Measures of Lexical Semantic Relatedness as a comparison of the performanceof various relatedness measures.
Accordingly, they sketch a number of measuresand identify three evaluation methods: firstly, the theoretical examination (of e.g.
themathematical properties of the respective measure); secondly, the comparison withhuman judgments; thirdly, the evaluation of a measure with respect to a given NLP-application.
They regard the second and third method as being the most appropriateones and therefore focus on them in their empirical work presented in the paper.
Asa basis for the second evaluation method, i.e.
the comparison between semantic mea-sure and human judgments, they use two word-pair lists: the first compiled by Ruben-stein and Goodenough (1965) and containing 65 word-pairs7, the second compiledby Miller and Charles (1991) and containing 30 word-pairs.
In order to evaluate theperformance of five different measures (and potentially in order to find a ranking),Budanitsky and Hirst (2006) compute the semantic relatedness values for the word-pairs and compare them with the human judgments.
They thus find the correlationcoefficients summarized in Table 2.Budanitsky and Hirst (2006) regard this evaluation method, i.e.
comparingmeasurevalues and human judgments, as the ideal approach.
However, in examining the resultsof this comparison, they identify several limitations; i.e.
they point out that the amountof data available (65 word-pairs) might be inadequate for real NLP-applications.
Theyadditionally emphasize that the development of a large-scale data set would be time-consuming and expensive.
Moreover, they argue that the experiments by Rubensteinand Goodenough (1965) as well as Miller and Charles (1991) focus on relations be-tween words rather than relations between word-senses (concepts), which would be6There are many more relevant studies; however, they all point to the same issue, namely, the incompat-ibility of the results.7Rubenstein and Goodenough (1965) investigated the relationship between ?similarity of context?
and?similarity of meaning?.
They asked 51 subjects to rate on a scale of 0 to 4 the similarity of meaning forthe 65 word-pairs.
Miller and Charles (1991) selected 30 out of the 65 original word-pairs (according totheir relatedness strength) and asked 38 subjects to rate this list.
They used the same experimental setup asRubenstein and Goodenough (1965).64 CramerTable 2: Correlation Coefficients by Budantisky and Hirstr Leac.- Hirst- Resnik Jiang- LinChod.
StO.
Conr.M&C 0.816 0.744 0.774 0.850 0.82R&G 0.838 0.786 0.779 0.781 0.819mean 0.83 0.77 0.78 0.82 0.82?
especially when taking potential NLP-applications into account ?
more appropri-ate.
They note that it might however be difficult to trigger a specific concept withoutbiasing the subjects.3.2 Boyd-Graber, Fellbaum, Osherson, and SchapireIn contrast to the above mentioned experiments by Budanitsky and Hirst (2006), theresearch reported in Adding Dense, Weighted Connections to WordNet aims at thedevelopment of a new, conceptually different layer of relations to be included intoa word net.
Boyd-Graber et al (2006) are motivated in their work by three widelyacknowledged shortcomings of word nets:?
The lack of cross-POS links connecting the sub-graphs containing nouns, verbs,or adjectives, respectively.?
The low density of relations in the sub-graphs, i.e.
potentially missing types ofrelations such as ?actor?
or ?instrument?.?
The absence of weights assigned to the relations, i.e.
representing the degreesof semantic distance of different subordinates of the same superordinate.In order to address these shortcomings, Boyd-Graber et al ask subjects to assignvalues of ?evocation?
representing the relations between 1,000 synsets.
They ask 20subjects to rate evocation in 120,000 pairs of synsets (these pairs form a random se-lection of all possible pairs of the above mentioned 1,000 core synsets considered inthe experiment).
The subjects are given a manual explaining a couple of details aboutthe task and are trained on a sample of 1,000 (two sets of 500) randomly selectedpairs.
Although the research objective of the work presented in this paper is to con-struct a new relations layer for Princeton WordNet rather than to evaluate semanticrelatedness measures, Boyd-Graber et al compare the results of their human judg-ment experiment with the relatedness values of four different semantic measures.
Thecorrelation coefficients of this comparison are summarized in Table 3.Boyd-Graber et al arrive at the conclusion that ?
given the obvious lack of cor-relation (see Table 3) ?
evocation constitutes an empirically supported semantic re-lation type which is still not captured by the semantic measures (at least not by thoseconsidered in this experiment).3.3 Gurevych et alSimilar to the study by Budanitsky and Hirst (2006), Gurevych (2005) gives insightinto a human judgment experiment conducted in order to compare the performanceHow Well Do Semantic Relatedness Measures Perform?
A Meta-Study 65Table 3: Correlation Coefficients by Boyd-Graber et alr Lesk Path LC LSAall 0.008verbs 0.046nouns 0.013 0.013closest 0.131Table 4: Correlation Coefficients by Gurevych (with Lesk1 = Lesk (DWDS); Lesk2= Lesk (radial); Lesk3 = Lesk (hypernym); Resn.
= Resnik)r Google Lesk1 Lesk2 Lesk3 Resn.R&G 0.57 0.53 0.55 0.60 0.72Germanof her own semantic relatedness measure8 with established ones.
For this purpose(Gurevych, 2005) translates the word-pair list by Rubenstein and Goodenough (1965)and asks 24 native speakers of German to rate the word-pairs with respect to their se-mantic relatedness on a 5-level scale; she thus replicates the study by Rubenstein andGoodenough (1965) for German.
Gurevych (2005) finally compares the human judg-ments with several semantic measures.
The correlation coefficients of this comparisonare summarized in Table 4.Gurevych (2005) comments on (among others) the following four issues: firstly, sheemphasizes the difference between semantic similarity and relatedness; she argues thatmost word-pair lists were constructed in order to measure semantic similarity ratherthan relatedness and that these lists might therefore be inappropriate for the task athand.
Secondly, Gurevych (2005) observes that, in contrast to the concept of seman-tic similarity, semantic relatedness is not well defined.
Thirdly, as the experimentsare based on words rather than concepts, the results attained thus far might exhibitadditional noise.
Finally, she notes that the amount of data is too limited in size andthat analytically created word-pair lists are inherently biased.
Accordingly, Zesch andGurevych (2006) propose a corpus based method for automatically constructing testdata and list a number of advantages of this approach: i.e.
lexical-semantic cohesionin texts accounts for various relation types, domain-specific and technical terms caneasily be included, and, in contrast to manually constructed, corpus based lists areprobably more objective.4 Meta-Level EvaluationTable 5 shows the minimum, maximum, and mean correlations reported in the threestudies as well as our own results.
The table illustrates the broad statistical spread:the mean correlation coefficients range between 0.8 and 0.04 for English and between0.61 and 0.29 for German.
Admittedly, the experimental setup and the goals of the8Her measure is able to manage limitations of some of the previously published measures.66 CramerTable 5: Comparison of the Correlation Coefficients of the Different Experiments(with B&G: Budanitsky and Hirst / B-G et al: Boyd-Graber et al / G et al: Gurevychet al / C&F, C: our results)B&H B-G et al G et al C&F, Cmax 0.83 0.131 0.72 0.36min 0.77 0.008 0.53 0.16mean 0.80 0.04 0.61 0.29stdv 0.03 0.05 0.08 0.06four studies differ in several aspects9.
However, the principle idea?
i.e.
using humanjudgments as a baseline or evaluation resource ?
is the same.We argue that ?
given the statistical spread shown in Table 5 ?
as long as thereasons for this discrepancy have not been determined and the methods have not beenharmonized as far as possible, the results of these experiments should not be used asa basis for e.g.
the evaluation or comparison of semantic measures.
As mentioned inSection 1 we suspect that (no fewer than) the following aspects influence the resultsof the human judgment experiments and thus the correlation between humans andsemantic measures:?
Research objective: The goals of the studies differ with respect to several as-pects.
Firstly, some studies, e.g.
Budanitsky and Hirst (2006), aim at comparingthe performance of different semantic (relatedness) measures, whereas Boyd-Graber et al (2006) intend to construct a new relations layer (potentially ableto substitute or complement established relatedness measures).
Secondly, insome cases, e.g.
Cramer and Finthammer (2008), relations between words areconsidered, whereas e.g.
Boyd-Graber et al (2006) examine relations betweenconcepts.
Thirdly, it seems to be unclear which types of relations are actuallysearched for (relatedness, similarity, evocation, distance) and in what aspectsthese correspond or differ.
Interestingly, in computational linguistics and psy-cholinguistics there is an additional strand of research investigating the so-called?association relation?, e.g.
Schulte im Walde and Melinger (2005) and Roth andSchulte im Walde (2008), which is not yet considered or integrated in the re-search on semantic relatedness measures.
We argue that such an integrationmight be fruitful for both research strands.?
Setting of the human judgment experiment: In all studies summarized above,the subjects are students (mostly of linguistics, computer sciences, and compu-tational linguistics).
In most cases, they are given a short manual explaining thetask, which certainly differs in many aspects, e.g.
due to the above mentionedfact that the relation type searched for is a still unsettled issue.
Furthermore, notraining phase is included in most of the studies except the one by Boyd-Graber9It seems unfeasible to determine all possible differences of the studies because, among other things, thepapers do not specify the experimental setup in detail.
We therefore assume that the definition of a sharedtask might bring us considerably closer to understanding the questions raised in this paper.How Well Do Semantic Relatedness Measures Perform?
A Meta-Study 67et al (2006), who are therefore able to identify potential training effects.
Againonly Boyd-Graber et al (2006) account for the handling of idiosyncrasies.?
Construction of experimental data: In Boyd-Graber et al (2006) the concept-pairs were randomly selected, whereas the word-pairs used by Budanitsky andHirst (2006) were constructed analytically.
In the studies by Gurevych (2005),Zesch and Gurevych (2006), and Cramer and Finthammer (2008), some wereanalytically constructed and some randomly (semi-automatically) selected.
Inaddition, the data sets vary with respect to their size: Budanitsky and Hirst(2006), Gurevych (2005), and Cramer and Finthammer (2008) only use smallsets of word-pairs (concept-pairs), i.e.
a few hundred pairs, whereas Boyd-Graber et al (2006) investigate a huge amount of data; their experiment there-fore certainly constitutes the most representative one.
All studies also indicatethe (mean/median) inter-subject correlation10 which varies from 0.48 (concept-pair based) in Zesch and Gurevych (2006) and 0.72 (concept-pair based) inBoyd-Graber et al (2006) to 0.85 (word-pair based) in Budanitsky and Hirst(2006).We think that this comparison of the various experiments points to two aspects whichprobably cause the large statistical spread shown in Table 5: the selection of the word-pairs (concept-pairs) and the type of relation (relatedness, similarity, evocation, dis-tance).
We assume that it should be possible to condense the comparison into one(more or less simple) rule: the narrower the relation concept (similarity < relatedness< evocation) and the narrower the data considered (lexical semantic selection rule <any kind of selection rule < random selection) the better the correlation between hu-man judgment and semantic measure11.
In any case, it seems essential to determinewhich relation types the subjects (knowingly or unknowingly) bear in mind when theyjudge word-pairswith respect to semantic relatedness.
In order to achieve this goal andbe able to integrate all relevant relations into the resources used for calculating seman-tic relatedness, the human judgments collected in the above-mentioned studies shouldbe dissected into components (i.e.
components for which systematic/unsystematic lex-ical semantic relations account etc.
); such a decomposition certainly also helps rendermore precisely the definition of semantic relatedness.Furthermore, it is ?
in our opinion ?
an unsettled issue whether the three typesof semantic relation at hand, thus the relations1.
represented in a word net or corpus (both computed via semantic measure),2. existing between any given word-pair in a text (which is mostly relevant forNLP-applications),3. and the one assigned by subjects in a human judgment experiment10The inter-subject correlation depends on various parameters, e.g.
the complexity of the task, the sub-jects (and their background, age, etc.)
as well as the experimental setup (task definition, training phase,etc.).11...
and obviously the easier the task!68 Cramercorrespond at all.
In principle, word nets, corpus statistics, and human judgmentsshould be related to (theoretically even represent) the (at least partially) shared knowl-edge of humans about the underlying ?lexical semantic system?, whereas relations be-tween words in a concrete text represent an instantiation of a system.
From this pointof view, at least the human judgments should correspond to the semantics encoded ina word net (or corpus statistics).
Instead of using human judgments as an evaluationresource (for e.g.
word net based semantic measures), they might as well be directlyintegrated into the word net as a (preferably dense) layer of (potentially cognitivelyrelevant, weighted but unlabeled) semantic relations, which is best adopted in Boyd-Graber et al (2006), as summarized in Section 3.
This approach has several advan-tages: firstly, the calculation of a semantic relatedness value is ?
given such a layer?
trivial, since it merely consists in a look-up procedure.
Secondly, NLP-applicationsusing word nets as a resource would certainly benefit from the thus enhanced densityof relations, i.e.
cross-POS relations.
Thirdly, an elaborate and standardized experi-mental setup for human judgment experiments could be used for the construction ofsuch a layer in different languages (and domains) and would also guarantee the mod-eling quality.
Finally, such a new word net layer would hopefully resolve the abovementioned open issue of the diverging correlation coefficients.Alternatively, since it is completely unclear if the evocation relation can really actas a substitute for classical semantic relatedness measures in NLP-applications, cur-rent word nets should be enhanced by systematically augmenting existing relationtypes and integrating new ones.
On that condition and given that a common evalua-tion framework exists, it should be possible to determine which semantic relatednessmeasure performs best under what conditions.Last but not least, in order to determine the relation between an underlying seman-tic system (represented by a semantic measure or as mentioned above the evocationlayer) and the instantiation of this system in a concrete text, a study similar to the onereported in Zesch and Gurevych (2006) should be conducted.
Such a study proba-bly also shows if the evocation relation is able to substitute (or at least complement)semantic relatedness measures typically used in NLP-applications.5 Conclusions and Future WorkWe have presented our own human judgment experiments for German and comparedthem with three similar studies.
This comparison illustrates that the results of thesestudies are incompatible.
We therefore argue that the experimental setup should beclarified and, if possible, harmonized.
We also think that the notion of associationshould be considered carefully, since it is an established concept for measuring relatedphenomena in several psycholinguistic and computational linguistic communities.We now plan to continue our work on three levels.
Firstly, we intend to conduct astudy similar to the one reported by Boyd-Graber et al (2006) with a small amountof German data.
We hope that this will provide us with insight into some of the openissues mentioned in Section 1 and Section 4.
Secondly, we plan to investigate if theevocation relation is able to substitute the semantic relatedness measures typicallyused in lexical chaining and similar NLP-applications.
Thirdly, we intend to run ex-periments using the database of noun associations in German constructed by Melingerand Weber (2006) as a resource for the evaluation of semantic relatedness measures.How Well Do Semantic Relatedness Measures Perform?
A Meta-Study 69Acknowledgements The author would like to thankMichael Bei?wenger, ChristianeFellbaum, Sabine Schulte im Walde, Angelika Storrer, Tonio Wandmacher, TorstenZesch, and the anonymous reviewers for their helpful comments.
This research wasfunded by the DFG Research Group 437 (HyTex).ReferencesBaroni, M. and S. Bernardini (Eds.)
(2006).
Wacky!
Working papers on the Web asCorpus.
GEDIT, Bologna.Barzilay, R. and M. Elhadad (1997).
Using lexical chains for text summarization.
InProceedings of the Intelligent Scalable Text Summarization Workshop, pp.
10?17.Boyd-Graber, J., C. Fellbaum, D. Osherson, and R. Schapire (2006).
Adding dense,weighted, connections to wordnet.
In Proceedings of the 3rd Global WordNet Meet-ing, pp.
29?35.Budanitsky, A. and G. Hirst (2006).
Evaluating wordnet-based measures of semanticrelatedness.
Computational Linguistics 32 (1), 13?47.Carthy, J.
(2004).
Lexical chains versus keywords for topic tracking.
In Compu-tational Linguistics and Intelligent Text Processing, Lecture Notes in ComputerScience, pp.
507?510.
Springer.Cilibrasi, R. and P. M. B. Vitanyi (2007).
The google similarity distance.
IEEETransactions on Knowledge and Data Engineering 19(3), 370?383.Cramer, I. and M. Finthammer (2008).
An evaluation procedure for word net basedlexical chaining: Methods and issues.
In Proceedings of the 4th Global WordNetMeeting, pp.
120?147.Fellbaum, C. (1998).
WordNet.
An Electronic Lexical Database.
The MIT Press.Gurevych, I.
(2005).
Using the structure of a conceptual network in computing se-mantic relatedness.
In Proceedings of the IJCNLP 2005, pp.
767?778.Hirst, G. and D. St-Onge (1998).
Lexical chains as representation of context for thedetection and correction malapropisms.
In C. Fellbaum (Ed.
), WordNet: An Elec-tronic Lexical Database, pp.
305?332.
The MIT Press.Jiang, J. J. and D. W. Conrath (1997).
Semantic similarity based on corpus statisticsand lexical taxonomy.
In Proceedings of ROCLING X, pp.
19?33.Leacock, C. and M. Chodorow (1998).
Combining local context and wordnet simi-larity for word sense identification.
In C. Fellbaum (Ed.
), WordNet: An ElectronicLexical Database, pp.
265?284.
The MIT Press.Lemnitzer, L. and C. Kunze (2002).
Germanet - representation, visualization, appli-cation.
In Proceedings of the 4th Language Resources and Evaluation Conference,pp.
1485?1491.70 CramerLemnitzer, L., H. Wunsch, and P. Gupta (2008).
Enriching germanet with verb-nounrelations - a case study of lexical acquisition.
In Proceedings of the 6th Interna-tional Language Resources and Evaluation.Lin, D. (1998).
An information-theoretic definition of similarity.
In Proceedings ofthe 15th International Conference on Machine Learning, pp.
296?304.Marrafa, P. and S. Mendes (2006).
Modeling adjectives in computational relationallexica.
In Proceedings of the COLING/ACL 2006 poster session, pp.
555?562.Melinger, A. and A. Weber (2006).
A database of noun associations in german.
On-line available database: http://www.coli.uni-saarland.de/projects/nag.Miller, G. A. and W. G. Charles (1991).
Contextual correlates of semantic similiarity.Language and Cognitive Processes 6(1), 1?28.Novischi, A. and D. Moldovan (2006).
Question answering with lexical chains prop-agating verb arguments.
In Proceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meeting of the Association for Com-putational Linguistics, pp.
897?904.Resnik, P. (1995).
Using information content to evaluate semantic similarity in ataxonomy.
In Proceedings of the IJCAI 1995, pp.
448?453.Roth, M. and S. Schulte im Walde (2008).
Corpus co-occurrence, dictionary andwikipedia entries as resources for semantic relatedness information.
In Proceed-ings of the 6th Conference on Language Resources and Evaluation.Rubenstein, H. and J.
B. Goodenough (1965).
Contextual correlates of synonymy.Communications of the ACM 8(10), 627?633.Schulte im Walde, S. and A. Melinger (2005).
Identifying semantic relations andfunctional properties of human verb associations.
In Proceedings of Human Lan-guage Technology Conference and Conference on Empirical Methods in NaturalLanguage Processing, pp.
612?619.Wu, Z. and M. Palmer (1994).
Verb semantics and lexical selection.
In Proceedingsof the 32nd Annual Meeting of the Association for Computational Linguistics, pp.133?138.Zesch, T. and I. Gurevych (2006).
Automatically creating datasets for measures ofsemantic relatedness.
In Proceedings of the Workshop on Linguistic Distances atCOLING/ACL 2006, pp.
16?24.
