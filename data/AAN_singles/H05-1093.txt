Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 740?747, Vancouver, October 2005. c?2005 Association for Computational LinguisticsBLANC1: Learning Evaluation Metrics for MTLucian Vlad Lita and Monica Rogati and Alon LavieCarnegie Mellon University{llita,mrogati,alavie}@cs.cmu.eduAbstractWe introduce BLANC, a family of dy-namic, trainable evaluation metrics for ma-chine translation.
Flexible, parametrizedmodels can be learned from past data andautomatically optimized to correlate wellwith human judgments for different cri-teria (e.g.
adequacy, fluency) using dif-ferent correlation measures.
Towards thisend, we discuss ACS (all common skip-ngrams), a practical algorithm with train-able parameters that estimates reference-candidate translation overlap by comput-ing a weighted sum of all common skip-ngrams in polynomial time.
We show thatthe BLEU and ROUGE metric families arespecial cases of BLANC, and we comparecorrelations with human judgments acrossthese three metric families.
We analyze thealgorithmic complexity of ACS and arguethat it is more powerful in modeling bothlocal meaning and sentence-level structure,while offering the same practicality as theestablished algorithms it generalizes.1 IntroductionAlthough recent MT evaluation methods showpromising correlations to human judgments in termsof adequacy and fluency, there is still considerableroom for improvement (Culy and Riehemann, 2003).Most of these studies have been performed at a sys-tem level and have not investigated metric robust-ness at a lower granularity.
Moreover, even thoughthe emphasis on adequacy vs. fluency is application-dependent, automatic evaluation metrics do not dis-tinguish between the need to optimize correlationwith regard to one or the other.Machine translation automatic evaluation metricsface two important challenges: the lack of powerfulfeatures to capture both sentence level structure andlocal meaning, and the difficulty of designing goodfunctions for combining these features into meaning-ful quality estimation algorithms.In this paper, we introduce BLANC1, an automaticMT evaluation metric family that is a generaliza-tion of popular and successful metric families cur-rently used in the MT community (BLEU, ROUGE, F-measure etc.).
We describe an efficient, polynomial-time algorithm for BLANC, and show how it can beoptimized to target adequacy, fluency or any othercriterion.
We compare our metric?s performancewith traditional and recent automatic evaluation met-rics.
We also describe the parameter conditions underwhich BLANC can emulate them.Throughout the remainder of this paper, we dis-tinguish between two components of automatic MTevaluation: the statistics computed on candidateand reference translations and the function used indefining evaluation metrics and generating transla-tion scores.
Commonly used statistics include bag-of-words overlap, edit distance, longest common sub-sequence, ngram overlap, and skip-bigram overlap.Preferred functions are various combinations of pre-cision and recall (Soricut and Brill, 2004), including1Since existing evaluation metrics (e.g.
BLEU, ROUGE) arespecial cases of our metric family, it is only natural to name itBroad Learning and Adaptation for Numeric Criteria (BLANC) ?white light contains light of all frequencies740weighted precision and F-measures (Van-Rijsbergen,1979).BLANC implements a practical algorithm withlearnable parameters for automatic MT evaluationwhich estimates the reference-candidate translationoverlap by computing a weighted sum of commonsubsequences (also known as skip-ngrams).
Com-mon skip-ngrams are sequences of words in theirsentence order that are found both in the referenceand candidate translations.
By generalizing and sep-arating the overlap statistics from the function usedto combine them, and by identifying the latter as alearnable component, BLANC subsumes the ngrambased evaluation metrics as special cases and canbetter reflect the need of end applications for ade-quacy/fluency tradeoffs .1.1 Related WorkInitial work in evaluating translation quality focusedon edit distance-based metrics (Su et al, 1992; Akibaet al, 2001).
In the MT context, edit distance (Lev-enshtein, 1965) represents the amount of word inser-tions, deletions and substitutions necessary to trans-form a candidate translation into a reference trans-lation.
Another evaluation metric based on edit dis-tance is the Word Error Rate (Niessen et al, 2000)which computes the normalized edit distance.
BLEUis a weighted precision evaluation metric introducedby IBM (Papineni et al, 2001).
BLEU and its exten-sions/variants (e.g.
NIST (Doddington, 2002)) havebecome de-facto standards in the MT community andare consistently being used for system optimizationand tuning.
These methods rely on local featuresand do not explicitly capture sentence-level features,although implicitly longer n-gram matches are re-warded in BLEU.
The General Text Matcher (GTM)(Turian et al, 2003) is another MT evaluation methodthat rewards longer ngrams instead of assigning themequal weight.
(Lin and Och, 2004) recently proposed a set ofmetrics (ROUGE) for MT evaluation.
ROUGE-L is alongest common subsequence (LCS) based automaticevaluation metric for MT.
The intuition behind it isthat long common subsequences reflect a large over-lap between a candidate translation and a referencetranslation.
ROUGE-W is also based on LCS, butassigns higher weights to sequences that have fewergaps.
However, these metrics still do not distinguishamong translations with the same LCS but differentnumber of shorter sized subsequences, also indica-tive of overlap.
ROUGE-S attempts to correct thisproblem by combining the precision/recall of skip-bigrams of the reference and candidate translations.However, by using skip-ngrams with n?=2, we mightbe able to capture more information encoded in thehigher level sentence structure.
With BLANC, wepropose a way to exploit local contiguity in a man-ner similar to BLEU and also higher level structuresimilar to ROUGE type metrics.2 ApproachWe have designed an algorithm that can perform afull overlap search over variable-size, non-contiguousword sequences (skip-ngrams) efficiently.
At firstglance, in order to perform this search, one has tofirst exhaustively generate all skip-ngrams in the can-didate and reference segments and then assess theoverlap.
This approach is highly prohibitive since thenumber of possible sequences is exponential in thenumber of words in the sentence.
Our algorithm ?ACS (all common skip-ngrams) ?
directly constructsthe set of overlapping skip-ngrams through incremen-tal composition of word-level matches.
With ACS,we can reduce computation complexity to a fifth de-gree polynomial in the number of words.Through the ACS algorithm, BLANC is not limitedonly to counting skip-ngram overlap: the contribu-tion of different skip-ngrams to the overall score isbased on a set of features.
ACS computes the over-lap between two segments of text and also allowslocal and global features to be computed during theoverlap search.
These local and global features aresubsequently used to train evaluation models withinthe BLANC family.
We introduce below several sim-ple skip-ngram-based features and show that special-case parameter settings for these features emulate thecomputation of existing ngram-based metrics.
In or-der to define the relative significance of a particularskip-ngram found by the ACS algorithm, we employan exponential model for feature integration.2.1 Weighted Skip-NgramsWe define skip-ngrams as sequences of n words takenin sentence order allowing for arbitrary gaps.
In algo-rithms literature skip-ngrams are equivalent to subse-quences.
As special cases, skip-ngrams with n=2 are741referred to as skip-bigrams and skip-ngrams with nogaps between the words are simply ngrams.
A sen-tence S of size |S| has C(|S|, n) = |S|!(|S|?n)!n!
skip-ngrams.For example, the sentence ?To be or not to be?
hasC(6, 2) = 15 corresponding skip-bigrams including?be or?, ?to to?, and three occurrences of ?to be?.It also has C(6, 4) = 15 corresponding skip-4grams(n = 4) including ?to be to be?
and ?to or not to?.Consider the following sample reference and can-didate translations:R0: machine translated text is evaluated automaticallyK1: machine translated stories are chosen automaticallyK2: machine and human together can forge a friendship thatcannot be translated into words automaticallyK3: machine code is being translated automaticallyThe skip-ngram ?machine translated automati-cally?
appears in both the reference R0 and all candi-date translations.
Arguably, a skip-bigram that con-tains few gaps is likely to capture local structureor meaning.
At the same time, skip-ngrams spreadacross a sentence are also very useful since they maycapture part of the high level sentence structure.We define a weighting feature function for skip-ngrams that estimates how likely they are to capturelocal meaning and sentence structure.
The weightingfunction ?
for a skip-ngram w1 ..wn is defined as:?
(w1..wn) = e??
?G(w1..wn) (1)where ?
?
0 is a decay parameter and G(w1..wn)measures the overall gap of the skip-ngram w1..wn ina specific sentence.
This overall skip-ngram weightcan be decomposed into the weights of its constituentskip-bigrams:?
(w1..wn) = e??
?G(w1,..,wn) (2)= e??
?Pn?1i=1 G(wi,wi+1)=n?1?i=1?
(wi wi+1) (3)In equation 3, ?
(wi wi+1) is the number of wordsbetween wi and wi+1 in the sentence.
In the exampleabove, the skip-ngram ?machine translated automat-ically?
has weight e?3?
for sentence K1 and weighte?12?
= 1 for sentence K2.In our initial experiments the gap G has been ex-pressed as a linear function, but different families offunctions can be explored and their corresponding pa-rameters learned.
The parameter ?
dictates the be-havior of the weighting function.
When ?
= 0 ?equals e0 = 1, rendering gap sizes irrelevant.
In thiscase, skip-ngrams are given the same weight as con-tiguous ngrams.
When ?
is very large, ?
approaches0 if there are any gaps in the skip-ngram and is 1 ifthere are no gaps.
This setting has the effect of con-sidering only contiguous ngrams and discarding allskip-ngrams with gaps.In the above example, although the skip-ngram?machine translated automatically?
has the same cu-mulative gap in both in K1 and K3, the occurrence inK1 has is a gap distribution that more closely reflectsthat of the reference skip-ngram in R0.
To model gapdistribution differences between two occurrences of askip-ngram, we define a piece-wise distance function?XY between two sentences x and y.
For two succes-sive words in the skip-ngram, the distance function isdefined as:?XY (w1w2) = e??
?|GX(w1,w2)?GY (w1,w2)| (4)where ?
?
0 is a decay parameter.
Intuitively, the?
parameter is used to reward better aligned skip-ngrams.
Similar to the ?
function, the overall ?XYdistance between two occurrences of a skip-ngramwith n > 1 is:?XY (w1..wn) =n?1?i=1?XY (wiwi+1) (5)Note that equation 5 takes into account pairs of skip-ngrams skip in different places by summing overpiecewise differences.
Finally, using an exponen-tial model, we assign an overall score to the matchedskip-ngram.
The skip-ngram scoring function Sxy al-lows independent features to be incorporated into theoverall score:Sxy(wi..wk) = ?
(wi..wk) ?
?xy(wi..wk)?e?1f1(wi..wk) ?
... ?
e?hfh(wi..wk) (6)where features f1..fh can be functions based on thesyntax, semantics, lexical or morphological aspectsof the skip-ngram.
Note that different models forcombining skip-ngram features can be used in con-junction with ACS.7422.2 Multiple ReferencesIn BLANC we incorporate multiple references in amanner similar to the ROUGE metric family.
Wecompute the precision and recall of each size skip-ngrams for individual references.
Based on these wecombine the maximum precision and maximum re-call of the candidate translation obtained using allreference translations and use them to compute an ag-gregate F-measure.The F-measure parameter ?F is modeled byBLANC.
In our experiments we optimized ?F indi-vidually for fluency and adequacy.2.3 The ACS AlgorithmWe present a practical algorithm for extracting AllCommon Skip-ngrams (ACS) of any size that appearin the candidate and reference translations.
For clar-ity purposes, we present the ACS algorithm as itrelates to the MT problem: find all common skip-ngrams (ACS) of any size in two sentences X and Y :wSKIP ?
Acs(?, ?,X, Y ) (7)= {wSKIP1..wSKIPmin(|X|,|Y |)} (8)where wSkipn is the set of all skip-ngrams of size nand is defined as:wSKIPn = {?w1..wn?
| wi ?
X,wi ?
Y,?i ?
[1..n]and wi ?
wj ,?i < j ?
[1..n]}Given two sentences X and Y we observe a match(w, x, y) if word w is found in sentence X at index xand in sentence Y at index y:(w, x, y) ?
{0 ?
x ?
|X|, 0 ?
y ?
|Y |,w ?
V, and X[x] = Y [y] = w} (9)where V is the vocabulary with a finite set of words.In the following subsections, we present the fol-lowing steps in the ACS algorithm:1. identify all matches ?
find matches and generatecorresponding nodes in the dependency graph2.
generate dependencies ?
construct edges ac-cording to pairwise match dependencies3.
propagate common subsequences ?
countall common skip-ngrams using correspondingweights and distancesIn the following sections we use the following exam-ple to illustrate the intermediate steps of ACS.X.
?to be or not to be?Y.
?to exist or not be?2.3.1 Step 1: Identify All MatchesIn this step we identify all word matches (w, x, y)in sentences X and Y .
Using the example above, theintermediate inputs and outputs of this step are:Input: X.
?to be or not to be?Y.
?to exist or not be?Output: (to,1,1); (to,5,1); (or,3,3); (be,2,5); .
.
.For each match we create a corresponding node Nin a dependency graph.
With each node we associatethe actual word matched and its corresponding indexpositions in both sentences.2.3.2 Step 2: Generate DependenciesA dependency N1 ?
N2 occurs when the twocorresponding matches (w1, x1, y1) and (w2, x2, y2)can form a valid common skip-bigram: i.e.
whenx1 < x2 and y1 < y2.
Note that the matches cancover identical words, but their indices cannot be thesame (x1 6= x2 and y1 6= y2) since a skip-bigramrequires two different word matches.In order to facilitate the generation of all commonsubsequences, the graph is populated with theappropriate dependency edges:for each node N in DAGfor each node M 6=N in DAGif N(x)?M(x) and N(y)?M(y)create edge E: N?Mcompute ?XY (E)compute ?
(E)This step incorporates the concepts of skip-ngramweight and distance into the graph.
With each edgeE : N1 ?
N2 we associate step-wise weight and dis-tance information for the corresponding skip-bigramformed by matches (w1, x1, y1) and (w2, x2, y2).Note that rather than counting all skip-ngrams,which would be exponential in the worst case sce-nario, we only construct a structure of match depen-dencies (i.e.
skip-bigrams).
As in dynamic program-ming, in order to avoid exponential complexity, wecompute individual skip-ngram scores only once.2.3.3 Step 3: Propagate Common SubsequencesIn this last step, the ACS algorithm counts all com-mon skip-ngrams using corresponding weights anddistances.
In the general case, this step is equiva-lent measuring the overlap of the two sentences Xand Y .
As a special case, if no features are used, the743ACS algorithm is equivalent to counting the numberof common skip-ngrams regardless of gap sizes.// depth first search (DFS)for each node N in DAGcompute node N?s depth// initialize skip-ngram countsfor each node N in DAGvN [1]?
1for i=2 to LCS(X,Y)vN [i] = 0// compute ngram countsfor d=1 to MAXDEPTHfor each node N of depth d in DAGfor each edge E: N?Mfor i=2 to dvM [i] += Sxy(?
(E), ?
(E), vN [i-1])After algorithm ACS is run, the number of skip-ngrams (weighted skip-ngram score) of size k is sim-ply the sum of the number of skip-ngrams of size kending in each node N ?s corresponding match:wSKIPk =?Ni?DAGvNi [k] (10)2.3.4 ACS Complexity and FeasibilityIn the worst case scenario, both sentences X and Yare composed of exactly the same repeated word: X= ?w w w w .. ?
and Y = ?w w w w ..?.
We let m = |X|and n = |Y |.
In this case, the number of matches isM = n ?
m. Therefore, Step 1 has worst case timeand space complexity of O(m ?
n).
However, em-pirical data suggest that there are far fewer matchesthan in the worst-case scenario and the actual spacerequirements are drastically reduced.
Even in theworst-case scenario, if we assume the average sen-tences is fewer than 100 words, the number of nodesin the DAG would only be 10, 000.
Step 2 of the al-gorithm consists of creating edges in the dependencygraph.
In the worst case scenario, the number of di-rected edges is O(M2) and furthermore if the sen-tences are uniformly composed of the same repeatedword as seen above, the worst-case time and spacecomplexity is m(m+1)/2 ?n(n+1)/2 = O(m2n2).In Step 3 of the algorithm, the DFS complexity forcomputing of node depths is O(M) and the complex-ity of LCS(X,Y ) is O(m ?
n).
The dominant stepis the propagation of common subsequences (skip-ngram counts).
Let l be the size of the LCS.
The up-per bound on the size of the longest common subse-quence is min(|X|, |Y |) = min(m,n).
In the worstcase scenario, for each node we propagate l count val-ues (the size of vector v) to all other nodes in theDAG.
Therefore, the time complexity for Step 3 isO(M2 ?
l) = O(m2n2l) (fifth degree polynomial).3 BLANC as a Generalization of BLEU andROUGEDue to its parametric nature, the All Common Sub-sequences algorithm can emulate the ngram compu-tation of several popular MT evaluation metrics.
Theweighting function ?
allows skip-ngrams with differ-ent gap sizes to be assigned different weights.
Param-eter ?
controls the shape of the weighting function.In one extreme scenario, if we allow ?
to takevery large values, the net effect is that all contiguousngrams of any size will have corresponding weightsof e0 = 1 while all other skip-ngrams will haveweights that are zero.
In this case, the distancefunction will only apply to contiguous ngrams whichhave the same size and no gaps.
Therefore, the dis-tance function will also be 1.
The overall result isthat the ACS algorithm collects contiguous commonngram counts for all ngram sizes.
This is equivalentto computing the ngram overlap between two sen-tences, which is equivalent to the ngram computa-tion performed BLEU metric.
In addition to comput-ing ngram overlap, BLEU incorporates a thresholding(clipping) on ngram counts based on reference trans-lations, as well as a brevity penalty which makes surethe machine-produced translations are not too short.In BLANC, this is replaced by standard F-measure,which research (Turian et al, 2003) has shown it canbe used successfully in MT evaluation.Another scenario consists of setting the ?
and ?parameters to 0.
In this case, all skip-ngrams are as-signed the same weight value of 1 and skip-ngrammatches are also assigned the same distance value of1 regardless of gap sizes and differences in gap sizes.This renders all skip-ngrams equivalent and the ACSalgorithm is reduced to counting the skip-ngram over-lap between two sentences.
Using these counts, pre-cision and recall-based metrics such as the F-measurecan be computed.
If we let the ?
and ?
parameters tobe zero, disregard redundant matches, and compute7440 50 100050100150200Arabic 2003Sentence Length#sentences0 50 100050100150200250300350Chinese 2003Sentence Length#sentences0 50 100100102104ACS #MatchesSentence LengthAvg#Matches0 50 100100105ACS #EdgesSentence LengthAvg#Edges0 50 1001001051010ACS #Feature CallsSentence LengthAvg#TotalArabicChineseWorst CaseFigure 1: Empirical and theoretical behavior of ACS on 2003 machine translation evaluation data (semilog scale).the ACS only for skip-ngrams of size 2, the ACS algo-rithm is equivalent to the ROUGE-S metric (Lin andOch, 2004).
This case represents a specific parametersetting in the ACS skip-ngram computation.The longest common subsequence statistic has alsobeen successfully used for automatic machine trans-lation evaluation in the ROUGE-L (Lin and Och,2004) algorithm.
In BLANC, if we set both ?
and?
parameters to zero, the net result is a set of skip-bigram (common subsequence) overlap counts for allskip-bigram sizes.
Although dynamic programmingor suffix trees can be used to compute the LCS muchfaster, under this parameter setting the ACS algorithmcan also produce the longest common subsequence:LCS(X,Y )?
argmaxkACS(wSKIPk) > 0where Acs(wSKIPk) is the number of commonskip-ngrams (common subsequences) produced bythe ACS algorithm.ROUGE-W (Lin and Och, 2004) relies on aweighted version of the longest common subse-quence, under which longer contiguous subsequencesare assigned a higher weight than subsequences thatincorporate gaps.
ROUGE-W uses the polynomialfunction xa in the weighted LCS computation.
Thissetting can also be simulated by BLANC by adjustingthe parameters ?
to reward tighter skip-ngrams and ?to assign a very high score to similar size gaps.
In-tuitively, ?
is used to reward skip-ngrams that havesmaller gaps, while ?
is used to reward better alignedskip-ngram overlap.4 Scalability & Data ExplorationIn Figure 1 we show theoretical and empirical prac-tical behavior for the ACS algorithm on the 2003TIDES machine translation evaluation data for Ara-bic and Chinese.
Sentence length distribution issomewhat similar for the two languages ?
only a verysmall amount of text segments have more than 50tokens.
We show the ACS graph size in the worstcase scenario, and the empirical average number ofmatches for both languages as a function of sentencelength.
We also show (on a log scale) the upper boundon time/space complexity in terms of total numberof feature computations.
Even though the worst-case scenario is tractable (polynomial), the empiricalamount of computation is considerably smaller in theform of polynomials of lower degree.
In Figure 1,sentence length is the average between reference andcandidate lengths.Finally, we also show the total number of fea-ture computations involved in performing a full over-lap search and computing a numeric score for the745reference-candidate translation pair.
We have exper-imented with the ACS algorithm using a worst-casescenario where all words are exactly the same for afifty words reference translation and candidate trans-lation.
In practice when considering real sentencesthe number of matches is very small.
In this setting,the algorithm takes less than two seconds on a low-end desktop system when working on the worst casescenario, and less then a second for all candidate-reference pairs in the TIDES 2003 dataset.
This re-sult renders the ACS algorithm very practical for au-tomatic MT evaluation.5 Experiments & ResultsIn the dynamic metric BLANC, we have implementedthe ACS algorithm using several parameters includ-ing the aggregate gap size ?, the displacement feature?, a parameter for regulating skip-ngram size contri-bution, and the F-measure ?F parameter.Until recently, most experiments that evaluate au-tomatic metrics correlation to human judgments havebeen performed at a system level.
In such experi-ments, human judgments are aggregated across sen-tences for each MT system and compared to aggre-gate scores for automatic metrics.
While high scor-ing metrics in this setting are useful for understand-ing relative system performance, not all of them arerobust enough for evaluating the quality of machinetranslation output at a lower granularity.
Sentence-level translation quality estimation is very usefulwhen MT is used as a component in a pipeline of text-processing applications (e.g.
question answering).The fact that current automatic MT evaluation met-rics including BLANC do not correlate well with hu-man judgments at the sentence level, does not meanwe should ignore this need and focus only on systemlevel evaluation.
On the contrary, further research isrequired to improve these metrics.
Due to its train-able nature, and by allowing additional features to beincorporated into its model, BLANC has the potentialto address this issue.For comparison purposes with previous literature,we have also performed experiments at system levelfor Arabic.
The datasets used consist of the MT trans-lation outputs from all systems available through theTides 2003 evaluation (663 sentences) for trainingand Tides 2004 evaluation (1353 sentences) for test-ing.We compare (Table 1) the performance of BLANCon Arabic translation output with the performanceof more established evaluation metrics: BLEU andNIST, and also with more recent metrics: ROUGE-L and ROUGE-S (using an unlimited size skip win-dow), which have been shown to correlate well withhuman judgments at system level ?
as confirmed byour results.
We have performed experiments in whichcase information is preserved as well as experimentsthat ignore case information.
Since the results arevery similar, we only show here experiments underthe former condition.
In order to maintain consis-tency, when using any metric we apply the same pre-processing provided by the MTEval script.
Whencomputing the correlation between metrics and hu-man judgments, we only keep strictly positive scores.While this is not fully equivalent to BLEU smooth-ing, it partially mitigates the same problem of zerocount ngrams for short sentences.
In future work weplan to implement smoothing for all metrics, includ-ing BLANC.We train BLANC separately for adequacy and flu-ency, as well as for system level and segment levelcorrelation with human judgments.
The BLANC pa-rameters are currently trained using a simple hill-climbing procedure and using several starting pointsin order to decrease the chance of reaching a localmaximum.BLANC proves to be robust across criteria andgranularity levels.
As expected, different parametervalues of BLANC optimize different criteria (e.g.
ad-equacy and fluency).
We have observed that train-ing BLANC for adequacy results in more bias to-wards recall (?F =3) compared to training it for flu-ency (?F =2).
This confirms our intuition that a dy-namic, parametric metric is justified for automaticevaluation.6 Conclusions & Future WorkIn previous sections we have defined simple distancefunctions.
More complex functions can also be incor-porated in ACS.
Skip-ngrams in the candidate sen-tence might be rewarded if they contain fewer gaps inthe candidate sentence and penalized if they containmore.
Different distance functions could also be usedin ACS, including functions based on surface-formfeatures and part-of-speech features.Most of the established MT evaluation methods are746Tides 2003 ArabicSystem Level Segment LevelMethod Adequacy Fluency Adequacy FluencyBLEU 0.950 0.934 0.382 0.286NIST 0.962 0.939 0.439 0.304ROUGE-L 0.974 0.926 0.440 0.328ROUGE-S 0.949 0.935 0.360 0.328BLANC 0.988 0.979 0.492 0.391Tides 2004 ArabicSystem Level Segment LevelMethod Adequacy Fluency Adequacy FluencyBLEU 0.978 0.994 0.446 0.337NIST 0.987 0.952 0.529 0.358ROUGE-L 0.981 0.985 0.538 0.412ROUGE-S 0.937 0.980 0.367 0.408BLANC 0.982 0.994 0.565 0.438Table 1: Pearson correlation of several metrics with human judgments at system level and segment level for fluency and adequacy.static functions according to which automatic evalu-ation scores are computed.
In this paper, we havelaid the foundation for a more flexible, parametric ap-proach that can be trained using existing MT data andthat can be optimized for highest agreement with hu-man assessors, for different criteria.We have introduced ACS, a practical algorithmwith learnable parameters for automatic MT evalu-ation and showed that ngram computation of popu-lar evaluation methods can be emulated through dif-ferent parameters by ACS.
We have computed timeand space bounds for the ACS algorithm and arguedthat while it is more powerful in modeling local andsentence structure, it offers the same practicality asestablished algorithms.In our experiments, we trained and tested BLANCon data from consecutive years, and therefore tai-lored the metric for two different operating pointsin MT system performance.
In this paper we showthat BLANC correlates well with human performancewhen trained on previous year data for both sentenceand system level.In the future, we plan to investigate the stabilityand performance of BLANC and also apply it to auto-matic summarization evaluation.
We plan to optimizethe BLANC parameters for different criteria in addi-tion to incorporating syntactic and semantic features(e.g.
ngrams, word classes, part-of-speech).In previous sections we have defined simple dis-tance functions.
More complex functions can alsobe incorporated in ACS.
Skip-ngrams in the candi-date sentence might be rewarded if they contain fewergaps in the candidate sentence and penalized if theycontain more.
Different distance functions could alsobe used in ACS, including functions based on surface-form features and part-of-speech features.Looking beyond the BLANC metric, this papermakes the case for the need to shift to trained, dy-namic evaluation metrics which can adapt to individ-ual optimization criteria and correlation functions.We plan to make available an implementation ofBLANC at http://www.cs.cmu.edu/ llita/blanc.ReferencesY.
Akiba, K. Iamamurfa, and E. Sumita.
2001.
Usingmultiple edit distances to automatically rank machinetranslation output.
MT Summit VIII.C.
Culy and S.Z.
Riehemann.
2003.
The limits of n-gram translation evaluation metrics.
Machine Transla-tion Summit IX.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
Human Language Technology Conference(HLT).V.I.
Levenshtein.
1965.
Binary codes capable of cor-recting deletions, insertions, and reversals.
DokladyAkademii Nauk SSSR.C.Y.
Lin and F.J. Och.
2004.
Automatic evaluation ofmachine translation quality using longest common sub-sequence and skip bigram statistics.
ACL.S.
Niessen, F.J. Och, G. Leusch, and H. Ney.
2000.
Anevaluation tool for machine translation: Fast evaluationfor mt research.
LREC.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.
IBM Research Report.R.
Soricut and E. Brill.
2004.
A unified framework forautomatic evaluation using n-gram co-occurence statis-tics.
ACL.K.Y.
Su, M.W.
Wu, and J.S.
Chang.
1992.
A new quanti-tative quality measure for machine translation systems.COLING.J.P.
Turian, L. Shen, and I.D.
Melamed.
2003.
Evaluationof machine translation and its evaluation.
MT SummitIX.C.J.
Van-Rijsbergen.
1979.
Information retrieval.747
