Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 486?496,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsCombining Intra- and Multi-sentential Rhetorical Parsing forDocument-level Discourse AnalysisShafiq Joty?sjoty@qf.org.qaQatar Computing Research InstituteQatar FoundationDoha, QatarGiuseppe Carenini, Raymond Ng, Yashar Mehdad{carenini, rng, mehdad}@cs.ubc.caDepartment of Computer ScienceUniversity of British ColumbiaVancouver, CanadaAbstractWe propose a novel approach for develop-ing a two-stage document-level discourseparser.
Our parser builds a discourse treeby applying an optimal parsing algorithmto probabilities inferred from two Con-ditional Random Fields: one for intra-sentential parsing and the other for multi-sentential parsing.
We present two ap-proaches to combine these two stages ofdiscourse parsing effectively.
A set ofempirical evaluations over two differentdatasets demonstrates that our discourseparser significantly outperforms the state-of-the-art, often by a wide margin.1 IntroductionDiscourse of any kind is not formed by inde-pendent and isolated textual units, but by relatedand structured units.
Discourse analysis seeksto uncover such structures underneath the surfaceof the text, and has been shown to be benefi-cial for text summarization (Louis et al, 2010;Marcu, 2000b), sentence compression (Sporlederand Lapata, 2005), text generation (Prasad et al,2005), sentiment analysis (Somasundaran, 2010)and question answering (Verberne et al, 2007).Rhetorical Structure Theory (RST) (Mann andThompson, 1988), one of the most influential the-ories of discourse, represents texts by labeled hier-archical structures, called Discourse Trees (DTs),as exemplified by a sample DT in Figure 1.
Theleaves of a DT correspond to contiguous Elemen-tary Discourse Units (EDUs) (six in the exam-ple).
Adjacent EDUs are connected by rhetori-cal relations (e.g., Elaboration, Contrast), form-ing larger discourse units (represented by internal?This work was conducted at the University of BritishColumbia, Vancouver, Canada.nodes), which in turn are also subject to this re-lation linking.
Discourse units linked by a rhetori-cal relation are further distinguished based on theirrelative importance in the text: nucleus being thecentral part, whereas satellite being the peripheralone.
Discourse analysis in RST involves two sub-tasks: discourse segmentation is the task of identi-fying the EDUs, and discourse parsing is the taskof linking the discourse units into a labeled tree.While recent advances in automatic discoursesegmentation and sentence-level discourse parsinghave attained accuracies close to human perfor-mance (Fisher and Roark, 2007; Joty et al, 2012),discourse parsing at the document-level still posessignificant challenges (Feng and Hirst, 2012) andthe performance of the existing document-levelparsers (Hernault et al, 2010; Subba and Di-Eugenio, 2009) is still considerably inferior com-pared to human gold-standard.
This paper aimsto reduce this performance gap and take discourseparsing one step further.
To this end, we addressthree key limitations of existing parsers as follows.First, existing discourse parsers typically modelthe structure and the labels of a DT separatelyin a pipeline fashion, and also do not considerthe sequential dependencies between the DT con-stituents, which has been recently shown to be crit-ical (Feng and Hirst, 2012).
To address this limi-tation, as the first contribution, we propose a noveldocument-level discourse parser based on proba-bilistic discriminative parsing models, representedas Conditional Random Fields (CRFs) (Sutton etal., 2007), to infer the probability of all possibleDT constituents.
The CRF models effectively rep-resent the structure and the label of a DT con-stituent jointly, and whenever possible, capture thesequential dependencies between the constituents.Second, existing parsers apply greedy and sub-optimal parsing algorithms to build the DT for adocument.
To cope with this limitation, our CRFmodels support a probabilistic bottom-up parsing486But he added:"Some people use the purchasers?index as a leading indicator, some use it as a coincident indicator.
But the thing it?s supposed to measure -- manufacturing strength --it missed altogether last month."
<P>ElaborationSame-UnitContrastContrastAttribution(1)(2) (3)(4) (5)(6)Figure 1: Discourse tree for two sentences in RST-DT.
Each of the sentences contains three EDUs.
Thesecond sentence has a well-formed discourse tree, but the first sentence does not have one.algorithm which is non-greedy and optimal.Third, existing discourse parsers do not dis-criminate between intra-sentential (i.e., buildingthe DTs for the individual sentences) and multi-sentential parsing (i.e., building the DT for thedocument).
However, we argue that distinguish-ing between these two conditions can result inmore effective parsing.
Two separate parsingmodels could exploit the fact that rhetorical re-lations are distributed differently intra-sententiallyvs.
multi-sententially.
Also, they could indepen-dently choose their own informative features.
Asanother key contribution of our work, we devisetwo different parsing components: one for intra-sentential parsing, the other for multi-sententialparsing.
This provides for scalable, modular andflexible solutions, that can exploit the strong cor-relation observed between the text structure (sen-tence boundaries) and the structure of the DT.In order to develop a complete and robust dis-course parser, we combine our intra-sententialand multi-sentential parsers in two different ways.Since most sentences have a well-formed dis-course sub-tree in the full document-level DT (forexample, the second sentence in Figure 1), our firstapproach constructs a DT for every sentence us-ing our intra-sentential parser, and then runs themulti-sentential parser on the resulting sentence-level DTs.
However, this approach would disre-gard those cases where rhetorical structures vio-late sentence boundaries.
For example, considerthe first sentence in Figure 1.
It does not have awell-formed sub-tree because the unit containingEDUs 2 and 3 merges with the next sentence andonly then is the resulting unit merged with EDU1.
Our second approach, in an attempt of dealingwith these cases, builds sentence-level sub-treesby applying the intra-sentential parser on a slidingwindow covering two adjacent sentences and bythen consolidating the results produced by over-lapping windows.
After that, the multi-sententialparser takes all these sentence-level sub-trees andbuilds a full rhetorical parse for the document.While previous approaches have been tested ononly one corpus, we evaluate our approach ontexts from two very different genres: news articlesand instructional how-to-do manuals.
The resultsdemonstrate that our contributions provide con-sistent and statistically significant improvementsover previous approaches.
Our final result com-pares very favorably to the result of state-of-the-artmodels in document-level discourse parsing.In the rest of the paper, after discussing relatedwork in Section 2, we present our discourse pars-ing framework in Section 3.
In Section 4, we de-scribe the intra- and multi-sentential parsing com-ponents.
Section 5 presents the two approachesto combine the two stages of parsing.
The exper-iments and error analysis, followed by future di-rections are discussed in Section 6.
Finally, wesummarize our contributions in Section 7.2 Related workThe idea of staging document-level discourseparsing on top of sentence-level discourse parsingwas investigated in (Marcu, 2000a; LeThanh et al,2004).
These approaches mainly rely on discoursemarkers (or cues), and use hand-coded rules tobuild DTs for sentences first, then for paragraphs,and so on.
However, often rhetorical relationsare not explicitly signaled by discourse markers(Marcu and Echihabi, 2002), and discourse struc-tures do not always correspond to paragraph struc-tures (Sporleder and Lascarides, 2004).
Therefore,rather than relying on hand-coded rules based ondiscourse markers, recent approaches employ su-pervised machine learning techniques with a largeset of informative features.Hernault et al, (2010) presents the publiclyavailable HILDA parser.
Given the EDUs in a doc-487Elaboration Joint AttributionSame-Unit Contrast Explanation051015202530 Multi-sententialIntra-sententialFigure 2: Distributions of six most frequent relations inintra-sentential and multi-sentential parsing scenarios.ument, HILDA iteratively employs two SupportVector Machine (SVM) classifiers in pipeline tobuild the DT.
In each iteration, a binary classifierfirst decides which of the adjacent units to merge,then a multi-class classifier connects the selectedunits with an appropriate relation label.
They eval-uate their approach on the RST-DT corpus (Carl-son et al, 2002) of news articles.
On a differentgenre of instructional texts, Subba and Di-Eugenio(2009) propose a shift-reduce parser that relies ona classifier for relation labeling.
Their classifieruses Inductive Logic Programming (ILP) to learnfirst-order logic rules from a set of features includ-ing compositional semantics.
In this work, we ad-dress the limitations of these models (described inSection 1) introducing our novel discourse parser.3 Our Discourse Parsing FrameworkGiven a document with sentences already seg-mented into EDUs, the discourse parsing prob-lem is determining which discourse units (EDUsor larger units) to relate (i.e., the structure), andhow to relate them (i.e., the labels or the discourserelations) in the resulting DT.
Since we alreadyhave an accurate sentence-level discourse parser(Joty et al, 2012), a straightforward approach todocument-level parsing could be to simply applythis parser to the whole document.
However thisstrategy would be problematic because of scalabil-ity and modeling issues.
Note that the number ofvalid trees grows exponentially with the numberof EDUs in a document.1 Therefore, an exhaus-tive search over the valid trees is often unfeasible,even for relatively small documents.For modeling, the problem is two-fold.
On theone hand, it appears that rhetorical relations aredistributed differently intra-sententially vs. multi-sententially.
For example, Figure 2 shows a com-parison between the two distributions of six most1For n + 1 EDUs, the number of valid discourse trees isactually the Catalan number Cn.modelAlgorithmSentences segmented into EDUsDocument-leveldiscourse treeIntra-sententialparser Multi-sententialparsermodelAlgorithmFigure 3: Discourse parsing framework.frequent relations on a development set containing20 randomly selected documents from RST-DT.Notice that relations Attribution and Same-Unitare more frequent than Joint in intra-sententialcase, whereas Joint is more frequent than the othertwo in multi-sentential case.
On the other hand,different kinds of features are applicable and in-formative for intra-sentential vs. multi-sententialparsing.
For example, syntactic features like dom-inance sets (Soricut and Marcu, 2003) are ex-tremely useful for sentence-level parsing, but arenot even applicable in multi-sentential case.
Like-wise, lexical chain features (Sporleder and Las-carides, 2004), that are useful for multi-sententialparsing, are not applicable at the sentence level.Based on these observations, our discourseparsing framework comprises two separate mod-ules: an intra-sentential parser and a multi-sentential parser (Figure 3).
First, the intra-sentential parser produces one or more discoursesub-trees for each sentence.
Then, the multi-sentential parser generates a full DT for the doc-ument from these sub-trees.
Both of our parsershave the same two components: a parsing modelassigns a probability to every possible DT, anda parsing algorithm identifies the most probableDT among the candidate DTs in that scenario.While the two models are rather different, thesame parsing algorithm is shared by the two mod-ules.
Staging multi-sentential parsing on top ofintra-sentential parsing in this way allows us to ex-ploit the strong correlation between the text struc-ture and the DT structure as explained in detail inSection 5.
Before describing our parsing modelsand the parsing algorithm, we introduce some ter-minology that we will use throughout the paper.Following (Joty et al, 2012), a DT can be for-mally represented as a set of constituents of theform R[i,m, j], referring to a rhetorical relationR between the discourse unit containing EDUs ithrough m and the unit containing EDUs m+1through j.
For example, the DT for the sec-ond sentence in Figure 1 can be represented as488{Elaboration-NS[4,4,5], Same-Unit-NN[4,5,6]}.Notice that a relation R also specifies the nuclear-ity statuses of the discourse units involved, whichcan be one of Nucleus-Satellite (NS), Satellite-Nucleus (SN) and Nucleus-Nucleus (NN).4 Parsing Models and Parsing AlgorithmThe job of our intra-sentential and multi-sententialparsing models is to assign a probability to eachof the constituents of all possible DTs at the sen-tence level and at the document level, respectively.Formally, given the model parameters ?, for eachpossible constituent R[i,m, j] in a candidate DTat the sentence or document level, the parsingmodel estimates P (R[i,m, j]|?
), which specifiesa joint distribution over the label R and the struc-ture [i,m, j] of the constituent.4.1 Intra-Sentential Parsing ModelRecently, we proposed a novel parsing modelfor sentence-level discourse parsing (Joty etal., 2012), that outperforms previous approachesby effectively modeling sequential dependenciesalong with structure and labels jointly.
Below webriefly describe the parsing model, and show howit is applied to obtain the probabilities of all possi-ble DT constituents at the sentence level.Figure 4 shows the intra-sentential parsingmodel expressed as a Dynamic Conditional Ran-dom Field (DCRF) (Sutton et al, 2007).
The ob-served nodes Uj in a sequence represent the dis-course units (EDUs or larger units).
The first layerof hidden nodes are the structure nodes, whereSj?
{0, 1} denotes whether two adjacent discourseunits Uj?1 and Uj should be connected or not.The second layer of hidden nodes are the relationnodes, with Rj?
{1 .
.
.M} denoting the relationbetween two adjacent unitsUj?1 andUj , whereMis the total number of relations in the relation set.The connections between adjacent nodes in a hid-den layer encode sequential dependencies betweenthe respective hidden nodes, and can enforce con-straints such as the fact that a Sj= 1 must not fol-low a Sj?1= 1.
The connections between the twohidden layers model the structure and the relationof a DT (sentence-level) constituent jointly.To obtain the probability of the constituentsof all candidate DTs for a sentence, we applythe parsing model recursively at different levelsof the DT and compute the posterior marginalsover the relation-structure pairs.
To illustrate theU UU U U2223 j t-1 tSS S S SR R R R R33 jj t-1t-1 tUnit sequenceat level iStructure sequenceRelationsequenceU1tFigure 4: A chain-structured DCRF as our intra-sentential parsing model.process, let us assume that the sentence containsfour EDUs.
At the first (bottom) level, when allthe units are the EDUs, there is only one possibleunit sequence to which we apply our DCRFmodel (Figure 5(a)).
We compute the posteriormarginals P (R2, S2=1|e1, e2, e3, e4,?
), P (R3,S3=1|e1, e2, e3, e4,?)
and P (R4, S4=1|e1, e2, e3,e4,?)
to obtain the probability of the con-stituents R[1, 1, 2], R[2, 2, 3] and R[3, 3, 4],respectively.
At the second level, there arethree possible unit sequences (e1:2, e3, e4),(e1,e2:3, e4) and (e1,e2,e3:4).
Figure 5(b) showstheir corresponding DCRFs.
The posteriormarginals P (R3, S3=1|e1:2,e3,e4,?
), P (R2:3S2:3=1|e1,e2:3,e4,?
), P (R4, S4=1|e1,e2:3,e4,?
)and P (R3:4, S3:4=1|e1,e2,e3:4,?)
computed fromthe three sequences correspond to the probabilityof the constituents R[1, 2, 3], R[1, 1, 3], R[2, 3, 4]and R[2, 2, 4], respectively.
Similarly, we attainthe probability of the constituents R[1, 1, 4],R[1, 2, 4] and R[1, 3, 4] by computing theirrespective posterior marginals from the threepossible sequences at the third (top) level.e 1 e e2223S S3R R3(a)e 1eSR1:2 333e eSR2:32:3(b)2:3e4S4R4e4S4R4e4S4R41e eSR222 e3:4S3:4R3:41 eSR1:3 444e eSR2:42:4(c)2:4 eeSR1:2e3:43:43:4(i) (ii)(iii)(i) (ii) (iii)Figure 5: Our parsing model applied to the sequences atdifferent levels of a sentence-level DT.
(a) Only possible se-quence at the first level, (b) Three possible sequences at thesecond level, (c) Three possible sequences at the third level.At this point what is left to be explained ishow we generate all possible sequences for agiven number of EDUs in a sentence.
Algorithm1 demonstrates how we do that.
More specifi-cally, to compute the probabilities of each DT con-489stituent R[i, k, j], we need to generate sequenceslike (e1, ?
?
?
, ei?1, ei:k, ek+1:j , ej+1, ?
?
?
, en) for1 ?
i ?
k < j ?
n. In doing so, we maygenerate some duplicate sequences.
Clearly, thesequence (e1, ?
?
?
, ei?1, ei:i, ei+1:j , ej+1, ?
?
?
, en)for 1 ?
i ?
k < j < n is already consideredfor computing the probability of R[i+ 1, j, j+ 1].Therefore, it is a duplicate sequence that we ex-clude from our list of all possible sequences.Input: Sequence of EDUs: (e1, e2, ?
?
?
, en)Output: List of sequences: Lfor i = 1?
n?
1 dofor j = i+ 1?
n doif j == n thenfor k = i?
j ?
1 doL.append((e1, .., ei?1, ei:k, ek+1:j , ej+1, .., en))endelsefor k = i+ 1?
j ?
1 doL.append((e1, .., ei?1, ei:k, ek+1:j , ej+1, .., en))endendendendAlgorithm 1: Generating all possible sequencesfor a sentence with n EDUs.Once we obtain the probability of all possibleDT constituents, the discourse sub-trees for thesentences are built by applying an optimal prob-abilistic parsing algorithm (Section 4.4) using oneof the methods described in Section 5.4.2 Multi-Sentential Parsing ModelGiven the discourse units (sub-trees) for all thesentences of a document, a simple approach tobuild the rhetorical tree of the document would beto apply a new DCRF model, similar to the onein Figure 4 (with different parameters), to all thepossible sequences generated from these units toinfer the probability of all possible higher-orderconstituents.
However, the number of possible se-quences and their length increase with the numberof sentences in a document.
For example, assum-ing that each sentence has a well-formed DT, fora document with n sentences, Algorithm 1 gener-ates O(n3) sequences, where the sequence at thebottom level has n units, each of the sequences atthe second level has n-1 units, and so on.
Sincethe model in Figure 4 has a ?fat?
chain structure,U Ut-1 tSR tAdjacent Unitsat level iStructureRelationtFigure 6: A CRF as a multi-sentential parsing model.we could use forwards-backwards algorithm forexact inference in this model (Sutton and McCal-lum, 2012).
However, forwards-backwards on asequence containing T units costs O(TM2) time,where M is the number of relations in our rela-tion set.
This makes the chain-structured DCRFmodel impractical for multi-sentential parsing oflong documents, since learning requires to run in-ference on every training sequence with an overalltime complexity of O(TM2n3) per document.Our model for multi-sentential parsing is shownin Figure 6.
The two observed nodes Ut?1 andUt are two adjacent discourse units.
The (hidden)structure node S?
{0, 1} denotes whether the twounits should be connected or not.
The hidden nodeR?
{1 .
.
.M} represents the relation between thetwo units.
Notice that like the previous model, thisis also an undirected graphical model.
It becomesa CRF if we directly model the hidden (output)variables by conditioning its clique potential (orfactor) ?
on the observed (input) variables:P (Rt, St|x,?)
= 1Z(x,?)?
(Rt, St|x,?)
(1)where x represents input features extracted fromthe observed variables Ut?1 and Ut, and Z(x,?
)is the partition function.
We use a log-linear rep-resentation of the factor:?
(Rt, St|x,?)
= exp(?T f(Rt, St, x)) (2)where f(Rt, St, x) is a feature vector derived fromthe input features x and the labels Rt and St, and?
is the corresponding weight vector.
Although,this model is similar in spirit to the model in Fig-ure 4, we now break the chain structure, whichmakes the inference much faster (i.e., complex-ity of O(M2)).
Breaking the chain structure alsoallows us to balance the data for training (equalnumber instances with S=1 and S=0), which dra-matically reduces the learning time of the model.We apply our model to all possible adjacentunits at all levels for the multi-sentential case, and490compute the posterior marginals of the relation-structure pairs P (Rt, St=1|Ut?1, Ut,?)
to obtainthe probability of all possible DT constituents.4.3 Features Used in our Parsing ModelsTable 1 summarizes the features used in our pars-ing models, which are extracted from two adjacentunitsUt?1 andUt.
Since most of these features areadopted from previous studies (Joty et al, 2012;Hernault et al, 2010), we briefly describe them.Organizational features include the length ofthe units as the number of EDUs and tokens.It also includes the distances of the units fromthe beginning and end of the sentence (or text inthe multi-sentential case).
Text structural fea-tures indirectly capture the correlation betweentext structure and rhetorical structure by countingthe number of sentence and paragraph boundariesin the units.
Discourse markers (e.g., because, al-though) carry informative clues for rhetorical re-lations (Marcu, 2000a).
Rather than using a fixedlist of discourse markers, we use an empiricallylearned lexical N-gram dictionary following (Jotyet al, 2012).
This approach has been shown tobe more robust and flexible across domains (Bi-ran and Rambow, 2011; Hernault et al, 2010).
Wealso include part-of-speech (POS) tags for the be-ginning and end N tokens in a unit.8 Organizational features Intra & Multi-SententialNumber of EDUs in unit 1 (or unit 2).Number of tokens in unit 1 (or unit 2).Distance of unit 1 in EDUs to the beginning (or to the end).Distance of unit 2 in EDUs to the beginning (or to the end).4 Text structural features Multi-SententialNumber of sentences in unit 1 (or unit 2).Number of paragraphs in unit 1 (or unit 2).8 N-gram features N?
{1, 2, 3} Intra & Multi-SententialBeginning (or end) lexical N-grams in unit 1.Beginning (or end) lexical N-grams in unit 2.Beginning (or end) POS N-grams in unit 1.Beginning (or end) POS N-grams in unit 2.5 Dominance set features Intra-SententialSyntactic labels of the head node and the attachment node.Lexical heads of the head node and the attachment node.Dominance relationship between the two units.8 Lexical chain features Multi-SententialNumber of chains start in unit 1 and end in unit 2.Number of chains start (or end) in unit 1 (or in unit 2).Number of chains skipping both unit 1 and unit 2.Number of chains skipping unit 1 (or unit 2).2 Contextual features Intra & Multi-SententialPrevious and next feature vectors.2 Substructure features Intra & Multi-SententialRoot nodes of the left and right rhetorical sub-trees.Table 1: Features used in our parsing models.Lexico-syntactic features dominance sets(Soricut and Marcu, 2003) are very effective forintra-sentential parsing.
We include syntacticlabels and lexical heads of head and attachmentnodes along with their dominance relationshipas features.
Lexical chains (Morris and Hirst,1991) are sequences of semantically related wordsthat can indicate topic shifts.
Features extractedfrom lexical chains have been shown to be usefulfor finding paragraph-level discourse structure(Sporleder and Lascarides, 2004).
We computelexical chains for a document following the ap-proach proposed in (Galley and McKeown, 2003),that extracts lexical chains after performing wordsense disambiguation.
Following (Joty et al,2012), we also encode contextual and rhetoricalsub-structure features in our models.
The rhetori-cal sub-structure features incorporate hierarchicaldependencies between DT constituents.4.4 Parsing AlgorithmGiven the probability of all possible DT con-stituents in the intra-sentential and multi-sententialscenarios, the job of the parsing algorithm is tofind the most probable DT for that scenario.
Fol-lowing (Joty et al, 2012), we implement a prob-abilistic CKY-like bottom-up algorithm for com-puting the most likely parse using dynamic pro-gramming.
Specifically, with n discourse units,we use the upper-triangular portion of the n?ndynamic programming table D. Given Ux(0) andUx(1) are the start and end EDU Ids of unit Ux:D[i, j] = P (R[Ui(0), Uk(1), Uj(1)]) (3)where, k = argmaxi?p?jP (R[Ui(0), Up(1), Uj(1)]).Note that, in contrast to previous studies ondocument-level parsing (Hernault et al, 2010;Subba and Di-Eugenio, 2009; Marcu, 2000b),which use a greedy algorithm, our approach findsa discourse tree that is globally optimal.5 Document-level Parsing ApproachesNow that we have presented our intra-sententialand our multi-sentential parsers, we are ready todescribe how they can be effectively combined toperform document-level discourse analysis.
Re-call that a key motivation for a two-stage parsing isthat it allows us to capture the correlation betweentext structure and discourse structure in a scalable,modular and flexible way.
Below we describe twodifferent approaches to model this correlation.4915.1 1S-1S (1 Sentence-1 Sub-tree)A key finding from several previous studies onsentence-level discourse analysis is that most sen-tences have a well-formed discourse sub-tree inthe full document-level DT (Joty et al, 2012;Fisher and Roark, 2007).
For example, Figure 7(a)shows 10 EDUs in 3 sentences (see boxes), wherethe DTs for the sentences obey their respectivesentence boundaries.
The 1S-1S approach aims tomaximally exploit this finding.
It first constructsa DT for every sentence using our intra-sententialparser, and then it provides our multi-sententialparser with the sentence-level DTs to build therhetorical parse for the whole document.1     2  3S 18  9   10S 34   5      6   7S 21    2   3S 18   9    10S 34   5    6    7S 2(a) (b)??
?Figure 7: Two possible DTs for three sentences.5.2 Sliding WindowWhile the assumption made by 1S-1S clearly sim-plifies the parsing process, it totally ignores thecases where discourse structures violate sentenceboundaries.
For example, in the DT shown in Fig-ure 7(b), sentence S2 does not have a well-formedsub-tree because some of its units attach to theleft (4-5, 6) and some to the right (7).
Vliet andRedeker (2011) call these cases as ?leaky?
bound-aries.
Even though less than 5% of the sentenceshave leaky boundaries in RST-DT, in other corporathis can be true for a larger portion of the sen-tences.
For example, we observe over 12% sen-tences with leaky boundaries in the Instructionalcorpus of (Subba and Di-Eugenio, 2009).
How-ever, we notice that in most cases where discoursestructures violate sentence boundaries, its units aremerged with the units of its adjacent sentences, asin Figure 7(b).
For example, this is true for 75%cases in our development set containing 20 newsarticles from RST-DT and for 79% cases in ourdevelopment set containing 20 how-to-do manualsfrom the Instructional corpus.
Based on this obser-vation, we propose a sliding window approach.In this approach, our intra-sentential parserworks with a window of two consecutive sen-tences, and builds a DT for the two sentences.
Forexample, given the three sentences in Figure 7, ourintra-sentential parser constructs a DT for S1-S2and a DT for S2-S3.
In this process, each sentencein a document except the first and the last will beassociated with two DTs: one with the previoussentence (say DTp) and one with the next (sayDTn).
In other words, for each non-boundary sen-tence, we will have two decisions: one from DTpand one from DTn.
Our parser consolidates thetwo decisions and generates one or more sub-treesfor each sentence by checking the following threemutually exclusive conditions one after another:?
Same in both: If the sentence has the same (interms of both structure and labels) well-formedsub-tree in both DTp and DTn, we take this sub-tree for the sentence.
For example, in Figure 8(a),S2 has the same sub-tree in the two DTs, i.e.
a DTfor S1-S2 and a DT for S2-S3.
The two decisionsagree on the DT for the sentence.?
Different but no cross: If the sentence has awell-formed sub-tree in both DTp and DTn, butthe two sub-trees vary either in structure or in la-bels, we pick the most probable one.
For example,consider the DT for S1-S2 in Figure 8(a) and theDT for S2-S3 in Figure 8(b).
In both cases S2 hasa well-formed sub-tree, but they differ in structure.We pick the sub-tree which has the higher proba-bility in the two dynamic programming tables.1     2  3S1 8  9   10S34   5      6   7S21    2   3S1 8   9    10S34   5    6    7S2(a)(c)8  9   10S 34   5    6     7S2 (b)4   5    6    7S2(i) (ii)4   5      6   7S2Figure 8: Extracting sub-trees for S2.?
Cross: If either or both of DTp and DTn seg-ment the sentence into multiple sub-trees, we pickthe one with more sub-trees.
For example, con-sider the two DTs in Figure 8(c).
In the DT forS1-S2, S2 has three sub-trees (4-5,6,7), whereasin the DT for S2-S3, it has two (4-6,7).
So, we ex-tract the three sub-trees for S2 from the first DT.
Ifthe sentence has the same number of sub-trees inboth DTp and DTn, we pick the one with higherprobability in the dynamic programming tables.At the end, the multi-sentential parser takes allthese sentence-level sub-trees for a document, andbuilds a full rhetorical parse for the document.4926 Experiments6.1 CorporaWhile previous studies on document-level parsingonly report their results on a particular corpus, toshow the generality of our method, we experimentwith texts from two very different genres.
Ourfirst corpus is the standard RST-DT (Carlson etal., 2002), which consists of 385 Wall Street Jour-nal articles, and is partitioned into a training setof 347 documents and a test set of 38 documents.53 documents, selected from both sets were anno-tated by two annotators, based on which we mea-sure human agreement.
In RST-DT, the original 25rhetorical relations defined by (Mann and Thomp-son, 1988) are further divided into a set of 18coarser relation classes with 78 finer-grained rela-tions.
Our second corpus is the Instructional cor-pus prepared by (Subba and Di-Eugenio, 2009),which contains 176 how-to-do manuals on home-repair.
The corpus was annotated with 26 informa-tional relations (e.g., Preparation-Act, Act-Goal).6.2 Experimental SetupWe experiment with our discourse parser on thetwo datasets using our two different parsing ap-proaches, namely 1S-1S and the sliding window.We compare our approach with HILDA (Hernaultet al, 2010) on RST-DT, and with the ILP-basedapproach of (Subba and Di-Eugenio, 2009) on theInstructional corpus, since they are the state-of-the-art on the respective genres.
On RST-DT, thestandard split was used for training and testingpurposes.
The results for HILDA were obtainedby running the system with default settings on thesame inputs we provided to our system.
Since wecould not run the ILP-based system of (Subba andDi-Eugenio, 2009) (not publicly available) on theInstructional corpus, we report the performancespresented in their paper.
They used 151 documentsfor training and 25 documents for testing.
Sincewe did not have access to their particular split,we took 5 random samples of 151 documents fortraining and 25 documents for testing, and reportthe average performance over the 5 test sets.To evaluate the parsing performance, we usethe standard unlabeled (i.e., hierarchical spans)and labeled (i.e., nuclearity and relation) preci-sion, recall and F-score as described in (Marcu,2000b).
To compare with previous studies, ourexperiments on RST-DT use the 18 coarser rela-tions.
After attaching the nuclearity statuses (NS,SN, NN) to these relations, we get 41 distinct re-lations.
Following (Subba and Di-Eugenio, 2009)on the Instructional corpus, we use 26 relations,and treat the reversals of non-commutative rela-tions as separate relations.
That is, Goal-Act andAct-Goal are considered as two different relations.Attaching the nuclearity statuses to these relationsgives 76 distinct relations.
Analogous to previousstudies, we map the n-ary relations (e.g., Joint)into nested right-branching binary relations.6.3 Results and Error AnalysisTable 2 presents F-score parsing results for ourparsers and the existing systems on the two cor-pora.2 On both corpora, our parser, namely, 1S-1S(TSP 1-1) and sliding window (TSP SW), outper-form existing systems by a wide margin (p<7.1e-05).3 On RST-DT, our parsers achieve absoluteF-score improvements of 8%, 9.4% and 11.4%in span, nuclearity and relation, respectively, overHILDA.
This represents relative error reductionsof 32%, 23% and 21% in span, nuclearity and rela-tion, respectively.
Our results are also close to theupper bound, i.e.
human agreement on this corpus.On the Instructional genre, our parsers deliverabsolute F-score improvements of 10.5%, 13.6%and 8.14% in span, nuclearity and relations, re-spectively, over the ILP-based approach.
Ourparsers, therefore, reduce errors by 36%, 27% and13% in span, nuclearity and relations, respectively.If we compare the performance of our parserson the two corpora, we observe higher resultson RST-DT.
This can be explained in at leasttwo ways.
First, the Instructional corpus has asmaller amount of data with a larger set of rela-tions (76 when nuclearity attached).
Second, somefrequent relations are (semantically) very similar(e.g., Preparation-Act, Step1-Step2), which makesit difficult even for the human annotators to distin-guish them (Subba and Di-Eugenio, 2009).Comparison between our two models revealsthat TSP SW significantly outperforms TSP 1-1only in finding the right structure on both corpora(p<0.01).
Not surprisingly, the improvement ishigher on the Instructional corpus.
A likely ex-planation is that the Instructional corpus containsmore leaky boundaries (12%), allowing the sliding2Precision, Recall and F-score are the same when manualsegmentation is used (see Marcu, (2000b), page 143).3Since we did not have access to the output or to the sys-tem of (Subba and Di-Eugenio, 2009), we were not able toperform a significance test on the Instructional corpus.493RST-DT InstructionalMetrics HILDA TSP 1-1 TSP SW Human ILP TSP 1-1 TSP SWSpan 74.68 82.47* 82.74*?
88.70 70.35 79.67 80.88?Nuclearity 58.99 68.43* 68.40* 77.72 49.47 63.03 63.10Relation 44.32 55.73* 55.71* 65.75 35.44 43.52 43.58Table 2: Parsing results of different models using manual (gold) segmentation.
Performances significantly superior to HILDA(with p<7.1e-05) are denoted by *.
Significant differences between TSP 1-1 and TSP SW (with p<0.01) are denoted by ?.T-CT-OT-CMM-MCMPEVSUCNDENCATEEXBACOJOS-UATELT-C  T-O  T-CM      M-M     CMP  EV  SU     CND  EN     CA  TE    EX     BA   CO      JO   S-U   AT     EL1073211122711412129130935900010203133350012722000010000000101853200201210079367571080000000302112332000000130010290192100100013200041121210080000000000704310010000100013051110060000000024221000001400000080000000100000001002210101220100001010000011110000000040000000020000000000000000000000001000000000000000020000000000000001000000000000000000Figure 9: Confusion matrix for relation labels on theRST-DT test set.
Y-axis represents true and X-axis repre-sents predicted relations.
The relations are Topic-Change(T-C), Topic-Comment (T-CM), Textual Organization (T-O), Manner-Means (M-M), Comparison (CMP), Evaluation(EV), Summary (SU), Condition (CND), Enablement (EN),Cause (CA), Temporal (TE), Explanation (EX), Background(BA), Contrast (CO), Joint (JO), Same-Unit (S-U), Attribu-tion (AT) and Elaboration (EL).window approach to be more effective in findingthose, without inducing much noise for the labels.This clearly demonstrates the potential of TSP SWfor datasets with even more leaky boundaries e.g.,the Dutch (Vliet and Redeker, 2011) and the Ger-man Potsdam (Stede, 2004) corpora.Error analysis reveals that although TSP SWfinds more correct structures, a corresponding im-provement in labeling relations is not present be-cause in a few cases, it tends to induce noise fromthe neighboring sentences for the labels.
For ex-ample, when parsing was performed on the firstsentence in Figure 1 in isolation using 1S-1S, ourparser rightly identifies the Contrast relation be-tween EDUs 2 and 3.
But, when it is consideredwith its neighboring sentences by the sliding win-dow, the parser labels it as Elaboration.
A promis-ing strategy to deal with this and similar problemsthat we plan to explore in future, is to apply bothapproaches to each sentence and combine them byconsolidating three probabilistic decisions, i.e.
theone from 1S-1S and the two from sliding window.To further analyze the errors made by our parseron the hardest task of relation labeling, Figure 9presents the confusion matrix for TSP 1-1 on theRST-DT test set.
The relation labels are orderedaccording to their frequency in the RST-DT train-ing set.
In general, the errors are produced by twodifferent causes acting together: (i) imbalanceddistribution of the relations, and (ii) semantic sim-ilarity between the relations.
The most frequentrelation Elaboration tends to mislead others es-pecially, the ones which are semantically similar(e.g., Explanation, Background) and less frequent(e.g., Summary, Evaluation).
The relations whichare semantically similar mislead each other (e.g.,Temporal:Background, Cause:Explanation).These observations suggest two ways to im-prove our parser.
We would like to employ a morerobust method (e.g., ensemble methods with bag-ging) to deal with the imbalanced distribution ofrelations, along with taking advantage of a richersemantic knowledge (e.g., compositional seman-tics) to cope with the errors caused by semanticsimilarity between the rhetorical relations.7 ConclusionIn this paper, we have presented a novel discourseparser that applies an optimal parsing algorithmto probabilities inferred from two CRF models:one for intra-sentential parsing and the other formulti-sentential parsing.
The two models exploittheir own informative feature sets and the distribu-tional variations of the relations in the two parsingconditions.
We have also presented two novel ap-proaches to combine them effectively.
Empiricalevaluations on two different genres demonstratethat our approach yields substantial improvementover existing methods in discourse parsing.AcknowledgmentsWe are grateful to Frank Tompa and the anony-mous reviewers for their comments, and theNSERC BIN and CGS-D for financial support.494ReferencesO.
Biran and O. Rambow.
2011.
Identifying Justi-fications in Written Dialogs by Classifying Text asArgumentative.
International Journal of SemanticComputing, 5(4):363?381.L.
Carlson, D. Marcu, and M. Okurowski.
2002.
RSTDiscourse Treebank (RST-DT) LDC2002T07.
Lin-guistic Data Consortium, Philadelphia.V.
Feng and G. Hirst.
2012.
Text-level Discourse Pars-ing with Rich Linguistic Features.
In Proceedingsof the 50th Annual Meeting of the Association forComputational Linguistics, ACL ?12, pages 60?68,Jeju Island, Korea.
Association for ComputationalLinguistics.S.
Fisher and B. Roark.
2007.
The Utility of Parse-derived Features for Automatic Discourse Segmen-tation.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics, ACL?07, pages 488?495, Prague, Czech Republic.
Asso-ciation for Computational Linguistics.M.
Galley and K. McKeown.
2003.
Improving WordSense Disambiguation in Lexical Chaining.
In Pro-ceedings of the 18th International Joint Conferenceon Artificial Intelligence, IJCAI ?07, pages 1486?1488, Acapulco, Mexico.H.
Hernault, H. Prendinger, D. duVerle, andM.
Ishizuka.
2010.
HILDA: A Discourse ParserUsing Support Vector Machine Classification.
Dia-logue and Discourse, 1(3):1?33.S.
Joty, G. Carenini, and R. T. Ng.
2012.
A NovelDiscriminative Framework for Sentence-Level Dis-course Analysis.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, EMNLP-CoNLL ?12, pages 904?915, Jeju Island, Korea.
Association for Computa-tional Linguistics.H.
LeThanh, G. Abeysinghe, and C. Huyck.
2004.Generating Discourse Structures for Written Texts.In Proceedings of the 20th international confer-ence on Computational Linguistics, COLING ?04,Geneva, Switzerland.
Association for Computa-tional Linguistics.A.
Louis, A. Joshi, and A. Nenkova.
2010.
DiscourseIndicators for Content Selection in Summarization.In Proceedings of the 11th Annual Meeting of theSpecial Interest Group on Discourse and Dialogue,SIGDIAL ?10, pages 147?156, Tokyo, Japan.
Asso-ciation for Computational Linguistics.W.
Mann and S. Thompson.
1988.
Rhetorical Struc-ture Theory: Toward a Functional Theory of TextOrganization.
Text, 8(3):243?281.D.
Marcu and A. Echihabi.
2002.
An UnsupervisedApproach to Recognizing Discourse Relations.
InProceedings of the 40th Annual Meeting on Associa-tion for Computational Linguistics, ACL ?02, pages368?375.
Association for Computational Linguis-tics.D.
Marcu.
2000a.
The Rhetorical Parsing of Unre-stricted Texts: A Surface-based Approach.
Compu-tational Linguistics, 26:395?448.D.
Marcu.
2000b.
The Theory and Practice of Dis-course Parsing and Summarization.
MIT Press,Cambridge, MA, USA.J.
Morris and G. Hirst.
1991.
Lexical CohesionComputed by Thesaural Relations as an Indicatorof Structure of Text.
Computational Linguistics,17(1):21?48.R.
Prasad, A. Joshi, N. Dinesh, A. Lee, E. Miltsakaki,and B. Webber.
2005.
The Penn Discourse Tree-Bank as a Resource for Natural Language Gener-ation.
In Proceedings of the Corpus LinguisticsWorkshop on Using Corpora for Natural LanguageGeneration, pages 25?32, Birmingham, U.K.S.
Somasundaran, 2010.
Discourse-Level Relations forOpinion Analysis.
PhD thesis, University of Pitts-burgh.R.
Soricut and D. Marcu.
2003.
Sentence LevelDiscourse Parsing Using Syntactic and Lexical In-formation.
In Proceedings of the 2003 Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics on Human Lan-guage Technology, NAACL-HLT ?03, pages 149?156, Edmonton, Canada.
Association for Computa-tional Linguistics.C.
Sporleder and M. Lapata.
2005.
Discourse Chunk-ing and its Application to Sentence Compression.In Proceedings of the conference on Human Lan-guage Technology and Empirical Methods in Nat-ural Language Processing, pages 257?264, Van-couver, British Columbia, Canada.
Association forComputational Linguistics.C.
Sporleder and A. Lascarides.
2004.
Combining Hi-erarchical Clustering and Machine Learning to Pre-dict High-Level Discourse Structure.
In Proceed-ings of the 20th international conference on Compu-tational Linguistics, COLING ?04, Geneva, Switzer-land.
Association for Computational Linguistics.M.
Stede.
2004.
The Potsdam Commentary Corpus.In Proceedings of the ACL-04 Workshop on Dis-course Annotation, Barcelona.
Association for Com-putational Linguistics.R.
Subba and B. Di-Eugenio.
2009.
An Effective Dis-course Parser that Uses Rich Linguistic Information.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, HLT-NAACL ?09, pages 566?574, Boul-der, Colorado.
Association for Computational Lin-guistics.495C.
Sutton and A. McCallum.
2012.
An Introductionto Conditional Random Fields.
Foundations andTrends in Machine Learning, 4(4):267?373.C.
Sutton, A. McCallum, and K. Rohanimanesh.
2007.Dynamic Conditional Random Fields: FactorizedProbabilistic Models for Labeling and SegmentingSequence Data.
Journal of Machine Learning Re-search (JMLR), 8:693?723.S.
Verberne, L. Boves, N. Oostdijk, and P. Coppen.2007.
Evaluating Discourse-based Answer Extrac-tion for Why-question Answering.
In Proceedingsof the 30th annual international ACM SIGIR confer-ence on Research and development in informationretrieval, pages 735?736, Amsterdam, The Nether-lands.
ACM.N.
Vliet and G. Redeker.
2011.
Complex Sentences asLeaky Units in Discourse Parsing.
In Proceedingsof Constraints in Discourse, Agay-Saint Raphael,September.496
