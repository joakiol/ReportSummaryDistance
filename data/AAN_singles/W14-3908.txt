Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 73?79,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsWord-level Language Identification using CRF: Code-switching SharedTask Report of MSR India SystemGokul ChittaranjanMicrosoft Research Indiat-gochit@microsoft.comYogarshi Vyas?University of Marylandyogarshi@cs.umd.eduKalika Bali Monojit ChoudhuryMicrosoft Research India{kalikab, monojitc}@microsoft.comAbstractWe describe a CRF based system forword-level language identification ofcode-mixed text.
Our method uses lexical,contextual, character n-gram, and specialcharacter features, and therefore, caneasily be replicated across languages.
Itsperformance is benchmarked against thetest sets provided by the shared task oncode-mixing (Solorio et al., 2014) forfour language pairs, namely, English-Spanish (En-Es), English-Nepali (En-Ne),English-Mandarin (En-Cn), and StandardArabic-Arabic (Ar-Ar) Dialects.
Theexperimental results show a consistentperformance across the language pairs.1 IntroductionCode-mixing and code-switching in conversationshas been an extensively studied topic for sev-eral years; it has been analyzed from structural,psycholinguistic, and sociolinguistic perspec-tives (Muysken, 2001; Poplack, 2004; Senaratne,2009; Boztepe, 2005).
Although bilingualismis very common in many countries, it has sel-dom been studied in detail in computer-mediated-communication, and more particularly in socialmedia.
A large portion of related work (Androut-sopoulos, 2013; Paolillo, 2011; Dabrowska, 2013;Halim and Maros, 2014), does not explicitly dealwith computational modeling of this phenomena.Therefore, identifying code-mixing in social me-dia conversations and the web is a very relevanttopic today.
It has garnered interest recently, inthe context of basic NLP tasks (Solorio and Liu,2008b; Solorio and Liu, 2008a), IR (Roy et al.,2013) and social media analysis (Lignos and Mar-cus, 2013).
It should also be noted that the identi-?The author contributed to this work during his intern-ship at Microsoft Research Indiafication of languages due to code-switching is dif-ferent from identifying multiple languages in doc-uments (Nguyen and Dogruz, 2013), as the dif-ferent languages contained in a single documentmight not necessarily be due to instances of codeswitching.In this paper, we present a system built withoff-the-shelf tools that utilize several character andword-level features to solve the EMNLP Code-Switching shared task (Solorio et al., 2014) oflabeling a sequence of words with six tags viz.lang1, lang2, mixed, ne, ambiguous, and others.Here, lang1 and lang2 refer to the two languagesthat are mixed in the text, which could be English-Spanish, English-Nepali, English-Mandarin orStandard Arabic-dialectal Arabic.
mixed refersto tokens with morphemes from both, lang1 andlang2, ne are named entities, a word whose labelcannot be determined with certainty in the givencontext is labeled ambiguous, and everything elseis tagged other (Smileys, punctuations, etc.
).The report is organized as follows.
In Sec.
2,we present an overview of the system and detailout the features.
Sec.
3 describes the training ex-periments to fine tune the system.
The shared taskresults on test data provided by the organizers isreported and discussed in Sec.
4.
In Sec.
5 we con-clude with some pointers to future work.2 System overviewThe task can be viewed as a sequence labelingproblem, where, like POS tagging, each token in asentence needs to be labeled with one of the 6 tags.Conditional Random Fields (CRF) are a reason-able choice for such sequence labeling tasks (Laf-ferty et al., 2001); previous work (King and Ab-ney, 2013) has shown that it provides good perfor-mance for the language identification task as well.Therefore, in our work, we explored various tokenlevel and contextual features to build an optimalCRF using the provided training data.
The features73Lang.
Given Ids Available Available (%)Train Test Train Test Train TestEs 11,400 3,014 11,400 1,672 100% 54.5%Ne 9,999 3,018 9,296 2,874 93% 95.2%Cn 999 316 995 313 99.6% 99.1%Ar 5,839 2,363 5,839 2,363 100% 100%Ar 2 - 1,777 - 1,777 - 100%Table 2: Number of tweets retrieved for the vari-ous datasets.used can be broadly grouped as described below:Capitalization Features: They capture if let-ter(s) in a token has been capitalized or not.
Thereason for using this feature is that in several lan-guages, capital Roman letters are used to denoteproper nouns which could correspond to namedentities.
This feature is meaningful only for lan-guages which make case distinction (e.g., Roman,Greek and Cyrillic scripts).Contextual Features: They constitute the cur-rent and surrounding tokens and the length of thecurrent token.
Code-switching points are contextsensitive and depend on various structural restric-tions (Muysken, 2001; Poplack, 1980).Special Character Features: They capture theexistence of special characters and numbers in thetoken.
Tweets contain various entities like hash-tags, mentions, links, smileys, etc., which are sig-naled by #, @ and other special characters.Lexicon Features: These features indicate theexistence of a token in lexicons.
Common wordsin a language and named entities can be curatedinto finite, manageable lexicons and were there-fore used for cases where such data was available.Character n-gram features: Following Kingand Abney (2013), we also used charagter n-gramsfor n=1 to 5.
However, instead of directly usingthe n-grams as features in the CRF, we trainedtwo binary maximum entropy classifiers to identifywords of lang1 and lang2.
The classifiers returnedthe probability that a word is of lang1 (or lang2),which were then binned into 10 equal buckets andused as features.The features are listed in Table 1.3 Experiments3.1 Data extraction and pre-processingThe ruby script provided by the shared task orga-nizers was used to retrieve tweets for each of thelanguage pairs.
Tweets that could not be down-loaded either because they were deleted or pro-Source Language Forinstance types en.nt.bz21English NEinstance types es.nt.bz21Spanish NEeng wikipedia 2010 1M-text.tar.gz2English FWspa wikipedia 2011 1M-text.tar.gz2Spanish FWTable 3: External resources used in the task.1http://wiki.dbpedia.org/Download,2http://corpora.uni-leipzig.de/download.html; NE:Named entities, FW:Word fre-quency listtected were excluded from the training set.
Ta-ble 2 shows the number of tweets that we wereable to retrieve for the released datasets.
Further,we found a few rare cases of tokenization errors,as evident from the occurrence of spaces withintokens.
These were not removed from the trainingset and instead, the spaces in these tokens were re-placed by an underscore.3.2 Feature extraction and labelingNamed entities for English and Spanish wereobtained from DBPedia instance types, namely,Agent, Award, Device, Holiday, Language, Mean-sOfTransportation, Name, PersonFunction, Place,and Work.
Frequency lists for these languageswere obtained from the Leipzig Copora Collec-tion(Quasthoff et al., 2006); words containing spe-cial characters and numbers were removed fromthe list.
The files used are listed in table 3.
Thecharacter n-gram classifiers were implementedusing the MaxEnt classifier provided in MAL-LET (McCallum, 2002).
The classifiers weretrained on 6,000 positive examples randomly sam-pled from the training set and negative examplessampled from both, the training set and from wordlists of multiple languages from (Quasthoff et al.,2006); the number of examples used for each ofthese classifiers is given in Table 4.We used CRF++ (Kudo, 2014) for labeling thetweets.
For all language pairs, CRF++ was rununder its default settings.3.3 Model selectionFor each language pair, we experimented with var-ious feature combinations using 3-fold cross vali-dation on the released training sets.
Table 5 reportsthe token-level labeling accuracies for the variousmodels, based on which the optimal feature setsfor each language pairs were chosen.
These opti-mal features are reported in Table 1, and the cor-responding performance for 3-fold cross valida-tion in Table 5.
The final runs submitted for the74ID Feature Description Type Features used in the final submission (Optimal set)En-Es En-Ne En-Cn Ar-ArCapitalization FeaturesCAP1 Is first letter capitalized?
True/False 3 3 3 NACAP2 Is any character capitalized?
True/False 3 3 3 NACAP3 Are all characters capitalized?
True/False 3 3 3 NAContextual FeaturesCON1 Current Token String 3 3 3 3CON2 Previous 3 and next 3 tokens Array (Strings) 3 3 3CON3 Word length String 3 3 3 3Special Character FeaturesCHR0 Is English alphabet word?
True/False 3 NACHR1 Contains @ in locations 2-end True/False 3 3 3 3CHR2 Contains # in locations 2-end True/False 3 3 3 3CHR3 Contains ?
in locations 2-end True/False 3 3 3 3CHR4 Contains / in locations 2-end True/False 3 3 3 3CHR5 Contains number in locations 2-end True/False 3 3 3 3CHR6 Contains punctuation in locations 2-endTrue/False 3 3 3 3CHR7 Starts with @ True/False 3 3 3 3CHR8 Starts with # True/False 3 3 3 3CHR9 Starts with ?
True/False 3 3 3 3CHR10 Starts with / True/False 3 3 3 3CHR11 Starts with number True/False 3 3 3 3CHR12 Starts with punctuation True/False 3 3 3 3CHR13 Token is a number?
True/False 3 3 3 3CHR14 Token is a punctuation?
True/False 3 3 3 3CHR15 Token contains a number?
True/False 3 3 3 3Lexicon FeaturesLEX1 In lang1 dictionary of most frequentwords?True/False 3 3 3 NALEX2 In lang2 dictionary of most frequentwords?True/False 3 NA NALEX3 Is NE?
True/False 3 3 NA NALEX4 Is Acronym True/False 3 3 NA NACharacter n-gram FeaturesCNG0 Output of two MaxEnt classifiersthat classify lang1 vs. others andlang2 vs. others.
This gives 2 prob-ability values binned into 10 bns,two from each classifier, for the twoclasses.Array (binnedprobability)3 3 NA NACRF Feature Type U U U BTable 1: A description of features used.
NA refers to features that were either not applicable to thelanguage pair or were not available.
B/U implies that the CRF has/does not have access to the featuresof the previous token.75Classifier Languages used (And # words)English-Spanish Language PairSpanish vs Others [es (6000)], [en (4000), fr (500), hi (500), it (500), po (500)]English vs Others [en (6000)], [es (4000), fr (500), hi (500), it (500), po (500)]English-Nepali Language PairNepali vs Others [ne (6000)], [en (3500), fr (500), hi (500), it (500), po (500)]English vs Others [en (6000)], [ne (3500), fr (500), hi (500), it (500), po (500)]Standard Arabic vs. Arabic DialectsStd vs. Dialect [lang1 (9000)], [lang2 (3256)]Table 4: Data to train character n-gram classifiers.shared task, including those for the surprise testsets, use the corresponding optimal feature sets foreach language pair.Feature Context Language PairEn-EsEn-Ne?En-CnAr-ArAr-Ar(2)Development SetAll B 92.8 94.3 93.1 85.5 -- CON2 B 93.8 95.6 94.9 81.2 -- CHR* B 92.3 93.5 91.0 85.3 -- CAP* B 92.7 94.2 90.1 - -- CON2 U 93.0 94.3 93.1 85.6 -- CNG0 B 92.7 94.2 - - -- LEX* B 92.7 94.1 - - -Optimal - 95.0 95.6 95.0 85.5 -Results on Test data for the optimal feature setsRegular 85.0 95.2 90.4 90.1 53.6Surprise 91.8 80.8 - 65.0 -Table 5: The overall token labeling accuracies (in%) for all language pairs on the training and testdatasets.
?-?
indicates the removal of the givenfeature.
?*?
is used to indicate a group of features.Refer tab.
1) for the feature Ids and the optimalset.
B and U stand for bigram and unigram respec-tively, where the former refers to the case when theCRF had access to features of the current and pre-vious tokens, and the latter to the case where theCRF had access only to the features of the currenttoken.
?
: Lexical resources available for En only.4 Results and Observations4.1 Overall token labeling accuracyThe overall token labeling accuracies for the regu-lar and surpise test sets (wherever applicable) anda second set of dialectal and standard Arabic arereported in the last two rows of Table 5.
The sametable also reports the results of the 3-fold cross val-idation on the training datasets.
Several importantobservations can be made from these accuracy val-ues.Firstly, accuracies observed during the trainingphase was quite high (?
95%) and exactly simi-lar for En-Es, En-Ne and En-Cn data; but for Ar-Ar dataset our method could achieve only up to85% accuracy.
We believe that this is due to un-availability of any of the lexicon features, whichin turn was because we did not have access to anylexicon for dialectal Arabic.
While complete setof lexical features were not available for En-Cn aswell, we did have English lexicon; also, we no-ticed that in the En-Cn dataset, almost always theEn words were written in Roman script and the Cnwords were written in the Chinese script.
Hence,in this case, script itself is a very effective featurefor classification, which has been indirectly mod-eled by the CHR0 feature.
On the other hand, inthe Ar-Ar datasets, both the dialects are written us-ing the same script (Arabic).
Further, we foundthat using the CNG0 feature that is obtained bytraining a character n-gram classifier for the lan-guage pairs resulted in the drop of performance.Since we are not familiar with arabic scripts, weare not sure how effective the character n-grambased features are in differentiating between thestandard and the dialectal Arabic.
Based on ourexperiment with CNG0, we hypothesize that thedialects may not show a drastic difference in theircharacter n-gram distributions and therefore maynot contribute to the performance of our system.Secondly, we observe that effectiveness of thedifferent feature sets vary across language pairs.Using all the features of the previous words (con-text = B) seems to hurt the performance, thoughjust looking at the previous 3 and next 3 tokenswas useful.
On the other hand, in Ar-Ar the re-verse has been observed.
Apart from lexicons,76character n-grams seems to be a very useful fea-ture in En-Es classification.
As discussed above,CHR* features are effective for En-Cn because,among other things, one of these features also cap-tures whether the word is in Roman script.
For En-Ne, we do not see any particular feature or sets offeatures that strongly influence the classification.The overall token labeling accuracy of theshared task runs, at least in some cases, differ quitesignificantly from our 3-fold cross validation re-sults.
On the regular test sets, the results for En-Ne is very similar to, and En-Cn and Ar-Ar arewithin expected range of the training set results.However, we observe a 10% drop in En-Es.
Weobserve an even bigger drop in the accuracy of thesecond Ar-Ar test set.
We will discuss the possiblereason for this in the next subsection.
The accura-cies on the surprise sets do not show any specifictrend.
While for En-Es the accuracy is higher by5% for the surprise set than the regular set, En-Neand Ar-Ar show the reverse, and a more expectedtrend.
The rather drastic drops in the accuracy forthese two pairs on the surprise sets makes erroranalysis and comparative analysis of the training,test and surprise datasets imperative.4.2 Error AnalysisTable 6 reports the F-scores for the six labels, i.e.,classes, and also an overall tweet/post level accu-racy.
The latter is defined as the percentage of in-put units (which could be either a tweet or a post orjust a sentence depending on the dataset) that arecorrectly identified as either code-mixed or mono-lingual; an input unit is considered code-mixed ifthere is at least one word labeled as lang1 and oneas lang2.For all the language pairs other than Arabic, theF-score for NE is much lower than that for lang1and lang2.
Thus, the performance of the systemcan be significantly improved by identifying NEsbetter.
Currently, we have used lexicons for onlyEnglish and Spanish.
This information was notavailable for the other languages, namely, Nepali,Mandarin, and Arabic.
The problem of NE detec-tion is further compounded by the informal natureof sentences, because of which they may not al-ways be capitalized or spelt properly.
Better de-tection of NEs in code-mixed and informal text isan interesting research challenge that we plan totackle in the future.Note that the ambiguous and mixed classes canbe ignored because their combined occurrence isless than 0.5% in all the datasets, and hence theyhave practically no effect on the final labeling ac-curacy.
In fact, their rarity (especially in the train-ing set) is also the reason behind the very poor F-scores for these classes.
In En-Cn, we also observea low F-score for other.In the Ar-Ar training data as well as the test set,there are fewer words of lang2, i.e., dialectal Ara-bic.
Since our system was trained primarily on thecontext and word features (and not lexicon or char-acter n-grams), there was not enough examples inthe training set for lang2 to learn a reliable modelfor identifying lang2.
Moreover, due to the dis-tributional skew, the system learnt to label the to-kens as lang1 with very high probability.
The highaccuracy in the Ar-Ar original test set is because81.5% of the tokens were indeed of type lang1in the test data while only 0.26% were labeled aslang2.
This is also reflected by the fact that thoughthe F-score for lang2 in Ar-Ar test set is 0.158, theoverall accuracy is still 90.1% because F-score forlang1 is 94.2%.As shown in Table 7, the distribution of theclasses in the second Ar-Ar test set and the sur-prise set is much less skewed and thus, very differ-ent from that of the training and original test sets.In fact, words of lang2 occur more frequently inthese sets than those of lang1.
This difference inclass distributions, we believe, is the primary rea-son behind the poorer performance of the systemon some of the Ar-Ar test sets.We also observe a significant drop in accuracyfor En-Ne surprise data, as compared to the accu-racy on the regular En-Ne test and training data.We suspect that it could be either due to the dif-ference in the class distribution or the genre/styleof the two datasets, or both.
An analysis of thesurprise test set reveals that a good fraction ofthe data consist of long song titles or part of thelyrics of various Nepali songs.
Many of thesewords were labeled as lang2 (i.e., Nepali) by oursystem, but were actually labeled as NEs in thegold annotations1While song titles can certainlybe considered as NEs, it is very difficult to iden-tify them without appropriate resources.
It shouldhowever be noted that the En-Ne surprise set hasonly 1087 tokens, which is too small to base anystrong claims or conclusions on.1Confirmed by the shared task organizers over email com-munication.77Language Pair F-measure (Token-level) Accuracy ofAmbiguous lang1 lang2 mixed NE Other Comment/PostEn-Es 0.000 0.856 0.879 0.000 0.156 0.856 82.1En-Ne - 0.948 0.969 0.000 0.454 0.972 95.3En-Cn - 0.980 0.762 0.000 0.664 0.344 81.8Ar-Ar 0.000 0.942 0.158 - 0.577 0.911 94.7Ar-Ar (2) 0.015 0.587 0.505 0.000 0.424 0.438 71.4En-Es Surprise 0.000 0.845 0.864 0.000 0.148 0.837 81.5En-Ne Surprise - 0.785 0.874 - 0.370 0.808 71.6Ar-Ar Surprise 0.000 0.563 0.698 0.000 0.332 0.966 84.8Table 6: Class-wise F-scores and comment/post level accuracy of the submitted runs.Dataset Percentage ofAmb.
lang1 lang2 mixed NE OtherTraining 0.89 66.36 13.60 0.01 11.83 7.30Test-1 0.02 81.54 0.26 0.00 10.97 7.21Test-2 0.37 32.04 45.34 0.01 13.24 9.01Surprise 0.91 22.36 57.67 0.03 9.13 9.90Table 7: Distribution (in %) of the classes in thetraining and the three test sets for Ar-Ar.5 ConclusionIn this paper, we have described a CRF based wordlabeling system for word-level language identifi-cation of code-mixed text.
The system relies onannotated data for supervised training and alsolexicons of the languages, if available.
Charactern-grams of the words were also used in a MaxEntclassifier to detect the language of a word.
Thisfeature has been found to be useful for some lan-guage pairs.
Since none of the techniques or con-cepts used here is language specific, we believethat this approach is applicable for word labelingfor code-mixed text between any two (or more)languages as long as annotated data is available.This is demonstrated by the fact that the sys-tem performs more or less consistently with accu-racies ranging from 80% - 95% across four lan-guage pairs (except for the case of Ar-Ar secondtest set and the surprise set which is due to starkdistributional differences between the training andtest sets).
NE detection is one of the most chal-lenging problems, improving which will definitelyimprove the overall performance of our system.
Itwill be interesting to explore semi-supervised andunsupervised techniques for solving this task be-cause creating annotated datasets is expensive andeffort-intensive.ReferencesJannis Androutsopoulos.
2013.
Code-switching incomputer-mediated communication.
In Pragmaticsof Computer-mediated Communication, pages 667?694.
Berlin/Boston: de Gruyter Mouton.Erman Boztepe.
2005.
Issues in code-switching:competing theories and models.
Teachers College,Columbia University Working Papers in TESOL &Applied Linguistics, 3.2.Marta Dabrowska.
2013.
Functions of code-switchingin polish and hindi facebook users?
post.
StudiaLinguistica Universitatis Lagellonicae Cracovien-sis, 130:63?84.Nur Syazwani Halim and Marlyana Maros.
2014.The functions of code-switching in facebook inter-actions.
In Proceedings of the International Con-ference on Knowledge-Innovation-Excellence: Syn-ergy in Language Research and Practice; Social andBehavioural Sciences, volume 118, pages 126?133.Ben King and Steven Abney.
2013.
Labeling the lan-guages of words in mixed-language documents us-ing weakly supervised methods.
In Proceedings ofNAACL-HLT, pages 1110?1119.Taku Kudo.
2014.
Crf++: Yet another crftoolkit.
http://crfpp.googlecode.com/svn/trunk/doc/index.html?source=navbar#links, Retrieved 11.09.2014.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the InternationalConference on Machine Learning (ICML), pages282?289.Constantine Lignos and Mitch Marcus.
2013.
Towardweb-scale analysis of codeswitching.
In 87th An-nual Meeting of the Linguistic Society of America.Andrew Kachites McCallum.
2002.
Mallet: A ma-chine learning for language toolkit.
http://mallet.cs.umass.edu.Pieter Muysken.
2001.
The study of code-mixing.
InBilingual Speech: A typology of Code-Mixing.
Cam-bridge University Press.78Dong Nguyen and A. Seza Dogruz.
2013.
Word levellanguage identification in online multilingual com-munication.
In Proceedings of the 2013 Conferenceon Empirical Methods in natural Language Process-ing, pages 857?862.John C. Paolillo.
2011.
Conversational codeswitch-ing on usenet and internet relay chat.
Lan-guage@Internet, 8.Shana Poplack.
1980.
Sometimes i?ll start a sentencein Spanish y termino en espanol: Toward a typologyof code-switching.
Linguistics, 18:581?618.Shana Poplack.
2004.
Code-switching.
In U. Am-mon, N. Dittmar, K.K.
Mattheier, and P. Turdgill,editors, Soziolinguistik.
An international handbookof the science of language.
Walter de Gruyter.U.
Quasthoff, M. Richter, and C. Biemann.
2006.
Cor-pus portal for search in monolingual corpora.
InProceedings of the fifth International Conference onLanguage Resource and Evaluation, pages 1799?1802.Rishiraj Saha Roy, Monojit Choudhury, Prasenjit Ma-jumder, and Komal Agarwal.
2013.
Overview anddatasets of fire 2013 track on transliterated search.In Proceedings of the FIRE 2013 Shared Task onTransliterated Search.Chamindi Dilkushi Senaratne, 2009.
Sinhala-Englishcode-mixing in Sri Lanka: A sociolinguistic study,chapter Code-mixing as a research topic.
LOT Pub-lications.Thamar Solorio and Yang Liu.
2008a.
Learning to pre-dict code-switching points.
In Proceedings of theEmpirical Methods on Natural Language Process-ing (EMNLP), pages 973?981.Thamar Solorio and Yang Liu.
2008b.
Part-of-speechtagging for English-Spanish code-switched text.
InProceedings of the Empirical Methods on NaturalLanguage Processing (EMNLP), pages 1051?1060.Thamar Solorio, Elizabeth Blair, Suraj Maharjan, SteveBethard, Mona Diab, Mahmoud Gonheim, AbdelatiHawwari, Fahad AlGhamdi, Julia Hirshberg, AlisonChang, and Pascale Fung.
2014.
Overview for thefirst shared task on language identifiation in code-switched data.
In Proceedings of the First Workshopon Computational Approaches to Code-Switching.Conferencfe on Empirical Methods in Natural Lan-guage Processing.79
