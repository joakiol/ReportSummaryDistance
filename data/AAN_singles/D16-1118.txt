Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1098?1107,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsAutomatic Extraction of Implicit Interpretations from Modal ConstructionsJordan Sanders and Eduardo BlancoHuman Intelligence and Language Technologies LabUniversity of North TexasDenton, TX, 76203jordansanders3@my.unt.edu, eduardo.blanco@unt.eduAbstractThis paper presents an approach to extractimplicit interpretations from modal construc-tions.
Importantly, our approach uses a de-terministic procedure to normalize eventuali-ties and generate potential interpretations.
Anannotation effort demonstrates that these in-terpretations are intuitive to humans and mostmodal constructions convey at least one inter-pretation.
Experimental results show that thetask is challenging but can be automated.1 IntroductionPeople use language to communicate not only facts,but also intentions, uncertain information and pointsof view.
Modality can be broadly defined as a gram-matical phenomenon used to express the speaker?sopinion or attitude towards a proposition (Lyons,1977).
Modality has also been defined as ?the cate-gory of meaning used to talk about possibilities andnecessities, essentially, states of affairs beyond theactual.?
(Hacquard, 2011).
Within computationallinguistics, processing modality has proven usefulfor, among others, recognizing textual entailment(Snow et al, 2006; MacCartney et al, 2006), ma-chine translation (Murata et al, 2005; Baker et al,2012), and sentiment analysis (Wiebe et al, 2005).In the absence of modality markers, it is un-derstood that the author of a proposition agreeswith it (Hengeveld and Mackenzie, 2008).
Addinga modality marker?also referred to as cue?castsdoubt on the truth of the proposition, e.g., Mary gota new job last week vs. Mary likely got a new job lastweek.
Modality is surprisingly common (Moranteand Sporleder, 2012), and notoriously difficult to an-notate and process automatically (Rubinstein et al,2013; Vincze et al, 2011).
In MEDLINE, 11% ofsentences contain speculative language (Light et al,2004) and in biomedical abstracts, 18% (Vincze etal., 2008).
Rubin (2006) reports that 59% of state-ments in 80 New York Times articles include epis-temic modality.
Despite modality being ubiquitous,there is not an agreed upon annotation schema.In this paper, we extract implicit interpretationsintuitively understood by humans when readingmodal constructions.
We do not follow any specifictheory of modality.
Instead, we manipulate modalconstructions to automatically generate potential in-terpretations, and then assign factuality scores tothem.
Consider statement (1) below:1.
John likely contracted the disease when amouse bit him in the Adirondacks.Even though likely syntactically attaches to con-tracted, a natural reading suggests that John con-tracted the disease is factual; the only bit of un-certain information is how (or when) he contractedthe disease.
In other words, assuming that the au-thor of statement (1) is truthful, event contractedoccurred with AGENT John and THEME the disease,but the MANNER (or TIME) may not have been whena mouse bit him in the Adirondacks.A key feature of the work presented in this paperis that the interpretations extracted from modal con-structions are not tied to any syntactic or semanticrepresentation.
Given modal constructions in plaintext, we extract implicit interpretations in plain text,and these interpretations can be processed with anyexisting NLP pipeline.
The main contributions of1098this paper are: (1) procedure to automatically gen-erate potential interpretations from modal construc-tions; (2) annotations assessing the factuality of po-tential interpretations generated from OntoNotes;1and (3) experimental results using several features.2 Previous WorkTheoretical works in philosophy and linguistics havestudied modality for decades (Palmer, 2001; Jes-persen, 1992).
Morante and Sporleder (2012) sum-marize some of these works and related phenom-ena, e.g., evidentiality, certainty, factuality, subjec-tivity.
There are several expressions that have modalmeanings (Fintel, 2006), including auxiliaries (must,should, etc.
), adverbs (perhaps, possibly, etc.)
nouns(possibility, chance, etc.)
adjectives (necessary, pos-sible, etc.)
and conditionals (e.g., If the light is on,Sandy is home).
Most previous works in computa-tional linguistics target modal adverbs (Rubinstein etal., 2013; Carretero and Zamorano-Mansilla, 2013;de Waard and Maat, 2012), and some also targetother modal triggers such as reporting verbs (e.g.,The evidence suggests that he caused the fire), ref-erences, or all verbs (Diab et al, 2009).
Followingthese previous works, we focus on modal adverbs.Beyond theoretical works, there are many propos-als to annotate modality.
Doing so has proven chal-lenging: following different annotations schemas onthe same source text yields little overlap (Vinczeet al, 2011), and Carretero and Zamorano-Mansilla(2013) present an analysis of disagreements whentargeting modal adverbs.
Annotation schemas typi-cally include 3 tasks: identifying modality triggers,their scopes, and sources (Quaresma et al, 2014;Sa?nchez and Vogel, 2015).
Many also classify themodality into several types (epistemic, circumstan-tial, ability, deontic, etc.)
or a fine-grained taxonomy(Rubinstein et al, 2013; Nissim et al, 2013).
In thispaper, we are not concerned with modeling modal-ity per se, or classifying instances of modality intopredefined classes or hierarchies.
Instead, we extractimplicit interpretations from modal constructions inorder to mirror intuitive readings.FactBank is probably the best-known corpus forevent factuality (Saur??
and Pustejovsky, 2009).
Itwas created following carefully crafted annotation1Available at www.sanders.techguidelines and examples comprising 34 pages.2 Theguidelines detail a manual normalization step to?identify the full event that needs to be assessedin terms of its factuality?
(p. 12), and the anno-tation process includes identifying the sources thatare assessing factuality (p. 15).
de Marneffe et al(2012) reannotate a subset of FactBank with factual-ity values from the reader?s perspective?they call itveridicality?using crowdsourcing.
Both FactBankand de Marneffe et al (2012), rely on manual nor-malization to identify the eventuality whose factual-ity is being annotated.
Instead, we present an auto-mated approach: we manipulate semantic roles andsyntactic dependencies deterministically to generateseveral potential interpretations per modal construc-tion, and then assess their factuality.Many other efforts expand on FactBank us-ing crowdsourced annotations, different annotationschemas (usually simpler) or other domains.
Prab-hakaran et al (2012) use crowdsourcing to classifypropositions into 5 modalities: ability, effort, inten-tion, success and want.
Soni et al (2014) targetthe factuality of quotes (direct and indirect) in Twit-ter.
Lee et al (2015) detect events and assess fac-tuality using easy-to-understand short instructionsto crowdsource annotations.
Unlike us, they anno-tate factuality at the individual token level, whereannotated tokens are deemed events by annotators.Prabhakaran et al (2015) define and annotate propo-sitional heads with four categories: (1) non-beliefpropositions, or (2) committed, non-committed orreported belief.
Instead of assessing factuality onlyfor propositional heads (usually verbs, one assess-ment per proposition), we do so for potential inter-pretations automatically generated by manipulatingverbs and their arguments deterministically.All works cited in the previous two paragraphseither manually normalize text prior to assess-ing factuality?making automation from plain textimpossible?or assess factuality for tokens deemedevents (ordered, delay, agreed, etc.)
or full propo-sitions (a verb and all its arguments).
Unlike them,we automatically generate potential interpretationsfrom a single modal construction?or, equivalently,automatically generate several normalizations?andthen assess their factuality.2https://catalog.ldc.upenn.edu/docs/LDC2009T23/annotationGuidelines.pdf10993 Terminology and BackgroundWe use the term modal construction to refer to verb-argument structures modified by a modal adverb(possibly, probably, etc.).
We use the term implicitinterpretation, or interpretation to save space, torefer to meaning intuitively understood by humanswhen reading a modal construction.
Potential in-terpretations are interpretations automatically gen-erated whose factuality has yet to be determined.The factuality of an interpretation is a score indi-cating its likelihood?whether it is true, false or un-known given the modal construction.We work on top of OntoNotes (Hovy et al, 2006)because it includes text from several genres (news,broadcast and telephone conversations, weblogs,etc.)
and includes part-of-speech tags, parse trees,PropBank-style semantic roles and other linguisticinformation.3 Very briefly, PropBank (Palmer etal., 2005) has two kinds of semantic roles: num-bered roles (ARG0, ARG1, etc.
), which are definedin verb-specific framesets, and argument modifiers(ARGM-TMP, ARGM-LOC, etc.
), we refer the readerto the aforementioned reference, and the guidelinesand framesets4 for more details.
We transformed theparse trees in OntoNotes into syntactic dependenciesusing Stanford CoreNLP (Manning et al, 2014).4 Corpus CreationWe define a two-step procedure to create a corpusof modal constructions and the implicit interpreta-tions intuitively understood by humans when read-ing them.
First, we automatically generate potentialinterpretations from modal constructions by manip-ulating syntactic dependencies and semantic roles.Second, we manually score potential interpretationsaccording to their likelihood.
These interpretationsand scores are later used to learn how to score po-tential interpretations automatically (Section 6).4.1 Generating Potential InterpretationsSelecting Modal Constructions.
OntoNotes is alarge corpus containing 63,918 sentences.
Creatinga corpus of interpretations for all modal construc-tions is outside the scope of this paper.
In order3We use the CoNLL-2011 Shared Task distribution (Pradhanet al, 2011), http://conll.cemantix.org/2011/4http://propbank.github.io/to alleviate the annotation effort, we focus on se-lected modal constructions.
Specifically, we selectverb-argument structures that have one ARGM-ADVor ARGM-MNR role, and that role is one of the fol-lowing modal adverbs: certainly, clearly, definitely,likely, obviously, possibly, probably, surely, or un-likely.
These adverbs are the most frequent that sat-isfy the above filter.
Additionally, we discard verb-argument structures with to be as the main verb.These rules retrieve 324 modal constructions.Automatic Normalization.
Modal constructionsoften occur in long multi-clause sentences.
In orderto identify the eventuality from which potential in-terpretations should be generated, we automaticallynormalize the original sentence.
Normalizing con-sists of a battery of deterministic steps implementedusing syntactic dependencies and semantic roles.
Incontrast with previous work (Section 2), our normal-ization is fully automated.
Hereafter, we use verbto refer to the main verb in the modal construction,adverb to the modal adverb, and sem roles to all se-mantic roles in the modal construction.1.
Remove adverb.2.
Convert negated verb-argument structures intotheir positive counterparts.
We follow 3 stepsinspired by the rules to form negation proposedby (Huddleston and Pullum, 2002):(a) Remove the negation mark by deleting thetoken whose syntactic dependency is neg.
(b) Remove auxiliaries, expand contractions,and fix third-person singular and pasttense.
For example (before: after), doesn?tgo: goes, didn?t go: went, won?t go: willgo.
To implement this step, we loopthrough tokens whose head is the negatedverb with dependency aux, and use a listof irregular verbs5 and grammar rules toconvert to third-person singular and pasttense based on orthographic patterns.
(c) Rewrite negatively-oriented polarity-sensitive items.
For example (before:after), anyone: someone, any longer:still, yet: already.
at all: somewhat.We use the correspondences betweennegatively-oriented and positively-5https://en.wikipedia.org/wiki/English_irregular_verbs1100Sent.
1: The danger is [probably]ARGM-ADV [he]ARG0 [can]ARGM-MOD [not]ARGM-NEG [deliver]verb [the promises that hemade during the campaign.
]ARG1NormalizationStep Output1 The danger is he cannot deliver the promises that he made during the campaign.2 The danger is he can deliver the promises that he made during the campaign.3 The danger is he will deliver the promises that he made during the campaign.4 He will deliver the promises that he made during the campaign.5 He will deliver the promises that he made during the campaign.Sent.
2: [...] I wouldn?t define victory as simply not raising taxes?although [I]ARG0 , v1, v2 [definitely]ARGM-ADV, v1[would]ARGM-MOD, v1 [like]v1 [to [defer]v2 [raising taxes]ARG1 , v2 [as long as prudently possible.
]ARG2, v2 ]ARG1 , v1NormalizationStep Output1 I wouldn?t define [...] although I would like to defer raising taxes as long as prudently possible.2, 3 I would define [...] although I will like to defer raising taxes as long as prudently possible.4 I will like to defer raising taxes as long as prudently possible.5 Normalization 1: I will like to defer raising taxes as long as prudently possible.Normalization 2: I will defer raising taxes as long as prudently possible.Interpretations From Potential Interpretationnorm.
1 {ARG0} will like to defer raising taxes as long as prudently possible.I will like {to ARG1}.norm.
2{ARG0} will defer raising taxes as long as prudently possible.I will defer {ARG1} as long as prudently possible.I will defer raising taxes {ARG2}.I will defer {ARG1} {ARG2}.Table 1: Step-by-step execution of the procedure to automatically normalize modal constructions (Sentences 1 and 2) and generatepotential interpretations (Sentence 2).oriented polarity-sensitive items by(Huddleston and Pullum, 2002, pp.
831).3.
Fix modal verbs and tense.
If a modal verb(can, could, may, would, should, must, etc.
)has as syntactic head verb, we transform themodal construction into past or future depend-ing on the modal and tense of verb.
For exam-ple: could go: went, can go: will go, shouldhave gone: went.
We use the same grammarrules and list of irregular verbs as in Step (2b).4.
Select relevant tokens.
We remove all tokensin the original sentence except verb and tokensbelonging to the roles in sem roles.
Addition-ally, we fix phrasal verbs by adding tokens withthe part-of-speech tag RP whose syntactic headis verb and dependency type prt (semantic rolesin OntoNotes are annotated for verb tokens,missing the preposition when verb is a phrasalverb would inadvertently change meaning).
Wealso add all tokens to the left of verb until wefind the first token whose part-of-speech tagdoes not start with VB, MD, RB or EX (verbs,modals, adverbs and existential there).5.
Generate additional normalizations.
If verb isfollowed by TO + verb2 (e.g., want to go, liketo play, intend to pass), we generate an addi-tional normalization for verb2 after merging thesemantic roles of verb and verb2.Table 1 exemplifies the automatic normalizationstep by step with 2 modal constructions.Generating Potential Interpretations in PlainText.
Inspired by the rules Blanco and Sarabi (2016)used to generate interpretations from negation, wegenerate potential interpretations from modal con-structions by toggling off combinations of roles insem roles.
We consider numbered roles (ARG0?ARG5), and argument modifiers (ARGM-) ending inLOC, TMP, MNR, PRP, CAU, EXT, PRD or DIR.Table 1 lists some potential interpretations gener-ated from a sample modal construction.
The totalnumber of potential interpretations for the 324 se-lected modal construction is 1,756 (average: 5.4).We recognize that our procedure to generate im-plicit interpretations is unable to generate some use-ful interpretations.
For example, from This is [aperson who]ARG1 [likely]ARGM-ADV [died]verb [on im-pact versus perhaps freezing to death]ARGM-MNR , we1101generate This is a person who died {ARGM-MNR},which is factual: the only uncertain information isthe manner in which the person died.
Since we tog-gle off semantic roles of verb, our procedure is un-able to generate A person died on impact and A per-son died freezing to death; the former interpretationwould receive a higher factuality score than the lat-ter.
We argue that automation is preferable, and re-serve for future work generating interpretations thatrequire splitting semantic roles.4.2 Scoring Potential InterpretationsAfter automatically generating potential interpreta-tions, we collected manual annotations to determinetheir factuality.
The annotation interface showed theoriginal sentence containing the modal construction,the previous and next sentences as context, and noadditional information.
Following previous work(Saur??
and Pustejovsky, 2009; de Marneffe et al,2012), we found it useful not to restrict answersto yes or no, but to allow for degrees of certainty.Specifically, we asked ?Given the 3 sentences above,do you believe that the statement [potential interpre-tation] below is true??.
Answers are a score rangingfrom ?5 to 5, where ?5 indicates Certainly no, 5 in-dicates Certainly yes, and the scores in between indi-cate a continuum of certainty (0 indicates unknown).After pilot annotations, we examined disagree-ments and defined the following simple guidelines:1.
Context (previous sentence, target sentence,and next sentence) is taken into account.2.
World knowledge available at the time the orig-inal sentence was authored?not new knowl-edge available after?is taken into account.3.
Semantic roles toggled off are replaced witha semantically related substitute (Turney andPantel, 2010) for the original role, e.g., give:take, customer: sales associate.5 Corpus AnalysisThe total number of modal constructions selected is324 and the number of potential interpretations au-tomatically generated in 1,756 (average: 5.4 inter-pretation per modal construction).
39.4% of inter-pretations are scored with a high degree of certainty.We define high certainty as a score below ?3 (inter-pretation is false) or larger than 3 (interpretation is# roles toggled off # % 6= 0 Mean score> 0 < 00 345 87.25 3.96 -3.941 800 48.50 3.67 -3.902 479 20.46 3.55 -4.033 120 5.83 3.50 -3.00Table 2: Number of interpretations generated by toggling off 0,1, 2 or 3 roles (#), percentage of interpretations not scored zero(% 6= 0), and mean scores of interpretations with positive andnegative scores.Role # % 6= 0 Mean score> 0 < 0None 345 87.25 3.96 -3.94ARG1 671 30.40 3.60 -3.92ARG0 604 25.50 3.72 -3.94ARG2 140 28.57 3.85 -3.84ARGM-MNR 271 32.84 3.40 -3.85ARGM-TMP 231 28.57 3.71 -3.84ARGM-LOC 82 23.17 3.43 -4.60Other 290 20.00 3.38 -3.87Table 3: Number of interpretations generated by toggling offeach semantic role (#), percentage of interpretations not scoredzero (% 6= 0), and mean score of interpretations with positiveand negative scores.true).
Importantly, on overage, modal constructionshave 2.13 interpretations scored with high certainty,and 1.23 scored 3 or higher.
In other words, on av-erage, our procedure generates over 2 interpretationthat are either true or false, and over 1 interpretationthat is true per modal construction.Tables 2 and 3 present basic corpus statistics.
Thepercentage of interpretations annotated with a scoredifferent than 0 depends greatly on the number ofroles toggled off (Table 2): 0: 87.25%, 1: 48.50%,2: 20.46%, 3: 5.83%.
Note that the number of rolestoggled off does not significantly affect the meanscore of interpretations not scored 0 (Table 2, last2 columns).
Most interpretations have either ARG0or ARG1 toggled off (Table 3), and the percentagesof interpretations not scored zero range from 20%to 32.84% depending on the semantic role.
Notethat the average score of interpretations scored pos-itively and negatively, however, does not depend onwhether a semantic role is toggled off.1102Original sentence and sample of automatically generated potential interpretations Score1Context, previous sentence: The last thing we want to do is react to every wild statement that they make.Original sentence: [But]ARGM-DIS [they]ARG0 [certainly]ARGM-ADV [chose]verb [that]ARG1 [to get our attentionand that of the international community.
]ARGM-PRPContext, next sentence: Uh but what they?ve got to realize is there is no magic bullet here.- Interpretation 1.1: But they chose that to get our attention and that of the international community.
5- Interpretation 1.2: But they chose {ARG1} to get our attention and that of the international community.
-52Context, previous sentence: Saddam Hussein (interrupting): Before you offer me your rotten goods, I ask youdid you find weapons of mass destruction in Iraq or not?Original sentence: Rumsfeld (disconcerted): We haven?t found them yet, but [we]ARG0 [will]ARGM-MOD[surely]ARGM-ADV [find]verb [them]ARG1 [one day]ARGM-TMP .Context, next sentence: Do you deny that you had intentions to manufacture a nuclear bomb?- Interpretation 2.1: We will find them one day.
4- Interpretation 2.2: We will find them {ARGM-TMP}.
-33?This is a rare case of [a company with a big majority holder which]ARG0 [will]ARGM-MOD [probably]ARGM-ADV[act]verb [in the interests of the minority holders]ARG1?, one investor says.- Interpretation 3.1: {ARG0} will act in the interests of the minority holders.
4- Interpretation 3.2: A company with a big majority holder will act {ARG1}.
44I wouldn?t define victory as simply not raising taxes?although [I]ARG0 , v1, v2 [definitely]ARGM-ADV, v1[would]ARGM-MOD, v1 [like]v1 [to [defer]v2 [raising taxes]ARG1 , v2 [as long as prudently possible.
]ARG2 , v2]ARG1 , v1- Interpretation 4.1: I will like to defer raising taxes as long as prudently possible.
5- Interpretation 4.2: I will defer raising taxes as long as prudently possible.
1Table 4: Annotation Examples.
For each example, we show the original sentence containing the modal construction, context ifhelpful to determine scores, and 2 selected interpretations and their scores.
Square brackets indicate semantic roles.5.1 Annotation QualityThe annotation guidelines (Section 4.2) to score po-tential interpretations were defined after examin-ing disagreements in pilot annotations.
After defin-ing the guidelines, inter-annotator agreement was0.92 on 18% of randomly selected interpretations.6Agreement measures designed for categorical labelsare unsuitable, as not all disagreements are equal,e.g., 4 vs. 5, -2 vs. 5.
Because of the high agreementand following previous work (Agirre et al, 2012),the rest of interpretations were annotated once.5.2 Annotation ExamplesTable 4 presents annotation examples.
For each ex-ample, we include the original sentence containinga selected modal construction, its context (previousand next sentence) if helpful for scoring, and 2 au-tomatically generated potential interpretations withtheir annotated scores.Example (1) shows that context helps in determin-ing the factuality of potential interpretations (item(1) in the guidelines).
After reading the three sen-6We set an internal deadline of 3 days after agreeing on theguidelines, and we could annotate 18% of instances in that time.tences, it is clear that they are making wild state-ments, and are hoping to get attention for it.
Inter-pretation 1.1 removes adverb certainly and receivesthe highest score, 5.
Interpretation 1.2 is obtained af-ter toggling off ARG1, and receives the lowest score,?5.
This low score is justified by item (3) in our an-notation guidelines: replacing wild statements witha semantically (different but) related substitute, e.g.,But they chose reasonable statements / good man-ners to get our attention and that of the internationalcommunity, yields an unlikely interpretation.The interpretations in Example (2) show again theimportance of context, and also exemplify item (2)in the annotation guidelines.
Interpretation 2.1, Wewill find them one day receives a high score (4/5),as given the context (and assuming that Rumsfeldis truthful), it is very likely that they will find theweapons of mass destruction, but it is not guaran-teed.
Note that annotators are not allowed to use thefact that the weapons were never found (item (2) inthe guidelines).
In Interpretation 2.2, one day couldbe replaced with never / at no time or similar con-structions, and doing so yields the opposite of theintended meaning (score: ?3).
A possible descrip-1103Type Feature Descriptionbaselineadverb Word form of adverbadverb pos Part-of-speech of adverbverb Word form of verbverb pos Part-of-speech of verbdistance Number of tokens between adverb and verbdirection Whether adverb occurs before or after verbadverb andverbadverb rel pos Part-of-speech tags of the parent, and left and right siblings of adverbadverb subcat Concatenation of part-of-speech tags of all siblings of adverbverb rel pos Part-of-speech tags of the parent, and left and right siblings of verbadverb subcat Concatenation of part-of-speech tags of all siblings of verbpath, path l Syntactic path between adverb and verb, and length of the pathancestor POS tag of the lowest common ancestor between verb and adverbhas sem role Flags indicating whether a semantic role is in the modal constructioninterpretationnum roles int Number of roles toggled off in the potential interpretationsem roles int Flags indicating which roles are toggled off in the interpretationroles distance Number of tokens between each semantic role and verbroles direction Whether each semantic role occurs before or after verbroles path Syntactic path between each role and verbroles path l Length of syntactic path between each role and verbTable 5: Features used to predict factuality scores to automatically generated potential interpretations.
Features extracted fromsemantic role are extracted for ARG0?ARG5 and modifiers (ARGM-) ending in LOC, TMP, MNR, PRP, CAU, EXT and PRD.tion of these scores could be ?almost certainly true?
(4 out of 5), and ?most probably false?
(-3 out of -5).We see scores as a continuum of certainty, but tex-tual description may help understand the examples.Example (3) demonstrates the usefulness of thenormalization process?specifically, Step 4, select-ing relevant tokens?and the importance of replac-ing roles with semantically related substitutes (item(3) in the guidelines).
In interpretation 3.1, {ARG0}will act in the interests of the minority holders, ARG0can be replaced with a company with several minor-ity holders, yielding a valid interpretation scored 4(out of 5).
Similarly, in interpretation 3.2, A com-pany with a big majority holder will act {ARG1},ARG1 can be replaced with in the interests of thebig majority holder, yielding another valid interpre-tation also scored 4 (out of 5).Finally, Example (4) shows Step 5 in the auto-matic normalization procedure (Section 4).
By cre-ating an additional verb-argument structure, we areable to differentiate between liking to do something(Interpretation 4.1, score 5/5) and actually doing thatsomething (Interpretation 4.2, score 1/5).6 Learning to Score PotentialInterpretationsIn order to automatically score potential interpre-tations, we follow a standard supervised machinelearning approach.
Each potential interpretation be-comes an instance, and we split modal construc-tions (and their potential interpretations) into train-ing (80%) and test (20%).
When splitting, we makesure that the amount of modal constructions for eachadverb in each split is proportional, i.e., 80% ofmodal constructions with each adverb are in thetrain split and the rest in the test split.
Splitting in-stances randomly would assign interpretations gen-erated from the same modal construction to the trainand test splits, and bias the results.We trained a Support Vector Machine (SVM) forregression with RBF kernel using scikit-learn (Pe-dregosa et al, 2011), which uses LIBSVM (Changand Lin, 2011).
The SVM parameters (C and ?
)were tuned using 10-fold cross-validation with thetraining set, and we report results using the test split.1104Features Pearsonbaseline -0.029adverb and verb 0.025interpretation 0.494baseline + adverb and verb -0.013baseline + interpretation 0.463adverb and verb + interpretation 0.465baseline + adverb and verb + interpretation 0.468Table 6: Pearson correlations obtained with test instances andseveral feature combinations.6.1 Feature SelectionThe full set of features is detailed in Table 5.
Base-line features are simple features characterizing ad-verb and verb and we do not elaborate on them.Adverb and verb features are extracted from themodal construction (constituent tree and semanticroles) and provide additional information about themodal construction.
Interpretation features charac-terize the potential interpretation whose factualityis being scored, and are also derived from the con-stituent tree and semantic roles.Most adverb and verb features are standard in se-mantic role labeling (Gildea and Jurafsky, 2002).We include the part-of-speech tags of the parent, andleft and right siblings of adverb and verb, as well astheir subcategorization, i.e., the concatenation of thesibling?s part-of-speech tags.
We also include syn-tactic path between adverb and verb, and its length.Additionally, we include the common ancestor, i.e.,the syntactic node of the lowest common node thatis an ancestor of both adverb and verb, and use bi-nary features to indicate whether each semantic roleis present in the modal construction.Finally, interpretation features characterize thesemantic roles toggled off to generate the potentialinterpretation.
We include the number of roles tog-gled off to generate the potential interpretation, andbinary flags indicating which roles.
Additionally, foreach role toggled off, we include the distance fromthe verb (number of tokens), whether it occurs be-fore or after the verb, the syntactic path to the verband the length of the path.7 Experimental ResultsTable 6 details results obtained with test instancesusing several feature combinations derived fromgold linguistic information (POS tags, parse trees,semantic roles, etc.).
Baseline and adverb and verbfeatures, which characterize the modal constructionfrom which potential interpretation are extracted, arevirtually useless.
They yield Pearson correlations of?0.029 and 0.025 individually, and ?0.013 com-bined.
These results suggest that the verb and ad-verb in the modal construction (word forms, syntac-tic paths, etc.)
are insufficient to rank potential inter-pretations generated from the modal construction.Interpretation features, which capture differ-ences between potential interpretations being scored(number of roles toggled off, roles toggled off,etc.
), obtain a modest Pearson correlation of 0.494.Combining interpretation features with other fea-tures proved detrimental, Pearson correlations arebetween 0.463 and 0.468.8 ConclusionsModality is a pervasive phenomenon used to talkabout what is not factual.
In this paper, we have pre-sented a methodology to extract implicit interpreta-tions from modal constructions.
First, we automati-cally generate potential interpretations using syntac-tic dependencies and semantic roles, and then assignto them a factuality score.The most important conclusion of the work pre-sented here is that several interpretations automati-cally generated from a single modal construction of-ten receive scores indicating high certainty.
Indeed,on average, modal constructions have 2.13 interpre-tations scored lower or equal than ?3, or higheror equal than 3.
This contrast with previous work,which only assess factuality of one normalizationper proposition.Experimental results using supervised machinelearning and relatively simple features show that thetask is challenging but can be automated.
We be-lieve better results could be obtained by incorporat-ing features capturing knowledge in the context ofthe modal construction, including other clauses inthe same sentence, and the previous and next sen-tences.
Another extension of the current work isto investigate a similar approach for other modalitymarkers such as nouns (e.g., possibility, chance), ad-jectives (e.g.necessary, probable, ) and certain verbs(e.g., claim, suggests).1105ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: A piloton semantic textual similarity.
In Proceedings of theSixth International Workshop on Semantic Evaluation(SemEval 2012), pages 385?393, Montre?al, Canada,7-8 June.Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr,Chris Callison-Burch, Nathaniel W. Filardo, ChristinePiatko, Lori Levin, and Scott Miller.
2012.
Use ofModality and Negation in Semantically-InformedSyn-tactic MT.
Comput.
Linguist., 38(2):411?438, June.Eduardo Blanco and Zahra Sarabi.
2016.
Automaticgeneration and scoring of positive interpretations fromnegated statements.
In Proceedings of the 2016 Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 1431?1441, San Diego, Califor-nia, June.
Association for Computational Linguistics.Marta Carretero and Juan Rafael Zamorano-Mansilla.2013.
An analysis of disagreement-provoking factorsin the analysis of epistemic modality and evidentiality:the case of english adverbials.
In Proceedings of IWCS2013 Workshop on Annotation of Modal Meanings inNatural Language (WAMM), pages 16?23, Potsdam,Germany, March.Chih-Chung Chang and Chih-Jen Lin.
2011.
Libsvm:A library for support vector machines.
ACM Trans.Intell.
Syst.
Technol., 2(3):27:1?27:27,May.Marie-Catherine de Marneffe, Christopher D. Manning,and Christopher Potts.
2012.
Did it happen?
the prag-matic complexity of veridicality assessment.
Comput.Linguist., 38(2):301?333, June.Anita de Waard and Henk Pander Maat.
2012.
Epis-temic modality and knowledge attribution in scientificdiscourse: A taxonomy of types and overview of fea-tures.
In Proceedings of the Workshop on DetectingStructure in Scholarly Discourse, ACL ?12, pages 47?55, Stroudsburg, PA, USA.Mona Diab, Lori Levin, Teruko Mitamura, Owen Ram-bow, Vinodkumar Prabhakaran, and Weiwei Guo.2009.
Committed belief annotation and tagging.
InProceedings of the Third Linguistic Annotation Work-shop, pages 68?73, Suntec, Singapore, August.Kai Von Fintel.
2006.
Modality and language.
InD.
Borchert, editor, Encyclopedia of Philosophy,pages 20?27.
Macmillan Reference.Daniel Gildea and Daniel Jurafsky.
2002.
Auto-matic labeling of semantic roles.
Comput.
Linguist.,28(3):245?288, September.Valentine Hacquard.
2011.
Modality.
In C. Maienborn,K.
von Heusinger, and P. Portner, editors, Seman-tics: An International Handbook of Natural LanguageMeaning, pages 1484?1515.
Mouton de Gruyter.Kees Hengeveld and J. Lachlan Mackenzie.
2008.
Func-tional Discourse Grammar: A Typologically-BasedTheory of Language Structure.
Oxford UniversityPress.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:the 90% Solution.
In NAACL ?06: Proceedings ofthe Human Language Technology Conference of theNAACL, Companion Volume: Short Papers on XX,pages 57?60, Morristown, NJ, USA.Rodney D. Huddleston and Geoffrey K. Pullum.
2002.The Cambridge Grammar of the English Language.Cambridge University Press, April.Otto Jespersen.
1992.
The philosophy of grammar.
Uni-versity of Chicago Press, Chicago.Kenton Lee, Yoav Artzi, Yejin Choi, and Luke Zettle-moyer.
2015.
Event detection and factuality assess-ment with non-expert supervision.
In Proceedings ofthe 2015 Conference on Empirical Methods in Natu-ral Language Processing, pages 1643?1648, Lisbon,Portugal, September.Marc Light, Xin Ying Qiu, and Padmini Srinivasan.2004.
The language of bioscience: Facts, spec-ulations, and statements in between.
In LynetteHirschman and James Pustejovsky, editors, HLT-NAACL 2004 Workshop: BioLINK 2004, Linking Bi-ological Literature, Ontologies and Databases, pages17?24, Boston, Massachusetts, USA, May 6.John Lyons.
1977.
Semantics.
Cambridge UniversityPress.
Cambridge Books Online.Bill MacCartney, Trond Grenager, Marie-Catherinede Marneffe, Daniel Cer, and Christopher D. Man-ning.
2006.
Learning to recognize features of validtextual entailments.
In Proceedings of the Main Con-ference on Human Language Technology Conferenceof the North American Chapter of the Association ofComputational Linguistics, HLT-NAACL ?06, pages41?48, Stroudsburg, PA, USA.Christopher D Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J Bethard, and David McClosky.2014.
The stanford corenlp natural language process-ing toolkit.
In Proceedings of 52nd Annual Meeting ofthe Association for Computational Linguistics: SystemDemonstrations, pages 55?60.Roser Morante and Caroline Sporleder.
2012.
Modal-ity and negation: An introduction to the special issue.Comput.
Linguist., 38(2):223?260, June.Masaki Murata, Masao Utiyama, Kiyotaka Uchimoto,Hitoshi Isahara, and Qing Ma.
2005.
Correction of er-rors in a verb modality corpus for machine translationwith a machine-learning method.
4(1):18?37, March.Malvina Nissim, Paola Pietrandrea, Andrea Sanso, andCaterina Mauri.
2013.
Cross-linguistic annotation of1106modality: a data-driven hierarchical model.
In Pro-ceedings of the 9th Joint ISO - ACL SIGSEM Work-shop on Interoperable Semantic Annotation, pages 7?14, Potsdam, Germany, March.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics,31(1):71?106.F.
R. Palmer.
2001.
Mood and Modality.
CambridgeUniversity Press, second edition.
Cambridge BooksOnline.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau,M.
Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.Vinodkumar Prabhakaran, Michael Bloodgood, MonaDiab, Bonnie Dorr, Lori Levin, Christine D. Piatko,Owen Rambow, and Benjamin Van Durme.
2012.
Sta-tistical modality tagging from rule-based annotationsand crowdsourcing.
In Proceedings of the Workshopon Extra-Propositional Aspects of Meaning in Com-putational Linguistics, pages 57?64, Jeju, Republic ofKorea, July.Vinodkumar Prabhakaran, Tomas By, Julia Hirschberg,Owen Rambow, Samira Shaikh, Tomek Strzalkowski,Jennifer Tracey, Michael Arrigo, Rupayan Basu,Micah Clark, Adam Dalton, Mona Diab, LouiseGuthrie, Anna Prokofieva, Stephanie Strassel, GregoryWerner, Yorick Wilks, and Janyce Wiebe.
2015.
Anew dataset and evaluation for belief/factuality.
InProceedings of the Fourth Joint Conference on Lexicaland Computational Semantics, pages 82?91, Denver,Colorado, June.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
Conll-2011 shared task: Modeling unrestrictedcoreference in ontonotes.
In Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 1?27, Portland,Oregon, USA, June.P.
Quaresma, A. Mendes, I. Hendrickx, and T. Gon?alves.2014.
Automatic tagging of modality: identifying trig-gers and modal value.
In Proceedings of the 10th JointACL SIGSEM - ISO Workshop on Interoperable Se-mantic Annotation.Victoria L. Rubin.
2006.
Identifying certainty in texts.Ph.D.
thesis, Syracuse University, Syracuse, NY.Aynat Rubinstein, Hillary Harner, Elizabeth Krawczyk,Daniel Simonson, Graham Katz, and Paul Portner.2013.
Toward fine-grained annotation of modalityin text.
In Proceedings of IWCS 2013 Workshop onAnnotation of Modal Meanings in Natural Language(WAMM), pages 38?46, Potsdam, Germany, March.Liliana Mamani Sa?nchez and Carl Vogel.
2015.
A hedg-ing annotation scheme focused on epistemic phrasesfor informal language.
In Proceedings of the Work-shop on Models for Modality Annotation.Roser Saur??
and James Pustejovsky.
2009.
Factbank:a corpus annotated with event factuality.
LanguageResources and Evaluation, 43(3):227?268.Rion Snow, Lucy Vanderwende, and Arul Menezes.2006.
Effectively using syntax for recognizing falseentailment.
In Proceedings of the Main Conferenceon Human Language Technology Conference of theNorth American Chapter of the Association of Compu-tational Linguistics, HLT-NAACL ?06, pages 33?40,Stroudsburg, PA, USA.Sandeep Soni, Tanushree Mitra, Eric Gilbert, and JacobEisenstein.
2014.
Modeling factuality judgments insocial media text.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Linguis-tics (Volume 2: Short Papers), pages 415?420, Balti-more, Maryland, June.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.J.
Artif.
Int.
Res., 37(1):141?188, January.Veronika Vincze, Gyo?rgy Szarvas, Richard Farkas,Gyo?rgy Mo?ra, and Ja?nos Csirik.
2008.
The Bio-Scope corpus: biomedical texts annotated for uncer-tainty, negation and their scopes.
BMC Bioinformat-ics, 9((Suppl 11)):S9.Veronika Vincze, Gyo?rgy Szarvas, Gyo?rgy Mo?ra,Tomoko Ohta, and Richa?rd Farkas.
2011.
Linguis-tic scope-based and biological event-based specula-tion and negation annotations in the bioscope and ge-nia event corpora.
Journal of Biomedical Semantics,2(5):1?11.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotionsin language.
Language Resources and Evaluation,39(2):165?210.1107
