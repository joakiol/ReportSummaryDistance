Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1234?1244,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsUsing Universal Linguistic Knowledge to Guide Grammar InductionTahira Naseem, Harr Chen, Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{tahira, harr, regina} @csail.mit.eduMark JohnsonDepartment of ComputingMacquarie Universitymark.johnson@mq.edu.auAbstractWe present an approach to grammar induc-tion that utilizes syntactic universals to im-prove dependency parsing across a range oflanguages.
Our method uses a single setof manually-specified language-independentrules that identify syntactic dependencies be-tween pairs of syntactic categories that com-monly occur across languages.
During infer-ence of the probabilistic model, we use pos-terior expectation constraints to require that aminimum proportion of the dependencies weinfer be instances of these rules.
We also auto-matically refine the syntactic categories givenin our coarsely tagged input.
Across six lan-guages our approach outperforms state-of-the-art unsupervised methods by a significant mar-gin.11 IntroductionDespite surface differences, human languages ex-hibit striking similarities in many fundamental as-pects of syntactic structure.
These structural corre-spondences, referred to as syntactic universals, havebeen extensively studied in linguistics (Baker, 2001;Carnie, 2002; White, 2003; Newmeyer, 2005) andunderlie many approaches in multilingual parsing.In fact, much recent work has demonstrated thatlearning cross-lingual correspondences from cor-pus data greatly reduces the ambiguity inherent insyntactic analysis (Kuhn, 2004; Burkett and Klein,2008; Cohen and Smith, 2009a; Snyder et al, 2009;Berg-Kirkpatrick and Klein, 2010).1The source code for the work presented in this paper isavailable at http://groups.csail.mit.edu/rbg/code/dependency/Root ?
Auxiliary Noun ?
AdjectiveRoot ?
Verb Noun ?
ArticleVerb ?
Noun Noun ?
NounVerb ?
Pronoun Noun ?
NumeralVerb ?
Adverb Preposition ?
NounVerb ?
Verb Adjective ?
AdverbAuxiliary ?
VerbTable 1: The manually-specified universal dependencyrules used in our experiments.
These rules specify head-dependent relationships between coarse (i.e., unsplit)syntactic categories.
An explanation of the ruleset is pro-vided in Section 5.In this paper, we present an alternative gram-mar induction approach that exploits these struc-tural correspondences by declaratively encoding asmall set of universal dependency rules.
As inputto the model, we assume a corpus annotated withcoarse syntactic categories (i.e., high-level part-of-speech tags) and a set of universal rules defined overthese categories, such as those in Table 1.
Theserules incorporate the definitional properties of syn-tactic categories in terms of their interdependenciesand thus are universal across languages.
They canpotentially help disambiguate structural ambiguitiesthat are difficult to learn from data alone ?
forexample, our rules prefer analyses in which verbsare dependents of auxiliaries, even though analyz-ing auxiliaries as dependents of verbs is also consis-tent with the data.
Leveraging these universal ruleshas the potential to improve parsing performancefor a large number of human languages; this is par-ticularly relevant to the processing of low-resource1234languages.
Furthermore, these universal rules arecompact and well-understood, making them easy tomanually construct.In addition to these universal dependencies, eachspecific language typically possesses its own id-iosyncratic set of dependencies.
We address thischallenge by requiring the universal constraints toonly hold in expectation rather than absolutely, i.e.,we permit a certain number of violations of the con-straints.We formulate a generative Bayesian model thatexplains the observed data while accounting fordeclarative linguistic rules during inference.
Theserules are used as expectation constraints on theposterior distribution over dependency structures.This approach is based on the posterior regular-ization technique (Grac?a et al, 2009), which weapply to a variational inference algorithm for ourparsing model.
Our model can also optionally re-fine common high-level syntactic categories intoper-language categories by inducing a clustering ofwords using Dirichlet Processes (Ferguson, 1973).Since the universals guide induction toward linguis-tically plausible structures, automatic refinement be-comes feasible even in the absence of manually an-notated syntactic trees.We test the effectiveness of our grammar induc-tion model on six Indo-European languages fromthree language groups: English, Danish, Portuguese,Slovene, Spanish, and Swedish.
Though these lan-guages share a high-level Indo-European ancestry,they cover a diverse range of syntactic phenomenon.Our results demonstrate that universal rules greatlyimprove the accuracy of dependency parsing acrossall of these languages, outperforming current state-of-the-art unsupervised grammar induction meth-ods (Headden III et al, 2009; Berg-Kirkpatrick andKlein, 2010).2 Related WorkLearning with Linguistic Constraints Our workis situated within a broader class of unsupervised ap-proaches that employ declarative knowledge to im-prove learning of linguistic structure (Haghighi andKlein, 2006; Chang et al, 2007; Grac?a et al, 2007;Cohen and Smith, 2009b; Druck et al, 2009; Lianget al, 2009a).
The way we apply constraints is clos-est to the latter two approaches of posterior regular-ization and generalized expectation criteria.In the posterior regularization framework, con-straints are expressed in the form of expectations onposteriors (Grac?a et al, 2007; Ganchev et al, 2009;Grac?a et al, 2009; Ganchev et al, 2010).
This de-sign enables the model to reflect constraints that aredifficult to encode via the model structure or as pri-ors on its parameters.
In their approach, parame-ters are estimated using a modified EM algorithm,where the E-step minimizes the KL-divergence be-tween the model posterior and the set of distributionsthat satisfies the constraints.
Our approach also ex-presses constraints as expectations on the posterior;we utilize the machinery of their framework withina variational inference algorithm with a mean fieldapproximation.Generalized expectation criteria, another tech-nique for declaratively specifying expectation con-straints, has previously been successfully applied tothe task of dependency parsing (Druck et al, 2009).This objective expresses constraints in the form ofpreferences over model expectations.
The objectiveis penalized by the square distance between modelexpectations and the prespecified values of the ex-pectation.
This approach yields significant gainscompared to a fully unsupervised counterpart.
Theconstraints they studied are corpus- and language-specific.
Our work demonstrates that a small set oflanguage-independent universals can also serve aseffective constraints.
Furthermore, we find that ourmethod outperforms the generalized expectation ap-proach using corpus-specific constraints.Learning to Refine Syntactic Categories Recentresearch has demonstrated the usefulness of auto-matically refining the granularity of syntactic cat-egories.
While most of the existing approachesare implemented in the supervised setting (Finkelet al, 2007; Petrov and Klein, 2007), Liang et al(2007) propose a non-parametric Bayesian modelthat learns the granularity of PCFG categories inan unsupervised fashion.
For each non-terminalgrammar symbol, the model posits a HierarchicalDirichlet Process over its refinements (subsymbols)to automatically learn the granularity of syntacticcategories.
As with their work, we also use non-parametric priors for category refinement and em-1235ploy variational methods for inference.
However,our goal is to apply category refinement to depen-dency parsing, rather than to PCFGs, requiring asubstantially different model formulation.
WhileLiang et al (2007) demonstrated empirical gains ona synthetic corpus, our experiments focus on unsu-pervised category refinement on real language data.Universal Rules in NLP Despite the recent surgeof interest in multilingual learning (Kuhn, 2004; Co-hen and Smith, 2009a; Snyder et al, 2009; Berg-Kirkpatrick and Klein, 2010), there is surprisinglylittle computational work on linguistic universals.On the acquisition side, Daume?
III and Campbell(2007) proposed a computational technique for dis-covering universal implications in typological fea-tures.
More closely related to our work is the posi-tion paper by Bender (2009), which advocates theuse of manually-encoded cross-lingual generaliza-tions for the development of NLP systems.
She ar-gues that a system employing such knowledge couldbe easily adapted to a particular language by spe-cializing this high level knowledge based on the ty-pological features of the language.
We also arguethat cross-language universals are beneficial for au-tomatic language processing; however, our focus ison learning language-specific adaptations of theserules from data.3 ModelThe central hypothesis of this work is that unsu-pervised dependency grammar induction can be im-proved using universal linguistic knowledge.
To-ward this end our approach is comprised of twocomponents: a probabilistic model that explainshow sentences are generated from latent dependencystructures and a technique for incorporating declar-ative rules into the inference process.We first describe the generative story in this sec-tion before turning to how constraints are appliedduring inference in Section 4.
Our model takes asinput (i.e., as observed) a set of sentences whereeach word is annotated with a coarse part-of-speechtag.
Table 2 provides a detailed technical descrip-tion of our model?s generative process, and Figure 1presents a model diagram.For each observed coarse symbol s:1.
Draw top-level infinite multinomial oversubsymbols ?s ?
GEM(?).2.
For each subsymbol z of symbol s:(a) Draw word emission multinomial?sz ?
Dir(?0).
(b) For each context value c:i.
Draw child symbol generationmultinomial ?szc ?
Dir(?0).ii.
For each child symbol s?:A.
Draw second-level infinitemultinomial over subsymbolspis?szc ?
DP(?, ?s?
).For each tree node i generated in context c byparent symbol s?
and parent subsymbol z?:1.
Draw coarse symbol si ?
Mult(?s?z?).2.
Draw subsymbol zi ?
Mult(pisis?z?c).3.
Draw word xi ?
Mult(?sizi).Table 2: The generative process for model parametersand parses.
In the above GEM, DP, Dir, and Mult referrespectively to the stick breaking distribution, Dirichletprocess, Dirichlet distribution, and multinomial distribu-tion.Generating Symbols and Words We describehow a single node of the tree is generated beforediscussing how the entire tree structure is formed.Each node of the dependency tree is comprised ofthree random variables: an observed coarse symbols, a hidden refined subsymbol z, and an observedword x.
In the following let the parent of the cur-rent node have symbol s?
and subsymbol z?
; the rootnode is generated from separate root-specific distri-butions.
Subsymbol refinement is an optional com-ponent of the full model and can be omitted by de-terministically equating s and z.
As we explain atthe end of this section, without this aspect the gener-ative story closely resembles the classic dependencymodel with valence (DMV) of Klein and Manning(2004).First we draw symbol s from a finite multinomial1236s - coarse symbol (observed)z - refined subsymbolx - word (observed)?szc - distr over child coarse symbols foreach parent s and z and context c?s - top-level distr over subsymbols for spiss?z?c - distr over subsymbols for each s,parent s?
and z?, and context c?sz - distr over words for s and zFigure 1: Graphical representation of the model and a summary of the notation.
There is a copy of the outer plate foreach distinct symbol in the observed coarse tags.
Here, node 3 is shown to be the parent of nodes 1 and 2.
Shadedvariables are observed, square variables are hyperparameters.
The elongated oval around s and z represents the twovariables jointly.
For clarity the diagram omits some arrows from ?
to each s, pi to each z, and ?
to each x.distribution with parameters ?s?z?c.
As the indicesindicate, we have one such set of multinomial pa-rameters for every combination of parent symbols?
and subsymbol z?
along with a context c. Herethe context of the current node can take one of sixvalues corresponding to every combination of di-rection (left or right) and valence (first, second, orthird or higher child) with respect to its parent.
Theprior (base distribution) for each ?s?z?c is a symmet-ric Dirichlet with hyperparameter ?0.Next we draw the refined syntactic category sub-symbol z from an infinite multinomial with parame-ters piss?z?c.
Here the selection of pi is indexed by thecurrent node?s coarse symbol s, the symbol s?
andsubsymbol z?
of the parent node, and the context cof the current node.For each unique coarse symbol s we tie togetherthe distributions piss?z?c for all possible parent andcontext combinations (i.e., s?, z?, and c) using a Hi-erarchical Dirichlet Process (HDP).
Specifically, fora single s, each distribution piss?z?c over subsymbolsis drawn from a DP with concentration parameter?
and base distribution ?s over subsymbols.
Thisbase distribution ?s is itself drawn from a GEM priorwith concentration parameter ?.
By formulating thegeneration of z as an HDP, we can share parame-ters for a single coarse symbol?s subsymbol distribu-tion while allowing for individual variability basedon node parent and context.
Note that parametersare not shared across different coarse symbols, pre-serving the distinctions expressed via the coarse tagannotations.Finally, we generate the word x from a finitemultinomial with parameters ?sz , where s and z arethe symbol and subsymbol of the current node.
The?
distributions are drawn from a symmetric Dirich-let prior.Generating the Tree Structure We now considerhow the structure of the tree arises.
We followan approach similar to the widely-referenced DMVmodel (Klein and Manning, 2004), which formsthe basis of the current state-of-the-art unsuper-vised grammar induction model (Headden III et al,2009).
After a node is drawn we generate childrenon each side until we produce a designated STOPsymbol.
We encode more detailed valence informa-tion than Klein and Manning (2004) and conditionchild generation on parent valence.
Specifically, af-ter drawing a node we first decide whether to pro-ceed to generate a child or to stop conditioned on theparent symbol and subsymbol and the current con-text (direction and valence).
If we decide to gener-ate a child we follow the previously described pro-cess for constructing a node.
We can combine thestopping decision with the generation of the childsymbol by including a distinguished STOP symbolas a possible outcome in distribution ?.No-Split Model Variant In the absence of sub-symbol refinement (i.e., when subsymbol z is set tobe identical to coarse symbol s), our model simpli-fies in some respects.
In particular, the HDP gener-1237ation of z is obviated and word x is drawn from aword distribution ?s indexed solely by coarse sym-bol s. The resulting simplified model closely resem-bles DMV (Klein and Manning, 2004), except that it1) explicitly generate words x rather than only part-of-speech tags s, 2) encodes richer context and va-lence information, and 3) imposes a Dirichlet prioron the symbol distribution ?.4 Inference with ConstraintsWe now describe how to augment our generativemodel of dependency structure with constraints de-rived from linguistic knowledge.
Incorporating arbi-trary linguistic rules directly in the generative storyis challenging as it requires careful tuning of eitherthe model structure or priors for each constraint.
In-stead, following the approach of Grac?a et al (2007),we constrain the posterior to satisfy the rules in ex-pectation during inference.
This effectively biasesthe inference toward linguistically plausible settings.In standard variational inference, an intractabletrue posterior is approximated by a distribution froma tractable set (Bishop, 2006).
This tractable set typ-ically makes stronger independence assumptions be-tween model parameters than the model itself.
To in-corporate the constraints, we further restrict the setto only include distributions that satisfy the specifiedexpectation constraints over hidden variables.In general, for some given model, let ?
denotethe entire set of model parameters and z and x de-note the hidden structure and observations respec-tively.
We are interested in estimating the posteriorp(?, z | x).
Variational inference transforms thisproblem into an optimization problem where we tryto find a distribution q(?, z) from a restricted set Qthat minimizes the KL-divergence between q(?, z)and p(?, z | x):KL(q(?, z) ?
p(?, z | x))=?q(?, z) logq(?, z)p(?, z, x)d?dz + log p(x).Rearranging the above yields:log p(x) = KL(q(?, z) ?
p(?, z | x)) + F ,where F is defined asF ?
?q(?, z) logp(?, z, x)q(?, z)d?dz.
(1)Thus F is a lower bound on likelihood.
Maximizingthis lower bound is equivalent to minimizing the KL-divergence between p(?, z | x) and q(?, z).
To makethis maximization tractable we make a mean fieldassumption that q belongs to a set Q of distributionsthat factorize as follows:q(?, z) = q(?
)q(z).We further constrain q to be from the subset of Qthat satisfies the expectation constraintEq[f(z)] ?
bwhere f is a deterministically computable functionof the hidden structures.
In our model, for exam-ple, f counts the dependency edges that are an in-stance of one of the declaratively specified depen-dency rules, while b is the proportion of the totaldependencies that we expect should fulfill this con-straint.2With the mean field factorization and the expec-tation constraints in place, solving the maximizationof F in (1) separately for each factor yields the fol-lowing updates:q(?)
= argminq(?)KL(q(?)
?
q?(?
)), (2)q(z) = argminq(z)KL(q(z) ?
q?(z))s.t.
Eq(z)[f(z)] ?
b, (3)whereq?(?)
?
expEq(z)[log p(?, z, x)], (4)q?
(z) ?
expEq(?
)[log p(?, z, x)].
(5)We can solve (2) by setting q(?)
to q?(?)
?
sinceq(z) is held fixed while updating q(?
), the expecta-tion function of the constraint remains constant dur-ing this update.
As shown by Grac?a et al (2007), theupdate in (3) is a constrained optimization problemand can be solved by performing gradient search onits dual:argmin?
?>b + log?zq?
(z) exp(?
?>f(z)) (6)For a fixed value of ?
the optimal q(z) ?q?
(z) exp(??>f(z)).
By updating q(?)
and q(z)as in (2) and (3) we are effectively maximizing thelower bound F .2Constraints of the form Eq[f(z)] ?
b are easily imposedby negating f(z) and b.12384.1 Variational UpdatesWe now derive the specific variational updates forour dependency induction model.
First we assumethe following mean-field factorization of our varia-tional distribution:q(?, ?, pi, ?, z)= q(z) ??s?q(?s?)
?T?z?=1q(?s?z?)?
?cq(?s?z?c) ?
?sq(piss?z?c), (7)where s?
varies over the set of unique symbols in theobserved tags, z?
denotes subsymbols for each sym-bol, c varies over context values comprising a pairof direction (left or right) and valence (first, second,or third or higher) values, and s corresponds to childsymbols.We restrict q(?s?z?c) and q(?s?z?)
to be Dirichletdistributions and q(z) to be multinomial.
As withprior work (Liang et al, 2009b), we assume a de-generate q(?)
?
???(?)
for tractability reasons, i.e.,all mass is concentrated on some single ??.
We alsoassume that the top level stick-breaking distributionis truncated at T , i.e., q(?)
assigns zero probabilityto integers greater than T .
Because of the truncationof ?, we can approximate q(piss?z?c) with an asym-metric finite dimensional Dirichlet.The factors are updated one at a time holding allother factors fixed.
The variational update for q(pi)is given by:q(piss?z?c) = Dir(piss?z?c;??
+ Eq(z)[Css?z?c(z)]),where term Eq(z)[Css?z?c(z)] is the expected countw.r.t.
q(z) of child symbol s and subsymbol z incontext c when generated by parent symbol s?
andsubsymbol z?.Similarly, the updates for q(?)
and q(?)
are givenby:q(?s?z?c) = Dir(?s?z?c; ?0 + Eq(z)[Cs?z?c(s)]),q(?s?z?)
= Dir(?s?z?
;?0 + Eq(z)[Cs?z?
(x)]),where Cs?z?c(s) is the count of child symbol s beinggenerated by the parent symbol s?
and subsymbol z?in context c and Cs?z?x is the count of word x beinggenerated by symbol s?
and subsymbol z?.The only factor affected by the expectation con-straints is q(z).
Recall from the previous section thatthe update for q(z) is performed via gradient searchon the dual of a constrained minimization problemof the form:q(z) = argminq(z)KL(q(z) ?
q?
(z)).Thus we first compute the update for q?(z):q?
(z) ?N?n=1len(n)?j=1(expEq(?
)[log ?snjznj (xnj)]?
expEq(?
)[log ?sh(nj)zh(nj)cnj (snj)]?
expEq(pi)[log pisnjsh(nj)zh(nj)cnj (znj)]),where N is the total number of sentences, len(n)is the length of sentence n, and index h(nj) refersto the head of the jth node of sentence n. Giventhis q?
(z) a gradient search is performed using (6) tofind the optimal ?
and thus the primal solution forupdating q(z).Finally, we update the degenerate factor q(?s)with the projected gradient search algorithm usedby Liang et al (2009b).5 Linguistic ConstraintsUniversal Dependency Rules We compile a set of13 universal dependency rules consistent with vari-ous linguistic accounts (Carnie, 2002; Newmeyer,2005), shown in Table 1.
These rules are definedover coarse part-of-speech tags: Noun, Verb, Adjec-tive, Adverb, Pronoun, Article, Auxiliary, Preposi-tion, Numeral and Conjunction.
Each rule specifiesa part-of-speech for the head and argument but doesnot provide ordering information.We require that a minimum proportion of the pos-terior dependencies be instances of these rules in ex-pectation.
In contrast to prior work on rule-drivendependency induction (Druck et al, 2009), whereeach rule has a separately specified expectation, weonly set a single minimum expectation for the pro-portion of all dependencies that must match one ofthe rules.
This setup is more relevant for learn-ing with universals since individual rule frequenciesvary greatly between languages.12391.
Identify non-recursive NPs:?
All nouns, pronouns and possessivemarker are part of an NP.?
All adjectives, conjunctions and deter-miners immediately preceding an NPare part of the NP.2.
The first verb or modal in the sentence is theheadword.3.
All words in an NP are headed by the lastword in the NP.4.
The last word in an NP is headed by theword immediately before the NP if it is apreposition, otherwise it is headed by theheadword of the sentence if the NP is be-fore the headword, else it is headed by theword preceding the NP.5.
For the first word set its head to be the head-word of the sentence.
For each other wordset its headword to be the previous word.Table 3: English-specific dependency rules.English-specific Dependency Rules For English,we also consider a small set of hand-crafted depen-dency rules designed by Michael Collins3 for deter-ministic parsing, shown in Table 3.
Unlike the uni-versals from Table 1, these rules alone are enough toconstruct a full dependency tree.
Thus they allow usto judge whether the model is able to improve upona human-engineered deterministic parser.
Moreover,with this dataset we can assess the additional benefitof using rules tailored to an individual language asopposed to universal rules.6 Experimental SetupDatasets and Evaluation We test the effective-ness of our grammar induction approach on English,Danish, Portuguese, Slovene, Spanish, and Swedish.For English we use the Penn Treebank (Marcus etal., 1993), transformed from CFG parses into depen-3Personal communication.dencies with the Collins head finding rules (Collins,1999); for the other languages we use data from the2006 CoNLL-X Shared Task (Buchholz and Marsi,2006).
Each dataset provides manually annotatedpart-of-speech tags that are used for both trainingand testing.
For comparison purposes with previ-ous work, we limit the cross-lingual experiments tosentences of length 10 or less (not counting punc-tuation).
For English, we also explore sentences oflength up to 20.The final output metric is directed dependency ac-curacy.
This is computed based on the Viterbi parsesproduced using the final unnormalized variationaldistribution q(z) over dependency structures.Hyperparameters and Training Regimes Un-less otherwise stated, in experiments with rule-basedconstraints the expected proportion of dependenciesthat must satisfy those constraints is set to 0.8.
Thisthreshold value was chosen based on minimal tun-ing on a single language and ruleset (English withuniversal rules) and carried over to each other ex-perimental condition.
A more detailed discussion ofthe threshold?s empirical impact is presented in Sec-tion 7.1.Variational approximations to the HDP are trun-cated at 10.
All hyperparameter values are fixed to 1except ?
which is fixed to 10.We also conduct a set of No-Split experiments toevaluate the importance of syntactic refinement; inthese experiments each coarse symbol correspondsto only one refined symbol.
This is easily effectedduring inference by setting the HDP variational ap-proximation truncation level to one.For each experiment we run 50 iterations of vari-ational updates; for each iteration we perform fivesteps of gradient search to compute the update forthe variational distribution q(z) over dependencystructures.7 ResultsIn the following section we present our primarycross-lingual results using universal rules (Sec-tion 7.1) before performing a more in-depth analysisof model properties such as sensitivity to ruleset se-lection and inference stability (Section 7.2).1240DMV PGI No-Split HDP-DEPEnglish 47.1 62.3 71.5 71.9 (0.3)Danish 33.5 41.6 48.8 51.9 (1.6)Portuguese 38.5 63.0 54.0 71.5 (0.5)Slovene 38.5 48.4 50.6 50.9 (5.5)Spanish 28.0 58.4 64.8 67.2 (0.4)Swedish 45.3 58.3 63.3 62.1 (0.5)Table 4: Directed dependency accuracy using our modelwith universal dependency rules (No-Split and HDP-DEP), compared to DMV (Klein andManning, 2004) andPGI (Berg-Kirkpatrick and Klein, 2010).
The DMV re-sults are taken from Berg-Kirkpatrick and Klein (2010).Bold numbers indicate the best result for each language.For the full model, the standard deviation in performanceover five runs is indicated in parentheses.7.1 Main Cross-Lingual ResultsTable 4 shows the performance of both our fullmodel (HDP-DEP) and its No-Split version usinguniversal dependency rules across six languages.We also provide the performance of two baselines?the dependency model with valence (DMV) (Kleinand Manning, 2004) and the phylogenetic grammarinduction (PGI) model (Berg-Kirkpatrick and Klein,2010).HDP-DEP outperforms both DMV and PGIacross all six languages.
Against DMV we achievean average absolute improvement of 24.1%.
Thisimprovement is expected given that DMV does nothave access to the additional information providedthrough the universal rules.
PGI is more relevantas a point of comparison, since it is able to lever-age multilingual data to learn information similar towhat we have declaratively specified using universalrules.
Specifically, PGI reduces induction ambigu-ity by connecting language-specific parameters viaphylogenetic priors.
We find, however, that we out-perform PGI by an average margin of 7.2%, demon-strating the benefits of explicit rule specification.An additional point of comparison is the lexi-calized unsupervised parser of Headden III et al(2009), which yields the current state-of-the-art un-supervised accuracy on English at 68.8%.
Ourmethod also outperforms this approach, without em-ploying lexicalization and sophisticated smoothingas they do.
This result suggests that combining thecomplementary strengths of their approach and oursEnglishRule Excluded Acc Loss Gold FreqPreposition ?
Noun 61.0 10.9 5.1Verb ?
Noun 61.4 10.5 14.8Noun ?
Noun 64.4 7.5 10.7Noun ?
Article 64.7 7.2 8.5SpanishRule Excluded Acc Loss Gold FreqPreposition ?
Noun 53.4 13.8 8.2Verb ?
Noun 61.9 5.4 12.9Noun ?
Noun 62.6 4.7 2.0Root ?
Verb 65.4 1.8 12.3Table 5: Ablation experiment results for universal depen-dency rules on English and Spanish.
For each rule, weevaluate the model using the ruleset excluding that rule,and list the most significant rules for each language.
Thesecond last column is the absolute loss in performancecompared to the setting where all rules are available.
Thelast column shows the percentage of the gold dependen-cies that satisfy the rule.can yield further performance improvements.Table 4 also shows the No-Split results where syn-tactic categories are not refined.
We find that suchrefinement usually proves to be beneficial, yieldingan average performance gain of 3.7%.
However, wenote that the impact of incorporating splitting variessignificantly across languages.
Further understand-ing of this connection is an area of future research.Finally, we note that our model exhibits low vari-ance for most languages.
This result attests to howthe expectation constraints consistently guide infer-ence toward high-accuracy areas of the search space.Ablation Analysis Our next experiment seeks tounderstand the relative importance of the variousuniversal rules from Table 1.
We study how accu-racy is affected when each of the rules is removedone at a time for English and Spanish.
Table 5 liststhe rules with the greatest impact on performancewhen removed.
We note the high overlap betweenthe most significant rules for English and Spanish.We also observe that the relationship betweena rule?s frequency and its importance for high ac-curacy is not straightforward.
For example, the?Preposition ?
Noun?
rule, whose removal de-grades accuracy the most for both English and Span-124150?55?60?65?70?75?Gold?
70?
75?
80?
85?
90?Accuracy?Constraints?Threshold?Average?
English?Figure 2: Accuracy of our model with different thresholdsettings, on English only and averaged over all languages.?Gold?
refers to the setting where each language?s thresh-old is set independently to the proportion of gold depen-dencies satisfying the rules ?
for English this proportionis 70%, while the average proportion across languages is63%.ish, is not the most frequent rule in either language.This result suggests that some rules are harder tolearn than others regardless of their frequency, sotheir presence in the specified ruleset yields strongerperformance gains.Varying the Constraint Threshold In our mainexperiments we require that at least 80% of the ex-pected dependencies satisfy the rule constraints.
Wearrived at this threshold by tuning on the basis of En-glish only.
As shown in Figure 2, for English a broadband of threshold values from 75% to 90% yields re-sults within 2.5% of each other, with a slight peak at80%.To further study the sensitivity of our method tohow the threshold is set, we perform post hoc ex-periments with other threshold values on each of theother languages.
As Figure 2 also shows, on averagea value of 80% is optimal across languages, thoughagain accuracy is stable within 2.5% between thresh-olds of 75% to 90%.
These results demonstrate thata single threshold is broadly applicable across lan-guages.Interestingly, setting the threshold value indepen-dently for each language to its ?true?
proportionbased on the gold dependencies (denoted as the?Gold?
case in Figure 2) does not achieve optimalLength?
10 ?
20Universal Dependency Rules1 HDP-DEP 71.9 50.4No Rules (Random Init)2 HDP-DEP 24.9 24.43 Headden III et al (2009) 68.8 -English-Specific Parsing Rules4 Deterministic (rules only) 70.0 62.65 HDP-DEP 73.8 66.1Druck et al (2009) Rules6 Druck et al (2009) 61.3 -7 HDP-DEP 64.9 42.2Table 6: Directed accuracy of our model (HDP-DEP) onsentences of length 10 or less and 20 or less from WSJwith different rulesets and with no rules, along with vari-ous baselines from the literature.
Entries in this table arenumbered for ease of reference in the text.performance.
Thus, knowledge of the true language-specific rule proportions is not necessary for highaccuracy.7.2 Analysis of Model PropertiesWe perform a set of additional experiments on En-glish to gain further insight into HDP-DEP?s behav-ior.
Our choice of language is motivated by thefact that a wide range of prior parsing algorithmswere developed for and tested exclusively on En-glish.
The experiments below demonstrate that 1)universal rules alone are powerful, but language-and dataset-tailored rules can further improve per-formance; 2) our model learns jointly from therules and data, outperforming a rules-only deter-ministic parser; 3) the way we incorporate posteriorconstraints outperforms the generalized expectationconstraint framework; and 4) our model exhibits lowvariance when seeded with different initializations.These results are summarized in Table 6 and dis-cussed in detail below; line numbers refer to entriesin Table 6.
Each run of HDP-DEP below is withsyntactic refinement enabled.Impact of Rules Selection We compare the per-formance of HDP-DEP using the universal rules ver-sus a set of rules designed for deterministically pars-ing the Penn Treebank (see Section 5 for details).1242As lines 1 and 5 of Table 6 show, language-specificrules yield better performance.
For sentences oflength 10 or less, the difference between the tworulesets is a relatively small 1.9%; for longer sen-tences, however, the difference is a substantiallylarger 15.7%.
This is likely because longer sen-tences tend to be more complex and thus exhibitmore language-idiosyncratic dependencies.
Suchdependencies can be better captured by the refinedlanguage-specific rules.We also test model performance when no linguis-tic rules are available, i.e., performing unconstrainedvariational inference.
The model performs substan-tially worse (line 2), confirming that syntactic cat-egory refinement in a fully unsupervised setup ischallenging.Learning Beyond Provided Rules Since HDP-DEP is provided with linguistic rules, a legitimatequestion is whether it improves upon what the rulesencode, especially when the rules are complete andlanguage-specific.
We can answer this question bycomparing the performance of our model seededwith the English-specific rules against a determin-istic parser that implements the same rules.
Lines4 and 5 of Table 6 demonstrate that the model out-performs a rules-only deterministic parser by 3.8%for sentences of length 10 or less and by 3.5% forsentences of length 20 or less.Comparison with Alternative Semi-supervisedParser The dependency parser based on the gen-eralized expectation criteria (Druck et al, 2009) isthe closest to our reported work in terms of tech-nique.
To compare the two, we run HDP-DEP usingthe 20 rules given by Druck et al (2009).
Our modelachieves an accuracy of 64.9% (line 7) compared to61.3% (line 6) reported in their work.
Note that wedo not rely on rule-specific expectation informationas they do, instead requiring only a single expecta-tion constraint parameter.4Model Stability It is commonly acknowledgedin the literature that unsupervised grammar induc-tion methods exhibit sensitivity to initialization.As in the previous section, we find that the pres-4As explained in Section 5, having a single expectation pa-rameter is motivated by our focus on parsing with universalrules.ence of linguistic rules greatly reduces this sensitiv-ity: for HDP-DEP, the standard deviation over fiverandomly initialized runs with the English-specificrules is 1.5%, compared to 4.5% for the parser de-veloped by Headden III et al (2009) and 8.0% forDMV (Klein and Manning, 2004).8 ConclusionsIn this paper we demonstrated that syntactic uni-versals encoded as declarative constraints improvegrammar induction.
We formulated a generativemodel for dependency structure that models syntac-tic category refinement and biases inference to co-here with the provided constraints.
Our experimentsshowed that encoding a compact, well-accepted setof language-independent constraints significantlyimproves accuracy on multiple languages comparedto the current state-of-the-art in unsupervised pars-ing.While our present work has yielded substantialgains over previous unsupervised methods, a largegap still remains between our method and fully su-pervised techniques.
In future work we intend tostudy ways to bridge this gap by 1) incorporat-ing more sophisticated linguistically-driven gram-mar rulesets to guide induction, 2) lexicalizing themodel, and 3) combining our constraint-based ap-proach with richer unsupervised models (e.g., Head-den III et al (2009)) to benefit from their comple-mentary strengths.AcknowledgmentsThe authors acknowledge the support of the NSF(CAREER grant IIS-0448168, grant IIS-0904684,and a Graduate Research Fellowship).
We are es-pecially grateful to Michael Collins for inspiring ustoward this line of inquiry and providing determin-istic rules for English parsing.
Thanks to TaylorBerg-Kirkpatrick, Sabine Iatridou, Ramesh Sridha-ran, and members of the MIT NLP group for theirsuggestions and comments.
Any opinions, findings,conclusions, or recommendations expressed in thispaper are those of the authors, and do not necessar-ily reflect the views of the funding organizations.1243ReferencesMark C. Baker.
2001.
The Atoms of Language: TheMind?s Hidden Rules of Grammar.
Basic Books.Emily M. Bender.
2009.
Linguistically na?
?ve != lan-guage independent: Why NLP needs linguistic typol-ogy.
In Proceedings of the EACL 2009 Workshopon the Interaction between Linguistics and Compu-tational Linguistics: Virtuous, Vicious or Vacuous?,pages 26?32.Taylor Berg-Kirkpatrick and Dan Klein.
2010.
Phylo-genetic grammar induction.
In Proceedings of ACL,pages 1288?1297.Christopher M. Bishop.
2006.
Pattern Recognition andMachine Learning.
Information Science and Statis-tics.
Springer.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of CoNLL, pages 149?164.David Burkett and Dan Klein.
2008.
Two languages arebetter than one (for syntactic parsing).
In Proceedingsof EMNLP, pages 877?886.Andrew Carnie.
2002.
Syntax: A Generative Introduc-tion (Introducing Linguistics).
Blackwell Publishing.Ming-Wei Chang, Lev Ratinov, and Dan Roth.2007.
Guiding semi-supervision with constraint-driven learning.
In Proceedings of ACL, pages 280?287.Shay B. Cohen and Noah A. Smith.
2009a.
Shared lo-gistic normal distributions for soft parameter tying inunsupervised grammar induction.
In Proceedings ofNAACL/HLT, pages 74?82.Shay B. Cohen and Noah A. Smith.
2009b.
Variationalinference for grammar induction with prior knowl-edge.
In Proceedings of ACL/IJCNLP 2009 Confer-ence Short Papers, pages 1?4.Michael Collins.
1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, Universityof Pennsylvania.Hal Daume?
III and Lyle Campbell.
2007.
A bayesianmodel for discovering typological implications.
InProceedings of ACL, pages 65?72.Gregory Druck, Gideon Mann, and Andrew McCal-lum.
2009.
Semi-supervised learning of dependencyparsers using generalized expectation criteria.
In Pro-ceedings of ACL/IJCNLP, pages 360?368.Thomas S. Ferguson.
1973.
A bayesian analysis ofsome nonparametric problems.
Annals of Statistics,1(2):209?230.Jenny Rose Finkel, Trond Grenager, and Christopher D.Manning.
2007.
The infinite tree.
In Proceedings ofACL, pages 272?279.Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.2009.
Dependency grammar induction via bitext pro-jection constraints.
In Proceedings of ACL/IJCNLP,pages 369?377.Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
Journal of MachineLearning Research, 11:2001?2049.Joa?o Grac?a, Kuzman Ganchev, Ben Taskar, and FernandoPereira.
2009.
Posterior vs. parameter sparsity in la-tent variable models.
In Advances in NIPS, pages 664?672.Joa?o Grac?a, Kuzman Ganchev, and Ben Taskar.
2007.Expectation maximization and posterior constraints.In Advances in NIPS, pages 569?576.Aria Haghighi and Dan Klein.
2006.
Prototype-drivengrammar induction.
In Proceedings of ACL, pages881?888.William P. Headden III, Mark Johnson, and David Mc-Closky.
2009.
Improving unsupervised dependencyparsing with richer contexts and smoothing.
In Pro-ceedings of NAACL/HLT, pages 101?109.Dan Klein and Christopher Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of ACL,pages 478?485.Jonas Kuhn.
2004.
Experiments in parallel-text basedgrammar induction.
In Proceedings of ACL, pages470?477.Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.2007.
The infinite PCFG using hierarchical Dirichletprocesses.
In Proceedings of EMNLP/CoNLL, pages688?697.Percy Liang, Michael I. Jordan, and Dan Klein.
2009a.Learning from measurements in exponential families.In Proceedings of ICML, pages 641?648.Percy Liang, Michael I. Jordan, and Dan Klein.
2009b.Probabilistic grammars and hierarchical Dirichlet pro-cesses.
The Handbook of Applied Bayesian Analysis.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of english: The penn treebank.
ComputationalLinguistics, 19(2):313?330.Frederick J. Newmeyer.
2005.
Possible and ProbableLanguages: A Generative Perspective on LinguisticTypology.
Oxford University Press.Slav Petrov and Dan Klein.
2007.
Learning and infer-ence for hierarchically split PCFGs.
In Proceeding ofAAAI, pages 1663?1666.Benjamin Snyder, Tahira Naseem, and Regina Barzilay.2009.
Unsupervised multilingual grammar induction.In Proceedings of ACL/IJCNLP, pages 73?81.Lydia White.
2003.
Second Language Acquisition andUniversal Grammar.
Cambridge University Press.1244
