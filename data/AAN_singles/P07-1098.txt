Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 776?783,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsExploiting Syntactic and Shallow Semantic Kernelsfor Question/Answer ClassificationAlessandro MoschittiUniversity of Trento38050 Povo di TrentoItalymoschitti@dit.unitn.itSilvia QuarteroniThe University of YorkYork YO10 5DDUnited Kingdomsilvia@cs.york.ac.ukRoberto Basili?Tor Vergata?
UniversityVia del Politecnico 100133 Rome, Italybasili@info.uniroma2.itSuresh ManandharThe University of YorkYork YO10 5DDUnited Kingdomsuresh@cs.york.ac.ukAbstractWe study the impact of syntactic and shallowsemantic information in automatic classifi-cation of questions and answers and answerre-ranking.
We define (a) new tree struc-tures based on shallow semantics encodedin Predicate Argument Structures (PASs)and (b) new kernel functions to exploit therepresentational power of such structureswith Support Vector Machines.
Our ex-periments suggest that syntactic informationhelps tasks such as question/answer classifi-cation and that shallow semantics gives re-markable contribution when a reliable set ofPASs can be extracted, e.g.
from answers.1 IntroductionQuestion answering (QA) is as a form of informa-tion retrieval where one or more answers are re-turned to a question in natural language in the formof sentences or phrases.
The typical QA system ar-chitecture consists of three phases: question pro-cessing, document retrieval and answer extraction(Kwok et al, 2001).Question processing is often centered on questionclassification, which selects one of k expected an-swer classes.
Most accurate models apply super-vised machine learning techniques, e.g.
SNoW (Liand Roth, 2005), where questions are encoded us-ing various lexical, syntactic and semantic features.The retrieval and answer extraction phases consist inretrieving relevant documents (Collins-Thompson etal., 2004) and selecting candidate answer passagesfrom them.
A further answer re-ranking phase is op-tionally applied.
Here, too, the syntactic structureof a sentence appears to provide more useful infor-mation than a bag of words (Chen et al, 2006), al-though the correct way to exploit it is still an openproblem.An effective way to integrate syntactic structuresin machine learning algorithms is the use of tree ker-nel (TK) functions (Collins and Duffy, 2002), whichhave been successfully applied to question classifi-cation (Zhang and Lee, 2003; Moschitti, 2006) andother tasks, e.g.
relation extraction (Zelenko et al,2003; Moschitti, 2006).
In more complex tasks suchas computing the relatedness between questions andanswers in answer re-ranking, to our knowledge nostudy uses kernel functions to encode syntactic in-formation.
Moreover, the study of shallow semanticinformation such as predicate argument structuresannotated in the PropBank (PB) project (Kingsburyand Palmer, 2002) (www.cis.upenn.edu/?ace) is apromising research direction.
We argue that seman-tic structures can be used to characterize the relationbetween a question and a candidate answer.In this paper, we extensively study new structuralrepresentations, encoding parse trees, bag-of-words,POS tags and predicate argument structures (PASs)for question classification and answer re-ranking.We define new tree representations for both simpleand nested PASs, i.e.
PASs whose arguments areother predicates (Section 2).
Moreover, we definenew kernel functions to exploit PASs, which we au-tomatically derive with our SRL system (Moschittiet al, 2005) (Section 3).Our experiments using SVMs and the above ker-776nels and data (Section 4) shows the following: (a)our approach reaches state-of-the-art accuracy onquestion classification.
(b) PB predicative structuresare not effective for question classification but showpromising results for answer classification on a cor-pus of answers to TREC-QA 2001 description ques-tions.
We created such dataset by using YourQA(Quarteroni and Manandhar, 2006), our basic Web-based QA system1.
(c) The answer classifier in-creases the ranking accuracy of our QA system byabout 25%.Our results show that PAS and syntactic parsingare promising methods to address tasks affected bydata sparseness like question/answer categorization.2 Encoding Shallow Semantic StructuresTraditionally, information retrieval techniques arebased on the bag-of-words (BOW) approach aug-mented by language modeling (Allan et al, 2002).When the task requires the use of more complex se-mantics, the above approaches are often inadequateto perform fine-level textual analysis.An improvement on BOW is given by the use ofsyntactic parse trees, e.g.
for question classification(Zhang and Lee, 2003), but these, too are inadequatewhen dealing with definitional answers expressed bylong and articulated sentences or even paragraphs.On the contrary, shallow semantic representations,bearing a more ?compact?
information, could pre-vent the sparseness of deep structural approachesand the weakness of BOW models.Initiatives such as PropBank (PB) (Kingsburyand Palmer, 2002) have made possible the design ofaccurate automatic Semantic Role Labeling (SRL)systems (Carreras and Ma`rquez, 2005).
Attemptingan application of SRL to QA hence seems natural,as pinpointing the answer to a question relies on adeep understanding of the semantics of both.Let us consider the PB annotation: [ARG1Antigens] were [AM?TMP originally] [reldefined] [ARG2 as non-self molecules].Such annotation can be used to design a shallowsemantic representation that can be matched againstother semantically similar sentences, e.g.
[ARG0Researchers] [rel describe] [ARG1 antigens][ARG2 as foreign molecules] [ARGM?LOC in1Demo at: http://cs.york.ac.uk/aig/aqua.PASreldefineARG1antigensARG2moleculesARGM-TMPoriginallyPASreldescribeARG0researchersARG1antigensARG2moleculesARGM-LOCbodyFigure 1: Compact predicate argument structures oftwo different sentences.the body].For this purpose, we can represent the above anno-tated sentences using the tree structures described inFigure 1.
In this compact representation, hereafterPredicate-Argument Structures (PAS), argumentsare replaced with their most important word ?
oftenreferred to as the semantic head.
This reducesdata sparseness with respect to a typical BOWrepresentation.However, sentences rarely contain a single pred-icate; it happens more generally that propositionscontain one or more subordinate clauses.
Forinstance let us consider a slight modification of thefirst sentence: ?Antigens were originally definedas non-self molecules which bound specifically toantibodies2 .?
Here, the main predicate is ?defined?,followed by a subordinate predicate ?bound?.
OurSRL system outputs the following two annotations:(1) [ARG1 Antigens] were [ARGM?TMPoriginally] [rel defined] [ARG2 as non-selfmolecules which bound specifically toantibodies].
(2) Antigens were originally defined as[ARG1 non-self molecules] [R?A1 which] [relbound] [ARGM?MNR specifically] [ARG2 toantibodies].giving the PASs in Figure 2.
(a) resp.
2.
(b).As visible in Figure 2.
(a), when an argument nodecorresponds to an entire subordinate clause, we labelits leaf with PAS, e.g.
the leaf of ARG2.
Such PASnode is actually the root of the subordinate clausein Figure 2.(b).
Taken as standalone, such PASs donot express the whole meaning of the sentence; itis more accurate to define a single structure encod-ing the dependency between the two predicates as in2This is an actual answer to ?What are antibodies??
fromour question answering system, YourQA.777PASreldefineARG1antigensARG2PASAM-TMPoriginally(a)PASrelboundARG1moleculesR-ARG1whichAM-ADVspecificallyARG2antibodies(b)PASreldefineARG1antigensARG2PASrelboundARG1moleculesR-ARG1whichAM-ADVspecificallyARG2antibodiesAM-TMPoriginally(c)Figure 2: Two PASs composing a PASNFigure 2.(c).
We refer to nested PASs as PASNs.It is worth to note that semantically equivalentsentences syntactically expressed in different waysshare the same PB arguments and the same PASs,whereas semantically different sentences result indifferent PASs.
For example, the sentence: ?Anti-gens were originally defined as antibodies whichbound specifically to non-self molecules?, uses thesame words as (2) but has different meaning.
Its PBannotation:(3) Antigens were originally definedas [ARG1 antibodies] [R?A1 which] [relbound] [ARGM?MNR specifically] [ARG2 tonon-self molecules],clearly differs from (2), as ARG2 is now non-self molecules; consequently, the PASs are alsodifferent.Once we have assumed that parse trees and PASscan improve on the simple BOW representation, weface the problem of representing tree structures inlearning machines.
Section 3 introduces a viable ap-proach based on tree kernels.3 Syntactic and Semantic Kernels for TextAs mentioned above, encoding syntactic/semanticinformation represented by means of tree structuresin the learning algorithm is problematic.
A first so-lution is to use all its possible substructures as fea-tures.
Given the combinatorial explosion of consid-ering subparts, the resulting feature space is usuallyvery large.
A tree kernel (TK) function which com-putes the number of common subtrees between twosyntactic parse trees has been given in (Collins andDuffy, 2002).
Unfortunately, such subtrees are sub-ject to the constraint that their nodes are taken withall or none of the children they have in the originaltree.
This makes the TK function not well suited forthe PAS trees defined above.
For instance, althoughthe two PASs of Figure 1 share most of the subtreesrooted in the PAS node, Collins and Duffy?s kernelwould compute no match.In the next section we describe a new kernel de-rived from the above tree kernel, able to evaluate themeaningful substructures for PAS trees.
Moreover,as a single PAS may not be sufficient for text rep-resentation, we propose a new kernel that combinesthe contributions of different PASs.3.1 Tree kernelsGiven two trees T1 and T2, let {f1, f2, ..} = F bethe set of substructures (fragments) and Ii(n) beequal to 1 if fi is rooted at node n, 0 otherwise.Collins and Duffy?s kernel is defined asTK(T1, T2) =?n1?NT1?n2?NT2 ?
(n1, n2), (1)where NT1 and NT2 are the sets of nodesin T1 and T2, respectively and ?
(n1, n2) =?|F|i=1 Ii(n1)Ii(n2).
The latter is equal to the numberof common fragments rooted in nodes n1 and n2.
?can be computed as follows:(1) if the productions (i.e.
the nodes with theirdirect children) at n1 and n2 are different then?
(n1, n2) = 0;(2) if the productions at n1 and n2 are the same, andn1 and n2 only have leaf children (i.e.
they are pre-terminal symbols) then ?
(n1, n2) = 1;(3) if the productions at n1 and n2 are the same, andn1 and n2 are not pre-terminals then ?
(n1, n2) =?nc(n1)j=1 (1+?
(cjn1 , cjn2)), where nc(n1) is the num-ber of children of n1 and cjn is the j-th child of n.Such tree kernel can be normalized and a ?
factorcan be added to reduce the weight of large structures(refer to (Collins and Duffy, 2002) for a completedescription).
The critical aspect of steps (1), (2) and(3) is that the productions of two evaluated nodeshave to be identical to allow the match of further de-scendants.
This means that common substructurescannot be composed by a node with only some of its778PASSLOTreldefineSLOTARG1antigens*SLOTARG2PAS*SLOTARGM-TMPoriginally*(a)PASSLOTreldefineSLOTARG1antigens*SLOTnullSLOTnull(b)PASSLOTreldefineSLOTnullSLOTARG2PAS*SLOTnull(c)Figure 3: A PAS with some of its fragments.children as an effective PAS representation wouldrequire.
We solve this problem by designing theShallow Semantic Tree Kernel (SSTK) which allowsto match portions of a PAS.3.2 The Shallow Semantic Tree Kernel (SSTK)The SSTK is based on two ideas: first, we changethe PAS, as shown in Figure 3.
(a) by adding SLOTnodes.
These accommodate argument labels in aspecific order, i.e.
we provide a fixed number ofslots, possibly filled with null arguments, that en-code all possible predicate arguments.
For simplic-ity, the figure shows a structure of just 4 arguments,but more can be added to accommodate the max-imum number of arguments a predicate can have.Leaf nodes are filled with the wildcard character *but they may alternatively accommodate additionalinformation.The slot nodes are used in such a way that theadopted TK function can generate fragments con-taining one or more children like for example thoseshown in frames (b) and (c) of Figure 3.
As pre-viously pointed out, if the arguments were directlyattached to the root node, the kernel function wouldonly generate the structure with all children (or thestructure with no children, i.e.
empty).Second, as the original tree kernel would generatemany matches with slots filled with the null label,we have set a new step 0:(0) if n1 (or n2) is a pre-terminal node and its childlabel is null, ?
(n1, n2) = 0;and subtract one unit to ?
(n1, n2), in step 3:(3) ?
(n1, n2) =?nc(n1)j=1 (1 + ?
(cjn1 , cjn2))?
1,The above changes generate a new ?
which,when substituted (in place of the original ?)
in Eq.1, gives the new Shallow Semantic Tree Kernel.
Toshow that SSTK is effective in counting the numberof relations shared by two PASs, we propose the fol-lowing:Proposition 1 The new ?
function applied to themodified PAS counts the number of all possible k-ary relations derivable from a set of k arguments,i.e.
?ki=1(ki)relations of arity from 1 to k (the pred-icate being considered as a special argument).Proof We observe that a kernel applied to a tree anditself computes all its substructures, thus if we eval-uate SSTK between a PAS and itself we must obtainthe number of generated k-ary relations.
We proveby induction the above claim.For the base case (k = 0): we use a PAS with noarguments, i.e.
all its slots are filled with null la-bels.
Let r be the PAS root; since r is not a pre-terminal, step 3 is selected and ?
is recursively ap-plied to all r?s children, i.e.
the slot nodes.
For thelatter, step 0 assigns ?
(cjr, cjr) = 0.
As a result,?
(r, r) = ?nc(r)j=1 (1 + 0)?
1 = 0 and the base caseholds.For the general case, r is the root of a PAS with k+1arguments.
?
(r, r) = ?nc(r)j=1 (1 + ?
(cjr, cjr)) ?
1=?kj=1(1+?
(cjr , cjr))?(1+?
(ck+1r , ck+1r ))?1.
Fork arguments, we assume by induction that?kj=1(1+?
(cjr, cjr))?
1 =?ki=1(ki), i.e.
the number of k-aryrelations.
Moreover, (1 + ?
(ck+1r , ck+1r )) = 2, thus?
(r, r) = ?ki=1(ki)?
2 = 2k ?
2 = 2k+1 = ?k+1i=1(k+1i), i.e.
all the relations until arity k + 1 2TK functions can be applied to sentence parsetrees, therefore their usefulness for text processingapplications, e.g.
question classification, is evident.On the contrary, the SSTK applied to one PAS ex-tracted from a text fragment may not be meaningfulsince its representation needs to take into account allthe PASs that it contains.
We address such problem779by defining a kernel on multiple PASs.Let Pt and Pt?
be the sets of PASs extracted fromthe text fragment t and t?.
We define:Kall(Pt, Pt?)
=?p?Pt?p?
?Pt?SSTK(p, p?
), (2)While during the experiments (Sect.
4) the Kallkernel is used to handle predicate argument struc-tures, TK (Eq.
1) is used to process parse trees andthe linear kernel to handle POS and BOW features.4 ExperimentsThe purpose of our experiments is to study the im-pact of the new representations introduced earlier forQA tasks.
In particular, we focus on question clas-sification and answer re-ranking for Web-based QAsystems.In the question classification task, we extend pre-vious studies, e.g.
(Zhang and Lee, 2003; Moschitti,2006), by testing a set of previously designed ker-nels and their combination with our new Shallow Se-mantic Tree Kernel.
In the answer re-ranking task,we approach the problem of detecting descriptionanswers, among the most complex in the literature(Cui et al, 2005; Kazawa et al, 2001).The representations that we adopt are: bag-of-words (BOW), bag-of-POS tags (POS), parse tree(PT), predicate argument structure (PAS) and nestedPAS (PASN).
BOW and POS are processed bymeans of a linear kernel, PT is processed with TK,PAS and PASN are processed by SSTK.
We imple-mented the proposed kernels in the SVM-light-TKsoftware available at ai-nlp.info.uniroma2.it/moschitti/ which encodes tree kernel functions inSVM-light (Joachims, 1999).4.1 Question classificationAs a first experiment, we focus on question classi-fication, for which benchmarks and baseline resultsare available (Zhang and Lee, 2003; Li and Roth,2005).
We design a question multi-classifier bycombining n binary SVMs3 according to the ONE-vs-ALL scheme, where the final output class is theone associated with the most probable prediction.The PASs were automatically derived by our SRL3We adopted the default regularization parameter (i.e., theaverage of 1/||~x||) and tried a few cost-factor values to adjustthe rate between Precision and Recall on the development set.system which achieves a 76% F1-measure (Mos-chitti et al, 2005).As benchmark data, we use the question train-ing and test set available at: l2r.cs.uiuc.edu/?cogcomp/Data/QA/QC/, where the test set are the500 TREC 2001 test questions (Voorhees, 2001).We refer to this split as UIUC.
The performance ofthe multi-classifier and the individual binary classi-fiers is measured with accuracy resp.
F1-measure.To collect statistically significant information, werun 10-fold cross validation on the 6,000 questions.Features Accuracy (UIUC) Accuracy (c.v.)PT 90.4 84.8?1.2BOW 90.6 84.7?1.2PAS 34.2 43.0?1.9POS 26.4 32.4?2.1PT+BOW 91.8 86.1?1.1PT+BOW+POS 91.8 84.7?1.5PAS+BOW 90.0 82.1?1.3PAS+BOW+POS 88.8 81.0?1.5Table 1: Accuracy of the question classifier with dif-ferent feature combinationsQuestion classification results Table 1 shows theaccuracy of different question representations on theUIUC split (Column 1) and the average accuracy ?the corresponding confidence limit (at 90% signifi-cance) on the cross validation splits (Column 2).
(i)The TK on PT and the linear kernel on BOW pro-duce a very high result, i.e.
about 90.5%.
This ishigher than the best outcome derived in (Zhang andLee, 2003), i.e.
90%, obtained with a kernel combin-ing BOW and PT on the same data.
Combined withPT, BOW reaches 91.8%, very close to the 92.5%accuracy reached in (Li and Roth, 2005) using com-plex semantic information from external resources.
(ii) The PAS feature provides no improvement.
Thisis mainly because at least half of the training andtest questions only contain the predicate ?to be?, forwhich a PAS cannot be derived by a PB-based shal-low semantic parser.
(iii) The 10-fold cross-validation experiments con-firm the trends observed in the UIUC split.
Thebest model (according to statistical significance) isPT+BOW, achieving an 86.1% average accuracy4.4This value is lower than the UIUC split one as the UIUCtest set is not consistent with the training set (it contains the7804.2 Answer classificationQuestion classification does not allow to fully ex-ploit the PAS potential since questions tend to beshort and with few verbal predicates (i.e.
the onlyones that our SRL system can extract).
A differ-ent scenario is answer classification, i.e.
decidingif a passage/sentence correctly answers a question.Here, the semantics to be generated by the classi-fier are not constrained to a small taxonomy and an-swer length may make the PT-based representationtoo sparse.We learn answer classification with a binary SVMwhich determines if an answer is correct for the tar-get question: here, the classification instances are?question, answer?
pairs.
Each pair component canbe encoded with PT, BOW, PAS and PASN repre-sentations (processed by previous kernels).As test data, we collected the 138 TREC 2001 testquestions labeled as ?description?
and for each, weobtained a list of answer paragraphs extracted fromWeb documents using YourQA.
Each paragraph sen-tence was manually evaluated based on whether itcontained an answer to the corresponding question.Moreover, to simplify the classification problem, weisolated for each paragraph the sentence which ob-tained the maximal judgment (in case more than onesentence in the paragraph had the same judgment,we chose the first one).
We collected a corpus con-taining 1309 sentences, 416 of which ?
labeled ?+1??
answered the question either concisely or withnoise; the rest ?
labeled ?-1??
were either irrele-vant to the question or contained hints relating to thequestion but could not be judged as valid answers5.Answer classification results To test the impactof our models on answer classification, we ran 5-foldcross-validation, with the constraint that two pairs?q, a1?
and ?q, a2?
associated with the same ques-tion q could not be split between training and test-ing.
Hence, each reported value is the average over 5different outcomes.
The standard deviations rangedTREC 2001 questions) and includes a larger percentage of eas-ily classified question types, e.g.
the numeric (22.6%) and de-scription classes (27.6%) whose percentage in training is 16.4%resp.
16.2%.5For instance, given the question ?What are invertebrates?
?,the sentence ?At least 99% of all animal species are inverte-brates, comprising .
.
.
?
was labeled ?-1?
, while ?Invertebratesare animals without backbones.?
was labeled ?+1?.                                  	  	                   fffi flffi !
"# flffi !
fi flffi !
"# fl $% !fi flffi !
"# fl$% & ffi !
fi fl$% & ffi !
"# fl $% & ffi !fi fl$% !
"# fl $% !
fi fl$% !
"# fl$% & ffi !fi fl$% !
"# flffi !Figure 4: Impact of the BOW and PT features onanswer classification'( )*'( )+' + )*' + )+' ' )*' ' )+' , )*' , )+'- )*'- )+'.
)*/)+ 0 )* 0 )+ 1 )* 1 )+ ( )* ( )+ + )* + )+ ' )* ' )+ , )*2 3 4 5 6 78 9 53 :;<=>?
@ABC?D EFGH IJK EFGH ID EFGH IJK ELM N F G H ID EFGH IJK EFGH NLM N LK O ID EFGH IJK EFGH NLM N LK O P ID EFGH IJK EFGH NLK O ID EFGH IJK EFGH NLK O P IFigure 5: Impact of the PAS and PASN featurescombined with the BOW and PT features on answerclassificationQ R STQ R SUQ V STQ V SUUT STUT SUU W STU W SUUX STW SU X ST X SU Y ST Y SU Q ST Q SU U ST U SU Z ST Z SU [ ST\ ] ^ _` ab c _] defghijklmin opqr stu ovu w sn opqr stu ovu w x sFigure 6: Comparison between PAS and PASNwhen used as standalone features for the answer onanswer classification781approximately between 2.5 and 5.
The experimentswere organized as follows:First, we examined the contributions of BOW andPT representations as they proved very important forquestion classification.
Figure 4 reports the plot ofthe F1-measure of answer classifiers trained with allcombinations of the above models according to dif-ferent values of the cost-factor parameter, adjustingthe rate between Precision and Recall.
We see herethat the most accurate classifiers are the ones usingboth the answer?s BOW and PT feature and eitherthe question?s PT or BOW feature (i.e.
Q(BOW) +A(PT,BOW) resp.
Q(PT) + A(PT,BOW) combina-tions).
When PT is used for the answer the sim-ple BOW model is outperformed by 2 to 3 points.Hence, we infer that both the answer?s PT and BOWfeatures are very useful in the classification task.However, PT does not seem to provide additionalinformation to BOW when used for question repre-sentation.
This can be explained by considering thatanswer classification (restricted to description ques-tions) does not require question type classificationsince its main purpose is to detect question/answerrelations.
In this scenario, the question?s syntacticstructure does not seem to provide much more infor-mation than BOW.Secondly, we evaluated the impact of the newlydefined PAS and PASN features combined with thebest performing previous model, i.e.
Q(BOW) +A(PT,BOW).
Figure 5 illustrates the F1-measureplots again according to the cost-factor param-eter.
We observe here that model Q(BOW)+ A(PT,BOW,PAS) greatly outperforms modelQ(BOW) + A(PT,BOW), proving that the PAS fea-ture is very useful for answer classification, i.e.the improvement is about 2 to 3 points while thedifference with the BOW model, i.e.
Q(BOW)+ A(BOW), exceeds 3 points.
The Q(BOW) +A(PT,BOW,PASN) model is not more effective thanQ(BOW) + A(PT,BOW,PAS).
This suggests eitherthat PAS is more effective than PASN or that whenthe PT information is added, the PASN contributionfades out.To further investigate the previous issue, we fi-nally compared the contribution of the PAS andPASN when combined with the question?s BOWfeature alone, i.e.
no PT is used.
The results, re-ported in Figure 6, show that this time PASN per-forms better than PAS.
This suggests that the depen-dencies between the nested PASs are in some waycaptured by the PT information.
Indeed, it shouldbe noted that we join predicates only in case one issubordinate to the other, thus considering only a re-stricted set of all possible predicate dependencies.However, the improvement over PAS confirms thatPASN is the right direction to encode shallow se-mantics from different sentence predicates.Baseline P R F1-measureGg@5 39.22?3.59 33.15?4.22 35.92?3.95QA@5 39.72?3.44 34.22?3.63 36.76?3.56Gg@all 31.58?0.58 100 48.02?0.67QA@all 31.58?0.58 100 48.02?0.67Gg QA Re-rankerMRR 48.97?3.77 56.21?3.18 81.12?2.12Table 2: Baseline classifiers accuracy and MRR ofYourQA (QA), Google (Gg) and the best re-ranker4.3 Answer re-rankingThe output of the answer classifier can be used tore-rank the list of candidate answers of a QA sys-tem.
Starting from the top answer, each instance canbe classified based on its correctness with respectto the question.
If it is classified as correct its rankis unchanged; otherwise it is pushed down, until alower ranked incorrect answer is found.We used the answer classifier with the highest F1-measure on the development set according to differ-ent cost-factor values6.
We applied such model tothe Google ranks and to the ranks of our Web-basedQA system, i.e.
YourQA.
The latter uses Web docu-ments corresponding to the top 20 Google results forthe question.
Then, each sentence in each documentis compared to the question via a blend of similar-ity metrics used in the answer extraction phase toselect the most relevant sentence.
A passage of upto 750 bytes is then created around the sentence andreturned as an answer.Table 2 illustrates the results of the answer classi-fiers derived by exploiting Google (Gg) and YourQA(QA) ranks: the top N ranked results are consideredas correct definitions and the remaining ones as in-6However, by observing the curves in Fig.
5, the selectedparameters appear as pessimistic estimates for the best modelimprovement: the one for BOW is the absolute maximum, butan average one is selected for the best model.782correct for different values of N .
We show N = 5and the maximum N (all), i.e.
all the available an-swers.
Each measure is the average of the Precision,Recall and F1-measure from cross validation.
TheF1-measure of Google and YourQA are greatly out-performed by our answer classifier.The last row of Table 2 reports the MRR7achieved by Google, YourQA (QA) and YourQA af-ter re-ranking (Re-ranker).
We note that Google isoutperformed by YourQA since its ranks are basedon whole documents, not on single passages.
ThusGoogle may rank a document containing severalsparsely distributed question words higher than doc-uments with several words concentrated in one pas-sage, which are more interesting.
When the answerclassifier is applied to improve the YourQA ranking,the MRR reaches 81.1%, rising by about 25%.Finally, it is worth to note that the answer clas-sifier based on Q(BOW)+A(BOW,PT,PAS) model(parameterized as described) gave a 4% higher MRRthan the one based on the simple BOW features.
Asan example, for question ?What is foreclosure?
?, thesentence ?Foreclosure means that the lender takespossession of your home and sells it in order to getits money back.?
was correctly classified by the bestmodel, while BOW failed.5 ConclusionIn this paper, we have introduced new structures torepresent textual information in three question an-swering tasks: question classification, answer classi-fication and answer re-ranking.
We have defined treestructures (PAS and PASN) to represent predicate-argument relations, which we automatically extractusing our SRL system.
We have also introduced twofunctions, SSTK and Kall, to exploit their repre-sentative power.Our experiments with SVMs and the above modelssuggest that syntactic information helps tasks suchas question classification whereas semantic informa-tion contained in PAS and PASN gives promising re-sults in answer classification.In the future, we aim to study ways to capture re-lations between predicates so that more general se-7The Mean Reciprocal Rank is defined as: MRR =1n?ni=11ranki, where n is the number of questions and rankiis the rank of the first correct answer to question i.mantics can be encoded by PASN.
Forms of general-ization for predicates and arguments within PASNslike LSA clusters, WordNet synsets and FrameNet(roles and frames) information also appear as apromising research area.AcknowledgmentsWe thank the anonymous reviewers for their helpful sugges-tions.
Alessandro Moschitti would like to thank the AMI2 labat the University of Trento and the EU project LUNA ?spokenLanguage UNderstanding in multilinguAl communication sys-tems?
contract no 33549 for supporting part of his research.ReferencesJ.
Allan, J. Aslam, N. Belkin, and C. Buckley.
2002.
Chal-lenges in IR and language modeling.
In Report of a Work-shop at the University of Amherst.X.
Carreras and L. Ma`rquez.
2005.
Introduction to the CoNLL-2005 shared task: SRL.
In CoNLL-2005.Y.
Chen, M. Zhou, and S. Wang.
2006.
Reranking answersfrom definitional QA using language models.
In ACL?06.M.
Collins and N. Duffy.
2002.
New ranking algorithms forparsing and tagging: Kernels over discrete structures, andthe voted perceptron.
In ACL?02.K.
Collins-Thompson, J. Callan, E. Terra, and C. L.A. Clarke.2004.
The effect of document retrieval quality on factoid QAperformance.
In SIGIR?04.
ACM.H.
Cui, M. Kan, and T. Chua.
2005.
Generic soft pattern mod-els for definitional QA.
In SIGIR?05.
ACM.T.
Joachims.
1999.
Making large-scale SVM learning practical.In Advances in Kernel Methods - Support Vector Learning.H.
Kazawa, H. Isozaki, and E. Maeda.
2001.
NTT questionanswering system in TREC 2001.
In TREC?01.P.
Kingsbury and M. Palmer.
2002.
From Treebank to Prop-Bank.
In LREC?02.C.
C. T. Kwok, O. Etzioni, and D. S. Weld.
2001.
Scalingquestion answering to the web.
In WWW?01.X.
Li and D. Roth.
2005.
Learning question classifiers: the roleof semantic information.
Journ.
Nat.
Lang.
Eng.A.
Moschitti, B. Coppola, A. Giuglea, and R. Basili.
2005.Hierarchical semantic role labeling.
In CoNLL 2005 sharedtask.A.
Moschitti.
2006.
Efficient convolution kernels for depen-dency and constituent syntactic trees.
In ECML?06.S.
Quarteroni and S. Manandhar.
2006.
User modelling forAdaptive Question Answering and Information Retrieval.
InFLAIRS?06.E.
M. Voorhees.
2001.
Overview of the TREC 2001 QA track.In TREC?01.D.
Zelenko, C. Aone, and A. Richardella.
2003.
Kernel meth-ods for relation extraction.
Journ.
of Mach.
Learn.
Res.D.
Zhang and W. Lee.
2003.
Question classification using sup-port vector machines.
In SIGIR?03.
ACM.783
