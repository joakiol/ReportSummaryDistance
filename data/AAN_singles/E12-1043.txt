Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 420?429,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsCombining Tree Structures, Flat Features and Patternsfor Biomedical Relation ExtractionMd.
Faisal Mahbub Chowdhury ?
?
and Alberto Lavelli ??
Fondazione Bruno Kessler (FBK-irst), Italy?
University of Trento, Italy{chowdhury,lavelli}@fbk.euAbstractKernel based methods dominate the currenttrend for various relation extraction tasksincluding protein-protein interaction (PPI)extraction.
PPI information is critical in un-derstanding biological processes.
Despiteconsiderable efforts, previously reportedPPI extraction results show that none of theapproaches already known in the literatureis consistently better than other approacheswhen evaluated on different benchmark PPIcorpora.
In this paper, we propose anovel hybrid kernel that combines (auto-matically collected) dependency patterns,trigger words, negative cues, walk fea-tures and regular expression patterns alongwith tree kernel and shallow linguistic ker-nel.
The proposed kernel outperforms theexiting state-of-the-art approaches on theBioInfer corpus, the largest PPI benchmarkcorpus available.
On the other four smallerbenchmark corpora, it performs either bet-ter or almost as good as the existing ap-proaches.
Moreover, empirical results showthat the proposed hybrid kernel attains con-siderably higher precision than the existingapproaches, which indicates its capabilityof learning more accurate models.
This alsodemonstrates that the different types of in-formation that we use are able to comple-ment each other for relation extraction.1 IntroductionKernel methods are considered the most effectivetechniques for various relation extraction (RE)tasks on both general (e.g.
newspaper text) andspecialized (e.g.
biomedical text) domains.
Inparticular, as the importance of syntactic struc-tures for deriving the relationships between en-tities in text has been growing, several graphand tree kernels have been designed and experi-mented.Early RE approaches more or less fall in one ofthe following categories: (i) exploitation of statis-tics about co-occurrences of entities, (ii) usage ofpatterns and rules, and (iii) usage of flat featuresto train machine learning (ML) classifiers.
Theseapproaches have been studied for a long periodand have their own pros and cons.
Exploitationof co-occurrence statistics results in high recallbut low precision, while rule or pattern based ap-proaches can increase precision but suffer fromlow recall.
Flat feature based ML approaches em-ploy various kinds of linguistic, syntactic or con-textual information and integrate them into thefeature space.
They obtain relatively good resultsbut are hindered by drawbacks of limited featurespace and excessive feature engineering.
Kernelbased approaches have become an attractive alter-native solution, as they can exploit huge amountof features without an explicit representation.In this paper, we propose a new hybrid kernelfor RE.
We apply the kernel to Protein?proteininteraction (PPI) extraction, the most widely re-searched topic in biomedical relation extraction.PPI1 information is very critical in understandingbiological processes.
Considerable progress hasbeen made for this task.
Nevertheless, empiricalresults of previous studies show that none of theapproaches already known in the literature is con-sistently better than other approaches when evalu-ated on different benchmark PPI corpora (see Ta-ble 4).
This demands further study and innovation1PPIs occur when two or more proteins bind together,and are integral to virtually all cellular processes, such asmetabolism, signalling, regulation, and proliferation (Tikket al 2010).420of new approaches that are sensitive to the varia-tions of complex linguistic constructions.The proposed hybrid kernel is the compositionof one tree kernel and two feature based kernels(one of them is already known in the literatureand the other is proposed in this paper for the firsttime).
The novelty of the newly proposed featurebased kernel is that it envisages to accommodatethe advantages of pattern based approaches.
Moreprecisely:1.
We propose a new feature based kernel (de-tails in Section 4.1) by using syntactic de-pendency patterns, trigger words, negativecues, regular expression (henceforth, regex)patterns and walk features (i.e.
e-walks andv-walks)2.2.
The syntactic dependency patterns are au-tomatically collected from a type of depen-dency subgraph (we call it reduced graph,more details in Section 4.1.1) during run-time.3.
We only use the regex patterns, trigger wordsand negative cues mentioned in the literature(Ono et al 2001; Fundel et al 2007; Bui etal., 2010).
The objective is to verify whetherwe can exploit knowledge which is alreadyknown and used.4.
We propose a hybrid kernel by combin-ing the proposed feature based kernel (out-lined above) with the Shallow Linguistic(SL) kernel (Giuliano et al 2006) and thePath-enclosed Tree (PET) kernel (Moschitti,2004).The aim of our work is to take advantage ofdifferent types of information (i.e., dependencypatterns, regex patterns, trigger words, negativecues, syntactic dependencies among words andconstituent parse trees) and their different repre-sentations (i.e.
flat features, tree structures andgraphs) which can complement each other to learnmore accurate models.2The syntactic dependencies of the words of a sentencecreate a dependency graph.
A v-walk feature consists of(wordi ?
dependency typei,i+1 ?
wordi+1), and an e-walk feature is composed of (dependency typei?1,i ?wordi ?
dependency typei,i+1).
Note that, in a depen-dency graph, the words are nodes while the dependencytypes are edges.The remainder of the paper is organized as fol-lows.
In Section 2, we briefly review previouswork.
Section 3 lists the datasets.
Then, in Sec-tion 4, we define our proposed hybrid kernel anddescribe its individual component kernels.
Sec-tion 5 outlines the experimental settings.
Follow-ing that, empirical results are discussed in Section6.
Finally, we conclude with a summary of ourstudy as well as suggestions for further improve-ment of our approach.2 Related WorkIn this section, we briefly discuss some of therecent work on PPI extraction.
Several RE ap-proaches have been reported to date for the PPItask, most of which are kernel based methods.Tikk et al(2010) reported a benchmark evalu-ation of various kernels on PPI extraction.
Aninteresting finding is that the Shallow Linguis-tic (SL) kernel (Giuliano et al 2006) (to be dis-cussed in Section 4.2), despite its simplicity, is onpar with the best kernels in most of the evaluationsettings.Kim et al(2010) proposed walk-weighted sub-sequence kernel using e-walks, partial matches,non-contiguous paths, and different weights fordifferent sub-structures (which are used to capturestructural similarities during kernel computation).Miwa et al(2009a) proposed a hybrid kernel,which combines the all-paths graph (APG) kernel(Airola et al 2008), the bag-of-words kernel, andthe subset tree kernel (Moschitti, 2006) (appliedon the shortest dependency paths between targetprotein pairs).
They used multiple parser inputs.The system is regarded as the current state-of-the-art PPI extraction system because of its high re-sults on different PPI corpora (see the results inTable 4).As an extension of their work, they boosted sys-tem performance by training on multiple PPI cor-pora instead of on a single corpus and adoptinga corpus weighting concept with support vectormachine (SVM) which they call SVM-CW (Miwaet al 2009b).
Since most of their results are re-ported by training on the combination of multi-ple corpora, it is not possible to compare themdirectly with the results published in the other re-lated works (that usually adopt 10-fold cross vali-dation on a single PPI corpus).
To be comparablewith the vast majority of the existing work, wealso report results using 10-fold cross validation421Corpus Sentences Positive pairs Negative pairsBioInfer 1,100 2,534 7,132AIMed 1,955 1,000 4,834IEPA 486 335 482HPRD50 145 163 270LLL 77 164 166Table 1: Basic statistics of the 5 benchmark PPI cor-pora.on single corpora.Apart from the approaches described above,there also exist other studies that used kernels forPPI extraction (e.g.
subsequence kernel (Bunescuand Mooney, 2006)).A notable exception is the work published byBui et al(2010).
They proposed an approach thatconsists of two phases.
In the first phase, theirsystem categorizes the data into different groups(i.e.
subsets) based on various properties and pat-terns.
Later they classify candidate PPI pairs in-side each of the groups using SVM trained withfeatures specific for the corresponding group.3 DataThere are 5 benchmark corpora for the PPI taskthat are frequently used: HPRD50 (Fundel et al2007), IEPA (Ding et al 2002), LLL (Ne?dellec,2005), BioInfer (Pyysalo et al 2007) and AIMed(Bunescu et al 2005).
These corpora adopt dif-ferent PPI annotation formats.
For a comparativeevaluation Pyysalo et al(2008) put all of themin a common format which has become the stan-dard evaluation format for the PPI task.
In ourexperiments, we use the versions of the corporaconverted to such format.Table 1 shows various statistics regarding the 5(converted) corpora.4 Proposed Hybrid KernelThe hybrid kernel that we propose is as follows:KHybrid (R1, R2) = KTPWF (R1, R2)+ KSL (R1, R2) + w * KPET (R1, R2)where KTPWF stands for the new featurebased kernel (henceforth, TPWF kernel) com-puted using flat features collected by exploitingpatterns, trigger words, negative cues and walkfeatures.
KSL and KPET stand for the ShallowLinguistic (SL) kernel and the Path-enclosed Tree(PET) kernel respectively.
w is a multiplicativeconstant used for the PET kernel.
It allows thehybrid kernel to assign more (or less) weight tothe information obtained using tree structures de-pending on the corpus.
The proposed hybrid ker-nel is valid according to the closure properties ofkernels.Both the TPWF and SL kernels are linear ker-nels, while PET kernel is computed using Unlex-icalized Partial Tree (uPT) kernel (Severyn andMoschitti, 2010).
The following subsections ex-plain each of the individual kernels in more detail.4.1 Proposed TPWF Kernel4.1.1 Reduced graph, trigger words,negative cues and dependency patternsFor each of the candidate entity pairs, weconstruct a type of subgraph from the depen-dency graph formed by the syntactic dependen-cies among the words of a sentence.
We call it?reduced graph?
and define it in the follow-ing way:A reduced graph is a subgraphof the dependency graph of a sentencewhich includes:?
the two candidate entities and theirgovernor nodes up to their leastcommon governor (if exists).?
dependent nodes (if exist) of all thenodes added in the previous step.?
the immediate governor(s) (if ex-ists) of the least common governor.Figure 1 shows an example of a reduced graph.A reduced graph is an extension of the smallestcommon subgraph of the dependency graph thataims at overcoming its limitations.
It is a knownissue that the smallest common subgraph (or sub-tree) sometimes does not contain cue words.
Pre-viously, Chowdhury et al(2011a) proposed a lin-guistically motivated extension of the minimal(i.e.
smallest) common subtree (which includesthe candidate entity pairs), known as Mildly Ex-tended Dependency Tree (MEDT).
However, therules used for MEDT are too constrained.
Our ob-jective in constructing the reduced graph is to in-clude any potential modifier(s) or cue word(s) thatdescribes the relation between the given pair ofentities.
Sometimes such modifiers or cue wordsare not directly dependent (syntactically) on any422BioInfer AIMed IEPA HPRD50 LLLP R F P R F P R F P R F P R FOnly walk features 51.8 71.2 60.0 48.7 63.2 55.0 61.0 75.2 67.4 60.2 65.0 62.5 64.6 87.8 74.4Features: dep.
patterns, 53.8 68.8 60.4 50.6 63.9 56.5 63.9 74.6 68.9 65.0 71.8 68.2 66.5 89.6 76.4trigger, neg.
cues, walksFeatures: dep.
patterns, 53.5 68.6 60.1 52.5 62.9 57.2 63.8 74.6 68.8 65.1 69.9 67.5 67.4 88.4 76.5trigger, neg.
cues, walks,regex patternsTable 2: Results of the proposed TPWF feature based kernel on 5 benchmark PPI corpora before and after addingfeatures collected using dependency patterns, regex patterns, trigger words and negative cues to the walk features.The TPWF kernel is a component of the new hybrid kernel.Figure 1: Dependency graph for the sentence ?A pVHL mutant containing a P154L substitution does not promotedegradation of HIF1-Alpha?
generated by the Stanford parser.
The edges with blue dots form the smallestcommon subgraph for the candidate entity pair pVHL and HIF1-Alpha, while the edges with red dots form thereduced graph for the pair.of the entities (of the candidate pair).
Rather theyare dependent on some other word(s) which is de-pendent on one (or both) of the entities.
The word?not?
in Figure 1 is one such example.
The re-duced graph aims to preserve these cue words.The following types of features are collectedfrom the reduced graph of a candidate pair:1.
HasTriggerWord: whether the least commongovernor(s) of the target entity pairs insidethe reduced graph matches any trigger word.2.
Trigger-X: whether the least common gov-ernor(s) of the target entity pairs inside thereduced graph matches the trigger word ?X?.3.
HasNegWord: whether the reduced graphcontains any negative word.4.
DepPattern-i: whether the reduced graphcontains all the syntactic dependencies of thei-th pattern of dependency pattern list.The dependency pattern list is automaticallyconstructed from the training data during thelearning phase.
Each pattern is a set of syntacticdependencies of the corresponding reduced graphof a (positive or negative) entity pair in the train-ing data.
For example, the dependency pattern forthe reduced graph in Figure 1 is {det, amod, part-mod, nsubj, aux, neg, dobj, prep of}.
The samedependency pattern might be constructed for mul-tiple (positive or negative) entity pairs.
However,if it is constructed for both positive and negativepairs, it has to be discarded from the pattern list.The dependency patterns allow some kind ofunderspecification as they do not contain the lex-ical items (i.e.
words) but contain the likely com-bination of syntactic dependencies that a given re-lated pair of entities would pose inside their re-duced graph.The list of trigger words contains 144 wordspreviously used by Bui et al(2010) and Fundelet al(2007).
The list of negative cues contain 18words, most of which are mentioned in Fundel etal.
(2007).4.1.2 Walk featuresWe extract e-walk and v-walk features fromthe Mildly Extended Dependency Tree (MEDT)(Chowdhury et al 2011a) of each candidate pair.Reduced graphs sometimes include some unin-423BioInfer AIMed IEPA HPRD50 LLLPos.
/ Neg.
2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166P R F P R F P R F P R F P R FProposed TPWF kernel 53.8 68.8 60.4 50.6 63.9 56.5 63.9 74.6 68.9 65.0 71.8 68.2 66.5 89.6 76.4(without regex)Proposed TPWF kernel 53.5 68.6 60.1 52.5 62.9 57.2 63.8 74.6 68.8 65.1 69.9 67.5 67.4 88.4 76.5(with regex)SL kernel 60.8 65.8 63.2 56.2 64.4 60.0 73.3 71.9 72.6 62.0 65.0 63.5 74.9 85.4 79.8PET kernel 72.8 74.9 73.9 44.8 72.8 55.5 70.7 77.9 74.2 65.0 73.0 68.8 72.1 89.6 79.9Proposed hybrid kernel 80.0 71.4 75.5 64.2 58.2 61.1 81.1 69.3 74.7 72.9 59.5 65.5 70.4 95.7 81.1(PET + SL + TPWF(without regex))Proposed hybrid kernel 80.1 72.0 75.9 64.4 58.3 61.2 79.3 69.6 74.1 71.9 61.4 66.2 70.6 95.1 81.0(PET + SL + TPWF(with regex))Table 3: Results of the proposed hybrid kernel and its individual components.
Pos.
and Neg.
refer to numberpositive and negative relations respectively.
PET refers to the path-enclosed tree kernel, SL refers to the shallowlinguistic kernel, and TPWF refers to the kernel computed using trigger, pattern, negative cue and walk features.formative words which produce uninformativewalk features.
Hence, they are not suitable forwalk feature generation.
MEDT suits better forthis purpose.
The walk features extracted fromMEDTs have the following properties:?
The directionality of the edges (or nodes) inan e-walk (or v-walk) is not considered.
Inother words, e.g., pos(stimulatory)?amod?pos(effects) and pos(effects) ?
amod ?pos(stimulatory) are treated as the same fea-ture.?
The v-walk features are of the form (posi ?dependency typei,i+1?posi+1).
Here, posi isthe POS tag of wordi, i is the governor nodeand i + 1 is the dependent node.?
The e-walk features are of the form(dep.
typei?1,i ?
posi ?
dep.
typei,i+1) and(dep.
typei?1,i ?
lemmai ?
dep.
typei,i+1).Here, lemmai is the lemmatized form ofwordi.?
Usually, the e-walk features are con-structed using dependency types be-tween {governor of X, node X} and{node X, dependent of X}.
However,we also extract e-walk features fromthe dependency types between any twodependents and their common governor(i.e.
{node X, dependent 1 of X} and{node X, dependent 2 of X}).Apart from the above types of features, we alsoadd features for lemmas of the immediate preced-ing and following words of the candidate entities.These feature names are augmented with -1 or +1depending on whether the corresponding wordsare preceded or followed by a candidate entity.4.1.3 Regular expression patternsWe use a set of 22 regex patterns as binaryfeatures.
These patterns were previously usedby Ono et al(2001) and Bui et al(2010).If there is a match for a pattern (e.g.
?En-tity 1.*activates.
*Entity 2?
where Entity 1 andEntity 2 form the candidate entity pair) in a givensentence, value 1 is added for the feature (i.e., pat-tern) inside the feature vector.4.2 Shallow Linguistic (SL) KernelThe Shallow Linguistic (SL) kernel was proposedby Giuliano et al(2006).
It is one of the bestperforming kernels applied on different biomedi-cal RE tasks such as PPI and DDI (drug-drug in-teraction) extraction (Tikk et al 2010; Segura-Bedmar et al 2011; Chowdhury and Lavelli,2011b; Chowdhury et al 2011c).
It is definedas follows:KSL (R1, R2) = KLC (R1, R2) + KGC(R1, R2)424BioInfer AIMed IEPA HPRD50 LLLPos.
/ Neg.
2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166P R F P R F P R F P R F P R FSL kernel ?
?
?
60.9 57.2 59.0 ?
?
?
?
?
?
?
?
?
(Giuliano et al 2006)APG kernel 56.7 67.2 61.3 52.9 61.8 56.4 69.6 82.7 75.1 64.3 65.8 63.4 72.5 87.2 76.8(Airola et al 2008)Hybrid kernel and 65.7 71.1 68.1 55.0 68.8 60.8 67.5 78.6 71.7 68.5 76.1 70.9 77.6 86.0 80.1multiple parser input(Miwa et al 2009a)SVM-CW, multiple ?
?
67.6 ?
?
64.2 ?
?
74.4 ?
?
69.7 ?
?
80.5parser input and graph,walk and BOW features(Miwa et al 2009b)kBSPS kernel 49.9 61.8 55.1 50.1 41.4 44.6 58.8 89.7 70.5 62.2 87.1 71.0 69.3 93.2 78.1(Tikk et al 2010)Walk weighted 61.8 54.2 57.6 61.4 53.3 56.6 73.8 71.8 72.9 66.7 69.2 67.8 76.9 91.2 82.4subsequence kernel(Kim et al 2010)2 phase extraction 61.7 57.5 60.0 55.3 68.5 61.2 ?
?
?
?
?
?
?
?
?
(Bui et al 2010)Our proposed hybrid 80.0 71.4 75.5 64.2 58.2 61.1 81.1 69.3 74.7 72.9 59.5 65.5 70.4 95.7 81.1kernel (PET + SL +TPWF without regex)Table 4: Comparison of the results on the 5 benchmark PPI corpora.
Pos.
and Neg.
refer to number positive andnegative relations respectively.
The underlined numbers indicate the best results for the corresponding corpusreported by any of the existing state-of-the-art approaches.
The results of Bui et al(2010) on LLL, HPRD50,and IEPA are not reported since thy did not use all the positive and negative examples during cross validation.Miwa et al(2009b) showed that better results can be obtained using multiple corpora for training.
However,we consider only those results of their experiments where they used single training corpus as it is the standardevaluation approach adopted by all the other studies on PPI extraction for comparing results.
All the results ofthe previous approaches reported in this table are directly quoted from their respective original papers.where KSL, KGC and KLC correspond to SL,global context (GC) and local context (LC) ker-nels respectively.
The GC kernel exploits contex-tual information of the words occurring before,between and after the pair of entities (to be in-vestigated for RE) in the corresponding sentence;while the LC kernel exploits contextual informa-tion surrounding individual entities.4.3 Path-enclosed tree (PET) KernelThe path-enclosed tree (PET) kernel3 was firstproposed by Moschitti (2004) for semantic rolelabeling.
It was later successfully adapted byZhang et al(2005) and other works for relationextraction on general texts (such as newspaper do-3Also known as shortest path-enclosed tree (SPT) kernel.main).
A PET is the smallest common subtree of aphrase structure tree that includes the two entitiesinvolved in a relation.A tree kernel calculates the similarity betweentwo input trees by counting the number of com-mon sub-structures.
Different techniques havebeen proposed to measure such similarity.
We usethe Unlexicalized Partial Tree (uPT) kernel (Sev-eryn and Moschitti, 2010) for the computation ofthe PET kernel since a comparative evaluation byChowdhury et al(2011a) reported that uPT ker-nels achieve better results for PPI extraction thanthe other techniques used for tree kernel compu-tation.4255 Experimental SettingsWe have followed the same criteria commonlyused for the PPI extraction tasks, i.e.
abstract-wise 10-fold cross validation on individual corpusand one-answer-per-occurrence criterion.
In fact,we have used exactly the same (abstract-wise)fold splitting of the 5 benchmark (converted) cor-pora used by Tikk et al(2010) for benchmarkingvarious kernel methods4.The Charniak-Johnson reranking parser (Char-niak and Johnson, 2005), along with a self-trainedbiomedical parsing model (McClosky, 2010), hasbeen used for tokenization, POS-tagging andparsing of the sentences.
Before parsing the sen-tences, all the entities are blinded by assigningnames as EntityX where X is the entity index.In each example, the POS tags of the two can-didate entities are changed to EntityX .
Theparse trees produced by the Charniak-Johnsonreranking parser are then processed by the Stan-ford parser5 (Klein and Manning, 2003) to obtainsyntactic dependencies according to the StanfordTyped Dependency format.The Stanford parser often skips some syntacticdependencies in output.
We use the following tworules to add some of such dependencies:?
If there is a ?conj and?
or ?conj or?
depen-dency between two words X and Y, then Xshould be dependent on any word Z on whichY is dependent and vice versa.?
If there are two verbs X and Y such that in-side the corresponding sentence they haveonly the word ?and?
or ?or?
between them,then any word Z dependent on X should bealso dependent on Y and vice versa.Our system exploits SVM-LIGHT-TK6 (Mos-chitti, 2006; Joachims, 1999).
We made minorchanges in the toolkit to compute the proposedhybrid kernel.
The ratio of negative and positiveexamples has been used as the value of the cost-ratio-factor parameter.
We have done parametertuning following the approach described by Hsuet al(2003).4Downloaded from http://informatik.hu-berlin.de/forschung /gebiete/wbi/ppi-benchmark .5http://nlp.stanford.edu/software/lex-parser.shtml6http://disi.unitn.it/moschitti/Tree-Kernel.htm6 Results and DiscussionTo measure the contribution of the features col-lected from the reduced graphs (using dependencypatterns, trigger words and negative cues) andregex patterns, we have applied the new TPWFkernel on the 5 PPI corpora before and after usingthese features.
Results shown in Table 2 clearlyindicate that usage of these features improve theperformance.
The improvement of performanceis primarily due to the usage of dependency pat-terns which resulted in higher precision for all thecorpora.We have tried to measure the contribution ofthe regex patterns.
However, from the empiricalresults a clear trend does not emerge (see Table2).Table 3 shows a comparison among the re-sults of the proposed hybrid kernel and its indi-vidual components.
As we can see, the overallresults of the hybrid kernel (with and without us-ing regex pattern features) are better than thoseby any of its individual component kernels.
Inter-estingly, precision achieved on the 4 benchmarkcorpora (other than the smallest corpus LLL) ismuch higher for the hybrid kernel than for the in-dividual components.
This strongly indicates thatthese different types of information (i.e.
depen-dency patterns, regex patterns, triggers, negativecues, syntactic dependencies among words andconstituent parse trees) and their different repre-sentations (i.e.
flat features, tree structures andgraphs) can complement each other to learn moreaccurate models.Table 4 shows a comparison of the PPI extrac-tion results of our proposed hybrid kernel withthose of other state-of-the-art approaches.
Sincethe contribution of regex patterns in the perfor-mance of the hybrid kernel was not relevant (asTables 2 and 3 show), we used the results of pro-posed hybrid kernel without regex for the compar-ison.
As we can see, the proposed kernel achievessignificantly higher results on the BioInfer corpus,the largest benchmark PPI corpus (2,534 positivePPI pair annotations) available, than any of theexisting approaches.
Moreover, the results of theproposed hybrid kernel are on par with the state-of-the-art results on the other smaller corpora.Furthermore, empirical results show that theproposed hybrid kernel attains considerablyhigher precision than the existing approaches.426Since a dependency pattern, by construction,contains all the syntactic dependencies inside thecorresponding reduced graph, it may happen thatsome of the dependencies (e.g.
det or determiner)are not informative for classifying the label of thecorresponding class label (i.e., positive or nega-tive relation) of the pattern.
Their presence in-side a pattern might make it unnecessarily rigidand less general.
So, we tried to identify and dis-card such non informative dependencies by mea-suring probabilities of the dependencies with re-spect to the class label and then removing any ofthem which has probability lower than a threshold(we tried with different threshold values).
But do-ing so decreased the performance.
This suggeststhat the syntactic dependencies of a dependencypattern are not independent of each other even ifsome of them might have low probability (withrespect to the class label) individually.
We plan tofurther investigate whether there could be differ-ent criteria for identifying non informative depen-dencies.
For the work reported in this paper, weused the dependency patterns as they are initiallyconstructed.We also did experiments to see whether collect-ing features for trigger words from the whole re-duced graph would help.
But that also decreasedperformance.
This suggests that trigger words aremore likely to appear in the least common gover-nors.7 ConclusionIn this paper, we have proposed a new hybridkernel for RE that combines two vector basedkernels and a tree kernel.
The proposed kerneloutperforms any of the exiting approaches by awide margin on the BioInfer corpus, the largestPPI benchmark corpus available.
On the otherfour smaller benchmark corpora, it performs ei-ther better or almost as good as the existing state-of-the art approaches.We have also proposed a novel feature basedkernel, called TPWF kernel, using (automaticallycollected) dependency patterns, trigger words,negative cues, walk features and regular expres-sion patterns.
The TPWF kernel is used as a com-ponent of the new hybrid kernel.Empirical results show that the proposed hy-brid kernel achieves considerably higher precisionthan the existing approaches, which indicates itscapability of learning more accurate models.
Thisalso demonstrates that the different types of infor-mation that we use are able to complement eachother for relation extraction.We believe there are at least three ways tofurther improve the proposed approach.
Firstof all, the 22 regular expression patterns (col-lected from Ono et al(2001) and Bui et al(2010)) are applied at the level of the sen-tences and this sometimes produces unwantedmatches.
For example, consider the sentence?X activates Y and inhibits Z?
where X, Y,and Z are entities.
The pattern ?Entity1.
?activates.
?Entity2?
matches both the X?Y andX?Z pairs in the sentence.
But only the X?Y pairshould be considered.
So, the patterns shouldbe constrained to reduce the number of unwantedmatches.
For example, they could be applied onsmaller linguistic units than full sentences.
Sec-ondly, different techniques could be used to iden-tify less-informative syntactic dependencies in-side dependency patterns to make them more ac-curate and effective.
Thirdly, usage of automati-cally collected paraphrases of regular expressionpatterns instead of the patterns directly could bealso helpful.
Weakly supervised collection ofparaphrases for RE has been already investigated(e.g.
Romano et al(2006)) and, hence, can betried for improving the TPWF kernel (which is acomponent of the proposed hybrid kernel).AcknowledgmentsThis work was carried out in the context of the project?eOnco - Pervasive knowledge and data managementin cancer care?.
The authors are grateful to Alessan-dro Moschitti for his help in the use of SVM-LIGHT-TK.
We also thank the anonymous reviewers for help-ful suggestions.ReferencesAntti Airola, Sampo Pyysalo, Jari Bjorne, TapioPahikkala, Filip Ginter, and Tapio Salakoski.
2008.All-paths graph kernel for protein-protein inter-action extraction with evaluation of cross-corpuslearning.
BMC Bioinformatics, 9(Suppl 11):S2.Quoc-Chinh Bui, Sophia Katrenko, and Peter M.A.Sloot.
2010.
A hybrid approach to extract protein-protein interactions.
Bioinformatics.Razvan Bunescu and Raymond J. Mooney.
2006.Subsequence kernels for relation extraction.
In Pro-ceedings of NIPS 2006, pages 171?178.427Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed-ward M. Marcotte, Raymond J. Mooney, Arun Ku-mar Ramani, and Yuk Wah Wong.
2005.
Compara-tive experiments on learning information extractorsfor proteins and their interactions.
Artificial Intelli-gence in Medicine, 33(2):139?155.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of ACL 2005.Md.
Faisal Mahbub Chowdhury and Alberto Lavelli.2011b.
Drug-drug interaction extraction using com-posite kernels.
In Proceedings of DDIExtrac-tion2011: First Challenge Task: Drug-Drug In-teraction Extraction, pages 27?33, Huelva, Spain,September.Md.
Faisal Mahbub Chowdhury, Alberto Lavelli, andAlessandro Moschitti.
2011a.
A study on de-pendency tree kernels for automatic extraction ofprotein-protein interaction.
In Proceedings ofBioNLP 2011 Workshop, pages 124?133, Portland,Oregon, USA, June.Md.
Faisal Mahbub Chowdhury, Asma Ben Abacha,Alberto Lavelli, and Pierre Zweigenbaum.
2011c.Two dierent machine learning techniques for drug-drug interaction extraction.
In Proceedings ofDDIExtraction2011: First Challenge Task: Drug-Drug Interaction Extraction, pages 19?26, Huelva,Spain, September.J.
Ding, D. Berleant, D. Nettleton, and E. Wurtele.2002.
Mining MEDLINE: abstracts, sentences, orphrases?
Pacific Symposium on Biocomputing,pages 326?337.Katrin Fundel, Robert Ku?ffner, and Ralf Zimmer.2007.
Relex?relation extraction using dependencyparse trees.
Bioinformatics, 23(3):365?371.Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-mano.
2006.
Exploiting shallow linguistic infor-mation for relation extraction from biomedical lit-erature.
In Proceedings of EACL 2006, pages 401?408.CW Hsu, CC Chang, and CJ Lin, 2003.
A practicalguide to support vector classification.
Departmentof Computer Science and Information Engineering,National Taiwan University, Taipei, Taiwan.Thorsten Joachims.
1999.
Making large-scale sup-port vector machine learning practical.
In Advancesin kernel methods: support vector learning, pages169?184.
MIT Press, Cambridge, MA, USA.Seonho Kim, Juntae Yoon, Jihoon Yang, and SeogPark.
2010.
Walk-weighted subsequence kernelsfor protein-protein interaction extraction.
BMCBioinformatics, 11(1).Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of ACL2003, pages 423?430, Sapporo, Japan.David McClosky.
2010.
Any Domain Parsing: Au-tomatic Domain Adaptation for Natural LanguageParsing.
Ph.D. thesis, Department of ComputerScience, Brown University.Makoto Miwa, Rune S?tre, Yusuke Miyao, andJun?ichi Tsujii.
2009a.
Protein-protein interac-tion extraction by leveraging multiple kernels andparsers.
International Journal of Medical Informat-ics, 78.Makoto Miwa, Rune S?tre, Yusuke Miyao, andJun?ichi Tsujii.
2009b.
A rich feature vector forprotein-protein interaction extraction from multiplecorpora.
In Proceedings of EMNLP 2009, pages121?130, Singapore.Alessandro Moschitti.
2004.
A study on convolutionkernels for shallow semantic parsing.
In Proceed-ings of ACL 2004, Barcelona, Spain.Alessandro Moschitti.
2006.
Making Tree KernelsPractical for Natural Language Learning.
In Pro-ceedings of EACL 2006, Trento, Italy.Claire Ne?dellec.
2005.
Learning language in logic -genic interaction extraction challenge.
Proceedingsof the ICML 2005 workshop: Learning Language inLogic (LLL05), pages 31?37.Toshihide Ono, Haretsugu Hishigaki, Akira Tanigami,and Toshihisa Takagi.
2001.
Automated ex-traction of information on protein?protein interac-tions from the biological literature.
Bioinformatics,17(2):155?161.Sampo Pyysalo, Filip Ginter, Juho Heimonen, JariBjo?rne, Jorma Boberg, Jouni Jarvinen, and TapioSalakoski.
2007.
Bioinfer: a corpus for informationextraction in the biomedical domain.
BMC Bioin-formatics, 8(1):50.Sampo Pyysalo, Antti Airola, Juho Heimonen, JariBjo?rne, Filip Ginter, and Tapio Salakoski.
2008.Comparative analysis of five protein-protein in-teraction corpora.
BMC Bioinformatics, 9(Suppl3):S6.Lorenza Romano, Milen Kouylekov, Idan Szpektor,Ido Dagan, and Alberto Lavelli.
2006.
Investi-gating a generic paraphrase?based approach for re-lation extraction.
In Proceedings of EACL 2006,pages 409?416.Isabel Segura-Bedmar, Paloma Mart?
?nez, and Cesar dePablo-Sa?nchez.
2011.
Using a shallow linguistickernel for drug-drug interaction extraction.
Jour-nal of Biomedical Informatics, In Press, CorrectedProof, Available online, 24 April.Aliaksei Severyn and Alessandro Moschitti.
2010.Fast cutting plane training for structural kernels.
InProceedings of ECML-PKDD 2010.Domonkos Tikk, Philippe Thomas, Peter Palaga,Jo?rg Hakenberg, and Ulf Leser.
2010.
A Compre-hensive Benchmark of Kernel Methods to ExtractProtein-Protein Interactions from Literature.
PLoSComputational Biology, 6(7), July.Min Zhang, Jian Su, Danmei Wang, Guodong Zhou,and Chew Lim Tan.
2005.
Discovering relations428between named entities from a large raw corpus us-ing tree similarity-based clustering.
In Natural Lan-guage Processing ?
IJCNLP 2005, volume 3651 ofLecture Notes in Computer Science, pages 378?389.Springer Berlin / Heidelberg.429
