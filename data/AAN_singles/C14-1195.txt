Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 2064?2074, Dublin, Ireland, August 23-29 2014.Effective Incorporation of Source Syntax intoHierarchical Phrase-based TranslationTong Xiao?
?, Adri`a de Gispert?, Jingbo Zhu?
?, Bill Byrne??
Northeastern University, Shenyang 110819, China?
Hangzhou YaTuo Company, Hangzhou 310012, China?
University of Cambridge, CB2 1PZ Cambridge, U.K.{xiaotong,zhujingbo}@mail.neu.edu.cn{ad465,wjb31}@eng.cam.ac.ukAbstractIn this paper we explicitly consider source language syntactic information in both rule extractionand decoding for hierarchical phrase-based translation.
We obtain tree-to-string rules by theGHKM method and use them to complement Hiero-style rules.
All these rules are then employedto decode new sentences with source language parse trees.
We experiment with our approach ina state-of-the-art Chinese-English system and demonstrate +1.2 and +0.8 BLEU improvementson the NIST newswire and web evaluation data of MT08 and MT12.1 IntroductionSynchronous context free grammars (SCFGs) are widely used in statistical machine translation (SMT),with hierarchical phrase-based translation (Chiang, 2005) as the dominant approach.
Hiero grammarsare easily extracted from word-aligned parallel corpora and can capture complex nested translation re-lationships.
Hiero grammars are formally syntactic, but rules are not constrained by source or targetlanguage syntax.
This lack of constraint can lead to intractable decoding and bad performance due tothe over-generation of derivations in translation.
To avoid these problems, the extraction and applicationof SCFG rules is typically constrained by a source language span limit; (non-glue) rules are lexicalised;and rules are limited to two non-terminals which are not allowed to be adjacent in the source language.These constraints can yield good performing translation systems, although at a sacrifice in the ability tomodel long-distance movement and complex reordering of multiple constituents.By contrast, the GHKM approach to translation (Galley et al., 2006) relies on a syntactic parse oneither the source or target language side to guide SCFG extraction and translation.
The parse tree provideslinguistically-motivated constraints both in grammar extraction and in translation.
This allows for looserspan constraints; rules need not be lexicalised; and rules can have more than two non-terminals to modelcomplex reordering multiple constituents.
There are also modelling benefits as more meaningful featurescan be used to encourage derivations with ?well-formed?
syntactic tree structures.
However, GHKM canhave robustness problems in that translation relies on the quality of the parse tree and the diversity ofrule types can lead to sparsity and limited coverage.In this paper we describe a simple but effective approach to introducing source language syntax intohierarchical phrase-based translation to get the benefits of both approaches.
Unlike previous work, wedo not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-stylerule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal,2006; Zhao and Al-Onaizan, 2008; Chiang, 2010).
We instead use GHKM syntactic rules to augment thebaseline Hiero grammar and decoder.
Our approach uses GHKM rules if possible and Hiero rules if not.We report performance on a state-of-the-art Chinese-English system.
In a large-scale NIST evaluationtask, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline onthe newswire and web evaluation data of MT08 and MT12.
We also investigate variations in the GHKMformalism and find, for example, that our approach works well with binarized trees.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/2064IPNPPN?VPPPP?NPNN??VPVV??NN?
?hewassatisfied with theanswerHiero-style SCFG Rulesh1X?
?
?, he?h2X?
?
?, with?h3X?
??
?, the answer?h4X?
????
?, was satisfied?h5X?
?X1???
?, was satisfied X1?h6X?
?X1??
X2, was X2X1?h7X?
?X1?
X2???
?,X1was satisfied with X2?Tree-to-String Rulesr1NP(PN(?))?
her2P(?)?
withr3NP(NN(??))?
the answerr4VP(VV(??)
NN(??))?
was satisfiedr5PP(x1:P x2:NP)?
x1x2r6VP(x1:PP x2:VP)?
x2x1r7IP(x1:NP x2:VP)?
x1x2r8VP(PP(P(?)
x1:NP) x2:VP)?
x2with x1Figure 1: Hiero-syle and tree-to-string rules extracted from a pair of word-aligned Chinese-Englishsentences with a source language (Chinese) parse tree.2 Background2.1 Hierarchical Phrase-based TranslationIn the hierarchical phrase-based approach, translation is modelled using SCFGs.
In general, probabilisticSCFGs can be learned from word-aligned parallel data using heuristic methods (Chiang, 2007).
We canfirst extract initial phrase pairs and then obtain hierarchical phrase rules (i.e., rules with non-terminalson the right hand side).
Once the SCFG is obtained, new sentences can be decoded by finding the mostlikely derivation of SCFG rules.
See Figure 1 for example rules extracted from a sentence pair with wordalignments.
A sequence of such rules covering the words of the source sentence is a SCFG derivation,e.g., rules h7, h1and h3generate a derivation for the sentence pair.The Hiero SCFG allows vast numbers of derivations which can make unconstrained decoding in-tractable.
In practice, several constraints are applied to control the model size and reduce ambiguity.Typically these are: (a) a rule span limit to be applied in decoding and sometimes also in rule extraction,set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), setto 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except theglue rules).2.2 Tree-to-String TranslationInstead of modelling the problem based on surface strings, tree-to-string systems model the translationequivalency relations from source language syntactic trees to target language strings using derivationsof tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012).
Atree-to-string rule is a tuple ?sr, tr,?
?, where sris a source language tree-fragment with terminals andnon-terminals at leaves; tris a string of target-language terminals and non-terminals; and ?
is a 1-to-1alignment between the non-terminals of srand tr, for example, VP(VV(??)
x1:NN)?
increases x1is a tree-to-string rule, where the non-terminals labeled with the same index x1indicate the alignment.To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al.,2006) on the bilingual sentences with both word alignment and source (or target) language phrase-structure tree annotations.
In GHKM extraction, we first compute the set of the minimally-sized transla-tion rules that can explain the mappings between source language tree and target-language string whilerespecting the alignment and reordering between the two languages.
More complex rules are then learnedby composing two or more minimal rules.
See Figure 1 for rules extracted using GHKM.One of the advantages of the above model is that non-terminals in tree-to-string rules are linguistically2065rulematchdecodinginputstringHieroSCFGouputstring(a) decoding with Hiero rules onlyrulematchdecodinginputstring&treelargerSCFGHieroSCFGt-to-srulesouputstring(b) decoding with Hiero and tree-to-string rulesFigure 2: Overview of the Hiero baseline (a) andour approach (b).
?means input or output of thedecoder.
t-to-s is a short for tree-to-string.VPPPP?x1:NPx2:VPx2withx1X??
?X1X2, X2withX1?tree-to-string:Hiero:Figure 3: Converting the tree-to-string rule r8from Figure 1 to a Hiero-style rule.motivated and can span word sequences with arbitrary length.
Also, one can use rules with consecutive(or more than two) source language non-terminals when the source language parse tree is available.
Forexample, r8in Figure 1 has a good Chinese syntactic structure indicating the reordered translations of NPand VP.
However, such a rule would not normally be included in a Hiero grammar, as it would requireconsecutive source language non-terminals (see Figure 3).3 The Proposed ApproachBoth the tree-to-string model and the hierarchical phrase-based model have their own strengths andweaknesses.
For example, tree-to-string systems are good at modelling long distance reordering, whilehierarchical phrase-based systems are relatively more powerful in handling ill-formed sentences1andfree translations (Zhao and Al-Onaizan, 2008; Vilar et al., 2010).
Here we present a method to enhancehierarchical phrase-based systems with tree-to-string rules and benefit from both models.
The idea issimple: we obtain both the tree-to-string grammar and the Hiero-style SCFG from the training data, andthen use tree-to-string rules as additional rules in decoding with the SCFG.Figure 2 shows an overview of our approach and the usual hierarchical phrase-based approach.
Ourapproach requires source language parse trees to be input in both rule extraction and decoding.
In ruleextraction, we acquire tree-to-string rules using the GHKM method and Hiero-style rules using the Hiero-style rule extraction method to form a larger SCFG.
Then, we make use of both the input string and parsetree to decode with the SCFG rules.
We now describe our approach.3.1 Transforming Tree-to-String Rules into SCFG RulesAs described in Section 2, tree-to-string rules have a different form from that of SCFG rules.
We will usetree-to-string rules in our hierarchical phrase-based systems by converting each tree-to-string rule into anSCFG rule.
The purpose of doing this is to make tree-to-string rules directly accessible to the Hiero-styledecoder which performs decoding with SCFG rules.The rule mapping is straightforward: given a tree-to-string rule ?sr, tr,?
?, we take the frontier nodesof sras the source language part of the right hand side of the resulting SCFG rule, and keep trand?
unchanged.
Then we replace the non-terminal label with that used in the hierarchical phrase-basedsystem (e.g., X).
See Figure 3 for rule mapping of rule r8of Figure 1.In this way, every tree-to-string rule is associated with exactly one SCFG rule.
Therefore we canobtain a larger SCFG by combining the rules from the original Hiero-style SCFG and the transformedtree-to-string rules.
As explained next, to prevent computational problems we will apply these new rules1For example, the parser fails for 4% of the sentences in our training corpus, and 3% and 6% of the newswire and webdevelopment/test sentences, indicating that the data is sometimes ill-formed.2066only on the spans that are consistent with the input parse trees.
The main goal is to use the tree and theadapted tree-to-string rules to provide the decoder with new linguistically-sensible translation hypothesesthat may be prevented by the usual Hiero constraints, and to do so without incurring a computationalexplosion.We categorize SCFG rules into two categories based on their availability in Hiero and GHKM extrac-tion.
If an SCFG rule is obtained from Hiero extraction, it is a type 1 rule; If not (i.e., this rule is onlyavailable in GHKM extraction), it is a type 2 rule.
E.g., the SCFG rule in Figure 3 is a type 2 rule becauseit is not available in the original Hiero-style SCFG but can be generated from the tree-to-string rule.Next we describe how each of these rule types are applied in decoding.
We also describe whichfeatures are used and how they are computed for each rule type.3.2 DecodingBoth types of SCFG rules can be employed by usual Hiero decoders with a slight modification.
Herewe follow the description of Hiero decoding by Iglesias et al.
(2011).
The source sentence is parsedunder the Hiero grammar using the CYK algorithm.
Each cell in the CYK grid has associated with it alist of rules that apply to its span; these rules are used to construct a recursive transition network (RTN)which represents all translations of the source sentence under the grammar.
The RTN is expanded to aweighted finite state automaton for composition with n-gram language models (de Gispert et al., 2010).Translations are produced via shortest path computation.This procedure accommodates type 1 rules directly.
For tree-to-string rules associated with type 2, weattempt to match rules to the source syntactic tree.
If a match is found: the source span of the matchingtree fragment is noted and the CYK cell for that span is selected; the tree-to-string rule is converted toa Hiero-style rule; and that rule is added to the list of rules in the selected CYK cell.
Once this processis finished, RTN construction, expansion, and language model composition proceeds as usual.
Similarmodifications could be made to incorporate these rules into cube pruning (Chiang, 2007), cube growing(Huang and Chiang, 2007), and PDT intersection and expansion (Iglesias et al., 2011).
We now elaborateon the rule matching strategy.Type 1 Rules The source sentence is parsed as is usual in Hiero-style translation, with the exceptionthat we impose no span limit on rule applications for source spans corresponding to constituents in theChinese syntactic tree.
Rule matching, the procedure that determines if a rule applies to a source span, isbased on string matching (see Figure 4(a)).
For example, the type 1 rule h9in Figure 4(c) can be appliedto spans (1,13) and (2,13) since both of them agree with tree constituents (see Figure 4(b)).
But h9isnot applied to span (3,13) because that span is longer than 10 words and agrees with no syntactic treeconstituent.Type 2 Rules If the source side of a tree-to-string rule matches an input tree fragment: 1) that ruleis converted to a Hiero-style SCFG rule (Section 3.1); and 2) the Hiero-style rule is added to the ruleslinked with the CYK grid cell associated with the span of the source syntactic tree fragment.
Here, rulesare applied via tree matching.
For example, rule h11in Figure 4(b) matches the tree fragment spanningpositions (2,13).It is worth noting that some type 1 rules may be found via both Hiero-style and tree-to-string grammarextraction.
In this case we monitor whether a rule can be applied as a tree-to-string rule using tree-matching so that features (Section 3.3) and weights can be set appropriately.
As an example, rule h10inFigure 4 is available in both extraction methods.
For span (2,11), this rule can be matched via both stringmatching and tree matching.
We then note that we can apply h10as a tree-to-string rule for span (2, 11)and activate the corresponding features defined in Section 3.3.
For other spans (e.g., spans (2,3)-(2,10)),no tree fragments can be matched and the baseline features are used for h10.3.3 FeaturesThe baseline feature set used in this work consists of 12 features (Pino et al., 2013), including a 4-gramlanguage model, a strong 5-gram language model, bidirectional translation probabilities, bidirectionallexical weights, a word count, a phrase count, a glue rule count, a frequency-1 rule count, a frequency-22067h9: X?
?X1??
, satisfied with X1 ????1?2??3?4?5?6?7??8?9??10??11??12??13.
.
..........Chart Used in Decodingspan(10,13)matching(a) matching a type 1 rule (h9) with the input stringIPNPNR???1VPPPP?2NP??3?4?5?6?7??8?9??10??11VPVV??12NN??13VP(PP(P(?)
x1:NP) x2:VP)?
x2with x1h11: X?
??
X1X2,X2with X1?converting.
.
..........Chart Used in Decodingmatchingspan(2,13)(b) matching a type 2 rule (h11) with the input parse treeID Type Hiero-style Rule Tree-to-string Rule Applicable Spansh8type 1 X?
????
?, is satisfied ?
N/A (12,13)h9type 1 X?
?
X1?
?, satisfied with X1?
N/A (i,13), i = 1, 2 or 4 ?
i ?
12h10type 1 X?
??
X1, with X1?
PP(P(?)
x1NP)?
with NPx1(2,j), 3 ?
j ?
11 or j = 13h11type 2 X?
??
X1X2, X2with X1?
VP(PP(P(?)
x1:NP) x2:VP) (2,13)?
x2with x1(c) example rules used in decodingFigure 4: Decoding with both Hiero-style and tree-to-string grammars (span limit = 10).
A span (i,j)means spanning from position i to position j.rule count, and a larger-than-frequency-2 rule count2.
In addition, we introduce several features forapplying tree-to-string rules.?
Rule type indicators.
We consider four indicator features, indicating tree-to-string rules, lexicalizedtree-to-string rules, rules with consecutive non-terminals, and non-lexicalized rules.
Note that the tree-to-string rule indicator feature is in principle a generalization of the soft syntactic features (Marton andResnik, 2008), in that a bonus (or penalty) is applied when a rule application is consistent with a sourcetree constituent.
The difference lies in that the tree-to-string rule indicator feature does not distinguishbetween different syntactic labels, whereas soft syntactic features do.?
Features in syntactic MT.
In general tree-to-string rules have their own features which are differentfrom those used in Hiero-style systems.
For example, the features in syntactic MT systems can bedefined as the generation probabilities conditioned on the root symbol of the tree-fragment.
Here wechoose five popular features used in syntactic MT systems, including the bi-directional phrase-basedconditional translation probabilities (Marcu et al., 2006) and three syntax-based conditional probabil-ities (Mi and Huang, 2008).
All these probabilities can be computed by relative-frequency estimates.For example, the phrase-based features are the probabilities of translating between the frontier nodesof srand tr.
The syntax-based features are the probabilities of generating r conditioned on its root,2We experimented with soft syntactic features (Marton and Resnik, 2008) but found no improvement over our baselinesystem.2068source and target language sides, respectively.
More formally, we use the following estimates for theseprobabilities:Pphr(tr| sr) =?r??:?(sr??)=?(sr)?tr??=trc(r??)?r?:?(sr?)=?(sr)c(r?
)Pphr(sr| tr) =?r??:?(sr??)=?(sr)?tr??=trc(r??)?r?:tr?=trc(r?
)P(r | root(r)) =c(r)?r?:root(r?)=root(r)c(r?
)P(r | sr) =c(r)?r?:sr?=src(r?
)P(r | tr) =c(r)?r?:tr?=trc(r?
)where c(r) is the count of r, and root(?)
and ?(?)
are functions that return the source root symbol fora tree-to-string rule and the sequence of leaf nodes for a tree-fragment respectively.4 Evaluation4.1 Experimental SetupWe report results in the NIST MT12 Chinese-English task, where our baseline system was among the topacademic systems.
The parallel training corpus consists of 9.2 million sentence pairs which are providedwithin the NIST Chinese-English MT12 track.
Word alignments are obtained using MTTK (Deng andByrne, 2008) in both Chinese-to-English and English-to-Chinese directions, and then unioning the links.The data from newswire and web genres was used for tuning and test.
The development sets contain1,755 sentences and 2160 sentences for the two genres respectively.
The test sets (newswire: 1,779sentences, web: 1768 sentences) contain all newswire and web evaluation data of MT08 (mt08), MT12(mt12), and MT08 progress test (mt08.p).
All Chinese sentences in the training, development and testsets were parsed using the Berkeley parser (Petrov and Klein, 2007).
A Kneser-Ney 4-gram languagemodel was trained on the AFP and Xinhua portions of the English Gigaword in addition to the Englishside of the parallel corpus.
A stronger 5-gram language model was trained on all English data of NISTMT12 and the Google counts corpus using the ?stupid?
backoff method (Brants et al., 2007).For decoding we use HiFST, which is implemented with weighted finite state transducers (de Gispertet al., 2010).
A two-pass decoding strategy is adopted; first, only the 4-gram language model and thetranslation model are activated; and then, the 5-gram language model is applied for second-pass rescoringof the translation lattices generated by the first-pass decoding stage.
We extracted SCFG rules fromthe parallel corpus using the standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al.,2009).
The span limit was set to 10 in extracting basic phrases and decoding.
All features weights wereoptimized using lattice-based minimum error rate training (Macherey et al., 2008).For tree-to-string extraction, we used a reimplementation of the GHKM method (Xiao et al., 2012) andextracted rules from a 600K-sentence portion of the parallel data.
To prune the tree-to-string rule set, werestricted the extraction to rules with at most 5 frontier non-terminals and 5 terminals.
Also, we discardedlexicalized rules with a Chinese-to-English translation probability of < 0.02 and non-lexicalized ruleswith a Chinese-to-English translation probability of < 0.10.4.2 ResultsWe report MT performance in Table 1 by case-insensitive BLEU (Papineni et al., 2002).
The experimentsare organized as follows:?
Baseline and Span Limits (exp01 and exp02)First we study the effect of removing the span limit for tree constituents, that is, SCFG rules can be2069Entry System Newswire Webtune mt08 mt12 mt08.p all test tune mt08 mt12 mt08.p all test(1755) (691) (400) (688) (1779) (2160) (666) (420) (682) (1768)exp01 baseline 35.84 35.85 35.47 35.50 35.63* 29.98 25.15 23.07 27.19 25.33*exp02 += no span limit 36.05 36.08 35.70 35.54 35.79* 30.11 25.28 23.08 27.17 25.37*exp03 += t-to-s rules 36.63 36.51 36.08 36.09 36.25* 30.80 26.00 23.08 27.80 25.83*exp04 += t-to-s features 36.82 36.49 36.53 36.16 36.38* 30.91 26.03 23.27 27.85 25.98*exp05 t-to-s baseline 34.63 34.44 34.87 33.66 34.25* 28.30 23.40 21.38 25.30 23.56*exp06 exp04 on spans > 10 36.17 36.11 35.71 35.86 35.92* 30.18 25.30 23.12 27.36 25.45*exp07 exp04 with null trans.
36.10 36.03 35.35 34.86 35.42* 29.96 25.32 22.58 23.33 24.12*exp08 exp04 + left binariz.
37.11 37.46 37.03 36.30 36.91* 31.18 26.15 23.54 27.98 26.13*exp09 exp04 + right binariz.
36.58 36.56 36.41 35.70 36.20* 31.06 25.94 23.47 27.48 25.88*exp10 exp04 + forest binariz.
37.03 37.27 37.09 36.62 36.98* 31.20 25.99 23.59 28.09 26.15*Table 1: Case-insensitive BLEU[%] scores of various systems.
+= means incrementally adding method-s/features to the previous system.
* means that a system is significantly different than the exp01 baselineat p < 0.01.applied to any spans when they respect the tree constituents of the input tree.
It can be regarded asthe simplest way of using source syntax in Hiero-style systems.
Seen from Table 1, removing thespan limit shows modest BLEU improvements.
It agrees with the previous result that loosening theconstraints on spans is helpful to systems based on the hard syntactic constraints (Li et al., 2013).?
GHKM+Hiero (exp03 and exp04)The results of our proposed approach (w/o new features) are reported in exp03 and exp04.
We see thatincorporating tree-to-string rules yields +0.6 and +0.5 improvements on the collected newswire andweb test sets (exp03 vs exp01).
The new features (Section 3.3) give a further improvement (exp04 vsexp03).
This result confirms that the system can learn a preference for certain types of rules using thenew features.?
Impact of Search Space (exp05)We also study the impact of search space on system performance.
To do this, we force the improvedsystem (exp04) to respect source tree constituents and to discard any hypotheses which violate thetree constituent constraints.
Seen from exp05, this system has a lower BLEU score than both theHiero baseline (exp01) and GHKM+Hiero system (exp04), strongly suggesting that restricting MTsystems to a smaller space of hypotheses is harmful.?
GHKM+Hiero, Spans > 10 Only (exp06)Another interesting question is whether tree-to-string rules and features are more helpful to largerspans.
We restricted our approach to spans > 10 only and conducted another experiment.
As is shownin exp06, applying tree-to-string rules and features for large spans is beneficial (exp06 vs. exp01).
Butit underperforms the system with the full use of tree-to-string rules (exp06 vs. exp04).
This interestingobservation implies that applying tree-to-string rules on smaller spans introduces good hypotheses thatcan be selected with our additional features.?
Impact of Failed Parses (exp07)As noted in Section 3, the parser fails to parse some of the sentences in our experiments.
In this caseour approach generates the baseline result using the Hiero model (i.e., type 1 rules only).
To investigatethe effect of failed parse trees on system performance, we also report the BLEU score including nulltranslations for which the parser fails.
As shown in exp07, there are significantly lower BLEU scoreswhen null translations are included.
It indicates that our approach is more robust than standard tree-to-string systems which would generate an empty translation if the source language parser fails.?
Results on Binarization (exp08-10)Tree binarization is a widely used method to improve syntactic MT systems (Wang et al., 2010).exp08-10 show the results of our improved system with left-heavy, right-heavy and forest-based bina-2070Reference: After North Korea demanded concessions from U.S. again before the start of a new round of six-nation talks , ...Baseline: In the new round of six-nation talks on North Korea again demanded that U.S. in the former promise concessions , ...GHKM+Hiero: AfterNorth Korea again demanded that U.S. promised concessions before the new round of six-nation talks, ...a Hiero rule X?
??
X1?, after X1?
is applied on span (1,15)Input:IPPPP?1LCPIP??2??3??4??5?6?7??8?9?10??11?12??13?
?14LC?15PU,VP...Reference: The Chinese star performance troupe presented a wonderful Peking opera as well as singing and dancingReference: performance to Hong Kong audience .Baseline: Star troupe of China, highlights of Peking opera and dance show to the audience of Hong Kong .GHKM+Hiero: Chinese star troupe presented a wonderful Peking opera singing and dancingtoHong Kong audience.Input:A tree-to-string rule is applied:(VP BA(?)
x1:NP x2:VP PP(P(?)
x3:NP))?
x2x1to x3IPNP??1??2???3VPBA?4NP?5?6??7?8??9??10VPVV??11PPP?12NP??13?
?14.Figure 5: Comparison of translations generated by the baseline and improved systems.rization3.
We see that left-heavy binarization is very helpful and exp08 achieves overall improvementsof 1.2 and 0.8 BLEU points on the newsire and web data.
In contrast, right-heavy binarization doesnot yield promising performance.
This agrees with the previous report (Wang et al., 2010) that MTsystems prefer to use certain ways of binarization in most cases.
exp10 shows that the additional treesintroduced in our forest-based scheme are not sufficient to make a big impact on BLEU scores.
Pos-sibly larger gains can be obtained if taking a forest of parse trees from the source parser, but this isoutside the scope of this paper.4.3 AnalysisWe then analyse rule usage in the 1-best derivations for our improved system on the tuning set.
We findthat type 2 rules represent 13.97% of the rules used in the 1-best derivations.
Also, 44.45% of the appliedrules are available from the tree-to-string model (i.e., rules that use the features described in Section 3.3).These numbers indicate that the tree-to-string rules are beneficial and our decoder likes to use them.Finally, we discuss two real translation examples from our tuning set.
See Figure 5 for translationsgenerated by different systems.
In the first example, the Chinese input sentence contains?
...?
whichis usually translated into after ...
(i.e., a Hiero rule X?
??
X1?, after X1?).
However, because the??
...??
pattern spans 15 words and that is beyond the span limit, our baseline is unable to apply thisdesired rule and chooses a wrong translation in for the Chinese word ?.
When the source parse tree3We found that the CTB-style parse trees usually have a very flat top-level IP (i.e., single clause) tree structure.
As the IPstructure in Chinese is very complicated, the system might prefer a flexible binarization scheme.
Thus we considered both leftand right-heavy binarization to form a binarization forest for IPs in Chinese parse trees, and binarized other tree constituents ina left-heavy fashion.2071is available, our approach removes the span limit for spans that agree with the tree constituents.
In thiscase, the MT system successfully applies the rule on span (1, 15) and generates a much better translation.In the second example, the translation of the input sentence requires complex reordering of adjacentconstituents.
The baseline system cannot handle this case and generates a monotonic translation usingthe glue rules.
This results in a wrong order for the translation of Chinese verb??
(show).
By contrast,the improved system chooses a tree-to-string rule with three non-terminals (some of which are adjacentin the source language) and perfectly performs a syntactic movement of the required tree constituents.5 Related WorkRecently linguistically-motivated models have been intensively investigated in MT.
In particular, sourcetree-based models (Liu et al., 2006; Huang et al., 2006; Eisner, 2003; Zhang et al., 2008; Liu et al.,2009a; Xie et al., 2011) have received growing interest due to their good abilities in modelling sourcelanguage syntax for better lexicon selection and reordering.
Alternatively, the hierarchical phrase-basedapproach (Chiang, 2005) considers the underlying hierarchical structures of sentences but does not re-quire linguistically syntactic trees on either language side.There are several lines of work for augmenting hierarchical phrase-based systems with the use ofsource language phrase-structure trees.
Liu et al.
(2009b) describe novel approaches to translation undermultiple translation grammars.
Their approach is very much motivated by system combination, and theydevelop procedures for joint decoding and optimisation within a single system that give the benefit ofcombining hypotheses from multiple systems.
They demonstrate their approach by combining full tree-to-string and Hiero systems.
Our approach is much simpler and emphasises changes to the grammarrather than the decoder or its parameter optimisation (MERT).
Our aim is to augment the search spaceof Hiero with linguistically-motivated hypotheses, and not to develop a new decoder that is capable oftranslation under multiple grammars.
Moreover, we consider Hiero as the backbone model and onlyintroduce tree-to-string rules where they can contribute; we show that extracting tree-to-string rules fromjust 10% of the data suffices to get good gains.
This results in a small number of tree-to-string rules anddoes not slow down the decoder.Another related line of work is to introduce syntactic constraints or annotations to hierarchical phrase-based systems.
Marton and Resnik (2008) and Li et al.
(2013) proposed several soft or hard constraints tomodel syntactic compatibility of Hiero derivations and input source language parse trees.
We note that,despite significant development effort, we were not able to improve our baseline through the use of thesesoft syntactic constraints; it was this experience that led us to develop the hybrid approach described inthis paper.Several research groups used syntactic labels as non-terminal symbols in their SCFG rules and developnew features (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010; Hoang andKoehn, 2010).
However, all these methods still resort to rule extraction procedures similar to that of thestandard phrase/hierarchical rule extraction method.
In contrast, we use the GHKM method which is amature technique to extract rules from tree-string pairs but does not impose those Hiero-style constraintson rule extraction.
More importantly, we consider the hierarchical syntactic tree structure to make use ofwell-formed rules in decoding, while such information is not used in standard SCFG-based systems.
Wealso keep to the simpler non-terminals of Hiero, and do not ?decorate?
any non-terminals with syntacticor other information.6 ConclusionWe have presented an approach to improving Hiero-style systems by augmenting the SCFG with tree-to-string rules and syntax-based features.
The input parse trees are used to introduce new linguistically-sensible hypotheses into the translation search space while maintaining the Hiero robustness qualitiesand avoiding computational explosion.
We obtain significant improvements over a strong Hiero baselinein Chinese-to-English.
Further improvements are achieved when applying tree binarization.2072AcknowledgementsThis work was done while the first author was visiting the speech group at University of Cambridge, andwas supported in part by the National Science Foundation of China (Grants 61272376 and 61300097),and the China Postdoctoral Science Foundation (Grant 2013M530131).
We would like to thank theanonymous reviewers for their pertinent and insightful comments.
We also would like to thank JuanPino, Rory Waite, Federico Flego and Gonzalo Iglesias for building parts of the baseline system.ReferencesThorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean.
2007.
Large Language Models inMachine Translation.
In Proceedings of EMNLP-CoNLL, pages 858?867, Prague, Czech Republic.David Chiang.
2005.
A Hierarchical Phrase-Based Model for Statistical Machine Translation.
In Proceedings ofACL, pages 263?270, Ann Arbor, Michigan, USA.David Chiang.
2007.
Hierarchical Phrase-Based Translation.
Computational Linguistics, 33:45?60.David Chiang.
2010.
Learning to Translate with Source and Target Syntax.
In Proceedings of ACL, pages 1443?1452, Uppsala, Sweden.Adri`a de Gispert, Gonzalo Iglesias, Graeme Blackwood, Eduardo R. Banga, and William Byrne.
2010.
Hierarchi-cal Phrase-Based Translation with Weighted Finite-State Transducers and Shallow-n Grammars.
ComputationalLinguistics, 36(3):505?533.Yonggang Deng and William Byrne.
2008.
HMM Word and Phrase Alignment for Statistical Machine Translation.IEEE Transactions on Audio, Speech & Language Processing, 16(3):494?507.Jason Eisner.
2003.
Learning Non-Isomorphic Tree Mappings for Machine Translation.
In Proceedings of ACL,pages 205?208, Sapporo, Japan.Yang Feng, Yang Liu, Qun Liu, and Trevor Cohn.
2012.
Left-to-Right Tree-to-String Decoding with Prediction.In Proceedings of EMNLP-CoNLL, pages 1191?1200, Jeju Island, Korea.Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thay-er.
2006.
Scalable Inference and Training of Context-Rich Syntactic Translation Models.
In Proceedings ofCOLING-ACL, pages 961?968, Sydney, Australia.Hieu Hoang and Philipp Koehn.
2010.
Improved translation with source syntax labels.
In Proceedings of the JointFifth Workshop on Statistical Machine Translation and MetricsMATR, pages 409?417, Uppsala, Sweden.Liang Huang and David Chiang.
2007.
Forest Rescoring: Faster Decoding with Integrated Language Models.
InProceedings of ACL, pages 144?151, Prague, Czech Republic.Liang Huang and Haitao Mi.
2010.
Efficient Incremental Decoding for Tree-to-String Translation.
In Proceedingsof EMNLP, pages 273?283, Cambridge, MA, USA.Liang Huang, Knight Kevin, and Aravind Joshi.
2006.
Statistical syntax-directed translation with extended domainof locality.
In Proceedings of AMTA, pages 66?73, Cambridge, MA, USA.Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga, and William Byrne.
2009.
Rule Filtering by Pattern forEfficient Hierarchical Translation.
In Proceedings of EACL, pages 380?388, Athens, Greece.Gonzalo Iglesias, Cyril Allauzen, William Byrne, Adri`a de Gispert, and Michael Riley.
2011.
Hierarchical Phrase-based Translation Representations.
In Proceedings of EMNLP, pages 1373?1383, Edinburgh, Scotland, UK.Junhui Li, Philip Resnik, and Hal Daum?e III.
2013.
Modeling Syntactic and Semantic Structures in HierarchicalPhrase-based Translation.
In Proceedings of NAACL-HLT, pages 540?549, Atlanta, Georgia.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-String Alignment Template for Statistical Machine Transla-tion.
In Proceedings of COLING-ACL, pages 609?616, Sydney, Australia.Yang Liu, Yajuan L?u, and Qun Liu.
2009a.
Improving Tree-to-Tree Translation with Packed Forests.
In Proceed-ings of ACL-IJCNLP, pages 558?566, Suntec, Singapore.2073Yang Liu, Haitao Mi, Yang Feng, and Qun Liu.
2009b.
Joint decoding with multiple translation models.
InProceedings of ACL-IJCNLP, pages 576?584, Suntec, Singapore.Wolfgang Macherey, Franz Och, Ignacio Thayer, and Jakob Uszkoreit.
2008.
Lattice-based Minimum Error RateTraining for Statistical Machine Translation.
In Proceedings of EMNLP, pages 725?734, Honolulu, Hawaii.Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight.
2006.
SPMT: Statistical Machine Translationwith Syntactified Target Language Phrases.
In Proceedings of EMNLP, pages 44?52, Sydney, Australia.Yuval Marton and Philip Resnik.
2008.
Soft Syntactic Constraints for Hierarchical Phrased-Based Translation.
InProceedings of ACL-HLT, pages 1003?1011, Columbus, Ohio.Haitao Mi and Liang Huang.
2008.
Forest-based Translation Rule Extraction.
In Proceedings of EMNLP, pages206?214, Honolulu, Hawaii, USA.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-Based Translation.
In Proceedings of ACL-HLT, pages192?199, Columbus, Ohio.Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu.
2002.
Bleu: a Method for Automatic Evaluationof Machine Translation.
In Proceedings of ACL, pages 311?318, Philadelphia, PA, USA.Slav Petrov and Dan Klein.
2007.
Improved inference for unlexicalized parsing.
In Proceedings of HLT-NAACL,pages 404?411, Rochester, New York, USA.Juan Pino, Aurelien Waite, Tong Xiao, Adri`a de Gispert, Federico Flego, and William Byrne.
2013.
The Universityof Cambridge Russian-English system at WMT13.
In Proceedings of WMT, pages 200?205, Sofia, Bulgaria.David Vilar, Daniel Stein, Stephan Peitz, and Hermann Ney.
2010.
If i only had a parser: poor man?s syntax forhierarchical machine translation.
In Proceedings of IWSLT, pages 345?352.Wei Wang, Jonathan May, Kevin Knight, and Daniel Marcu.
2010.
Re-structuring, Re-labeling, and Re-aligningfor Syntax-Based Machine Translation.
Computational Linguistics, 36(2):247?277.Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li.
2012.
NiuTrans: An Open Source Toolkit for Phrase-basedand Syntax-based Machine Translation.
In Proceedings of ACL: System Demonstrations, pages 19?24, JejuIsland, Korea.Jun Xie, Haitao Mi, and Qun Liu.
2011.
A novel dependency-to-string model for statistical machine translation.In Proceedings of EMNLP, pages 216?226, Edinburgh, Scotland.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li.
2008.
A Tree SequenceAlignment-based Tree-to-Tree Translation Model.
In Proceedings of ACL-HLT, pages 559?567, Columbus,Ohio, USA.Bing Zhao and Yaser Al-Onaizan.
2008.
Generalizing Local and Non-Local Word-Reordering Patterns for Syntax-Based Machine Translation.
In Proceedings of EMNLP, pages 572?581, Honolulu, Hawaii.Andreas Zollmann and Ashish Venugopal.
2006.
Syntax Augmented Machine Translation via Chart Parsing.
InProceedings of WMT, pages 138?141, New York City.2074
