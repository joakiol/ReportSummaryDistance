Proceedings of NAACL-HLT 2015, pages 46?50,Denver, Colorado, May 31 ?
June 5, 2015. c?2015 Association for Computational LinguisticsELCO3: Entity Linking with Corpus Coherence CombiningOpen Source AnnotatorsPablo Ruiz, Thierry Poibeau and Fr?d?rique M?lanieLaboratoire LATTICECNRS, ?cole Normale Sup?rieure, U Paris 3 Sorbonne Nouvelle1, rue Maurice Arnoux, 92120 Montrouge, France{pablo.ruiz.fabo,thierry.poibeau,frederique.melanie}@ens.frAbstractEntity Linking (EL) systems?
performance isuneven across corpora or depending on entitytypes.
To help overcome this issue, we pro-pose an EL workflow that combines the out-puts of several open source EL systems, andselects annotations via weighted voting.
Theresults are displayed on a UI that allows theusers to navigate the corpus and to evaluateannotation quality based on several metrics.1 IntroductionThe Entity Linking (EL) literature has shown thatthe performance of EL systems varies widely de-pending on the corpora they are applied to and ofthe types of entities considered (Cornolti et al,2013).
For instance, a system linking to a wide setof entity types can be less accurate at basic typeslike Organization, Person, Location than systemsspecializing in those basic types.
These issuesmake it difficult for users to choose an optimal ELsystem for their corpora.To help overcome these difficulties, we havecreated a workflow whereby entities can be linkedto Wikipedia via a combination of the results ofseveral existing open source EL systems.
The out-puts of the different systems are weighted accord-ing to how well they performed on corpora similarto the user?s corpus.Our target users are social science researchers,who need to apply EL in order to, for instance,create entity co-occurrence network visualizations.These researchers need to make informed choicesabout which entities to include in their analyses,and our tool provides metrics to facilitate thesechoices.The paper is structured as follows: Section 2 de-scribes related work.
Section 3 presents the differ-ent steps in the workflow, and Section 4 focuses onthe steps presented in the demo.2 Related workCornolti et al (2013) provide a general survey onEL.
Work on combining EL systems and on help-ing users select a set of linked entities to navigate acorpus is specifically relevant to our workflow.Systems that combine entity linkers exist, e.g.NERD (Rizzo et al, 2012).
However, there are twoimportant differences in our workflow.
First, theset of entity linkers we combine is entirely opensource and public.
Second, we use a simple votingscheme to optionally offer automatically chosenannotations when linkers provide conflicting out-puts.
This type of weighted vote had not previouslybeen attempted for EL outputs to our knowledge,and is inspired on the ROVER method (Fiscus,1997, De la Clergerie et al, 2008).Regarding systems that help users navigate acorpus by choosing a representative set of linkedentities, our reference is the ANTA tool (Venturiniand Guido, 2012).1 This tool helps users chooseentities via an assessment of their corpus frequencyand document frequency.
Our tool provides suchinformation, besides a measure of each entity?scoherence with the rest of entities in the corpus.1 https://github.com/medialab/ANTA463 Workflow descriptionThe user?s corpus is first annotated by makingrequests to three EL systems?
web services: Tag-me 22 (Ferragina and Scaiella, 2010), DBpediaSpotlight3 (Mendes et al 2011) and WikipediaMiner4 (Milne and Witten, 2008).
Annotations arefiltered out if their confidence score is below theoptimal thresholds for those services, reported inCornolti et al (2013) and verified using the BAT-Framework.53.1 Annotation votingThe purpose of combining several linkers?
resultsis obtaining combined annotations that are moreaccurate than each of the linkers?
individual re-sults.
To select among the different linkers?
out-puts, a vote is performed on the annotations thatremain after the initial filtering described above.Our voting scheme is based on De la Clergerieet al?s (2008) version of the ROVER method.
Animplementation was evaluated in (Ruiz andPoibeau, 2015).
Two factors that our votingscheme considers are annotation confidence, andthe number of linkers having produced an annota-tion.
An important factor is also the performanceof the annotator having produced each annotationon a corpus similar to the user?s corpus: At theoutset of the workflow, the user?s corpus is com-pared to a set of reference corpora along dimen-sions that affect EL results, e.g.
text-length orlexical cohesion6 in the corpus?
documents.
Anno-tators that perform better on the reference corpusthat is most similar along those dimensions to theuser?s corpus are given more weight in the vote.In sum, the vote helps to select among conflict-ing annotation candidates, besides helping identifyunreliable annotations.3.2 Entity typesEntity types are assigned by exploiting infor-mation provided in the linkers?
responses, e.g.DBpedia ontology types or Wikipedia category2 http://tagme.di.unipi.it/tagme_help.html3 https://github.com/dbpedia-spotlight/dbpedia-spotlight/wiki4 http://wikipedia-miner.cms.waikato.ac.nz/5 https://github.com/marcocor/bat-framework6 Our notion of lexical cohesion relies on token overlap acrossconsecutive token sequences, inspired on the block compari-son method from Hearst (1997).labels.
The entity types currently assigned are Or-ganization, Person, Location, Concept.3.3 Entity coherence measuresOnce entity selection is completed, a score thatquantifies an entity?s coherence with the rest ofentities in the corpus is computed.
This notion ofcoherence consists of two components.
The firstone is an entity?s relatedness to other entities interms of Milne and Witten?s (2008) WikipediaLink-based Measure (WLM, details below).
Thesecond component is the distance between entities?categories in a Wikipedia category graph.WLM scores were obtained with WikipediaMiner?s compare method for Wikipedia entityIDs.7 WLM evaluates the relatedness of two Wik-ipedia pages as a function of the number of Wik-ipedia pages linking to both, and the number ofpages linking to each separately.
In the literature,WLM has been exploited to disambiguate amongcompeting entity senses within a document, takinginto account each sense?s relatedness to each of thepossible senses for the remaining entity-mentionsin the document.
We adopt this idea to assess enti-ty relatedness at corpus level rather than at docu-ment level.
To do so, we obtain each entity?saveraged WLM relatedness to the most representa-tive entities in the corpus.
The most representativeentities in the corpus were defined as a top per-centage of the entities, sorted by decreasing anno-tation confidence, whose annotation frequency andconfidence are above given thresholds.The second component of our entity coherencemeasure is based on distance between nodes in aWikipedia category graph (see Strube andPonzetto, 2006 for a review of similar methods).Based on the category graph, the averaged shortestpath8 between an entity and the most representativeentities (see criteria above) of the same type wascomputed.
Some categories like ?People from{City}?
were ignored, since they created spuriousconnections.3.4 Annotation attributesThe final annotations contain information like po-sition (document, character and sentence), confi-7 http://wikipedia-miner.cms.waikato.ac.nz/services/?compare8 Using igraph.GraphBase.get_all_shortest_paths from thePython interface to igraph: http://igraph.org/python/47dence, and entity-type.
This can be exploited forfurther textual analyses, e.g.
co-occurrence net-works.4 DemonstratorThe goal of the workflow is to help users choose arepresentative set of entities to model a corpus,with the help of descriptive statistics and othermeasures like annotation confidence, or the coher-ence scores described above.
A practical way toaccess this information is a UI, where users canassess the validity of an entity by simultaneouslylooking at its metrics, and at the documents wherethat entity was annotated.
We present an earlystage prototype of such a UI, which shows some ofthe features of the workflow above, using prepro-cessed content?the possibility to tag a new corpusis not online.The demo interface9 allows navigating a corpusthrough search and entity facets.
In Figure 1, aSearch Text query displays, on the right panel, thedocuments matching the query,10 while the entitiesannotated in those documents are shown in the leftpanel.
A Search Entities query displays the entitiesmatching the query on the left panel, and, on theright, the documents where those entities wereannotated.
Refine Search restricts the results on theright panel to documents containing certain entitiesor entity types, if the corresponding checkboxes at9 http://129.199.228.10/nav/gui/10 The application?s Solr search server requires access totraffic on port 8983.
A connection refused (or similar) errormessage in the search results panel is likely due to trafficblocked on that port at the user?s network.the end of each entity row, or items on the entity-types list have been selected.
The colors provide avisual indication of the entity?s confidence for eachlinker (columns, T, S, W, All), scaled11 to a rangebetween 0 (red) and 1 (green).
Hovering over thetable reveals the scores in each cell.For the prototype, the corpus was indexed inSolr12 and the annotations were stored in a MySQLDB.
The EL workflow was implemented in Pythonand the UI is in PHP.Examples of the utility of the information on theUI and of the workflow?s outputs follow.Usage example 1: Spotting incorrect annota-tions related to a search term.
The demo corpusis about the 2008 financial crisis.
Suppose the useris interested in organizations appearing in texts thatmention credit ratings (Figure 1).
Several relevantorganizations are returned for documents matchingthe query, but also an incorrect one: Nielsen rat-ings.
This entity is related to ratings in the sense ofaudience ratings, not credit ratings.
The coherencescore (column Coh) for the incorrect entity is muchlower (red, dark) than the scores for the relevantentities (green, light).
The score helps to visuallyidentify the incorrect annotation, based on its lackof coherence with representative entities in thecorpus.Figure 1 also gives an indication how the differ-ent linkers complement each other: Some annota-tions have been missed by one linker (grey cells),but the other two provide the annotation.11 scikit-learn: sklearn.preprocessing.MinMaxScaler.html12 http://lucene.apache.org/solr/Figure 1: Results for query credit ratings.
The right panel shows documents matching the query; the left panel shows theentities that have been annotated in those documents.48Usage example 2: Verifying correctness of en-tities in networks.
A common application of EL iscreating co-occurrence networks, e.g.
based on anautomatic selection of entities above a certain fre-quency.
This can result in errors.
Figure 2 shows asmall area from an entity co-occurrence networkfor our corpus.
Our corpus comes from the 2014PoliInformatics challenge (Smith et al, 2014), andthe corpus topic is the 2008 financial crisis.
Thenetwork was created independently of the work-flow described in this paper, using Gephi,13 basedon entities annotated by Wikipedia Miner, which isone of the EL systems whose outputs our workflowcombines.
Node Continental Airlines in the net-work seems odd for the corpus, in the sense thatthe corpus is about the financial crisis, and Conti-nental Airlines was not a major actor in the crisis.A Search Entities query for Continental on our13 http://gephi.github.ioGUI returns two annotations (Figure 3): the airline,and Continental Illinois (a defunct bank).
The co-herence (Coh) score for the bank is higher than forthe airline.
If we run a Search Text query for Con-tinental on our GUI, the documents returned forthe query confirm that the correct entity for thecorpus is the bank (Figure 4 shows one of the doc-uments returned).The example just discussed also shows that thecoherence scores can provide information that isnot redundant with respect to annotation frequencyor annotation confidence.
It is the bank?s coher-ence score that suggests its correctness: The incor-rect annotation (for the airline) is more frequent,and the confidence scores for both annotations areequivalent.In short, this second example is another indica-tion how our workflow helps spot errors made byannotation systems and decide among conflictingannotations.A final remark about entity networks: Our work-flow segments documents into sentences, whichwould allow to create co-occurrence networks atsentence level.
Some example networks based onour outputs and created with Gephi are availableon the demo site.14 These networks were not creat-ed programmatically from the workflow: The cur-rent implementation does not automatically call avisualization tool to create networks, but this isfuture work that would be useful for our targetusers.5 ConclusionSince entity linking (EL) systems?
results varywidely according to the corpora and to the annota-tion types needed by the user, we present a work-flow that combines different EL systems?
results,so that the systems complement each other.
Con-flicting annotations are resolved by a votingscheme which had not previously been attemptedfor EL.
Besides an automatic entity selection, ameasure of coherence helps users decide on thevalidity of an annotation.
The workflow?s resultsare presented on a UI that allows navigating a cor-pus using text-search and entity facets.
The UIhelps users assess annotations via the measuresdisplayed and via access to the corpus documents.14 Follow link Charts on http://129.199.228.10/nav/guiFigure 4: Example document showing that ContinentalIllinois is the correct entity in the corpusFigure 2: Region of an entity network created outside ofour workflow, based on the individual output of one ofthe EL systems we combine.
Node Continental Airlinesin the network is an error made by that EL system.Figure 3: Result of a search in our GUI for entity labelscontaining Continental.
The lower coherence score(Coh) for Continental Airlines (orange, dark) vs.Continental Illinois (green, light) suggests that the latteris correct and that the airline annotation is an error.49AcknowledgementsPablo Ruiz was supported through a PhD scholarshipfrom R?gion ?le-de-France.ReferencesCornolti, M., Ferragina, P., & Ciaramita, M. (2013).
Aframework for benchmarking entity-annotation sys-tems.
In Proc.
of WWW, 249?260.De La Clergerie, ?.
V., Hamon, O., Mostefa, D.,Ayache, C., Paroubek, P., & Vilnat, A.
(2008).
Pas-sage: from French parser evaluation to large sizedtreebank.
In Proc.
LREC 2008, 3570?3576.Ferragina, P., & Scaiella, U.
(2010).
Tagme: on-the-flyannotation of short text fragments (by wikipedia enti-ties).
In Proc.
of CIKM?10, 1625?1628.Fiscus, J. G. (1997).
A post-processing system to yieldreduced word-error rates: Recognizer output votingerror reduction (ROVER).
In Proc.
of the IEEEWorkshop on Automatic Speech Recognition andUnderstanding, 347?354.Hearst, M. A.
(1997).
TextTiling: Segmenting text intomulti-paragraph subtopic passages.
ComputationalLinguistics, 23(1), 33?64.Mendes, P. N., Jakob, M., Garc?a-Silva, A., & Bizer, C.(2011).
DBpedia spotlight: shedding light on the webof documents.
In Proc.
I-SEMANTICS?11, 1?8.Milne, D. & Witten, I.
(2008).
An effective, low-costmeasure of semantic relatedness obtained from Wik-ipedia links.
In Proc.
of AAAI Workshop on Wikipe-dia and Artificial Intelligence, 25?30Rizzo, G., & Troncy, R. (2012).
NERD: a frameworkfor unifying named entity recognition and disambig-uation extraction tools.
In Proc.
of the Demonstra-tions at EACL?12, 73?76.Ruiz, P. and Poibeau, T. (2015).
Combining OpenSource Annotators for Entity Linking throughWeighted Voting.
In Proceedings of *SEM.
FourthJoint Conference on Lexical and Computational Se-mantics.
Denver, U.S.Venturini, T. and Daniele Guido.
2012.
Once upon atext: an ANT tale in Text Analytics.
Sociologica, 3:1-17.
Il Mulino, Bologna.Smith, N. A., Cardie, C., Washington, A. L., Wilkerson,J.
D. (2014).
Overview of the 2014 NLP UnsharedTask in PoliInformatics.
Proceedings of the ACLWorkshop on Language Technologies and Computa-tional Social Science, 5?7.Strube, M. and Ponzetto, S. (2006).
WikiRelate!
Com-puting semantic relatedness using Wikipedia.
InAAAI, vol.
6, 1419?1424.50
