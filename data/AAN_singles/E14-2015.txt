Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 57?60,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsA Graphical Interface for Automatic Error Mining in CorporaGregor Thiele Wolfgang Seeker Markus G?artner Anders Bj?orkelund Jonas KuhnInstitute for Natural Language ProcessingUniversity of Stuttgart{thielegr,seeker,gaertnms,anders,kuhn}@ims.uni-stuttgart.deAbstractWe present an error mining tool that is de-signed to help human annotators to finderrors and inconsistencies in their anno-tation.
The output of the underlying al-gorithm is accessible via a graphical userinterface, which provides two aggregateviews: a list of potential errors in con-text and a distribution over labels.
Theuser can always directly access the ac-tual sentence containing the potential er-ror, thus enabling annotators to quicklyjudge whether the found candidate is in-deed incorrectly labeled.1 IntroductionManually annotated corpora and treebanks are theprimary tools that we have for developing andevaluating models and theories for natural lan-guage processing.
Given their importance for test-ing our hypotheses, it is imperative that they areof the best quality possible.
However, manual an-notation is tedious and error-prone, especially ifmany annotators are involved.
It is therefore desir-able to have automatic means for detecting errorsand inconsistencies in the annotation.Automatic methods for error detection in tree-banks have been developed in the DECCAproject1for several different annotation types, forexample part-of-speech (Dickinson and Meurers,2003a), constituency syntax (Dickinson and Meur-ers, 2003b), and dependency syntax (Boyd et al.,2008).
These algorithms work on the assumptionthat two data points that appear in identical con-texts should be labeled in the same way.
Whilethe data points in question, or nuclei, can be singletokens, spans of tokens, or edges between two to-kens, context is usually modeled as n-grams overthe surrounding tokens.
A nucleus that occurs1http://www.decca.osu.edumultiple times in identical contexts but is labeleddifferently shows variation and is considered a po-tential error.Natural language is ambiguous and variationfound by an algorithm may be a genuine ambigu-ity rather than an annotation error.
Although wecan support an annotator in finding inconsisten-cies in a treebank, these inconsistencies still needto be judged by humans.
In this paper, we presenta tool that allows a user to run automatic error de-tection on a corpus annotated with part-of-speechor dependency syntax.2The tool provides the userwith a graphical interface to browse the variationnuclei found by the algorithm and inspect their la-bel distribution.
The user can always switch be-tween high-level aggregate views and the actualsentences containing the potential error in order todecide if that particular annotation is incorrect ornot.
The interface thus brings together the outputof the error detection algorithm with a direct ac-cess to the corpus data.
This speeds up the pro-cess of tracking down inconsistencies and errorsin the annotation considerably compared to work-ing with the raw output of the original DECCAtools.
Several options allow the user to fine-tunethe behavior of the algorithm.
The tool is part ofICARUS (G?artner et al., 2013), a general searchand exploration tool.32 The Error Detection AlgorithmThe algorithm, described in Dickinson and Meur-ers (2003a) for POS tags, works by starting fromindividual tokens (the nuclei) by recording theirassigned part-of-speech over an entire treebank.From there, it iteratively increases the context foreach instance by extending the string to both sidesto include adjacent tokens.
It thus successivelybuilds larger n-grams by adding tokens to the left2Generalizing the tool to support any kind of positionalannotation is planned.3http://www.ims.uni-stuttgart.de/data/icarus.html57Figure 1: The variation n-gram view.or to the right.
Instances are grouped together iftheir context is identical, i. e. if their token n-grams match.
Groups where all instances havethe same label do not show variation and are dis-carded.
The algorithm stops when either no vari-ation nuclei are left or when none of them can befurther extended.
All remaining groups that showvariation are considered potential errors.
Erro-neous annotations that do not show variation in thedata cannot be found by the algorithm.
This limitsthe usefulness of the method for very small datasets.
Also, given the inherent ambiguity of nat-ural language, the algorithm is not guaranteed toexclusively output errors, but it achieves very highprecision in experiments on several languages.The algorithm has been extended to find errorsin constituency and dependency structures (Dick-inson and Meurers, 2003b; Boyd et al., 2008),where the definition of a nucleus is changed tocapture phrases and dependency edges.
Contextis always modeled using n-grams over surround-ing tokens, but see, e. g., Boyd et al.
(2007) forextensions.3 Graphical Error MiningTo start the error mining, a treebank and an errormining algorithm (part-of-speech or dependency)must be selected.
The algorithm is then executedon the data to create the variation n-grams.
Theuser can choose between two views for browsingthe potential errors in the treebank: (1) a viewshowing the list of variation n-grams found by theerror detection algorithm and (2) a view showinglabel distributions over word forms.3.1 The Variation N-Gram ViewFigure 1 shows a screenshot of the view where theuser is presented with the list of variation n-gramsoutput by the error detection algorithm.
The mainwindow shows the list of n-grams.
When the userselects one of the n-grams, information about thenucleus is displayed below the main window.
Theuser can inspect the distribution over labels (herepart-of-speech tags) with their absolute frequen-cies.
Above the main window, the user can adjustthe length of the presented n-grams, sort them, orsearch for specific strings.For example, Figure 1 shows a part of the vari-ation n-grams found in the German TiGer corpus(Brants et al., 2002).
The minimum and maximumlength was restricted to four, thus the list containsonly 4-grams.
The 4-gram so hoch wie in was se-lected, which contains wie as its nucleus.
In thelower part, the user can see that wie occurs withfour different part-of-speech tags in the treebank,namely KOKOM, PWAV, KON, and KOUS.
Notethat the combination with KOUS occurs only oncein the entire treebank.Double clicking on the selected 4-gram in thelist will open up a new tab that displays all sen-tences that contain this n-gram, with the nucleusbeing highlighted.
The user can then go througheach of the sentences and decide whether the an-notated part-of-speech tag is correct.
Each timethe user clicks on an n-gram, a new tab will becreated, so that the user can jump back to previousresults without having to recreate them.A double click on one of the lines in the lowerpart of the window will bring up all sentences thatcontain that particular combination of word form58Figure 2: The label distribution view.and part-of-speech tag.
The fourth line will, forexample, show the one sentence where wie hasbeen tagged as KOUS, making it easy to quicklyjudge whether the tag is correct.
In this case, theannotation is incorrect (it should have been PWAV)and should thus be marked for correction.3.2 The Label Distribution ViewIn addition to the output of the algorithm by Dick-inson and Meurers (2003a), the tool also providesa second view, which displays tag distributions ofword forms to the user (see Figure 2).
To the left,a list of unique label combinations is shown.
Se-lecting one of them displays a list of word formsthat occur with exactly these tags in the corpus.This list is shown below the list of label combina-tions.
To the right, the frequencies of the differ-ent labels are shown in a bar chart.
The leftmostbar for each label always shows the total frequencysummed over all word forms in the set.
Selectingone or more in the list of word forms adds addi-tional bars to the chart that show the frequenciesfor each selected word form.As an example, Figure 2 shows the tag combi-nation [VVINF][VVIZU], which are used to tag in-finitives with and without incorporated zu in Ger-man.
There are three word forms in the cor-pus that occur with these two part-of-speech tags:hinzukommen, aufzul?osen, and anzun?ahern.
Thechart on the right shows the frequencies for eachword form and part-of-speech tag, revealing thathinzukommen is mostly tagged as VVINF but onceas VVIZU, whereas for the other two word forms itis the other way around.
This example is interest-ing if one is looking for annotation errors in theTiGer treebank, because the two part-of-speechtags should have a complementary distribution (aGerman verb either incorporates zu or it does not).Double clicking on the word forms in the list inthe lower left corner will again open up a tab thatshows all sentences containing this word form, re-gardless of their part-of-speech tag.
The user maythen inspect the sentences and decide whether theannotations are erroneous or not.
If the user wantsto see a specific combination, which is more use-ful if the total number of sentences is large, shecan also click on one of the bars in the chart to getall sentences matching that combination.
In theexample, the one instance of hinzukommen beingtagged as VVIZU is incorrect,4and the instances ofthe two other verbs tagged as VVINF are as well.3.3 Dependency Annotation ErrorsAs mentioned before, the tool also allows the userto search for errors in dependency structures.
Theerror mining algorithm for dependency structures(Boyd et al., 2008) is very similar to the one forpart-of-speech tags, and so is the interface to then-gram list or the distribution view.
Dependencyedges are therein displayed as triples: the head,the dependent, and the edge label with the edge?sdirection.
As with the part-of-speech tags, the usercan always jump directly to the sentences that con-tain a particular n-gram or dependency relation.4Actually, the word form hinzukommen can belong to twodifferent verbs, hinzu-kommen and hin-kommen.
However,the latter, which incorporates zu, does not occur in TiGer.594 Error Detection on TiGerWe ran the error mining algorithm for part-of-speech on the German TiGer Treebank (the de-pendency version by Seeker and Kuhn (2012)) andmanually evaluated a small sample of n-grams inorder to get an idea of how useful the output is.We manually checked 115 out of the 207 vari-ation 6-grams found by the tool, which amountsto 119 different nuclei.
For 99.16% of these nu-clei, we found erroneous annotations in the asso-ciated sentences.
95.6% of these are errors wherewe are able to decide what the right tag shouldbe, the remaining ones are more difficult to disam-biguate because the annotation guidelines do notcover them.These results are in line with findings by Dick-inson and Meurers (2003a) for the Penn Treebank.They show that even manually annotated corporacontain errors and an automatic error mining toolcan be a big help in finding them.
Furthermore,it can help annotators to improve their annotationguidelines by pointing out phenomena that are notcovered by the guidelines, because these phenom-ena will be more likely to show variation.5 Related WorkWe are aware of only one other graphical tool thatwas developed to help with error detection in tree-banks: Ambati et al.
(2010) and Agarwal et al.
(2012) describe a graphical tool that was used inthe annotation of the Hindi Dependency Treebank.To find errors, it uses a statistical and a rule-basedcomponent.
The statistical component is recall-oriented and learns a MaxEnt model, which is usedto flag dependency edges as errors if their proba-bility falls below a predefined threshold.
In or-der to increase the precision, the output is post-processed by the rule-based component, which istailored to the treebank?s annotation guidelines.Errors are presented to the annotators in tables,also with the option to go to the sentences di-rectly from there.
Unlike the algorithm we im-plemented, this approach needs annotated trainingdata for training the classifier and tuning the re-spective thresholds.6 ConclusionHigh-quality annotations for linguistic corpora areimportant for testing hypotheses in NLP and lin-guistic research.
Automatically marking potentialannotation errors and inconsistencies are one wayof supporting annotators in their work.
We pre-sented a tool that provides a graphical interface forannotators to find and evaluate annotation errorsin treebanks.
It implements the error detection al-gorithms by Dickinson and Meurers (2003a) andBoyd et al.
(2008).
The user can view errors fromtwo perspectives that aggregate error informationfound by the algorithm, and it is always easy togo directly to the actual sentences for manual in-spection.
The tool is currently extended such thatannotators can make changes to the data directlyin the interface when they find an error.AcknowledgementsWe thank Markus Dickinson for his comments.Funded by BMBF via project No.
01UG1120F,CLARIN-D, and by DFG via SFB 732, project D8.ReferencesRahul Agarwal, Bharat Ram Ambati, and Anil KumarSingh.
2012.
A GUI to Detect and Correct Errors inHindi Dependency Treebank.
In LREC 2012, pages1907?1911.Bharat Ram Ambati, Mridul Gupta, Samar Husain, andDipti Misra Sharma.
2010.
A High Recall ErrorIdentification Tool for Hindi Treebank Validation.In LREC 2010.Adriane Boyd, Markus Dickinson, and Detmar Meur-ers.
2007.
Increasing the Recall of Corpus Annota-tion Error Detection.
In TLT 2007, pages 19?30.Adriane Boyd, Markus Dickinson, and Detmar Meur-ers.
2008.
On Detecting Errors in DependencyTreebanks.
Research on Language and Computa-tion, 6(2):113?137.Sabine Brants, Stefanie Dipper, Silvia Hansen-Shirra,Wolfgang Lezius, and George Smith.
2002.
TheTIGER treebank.
In TLT 2002, pages 24?41.Markus Dickinson and W. Detmar Meurers.
2003a.Detecting Errors in Part-of-Speech Annotation.
InEACL 2003, pages 107?114.Markus Dickinson and W. Detmar Meurers.
2003b.Detecting Inconsistencies in Treebanks.
In TLT2003, pages 45?56.Markus G?artner, Gregor Thiele, Wolfgang Seeker, An-ders Bj?orkelund, and Jonas Kuhn.
2013.
ICARUS?
An Extensible Graphical Search Tool for Depen-dency Treebanks.
In ACL: System Demonstrations,pages 55?60.Wolfgang Seeker and Jonas Kuhn.
2012.
Making El-lipses Explicit in Dependency Conversion for a Ger-man Treebank.
In LREC 2012, pages 3132?3139.60
