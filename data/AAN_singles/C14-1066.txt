Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 698?708, Dublin, Ireland, August 23-29 2014.Inducing Latent Semantic Relations for Structured DistributionalSemanticsSujay Kumar JauharLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon Universitysjauhar@cs.cmu.eduEduard HovyLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon Universityhovy@cs.cmu.eduAbstractStructured distributional semantic models aim to improve upon simple vector space models ofsemantics by hypothesizing that the meaning of a word is captured more effectively through itsrelational ?
rather than its raw distributional ?
signature.
In accordance, they extend the vectorspace paradigm by structuring elements with relational information that decompose distributionalsignatures over discrete relation dimensions.
However, the number and nature of these relationsremains an open research question, with most previous work in the literature employing syn-tactic dependencies as surrogates for truly semantic relations.
In this paper we propose a novelstructured distributional semantic model with latent relation dimensions, and instantiate it usinglatent relational analysis.
Evaluation of our model yields results that significantly outperformseveral other distributional approaches on two semantic tasks and performs competitively on athird relation classification task.1 IntroductionThe distributional hypothesis, articulated by Firth (1957) in the popular dictum ?You shall know theword by the company it keeps?, has established itself as one of the most popular models of moderncomputational semantics.
With the rise of massive and easily-accessible digital corpora, computation ofco-occurrence statistics has enabled researchers in NLP to build distributional semantic models (DSMs)that have found relevance in many application areas.
These include information retrieval (Manning etal., 2008), question answering (Tellex et al., 2003), word-sense disambiguation (McCarthy et al., 2004)and selectional preference modelling (Erk, 2007), to name only a few.The standard DSM framework, which models the semantics of a word by co-occurrence statisticscomputed over its neighbouring words, has several known short-comings.
One severe short-comingderives from the fundamental nature of the vector space model, which characterizes the semantics of aword by a single vector in a high dimensional space (or some lower dimensional embedding thereof).Such a modelling paradigm goes against the grain of the intuition that the semantics of a word is neitherunique nor constant.
Rather, it is composed of many facets of meaning, and similarity (or dissimilarity)to other words is an outcome of the aggregate harmony (or dissonance) between the individual facetsunder consideration.
For example, a shirt may be similar along one facet to a balloon in that they areboth coloured blue, at the same time being similar to a shoe along another facet for both being articlesof clothing, while being dissimilar along yet another facet to a t-shirt because one is stitched from linenwhile the other is made from polyester.Structured distributional semantic models (SDSMs) aim to remedy this fault with DSMs by decompos-ing distributional signatures over discrete relation dimensions, or facets.
This leads to a representationthat characterizes the semantics of a word by a distributional tensor, rather than a vector.
Previous at-tempts in the literature include the work of Pad?
and Lapata (2007), Baroni and Lenci (2010) and Goyalet al.
(2013).
However, all these approaches assume a simplified representation in which truly semanticrelations are substituted by syntactic relations obtained from a dependency parser.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/698We believe that there are limiting factors to this approximation.
Most importantly, the set of syntacticrelations, while relatively uncontroversial, is unable to capture the full extent of semantic nuance encoun-tered in natural language text.
Often, syntax is ambiguous and leads to multiple semantic interpretations.Conversely, passivization and dative shift are common examples of semantic invariance in which mul-tiple syntactic realizations are manifested.
Additionally, syntax falls utterly short in explaining morecomplex phenomena ?
such as the description of buying and selling ?
in which implicit semantics aretacit from complex interactions between multiple participants.While it is useful to consider relations that draw their origins from semantic roles such as Agent,Patient and Recipient, it remains unclear what this set of semantic roles should be.
This problem isone that has long troubled linguists (Fillmore, 1967; Sowa, 1991), and has been previously noted byresearchers in NLP as well (M?rquez et al., 2008).
Proposed solutions range from a small set of genericAgent-like or Patient-like roles in Propbank (Kingsbury and Palmer, 2002) to an effectively open-endedset of highly specific and fine-grained roles in Framenet (Baker et al., 1998).
In addition to the theoreticuncertainty of the set of semantic relations there is the very real problem of the lack of high-performance,robust semantic parsers to annotate corpora.
These issues effectively render the use of pre-defined,linguistically ordained semantic relations intractable for use in SDSM.In this paper we propose a novel approach to structuring distributional semantic models with latentrelations that are automatically discovered from corpora.
This approach effectively solves the conceptualdilemma of selecting the most expressive set of semantic relations.
To the best of our knowledge thisis the first paper to propose latent relation dimensions for SDSMs.
The intuition for generating theselatent relation dimensions leads to a generic framework, which ?
in this paper ?
is instantiated withembeddings obtained from latent relational analysis (Turney, 2005).We conduct experiments on three different semantic tasks to evaluate our model.
On a similarityscoring task and another synonym ranking task the model significantly outperforms other distributionalsemantic models, including a standard window-based model, a syntactic SDSM based on previous ap-proaches proposed in the literature, and a state-of-the-art semantic model trained using recursive neuralnetworks.
On a relation classification task, our model performs competitively, outperforming all but oneof the models it is compared against.2 Related WorkSince the distributional hypothesis was first proposed by Firth (1957), a number of different researchinitiatives have attempted to extend and improve the standard distributional vector space model of se-mantics.
Insensitivity to the multi-faceted nature of semantics has been one of the focal points of severalpapers.
Earlier work in this regard is a paper by Turney (2012), who proposes that the semantics of a wordis not obtained along a single distributional axis but simultaneously in two different spaces.
He proposesa DSM in which co-occurrence statistics are computed for neighbouring nouns and verbs separately toyield independent domain and function spaces of semantics.This intuition is taken further by a stance which proposes that a word?s semantics is distributionallydecomposed over many independent spaces ?
each of which is a unique relation dimension.
Authorswho have endorsed this perspective are Erk and Pad?
(2008), Goyal et al.
(2013), Reisinger and Mooney(2010) and Baroni and Lenci (2010).
Our work relates to these papers in that we subscribe to the multiplespace semantics view.
However, we crucially differ from them by structuring our semantic space withinformation obtained from latent semantic relations rather than from a syntactic parser.
In this paper theinstantiation of the SDSM with latent relation dimensions is obtained using LRA (Turney, 2005), whichis an extension of LSA (Deerwester et al., 1990) to induce relational embeddings for pairs of words.From a modelling perspective, SDSMs characterize the semantics of a word by a distributional ten-sor.
Other notable papers on tensor based semantics or semantics of compositional structures are thesimple additive and multiplicative models of Mitchell and Lapata (2009), the matrix-vector neural net-work approach of Socher et al.
(2012), the physics inspired quantum view of semantic composition ofGrefenstette and Sadrzadeh (2011) and the tensor-factorization model of Van de Cruys et al.
(2013).A different, partially overlapping strain of research attempts to induce word embeddings using meth-699ods from deep learning, yielding state-of-the-art results on a number of different tasks.
Notable researchpapers on this topic are the ones by Collobert et al.
(2011), Turian et al.
(2010) and Socher et al.
(2010).Other related work to note is the body of research concerned with semantic relation classification,which is one of our evaluation tasks.
Research community wide efforts in the SemEval-2007 task 4(Girju et al., 2007), the SemEval-2010 task 8 (Hendrickx et al., 2009) and the SemEval-2012 task 2(Jurgens et al., 2012) are notable examples.
However, different from our work, most previous attempts atsemantic relation classification operate on the basis of feature engineering and contextual cues (Bethardand Martin, 2007).3 Structured Distributional Semantics and Latent Semantic Relation InductionIn this section we formalize the notion of SDSM as an extension of DSM and present a novel SDSMwith latent relation dimensions.A DSM is a vector space V that contains |?| elements in Rn, where ?
= {w1, w2, ..., wk} is a vocab-ulary of k distinct words.
Every vocabulary word wihas an associated semantic vector ~virepresenting itsdistributional signature.
Each of the n elements of ~viis associated with a single dimension of its distribu-tion.
This dimension may correspond to another word ?
that may or may not belong to ?
?
or a latentdimension as might be obtained from an SVD projection or an embedding learned via a deep neural net-work.
Additionally, each element in ~viis typically a normalized co-occurrence frequency count, a PMIscore, or a number obtained from an SVD or RNN transformation.
The semantic similarity between twowords wiand wjin a DSM is the vector distance defined by cos(~vi, ~vj) on their associated distributionalvectors.An SDSM is an extension of DSM.
Formally, it is a space U that contains |?| elements in Rd?n, where?
= {w1, w2, ..., wk} is a vocabulary of k distinct words.
Every vocabulary word wihas an associatedsemantic tensor~~ui, which is itself composed of d vectors ~ui1, ~ui2, ..., ~uideach having n dimensions.Every vector ~uil?~~uirepresents the distributional signature of the word wiin a relation (or along afacet) rl.
The d relations of the SDSM may be syntactic, semantic, or latent (as in this paper).
The ndimensional relational vector ~uilis configurationally the same as a vector ~viof a DSM.
This definitionof an SDSM closely relates to an alternate view of Distributional Memories (DMs) (Baroni and Lenci,2010) where the semantic space is a third-order tensor, whose modes are Word?
Link?Word.The semantic similarity between two wordswiandwjin an SDSM is the similarity function defined bysim(~~ui,~~uj) on their associated semantic tensors.
We use the following decomposition of the similarityfunction:sim(~~ui,~~uj) =1dd?l=1cos( ~uil, ~ujl) (1)Mathematically, this corresponds to the ratio of the normalized Frobenius product of the two matricesrepresenting~~uiand~~ujto the number of rows in both matrices.
Intuitively it is simply the averagerelation-wise similarity between the two words wiand wj.3.1 Latent Relation Induction for SDSMThe intuition behind our approach for inducing latent relation dimensions revolves around the simple ob-servation that SDSMs, while representing semantics as distributional signatures over relation dimensions,also effectively encode relational vectors between pairs of words.
Our method thus works backwardsfrom this observation ?
beginning with a relational embedding for pairs of words, that are subsequentlytransformed to yield an SDSM.Concretely, given a vocabulary ?
= {w1, w2, ..., wk} and a list of word pairs of interest from thevocabulary ?V?
?
?
?, we assume that we have some method for inducing a DSM V?that has avector representation~v?ijof length d for every word pair wi, wj?
?V, which intuitively embeds thedistributional signature of the relation binding the two words in d latent dimensions.
We then constructan SDSM U where ?U= ?.
For every word wi?
?
a tensor~~ui?
Rd?kis generated.
The tensor~~ui700has d unique k dimensional vectors ~ui1, ~ui2, ..., ~uid.
For a given relational vector ~uil, the value of thejth element is taken from the lth element of the vector~v?ijbelonging to the DSM V?.
If the vector~v?ijdoes not exist in V??
as is the case where the pair wi, wj/?
?V?
the value of the jth element of ~uilisset to 0.
By applying this mapping to generate semantic tensors for every word in ?, we are left with anSDSM U that effectively embeds latent relation dimensions.
From the perspective of DMs we matricizethe third-order tensor and perform truncated SVD, before restoring the resulting matrix to a third-ordertensor.3.1.1 Latent Relational AnalysisIn what follows, we present our instantiation of this model with an implementation that is based onLatent Relational Analysis (LRA) (Turney, 2005) to generate the DSM V?.
While other methods (suchas RNNs) are equally applicable in this scenario, we use LRA for its operational simplicity as well asproven efficacy on semantic tasks such as analogy detection.
The parameter values we chose in ourexperiments are not fine-tuned and are guided by recommended values from Turney (2005), or scaledsuitably to accommodate the size of ?V.The input to LRA is a vocabulary ?
= {w1, w2, ..., wk} and a list of word pairs of interest from thevocabulary ?V?
?
?
?.
While one might theoretically consider a large vocabulary with all possi-ble pairs, for computational reasons we restrict our vocabulary to approximately 4500 frequent Englishwords and only consider about 2.5% word pairs with high PMI (as computed on the whole of EnglishWikipedia) in ??
?.
For each of the word pairs wi, wj?
?Vwe extract a list of contexts by querying asearch engine indexed over the combined texts of the whole of English Wikipedia and Gigaword corpora(approximately 5.8?
109tokens).
Suitable query expansion is performed by taking the top 4 synonymsof wiand wjusing Lin?s thesaurus (Lin, 1998).
Each of these contexts must contain both wi, wj(orappropriate synonyms) and optionally some intervening words, and some words to either side.Given such contexts, patterns for every word pair are generated by replacing the two target words wiand wjwith placeholder characters X and Y , and replacing none, some or all of the other words by theirassociated part-of-speech tag or a wildcard symbol.
For example, if wiand wjare ?eat?
and ?pasta?respectively, and the queried context is ?I eat a bowl of pasta with a fork?, one would generate patternssuch as ?
* X * NN * Y IN a *?, ?
* X DT bowl IN Y with DT *?, etc.
For every word pair, only the 5000most frequent patterns are stored.Once the set of all relevant patterns P = p1, p2, ..., pnhave been computed a DSM V is constructed.In particular, the DSM constitutes a ?Vbased on the list of word pairs of interest, and every word pairwi, wjof interest has an associated vector ~vij.
Each element m of the vector ~vijis a count pertaining tothe number of times that the pattern pmwas generated by the word pair wi, wj.3.1.2 SVD TransformationThe resulting DSM V is noisy and very sparse.
Two transformations are thus applied to V .
Firstly allco-occurrence counts between word pairs and patterns are transformed to PPMI scores (Bullinaria andLevy, 2007).
Then given the matrix representation of V ?
where rows correspond to word pairs andcolumns correspond to patterns ?
SVD is applied to yield V = M?N .
HereM andN are matrices thathave unit-length orthogonal columns and ?
is a matrix of singular values.
By selecting the d top singularvalues, we approximate V with a lower dimension projection matrix that reduces noise and compensatesfor sparseness: V?= Md?d.
This DSM V?in d latent dimensions is precisely the one we then use toconstruct an SDSM, using the transformation described above.Since the large number of patterns renders it effectively impossible to store the entire matrix V inmemory we use a memory friendly implementation1of a multi-pass stochastic algorithm to directlyapproximate the projection matrix (Halko et al., 2011; Rehurek, 2010).
A detailed analysis to see howchange in the parameter d effects the quality of the model is presented in section 4.The optimal SDSM embeddings we trained and used in the experiments detailed below are availablefor download at http://www.cs.cmu.edu/~sjauhar/Software_files/LR-SDSM.tar.1http://radimrehurek.com/gensim/701Model Spearman?s ?Random 0.000DSM 0.179synSDSM 0.315SENNA 0.510LR-SDSM (300) 0.567LR-SDSM (130) 0.586Table 1Model Acc.Random 0.25DSM 0.28synSDSM 0.27SENNA 0.38LR-SDSM (300) 0.47LR-SDSM (130) 0.51Table 2Results on the WS-353 similarity scoring task and the ESL synonym selection task.
LRA-SDSM signif-icantly outperforms other structured and non-structured distributional semantic models.gz.
This SDSM contains a vocabulary of 4546 frequent English words with 130 latent relation dimen-sions.4 EvaluationSection 3 has described a method for embedding latent relation dimensions in SDSMs.
We now turn tothe problem of evaluating these relations within the scope of the distributional paradigm in order to ad-dress two research questions: 1) Are latent relation dimensions a viable and empirically competitive solu-tion for SDSM?
2) Does structuring lead to a semantically more expressive model than a non-structuredDSM?
In order to answer these questions we evaluate our model on two generic semantic tasks andpresent comparative results against other structured and non-structured distributional models.
We showthat we outperform all of them significantly, thus answering both research questions affirmatively.While other research efforts have produced better results on these tasks (Jarmasz and Szpakowicz,2003; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011), they are either lexicon or knowl-edge based, or are driven by corpus statistics that tie into auxiliary resources such as multi-lingual in-formation and structured ontologies like Wikipedia.
Hence they are not relevant to our experimentalvalidation, and are consequently ignored in our comparative evaluation.4.1 Word-Pair Similarity Scoring TaskThe first task consists in using a semantic model to assign similarity scores to pairs of words.
The datasetused in this evaluation setting is the WS-353 dataset from Finkelstein et al.
(2002).
It consists of 353pairs of words along with an averaged similarity score on a scale of 1.0 to 10.0 obtained from 13?16human judges.
Word pairs are presented as-is, without any context.
For example, an item in this datasetmight be ?book, paper?
7.46?.System scores are obtained by using the standard cosine similarity measure between distributionalvectors in a non-structured DSM.
In the case of a variant of SDSM, these scores can be found by usingthe cosine-based similarity functions in Equation 1 of the previous section.
System generated outputscores are evaluated against the gold standard using Spearman?s rank correlation coefficient.4.2 Synonym Selection TaskIn the second task, the same set of semantic space representations is used to select the semantically closestword to a target from a list of candidates.
The ESL dataset from Turney (2002) is used for this task, andwas selected over the slightly larger TOEFL dataset (Landauer and Dumais, 1997).
The reason for thischoice was because the latter contained more complex vocabulary words ?
several of which were notpresent in our simple vocabulary model.
The ESL dataset consists of 50 target words that appear with 4candidate lexical substitutes each.
While disambiguating context is also given in this dataset, we discardit in our experiments.
An example item in this dataset might be ?rug?
sofa, ottoman, carpet, hallway?,with ?carpet?
being the most synonym-like candidate to the target.702Figure 1: Evaluation results on WS-353 and ESL with varying number of latent dimensions.
Generallyhigh scores are obtained in the range of 100-150 latent dimensions, with optimal results on both datasetsat 130 latent dimensions.Similarity scores ?
which are obtained in the same manner as for the previous evaluation task ?are extracted between the target and each of the candidates in turn.
These scores are then sorted indescending order, with the top-ranking score yielding the semantically closest candidate to the target.Systems are evaluated on the basis of their accuracy at discriminating the top-ranked candidate.4.3 ResultsWe compare our model (LR-SDSM) to several other distributional models in these experiments.
Theseinclude a standard distributional vector space model (DSM) trained on the combined text of EnglishWikipedia and Gigaword with a window-size of 3 words to either side of a target, a syntax-based SDSM(Goyal et al., 2013; Baroni and Lenci, 2010) (synSDSM) trained on the same corpus parsed with adependency parser (Tratz and Hovy, 2011) and the state-of-the-art neural network embeddings fromCollobert et al.
(2011) (SENNA).
We also give the expected evaluation scores from a random baseline,for comparison.An important factor to consider when constructing an SDSM using LRA is the number of latent di-mensions selected in the SVD projection.
In Figure 1 we investigate the effects of selecting differentnumber of latent relation dimensions on both semantic evaluation tasks, starting with 10 dimensions upto a maximum of 800 (which was the maximum that was computationally feasible), in increments of 10.We note that optimal results on both datasets are obtained at 130 latent dimensions.
In addition to theSDSM obtained in this setting we also give results for an SDSM with 300 latent dimensions (which hasbeen a recommended value for SVD projections in the literature (Landauer and Dumais, 1997)) in ourcomparisons against other models.
Comparative results on the Finkelstein WS-353 similarity scoringtask are given in Table 1, while those on the ESL synonym selection task are given in Table 2.4.4 DiscussionThe results in Tables 1 and 2 show that LR-SDSM outperforms the other distributional models by aconsiderable and statistically significant margin (p-value < 0.05) on both types of semantic evaluationtasks.
It should be noted that we do not tune to the test sets.
While the 130 latent dimension SDSM yieldsthe best results, 300 latent dimensions also gives comparable performance and moreover outperforms allthe other baselines.
In fact, it is worth noting that the evaluation results in figure 1 are almost all better703Random SENNA-MikDSM SENNALR-SDSMAVC MVC AVC MVCPrec.
0.111 0.273 0.419 0.382 0.489 0.416 0.431Rec.
0.110 0.343 0.449 0.443 0.516 0.457 0.475F-1.
0.110 0.288 0.426 0.383 0.499 0.429 0.444% Acc.
11.03 34.30 44.91 44.26 51.55 45.65 47.48Table 3: Results on Relation Classification Task.
LR-SDSM scores competitively, outperforming all butthe SENNA-AVC model.than the results of the other models on either datasets.We conclude that structuring of a semantic model with latent relational information in fact leads toperformance gains over non-structured variants.
Also, the latent relation dimensions we propose offer aviable and empirically competitive alternative to syntactic relations for SDSMs.Figure 1 shows the evaluation results on both semantic tasks as a function of the number of latentdimensions.
The general trend of both curves on the figure indicate that the expressive power of themodel quickly increases with the number of dimensions until it peaks in the range of 100?150, and thendecreases or evens out after that.
Interestingly, this falls roughly in the range of the 166 frequent (thosethat appear 50 times or more) frame elements, or fine-grained relations, from FrameNet that O?Hara andWiebe (2009) find in their taxonomization and mapping of a number of lexical resources that containsemantic relations.5 Semantic Relation Classification and Analysis of the Latent Structure of DimensionsIn this section we conduct experiments on the task of semantic relation classification.
We also perform amore detailed analysis of the induced latent relation dimensions in order to gain insight into our model?sperception of semantic relations.5.1 Semantic Relation ClassificationIn this task, a relational embedding is used as a feature vector to train a classifier for predicting thesemantic relation between previously unseen word pairs.
The dataset used in this experiment is fromthe SemEval-2012 task 2 on measuring the degree of relational similarity (Jurgens et al., 2012), sinceit characterizes a number of very distinct and interesting semantic relations.
In particular it consists ofan aggregated set of 3464 word pairs evidencing 10 kinds of semantic relations.
We prune this set todiscard pairs that don?t contain words in the vocabularies of the models we consider in our experiments.This leaves us with a dataset containing 933 word pairs in 9 classes (1 class was discarded altogetherbecause it contained too few instances).
The 9 semantic relation classes are: ?Class Inclusion?, ?Part-Whole?, ?Similar?, ?Contrast?, ?Attribute?, ?Non-Attribute?, ?Case Relation?, ?Cause-Purpose?
and?Space-Time?.
For example, an instance of a word pair that exemplifies the ?Part-Whole?
relationship is?engine:car?.
Note that, as with previous experiments, word pairs are given without any context.5.2 ResultsWe compare LR-SDSM on the semantic relation classification task to several different models.
Theseinclude the additive vector composition (AVC) and multiplicative vector composition methods (MVC)proposed by Mitchell and Lapata (2009); we present both DSM and SENNA based variants of thesemodels.
We also compare against the vector difference method of Mikolov et al.
(2013) (SENNA-Mik) which sees semantic relations as a meaning preserving vector translation in an RNN embeddedvector space.
Finally, we note the performance of random classification as a baseline, for reference.
Weattempted to produce results of a syntactic SDSM on the task; however, the hard constraint imposed bysyntactic adjacency meant that effectively all the word pairs in the dataset yielded zero feature vectors.To avoid overfitting on all 130 original dimensions in our optimal SDSM, and also to render resultscomparable, we reduce the number of latent relation dimensions of LR-SDSM to 50.
We similarly reduce704Figure 2: Correlation distances between semantic relations?
classifier weights.
The plot shows how ourlatent relations seem to perceive humanly interpretable semantic relations.
Most points are fairly wellspaced out, with opposites such as ?Attribute?
and ?Non-Attribute?
as well as ?Similar?
and ?Contrast?being relatively further apart.the feature vector dimension of DSM-AVC and DSM-MVC to 50 by feature selection.
The dimensionsof SENNA-AVC, SENNA-MVC and SENNA-Mik are already 50, and are not reduced further.For each of the methods we train a logistic regression classifier.
We don?t perform any tuning ofparameters and set a constant ridge regression value of 0.2, which seemed to yield roughly the bestresults for all models.
The performance on the semantic relation classification task in terms of averagedprecision, recall, F-measure and percentage accuracy using 10-fold cross-validation is given in Table 3.Additionally, to gain further insight into the LR-SDSM?s understanding of semantic relations, weconduct a secondary analysis.
We begin by training 9 one-vs-all logistic regression classifiers for each ofthe 9 semantic relations under consideration.
Then pairwise correlation distances are measured betweenall pairs of weight vectors of the 9 models.
Finally, the distance adjacency matrix is projected into 2-dspace using multidimensional scaling.
The result of this analysis is presented in Figure 2.5.3 DiscussionTable 3 shows that LR-SDSM performs competitively on the relation classification task and outperformsall but one of the other models.
The performance differences are statistically significant with a p-value< 0.5.
We believe that some of the expressive power of the model is lost by compressing to 50 latentrelation dimensions, and that a greater number of dimensions might improve performance.
However,testing a model with a 130-length dense feature vector on a dataset containing 933 instances wouldlikely lead to overfitting and also not be comparable to the SENNA-based models that operate on 50-length feature vectors.Other points to note from Table 3 are that the AVC variants of the the DSM and SENNA compositionmodels tend to perform better than their MVC counterparts.
Also, SENNA-Mik performs surprisinglypoorly.
It is worth noting, however, that Mikolov et al.
(2013) report results on fairly simple lexico-syntactic relations between words ?
such as plural forms, possessives and gender ?
while the semanticrelations under consideration in the SemEval-2012 dataset are relatively more complex.In the analysis of the latent structure of dimensions presented in Figure 2, there are few interestingpoints to note.
To begin with, all the points (with the exception of one pair) are fairly well spaced out.
At705the weight vector level, this implies that different latent dimensions need to fire in different combinationsto characterize distinct semantic relations, thus resulting in low correlation between their correspondingweight vectors.
This indicates the fact that the latent relation dimensions seem to capture the intuitionthat each of the classes encodes a distinctly different semantic relation.
The notable exception is ?Space-Time?, which is very close to ?Contrast?.
This is probably due to the fact that distributional models areineffective at capturing spatio-temporal semantics.
Moreover, it is interesting to note that ?Attribute"and ?Non-Attribute?
as well as ?Similar?
and ?Contrast?, which are intuitively semantic inverses of eachother are also (relatively) distant from each other in the plot.These general findings indicate an interesting avenue for future research, which involves mapping theempirically learnt latent relations to hand-built semantic lexicons or frameworks.
This could help to vali-date the empirical models at various levels of linguistic granularity, as well as establish correspondencesbetween different views of semantic representation.6 Conclusion and Future WorkIn this paper we have proposed a novel paradigm for SDSMs, that allows for structuring via latentrelational information.
We have introduced a generic operational framework that allows for buildingsuch SDSMs and outlined an instantiation of the model with LRA.
Experimental results of the modelsupport our claim that the resulting SDSM captures the semantics of words more effectively than anumber of other semantic models, and presents a viable ?
and empirically competitive ?
alternativeto syntactic SDSMs.
Additionally we have conducted experiments on a relation classification task andshown promising results, as well as performed analyses to investigate the structure of, and interactionsbetween, the latent relation dimensions.These findings motivate a number of future directions of research.
Since our framework is fairly gen-eral we hope to explore techniques other than LRA (such as RNNs) to generate relational embeddingsfor word pairs.
A desiderata for future techniques is scalability so that we can characterize vocabular-ies that are larger than the one in our current experiments.
We also hope to explore mappings betweenour empirically learnt latent relations, and semantic lexicons and frameworks that catalog semantic re-lations.
Finally, we hope to test our model on more realistic application task such as event coreference,recognizing textual entailment, and semantic parsing in future work.AcknowledgmentsThe authors would like to thank the anonymous reviewers for their valuable comments and suggestionsto improve the quality of the paper.
This work was supported in part by the following grants: NSF grantIIS-1143703, NSF award IIS-1147810, DARPA grant FA87501220342.ReferencesCollin F Baker, Charles J Fillmore, and John B Lowe.
1998.
The berkeley framenet project.
In Proceedings of the17th international conference on Computational linguistics-Volume 1, pages 86?90.
Association for Computa-tional Linguistics.Marco Baroni and Alessandro Lenci.
2010.
Distributional memory: A general framework for corpus-basedsemantics.
Comput.
Linguist., 36(4):673?721, December.Steven Bethard and James H Martin.
2007.
Cu-tmp: temporal relation classification using syntactic and semanticfeatures.
In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 129?132.
Associa-tion for Computational Linguistics.John A Bullinaria and Joseph P Levy.
2007.
Extracting semantic representations from word co-occurrence statis-tics: A computational study.
Behavior Research Methods, 39(3):510?526.Ronan Collobert, Jason Weston, L?on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.
J. Mach.
Learn.
Res., 999888:2493?2537, November.Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman.
1990.Indexing by latent semantic analysis.
JASIS, 41(6):391?407.706Katrin Erk and Sebastian Pad?.
2008.
A structured vector space model for word meaning in context.
In Proceed-ings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?08, pages 897?906,Stroudsburg, PA, USA.
Association for Computational Linguistics.Katrin Erk.
2007.
A simple, similarity-based model for selectional preferences.Charles J Fillmore.
1967.
The case for case.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Rup-pin.
2002.
Placing search in context: The concept revisited.
In ACM Transactions on Information Systems,volume 20, pages 116?131, January.John R. Firth.
1957.
A Synopsis of Linguistic Theory, 1930-1955.
Studies in Linguistic Analysis, pages 1?32.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Computing semantic relatedness using wikipedia-based ex-plicit semantic analysis.
In IJCAI, volume 7, pages 1606?1611.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret.
2007.
Semeval-2007 task 04: Classification of semantic relations between nominals.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations, pages 13?18.
Association for Computational Linguistics.Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrinmaya Sachan, Shashank Srivastava, and Eduard Hovy.
2013.A structured distributional semantic model for event co-reference.
In Proceedings of the 51st Annual Meetingof the Association for Computational Linguistics (ACL ?
2013).Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.
Experimental support for a categorical compositionaldistributional model of meaning.
In Proceedings of the Conference on Empirical Methods in Natural LanguageProcessing, EMNLP ?11, pages 1394?1404, Stroudsburg, PA, USA.
Association for Computational Linguistics.Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp.
2011.
Finding structure with randomness: Probabilisticalgorithms for constructing approximate matrix decompositions.
SIAM review, 53(2):217?288.Samer Hassan and Rada Mihalcea.
2011.
Semantic relatedness using salient semantic analysis.
In AAAI.Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid ?
S?aghdha, Sebastian Pad?, MarcoPennacchiotti, Lorenza Romano, and Stan Szpakowicz.
2009.
Semeval-2010 task 8: Multi-way classificationof semantic relations between pairs of nominals.
In Proceedings of the Workshop on Semantic Evaluations:Recent Achievements and Future Directions, pages 94?99.
Association for Computational Linguistics.Mario Jarmasz and Stan Szpakowicz.
2003.
Roget?s thesaurus and semantic similarity.
Recent Advances inNatural Language Processing III: Selected Papers from RANLP.David A Jurgens, Peter D Turney, Saif M Mohammad, and Keith J Holyoak.
2012.
Semeval-2012 task 2: Measur-ing degrees of relational similarity.
In Proceedings of the First Joint Conference on Lexical and ComputationalSemantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedingsof the Sixth International Workshop on Semantic Evaluation, pages 356?364.
Association for ComputationalLinguistics.Paul Kingsbury and Martha Palmer.
2002.
From treebank to propbank.
In LREC.
Citeseer.Thomas K Landauer and Susan T. Dumais.
1997.
A solution to plato?s problem: The latent semantic analysistheory of acquisition, induction, and representation of knowledge.
Psychological review, pages 211?240.Dekang Lin.
1998.
Automatic retrieval and clustering of similar words.
In Proceedings of the 17th internationalconference on Computational linguistics-Volume 2, pages 768?774.
Association for Computational Linguistics.Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch?tze.
2008.
Introduction to Information Retrieval.Cambridge University Press, New York, NY, USA.Llu?s M?rquez, Xavier Carreras, Kenneth C Litkowski, and Suzanne Stevenson.
2008.
Semantic role labeling: anintroduction to the special issue.
Computational linguistics, 34(2):145?159.Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll.
2004.
Finding predominant word senses in un-tagged text.
In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL?04, Stroudsburg, PA, USA.
Association for Computational Linguistics.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013.
Linguistic regularities in continuous space word repre-sentations.
Proceedings of NAACL-HLT, pages 746?751.707Jeff Mitchell and Mirella Lapata.
2009.
Language models based on semantic composition.
In Proceedings of the2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ?09,pages 430?439, Stroudsburg, PA, USA.
Association for Computational Linguistics.Tom O?Hara and Janyce Wiebe.
2009.
Exploiting semantic role resources for preposition disambiguation.
Com-putational Linguistics, 35(2):151?184.Sebastian Pad?
and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Computa-tional Linguistics, 33(2):161?199.Radim Rehurek.
2010.
Fast and faster: A comparison of two streamed matrix decomposition algorithms.
NIPS2010 Workshop on Low-rank Methods for Large-scale Machine Learning.Joseph Reisinger and Raymond J Mooney.
2010.
Multi-prototype vector-space models of word meaning.
InHuman Language Technologies: The 2010 Annual Conference of the North American Chapter of the Associationfor Computational Linguistics, pages 109?117.
Association for Computational Linguistics.Richard Socher, Christopher D Manning, and Andrew Y Ng.
2010.
Learning continuous phrase representationsand syntactic parsing with recursive neural networks.
Proceedings of the NIPS-2010 Deep Learning and Unsu-pervised Feature Learning Workshop.Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceedings of the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ?12, pages1201?1211, Stroudsburg, PA, USA.
Association for Computational Linguistics.John F. Sowa.
1991.
Principles of semantic networks.
Morgan Kaufmann.Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fernandes, and Gregory Marton.
2003.
Quantitative evaluationof passage retrieval algorithms for question answering.
In SIGIR, pages 41?47.Stephen Tratz and Eduard Hovy.
2011.
A fast, accurate, non-projective, semantically-enriched parser.
In Proceed-ings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages 1257?1268,Stroudsburg, PA, USA.
Association for Computational Linguistics.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.
Word representations: a simple and general method forsemi-supervised learning.
In Proceedings of the 48th Annual Meeting of the Association for ComputationalLinguistics, pages 384?394.
Association for Computational Linguistics.Peter D. Turney.
2002.
Mining the web for synonyms: Pmi-ir versus lsa on toefl.
CoRR.Peter Turney.
2005.
Measuring semantic similarity by latent relational analysis.
In Proceedings of the 19thinternational Conference on Aritifical Intelligence, pages 1136?1141.Peter D Turney.
2012.
Domain and function: A dual-space model of semantic relations and compositions.
Journalof Artificial Intelligence Research, 44:533?585.Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen.
2013.
A tensor-based factorization model of semanticcompositionality.
In Proceedings of NAACL-HLT, pages 1142?1151.708
