Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 539?546,Sydney, July 2006. c?2006 Association for Computational LinguisticsStochastic Iterative Alignment for Machine Translation EvaluationDing Liu and Daniel GildeaDepartment of Computer ScienceUniversity of RochesterRochester, NY 14627AbstractA number of metrics for automatic eval-uation of machine translation have beenproposed in recent years, with some met-rics focusing on measuring the adequacyof MT output, and other metrics focus-ing on fluency.
Adequacy-oriented met-rics such as BLEU measure n-gram over-lap of MT outputs and their references, butdo not represent sentence-level informa-tion.
In contrast, fluency-oriented metricssuch as ROUGE-W compute longest com-mon subsequences, but ignore words notaligned by the LCS.
We propose a metricbased on stochastic iterative string align-ment (SIA), which aims to combine thestrengths of both approaches.
We com-pare SIA with existing metrics, and findthat it outperforms them in overall evalu-ation, and works specially well in fluencyevaluation.1 IntroductionEvaluation has long been a stumbling block inthe development of machine translation systems,due to the simple fact that there are many correcttranslations for a given sentence.
Human evalu-ation of system output is costly in both time andmoney, leading to the rise of automatic evalua-tion metrics in recent years.
In the 2003 JohnsHopkins Workshop on Speech and Language En-gineering, experiments on MT evaluation showedthat BLEU and NIST do not correlate well withhuman judgments at the sentence level, even whenthey correlate well over large test sets (Blatz etal., 2003).
Liu and Gildea (2005) also pointedout that due to the limited references for everyMT output, using the overlapping ratio of n-gramslonger than 2 did not improve sentence level eval-uation performance of BLEU.
The problem leadsto an even worse result in BLEU?S fluency eval-uation, which is supposed to rely on the long n-grams.
In order to improve sentence-level evalu-ation performance, several metrics have been pro-posed, including ROUGE-W, ROUGE-S (Lin andOch, 2004) and METEOR (Banerjee and Lavie,2005).
ROUGE-W differs from BLEU and NISTin that it doesn?t require the common sequence be-tween MT output and the references to be consec-utive, and thus longer common sequences can befound.
There is a problem with loose-sequence-based metrics: the words outside the longest com-mon sequence are not considered in the metric,even if they appear both in MT output and thereference.
ROUGE-S is meant to alleviate thisproblem by computing the common skipped bi-grams instead of the LCS.
But the price ROUGE-S pays is falling back to the shorter sequences andlosing the advantage of long common sequences.METEOR is essentially a unigram based metric,which prefers the monotonic word alignment be-tween MT output and the references by penalizingcrossing word alignments.
There are two prob-lems with METEOR.
First, it doesn?t considergaps in the aligned words, which is an importantfeature for evaluating the sentence fluency; sec-ond, it cannot use multiple references simultane-ously.1 ROUGE and METEOR both use WordNetand Porter Stemmer to increase the chance of theMT output words matching the reference words.Such morphological processing and synonym ex-traction tools are available for English, but are notalways available for other languages.
In order totake advantage of loose-sequence-based metricsand avoid the problems in ROUGE and METEOR,we propose a new metric SIA, which is based onloose sequence alignment but enhanced with thefollowing features:1METEOR and ROUGE both compute the score based onthe best reference539?
Computing the string alignment score basedon the gaps in the common sequence.
ThoughROUGE-W also takes into consider the gapsin the common sequence between the MToutput and the reference by giving more cred-its to the n-grams in the common sequence,our method is more flexible in that not onlydo the strict n-grams get more credits, butalso the tighter sequences.?
Stochastic word matching.
For the purposeof increasing hitting chance of MT outputs inreferences, we use a stochastic word match-ing in the string alignment instead of WORD-STEM and WORD-NET used in METEORand ROUGE.
Instead of using exact match-ing, we use a soft matching based on the sim-ilarity between two words, which is trainedin a bilingual corpus.
The corpus is alignedin the word level using IBM Model4 (Brownet al, 1993).
Stochastic word matching is auniform replacement for both morphologicalprocessing and synonym matching.
More im-portantly, it can be easily adapted for differ-ent kinds of languages, as long as there arebilingual parallel corpora available (which isalways true for statistical machine transla-tion).?
Iterative alignment scheme.
In this scheme,the string alignment will be continued untilthere are no more co-occuring words to befound between the MT output and any one ofthe references.
In this way, every co-occuringword between the MT output and the refer-ences can be considered and contribute to thefinal score, and multiple references can beused simultaneously.The remainder of the paper is organized as fol-lows: section 2 gives a recap of BLEU, ROUGE-W and METEOR; section 3 describes the threecomponents of SIA; section 4 compares the per-formance of different metrics based on experimen-tal results; section 5 presents our conclusion.2 Recap of BLEU, ROUGE-W andMETEORThe most commonly used automatic evaluationmetrics, BLEU (Papineni et al, 2002) and NIST(Doddington, 2002), are based on the assumptionthat ?The closer a machine translation is to a pro-mt1: Life is like one nice chocolate in boxref: Life is just like a box of tasty chocolateref: Life is just like a box of tasty chocolatemt2: Life is of one nice chocolate in boxFigure 1: Alignment Example for ROUGE-Wfessional human translation, the better it is?
(Pa-pineni et al, 2002).
For every hypothesis, BLEUcomputes the fraction of n-grams which also ap-pear in the reference sentences, as well as a brevitypenalty.
NIST uses a similar strategy to BLEU butfurther considers that n-grams with different fre-quency should be treated differently in the evalu-ation (Doddington, 2002).
BLEU and NIST havebeen shown to correlate closely with human judg-ments in ranking MT systems with different qual-ities (Papineni et al, 2002; Doddington, 2002).ROUGE-W is based on the weighted longestcommon subsequence (LCS) between the MT out-put and the reference.
The common subsequencesin ROUGE-W are not necessarily strict n-grams,and gaps are allowed in both the MT output andthe reference.
Because of the flexibility, longcommon subsequences are feasible in ROUGE-W and can help to reflect the sentence-wide sim-ilarity of MT output and references.
ROUGE-Wuses a weighting strategy where the LCS contain-ing strict n-grams is favored.
Figure 1 gives twoexamples that show how ROUGE-W searches forthe LCS.
For mt1, ROUGE-W will choose eitherlife is like chocolate or life is like box as the LCS,since neither of the sequences ?like box?
and ?likechocolate?
are strict n-grams and thus make no dif-ference in ROUGE-W (the only strict n-grams inthe two candidate LCS is life is).
For mt2, thereis only one choice of the LCS: life is of chocolate.The LCS of mt1 and mt2 have the same length andthe same number of strict n-grams, thus they getthe same score in ROUGE-W.
But it is clear to usthat mt1 is better than mt2.
It is easy to verify thatmt1 and mt2 have the same number of common 1-grams, 2-grams, and skipped 2-grams with the ref-erence (they don?t have common n-grams longerthan 2 words), thus BLEU and ROUGE-S are alsonot able to differentiate them.METEOR is a metric sitting in the middleof the n-gram based metrics and the loose se-540mt1: Life is like one nice chocolate in boxref: Life is just like a box of tasty chocolateref: Life is just like a box of tasty chocolatemt2: Life is of one nice chocolate in boxFigure 2: Alignment Example for METEORquence based metrics.
It has several phases andin each phase different matching techniques (EX-ACT, PORTER-STEM, WORD-NET) are used tomake an alignment for the MT output and the ref-erence.
METEOR doesn?t require the alignment tobe monotonic, which means crossing word map-pings (e.g.
a b is mapped to b a) are allowed,though doing so will get a penalty.
Figure 2 showsthe alignments of METEOR based on the sameexample as ROUGE.
Though the two alignmentshave the same number of word mappings, mt2 getsmore crossed word mappings than mt1, thus it willget less credits in METEOR.
Both ROUGE andMETEOR normalize their evaluation result basedon the MT output length (precision) and the ref-erence length (recall), and the final score is com-puted as the F-mean of them.3 Stochastic Iterative Alignment (SIA)for Machine Translation EvaluationWe introduce three techniques to allow more sen-sitive scores to be computed.3.1 Modified String AlignmentThis section introduces how to compute the stringalignment based on the word gaps.
Given a pairof strings, the task of string alignment is to obtainthe longest monotonic common sequence (wheregaps are allowed).
SIA uses a different weightingstrategy from ROUGE-W, which is more flexible.In SIA, the alignments are evaluated based on thegeometric mean of the gaps in the reference sideand the MT output side.
Thus in the dynamic pro-gramming, the state not only includes the currentcovering length of the MT output and the refer-ence, but also includes the last aligned positions inthem.
The algorithm for computing the alignmentscore in SIA is described in Figure 3.
The sub-routine COMPUTE SCORE, which computes thescore gained from the current aligned positions, isshown in Figure 4.
From the algorithm, we canfunction GET ALIGN SCORE(mt, M, ref, N).
Compute the alignment score of the MT output mtwith length M and the reference ref with length Nfor i = 1; i ?
M; i = i +1 dofor j = 1; j ?
N; j = j +1 dofor k = 1; k ?
i; k = k +1 dofor m = 1; m ?
j; m = m +1 doscorei,j,k,m= max{scorei?1,j,k,m,scorei,j?1,k,m } ;end forend forscorei,j,i,j =maxn=1,M ;p=1,N{scorei,j,i,j , scorei?1,j?1,n,p+ COMPUTE SCORE(mt,ref, i, j, n, p)};end forend forreturn scoreM,N,M,NM ;end functionFigure 3: Alignment Algorithm Based on Gapsfunction COMPUTE SCORE(mt, ref, i, j, n, p)if mt[i] == ref [j] thenreturn 1/p(i ?
n) ?
(j ?
p);elsereturn 0;end ifend functionFigure 4: Compute Word Matching Score Basedon Gapssee that not only will strict n-grams get higherscores than non-consecutive sequences, but alsothe non-consecutive sequences with smaller gapswill get higher scores than those with larger gaps.This weighting method can help SIA capture moresubtle difference of MT outputs than ROUGE-Wdoes.
For example, if SIA is used to align mt1and ref in Figure 1, it will choose life is like boxinstead of life is like chocolate, because the aver-age distance of ?box-box?
to its previous mapping?like-like?
is less than ?chocolate-chocolate?.
Thenthe score SIA assigns to mt1 is:( 11 ?
1 +11 ?
1 +1?1 ?
2+ 1?2 ?
5)?18 = 0.399(1)For mt2, there is only one possible alignment,its score in SIA is computed as:( 11 ?
1 +11 ?
1 +1?1 ?
5+ 1?2 ?
3)?18 = 0.357(2)Thus, mt1 will be considered better than mt2 inSIA, which is reasonable.
As mentioned in sec-tion 1, though loose-sequence-based metrics givea better reflection of the sentence-wide similarityof the MT output and the reference, they cannot541make full use of word-level information.
This de-fect could potentially lead to a poor performancein adequacy evaluation, considering the case thatthe ignored words are crucial to the evaluation.
Inthe later part of this section, we will describe an it-erative alignment scheme which is meant to com-pensate for this defect.3.2 Stochastic Word MappingIn ROUGE and METEOR, PORTER-STEM andWORD-NET are used to increase the chance ofthe MT output words matching the references.We use a different stochastic approach in SIA toachieve the same purpose.
The string alignmenthas a good dynamic framework which allows thestochastic word matching to be easily incorporatedinto it.
The stochastic string alignment can be im-plemented by simply replacing the function COM-PUTE SCORE with the function of Figure 5.
Thefunction similarity(word1, word2) returns a ratiowhich reflects how similar the two words are.
Nowwe consider how to compute the similarity ratio oftwo words.
Our method is motivated by the phraseextraction method of Bannard and Callison-Burch(2005), which computes the similarity ratio of twowords by looking at their relationship with wordsin another language.
Given a bilingual parallelcorpus with aligned sentences, say English andFrench, the probability of an English word givena French word can be computed by training wordalignment models such as IBM Model4.
Then forevery English word e, we have a set of conditionalprobabilities given each French word: p(e|f1),p(e|f2), ... , p(e|fN ).
If we consider these proba-bilities as a vector, the similarities of two Englishwords can be obtained by computing the dot prod-uct of their corresponding vectors.2 The formulais described below:similarity(ei, ej) =N?k=1p(ei|fk)p(ej |fk) (3)Paraphrasing methods based on monolingual par-allel corpora such as (Pang et al, 2003; Barzilayand Lee, 2003) can also be used to compute thesimilarity ratio of two words, but they don?t haveas rich training resources as the bilingual methodsdo.2Although the marginalized probability (over all Frenchwords) of an English word given the other English word(PNk=1 p(ei|fk)p(fk|ej)) is a more intuitive way of measur-ing the similarity, the dot product of the vectors p(e|f) de-scribed above performed slightly better in our experiments.function STO COMPUTE SCORE(mt, ref, i, j, n, p)if mt[i] == ref [j] thenreturn 1/p(i ?
n) ?
(j ?
p);elsereturn similarity(mt[i],ref [i])?(i?n)?
(j?p);end ifend functionFigure 5: Compute Stochastic Word MatchingScore3.3 Iterative Alignment SchemeROUGE-W, METEOR, and WER all score MToutput by first computing a score based on eachavailable reference, and then taking the highestscore as the final score for the MT output.
Thisscheme has the problem of not being able to usemultiple references simultaneously.
The itera-tive alignment scheme proposed here is meant toalleviate this problem, by doing alignment be-tween the MT output and one of the available ref-erences until no more words in the MT outputcan be found in the references.
In each align-ment round, the score based on each referenceis computed and the highest one is taken as thescore for the round.
Then the words which havebeen aligned in best alignment will not be con-sidered in the next round.
With the same num-ber of aligned words, the MT output with feweralignment rounds should be considered better thanthose requiring more rounds.
For this reason, adecay factor ?
is multiplied with the scores ofeach round.
The final score of the MT output isthen computed by summing the weighted scoresof each alignment round.
The scheme is describedin Figure 6.The function GET ALIGN SCORE 1 usedin GET ALIGN SCORE IN MULTIPLE REFSis slightly different from GET ALIGN SCOREdescribed in the prior subsection.
The dynamicprogramming algorithm for getting the bestalignment is the same, except that it has two moretables as input, which record the unavailable po-sitions in the MT output and the reference.
Thesepositions have already been used in the prior bestalignments and should not be considered in theongoing alignment.
It also returns the alignedpositions of the best alignment.
The pseudocodefor GET ALIGN SCORE 1 is shown in Figure 7.The computation of the length penalty is similarto BLEU: it is set to 1 if length of the MT outputis longer than the arithmetic mean of length of the542function GET ALIGN SCORE IN MULTIPLE REFS(mt,ref 1, ..., ref N , ?).
Iteratively Compute the Alignment Score Based onMultiple References and the Decay Factor ?final score = 0;while max score != 0 dofor i = 1, ..., N do(score, align) =GET ALIGN SCORE 1(mt, ref i, mt table, ref tablei);if score > max score thenmax score = score;max align = align;max ref = i;end ifend forfinal score += max score ??;?
?
= ?
;Add the words in align to mt table andref tablemax ref ;end whilereturn final score?
length penalty;end functionFigure 6: Iterative Alignment Schemereferences, and otherwise is set to the ratio of thetwo.
Figure 8 shows how the iterative alignmentscheme works with an evaluation set containingone MT output and two references.
The selectedalignment in each round is shown, as well as theunavailable positions in MT output and refer-ences.
With the iterative scheme, every commonword between the MT output and the referenceset can make a contribution to the metric, andby such means SIA is able to make full use ofthe word-level information.
Furthermore, theorder (alignment round) in which the words arealigned provides a way to weight them.
In BLEU,multiple references can be used simultaneously,but the common n-grams are treated equally.4 ExperimentsEvaluation experiments were conducted to com-pare the performance of different metrics includ-ing BLEU, ROUGE, METEOR and SIA.3 The testdata for the experiments are from the MT evalu-ation workshop at ACL05.
There are seven setsof MT outputs (E09 E11 E12 E14 E15 E17 E22),all of which contain 919 English sentences.
Thesesentences are the translation of the same Chineseinput generated by seven different MT systems.The fluency and adequacy of each sentence aremanually ranked from 1 to 5.
For each MT output,there are two sets of human scores available, and3METEOR and ROUGE can be downloaded athttp://www.cs.cmu.edu/?alavie/METEOR andhttp://www.isi.edu/licensed-sw/see/rougefunction GET ALIGN SCORE1(mt, ref, mttable, reftable).
Compute the alignment score of the MT output mtwith length M and the reference ref with length N, withoutconsidering the positions in mttable and reftableM = |mt|; N = |ref |;for i = 1; i ?
M; i = i +1 dofor j = 1; j ?
N; j = j +1 dofor k = 1; k ?
i; k = k +1 dofor m = 1; m ?
j; m = m +1 doscorei,j,k,m= max{scorei?1,j,k,m, scorei,j?1,k,m};end forend forif i is not in mttable and j is not in reftable thenscorei,j,i,j = maxn=1,M ;p=1,N{scorei,j,i,j ,scorei?1,j?1,n,p + COMPUTE SCORE(mt, ref, i, j, n, p)};end ifend forend forreturn scoreM,N,M,NM and the corresponding alignment;end functionFigure 7: Alignment Algorithm Based on GapsWithout Considering Aligned Positionsm: England with France discussed this crisis in Londonr1: Britain and France consulted about this crisis in London with each otherr2: England and France discussed the crisis in Londonm: England with France discussed this crisis in Londonr2: England and France discussed the crisis in Londonr1: Britain and France consulted about this crisis in London with each otherm: England with France discussed this crisis in Londonr1: Britain and France consulted about this crisis in London with each otherr2: England and France discussed the crisis in LondonFigure 8: Alignment Example for SIA543we randomly choose one as the score used in theexperiments.
The human overall scores are calcu-lated as the arithmetic means of the human fluencyscores and adequacy scores.
There are four setsof human translations (E01, E02, E03, E04) serv-ing as references for those MT outputs.
The MToutputs and reference sentences are transformed tolower case.
Our experiments are carried out as fol-lows: automatic metrics are used to evaluate theMT outputs based on the four sets of references,and the Pearson?s correlation coefficient of the au-tomatic scores and the human scores is computedto see how well they agree.4.1 N -gram vs.
Loose SequenceOne of the problems addressed in this paper isthe different performance of n-gram based metricsand loose-sequence-based metrics in sentence-level evaluation.
To see how they really differin experiments, we choose BLEU and ROUGE-W as the representative metrics for the two types,and used them to evaluate the 6433 sentences inthe 7 MT outputs.
The Pearson correlation coeffi-cients are then computed based on the 6433 sam-ples.
The experimental results are shown in Ta-ble 1.
BLEU-n denotes the BLEU metric withthe longest n-gram of length n. F denotes flu-ency, A denotes adequacy, and O denotes overall.We see that with the increase of n-gram length,BLEU?s performance does not increase monoton-ically.
The best result in adequacy evaluation isachieved at 2-gram and the best result in fluency isachieved at 4-gram.
Using n-grams longer than 2doesn?t buy much improvement for BLEU in flu-ency evaluation, and does not compensate for theloss in adequacy evaluation.
This confirms Liu andGildea (2005)?s finding that in sentence level eval-uation, long n-grams in BLEU are not beneficial.The loose-sequence-based ROUGE-W does muchbetter than BLEU in fluency evaluation, but it doespoorly in adequacy evaluation and doesn?t achievea significant improvement in overall evaluation.We speculate that the reason is that ROUGE-Wdoesn?t make full use of the available word-levelinformation.4.2 METEOR vs. SIASIA is designed to take the advantage of loose-sequence-based metrics without losing word-levelinformation.
To see how well it works, we chooseE09 as the development set and the sentences inthe other 6 sets as the test data.
The decay fac-B-3 R 1 R 2 M SF 0.167 0.152 0.192 0.167 0.202A 0.306 0.304 0.287 0.332 0.322O 0.265 0.256 0.266 0.280 0.292Table 2: Sentence level evaluation results ofBLEU, ROUGE, METEOR and SIAtor in SIA is determined by optimizing the over-all evaluation for E09, and then used with SIAto evaluate the other 5514 sentences based on thefour sets of references.
The similarity of Englishwords is computed by training IBM Model 4 inan English-French parallel corpus which containsseven hundred thousand sentence pairs.
For everyEnglish word, only the entries of the top 100 mostsimilar English words are kept and the similarityratios of them are then re-normalized.
The wordsoutside the training corpus will be considered asonly having itself as its similar word.
To com-pare the performance of SIA with BLEU, ROUGEand METEOR, the evaluation results based onthe same testing data is given in Table 2.
B-3 denotes BLEU-3; R 1 denotes the skipped bi-gram based ROUGE metric which considers allskip distances and uses PORTER-STEM; R 2 de-notes ROUGE-W with PORTER-STEM; M de-notes the METEOR metric using PORTER-STEMand WORD-NET synonym; S denotes SIA.We see that METEOR, as the other metricsitting in the middle of n-gram based metricsand loose sequence metrics, achieves improve-ment over BLEU in both adequacy and fluencyevaluation.
Though METEOR gets the best re-sults in adequacy evaluation, in fluency evaluation,it is worse than the loose-sequence-based metricROUGE-W-STEM.
SIA is the only one amongthe 5 metrics which does well in both fluency andadequacy evaluation.
It achieves the best results influency evaluation and comparable results to ME-TEOR in adequacy evaluation, and the balancedperformance leads to the best overall evaluationresults in the experiment.
To estimate the signif-icance of the correlations, bootstrap resampling(Koehn, 2004) is used to randomly select 5514sentences with replacement out of the whole testset of 5514 sentences, and then the correlation co-efficients are computed based on the selected sen-tence set.
The resampling is repeated 5000 times,and the 95% confidence intervals are shown in Ta-bles 3, 4, and 5.
We can see that it is very diffi-544BLEU-1 BLEU-2 BLEU-3 BLEU-4 BLEU-5 BLEU-6 ROUGE-WF 0.147 0.162 0.166 0.168 0.165 0.164 0.191A 0.288 0.296 0.291 0.285 0.279 0.274 0.268O 0.243 0.256 0.255 0.251 0.247 0.244 0.254Table 1: Sentence level evaluation results of BLEU and ROUGE-Wlow mean highB-3 (-16.6%) 0.138 0.165 0.192 (+16.4%)R 1 (-17.8%) 0.124 0.151 0.177 (+17.3%)R 2 (-14.3%) 0.164 0.191 0.218 (+14.2%)M (-15.8%) 0.139 0.166 0.191 (+15.5%)S (-13.3%) 0.174 0.201 0.227 (+13.3%)Table 3: 95% significance intervals for sentence-level fluency evaluationlow mean highB-3 (-08.2%) 0.280 0.306 0.330 (+08.1%)R 1 (-08.5%) 0.278 0.304 0.329 (+08.4%)R 2 (-09.2%) 0.259 0.285 0.312 (+09.5%)M (-07.3%) 0.307 0.332 0.355 (+07.0%)S (-07.9%) 0.295 0.321 0.346 (+07.8%)Table 4: 95% significance intervals for sentence-level adequacy evaluationcult for one metric to significantly outperform an-other metric in sentence-level evaluation.
The re-sults show that the mean of the correlation factorsconverges right to the value we computed based onthe whole testing set, and the confidence intervalscorrelate with the means.While sentence-level evaluation is useful if weare interested in a confidence measure on MT out-puts, syste-x level evaluation is more useful forcomparing MT systems and guiding their develop-ment.
Thus we also present the evaluation resultsbased on the 7 MT output sets in Table 6.
SIA usesthe same decay factor as in the sentence-level eval-uation.
Its system-level score is computed as thearithmetic mean of the sentence level scores, andlow mean highB-3 (-09.8%) 0.238 0.264 0.290 (+09.9%)R 1 (-10.2%) 0.229 0.255 0.281 (+10.0%)R 2 (-10.0%) 0.238 0.265 0.293 (+10.4%)M (-09.0%) 0.254 0.279 0.304 (+08.8%)S (-08.7%) 0.265 0.291 0.316 (+08.8%)Table 5: 95% significance intervals for sentence-level overall evaluationWLS WLS WLS WLSPROB INCS PROBINCSF 0.189 0.202 0.188 0.202A 0.295 0.310 0.311 0.322O 0.270 0.285 0.278 0.292Table 7: Results of different components in SIAWLS WLS WLS WLSINCS INCS INCS INCSSTEM WN STEMWNF 0.188 0.188 0.187 0.191A 0.311 0.313 0.310 0.317O 0.278 0.280 0.277 0.284Table 8: Results of SIA working with Porter-Stemand WordNetso are ROUGE, METEOR and the human judg-ments.
We can see that SIA achieves the best per-formance in both fluency and adequacy evaluationof the 7 systems.
Though the 7-sample based re-sults are not reliable, we can get a sense of howwell SIA works in the system-level evaluation.4.3 Components in SIATo see how the three components in SIA con-tribute to the final performance, we conduct exper-iments where one or two components are removedin SIA, shown in Table 7.
The three componentsare denoted as WLS (weighted loose sequencealignment), PROB (stochastic word matching),and INCS (iterative alignment scheme) respec-tively.
WLS without INCS does only one roundof alignment and chooses the best alignment scoreas the final score.
This scheme is similar toROUGE-W and METEOR.
We can see that INCS,as expected, improves the adequacy evaluationwithout hurting the fluency evaluation.
PROBimproves both adequacy and fluency evaluationperformance.
The result that SIA works withPORTER-STEM and WordNet is also shown inTable 8.
When PORTER-STEM and WordNet are545B-6 R 1 R 2 M SF 0.514 0.466 0.458 0.378 0.532A 0.876 0.900 0.906 0.875 0.928O 0.794 0.790 0.792 0.741 0.835Table 6: Results of BLEU, ROUGE, METEOR and SIA in system level evaluationboth used, PORTER-STEM is used first.
We cansee that they are not as good as using the stochasticword matching.
Since INCS and PROB are inde-pendent of WLS, we believe they can also be usedto improve other metrics such as ROUGE-W andMETEOR.5 ConclusionThis paper describes a new metric SIA for MTevaluation, which achieves good performance bycombining the advantages of n-gram-based met-rics and loose-sequence-based metrics.
SIA usesstochastic word mapping to allow soft or partialmatches between the MT hypotheses and the ref-erences.
This stochastic component is shown tobe better than PORTER-STEM and WordNet inour experiments.
We also analyzed the effect ofother components in SIA and speculate that theycan also be used in other metrics to improve theirperformance.Acknowledgments This work was supportedby NSF ITR IIS-09325646 and NSF ITR IIS-0428020.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
Meteor: Anautomatic metric for mt evaluation with improvedcorrelation with human judegments.
In Proceed-ings of the ACL-04 workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, Ann Arbor, Michigan.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Pro-ceedings of the 43rd Annual Conference of the As-sociation for Computational Linguistics (ACL-05).Regina Barzilay and Lillian Lee.
2003.
Learningto paraphrase: An unsupervised approach usingmultiple-sequence alignment.
In Proceedings of the2003 Meeting of the North American chapter of theAssociation for Computational Linguistics (NAACL-03), pages 16?23.John Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, AlbertoSanchis, and Nicola Ueffing.
2003.
Confidence es-timation for machine translation.
Technical report,Center for Language and Speech Processing, JohnsHopkins University, Baltimore.
Summer WorkshopFinal Report.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics,19(2):263?311.G.
Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In In HLT 2002, Human Lan-guage Technology Conference, San Diego, CA.Philipp Koehn.
2004.
Statistical significance testsfor machine translation evaluation.
In 2004 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 388?395, Barcelona,Spain, July.Chin-Yew Lin and Franz Josef Och.
2004.
Auto-matic evaluation of machine translation quality us-ing longest common subsequence and skip-bigramstatistics.
In Proceedings of the 42th Annual Confer-ence of the Association for Computational Linguis-tics (ACL-04), Barcelona, Spain.Ding Liu and Daniel Gildea.
2005.
Syntactic fea-tures for evaluation of machine translation.
In ACL2005 Workshop on Intrinsic and Extrinsic Evalua-tion Measures for Machine Translation and/or Sum-marization.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations:Extracting paraphrases and generating new sen-tences.
In Proceedings of the 2003 Meeting of theNorth American chapter of the Association for Com-putational Linguistics (NAACL-03).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Conference of the Association for Com-putational Linguistics (ACL-02), Philadelphia, PA.546
