Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 19?28,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsSAMAR: A System for Subjectivity and Sentiment Analysis of Arabic SocialMediaMuhammad Abdul-Mageed, Sandra Ku?blerIndiana UniversityBloomington, IN, USA{mabdulma,skuebler}@indiana.eduMona DiabColumbia UniversityNew York, NY, USAmdiab@ccls.columbia.eduAbstractIn this work, we present SAMAR, a sys-tem for Subjectivity and Sentiment Analysis(SSA) for Arabic social media genres.
Weinvestigate: how to best represent lexical in-formation; whether standard features are use-ful; how to treat Arabic dialects; and, whethergenre specific features have a measurable im-pact on performance.
Our results suggest thatwe need individualized solutions for each do-main and task, but that lemmatization is a fea-ture in all the best approaches.1 IntroductionIn natural language, subjectivity refers to aspects oflanguage used to express opinions, feelings, eval-uations, and speculations (Banfield, 1982) and, assuch, it incorporates sentiment.
The process of sub-jectivity classification refers to the task of classify-ing texts as either objective (e.g., The new iPhonewas released.)
or subjective.
Subjective text canfurther be classified with sentiment or polarity.
Forsentiment classification, the task consists of iden-tifying whether a subjective text is positive (e.g.,The Syrians continue to inspire the world with theircourage!
), negative (e.g., The bloodbaths in Syriaare horrifying!
), neutral (e.g., Obama may sign thebill.
), or, sometimes, mixed (e.g., The iPad is cool,but way too expensive).In this work, we address two main issues in Sub-jectivity and Sentiment Analysis (SSA): First, SSAhas mainly been conducted on a small number ofgenres such as newspaper text, customer reports,and blogs.
This excludes, for example, social me-dia genres (such as Wikipedia Talk Pages).
Second,despite increased interest in the area of SSA, onlyfew attempts have been made to build SSA systemsfor morphologically-rich languages (Abbasi et al,2008; Abdul-Mageed et al, 2011b), i.e.
languagesin which a significant amount of information con-cerning syntactic units and relations is expressed atthe word-level, such as Finnish or Arabic.
We thusaim at partially bridging these two gaps in researchby developing an SSA system for Arabic, a mor-phologically highly complex languages (Diab et al,2007; Habash et al, 2009).
We present SAMAR, asentence-level SSA system for Arabic social mediatexts.
We explore the SSA task on four different gen-res: chat, Twitter, Web forums, and Wikipedia TalkPages.
These genres vary considerably in terms oftheir functions and the language variety employed.While the chat genre is overridingly in dialectal Ara-bic (DA), the other genres are mixed between Mod-ern Standard Arabic (MSA) and DA in varying de-grees.
In addition to working on multiple genres,SAMAR handles Arabic that goes beyond MSA.1.1 Research QuestionsIn the current work, we focus on investigating fourmain research questions:?
RQ1: How can morphological richness betreated in the context of Arabic SSA??
RQ2: Can standard features be used for SSAfor social media despite the inherently shorttexts typically used in these genres??
RQ3: How do we treat dialects?19?
RQ4: Which features specific to social mediacan we leverage?RQ1 is concerned with the fact that SSA hasmainly been conducted for English, which has lit-tle morphological variation.
Since the features usedin machine learning experiments for SSA are highlylexicalized, a direct application of these methods isnot possible for a language such as Arabic, in whichone lemma can be associated with thousands of sur-face forms.
For this reason, we need to investigatehow to avoid data sparseness resulting from usinglexical features without losing information that isimportant for SSA.
More specifically, we concen-trate on two questions: Since we need to reduceword forms to base forms to combat data sparseness,is it more useful to use tokenization or lemmatiza-tion?
And given that the part-of-speech (POS) tagsetfor Arabic contains a fair amount of morphologicalinformation, how much of this information is usefulfor SSA?
More specifically, we investigate two dif-ferent reduced tagsets, the RTS and the ERTS.
Formore detailed information see section 4.RQ2 addresses the impact of using two stan-dard features, frequently employed in SSA studies(Wiebe et al, 2004; Turney, 2002), on social mediadata, which exhibit DA usage and text length vari-ations, e.g.
in twitter data.
First, we investigate theutility of applying a UNIQUE feature (Wiebe et al,2004) where low frequency words below a thresh-old are replaced with the token ?UNIQUE?.
Giventhat our data includes very short posts (e.g., twitterdata has a limit of only 140 characters per tweet),it is questionable whether the UNIQUE feature willbe useful or whether it replaces too many contentwords.
Second, we test whether a polarity lexiconextracted in a standard domain using Modern Stan-dard Arabic (MSA) transfers to social media data.Third, given the inherent lack of a standardized or-thography for DA, the problem of replacing contentwords is expected to be increased since many DAcontent words would be spelled in different ways.RQ3 is concerned with the fact that for Arabic,there are significant differences between dialects.However, existing NLP tools such as tokenizers andPOS taggers are exclusively trained on and for MSA.We thus investigate whether using an explicit featurethat identifies the dialect of the text improves SSAperformance.RQ4 is concerned with attempting to improveSSA performance, which suffers from the problemsdescribed above, by leveraging information that istypical for social media genres, such as author orgender information.The rest of the paper is organized as follows: InSection 2, we review related work.
Section 3 de-scribes the social media corpora and the polarity lex-icon used in the experiments, Section 4 describesSAMAR, the SSA system and the features used inthe experiments.
Section 5 describes the experi-ments and discusses the results.
In Section 6, wegive an overview of the best settings for the differ-ent corpora, followed by a conclusion in Section 7.2 Related WorkThe bulk of SSA work has focused on movie andproduct reviews (Dave et al, 2003; Hu and Liu,2004; Turney, 2002).
A number of sentence- andphrase-level classifiers have been built: For exam-ple, whereas Yi et al (2003) present a system thatdetects sentiment toward a given subject, Kim andHovy?s (2004) system detects sentiment towards aspecific, predefined topic.
Our work is similar to Yuand Hatzivassiloglou (2003) and Wiebe et al (1999)in that we use lexical and POS features.Only few studies have been performed on Arabic.Abbasi et al (2008) use a genetic algorithm for bothEnglish and Arabic Web forums sentiment detectionon the document level.
They exploit both syntacticand stylistic features, but do not use morphologicalfeatures.
Their system is not directly comparable toours due to the difference in data sets.More related toour work is our previous effort (2011b) in which webuilt an SSA system that exploits newswire data.
Wereport a slight system improvement using the gold-labeled morphological features and a significant im-provement when we use features based on a polaritylexicon from the news domain.
In that work, oursystem performs at 71.54% F for subjectivity classi-fication and 95.52% F for sentiment detection.
Thiscurrent work is an extension on our previous workhowever it differs in that we use automatically pre-dicted morphological features and work on data be-longing to more genres and DA varieties, hence ad-dressing a more challenging task.203 Data Sets and AnnotationTo our knowledge, no gold-labeled social mediaSSA data exist.
Thereby, we create annotated datacomprising a variety of data sets:DARDASHA (DAR): (Arabic for ?chat?)
com-prises the first 2798 chat turns collected from a ran-domly selected chat session from ?Egypt?s room?
inMaktoob chat chat.mymaktoob.com.
Maktoobis a popular Arabic portal.
DAR is an Egyptian Ara-bic subset of a larger chat corpus that was harvestedbetween December 2008 and February 2010.TAGREED (TGRD): (?tweeting?)
is a corpusof 3015 Arabic tweets collected during May 2010.TRGD has a mixture of MSA and DA.
The MSApart (TRGD-MSA) has 1466 tweets, and the dialec-tal part (TRGD-DA) has 1549 tweets.TAHRIR (THR): (?editing?)
is a corpus of 3008sentences sampled from a larger pool of 30 MSAWikipedia Talk Pages that we harvested.MONTADA (MONT): (?forum?)
comprises of3097 Web forum sentences collected from a largerpool of threaded conversations pertaining to differ-ent varieties of Arabic, including both MSA and DA,from the COLABA data set (Diab et al, 2010).
Thediscussions covered in the forums pertain to socialissues, religion or politics.
The sentences were au-tomatically filtered to exclude non-MSA threads.Each of the data sets was labeled at the sentencelevel by two college-educated native speakers ofArabic.
For each sentence, the annotators assignedone of 3 possible labels: (1) objective (OBJ), (2)subjective-positive (S-POS), (3) subjective-negative(S-NEG), and (3) subjective-mixed (S-MIXED).Following (Wiebe et al, 1999), if the primary goalof a sentence is judged as the objective reportingof information, it was labeled as OBJ.
Otherwise, asentence was a candidate for one of the three SUBJclasses.
We also labeled the data with a number ofother metadata1 tags.
Metadata labels included theuser gender (GEN), the user identity (UID) (e.g.
theuser could be a person or an organization), and thesource document ID (DID).
We also mark the lan-guage variety (LV) (i.e., MSA or DA) used, taggedat the level of each unit of analysis (i.e., sentence,tweet, etc.).
Annotators were instructed to label a1We use the term ?metadata?
as an approximation, as somefeatures are more related to social interaction phenomena.Data set SUBJ GEN LV UID DIDDAR X XMONT X X XTRGD X X X XTHR X XTable 1: Types of annotation labels (features) manuallyassigned to the data.tweet as MSA if it mainly employs MSA words andadheres syntactically to MSA rules, otherwise it istreated as dialectal.
Table 1 shows the annotationsfor each data set.
Data statistics, distribution ofclasses, and inter-annotator agreement in terms ofKappa (K) are provided in Table 2.Polarity Lexicon: We manually created a lexiconof 3982 adjectives labeled with one of the followingtags {positive, negative, neutral}, as is reported inour previous work (2011b).
We focus on adjectivessince they are primary sentiment bearers.
The ad-jectives pertain to the newswire domain, and wereextracted from the first four parts of the Penn ArabicTreebank (Maamouri et al, 2004).4 SAMAR4.1 Automatic ClassificationSAMAR is a machine learning system for ArabicSSA.
For classification, we use SVMlight (Joachims,2008).
In our experiments, we found that linear ker-nels yield the best performance.
We perform all ex-periments with presence vectors: In each sentencevector, the value of each dimension is binary, regard-less of how many times a feature occurs.In the current study, we adopt a two-stage clas-sification approach.
In the first stage (i.e., Subjec-tivity), we build a binary classifier to separate objec-tive from subjective cases.
For the second stage (i.e.,Sentiment) we apply binary classification that distin-guishes S-POS from S-NEG cases.
We disregard theneutral and mixed classes for this study.
SAMARuses different feature sets, each of which is designedto address an individual research question:4.2 Morphological FeaturesWord forms: In order to minimize data sparse-ness as a result of the morphological richness ofArabic, we tokenize the text automatically.
Weuse AMIRA (Diab, 2009), a suite for automatic21Data set # instances # types # tokens # OBJ # S-POS # S-NEG # S-MIXED Kappa (K)DAR 2,798 11,810 3,133 328 1647 726 97 0.89MONT 3,097 82,545 20,003 576 1,101 1,027 393 0.88TRGD 3,015 63,383 16,894 1,428 483 759 345 0.85TRGD-MSA 1,466 31,771 9,802 960 226 186 94 0.85TRGD-DIA 1,549 31,940 10,398 468 257 573 251 0.82THR 3,008 49,425 10,489 1,206 652 1,014 136 0.85Table 2: Data and inter-annotator agreement statistics.processing of MSA, trained on Penn Arabic Tree-bank (Maamouri et al, 2004) data, which consistsof newswire text.
We experiment with two differentconfigurations to extract base forms of words: (1)Token (TOK), where the stems are left as is with nofurther processing of the morpho-tactics that resultfrom the segmentation of clitics; (2) Lemma (LEM),where the words are reduced to their lemma forms,(citation forms): for verbs, this is the 3rd personmasculine singular perfective form and for nouns,this corresponds to the singular default form (typi-cally masculine).
For example, the word ?
?EA 	J?m'.
?
(wbHsnAtHm) is tokenized as ?
+ H. +HAJ?k + ??
(w+b+HsnAt+Hm) (note that in TOK, AMIRA doesnot split off the pluralizing suffix H@ (At) from thestem 	?
?k (Hsn)), while in the lemmatization stepby AMIRA, the lemma rendered is ?
J?k (Hsnp).Thus, SAMAR uses the form of the word as Hsnpin the LEM setting, and HsnAt in the TOK setting.POS tagging: Since we use only the base formsof words, the question arises whether we lose mean-ingful morphological information and consequentlywhether we could represent this information in thePOS tags instead.
Thus, we use two sets of POSfeatures that are specific to Arabic: the reducedtag set (RTS) and the extended reduced tag set(ERTS) (Diab, 2009).
The RTS is composed of 42tags and reflects only number for nouns and sometense information for verbs whereas the ERTS com-prises 115 tags and enriches the RTS with gender,number, and definiteness information.
Diab (2007b;2007a) shows that using the ERTS improves re-sults for higher processing tasks such as base phrasechunking of Arabic.4.3 Standard FeaturesThis group includes two features that have been em-ployed in various SSA studies.Unique: Following Wiebe et al (2004), we ap-ply a UNIQUE (Q) feature: We replace low fre-quency words with the token ?UNIQUE?.
Exper-iments showed that setting the frequency thresholdto 3 yields the best results.Polarity Lexicon (PL): The lexicon (cf.
section3) is used in two different forms for the two tasks:For subjectivity classification, we follow Bruce andWiebe (1999; 2011b) and add a binary has adjectivefeature indicating whether or not any of the ad-jectives in the sentence is part of our manuallycreated polarity lexicon.
For sentiment classifica-tion, we apply two features, has POS adjective andhas NEG adjective.
These binary features indicatewhether a POS or NEG adjective from the lexiconoccurs in a sentence.4.4 Dialectal Arabic FeaturesDialect: We apply the two gold language varietyfeatures, {MSA, DA}, on the Twitter data set to rep-resent whether the tweet is in MSA or in a dialect.4.5 Genre Specific FeaturesGender: Inspired by gender variation research ex-ploiting social media data (e.g., (Herring, 1996)),we apply three gender (GEN) features correspond-ing to the set {MALE, FEMALE, UNKNOWN}.Abdul-Mageed and Diab (2012a) suggest that thereis a relationship between politeness strategies andsentiment expression.
And gender variation researchin social media shows that expression of linguisticpoliteness (Brown and Levinson, 1987) differs basedon the gender of the user.User ID: The user ID (UID) labels are inspiredby research on Arabic Twitter showing that a consid-erable share of tweets is produced by organizationssuch as news agencies (Abdul-Mageed et al, 2011a)as opposed to lay users.
We hence employ two fea-tures from the set {PERSON, ORGANIZATION} to22classification of the Twitter data set.
The assumptionis that tweets by persons will have a higher correla-tion with expression of sentiment.Document ID: Projecting a document ID (DID)feature to the paragraph level was shown to im-prove subjectivity classification on data from thehealth policy domain (Abdul-Mageed et al, 2011c).Hence, by employing DID at the instance level, weare investigating the utility of this feature for socialmedia as well as at a finer level of analysis, i.e., thesentence level.5 Empirical EvaluationFor each data set, we divide the data into 80% train-ing (TRAIN), 10% for development (DEV), and10% for testing (TEST).
The classifier was opti-mized on the DEV set; all results that we report be-low are on TEST.
In each case, our baseline is themajority class in the training set.
We report accu-racy as well as the F scores for the individual classes(objective vs. subjective and positive vs. negative).5.1 Impact of Morphology on SSAWe run two experimental conditions: 1.
A compari-son of TOK to LEM (cf.
sec.
4.2); 2.
A combinationof RTS and ERTS with TOK and LEM.TOK vs. LEM: Table 3 shows the results for themorphological preprocessing conditions.
The base-line, Base, is the majority class in the training data.For all data sets, Subjective is the majority class.For subjectivity classification we see varying per-formance.
DAR: TOK outperforms LEM for allmetrics, yet performance is below Base.
TGRD:LEM preprocessing yields better accuracy resultsthan Base.
LEM is consistently better than TOKfor all metrics.
THR: We see the opposite perfor-mance compared to the TGRD data set where TOKoutperforms LEM and also outperforming Base.
Fi-nally for MONT: the performance of LEM and TOKare exactly the same yielding the same results as inBase.For sentiment classification, the majority classis positive for DAR and MONT and negative forTGRD and THR.
We note that there are no obvi-ous trends between TOK and LEM.
DAR: we ob-serve better performance of LEM over Base andSUBJ SENTIData Cond.
Acc F-O F-S Acc F-P F-NDAR Base 84.75 0.00 91.24 63.02 77.32 0.00TOK 83.90 0.00 91.24 67.71 77.04 45.61LEM 83.76 0.00 91.16 70.16 78.65 50.43TRGD Base 61.59 0.00 76.23 56.45 0.00 72.16TOK 69.54 64.06 73.56 65.32 49.41 73.62LEM 71.19 64.78 75.63 62.10 41.98 71.86THR Base 52.92 0.00 69.21 75.00 0.00 85.71TOK 58.44 28.09 70.78 60.47 37.04 71.19LEM 57.79 26.97 70.32 63.37 38.83 73.86MONT Base 83.44 0.00 90.97 86.82 92.94 0.00TOK 83.44 0.00 90.97 74.55 83.63 42.86LEM 83.44 0.00 90.97 72.27 81.68 42.99Table 3: SSA results with preprocessing TOK and LEM.TOK.
TGRD: Both preprocessing schemes outper-form Base on all metrics with TOK outperformingLEM across the board.
THR: LEM outperformsTOK for all metrics of sentiment, yet they are be-low Base performance.
MONT: TOK outperformsLEM in terms of accuracy, and positive sentiment,yet LEM slightly outperforms TOK for negative sen-timent classification.
Both TOK and LEM are beatby Base in terms of accuracy and positive classifica-tion.
Given the observed results, we observe no cleartrends for the impact for morphological preprocess-ing alone on performance.Adding POS tags: Table 4 shows the results ofadding POS tags based on the two tagsets RTSand ERTS.
Subjectivity classification: The resultsshow that adding POS information improves ac-curacy and F score for all the data sets exceptMONT which is still at Base performance.
RTSoutperforms ERTS with TOK, and the opposite withLEM where ERTS outperforms RTS, however, over-all TOK+RTS yields the highest performance of91.49% F score on subjectivity classification for theDAR dataset.
For the TGRD and THR data sets, wenote that TOK+ERTS is equal to or outperforms theother conditions on subjectivity classification.
ForMONT there is no difference between experimentalconditions and no impact for adding the POS tag in-formation.
In the sentiment classification task:The sentiment task shows a different trend: here,the highest performing systems do not use POS tags.This is attributed to the variation in genre betweenthe training data on which AMIRA is trained (MSAnewswire) and the data sets we are experimentingwith in this work.
However in relative compari-23SUBJ SENTIData Cond.
Acc F-O F-S Acc F-P F-NDAR Base 84.75 91.24 63.02 77.32TOK+RTS 84.32 0.00 91.49 66.15 76.36 40.37TOK+ERTS 83.90 0.00 91.24 67.19 77.09 42.20LEM+RTS 83.47 0.00 90.99 67.71 77.21 44.64LEM+ERTS 83.47 0.00 90.99 68.75 77.94 46.43TGRD Base 61.59 76.23 56.45 72.16TOK+RTS 70.20 64.57 74.29 62.90 43.90 72.29TOK+ERTS 71.19 65.06 75.49 62.90 42.50 72.62LEM+RTS 70.20 64.57 74.29 62.90 46.51 71.60LEM+ERTS 72.19 76.54 71.19 65.32 48.19 73.94THR Base 52.92 69.21 75.00 85.71TOK+RTS 57.47 28.42 69.75 59.30 33.96 70.59TOK+ERTS 59.42 28.57 71.66 59.88 38.94 70.13LEM+RTS 59.42 28.57 71.66 59.88 33.01 71.37LEM+ERTS 58.77 25.73 71.46 60.47 37.04 71.19MONT Base 83.44 90.97 86.82 92.94TOK+RTS 83.44 0.00 90.97 69.09 79.27 39.29TOK+ERTS 83.44 0.00 90.97 71.82 81.55 40.38LEM+RTS 83.44 0.00 90.97 70.00 80.36 36.54LEM+ERTS 83.44 0.00 90.97 69.55 79.64 39.64Table 4: SSA results with different morphological preprocessing and POS features.son between RTS and ERTS for sentiment showsthat in a majority of the cases, ERTS outperformsRTS, thus indicating that the additional morpholog-ical features are helpful.
One possible explanationmay be that variations of some of the morphologi-cal features (e.g., existence of a gender, person, ad-jective feature) may correlate more frequently withpositive or negative sentiment.5.2 Standard Features for Social Media DataRQ2 concerns the question whether standard fea-tures can be used successfully for classifying socialmedia text characterized by the usage of dialect andby differing text lengths.
We add the standard fea-tures, polarity (PL) and UNIQUE (Q), to the two to-kenization schemes and the POS tag sets.
We reportonly the best performing conditions here.Table 5 shows the best performing settings percorpus from the previous section as well as the bestperforming setting given the new features.
The re-sults show that apart from THR and TGRD for sen-timent, all corpora gain in accuracy for both sub-jectivity and sentiment.
In the case of subjectiv-ity, while considerable improvements are gained forboth DAR (11.51% accuracy) and THR (32.90% ac-curacy), only slight improvements (< 1% accuracy)are reached for both TGRD and MONT.
For sen-timent classification, the improvements in accuracyare less than the case of subjectivity: 1.84% for DARand 6.81% for MONT.
The deterioration on THR issurprising and may be a result of the nature of sen-timent as expressed in the THR data set: Wikipediahas a ?Neutral Point of View?
policy based on whichusers are required to focus their contributions noton other users but content, and as such sentiment isexpressed in nuanced indirect ways in THR.
Whilethe subjectivity results show that it is feasible to usethe combination of the UNIQUE feature and the po-larity lexicon features successfully, even for shortertexts, such as in the twitter data (TGRD), this con-clusion does not always hold for sentiment classi-fication.
However, we assume that the use of thepolarity lexicon would result in higher gains if thelexicon were adapted to the new domains.5.3 SSA Given Arabic DialectsRQ3 investigates how much the results of SSA areaffected by the presence or absence of dialectal Ara-bic in the data.
For this question, we focus on theTGRD data set because it contains a non-negligibleamount (i.e., 48.62%) of tweets in dialect.First, we investigate how our results change whenwe split the TGRD data set into two subsets, onecontaining only MSA, the other one containing onlyDA.
We extract the 80-10-10% data split, then trainand test the classifier exclusively on either MSA ordialect data.
The subjectivity results for this exper-iment are shown in Table 6, and the sentiment re-24SUBJ SENTIData Best condition Acc F-O F-S Best condition Acc F-P F-NDAR TOK+RTS 84.32 0.00 91.49 LEM+ERTS 68.75 77.94 46.43TOK+ERTS+PL+Q3 95.83 0.00 97.87 LEM+ERTS+PL+Q3 70.59 79.51 47.92TGRD LEM+ERTS 72.19 76.54 71.19 LEM+ERTS 65.32 73.94 48.19LEM+ERTS+PL 72.52 65.84 77.01 LEM+ERTS+PL 65.32 73.94 48.19THR L./T.+ERTS 59.42 28.57 71.66 LEM+ERTS 63.37 38.83 73.86TOK+ERTS +PL+Q3 83.33 0.00 90.91 LEM+RTS+PL+Q3 61.05 34.95 72.20MONT LEM+ERTS 83.44 0.00 90.97 TOK 74.55 83.63 42.86LEM+RTS+PL+Q3 84.19 3.92 91.39 TOK+PL+Q3 81.36 88.64 48.10Table 5: SSA results with standard features.
Number in bold signify improvements over the best results in section 5.1.TGRD TGRD-MSA TGRD-DACond.
Acc F-O F-S Acc F-O F-S Acc F-O F-SBase 61.59 0.00 76.23 51.68 68.14 0.00 78.40 0.00 87.89TOK 69.54 64.06 73.56 61.74 70.16 46.73 78.40 5.41 87.80LEM 71.19 64.78 75.63 65.10 72.04 53.57 79.01 15.00 88.03Table 6: Dialect-specific subjectivity experiments.sults are shown in Table 7.
For both tasks, the re-sults show considerable differences between MSAand DA: For TGRD-MSA, the results are lower thanfor TGRD-DA, which is a direct consequence ofthe difference in distribution of subjectivity betweenthe two subcorpora.
TGRD-DA is mostly subjectivewhile TGRD-MSA is more balanced.
With regardto sentiment, TGRD-DA consists of mostly negativetweets while TGRD-MSA again is more balanced.These results suggest that knowing whether a tweetis in dialect would help classification.For subjectivity, we can see that TGRD-MSA im-proves by 13.5% over the baseline while for TGRD-DA, the improvement is more moderate, < 3%.
Weassume that this is partly due to the higher skew inTGRD-DA, moreover, it is known that our prepro-cessing tools yield better performance on MSA dataleading to better tokenization and lemmatization.For sentiment classification on TGRD-MSA, nei-ther tokenization nor lemmatization improve overthe baseline.
This is somewhat surprising since weexpect AMIRA to work well on this data set and thusto lead to better classification results.
However, aconsiderable extent of the MSA tweets are expectedto come from news headlines (Abdul-Mageed etal., 2011a), and headlines usually are not loci of ex-plicitly subjective content and hence are difficult toclassify and in essence harder to preprocess sincethe genre is different from regular newswire even ifMSA.
For the TGRD-DA data set, both lemmatiza-tion and tokenization improve over the baseline.The results for both subjectivity and sentiment onthe MSA and DA sets suggest that processing errorsby AMIRA trained exclusively on MSA newswiredata) result in deteriorated performance.
Howeverwe do not observe such trends on the TGRD-DAdata sets.
This is not surprising since the TGRD-DA is not very different from the newswire data onwhich AMIRA was trained: Twitter users discusscurrent events topics also discussed in newswire.There is also a considerable lexical overlap betweenMSA and DA.
Furthermore, dialectal data may beloci for more sentiment cues like emoticons, certainpunctuation marks (e.g.
exclamation marks), etc.Such clues are usually absent (or less frequent) inMSA data and hence the better sentiment classifica-tion on TGRD-DA.We also experimented with adding POS tags andstandard features.
These did not have any positiveeffect on the results with one exception, which isshown in Table 8: For sentiment, adding the RTStagset has a positive effect on the two data sets.In a second experiment, we used the originalTGRD corpus but added the language variety (LV)(i.e., MSA and DA) features.
For both subjectiv-ity and sentiment, the best results are acquired us-ing the LEM+PL+LV settings.
However, for subjec-tivity, we observe a drop in accuracy from 72.52%(LEM+ERTS+PL) to 69.54%.
For sentiment, wealso observe a performance drop in accuracy, from65.32% (LEM+ERTS+PL) to 64.52%.
This meansthat knowing the language variety does not provide25TGRD TGRD-MSA TGRD-DACond.
Acc F-P F-N Acc F-P F-N Acc F-P F-NBase 56.45 0.00 72.16 53.49 69.70 0.00 67.47 0.00 80.58TOK 65.32 49.41 73.62 53.49 56.52 50.00 68.67 23.53 80.30LEM 62.10 41.98 71.86 48.84 52.17 45.00 73.49 38.89 83.08TOK+RTS 70.20 64.57 74.29 55.81 61.22 48.65 71.08 29.41 81.82Table 7: Dialect-specific sentiment experiments.SUBJ SENTIData Condition Acc F-O F-S Condition Acc F-P F-NDAR TOK+ERTS+PL+Q3 95.83 0.00 97.87 LEM+PL+GEN 71.28 79.86 50.00TGRD LEM+ERTS+PL 72.52 65.84 77.01 TOK+ERTS+PL+GEN+LV+UID 65.87 49.41 74.25THR TOK+ERTS+PL+Q3 83.33 0.00 90.91 TOK+PL+GEN+UID 67.44 39.13 77.78MONT LEM+RTS+PL+Q3 84.19 3.92 91.39 TOK+PL+Q3 81.36 88.64 48.10Table 8: Overall best SAMAR performance.
Numbers in bold show improvement over the baseline.Data Condition Acc F-O F-SDAR TOK+ERTS+PL+GEN 84.30 0.00 91.48TGRD LEM+RTS+PL+UID 71.85 65.31 76.32THR LEM+RTS+PL+GEN+UID 66.67 0.00 80.00MONT LEM+RTS+PL+DID 83.17 0.00 90.81Table 9: Subjectivity results with genre features.Data Condition Acc F-P F-NDAR LEM+PL+GEN 71.28 79.86 50.00TGRD TOK+ERTS+PL+GEN+LV+UID65.87 49.41 74.25THR TOK+PL+GEN+UID 67.44 39.13 77.78MONT LEM+PL+DID 76.82 47.42 85.13Table 10: Sentiment results with genre features.
Numbersin bold show improvement over table 5.enough information for successfully conquering thedifferences between those varieties.5.4 Leveraging Genre Specific FeaturesRQ4 investigates the question whether we can lever-age features typical for social media for classifica-tion.
We apply all GENRE features exhaustively.Wereport the best performance on each data set.Table 9 shows the results of adding the genre fea-tures to the subjectivity classifier.
For this task, nodata sets profit from these features.Table 10 shows the results of adding the genre fea-tures to the sentiment classifier.
Here, all the datasets, with the exception of MONT, profit from thenew features.
In the case of DAR, adding genderinformation improves classification by 1.73% in ac-curacy.
For TGRD, the combination of the gender(GN), language variety (LV), and user ID slightly(0.52%) improves classification over previous bestsettings.
For THR, adding the gender and user IDinformation improves classification by 4.07%.Our results thus show the utility of the gender,LV, and user ID features for sentiment classification.The results for both subjectivity and sentiment showthat the document ID feature is not a useful feature.6 Overall PerformanceTable 8 provides the best results reached bySAMAR.
For subjectivity classification, SAMARimproves on all data sets when the POS features arecombined with the standard features.
For sentimentclassification, SAMAR also improves over the base-line on all the data sets, except MONT.
The resultsalso show that all optimal feature settings for sub-jectivity, except with the MONT data set, includethe ERTS POS tags while the results in Section 5.1showed that adding POS information without addi-tional features, while helping in most cases with sub-jectivity, does not help with sentiment classification.7 Conclusion and Future WorkIn this paper, we presented SAMAR, an SSA systemfor Arabic social media.
We explained the rich fea-ture set SAMAR exploits and showed how complexmorphology characteristic of Arabic can be handledin the context of SSA.
For the future, we plan tocarry out a detailed error analysis of SAMAR in anattempt to improve its performance, use a recently-developed wider coverage polarity lexicon (Abdul-Mageed and Diab, 2012b) together with another DAlexicon that we are currently developing.26ReferencesAhmed Abbasi, Hsinchun Chen, and Arab Salem.
2008.Sentiment analysis in multiple languages: Feature se-lection for opinion classification in Web forums.
ACMTransactions on Information Systems, 26:1?34.Muhammad Abdul-Mageed and Mona Diab.
2012a.AWATIF: A multi-genre corpus for Modern StandardArabic subjectivity and sentiment analysis.
In Pro-ceedings of LREC, Istanbul, Turkey.Muhammad Abdul-Mageed and Mona Diab.
2012b.
To-ward building a large-scale Arabic sentiment lexicon.In Proceedings of the 6th International Global Word-Net Conference, Matsue, Japan.Muhammad Abdul-Mageed, Hamdan Albogmi, Abdul-rahman Gerrio, Emhamed Hamed, and Omar Aldibasi.2011a.
Tweeting in Arabic: What, how and whither.Presented at the 12th Annual Conference of the As-sociation of Internet Researchers (Internet Research12.0, Performance and Participation), Seattle, WA.Muhammad Abdul-Mageed, Mona Diab, and MohamedKorayem.
2011b.
Subjectivity and sentiment analy-sis of Modern Standard Arabic.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 587?591, Portland, OR.Muhammad Abdul-Mageed, Mohamed Korayem, andAhmed YoussefAgha.
2011c.
?Yes we can??
: Sub-jectivity annotation and tagging for the health domain.In Proceedings of RANLP2011, Hissar, Bulgaria.Ann Banfield.
1982.
Unspeakable Sentences: Narrationand Representation in the Language of Fiction.
Rout-ledge, Boston.Penelope Brown and Stephen Levinson.
1987.
Polite-ness: Some Universals in Language Usage.
Cam-bridge University Press.Rebecca Bruce and Janyce Wiebe.
1999.
Recognizingsubjectivity.
A case study of manual tagging.
NaturalLanguage Engineering, 5(2):187?205.Kushal Dave, Steve Lawrence, and David Pennock.2003.
Mining the peanut gallery: Opinion extrac-tion and semantic classification of product reviews.
InProceedings of the 12th International Conference onWorld Wide Web, pages 519?528, Budapest, Hungary.ACM.Mona Diab, Dan Jurafsky, and Kadri Hacioglu.
2007.Automatic processing of Modern Standard Arabic text.In Abdelhadi Soudi, Antal van den Bosch, and Gu?nterNeumann, editors, Arabic Computational Morphol-ogy.
Springer.Mona Diab, Nizar Habash, Owen Rambow, MohamedAltantawy, and Yassin Benajiba.
2010.
COLABA:Arabic dialect annotation and processing.
In LRECWorkshop on Semitic Language Processing, pages 66?74, Valetta, Malta.Mona Diab.
2007a.
Improved Arabic base phrase chunk-ing with a new enriched POS tag set.
In Proceedingsof the 2007 Workshop on Computational Approachesto Semitic Languages: Common Issues and Resources,pages 89?96, Prague, Czech Republic.Mona Diab.
2007b.
Towards an optimal POS tag set forModern Standard Arabic processing.
In Proceedingsof Recent Advances in Natural Language Processing(RANLP), Borovets, Bulgaria.Mona Diab.
2009.
Second generation AMIRA toolsfor Arabic processing: Fast and robust tokenization,POS tagging, and base phrase chunking.
In Proceed-ings of the Second International Conference on ArabicLanguage Resources and Tools, pages 285?288, Cairo,Egypt.Nizar Habash, Owen Rambow, and Ryan Roth.
2009.MADA+TOKAN: A toolkit for Arabic tokenization,diacritization, morphological disambiguation, POStagging, stemming and lemmatization.
In Proceed-ings of the Second International Conference on ArabicLanguage Resources and Tools, pages 102?109, Cairo,Egypt.Susan Herring.
1996.
Bringing familiar baggage tothe new frontier: Gender differences in computer-mediated communication.
In J. Selzer, editor, Con-versations.
Allyn & Bacon.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the TenthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 168?177,Seattle, WA.Thorsten Joachims.
2008.
Svmlight: Support vector ma-chine.
http://svmlight.joachims.org/, Cornell Univer-sity, 2008.Soo-Min Kim and Eduard Hovy.
2004.
Determining thesentiment of opinions.
In Proceedings of the 20th In-ternational Conference on Computational Linguistics,pages 1367?1373, Geneva, Switzerland.Mohamed Maamouri, Anne Bies, Tim Buckwalter, andW.
Mekki.
2004.
The Penn Arabic Treebank: Build-ing a large-scale annotated Arabic corpus.
In NEM-LAR Conference on Arabic Language Resources andTools, pages 102?109, Cairo, Egypt.Peter Turney.
2002.
Thumbs up or thumbs down?
Se-mantic orientation applied to unsupervised classifica-tion of reviews.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguis-tics (ACL?02), Philadelphia, PA.Janyce Wiebe, Rebecca Bruce, and Tim O?Hara.
1999.Development and use of a gold standard data set forsubjectivity classifications.
In Proceedings of the 37th27Annual Meeting of the Association for ComputationalLinguistics (ACL-99), pages 246?253, University ofMaryland.Janyce Wiebe, Theresa Wilson, Rebecca Bruce, MatthewBell, and Melanie Martin.
2004.
Learning subjectivelanguage.
Computational Linguistics, 30:227?308.Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, andWayne Niblack.
2003.
Sentiment analyzer: Extract-ing sentiments about a given topic using natural lan-guage processing techniques.
In Proceedings of the3rd IEEE International Conference on Data Mining,pages 427?434, Melbourne, FL.Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towardsanswering opinion questions: Separating facts fromopinions and identifying the polarity of opinion sen-tences.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP),Sapporo, Japan.28
