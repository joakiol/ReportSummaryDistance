Automatic Extraction of Briefing TemplatesDipanjan Das Mohit KumarLanguage Technologies InstituteCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213, USA{dipanjan, mohitkum, air}@cs.cmu.eduAlexander I. RudnickyAbstractAn approach to solving the problem of au-tomatic briefing generation from non-textualevents can be segmenting the task into twomajor steps, namely, extraction of briefingtemplates and learning aggregators that col-late information from events and automati-cally fill up the templates.
In this paper, wedescribe two novel unsupervised approachesfor extracting briefing templates from hu-man written reports.
Since the problem isnon-standard, we define our own criteria forevaluating the approaches and demonstratethat both approaches are effective in extract-ing domain relevant templates with promis-ing accuracies.1 IntroductionAutomated briefing generation from non-textualevents is an unsolved problem that currently lacks astandard approach in the NLP community.
Broadly,it intersects the problem of language generationfrom structured data and summarization.
The prob-lem is relevant in several domains where the userhas to repeatedly write reports based on events inthe domain, for example, weather reports (Reiteret al, 2005), medical reports (Elhadad et al, 2005),weekly class project reports (Kumar et al, 2007) andso forth.
On observing the data from these domains,we notice a templatized nature of report items.
Ex-amples (1)-(3) demonstrate equivalents in a particu-lar domain (Reiter et al, 2005).
(1) [A warm front] from [Iceland] to[northern Scotland] will move [SE]across [the northern North Sea] [todayand tomorrow](2) [A warm front] from [Iceland] to [theFaeroes] will move [ENE] across [theNorwegian Sea] [this evening](3) [A ridge] from [the British Isles] to[Iceland] will move [NE] across [theNorth Sea] [today]In each sentence, the phrases in square brackets atthe same relative positions form the slots that takeup different values at different occasions.
The cor-responding template is shown in (4) with slots con-taining their respective domain entity types.
Instan-tiations of (4) may produce (1)-(3) and similar sen-tences.
This kind of sentence structure motivates anapproach of segmenting the problem of closed do-main summarization into two major steps of auto-matic template extraction and learning aggregators,which are pattern detectors that assimilate informa-tion from the events, to populate these templates.
(4) [PRESSURE ENTITY] from [LOCATION] to[LOCATION] will move [DIRECTION] across[LOCATION] [TIME]In the current work we address the first problem ofautomatically extracting domain templates from hu-man written reports.
We take a two-step approach tothe problem; first, we cluster report sentences basedon similarity and second, we extract template(s) cor-responding to each cluster by aligning the instancesin the cluster.
We experimented with two indepen-dent, arguably complementary techniques for clus-tering and aligning ?
a predicate argument based ap-proach that extracts more general templates contain-ing one predicate and a ROUGE (Lin, 2004) based265approach that can extract templates containing mul-tiple verbs.
As we will see below, both approachesshow promise.2 Related WorkThere has been instances of template based sum-marization in popular Information Extraction (IE)evaluations like MUC (Marsh & Perzanowski, 1998;Onyshkevych, 1994) and ACE (ACE, 2007) wherehand engineered slots were to be filled for events intext; but the focus lay on template filling rather thantheir creation.
(Riloff, 1996) describes an interestingwork on the generation of extraction patterns fromuntagged text, but the analysis is syntactic and thepatterns do not resemble the templates that we aimto extract.
(Yangarber et al, 2000) describe anothersystem called ExDisco, that extracts event patternsfrom un-annotated text starting from seed patterns.Once again, the text analysis is not deep and the pat-terns extracted are not sentence surface forms.
(Collier, 1998) proposed automatic domain tem-plate extraction for IE purposes where MUC typetemplates for particular types of events were con-structed.
The method relies on the idea from (Luhn,1958) where statistically significant words of a cor-pus were extracted.
Based on these words, sen-tences containing them were chosen and alignedusing subject-object-verb patterns.
However, thismethod did not look at arbitrary syntactic patterns.
(Filatova et al, 2006) improved the paradigm bylooking at the most frequent verbs occurring in acorpus and aligning subtrees containing the verb,by using the syntactic parses as a similarity metric.However, long distance dependencies of verbs withconstituents were not looked at and deep semanticanalysis was not performed on the sentences to findout similar verb subcategorization frames.
In con-trast, in our predicate argument based approach welook into deeper semantic structures, and align sen-tences not only based on similar syntactic parses,but also based on the constituents?
roles with re-spect to the main predicate.
Also, they relied ontypical Named Entities (NEs) like location, organi-zation, person etc.
and included another entity thatthey termed as NUMBER.
However, for specificdomains like weather forecasts, medical reports orstudent reports, more varied domain entities formslots in templates, as we observe in our data; hence,existence of a module handling domain specific en-tities become essential for such a task.
(Surdeanuet al, 2003) identify arguments for predicates in asentence and emphasize how semantic role infor-mation may assist in IE related tasks, but their pri-mary focus remained on the extraction of PropBank(Kingsbury et al, 2002) type semantic roles.To our knowledge, the ROUGE metric has notbeen used for automatic extraction of templates.3 The Data3.1 Data DescriptionSince our focus is on creating summary items fromevents or structured data rather than from text, weused a corpus from the domain of weather fore-casts (Reiter et al, 2005).
This is a freely avail-able parallel corpus1 consisting of weather dataand human written forecasts describing them.
Thedataset showed regularity in sentence structure andbelonged to a closed domain, making the variationsin surface forms more constrained than completelyfree text.
After sentence segmentation we arrived ata set of 3262 sentences.
From this set, we selected3000 for template extraction and kept aside 262 sen-tences for testing.3.2 PreprocessingFor semantic analysis, we used the ASSERT toolkit(Pradhan et al, 2004) that produces shallow seman-tic parses using the PropBank conventions.
As aby product, it also produces syntactic parses of sen-tences, using the Charniak parser (Charniak, 2001).For each sentence, we maintained a part-of-speechtagged (leaves of the parse tree), parsed, baseNP2tagged and semantic role tagged version.
ThebaseNPs were retrieved by pruning the parse treesand not by using a separate NP chunker.
The rea-son for having a baseNP tagged corpus will becomeclear as we go into the detail of our template ex-traction techniques.
Figure 1 shows a typical out-put from the Charniak parser and Figure 2 shows thesame tree with nodes under the baseNPs pruned.We identified the need to have a domain entitytagger for matching constituents in the sentences.1http://www.csd.abdn.ac.uk/research/sumtime/2A baseNP is a noun-phrase with no internal noun-phrase266ADVPINA low theover Norwegian Sea will move North and weakenDT NN DT JJ NN MD VB RB CC VBNPNPPPNPSVPVPVPVPFigure 1: Parse tree for a sentence in the data.ADVPINA low theover Norwegian Sea will move North and weakenMD VB RB CC VBNPNPPPNPSVPVPVPVPFigure 2: Pruned parse tree for a sentence in the cor-pusAny tagger for named entities was not suitable forweather forecasts since unique constituent types as-sumed significance unlike newswire data.
Since thedevelopment of such a tagger was beyond the scopeof the present work, we developed a module thattook baseNP tagged sentences as input and producedtags across words and baseNPs that were domain en-tities.
The development of such a module by handwas easy because of a limited vocabulary (< 1000words) of the data and the closed set nature of mostentity types (e.g the direction entity could take up afinite set of values).
From inspection, thirteen dis-tinct entity types were recognized in the domain.Figure 3 shows an example output from the entityrecognizer with the sentence from Figure 2 as input.
[ A low ]DIRECTION and weakenA low over the Norwegian Sea will move North and weakenENTITY RECOGNIZERLOCATIONover [ the Norwegian Sea ]PRESSURE ENTITYwill move [ North ]Figure 3: Example output of the entity recognizerWe now provide a detailed description of our clus-tering and template extraction algorithms.4 Approach and ExperimentsWe adopted two parallel approaches.
First, weinvestigated a predicate-argument based approachwhere we consider the set of all propositions in ourdataset, and cluster them based on their verb sub-categorization frame.
Second, we used ROUGE,a summarization evaluation metric that is generallyused to compare machine generated and human writ-ten summaries.
We uniquely used this metric forclustering similar summary items, after abstractingthe surface forms to a representation that facilitatescomparison of a pair of sentences.
The followingsubsections detail both the techniques.4.1 A Predicate-Argument Based ApproachAnalysis of predicate-argument structures seemedappropriate for template extraction for a few rea-sons: Firstly, complicated sentences with multipleverbs are broken down into propositions by a seman-tic role labeler.
The propositions3 are better gen-eralizable units than whole sentences across a cor-pus.
Secondly, long distance dependencies of con-stituents with a particular verb, are captured well bya semantic role labeler.
Finally, if verbs are con-sidered to be the center of events, then groups ofsentences with the same semantic role sequencesseemed to form clusters conveying similar meaning.We explain the complete algorithm for template ex-traction in the following subsections.
(5) [ARG0 A low over the Norwegian Sea][AGM-MOD will] [TARGET move ][ARGM-DIR North ] and weaken(6) [ARG0 A high pressure area ] [AGM-MODwill ] [TARGET move] [ARGM-DIRsouthwestwards] and build on Sunday.4.1.1 Verb based clusteringWe performed a verb based clustering as the firststep.
Instead of considering a unique set of verbs,we considered related verbs as a single verb type.The relatedness of verbs was derived from Word-net (Fellbaum, 1998), by merging verbs that appearin the same synset.
This kind of clustering is not3sentence fragments with one verb267ideal in a corpus containing a huge variation in eventstreams, like newswire.
However, the results weregood for the weather domain where the number ofverbs used is limited.
The grouping procedure re-sulted in a set of 82 clusters with 6632 propositions.4.1.2 Matching Role SequencesEach verb cluster was considered next.
Insteadof finding structural similarities of the propositionsin one go, we first considered the semantic rolesequences for each proposition.
We searched forpropositions that had exactly similar role sequencesand grouped them together.
To give an exam-ple, both sentences 5 and 6 have the matching rolesequence ARG0?ARGM-MOD?TARGET?ARGM-DIR.
The intuition behind such clustering is straight-forward.
Propositions with a matching verb typewith the same set of roles arranged in a similar fash-ion would convey similar meaning.
We observedthat this was indeed true for sentences tagged withcorrect semantic role labels.Instead of considering matching role sequencesfor a set of propositions, we could as well haveconsidered matching bag of roles.
However, forthe present corpus, we decided to use strict role se-quence instead because of the sentences?
rigid struc-ture and absence of any passive sentences.
Thissubclustering step resulted in smaller clusters, andmany of them contained a single proposition.
Wethrew out these clusters on the assumption that thehuman summarizers did not necessarily have a tem-plate in mind while writing those summary items.As a result, many verb types were eliminated andonly 33 verb-type clusters containing several sub-clusters each were produced.4.1.3 Looking inside RolesGroups of propositions with the same verb-typeand semantic role sequences were considered in thisstep.
For each group, we looked at individual se-mantic roles to find out similarity between them.
Wedecided at first to look at syntactic parse tree similar-ities between constituents.
However, there is a needto decide at what level of abstraction should one con-sider matching the parse trees.
After considerablespeculation, we decided on pruning the constituents?parse trees till the level of baseNPs and then matchthe resulting tag sequences.ScotlandINA low theover SeaNPNPPPNPNPNP PPNPNorwegian A frontal troughINacrossFigure 4: Matching ARG0s for two propositionsLOCATIONINA low theover SeaNorwegian A frontal troughINacross ScotlandPRESSURE ENTITYLOCATIONPRESSURE ENTITYFigure 5: Abstracted tag sequences for two con-stituentsThe parses with pruned trees from the preprocess-ing steps provide the necessary information for con-stituent matching.
Figure 4 shows matching syntac-tic trees for two ARG0s from two propositions of acluster.
It is at this step that we use the domain entitytags to abstract away the constituents?
syntactic tags.Figure 5 shows the constituents of Figure 4 with thetree structure reduced to tag sequences and domainentity types replacing the tags whenever necessary.This abstraction step produces a number of uniquedomain entity augmented tag sequences for a partic-ular semantic role.
As a final step of template gen-eration, we concatenate these abstracted constituenttypes for all the semantic roles in the given group.To focus on template-like structures we only con-sider tag sequences that occur twice or more in thegroup.The templates produced at the end of this step areessentially tag sequences interspersed with domainentities.
In our definition of templates, the slots arethe entity types and the fixed parts are constitutedby word(s) used by the human experts for a partic-ular tag sequence.
Figure 6 shows some exampletemplates.
The upper case words in the figure corre-spond to the domain entities identified by the entitytagger and they form the slots in the templates.
Atotal of 209 templates were produced.268PRESSURE_ENTITY to DIRECTION of LOCATION will drift slowlyWAVE will run_0.5/move_0.5 DIRECTION then DIRECTIONAssociated PRESSURE_ENTITY will move DIRECTION across LOCATION TIMEPRESSURE_ENTITY expected over LOCATION by_0.5/on_0.5 DAYFigure 6: Example Templates.
Upper case tokenscorrespond to slots.
For fixed parts, when there is achoice between words, the probability of the occur-rence of words in that particular syntactic structureare tagged alongside.4.2 A ROUGE Based ApproachROUGE (Lin, 2004) is the standard automatic eval-uation metric in the Summarization community.
It isderived from the BLEU (Papineni et al, 2001) scorewhich is the evaluation metric used in the MachineTranslation community.
The underlying idea in themetric is comparing the candidate and the refer-ence sentences (or summaries) based on their tokenco-occurrence statistics.
For example, a unigrambased measure would compare the vocabulary over-lap between the candidate and reference sentences.Thus, intuitively, we may use the ROUGE score asa measure for clustering the sentences.
Amongstthe various ROUGE statistics, the most appealing isWeighted Longest Common Subsequence(WLCS).WLCS favors contiguous LCS which correspondsto the intuition of finding the common template.We experimented with other ROUGE statistics butwe got better and easily interpretable results usingWLCS and so we chose it as the final metric.
Inall the approaches the data was first preprocessed(baseNP and NE tagged) as described in the previ-ous subsection.
In the following subsections, we de-scribe the various clustering techniques that we triedusing the ROUGE score followed by the alignmenttechnique.4.2.1 ClusteringUnsupervised Clustering: As the ROUGE scoredefines a distance metric, we can use this score fordoing unsupervised clustering.
We tried hierarchicalclustering approaches but did not obtain good clus-ters, evaluated empirically.
In empirical evaluation,we manually looked at the output clusters and madea judgement call whether the candidate clusters arereasonably coherent and potentially correspond totemplates.
The reason for the poor performance ofthe approach was the classical parameter estimationproblem of determining a priori the number of clus-ters.
We could not find an elegant solution for theproblem without losing the motivation of an auto-mated approach.Figure 7: Deterministic clustering based on Graphconnectivity.
In the figure the squares with the samepattern belong to the same cluster.Non-parametric Unsupervised Clustering:Since the unsupervised technique did not givegood results, we experimented with a non-parametric clustering approach, namely, Cross-Association(Chakrabarti et al, 2004).
It is anon-parametric unsupervised clustering algorithmfor similarity (boolean) matrices.
We obtain thesimilarity matrix in our domain by thresholding theROUGE similarity score matrix.
This techniquealso did not give us good clusters, evaluated empiri-cally.
The plausible reason for the poor performanceseems to be that the technique is based on MDL(Minimum Description Length) principle.
Since inour domain we expect a large number of clusterswith small membership along many singletons,MDL principle is not likely to perform well.Deterministic Clustering:As the unsupervised techniques did not performwell, we tried deterministic clustering based ongraph connectivity.
The underlying intuition is thatall the sentences X1...n that are ?similar?
to anyother sentence Yi should be in the same cluster eventhough Xj and Xk may not be ?similar?
to eachother.
Thus we find the connected components in thesimilarity matrix and label them as individual clus-ters.44This approach is similar to agglomerative single linkageclustering.269We created a similarity matrix by thresholding theROUGE score.
In the event, the clusters obtained bythis approach were also not good, evaluated empir-ically.
This led us to revisit the similarity functionand tune it.
We factored the ROUGE-WLCS score,which is an F-measure score, into its component Pre-cision and Recall scores and experimented with var-ious combinations of using the Precision and Recallscores.
We finally chose a combined Precision andRecall measure (not f-measure) in which both thescores were independently thresholded.
The moti-vation for the measure is that in our domain we de-sire to have high precision matches.
Additionallywe need to control the length of the sentences in thecluster for which we require a Recall threshold.
F-measure (which is the harmonic mean of Precisionand Recall) does not give us the required individualcontrol.
We set up our experiments such that whilecomparing two sentences the longer sentence is al-ways treated as the reference and the shorter one asthe candidate.
This helps us in interpreting the Pre-cision/Recall measures better and thresholding themaccordingly.
The approach gave us 149 clusters,which looked good on empirical evaluation.
We canargue that using this modified similarity function forprevious unsupervised approaches could have givenbetter results, but we did not reevaluate those ap-proaches as our aim of getting a reasonable cluster-ing approach is fulfilled with this simple scheme andtuning the unsupervised approaches can be interest-ing future work.4.3 AlignmentAfter obtaining the clusters using the Deterministicapproach we needed to find out the template corre-sponding to each of the cluster.
Fairly intuitively wecomputed the Longest Common Subsequence(LCS)between the sentences in each cluster which we thenclaim to be the template corresponding to the clus-ter.
This resulted in a set of 149 templates, similar tothe Predicate Argument based approach, as shownin figure 6.5 Results5.1 Evaluation SchemeSince there is no standard way to evaluate templateextraction for summary creation, we adopted a mixof subjective and automatic measures for evaluatingthe templates extracted.
We define precision for thisparticular problem as:precision = number of domain relevant templatestotal number of extracted templatesThis is a subjective measure and we undertook astudy involving three subjects who were accustomedto the language used in the corpus.
We asked thehuman subjects to mark each template as relevantor non-relevant to the weather forecast domain.
Wealso asked them to mark the template as grammaticalor ungrammatical if it is non-relevant.Our other metric for evaluation is automatic re-call.
It is based on using the ROUGE-WLCS met-ric to determine a match between the preprocessed(baseNP and NE tagged) test corpora with the pro-posed set of correct templates, a set determinedby taking an intersection of only the relevant tem-plates marked by each judge.
For the ROUGE basedmethod, the test corpus consists of 262 sentences,while for the predicate-argument based method itconsists of a set of 263 propositions extracted fromthe 262 sentences using ASSERT followed by a fil-tering of invalid propositions (e.g.
ones startingwith a verb).
Amongst different ROUGE scores(precision/recall/f-measure), we consider precisionas the criterion for deciding a match and experi-mented with different thresholding values.Main Verb Precision Main Verb Precisiondeepen 0.67 weaken 0.83expect 0.76 lie 0.57drift 0.93 continue 0.97build 0.95 fill 0.80cross 0.78 move 0.86Table 1: Precision for top 10 most frequently occur-ring verbs5.2 Results: Predicate-Argument BasedApproachTable 1 shows the precision values for top 10 mostfrequently occurring verbs.
(Since a major propor-tion (> 90%) of the templates are covered by theseverbs, we don?t show all the precision values; it alsohelps to contain space.)
The overall precision valueachieved was 84.21%, the inter-rater Fleiss?
kappameasure (Fleiss, 1971) between the judges being270?
= 0.69, demonstrating substantial agreement.
Theprecision values are encouraging, and in most casesthe reason for low precision is because of erroneousperformance of the semantic role labeler system,which is corroborated by the percentage (47.47%) ofungrammatical templates among the irrelevant ones.Results for the automated recall values are shownin Figure 8, where precision values are varied toobserve the recall.
For 0.9 precision in ROUGE-WLCS, the recall is 0.3 which shows that there isa 30% near exact coverage over propositions, whilefor 0.6 precision in ROUGE-WLCS, the recall is anencouraging 81%.00.20.40.60.810.4 0.5 0.6 0.7 0.8 0.9RecallPrecision Threshold forMatching Test SentencesROUGESRLFigure 8: Automated Recall based on ROUGE-WLCS measure comparing the test corpora withthe set of templates extracted by the Predicate-Argument (SRL) and the ROUGE based method.5.3 Results: ROUGE based approachVarious precision and recall thresholds for ROUGEwere considered for clustering.
We empirically set-tled on a recall threshold of 0.8 since this producesthe set of clusters with optimum number of sen-tences.
The number of clusters and number of sen-tences in clusters at this recall values are shown inFigure 9 for various precision thresholds.Precision was measured in the same way as thepredicate argument approach and the value obtainedwas 76.3%, with Fleiss?
kappa measure of ?
= 0.79.The percentage of ungrammatical templates amongthe irrelevant ones was 96.7%, strongly indicatingthat post processing the templates using a parser can,in future, give substantial improvement.
During er-ror analysis, we observed simple grammatical er-rors in templates; first or last word being preposi-1301401501601701801900.65  0.7  0.75  0.8  0.85  0.9  0.95  1  0200400600800100012001400No.
of ClustersNo.
of SentencesinClustersPrecision ThresholdNo.
of ClustersNo.
of Sentences in ClustersFigure 9: Number of clusters and total number ofsentences in clusters for various Precision Thresh-olds at Recall Threshold=0.8tions.
So a fairly simple error recovery module thatstrips the leading and trailing prepositions was in-troduced.
20 templates out of the 149 were mod-ified by the error recovery module and they wereevaluated again by the three judges.
The precisionobtained for the modified templates was 35%, withFleiss?
kappa ?
= 1, boosting the overall precisionto 80.98%.
The overall high precision is motivat-ing as this is a fairly general approach that does notrequire any NLP resources.
Figure 8 shows the auto-mated recall values for the templates and abstractedsentences from the held-out dataset.
For high preci-sion points, the recall is low because there is not anexact match for most cases.6 Conclusion and Future WorkIn this paper, we described two new approachesfor template extraction for briefing generation.
Forboth approaches, high precision values indicate thatmeaningful templates are being extracted.
However,the recall values were moderate and they hint atpossible improvements.
An interesting direction offuture research is merging the two approaches andhave one technique benefit from the other.
The ap-proaches seem complementary as the ROUGE basedtechnique does not use the structure of the sentenceat all whereas the predicate-argument approach isheavily dependent on it.
Moreover, the predicateargument based approach gives general templateswith one predicate while ROUGE based approach271can extract templates containing multiple verbs.
Itwould also be desirable to establish the generalityof the techniques, by using other domains such asnewswire, medical reports and others.Acknowledgements We would like to express ourgratitude to William Cohen and Noah Smith for theirvaluable suggestions and inputs during the course ofthis work.
We also thank the three anonymous re-viewers for helpful suggestions.
This work was sup-ported by DARPA grant NBCHD030010.
The con-tent of the information in this publication does notnecessarily reflect the position or the policy of theUS Government, and no official endorsement shouldbe inferred.ReferencesACE (2007).
Automatic content extraction program.http://www.nist.gov/speech/tests/ace/.Chakrabarti, D., Papadimitriou, S., Modha, D. S.,& Faloutsos, C. (2004).
Fully automatic cross-associations.
Proceedings of KDD ?04 (pp.
79?88).
New York, NY, USA: ACM Press.Charniak, E. (2001).
Immediate-head parsing forlanguage models.
Proceedings of ACL ?01 (pp.116?123).Collier, R. (1998).
Automatic template creationfor information extraction.
Doctoral dissertation,University of Sheffield.Elhadad, N., Kan, M.-Y., Klavans, J. L., & McKe-own, K. (2005).
Customization in a unified frame-work for summarizing medical literature.
Artifi-cial Intelligence in Medicine, 33, 179?198.Fellbaum, C. (1998).
WordNet ?
An Electronic Lex-ical Database.
MIT Press.Filatova, E., Hatzivassiloglou, V., & McKeown,K.
(2006).
Automatic creation of domain tem-plates.
Proceedings of COLING/ACL 2006 (pp.207?214).Fleiss, J.
(1971).
Measuring nominal scale agree-ment among many raters.
Psychological Bulletin(pp.
378?382).Kingsbury, P., Palmer, M., & Marcus, M. (2002).Adding semantic annotation to the penn treebank.Proceedings of the HLT?02.Kumar, M., Garera, N., & Rudnicky, A. I.
(2007).Learning from the report-writing behavior of in-dividuals.
IJCAI (pp.
1641?1646).Lin, C.-Y.
(2004).
ROUGE: A package for auto-matic evaluation of summaries.
Proceedings ofWorkshop on Text Summarization.Luhn, H. P. (1958).
The automatic creation of litera-ture abstracts.
IBM Journal of Research Develop-ment, 2, 159?165.Marsh, E., & Perzanowski, D. (1998).
MUC-7 Eval-uation of IE Technology: Overview of Results.Proceedings of MUC-7.
Fairfax, Virginia.Onyshkevych, B.
(1994).
Issues and methodologyfor template design for information extraction.Proceedings of HLT ?94 (pp.
171?176).
Morris-town, NJ, USA.Papineni, K., Roukos, S., Ward, T., & Zhu, W.(2001).
Bleu: a method for automatic evaluationof machine translation.Pradhan, S., Ward, W., Hacioglu, K., Martin, J., &Jurafsky, D. (2004).
Shallow semantic parsingusing support vector machines.
Proceedings ofHLT/NAACL ?04.
Boston, MA.Reiter, E., Sripada, S., Hunter, J., Yu, J., & Davy,I.
(2005).
Choosing words in computer-generatedweather forecasts.
Artif.
Intell., 167, 137?169.Riloff, E. (1996).
Automatically generating extrac-tion patterns from untagged text.
AAAI/IAAI, Vol.2 (pp.
1044?1049).Surdeanu, M., Harabagiu, S., Williams, J., &Aarseth, P. (2003).
Using predicate-argumentstructures for information extraction.
Proceedingsof ACL 2003.Yangarber, R., Grishman, R., Tapanainen, P., & Hut-tunen, S. (2000).
Automatic acquisition of domainknowledge for information extraction.
Proceed-ings of the 18th conference on Computational lin-guistics (pp.
940?946).
Morristown, NJ, USA.272
