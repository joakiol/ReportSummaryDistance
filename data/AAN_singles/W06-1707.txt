Corporator: A tool for creating RSS-based specialized corporaC?drick FaironCentre de traitement automatique du langageUCLouvainBelgiquecedrick.fairon@uclouvain.beAbstractThis paper presents a new approach and asoftware for collecting specialized cor-pora on the Web.
This approach takesadvantage of a very popular  XML-basednorm used on the Web for sharing con-tent among websites: RSS (Really SimpleSyndication).
After a brief introduction toRSS, we explain the interest of this typeof data sources in the framework of cor-pus development.
Finally, we presentCorporator, an Open Source softwarewhich was designed for collecting corpusfrom RSS feeds.1 Introduction1Over the last years, growing needs in the fieldsof Corpus Linguistics and NLP have led to anincreasing demand for text corpora.
The automa-tion of corpus development has therefore becamean important and active field of research.
Untilrecently, constructing corpora required largeteams and important means (as text was rarelyavailable on electronic support and computer hadlimited capacities).
Today, the situation is quitedifferent as any published text is recorded, atsome point of its ?life?
on digital media.
Also,increasing number of electronic publication (tex-tual databank, CD?ROM, etc.)
and the expansionof the Internet have made text more accessiblethan ever in our history.The Internet is obviously a great source ofdata for corpus development.
It is either consid-ered as a corpus by itself (see the WebCorp Pro-ject of Renouf, 2003) or as a huge databank inwhich to look for specific texts to be selected and1 I would like to thank CENTAL members who took part inthe development and the administration of GlossaNet andthose who contributed to the development of Corporator andGlossaRSS.
Thanks also to Herlinda Vekemans who helpedin the preparation of this paper.gathered for further treatment.
Examples of pro-jects adopting the latter approach are numerous(among many Sekigushi and Yammoto, 2004;Emirkanian et al 2004).
It is also the goal of theWaCky Project for instance which aims at devel-oping tools ?that will allow linguists to crawl asection of the web, process the data, index themand search them?2.So we have the Internet: it is immense, free,easily accessible and can be used for all mannerof language research (Kilgarriff and Grefenstette,2003).
But text is so abundant, that it is not soeasy to find appropriate textual data for a giventask.
For this reason, researchers have been de-veloping softwares that are able to crawl the Weband find sources corresponding to specific crite-ria.
Using clustering algorithms or similaritymeasures, it is possible to select texts that aresimilar to a training set.
These techniques canachieve good results, but they are sometimes lim-ited when it comes to distinguishing betweenwell-written texts vs. poorly written, or othersubtle criteria.
In any case, it will require filter-ing and cleaning of the data (Berland and Grabar,2002).One possibility to address the difficulty to findgood sources is to avoid ?wide crawling?
butinstead to bind the crawler to manually identifiedWeb domains which are updated on a regularbasis and which offer textual data of good quality(this can be seen as ?vertical crawling?
as op-posed to ?horizontal?
or ?wide crawling?).
Thisis the choice made in the GlossaNet system (Fai-ron, 1998; 2003).
This Web service gives to theusers access to a linguistics based search enginefor querying online newspapers (it is based onthe Open Source corpus processor Unitex3 ?Paumier, 2003).
Online newspapers are an inter-esting source of textual data on the Web becausethey are continuously updated and they usuallypublish articles reviewed through a full editorial2 http://wacky.sslmit.unibo.it3 http://www-igm.univ-mlv.fr/~unitex/43process which ensures (a certain) quality of thetext.Figure 1.
GlossaNet interfaceGlossaNet downloads over 100 newspapers (in10 languages) on a daily basis and parses themlike corpora.
The Web-based interface4 of thisservice enable the user to select a list of newspa-pers and to register a query.
Every day, theuser?s query is applied on the updated corpusand results are sent by email to the user underthe form of a concordance.
The main limitationof GlossaNet is that it works only on a limitedset of sources which all are of the same kind(newspapers).In this paper we will present a new approachwhich takes advantage of a very popular XML-based format used on the Web for sharing con-tent among websites: RSS (Really Simple Syn-dication).
We will briefly explain what RSS isand discuss its possibilities of use for buildingcorpora.We will also present Corporator, an OpenSource program we have developed for creatingRSS-fed specialized corpora.
This system is notmeant to replace broad Web crawling ap-proaches but rather systems like GlossaNet,which collect Web pages from a comparativelysmall set of homogeneous Web sites.2 From RSS news feeds to corpora2.1 What is RSSRSS is the acronym for Really Simple Syndica-tion5.
It is an XML-based format used for facili-4 http://glossa.fltr.ucl.ac.be5 To be more accurate, ?r?
in RSS was initially a reference toRDF.
In fact, at the beginning of RSS the aim was to enableautomatic Web site summary and at that time, RSS stood fortating news publication on the Web and contentinterchange between websites6.
Netscape createdthis standard in 1999, on the basis of DaveWiner?s work on the ScriptingNews format (his-torically the first syndication format used on theWeb)7.
Nowadays many of the press groupsaround the world offer RSS-based news feeds ontheir Web sites which allow easy access to therecently published news articles:FR Le monde :http://www.lemonde.fr/web/rss/0,48-0,1-0,0.htmlIT La Repubblicahttp://www.repubblica.it/servizi/rss/index.htmlPT P?blicohttp://www.publico.clix.pt/homepage/site/rss/default.aspUS New York Timeshttp://www.nytimes.com/services/xml/rss/index.htmlES El Pais :http://www.elpais.es/static/rss/index.htmlAF Allafrica.com8http://fr.allafrica.com/tools/headlines/rss.htmletc.RDF Site Summary format.
But over the time this standardchanged for becoming a news syndication tools and theRDF headers were removed.6 Atom is another standard built with the same objective butis more flexible from a technical point of view.
For a com-parison, see http://www.tbray.org/atom/RSS-and-Atom orHammersley (2005).7 After 99, many groups were involved in the developmentof RSS and it is finally Harvard which published RSS 2.0specifications under Creative Commons License in 2003.For further details on the RSS?
history, seehttp://blogs.law.harvard.edu/tech/rssVersionHistory/8 AllAfrica gathers and indexes content from more than 125African press agencies and other sources.Figure 2.
Example of RSS feeds proposed byReuters (left) and the New York Times (right)44Figure 2 shows two lists of RSS proposed byReuters and the New York Times respectively.Each link points to a RSS file that contains a listof articles recently published and correspondingto the selected theme or section.
RSS files do notcontain full articles, but only the title, a briefsummary, the date of publication, and a link tothe full article available on the publisher Website.
On a regular basis (every hour or even morefrequently), RSS documents are updated withfresh content.News publishers usually organize news feedsby theme (politics, health, business, etc.)
and/orin accordance with the various sections of thenewspaper (front page, job offers, editorials, re-gions, etc.).
Sometimes they even create feedsfor special hot topics such as ?Bird flu?, in Fig-ure 2 (Reuters).There is a clear tendency to increase the num-ber of available feeds.
We can even say that thereis some kind of competition going on as competi-tors tend to offer more or better services than theothers.
By proposing accurate feeds of informa-tion, content publishers try to increase theirchance to see their content reused and publishedon other websites (see below ?2.2).
Another in-dicator of the attention drawn to RSS applica-tions is that some group initiatives are taken forpromoting publishers by publicizing their RSSsources.
For instance, the French association ofonline publishers (GESTE9) has released anOpen Source RSS reader10 which includes morethan 274 French news feeds (among which wecan find feeds from Le Monde, Lib?ration,L?Equipe, ZDNet, etc.
).2.2 What is RSS?RSS is particularly well suited for publishingcontent that can be split into items and that isupdated regularly.
So it is very convenient forpublishing news, but it is not limited to news.There are two main situations of use for RSS.First, on the user side, people can use an RSSenabled Web client (usually called news aggre-gator) to read news feeds.
Standalone applica-tions (like BottomFeeder11 ou Feedreader12) co-exist with plug-ins readers to be added to a regu-lar Web browser.
For example, Wizz RSS NewsReader is an extension for Firefox.
It is illus-trated in Figure 3: the list of items provided by a9 http://www.geste.fr10 AlerteInfo, http://www.geste.fr/alertinfo/home.html11 http://www.cincomsmalltalk.com/BottomFeeder/12 http://www.feedreader.comRSS is displayed in the left frame.
A simple clickon one item opens the original article in the rightframe.Figure 3.
News aggregator plugin in FirefoxSecond, on the Web administrator side, thisformat facilitates the integration in one Web siteof content provided by another Web site underthe form of a RSS.
Thanks to this format, Googlecan claim to integrate news from 4500 onlinesources updated every 15 minutes13.2.3 How does the XML code looks like?As can be see in Figure 414, the XML-based for-mat of RSS is fairly simple.
It mainly consists ofa ?channel?
which contains a list of ?items?
de-scribed by a title, a link, a short description (orsummary), a publication date, etc.
This exampleshows only a subset of all the elements (tags)described by the standard15.Figure 4: example of RSS feed2.4 Can RSS feed corpora?As mentioned above, RSS feeds contain few text.They are mainly a list of items, but each item hasa link pointing to the full article.
It is therefore13 http://news.google.com14 This example comes from the New York Times ?WorldBusiness?
RSS feed and was simplified to fit our needs.15 It is also possible to add elements not described in RSS2.0 if they are described in a namespace.45easy to create a kind of ?greedy?
RSS readerwhich does not only read the feed, but alsodownload each related Web page.
This was ourgoal when we developed Corporator, the pro-gram presented in section 3.2.5 Why using RSS feeds?The first asset of RSS feeds in the framework ofcorpus development is that they offer pre-classified documents by theme, genre or othercategories.
If the classification fits the researcherneeds, it can be used for building a specializedcorpus.
Paquot and Fairon (Forthcoming), forinstance, used this approach for creating corporaof editorials in several languages, which canserve as comparable corpora to the ICLE16 argu-mentative essays, see section 3.1).
Classificationis of course extremely interesting for buildingspecialized corpora, but there are two limitationsof this asset:?
The classification is not standardizedamong content publishers.
So it will re-quire some work to find equivalent newsfeeds from different publishers.
Figure 2offers a good illustration of this: the cate-gories proposed by Reuters and the NewYork Times do not exactly match (even ifthey both have in common some feeds likesports or science).?
We do not have a clear view on how theclassification is done (manually by theauthors, by the system administrators, oreven automatically?
).A second asset is that RSS are updated on aregular basis.
As such, an RSS feed provides acontinuous flow of data that can be easily col-lected in a corpus.
We could call this a dynamiccorpus (Fairon, 1999) as it will grow over time.We could also use the term monitor corpuswhich was proposed by Renouf (1993) andwhich is widely used in the Anglo-Saxon com-munity of corpus linguistics.A third asset is that the quality of the languagein one feed will be approximately constant.
Weknow that one of the difficulties when we crawlthe Web for finding sources is that we can comeacross any kind of document of different quality.By selecting ?trusted?
RSS sources, we can in-sure an adequate quality of the retrieved texts.We can also note that RSS feeds comprise thetitle, date of publication and the author?s name of16 See Granger et al (2002).the articles referred to.
This is also an advantagebecause this information can be difficult to ex-tract from HTML code (as it is rarely well struc-tured).
As soon as we know the date of publica-tion, we can easily download only up to date in-formation, a task that is not always easy withregular crawlers.On the side of these general assets, it is alsoeasy to imagine the interest of this type ofsources for specific applications such as linguis-tic survey of the news (neologism identification,term extraction, dictionary update, etc.
).All these advantages would not be very sig-nificant if the number of sources was limited.But as we indicated above, the number of newsfeeds is rapidly and continuously growing, andnot only on news portals.
Specialized websitesare building index of RSS feeds17 (but we need toremark that for the time being traditional searchengines such as Google, MSN, Yahoo, etc.
han-dle RSS feeds poorly).
It is possible to find feedson virtually any domain (cooking, health, sport,education, travels, sciences) and in many lan-guages.3 Corporator: a ?greedy?
news agreg-gatorCorporator18 is a simple command line pro-gram which is able to read an RSS file, find thelinks in it and download referenced documents.All these HTML documents are filtered andgathered in one file as illustrated in Figure 5.Figure 5.
Corporator ProcessThe filtering step is threefold:- it removes HTML tags, comments andscripts;- it removes (as much as possible) theworthless part of the text  (text from ads,17 Here is just a short selection: http://www.newsxs.com,http://www.newsisfree.com, http://www.rss-scout.de,http://www.2rss.com, http://www.lamoooche.com.18 Corporator is an Open Source program written in Perl.
Itwas developed on the top of a preexisting Open Sourcescommand line RSS reader named  The Yoke.
It will beshortly made available on CENTAL?s web site:http://cental.fltr.ucl.ac.be.46links, options and menu from the originalWeb page)19.- it converts the filtered text from its origi-nal character encoding to UTF8.
Corpora-tor can handle the download of news feedsin many languages (and encodings: UTF,latin, iso, etc.
)20.The program can easily be set up in a taskscheduler so that it runs repeatedly to check ifnew items are available.
As long as the task re-mains scheduled, the corpus will keep on grow-ing.Figure 6 shows a snapshot of the resultingcorpus.
Each downloaded news item is precededby a header that contains information found inthe RSS feed.Figure 6.
Example or resulting corpusCorporator is a generic tool, built fordownloading any feeds in any language.
Thisgoal of genericity comes along with somelimitations.
For instance, for any item in the RSSfeed, the program will download only one Webpage even if, on some particular websites,articles can be split over several pages: Reuters21for instance splits its longer articles into severalpages so that each one can fit on the screen.
TheRSS news item will only refer to the first pageand Corporator will only download that page.
Itwill therefore insert an incomplete article in thecorpus.
We are still working on this issue.19 This is obviously the most difficult step.
Several optionshave been implemented to improve the accuracy of thisfilter : delete text above the article title, delete text afterpattern X, delete line if matches pattern X, etc.20 It can handle all the encodings supported by the Perlmodules Encode (for information, see Encode::Supportedon Cpan).
Although, experience shows that using the En-code can be complicated.21 http://today.reuters.com3.1 Example of corpus creationIn order to present a first evaluation of the sys-tem, we provide in Figure 7 some informationabout an ongoing corpus development project.Our aim is to build corpora of editorials in sev-eral languages, which can serve as comparablecorpora to the ICLE argumentative essays(Paquot and Fairon, forthcoming).
We havetherefore selected ?Editorial?, ?Opinion?
andother sections of various newspapers, which areexpected to contain argumentative texts.
Figure 7gives for four of these sources the number of ar-ticles22 downloaded between January 1st 2006and January 31st 2006 (RSS feed names are givenbetween brackets and URLs are listed in thefootnotes).
Tokens were counted using Unitex(see above) on the filtered text (i.e.
text alreadycleaned from HTML and non-valuable text).Figure 7 shows that the amount of text pro-vided for a given section (here, Opinion) by dif-ferent publishers can be very different.
It alsoillustrates the fact that it is not always possible tofind corresponding news feeds among differentpublishers: Le Monde, for instance, does not pro-vide its editorials on a particular news feed.
Wehave therefore selected a rubric named Rendez-vous in replacement (we have considered that itcontains a text genre of interest to our study).Le Monde23 (Rendez-vous)58 articles90,208 tokensNew York Times24 (Opinion)220 articles246,104 tokensWashington Post25 (Opinion)95 articles137,566 tokensEl Pais26 (Opini?n)337 articles399,831 tokensFigure 7.
Download statistics: number of articlesdownloaded in January 200622 This is the number of articles recorded by the programafter filtering.
It may not correspond exactly to the numberof articles really published on this news feed.23 www.lemonde.fr/rss/sequence/0,2-3238,1-0,0.xml24 www.nytimes.com/services/xml/rss/nyt/Opinion.xml25 www.washingtonpost.com/wp-dyn/rss/index.html#opinion26 www.elpais.es/rss/feed.html?feedId=1003473.2 Towards an online serviceLinguists may find command line tools hard touse.
For this reason, we have also developed aWeb-based interface for facilitating RSS-basedcorpus development.
GlossaRSS provides a sim-ple Web interface in which users can create?corpus-acquisition tasks?.
They just choose aname for the corpus, provide a list of URL corre-sponding to RSS feeds and activate the down-load.
The corpus will grow automatically overtime and the user can at any moment log in todownload the latest version of the corpus.
Forefficiency reasons, the download managing pro-gram checks that news feeds are downloadedonly once.
If several users require the same feed,it will be downloaded once and then appended toeach corpus.Figure 8.
Online service for buildingRSS-based corporaThis service is being tested and will be madepublic shortly.
Furthermore, we plan to integratethis procedure to GlossaNet.
At the moment,GlossaNet provides language specialists with alinguistic search engine that can analyze a littlemore than 100 newspapers (as seen in Figure 1,users who register a linguistic query can com-pose a corpus by selecting newspapers in a pre-defined list).
Our goal is to offer the same servicein the future but on RSS-based corpora.
So it willbe possible to create a new corpus, register a lin-guistic query and get concordance on a daily orweekly basis by email.
There is no programmingdifficulty, but there is a clear issue on the side of?scalability?
(at the present time, GlossaNetcounts more than 1,300 users and generates morethan 18,800 queries a day.
The computing chargewould probably be difficult to cope with if eachuser started to build and work on a different cor-pus).
An intermediate approach between the cur-rent list of newspapers and an open systemwould be to define in GlossaNet some thematiccorpora that would be fed by RSS from differentnewspapers.3.3 From text to RSS-based speech corporaThe approach presented in this paper focuses ontext corpora, but could be adapted for collectingspeech corpora.
In fact RSS are also used as away for publishing multimedia files through Webfeeds named ?podcasts?.
Many medias, corpora-tions or individuals use podcasting for placingaudio and video files on the Internet.
The advan-tage of podcast compared with streaming or sim-ple download, is ?integration?.
Users can collectprograms from a variety of sources and subscribeto them using a podcast-aware software whichwill regularly check if new content is available.This technology has been very successful in thelast two years and has been rapidly growing inimportance.
Users have found many reasons touse it, sometimes creatively: language teachers,for example, have found there a very practicalsource of authentic recordings for their lessons.Regarding corpus development, the interest ofpodcasting is similar to the ones of text-basedRSS (categorization, content regularly updated,etc.).
Another interesting fact is that sometimestranscripts are published together with the pod-cast and it is therefore a great source for creatingsound/text corpora27.Many portals offer lists of poscast28.
One ofthe most interesting ones, is Podzinger29 whichnot only indexes podcasts metadata (title, author,date, etc.
), but uses a speech recognition systemfor indexing podcast content.It would require only minor technical adapta-tion to enable Corporator to deal with podcasts,something that will be done shortly.
Of course,this will only solve the problem of collectingsound files, not the problem of converting thesefiles into speech data useful for linguistic re-search.4 ConclusionCorpora uses and applications are every yearmore numerous in NLP, language teaching, cor-pus linguistics, etc.
and there is therefore a grow-ing demand for large well-tailored corpora.
Atthe same time the Internet has grown enor-mously, increasing its diversity and its world27 It is even possible to find services that do podcast tran-scripts (http://castingwords.com).28 http://www.podcastingnews.com, http://www.podcast.net,etc.29 http://www.podzinger.com48wide coverage.
It is now an ideal ?ground?
forfinding corpus sources.
But these assets (size,diversity) is at the same time an issue for findinggood, reliable, well-written, sources that suit ourneeds.
This is the reason why we need to developintelligent source-finder crawlers and other soft-wares specialized in corpus collection.
Our con-tribution to this effort is to bring the researchers?attention to a particularly interesting source oftext on the Internet: RSS news feeds.
The maininterest of this source is to provide classified listsof documents continuously updated and consis-tent in terms of language quality.To build specialized corpora with a traditionalcrawler approach, the process will probably con-sist in retrieving documents (using a search en-gine as starting point) and then sorting the re-trieved documents and selecting the ones thatpass some kind of validity tests.
With RSS-basedcorpus, the approach is different and could besummarized as follows: do not sort a list of re-trieved documents, but retrieve a list of sorteddocuments.
This is of course only possible if wecan find RSS-feeds compatible with the themeand/or language we want in our corpus.ReferencesBerland, Sophie and Natalia Grabar.
2002.
Assistanceautomatique pour l'homog?n?isation d'un corpusWeb de sp?cialit?.
In Actes  des 6?mes Journ?es in-ternationales d'analyse statistique des donn?es tex-tuelles (JADT 2002).
Saint-Malo.Fairon, C?drick.
1999.
Parsing a Web site as a corpus.In C. Fairon (ed.).
Analyse lexicale et syntaxique:Le syst?me INTEX, Lingvisticae InvestigationesTome XXII (Volume sp?cial).
John BenjaminsPublishing, Amsterdam/Philadelphia, pp.
327-340.Granger, Sylviane, Estelle Dagneaux and Fanny Meu-nier (eds).
2002.
The International Corpus ofLearner English.
CD-ROM and Handbook.
Pressesuniversitaires de Louvain, Louvain-la-Neuve.Hammersley, Ben.
2005.
Developing Feeds with RSSand Atom.
O?Reilly, Sebastopol, CA.Kilgarriff, Adam and Gregory Grefenstette.
2003.Introduction to the Special Issue on the Web asCorpus.
Computational Linguistics, Vol.
29(3):333-348.Paquot, Magali and C?drick Fairon.
(forthcoming).Investigating L1-induced learner variability: Usingthe Web as a source of L1 comparable data.Paumier, S?bastien.
2003.
De la reconnaissance deformes linguistiques ?
l'analyse syntaxique, Ph.D.,Universit?
de Marne-la-Vall?e.Renouf, Antoinette.
1993.
'A Word in Time: first find-ings from the investigation of dynamic text'.
In J.Aarts, P. de Haan and N. Oostdijk (eds), EnglishLanguage Corpora: Design, Analysis and Exploi-tation, Rodopi, Amsterdam, pp.
279-288.Renouf, Antoinette.
2003.
'WebCorp: providing arenewable energy source for corpus linguistics'.
InS.
Granger and S. Petch-Tyson (eds), Extending thescope of corpus-based research: new applications,new challenges, Rodopi , Amsterdam, pp.
39-58.Sekiguchi, Youichi and Kazuhide Yamamoto.
2004.
'Improving Quality of the Web Corpus'.
In Pro-ceedings of The First International Joint Confer-ence on Natural Language Processing (IJCNLP-04), pp.
201-206.Emirkanian Louisette, Christophe Fouquer?
and Fab-rice Issac.
2004.
Corpus issus du Web : analyse despertinences th?matique et informationnelle.
In G.Purnelle, C. Fairon and A. Dister (eds), Le Poidsdes mots.
Actes des 7?mes Journ?es internationalesd'analyse statistique des donn?es textuelles (JADT2004), Presses universitaires de Louvain, Louvain-La-Neuve, pp.
390-398.4950
