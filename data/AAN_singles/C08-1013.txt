Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 97?104Manchester, August 2008ParaMetric: An Automatic Evaluation Metric for ParaphrasingChris Callison-BurchCenter for Speech and Language ProcessingJohns Hopkins University3400 N. Charles St.Baltimore, MD 21218Trevor Cohn Mirella LapataSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh, EH8 9LWAbstractWe present ParaMetric, an automatic eval-uation metric for data-driven approaches toparaphrasing.
ParaMetric provides an ob-jective measure of quality using a collec-tion of multiple translations whose para-phrases have been manually annotated.ParaMetric calculates precision and recallscores by comparing the paraphrases dis-covered by automatic paraphrasing tech-niques against gold standard alignments ofwords and phrases within equivalent sen-tences.
We report scores for several estab-lished paraphrasing techniques.1 IntroductionParaphrasing is useful in a variety of natural lan-guage processing applications including naturallanguage generation, question answering, multi-document summarization and machine translationevaluation.
These applications require paraphrasesfor a wide variety of domains and language us-age.
Therefore building hand-crafted lexical re-sources such as WordNet (Miller, 1990) would befar too laborious.
As such, a number of data-drivenapproaches to paraphrasing have been developed(Lin and Pantel, 2001; Barzilay and McKeown,2001; Barzilay and Lee, 2003; Pang et al, 2003;Quirk et al, 2004; Bannard and Callison-Burch,2005).
Despite this spate of research, no objectiveevaluation metric has been proposed.In absence of a repeatable automatic evaluation,the quality of these paraphrasing techniques wasgauged using subjective manual evaluations.
Sec-tion 2 gives a survey of the various evaluationmethodologies used in previous research.
It hasnot been possible to directly compare paraphrasingc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.techniques, because each one was evaluated usingits own idiosyncratic experimental design.
More-over, because these evaluations were performedmanually, they are difficult to replicate.We introduce an automatic evaluation metric,called ParaMetric, which uses paraphrasing tech-niques to be compared and enables an evaluationto be easily repeated in subsequent research.
Para-Metric utilizes data sets which have been annotatedwith paraphrases.
ParaMetric compares automaticparaphrases against reference paraphrases.In this paper we:?
Present a novel automatic evaluation metricfor data-driven paraphrasing methods;?
Describe how manual alignments are cre-ated by annotating correspondences betweenwords in multiple translations;?
Show how phrase extraction heuristics fromstatistical machine translation can be used toenumerate paraphrases from the alignments;?
Report ParaMetric scores for a number of ex-isting paraphrasing methods.2 Related WorkNo consensus has been reached with respect to theproper methodology to use when evaluating para-phrase quality.
This section reviews past methodsfor paraphrase evaluation.Researchers usually present the quality of theirautomatic paraphrasing technique in terms of asubjective manual evaluation.
These have useda variety of criteria.
For example, Barzilayand McKeown (2001) evaluated their paraphrasesby asking judges whether paraphrases were ?ap-proximately conceptually equivalent.?
Ibrahimet al (2003) asked judges whether their para-phrases were ?roughly interchangeable given thegenre.?
Bannard and Callison-Burch (2005) re-placed phrases with paraphrases in a number of97sentences and asked judges whether the substi-tutions ?preserved meaning and remained gram-matical.?
These subjective evaluations are rathervaguely defined and not easy to reproduce.Others evaluate paraphrases in terms of whetherthey improve performance on particular tasks.Callison-Burch et al (2006b) measure improve-ments in translation quality in terms of Bleu score(Papineni et al, 2002) and in terms of subjectivehuman evaluation when paraphrases are integratedinto a statistical machine translation system.
Linand Pantel (2001) manually judge whether a para-phrase might be used to answer questions from theTREC question-answering track.
To date, no onehas used task-based evaluation to compare differ-ent paraphrasing methods.
Even if such an eval-uation were performed, it is unclear whether theresults would hold for a different task.
Because ofthis, we strive for a general evaluation rather thana task-specific one.Dolan et al (2004) create a set of manual wordalignments between pairs of English sentences.We create a similar type of data, as described inSection 4.
Dolan et al use heuristics to draw pairsof English sentences from a comparable corpusof newswire articles, and treat these as potentialparaphrases.
In some cases these sentence pairsare good examples of paraphrases, and in somecases they are not.
Our data differs because itis drawn from multiple translations of the sameforeign sentences.
Barzilay (2003) suggested thatmultiple translations of the same foreign sourcetext were a perfect source for ?naturally occur-ring paraphrases?
because they are samples of textwhich convey the same meaning but are producedby different writers.
That being said, it may bepossible to use Dolan et als data toward a similarend.
Cohn et al (to appear) compares the use ofthe multiple translation corpus with the MSR cor-pus for this task.The work described here is similar to work insummarization evaluation.
For example, in thePyramid Method (Nenkova et al, 2007) contentunits that are similar across human-generated sum-maries are hand-aligned.
These can have alter-native wordings, and are manually grouped.
Theidea of capturing these and building a resource forevaluating summaries is in the same spirit as ourmethodology.3 Challenges for Evaluating ParaphrasesAutomaticallyThere are several problems inherent to automati-cally evaluating paraphrases.
First and foremost,developing an exhaustive list of paraphrases forany given phrase is difficult.
Lin and Pantel (2001)illustrate the difficulties that people have generat-ing a complete list of paraphrases, reporting thatthey missed many examples generated by a sys-tem that were subsequently judged to be correct.
Ifa list of reference paraphrases is incomplete, thenusing it to calculate precision will give inaccuratenumbers.
Precision will be falsely low if the sys-tem produces correct paraphrases which are not inthe reference list.
Additionally, recall is indeter-minable because there is no way of knowing howmany correct paraphrases exist.There are further impediments to automaticallyevaluating paraphrases.
Even if we were able tocome up with a reasonably exhaustive list of para-phrases for a phrase, the acceptability of each para-phrase would vary depending on the context ofthe original phrase (Szpektor et al, 2007).
Whilelexical and phrasal paraphrases can be evaluatedby comparing them against a list of known para-phrases (perhaps customized for particular con-texts), this cannot be naturally done for struc-tural paraphrases which may transform whole sen-tences.We attempt to resolve these problems by hav-ing annotators indicate correspondences in pairsof equivalent sentences.
Rather than having peo-ple enumerate paraphrases, we asked that they per-form the simper task of aligning paraphrases.
Af-ter developing these manual ?gold standard align-ments?
we can gauge how well different automaticparaphrases are at aligning paraphrases withinequivalent sentences.
By evaluating the perfor-mance of paraphrasing techniques at alignment,rather than at matching a list of reference para-phrases, we obviate the need to have a completeset of paraphrases.We describe how sets of reference paraphrasescan be extracted from the gold standard align-ments.
While these sets will obviously be frag-mentary, we attempt to make them more completeby aligning groups of equivalent sentences ratherthan only pairs.
The paraphrase sets that we extractare appropriate for the particular contexts.
More-over they may potentially be used to study struc-tural paraphrases, although we do not examine that98andhimimpeachtowantsome.downsteptohimexpectothers.resigntohimwantotherswhile,himimpeachtoproposepeoplesomeandhimimpeachtowantsome.downsteptohimexpectothers.resignationhistendertohimwantwhothoseandhimimpeachingproposewhothosearethere.voluntarilyofficeleavetohimwantsomeandhimagainstindictmentanproposingaresomeandhimimpeachtowantsome.downsteptohimexpectothersFigure 1: Pairs of English sentences were alignedby hand.
Black squares indicate paraphrase corre-spondences.in this paper.4 Manually Aligning ParaphrasesWe asked monolingual English speakers to aligncorresponding words and phrases across pairs ofequivalent English sentences.
The English sen-tences were equivalent because they were transla-tions of the same foreign language text created bydifferent human translators.
Our annotators wereinstructed to align parts of the sentences whichhad the same meaning.
Annotators were askedto prefer smaller one-to-one word alignments, butwere allowed to create one-to-many and many-to-many alignments where appropriate.
They weregiven a set of annotation guidelines covering spe-cial cases such as repetition, pronouns, genitives,phrasal verbs and omissions (Callison-Burch et al,2006a).
The manual correspondences are treatedas gold standard alignments.We use a corpus that contains eleven En-glish translations of Chinese newswire documents,which were commissioned from different transla-tion agencies by the Linguistics Data Consortium1.The data was created for the Bleu machine trans-lation evaluation metric (Papineni et al, 2002),which uses multiple translations as a way of cap-turing allowable variation in translation.
Whereasthe Bleu metric requires no further information,our method requires a one-off annotation to explic-itly show which parts of the multiple translationsconstitute paraphrases.The rationale behind using a corpus with eleventranslations was that a greater number of transla-tions would likely result in a greater number ofparaphrases for each phrase.
Figure 1 shows thealignments that were created between one sen-tence and three of its ten corresponding transla-tions.
Table 1 gives a list of non-identical wordsand phrases that can be paired by way of the wordalignments.
These are the basic paraphrases con-tained within the three sentence pairs.
Each phrasehas up to three paraphrases.
The maximum num-ber of paraphrases for a given span in each sen-tence is bounded by the number of equivalent sen-tences that it is paired with.In addition to these basic paraphrases, longerparaphrases can also be obtained using the heuris-tic presented in Och and Ney (2004) for extract-ing phrase pairs (PP) from word alignments A, be-tween a foreign sentence fJ1and an English sen-1See LDC catalog number 2002T01.99some some people, there are those whowant propose, are proposingto impeach an indictment against, impeach-ingand whileothers some, those whoexpect wantstep down resign, leave office voluntarily,tender his resignationTable 1: Non-identical words and phrases whichare identified as being in correspondence by thealignments in Figure 1.tence eI1:PP (fJ1, eI1, A) = {(fj+mj, ei+ni) :?
(i?, j?)
?
A : j ?
j??
j + m ?
i ?
i??
i + n??
(i?, j?)
?
A : j ?
j??
j + m?
?
i ?
i??
i + n}When we apply the phrase extraction heuris-tic to aligned English sentences, we add the con-straint fj+mj6= ei+nito exclude phrases that areidentical.
This heuristic would allow ?some peo-ple propose to impeach him,?
?some are proposingan indictment against him,?
and ?there are thosewho propose impeaching him?
to be extractedas paraphrases of ?some want to impeach him.
?The heuristic extracts a total of 142 non-identicalphrase pairs from the three sentences given in Fig-ure 1.For the results reported in this paper, annotatorsaligned 50 groups of 10 pairs of equivalent sen-tences, for a total of 500 sentence pairs.
Thesewere assembled by pairing the first of the LDCtranslations with the other ten (i.e.
1-2, 1-3, 1-4,..., 1-11).
The choice of pairing one sentence withthe others instead of doing all pairwise combina-tions was made simply because the latter wouldnot seem to add much information.
However, thechoice of using the first translator as the key wasarbitrary.Annotators corrected a set of automatic wordalignments that were created using Giza++ (Ochand Ney, 2003), which was trained on a total of109,230 sentence pairs created from all pairwisecombinations of the eleven translations of 993 Chi-nese sentences.The average amount of time spent on each ofthe sentence pairs was 77 seconds, with just overeleven hours spent to annotate all 500 sentencepairs.
Although each sentence pair in our dataset was annotated by a single annotator, Cohn etal.
(to appear) analyzed the inter-annotator agree-ment for randomly selected phrase pairs from thesame corpus, and found inter-annotator agreementof?C = 0.85 over the aligned words and?C = 0.63over the alignments between basic phrase pairs,where?C is measure of inter-rater agreement in thestyle of Kupper and Hafner (1989).5 ParaMetric ScoresWe can exploit the manually aligned data to com-pute scores in two different fashions.
First, wecan calculate how well an automatic paraphrasingtechnique is able to align the paraphrases in a sen-tence pair.
Second, we can calculate the lower-bound on precision for a paraphrasing techniqueand its relative recall by enumerating the para-phrases from each of the sentence groups.
The firstof these score types does not require groups of sen-tences, only pairs.We calculate alignment accuracy by comparingthe manual alignments for the sentence pairs in thetest corpus with the alignments that the automaticparaphrasing techniques produce for the samesentence pairs.
We enumerate all non-identicalphrase pairs within the manually word-alignedsentence pairs and within the automatically wordaligned sentence pairs using PP .
We calculate theprecision and recall of the alignments by takingthe intersection of the paraphrases extracted fromthe manual alignments M , and the paraphrasesextracted from a system?s alignments S:AlignPrec=?<e1,e2>?C|PP (e1, e2, S) ?
PP (e1, e2,M)|?<e1,e2>?C|PP (e1, e2, S)|AlignRecall=?<e1,e2>?C|PP (e1, e2, S) ?
PP (e1, e2,M)|?<e1,e2>?C|PP (e1, e2,M)|Where e1, e2are pairs of English sentence fromthe test corpus.Measuring a paraphrasing method?s perfor-mance on the task of aligning the paraphrases issomewhat different than what most paraphrasingmethods do.
Most methods produce a list of para-phrases for a given input phrase, drawing froma large set of rules or a corpus larger than oursmall test set.
We therefore also attempt to mea-sure precision and recall by comparing the set of100paraphrases that method M produces for phrase pthat occurs in sentence s. We denote this set asparaM(p, s), where s is an optional argument formethods that constrain their paraphrases based oncontext.Our reference sets of paraphrases are generatedin a per group fashion.
We enumerate the referenceparaphrases for phrase p in sentence s in group GasparaREF(p1, s1, G) ={p2: ?
(p1, p2) ?
?<s1,s2,A>?GPP (s1, s2, A)}The maximum size of this set is the number ofsentence pairs in G. Because this set of referenceparaphrases is incomplete, we can only calculatea lower bound on the precision of a paraphrasingmethod and its recall relative to the referenceparaphrases.
We call these LB-Precision andRel-Recall and calculate them as follows:LB-Precision =?<s,G>?C?p?s|paraM(p, s) ?
paraREF(p1, s,G)||paraM(p, s)|Rel-Recall =?<s,G>?C?p?s|paraM(p, s) ?
paraREF(p1, s,G)||paraREF(p1, s,G)|For these metrics we require the test corpusC to be a held-out set and restrict the automaticparaphrasing techniques from drawing paraphrasesfrom it.
The idea is instead to see how well thesetechniques are able to draw paraphrases from theother sources of data which they would normallyuse.6 Paraphrasing TechniquesThere are a number of established methods forextracting paraphrases from data.
We describethe following methods in this section and evaluatethem in the next:?
Pang et al (2003) used syntactic alignment tomerge parse trees of multiple translations,?
Quirk et al (2004) treated paraphrasing asmonolingual statistical machine translation,?
Bannard and Callison-Burch (2005) usedbilingual parallel corpora to extract para-phrases.SNP VPNNpersonsAUXwereCD12VPVBkilledSNP VPNNpeopleVBdiedCDtwelveVBNP VPCD NN12twelvepeoplepersons...were...died...killedAUX VPBEG END12twelvepeoplepersonsdiedwerekilledTree 1 Tree 2+Parse ForestWord LatticeMergeLinearizeFigure 2: Pang et al (2003) created word graphsby merging parse trees.
Paths with the same startand end nodes are treated as paraphrases.Pang et al (2003) use multiple translations tolearn paraphrases using a syntax-based alignmentalgorithm, illustrated in Figure 2.
Parse trees weremerged into forests by grouping constituents of thesame type (for example, the two NPs and two VPsare grouped).
Parse forests were mapped onto fi-nite state word graphs by creating alternative pathsfor every group of merged nodes.
Different pathswithin the resulting word lattice are treated as para-phrases of each other.
For example, in the word lat-tice in Figure 2, people were killed, persons died,persons were killed, and people died are all possi-ble paraphrases of each other.Quirk et al (2004) treated paraphrasing as?monolingual statistical machine translation.
?They created a ?parallel corpus?
containing pairsof English sentences by drawing sentences with alow edit distance from news articles that were writ-ten about the same topic on the same date, but pub-lished by different newspapers.
They formulatedthe problem of paraphrasing in probabilistic termsin the same way it had been defined in the statisti-cal machine translation literature:e?2= argmaxe2p(e2|e1)= argmaxe2p(e1|e2)p(e2)101I do not believe in mutilating dead bodiescad?veresno soy partidaria mutilardecad?veres de inmigrantes ilegales ahogados a la playatantosarrojaEl mar ...corpsesSo many of drowned illegals get washed up on beaches ...Figure 3: Bannard and Callison-Burch (2005) ex-tracted paraphrases by equating English phrasesthat share a common translation.Where p(e1|e2) is estimated by training wordalignment models over the ?parallel corpus?
as inthe IBM Models (Brown et al, 1993), and phrasetranslations are extracted from word alignments asin the Alignment Template Model (Och, 2002).Bannard and Callison-Burch (2005) also usedtechniques from statistical machine translation toidentify paraphrases.
Rather than drawing pairsof English sentences from a comparable corpus,Bannard and Callison-Burch (2005) used bilingualparallel corpora.
They identified English para-phrases by pivoting through phrases in another lan-guage.
They located foreign language translationsof an English phrase, and treated the other En-glish translations of those foreign phrases as poten-tial paraphrases.
Figure 3 illustrates how a Span-ish phrase can be used as a point of identifica-tion for English paraphrases in this way.
Bannardand Callison-Burch (2005) defined a paraphraseprobability p(e2|e1) in terms of the translationmodel probabilities p(f |e1) and p(e2|f).
Since e1can translate as multiple foreign language phrases,they sum over f , and since multiple parallel cor-pora can be used they summed over each parallelcorpus C:e?2= arg maxe26=e1p(e2|e1)?
arg maxe26=e1?C?f in Cp(f |e1)p(e2|f)7 Comparing Paraphrasing Techniqueswith ParaMetric7.1 Training data for word alignmentsIn order to calculate AlignPrecand AlignRecallfor the different paraphrasing techniques, we hadthem automatically align the 500 manually alignedsentence pairs in our test sets.ParallelCorporaSyntacticAlignmentMonolingualSMTAlignPrec.62 .65 .73AlignRecall.11 .10 .46LB-Precision .14 .33 .68Rel-Recall .07 .03 .01Table 2: Summary results for scoring the differentparaphrasing techniques using our proposed auto-matic evaluations.Bo Pang provided syntactic alignments for the500 sentence pairs.
The word lattices combine thegroups of sentences.
When measuring alignmentquality, we took pains to try to limit the extractedphrase pairs to those which occurred in each sen-tence pair, but we acknowledge that our methodol-ogy may be flawed.We created training data for the monolingualstatistical machine translation method using allpairwise combination of eleven English transla-tions in LDC2002T01.
All combinations of theeleven translations of the 993 sentences in thatcorpus resulted in 109,230 sentence pairs with3,266,769 words on each side.
We used this datato train an alignment model, and applied it to the500 sentence pairs in our test set.We used the parallel corpus method to aligneach pair of English sentences by creating interme-diate alignments through their Chinese source sen-tences.
The bilingual word alignment model wastrained on a Chinese-English parallel corpus fromthe NIST MT Evaluation consisting of 40 millionwords.
This was used to align the 550 Chinese-English sentence pairs constructed from the testset.7.2 Training data for precision and recallEach of the paraphrasing methods generated para-phrases for LB-Precision and Rel-Recall us-ing larger training sets of data than for the align-ments.
For the syntax-based alignment method,we excluded the 50 word lattices correspondingto the test set.
We used the remaining 849 lat-tices for the LDC multiple translation corpus.For the monolingual statistical machine transla-tion method, we downloaded the Microsoft Re-search Paraphrase Phrase Table, which containedparaphrases for nearly 9 million phrases, gener-102ated from the method described in Quirk et al(2004).
For the parallel corpus method, we de-rived paraphrases from the entire Europarl corpus,which contains parallel corpora between Englishand 10 other languages, with approximately 30million words per language.
We limited both theQuirk et al (2004) and the Bannard and Callison-Burch (2005) paraphrases to those with a probabil-ity greater than or equal to 1%.7.3 ResultsTable 2 gives a summary of how each of the para-phrasing techniques scored using the four differentautomatic metrics.
The precision of their align-ments was in the same ballpark, with each para-phrasing method reaching above 60%.
The mono-lingual SMT method vastly outstripped the othersin terms of recall and therefore seems to be thebest on the simplified task of aligning paraphraseswithin pairs of equivalent sentences.For the task of generating paraphrases from un-restricted resources, the monolingual SMT methodagain had the highest precision, although timetime its recall was quite low.
The 500 manuallyaligned sentence pairs contained 14,078 uniqueparaphrases for phrases of 5 words or less.
Themonolingual SMT method only posited 230 para-phrases with 156 of them being correct.
By con-trast, the syntactic alignment method posited 1,213with 399 correct, and the parallel corpus methodposited 6,914 with 998 correct.
Since the refer-ence lists are incomplete by their very nature, theLB-Precision score gives a lower-bound on theprecision, and the Rel-Recall gives recall onlywith respect to the partial list of paraphrases.Table 3 gives the performance of the differ-ent paraphrasing techniques for different phraselengths.8 ConclusionsIn this paper we defined a number of automaticscores for data-driven approaches to paraphrasing,which we collectively dub ?ParaMetric?.
We dis-cussed the inherent difficulties in automatically as-sessing paraphrase quality.
These are due primar-ily to the fact that it is exceedingly difficult tocreate an exhaustive list of paraphrases.
To ad-dress this problem, we introduce an artificial taskof aligning paraphrases within pairs of equivalentEnglish sentences, which guarantees accurate pre-cision and recall numbers.
In order to measurealignment quality, we create a set of gold standardalignments.
While the creation of this data doesrequire some effort, it seems to be a manageableamount, and the inter-annotator agreement seemsreasonable.Since alignment is not perfectly matched withwhat we would like automatic paraphrasing tech-niques to do, we also use the gold standard align-ment data to measure a lower bound on the preci-sion of a method?s paraphrases, as well as its recallrelative to the limited set of paraphrases.
Futurestudies should examine how well these scores rankdifferent paraphrasing methods when compared tohuman judgments.
Follow up work should inves-tigate the number of equivalent English sentencesthat are required for reasonably complete lists ofparaphrases.
In this work we aligned sets of elevendifferent English sentences, but we acknowledgethat such a data set is rare and might make it dif-ficult to port this method to other domains or lan-guages.The goal of this work is to develop a set ofscores that both allows different paraphrasing tech-niques to be compared objectively and provides aneasily repeatable method for automatically evalu-ating paraphrases.
This has hitherto not been pos-sible.
The availability of an objective, automaticevaluation metric for paraphrasing has the poten-tial to impact research in the area in a number ofways.
It not only allows for the comparison of dif-ferent approaches to paraphrasing, as shown in thispaper, but also provides a way to tune the parame-ters of a single system in order to optimize its qual-ity.AcknowledgmentsThe authors are grateful to Bo Pang for providingthe word lattices from her method, to Stefan Rie-zler for his comments on an early draft of this pa-per, and to Michelle Bland for proofreading.
Thiswork was supported by the National Science Foun-dation under Grant No.
0713448.
The views andfindings are the authors?
alone.ReferencesBannard, Colin and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of the 43rd Annual Meeting of the Associationfor Computational Linguistics (ACL-2005), Ann Ar-bor, Michigan.Barzilay, Regina and Lillian Lee.
2003.
Learn-ing to paraphrase: An unsupervised approach us-103AlignPrecAlignRecallLB-Precision Rel-RecallParallelCorporaSyntacticAlignmentMonolingualSMTParallelCorporaSyntacticAlignmentMonolingualSMTParallelCorporaSyntacticAlignmentMonolingualSMTParallelCorporaSyntacticAlignmentMonolingualSMTLength = 1 .54 .48 .64 .24 .18 .56 .15 .25 .59 .20 .16 .02Length ?
2 .56 .56 .69 .19 .13 .52 .15 .31 .66 .18 .10 .03Length ?
3 .59 .60 .71 .14 .12 .49 .15 .32 .66 .13 .06 .02Length ?
4 .60 .63 .72 .12 .11 .48 .14 .33 .68 .09 .04 .01Length ?
5 .62 .65 .73 .11 .10 .46 .14 .33 .68 .07 .03 .01Table 3: Results for paraphrases of continuous subphrases of various lengths.ing multiple-sequence alignment.
In Proceedings ofHLT/NAACL-2003, Edmonton, Alberta.Barzilay, Regina and Kathleen McKeown.
2001.
Ex-tracting paraphrases from a parallel corpus.
In Pro-ceedings of the 39th Annual Meeting of the Associa-tion for Computational Linguistics (ACL-2001).Barzilay, Regina.
2003.
Information Fusion for Mutli-document Summarization: Paraphrasing and Gener-ation.
Ph.D. thesis, Columbia University, New York.Brown, Peter, Stephen Della Pietra, Vincent DellaPietra, and Robert Mercer.
1993.
The mathematicsof machine translation: Parameter estimation.
Com-putational Linguistics, 19(2):263?311, June.Callison-Burch, Chris, Trevor Cohn, and Mirella Lap-ata.
2006a.
Annotation guidelines for paraphrasealignment.
Tech report, University of Edinburgh.Callison-Burch, Chris, Philipp Koehn, and Miles Os-borne.
2006b.
Improved statistical machinetranslation using paraphrases.
In Proceedings ofHLT/NAACL-2006, New York, New York.Cohn, Trevor, Chris Callison-Burch, and Mirella Lap-ata.
to appear.
Constructing corpora for the develop-ment and evaluation of paraphrase systems.
Compu-tational Linguistics.Dolan, Bill, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: Exploiting massively parallel news sources.
InProceedings of the 20th International Conference onComputational Linguistics.Ibrahim, Ali, Boris Katz, and Jimmy Lin.
2003.
Ex-tracting structural paraphrases from aligned mono-lingual corpora.
In Proceedings of the Second Inter-national Workshop on Paraphrasing (ACL 2003).Kupper, Lawrence L. and Kerry B. Hafner.
1989.
Onassessing interrater agreement for multiple attributeresponses.
Biometrics, 45(3):957?967.Lin, Dekang and Patrick Pantel.
2001.
Discovery ofinference rules from text.
Natural Language Engi-neering, 7(3):343?360.Miller, George A.
1990.
Wordnet: An on-line lexicaldatabase.
Special Issue of the International Journalof Lexicography, 3(4).Nenkova, Ani, Rebecca Passonneau, and KathleenMcKeown.
2007.
The pyramid method: incorporat-ing human content selection variation in summariza-tion evaluation.
ACM Transactions on Speech andLanguage Processing, 4(2).Och, Franz Josef and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Och, Franz Josef and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational Linguistics, 30(4):417?449.Och, Franz Josef.
2002.
Statistical Machine Transla-tion: From Single-Word Models to Alignment Tem-plates.
Ph.D. thesis, RWTH Aachen Department ofComputer Science, Aachen, Germany.Pang, Bo, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations: Ex-tracting paraphrases and generating new sentences.In Proceedings of HLT/NAACL-2003, Edmonton,Alberta.Papineni, Kishore, Salim Roukos, ToddWard, andWei-Jing Zhu.
2002.
Bleu: A method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics (ACL-2002), Philadelphia, Penn-sylvania.Quirk, Chris, Chris Brockett, and William Dolan.2004.
Monolingual machine translation for para-phrase generation.
In Proceedings of the 2004 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP-2004), Barcelona, Spain.Szpektor, Idan, Eyal Shnarch, and Ido Dagan.
2007.Instance-based evaluation of entailment rule acquisi-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics (ACL-2007), Prague, Czech Republic.104
