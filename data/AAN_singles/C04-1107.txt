Probabilistic Sentence Reduction Using Support Vector MachinesMinh Le Nguyen, Akira Shimazu, Susumu HoriguchiBao Tu Ho and Masaru FukushiJapan Advanced Institute of Science and Technology1-8, Tatsunokuchi, Ishikawa, 923-1211, JAPAN{nguyenml, shimazu, hori, bao, mfukushi}@jaist.ac.jpAbstractThis paper investigates a novel application of sup-port vector machines (SVMs) for sentence reduction.We also propose a new probabilistic sentence reduc-tion method based on support vector machine learn-ing.
Experimental results show that the proposedmethods outperform earlier methods in term of sen-tence reduction performance.1 IntroductionThe most popular methods of sentence reduc-tion for text summarization are corpus basedmethods.
Jing (Jing 00) developed a methodto remove extraneous phrases from sentencesby using multiple sources of knowledge to de-cide which phrases could be removed.
However,while this method exploits a simple model forsentence reduction by using statistics computedfrom a corpus, a better model can be obtainedby using a learning approach.Knight and Marcu (Knight and Marcu 02)proposed a corpus based sentence reductionmethod using machine learning techniques.They discussed a noisy-channel based approachand a decision tree based approach to sentencereduction.
Their algorithms provide the bestway to scale up the full problem of sentence re-duction using available data.
However, these al-gorithms require that the word order of a givensentence and its reduced sentence are the same.Nguyen and Horiguchi (Nguyen and Horiguchi03) presented a new sentence reduction tech-nique based on a decision tree model withoutthat constraint.
They also indicated that se-mantic information is useful for sentence reduc-tion tasks.The major drawback of previous works onsentence reduction is that those methods arelikely to output local optimal results, which mayhave lower accuracy.
This problem is caused bythe inherent sentence reduction model; that is,only a single reduced sentence can be obtained.As pointed out by Lin (Lin 03), the best sen-tence reduction output for a single sentence isnot approximately best for text summarization.This means that ?local optimal?
refers to thebest reduced output for a single sentence, whilethe best reduced output for the whole text is?global optimal?.
Thus, it would be very valu-able if the sentence reduction task could gener-ate multiple reduced outputs and select the bestone using the whole text document.
However,such a sentence reduction method has not yetbeen proposed.Support Vector Machines (Vapnik 95), on theother hand, are strong learning methods in com-parison with decision tree learning and otherlearning methods (Sholkopf 97).
The goal ofthis paper is to illustrate the potential of SVMsfor enhancing the accuracy of sentence reduc-tion in comparison with previous work.
Accord-ingly, we describe a novel deterministic methodfor sentence reduction using SVMs and a two-stage method using pairwise coupling (Hastie98).
To solve the problem of generating mul-tiple best outputs, we propose a probabilisticsentence reduction model, in which a variant ofprobabilistic SVMs using a two-stage methodwith pairwise coupling is discussed.The rest of this paper will be organized asfollows: Section 2 introduces the Support Vec-tor Machines learning.
Section 3 describes theprevious work on sentence reduction and ourdeterministic sentence reduction using SVMs.We also discuss remaining problems of deter-ministic sentence reduction.
Section 4 presentsa probabilistic sentence reduction method usingsupport vector machines to solve this problem.Section 5 discusses implementation and our ex-perimental results; Section 6 gives our conclu-sions and describes some problems that remainto be solved in the future.2 Support Vector MachineSupport vector machine (SVM)(Vapnik 95) is atechnique of machine learning based on statisti-cal learning theory.
Suppose that we are givenl training examples (xi, yi), (1 ?
i ?
l), wherexi is a feature vector in n dimensional featurespace, yi is the class label {-1, +1 } of xi.
SVMfinds a hyperplane w.x + b = 0 which correctlyseparates the training examples and has a max-imum margin which is the distance between twohyperplanes w.x+ b ?
1 and w.x+ b ?
?1.
Theoptimal hyperplane with maximum margin canbe obtained by solving the following quadraticprogramming.min 12 ?w?+ C0l?i?is.t.
yi(w.xi + b) ?
1?
?i?i ?
0(1)where C0 is the constant and ?i is a slack vari-able for the non-separable case.
In SVM, theoptimal hyperplane is formulated as follows:f(x) = sign( l?1?iyiK(xi, x) + b)(2)where ?i is the Lagrange multiple, andK(x?, x??)
is a kernel function, the SVM calcu-lates similarity between two arguments x?
andx??.
For instance, the Polynomial kernel func-tion is formulated as follow:K(x?, x??)
= (x?.x??
)p (3)SVMs estimate the label of an unknown ex-ample x whether the sign of f(x) is positive ornot.3 Deterministic Sentence ReductionUsing SVMs3.1 Problem DescriptionIn the corpus-based decision tree approach, agiven input sentence is parsed into a syntax treeand the syntax tree is then transformed into asmall tree to obtain a reduced sentence.Let t and s be syntax trees of the original sen-tence and a reduced sentence, respectively.
Theprocess of transforming syntax tree t to smalltree s is called ?rewriting process?
(Knight andMarcu 02), (Nguyen and Horiguchi 03).
Totransform the syntax tree t to the syntax trees, some terms and five rewriting actions are de-fined.An Input list consists of a sequence of wordssubsumed by the tree t where each word in theInput list is labelled with the name of all syntac-tic constituents in t. Let CSTACK be a stackthat consists of sub trees in order to rewrite asmall tree.
Let RSTACK be a stack that con-sists of sub trees which are removed from theInput list in the rewriting process.?
SHIFT action transfers the first word from theInput list into CSTACK.
It is written mathe-matically and given the label SHIFT.?
REDUCE(lk,X) action pops the lk syntactictrees located at the top of CSTACK and com-bines them in a new tree, where lk is an integerand X is a grammar symbol.?
DROP X action moves subsequences of wordsthat correspond to syntactic constituents fromthe Input list to RSTACK.?
ASSIGN TYPE X action changes the label oftrees at the top of the CSTACK.
These POStags might be different from the POS tags inthe original sentence.?
RESTORE X action takes the X element inRSTACK and moves it into the Input list,where X is a subtree.For convenience, let configuration be a statusof Input list, CSTACK and RSTACK.
Let cur-rent context be the important information in aconfiguration.
The important information aredefined as a vector of features using heuristicmethods as in (Knight and Marcu 02), (Nguyenand Horiguchi 03).The main idea behind deterministic sentencereduction is that it uses a rule in the currentcontext of the initial configuration to select adistinct action in order to rewrite an input sen-tence into a reduced sentence.
After that, thecurrent context is changed to a new context andthe rewriting process is repeated for selectingan action that corresponds to the new context.The rewriting process is finished when it meetsa termination condition.
Here, one rule corre-sponds to the function that maps the currentcontext to a rewriting action.
These rules arelearned automatically from the corpus of longsentences and their reduced sentences (Knightand Marcu 02), (Nguyen and Horiguchi 03).3.2 ExampleFigure 1 shows an example of applying a se-quence of actions to rewrite the input sentence(a, b, c, d, e), when each character is a word.
Itillustrates the structure of the Input list, twostacks, and the term of a rewriting process basedon the actions mentioned above.
For example,in the first row, DROP H deletes the sub-treewith its root node H in the Input list and storesit in the RSTACK.
The reduced tree s can beobtained after applying a sequence of actionsas follows: DROP H; SHIFT; ASSIGN TYPE K;DROP B; SHIFT; ASSIGN TYPE H; REDUCE 2F; RESTORE H; SHIFT; ASSIGN TYPE D; RE-DUCE 2G.
In this example, the reduced sentenceis (b, e, a).Figure 1: An Example of the Rewriting Process3.3 Learning Reduction Rules UsingSVMsAs mentioned above, the action for each config-uration can be decided by using a learning rule,which maps a context to an action.
To obtainsuch rules, the configuration is represented bya vector of features with a high dimension.
Af-ter that, we estimate the training examples byusing several support vector machines to dealwith the multiple classification problem in sen-tence reduction.3.3.1 FeaturesOne important task in applying SVMs to textsummarization is to define features.
Here, wedescribe features used in our sentence reductionmodels.The features are extracted based on the cur-rent context.
As it can be seen in Figure 2, acontext includes the status of the Input list andthe status of CSTACK and RSTACK.
We de-fine a set of features for a current context asdescribed bellow.Operation featureThe set of features as described in (Nguyen andHoriguchi 03) are used in our sentence reductionmodels.Original tree featuresThese features denote the syntactic constituentsFigure 2: Example of Configurationthat start with the first unit in the Input list.For example, in Figure 2 the syntactic con-stituents are labels of the current element in theInput list from ?VP?
to the verb ?convince?.Semantic featuresThe following features are used in our model assemantic information.?
Semantic information about current wordswithin the Input list; these semantic typesare obtained by using the named entities suchas Location, Person, Organization and Timewithin the input sentence.
To define thesename entities, we use the method described in(Borthwick 99).?
Semantic information about whether or not theword in the Input list is a head word.?
Word relations, such as whether or not a wordhas a relationship with other words in the sub-categorization table.
These relations and thesub-categorization table are obtained using theCommlex database (Macleod 95).Using the semantic information, we are able toavoid deleting important segments within thegiven input sentence.
For instance, the mainverb, the subject and the object are essentialand for the noun phrase, the head noun is essen-tial, but an adjective modifier of the head nounis not.
For example, let us consider that theverb ?convince?
was extracted from the Com-lex database as follows.convinceNP-PP: PVAL (?of?
)NP-TO-INF-OCThis entry indicates that the verb ?convince?can be followed by a noun phrase and a preposi-tional phrase starting with the preposition ?of?.It can be also followed by a noun phrase and ato-infinite phrase.
This information shows thatwe cannot delete an ?of?
prepositional phraseor a to-infinitive that is the part of the verbphrase.3.3.2 Two-stage SVM Learning usingPairwise CouplingUsing these features we can extract trainingdata for SVMs.
Here, a sample in our trainingdata consists of pairs of a feature vector andan action.
The algorithm to extract trainingdata from the training corpus is modified usingthe algorithm described in our pervious work(Nguyen and Horiguchi 03).Since the original support vector machine(SVM) is a binary classification method, whilethe sentence reduction problem is formulated asmultiple classification, we have to find a methodto adapt support vector machines to this prob-lem.
For multi-class SVMs, one can use strate-gies such as one-vs all, pairwise comparison orDAG graph (Hsu 02).
In this paper, we use thepairwise strategy, which constructs a rule fordiscriminating pairs of classes and then selectsthe class with the most winning among two classdecisions.To boost the training time and the sentencereduction performance, we propose a two-stageSVM described below.Suppose that the examples in training dataare divided into five groups m1,m2, ...,m5 ac-cording to their actions.
Let Svmc be multi-class SVMs and let Svmc-i be multi-class SVMsfor a group mi.
We use one Svmc classifier toidentify the group to which a given context eshould be belong.
Assume that e belongs tothe group mi.
The classifier Svmc-i is then usedto recognize a specific action for the context e.The five classifiers Svmc-1, Svmc-2,..., Svmc-5are trained by using those examples which haveactions belonging to SHIFT, REDUCE, DROP,ASSIGN TYPE and RESTORE.Table 1 shows the distribution of examples infive data groups.3.4 Disadvantage of DeterministicSentence ReductionsThe idea of the deterministic algorithm is touse the rule for each current context to selectthe next action, and so on.
The process termi-nates when a stop condition is met.
If the earlysteps of this algorithm fail to select the best ac-Table 1: Distribution of example data on fivedata groupsName Number of examplesSHIFT-GROUP 13,363REDUCE-GROUP 11,406DROP-GROUP 4,216ASSIGN-GROUP 13,363RESTORE-GROUP 2,004TOTAL 44,352tions, then the possibility of obtaining a wrongreduced output becomes high.One way to solve this problem is to select mul-tiple actions that correspond to the context ateach step in the rewriting process.
However,the question that emerges here is how to deter-mine which criteria to use in selecting multipleactions for a context.
If this problem can besolved, then multiple best reduced outputs canbe obtained for each input sentence and the bestone will be selected by using the whole text doc-ument.In the next section propose a model for se-lecting multiple actions for a context in sentencereduction as a probabilistic sentence reductionand present a variant of probabilistic sentencereduction.4 Probabilistic Sentence ReductionUsing SVM4.1 The Probabilistic SVM ModelsLet A be a set of k actions A ={a1, a2...ai, ..., ak} and C be a set of n con-texts C = {c1, c2...ci, ..., cn} .
A probabilisticmodel ?
for sentence reduction will select anaction a ?
A for the context c with probabilityp?(a|c).
The p?
(a|c) can be used to score ac-tion a among possible actions A depending thecontext c that is available at the time of deci-sion.
There are several methods for estimatingsuch scores; we have called these ?probabilisticsentence reduction methods?.
The conditionalprobability p?
(a|c) is estimated using a variantof probabilistic support vector machine, whichis described in the following sections.4.1.1 Probabilistic SVMs usingPairwise CouplingFor convenience, we denote uij = p(a = ai|a =ai?aj , c).
Given a context c and an action a, weassume that the estimated pairwise class prob-abilities rij of uij are available.
Here rij canbe estimated by some binary classifiers.
Forinstance, we could estimate rij by using theSVM binary posterior probabilities as describedin (Plat 2000).
Then, the goal is to estimate{pi}ki=1 , where pi = p(a = ai|c), i = 1, 2, ..., k.For this propose, a simple estimate of theseprobabilities can be derived using the followingvoting method:pi = 2?j:j 6=iI{rij>rji}/k(k ?
1)where I is an indicator function and k(k?
1) isthe number of pairwise classes.
However, thismodel is too simple; we can obtain a better onewith the following method.Assume that uij are pairwise probabilities ofthe model subject to the condition that uij =pi/(pi+pj).
In (Hastie 98), the authors proposedto minimize the Kullback-Leibler (KL) distancebetween the rij and uijl(p) =?i 6=jnijrij log rijuij (4)where rij and uij are the probabilities of a pair-wise ai and aj in the estimated model and inour model, respectively, and nij is the numberof training data in which their classes are ai oraj .
To find the minimizer of equation (6), theyfirst calculate?l(p)?pi =?i 6=jnij(?rijpi +1pi + pj ).Thus, letting ?l(p) = 0, they proposed to finda point satisfying?j:j 6=inijuij =?j:j 6=inijrij ,k?i=1pi = 1,where i = 1, 2, ...k and pi > 0.Such a point can be obtained by using an algo-rithm described elsewhere in (Hastie 98).
Weapplied it to obtain a probabilistic SVM modelfor sentence reduction using a simple method asfollows.
Assume that our class labels belong tol groups: M = {m1,m2...mi, ...,ml} , where lis a number of groups and mi is a group e.g.,SHIFT, REDUCE ,..., ASSIGN TYPE.
Thenthe probability p(a|c) of an action a for a givencontext c can be estimated as follows.p(a|c) = p(mi|c)?
p(a|c,mi) (5)where mi is a group and a ?
mi.
Here, p(mi|c)and p(a|c,mi) are estimated by the method in(Hastie 98).4.2 Probabilistic sentence reductionalgorithmAfter obtaining a probabilistic model p, we thenuse this model to define function score, by whichthe search procedure ranks the derivation of in-complete and complete reduced sentences.
Letd(s) = {a1, a2, ...ad} be the derivation of a smalltree s, where each action ai belongs to a set ofpossible actions.
The score of s is the productof the conditional probabilities of the individualactions in its derivation.Score(s) =?ai?d(s)p(ai|ci) (6)where ci is the context in which ai was decided.The search heuristic tries to find the best re-duced tree s?
as follows:s?
= argmax?
??
?s?tree(t)Score(s) (7)where tree(t) are all the complete reduced treesfrom the tree t of the given long sentence.
As-sume that for each configuration the actions{a1, a2, ...an} are sorted in decreasing order ac-cording to p(ai|ci), in which ci is the contextof that configuration.
Algorithm 1 shows aprobabilistic sentence reduction using the topK-BFS search algorithm.
This algorithm usesa breadth-first search which does not expandthe entire frontier, but instead expands at mostthe top K scoring incomplete configurations inthe frontier; it is terminated when it finds Mcompleted reduced sentences (CL is a list of re-duced trees), or when all hypotheses have beenexhausted.
A configuration is completed if andonly if the Input list is empty and there is onetree in the CSTACK.
Note that the functionget-context(hi, j) obtains the current context ofthe jth configuration in hi, where hi is a heap atstep i.
The function Insert(s,h) ensures that theheap h is sorted according to the score of eachelement in h. Essentially, in implementation wecan use a dictionary of contexts and actions ob-served from the training data in order to reducethe number of actions to explore for a currentcontext.5 Experiments and DiscussionWe used the same corpus as described in(Knight and Marcu 02), which includes 1,067pairs of sentences and their reductions.
Toevaluate sentence reduction algorithms, we ran-domly selected 32 pairs of sentences from ourparallel corpus, which is refered to as the testcorpus.
The training corpus of 1,035 sentencesextracted 44,352 examples, in which each train-ing example corresponds to an action.
TheSVM tool, LibSVM (Chang 01) is applied totrain our model.
The training examples wereAlgorithm 1 A probabilistic sentence reductionalgorithm1: CL={Empty};i = 0; h0={ Initial configuration}2: while |CL| < M do3: if hi is empty then4: break;5: end if6: u =min(|hi|, K)7: for j = 1 to u do8: c=get-context(hi, j)9: Select m so thatm?i=1p(ai|c) < Q is maximal10: for l=1 to m do11: parameter=get-parameter(al);12: Obtain a new configuration s by performing action alwith parameter13: if Complete(s) then14: Insert(s, CL)15: else16: Insert(s, hi+1)17: end if18: end for19: end for20: i = i + 121: end whiledivided into SHIFT, REDUCE, DROP, RE-STORE, and ASSIGN groups.
To train oursupport vector model in each group, we usedthe pairwise method with the polynomial ker-nel function, in which the parameter p in (3)and the constant C0 in equation (1) are 2 and0.0001, respectively.The algorithms (Knight and Marcu 02) and(Nguyen and Horiguchi 03) served as the base-line1 and the baseline2 for comparison with theproposed algorithms.
Deterministic sentence re-duction using SVM and probabilistic sentencereduction were named as SVM-D and SVMP, re-spectively.
For convenience, the ten top reducedoutputs using SVMP were called SVMP-10.
Weused the same evaluation method as describedin (Knight and Marcu 02) to compare the pro-posed methods with previous methods.
For thisexperiment, we presented each original sentencein the test corpus to three judges who are spe-cialists in English, together with three sentencereductions: the human generated reduction sen-tence, the outputs of the proposed algorithms,and the output of the baseline algorithms.The judges were told that all outputs weregenerated automatically.
The order of the out-puts was scrambled randomly across test cases.The judges participated in two experiments.
Inthe first, they were asked to determine on a scalefrom 1 to 10 how well the systems did with re-spect to selecting the most important words inthe original sentence.
In the second, they wereasked to determine the grammatical criteria ofreduced sentences.Table 2 shows the results of English languagesentence reduction using a support vector ma-chine compared with the baseline methods andwith human reduction.
Table 2 shows compres-sion rates, and mean and standard deviation re-sults across all judges, for each algorithm.
Theresults show that the length of the reduced sen-tences using decision trees is shorter than usingSVMs, and indicate that our new methods out-perform the baseline algorithms in grammaticaland importance criteria.
Table 2 shows that theTable 2: Experiment results with Test CorpusMethod Comp Gramma ImpoBaseline1 57.19% 8.60?
2.8 7.18?
1.92Baseline2 57.15% 8.60?
2.1 7.42?
1.90SVM-D 57.65% 8.76?
1.2 7.53?
1.53SVMP-10 57.51% 8.80?
1.3 7.74?
1.39Human 64.00% 9.05?
0.3 8.50?
0.80first 10 reduced sentences produced by SVMP-10 (the SVM probabilistic model) obtained thehighest performances.
We also compared thecomputation time of sentence reduction usingsupport vector machine with that in previousworks.
Table 3 shows that the computationaltimes for SVM-D and SVMP-10 are slower thanbaseline, but it is acceptable for SVM-D.Table 3: Computational times of performing re-ductions on test-set.
Average sentence lengthwas 21 words.Method Computational times (sec)Baseline1 138.25SVM-D 212.46SVMP-10 1030.25We also investigated how sensitive the pro-posed algorithms are with respect to the train-ing data by carrying out the same experi-ment on sentences of different genres.
Wecreated the test corpus by selecting sentencesfrom the web-site of the Benton Foundation(http://www.benton.org).
The leading sen-tences in each news article were selected as themost relevant sentences to the summary of thenews.
We obtained 32 leading long sentencesand 32 headlines for each item.
The 32 sen-tences are used as a second test for our methods.We use a simple ranking criterion: the more thewords in the reduced sentence overlap with thewords in the headline, the more important thesentence is.
A sentence satisfying this criterionis called a relevant candidate.For a given sentence, we used a simplemethod, namely SVMP-R to obtain a re-duced sentence by selecting a relevant candi-date among the ten top reduced outputs usingSVMP-10.Table 4 depicts the experiment results forthe baseline methods, SVM-D, SVMP-R, andSVMP-10.
The results shows that, when ap-plied to sentence of a different genre, the per-formance of SVMP-10 degrades smoothly, whilethe performance of the deterministic sentencereductions (the baselines and SVM determinis-tic) drops sharply.
This indicates that the prob-abilistic sentence reduction using support vectormachine is more stable.Table 4 shows that the performance ofSVMP-10 is also close to the human reductionoutputs and is better than previous works.
Inaddition, SVMP-R outperforms the determin-istic sentence reduction algorithms and the dif-ferences between SVMP-R?s results and SVMP-10 are small.
This indicates that we can ob-tain reduced sentences which are relevant to theheadline, while ensuring the grammatical andthe importance criteria compared to the origi-nal sentences.Table 4: Experiment results with Benton Cor-pusMethod Comp Gramma ImpoBaseline1 54.14% 7.61?
2.10 6.74?
1.92Baseline2 53.13% 7.72?
1.60 7.02?
1.90SVM-D 56.64% 7.86?
1.20 7.23?
1.53SVMP-R 58.31% 8.25?
1.30 7.54?
1.39SVMP-10 57.62% 8.60?
1.32 7.71?
1.41Human 64.00% 9.01?
0.25 8.40?
0.606 ConclusionsWe have presented a new probabilistic sentencereduction approach that enables a long sentenceto be rewritten into reduced sentences based onsupport vector models.
Our methods achievesbetter performance when compared with earliermethods.
The proposed reduction approach cangenerate multiple best outputs.
Experimentalresults showed that the top 10 reduced sentencesreturned by the reduction process might yieldaccuracies higher than previous work.
We be-lieve that a good ranking method might improvethe sentence reduction performance further in atext.ReferencesA.
Borthwick, ?A Maximum Entropy Approachto Named Entity Recognition?, Ph.D the-sis, Computer Science Department, New YorkUniversity (1999).C.-C. Chang and C.-J.
Lin, ?LIB-SVM: a library for support vec-tor machines?, Software available athttp://www.csie.ntu.edu.tw/ cjlin/libsvm.H.
Jing, ?Sentence reduction for automatictext summarization?, In Proceedings of theFirst Annual Meeting of the North Ameri-can Chapter of the Association for Compu-tational Linguistics NAACL-2000.T.T.
Hastie and R. Tibshirani, ?Classificationby pairwise coupling?, The Annals of Statis-tics, 26(1): pp.
451-471, 1998.C.-W. Hsu and C.-J.
Lin, ?A comparison ofmethods for multi-class support vector ma-chines?, IEEE Transactions on Neural Net-works, 13, pp.
415-425, 2002.K.
Knight and D. Marcu, ?Summarization be-yond sentence extraction: A Probabilistic ap-proach to sentence compression?, ArtificialIntelligence 139: pp.
91-107, 2002.C.Y.
Lin, ?Improving Summarization Perfor-mance by Sentence Compression ?
A Pi-lot Study?, Proceedings of the Sixth Inter-national Workshop on Information Retrievalwith Asian Languages, pp.1-8, 2003.C.
Macleod and R. Grishman, ?COMMLEXsyntax Reference Manual?
; Proteus Project,New York University (1995).M.L.
Nguyen and S. Horiguchi, ?A new sentencereduction based on Decision tree model?,Proceedings of 17th Pacific Asia Conferenceon Language, Information and Computation,pp.
290-297, 2003V.
Vapnik, ?The Natural of Statistical LearningTheory?, New York: Springer-Verlag, 1995.J.
Platt,?
Probabilistic outputs for support vec-tor machines and comparison to regularizedlikelihood methods,?
in Advances in LargeMargin Classifiers, Cambridege, MA: MITPress, 2000.B.
Scholkopf et al ?Comparing Support Vec-tor Machines with Gausian Kernels to RadiusBasis Function Classifers?, IEEE Trans.
Sig-nal Procesing, 45, pp.
2758-2765, 1997.
