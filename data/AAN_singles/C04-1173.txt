Word Sense Disambiguation using a dictionary for sense similarity measureBruno Gaume Nabil Hathout Philippe MullerIRIT ?
CNRS, UPS & INPT ERSS ?
CNRS & UTM IRIT ?
CNRS, UPS & INPTToulouse, France Toulouse, France Toulouse, Francegaume@irit.fr hathout@univ-tlse2.fr muller@irit.frAbstractThis paper presents a disambiguationmethod in which word senses are deter-mined using a dictionary.
We use a seman-tic proximity measure between words in thedictionary, taking into account the wholetopology of the dictionary, seen as a graphon its entries.
We have tested the method onthe problem of disambiguation of the dic-tionary entries themselves, with promisingresults considering we do not use any priorannotated data.1 IntroductionVarious tasks dealing with natural languagedata have to cope with the numerous differentsenses possessed by every lexical item: ma-chine translation, information retrieval, infor-mation extraction ...
This very old issue is farfrom being solved, and evaluation of methodsaddressing it is far from obvious (Resnik andYarowsky, 2000).
This problem has been tack-led in a number of ways1: by looking at con-texts of use (with supervised learning or un-supervised sense clustering) or by using lexi-cal resources such as dictionaries or thesauri.The first kind of approach relies on data thatare hard to collect (supervised) or very sensitiveto the type of corpus (unsupervised).
The sec-ond kind of approach tries to exploit the lexicalknowledge that is represented in dictionaries orthesaurus, with various results from its incep-tion up to now (Lesk, 1986; Banerjee and Ped-ersen, 2003).
In all cases, a distance betweenwords or word senses is used as a way to findthe right sense in a given context.
Dictionary-based approaches usually rely on a comparisonof the set of words used in sense definitions and1A good introduction is (Ide and V?ronis, 1998), or(Manning and Sch?tze, 1999), chap.
7.in the context to disambiguate2.This paper presents an algorithm which usesa dictionary as a network of lexical items (cf.sections 2 and 3) to compute a semantic simi-larity measure between words and word senses.It takes into account the whole topology of thedictionary instead of just the entry of targetwords.
This arguably gives a certain robustnessof the results with respect to the dictionary.
Wehave begun testing this approach on word sensedisambiguation on definitions of the dictionaryitself (section 5), but the method is expectedto be more general, although this has not beentested yet.
Preliminary results are quite encour-aging considering that the method does not re-quire any prior annotated data, while operatingon an unconstrained vocabulary.2 Building the graph of adictionaryThe experiment we describe here has beenachieved on a dictionary restricted to nouns andverbs only: we considered dictionary entriesclassified as nouns and verbs and noun and verblemmas occurring within those entries.
Thebasic idea is to consider the dictionary as anundirected graph whose nodes are noun entries,and an edge exists between two nodes when-ever one of them occur in the definition of theother.
More precisely, the graph of the dictio-nary encodes two types of lexicographical in-formations: (1) the definitions of the entriessub-senses and (2) the structure of the entriesthat is the hierarchical organisation of their sub-senses.
The graph then includes two types ofnodes: w-nodes used for the words that occur2With the exceptions of the methods of (Kozima andFurugori, 1993; Ide and V?ronis, 1990), both based onmodels of activation of lexical relations, but who presentno quantified results.in the definitions and ?-nodes used for the def-initions of the sub-senses of the entries.
Thegraph is created in three phases:1.
For each dictionary entry, there is a ?-node for the entry as a whole and there isone ?-node for each of the sub-senses ofthe entry.
Then an edge is added betweeneach ?-node and the ?-nodes which rep-resent the sub-senses of the next lowerlevel.
In other words, the graph includesa tree of ?-nodes which encodes the hier-archical structure of each entry.2.
A w-node is created in the graph for eachword occurring in a definition and an edgeis added between the w-node and the ?-node of that definition.3.
An edge is added between each w-nodeand the top ?-node representing the dic-tionary entry for that word.For instance, given the entry for "tooth"3:1.
(Anat.)
One of the hard, bony appendageswhich are borne on the jaws, or on otherbones in the walls of the mouth or pharynxof most vertebrates, and which usually aidin the prehension and mastication of food.2.
Fig.
: Taste; palate.These are not dishes for thy dainty tooth.?Dryden.3.
Any projection corresponding to the toothof an animal, in shape, position, or office;as, the teeth, or cogs, of a cogwheel; atooth, prong, or tine, of a fork; a tooth, orthe teeth, of a rake, a saw, a file, a card.4.
(a) A projecting member resembling atenon, but fitting into a mortise thatis only sunk, not pierced through.
(b) One of several steps, or offsets, in atusk.
See Tusk.We would consider one ?-node for tooth as thetop-level entry, let us call it ?0.
?0 is con-nected with an edge to the ?-nodes ?1, ?2 ,?3and ?4 corresponding to the senses 1., 2., 3.3Source: Webster?s Revised Unabridged Dictionary,1996.
The experiment has actually been done on aFrench dictionary, Le Robert.and 4.; the latter will have an edge towards thetwo ?-nodes ?4.1 and ?4.2 for the sub-senses4.a.
and 4.b.
; ?4.1 will have an edge to each w-node built for nouns and verbs occurring in itsdefinition (member, resemble, tenon, fit, mor-tise, sink, pierce).
Then the w-node for tenonwill have an edge to the ?-node of the top-level entry of tenon.
We do not directly con-nect ?4.1to the ?-nodes of the top-level entriesbecause these may have both w- and ?-nodedaughters.In the graph, ?-nodes have tags which indi-cates their homograph number and their loca-tion in the hierarchical structure of the entry.These tags are sequences of integers where thefirst one gives the homograph number and thenext ones indicate the rank of the sense-numberat each level.
For instance, the previous nodes?4.1 is tagged (0, 4, 1).3 Prox, a distance between graphnodesWe describe here our method (dubbed Prox) tocompute a distance between nodes in the kindof graph described in the previous section.
It isa stochastic method for the study of so-calledhierarchical small-world graphs (Gaume et al,2002) (see also the next section).
The idea is tosee a graph as a Markov chain whose states arethe graph nodes and whose transitions are itsedges, with equal probabilities.
Then we sendrandom particles walking through this graph,and their trajectories and the dynamics of theirtrajectories reveal their structural properties.
Inshort, we assume the average distance a parti-cle has made between two nodes after a giventime is an indication of the semantic distancebetween these nodes.
Obviously, nodes locatedin highly clustered areas will tend to be sepa-rated by smaller distance.Formally, if G = (V, E) is an irreflexivegraph with |V | = n, we note [G] the n ?
n ad-jacency matrix of G that is such that [G]i,j (theith row and jth column) is 1 if there is an edgebetween node i and node j and 0 otherwise.We note [G?]
the Markovian matrix of G, suchthat [G?
]r,s = [G]r,s?x?V ([G]r,x).In the case of graphs built from a dictionaryas above,[G?
]r,s is 0 if there is no edge betweennodes r and s, and 1/D otherwise, where Dis the number of neighbours of r. This is in-deed a Markovian transition matrix since thesum of each line is one (the graph consideredbeing connected).We note [G?
]i the matrix [G?]
multiplied itimes by itself.Let now PROX(G,i,r,s) be [G?]ir,s.
This is thusthe probability that a random particle leavingnode r will be in node s after i time steps.
Thisis the measure we will use to determine if anode s is closer to a node r than another nodet.
Now we still have to find a proper value fori.
The next section explains the choice we havemade.4 Dictionaries as hierarchicalsmall-worldsRecent work in graph theory has revealed a setof features shared by many graphs observed"in the field" These features define the classof "hierarchical small world" networks (hence-forth HSW) (Watts and Strogatz, 1998; New-man, 2003).
The relevant features of a graph inthis respect are the following:D the density of the network.
HSWs typicallyhave a low D, i.e.
they have rather fewedges compared to their number of ver-tices.L the average shortest path between two nodes.It is also low in a HSW.C the clustering rate.
This is a measure of howoften neighbours of a vertex are also con-nected in the graph.
In a HSW, this featureis typically high.I the distribution of incidence degrees (i.e.
thenumber of neighbours) of vertices accord-ing to the frequency of nodes (how manynodes are there that have an incidence de-gree of 1, 2, ... n).
In a HSW network, thisdistribution follows a power law: the prob-ability P(k) that a given node has k neigh-bours decreases as k?
?, with lambda > 0.It means also that there are very few nodeswith a lot of neighbours, and a lot morenodes with very few neighbours.As a mean of comparison, table 1 shows thedifferences between randoms graphs (nodesare given, edges are drawn randomly betweennodes), regular graphs and HSWs.The graph of a dictionary belongs to the classof HSW.
For instance, on the dictionary weused, D=7, C=0.183, L=3.3.
Table 2 gives afew characteristics of the graph of nouns onlyon the dictionary we used (starred columns in-dicate values for the maximal self-connectedcomponent).We also think that the hierarchical aspect ofdictionaries (reflected in the distribution of in-cidence degrees) is a consequence of the roleof hypernymy associated to the high polysemyof some entries, while the high clustering ratedefine local domains that are useful for dis-ambiguation.
We think these two aspects de-termine the dynamics of random walks in thegraph as explained above, and we assume theyare what makes our method interesting forsense disambiguation.5 Word sense disambiguationusing Prox semantic distanceWe will now present a method for disambiguat-ing dictionary entries using the semantic dis-tance introduced section (3).The task can be defined as follows: we con-sider a word lemma ?
occurring in the defini-tion of one of the senses of a word ?.
We wantto tag ?
with the most likely sense it has inthis context.
Each dictionary entry is coded asa tree of senses in the graph of the dictionary,with a number list associated to each sub-entry.For instance for a given word sense of word W,listed as sub-sense II.3.1 in the dictionary, wewould record that sense as a node W(2,3,1) inthe graph.
In fact, to take homography into ac-count we had to add another level to this, forinstance W(1,1,2) is sense 1.2 of the first ho-mograph of word W. In the absence of an ho-mograph, the first number for a word sense willconventionally be 0.Let G=(V,E) the graph of words built as ex-plained section 2, [G] is the adjacency matrixof G, and [G?]
is the corresponding Markovianmatrix .
The following algorithm has then beenapplied:1.
Delete all neighbours of ?
in G, i.e.
make?x ?
V, [G]?,x = [G]x,?
= 02.
Compute the new [G?
]i where i is taken tobe 6(with equal D) L C IRandom graphs small L small C Poisson LawHSW small L high C power lawRegular graphs high L high C constantTable 1: Comparing classes of graphsnb nodes nb edges nb N* nb E* Diam* C* L*Nouns 53770 392161 51511 392142 7 0.1829 3.3249Nouns and sub-senses 140080 399969 140026 399941 11 0.0081 5.21Table 2: Dictionary used3.
Let L be the line ?
in the result.
?k, L[k] =[G?]i?,k4.
Let E = {x1, x2, ..., xn} be the nodes cor-responding to all the sub-senses inducedby the definition of ?.Then take xk = argmaxx?E(L[x])Then xk is the sub-sense with the best rank ac-cording to the Prox distance.The following steps needs a little explana-tion:1 This neighbours are deleted because other-wise there is a bias towards the sub-sensesof ?, which then form a sort of "artificial"cluster with respect to the given task.
Thisis done to allow the random walk to reallygo into the larger network of senses.2 Choosing a good value for the length ofthe random walk through the graph is nota simple matter.
If it is too small, only lo-cal relations appear (near synonyms, etc)which might not appear in contexts to dis-ambiguate (this is the main problem ofLesk?s method); if it is too large, the dis-tances between words will converge to aconstant value.
So it has to be related insome way to the average length betweentwo senses in the graph.
A reasonable as-sumption is therefore to stay close to thisaverage length.
Hence we took i = 6 sincethe average length is 5.21 (in the graphwith a node for every sub-sense, the graphwith a node for each entry having L=3.3)6 Evaluating the resultsThe following methodology has been followedto evaluate the process.We have randomly taken about a hundredof sub-entries in the chosen dictionary (outof about 140,000 sub-entries), and have hand-tagged all nouns and verbs in the entries withtheir sub-senses (and homography number), ex-cept those with only one sense, for a total ofabout 350 words to disambiguate.
For all pairof (context,target), the algorithm gives a rankedlist of all the sub-senses of the target.
Althoughwe have used both nouns and verbs to build thegraph of senses, we have tested disambiguationfirst on nouns only, for a total of 237 nouns.
Wehave looked how the algorithm behaves whenwe used both nouns and verbs in the graph ofsenses.To compare the human annotation to the au-tomated one, we have applied the followingmeasures, where (h1, h2, , ...) is the human tag,and (s1, s2, ..) is the top-ranked system outputfor a context i defined as the entry and the targetword to disambiguate:1. if h1 = 0 then do nothing else the homo-graph score is 1 if h1 = s1, 0 otherwise;2. in all cases, coarse polysemy count = 1 ifh2 = s2, 0 otherwise;3. in all cases, fine polysemy count = 1 if ?ihi = siThus, the coarse polysemy score computes howmany times the algorithm gives a sub-sense thathas the same "main" sense as the human tag(the main-sense corresponds to the first level inthe hierarchy of senses as defined above).
Thefine polysemy score gives the number of timesthe algorithm gives exactly the same sense asthe human.To give an idea of the difficult of the task,we have computed the average number of mainentry target system output human tagcorrect bal#n._m.
*0_3 lieu#n.
1_1_3 1_1_1correct van#n._m.
*2_0_0_0_0 voiture#n.
0_2 0_2_3error phon?tisme#n._m.
*0 moyen#n.
1_1_1 2_1error cr?ativit?#n._f.
*0 pouvoir#n.
2_3 2_1error acm?#n._m._ou_f.
*0_1 phase#n.
0_1 0_4Table 3: Detailed, main-sense evaluation of a couple of examples.sub-senses and the number of all senses, foreach target word.
This corresponds to a ran-dom algorithm, choosing between all senses ofa given word.
The expected value of this base-line is thus:?
homograph score=?x 1/(number of ho-mographs of x)?
coarse polysemy = ?x 1/(number of mainsub-senses of x)?
fine polysemy = ?x 1/(number of all sub-senses of x)A second baseline consists in answering al-ways the first sense of the target word, sincethis is often (but not always) the most commonusage of the word.
We did not do this for homo-graphs since the order in which they are givenin the dictionary does not seem to reflect theirimportance.Table 4 sums up the results.7 DiscussionThe result for homographs is very good but notvery significant given the low number of occur-rences; this all the more true as we used a part-of-speech tagger to disambiguate homographswith different part-of-speech beforehand (thesehave been left out of the computation of thescore).The scores we get are rather good for coarsepolysemy, given the simplicity of the method.As a means of comparison, (Patwardhan etal., 2003) applies various measures of seman-tic proximity (due to a number of authors), us-ing the WordNet hierarchy, to the task of wordsense disambiguation of a few selected wordswith results ranging from 0.2 to 0.4 with respectto sense definition given in WordNet (the aver-age of senses for each entry giving a randomscore of about 0.2).Our method already gives similar resultson the fine polysemy task (which has aneven harder random baseline) when using bothnouns and verbs as nodes, and does not focuson selected targets.A method not evaluated by (Patwardhan etal., 2003) and using another semantic related-ness measure ("conceptual density") is (Agirreand Rigau, 1996).
It is also based on a dis-tance within the WordNet hierarchy.
They useda variable context size for the task and presentresults only for the best size (thus being anot fully unsupervised method).
Their randombaseline is around 30%, and their precision is43% for 80% attempted disambiguations.Another study of disambiguation using a se-mantic similarity derived from WordNet is (Ab-ney and Light, 1999); it sees the task as a Hid-den Markov Model, whose parameters are es-timated from corpus data, so this is a mixedmodel more than a purely dictionary-basedmodel.
With a baseline of 0.285, they reach ascore of 0.423.
Again, the method we used ismuch simpler, for comparable or better results.Besides, by using all connections simultane-ously between words in the context to disam-biguate and the rest of the lexicon, this methodavoids the combinatorial explosion of methodspurely based on a similarity measure, where ev-ery potential sense of every meaningful wordin the context must be considered (unless ev-ery word sense of words other than the target isknown beforehand, which is not a very realis-tic assumption), so that only local optimizationcan be achieved.
In our case disambiguatinga lot of different words appearing in the samecontext may result in poorer results than withonly a few words, but it will not take longer.The only downside is heavy memory usage, aswith any dictionary-based method.We have made the evaluation on dictionaryentries because they are already part of the net-random first sense algorithm(n+v)homographs 0.49 - 0.875 (14/16)coarse polysemy 0.35 0.493 0.574 (136/237)fine polysemy 0.18 0.40 0.346 (82/237)Table 4: Resultswork of senses, to avoid raising other issues tooearly.
Thus, we are not exactly in the contextof disambiguating free text.
It could then beargued that our task is simpler than standarddisambiguation, because dictionary definitionsmight just be written in a more constrained andprecise language.
That is why we give the scorewhen taking always the first sense for each en-try, as an approximation of the most commonsense (since the dictionary does not have fre-quency information).
We can see that this scoreis about 50% only for the coarse polysemy, and40% for the fine polysemy, compared to a typ-ical 70-80% in usual disambiguation test sets,for similar sense dispersion (given by the ran-dom baseline); in (Abney and Light, 1999), thefirst-sense baseline gives 82%.
So we couldin fact argue that disambiguating dictionary en-tries seems harder.
This fact remains howeverto be confirmed with the actual most frequentsenses.
Let us point out again that our al-gorithm does not make use of the number ofsenses in definitions.Among the potential sources of improvementfor the future, or sources of errors in the past,we can list at least the following:?
overlapping of some definitions for sub-senses of an entry.
Some entries of thedictionary we used have sub-senses thatare very hard to distinguish.
In order tomeasure the impact of this, we should havemultiple annotations of the same data andmeasure inter-annotator agreement, some-thing that has not been done yet.?
part of speech tagging generates a few er-rors when confusing adjectives and nounsor adjectives and verbs having the samelemma; this should be compensated whenwe enrich the graph with entries for adjec-tives.?
some time should be spent studying theprecise influence of the length of the ran-dom walk considered; we have chosen avalue a priori to take into account the aver-age length of a path in the graph, but oncewe have more hand-tagged data we shouldbe able to have a better idea of the bestsuited value for that parameter.8 ConclusionWe have presented here an algorithm giving ameasure of lexical similarity, built from infor-mation found in a dictionary.
This has beenused to disambiguate dictionary entries, with amethod that needs no other source of informa-tion (except part-of-speech tagging), no anno-tated data.
The coverage of the method dependsonly on the lexical coverage of the dictionaryused.
It seems to give promising results on dis-ambiguating nouns, using only nouns or nounsand verbs.
We intend to try the method afterenriching the network of senses with adjectivesand/or adverbs.
We also intend, of course, to trythe method on disambiguating verbs and adjec-tives.Moreover, the method can be rather straight-forwardly extended to any type of disambigua-tion by considering a context with a targetword as a node added in the graph of senses(a kind of virtual definition).
We have nottested this idea yet.
Since our method gives aranked list of sense candidates, we also con-sider using finer performance measures, takinginto account confidence degrees, as proposed in(Resnik and Yarowsky, 2000).ReferencesSteven Abney and Marc Light.
1999.
Hidinga semantic hierarchy in a markov model.
InACL?99 Workshop Unsupervised Learning inNatural Language Processing, University ofMaryland.E.
Agirre and G. Rigau.
1996.
Word sensedisambiguation using conceptual density.
InProceedings of COLING?96, pages 16?22,Copenhagen (Denmark).S.
Banerjee and T. Pedersen.
2003.
Extendedgloss overlaps as a measure of semantic re-latedness.
In Proceedings of the EighteenthInternational Conference on Artificial Intel-ligence (IJCAI-03), Acapulco, Mexico.B.
Gaume, K. Duvignau, O. Gasquet, and M.-D. Gineste.
2002.
Forms of meaning, mean-ing of forms.
Journal of Experimental andTheoretical Artificial Intelligence, 14(1):61?74.N.
Ide and J. V?ronis.
1998.
Introduction tothe special issue on word sense disambigua-tion: The state of the art.
ComputationalLinguistics, 24(1).N.
Ide and J. V?ronis.
1990.
Word sense dis-ambiguation with very large neural networksextracted from machine readable dictionar-ies.
In Proceedings of the 14th InternationalConference on Computational Linguistics(COLING?90), volume 2, pages 389?394.H.
Kozima and T. Furugori.
1993.
Similaritybetween words computed by spreading acti-vation on an english dictionary.
In Proceed-ings of the conference of the European chap-ter of the ACL, pages 232?239.M.
Lesk.
1986.
Automatic sense disambigua-tion using machine readable dictionaries:how to tell a pine code from an ice creamcone.
In Proceedings of the 5th annual in-ternational conference on Systems documen-tation, pages 24?26, Toronto, Canada.C.
Manning and H. Sch?tze.
1999.
Founda-tions of Statistical Natural Language Pro-cessing.
MIT Press.M.
E. J. Newman.
2003.
The structure andfunction of complex networks.
SIAM Re-view, 45:167?256.S.
Patwardhan, S. Banerjee, and T. Pedersen.2003.
Using measures of semantic related-ness for word sense disambiguation.
In Pro-ceedings of the Fourth International Confer-ence on Intelligent Text Processing and Com-putational Linguistics (CICLING-03).P.
Resnik and D. Yarowsky.
2000.
Distinguish-ing systems and distinguishing senses: Newevaluation methods for word sense disam-biguation.
Natural Language Engineering,5(2):113?133.D.J.
Watts and S.H Strogatz.
1998.
Collectivedynamics of ?small-world?
networks.
Na-ture, (393):440?442.
