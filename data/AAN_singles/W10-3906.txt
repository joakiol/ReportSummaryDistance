Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 32?39,Beijing, August 2010Even Unassociated Features Can ImproveLexical Distributional SimilarityKazuhide Yamamoto and Takeshi AsakuraDepartment of Electrical EngineeringNagaoka University of Technology{yamamoto, asakura}@jnlp.orgAbstractThis paper presents a new computationof lexical distributional similarity, whichis a corpus-based method for computingsimilarity of any two words.
Althoughthe conventional method focuses on em-phasizing features with which a givenword is associated, we propose that evenunassociated features of two input wordscan further improve the performance intotal.
We also report in addition thatmore than 90% of the features has nocontribution and thus could be reducedin future.1 IntroductionSimilarity calculation is one of essential tasks innatural language processing (1990; 1992; 1994;1997; 1998; 1999; 2005).
We look for a seman-tically similar word to do corpus-driven summa-rization, machine translation, language genera-tion, recognition of textual entailment and othertasks.
In task of language modeling and disam-biguation we also need to semantically general-ize words or cluster words into some groups.
Asthe amount of text increases more and more inthe contemporary world, the importance of sim-ilarity calculation also increases concurrently.Similarity is computed by roughly two ap-proaches: based on thesaurus and based on cor-pus.
The former idea uses thesaurus, such asWordNet, that is a knowledge resource of hi-erarchical word classification.
The latter idea,that is the target of our work, originates fromHarris?s distributional hypothesis more than fourdecades ago (1968), stating that semanticallysimilar words tend to appear in similar contexts.In many cases a context of a word is representedas a feature vector, where each feature is anotherexpression that co-occurs with the given word inthe context.Over a long period of its history, in partic-ular in recent years, several works have beendone on distributional similarity calculation.
Al-though the conventional works have attained thefine performance, we attempt to further improvethe quality of this measure.
Our motivation ofthis work simply comes from our observationand analysis of the output by conventional meth-ods; Japanese, our target language here, is writ-ten in a mixture of four scripts: Chinese char-acters, Latin alphabet, and two Japanese-origincharacters.
In this writing environment somewords which have same meaning and same pro-nunciation are written in two (or more) differentscripts.
This is interesting in terms of similaritycalculation since these two words are completelysame in semantics so the similarity should beideally 1.0.
However, the reality is, as far as wehave explored, that the score is far from 1.0 inmany same word pairs.
This fact implies that theconventional calculation methods are far enoughto the goal and are expected to improve further.The basic framework for computing distribu-tional similarity is same; for each of two inputwords a context (i.e., surrounding words) is ex-tracted from a corpus, a vector is made in whichan element of the vector is a value or a weight,and two vectors are compared with a formula tocompute similarity.
Among these processes wehave focused on features, that are elements of32the vector, some of which, we think, adverselyaffect the performance.
That is, traditional ap-proaches such as Lin (1998) basically use all ofobserved words as context, that causes noise infeature vector comparison.
One may agree thatthe number of the characteristic words to deter-mine the meaning of a word is some, not all, ofwords around the target word.
Thus our goal isto detect and reduce such noisy features.Zhitomirsky-Geffet and Dagan (2009) havesame motivation with us and introduced a boot-strapping strategy that changes the original fea-tures weights.
The general idea here is to pro-mote the weights of features that are commonfor associated words, since these features arelikely to be most characteristic for determiningthe word?s meaning.
In this paper, we proposeinstead a method to using features that are bothunassociated to the two input words, in additionto use of features that are associated to the input.2 MethodThe lexical distributional similarity of the inputtwo words is computed by comparing two vec-tors that express the context of the word.
In thissection we first explain the feature vector, andhow we define initial weight for each feature ofthe vector.
We then introduce in Subsection 2.3the way to compute similarity by two vectors.After that, we emphasize some of the features bytheir association to the word, that is explained inSubsection 2.4.
We finally present in Subsection2.5 feature reduction which is our core contribu-tion of this work.
Although our target languageis Japanese, we use English examples in order toprovide better understanding to the readers.2.1 Feature VectorWe first explain how to construct our feature vec-tor from a text corpus.A word is represented by a feature vector,where features are collection of syntactically de-pendent words co-occurred in a given corpus.Thus, we first collect syntactically dependentwords for each word.
This is defined, as inLin (1998), as a triple (w, r,w?
), where w andw?
are words and r is a syntactic role.
As fordefinition of word, we use not only words givenby a morphological analyzer but also compoundwords.
Nine case particles are used as syntacticroles, that roughly express subject, object, modi-fier, and so on, since they are easy to be obtainedfrom text with no need of semantic analysis.
Inorder to reduce noise we delete triples that ap-pears only once in the corpus.We then construct a feature vector out of col-lection of the triples.
A feature of a word is ananother word syntactically dependent with a cer-tain role.
In other words, given a triple (w, r,w?
),a feature of w corresponds to a dependent wordwith a role (r,w?
).2.2 (Initial) Filtering of FeaturesThere are several weighting functions to deter-mine a value for each feature element.
As faras we have investigated the literature the mostwidely used feature weighting function is point-wise mutual information (MI), that is defined asfollows:MI(w, r,w?)
= log2freq(w, r,w?)Sfreq(w)freq(r,w?)
(1)where freq(r,w?)
is the frequency of the co-occurrence word w?
with role r, freq(w)is the independent frequency of a word w,freq(w, r,w?)
is the frequency of the triples(w, r,w?
), and S is the number of all triples.In this paper we do not discuss what is thebest weighting functions, since this is out of tar-get.
We use mutual information here because itis most widely used, i.e., in order to compare per-formance with others we want to adopt the stan-dard approach.As other works do, we filter out features thathave a value lower than a minimal weight thresh-olds ?.
The thresholds are determined accordingto our preliminary experiment, that is explainedlater.2.3 Vector SimilaritySimilarity measures of the two vectors are com-puted by various measures.
Shibata and Kuro-hashi (2009) have compared several similaritymeasures including Cosine (Ruge, 1992), (Lin,33 (input word) w: boy(feature) v: guardOBJ(synonyms of w, shown with its similarity to w) Syn(w) ={ child(0.135), girl(0.271), pupil(0.143), woman(0.142), young people(0.147) } (feature vectors V ):V(boy) = { parentsMOD, runawaySUBJ, reclaimOBJ, fatherMOD, guardOBJ , ?
?
?
}V(child) = { guardOBJ, lookOBJ, bringOBJ, give birthOBJ, careOBJ , ?
?
?
}V(girl) = { parentsMOD, guardOBJ, fatherMOD, testifySUBJ, lookOBJ, ?
?
?
}V(pupil) = { targetOBJ, guardOBJ, careOBJ, aimOBJ, increaseSUBJ, ?
?
?
}V(woman) = { nameMOD, give birthOBJ, groupMOD, together+with, parentsMOD , ?
?
?
}V(young people) = { harmfulTO, globalMOD, reclaimOBJ, wrongdoingMOD , ?
?
?
} (words that has feature v) Asc(v) = {boy, child, girl, pupil, ?
?
?
}weight(w, v) = weight (boy, guardOBJ) =?wf?Asc(v)?Syn(w) sim(w,wf )= 0.135 + 0.271 + 0.143 = 0.549 Figure 1: Example of feature weighting for word boy.1998), (Lin, 2002), Simpson, Simpson-Jaccard,and conclude that Simpson-Jaccard index attainsbest performance of all.
Simpson-Jaccard indexis an arithmetic mean of Simpson index and Jac-card index, defined in the following equation:sim(w1, w2) =12(simJ (w1, w2)+simS(w1, w2))(2)simJ(w1, w2) =|V1 ?
V2||V1 ?
V2|(3)simS(w1, w2) =|V1 ?
V2|min(|V1|, |V2|)(4)where V1 and V2 is set of features for w1 andw2, respectively, and |A| is the number of set A.It is interesting to note that both Simpson andJaccard compute similarity according to degreeof overlaps of the two input sets, that is, the re-ported best measure computes similarity by ig-noring the weight of the features.
In this paperwe adopt Simpson-Jaccard index, sim, whichindicates that the weight of features that is ex-plained below is only used for feature reduction,not for similarity calculation.2.4 Feature Weighting by AssociationWe then compute weights of the features of theword w according to the degree of semantic as-sociation to w. The weight is biased because allof the features, i.e., the surrounding words, arenot equally characteristic to the input word.
Thecore idea for feature weighting is that a featurev in w is more weighted when more synonyms(words of high similarity) of w also have v.Figure 1 illustrates this process by examples.Now we calculate a feature guardOBJ for a wordboy, we first collect synonyms of w, denoted bySyn(w), from a thesaurus.
We then computesimilarities between w and each word in Syn(w)by Equation 2.
The weight is the sum of the sim-ilarities of words in Syn(w) that have feature v,defined in Equation 5.weight(w, v) =?wf?Asc(v)?Syn(w)sim(w,wf )(5)34Figure 2: An illustration of similarity calculation of Zhitomirsky-Geffet and Dagan (2009) (a) andthe proposed method (b1 and b2) in feature space.
In order to measure the distance of the two words(shown in black dots) they use only associated words, while we additionally use unassociated wordsin which the distances to the words are similar.2.5 Feature ReductionWe finally reduce features according to the dif-ference of weights of each feature in words wecompare.
In computing similarity of two words,w1 and w2, a feature v satisfying Equation 6 isreduced.abs(weight(w1, v) ?
weight(w2, v)) > ?
(6)where abs() is a function of absolute value, and?
is a threshold for feature reduction.Figure 2 illustrates our idea and comparesthe similar approach proposed by Zhitomirsky-Geffet and Dagan (2009).
Roughly speaking,Zhitomirsky-Geffet and Dagan (2009) computesimilarity of two words, shown as black dotsin (a), mainly according to associated features(dark-colored circle), or features that has highweights in Equation 5.
And the associated fea-tures are determined word by word indepen-dently.In contrast, the proposed method relatively re-duces features, depending on location of inputtwo words.
At (b1) in the figure, not only asso-ciated (high-colored area) but unassociated fea-tures (light-colored area) are used for similar-ity computation in our method.
As Equation 6shows, regardless of how much a feature is as-sociated to the word, the feature is not reducedwhen it has similar weight to both w1 and w2,located at the middle area of the two words inthe figure.This idea seems to work more effectively,compared with Zhitomirsky-Geffet and Da-gan (2009), in case that input two words are notso similar, that is shown at (b2) of the figure.As they define associated features independently,it is likely that the overlapped area is little ornone between the two words.
In contrast, ourmethod uses features at the middle area of twoinput words, where there is always certain fea-tures provided for similarity computation, shownin case (b2).
Simplified explanation is that oursimilarity is computed as the ratio of the associ-ated area to the unassociated area in the figure.We will verify later if the method works better inlow similarity calculation.2.6 Final SimilarityThe final similarity of two words are calculatedby two shrunk vectors (or feature sets) and Equa-tion 2, that gives a value between 0 and 1.353 Evaluation3.1 Evaluation MethodIn general it is difficult to answer how similartwo given words are.
Human have no way tojudge correctness if computed similarity of twowords is, for instance, 0.7.
However, given twoword pairs, such as (w,w1) and (w,w2), we mayanswer which of two words, w1 or w2, is moresimilar to w than the other one.
That is, degreeof similarity is defined relatively hence accuracyof similarity measures is evaluated by way of rel-ative comparisons.In this paper we employ an automatic eval-uation method in order to reduce time, humanlabor, and individual variations.
We first col-lect four levels of similar word pairs from a the-saurus1.
Thesaurus is a resource of hierarchi-cal words classification, hence we can collectseveral levels of similar word pairs accordingto the depth of common parent nodes that twowords have.
Accordingly, we constructed fourlevels of similarity pairs, Level 0, 1, 2, and 3,where the number increases as the similarity in-creases.
Each level includes 800 word pairs thatare randomly selected.
The following examplesare pairs with word Asia in each Level. Example: Four similarity levels for pair ofAsia.Level 3(high): Asia vs. EuropeLevel 2: Asia vs. BrazilLevel 1: Asia vs. my countryLevel 0(low): Asia vs. system We then combine word pairs of adjacent sim-ilarity Levels, such as Level 0 and 1, that is atest set to see low-level similarity discriminationpower.
The performance is calculated in termsof how clearly the measure distinguishes the dif-ferent levels.
In a similar fashion, Level 1 and 2,as well as 2 and 3, are combined and tested formiddle-level and high-level similarity discrimi-nation, respectively.
The number of pairs in each1In this experiment we use Bunrui Goi Hyo also forevaluation.
Therefore, this experimental setting is a kindof closed test.
However, we see that the advantage to usethe same thesaurus in the evaluation seems to be small.Figure 3: Relation between threshold ?
and per-formance in F-measures for Level 3+2 test set.test set is 1,600 as two Levels are combined.3.2 Experimental SettingThe corpus we use in this experiment is all thearticles in The Nihon Keizai Shimbun Database,a Japanese business newspaper corpus cover-ing the years 1990 through 2004.
As morpho-logical analyzer we use Chasen 2.3.3 with IPAmorpheme dictionary.
The number of collectedtriples is 2,584,905, that excludes deleted onesdue to one time appearance and words includingsome symbols.In Subsection 2.4 we use Bunrui Goi Hyo, aJapanese thesaurus for synonym collection.
Thepotential target words are all content words, ex-cept words that have less than twenty features.The number of words after exclusion is 75,530.Moreover, words that have four or less words inthe same category in the thesaurus are regardedas out of target in this paper, due to limitationof Syn(w) in Subsection 2.4.
Also, in orderto avoid word sense ambiguity, words that havemore than two meanings, i.e., those classified inmore than two categories in the thesaurus, alsoremain to be solved.3.3 Threshold for Initial FilteringFigure 3 shows relation between threshold ?
andthe performance of similarity distinction that isdrawn in F-measures, for Level 3+2 test set.
Ascan be seen, the plots seem to be concave down36Figure 4: Threshold vs. accuracy in Level 3+2set.Figure 5: Threshold vs. accuracy in Level 2+1set.and there is a clear peak when ?
is between 2and 3.In the following experiments we set ?
thevalue where the best performance is given foreach test set.
We have observed similar phenom-ena in other test sets.
The thresholds we use is2.1 for Level 3+2, 2.4 for Level 2+1, and 2.4 forLevel 1+0.3.4 Threshold for Weighting FunctionFigure 4, 5, and 6 show relation between thresh-old ?
and performance in Level 3+2, 2+1, 1+0test set, respectively.
The threshold at the pointwhere highest performance is obtained greatlydepends on Levels: 0.3 in Level 3+2, 0.5 in Level2+1, and 0.9 in Level 1+0.
Comparison of thesethree figures indicates that similarity distinctionFigure 6: Threshold vs. accuracy in Level 1+0set.Table 1: Performance comparison of three meth-ods in each task (in F-measures).Level S&K ZG&D proposedLvl.3+Lvl.2 0.702 0.791 0.797Lvl.2+Lvl.1 0.747 0.771 0.773Lvl.1+Lvl.0 0.838 0.789 0.840power in higher similarity region requires lowerthreshold, i.e., fewer features.
In contrast, con-ducting fine distinction in lower similarity levelrequires higher threshold, i.e., a lot of featuresmost of which may be unassociated ones.3.5 PerformanceTable 1 shows performance of the pro-posed method, compared with Shibataand Kurohashi (2009) (S&K in the table)and Zhitomirsky-Geffet and Dagan (2009)(ZG&D)2.
The method of Shibata and Kuro-hashi (2009) here is the best one among thosecompared.
It uses only initial filtering describedin Subsection 2.2.
The method of Zhitomirsky-Geffet and Dagan (2009) in addition emphasizeassociated features as explained in Subsection2.4.
All of the results in the table are the bestones among several threshold settings.The result shows that the accuracy is 0.797(+0.006) in Level 3+2, 0.773 (+0.002) in Level2The implementations of providing associated wordsand the bootstrapping are slightly different to Zhitomirsky-Geffet and Dagan (2009).372+1, and 0.840 (+0.001) in Level 1+0, where thedegree of improvement here are those comparedwith best ones except our proposed method.
Thisconfirms that our method attains equivalent orbetter performance in all of low, middle, andhigh similarity levels.We also see in the table that S&K and ZG&Dshow different behavior according to the Level.However, it is important to note here that ourproposed method performs equivalent or outper-forms both methods in all Levels.4 Discussions4.1 Behavior at Each Similarity LevelAs we have discussed in Subsection 2.5, ourmethod is expected to perform better thanZhitomirsky-Geffet and Dagan (2009) in distinc-tion in lower similarity area.
Roughly speak-ing, we interpret the results as follows.
Shi-bata and Kurohashi (2009) always has many fea-tures that degrades the performance in highersimilarity level, since the ratio of noisy fea-tures may throw into confusion.
Zhitomirsky-Geffet and Dagan (2009) reduces such noisethat gives better performance in higher similaritylevel and is stable in all levels.
And our proposedmethod maintains performance of Zhitomirsky-Geffet and Dagan (2009) in higher level whileimproves performance that is close to Shibataand Kurohashi (2009) in lower level, utilizingfewer features.
We think our method can includeadvantages over the two methods.4.2 Error AnalysisWe overview the result and see that the major er-rors are NOT due to lack of features.
Table 2illustrates the statistics of words with a few fea-tures (less than 50 or 20).
This table clearly tellsus that, in the low similarity level (Level 1+0) inparticular, there are few pairs in which the wordhas less than 50 or 20, that is, these pairs are con-sidered that the features are erroneously reduced.4.3 Estimation of Potential FeatureReductionIt is interesting to note that we may reduce 81%of features in Level 3+2 test set while keepingTable 2: Relation of errors and words with a fewfeatures.
In the table, (h) and (l) shows pairs thatare judged higher (lower) by the system.
Columnof < 50 (< 20) means number of pairs each ofwhich has less than 50 (20) features.Level #errs < 50 fea.
< 20 fea.Lvl.3+2 (h) 125 76 (61%) 32 (26%)Lvl.3+2 (l) 220 150 (68%) 60 (27%)Lvl.2+1 (h) 137 75 (55%) 32 (23%)Lvl.2+1 (l) 253 135 (53%) 52 (21%)Lvl.1+0 (h) 149 23 (15%) 4 ( 3%)Lvl.1+0 (l) 100 17 (17%) 3 ( 3%)the performance, if we can reduce them prop-erly.
In a same way, 87% of features in Level2+1 set, and 52% of features in Level 1+0 set,may also be reduced.
These numbers are givenat the situation in which F-measure attains bestperformance.
Here, it is not to say that we aresure to reduce them in future, but to estimate howmany features are really effective to distinguishthe similarity.Here we have more look at the statistics.
Thenumber of initial features on average is 609 inLevel 3+2 test set.
If we decrease threshold by0.1, we can reduce 98% of features at the thresh-old of 0.8, where the performance remains best(0.791).
This is a surprising fact for us sinceonly 12 (; 609?
(1?0.98)) features really con-tribute the performance.
Therefore, we estimatethat there is a lot to be reduced further in orderto purify the features.5 Conclusion and Future WorkThis paper illustrates improvement of lexicaldistributional similarity by not only associatedfeatures but also utilizing unassociated features.The core idea is simple, and is reasonable whenwe look at machine learning; in many cases weuse training instances of not only something pos-itive but something negative to make the distinc-tion of the two sides clearer.
Similarly, in ourtask we use features of not only associated butunassociated to make computation of similarity(or distance in semantic space) clearer.
We as-38sert in this work that a feature that has similarweight to two given words also plays importantrole, regardless of how much it is associated tothe given words.Among several future works we need to fur-ther explore reduction of features.
It is reportedby some literature such as Hagiwara et al (2006)that we can reduce so many features while pre-serving the same accuracy in distributional sim-ilarity calculation.
This implies that, some ofthem are still harmful and are expected to be re-duced further.List of Tools and Resources1.
Chasen, a morphological analyzer,Ver.2.3.3.
Matsumoto Lab., Nara Instituteof Science and Technology.
http://chasen-legacy.sourceforge.jp/2.
IPADIC, a dictionary for morphologi-cal analyzer.
Ver.2.7.0.
Information-Technology Promotion Agency, Japan.http://sourceforge.jp/projects/ipadic/3.
Bunrui Goihyo, a word list by semanticprinciples, revised and enlarged edi-tion.
The National Institute for JapaneseLanguage.
http://www.kokken.go.jp/en/publications/bunrui goihyo/4.
Nihon Keizai Shimbun Newspaper Corpus,years 1990-2004, Nihon Keizai Shimbun,Inc.ReferencesDagan, Ido, Lillian Lee, and Fernando Pereira.
1999.Similarity-based Models of Co-occurrence Proba-bilities.
Machine Learning, 34(1-3):43?69.Grefenstette, Gregory.
1994.
Exploration in Auto-matic Thesaurus Discovery.
Kluwer AcademicPublishers.
Norwell, MA.Hagiwara, Masato, Yasuhiro Ogawa, KatsuhikoToyama.
2006.
Selection of Effective ContextualInformation for Automatic Synonym Acquisition.In Proceedings of the 21st International Confer-ence on Computational Linguistics and 44th An-nual Meeting of the Association for ComputationalLinguistics, pp.353?360.Harris, Zelig S. 1968.
Mathematical Structures ofLanguage.
Wiley, New Jersey.Hindle, Donald.
1990.
Noun Classification fromPredicate-Argument Structures.
In Proceedingsof the 28th Annual Meeting of the Association forComputational Linguistics, pp.268?275.Lee, Lillian.
1997.
Similarity-Based Approaches toNatural Language Processing.
Ph.D. thesis, Har-vard University, Cambridge, MA.Lee, Lillian.
1999.
Measures of distributional simi-larity.
In Proceedings of the 37th Annual Meetingof the Association for Computational Linguistics,pp.
25?32, College Park, MD.Lin, Dekang.
1998.
Automatic Retrieval and Cluster-ing of Similar Words.
In Proceedings of the 36thAnnual Meeting of the Association for Computa-tional Linguistics and 17th International Confer-ence on Computational Linguistics, pp.768?774.Montreal.Lin, Dekang and and Patrick Pantel.
2002.
Con-cept Discovery from Text.
In Proceedings of 19thInternational Conference on Computational Lin-guistics, pp.577?583.
Taipei.Ruge, Gerda.
1992.
Experiments of Linguistically-based Term Associations.
Information Processing& Management, 28(3):317?332.Shibata, Tomohide and Sadao Kurohashi.
2009.
Dis-tributional similarity calculation using very largescale Web corpus.
In Proceedings of Annual Meet-ing of Association for Natural Language Process-ing.
pp.
705?708.Weeds, Julie and David Weir.
2005.
Co-occurrenceretrieval: A Flexible Framework for Lexical Dis-tributional Similarity.
Computational Linguistics.31(4):439?476.Zhitomirsky-Geffet, Maayan and Ido Dagan.
2009.Bootstrapping Distributional Feature Vector Qual-ity.
Computational Linguistics, 35(3):435?461.39
