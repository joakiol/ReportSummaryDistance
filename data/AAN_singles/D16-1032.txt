Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 329?339,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsGlobally Coherent Text Generation with Neural Checklist ModelsChloe?
Kiddon Luke Zettlemoyer Yejin ChoiComputer Science & EngineeringUniversity of Washington{chloe, lsz, yejin}@cs.washington.eduAbstractRecurrent neural networks can generate lo-cally coherent text but often have difficultiesrepresenting what has already been generatedand what still needs to be said ?
especiallywhen constructing long texts.
We present theneural checklist model, a recurrent neural net-work that models global coherence by stor-ing and updating an agenda of text stringswhich should be mentioned somewhere in theoutput.
The model generates output by dy-namically adjusting the interpolation among alanguage model and a pair of attention mod-els that encourage references to agenda items.Evaluations on cooking recipes and dialoguesystem responses demonstrate high coherencewith greatly improved semantic coverage ofthe agenda.1 IntroductionRecurrent neural network (RNN) architectures haveproven to be well suited for many natural languagegeneration tasks (Mikolov et al, 2010; Mikolov etal., 2011; Sordoni et al, 2015; Xu et al, 2015;Wen et al, 2015; Mei et al, 2016).
Previous neu-ral generation models typically generate locally co-herent language that is on topic; however, overallthey can miss information that should have been in-troduced or introduce duplicated or superfluous con-tent.
These errors are particularly common in situ-ations where there are multiple distinct sources ofinput or the length of the output text is sufficientlylong.
In this paper, we present a new recurrent neu-ral model that maintains coherence while improv-Place tomatoes in a bowl .
Dice the onionPico de gallochopped tomatoesonionjalape?ossaltlimeand add to the tomatoes???
?Figure 1: Example checklist recipe generation.
A checklist(right dashed column) tracks which agenda items (top boxes;?salt,?
?lime,?
etc.)
have already been used (checked boxes).The model is trained to interpolate an RNN (e.g., encode ?picode gallo?
and decode a recipe) with attention models over new(left column) and used (middle column) items that identifylikely items for each time step (shaded boxes; ?tomatoes,?
etc.
).ing coverage by globally tracking what has been saidand what is still left to be said in complete texts.For example, consider the challenge of generat-ing a cooking recipe, where the title and ingredientlist are provided as inputs and the system must gen-erate a complete text that describes how to producethe desired dish.
Existing RNN models may losetrack of which ingredients have already been men-tioned, especially during the generation of a longrecipe with many ingredients.
Recent work has fo-cused on adapting neural network architectures toimprove coverage (Wen et al, 2015) with applica-tion to generating customer service responses, suchas hotel information, where a single sentence is gen-erated to describe a few key ideas.
Our focus is in-stead on developing a model that maintains coher-ence while producing longer texts or covering longer329input specifications (e.g., a long ingredient list).More specifically, our neural checklist model gen-erates a natural language description for achievinga goal, such as generating a recipe for a particu-lar dish, while using a new checklist mechanismto keep track of an agenda of items that shouldbe mentioned, such as a list of ingredients (seeFig.
1).
The checklist model learns to interpolateamong three components at each time step: (1)an encoder-decoder language model that generatesgoal-oriented text, (2) an attention model that tracksremaining agenda items that need to be introduced,and (3) an attention model that tracks the used, orchecked, agenda items.
Together, these compo-nents allow the model to learn representations thatbest predict which words should be included in thetext and when references to agenda items should bechecked off the list (see check marks in Fig.
1).We evaluate our approach on a new cookingrecipe generation task and the dialogue act genera-tion from Wen et al (2015).
In both cases, the modelmust correctly describe a list of agenda items: an in-gredient list or a set of facts, respectively.
Gener-ating recipes additionally tests the ability to main-tain coherence in long procedural texts.
Experi-ments in dialogue generation demonstrate that ourapproach outperforms previous work with up to a 4point BLEU improvement.
Our model also scales tocooking recipes, where both automated and manualevaluations demonstrate that it maintains the stronglocal coherence of baseline RNN techniques whilesignificantly improving the global coverage by ef-fectively integrating the agenda items.2 TaskGiven a goal g and an agenda E = {e1, .
.
.
, e|E|},our task is to generate a goal-oriented text x by mak-ing use of items on the agenda.
For example, in thecooking recipe domain, the goal is the recipe title(?pico de gallo?
in Fig.
1), and the agenda is the in-gredient list (e.g., ?lime,?
?salt?).
For dialogue sys-tems, the goal is the dialogue type (e.g., inform orquery) and the agenda contains information to bementioned (e.g., a hotel name and address).
Forexample, if g =?inform?
and E = {name(HotelStratford), has internet(no)}, an output text might bex =?Hotel Stratford does not have internet.
?3 Related WorkAttention models have been used for many NLPtasks such as machine translation (Balasubramanianet al, 2013; Bahdanau et al, 2014), abstractive sen-tence summarization (Rush et al, 2015), machinereading (Cheng et al, 2016), and image caption gen-eration (Xu et al, 2015).
Our model uses new typesof attention to record what has been said and to se-lect new agenda items to be referenced.Recently, other researchers have developed newways to use attention mechanisms for related gen-eration challenges.
Most closely related, Wen et al(2015) and Wen et al (2016) present neural networkmodels for generating dialogue system responsesgiven a set of agenda items.
They focus on gener-ating short texts (1-2 sentences) in a relatively smallvocabulary setting and assume a fixed set of possi-ble agenda items.
Our model composes substantiallylonger texts, such as recipes, with a more varied andopen ended set of possible agenda items.
We alsocompare performance for our model on their data.Maintaining coherence and avoiding duplicationhave been recurring challenges when generating textusing RNNs for other applications, including imagecaptioning (Jia et al, 2015; Xu et al, 2015) and ma-chine translation (Tu et al, 2016b; Tu et al, 2016a).A variety of solutions have been developed to ad-dress infrequent or out-of-vocabulary words in par-ticular (Gu?lc?ehre et al, 2016; Jia and Liang, 2016).Instead of directly copying input words or determin-istically selecting output, our model can learn howto generate them (e.g., it might prefer to producethe word ?steaks?
when the original recipe ingre-dient was ?ribeyes?).
Finally, recent work in ma-chine translation models has introduced new train-ing objectives to encourage attention to all inputwords (Luong et al, 2015), but these models do notaccumulate attention while decoding.Generating recipes was an early task in planning(Hammond, 1986) and generating referring expres-sion research (Dale, 1988).
These can be seen askey steps in classic approaches to generating natu-ral language text: a formal meaning representationis provided as input and the model first does contentselection to determine the non-linguistic concepts tobe conveyed by the output text (i.e., what to say)and then does realization to describe those concepts330EtGenerateoutputEt+1?t?ht-1gEtxt+rtstqtzthtref-type(ht)PhtEtxxxx?tftot+at-1atftnewEt+1new1-atEatxExEE2xGRU language modelAttention mechanismsUpdate checklisthidden stateprojected intoagenda spacehidden stateclassifierprobability ofusing new itemavailableitemsuseditemsUpdate available and used agenda itemssum?sigmoidlinear projectionmultiplicationsoftmaxlinear interpolationgateselect dimension ii+xkeyupdatechecklistlanguagemodelhtusednewusedusednewnewavailableitemsagendaagendaFigure 2: A diagram of the neural checklist model.
The bottom portion depicts how the model generates the output embedding ot.The top portion shows how the checklist and available/used agenda item matrices are updated.in natural language text (i.e., how to say it) (Thomp-son, 1977; Reiter and Dale, 2000).
More recently,machine learning methods have focused on parts ofthis approach (Barzilay and Lapata, 2005; Liang etal., 2009) or the full two-stage approach (Angeli etal., 2010; Konstas and Lapata, 2013).
Most of thesemodels shorter texts, although Mori et al (2014) didconsider longer cooking recipes.
Our approach is ajoint model that instead operates with textual inputand tries to cover all of the content it is given.4 ModelFig.
2 shows a graphical representation of the neu-ral checklist model.
At a high level, our model usesa recurrent neural network (RNN) language modelthat encodes the goal as a bag-of-words and thengenerates output text token by token.
It additionallystores a vector that acts as a soft checklist of whatagenda items have been used so far during genera-tion.
This checklist is updated every time an agendaitem reference is generated and is used to computethe available agenda items at each time step.
Theavailable items are used as an input to the languagemodel and to constrain which agenda items can stillbe referenced during generation.
Agenda embed-dings are also used when generating item references.4.1 Input variable definitionsWe assume the goal g and agenda items E (seeSec.
2) are each defined by a set of tokens.
Goaltokens come from a fixed vocabulary Vgoal, the itemtokens come from a fixed vocabulary Vagenda, andthe tokens of the text xt come from a fixed vocab-ulary Vtext.
In an abuse of notation, we representeach goal g, agenda item ei, and text token xt asa k-dimensional word embedding vector.
We com-pute these embeddings by creating indicator vec-tors of the vocabulary token (or set of tokens forgoals and agenda items) and embed those vectorsusing a trained k ?
|Vz| projection matrix, wherez ?
{goal, agenda, text} depending whether weare generating a goal, agenda item, or text token.Given a goal embedding g ?
Rk, a matrix of Lagenda items E ?
RL?k, a checklist soft record ofwhat items have been used at?1 ?
RL, a previoushidden state ht?1 ?
Rk, and the current input wordembedding xt ?
Rk, our architecture computes thenext hidden state ht, an embedding used to generatethe output word ot, and the updated checklist at.4.2 Generating output token probabilitiesTo generate the output token probability distribution(see ?Generate output?
box in Fig.
2), wt ?
R|Vtext|,we project the output hidden state ot into the vocab-ulary space and apply a softmax:wt = softmax(Woot),where Wo ?
R|V |?k is a trained projection ma-trix.
The output hidden state is the linear interpola-tion of (1) content cgrut from a Gated Recurrent Unit331(GRU) language model, (2) an encoding cnewt gen-erated from the new agenda item reference model(Sec.
4.3), and (3) and an encoding cusedt generatedfrom a previously used item model (Sec.
4.4):ot = fgrut cgrut + fnewt cnewt + fusedt cusedt .The interpolation weights, fgrut , fnewt , and fusedt ,are probabilities representing how much the outputtoken should reflect the current state of the languagemodel or a chosen agenda item.
fgrut is the proba-bility of a non-agenda-item token, fnewt is the prob-ability of an new item reference token, and fusedtis the probability of a used item reference.
In theFig.
1 example, fnewt is high in the first row whennew ingredient references ?tomatoes?
and ?onion?are generated; fusedt is high when the reference backto ?tomatoes?
is made in the second row, and fgrutis high the rest of the time.To generate these weights, our model uses a three-way probabilistic classifier, ref -type(ht), to deter-mine whether the hidden state of the GRU ht willgenerate non-agenda tokens, new agenda item refer-ences, or used item references.
ref -type(ht) gener-ates a probability distribution ft ?
R3 asft = ref -type(ht) = softmax(?Sht),where S ?
R3?k is a trained projection matrix and?
is a temperature hyper-parameter.
fgrut = f1t ,fnewt = f2t , and fusedt = f3t .
ref -type() does notuse the agenda, only the hidden state ht: ht mustencode when to use the agenda, and ref -type() istrained to identify that in ht.4.3 New agenda item reference modelThe two key features of our model are that it (1) pre-dicts which agenda item is being referred to, if any,at each time step and (2) stores those predictions foruse during generation.
These components allow forimproved output texts that are more likely to men-tion agenda items while avoiding repetition and ref-erences to irrelevant items not in the agenda.These features are enabled by a checklist vectorat ?
RL that represents the probability each agendaitem has been introduced into the text.
The checklistvector is initialized to all zeros at t = 1, representingthat all items have yet to be introduced.
The check-list vector is a soft record with each at,i ?
[0, 1].1We introduce the remaining items as a matrixEnewt ?
RL?k, where each row is an agenda itemembedding weighted by how likely it is to still needto be referenced.
For example, in Fig.
1, after thefirst ?tomatoes?
is generated, the row representing?chopped tomatoes?
in the agenda will be weightedclose to 0.
We calculate Enewt using the checklistvector (see ?Update [...] items?
box in Fig.
2):Enewt = ((1L ?
at?1)?
1k) ?
E,where 1L = {1}L, 1k = {1}k, and the outer prod-uct ?
replicates 1L ?
at?1 for each dimension ofthe embedding space.
?
is the Hadamard product(i.e., element-wise multiplication) of two matriceswith the same dimensions.The model predicts when an agenda item will begenerated using ref -type() (see Sec.
4.2 for de-tails).
When it does, the encoding cnewt approxi-mates which agenda item is most likely.
cnewt iscomputed using an attention model that generates alearned soft alignment ?newt ?
RL between the hid-den state ht and the rows of Enewt (i.e., availableitems).
The alignment is a probability distributionrepresenting how close ht is to each item:?newt ?
exp(?Enewt Pht),where P ?
Rk?k is a learned projection matrixand ?
is a temperature hyper-parameter.
In Fig.
1,the shaded squares in the top line (i.e., the first?tomatoes?
and the onion references) represent thisalignment.
The attention encoding cnewt is then theattention-weighted sum of the agenda items:cnewt = ET?newt .At each step, the model updates the checklist vectorbased on the probability of generating a new agendaitem reference, fnewt , and the attention alignment?newt .
We calculate the update to checklist, anewt ,as anewt = fnewt ?
?newt .
Then, the new checklist atis at = at?1 + anewt .1By definition, at is non-negative.
We truncate any valuesgreater than 1 using a hard tanh function.3324.4 Previously used item reference modelWe also allow references to be generated for previ-ously used agenda items through the previously useditem encoding cusedt .
This is useful in longer texts?
when agenda items can be referred to more thanonce ?
so that the agenda is always responsible forgenerating its own referring expressions.
The exam-ple in Fig.
1 refers back to tomatoes when generatingto what to add the diced onion.At each time step t, we use a second atten-tion model to compare ht to a used items matrixEusedt ?
RL?k.
Like the remaining agenda itemmatrix Enewt , Eusedt is calculated using the checklistvector generated at the previous time step:Eusedt = (at?1 ?
1k) ?
E.The attention over the used items, ?usedt ?
RL, andthe used attention encoding cusedt are calculated inthe same way as those over the available items (seeSec.
4.3 for comparison):?usedt ?
exp(?Eusedt Pht),cusedt = ET?usedt .4.5 GRU language modelOur decoder RNN adapts a Gated Recurrent Unit(GRU) (Cho et al, 2014).
Given an input xt ?
Rk attime step t and the previous hidden state ht?1 ?
Rk,a GRU computes the next hidden state ht asht = (1?
zt)ht?1 + zth?t.The update gate, zt, interpolates between ht?1 andnew content, h?t, defined respectively aszt = ?
(Wzxt + Uzht?1),h?t = tanh(Wxt + rt  Uht?1).is an element-wise multiplication, and the resetgate, rt, is calculated asrt = ?
(Wrxt + Urht?1).Wz , Uz , W , U , Wr, Ur ?
Rk?k are trained projec-tion matrices.We adapted a GRU to allow extra inputs, namelythe goal g and the available agenda items Enewt (see?GRU language model?
box in Fig.
2).
These extrainputs help guide the language model stay on topic.Our adapted GRU has a change to the computationof the new content h?t as follows:h?t = tanh(Whxt + rt  Uhht?1+ st  Y g + qt  (1TLZEnewt )T ,where st is a goal select gate and qt is a item selectgate, respectively defined asst = ?
(Wsxt + Usht?1),qt = ?
(Wqxt + Uqht?1).1L sums the rows of the available item matrixEnewt .Y , Z, Ws, Us, Wq, Uq ?
Rk?k are trained projec-tion matrices.
The goal select gate controls whenthe goal should be taken into account during genera-tion: for example, the recipe title may be used to de-cide what the imperative verb for a new step shouldbe.
The item select gate controls when the avail-able agenda items should be taken into account (e.g.,when generating a list of ingredients to combine).The GRU hidden state is initialized with a projec-tion of the goal: h0 = Ugg, where Ug ?
Rk?k.The content vector cgrut that is used to computethe output hidden state ot is a linear projection of theGRU hidden state, cgrut = Pht, where P is the samelearned projection matrix used in the computation ofthe attention weights (see Sections 4.3 and 4.4).4.6 TrainingGiven a training set of (goal, agenda, output text)triples {(g(1), E(1),x(1)), .
.
.
, (g(J), E(J),x(J))},we train model parameters by minimizing negativelog-likelihood: NLL(?)
=?J?j=1Nj?i=2log p(x(j)i |x(j)1 , .
.
.
,x(j)i?1,g(j), E(j); ?
),where x(j)1 is the start symbol.
We use mini-batchstochastic gradient descent, and back-propagatethrough the goal, agenda, and text embeddings.It is sometimes the case that weak heuristic su-pervision on latent variables can be easily gatheredto improve training.
For example, for recipe gen-eration, we can approximate the linear interpolationweights ft and the attention updates anewt and ausedtusing string match heuristics comparing tokens in333the text to tokens in the ingredient list.2 When thisextra signal is available, we add mean squared lossterms toNLL(?)
to encourage the latent variables totake those values; for example, if f?t is the true valueand ft is the predicted value, a loss term ?
(f?t ?
ft)2is added.
When this signal is not available, as is thecase with our dialogue generation task, we insteadintroduce a mean squared loss term that encouragesthe final checklist a(j)Nj to be a vector of 1s (i.e., everyagenda item is accounted for).4.7 GenerationWe generate text using beam search, which has beenshown to be fast and accurate for RNN decoding(Graves, 2012; Sutskever et al, 2014).
When thebeam search completes, we select the highest prob-ability sequence that uses the most agenda items.This is the count of how many times the three-wayclassifier, ref -type(ht), chose to generate an newitem reference with high probability (i.e., > 50%).5 Experimental setupOur model was implemented and trained using theTorch scientific computing framework for Lua.3Experiments We evaluated neural checklist mod-els on two natural language generation tasks.
Thefirst task is cooking recipe generation.
Given arecipe title (i.e., the name of the dish) as the goal andthe list of ingredients as the agenda, the system mustgenerate the correct recipe text.
Our second evalua-tion is based on the task from Wen et al (2015) forgenerating dialogue responses for hotel and restau-rant information systems.
The task is to generate anatural language response given a query type (e.g.,informing or querying) and a list of facts to convey(e.g., a hotel?s name and address).Parameters We constrain the gradient normto 5.0 and initialize parameters uniformly on[?0.35, 0.35].
We used a beam of size 10 for gen-eration.
Based on dev set performance, a learningrate of 0.1 was chosen, and the temperature hyper-parameters (?, ?)
were (5, 2) for the recipe task and(1, 10) for the dialogue task.
The models for therecipe task had a hidden state size of k = 256; the2Similar to anewt , ausedt = fusedt ?
?usedt .3http://torch.ch/models for the dialogue task had k = 80 to compareto previous models.
We use a batch size 30 for therecipe task and 10 for the dialogue task.Recipe data and pre-processing We use the NowYou?re Cooking!
recipe library: the data set containsover 150,000 recipes in the Meal-MasterTM for-mat.4 We heuristically removed sentences that werenot recipe steps (e.g., author notes, nutritional in-formation, publication information).
82,590 recipeswere used for training, and 1,000 each for develop-ment and testing.
We filtered out recipes to avoidexact duplicates between training and dev (test) sets.We collapsed multi-word ingredient names intosingle tokens using word2phrase5 ran on the train-ing data ingredient lists.
Titles and ingredients werecleaned of non-word tokens.
Ingredients addition-ally were stripped of amounts (e.g., ?1 tsp?).
Asmentioned in Sec.
4.6, we approximate true valuesfor the interpolation weights and attention updatesfor recipes based on string match between the recipetext and the ingredient list.
The first ingredient ref-erence in a sentence cannot be the first token or aftera comma (e.g., the bold tokens cannot be ingredientsin ?oil the pan?
and ?in a large bowl, mix [...]?
).Recipe data statistics Automatic recipe genera-tion is difficult due to the length of recipes, the sizeof the vocabulary, and the variety of possible dishes.In our training data, the average recipe length is 102tokens, and the longest recipe has 814 tokens.
Thevocabulary of the recipe text from the training data(i.e., the text of the recipe not including the title oringredient list) has 14,103 unique tokens.
About31% of tokens in the recipe vocabulary occur at least100 times in the training data; 8.6% of the tokens oc-cur at least 1000 times.
The training data also repre-sents a wide variety of recipe types, defined by therecipe titles.
Of 3793 title tokens, only 18.9% of thetitle tokens in the title vocabulary occur at least 100times in the training data, which demonstrates thelarge variability in the titles.Dialogue system data and processing We usedthe hotel and restaurant dialogue system corpus andthe same train-development-test split from Wen etal.
(2015).
We used the same pre-processing, sets4Recipes and format at http://www.ffts.com/recipes.htm5See https://code.google.com/p/word2vec/334of reference samples, and baseline output, and wewere given model output to compare against.6 Fortraining, slot values (e.g., ?Red Door Cafe?)
were re-placed by generic tokens (e.g., ?NAME TOKEN?
).After generation, generic tokens were swapped backto specific slot values.
Minor post-processing in-cluded removing duplicate determiners from the re-lexicalization and merging plural ?-s?
tokens ontotheir respective words.
After replacing specific slotvalues with generic tokens, the training data vocab-ulary size of the hotel corpus is 445 tokens, and thatof the restaurant corpus is 365 tokens.
The task haseight goals (e.g., inform, confirm).Models Our main baseline EncDec is a model us-ing the RNN Encoder-Decoder framework proposedby Cho et al (2014) and Sutskever et al (2014).
Themodel encodes the goal and then each agenda itemin sequence and then decodes the text using GRUs.The encoder has two sets of parameters: one for thegoal and the other for the agenda items.
For the di-alogue task, we also compare against the SC-LSTMsystem from Wen et al (2015) and the handcraftedrule-based generator described in that paper.For the recipe task, we also compare against threeother baselines.
The first is a basic attention model,Attention, that generates an attention encoding bycomparing the hidden state ht to the agenda.
Thatencoding is added to the hidden state, and a non-linear transformation is applied to the result beforeprojecting into the output space.
We also present anearest neighbor baseline (NN) that simply copiesover an existing recipe text based on the input simi-larity computed using cosine similarity over the titleand the ingredient list.
Finally, we present a hybridapproach (NN-Swap) that revises a nearest neighborrecipe using the neural checklist model.
The neuralchecklist model is forced to generate the returnedrecipe nearly verbatim, except that it can generatenew strings to replace any extraneous ingredients.Our neural checklist model is labeled Checklist.We also present the Checklist+ model, which in-teractively re-writes a recipe to better cover the in-put agenda: if the generated text does not use everyagenda item, embeddings corresponding to missingitems are multiplied by increasing weights and a newrecipe is generated.
This process repeats until the6We thank the authors for sharing their system outputs.Model BLEU-4 METEOR Avg.
%givenitemsAvg.extraitemsAttention 2.8 8.6 22.8% 3.0EncDec 3.1 9.4 26.9% 2.0NN 7.1 12.1 40.0% 4.2NN-Swap 7.1 12.8 58.2% 2.1Checklist 3.0 10.3 67.9% 0.6- ot = ht 2.1 8.3 29.1% 2.4- no used 3.0 10.4 62.2% 1.9- no supervision 3.7 10.1 38.9% 1.8Checklist+ 3.8 11.5 83.4% 0.8Table 1: Quantitative results on the recipe task.
The line withot = ht has the results for the non-interpolation ablation.new recipe does not contain new items.We also report the performance of our check-list model without the additional weak supervisionof heuristic ingredient references (- no supervision)(see Sec.
4.6).7 we also evaluate two ablations ofour checklist model on the recipe task.
First, we re-move the linear interpolation and instead use ht asthe output (see Sec.
4.2).
Second, we remove thepreviously used item reference model by changingref -type() to a 2-way classifier between new ingre-dient references and all other tokens (see Sec.
4.4).Metrics We include commonly used metrics likeBLEU-4,8 and METEOR (Denkowski and Lavie,2014).
Because neither of these metrics can measurehow well the generated recipe follows the input goaland the agenda, we also define two additional met-rics.
The first measures the percentage of the agendaitems corrected used, while the second measures thenumber of extraneous items incorrectly introduced.Both these metrics are computed based on simplestring match and can miss certain referring expres-sions (e.g., ?meat?
to refer to ?pork?).
Because ofthe approximate nature of these automated metrics,we also report a human evaluation.6 Recipe generation resultsFig.
1 results for recipe generation.
All BLEU andMETEOR scores are low, which is expected for longtexts.
Our checklist model performs better than bothneural network baselines (Attention and EncDec) inall metrics.
Nearest neighbor baselines (NN andNN-Swap) perform the best in terms of BLEU and7For this model, parameters were initialized on [-0.2, 0.2] tomaximize development accuracy.8See Moses system (http://www.statmt.org/moses/)335Model Syntax Ingredient use Follows goalAttention 4.47 3.02 3.47EncDec 4.58 3.29 3.61NN 4.22 3.02 3.36NN-Swap 4.11 3.51 3.78Checklist 4.58 3.80 3.94Checklist+ 4.39 3.95 4.10Truth 4.39 4.03 4.34Table 2: Human evaluation results on the generated and truerecipes.
Scores range in [1, 5].1.0010.00100.001000.0010000.000  500  1000  1500  2000TokencountsindevrecipesTokens (sorted by count)True recipesEncDecChecklist+Figure 3: Counts of the most used vocabulary tokens (sortedby count) in the true dev set recipes and in generated recipes.METEOR; this is due to a number of recipes thathave very similar text but make different dishes.However, NN baselines are not successful in gen-erating a goal-oriented text that follows the givenagenda: compared to Checklist+ (83.4%), they usesubstantially less % of the given ingredients (40% -58.2%) while also introducing extra ingredients notprovided.
EncDec and Attention baselines similarlygenerate recipes that are not relevant to the given in-put, using only 22.8% - 26.9% of the agenda items.Checklist models rarely introduce extraneous ingre-dients not provided (0.6 - 0.8), while other baselinesmake a few mistakes on average (2.0 - 4.2).The ablation study demonstrates the empiricalcontribution of different model components.
(ot =ht) shows the usefulness of the attention encodingswhen generating the agenda references, while (-noused) shows the need for separate attention mech-anisms between new and used ingredient referencesfor more accurate use of the agenda items.
Similarly,(-no supervision) demonstrates that the weak super-vision encourages the model to learn more accuratemanagement of the agenda items.Human evaluation Because neither BLEU norMETEOR is suitable for evaluating generated textin terms of their adherence to the provided goal andthe agenda, we also report human evaluation usingAmazon Mechanical Turk.
We evaluate the gener-ated recipes on (1) grammaticality, (2) how well therecipe adheres to the provided ingredient list, and (3)how well the generated recipe accomplishes the de-sired dish.
We selected 100 random test recipes.
Foreach question we used a Likert scale (?
[1, 5]) andreport averaged ratings among five turkers.Table 2 shows the averaged scores over the re-sponses.
The checklist models outperform all base-lines in generating recipes that follow the providedagenda closely and accomplish the desired goal,where NN in particular often generates the wrongdish.
Perhaps surprisingly, both the Attention andEncDec baselines and the Checklist model beat thetrue recipes in terms of having better grammar.
Thiscan partly be attributed to noise in the parsing of thetrue recipes, and partly because the neural modelstend to generate shorter, simpler texts.Fig.
3 shows the counts of the most used vocab-ulary tokens in the true dev set recipes comparedto the recipes generated by EncDec and Checklist+.Using the vocabulary from the training data, the truedev recipes use 5206 different tokens.
The EncDec?svocabulary is only ?16% of that size, while theChecklist+ model is a third of the size.An error analysis on the dev set shows that theEncDec baseline over-generates catch-all phraseslike ?all ingredients?
or ?the ingredients,?
used in21% of the generated recipes, whereas only 7.8% oftrue recipes use that construction.
This phrase typesimplifies the recipe, but using all ingredients in onestep reduces the chance of accomplishing the desireddish correctly.
The Checklist model only generatesthose phrases 13% of the time.Qualitative analysis Fig.
4 shows two dev setrecipes with generations from the EncDec andChecklist+ models.
The EncDec model is muchmore likely to both use incorrect ingredients and tointroduce ingredients more than once (e.g., ?bakingpower?
and ?salt?
in the bottom example are not inthe ingredient list, and ?milk?
in the top exampleis duplicated).
In the top example, the Checklist+model refers to both Parmesean and Swiss cheeseas ?cheese?
; generating more precise referring ex-pressions is an important area for future work.
TheChecklist+ recipes generate the correct dishes to anextent: for example, the top recipe makes a casse-role but does not cook the ingredients together be-fore baking and mixes in biscuits instead of putting336Title: Casino royale casseroleIngredients: 10 oz chopped broccoli, 2 tbsp butter, 2 tbsp flour, 1/2 tsp salt, 1/4 tsp black pepper, 1/4 tsp ground nutmeg, 1 cup milk,1 1/2 cup shredded swiss cheese, 2 tsp lemon juice, 2 cup cooked cubed turkey, 4 oz mushrooms, 1/4 cup grated Parmesan cheese, 1can refrigerated biscuitsTruth EncDec Checklist+Preheat oven to 375.
Melt butter in saucepan.Blend in flour, salt, pepper, and nutmeg; cook1-2 minutes.
Gradually add milk; cook, stir-ring, until slightly thickened.
Stir in frozenvegetables.
Remove from heat; stir in cheeseuntil melted.
Add lemon juice, turkey, mush-rooms, and broccoli.
Pour mixture into alightly greased 8?x8?
baking dish; sprinklewith parmesean and top with biscuits.
Bake 20minutes, or until biscuits are golden brown.Cook broccoli according to package directions;drain.
In medium saucepan, melt butter.
Stir inflour, salt and pepper.
Gradually add milk, stir-ring constantly.
Cook over medium heat, stir-ring constantly, until mixture comes to a boil.Reduce heat to low.
Cook, stirring constantly,until mixture boils and thickens, about 5 min-utes.
Stir in milk.
Gradually heat to boiling;reduce heat.
Cover; simmer 10 minutes or untilheated through.
Serve immediately.Mix the turkey with the flour, salt,pepper, milk and lemon juice.
Stirin the cheese.
Add broccoli, mush-rooms, nutmeg, and cheese.
Mix well.Add the butter and mix well.
Add thebiscuits and mix well.
Spread in agreased 13x9 inch baking pan.
Bakein a 350f oven for 35 to 40 minutes, oruntil pick inserted in the center comesout clean.
Serve hot.Title: Kathie Frieman?s cheese cakeIngredients: 2 lb cream cheese, 1 3/4 cups sugar, 5 eggs, 15 oreo cookies, 1/2 tsp vanilla, 1/2 tsp almond extractTruth EncDec Checklist+Butter pan well.
Whip cheese until soft and addsugar, combine until fluffy.
Add extracts and eggs.Chop oreos into pieces and fold into mixture.
Pourinto pan and place in a pan of 1 or more inchesof water.
Bake at 350 1 1/2 hours ( check after 11/4 hours ) until center is set.
Cool in pan of wa-ter.
When cool turn upside-down on plate.
Sprinklewith cookie crumbs.Beat cream cheese and sugar together untillight and fluffy.
Add eggs, one at a time, beat-ing well after each addition.
Blend in vanilla.Sift flour, measure, and sift with baking powerand salt.
Add to creamed mixture alternatelywith milk.
Fold in stiffly beaten egg whites.Pour into 9x13 pan.
Bake at 350 degrees for 25to 30 minutes.Cream sugar and cream cheeseuntil smooth.
Add eggs, one at atime, beating well after each ad-dition.
Add vanilla, almond ex-tract, and cookies.
Mix well.
Pourinto greased and floured tube pan.Bake at 350 degrees for 30 min-utes.Figure 4: Example dev set generated recipes.
Tokenization, newlines, and capitalization changed for space and readability.
Boldedingredient references are either ingredients not in the list and/or duplicated initial ingredient references.Model Hotel RestaurantBLEU METEOR BLEU METEORHDC 55.52 48.10 44.39 43.42SC-LSTM 86.53 60.84 74.49 54.31Checklist 90.61 62.10 77.82 54.42Table 3: Quantitative evaluation of the top generations in thehotel and restaurant domainsthem on top.
Future work could better model the fullset of steps needed to achieve the overall goal.7 Dialogue system resultsFigure 3 shows our results on the hotel and restau-rant dialogue system generation tasks.
HDC is therule-based baseline from Wen et al (2015).
For bothdomains, the checklist model achieved the highestBLEU-4 and METEOR scores, but both neural sys-tems performed very well.
The power of our modelis in generating long texts, but this experiment showsthat our model can generalize well to other taskswith different kinds of agenda items and goals.8 Future work and conclusionsWe present the neural checklist model that gener-ates globally coherent text by keeping track of whathas been said and still needs to be said from a pro-vided agenda.
Future work includes incorporatingreferring expressions for sets or compositions ofagenda items (e.g., ?vegetables?).
The neural check-list model is sensitive to hyperparameter initializa-tion, which should be investigated in future work.The neural checklist model can also be adapted tohandle multiple checklists, such as checklists overcomposite entities created over the course of a recipe(see Kiddon (2016) for an initial proposal).AcknowledgementsThis research was supported in part by the In-tel Science and Technology Center for PervasiveComputing (ISTC-PC), NSF (IIS-1252835 and IIS-1524371), DARPA under the CwC program throughthe ARO (W911NF-15-1-0543), and gifts by Googleand Facebook.
We thank our anonymous review-ers for their comments and suggestions, as well asYannis Konstas, Mike Lewis, Mark Yatskar, AntoineBosselut, Luheng He, Eunsol Choi, Victoria Lin,Kenton Lee, and Nicholas FitzGerald for helping usread and edit.
We also thank Mirella Lapata and An-nie Louis for their suggestions for baselines.337ReferencesGabor Angeli, Percy Liang, and Dan Klein.
2010.
Asimple domain-independent probabilistic approach togeneration.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 502?512.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
In ICLR 2015.Niranjan Balasubramanian, Stephen Soderland, Mausam,and Oren Etzioni.
2013.
Generating coherent eventschemas at scale.
In Proceedings of the 2013 Con-ference on Empirical Methods on Natural LanguageProcessing, pages 1721?1731.Regina Barzilay and Mirella Lapata.
2005.
Collec-tive content selection for concept-to-text generation.In Proceedings of the 2005 Conference on EmpiricalMethods in Natural Language Processing, pages 331?338.Jianpeng Cheng, Li Dong, and Mirella Lapata.
2016.Long short-term memory-networks for machine read-ing.
In Proceedings of the 2016 Conference on Empir-ical Methods in Natural Language Processing.Kyunghyun Cho, Bart van Merrienboer, C?aglar Gu?lc?ehre,Fethi Bougares, Holger Schwenk, and Yoshua Ben-gio.
2014.
Learning phrase representations usingRNN encoder-decoder for statistical machine trans-lation.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 1724?1734.Robert Dale.
1988.
Generating Referring Expressionsin a Domain of Objects and Processes.
Ph.D. the-sis, Centre for Cognitive Science, University of Ed-inburgh.Michael Denkowski and Alon Lavie.
2014.
Meteor uni-versal: Language specific translation evaluation forany target language.
In Proceedings of the EACL 2014Workshop on Statistical Machine Translation, pages376?380.Alex Graves.
2012.
Sequence transduction with recur-rent neural networks.
Representation Learning Work-sop, ICML.C?aglar Gu?lc?ehre, Sungjin Ahn, Ramesh Nallapati,Bowen Zhou, and Yoshua Bengio.
2016.
Pointing theunknown words.
In Proceedings of the 54th AnnualMeeting of the Association for Computational Linguis-tics, pages 140?149.Kristian J. Hammond.
1986.
CHEF: A model of case-based planning.
In Proceedings of the Fifth NationalConference on Artificial Intelligence (AAAI-86), pages267?271.R.
Jia and P. Liang.
2016.
Data recombination for neuralsemantic parsing.
In Proceedings of the 54th AnnualMeeting of the Association for Computational Linguis-tics, pages 12?22.Xu Jia, Efstratios Gavves, Basura Fernando, and TinneTuytelaars.
2015.
Guiding long-short term memoryfor image caption generation.
In Proceedings of theIEEE International Conference on Computer Vision,pages 2407?2415.Chloe?
Kiddon.
2016.
Learning to Interpret and GenerateInstructional Recipes.
Ph.D. thesis, Computer Science& Engineering, University of Washington.Ioannis Konstas and Mirella Lapata.
2013.
A globalmodel for concept-to-text generation.
Journal of Ar-tificial Intelligence Research (JAIR), 48:305?346.Percy Liang, Michael I. Jordan, and Dan Klein.
2009.Learning semantic correspondences with less supervi-sion.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP: Volume 1 - Volume 1, pages 91?99.Minh-Thang Luong, Hieu Pham, and Christopher D.Manning.
2015.
Effective approaches to attention-based neural machine translation.
In Proceedings ofthe 2015 Conference on Empirical Methods in NaturalLanguage Processing, pages 1412?1421, September.Hongyuan Mei, Mohit Bansal, and Matthew R. Walter.2016.
What to talk about and how?
Selective genera-tion using lstms with coarse-to-fine alignment.
In The15th Annual Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 720?730.Tomas Mikolov, Martin Karafia?t, Luka?s Burget, Jan Cer-nocky?, and Sanjeev Khudanpur.
2010.
Recurrent neu-ral network based language model.
In Proceedings ofINTERSPEECH 2010, the 11th Annual Conference ofthe International Speech Communication Association,pages 1045?1048.Tomas Mikolov, Stefan Kombrink, Luka?s Burget, JanCernocky?, and Sanjeev Khudanpur.
2011.
Exten-sions of recurrent neural network language model.
InProceedings of the IEEE International Conference onAcoustics, Speech, and Signal Processing, (ICASSP2011), pages 5528?5531.Shinsuke Mori, Hirokuni Maeta, Tetsuro Sasada,Koichiro Yoshino, Atsushi Hashimoto, Takuya Fu-natomi, and Yoko Yamakata.
2014.
FlowGraph2Text:Automatic sentence skeleton compilation for proce-dural text generation.
In Proceedings of the 8th In-ternational Natural Language Generation Conference,pages 118?122.Ehud Reiter and Robert Dale.
2000.
Building NaturalLanguage Generation Systems.
Cambridge UniversityPress, New York, NY, USA.338Alexander M. Rush, Sumit Chopra, and Jason Weston.2015.
A neural attention model for abstractive sen-tence summarization.
In Proceedings of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 379?389.Alessandro Sordoni, Michel Galley, Michael Auli, ChrisBrockett, Yangfeng Ji, Meg Mitchell, Jian-Yun Nie,Jianfeng Gao, and Bill Dolan.
2015.
A neural networkapproach to context-sensitive generation of conversa-tional responses.
In Conference of the North AmericanChapter of the Association for Computational Linguis-tics Human Language Technologies (NAACL-HLT),pages 196?205.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to sequence learning with neural networks.In Z. Ghahramani, M. Welling, C. Cortes, N. D.Lawrence, and K. Q. Weinberger, editors, Advancesin Neural Information Processing Systems 27, pages3104?3112.
Curran Associates, Inc.Henry S. Thompson.
1977.
Strategy and tactics: a modelfor language production.
In Papers from the Thir-teenth Regional Meeting of the Chicago LinguisticsSociety, pages 89?95.
Chicago Linguistics Society.Zhaopeng Tu, Yang Liu, Zhengdong Lu, Xiaohua Liu,and Hang Li.
2016a.
Context gates for neural machinetranslation.
CoRR, abs/1608.06043.Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,and Hang Li.
2016b.
Modeling coverage for neu-ral machine translation.
In Proceedings of the 54thAnnual Meeting of the Association for ComputationalLinguistics, pages 76?85.Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-haoSu, David Vandyke, and Steve J.
Young.
2015.
Se-mantically conditioned LSTM-based natural languagegeneration for spoken dialogue systems.
In Proceed-ings of the 2015 Conference on Empirical Methods inNatural Language Processing, pages 1711?1721.Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic,Lina Maria Rojas-Barahona, Pei-hao Su, DavidVandyke, and Steve J.
Young.
2016.
Multi-domainneural network language generation for spoken dia-logue systems.
In Proceedings of the 15th AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics: Human Lan-guage Technologies, pages 120?129.Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,Aaron C. Courville, Ruslan Salakhutdinov, Richard S.Zemel, and Yoshua Bengio.
2015.
Show, attend andtell: Neural image caption generation with visual at-tention.
Proceedings of the 32nd International Con-ference on Machine Learning, pages 2048?2057.339
