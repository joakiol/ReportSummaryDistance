Extraction of Chinese Compound Words -An Experimental Study on a Very Large CorpusJian ZhangDepartment of Computer Science andTechnology of Tsinghua University, Chinaajian@sl000e.cs.tsinghua.edu.cnJianfeng Gao, Ming ZhouMicrosoft Research China{jfgao, mingzhou }@microsoft.cornAbstractThis paper is to introduce a statisticalmethod to extract Chinese compound wordsfrom a very large corpusL This method isbased on mutual information and contextdependency.
Experimental results show thatthis method is efficient and robustcompared with other approaches.
We alsoexamined the impact of different parametersettings, corpus size and heterogeneousnesson the extraction results.
We finally presentresults on information retrieval to show theusefulness of extracted compounds.1 IntroductionAlmost all techniques to statistical anguageprocessing, including speech recognition, machinetranslation and information retrieval, are based onwords.
Although word-based approaches workvery well for western languages, where words arewell defined, it is difficult to apply to Chinese.Chinese sentences are written as characters stringswith no spaces between words.
Therefore, wordsin Chinese are actually not well marked insentences, and there does not exist a commonlyaccepted Chinese lexicon.Furthermore, since new compounds (wordsformed with at least wo characters) are constantlycreated, it is impossible to list them exhaustivelyin a lexicon.
Therefore, automatic extraction ofcompounds is an important issue.
Traditionalextraction approaches used rules.
However,compounds extracted in this way are not alwaysdesirable.
So, human effort is still required to findthe preferred compounds from a large compoundl This work was done while the author worked forMicrosoft Research China as a visiting student.candidate list.
Some statistical approaches toextract Chinese compounds from corpus havebeen proposed (Lee-Feng Chien 1997, WU Dekaiand Xuanyin XIA 1995, Ming-Wen Wu and Keh-Yih Su 1993) as well, but almost all experimentsare based on relatively small corpus, it is not clearwhether these methods till work well with largecorpus.In this paper, we investigate statisticalapproaches to Chinese compound extraction fromvery large corpus by using statistical features,namely mutual information and contextdependency.
There are three main contributions inthis paper.
First, we apply our procedure on a verylarge corpus while other experiments were basedon small or medium size corpora.
We show thatbetter esults can be obtained with a large corpus.Second, we examine how the results can beinfluenced by parameter settings including mutualinformation and context dependency restrictions.It turns out that mutual information mainly affectsprecision while context dependency affects thecount of extracted items.
Third, we test theusefulness of the extracted compounds forinformation retrieval.
Our experimental results onIR show that the new compounds have a positiveeffect on IR.The rest of this paper is structured as follows.
Insection 2, we describe the techniques we used.
Insection 3, we present several sets of experimentalresults.
In section 4, we outline the related worksas well as their results.
Finally, we give ourconclusions in section 5.2 Technique descriptionStatistical extraction of Chinese compounds hasbeen used in (Lee-Feng Chien 1997)(WU Dekaiand Xuanyin XIA 1995) and (Ming-Wen Wu andKeh-Yih Su 1993).
The basic idea is that a132Chinese compound should appear as a stablesequence in corpus.
That is, the components in thecompound are strongly correlated, while thecomponents lie at both ends should have lowcorrelations with otiter words.The method consists of two steps.
At fast, a fistof candidate compounds i extracted from a verylarge corpus by using mutual information.
Then,context dependency is used to remove undesirablecompounds.
In what follows, we will describethem in more detail.2.1 Mutual InformationAccording to our study on Chinese corpora,most compounds are of length less than 5characters.
The average length of words in thesegmented-corpus i of approximately 1.6characters.
Therefore, only word bi-gram, tn'-gram, and quad-gram in the corpus are of interestto us in compound extraction.We use a criterion, called mutual inform~on,to evaluate the correlation of different componentsin the compound.
Mutual information Ml(x,y) of abi-gram (x, y) is estimated by:Ml(x ,y )  = f (x ,y )f (x )  + f (y ) -  f (x ,y )Where f(x) is the occurrence frequency of wordx in the corpus, and fix, y) is the occurrencefrequency of the word pair (x,y) in the corpus.
Thehigher the value of MI is, the more likely x and yare to form a compound.The mutual information MI(x,y,z) of tri-gram(x,y,z) is estimated by:Ml(x,  y, z) = f (x ,  y, z)f (x)  + f (y)  + f ( z ) -  f(x,  y, z)The estimation of mutual information of quad-grams is similar to that of tri-grams.
The extractedcompounds should be of higher value of MI than apre-set threshold.2.2 Context DependencyFigure 1The extracted Chinese compounds hould becomplete.
That is, we should generate a wholeword, not a part of it.
For example,~,~-~-~t'~J(missi le defense plan) is acomplete word, and -~.~0-1~J~ (missile defense) isnot, although both have relatively high value ofmutual information.Therefore, we use another feature, calledcontext dependency.
The contexts of the wordl~(defense)  are illustrated by figure 1.A compound X has NO left context dependencyifLSize -~ L I > tl orf (ctX)  MaxL = MAX ~ - -  < t2f (X )Where tl, t2 are threshold value, j\[.)
isfrequency, L is the set of left adjacent strings of X,tz~L and ILl means the number of unique leftadjacent strings.
Similarly, a compound X has NOright context dependency ifRSize ~ R l> t3 orf ( /~)  < t4 MaxR = MAX a f ( X )Where tl, t2, t3, t4 are threshold value, f(.)
isfrequency, R is the set of right adjacent strings ofX, tiER and \[R I means the number of unique leftadjacent strings.The extracted complete compounds should haveneither left nor fight context dependency.3 Experimental resultsIn our experiments, three corpora were used totest the performance of the presented approach.These corpora re described in table 1.
Corpus Aconsists of local news with more than 325 millioncharacters.
Corpus B consists of documents fromdifferent domains of novel, news, techniquereport, etc., with approximately 650 millioncharacters.
Corpus C consists of People's Dailynews and Xinhua news from TREC5 and TREC6(Harman and Voorhees, 1996) with 75 millioncharacters.Table 1: Characteristics of Corpora!Corpus Source Size(char#)Corpus political, economic news 325 MACorpus Corpus A + novels + 650 MB technique reports, etc.133'cCOrpus \[TREC 5/6 Chinese 75 M,, ,coMus I IIn the first experiment, we test the perfomaanceof our method on corpus A, which is homogeneityin style.
We then use corpus B in the secondexperiment totest if the method works as well onthe corpus that is heterogeneity n style.
We alsouse different parameter settings in order to figureout the best combination of the two statisticalfeatures, i.e.
mutual information and contextdependency.
In the third experiment, we apply theresults of the method to information retrievalsystem.
We extract new compounds on corpus C,and add them to the indexing lexicon, and weachieve a higher average precision-recall.
In allexperiments, corpora re segmented automaticallyinto words using a lexicon consisting of 65,502entries.3.1 Compounds Extraction from HomogeneousCorpusCorpus A contains political and economic news.In this series of tests, we gradually loosen theconditions to form a compound, i.e.
MI thresholdbecomes smaller and MaxL/MaxR becomes larger.Results for quad-graras, tri-graras and bi-gramsare shown in tables 2,3,4.
Some compoundsextracted are fisted in table 5.12345Table2: Performance of quad-sram compoundsParameter setting(MI, LSize, MaxL, RSize, MaxR) compounds found0.01 1 0.75 1 0.75 !
10extractionNumber of New Precision27(correctcompounds/compounds checked)0% (27/27)0.005 1 0.85 1 0.85 92 98.9% (91/92)0.002 1 0.90 1 0.90 513 95.8% (113/118)0.001 1 0.95 1 0.95 1648 96.2% (179/186)0.0005 1 0.95 1 0.95 4707 96.7% (206/213)12345Table3: Performance of tri-sram compounds extractionParameter setting(MI, LSize, MaxL, RSize, MaxR)0.02 2 0.70 2 0.70Number of Newcompounds found167Precision (correctcompounds/compounds checked)100% (167/167)0.01 2 0.75 2 0.75 538 100% (205/205)0.005 2 0.80 2 0.80 1607 100% (262/262)0.003 2 0.80 2 0.80 3532 98.3% (341/347)0.001 2 0.80 2 0.80 16849 96.6% (488/501)12345Table4: Performance of bi-sram compounds extractionParameter setting Number of New Precision(MI, LSize, MaxL, RSize, MaxR) compounds found compound#0.05 3 0.5 3 0.5 1622(correct:~ s/compounds checked)98.9% (184/186)0.05 3 0.6 3 0.6 1904 98.6% (309/212)0.03 3 0.6 3 0.6 3938 97.8% (218/223)0.01 3 0.5 3 0.5 14666 97.5% (354/363)0.005 3 0.5 3 0.5 32899 97.3% (404/415)N-gramN=2N=3N=4Table 5: Some N-gram compounds found by our methodExtracted Compounds~\]Jg~(graindepot), ~~J~(CD-ROM Driver), ~ \ [~(B i l l  Gates)(XuanWu Gate), (asynchronous t ransfer  model),(Amazon)~\[~\ [~\ [~ (Eiysee).
~\]~\[~i\[~H (Ohio), ~\ [~\ [~\ [~ (Mr. Dong Jianhua)134It turns out that our algorithm successfullyextracted a large number of new compounds(>50000) from raw texts.
Compared with previousmethods described in the next section, theprecision is very high.
We can also find that thereis little precision loss when we loose restriction.The result may be due to three reasons.
First, thetwo statistical features really characterize thenature of compounds, and provide a simple andefficient way to estimate the possibility of a wordsequence being a compound.
Second, the corpuswe use is very large.
It is always true that moredata leads to better esults.
Third, the corpus weused in this experiment is homogeneity in style.The raw corpus is composed of news on politics,economy, science and technology.
These areformal articles, and the sentences and compoundsare well normalized and strict.
This is very helpfulfor compound extraction.3.2 Compounds Extraction from HeterogeneousCorpusIn this experiment, we use a heterogeneouscorpus.
It is a combination of corpus A, and someother novels, technique reports, etc.
Forsimplicity, we discuss the extraction of bi-gramcompounds only.
In comparison with the firstexperiment, we find that the precision is stronglyaffected by the corpus we used.
As shown in table6, for each corpus, we use the same parametersetting, say MI >0.005, LSize >3, MaxL <0.5,RSize>3 and MaxR<0.5.Table 6: Impact of heterogeneousness of corporaCorpus Compounds Extractextracted precisionCorpus A 32899 97.3%(4041415)Corpus B 36383 88.3%(362/410)As we mentioned early, the larger the corpus weuse, the better results we obtain.
Therefore, weintuitively expect better esult on corpus B, whichis larger than corpus A.
But, the result shown intable 6 is just the opposite.There are mainly two reasons for this.
The firstone is that our method works better onhomogeneous corpus than on heterogeneouscorpus.
The second one is that it might not besuitable to use the same parameter settings on twodifferent corpora.
We then try different parametersettings on corpus B.There are two groups of parameters.
MImeasures the correlation between adjacent words,and other four parameters, namely LSize, RSize,MaxL, and MaxR, measure the contextdependency.
Therefore, each time, we fix oneparameter, and relax another from fight to loose tosee what happens.
The Number of extractedcompounds and precision of each parametersetting are shown in table 7.MRCD0.00020.00040.00060.00080.00100.00120.0014Table 7: Extraction results with different parameter settings(Ml=Mutual Information, CD = Context Dependency=(LSize, MaxL, RSize, MaxR(2, 0.8, 2, (6, 0.7, 6, (10, 0.6, (14, 0.5, 4, (18, 0.4, (22, 0.3,0.8)1457781(39.06%)784082(48.98%)530723(51.28%)396602(54.63%)313868(59.11%)257990(58.94%)217766(58.93%)0.7)809502(42.24%)485143(46.84%)349882(53.96%)273231(58.00%)223827(66.51%)189014(59.50%)163189(67.91%)10, 0.6)570601(43.98%)359499(52.53%)266068(60.39%)211044(55.19%)175050(61.14%)149315(60.98%)129978(60.19%)0.5)426223(44.67%)277673(49.25%)208921(52.48%)167660(65.24%)140197(57.66%)120312(65.28%)105334(65.84%)18, 0.4)314810(43.96%)209634(53.92%)159363(49.49%)128819(60.54%)108322(67.38%)93323(70.47%)82083(66.83%)22, 0.3)209910(43.38%)141215(49.55%)108120(63.35%)87869(64.40%)74104(63.08%)64079(65.32%)56582(67.50%)(26, 0.2, 6,0.2)96383(40.93%)63907/(52.53%)48683(61.65%)39502(54.86%)33354(67.50%)28879(64.65%)25486(65.46%)135Table 7 shows the extraction results withdifferent parameters.
These results fit ourintuition.
While parameters become more andmore strict, less and less compounds are foundand precisions become higher.
This phenomena isalso illustrated in figure 2 and 3, in which the"correct compounds extracted" is an estimationfrom tableT, i.e.
number of compounds found xprecision.
(These two figures are very useful forone who wants to automatically extract a newlexicon with pre-defined size from a large corpus.
)600500400300~ 2oo~ loooo2 3 4 5 6 7mutual informationFigure 2 Impact of Parameter Mutual Information-(2 0.8 2 0.8)-(6 0.7 6 0.7)?
(I0 0.6 I0 0.6)-(14 0.5 14 0.5)-(18 0.4 18 0.4)-(22 0.3 22 0.3)-(26 0.2 26 0.2)oO)o~gg60050040030020010002 3 4 5 6 7context dependencyix=0.0002ix=0.0004ix=0.0006ix=0.0008ix=0.0010ix=0.0012ix=0.0014Figure 3 Impact of Parameter Context Dependency136The precision of extraction is estimated in thefollowing way.
We extract a set of compoundsbased on a seres of pre-defined parameter set.
Foreach set of compotinds, we randomly select 200compounds.
Then we merge those selectedcompounds to a new file for manually check.
Thisfile consists of about 9,800 new compoundsbecause there are 49 compounds lists.
One personwill group these 'compounds' into two sets, sayset A and set B.
Set A contains the items that areconsidered to be correct, and set B containsincorrect ones.
Then for each original group ofabout 200 compounds we select in the first step,we check how many items that also appear in setA and how many items in set B.
Suppose thesetwo values are al and bl, then we estimate theprecision as al/(al+bl).So, there are two important points in ourevaluation process.
First, it is difficult to give adefinition of the term "compound" to be acceptedpopularly.
Different people may have differentjudgement.
Only one person takes part in theevaluation in our experiment.
This can eliminatethe effect of divergence among different persons.Second, we merge those items together.
This caneliminate the effect of different ime period.
Onemay feel tired after checked too many items.
If hechecks those 49 files one by one, the latter resultsare incomparable with the previous one.The precisions estimated by the above methodare not exactly correct.
However, as describedabove, the precisions of different parametersettings are comparable.
In this experiment, whatwe want to show is how the parameter settingsaffect he results.Both MI and CD can affect number of extractedcompounds, as shown in table 7.
Compared withMI, CD has stronger effect in this aspect.
For eachrow in table 7, numbers of extracted compoundsfinally decrease to 10% of that showed in the firstcolumn.
For each column, while MI changes from0.0002 to 0.0014, the number is decreased ofabout 20%.
This may be explained by the fact thatit is difficult for candidate to fulfill all fourrestrictions in CD simultaneously.
Manydisqualified candidates are cut off.
Table 7 liststhe precisions of extracted results.
It shows thatthere is no clear increasing/decreasing pattern ineach row.
That is to say, CD doesn't stronglyaffect he precision.
When we check each column,we can see that precision is in a growing progress.As we defined above, MI and CD are twodifferent measurements.
What role they play inour extraction procedure?
Our conclusion is thatmutual information mainly affects the precisionwhile context dependency mainly affects the countof extracted items.
This conclusion is alsoconfirmed by Fig2 and Fig3.
That is, the curves inFig2 are more fiat than corresponding curves inFig3.3.3 Testing the Extracted Compounds inInformation RetrievalIn this experiment, we apply our method toimprove information retrieval results.
We useSMART system (Buckley 1985) for ourexperiments.
SMART is a robust, efficient andflexible information retrieval system.
The corpusused in this experiment is TREC Chinese corpus(Harman and Voorhees, 1996).
The corpuscontains about 160,000 articles, including articlespublished in the People's Daily from 1991 to1993, and a part of the news released by theXinhua News Agency in 1994 and 1995.
A set of54 queries has been set up and evaluated bypeople in NIST(Nafional Institute of Standardsand Technology).We first use an initial lexicon consisting of65,502 entries to segment the corpus.
Whenrunning SMART on the segmented corpus, weobtain an average precision of 42.90%.Then we extract new compounds from thesegmented corpus, and add them into the initiallexicon.
With the new lexicon, the TREC Chinesecorpus is re-segmented.
When running SMARTon this re-segmented corpus, we obtain an averageprecision of 43.42%, which shows a slightimprovement of 1.2%.Further analysis shows that the new lexiconbrings positive effect to 10 queries and negativeeffect to 4 queries.
For other 40 queries, there isno obvious effect.
Some improved queries arelisted in table 8 as well as new compounds beingcontained.As an example, we give the segmentationresults with the two lexicons for query 23 in table9.137QueryID923Base lineprecision0.36480.3940Newprecision0.41730.5154Table 8: Improved Query SamplesImprovement Extracted compounds14.4%30.8%ME(drugssale), ~ \[\] :~li~ Ih\] ~(Drug Problemsin China)I~ -~- \ [ \ ] : '~( the  UN SecurityCouncil),~l\] ~1~ ,~(peace proposal)30 0.3457 0.3639 5.3%46 0.3483 0.4192 20.4% ~ ~(Claina nd Vietnam)47 0.5369 0.5847 8.9% /~ 1~ ~-~k~ tl.l (MountMinatubo),~U-~(ozone layer),~ ~(Subic)Table 9: Se~rnented Corpus with the Two Lexicons for Query 23Query 23 segment with small lexicon, bkQuery 23 segment with new lexiconAnother interesting example is query 30.
Thereis no new compound extracted from that query.
Itsresult is also improved significantly because itsrelevant documents are segmented better thanbefore.Because the compounds extracted from thecorpus are not exactly correct, the new lexiconwill bring negative ffect o some queries, such asquery 10.
The retrieval precision changes from0.3086 to 0.1359.
The main reason is that"~ \ [ \ ]~" (Ch inese  XinJiang) is taken as a newcompound in the query.4 Related worksSeveral methods have been proposed forextracting compounds from corpus by statisticalapproaches.
In this section, we will brieflydescribe some of them.
(Lee-Feng Chien 1997) proposed an approachbased on PAT-Tree to automatically extractingdomain specific terms from online textcollections.
Our method is primary derived from(Lee-Feng Chien 1997), and use the similarstatistical features, i.e.
mutual informan'on andcontext dependency.
The difference is that we usen-gram instead of PAT-Tree, due to the efficiencyissue.
Another difference lies in the experiments.In Chien's work, only domain specific terms areextracted from domain specific corpus, and thesize of the corpus is relatively small, namely1,872 political news abstracts.
(Cheng-Huang Tung and His-Jian Lee 1994)also presented an efficient method for identifyingunknown words from a large corpus.
Thestatistical features used consist of string (charactersequence) frequency and entropy of left/fightneighbonng characters (similar to left/fightcontext dependency).
The corpus consists of178,027 sentences, representing a total of morethan 2 million Chinese characters.
8327 unknownwords were identified and 5366 items of themwere confirmed manually.
(Ming-Wen Wu and Keh-Yih Su 1993)presented a method using mutual information andrelative frequency.
9,124 compounds are extractedfrom the corpus consists of 74,404 words, with theprecision of 47.43%.
In this method, thecompound extraction problem is formulated asclassification problem.
Each bi-grarn (tri-grarn) isassigned to one of those two clusters.
It also needsa training corpus to estimate parameters forclassification model.
In our method, we didn't138make use of any training corpus.
Anotherdifference is that they use the method for Englishcompounds extraction while we extract Chinesecompounds in our experiments.
(Pascale Fung 1998) presented two simplesystems for Chinese compound extraction----CXtract.
CXtract uses predominantly statisticallexical information to find term boundaries inlarge text.
Evaluations on the corpus consisting of2 million characters show that the averageprecision is 54.09%.We should note that since the experiment setupand evaluation systems of the methods mentionedabove are not identical, the results are notcomparable.
However, by showing ourexperimental results on much larger andheterogenous corpus, we can say that our methodis an efficient and robust one.5 ConclusionIn this paper, we investigate a statisticalapproach to Chinese compounds extraction fromvery large corpora using mutual information andcontext dependency.We explained how the performance can beinfluenced by different parameter settings, corpussize, and corpus heterogeneousness.
We alsorefine the lexicon with information retrievalsystem by adding compounds obtained by ourmethods, and achieve 1.2% improvements onprecision of IR.Through our experiments, we conclude thatstatistical method based on mutual informationand context dependency is efficient and robust forChinese compounds extraction.
And, mutualinformation mainly affects the precision whilecontext dependency mainly affects the count ofextracted items.ReferenceLee-Feng Chien, (1997) "PAT-tree-based keywordextraction for Chinese Information retrieval", ACMSIGIR'97, Philadelphia, USA, 50-58WU, Dekai and Xuanyin XIA.
(1995).
"Large-scaleautomatic extraction of an English-Chinese l xicon",Machine Translation 9(3-4), pp.285-313.Ming-Wen Wu and Keh-Yih Su.
(1993).
"Corpus-based Automatic Compound Extraction with MutualInformation and Relative Frequency Count,"Proceedings of R. 0.
C. Computational LinguisticsConference V I .
Nantou, Taiwan, R. O. C., pp.207-216.Pascale Fung.
(1998).
"Extracting Key Terms fromChinese and Japanese texts ".
The InternationalJournal on Computer Processing of OrientalLanguage, Special Issue on Information Retrieval onOriental Languages, pp.99-121.Cheng-Huang Tung and His-Jian Lee.
(1994).
"Identification of Unknown Words From a Corpus".Compouter Processing of Chinese and OrientalLanguages Vol.8, pp.
131 -145.Buckley, C. (1985).
Implementation f the SMARTinformation retrieval system, Technical report, #85-686, Cornell University.Harman, D. K. and Voorhees, E. M., Eds.
(1996).Information Technology: The Fifth Text RetrievalConference(TREC5), NIST SP 500-238.Gaithersburg, National Institute fo standards andTechnology.139
