Unsupervised WSD based on automatically retrieved examples:The importance of biasEneko AgirreIXA NLP GroupUniversity of the Basque CountryDonostia, Spaineneko@si.ehu.esDavid MartinezIXA NLP GroupUniversity of the Basque CountryDonostia, Spaindavidm@si.ehu.esAbstractThis paper explores the large-scale acquisition ofsense-tagged examples for Word Sense Disam-biguation (WSD).
We have applied the ?WordNetmonosemous relatives?
method to construct auto-matically a web corpus that we have used to traindisambiguation systems.
The corpus-building pro-cess has highlighted important factors, such as thedistribution of senses (bias).
The corpus has beenused to train WSD algorithms that include super-vised methods (combining automatic and manually-tagged examples), minimally supervised (requiringsense bias information from hand-tagged corpora),and fully unsupervised.
These methods were testedon the Senseval-2 lexical sample test set, and com-pared successfully to other systems with minimumor no supervision.1 IntroductionThe results of recent WSD exercises, e.g.
Senseval-21 (Edmonds and Cotton, 2001) show clearly thatWSD methods based on hand-tagged examples arethe ones performing best.
However, the main draw-back for supervised WSD is the knowledge acqui-sition bottleneck: the systems need large amountsof costly hand-tagged data.
The situation is moredramatic for lesser studied languages.
In order toovercome this problem, different research lines havebeen explored: automatic acquisition of training ex-amples (Mihalcea, 2002), bootstrapping techniques(Yarowsky, 1995), or active learning (Argamon-Engelson and Dagan, 1999).
In this work, we havefocused on the automatic acquisition of examples.When supervised systems have no specific train-ing examples for a target word, they need to rely onpublicly available all-words sense-tagged corporalike Semcor (Miller et al, 1993), which is taggedwith WordNet word senses.
The systems perform-ing best in the English all-words task in Senseval-2were basically supervised systems trained on Sem-cor.
Unfortunately, for most of the words, this cor-1http://www.senseval.org.pus only provides a handful of tagged examples.
Infact, only a few systems could overcome the MostFrequent Sense (MFS) baseline, which would tageach word with the sense occurring most frequentlyin Semcor.
In our approach, we will also rely onSemcor as the basic resource, both for training ex-amples and as an indicator of the distribution of thesenses of the target word.The goal of our experiment is to evaluate up towhich point we can automatically acquire examplesfor word senses and train accurate supervised WSDsystems on them.
This is a very promising line ofresearch, but one which remains relatively under-studied (cf.
Section 2).
The method we appliedis based on the monosemous relatives of the targetwords (Leacock et al, 1998), and we studied someparameters that affect the quality of the acquiredcorpus, such as the distribution of the number oftraining instances per each word sense (bias), andthe type of features used for disambiguation (localvs.
topical).Basically, we built three systems, one fully su-pervised (using examples from both Semcor and au-tomatically acquired examples), one minimally su-pervised (using the distribution of senses in Semcorand automatically acquired examples) and anotherfully unsupervised (using an automatically acquiredsense rank (McCarthy et al, 2004) and automati-cally acquired examples).This paper is structured as follows.
First, Section2 describes previous work on the field.
Section 3 in-troduces the experimental setting for evaluating theacquired corpus.
Section 4 is devoted to the processof building the corpus, which is evaluated in Section5.
Finally, the conclusions are given in Section 6.2 Previous workAs we have already mentioned, there is little workon this very promising area.
In (Leacock et al,1998), the method to obtain sense-tagged examplesusing monosemous relatives is presented.
In thiswork, they retrieve the same number of examplesper each sense, and they give preference to monose-mous relatives that consist in a multiword contain-ing the target word.
Their experiment is evaluatedon 3 words (a noun, a verb, and an adjective) withcoarse sense-granularity and few senses.
The resultsshowed that the monosemous corpus provided pre-cision comparable to hand-tagged data.In another related work, (Mihalcea, 2002) gener-ated a sense tagged corpus (GenCor) by using a setof seeds consisting of sense-tagged examples fromfour sources: SemCor, WordNet, examples createdusing the method above, and hand-tagged examplesfrom other sources (e.g., the Senseval-2 corpus).
Bymeans of an iterative process, the system obtainednew seeds from the retrieved examples.
An exper-iment in the lexical-sample task showed that themethod was useful for a subset of the Senseval-2testing words (results for 5 words are provided).3 Experimental Setting for EvaluationIn this section we will present the Decision Listmethod, the features used to represent the context,the two hand-tagged corpora used in the experimentand the word-set used for evaluation.3.1 Decision ListsThe learning method used to measure the quality ofthe corpus is Decision Lists (DL).
This algorithm isdescribed in (Yarowsky, 1994).
In this method, thesense skwith the highest weighted feature fiis se-lected, according to its log-likelihood (see Formula1).
For our implementation, we applied a simplesmoothing method: the cases where the denomina-tor is zero are smoothed by the constant 0.1 .weight(sk, fi) = log(Pr(sk|fi)?j =kPr(sj|fi)) (1)3.2 FeaturesIn order to represent the context, we used a basic setof features frequently used in the literature for WSDtasks (Agirre and Martinez, 2000).
We distinguishtwo types of features: Local features: Bigrams and trigrams, formedby the word-form, lemma, and part-of-speech2of the surrounding words.
Also the contentlemmas in a ?4 word window around the tar-get. Topical features: All the content lemmas in thecontext.2The PoS tagging was performed using TnT (Brants, 2000)We have analyzed the results using local and top-ical features separately, and also using both typestogether (combination).3.3 Hand-tagged corporaSemcor was used as training data for our supervisedsystem.
This corpus offers tagged examples formany words, and has been widely used for WSD.It was necessary to use an automatic mapping be-tween the WordNet 1.6 senses in Semcor and theWordNet 1.7 senses in testing (Daude et al, 2000).For evaluation, the test part of the Senseval-2 En-glish lexical-sample task was chosen.
The advan-tage of this corpus was that we could focus on aword-set with enough examples for testing.
Be-sides, it is a different corpus, so the evaluation ismore realistic than that made using cross-validation.The test examples whose senses were multiwordsor phrasal verbs were removed, because they can beefficiently detected with other methods in a prepro-cess.It is important to note that the training part ofSenseval-2 lexical-sample was not used in the con-struction of the systems, as our goal was to testthe performance we could achieve with minimal re-sources (i.e.
those available for any word).
We onlyrelied on the Senseval-2 training bias in preliminaryexperiments on local/topical features (cf.
Table 4),and to serve as a reference for unsupervised perfor-mance (cf.
Table 5).3.4 Word-setThe experiments were performed on the 29 nounsavailable for the Senseval-2 lexical-sample task.
Weseparated these nouns in 2 sets, depending on thenumber of examples they have in Semcor: Set Acontained the 16 nouns with more than 10 examplesin Semcor, and Set B the remaining low-frequencywords.4 Building the monosemous relatives webcorpusIn order to build this corpus3, we have acquired1000 Google snippets for each monosemous wordin WordNet 1.7.
Then, for each word sense of theambiguous words, we gathered the examples of itsmonosemous relatives (see below).
This method isinspired in (Leacock et al, 1998), and has shown tobe effective in experiments of topic signature acqui-sition (Agirre and Lopez, 2004).
This last paper alsoshows that it is possible to gather examples based on3The automatically acquired corpus will be referred indis-tinctly as web-corpus, or monosemous-corpusmonosemous relatives for nearly all noun senses inWordNet4.The basic assumption is that for a given wordsense of the target word, if we had a monosemoussynonym of the word sense, then the examples ofthe synonym should be very similar to the targetword sense, and could therefore be used to train aclassifier of the target word sense.
The same, butin a lesser extent, can be applied to other monose-mous relatives, such as direct hyponyms, direct hy-pernyms, siblings, indirect hyponyms, etc.
The ex-pected reliability decreases with the distance in thehierarchy from the monosemous relative to the tar-get word sense.The monosemous-corpus was built using the sim-plest technique: we collected examples from theweb for each of the monosemous relatives.
The rel-atives have an associated number (type), which cor-relates roughly with the distance to the target word,and indicates their relevance: the higher the type,the less reliable the relative.
A sample of monose-mous relatives for different senses of church, to-gether with its sense inventory in WordNet 1.7 isshown in Figure 1.Distant hyponyms receive a type number equalto the distance to the target sense.
Note that weassigned a higher type value to direct hypernymsthan to direct hyponyms, as the latter are more use-ful for disambiguation.
We also decided to includesiblings, but with a high type value (3).In the following subsections we will describe stepby step the method to construct the corpus.
First wewill explain the acquisition of the highest possibleamount of examples per sense; then we will explaindifferent ways to limit the number of examples persense for a better performance; finally we will seethe effect of training on local or topical features onthis kind of corpora.4.1 Collecting the examplesThe examples are collected following these steps1: We query Google5 with the monosemous rel-atives for each sense, and we extract the snippets asreturned by the search engine.
All snippets returnedby Google are used (up to 1000).
The list of snippetsis sorted in reverse order.
This is done because thetop hits usually are titles and incomplete sentencesthat are not so useful.2: We extract the sentences (or fragments of sen-tences) around the target search term.
Some of the4All the examples in this work are publicly available inhttp://ixa2.si.ehu.es/pub/sensecorpus5We use the offline XML interface kindly provided byGoogle for research.Sense 0 1 2 3 Total Semcorchurch#1 0 476 524 0 1000 60church#2 306 100 561 0 967 58church#3 147 0 20 0 167 10Overall 453 576 1105 0 2134 128Table 1: Examples per type (0,1,...) that are ac-quired from the web for the three senses of churchfollowing the Semcor bias, and total examples inSemcor.sentences are discarded, according to the followingcriteria: length shorter than 6 words, having morenon-alphanumeric characters than words divided bytwo, or having more words in uppercase than in low-ercase.3: The automatically acquired examples containa monosemous relative of the target word.
In or-der to use these examples to train the classifiers,the monosemous relative (which can be a multi-word term) is substituted by the target word.
Inthe case of the monosemous relative being a mul-tiword that contains the target word (e.g.
ProtestantChurch for church) we can choose not to substitute,because Protestant, for instance, can be a useful fea-ture for the first sense of church.
In these cases, wedecided not to substitute and keep the original sen-tence, as our preliminary experiments on this corpussuggested (although the differences were not signif-icant).4: For a given word sense, we collect the desirednumber of examples (see following section) in or-der of type: we first retrieve all examples of type0, then type 1, etc.
up to type 3 until the necessaryexamples are obtained.
We did not collect exam-ples from type 4 upwards.
We did not make anydistinctions between the relatives from each type.
(Leacock et al, 1998) give preference to multiwordrelatives containing the target word, which could bean improvement in future work.On average, we have acquired roughly 24,000 ex-amples for each of the target words used in this ex-periment.4.2 Number of examples per sense (bias)Previous work (Agirre and Martinez, 2000) has re-ported that the distribution of the number of exam-ples per word sense (bias for short) has a stronginfluence in the quality of the results.
That is, theresults degrade significantly whenever the trainingand testing samples have different distributions ofthe senses.As we are extracting examples automatically, wehave to decide how many examples we will use forSense 1church, Christian church, Christianity -- (a group of Christians; any group professingChristian doctrine or belief)Sense 2church, church building -- (a place for public (especially Christian) worship)Sense 3church service, church -- (a service conducted in a church)Monosemous relatives for different senses of churchSynonyms (Type 0): church building (sense 2), church service (sense 3) ...Direct hyponyms (Type 1): Protestant Church (sense 1), Coptic Church (sense 1) ...Direct hypernyms (Type 2): house of prayer (sense 2), religious service (sense 3) ...Distant hyponyms (Type 2,3,4...): Greek Church (sense 1), Western Church (sense 1)...Siblings (Type 3): Hebraism (sense 2), synagogue (sense 2) ...Figure 1: Sense inventory and some monosemous relatives in WordNet 1.7 for church.Web corpusSenseSemcor Web bias Semcor Pr Semcor MR Automatic MR Senseval test# ex % # ex % # ex % # ex % # ex % # ex %authority#1 18 60 338 0.5 338 33.7 324 59.9 138 19.3 37 37.4authority#2 5 16.7 44932 66.4 277 27.6 90 16.6 75 10.5 17 17.2authority#3 3 10 10798 16 166 16.6 54 10.0 93 13.0 1 1.0authority#4 2 6.7 886 1.3 111 11.1 36 6.7 67 9.4 0 0authority#5 1 3.3 6526 9.6 55 5.5 18 3.3 205 28.6 34 34.3authority#6 1 3.3 71 0.1 55 5.5 18 3.3 71 9.9 10 10.1authority#7 0 0 4106 6.1 1 0.1 1 0.2 67 9.4 0 0Overall 30 100 67657 100 1003 100 541 100 716 100 99 100Table 2: Distribution of examples for the senses of authority in different corpora.
Pr (proportional) and MR(minimum ratio) columns correspond to different ways to apply Semcor bias.each sense.
In order to test the impact of bias, dif-ferent settings have been tried: No bias: we take an equal amount of examplesfor each sense. Web bias: we take all examples gathered fromthe web. Automatic ranking: the number of examplesis given by a ranking obtained following themethod described in (McCarthy et al, 2004).They used a thesaurus automatically createdfrom the BNC corpus with the method from(Lin, 1998), coupled with WordNet-based sim-ilarity measures. Semcor bias: we take a number of examplesproportional to the bias of the word senses inSemcor.For example, Table 1 shows the number of exam-ples per type (0,1,...) that are acquired for churchfollowing the Semcor bias.
The last column givesthe number of examples in Semcor.We have to note that the 3 first methods do notrequire any hand-labeled data, and that the fourthrelies in Semcor.The way to apply the bias is not straightforwardin some cases.
In our first approach for Semcor-bias, we assigned 1,000 examples to the major sensein Semcor, and gave the other senses their propor-tion of examples (when available).
But in somecases the distribution of the Semcor bias and thatof the actual examples in the web would not fit.
Theproblem is caused when there are not enough exam-ples in the web to fill the expectations of a certainword sense.We therefore tried another distribution.
We com-puted, for each word, the minimum ratio of exam-ples that were available for a given target bias and agiven number of examples extracted from the web.We observed that this last approach would reflectbetter the original bias, at the cost of having less ex-amples.Table 2 presents the different distributions ofexamples for authority.
There we can see theSenseval-testing and Semcor distributions, togetherwith the total number of examples in the web; theSemcor proportional distribution (Pr) and minimumratio (MR); and the automatic distribution.
Thetable illustrates how the proportional Semcor biasproduces a corpus where the percentage of some ofWord Web bias Semcor bias Automatic biasart 15,387 10,656 2,610authority 67,657 541 716bar 50,925 16,627 5,329bum 17,244 2,555 4,745chair 24,625 8,512 2,111channel 31,582 3,235 10,015child 47,619 3,504 791church 8,704 5,376 6,355circuit 21,977 3,588 5,095day 84,448 9,690 3,660detention 2,650 1,510 511dyke 4,210 1,367 843facility 11,049 8,578 1,196fatigue 6,237 3,438 5,477feeling 9,601 1,160 945grip 20,874 2,209 277hearth 6,682 1,531 2,730holiday 16,714 1,248 1,846lady 12,161 2,959 884material 100,109 7,855 6,385mouth 648 287 464nation 608 594 608nature 32,553 24,746 9,813post 34,968 4,264 8,005restraint 33,055 2,152 2,877sense 10,315 2,059 2,176spade 5,361 2,458 2,657stress 10,356 2,175 3,081yew 10,767 2,000 8,013Average 24,137 4,719 3,455Total 699,086 136,874 100,215Table 3: Number of examples following differentsense distributions.
Minimum-ratio is applied forthe Semcor and automatic bias.the senses is different from that in Semcor, e.g.
thefirst sense only gets 33.7% of the examples, in con-trast to the 60% it had in Semcor.We can also see how the distributions of sensesin Semcor and Senseval-test have important differ-ences, although the main sense is the same.
For theweb and automatic distributions, the first sense isdifferent; and in the case of the web distribution, thefirst hand-tagged sense only accounts for 0.5% ofthe examples retrieved from the web.
Similar distri-bution discrepancies can be observed for most of thewords in the test set.
The Semcor MR column showshow using minimum ratio we get a better reflectionof the proportion of examples in Semcor, comparedto the simpler proportional approach (Semcor Pr) .For the automatic bias we only used the minimumratio.To conclude this section, Table 3 shows the num-ber of examples acquired automatically followingthe web bias, the Semcor bias with minimum ratio,and the Automatic bias with minimum ratio.4.3 Local vs. topical featuresPrevious work on automatic acquisition of examples(Leacock et al, 1998) has reported lower perfor-mance when using local collocations formed by PoStags or closed-class words.
We performed an earlyexperiment comparing the results using local fea-tures, topical features, and a combination of both.In this case we used the web corpus with Sensevaltraining bias, distributed according to the MR ap-proach, and always substituting the target word.
Therecall (per word and overall) is given in Table 4.In this setting, we observed that local collocationsachieved the best precision overall, but the combina-tion of all features obtained the best recall.
The tabledoes not show the precision/coverage figures due tospace constraints, but local features achieve 58.5%precision for 96.7% coverage overall, while topicaland combination of features have full-coverage.There were clear differences in the results perword, showing that estimating the best feature-setper word would improve the performance.
For thecorpus-evaluation experiments, we chose to workwith the combination of all features.5 EvaluationIn all experiments, the recall of the systems is pre-sented as evaluation measure.
There is total cover-age (because of the high overlap of topical features)and the recall and precision are the same6.In order to evaluate the acquired corpus, our firsttask was to analyze the impact of bias.
The resultsare shown in Table 5.
There are 2 figures for eachdistribution: (1) simply assign the first ranked sense,and (2) use the monosemous corpus following thepredetermined bias.
As we described in Section 3,the testing part of the Senseval-2 lexical sample datawas used for evaluation.
We also include the resultsusing Senseval2 bias, which is taken from the train-ing part.
The recall per word for some distributionscan be seen in Table 4.The results show clearly that when bias informa-tion from a hand-tagged corpora is used the recallimproves significantly, even when the bias comesfrom a corpus -Semcor- different from the targetcorpus -Senseval-.
The bias is useful by itself, andwe see that the higher the performance of the 1stranked sense heuristic, the lower the gain using themonosemous corpus.
We want to note that in fullyunsupervised mode we attain a recall of 43.2% withthe automatic ranking.
Using the minimally su-pervised information of bias, we get 49.8% if wehave the bias from an external corpus (Semcor) and6Except for the experiment in Section 4.3, where using localfeatures the coverage is only partial.Senseval bias Semcor Autom.Word Loc.
Top.
Comb.
bias biasart 54.2 45.6 47.0 55.6 45.6authority 47.8 43.2 46.2 41.8 40.0bar 52.1 55.9 57.2 51.6 26.4bum 81.2 87.5 85.0 5.0 57.5chair 88.7 88.7 88.7 88.7 69.4channel 39.7 53.7 55.9 16.2 30.9child 56.5 55.6 56.5 54.0 34.7church 67.7 51.6 54.8 48.4 49.7circuit 45.3 54.2 56.1 41.5 49.1day 59.4 54.7 56.8 48.0 12.5detention 87.5 87.5 87.5 52.1 87.5dyke 89.3 89.3 89.3 92.9 80.4facility 28.6 21.4 21.4 26.8 22.0fatigue 82.5 82.5 82.5 82.5 75.0feeling 55.1 60.2 60.2 60.2 42.5grip 19.0 38.0 39.0 16.0 28.2hearth 73.4 75.0 75.0 75.0 60.4holiday 96.3 96.3 96.3 96.3 72.2lady 80.4 73.9 73.9 80.4 23.9material 43.2 44.2 43.8 54.2 52.3mouth 36.8 38.6 39.5 54.4 46.5nation 80.6 80.6 80.6 80.6 80.6nature 44.4 39.3 40.7 46.7 34.1post 43.9 40.5 40.5 34.2 47.4restraint 29.5 37.5 37.1 27.3 31.4sense 58.1 37.2 38.4 47.7 41.9spade 74.2 72.6 74.2 67.7 85.5stress 53.9 46.1 48.7 2.6 27.6yew 81.5 81.5 81.5 66.7 77.8Overall 56.5 56.0 57.0 49.8 43.2Table 4: Recall for all the nouns using the monose-mous corpus with Senseval-2 training bias (MR, andsubstitution), Semcor bias, and Automatic bias.
TheSenseval-2 results are given by feature type.57.5% if we have access to the bias of the targetcorpus (Senseval7).
This results show clearly thatthe acquired corpus has useful information about theword senses, and that bias is extremely important.We will present two further experiments per-formed with the monosemous corpus resource.
Thegoal of the first will be to measure the WSD per-formance that we achieve using Semcor as the onlysupervised data source.
In our second experiment,we will compare the performance of our totally un-supervised approach (monosemous corpus and au-tomatic bias) with other unsupervised approaches inthe Senseval-2 English lexical task.5.1 Monosemous corpus and Semcor biasIn this experiment we compared the performanceusing the monosemous corpus (with Semcor biasand minimum ratio), and the examples from Sem-cor.
We noted that there were clear differencesdepending on the number of training examples for7Bias obtained from the training-set.each word, therefore we studied each word-set de-scribed in Section 3.4 separately.
The results perword-set are shown in Table 6.
The figures cor-respond to the recall training in Semcor, the web-corpus, and the combination of both.If we focus on set B (words with less than 10 ex-amples in Semcor), we see that the MFS figure isvery low (40.1%).
There are some words that do nothave any occurrence in Semcor, and thus the senseis chosen at random.
It made no sense to train theDL for this set, therefore this result is not in the ta-ble.
For this set, the bias information from Semcoris also scarce, but the DLs trained on the web-corpusraise the performance to 47.8%.For set A, the average number of examples ishigher, and this raises the results for Semcor MFS(51.9%).
We see that the recall for DL trainingin Semcor is lower that the MFS baseline (50.5%).The main reasons for these low results are the dif-ferences between the training and testing corpora(Semcor and Senseval).
There have been previousworks on portability of hand-tagged corpora thatshow how some constraints, like the genre or topicof the corpus, affect heavily the results (Martinezand Agirre, 2000).
If we train on the web-corpusthe results improve, and the best results are ob-tained with the combination of both corpora, reach-ing 51.6%.
We need to note, however, that this isstill lower than the Semcor MFS.Finally, we will examine the results for the wholeset of nouns in the Senseval-2 lexical-sample (lastrow in Table 6), where we see that the best approachrelies on the web-corpus.
In order to disambiguatethe 29 nouns using only Semcor, we apply MFSwhen there are less than 10 examples (set B), andtrain the DLs for the rest.The results in Table 6 show that the web-corpusraises recall, and the best results are obtained com-bining the Semcor data and the web examples(50.3%).
As we noted, the web-corpus is speciallyuseful when there are few examples in Semcor (setB), therefore we made another test, using the web-corpus only for set B, and applying MFS for set A.The recall was slightly better (50.5%), as is shownin the last column.5.2 Monosemous corpus and Automatic bias(unsupervised method)In this experiment we compared the performanceof our unsupervised system with other approaches.For this goal, we used the resources available fromthe Senseval-2 competition8, where the answers ofthe participating systems in the different tasks were8http://www.senseval.org.Bias Type 1stsenseTrainexam.
Diff.no bias 18.3 38.0 +19.7web bias unsuperv.
33.3 39.8 +6.5autom.
ranking 36.1 43.2 +7.1Semcor bias minimally- 47.8 49.8 +2.0Senseval2 bias supervised 55.6 57.5 +1.9Table 5: Performance (recall) on Senseval-2 lexical-sample, using different bias to create the corpus.The type column shows the kind of system.Word-set MFS Semcor Web Semcor+ WebMFS &Webset A (> 10) 51.9 50.5 50.9 51.6 51.9set B (< 10) 40.1 - 47.7 47.8 47.8all words 47.8 47.4 49.8 50.3 50.5Table 6: Recall training in Semcor, the acquiredweb corpus (Semcor bias), and a combination ofboth, compared to that of the Semcor MFS.available.
This made possible to compare our re-sults and those of other systems deemed unsuper-vised by the organizers on the same test data and setof nouns.From the 5 unsupervised systems presented inthe Senseval-2 lexical-sample task as unsupervised,the WASP-Bench system relied on lexicographersto hand-code information semi-automatically (Tug-well and Kilgarriff, 2001).
This system does notuse the training data, but as it uses manually codedknowledge we think it falls clearly in the supervisedcategory.The results for the other 4 systems and our ownare shown in Table 7.
We show the results for thetotally unsupervised system and the minimally un-supervised system (Semcor bias).
We classified theUNED system (Fernandez-Amoros et al, 2001) asminimally supervised.
It does not use hand-taggedexamples for training, but some of the heuristics thatare applied by the system rely on the bias informa-tion available in Semcor.
The distribution of sensesis used to discard low-frequency senses, and also tochoose the first sense as a back-off strategy.
On thesame conditions, our minimally supervised systemattains 49.8 recall, nearly 5 points more.The rest of the systems are fully unsupervised,and they perform significantly worse than our sys-tem.6 Conclusions and Future WorkThis paper explores the large-scale acquisition ofsense-tagged examples for WSD, which is a veryMethod Type RecallWeb corpus (Semcor bias) minimally- 49.8UNED supervised 45.1Web corpus (Autom.
bias) 43.3Kenneth Litkowski-clr-ls unsupervised 35.8Haynes-IIT2 27.9Haynes-IIT1 26.4Table 7: Our minimally supervised and fully unsu-pervised systems compared to the unsupervised sys-tems (marked in bold) in the 29 noun subset of theSenseval-2 Lexical Sample.promising line of research, but remains relativelyunder-studied.
We have applied the ?monosemousrelatives?
method to construct automatically a webcorpus which we have used to train three systemsbased on Decision Lists: one fully supervised (ap-plying examples from Semcor and the web corpus),one minimally supervised (relying on the distribu-tion of senses in Semcor and the web corpus) andanother fully unsupervised (using an automaticallyacquired sense rank and the web corpus).
Thosesystems were tested on the Senseval-2 lexical sam-ple test set.We have shown that the fully supervised systemcombining our web corpus with the examples inSemcor improves over the same system trained onSemcor alone.
This improvement is specially no-ticeable in the nouns that have less than 10 examplesin Semcor.
Regarding the minimally supervisedand fully unsupervised systems, we have shownthat they perform well better than the other systemsof the same category presented in the Senseval-2lexical-sample competition.The system can be trained for all nounsin WordNet, using the data available athttp://ixa2.si.ehu.es/pub/sensecorpus.The research also highlights the importance ofbias.
Knowing how many examples are to be fedinto the machine learning system is a key issue.
Wehave explored several possibilities, and shown thatthe learning system (DL) is able to learn from theweb corpus in all the cases, beating the respectiveheuristic for sense distribution.We think that this research opens the opportu-nity for further improvements.
We have to note thatthe MFS heuristic and the supervised systems basedon the Senseval-2 training data are well ahead ofour results, and our research aims at investigatingideas to close this gap.
Some experiments on theline of adding automatically retrieved examples toavailable hand-tagged data (Semcor and Senseval-2) have been explored.
The preliminary results indi-cate that this process has to be performed carefully,taking into account the bias of the senses and apply-ing a quality-check of the examples before they areincluded in the training data.For the future we also want to test the perfor-mance of more powerful Machine Learning meth-ods, explore feature selection methods for each in-dividual word, and more sophisticated ways to com-bine the examples from the web corpus with thoseof Semcor or Senseval.
Now that the monosemouscorpus is available for all nouns, we would also liketo test the system on the all-words task.
In addition,we will give preference to multiwords that containthe target word when choosing the relatives.
Finally,more sophisticated methods to acquire examples arenow available, like ExRetriever (Fernandez et al,2004), and they could open the way to better exam-ples and performance.7 AcknowledgmentsWe wish to thank Diana McCarthy, from the Univer-sity of Sussex, for providing us the sense rank forthe target nouns.
This research has been partiallyfunded by the European Commission (MEANINGIST-2001-34460).ReferencesE.
Agirre and O. Lopez.
2004.
Publicly availabletopic signatures for all wordnet nominal senses.In Proceedings of the 4rd International Con-ference on Language Resources and Evaluation(LREC), Lisbon, Portugal.E.
Agirre and D. Martinez.
2000.
Exploring auto-matic word sense disambiguation with decisionlists and the web.
In Procedings of the COLING2000 Workshop on Semantic Annotation and In-telligent Content, Luxembourg.S.
Argamon-Engelson and I. Dagan.
1999.Committee-based sample selection for proba-bilistic classifiers.
In Journal of Artificial Intel-ligence Research, volume 11, pages 335?360.T.
Brants.
2000.
Tnt - a statistical part-of-speechtagger.
In Proceedings of the Sixth Applied Nat-ural Language Processing Conference, Seattle,WA.J.
Daude, L. Padro, and G. Rigau.
2000.
Mappingwordnets using structural information.
In 38thAnual Meeting of the Association for Computa-tional Linguistics (ACL?2000), Hong Kong.P.
Edmonds and S. Cotton.
2001.
Senseval-2:Overview.
In Proceedings of the Second Interna-tional Workshop on evaluating Word Sense Dis-ambiguation Systems, Toulouse, France.D.
Fernandez-Amoros, J. Gonzalo, and F. Verdejo.2001.
The uned systems at senseval-2.
In Pro-ceedings of the SENSEVAL-2 Workshop.
In con-junction with ACL, Toulouse, France.J.
Fernandez, M. Castillo, G. Rigau, J. Atserias, andJ.
Turmo.
2004.
Automatic acquisition of senseexamples using exretriever.
In Proceedings of the4rd International Conference on Language Re-sources and Evaluation (LREC), Lisbon, Portu-gal.C.
Leacock, M. Chodorow, and G. A. Miller.
1998.Using corpus statistics and WordNet relations forsense identification.
In Computational Linguis-tics, volume 24, pages 147?165.D.
Lin.
1998.
Automatic retrieval and clusteringof similar words.
In In Proceedings of COLING-ACL, Montreal, Canada.D.
Martinez and E. Agirre.
2000.
One sense percollocation and genre/topic variations.
In Pro-ceedings of the Joint SIGDAT Conference on Em-pirical Methods in Natural Language Processingand Very Large Corpora, Hong Kong.D.
McCarthy, R. Koeling, J. Weeds, and J. Car-roll.
2004.
Finding predominant word senses inuntagged text.
In Proceedings of the 42nd An-nual Meeting of the Association for Computa-tional Linguistics (ACL) (to appear), Barcelona,Spain.R.
Mihalcea.
2002.
Bootstrapping large sensetagged corpora.
In Proceedings of the 3rd Inter-national Conference on Language Resources andEvaluation (LREC), Las Palmas, Spain.G.
A. Miller, C. Leacock, R. Tengi, and R. Bunker.1993.
A semantic concordance.
In Proceedingsof the ARPA Human Language Technology Work-shop, pages 303?308, Princeton, NJ.D.
Tugwell and A. Kilgarriff.
2001.
Wasp-bench:a lexicographic tool supporting word sense dis-ambiguation.
In Proceedings of the SENSEVAL-2Workshop.
In conjunction with ACL-2001/EACL-2001, Toulouse, France.D.
Yarowsky.
1994.
Decision lists for lexical am-biguity resolution: Application to accent restora-tion in spanish and french.
In Proceedings of the32nd Annual Meeting of the Association for Com-putational Linguistics, Las Cruces, NM.D.
Yarowsky.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.
InProceedings of the 33rd Annual Meeting ofthe Association for Computational Linguistics(ACL), Cambridge, MA.
