Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 787?794, Vancouver, October 2005. c?2005 Association for Computational LinguisticsComparing and Combining Finite-State and Context-Free ParsersKristy Hollingshead and Seeger Fisher and Brian RoarkCenter for Spoken Language UnderstandingOGI School of Science & EngineeringOregon Health & Science UniversityBeaverton, Oregon, 97006{hollingk,fishers,roark}@cslu.ogi.eduAbstractIn this paper, we look at comparing high-accuracy context-free parsers with high-accuracy finite-state (shallow) parsers onseveral shallow parsing tasks.
Weshow that previously reported compar-isons greatly under-estimated the perfor-mance of context-free parsers for thesetasks.
We also demonstrate that context-free parsers can train effectively on rel-atively little training data, and are morerobust to domain shift for shallow pars-ing tasks than has been previously re-ported.
Finally, we establish that combin-ing the output of context-free and finite-state parsers gives much higher resultsthan the previous-best published results,on several common tasks.
While theefficiency benefit of finite-state modelsis inarguable, the results presented hereshow that the corresponding cost in accu-racy is higher than previously thought.1 IntroductionFinite-state parsing (also called chunking or shallowparsing) has typically been motivated as a fast first-pass for ?
or approximation to ?
more expensivecontext-free parsing (Abney, 1991; Ramshaw andMarcus, 1995; Abney, 1996).
For many very-large-scale natural language processing tasks (e.g.
open-domain question answering from the web), context-free parsing may be too expensive, whereas finite-state parsing is many orders of magnitude faster andcan also provide very useful syntactic annotationsfor large amounts of text.
For this reason, finite-stateparsing (hereafter referred to as shallow parsing) hasreceived increasing attention in recent years.In addition to the clear efficiency benefit ofshallow parsing, Li and Roth (2001) have furtherclaimed both an accuracy and a robustness benefitversus context-free parsing.
The output of a context-free parser, such as that of Collins (1997) or Char-niak (2000), can be transformed into a sequence ofshallow constituents for comparison with the outputof a shallow parser.
Li and Roth demonstrated thattheir shallow parser, trained to label shallow con-stituents along the lines of the well-known CoNLL-2000 task (Sang and Buchholz, 2000), outperformedthe Collins parser in correctly identifying these con-stituents in the Penn Wall Street Journal (WSJ) Tree-bank (Marcus et al, 1993).
They argued that theirsuperior performance was due to optimizing directlyfor the local sequence labeling objective, rather thanfor obtaining a hierarchical analysis over the entirestring.
They further showed that their shallow parsertrained on the Penn WSJ Treebank did a far betterjob of annotating out-of-domain sentences (e.g.
con-versational speech) than the Collins parser.This paper re-examines the comparison of shal-low parsers with context-free parsers, beginningwith a critical examination of how their outputsare compared.
We demonstrate that changes to theconversion routine, which take into account differ-ences between the original treebank trees and thetrees output by context-free parsers, eliminate thepreviously-reported accuracy differences.
Second,we show that a convention that is widely acceptedfor evaluation of context-free parses ?
ignoringpunctuation when setting the span of a constituent ?results in improved shallow parsing performance bycertain context-free parsers across a variety of shal-low parsing tasks.
We also demonstrate that context-free parsers perform competitively when applied toout-of-domain data.
Finally, we show that large im-provements can be obtained in several shallow pars-ing tasks by using simple strategies to incorporatecontext-free parser output into shallow parsing mod-els.
Our results demonstrate that a rich context-free787parsing model is, time permitting, worth applying,even if only shallow parsing output is needed.
Inaddition, our best results, which greatly improve onthe previous-best published results on several tasks,shed light on how much accuracy is sacrificed inshallow parsing to get finite-state efficiency.2 Evaluating Heterogeneous Parser OutputTwo commonly reported shallow parsing tasks areNoun-Phrase (NP) Chunking (Ramshaw and Mar-cus, 1995) and the CoNLL-2000 Chunking task(Sang and Buchholz, 2000), which extends the NP-Chunking task to recognition of 11 phrase types1annotated in the Penn Treebank.
Reference shal-low parses for this latter task were derived fromtreebank trees via a conversion script known aschunklink2.
We follow Li and Roth (2001) inusing chunklink to also convert trees output by acontext-free parser into a flat representation of shal-low constituents.
Figure 1(a) shows a Penn Tree-bank tree and Figure 1(c) its corresponding shallowparse constituents, according to the CoNLL-2000guidelines.
Note that consecutive verb phrase (VP)nodes result in a single VP shallow constituent.Just as the original treebank trees are convertedfor training shallow parsers, they are also typ-ically modified for training context-free parsers.This modification includes removal of empty nodes(nodes tagged with ?-NONE-?
in the treebank), andremoval of function tags on non-terminals; e.g., NP-SBJ (subject NP) and NP-TMP (temporal NP) areboth mapped to NP.
The output of the context-freeparser is, of course, in the same format as the train-ing input, so empty nodes and function tags are notpresent.
This type of modified tree is what is shownin Figure 1(b); note that the original treebank tree,shown in Figure 1(a), had an empty subject NP inthe embedded clause which has been removed forthe modified tree.To compare the output of their shallow parser withthe output of the well-known Collins (1997) parser,Li and Roth applied the chunklink conversionscript to extract the shallow constituents from theoutput of the Collins parser on WSJ section 00.
Un-1These include: ADJP, ADVP, CONJP, INTJ, LST, NP, PP,PRT, SBAR, UCP and VP.
Anything not in one of these basephrases is designated as ?outside?.2Downloaded from http://ilk.kub.nl/?sabine/chunklink/.
(a) SHHHNP-SBJ-1TheyVP HHare VP HHstarting SHHHNP-SBJ-NONE-*-1VPHHHto VPHHHbuy NP HHgrowth stocks(b) SHHHNPTheyVPHHHare VPHHHstarting SVPHHHto VPHHHbuy NP HHgrowth stocks(c) [NP They] [VP are starting to buy] [NP growth stocks]Figure 1: (a) Penn WSJ treebank tree, (b) modified treebanktree, and (c) CoNLL-2000 style shallow bracketing, all of thesame string.fortunately, the script was built to be applied to theoriginal treebank trees, complete with empty nodes,which are not present in the output of the Collinsparser, or any well-known context-free parser.
Thechunklink script searches for empty nodes in theparse tree to perform some of its operations.
In par-ticular, any S node that contains an empty subjectNP and a VP is reduced to just a VP node, andthen combined with any immediately-preceding VPnodes to create a single VP constituent.
If the Snode does not contain an empty subject NP, as inFigure 1(b), the chunklink script creates two VPconstituents: [VP are starting] [VP to buy], whichin this case results in a bracketing error.
However,it is a simple matter to insert an empty subject NPinto unary S?VP productions so that these nodesare processed correctly by the script.Various conventions have become standard inevaluating parser output over the past decade.
Per-haps the most widely accepted convention is thatof ignoring punctuation for the purposes of assign-ing constituent span, under the perspective that, fun-788Phrase Evaluation ScenarioSystem Type (a) (b) (c)?Modified?
All 98.37 99.72 99.72Truth VP 92.14 98.70 98.70Li and Roth All 94.64 - -(2001) VP 95.28 - -Collins (1997) All 92.16 93.42 94.28VP 88.15 94.31 94.42Charniak All 93.88 95.15 95.32(2000) VP 88.92 95.11 95.19Table 1: F-measure shallow bracketing accuracy under threedifferent evaluation scenarios: (a) baseline, used in Li and Roth(2001), with original chunklink script converting treebanktrees and context-free parser output; (b) same as (a), except thatempty subject NPs are inserted into every unary S?VP produc-tion; and (c) same as (b), except that punctuation is ignored forsetting constituent span.
Results for Li and Roth are reportedfrom their paper.
The Collins parser is provided with part-of-speech tags output by the Brill tagger (Brill, 1995).damentally, constituents are groupings of words.Interestingly, this convention was not followed inthe CoNLL-2000 task (Sang and Buchholz, 2000),which as we will see has a variable effect on context-free parsers, presumably depending on the degree towhich punctuation is moved in training.2.1 Evaluation AnalysisTo determine the effects of the conversion routineand different evaluation conventions, we comparethe performance of several different models on oneof the tasks presented in Li and Roth (2001).
Forthis task, which we label the Li & Roth task, sec-tions 2-21 of the Penn WSJ Treebank are used astraining data, section 24 is held out, and section 00is for evaluation.For all trials in this paper, we report F-measurelabeled bracketing accuracy, which is the harmonicmean of the labeled precision (P ) and labeled recall(R), as they are defined in the widely used PARSE-VAL metrics; i.e.
the F-measure accuracy is 2PRP+R .Table 1 shows baseline results for the Li andRoth3 shallow parser, two well-known, high-accuracy context-free parsers, and the reference(true) parses after being modified as described3We were unable to obtain the exact model used in Li andRoth (2001), and so we use their reported results here.
Notethat they used reference part-of-speech (POS) tags for their re-sults on this task.
All other results reported in this paper, unlessotherwise noted, were obtained using Brill-tagger POS tags.above (by removing empty nodes and functiontags).
Evaluation scenario (a) in Table 1 corre-sponds to what was used in Li and Roth (2001) fol-lowing CoNLL-2000 guidelines, with the originalchunklink script used to transform the context-free parser output into shallow constituents.
Wecan see from the performance of the modified truthin this scenario that there are serious problemswith this conversion, due to the way in whichit handles unary S?VP productions.
If we de-terministically insert empty subject NP nodes forall such unary productions prior to the use of thechunklink script, which we do in evaluation sce-nario (b) of Table 1, this repairs the bulk of theerrors.
Some small number of errors remain, duelargely to the fact that if the S node has been an-notated with a function tag (e.g.
S-PRP, S-PRD, S-CLR), then chunklink will not perform its re-duction operation on that node.
However, for ourpurposes, this insertion repair sufficiently correctsthe error to perform meaningful comparisons.
Fi-nally, evaluation scenario (c) follows the context-free parsing evaluation convention of ignoring punc-tuation when assigning constituent span.
This af-fects some parsers more than others, depending onhow the parser treats punctuation internally; forexample, Bikel (2004) documents that the Collinsparser raises punctuation nodes within the parsetree.
Since ignoring punctuation cannot hurt perfor-mance, only improve it, even the smallest of thesedifferences are statistically significant.Note that after inserting empty nodes and ignor-ing punctuation, the accuracy advantage of Li andRoth over Collins is reduced to a dead heat.
Ofthe two parsers we evaluated, the Charniak (2000)parser gave the best performance, which is consis-tent with its higher reported performance on thecontext-free parsing task versus other context-freeparsers.
Collins (2000) reported a reranking modelthat improved his parser output to roughly the samelevel of accuracy as Charniak (2000), and Charniakand Johnson (2005) report an improvement usingreranking over Charniak (2000).
For the purposesof this paper, we needed an available parser thatwas (a) trainable on different subsets of the data tobe applied to various tasks; and (b) capable of pro-ducing n-best candidates, for potential combinationwith a shallow parser.
Both the Bikel (2004) imple-789System NP-Chunking CoNLL-2000 Li & Roth taskSPRep averaged perceptron 94.21 93.54 95.12Kudo and Matsumoto (2001) 94.22 93.91 -Sha and Pereira (2003) CRF 94.38 - -Voted perceptron 94.09 - -Zhang et al (2002) - 94.17 -Li and Roth (2001) - 93.02 94.64Table 2: Baseline results on three shallow parsing tasks: the NP-Chunking task (Ramshaw and Marcus, 1995); the CoNLL-2000Chunking task (Sang and Buchholz, 2000); and the Li & Roth task (Li and Roth, 2001), which is the same as CoNLL-2000 butwith more training data and a different test section.
The results reported in this table include the best published results on each ofthese tasks.mentation of the Collins parser and the n-best ver-sion of the Charniak (2000) parser, documented inCharniak and Johnson (2005), fit the requirements.Since we observed higher accuracy from the Char-niak parser, from this point forward we report justCharniak parser results4.2.2 Shallow ParserIn addition to the trainable n-best context-free parserfrom Charniak (2000), we needed a trainable shal-low parser to apply to the variety of tasks we wereinterested in investigating.
To this end, we repli-cated the NP-chunker described in Sha and Pereira(2003) and trained it as either an NP-chunker or withthe tagset extended to classify all 11 phrase typesincluded in the CoNLL-2000 task (Sang and Buch-holz, 2000).
Our shallow parser uses exactly the fea-ture set delineated by Sha and Pereira, and performsthe decoding process using a Viterbi search with asecond-order Markov assumption as they described.These features include unigram and bigram wordsup to two positions to either side of the current word;unigram, bigram, and trigram part-of-speech (POS)tags up to two positions to either side of the currentword; and unigram, bigram, and trigram shallowconstituent tags.
We use the averaged perceptron al-gorithm, as presented in Collins (2002), to train theparser.
See (Sha and Pereira, 2003) for more detailson this approach.To demonstrate the competitiveness of our base-line shallow parser, which we label the SPRep av-eraged perceptron, Table 2 shows results on threedifferent shallow parsing tasks.
The NP-Chunking4The parser is available for research purposes atftp://ftp.cs.brown.edu/pub/nlparser/ and can be run in n-best mode.
The one-best performance of the parser is the sameas what was presented in Charniak (2000).task, originally introduced in Ramshaw and Marcus(1995) and also described in (Collins, 2002; Sha andPereira, 2003), brackets just base NP constituents5.The CoNLL-2000 task, introduced as a shared taskat the CoNLL workshop in 2000 (Sang and Buch-holz, 2000), extends the NP-Chunking task to label11 different base phrase constituents.
For both ofthese tasks, the training set was sections 15-18 ofthe Penn WSJ Treebank and the test set was section20.
We follow Collins (2002) and Sha and Pereira(2003) in using section 21 as a heldout set.
The thirdtask, introduced by Li and Roth (2001), performs thesame labeling as in the CoNLL-2000 task, but withmore training data and different testing sets: trainingwas WSJ sections 2-21 and test was section 00.
Weused section 24 as a heldout set; this section is oftenused as heldout for training context-free parsers.Training and testing data for the CoNLL-2000task is available online6.
For the heldout sets foreach of these tasks, as well as for all data setsneeded for the Li & Roth task, reference shallowparses were generated using the chunklink scripton the original treebank trees.
All data was taggedwith the Brill POS tagger (Brill, 1995) after thechunklink conversion.
We verified that usingthis method on the original treebank trees in sections15-18 and 20 generated data that is identical to theCoNLL-2000 data sets online.
Replacing the POStags in the input text with Brill POS tags before the5We follow Sha and Pereira (2003) in deriving the NP con-stituents from the CoNLL-2000 data sets, by replacing all non-NP shallow tags with the ?outside?
(?O?)
tag.
They mentionthat the resulting shallow parse tags are somewhat different thanthose used by Ramshaw and Marcus (1995), but that they foundno significant accuracy differences in training on either set.6Downloaded from the CoNLL-2000 Shared Task websitehttp://www.cnts.ua.ac.be/conll2000/chunking/.790chunklink conversion results in slightly differentshallow parses.From Table 2 we can see that our shallow parseris competitive on all three tasks7.
Sha and Pereira(2003) noted that the difference between their per-ceptron and CRF results was not significant, andour performance falls between the two, thus repli-cating their result within noise.
Our performancefalls 0.6 percentage points below the best publishedresult on the CoNLL-2000 task, and 0.5 percentagepoints above the performance by Li and Roth (2001)on their task.
Overall, ours is a competitive approachfor shallow parsing.3 Experimental Results3.1 Comparing Finite-State andContext-Free ParsersThe first two rows of Table 3 present a comparisonbetween the SPRep shallow parser and the Charniak(2000) context-free parser detailed in Charniak andJohnson (2005).
We can see that the performanceof the two models is virtually indistinguishable forall three of these tasks, with or without ignoring ofpunctuation.
As mentioned earlier, we used the ver-sion of this parser with improved n-best extraction,as documented in Charniak and Johnson (2005), al-though without the reranking of the candidates thatthey also report in that paper.
For these trials, weused just the one-best output of that model, which isthe same as in Charniak (2000).Note that the standard training set for context-freeparsing (sections 2-21) is only used for the Li &Roth task; for the other two tasks, both the SPRepand the Charniak parsers were trained on sections15-18, with section 21 as heldout.
This demonstratesthat the context-free parser, even when trained on asmall fraction of the total treebank, is able to learn acompetitive model for this task.3.2 Combining Finite-State andContext-Free ParsersIt is likely true that a context-free parser which hasbeen optimized for global parse accuracy will, onoccasion, lose some shallow parse accuracy to sat-isfy global structure constraints that do not constrain7Sha and Pereira (2003) reported the Kudo and Matsumoto(2001) performance on the NP-Chunking task to be 94.39 andto be the best reported result on this task.
In the cited paper,however, the result is as reported in our table.a shallow parser.
However, it is also likely truethat these longer distance constraints will on occa-sion enable the context-free parser to better identifythe shallow constituent structure.
In other words,despite having very similar performance, our shal-low parser and the Charniak context-free parser arelikely making complementary predictions about theshallow structure that can be exploited for furtherimprovements.
In this section, we explore two sim-ple methods for combining the system outputs.The first combination of the system outputs,which we call unweighted intersection, is the sim-plest kind of ?rovered?
system, which restricts theset of shallow parse candidates to the intersectionof the sets output by each system, but does notcombine the scores.
Since the Viterbi search ofthe SPRep model provides a score for all possi-ble shallow parses, the intersection of the two setsis simply the set of shallow-parse sequences in the50-best candidates output by the Charniak parser.We then use the SPRep perceptron-model scores tochoose from among just these candidates.
We con-verted the 50-best lists returned by the Charniakparser into k-best lists of shallow parses by usingchunklink to convert each candidate context-freeparse into a shallow parse.
Many of the context-freeparses map to the same shallow parse, so the size ofthis list is typically much less than 50, with an aver-age of around 7.
Each of the unique shallow-parsecandidates is given a score by the SPRep percep-tron, and the best-scoring candidate is selected.
Ef-fectively, we used the Charniak parser?s k-best shal-low parses to limit the search space for our shallowparser.The second combination of the system outputs,which we call weighted intersection, extends the un-weighted intersection by including the scores fromthe Charniak parser, which are log probabilities.The score for a shallow parse output by the Char-niak parser is the log of the sum of the probabili-ties of all context-free parses mapping to that shal-low parse.
We normalize across all candidates fora given string, hence these are conditional log prob-abilities.
We multiply these conditional log proba-bilities by a scaling factor ?
before adding them tothe SPRep perceptron score for a particular candi-date.
Again, the best-scoring candidate using thiscomposite score is selected from among the shallow791NP-Chunking CoNLL-2000 Li & Roth taskPunctuation Punctuation PunctuationSystem Leave Ignore Leave Ignore Leave IgnoreSPRep averaged perceptron 94.21 94.25 93.54 93.70 95.12 95.27Charniak (2000) 94.17 94.20 93.77 93.92 95.15 95.32Unweighted intersection 95.13 95.16 94.52 94.64 95.77 95.92Weighted intersection 95.57 95.58 95.03 95.16 96.20 96.33Table 3: F-measure shallow bracketing accuracy on three shallow parsing tasks, for the SPRep perceptron shallow parser, theCharniak (2000) context-free parser, and for systems combining the SPRep and Charniak system outputs.parse candidates output by the Charniak parser.
Weused the heldout data to empirically estimate an op-timal scaling factor for the Charniak scores, whichis 15 for all trials reported here.
This factor com-pensates for differences in the dynamic range of thescores of the two parsers.Both of these intersections are done at test-time,i.e.
the models are trained independently.
To remainconsistent with task-specific training and testing sec-tion conventions, the individual models were alwaystrained on the appropriate sections for the given task,i.e.
WSJ sections 15-18 for NP-Chunking and theCoNLL-2000 tasks, and sections 2-21 for the Li &Roth task.Results from these methods of combination areshown in the bottom two rows of Table 3.
Eventhe simple unweighted intersection gives quite largeimprovements over each of the independent systemsfor all three tasks.
All of these improvements aresignificant at p < 0.001 using the Matched PairSentence Segment test (Gillick and Cox, 1989).
Theweighted intersection gives further improvementsover the unweighted intersection for all tasks, andthis improvement is also significant at p < 0.001,using the same test.3.3 Robustness to Domain ShiftOur final shallow parsing task was also proposed inLi and Roth (2001).
The purpose of this task wasto examine the degradation in performance whenparsers, trained on one relatively clean domain suchas WSJ, are tested on another, mismatched domainsuch as Switchboard.
The systems that are reportedin this section are trained on sections 2-21 of theWSJ Treebank, with section 24 as heldout, andtested on section 4 of the Switchboard Treebank.Note that the systems used here are exactly the onespresented for the original Li & Roth task, in Sec-PunctuationSystem Leave IgnoreLi & Roth (reference tags) 88.47 -SPRep avg perceptronReference tags 91.37 91.86Brill tags 87.94 88.42Charniak (2000) 87.94 88.44Unweighted intersection 88.66 89.16Weighted intersection 89.22 89.69Table 4: Shallow bracketing accuracy of several different sys-tems, trained on sections 2-21 of WSJ Treebank and appliedto section 4 of the Switchboard Treebank.
Li and Roth (2001)results are as reported in their paper, with reference POS tagsrather than Brill-tagger POS tags.tions 3.1 and 3.2; only the test set has changed, train-ing and heldout sets remain exactly the same, as dothe mixing parameters for the weighted intersection.In the trials reported in Li and Roth (2001), both ofthe evaluated systems were provided with referencePOS tags from the Switchboard Treebank.
In thecurrent results, we show our SPRep averaged per-ceptron system provided both with reference POStags for comparison with the Li and Roth results,and provided with Brill-tagger POS tags for com-parison with other systems.
Table 4 shows our re-sults for this task.
Whereas Li and Roth reporteda more marked degradation in performance whenusing a context-free parser as compared to a shal-low parser, we again show virtually indistinguish-able performance between our SPRep shallow parserand the Charniak context-free parser.
Again, using aweighted combined model gave us large improve-ments over each independent model, even in thismismatched domain.3.4 Reranked n-best ListJust prior to the publication of this paper, we wereable to obtain the trained reranker from Charniak792WSJ Sect.
00 SWBD Sect.
4Punctuation PunctuationSystem Leave Ignore Leave IgnoreSPRep 95.12 95.27 87.94 88.43C & J one-best 95.15 95.32 87.94 88.44(2005) reranked 95.81 96.04 88.64 89.17Weighted intersection 96.32 96.47 89.32 89.80Table 5: F-measure shallow bracketing accuracy when trainedon WSJ sections 2-21 and applied to either WSJ section 00 orSWBD section 4.
Systems include our shallow parser (SPRep);the Charniak and Johnson (2005) system (C & J), both initialone-best and reranked-best; and the weighted intersection be-tween the reranked 50-best list and the SPRep system.and Johnson (2005), which allows a comparison ofthe shallow parsing gains that they obtain from thatsystem with those documented here.
The reranker isa discriminatively trained Maximum Entropy modelwith an F-measure parsing accuracy objective.
Ituses a large number of features, and is applied to the50-best output from the generative Charniak parsingmodel.
The reranking model was trained on sections2-21, with section 24 used as heldout.
This allows usto compare its shallow parsing accuracy with othersystems on the tasks that use this training setup: theLi & Roth task (testing on WSJ section 00) and thedomain shift task (testing on Switchboard section4).
Table 5 shows two new trials making use of thisreranking model.The Charniak and Johnson (2005) system out-put (denoted C & J in the table) before rerank-ing (denoted one-best) is identical to the Charniak(2000) results that have been reported in the othertables.
After reranking (denoted reranked), the per-formance improves by roughly 0.7 percentage pointsfor both tasks, nearly reaching the performancethat we obtained with weighted intersection of theSPRep model and the n-best list before reranking.Weighted intersection between the reranked list andthe shallow parser as described earlier, with a newlyestimated scaling factor (?=30), provides a roughly0.5 percentage point increase over the result ob-tained by the reranker.
The difference between theCharniak output before and after reranking is statis-tically significant at p < 0.001, as is the differencebetween the reranked output and the weighted inter-section, using the same test reported earlier.3.5 DiscussionWhile it may be seen to be overkill to apply acontext-free parser for these shallow parsing tasks,we feel that these results are very interesting fora couple of reasons.
First, they go some way to-ward correcting the misperception that context-freeparsers are less applicable in real-world scenariosthan finite-state sequence models.
Finite-state mod-els are undeniably more efficient; however, it isimportant to have a clear idea of how much ac-curacy is being sacrificed to reach that efficiency.Any given application will need to examine the ef-ficiency/accuracy trade-off with different objectivesfor optimality.
For those willing to trade efficiencyfor accuracy, it is worthwhile knowing that it is pos-sible to do much better on these tasks than what hasbeen reported in the past.4 Conclusion and Future WorkIn summary, we have demonstrated in this paper thatthere is no accuracy or robustness benefit to shal-low parsing with finite-state models over using high-accuracy context-free models.
Even more, there is alarge benefit to be had in combining the output ofhigh-accuracy context-free parsers with the outputof shallow parsers.
We have demonstrated a largeimprovement over the previous-best reported re-sults on several tasks, including the well-known NP-Chunking and CoNLL-2000 shallow parsing tasks.Part of the misperception of the relative benefitsof finite-state and context-free models is due to dif-ficulty evaluating across these differing annotationstyles.
Mapping from context-free parser outputto the shallow constituents defined in the CoNLL-2000 task depends on many construction-specificoperations that have unfairly penalized context-freeparsers in previous comparisons.While the results of combining system outputsshow one benefit of combining systems, as presentedin this paper, they hardly exhaust the possibilitiesof exploiting the differences between these models.Making use of the scores for the shallow parses out-put by the Charniak parser is a demonstrably ef-fective way to improve performance.
Yet there areother possible features explicit in the context-freeparse candidates, such as head-to-head dependen-cies, which might be exploited to further improveperformance.
We intend to explore including fea-tures from the context-free parser output in our per-ceptron model to improve shallow parsing accuracy.Another possibility is to look at improving793context-free parsing accuracy.
Within a multi-passparsing strategy, the high-accuracy shallow parsesthat result from system combination could be usedto restrict the search within yet another pass of acontext-free parser.
That parser could then searchfor the best global analysis from within just thespace of parses consistent with the provided shallowparse.
Also, features of the sort used in our shallowparser could be included in a reranker, such as thatin Charniak and Johnson (2005), with a context-freeparsing accuracy objective.A third possibility is to optimize the definition ofthe shallow-parse phrase types themselves, for usein other applications.
The composition of the set ofphrase types put forth by Sang and Buchholz (2000)may not be optimal for certain applications.
Onesuch application is discourse parsing, which relieson accurate detection of clausal boundaries.
Shal-low parsing could provide reliable information onthe location of these boundaries, but the current setof phrase types may be too general for such use.
Forexample, consider infinitival verb phrases, which of-ten indicate the start of a clause whereas other typesof verb phrases do not.
Unfortunately, with only oneVP category in the CoNLL-2000 set of phrase types,this distinction is lost.
Expanding the defined set ofphrase types could benefit many applications.Future work will also include continued explo-ration of possible features that can be of use for ei-ther shallow parsing models or context-free parsingmodels.
In addition, we intend to investigate waysin which to encode approximations to context-freeparser derived features that can be used within finite-state models, thus perhaps preserving finite-state ef-ficiency while capturing at least some of the accu-racy gain that was observed in this paper.AcknowledgmentsWe would like to thank Eugene Charniak and MarkJohnson for help with the parser and reranker doc-umented in their paper.
The first author of this pa-per was supported under an NSF Graduate ResearchFellowship.
In addition, this research was supportedin part by NSF Grant #IIS-0447214.
Any opin-ions, findings, conclusions or recommendations ex-pressed in this publication are those of the authorsand do not necessarily reflect the views of the NSF.ReferencesSteven Abney.
1991.
Parsing by chunks.
In Robert Berwick,Steven Abney, and Carol Tenny, editors, Principle-BasedParsing.
Kluwer Academic Publishers, Dordrecht.Steven Abney.
1996.
Partial parsing via finite-state cascades.Natural Language Engineering, 2(4):337?344.Daniel M. Bikel.
2004.
Intricacies of Collins?
parsing model.Computational Linguistics, 30(4).Eric Brill.
1995.
Transformation-based error-driven learningand natural language processing: A case study in part-of-speech tagging.
Computational Linguistics, 21(4):543?565.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.
In Pro-ceedings of the 43rd Annual Meeting of ACL.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.In Proceedings of the 1st Annual Meeting of NAACL, pages132?139.Michael Collins.
1997.
Three generative, lexicalised modelsfor statistical parsing.
In Proceedings of the 35th AnnualMeeting of ACL, pages 16?23.Michael Collins.
2000.
Discriminative reranking for naturallanguage parsing.
In Proceedings of the 17th ICML Confer-ence.Michael Collins.
2002.
Discriminative training methods forhidden Markov models: Theory and experiments with per-ceptron algorithms.
In Proceedings of the Conference onEMNLP, pages 1?8.L.
Gillick and S. Cox.
1989.
Some statistical issues in the com-parison of speech recognition algorithms.
In Proceedings ofICASSP, pages 532?535.Taku Kudo and Yuji Matsumoto.
2001.
Chunking with supportvector machines.
In Proceedings of the 2nd Annual Meetingof NAACL.Xin Li and Dan Roth.
2001.
Exploring evidence for shallowparsing.
In Proceedings of the 5th Conference on CoNLL.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated corpusof English: The Penn Treebank.
Computational Linguistics,19(2):313?330.Lance A. Ramshaw and Mitchell P. Marcus.
1995.
Text chunk-ing using transformation-based learning.
In Proceedings ofthe 3rd Workshop on Very Large Corpora, pages 82?94.Erik F. Tjong Kim Sang and Sabine Buchholz.
2000.
Introduc-tion to the CoNLL-2000 shared task: Chunking.
In Proceed-ings of the 4th Conference on CoNLL.Fei Sha and Fernando Pereira.
2003.
Shallow parsing with con-ditional random fields.
In Proceedings of the HLT-NAACLAnnual Meeting.Tong Zhang, Fred Damerau, and David Johnson.
2002.
Textchunking based on a generalization of Winnow.
Journal ofMachine Learning Research, 2:615?637.794
