Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 477?487,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsA Joint Segmentation and Classification Frameworkfor Sentiment AnalysisDuyu Tang\?, Furu Wei?, Bing Qin\, Li Dong]?, Ting Liu\, Ming Zhou?\Research Center for Social Computing and Information Retrieval,Harbin Institute of Technology, China?Microsoft Research, Beijing, China]Beihang University, Beijing, China\{dytang, qinb, tliu}@ir.hit.edu.cn?
{fuwei, mingzhou}@microsoft.com]donglixp@gmail.comAbstractIn this paper, we propose a joint segmenta-tion and classification framework for sen-timent analysis.
Existing sentiment clas-sification algorithms typically split a sen-tence as a word sequence, which does noteffectively handle the inconsistent senti-ment polarity between a phrase and thewords it contains, such as ?not bad?
and?a great deal of ?.
We address this issueby developing a joint segmentation andclassification framework (JSC), which si-multaneously conducts sentence segmen-tation and sentence-level sentiment classi-fication.
Specifically, we use a log-linearmodel to score each segmentation candi-date, and exploit the phrasal informationof top-ranked segmentations as features tobuild the sentiment classifier.
A marginallog-likelihood objective function is de-vised for the segmentation model, whichis optimized for enhancing the sentimentclassification performance.
The joint mod-el is trained only based on the annotat-ed sentiment polarity of sentences, with-out any segmentation annotations.
Experi-ments on a benchmark Twitter sentimen-t classification dataset in SemEval 2013show that, our joint model performs com-parably with the state-of-the-art methods.1 IntroductionSentiment classification, which classifies the senti-ment polarity of a sentence (or document) as posi-tive or negative, is a major research direction in thefield of sentiment analysis (Pang and Lee, 2008;Liu, 2012; Feldman, 2013).
Majority of existingapproaches follow Pang et al.
(2002) and treat sen-?This work was partly done when the first and fourthauthors were visiting Microsoft Research.timent classification as a special case of text cate-gorization task.
Under this perspective, previousstudies typically use pipelined methods with twosteps.
They first produce sentence segmentation-s with separate text analyzers (Choi and Cardie,2008; Nakagawa et al., 2010; Socher et al., 2013b)or bag-of-words (Paltoglou and Thelwall, 2010;Maas et al., 2011).
Then, feature learning and sen-timent classification algorithms take the segmenta-tion results as inputs to build the sentiment classi-fier (Socher et al., 2011; Kalchbrenner et al., 2014;Dong et al., 2014).The major disadvantage of a pipelined methodis the problem of error propagation, since sen-tence segmentation errors cannot be corrected bythe sentiment classification model.
A typical kindof error is caused by the polarity inconsistency be-tween a phrase and the words it contains, suchas ?not bad, bad?
and ?a great deal of, great?.The segmentations based on bag-of-words or syn-tactic chunkers are not effective enough to han-dle the polarity inconsistency phenomenons.
Thereason lies in that bag-of-words segmentations re-gard each word as a separate unit, which lossesthe word order and does not capture the phrasalinformation.
The segmentations based on syntac-tic chunkers typically aim to identify noun group-s, verb groups or named entities from a sentence.However, many sentiment indicators are phrasesconstituted of adjectives, negations, adverbs or id-ioms (Liu, 2012; Mohammad et al., 2013a), whichare splitted by syntactic chunkers.
Besides, a bet-ter approach would be to utilize the sentiment in-formation to improve the segmentor.
Accordingly,the sentiment-specific segmentor will enhance theperformance of sentiment classification in turn.In this paper, we propose a joint segmentationand classification framework (JSC) for sentimen-t analysis, which simultaneous conducts sentencesegmentation and sentence-level sentiment clas-sification.
The framework is illustrated in Fig-477Segmentations Inputthat is not badthat  is  not  badthat  is  not  badthat  is  not  badthat  is  not  badPolarity: +1-1-1+1+1<+1,-1>   NOPolarity Update<+1,-1>   NO<+1,+1>  YES<+1,+1>  YESSC SEG CGUpdateSC2.31.60.60.40.60.42.31.6SEGRankTop KFigure 1: The joint segmentation and classification framework (JSC) for sentiment classification.
CGrepresents the candidate generation model, SC means the sentiment classification model and SEG standsfor the segmentation ranking model.
Down Arrow means the use of a specified model, and Up Arrowindicates the update of a model.ure 1.
We develop (1) a candidate generation mod-el to generate the segmentation candidates of asentence, (2) a segmentation ranking model to s-core each segmentation candidate of a given sen-tence, and (3) a classification model to predic-t the sentiment polarity of each segmentation.
Thephrasal information of top-ranked candidates fromthe segmentation model are utilized as features tobuild the sentiment classifier.
In turn, the predict-ed sentiment polarity of segmentation candidatesfrom classification model are leveraged to updatethe segmentor.
We score each segmentation can-didate with a log-linear model, and optimize thesegmentor with a marginal log-likelihood objec-tive.
We train the joint model from sentences an-notated only with sentiment polarity, without anysegmentation annotations.We evaluate the effectiveness of our joint mod-el on a benchmark Twitter sentiment classifica-tion dataset in SemEval 2013.
Results show thatthe joint model performs comparably with state-of-the-art methods, and consistently outperformspipeline methods in various experiment settings.The main contributions of the work presented inthis paper are as follows.?
To our knowledge, this is the first work thatautomatically produces sentence segmenta-tion for sentiment classification within a jointframework.?
We show that the joint model yields com-parable performance with the state-of-the-artmethods on the benchmark Twitter sentimentclassification datasets in SemEval 2013.2 Related WorkExisting approaches for sentiment classificationare dominated by two mainstream directions.Lexicon-based approaches (Turney, 2002; Dinget al., 2008; Taboada et al., 2011; Thelwall etal., 2012) typically utilize a lexicon of sentimentwords, each of which is annotated with the sen-timent polarity or sentiment strength.
Linguis-tic rules such as intensifications and negations areusually incorporated to aggregate the sentimen-t polarity of sentences (or documents).
Corpus-based methods treat sentiment classification as aspecial case of text categorization task (Pang et al.,2002).
They mostly build the sentiment classifierfrom sentences (or documents) with manually an-notated sentiment polarity or distantly-supervisedcorpora collected by sentiment signals like emoti-cons (Go et al., 2009; Pak and Paroubek, 2010;Kouloumpis et al., 2011; Zhao et al., 2012).Majority of existing approaches follow Pang etal.
(2002) and employ corpus-based method forsentiment classification.
Pang et al.
(2002) pi-oneer to treat the sentiment classification of re-views as a special case of text categorization prob-lem and first investigate machine learning meth-ods.
They employ Naive Bayes, Maximum En-tropy and Support Vector Machines (SVM) with adiverse set of features.
In their experiments, thebest performance is achieved by SVM with bag-of-words feature.
Under this perspective, many s-tudies focus on designing or learning effective fea-tures to obtain better classification performance.On movie or product reviews, Wang and Man-ning (2012) present NBSVM, which trades-off478between Naive Bayes and NB-feature enhancedSVM.
Kim and Zhai (2009) and Paltoglou andThelwall (2010) learn the feature weights by in-vestigating variants weighting functions from In-formation Retrieval.
Nakagawa et al.
(2010) uti-lize dependency trees, polarity-shifting rules andconditional random fields (Lafferty et al., 2001)with hidden variables to compute the documen-t feature.
On Twitter, Mohammad et al.
(2013b)develop a state-of-the-art Twitter sentiment classi-fier in SemEval 2013, using a variety of sentimentlexicons and hand-crafted features.With the revival of deep learning (representa-tion learning (Hinton and Salakhutdinov, 2006;Bengio et al., 2013; Jones, 2014)), more recen-t studies focus on learning the low-dimensional,dense and real-valued vector as text features forsentiment classification.
Glorot et al.
(2011) inves-tigate Stacked Denoising Autoencoders to learndocument vector for domain adaptation in sen-timent classification.
Yessenalina and Cardie(2011) represent each word as a matrix andcompose words using iterated matrix multipli-cation.
Socher et al.
propose Recursive Au-toencoder (RAE) (2011), Matrix-Vector RecursiveNeural Network (MV-RNN) (2012) and Recur-sive Neural Tensor Network (RNTN) (2013b) tolearn the composition of variable-length phrasesbased on the representation of its children.
Tolearn the sentence representation, Kalchbrenner etal.
(2014) exploit Dynamic Convolutional Neu-ral Network and Le and Mikolov (2014) inves-tigate Paragraph Vector.
To learn word vectorsfor sentiment analysis, Maas et al.
(2011) proposea probabilistic document model following Blei etal.
(2003), Labutov and Lipson (2013) re-embedwords from existing word embeddings and Tanget al.
(2014b) develop three neural networks tolearn word vectors from tweets containing posi-tive/negative emoticons.Unlike most previous corpus-based algorithmsthat build sentiment classifier based on splitting asentence as a word sequence, we produce sentencesegmentations automatically within a joint frame-work, and conduct sentiment classification basedon the segmentation results.3 The Proposed ApproachIn this section, we first give the task definitionof two tasks, namely sentiment classification andsentence segmentation.
Then, we present theoverview of the proposed joint segmentation andclassification model (JSC) for sentiment analysis.The segmentation candidate generation model andthe segmentation ranking model are described inSection 4.
The details of the sentiment classifica-tion model are presented in Section 5.3.1 Task DefinitionThe task of sentiment classification has been wellformalized in previous studies (Pang and Lee,2008; Liu, 2012).
The objective is to identify thesentiment polarity of a sentence (or document) aspositive or negative1.The task of sentence segmentation aims to s-plit a sentence into a sequence of exclusive part-s, each of which is a basic computational unit ofthe sentence.
An example is illustrated in Table 1.The original text ?that is not bad?
is segmentedas ?
[that] [is] [not bad]?.
The segmentation re-sult is composed of three basic computational u-nits, namely [that], [is] and [not bad].Type SampleSentence that is not badSegmentation [that] [is] [not bad]Basic units [that], [is], [not bad]Table 1: Example for sentence segmentation.3.2 Joint Model (JSC)The overview of the proposed joint segmentationand classification model (JSC) for sentiment anal-ysis is illustrated in Figure 1.
The intuitions of thejoint model are two-folds:?
The segmentation results have a strong influ-ence on the sentiment classification perfor-mance, since they are the inputs of the sen-timent classification model.?
The usefulness of a segmentation can bejudged by whether the sentiment classifiercan use it to predict the correct sentence po-larity.Based on the mutual influence observation, weformalize the joint model in Algorithm 1.
The in-puts contain two parts, training data and featureextractors.
Each sentence siin the training data1In this paper, the sentiment polarity of a sentence is notrelevant to the target (or aspect) it contains (Hu and Liu, 2004;Jiang et al., 2011; Mitchell et al., 2013).479Algorithm 1 The joint segmentation and classifi-cation framework (JSC) for sentiment analysisInput:training data: T = [si, polgi], 1 ?
i ?
|T |segmentation feature extractor: sfe(?
)classification feature extractor: cfe(?
)Output:sentiment classifier: SCsegmentation ranking model: SEG1: Generate segmentation candidates ?ifor eachsentence siin T , 1 ?
i ?
|T |2: Initialize sentiment classifier SC(0)based oncfe(?ij), randomize j ?
[1, |?i|], 1 ?
i ?|T |3: Randomly initialize the segmentation rankingmodel SEG(0)4: for r ?
1 ... R do5: Predict the sentiment polarity polifor ?ibased on SC(r?1)and cfe(?i?
)6: Update the segmentation model SEG(r)with SEG(r?1)and [?i, sfe(?i?
),poli?, polgi], 1 ?
i ?
|T |7: for i?
1 ... |T | do8: Calculate the segmentation score for ?i?based on SEG(r)and sfe(?i?
)9: Select the top-ranked K segmentationcandidates ?i?from ?i10: end for11: Train the sentiment classifier SC(r)withcfe(?i?
), 1 ?
i ?
|T |12: end for13: SC?
SC(R)14: SEG?
SEG(R)T is annotated only with its gold sentiment po-larity polgi, without any segmentation annotation-s.
There are two feature extractors for the taskof sentence segmentation (sfe(?))
and sentimentclassification (cfe(?
)), respectively.
The output-s of the joint model are the segmentation rankingmodel SEG and the sentiment classifier SC.In Algorithm 1, we first generate segmentationcandidates ?ifor each sentence siin the trainingset (line 1).
Each ?icontains no less than onesegmentation candidates.
We randomly select onesegmentation result from each ?iand utilize theirclassification features to initialize the sentimen-t classifier SC(0)(line 2).
We randomly initializethe segmentation model SEG(0)(line 3).
Subse-quently, we iteratively train the segmentation mod-el SEG(r)and sentiment classifier SC(r)in a join-t manner (line 4-12).
At each iteration, we pre-dict the sentiment polarity of each segmentationcandidate ?i?with the current sentiment classifi-er SC(r?1)(line 5), and then leverage them to up-date the segmentation model SEG(r)(line 6).
Af-terwards, we utilize the recently updated segmen-tation ranking model SEG(r)to update the senti-ment classifier SC(r)(line 7-11).
We extract thesegmentation features for each segmentation can-didate ?i?, and employ them to calculate the seg-mentation score (line 8).
The top-ranked K seg-mentation results ?i?of each sentence siis select-ed (line 9), and further used to train the sentimen-t classifier SC(r)(line 11).
Finally, after trainingR iterations, we dump the segmentation rankingmodel SEG(R)and sentiment classifier SC(R)inthe last iteration as outputs (line 13-14).At training time, we train the segmentationmodel and classification model from sentenceswith manually annotated sentiment polarity.
Atprediction time, given a test sentence, we gener-ate its segmentation candidates, and then calculatesegmentation score for each candidate.
Afterward-s, we select the top-ranked K candidates and votetheir predicted sentiment polarity from sentimentclassifier as the final result.4 Segmentation ModelIn this section, we present details of the segmenta-tion candidate generation model (Section 4.1), thesegmentation ranking model (Section 4.2) and thefeature description for segmentation ranking mod-el (Section 4.3).4.1 Segmentation Candidate GenerationIn this subsection, we describe the strategy to gen-erate segmentation candidates for each sentence.Since the segmentation results have an exponen-tial search space in the number of words in asentence, we approximate the computation usingbeam search with constrains on a phrase table,which is induced from massive corpora.Many studies have been previously proposed torecognize phrases in the text.
However, it is outof scope of this work to compare them.
We ex-ploit a data-driven approach given by Mikolov etal.
(2013), which identifies phrases based on theoccurrence frequency of unigrams and bigrams,freq(wi, wj) =freq(wi, wj)?
?freq(wi)?
freq(wj)(1)480where ?
is a discounting coefficient that preventstoo many phrases consisting of very infrequen-t words.
We run 2-4 times over the corpora to getlonger phrases containing more words.
We em-pirically set ?
as 10 in our experiment.
We usethe default frequency threshold (value=5) in theword2vec toolkit2to select bi-terms.Given a sentence, we initialize the beam of eachindex with the current word, and sequentially addphrases into the beam if the new phrase is con-tained in the phrase table.
At each index of a sen-tence, we rank the segmentation candidates by theinverted number of items within a segmentation,and save the top-ranked N segmentation candi-dates into the beam.
An example of the generatedsegmentation candidates is given in Table 2.Type SampleSentence that is not badPhrase Table [is not], [not bad], [is not bad]Segmentations[that] [is not bad][that] [is not] [bad][that] [is] [not bad][that] [is] [not] [bad]Table 2: Example for segmentation candidate gen-eration.4.2 Segmentation Ranking ModelThe objective of the segmentation ranking modelis to assign a scalar to each segmentation candi-date, which indicates the usefulness of the seg-mentation result for sentiment classification.
Inthis subsection, we describe a log-linear model tocalculate the segmentation score.
To effectivelytrain the segmentation ranking model, we devise amarginal log-likelihood as the optimization objec-tive.Given a segmentation candidate ?ijof the sen-tence si, we calculate the segmentation scorefor ?ijwith a log-linear model, as given in Equa-tion 2.?ij= exp(b+?ksfeijk?
wk) (2)where ?ijis the segmentation score of ?ij; sfeijkis the k-th segmentation feature of ?ij; w and b arethe parameters of the segmentation ranking model.During training, given a sentence siand its goldsentiment polarity polgi, the optimization objec-2Available at https://code.google.com/p/word2vec/tive of the segmentation ranking model is to max-imize the segmentation scores of the hit candi-dates, whose predicted sentiment polarity equal-s to the gold polarity of sentence polpi.
The lossfunction of the segmentation model is given in E-quation 3.loss = ?|T |?i=1log(?j?Hi?ij?j??Ai?ij?)
+ ?||w||22(3)where T is the training data; Airepresents all thesegmentation candidates of sentence si; Himean-s the hit candidates of si; ?
is the weight of theL2-norm regularization factor.
We train the seg-mentation model with L-BFGS (Liu and Nocedal,1989), running over the complete training data.4.3 FeatureWe design two kinds of features for sentence seg-mentation, namely the phrase-embedding featureand the segmentation-specific feature.
The finalfeature representation of each segmentation is theconcatenation of these two features.
It is worthnoting that, the phrase-embedding feature is usedin both sentence segmentation and sentiment clas-sification.Segmentation-Specific Feature We empiricallydesign four segmentation-specific features to re-flect the information of each segmentation, as list-ed in Table 3.Phrase-Embedding Feature We leveragephrase embedding to generate the features ofsegmentation candidates for both sentence seg-mentation and sentiment classification.
Thereason is that, in both tasks, the basic compu-tational units of each segmentation candidatemight be words or phrases of variable length.Under this scenario, phrase embedding is highlysuitable as it is capable to represent phrases withdifferent length into a consistent distributed vectorspace (Mikolov et al., 2013).
For each phrase,phrase embedding is a dense, real-valued andcontinuous vector.
After the phrase embedding istrained, the nearest neighbors in the embeddingspace are favored to have similar grammatical us-ages and semantic meanings.
The effectiveness ofphrase embedding has been verified for buildinglarge-scale sentiment lexicon (Tang et al., 2014a)and machine translation (Zhang et al., 2014).We learn phrase embedding with Skip-Grammodel (Mikolov et al., 2013), which is the state-of-481Feature Feature Description#unit the number of basic computation units in the segmentation candidate#unit / #word the ratio of units?
number in a candidate to the length of original sentence#word ?
#unit the difference between sentence length and the number of basic computational units#unit > 2 the number of basic component units composed of more than two wordsTable 3: Segmentation-specific features for segmentation ranking.Feature Feature DescriptionAll-Caps the number of words with all characters in upper caseEmoticon the presence of positive (or negative) emoticons, whether the last unit is emoticonHashtag the number of hashtagElongated units the number of basic computational containing elongated words (with one characterrepeated more than two times), such as goooodSentiment lexicon the number of sentiment words, the score of last sentiment words, the total sentimentscore and the maximal sentiment score for each lexiconNegation the number of negations as individual units in a segmentationBag-of-Units an extension of bag-of-word for a segmentationPunctuation the number of contiguous sequences of dot, question mark and exclamation mark.Cluster the presence of units from each of the 1,000 clusters from Twitter NLP tool (Gimpelet al., 2011)Table 4: Classification-specific features for sentiment classification.the-art phrase embedding learning algorithm.
Wecompose the representation (or feature) of a seg-mentation candidate from the embedding of thebasic computational units (words or phrases) itcontains.
In this paper, we explore min, max andaverage convolution functions, which have beenused as simple and effective methods for composi-tion learning in vector-based semantics (Mitchelland Lapata, 2010; Collobert et al., 2011; Socher etal., 2013a; Shen et al., 2014; Tang et al., 2014b),to calculate the representation of a segmentationcandidate.
The final phrase-embedding feature isthe concatenation of vectors derived from differentconvolutional functions, as given in Equation 4,pf(seg) = [pfmax(seg), pfmin(seg), pfavg(seg)](4)where pf(seg) is the representation of the givensegmentation; pfx(seg) is the result of the con-volutional function x ?
{min,max, avg}.
Eachconvolutional function pfx(?)
conducts the matrix-vector operation of x on the sequence representedby columns in the lookup table of phrase embed-ding.
The output of pfx(?)
is calculated aspfx(seg) = ?x?Lph?seg(5)where ?xis the convolutional function of pfx;?Lph?segis the concatenated column vectors ofthe basic computational units in the segmentation;Lphis the lookup table of phrase embedding.5 Classification ModelFor sentiment classification, we follow the su-pervised learning framework (Pang et al., 2002)and build the classifier from sentences with man-ually labelled sentiment polarity.
We extend thestate-of-the-art hand-crafted features in SemEval2013 (Mohammad et al., 2013b), and design theclassification-specific features for each segmenta-tion.
The detailed feature description is given inTable 4.6 ExperimentIn this section, we conduct experiments to evaluatethe effectiveness of the joint model.
We describethe experiment settings and the result analysis.6.1 Dataset and Experiment SettingsWe conduct sentiment classification of tweets on abenchmark Twitter sentiment classification datasetin SemEval 2013.
We run 2-class (positive vs neg-ative) classification as sentence segmentation has agreat influence on the positive/negative polarity oftweets due to the polarity inconsistency between aphrase and its constitutes, such as ?not bad, bad?.482We leave 3-class classification (positive, negative,neutral) and fine-grained classification (very neg-ative, negative, neutral, positive, very positive) inthe future work.Positive Negative TotalTrain 2,642 994 3,636Dev 408 219 627Test 1,570 601 2,171Table 5: Statistics of the SemEval 2013 Twittersentiment classification dataset (positive vs nega-tive).The statistics of our dataset crawled from Se-mEval 2013 are given in Table 5.
The evalua-tion metric is the macro-F1 of sentiment classifi-cation.
We train the joint model on the trainingset, tune parameters on the dev set and evaluateon the test set.
We train the sentiment classifierwith LibLinear (Fan et al., 2008) and utilize exist-ing sentiment lexicons3to extract classification-specific features.
We randomly crawl 100M tweetsfrom February 1st, 2013 to April 30th, 2013 withTwitter API, and use them to learn the phrase em-bedding with Skip-Gram4.
The vocabulary sizeof the phrase embedding is 926K, from unigramto 5-gram.
The parameter -c in SVM is tuned onthe dev-set in both baseline and our method.
Werun the L-BFGS for 50 iterations, and set the reg-ularization factor ?
as 0.003.
The beam size N ofthe candidate generation model and the top-rankedsegmentation number K are tuned on the dev-set.6.2 Baseline MethodsWe compare the proposed joint model with the fol-lowing sentiment classification algorithms:?
DistSuper: We collect 10M balanced tweetsselected by positive and negative emoticons5astraining data, and build classifier using the Lib-Linear and ngram features (Go et al., 2009; Zhaoet al., 2012).?
SVM: The n-gram features and Support Vec-tor Machine are widely-used baseline methods tobuild sentiment classifiers (Pang et al., 2002).
Weuse LibLinear to train the SVM classifier.3In this work, we use HL (Hu and Liu, 2004), M-PQA (Wilson et al., 2005), NRC Emotion Lexicon (Moham-mad and Turney, 2012), NRC Hashtag Lexicon and Senti-ment140Lexicon (Mohammad et al., 2013b).4https://code.google.com/p/word2vec/5We use the emoticons selected by Hu et al.
(2013).
Thepositive emoticons are :) : ) :-) :D =), and the negative emoti-cons are :( : ( :-( .?
NBSVM: NBSVM (Wang and Manning,2012) trades-off between Naive Bayes and NB-features enhanced SVM.
We use NBSVM-bi be-cause it performs best on sentiment classificationof reviews.?
RAE: Recursive Autoencoder (Socher et al.,2011) has been proven effective for sentiment clas-sification by learning sentence representation.
Wetrain the RAE using the pre-trained phrase embed-ding learned from 100M tweets.?
SentiStrength: Thelwall et al.
(2012) build alexicon-based classifier which uses linguistic rulesto detect the sentiment strength of tweets.?
SSWEu: Tang et al.
(2014b) propose to learnsentiment-specific word embedding (SSWE) from10M tweets collected by emoticons.
They applySSWE as features for Twitter sentiment classifica-tion.?
NRC: NRC builds the state-of-the-art systemin SemEval 2013 Twitter Sentiment Classifica-tion Track, incorporating diverse sentiment lexi-cons and hand-crafted features (Mohammad et al.,2013b).
We re-implement this system because thecodes are not publicly available.
We do not di-rectly report their results in the evaluation task,as our training and development sets are smallerthan their dataset.
In NRC + PF, We concatenatethe NRC features and the phrase embeddings fea-ture (PF), and build the sentiment classifier withLibLinear.Except for DistSuper, other baseline method-s are conducted in a supervised manner.
We donot compare with RNTN (Socher et al., 2013b) be-cause the tweets in our dataset do not have accu-rately parsed results.
Another reason is that, due tothe differences between domains, the performanceof RNTN trained on movie reviews might be de-creased if directly applied on the tweets (Xiao etal., 2013).6.3 Results and AnalysisTable 6 shows the macro-F1 of the baseline sys-tems as well as our joint model (JSC) on senti-ment classification of tweets (positive vs negative).As is shown in Table 6, distant supervision isrelatively weak because the noisy-labeled tweetsare treated as the gold standard, which decreasesthe performance of sentiment classifier.
The resultof bag-of-unigram feature (74.50%) is not satisfiedas it losses the word order and does not well cap-483Method Macro-F1DistSuper + unigram 61.74DistSuper + 5-gram 63.92SVM + unigram 74.50SVM + 5-gram 74.97Recursive Autoencoder 75.42NBSVM 75.28SentiStrength 73.23SSWEu84.98NRC (Top System in SemEval 2013) 84.73NRC + PF 84.75JSC 85.51Table 6: Macro-F1 for positive vs negative classi-fication of tweets.ture the semantic meaning of phrases.
The integra-tion of high-order n-ngram (up to 5-gram) does notachieve significant improvement (+0.47%).
Thereason is that, if a sentence contains a bigram ?notbad?, they will use ?bad?
and ?not bad?
as par-allel features, which confuses the sentiment clas-sification model.
NBSVM and Recursive Autoen-coder perform comparatively and have a big gapin comparison with JSC.
In RAE, the representa-tion of a sentence is composed from the represen-tation of words it contains.
Accordingly, ?great?in ?a great deal of ?
also contributes to the finalsentence representation via composition function.JSC automatically conducts sentence segmenta-tion by considering the sentiment polarity of sen-tence, and utilize the phrasal information from thesegmentations.
Ideally, JSC regards phrases like?not bad?
and ?a great deal of ?
as basic compu-tational units, and yields better classification per-formance.
JSC (85.51%) performs slightly betterthan the state-of-the-art systems (SSWEu, 84.98%;NRC+PF, 84.75%), which verifies its effective-ness.6.4 Comparing Joint and Pipelined ModelsWe compare the proposed joint model withpipelined methods on Twitter sentiment classifi-cation with different feature sets.
Figure 2 givesthe experiment results.
The tick [A, B] on x-axis means the use of A as segmentation featureand the use of B as classification feature.
PFrepresents the phrase-embedding feature; SF andCF stand for the segmentation-specific feature andclassification-specific feature, respectively.
Weuse the bag-of-word segmentation result to buildsentiment classier in Pipeline 1, and use the seg-mentation candidate with maximum phrase num-ber in Pipeline 2.0.70.720.740.760.780.80.820.840.86Macro?F1[PF, PF] [PF+SF, PF] [PF, PF+CF] [PF+SF, PF+CF]Pipeline 1Pipeline 2JointFigure 2: Macro-F1 for positive vs negative classi-fication of tweets with joint and pipelined models.From Figure 2, we find that the joint modelconsistently outperforms pipelined baseline meth-ods in all feature settings.
The reason is thatthe pipelined methods suffer from error propaga-tion, since the errors from linguistic-driven andbag-of-word segmentations cannot be corrected bythe sentiment classification model.
Besides, tra-ditional segmentors do not update the segmenta-tion model with the sentiment information of tex-t.
Unlike pipelined methods, the joint model iscapable to address these problems by optimizingthe segmentation model with the classification re-sults in a joint framework, which yields betterperformance on sentiment classification.
We alsofind that Pipeline 2 always outperforms Pipeline1, which indicates the usefulness of phrase-basedsegmentation for sentiment classification.6.5 Effect of the beam size NWe investigate the influence of beam size N ,which is the maximum number of segmentationcandidates of a sentence.
In this part, we clamp thefeature set as [PF+SF, PF+CF], and vary the beamsize N in [1,2,4,8,16,32,64].
The experiment re-sults of macro-F1 on the development set are il-lustrated in Figure 3 (a).
The time cost of eachtraining iteration is given in Figure 3 (b).From Figure 3 (a), we can see that when larg-er beam size is considered, the classification per-formance is improved.
When beam size is 1, themodel stands for the greedy search with the bag-of-words segmentation.
When the beam size is s-mall, such as 2, beam search losses many phrasalinformation of sentences and thus the improve-ment is not significant.
The performance remainssteady when beam size is larger than 16.
From4841 2 4 8 16 32 640.810.820.830.840.850.86Beam SizeMacro?F1(a) Macro-F1 score for senti-ment classification.1 2 4 8 16 32 64020406080100120Beam SizeRuntime(Second)(b) Time cost (seconds) ofeach training iteration.Figure 3: Sentiment classification of tweets withdifferent beam size N .Figure 3 (b), we can find that the runtime of eachtraining iteration increases with larger beam size.It is intuitive as the joint model with larger beamconsiders more segmentation results, which in-creases the training time of the segmentation mod-el.
We set beam size as 16 after parameter learn-ing.6.6 Effect of the top-ranked segmentationnumber KWe investigate how the top-ranked segmentationnumber K affects the performance of sentimen-t classification.
In this part, we set the feature as[PF+SF, PF+CF], and the beam size as 16.
Theresults of macro-F1 on the development set are il-lustrated in Figure 4.1 3 5 7 9 11 13 150.820.830.840.850.86Top?ranked candidate numberMacro?F1Figure 4: Sentiment classification of tweets withdifferent top-ranked segmentation number K.From Figure 4, we find that the classificationperformance increases with K being larger.
Thereason is that when a larger K is used, (1) at train-ing time, the sentiment classifier is built by usingmore phrasal information from multiple segmen-tations, which benefits from the ensembles; (2) attest time, the joint model considers several top-ranked segmentations and get the final sentimentpolarity through voting.
The performance remain-s stable when K is larger than 7, as the phrasalinformation has been mostly covered.7 ConclusionIn this paper, we develop a joint segmentationand classification framework (JSC) for sentimentanalysis.
Unlike existing sentiment classificationalgorithms that build sentiment classifier basedon the segmentation results from bag-of-words orseparate segmentors, the proposed joint model si-multaneously conducts sentence segmentation andsentiment classification.
We introduce a marginallog-likelihood function to optimize the segmenta-tion model, and effectively train the joint mod-el from sentences annotated only with sentimentpolarity, without segmentation annotations of sen-tences.
The effectiveness of the joint model hasbeen verified by applying it on the benchmarkdataset of Twitter sentiment classification in Se-mEval 2013.
Results show that, the joint modelperforms comparably with state-of-the-art meth-ods, and outperforms pipelined methods in varioussettings.
In the future, we plan to apply the join-t model on other domains, such as movie/productreviews.AcknowledgementsWe thank Nan Yang, Yajuan Duan, YamingSun and Meishan Zhang for their helpful dis-cussions.
We thank the anonymous reviewersfor their insightful comments and feedbacks onthis work.
This research was partly supportedby National Natural Science Foundation of Chi-na (No.61133012, No.61273321, No.61300113).The contact author of this paper, according to themeaning given to this role by Harbin Institute ofTechnology, is Bing Qin.ReferencesYoshua Bengio, Aaron Courville, and Pascal Vincent.2013.
Representation learning: A review and newperspectives.
IEEE Trans.
Pattern Analysis and Ma-chine Intelligence.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet allocation.
the Journal of ma-chine Learning research, 3:993?1022.Yejin Choi and Claire Cardie.
2008.
Learning withcompositional semantics as structural inference forsubsentential sentiment analysis.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, pages 793?801.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.4852011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537.Xiaowen Ding, Bing Liu, and Philip S Yu.
2008.
Aholistic lexicon-based approach to opinion mining.In Proceedings of the International Conference onWeb Search and Data Mining, pages 231?240.Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, MingZhou, and Ke Xu.
2014.
Adaptive recursive neuralnetwork for target-dependent twitter sentiment clas-sification.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistic-s, pages 49?54.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
The Journal ofMachine Learning Research, 9:1871?1874.Ronen Feldman.
2013.
Techniques and application-s for sentiment analysis.
Communications of theACM, 56(4):82?89.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor twitter: Annotation, features, and experiments.In Proceedings of the Annual Meeting of the Associ-ation for Computational Linguistics, pages 42?47.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
Proceed-ings of International Conference on Machine Learn-ing.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.CS224N Project Report, Stanford, pages 1?12.G.E.
Hinton and R.R.
Salakhutdinov.
2006.
Reduc-ing the dimensionality of data with neural networks.Science, 313(5786):504?507.Ming Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDDConference on Knowledge Discoveryand Data Mining, pages 168?177.Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.2013.
Unsupervised sentiment analysis with emo-tional signals.
In Proceedings of the InternationalWorld Wide Web Conference, pages 607?618.Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, andTiejun Zhao.
2011.
Target-dependent twitter sen-timent classification.
The Proceeding of AnnualMeeting of the Association for Computational Lin-guistics.Nicola Jones.
2014.
Computer science: The learningmachines.
Nature, 505(7482):146.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A sentence model based on convolu-tional neural networks.
In Procedding of the 52thAnnual Meeting of Association for ComputationalLinguistics.Hyun Duk Kim and ChengXiang Zhai.
2009.
Gener-ating comparative summaries of contradictory opin-ions in text.
In Proceedings of CIKM 2009.
ACM.Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter sentiment analysis: The goodthe bad and the omg!
In The International AAAIConference on Weblogs and Social Media.Igor Labutov and Hod Lipson.
2013.
Re-embeddingwords.
In Annual Meeting of the Association forComputational Linguistics.John Lafferty, Andrew McCallum, and FernandoPereira.
2001. conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of international con-ference on Machine learning.
ACM.Quoc Le and Tomas Mikolov.
2014.
Distributed repre-sentations of sentences and documents.
Proceedingsof International Conference on Machine Learning.Dong C Liu and Jorge Nocedal.
1989.
On the limitedmemory bfgs method for large scale optimization.Mathematical programming, 45(1-3):503?528.Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis Lectures on Human Language Tech-nologies, 5(1):1?167.Andrew L Maas, Raymond E Daly, Peter T Pham, DanHuang, Andrew Y Ng, and Christopher Potts.
2011.Learning word vectors for sentiment analysis.
InProceedings of the Annual Meeting of the Associ-ation for Computational Linguistics.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013.
Distributed represen-tations of words and phrases and their composition-ality.
Conference on Neural Information ProcessingSystems.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Margaret Mitchell, Jacqui Aguilar, Theresa Wilson,and Benjamin Van Durme.
2013.
Open domain tar-geted sentiment.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1643?1654.Saif M Mohammad and Peter D Turney.
2012.
Crowd-sourcing a word?emotion association lexicon.
Com-putational Intelligence.Saif M Mohammad, Bonnie J Dorr, Graeme Hirst, andPeter D Turney.
2013a.
Computing lexical contrast.Computational Linguistics, 39(3):555?590.486Saif M Mohammad, Svetlana Kiritchenko, and Xiao-dan Zhu.
2013b.
Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets.
Proceedingsof the International Workshop on Semantic Evalua-tion.Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.2010.
Dependency tree-based sentiment classifica-tion using crfs with hidden variables.
In Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 786?794.Alexander Pak and Patrick Paroubek.
2010.
Twitter asa corpus for sentiment analysis and opinion mining.In Proceedings of Language Resources and Evalua-tion Conference, volume 2010.Georgios Paltoglou and Mike Thelwall.
2010.
A s-tudy of information retrieval weighting schemes forsentiment analysis.
In Proceedings of Annual Meet-ing of the Association for Computational Linguistic-s, pages 1386?1395.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 79?86.Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,and Gr?egoire Mesnil.
2014.
Learning semantic rep-resentations using convolutional neural networks forweb search.
In Proceedings of the companion publi-cation of the 23rd international conference on Worldwide web companion, pages 373?374.Richard Socher, J. Pennington, E.H. Huang, A.Y.
Ng,and C.D.
Manning.
2011.
Semi-supervised recur-sive autoencoders for predicting sentiment distribu-tions.
In Conference on Empirical Methods in Nat-ural Language Processing, pages 151?161.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic Com-positionality Through Recursive Matrix-Vector S-paces.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing.Richard Socher, Danqi Chen, Christopher D Manning,and Andrew Y Ng.
2013a.
Reasoning with neu-ral tensor networks for knowledge base completion.The Conference on Neural Information ProcessingSystems.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts.
2013b.
Recursive deep modelsfor semantic compositionality over a sentiment tree-bank.
In Conference on Empirical Methods in Nat-ural Language Processing, pages 1631?1642.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, and Manfred Stede.
2011.
Lexicon-based methods for sentiment analysis.
Computa-tional linguistics, 37(2):267?307.Duyu Tang, Furu Wei, Bing Qin, Ming Zhou, andTing Liu.
2014a.
Building large-scale twitter-specific sentiment lexicon : A representation learn-ing approach.
In Proceedings of COLING 2014,the 25th International Conference on Computation-al Linguistics, pages 172?182.Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, TingLiu, and Bing Qin.
2014b.
Learning sentiment-specific word embedding for twitter sentiment clas-sification.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistic-s, pages 1555?1565.Mike Thelwall, Kevan Buckley, and Georgios Pal-toglou.
2012.
Sentiment strength detection for thesocial web.
Journal of the American Society for In-formation Science and Technology, 63(1):163?173.Peter D Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classi-fication of reviews.
In Proceedings of Annual Meet-ing of the Association for Computational Linguistic-s, pages 417?424.Sida Wang and Christopher D Manning.
2012.
Base-lines and bigrams: Simple, good sentiment and topicclassification.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguistic-s, pages 90?94.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 347?354.Min Xiao, Feipeng Zhao, and Yuhong Guo.
2013.Learning latent word representations for domainadaptation using supervised word clustering.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 152?162, October.Ainur Yessenalina and Claire Cardie.
2011.
Compo-sitional matrix-space models for sentiment analysis.In Proceedings of Conference on Empirical Methodsin Natural Language Processing, pages 172?182.Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, andChengqing Zong.
2014.
Bilingually-constrainedphrase embeddings for machine translation.
In Pro-ceedings of the 52nd Annual Meeting of the Associa-tion for Computational Linguistics, pages 111?121.Jichang Zhao, Li Dong, Junjie Wu, and Ke Xu.
2012.Moodlens: an emoticon-based sentiment analysissystem for chinese tweets.
In Proceedings of the18th ACM SIGKDD international conference onKnowledge discovery and data mining.487
