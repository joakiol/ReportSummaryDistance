Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 153?157, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsIIITH: A Corpus-Driven Co-occurrence Based Probabilistic Model forNoun Compound ParaphrasingNitesh Surtani, Arpita Batra, Urmi Ghosh and Soma PaulLanguage Technologies Research CentreIIIT HyderabadHyderabad, Andhra Pradesh-500032{nitesh.surtaniug08, arpita.batra, urmi.ghosh}@students.iiit.ac.in, soma@iiit.ac.inAbstractThis paper presents a system for automaticallygenerating a set of plausible paraphrases for agiven noun compound and rank them in de-creasing order of their usage represented bythe confidence value provided by the humanannotators.
Our system implements a corpus-driven probabilistic co-occurrence basedmodel for predicting the paraphrases, that usesa seed list of paraphrases extracted from cor-pus to predict other paraphrases based on theirco-occurrences.
The corpus study reveals thatthe prepositional paraphrases for the nouncompounds are quite frequent and well cov-ered but the verb paraphrases, on the otherhand, are scarce, revealing the unsuitability ofthe model for standalone corpus-driven ap-proach.
Therefore, to predict other paraphras-es, we adopt a two-fold approach: (i)Prediction based on Verb-Verb co-occurrences, in case the seed paraphrases aregreater than threshold; and (ii) Predictionbased on Semantic Relation of NC, otherwise.The system achieves a comparabale score of0.23 for the isomorphic system while main-taining a score of 0.26 for the non-isomorphicsystem.1 IntroductionSemeval 2013 Task 4 (Hendrickx et.
al., 2013),?Free Paraphrases of Noun Compounds?
is a pa-raphrase generation task that requires the system togenerate multiple paraphrases for a given nouncompound and rank them to the best approxima-tion of the human rankings, represented by the cor-responding confidence value.
The task is anextension of Semeval 2010 Task 9 (Butnariu et al2010), where the participants were asked to rankthe set of given paraphrases for each noun com-pound.
Although the ranking task is quite distinctfrom the task of generating paraphrases, however,we have taken many insights from the systems de-veloped for the ranking task, and have reportedthem appropriately in our system description.This paper describes a system for generating aranked set of paraphrases for a given NC.
A pa-raphrase can be Prepositional, Verb or Verb + Pre-positional.
Since the prepositional paraphrases areeasily available in the corpus while the occurrencesof verb or verb+prep paraphrases is scarce, the taskof paraphrasing becomes significant in finding outa method for predicting reliable paraphrases withverbs for a given NC.
Our system implements amodel that is based on co-occurrences of the pa-raphrases and selects those paraphrases that have ahigher probability of co-occurring with a set ofextracted paraphrases which are referred to as SeedParaphrases.
Keeping the verb-paraphrase scarcityissue in mind, we develop a two-way model: (i)Model 1 is used when the seed paraphrases areconsiderable in number i.e., greater than the thre-shold value.
In this case, other verb paraphrases arepredicted based on their co-occurrence with the setof extracted verb paraphrases.
(ii) Model 2 is usedwhen the size of the seed list falls below the thre-shold value, in which case, we make use of theprepositional paraphrases to predict the relation ofthe noun compound and select verbs that mostlyco-occur with that relation.
Our system achieves anisomorphic score of 0.23 with a non-isomorphic of0.26 with the human generated paraphrases.
Thenext section discusses the system.2 System DescriptionThis section of the paper describes each module ofthe system in detail.
The first module of the system153talks about the Seed data extraction using corpussearch.
The next module uses the seed data forpredicting more verbs that would be used in pa-raphrasing.
The third module uses these predictedverbs in template generation for generating NCParaphrasing and the generated paraphrases areranked in the last module.2.1 Seed Data Extraction ModuleWe have relied mostly on the Google N-gram Cor-pus for extracting the seed paraphrases.
Google haspublicly released their web data as n-grams, alsoknown as Web-1T corpus, via the Linguistic DataConsortium (Brants and Franz, 2006).
It containssequences of n-terms that occur more than 40 timeson the web.
Since the corpus consists of raw datafrom the web, certain pre-processing steps are es-sential before it can be used.
We extract a set ofPOS templates from the training data, and general-ize them enough to accommodate the legitimateparaphrases extracted from the corpus.
The follow-ing templates are used for extracting n-gram data:Head-Mod N-gram: This template includes boththe head and the modifier in the same regular ex-pression.
A corresponding 5-gram template for aNC Amateur-Championship is shown in Table 1.Head <*> <*><*>Modchampionship conducted for theamateursHead <*><*>Mod <*>championship for all amateurplayersHead <*>Mod<*><*>championship where amateur iscompetingTable 1: Templates for paraphrase extractionThe paraphrases obtained from the above templateare quite useful, but scarce.
To overcome the issueof coverage of verb paraphrases, a loosely coupledanalysis and representation of compounds can beemployed, as suggested by (Li et.al, 2010).
Weretrieve the partial triplets from the n-gram corpusin the form of ?Head Para?
and ?Para Modifier?.Head Template: Head <*> <*>Mod Template: <*> <*> Mod; <*> Mod <*>But the process of generating paraphrases fromhead and the modifier n-gram incorporates a hugeamount of noise and produces a lot of irrelevantparaphrases.
Therefore, these partial paraphrasesare not directly used for generating the paraphrasesbut are instead used to diagnose the compatibilityof the selected verb with the head and the modifierof the given NC in Section 2.2.2.
We also extractparaphrases from ANC and BNC corpus.2.2 Verb Prediction ModuleThis module is the heart of our system.
It imple-ments two models for predicting the verb paraph-rases: a Verb Co-occurrence model and a RelationPrediction model.
The decision of selection ofmodel for verb prediction is based on the size ofthe seed list.
If the number of seed paraphrases isabove the threshold value, the verb co-occurrencemodel is used whereas the relation prediction mod-el is used if it is below the threshold value.2.2.1 Verb Co-occurrence ModelThis model uses the seed paraphrases extractedfrom the corpus to predict other verb paraphrasesby computing their co-occurrences.
The modelgains insights from the UCD-PN system (Nultyand Costello, 2010) which tries to identify a moregeneral paraphrase by computing the co-occurrence of a paraphrase with other paraphrases.But the task of generating paraphrases has two sub-tle but significant differences: (i) The list of seedverb paraphrases for a given NC is usually small,with each seed verb having a corresponding proba-bility of occurrence; and (ii) Not all the seed verbshave legitimate representation of the noun com-pound.
Our system incorporates these distinctionsin the co-occurrence model discussed below.Using the training data at hand, we build a Verb-Verb co-occurrence matrix, a 2-D matrix whereeach cell (i,j) represents the probability of occur-rence of Vj when Vi has already occurred.?
??
??
=?(??
,??
)?(??)=?????(??
,??
)?????(??
)The verbs used in co-occurrence matrix are storedin a List A.
Now, for a given test NC, the modelextracts the seed list of verb paraphrases (referredas List B) from the corpus with their correspondingprobabilities.
The above model calculates a scorefor each verb in List A, by computing its co-occurrence with the verbs in List B.????????
??
=  ?
??
??
?
?(??)???
(Head, Para, ?
)(?, Para, Mod)(Head, Para, Mod)154The term ?(??)
in the above equation representsthe relative occurrence of the verb ??
with the giv-en NC.
The relevance of this term becomes evidentin the next model.
The verbs achieving higherscore are selected, suggesting a higher probabilityof co-occurrence with the seed verbs.2.2.2 Semantic Relation Prediction ModelThis module describes the second model of thetwo-way model, and is used by the system whenthe verbs extracted from the corpus are less thanthe threshold.
In this model, we use prepositionalparaphrases, having a pretty good coverage in thecorpus, to predict the semantic relation of the com-pound which helps us in predicting the other pa-raphrases.
The intuition behind using semanticclass for predicting paraphrases is that they tend tocapture the behavior of the noun compound andcan be represented by general paraphrases.Noun Compound Relation Paraphrase Sel.Prep VerbGarden Party Location In, At HeldCommunity Life Theme Of, In MadeAdvertising Agency Purpose For, Of, In DoingTable 2: Occurrence of Prepositional ParaphrasesRelation Annotation: Since a supervised ap-proach is used for identifying the semantic relationof the noun compound, we manually annotate thenoun compounds with a semantic relation.
We tageach noun compound with one semantic relationfrom the set used in (Moldovan et.
al.
2004).Prep-Rel and Verb-Rel Co-occurrence: A Prep-Rel co-occurrence matrix similar to Verb-Verb co-occurrence matrix discussed in last subsection.This 2-D matrix consists of co-occurrence proba-bilities between the prepositional paraphrases andthe semantic relation of the compound, where eachcell (i,j) represents the probability of occurrence ofpreposition Pj with relation Ri.
This matrix is usedas a model to identify semantic relation using pre-positional paraphrases extracted from the corpus.The Verb-Relation co-occurrence matrix is used topredict the most co-occurring verbs with the identi-fied relation.
Each cell (i,j) in the matrix representsthe probability of the verb Vj co-occurring withrelation Ri.Relation Extraction: Research focusing on se-mantic relation extraction has followed two direc-tions: (i) Statistical approaches to using very largecorpus (Berland and Charniak (1999); Hearst(1998)); and (ii) Ontology based approaches usinghierarchical structure of wordnet (Moldovan et.
al.,2004).
We employ a statistical model based on thePreposition-Relation co-occurrence for identifyingthe relation.
The model is quite similar to the oneused in Section 2.2, but it is here that the modelreveals its actual power.
Since two or more rela-tions can be represented by same set of preposi-tional paraphrases, as Theme and Purpose in Table2, it is important to take into account the probabili-ties with which the extracted prepositions occur inthe corpus.
In Table 2, the NC Community Life(Theme) occurs frequently with preposition ?of?whereas the NC Advertising Agency (Purpose) ismostly represented by preposition ?for?
in the cor-pus.
The term ?(??)
in the equation below cap-tures this phenomenon and classifies these twoNCs in their respective classes.????????
?
=  ?
?
??
?
?(??)??
?The relation with the highest score is selected asthe semantic class of the noun compound.
A set ofverbs highly co-occurring with that class are se-lected, and their compatibility with the correspond-ing noun compound is judged from theiroccurrences with the partial head and the modifierparaphrases as discussed in Section 2.1.
The aboveclassifier performs moderately and classifies a giv-en NC with 42.5% accuracy.
We have also triedthe Wordnet based Semantic Scattering model(Moldovan et.
al., 2004), trained on a set of 400instances, but achieved an accuracy of 38%, thereason for which can be attributed to the smalltraining set.
Since the accuracy of identifying thecorrect relation is low, we select some paraphrasesfrom the 2nd most probable relation, as assigned bythe probabilistic classifier.2.3 Paraphrase Generator ModuleAfter predicting a set of verb for a test noun com-pound, we use the following templates to generatethe paraphrases:a) Head VP Modb) Head VP PP Modc) Head [that|which] VP PP ModThe paraphrases that are extracted from the corpusare also cleaned using the POS templates extractedfrom the training data.1552.4 Paraphrase Ranker ModuleMotivated by the observations from Nulty andCostello (2010) that ?people tend to use general,semantically light paraphrases more often than de-tailed, semantically heavy ones?, we perform rank-ing of the paraphrases in two steps: (i) Assigningdifferent weights to different type of paraphrases,i.e.
a light weight prepositional paraphrases achiev-ing higher score than the verb paraphrases; and (ii)Ranking a more general paraphrase with the samecategory higher.
A paraphrase A is more generalthat paraphrase B (Nulty and Costello, 2010) if?
?|?
> ?(?|?
)For a list of paraphrases A generated for a givencompound, each paraphrase b in that list is scoredusing the below eq., where more general paraph-rase achieves a high score and is ranked higher.?????
?
=  ?
?
???
?The seed paraphrases extracted from the corpus areranked higher than the predicted paraphrases.3 AlgorithmThis section presents the implementation of theoverall system.4 ResultsThe set of generated paraphrases are evaluated ontwo metrics: a) Isomorphic; b) Non-isomorphic.
Inthe isomorphic setting, the test paraphrase ismatched to the closest reference paraphrases, butthe reference paraphrase is removed from the setwhereas in non-isomorphic setting, the referenceparaphrase which is mapped to a test paraphrasecan still be used for matching other test paraphras-es.
Table 3 presents the scores of the 3 participat-ing teams who have submitted total of 4 systems.Systems Isomorphic Non-IsomorphicSFS 0.2313 0.1794IIITH 0.2309 0.2583MELODI-Pri 0.1298 0.5484MELODI-Cont 0.1357 0.536Table 3: Results of the submitted systemsOur system achieves an isomorphic score of 0.23,just below the SFS system maintaining a score of0.26 for the non-isomorphic system.
The two va-riants of MELODI system get a high score for thenon-isomorphic metric but low scores for isomor-phic metric as compared to other systems.5 ConclusionWe have described a system for automatically ge-nerating a set of paraphrases for a given nouncompound, based on the co-occurrences of the pa-raphrases.
The system describes an approach forhandling those 38% cases (calculated for optimumthreshold value) of NCs where it is not convenientto predict the verbs using their co-occurrences withthe seed verbs, because the size of the seed list isbelow a threshold value.
For other cases, the verbco-occurrence model is used to predict the verbsfor NC paraphrasing.
The optimum value of thre-shold parameter investigated from experiments isfound to be 3, showing that atleast 3 verb paraph-rases are necessary to capture the concept of a NC.// Training Phase ?
Build Co-occurrence MatricesVerb_Co-occur = 2-D MatrixPrep-Rel_Co-occur = 2-D MatrixVerb-Rel_Co-occur = 2-D MatrixVerb_List = Verb List extracted from training corpus// Testing ?
Extract paraphrases with probabilitiesExt_Verb = List of extracted verb paraphraseVProb = Probability of each Ext_VerbExt_Prep = List of extracted prepositional paraphrasesPProb = Probability of each Ext_PrepProb_Verb = List // Verbs with their selection scoreProb_Rel = List // Relations with their selection scoreThreshold = 3 // Verb threshold for two-way modelif count( Ext_Verb ) > ThresholdCandidate_Verbs = {Verb_List } - { Ext_Verbs }foreach Candidate_Verbs Vi :Prob_Verb[Vi] = 0foreach Ext_Verb Vj :Prob_Verb[Vi] += Verb_Co-occur [Vi][Vj] *VProb[Vj]elseforeach Prep-Rel_Co-occur as rel :Prob_Rel[rel] = 0foreach Ext_Prep as prep :Prob_Rel[rel] += Prep-Rel_Co-occur[rel][prep]* PProb[prep]Rel=select highestProb(Prob_Rel)Prob_Verb = Verb-Rel_Co-occur[Rel]sort(Prob_Verb)Verb_Predicted = select top(N)Paraphrase = generate_paraphrase(verb_predicted)rank(Paraphrase)156ReferencesMatthew Berland and Eugene Charniak.
1999.
Findingparts in very large.
In Proceeding of ACL 1999T.
Brants and A. Franz.
2006.
Web 1T 5-gram Version1.Linguistic Data ConsortiumCristina Butnariu, Su Nam Kim, Preslav Nakov, Di-armuid O S?
eaghdha, Stan Szpakowicz, and Tony-Veale.
2010.
Semeval-2 task 9: The interpreta-tion ofnoun compounds using paraphrasing verbs and pre-positions.
In Proceedings of the 5th SIGLEX Work-shop on Semantic EvaluationCristina Butnariu, Su Nam Kim, Preslav Nakov, Di-armuid O S?eaghdha, Stan Szpakowicz, and Tony-Veale.
2013.
Semeval?13 task 4: Free Paraphrases ofNoun Compounds.
In Proceedings of the Internation-al Workshop on Semantic Evaluation, Atlanta, Geor-giaMarti Hearst.
1998.
Automated Discovery of Word-Netrelations.
In An Electronic Lexical Database and-Some of its Applications.
MIT Press, Cambridge MAMark Lauer.
1995.
Designing Statistical Language-Learners: Experiments on Noun Compounds.
Ph.D.Thesis, Macquarie UniversityGuofu Li, Alejandra Lopez-Fernandez and Tony Veale.2010.
UCD-Goggle: A Hybrid System for NounCompound Paraphrasing.
In Proceedings of the 5thInternational Workshop on Semantic Evaluation(SemEval-2), Uppsala, SwedenDan Moldovan, Adriana Badulescu, Marta Tatu, DanielAntohe, and Roxana Girju.
2004.
Models for the Se-mantic Classification of Noun Phrases.
In Proceed-ings of the HLT-NAACL-04 Workshop onComputational Lexical Semantics, pages 60?67, Bos-ton, MAPaul Nulty and Fintan Costello.
2010.
UCD-PN: Select-ing general paraphrases using conditional probabili-ty.
In Proceedings of the 5th International Workshopon Semantic Evaluation (SemEval-2), Uppsala, Swe-den157
