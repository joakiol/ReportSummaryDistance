An Improved Parser for Data-Oriented Lexical-Functional AnalysisRens BodInformatics Research Institute, University of Leeds, Leeds LS2 9JT, UK, &Institute for Logic, Language and Computation, University of Amsterdamrens@scs.leeds.ac.ukAbstractWe present an LFG-DOP parser which usesfragments from LFG-annotated sentences to parsenew sentences.
Experiments with the Verbmobiland Homecentre corpora show that (1) Viterbi nbest search performs about 100 times faster thanMonte Carlo search while both achieve the sameaccuracy; (2) the DOP hypothesis which statesthat parse accuracy increases with increasing frag-ment size is confirmed for LFG-DOP; (3) LFG-DOP's relative frequency estimator performsworse than a discounted frequency estimator; and(4) LFG-DOP significantly outperforms Tree-DOP if evaluated on tree structures only.1  IntroductionData-Oriented Parsing (DOP) models learn howto provide linguistic representations for anunlimited set of utterances by generalizing from agiven corpus of properly annotated exemplars.They operate by decomposing the givenrepresentations into (arbitrarily large) fragmentsand recomposing those pieces to analyze newutterances.
A probability model is used to choosefrom the collection of different fragments ofdifferent sizes those that make up the mostappropriate analysis of an utterance.DOP models have been shown to achievestate-of-the-art parsing performance onbenchmarks such as the Wall Street Journalcorpus (see Bod 2000a).
The original DOP modelin Bod (1993) was based on utterance analysesrepresented as surface trees, and is equivalent to aStochastic Tree-Substitution Grammar.
But themodel has also been applied to several othergrammatical frameworks, e.g.
Tree-InsertionGrammar (Hoogweg 2000), Tree-AdjoiningGrammar (Neumann 1998), Lexical-FunctionalGrammar (Bod & Kaplan 1998; Cormons 1999),Head-driven Phrase Structure Grammar(Neumann & Flickinger 1999), and MontagueGrammar (Bonnema et al 1997; Bod 1999).Most probability models for DOP use the relativefrequency estimator to estimate fragmentprobabilities, although Bod (2000b) trainsfragment probabilities by a maximum likelihoodreestimation procedure belonging to the class ofexpectation-maximization algorithms.
The DOPmodel has also been tested as a model for humansentence processing (Bod 2000d).This paper presents ongoing work onDOP models for Lexical-Functional Grammarrepresentations, known as LFG-DOP (Bod &Kaplan 1998).
We develop a parser which usesfragments from LFG-annotated sentences to parsenew sentences, and we derive some experimentalproperties of LFG-DOP on two LFG-annotatedcorpora: the Verbmobil and Homecentre corpus.The experiments show that the DOP hypothesis,which states that there is an increase in parseaccuracy if larger fragments are taken into account(Bod 1998), is confirmed for LFG-DOP.
Wereport on an improved search technique forestimating the most probable analysis.
While aMonte Carlo search converges provably to themost probable parse, a Viterbi n best searchperforms as well as Monte Carlo while itsprocessing time is two orders of magnitude faster.We also show that LFG-DOP outperforms Tree-DOP if evaluated on tree structures only.2  Summary of LFG-DOPIn accordance with Bod (1998), a particular DOPmodel is described by?
a definition of a well-formed representation forutterance analyses,?
a set of decomposition operations  that divide agiven utterance analysis into a set of fragments,?
a set of composition operations  by which suchfragments may be recombined to derive ananalysis of a new utterance, and?
a definition of a probability model that indicateshow the probability of a new utterance analysis iscomputed.In defining a DOP model for LFGrepresentations, Bod & Kaplan (1998) give thefollowing settings for DOP's four parameters.2.1  RepresentationsThe representations used by LFG-DOP aredirectly taken from LFG: they consist of a c-structure, an f-structure and a mapping ?
betweenthem.
The following figure shows an examplerepresentation for Kim eats.
(We leave out somefeatures to keep the example simple.
)SNP VPKim eatsPRED   'Kim'NUM	    SGSUBJTENSE      PRESPRED       'eat(SUBJ)'Figure 1Bod & Kaplan also introduce the notion ofaccessibility which they later use for defining thedecomposition operations of LFG-DOP:An f-structure unit f is ?-accessible from a noden iff either n is ?-linked to f (that is, f = ?
(n) )or f is contained within ?
(n) (that is, there is achain of attributes that leads from ?
(n) to f).According to the LFG representation theory, c-structures and f-structures must satisfy certainformal well-formedness conditions.
A c-structure/f-structure pair is a valid LFG represent-ation only if it satisfies the NonbranchingDominance, Uniqueness, Coherence and Com-pleteness conditions (Kaplan & Bresnan 1982).2.2  Decomposition operations and FragmentsThe fragments for LFG-DOP consist ofconnected subtrees whose nodes are in ?-correspondence with the correponding sub-unitsof f-structures.
To give a precise definition ofLFG-DOP fragments, it is convenient to recall thedecomposition operations employed by theorginal DOP model which is also known as the"Tree-DOP" model (Bod 1993, 1998):(1)  Root: the Root operation selects any node ofa tree to be the root of the new subtree anderases all nodes except the selected node and thenodes it dominates.
(2)  Frontier : the Frontier operation thenchooses a set (possibly empty) of nodes in thenew subtree different from its root and erases allsubtrees dominated by the chosen nodes.Bod & Kaplan extend Tree-DOP's Root andFrontier  operations so that they also apply to thenodes of the c-structure in LFG, while respectingthe principles of c/f-structure correspondence.When a node is selected by the Rootoperation, all nodes outside of that node's subtreeare erased, just as in Tree-DOP.
Further, forLFG-DOP, all ?
links leaving the erased nodesare removed and all f-structure units that are not?-accessible from the remaining nodes are erased.For example, if Root selects the NP in figure 1,then the f-structure corresponding to the S node iserased, giving figure 2 as a possible fragment:NPKimPRED   'Kim'NUM	    SGFigure 2In addition the Root operation deletes from theremaining f-structure all semantic forms that arelocal to f-structures that correspond to erased c-structure nodes, and it thereby also maintains thefundamental two-way connection between wordsand meanings.
Thus, if Root selects the VP nodeso that the NP is erased, the subject semanticform "Kim" is also deleted:VPeatsNUM	    SG  SUBJTENSE      PRESPRED      'eat(SUBJ)'Figure 3As with Tree-DOP, the Frontier operation thenselects a set of frontier nodes and deletes allsubtrees they dominate.
Like Root , it alsoremoves the ?
links of the deleted nodes anderases any semantic form that corresponds to anyof those nodes.
For instance, if the NP in figure 1is selected as a frontier node, Frontier erases thepredicate "Kim" from the fragment:eatsSNP VPNUM	   SG  SUBJTENSE      PRESPRED      'eat(SUBJ)'Figure 4Finally, Bod & Kaplan present a thirddecomposition operation, Discard , defined toconstruct generalizations of the fragmentssupplied by Root and Frontier .
Discard acts todelete combinations of attribute-value pairssubject to the following condition: Discard doesnot delete pairs whose values ?
-correspond toremaining c-structure nodes.
According to Bod &Kaplan (1998), Discard-generated fragments areneeded to parse sentences that are "ungrammaticalwith respect to the corpus", thus increasing therobustness of the model.2.3  The composition operationIn LFG-DOP the operation for combiningfragments is carried out in two steps.
First the c-structures are combined by leftmost substitutionsubject to the category-matching condition, as inTree-DOP.
This is followed by the recursiveunification of the f-structures corresponding to thematching nodes.
A derivation for an LFG-DOPrepresentation R  is a sequence of fragments thefirst of which is labeled with S and for which theiterative application of the composition operationproduces R .
For an illustration of the compositionoperation, see Bod & Kaplan (1998).2.4  Probability modelsAs in Tree-DOP, an LFG-DOP representation Rcan typically be derived in many different ways.
Ifeach derivation D has a probability P(D), then theprobability of deriving R  is the sum of theindividual derivation probabilities:(1) P(R)  =  ?D derives R P(D)An LFG-DOP derivation is produced by astochastic process which starts by randomlychoosing a fragment whose c-structure is labeledwith the initial category.
At each subsequent step,a next fragment is chosen at random from amongthe fragments that can be composed with thecurrent subanalysis.
The chosen fragment iscomposed with the current subanalysis to producea new one; the process stops when an analysisresults with no non-terminal leaves.
We will callthe set of composable fragments at a certain stepin the stochastic process the competition set at thatstep.
Let CP( f | CS) denote the probability ofchoosing a fragment f from a competition set CScontaining f, then the probability of a derivation D= < f1, f2 ... fk> is(2) P(< f1, f2 ... fk>)  =  ?i CP(f i | CSi)where the competition probability  CP(f | CS) isexpressed in terms of fragment probabilities P( f):?f'?CS  P(f')P( f)  (3) CP(f | CS)  =Bod & Kaplan give three definitions of increasingcomplexity for the competition set: the firstdefinition groups all fragments that only satisfythe Category-matching condition of thecomposition operation; the second definitiongroups all fragments which satisfy both Category-matching and Uniqueness; and the third definitiongroups all fragments which satisfy Category-matching, Uniqueness and Coherence.
Bod &Kaplan point out that the Completeness conditioncannot be enforced at each step of the stochasticderivation process, and is a property of the finalrepresentation which can only be enforced bysampling valid representations from the output ofthe stochastic process.
In this paper, we will onlydeal with the third definition of competition set, asit selects only those fragments at each derivationstep that may finally result into a valid LFGrepresentation, thus reducing the off-line validitychecking to the Completeness condition.Note that the computation of thecompetition probability in the above formulas stillrequires a definition for the fragment probabilityP(f).
Bod and Kaplan define the probability of afragment simply as its relative frequency in thebag of all fragments generated from the corpus,just as in most Tree-DOP models.
We will referto this fragment estimator as "simple relativefrequency" or "simple RF".We will also use an alternative definitionof fragment probability which is a refinement ofsimple RF.
This alternative fragment probabilitydefinition distinguishes between fragmentssupplied by Root/Frontier  and fragmentssupplied by Discard .
We will treat the first typeof fragments as seen events, and the second typeof fragments as previously unseen events.
Wethus create two separate bags corresponding totwo separate distributions: a bag with fragmentsgenerated by Root and Frontier , and a bag withfragments generated by Discard.
We assignprobability mass to the fragments of each bag bymeans of discounting : the relative frequencies ofseen events are discounted and the gainedprobability mass is reserved for the bag of unseenevents (cf.
Ney et al 1997).
We accomplish thisby a very simple estimator: the Turing-Goodestimator (Good 1953) which computes theprobability mass of unseen events as n1/N wheren1 is the number of singleton events and N is thetotal number of seen events.
This probabilitymass is assigned to the bag of Discard-generatedfragments.
The remaining mass (1 ?
n1/N ) isassigned to the bag of Root/Frontier-generatedfragments.
The probability of each fragment isthen computed as its relative frequency in its bagmultiplied by the probability mass assigned to thisbag.
Let | f | denote the frequency of a fragment f,then its probability is given by:| f |?f': f' is generated by Root/Frontier | f'|(1 ?
n1/N)(4) P(f | f is generated by Root/Frontier)  =(5) P(f | f is generated by Discard)  =(n1/N)| f |?f': f' is generated by Discard | f'|We will refer to this fragment probabilityestimator as "discounted relative frequency" or"discounted RF".4  Parsing with LFG-DOPIn his PhD-thesis, Cormons (1999) presents aparsing algorithm for LFG-DOP which is basedon the Tree-DOP parsing technique described inBod (1998).
Cormons first converts LFG-representations into more compact indexed trees:each node in the c-structure is assigned an indexwhich refers to the ?-corresponding f-structureunit.
For example, the representation in figure 1 isindexed as(S.1 (NP.2 Kim.2)(VP.1 eats.1))where1 --> [  (SUBJ = 2)(TENSE = PRES)(PRED = eat(SUBJ)) ]2 --> [  (PRED = Kim)(NUM = SG) ]The indexed trees are then fragmented byapplying the Tree-DOP decomposition operationsdescribed in section 2.
Next, the LFG-DOPdecomposition operations Root, Frontier  andDiscard  are applied to the f-structure units thatcorrespond to the indices in the c-structuresubtrees.
Having obtained the set of LFG-DOPfragments in this way, each test sentence is parsedby a bottom-up chart parser using initially theindexed subtrees only.Thus only the Category-matchingcondition is enforced during the chart-parsingprocess.
The Uniqueness and Coherenceconditions of the corresponding f-structure unitsare enforced during the disambiguation or chart -decoding process.
Disambiguation isaccomplished by computing a large number ofrandom derivations from the chart and byselecting the analysis which results most oftenfrom these derivations.
This technique is knownas "Monte Carlo disambiguation" and has beenextensively described in the literature (e.g.
Bod1993, 1998; Chappelier & Rajman 2000;Goodman 1998; Hoogweg 2000).
Sampling arandom derivation from the chart consists ofchoosing at random one of the fragments fromthe set of composable  fragments at every labeledchart-entry (where the random choices at eachchart-entry are based on the probabilities of thefragments).
The derivations are sampled in a top-down, leftmost order so as to maintain the LFG-DOP derivation order.
Thus the competition setsof composable fragments are computed on the flyduring the Monte Carlo sampling process bygrouping the f-structure units that unify and thatare coherent with the subderivation built so far.As mentioned in section 3, theCompleteness condition can only be checked afterthe derivation process.
Incomplete derivations aresimply removed from the sampling distribution.After sampling a sufficiently large number ofrandom derivations that satisfy the LFG validityrequirements, the most probable analysis isestimated by the analysis which results most oftenfrom the sampled derivations.
As a stop conditionon the number of sampled derivations, wecompute the probability of error, which is theprobability that the analysis that is most frequentlygenerated by the sampled derivations is not equalto the most probable analysis, and which is set to0.05 (see Bod 1998).
In order to rule out thepossibility that the sampling process never stops,we use a maximum sample size of 10,000derivations.While the Monte Carlo disambiguationtechnique converges provably to the mostprobable analysis, it is quite inefficient.
It ispossible to use an alternative, heuristic searchbased on Viterbi n  best (we will not go into thePCFG-reduction technique presented in Goodman(1998) since that heuristic only works for Tree-DOP and is beneficial only if all subtrees aretaken into account and if the so-called "labeledrecall parse" is computed).
A Viterbi n  best searchfor LFG-DOP estimates the most probableanalysis by computing n  most probablederivations, and by then summing up theprobabilities of the valid derivations that producethe same analysis.
The algorithm for computing nmost probable derivations follows straight-forwardly from the algorithm which computes themost probable derivation by means of Viterbioptimization (see e.g.
Sima'an 1999).5  Experimental EvaluationWe derived some experimental properties ofLFG-DOP by studying its behavior on the twoLFG-annotated corpora that are currentlyavailable: the Verbmobil corpus and theHomecentre corpus.
Both corpora were annotatedat Xerox PARC.
They contain packed LFG-representations (Maxwell & Kaplan 1991) of thegrammatical parses of each sentence together withan indication which of these parses is the correctone.
For our experiments we only used the correctparses of each sentence resulting in 540Verbmobil parses and 980 Homecentre parses.Each corpus was divided into a 90% training setand a 10% test set.
This division was randomexcept for one constraint: that all the words in thetest set actually occurred in the training set.
Thesentences from the test set were parsed anddisambiguated by means of the fragments fromthe training set.
Due to memory limitations, werestricted the maximum depth of the indexedsubtrees to 4.
Because of the small size of thecorpora we averaged our results on 10 differenttraining/test set splits.
Besides an exact matchaccuracy metric, we also used a more fine-grainedscore based on the well-known PARSEVALmetrics that evaluate phrase-structure trees (Blacket al 1991).
The PARSEVAL metrics compare aproposed parse P with the corresponding correcttreebank parse T  as follows:Precision =# correct constituents in P# constituents in P# correct constituents in P# constituents in TRecall =A constituent in P  is correct if there exists aconstituent in T of the same label that spans thesame words and that ?-corresponds to the samef-structure unit (see Bod 2000c for someillustrations of these metrics for LFG-DOP).5.1 Comparing the two fragment estimatorsWe were first interested in comparing theperformance of the simple RF estimator againstthe discounted RF estimator.
Furthermore, wewant to study the contribution of generalizedfragments to the parse accuracy.
We thereforecreated for each training set two sets of fragments:one which contains all fragments (up to depth 4)and one which excludes the generalized fragmentsas generated by Discard .
The exclusion of theseDiscard-generated fragments means that allprobability mass goes to the fragments generatedby Root and Frontier  in which case the twoestimators are equivalent.
The following twotables present the results of our experimentswhere +Discard refers to the full set of fragmentsand ?Discard refers to the fragment set withoutDiscard-generated fragments.Exact Match         Precision   Recall+Discard  ?Discard +Discard  ?Discard +Discard  ?DiscardSimple RF 1.1%       35.2% 13.8%        76.0% 11.5%       74.9%35.9%        35.2% 77.5%        76.0% 76.4%       74.9%Discounted RFEstimatorTable 1.
Experimental results on the VerbmobilExact Match         Precision   Recall+Discard  ?Discard +Discard  ?Discard +Discard  ?Discard2.7%       37.9% 17.1%        77.8% 15.5%       77.2%38.4%        37.9% 80.0%        77.8% 78.6%       77.2%Simple RFDiscounted RFEstimatorTable 2.
Experimental results on the HomecentreThe tables show that the simple RF estimatorscores extremely bad if all fragments are used: theexact match is only 1.1% on the Verbmobilcorpus and 2.7% on the Homecentre corpus,whereas the discounted RF estimator scoresrespectively 35.9% and 38.4% on these corpora.Also the more fine-grained precision and recallscores obtained with the simple RF estimator arequite low: e.g.
13.8% and 11.5% on theVerbmobil corpus, where the discounted RFestimator obtains 77.5% and 76.4%.
Interestingly,the accuracy of the simple RF estimator is muchhigher if Discard-generated fragments areexcluded.
This suggests that treating generalizedfragments probabilistically in the same way asungeneralized fragments is harmful.The tables also show that the inclusion ofDiscard-generated fragments leads only to aslight accuracy increase under the discounted RFestimator.
Unfortunately, according to paired t -testing only the differences for the precisionscores on the Homecentre corpus werestatistically significant.5.2 Comparing different fragment sizesWe were also interested in the impact of fragmentsize on the parse accuracy.
We thereforeperformed a series of experiments where thefragment set is restricted to fragments of a certainmaximum depth (where the depth of a fragmentis defined as the longest path from root to leaf ofits c-structure unit).
We used the sametraining/test set splits as in the previousexperiments and used both ungeneralized andgeneralized fragments together with thediscounted RF estimator.Fragment Depth Exact Match Precision Recall1 30.6% 74.2% 72.2%?2 34.1% 76.2% 74.5%?3 35.6% 76.8% 75.9%?4 35.9% 77.5% 76.4%Table 3.
Accuracies on the VerbmobilFragment Depth Exact Match Precision Recall1 31.3% 75.0% 71.5%?2 36.3% 77.1% 74.7%?3 37.8% 77.8% 76.1%?4 38.4% 80.0% 78.6%Table 4.
Accuracies on the HomecentreTables 3 and 4 show that there is a consistentincrease in parse accuracy for all metrics if largerfragments are included, but that the increase itselfdecreases.
This phenomenon is also known as theDOP hypothesis (Bod 1998), and has beenconfirmed for Tree-DOP on the ATIS, OVIS andWall Street Journal treebanks (see Bod 1993,1998, 1999, 2000a; Sima'an 1999; Bonnema et al1997; Hoogweg 2000).
The current result thusextends the validity of the DOP hypothesis toLFG annotations.
We do not yet know whetherthe accuracy continues to increase if even largerfragments are included (for Tree-DOP it has beenshown that the accuracy decreases  after a certaindepth, probably due to overfitting -- cf.
Bonnemaet al 1997; Bod 2000a).5.3 Comparing LFG-DOP to Tree-DOPIn the following experiment, we are interested inthe impact of functional structures on predictingthe correct tree structures.
We therefore removedall f-structure units from the fragments, thusyielding a Tree-DOP model, and compared theresults against the full LFG-DOP model (usingthe discounted RF estimator and all fragments upto depth 4).
We evaluated the parse accuracy onthe tree structures only, using exact matchtogether with the standard PARSEVALmeasures.
We used the same training/test setsplits as in the previous experiments.Exact Match   Precision   RecallTree-DOP     46.6%     88.9% 86.7%LFG-DOP     50.8%     90.3%         88.4%ModelTable 5.
Tree accuracy on the VerbmobilExact Match   Precision   RecallTree-DOP     49.0%     93.4% 92.1%LFG-DOP     53.2%     95.8%         94.7%ModelTable 6.
Tree accuracy on the HomecentreThe results indicate that LFG-DOP's functionalstructures help to improve the parse accuracy oftree structures.
In other words, LFG-DOPoutperforms Tree-DOP if evaluated on treestructures only.
According to paired t-tests alldifferences in accuracy were statisticallysignificant.
This result is promising since Tree-DOP has been shown to obtain state-of-the-artperformance on the Wall Street Journal corpus(see Bod 2000a).5.4 Comparing Viterbi n  best to Monte CarloFinally, we were interested in comparing analternative, more efficient search method forestimating the most probable analysis.
In thefollowing set of experiments we use a Viterbi nbest search heuristic (as explained in section 4),and let n range from 1 to 10,000 derivations.
Wealso compute the results obtained by Monte Carlofor the same number of derivations.
We used thesame training/test set splits as in the previousexperiments and used both ungeneralized andgeneralized fragments up to depth 4 together withthe discounted RF estimator.Nr.
of derivations Viterbi n best Monte Carlo1 74.8% 20.1%10 75.3% 36.7%100 77.5% 67.0%1,000 77.5% 77.1%10,000 77.5% 77.5%Table 7.
Precision on the VerbmobilNr.
of derivations Viterbi n best Monte Carlo1 75.6% 25.6%10 76.2% 44.3%100 79.1% 74.6%1,000 79.8% 79.1%10,000 79.8% 80.0%Table 8.
Precision on the HomecentreThe tables show that Viterbi n best alreadyachieves a maximum accuracy at 100 derivations(at least on the Verbmobil corpus) while MonteCarlo needs a much larger number of derivationsto obtain these results.
On the Homecentrecorpus, Monte Carlo slightly outperforms Viterbin best at 10,000 derivations, but these differencesare not statistically significant.
Also remarkableare the relatively high results obtained with Viterbin best if only one derivation is used.
This scorecorresponds to the analysis generated by the mostprobable (valid) derivation.
Thus Viterbi n  best isa promising alternative to Monte Carlo resultingin a speed up of about two orders of magnitude.6  ConclusionWe presented a parser which analyzes new inputby probabilistically combining fragments fromLFG-annotated corpora into new analyses.
Wehave seen that the parse accuracy increased withincreasing fragment size, and that LFG'sfunctional structures contribute to significantlyhigher parse accuracy on tree structures.
Wetested two search techniques for the mostprobable analysis, Viterbi n best  and Monte Carlo.While these two techniques achieved about thesame accuracy, Viterbi n best was about 100times faster than Monte Carlo.ReferencesE.
Black et al, 1991.
"A Procedure forQuantitatively Comparing the SyntacticCoverage of English", Proceedings DARPAWorkshop, Pacific Grove, Morgan Kaufmann.R.
Bod, 1993.
"Using an Annotated LanguageCorpus as a Virtual Stochastic Grammar",Proceedings AAAI'93, Washington D.C.R.
Bod, 1998.
Beyond Grammar: An Experience-Based Theory of Language, CSLI Publications,Cambridge University Press.R.
Bod 1999.
"Context-Sensitive DialogueProcessing with the DOP Model", NaturalLanguage Engineering 5(4), 309-323.R.
Bod, 2000a.
"Parsing with the ShortestDerivation", Proceedings COLING-2000,Saarbr?cken, Germany.R.
Bod 2000b.
"Combining Semantic and SyntacticStructure for Language Modeling", Proceed-ings ICSLP-2000, Beijing, China.R.
Bod 2000c.
"An Empirical Evaluation of LFG-DOP", Proceedings COLING-2000, Saar-br?cken, Germany.R.
Bod, 2000d.
"The Storage and Computation ofFrequent Sentences", Proceedings AMLAP-2000, Leiden, The Netherlands.R.
Bod and R. Kaplan, 1998.
"A ProbabilisticCorpus-Driven Model for Lexical FunctionalAnalysis", Proceedings COLING-ACL'98,Montreal, Canada.R.
Bonnema, R. Bod and R. Scha, 1997.
"A DOPModel for Semantic Interpretation",Proceedings ACL/EACL-97, Madrid, Spain.J.
Chappelier and M. Rajman, 2000.
"Monte CarloSampling for NP-hard Maximization Problemsin the Framework of Weighted Parsing", inNLP 2000, Lecture Notes in ArtificialIntelligence 1835, 106-117.B.
Cormons, 1999.
Analyse et d?sambiguisation:Une approche ?
base de corpus (Data-OrientedParsing) pour les r?presentations lexicalesfonctionnelles .
PhD thesis, Universit?
deRennes, France.I.
Good, 1953.
"The Population Frequencies ofSpecies and the Estimation of PopulationParameters", Biometrika 40, 237-264.J.
Goodman, 1998.
Parsing Inside-Out, PhD thesis,Harvard University, Mass.L.
Hoogweg, 2000.
Enriching DOP1 with theInsertion Operation, MSc Thesis, Dept.
ofComputer Science, University of Amsterdam.R.
Kaplan, and J. Bresnan, 1982.
"Lexical-Functional Grammar: A Formal System forGrammatical Representation", in J.
Bresnan(ed.
), The Mental Representation ofGrammatical Relations, The MIT Press,Cambridge, Mass.J.
Maxwell and R. Kaplan, 1991.
"A Method forDisjunctive Constraint Satisfaction", in M.Tomita (ed.
), Current Issues in ParsingTechnology, Kluwer Academic Publishers.G.
Neumann, 1998.
"Automatic Extraction ofStochastic Lexicalized Tree Grammars fromTreebanks", Proceedings of the 4th Workshopon Tree-Adjoining Grammars and RelatedFrameworks, Philadelphia, PA.G.
Neumann and D. Flickinger, 1999.
"LearningStochastic Lexicalized Tree Grammars fromHPSG", DFKI Technical Report, Saarbr?cken,Germany.H.
Ney, S. Martin and F. Wessel, 1997.
"StatisticalLanguage Modeling Using Leaving-One-Out",in S. Young & G. Bloothooft (eds.
), Corpus-Based Methods in Language and SpeechProcessing, Kluwer Academic Publishers.K.
Sima'an, 1999.
Learning Efficient Disambigu-ation.
PhD thesis, ILLC dissertation seriesnumber 1999-02.
Utrecht / Amsterdam.
