R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
34 ?
45, 2005.?
Springer-Verlag Berlin Heidelberg 2005Automatic Image Annotation UsingMaximum Entropy ModelWei Li and Maosong SunState Key Lab of Intelligent Technology and Systems,Department of Computer Science and Technology, Tsinghua University,Beijing 100084, Chinawei.lee04@gmail.com, sms@mail.tsinghua.edu.cnAbstract.
Automatic image annotation is a newly developed and promisingtechnique to provide semantic image retrieval via text descriptions.
It concernsa process of automatically labeling the image contents with a pre-defined set ofkeywords which are exploited to represent the image semantics.
A MaximumEntropy Model-based approach to the task of automatic image annotation isproposed in this paper.
In the phase of training, a basic visual vocabulary con-sisting of blob-tokens to describe the image content is generated at first; thenthe statistical relationship is modeled between the blob-tokens and keywords bya Maximum Entropy Model constructed from the training set of labeled images.In the phase of annotation, for an unlabeled image, the most likely associatedkeywords are predicted in terms of the blob-token set extracted from the givenimage.
We carried out experiments on a medium-sized image collection withabout 5000 images from Corel Photo CDs.
The experimental results demon-strated that the annotation performance of this method outperforms some tradi-tional annotation methods by about 8% in mean precision, showing a potentialof the Maximum Entropy Model in the task of automatic image annotation.1   IntroductionLast decade has witnessed an explosive growth of multimedia information such asimages and videos.
However, we can?t access to or make use of the relevant informa-tion more leisurely unless it is organized so as to provide efficient browsing and que-rying.
As a result, an important functionality of next generation multimedia informa-tion management system will undoubtedly be the search and retrieval of images andvideos on the basis of visual content.In order to fulfill this ?intelligent?
multimedia search engines on the world-wide-web, content-based image retrieval techniques have been studied intensively duringthe past few years.
Through the sustained efforts, a variety of state-of-the-art methodsemploying the query-by-example (QBE) paradigm have been well established.
By thiswe mean that queries are images and the targets are also images.
In this manner, vis-ual similarity is computed between user-provided image and database images basedon the low-level visual features such as color, texture, shape and spatial relationships.However, two important problems still remain.
First, due to the limitation of objectrecognition and image understanding, semantics-based image segmentation algorithmAutomatic Image Annotation Using Maximum Entropy Model 35is unavailable, so segmented region may not correspond to users?
query object.
Sec-ond, visual similarity is not semantic similarity which means that low-level featuresare easily extracted and measured, but from the users?
point of view, they are non-intuitive.
It is not easy to use them to formulate the user?s needs.
We encounter a so-called semantic gap here.
Typically the starting point of the retrieval process is thehigh-level query from users.
So extracting image semantics based on the low-levelvisual features is an essential step.
As we know, semantic information can be repre-sented more accurately by using keywords than by using low-level visual features.Therefore, building relationship between associated text and low-level image featuresis considered to an effective solution to capture the image semantics.
By means of thishidden relationship, images can be retrieved by using textual descriptions, which isalso called query-by-keyword (QBK) paradigm.
Furthermore, textual queries are adesirable choice for semantic image retrieval which can resort to the powerful text-based retrieval techniques.
The key to image retrieval using textual queries is imageannotation.
But most images are not annotated and manually annotating images is atime-consuming, error-prone and subjective process.
So, automatic image annotationis the subject of much ongoing research.
Its main goal is to assign descriptive wordsto whole images based on the low-level perceptual features, which has been recog-nized as a promising technique for bridging the semantic gap between low-level im-age features and high-level semantic concepts.Given a training set of images labeled with text (e.g.
keywords, captions) that de-scribe the image content, many statistical models have been proposed by research-ers to construct the relation between keywords and image features.
For example, co-occurrence model, translation model and relevance-language model.
By exploitingtext and image feature co-occurrence statistics, these methods can extract hiddensemantics from images, and have been proven successful in constructing a niceframework for the domain of automatic image annotation and retrieval.In this paper, we propose a novel approach for the task of automatic image anno-tation using Maximum Entropy Model.
Though Maximum Entropy method hasbeen successfully applied to a wide range of application such as machine transla-tion, it is not much used in computer vision domain, especially in image autoannotation.This paper is organized as follows: Section 2 presents related work.
Section 3 de-scribes the representation of labeled and unlabeled images, gives a brief introduc-tion to Maximum Entropy Model and then details how to use it for automaticallyannotating unlabeled images.
Section 4 demonstrates our experimental results.
Sec-tion 5 presents conclusions and a comment for future work.2   Related WorkRecently, many statistical models have been proposed for automatic image annotationand retrieval.
The work of associating keywords with low-level visual features can beaddressed from two different perspectives.36 W. Li and M. Sun2.1   Annotation by Keyword PropagationThis kind of approach usually formulates the process of automatic image annotationas one of supervised classification problems.
With respect to this method, accurateannotation information is demanded.
That is to say, given a set of training imageslabeled with semantic keywords, detailed labeling information should be provided.For example, from training samples, we can know which keyword corresponds towhich image region or what kind of concept class describes a whole-image.
So eachor a set of annotated keyword can be considered as an independent concept class,followed by training each class model with manually labeled images, then the modelis applied to classify each unlabeled image into a relevant concept class, and finallyproducing annotation by propagating the corresponding class words to unlabeledimages.Wang and Li [8] introduced a 2-D multi- resolution HMM model to automate lin-guistic indexing of images.
Clusters of fixed-size blocks at multiple resolution and therelationships between these clusters is summarized both across and within the resolu-tions.
To annotate the unlabeled image, words of the highest likelihood is selectedbased on the comparison between feature vectors of new image and the trained con-cept models.
Chang et al[5] proposed content-based soft annotation (CBSA) for pro-viding images with semantic labels using (BPM) Bayesian Point Machine.
Startingwith labeling a small set of training images, an ensemble of binary classifier for eachkeyword is then trained for predicting label membership for images.
Each image isassigned one keyword vector, with each keyword in the vector assigned a confidencefactor.
In the process of annotation, words with high confidence are considered to bethe most likely descriptive words for the new images.
The main practical problemwith this kind of approaches is that a large labeled training corpus is needed.
More-over, during the training and application stages, the training set is fixed and not in-cremented.
Thus if a new domain is introduced, new labeled examples must be pro-vided to ensure the effectiveness of such classifiers.2.2   Annotation by Statistical InferenceMore recently, there have been some efforts to solve this problem in a more generalway.
The second approach takes a different strategy which focuses on discovering thestatistical links between visual features and words using unsupervised learning meth-ods.
During training, a roughly labeled image datasets is provided where a set of se-mantic labels is assigned to a whole image, but the word-to-region information ishidden in the space of image features and keywords.
So an unsupervised learningalgorithm is usually adopted to estimate the joint probability distribution of words andimage features.Mori et al[4] were the earliest to model the statistics using a co-occurrence prob-abilistic model, which predicate the correct probability of associating keywords bycounting the co-occurrence of words with image regions generated using a fixed-sizeblocks.
Blocks are vector quantized to form clusters which inherit the whole set ofAutomatic Image Annotation Using Maximum Entropy Model 37keywords assigned to each image.
Then clusters are in turn used to predict the key-words for unlabeled images.
The disadvantage is that the model is a little simple andthe rough fixed-size blocks are unable to model objects effectively, leading to poorannotation accuracy.
Instead of using fixed-size blocks, Barnard et al[1] performedBlobworld segmentation and Normalized cuts to produce semantic meaningful re-gions.
They constructed a hierarchical model via EM algorithm.
This model combinesboth asymmetric clustering model which maps words and image regions into clustersand symmetric clustering model which models the joint distribution of words andregions.
Duygulu et al[2] proposed a translation model to map keywords to individ-ual image regions.
First, image regions are created by using a segmentation algorithm.For each region, visual features are extracted and then blob-tokens are generated byclustering the features for each region across whole image datasets.
Each image canbe represented by a certain number of these blob-tokens.
Their Translation Modeluses machine translation model ?of IBM to annotate a test set of images based on alarge number of annotated training images.
Another approach using cross-media rele-vance models (CMRM) was introduced by Jeon et al[3].
They assumed that thiscould be viewed as analogous to the cross-lingual retrieval problem and a set of key-words{ }nwww ...,,, 21  is related to the set of blob-tokens{ }nbbb ...,,, 21 , ratherthan one-to-one correspondence between the blob-tokens and keywords.
Here thejoint distribution of blob-tokens and words was learned from a training set of anno-tated images to perform both automatic image annotation and ranked retrieval.
Jeon etal [9] introduced using Maximum Entropy to model the fixed-size block and key-words, which gives us a good hint to implement it differently.
Lavrenko et al[11]extended the cross-media relevance model using actual continuous-valued featuresextracted from image regions.
This method avoids the clustering and constructing thediscrete visual vocabulary stage.3   The Implementation of Automatic Annotation Model3.1   The Hierarchical Framework of Automatic Annotation and RetrievalThe following Fig.
1 shows the framework for automatic image annotation and key-word-based image retrieval.
Given a training dataset of images labeled with key-words.
First, we segment a whole image into a collection of sub-images, followed byextracting a set of low-level visual features to form a feature vector to describe thevisual content of each region.
Second, a visual vocabulary of blob-tokens is generatedby clustering all the regions across the whole dataset so that each image can be repre-sented by a number of blob-tokens from a finite set of visual symbols.
Third, bothtextual information and visual information is provided to train the Maximum Entropymodel, and the learned model is then applied to automatically generate keywords todescribe the semantic content of an unlabeled image based on the low-level features.Consequently, both the users?
information needs and the semantic content of imagescan be represented by textual information, which can resort to the powerful text IRtechniques to implement this cross-media retrieval, suggesting the importance oftextual information in semantics-based image retrieval.38 W. Li and M. SunFig.
1.
Hierarchical Framework of Automatic Annotation and Retrievallearning correlations between blob-tokens and textual annotationsapplying correlations to generate annotations for unlabeled images3.2   Image Representation and Pre-processingA central issue in content-based image annotation and retrieval is how to describe thevisual information in a way compatible with human visual perception.
But until now,no general framework is proposed.
For different tasks and goals, different low-levelfeatures are used to describe and analyze the visual content of images.
On the whole,there are two kinds of interesting open questions remain unresolved.
First, what fea-ture sets should be selected to be the most expressive for any image region.
Second,how blob-tokens can be generated, that is to say, how can one create such a visualvocabulary of blob-tokens to represent each image in the collection using a number ofsymbols from this finite set?
In our method, we carried out these following two steps:First, segment images into sub-images, Second, extract appropriate features for anysub-images, cluster similar regions by k-means and then use the centroid in each clus-Automatic Image Annotation Using Maximum Entropy Model 39ter as a blob-token.
The first step can be employed by either using a segmentationalgorithm to produce semantically meaningful units or partitioning the image intofixed-size rectangular grids.
Both methods have pros and cons, a general purposesegmentation algorithm may produce semantic regions, but due to the limitation incomputer vision and image processing, there are also the problems of erroneous andunreliable region segmentation.
The advantage of regular grids is that is does not needto perform complex image segmentation and is easy to be conducted.
However, due torough fixed-size rectangular grids, the extracted blocks are unable to model objectseffectively, leading to poor annotation accuracy in our experiment.Fig.
2.
Segmentation Results using Normalized cuts and JSEGIn this paper, we segment images into a number of meaningful regions using Nor-malized cuts [6] against using JSEG.
Because the JSEG is only focusing on localfeatures and their consistencies, but Ncuts aims at extracting the global impression ofan image data.
So Ncuts may get a better segmentation result than JSEG.
Fig.
2 showssegmentation result using Normalized cuts and JSEG respectively, the left is the origi-nal image, the mid and the right are the segmentation result using Ncuts and JSEGrespectively.
After segmentation, each image region is described by a feature vectorformed by HSV histograms and Gabor filters.
Similar regions will be grouped to-gether based on k-means clustering to form the visual vocabulary of blob-tokens.
Toomuch clusters may cause data sparseness and too few can not converge.
Then each ofthe labeled and unlabeled images can be described by a number of blob-tokens, in-stead of the continuous-valued feature vectors.
So we can avoid the image data mod-eling in a high-dimensional and complex feature space.3.3   The Annotation Strategy Based on Maximum EntropyMaximum Entropy Model is a general purpose machine learning and classificationframework whose main goal is to account for the behavior of a discrete-valued ran-dom process.
Given a random process whose output value y may be influenced bysome specific contextual information x, such a model is a method of estimating theconditional probability.
?==kjyxfjjxZxyp1),()(1)|( ?
(1)In the process of annotation, images are segmented using normalized cuts, everyimage region is represented by a feature vector consisting of HSV color histogramand the Gabor filters, and then a basic visual vocabulary containing 500 blob-tokensis generated by k-means clustering.
Finally, each segmented region is assigned to thelabel of its closest blob-token.
Thus the complex visual contents of images can be40 W. Li and M. Sunrepresented by a number of blob-tokens.
Due to the imbalanced distribution of key-words frequency and the data sparseness problem, the size of the pre-defined keywordvocabulary is reduced from 1728 to 121 keywords, by keeping only the keywordsappearing more than 30 times in the training dataset.We use a series of feature function ( )ji wbf ,Label,FC  to model the co-occurrencestatistics of blob-tokens ib  and keywords jw , where FC denote the context of featureconstraints for each blob-token.
The following example represents the co-occurrenceof the blob-token?b  and the keyword ?water?
in an image I.
( ) ( )???
=====otherwisetruebFCandwaterwifwbf iwjji 0''1,water ,FC w       (2)If blob-token ib  satisfies the context of feature constraints and keyword ?water?also occurs in image I.
In other words, if the color and texture feature components arecoordinated with the semantic label ?water?, and then the value of the feature functionis 1, otherwise 0.The following Fig.
3 shows the annotation procedure that using MaxEnt capturesthe hidden relationship between blob-tokens and keywords from a roughly labeledtraining image sets.Fig.
3.
Learning the statistics of blob-tokens and wordsIn the recent past, many models for automatic image annotation are limited by thescope of the representation.
In particular, they fail to exploit the context in the imagesand words.
It is the context in which an image region is placed that gives it meaning-ful interpretation.Automatic Image Annotation Using Maximum Entropy Model 41In our annotation procedure, each annotated word is predicted independently by theMaximum Entropy Model, word correlations are not taken into consideration.
How-ever, correlations between annotated words are essentially important in predictingrelevant text descriptions.
For example, the words ?trees?
and ?grass?
are more likelyto co-occur than the words ?trees?
and ?computers?.
In order to generate appropriateannotations, a simple language model is developed that takes the word-correlationinformation into account, and then the textual description is determined not only bythe model linking keywords and blob-tokens but also by the word-to-word correla-tion.
We simply count the co-occurrence information between words in the pre-defined textual set to produce a simple word correlation model to improve the annota-tion accuracy.4   Experiments and AnalysisWe carried out experiments using a mid-sized image collection, comprising about5,000 images from Corel Stock Photo CDs, 4500 images for training and 500 fortesting.
The following table 1 shows the results of automatic image annotation usingMaximum Entropy.Table 1.
Automatic image annotation resultsImages Original Annotation Automatic Annotationsun city sky mountain Sun sky mountaincloudsflowers tulips mountain sky Flowers sky trees grasstufa snow sky grass snow sky grass stonepolar bear snow post bear snow sky rocks42 W. Li and M. SunFor our training datasets, the visual vocabulary and the pre-defined textual set con-tain 500 blob-tokens and 121 keywords respectively, so the number of the trainingpairs ( )ji wb ,  is 60500.
After the procedure of feature selection, only 9550 pairs left.For model parameters estimation, there are a few algorithms including GeneralizedIterative Scaling and Improved Iterative Scaling which are widely used.
Here we useLimited Memory Variable Metric method which has been proved effective for Maxi-mum Entropy Model [10].
Finally, we can get the model linking blob-tokens andkeywords, and then the trained model ( )xyp  is applied to predict textual annota-tions { }nwww ,,, 21 K  given an unseen image formed by{ }mbbb ,,, 21 K .To further verify the feasibility and effectiveness of Maximum Entropy model, wehave implemented the co-occurrence model as one of the baselines whose conditionalprobability ( )ij bwp  can be estimated as follows:( ) ( ) ( )( ) ( )( )( )( )( ) iijNkikijNkkkikjjijNkikiijiij MmmmNnnmNnnmwpwbpwpwbpbwp ==?=??
?=== 111(3)Where ijm denote the co-occurrence of ib  and jw , jn denote the occurring num-ber of  jw in the total N words.The following Fig.
4 shows the some of the retrieval results using the keyword?water?
as a textual query.Fig.
4.
Some of retrieved images using ?water?
as a queryThe following Fig.
5 and Fig.
6 show the precision and recall of using a se of high-frequency keywords as user queries.
We implemented two statistical models to linkblob-tokens and keywords.Automatic Image Annotation Using Maximum Entropy Model 4300.10.20.30.40.50.60.70.8waterskytreespeoplegrasssnowcloudsflowersmountainsbuildingstonebuildingsstreetsandfieldbearbeachtreejetPrecisionMaximum EntropyCo-occurrenceFig.
5.
Precision of retrieval using some high-frequency keywords00.10.20.30.40.50.60.70.80.9waterskytreespeoplegrasssnowcloudsflowersmountainsbuildingstonebuildingsstreetsandfieldbearbeachtreejetRecallMaximum EntropyCo-OccurrenceFig.
6.
Recall of retrieval using some high-frequency keywordsThe annotation accuracy is evaluated by using precision and recall indirectly.
Afterposing a keyword query for images, the measure of precision and recall can be de-fined as follows:BAAprecision+=CAArecall+=                              (4)Where A denote the number of relevant images retrieved, B denote the number ofirrelevant images retrieved, C denote the number of relevant images not retrieved inthe image datasets, and images whose labels containing the query keyword is consid-ered relevant, otherwise irrelevant.Table 2.
Experimental results with average precision and meanMethod Mean precision Mean recallCo-occurrence 0.11 0.18Maximum Entropy 0.17 0.2544 W. Li and M. SunThe above experimental results in table 2 show that our method outperforms theCo-occurrence model [4] in the average precision and recall.
Since our model uses theblob-tokens to represent the contents of the image regions and converts the task ofautomatic image annotation to a process of translating information from visual lan-guage (blob-tokens) to textual language (keywords).
So Maximum Entropy Model isa natural and effective choice for our task, which has been successfully applied to thedyadic data in which observations are made from two finite sets of objects.
But disad-vantages also exist.
There are two fold problems to be considered.
First, since Maxi-mum Entropy is constrained by the equation ( ) ( )fpfp ~= , which assumes that theexpected value of output of the stochastic model should be the same as the expectedvalue of the training sample.
However, due to the unbalanced distribution of key-words frequency in the training subset of Corel data, this assumption will lead to anundesirable problem that common words with high frequency are usually associatedwith too many irrelevant blob-tokens, whereas uncommon words with low frequencyhave little change to be selected as annotations for any image regions, consider word?sun?
and ?apple?
, since both words may be related to regions with ?red?
color and?round?
shape, but it is difficult to make a decision between the word ?sun?
and ?ap-ple?.
However, since ?sun?
is a common word as compared to ?apple?
in the lexicalset, the word ?sun?
will definitely used as the annotation for these kind of regions.
Toaddress this kind of problems, our future work will mainly focus on the more sophis-ticated language model to improve the statistics between image features and key-words.
Second, the effects of segmentation may also affect the annotation perform-ance.
As we know, semantic image segmentation algorithm is a challenging and com-plex problem, current segmentation algorithm based on the low-level visual featuresmay break up the objects in the images, that is to say, segmented regions do not defi-nitely correspond to semantic objects or semantic concepts, which may cause theMaximum Entropy Model to derive a wrong decision given an unseen image.5   Conclusion and Future WorkIn this paper, we propose a novel approach for automatic image annotation and re-trieval using Maximum Entropy Model.
Compared to other traditional classical meth-ods, the proposed model gets better annotation and retrieval results.
But three mainchallenges are still remain:1) Semantically meaningful segmentation algorithm is still not available, so thesegmented region may not correspond to a semantic object and region featuresare insufficient to describe the image semantics.2) The basic visual vocabulary construction using k-means is only based on thevisual features, which may lead to the fact that two different semantic objectswith similar visual features fall into the same blob-token.
This may degrade theannotation quality.3) Our annotation task mainly depend on the trained model linking image featuresand keywords, the spatial context information of image regions and the word cor-relations are not fully taken into consideration.In the future, more work should be done on image segmentation techniques, clus-tering algorithms, appropriate feature extraction and contextual information betweenregions and words to improve the annotation accuracy and retrieval performance.Automatic Image Annotation Using Maximum Entropy Model 45AcknowledgementsWe would like to express our deepest gratitude to Kobus Barnard and J.Wang formaking their image datasets available.
This research is supported by the NationalNatural Science Foundation of China under grant number 60321002 and the National863 Project of China under grant number 2001AA114210-03, and the ALVIS Projectco-sponsored by EU PF6 and NSFC.References1.
K. Barnard, P. Dyugulu, N. de Freitas, D. Forsyth, D. Blei, and M. I. Jordan.
Matchingwords and pictures.
Journal of Machine Learning Research, 3: 1107-1135, 2003.2.
P. Duygulu, K. Barnard, N. de Freitas, and D. Forsyth.
Ojbect recognition as machinetranslation: Learning a lexicon fro a fixed image vocabulary.
In Seventh European Conf.on Computer Vision, 97-112, 2002.3.
J. Jeon, V. Lavrenko and R. Manmatha.
Automatic image annotation and retrieval usingcross-media relevance models.
In Proceedings of the 26th intl.
SIGIR Conf, 119-126, 2003.4.
Y. Mori, H. Takahashi, and R. Oka, Image-to-word transformation based on dividing andvector quantizing images with words.
First International Workshop on Multimedia Intelli-gent Storage and Retrieval Management, 1999.5.
Edward Chang, Kingshy Goh, Gerard Sychay and Gang Wu.
CBSA: Content-based softannotation for multimodal image retrieval using bayes point machines.
IEEE Transactionson Circuts and Systems for Video Technology Special Issue on Conceptual and DynamicalAspects of Multimedia Content Descriptions, 13(1): 26-38, 2003.6.
J. shi and J. Malik.
Normalized cuts and image segmentation.
IEEE Transactions On Pat-tern Analysis and Machine Intelligence, 22(8): 888-905, 2000.7.
A. Berger, S. Pietra and V. Pietra.
A maximum entropy approach to natural language proc-essing.
In Computational Linguistics, 39-71, 1996.8.
J. Li and J.
A. Wang.
Automatic linguistic indexing of pictures by a statistical modelingapproach.
IEEE Transactions on PAMI, 25(10): 175-1088, 2003.9.
Jiwoon Jeon, R. Manmatha.
Using maximum entropy for automatic image annotation.
Inproceedings of third international conference on image and video retrieval, 24-31, 2004.10.
Robert Malouf.
A comparison of algorithms for maximum entropy parameter estimation.In Proceedings of the 6th Workshop on Computational Language Learning, 2003.11.
V. Lavrenko, R. Manmatha and J. Jeon.
A model for learning  the semantics of pictures.
InProceedings of the 16th Annual Conference on Neural Information Processing Systems,2004.
