Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 483?493,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsCorpus-based discovery of semantic intensity scalesChaitanya Shivade?, Marie-Catherine de Marneffe?, Eric Fosler-Lussier?, Albert M.
Lai?
?Department of Computer Science and Engineering,?Department of Linguistics,?Department of Biomedical Informatics,The Ohio State University, Columbus OH 43210, USA.shivade@cse.ohio-state.edu, mcdm@ling.ohio-state.edufosler@cse.ohio-state.edu, albert.lai@osumc.eduAbstractGradable terms such as brief, lengthy and ex-tended illustrate varying degrees of a scale andcan therefore participate in comparative con-structs.
Knowing the set of words that can becompared on the same scale and the associ-ated ordering between them (brief < lengthy< extended) is very useful for a variety oflexical semantic tasks.
Current techniques toderive such an ordering rely on WordNet todetermine which words belong on the samescale and are limited to adjectives.
Here wedescribe an extension to recent work: we in-vestigate a fully automated pipeline to extractgradable terms from a corpus, group them intoclusters reflecting the same scale and estab-lish an ordering among them.
This method-ology reduces the amount of required hand-crafted knowledge, and can infer gradabilityof words independent of their part of speech.Our approach infers an ordering for adjec-tives with comparable performance to previ-ous work, but also for adverbs with an accu-racy of 71%.
We find that the technique is use-ful for inferring such rankings among wordsacross different domains, and present an ex-ample using biomedical text.1 IntroductionGradability (Sapir, 1944) is a property of words thatidentifies different degrees of the quality the worddenotes.
For example, adjectives such as large, hugeand gigantic present different degrees of size or vol-ume.
Similarly, adverbs such as approximately, al-most and roughly present different degrees of howaccurate a measurement is.
Thus, one of the charac-teristics of gradable terms is that they participate ina scale and can be ordered along that scale: for ex-ample, good < great < excellent (Kennedy, 2007).Another characteristic is that gradable terms can ap-pear in comparative constructions, e.g., ?A is largerthan B?.
Such comparative judgments are a psycho-logical process that precedes judgments of counting,e.g., ?A is twice as large as B?
(Sapir, 1944).Modern NLP systems face the challenge of inter-preting language as close to human perception aspossible.
Modeling gradable terms as well as theirassociated meaning and ordering is an important as-pect of this challenge.
Such information can be veryuseful for a variety of inference tasks, such as senti-ment analysis (Pang and Lee, 2008) and textual in-ference (Dagan et al, 2006).
However, current lex-ical resources, like WordNet (Fellbaum, 1998), lackannotations capturing the gradability of words.
Thisweakens the notion of similarity: although wordssuch as small and minuscule illustrate varying de-grees of size, they are listed as synonyms in Word-Net.Recently, there has been a lot of interest in ex-ploring different approaches to derive an orderingamong gradable adjectives based on their semantics(Ruppenhofer et al, 2014; Sheinman et al, 2013;Schulam and Fellbaum, 2010).
de Melo and Bansal(2013) propose a novel Mixed Integer Linear Pro-gramming (MILP) based approach, publish a goldstandard dataset and report the best performance onordering scalar adjectives on this dataset.
However,these approaches are limited in two ways.
First,they depend on a manually created resource, such483as WordNet or FrameNet (Baker et al, 1998).
Lex-ical patterns (e.g., ?not just x but y?)
are used bothto extract words that belong to the same scale and todetermine the direction of the ordering (e.g., in theabove pattern, x is weaker than y).
However, thisextraction process gives noisy results that require fil-tering using an electronic thesaurus.
The domain ofapplication is thus restricted to words that exist inan electronic thesaurus.
Second, previous work islimited to the study of adjectives.In this paper, we propose a fully automatedpipeline that uses structural patterns to extract grad-able terms from a corpus, cluster them into groupsthat reflect the same semantic scale of comparison,and finally rank them using de Melo and Bansal?sMILP technique to establish an ordering amongthem.
We also explore how the technique fares ondomain-specific (biomedical) text, deriving scalesfor domain-specific terms that might not exist in the-sauri.
Our approach achieves a comparable perfor-mance to previous studies on scalar adjectives, andcan be reliably extended to adverbs.2 Related workHatzivassiloglou and McKeown (1993) present thefirst work on automatically clustering adjectives thatbelong to the same scale, identifying scalar adjec-tives based on the intuition that similar nouns aremodified by similar adjectives.
They use a hierar-chical clustering algorithm on a newswire corpus forgrouping similar adjectives, but do not provide rank-ing among a given cluster of related adjectives.Assuming a pair of related adjectives, de Marn-effe et al (2010) use reviews from the InternetMovie Database and their associated ratings to inferan ordering in the adjective pair.
Kim and de Marn-effe (2013) also obtain an ordering given a pair ofadjectives, using distributional word vectors derivedfrom a recursive neural network.Sheinman et al (2013) and de Melo and Bansal(2013) present similar approaches, which make useof WordNet dumbbells to determine words that be-long to the same scale as proposed in Sheinmanet al (2012).
A WordNet dumbbell is a represen-tation involving an antonym pair (e.g., small andlarge) as two ends of a semantic scale with seman-tically similar adjectives arranged in a radial fash-ion around each adjective.
The antonym acting asa centroid and its synonyms as members of a clus-ter represent words that most likely participate inthe same scale.
For example, the antonym pair(small, large) results in the dumbbell with clusters(small, tiny, pocket-size, smallish) and (large, gigan-tic, monstrous, huge) at the two ends.
It should benoted that even with such a representation, there canbe words that fall into the same WordNet synset butdo not participate in the scalar relationship (e.g., vio-lent with respect to supernatural and affected).
Thisis primarily because of polysemy and semantic drift(de Melo and Bansal, 2013).Sheinman et al (2013) present a two-step ap-proach for establishing an ordering among scalar ad-jectives.
They extract adjectives from the Web us-ing lexical patterns indicative of the direction of thescalar relationship between a pair of adjectives.
Twosets of patterns are defined: mild patterns in whichparticipating words are such that the first word hasa weaker semantic intensity than the second word(e.g., ??
but not ??
?
good but not great); and intensepatterns, in which the first word has a stronger se-mantic intensity than the second word (e.g., ?not ?but still ??
?
not freezing but still cold).
In the firststep, they assign a positive score to an adjective if itis seen as a part of the intense pattern and a nega-tive score if seen as part of the mild pattern.
In thesecond step, they use these scores to partition the ad-jectives into two subsets one representing mild andthe other representing intense adjectives.
They per-form this partitioning recursively to obtain a com-plete ordering for a given cluster of adjectives froma WordNet dumbbell.de Melo and Bansal (2013) improve upon Shein-man et al (2013) by refining their lexical patterns,and refer to them as ?strong-weak?
and ?weak-strong?
patterns.
Using frequencies of occurrencefor a pair of adjectives across the strong-weak andweak-strong patterns in a corpus, they define anoverall weak-strong score.
They optimize for thisscore using MILP.
The constraints of the MILPmodel two types of strength relationships: thestrength relationships between two adjectives in apair with a possible third adjective, and synonymyrelationship between two adjectives based on infor-mation from an external resource.
Given a clus-ter of terms, the MILP produces an ordering of the484cluster members using frequency counts of instanceswhere these members are found in strong-weak andweak-strong patterns.
To evaluate their approach, deMelo and Bansal construct a manually curated goldstandard of 88 clusters, each with a cardinality ofthree or more adjectives.
These 88 clusters are ran-domly drawn from all possible clusters that are ei-ther half of a WordNet dumbbell.
Two annotatorsmanually examined these clusters to remove wordsthat did not belong to the same scale.
Further, allpairs within these clusters were annotated for scalarrelationship: is the adjective in a pair weaker thanthe other, stronger than the other, or of equivalent in-tensity.
The output of the MILP was tested on these88 clusters (569 word pairs).
They achieve a pair-wise accuracy of 78.2%.3 Our approach3.1 Extraction using structural patternsAs observed by Ruppenhofer et al (2014), lexicalpattern-based approaches suffer from a coverage is-sue.
This is because these patterns consist of longern-grams, which are sparsely found in a small dataset.Therefore, Sheinman et al (2013) use the Web astheir corpus, and de Melo & Bansal use GoogleN-grams (Brants and Franz, 2006).
However, thisresults in a large number of instances where satis-fied lexical patterns do not correspond to adjectives(e.g., sometimes but not always).
Moreover, sincethe Google N-grams corpus is limited to 5-grams,adjective pairs of interest beyond a five-word win-dow are lost.To deal with these shortcomings, we use Tregex(Levy and Andrew, 2006), which enables pat-tern matching on parse trees based on syntacticrelationships and regular expression matches onnodes.
Using Tregex, we transform de Melo andBansal?s weak-strong and strong-weak lexical pat-terns into structural patterns.
For example, oneway of expanding the lexical pattern ??
but not??
into a structural Tregex pattern for adjectivesis ?ADJP< ((ADJP<JJ) $ (CC<but)$(RB<not)$(ADJP<JJ)).?
Similarly, a structural pattern for ad-verbs can be written as ?ADVP< ((ADVP<RB) $(CC<but)$(RB<not)$ (ADVP<RB)).?
These pat-terns are available for download1.1http://web.cse.ohio-state.edu/?shivade/naacl2015Introducing tree patterns requires parsing a cor-pus: while this additional step in the pipeline mightlead to error propagation, the advantages of thestructural patterns are that (i) they are more robustthan the lexical ones and (ii) restricting results toa desired part-of-speech comes for free.
In the ex-periments reported here, we use the Stanford parserv3.3.1 (Klein and Manning, 2003).3.2 Automatic clusteringIn order to determine a ranking of words based ontheir semantic intensity, the first step is to deter-mine words that belong to the same scale of mean-ing.
As pointed out earlier, previous work (deMelo and Bansal, 2013; Sheinman et al, 2013) useWordNet dumbbells, and this restricts the utility ofthese approaches to the scope of a manually cre-ated lexical resource.
We overcome this limitationby automatically clustering words that belong to thesame scale.
As the clustering algorithm, we usethe Matlab (2014) implementation of K-means++(Arthur and Vassilvitskii, 2007), a hard clustering al-gorithm2with cosine similarity as a distance metric.Following Hatzivassiloglou and McKeown (1993),we use context vectors to represent the words tocluster.
They make use of standard context vectorsfor clustering adjectives, where context for every ad-jective comprises of nouns it modifies across all sen-tences in a corpus.However, recent work shows promise for contextvectors embedded in a compressed semantic spacethat are derived using neural networks: Baroni et al(2014) compare standard context vectors with em-bedded vectors for a wide range of lexical seman-tic tasks and found embedded vectors to yield betterresults.
We therefore generate context vectors andcompare the utility of both skip-gram and contin-uous bag of words (CBOW) representations usingthe word2vec tool (Mikolov et al, 2013) for ourtask.
These two representations have demonstratedvarying degrees of success in different NLP tasks(Baroni et al, 2014; Bansal et al, 2014).
Given a2The choice of a hard-clustering algorithm was mostly forimplementational convenience, but carries with it the issue thatpolysemous words can only appear in one semantic cluster.
Weleave the issue of deriving a soft clustering approach that workswith context vectors, a separate research problem in its ownright, to future work.485window size w, the CBOW model predicts the cur-rent word given the neighboring words as context.
Incontrast, the skip-gram model predicts the neighbor-ing words given the current word.
We used w = 5and found CBOW to yield better results for our task.Thus the terms extracted from a corpus by the struc-tural patterns are automatically clustered, and theseclusters are used as an input to the ranking algo-rithm.3.3 Ranking based on semantic intensityOnce the terms have been clustered, the second stepis to provide a ranking between the cluster members.To do so, we use the MILP implementation providedby de Melo and Bansal (2013).
This method com-putes an overall weak-strong score for a pair of ad-jectives based on the frequency of that pair in thematches for weak-strong and strong-weak patterns.The MILP then uses these scores among all rele-vant pairs of adjectives belonging to the same scale,capturing complex interactions to infer an orderingamong them.3.4 Data: PubMed corpusIn this work, we want to provide an approach thatcan infer scalar orderings for any domain-specificterms.
Such terms might be absent from existingthesauri.
Our approach is thus corpus-based as out-lined above.
We chose to test the robustness of ourtechnique on PubMed, a large domain-specific cor-pus of biomedical texts.
It is a free resource de-veloped and maintained by the National Center forBiotechnology Information at the National Libraryof Medicine.
It provides access to scientific ab-stracts, full text articles and associated resources.We used 10, 875, 982 freely available abstracts (notfull text articles) from PubMed as our corpus.
Thiscorresponds to 88, 303, 272 sentences in total, wherethe average length of a sentence is 28 words (in-cluding punctuations).
We used this corpus to findinstances of the structural strong-weak and weak-strong patterns, both for adjectives and adverbs.4 Comparison with the gold standard of deMelo & BansalTo evaluate our approach, we need to establish howgood the clustering step is, as well as how good theranking step is.
Each step is evaluated separately us-ing annotations obtained from Amazon MechanicalTurk.4.1 ClusteringIn order to evaluate the automatic clustering pro-cedure that uses K-means++ and word vectors, westart with the gold standard provided by de Melo andBansal (2013): as mentioned above, their data sethas 88 gold standard clusters, corresponding to 346adjectives, annotated by humans for scale ordering.One problem with evaluating a hard clustering algo-rithm is that the same word may appear in multipleWordNet synsets, corresponding to multiple clusters(soft clustering).
We therefore made a ?hard clusterversion?
of the de Melo & Bansal dataset by remov-ing any adjectives that occur in multiple clusters,and then eliminating any singleton clusters.
This re-sulted in a gold standard set of 256 adjectives be-longing to 84 clusters.We clustered the 256 adjectives from the goldstandard data subset into 84 clusters: the represen-tation for each adjective was a neural embeddingderived using the word2vec tool trained on ourPubMed data.
We experimented with both the skip-gram and continuous bag of words (CBOW) modelsto derive vectors of dimension sizes varying from200 to 800 in increments of 100.
To choose theright dimensionality and the best model, we evalu-ated the quality of the automatically derived 84 clus-ters against the gold standard.
As a metric of evalu-ation for cluster quality, we follow Hatzivassiloglouand McKeown (1993) and use F1 calculated by com-paring equivalence relations generated by the clus-ters (as implemented in LingPipe (2008)).
We foundthat the CBOW model gave clusters closer to thegold standard than the skip-gram model.
We foundthat a dimension size of 600 for the vectors yieldedclusters with a maximum F1 score of 57%.
Thus,we were able to fix the parameters for our clusteringtask.
Figure 1 summarizes the results of this experi-ment.In their study, Hatzivassiloglou and McKeown(1993) evaluate the results of their clustering on asmall set of 21 adjectives.
They presented the 21adjectives to 9 annotators and asked them to parti-tion these adjectives such that each partition containadjectives that belong to the same scale.
They re-486Figure 1: Comparison of CBOW and skip-gram for clus-tering of adjectives.port a best F1 score of 48% but note that such scoredoes not seem to reflect the quality of the clustering.We observe the same problem: our automatically de-rived clusters have a different organization for wordsthat belong to the same cluster than the gold standardclusters, but in a way that seems intuitive.
Somedifferences between our automatically derived clus-ters and the gold standard clusters are illustrated inTable 1.
For example, the adjectives false and mis-leading belong to the same cluster in both the goldstandard as well as the automatic clustering output.However, the automatic clustering groups the ad-jectives false and misleading together with unreli-able and wrong, whereas the gold standard groupsfalse and misleading with deceptive and fraudulent.Both clusterings are plausible, though.
The adjec-tives fraudulent and deceptive become part of newclusters in our automatic clustering.
It could be ar-gued that the gold standard cluster ?deceptive, false,fraudulent, misleading?
represents different degreesof ?trickery,?
whereas the automatic cluster ?false,misleading, unreliable, wrong?
represent differentdegrees of ?wrongness.?
Thus, although both clus-ters contain different adjectives, they group adjec-tives that are on the same scale of a different mean-ing.Therefore, to evaluate the quality of the automaticclustering, we sampled 50 clusters containing threeor more adjectives (corresponding to a total of 190adjectives) from all the generated clusters and ob-tained annotations using Amazon Mechanical Turk(AMT), a crowdsourcing platform that has beenshown useful for a number of NLP tasks (Snow etal., 2008).
Annotators (workers, in AMT parlance)were presented with 15 clusters in each worker ses-sion, whose members were each associated with acheckbox.
For each cluster, workers had to uncheckthe adjectives that did not belong to the same scale.The nature of the annotation task does involve inher-ent subjectivity which cannot be avoided.
We triedto minimize this by giving detailed instructions withaccompanying examples to achieve coherent anno-tations.
To make sure workers were paying attentionto the task, 2 clusters among the 15 clusters they sawwere clusters for which we a priori knew which ad-jectives should be removed (e.g., beautiful, pretty,and rainy where rainy had to be unchecked).
Mostworkers did the task well: we only had to discard an-notations from 4 worker sessions (out of 140).
Weended up with annotations from 8 to 10 workers percluster.
To create a gold standard, we retained ineach cluster only those words that were ascertainedto be in the same cluster by 6 or more annotators.For each cluster, we calculated an accuracy scoreequivalent to the number of correct adjectives (de-termined to be on the same scale by the annotators)divided by the total number of adjectives in the gen-erated cluster.
This accuracy was averaged acrossall 50 clusters, and yielded a final micro-averagedaccuracy of 74.36% as seen in Table 2.4.2 RankingSince our end goal is to establish an ordering amongscalar adjectives, we use the automatically derivedclusters (rather than the WordNet dumbbells) as in-put to the MILP algorithm.
To determine the per-formance of the ranking produced by the MILP al-gorithm, we use AMT to obtain pairwise rankingannotations for all unique adjective pairs within acluster.
Workers were presented with 15 word pairsin each worker session.
For each pair (a1, a2), theworker had to pick one of four options: (1) a1 isstronger than a2, (2) a2 is stronger than a1, (3) bothare equally strong, and (4) a1 and a2 are not com-parable.
Option (4) was present because our clus-ters possibly contained adjectives that are not on thesame scale.
As in the previous task for getting an-notations for clusters, we inserted two items with aclear ranking (e.g., hot, hotter) for every set of 15487Gold standard clusters Automatic clustersdeceptive, false, fraudulent, misleading false, misleading, unreliable, wrongevil, immoral, sinful, wrong desperate, humiliated, immoral, insane, sinfuldangerous, risky, suicidal, unreliable dangerous, harmful, toxicTable 1: Example comparison of automatically derived clusters against gold standard clusters from WordNet.Data Corpus for strength counts Clustering Rankingin MILP ranking Accuracy Pairwise AccuracyClusters automatically derived from Google N-grams 74.36 84.74non-polysemous WordNet adjectives PubMed 74.36 69.23PubMed-derived clustering:Regular adjectives PubMed 86.26 70.37Domain-specific adjectives PubMed 64.30 ?PubMed-derived clustering:Regular adverbs PubMed 89.36 71.00Domain-specific adverbs PubMed 53.80 ?Table 2: AMT-based evaluations of cluster accuracy and pairwise ranking accuracy of systems that vary in the sourceof clustering data, source of strength counts, and part of speech.
For comparison, the approach used by de Melo andBansal (2013) achieves a pairwise ranking accuracy of 76.1% on the non-polysemous WordNet clusters.pairs to avoid random annotations.
Each set was an-notated by 10 workers.
All workers passed all thechecks and we did not discard any annotations forthis task.
To create a gold standard we assigned eachpair one of four labels, weaker, stronger, equal, ornot comparable.
A value was assigned based on amajority vote.
In case of a tie, the pair was assigneda label of being equal.In order to compute a ranking, the MILP needstwo inputs: 1) the cluster of terms that are onthe same scale, and 2) the counts for how manytimes all pairs of adjectives in that cluster satisfiedthe weak-strong and strong-weak patterns (hence-forth referred to as ?strength counts?).
In the firstexperiment of ranking adjectives, we ran the fullpipeline used by (de Melo and Bansal, 2013) on the256 adjective (84 hard cluster) subset of their goldstandard (see Section 4.1).
Thus, this experimentuses hand-corrected WordNet dumbbells to deter-mine adjectives on the same scale of semantic in-tensity, followed by the MILP using strength countsfrom the Google N-gram corpus, to determine theranking.
Their pipeline resulted in a pairwise accu-racy of 76.1% which serves as a baseline for com-parison.
In the second experiment of ranking ad-jectives, we used the 50 automatically derived ad-jective clusters described in Section 4.1 as an inputfor the MILP.
Since these adjectives originate fromWordNet dumbbells, we refer to them as ?Word-Net adjective clusters.?
We determined the rankingfor adjectives within these clusters using strengthcounts obtained from our PubMed corpus.
We ob-tained an accuracy of 69.23% across 105 pairs.
Thestrength counts for all adjectives in these clusters,from Google N-grams corpus, used in the exper-iments of (de Melo and Bansal, 2013) were alsoavailable to us by the authors.
We repeated the previ-ous experiment by substituting strength counts fromPubMed corpus with these strength counts from theGoogle N-grams corpus and obtained an accuracy of84.74% across 119 pairs.3It appears from our ex-periment that pattern counts from a general corpus3The MILP does not produce a strength relationship be-tween a pair of adjectives if there are no strength counts for thispair.
Hence, we observe a difference in the number of pairs forwhich accuracy is determined in the two ranking experiments.488is a better match for determining the adjective or-dering than a more-limited domain corpus, despitethe limitation of Google N-grams being restrictedto 5-word sequences.
We think this is because oftwo reasons: First, Google N-grams is a very largecorpus compared to the one we use.
Second, ourcorpus consists of abstracts and not full text of sci-entific articles from PubMed.
Hence there is lessvariety in the language used; capturing fewer com-parative constructs than Google N-grams.
However,it is interesting that we can still extract patterns fromdomain-specific corpora to act as constraints for theMILP process.5 Rankings for adjectives extracted fromPubMedWe also desired to see how well our approachdoes on terms that are not specifically in Word-Net, but present in a domain-specific corpus suchas PubMed.
We therefore also evaluate the cluster-ing and ranking steps on a set of adjectives extractedfrom the PubMed data using structural patterns.5.1 ClusteringSince there was no gold standard reflecting idealclustering of data, we explored heuristic measuresto choose parameters for our clustering step.
Weused CBOW vectors over skip-gram vectors sincethese were more effective in the previous experi-ment.
Since the true value for number of clustersk was unknown, we chose k such that the averagecardinality of a cluster was three.
The value of kwas found to be the same (k = 375) for all cluster-ing experiments conducted using vector dimensionsizes varying from 200 to 800 in increments of 100.To choose the right dimension size d of the CBOWvectors for this fixed value of k, we obtained clustersfor incremental values of d from 200 to 800 in incre-ments of 100.
We determined the number of iden-tical clusters obtained using a particular value of dwith its next increment.
The lowest value of d whichresulted in a maximum number of identical clusterswith its next increment was chosen: d = 400.Using vectors of 400 dimensions, we obtained375 adjective clusters with cardinality varying from1 to 9.
Since these clusters were derived fromour biomedical dataset, they comprised of domain-specific adjectives, which are quite unfamiliar evento native English speakers.
We manually partitionedthe clusters into two sets: (i) containing domain-specific words, and (ii) containing words used inday-to-day English (henceforth referred to as ?regu-lar?
terms).
Examples of clusters from both sets aresummarized in Table 3.
The clusters we obtain lookreasonable, grouping together adjectives that per-tain to the same scale.
The first cluster of domain-specific adjectives qualifies the nouns correspond-ing to different types of protein with varying degreeof specificity, the second cluster contains differentqualifications of a tumor, and adjectives in the thirdcluster qualify different parts of a living cell.
Forthe regular adjective clusters, the clusters look intu-itive too, except for the first cluster.
The adjectivesmale and female are not scalar, but match the struc-tural patterns, and are grouped together with adjec-tives describing age qualifications, due to a strongcontext overlap in which these words are used.Clusters of domain-specific adjectivescytokine, gm-csf, ifn-gamma, il-10, il-12, il-2benign, malignant, metastatic, neoplastic, squamousmitochondrial, nuclear, ribosomalClusters of regular adjectivesfemale, male, middle-aged, older, young, youngeraccurate, precise, reliable, reproducible, robustadditive, insignificant, negligibleTable 3: Examples of automatically derived adjectiveclusters from PubMed abstracts.We randomly sampled 25 clusters from eachset, ?regular adjectives?
and ?domain-specific adjec-tives?, for our evaluation.
We evaluated the cluster-ing quality of the regular adjectives using the exactsame approach as described in Section 4.1.
We ob-tained a clustering accuracy of 86.26% for 25 clus-ters across 101 regular adjectives.
This is substan-tially better than the performance of clustering in theprevious experiment.
We believe that this is due tothe fact that the adjectives in the dataset used in theprevious experiment originate from WordNet andcontain many words (e.g., handsome, crazy, spicy),489which are less likely to be found in scientific ab-stracts.
Therefore, the context vectors learnt forthese words are possibly less accurate compromis-ing the clustering quality.For the domain-specific adjectives, the annota-tions require specialized skills.
PubMed hosts sci-entific articles from different disciplines of biolog-ical sciences.
We obtained annotations from threeannotators specializing in disciplines of BiomedicalInformatics, Biochemistry, and Nursing.
To createthe gold standard, a word was retained or discardedfrom a cluster if two or more annotators agreed onit.
We obtained a clustering accuracy of 64.3% for25 clusters across 101 domain-specific adjectives.5.2 RankingWe obtained gold standard annotations for rankingusing AMT for these 25 ?regular adjective?
clus-ters derived from the PubMed corpus using the ex-act same methodology as described in Section 4.2.The strength counts for these adjectives were alsoderived from the PubMed corpus.
We obtained anaccuracy of 70.37% across 109 pairs, indicating asimilar level of performance to WordNet-based clus-ters.Our expert annotators for the domain-specific ad-jectives faced problems in assessing an ordering be-tween adjectives in a cluster.
They report that formajority of the clusters, the ordering of the wordswould vary given the context.
For example, con-sider the following modifications of the domain spe-cific adjectives of the third cluster in Table 3: ri-bosomal particles, mitochondiral compartments andnuclear compartments, representing different partsof a living cell.
If we consider ?number of?
as therelation in context, we get (ribosomal > mitochon-drial > nuclear) as an ordering since number of ri-bosomal particles is greater than number of mito-chondrial compartments, and number of mitochon-drial compartments is greater than number of nu-clear compartments.
However, if we consider ?sizeof?
as the context, the ordering is reversed.6 Extension to adverbsA novelty of our approach is that we can also applythe technique to other parts of speech (e.g., adverbs).The structural patterns we describe in Section 3.1Clusters of domain-specific adverbsanteriorly, caudally, distally, proximallychromosomally, clonally, genetically, phenotypicallyneonatally, prenatally, postnatallyClusters of regular adverbsalways, certainly, inevitably, invariably, universallymarginally, modestly, slightly, somewhatexcessively, inappropriately, overlyTable 4: Examples of automatically derived adverb clus-ters from PubMed abstracts.also enable us to extract candidate scalar adverbs.We follow a similar approach to adjectives: extractadverbs, derive strength counts and rank them usingthe MILP.6.1 ClusteringWe used CBOW vectors to perform clustering andderived k = 300 and d = 250 using the approachdescribed in Section 5.
As with the adjective clus-ters, we found that there were also domain-specificadverbs, illustrated in Table 4.
Again, the clus-ters obtained look reasonable.
The first cluster ofdomain-specific adverbs describes relative positionof a body part, the second cluster corresponds to ad-verbs describing identity of a gene that may have anobservable effect, the third cluster represents tem-poral descriptions that relate an event to child birth.The clustering of regular adverbs is accurate, ex-cept for the third cluster where inappropriately wasfound to be an outlier based on our annotations.
Wefollowed a similar approach to the adjective exper-iment, creating two partitions for domain-specificand regular adverbs and sampling 25 clusters fromeach.
Annotations for regular adverbs were obtainedfrom AMT while annotations for domain-specificadverbs were obtained from 3 domain experts.
Theannotation process for both clusters of adverbs wasidentical to that of adjectives.
We obtained a micro-averaged accuracy of 89.36% for 25 clusters across104 regular adverbs and a 53.8% for 25 clustersacross 89 domain-specific adverbs.490Accuracy POS ExamplesGood Adj serious< life-threatening< fatalGood Adv considerably< significantly< dramaticallyAverage Adj common< frequent= prevalentAverage Adv slightly< modestly< marginallyBad Adj useful < helpfulBad Adv continuously = regularlyTable 5: Example rankings for adjectives and adverbsfrom PubMed data.6.2 RankingAs in the case of adjectives, our annotators fordomain-specific adverbs faced a challenge in rank-ing adverbs due to lack of context.
Therefor we donot report results on ranking of adverbs.
We ob-tained gold standard annotations for ranking usingAMT for 25 clusters of regular adverbs derived fromthe PubMed corpus, using the exact same method-ology as described in Section 4.2.
The strengthcounts for these adverbs were also derived fromthe PubMed corpus.
We obtained an accuracy of71.00% across 38 pairs ?
a performance similar tothe adjectives.
However, we observe that there are alarge number of pairs for which there are no strengthcounts, and the MILP does not generate a ranking.Table 5 shows sample results for ranking adjectivesand adverbs from the PubMed data.7 Limitations and future workWe present an approach to gradable modifier order-ing that replaces WordNet-based clusters with au-tomatically derived word clusters, replaces lexicalpatterns with structural patterns, and show that theapproach has utility for not only discovering adjec-tive patterns but also adverb patterns in biomedicaltext.
We observe that while automatic ranking basedon semantic intensity can be successful establishedbetween regular terms, doing so for domain-specificterms requires knowledge of context.We plan to expand the structure patterns derivedfrom the lexical patterns of de Melo and Bansal(2013), looking for new patterns that could be moresuited for adverbs.
We also plan to investigate softclustering algorithms such as (Pereira et al, 1993)that may allow us to model polysemous words bet-ter.
Furthermore, recent studies have compared tra-ditional vectors against embedded vectors (such asthe CBOW vectors used in this study) for differentlexical semantic tasks (Levy and Goldberg, 2014;Baroni et al, 2014), which suggests that such a com-parison for our clustering task could be insightful.Our experimental results show that automaticclustering of gradable words produces promising re-sults.
However, we also observe that with domain-specific words, context is important for establishinga ranking between words that is based on seman-tic intensity.
Thus, rather than clustering adjectivesor adverbs in isolation, a joint with the clusteringof nouns or verbs with which they occur is a pos-sible direction of research.
Finally, studies derivinga ranking based on semantic intensities are limitedto unigrams belonging to different parts of speech.Our future work would focus on performing a sim-ilar task on bigrams consisting of adverb-adjectivepairs (e.g., somewhat unclear < quite hard < verydifficult) that exhibit properties of gradability.AcknowledgementsWe would like to thank Mohit Bansal and Gerardde Melo for providing the implementation and re-sults of the experiments from their previous work.Research reported in this publication was supportedby the National Library of Medicine of the Na-tional Institutes of Health under award numberR01LM011116.
The content is solely the respon-sibility of the authors and does not necessarily rep-resent the official views of the National Institutes ofHealth.ReferencesDavid Arthur and Sergei Vassilvitskii.
2007.
K-means++: The advantages of careful seeding.
In Pro-ceedings of the Eighteenth Annual ACM-SIAM Sympo-491sium on Discrete Algorithms, SODA ?07, pages 1027?1035, Philadelphia, PA, USA.
Society for Industrialand Applied Mathematics.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proceed-ings of the 36th Annual Meeting of the Associationfor Computational Linguistics and 17th InternationalConference on Computational Linguistics - Volume 1,ACL ?98, pages 86?90, Stroudsburg, PA, USA.Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014.Tailoring continuous word representations for depen-dency parsing.
In Proceedings of the Annual Meetingof the Association for Computational Linguistics.Marco Baroni, Georgiana Dinu, and Germ?an Kruszewski.2014.
Don?t count, predict!
A systematic compari-son of context-counting vs. context-predicting seman-tic vectors.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 238?247.
Associationfor Computational Linguistics.Thorsten Brants and Alex Franz.
2006.
The Google Web1T 5-gram Version 1.1.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL recognising textual entailmentchallenge.
In Joaquin Qui?nonero Candela, Ido Dagan,Bernardo Magnini, and Florence DAlch?e-Buc, editors,Proceedings of the First International Conference onMachine Learning Challenges: Evaluating PredictiveUncertainty Visual Object Classification, and Recog-nizing Textual Entailment, volume 3944 of LectureNotes in Computer Science, pages 177?190, Berlin,Heidelberg, April.Marie-Catherine de Marneffe, Christopher D. Manning,and Christopher Potts.
2010.
?Was it good?
It wasprovocative?.
Learning the meaning of scalar adjec-tives.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages167?176, July.Gerard de Melo and Mohit Bansal.
2013.
Good, Great,Excellent: Global Inferences of Semantic Intensities.Transactions of the Association of Computational Lin-guistics, 1(July):279?290.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1993.
Towards the automatic identification of adjecti-val scales.
In Proceedings of the 31st Annual meetingon Association for Computational Linguistics, pages172?182, Morristown, NJ, USA, June.Christopher Kennedy.
2007.
Vagueness and grammar:the semantics of relative and absolute gradable adjec-tives.
Linguistics and Philosophy, 30(1):1?45, March.Joo-Kyung Kim and Marie-Catherine de Marneffe.
2013.Deriving Adjectival Scales from Continuous SpaceWord Representations.
Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1625?1630.Dan Klein and Christopher D. Manning.
2003.
AccurateUnlexicalized Parsing.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics - Volume 1, ACL ?03, pages 423?430, Strouds-burg, PA, USA.Roger Levy and Galen Andrew.
2006.
Tregex and Tsur-geon: tools for querying and manipulating tree datastructures.
In Proceedings of the Fifth InternationalConference on Language Resources and Evaluation,pages 2231?2234.
Citeseer.Omer Levy and Yoav Goldberg.
2014.
Linguistic Reg-ularities in Sparse and Explicit Word Representations.In Proceedings of the Eighteenth Conference on Com-putational Language Learning, Batimore, MarylandUSA.
Association for Computational Linguistics.2008.
LingPipe 4.1.0.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-rado, and Jeff Dean.
2013.
Distributed Representa-tions of Words and Phrases and their Composition-ality.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135, January.Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993.Distributional clustering of English words.
In Pro-ceedings of the 31st Annual Meeting on Associationfor Computational Linguistics, ACL ?93, pages 183?190, Stroudsburg, PA, USA.Josef Ruppenhofer, Michael Wiegand, and Jasper Bran-des.
2014.
Comparing methods for deriving intensityscores for adjectives.
In 14th Conference of the Eu-ropean Chapter of the Association for ComputationalLinguistics, pages 117?122, Gothenburg, Sweden.Edward Sapir.
1944.
Grading, A Study in Semantics.Philosophy of Science, 11(2):93?116.Peter F Schulam and Christiane Fellbaum.
2010.
Auto-matically determining the semantic gradation of Ger-man adjectives.
Semantic Approaches to NaturalLanguage Proceedings, Saarbruecken, Germany, page163.Vera Sheinman, Takenobu Tokunaga, Isaac Julien, Pe-ter Schulam, and Christiane Fellbaum.
2012.
Refin-ing WordNet adjective dumbbells using intensity rela-tions.
In Sixth International Global Wordnet Confer-ence, pages 330?337, Matsue, Japan.Vera Sheinman, Christiane Fellbaum, Isaac Julien, Pe-ter Schulam, and Takenobu Tokunaga.
2013.
Large,huge or gigantic?
Identifying and encoding intensityrelations among adjectives in WordNet.
Language Re-sources and Evaluation, 47(3):797?816, January.492Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural languagetasks.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 254?263, Honolulu, USA, October.493
