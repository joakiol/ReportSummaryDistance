ROBUST CONTINUOUS SPEECH RECOGNIT IONJohn Makhoul and Richard Schwartzmakhoul@bbn.com, schwartz@bbn.comBBN Systems and Technologies70 Fawcett St.Cambridge, MA 021381.
PROJECT GOALSThe primary objective of this basic research program is todevelop robust methods and models for speaker-independentacoustic recognition of spontaneously-produced, continuousspeech.
The work has focussed on developing accurate anddetailed models of phonemes and their coartieulation for thepurpose of large-vocabulary continuous peech recognition.Important goals of this work are to achieve the highestpossible word recognition accuracy in cominuous peech andto develop methods for the rapid adaptation of phoneticmodels to the voice of a new speaker.2.
RECENT RESULTSDuring the last year, we have:Developed a new 5-pass decoding algorithm that allows usto incorporate trigram language models and cross-wordcoarticulation models directly within the N-best search.The new decoder is considerably faster than the previousone and results in slightly higher accuracy.Participated in the December 1993 ARPA evaluations.
Onthe baseline hub test, we achieved a 14.3% word error rate.Our result for the primary test in which we expanded thevocabulary and grammar was 12.3%, which wassubstantially better than any result produced by an ARPAsite, and second only to one other result.In a spoke test for outlier speakers, our overall results howthat the baseline performance for speakers with foreignaccents is 4 times worse than that for native speakers.
Byusing speaker adaptation, the error rate was reduced by morethan a factor of 2.In a spoke test for known alternate microphones, ourrecognition performance with the boom microphone in thecross-channel condition did not degrade much relative tothe control condition.In the spoke for spomaneous dictation, we increased thevocabulary from 20K to 4OK words, and also added abouti000 words that occurred in the spontaneous training databut not in the original vocabulary.
This reduced the worderror from 26% to 20%.Considered several powerful models to use in searchalgorithms, including segmental neural networks (under aseparate effort), a 13-state phoneme model, and astochastic segment model (in collaboration with BostonUniversity).
The combination of all of the modelsproduced the lowest error rate.Began exploring a new method for system adaptation tospeakers, called auto-adaptation.
This method willimprove performance by making appropriate use of theinformation that a whole utterance is spoken by the samespeaker in a single envLonment.Performed experiments o better understand issues relatingto microphone independence.
We developed a technique inwhich the training is performed with a single high qualitymicrophone, and the test utterance with the unknownmicrophone is transformed to resemble the trainingmicrophone as much as possible.
We found that ouralgorithm was able to classify the microphone into thecorrect microphone class about 98% of the time, and theresulting normalization reduced the word error rate by 33%.ChaSed the CCCC (CSR Corpus Coordinating Committee),and participated in other committees.
The CCCC wasresponsible for developing the "hub and spokes" paradigmfor the evaluation of CSR systems.3.
PLANS FOR THE COMING YEARWe will continue our work on improving speech recognitionperformance both on the Wall Street Journal corpus and on thespontaneous ATIS speech corpus.
Work will focus onimproved phonetic models, adaptation methods, androbustness against different acoustic channels and newvocabularies and grammars.445
