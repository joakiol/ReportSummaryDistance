Modeling Context in Scenario Template CreationLong Qiu, Min-Yen Kan, Tat-Seng ChuaDepartment of Computer ScienceNational University of SingaporeSingapore, 117590{qiul,kanmy,chuats}@comp.nus.edu.sgAbstractWe describe a graph-based approach to Sce-nario Template Creation, which is the taskof creating a representation of multiple re-lated events, such as reports of different hur-ricane incidents.
We argue that context isvaluable to identify important, semanticallysimilar text spans from which template slotscould be generalized.
To leverage context,we represent the input as a set of graphswhere predicate-argument tuples are ver-tices and their contextual relations are edges.A context-sensitive clustering framework isthen applied to obtain meaningful tuple clus-ters by examining their intrinsic and extrin-sic similarities.
The clustering frameworkuses Expectation Maximization to guide theclustering process.
Experiments show that:1) our approach generates high quality clus-ters, and 2) information extracted from theclusters is adequate to build high coveragetemplates.1 IntroductionScenario template creation (STC) is the problem ofgenerating a common semantic representation froma set of input articles.
For example, given multiplenewswire articles on different hurricane incidents,an STC algorithm creates a template that may in-clude slots for the storm?s name, current location, di-rection of travel and magnitude.
Slots in such a sce-nario template are often to be filled by salient entitiesin the scenario instance (e.g., ?Hurricane Charley?or ?the coast area?)
but some can also be filled byprominent clauses, verbs or adjectives that describethese salient entities.
Here, we use the term salientaspect (SA) to refer to any of such slot fillers thatpeople would regard as important to describe a par-ticular scenario.
Figure 1 shows such a manually-built scenario template in which details about im-portant actions, actors, time and locations are codedas slots.STC is an important task that has tangible bene-fits for many downstream applications.
In the Mes-sage Understanding Conference (MUC), manually-generated STs were provided to guide InformationExtraction (IE).
An ST can also be viewed as reg-ularizing a set of similar articles as a set of at-tribute/value tuples, enabling multi-document sum-marization from filled templates.Despite these benefits, STC has not receivedmuch attention by the community.
We believe thisis because it is considered a difficult task that re-quires deep NL understanding of the source articles.A problem in applications requiring semantic simi-larity is that the same word in different contexts mayhave different senses and play different roles.
Con-versely, different words in similar contexts may playsimilar roles.
This problem makes approaches thatrely on word similarity alone inadequate.We propose a new approach to STC that incor-porates the use of contextual information to addressthis challenge.
Unlike previous approaches that con-centrate on the intrinsic similarity of candidate slotfillers, our approach explicitly models contextual ev-idence.
And unlike approaches to word sense disam-biguation (WSD) and other semantic analyses that157use neighboring or syntactically related words ascontextual evidence, we define contexts by semanticrelatedness which extends beyond sentence bound-aries.
Figure 2 illustrates a case in point with twoexcerpts from severe storm reports.
Here, althoughthe intrinsic similarity of the main verbs ?hit?
and?land?
is low, their contextual similarity is high asboth are followed by clauses sharing similar subjects(hurricanes) and the same verbs.
Our approach en-codes such contextual information as graphs, map-ping the STC problem into a general graph overlayproblem that is solvable by a variant of ExpectationMaximization (EM).Our work also contributes resources for STC re-search.
Until now, few scenario templates have beenpublicly available (as part of MUC), rendering anypotential evaluation of automated STC statisticallyinsignificant.
As part of our study, we have com-piled a set of input articles with annotations that weare making available to the research community.Scenario Template: StormStorm Name CharleyStorm Action landedLocation Florida?s Gulf coastTime Friday at 1950GMTSpeed 145 mphVictim Category 1 13 peopleAction diedVictim Category 2 over one millionAction affectedFigure 1: An example scenario template (filled).2 Related WorkA natural way to automate the process of STC is tocluster similar text spans in the input article set.
SAsthen emerge through clustering; if a cluster of textspans is large enough, the aspects contained in it willbe considered as SAs.
Subsequently, these SAs willbe generalized into one or more slots in the template,depending on the definition of the text span.
As-suming scenarios are mainly defined by actions, thefocus should be on finding appropriate clusters fortext spans each of which represents an action.
Mostof the related work (although they may not directlyaddress STC) shares this assumption and performsCharley landed further south on Florida?sGulf coast than predicted, ...
The hurricane... has weakened and is moving over SouthCarolina.At least 21 others are missing after the stormhit on Wednesday.
....
But Tokage hadweakened by the time it passed over Japan?scapital, Tokyo, where it left little damage be-fore moving out to sea.Figure 2: Contextual evidence of similarity.
Curvedlines indicate similar contexts, providing evidencethat ?land?
and ?hit?
from two articles are semanti-cally similar.action clustering accordingly.
While the target ap-plication varies, most systems that need to group textspans by similarity measures are verb-centric.In addition to the verb, many systems expandtheir representation by including named entity tags(Collier, 1998; Yangarber et al, 2000; Sudo et al,2003; Filatova et al, 2006), as well as restrict-ing matches (using constraints on subtrees (Sudo etal., 2003; Filatova et al, 2006), predicate argumentstructures (Collier, 1998; Riloff and Schmelzen-bach, 1998; Yangarber et al, 2000; Harabagiu andMaiorano, 2002) or semantic roles).Given these representations, systems then clustersimilar text spans.
To our knowledge, all currentsystems use a binary notion of similarity, in whichpairs of spans are either similar or not.
How they de-termine similarity is tightly coupled with their textspan representation.
One criterion used is patternoverlap: for example, (Collier, 1998; Harabagiu andLacatusu, 2005) judge text spans to be similar if theyhave similar verbs and share the same verb argu-ments.
Working with tree structures, Sudo et al andFilatova et al instead require shared subtrees.Calculating text span similarity ultimately boilsdown to calculating word phrase similarity.
Ap-proaches such as Yangarber?s or Riloff andSchmelzenbach?s do not employ a thesaurus andthus are easier to implement, but can suffer fromover- or under-generalization.
In certain cases, ei-ther the same actor is involved in different actions ordifferent verbs realize the same action.
Other sys-tems (Collier, 1998; Sudo et al, 2003) do employ158lexical similarity but threshold it to obtain binaryjudgments.
Systems then rank clusters by clustersize and correlation with the relevant article set andequate top clusters as output scenario slots.3 Context-Sensitive Clustering (CSC)Automating STC requires handling a larger degreeof variations than most previous work we have sur-veyed.
Note that the actors involved in actions in ascenario generally differ from event to event, whichmakes most related work on text span similarity cal-culation unsuitable.
Also, action participants are notlimited to named entities, so our approach needs toprocess all NPs.
As both actions and actors may berealized using different words, a similarity thesaurusis necessary.
Our approach to STC uses a thesaurusbased on corpus statistics (Lin, 1998) for real-valuedsimilarity calculation.
In contrast to previous ap-proaches, we do not threshold word similarity re-sults; we retain their fractional values and incorpo-rate these values holistically.
Finally, as the sameaction can be realized in different constructions, thesemantic (not just syntactic) roles of verb argumentsmust be considered, lest agent and patient roles beconfused.
For these reasons, we use a semantic rolelabeler (Pradhan et al, 2004) to provide and delimitthe text spans that contain the semantic argumentsof a predicate.
We term the obtained text spans aspredicate argument tuples (tuples) throughout thepaper.
The semantic role labeler reportedly achievesan F 1 measure equal to 68.7% on identification-classification of predicates and core arguments on anewswire text corpus (LDC, 2002).
Within the con-fines of our study, we find it is able to capture mostof the tuples of interest.Our approach explicitly captures contextual ev-idence.
We define a tuple?s contexts as other tu-ples in the same article segment where no topic shiftoccurs.
This definition refines the n-surroundingword constraint commonly used in spelling correc-tion (for example, (Hirst and Budanitsky, 2005)),Word Sense Disambiguation ((Preiss, 2001), (Leeand Ng, 2002), for instance), etc.
while still en-sures the relatedness between a tuple and its con-texts.
Specifically, a tuple is contextually related toother tuples by two quantifiable contextual relations:argument-similarity and position-similarity.
For ourexperiments, we use the leads of newswire articlesas they normally summarize the news.
We also as-sume a lead qualifies as a single article segment, thusmaking all of its tuples as potential contexts to eachother.from A2from A1weakened(storm)v21hit(storm)v22moving(storm)v23weakened(hurricane)v11landed(hurricane)v12moving(hurricane)v13e21,2e22,1 e21,3e23,1e22,3e23,2e11,2e12,1 e11,3e13,1e12,3e13,2Figure 3: Being similar contexts, ?weakened?
and?moving?
provide contextual evidence that ?land?and ?hit?
are similar.First, we split the input article leads into sentencesand perform semantic role labeling immediately af-terwards.
Our system could potentially benefit fromadditional pre-processing such as co-reference reso-lution.
Currently these pre-processing steps have notbeen properly integrated with the rest of the system,and thus we have not yet measured their impact.We then transform each lead Ai into a graph Gi ={V i, Ei}.
As shown in Figure 3, vertices V i ={vij}(j = 1, ..., N) are the N predicate argumenttuples extracted from the ith article, and directededges Ei = {eim,n = (vim, vin)} reflect contextualrelations between tuple vim and vin.
Edges only con-nect tuples from the same article, i.e., within eachgraph Gi.
We differentiate between two types ofedges.
One is argument-similarity, where the twotuples have semantically similar arguments.
Thismodels tuple cohesiveness, where the edge weight isdetermined by the similarity score of the most sim-ilar inter-tuple argument pair.
The other is position-similarity, represented as the offset of the ending tu-ple with respect to the other, measured in sentences.This edge type is directional to account for simplecausality.Given this set of graphs, the clustering task is tofind an optimal alignment of all graphs (i.e., super-imposing the set of article graphs to maximize vertexoverlap, constrained by the edges).
We adapt Expec-tation Maximization (Dempster et al, 1977) to find159an optimal clustering.
This process assigns tuples tosuitable clusters where they are semantically similarand share similar contexts with other tuples.
Algo-rithm 1 outlines this alignment process.Algorithm 1 Graph Alignment(G)/*G is a set of graph {Gi}*/T ?
all tuples in GC ?
highly cohesive tuples clustersother?
remaining tuples semantically connected with CC[C.length]?
otherrepeat/*E step*/for each i such that i < C.length dofor each j such that j < C.length doif i == j thencontinue;re-estimate parameters[C[i],C[j]] /*distributionparameters of edges between two clusters*/tupleReassigned = false /*reset*//*M step*/for each i such that i < T.length doaBestLikelihood = T [i].likelihood; /*likelihood ofbeing in its current cluster*/for each tuple tcontxt that contextually related withT [i] dofor each cluster ccand, any candidate cluster thatcontextually related with tcontxt.cluster doP (T [i] ?
ccand) = comb(Ps, Pc)likelihood = log(P (T [i] ?
ccand))if likelihood > aBestLikelihood thenaBestLikelihood = likelihoodT [i].cluster = ccandtupleReassigned = trueuntil tupleReassigned == false /*alignment stable*/returnDuring initialization, tuples whose pairwise simi-larity higher than a threshold ?
are merged to formhighly cohesive seed clusters.
To compute a con-tinuous similarity Sim(ta, tb) of tuples ta and tb,we use the similarity measure described in (Qiu etal., 2006), which linearly combines similarities be-tween the semantic roles shared by the two tuples.Some other tuples are related to these seed clus-ters by argument-similarity.
These related tuples aretemporarily put into a special ?other?
cluster.
Thecluster membership of these related tuples, togetherwith those currently in the seed clusters, are to befurther adjusted.
The ?other?
cluster is so called be-cause a tuple will end up being assigned to it if itis not found to be similar to any other tuple.
Tuplesthat are neither similar to nor contextually related byargument-similarity to another tuple are termed sin-gletons and excluded from being clustered.We then iteratively (re-)estimate clusters of tuplesacross the set of article graphs G. In the E-step of theEM algorithm, all contextual relations between eachpair of clusters are collected as two set of edges.Here we assume argument-similarity and position-similarity are independent and thus we differenti-ate them in the computation.
Accordingly, thereare two sets: edgesas and edgesps.
For simplicity,we assume independent normal distributions for thestrength of each set (inter-tuple argument similarityfor edgesas and sentence distance for edgesps).
Theedge strength distribution parameters for both setsbetween each pair of clusters are re-estimated basedon current edges in edgesas and edgesps.In the M-step, we examine each tuple?s fitness forbelonging to its cluster and relocate some tuples tonew clusters to maximize the likelihood given thelatest estimated edge strength distributions.
In thefollowing equations, we denote the proposition thatpredicate argument tuple ta belongs to cluster cm asta?cm; a typical tuple (the centroid) of the clustercm as tcm ; and the cluster of ta as cta .
The objectivefunction to maximize is:Obj(G) =Xta?Glog(P (ta?cta)), (1)where P (ta?cm) = 2Ps(ta?cm) Pc(ta?cm)Ps(ta?cm) + Pc(ta?cm) .
(2)Equation 2 takes the harmonic mean of two factors:a contextual factor Pc and and a semantic factor Ps:Pc(ta?cm) = max{P (edges(ta, tb)|tb:edges(ta,tb)6=nulledges(cm, ctb ))}, (3)Ps(ta?cm) =(simdefault, cm = cother,Sim(ta, tcm), otherwise.
(4)Here the contextual factor Pc models how likelyta belongs to cm according to the contextual infor-mation, i.e., the conditional probability of the con-textual relations between cm and ctb given the con-textual relations between ta and one particular con-text tb, which maximizes this probability.
Accord-ing to Bayes?
theorem, it is computed as shown inEquation 3.
In practice, we multiply two conditionalprobabilities: P (edgeas(ta, tb)|edgesas(cm, ctb))and P (edgeps(ta, tb)|edgesps(cm, ctb)), assumingindependence between edgesas and edgesps.We assume there are still singleton tuples that arenot semantically similar to another tuple and shouldbelong to the special ?other?
cluster.
Given that they160are dissimilar to each other, we set simdefault toa small nonzero value in Equation 4 to prevent the?other?
cluster from expelling them based on theirlow semantic similarity.
Tuples?
cluster member-ships are recalculated, and the parameters describ-ing the contextual relations between clusters are re-estimated.
New EM iterations are performed as longas one or more tuple relocations occur.
Once theEM halts, clusters of equivalent tuples are formed.Among these clusters, some correspond to salientactions that, together with their actors, are all SAsto be generalized into template slots.
Cluster sizeis a good indicator of salience, and each large clus-ter (excluding the ?other?
cluster) can be viewed ascontaining instances of a salient action.Formulating the clustering process as a variant ofiterative EM is well-motivated as we consider thesimilarity scores as noisy and having missing obser-vations.
Calculating semantic similarity is at bestinaccurate.
Thus it is difficult to cluster tuples cor-rectly based only on their semantic similarity.
Alsoto check whether a tuple shares contexts with a clus-ter of tuples, the cluster has to be relatively clean.An iterative EM as we have proposed naturally im-prove the cleanness of these tuple clusters graduallyas new similarity information comes to light.4 EvaluationFor STC, we argue that it is crucial to cluster tupleswith high recall so that an SA?s various surfaceforms can be captured and the size of clusters canserve as a salience indicator.
Meanwhile, precisionshould not be sacrificed, as more noise will hamperthe downstream generalization process whichoutputs template slots.
We conduct experimentsdesigned to answer two relevant research questions:1) Cluster Quality: Whether using contexts (inCSC) produces better clustering results than ignor-ing it (in the K-means baseline); and2) Template Coverage: Whether slots generalizedfrom CSC clusters cover human-defined templates.4.1 Data Set and BaselineA straightforward evaluation of a STC system wouldcompare its output against manually-prepared goldstandard templates, such as those found in MUC.Unfortunately, such scenario templates are severelylimited and do not provide enough instances for aproper evaluation.
To overcome this problem, wehave prepared a balanced news corpus, where wehave manually selected articles covering 15 scenar-ios.
Each scenario is represented by a total of 45 to50 articles which describe 10 different events.Our baseline is a standard K-means clusterer.
Itsinput is identical to that of CSC ?
the tuples ex-tracted from relevant news articles and are not ex-cluded from being clustered by CSC in the initial-ization stage (refer to Section 3) ?
and employs thesame tuple similarity measure (Qiu et al, 2006).
Thedifferentiating factor between CSC and K-means isthe use of contextual evidence.
A standard K-meansclusterer requires a k to be specified.
For each sce-nario, we set its k as the number of clusters gener-ated by CSC for direct comparison.We fix the test set for each scenario as ten ran-domly selected news articles, each reporting a dif-ferent instance of the scenario; the development set(which also serves as the training set for determin-ing the EM initialization threshold ?
and simdefaultin Equation 4) is a set of ten articles from the ?Air-linerCrash?
scenario, which are excluded from thetest set.
Both systems analyze the first 15 sentencesof each article, and sentences generate 2 to 3 predi-cate argument tuples on average, resulting in a totalof 10 ?
15 ?
(2 to 3) = 300 to 450 tuples for eachscenario.4.2 Cluster QualityThis experiment compares the clustering results ofCSC and K-means.
We use the standard cluster-ing metrics of purity and inverse purity (Hotho etal., 2003).
The first author manually constructed thegold standard clusters for each scenario using a GUIbefore conducting any experiments.
A special clus-ter, corresponding to the ?other?
cluster in the CSCclusters, was created to hold the singleton tuples foreach scenario.
Table 1 shows this under the column?#Gold Standard Clusters?.Using the manual clusters as the gold standard, weobtain the purity (P) and inverse purity (IP) scoresof CSC and K-means on each scenario.
In Table 1,we see that CSC outperforms K-means on 10 of 15scenarios for both P and IP.
For the remaining 5 sce-narios, where CSC and K-means have comparable161P scores, the IP scores of CSC are all significantlyhigher than that of K-means.
This suggests clus-ters tend to be split apart more in K-means than inCSC when they have similar purity.
One thing worthmentioning here is that the ?other?
cluster normallyis relatively large for each scenario, and thus mayskew the results.
To remove this effect, we excludedtuples belonging to the CSC ?other?
cluster from theK-means input, generating one fewer cluster.
Run-ning the evaluation again, the resulting P-IP scoresagain show that CSC outperforms the baseline K-means.
We only report the results for all tuples inour paper for simplicity.#Gold Std.
CSC K-meansScenario Clusters P IP P IPAirlinerCrash 23 .61 .42 .52 .28Earthquake 18 .60 .44 .53 .30Election 10 .77 .49 .75 .21Fire 14 .65 .44 .64 .26LaunchEvent 12 .77 .37 .73 .22Layoff 10 .71 .28 .70 .19LegalCase 8 .75 .37 .75 .18Nobel 6 .77 .28 .77 .19Obituary 7 .85 .46 .81 .28RoadAccident 20 .61 .49 .56 .40SoccerFinal 5 .88 .39 .88 .15Storm 14 .61 .31 .61 .22Tennis 6 .87 .19 .87 .12TerroristAttack 14 .64 .48 .62 .25Volcano 16 .68 .38 .66 .17Average 12.2 .72 .39 .69 .23Table 1: CSC outperforms K-means with respect tothe purity (P) and inverse purity (IP) scores.A close inspection of the results reveals someproblematic cases.
One issue worth mentioning isthat for certain actions both CSC and K-means pro-duce split clusters.
In the CSC case, we traced thisproblem back to the thesaurus, where predicates forone action seem to belong to two or more totally dis-similar semantic categories.
The corresponding tu-ples are thus assigned to different clusters as theirlow semantic similarity forces the tuples to remainseparate, despite the shared contexts trying to jointhem.
One example is ?blast (off)?
and ?lift (off)?
inthe ?Launch Event?
scenario.
The thesaurus showsthe two verbs are dissimilar and the correspondingtuples end up being in two split clusters.
This cannot be solved easily without an improved thesaurus.We are considering adding a prior to model the op-timal size for clusters, which may help to compactsuch cases.4.3 Template CoverageWe also assess how well the resulting, CSC-generated tuple clusters serve in creating good sce-nario template slots.
We start from the top largestclusters from each scenario, and decompose eachof them into six sets: the predicates, agents, pa-tients, predicate modiers, agent modiers and pa-tient modiers.
For each of the first three sets foreach cluster, we create a generalized term to repre-sent it using an extended version of a generaliza-tion algorithm (Tseng et al, 2006).
These termsare deemed output slots, and are put into the tem-plate with their agent-predicate-patient relations pre-served.
The size of the template may increase whenmore clusters are generalized, as new slots may re-sult.We manually compare the slots that are outputfrom the system with those defined in existing sce-nario templates in MUC.
The results here are onlyindicative and not conclusive, as there are only twoMUC7 templates available for comparison: AviationDisaster and Launch Event.Template semantic role general termaction crashcluster 1 agent aircraftpatient ?action killcluster 2 agent heavier-than-air-craftpatient peopleFigure 4: Automated scenario template of ?Avia-tionDisaster?.Figure 4 shows an excerpt of the automaticallygenerated template ?AviationDisaster?
(?Airliner-Crash?
in our corpus) where the semantic roles inthe top two biggest clusters have been generalized.Their modifiers are quite semantically diverse, asshown in Table 2.
Thus, generalization (probablyafter a categorization operation) remains as a chal-lenging problem.Nonetheless, the information contained in thesesemantic roles and their modifiers covers human-162semantic role modifier head samplesagent:aircraft A, U.N., The, Swiss, Canadian-built, AN, China, CRJ-200, mil-itary, Iranian, Air, refueling, US,...action:crash Siberia, mountain, rain, Tues-day, flight, Sharjah, flames, Sun-day, board, Saturday, 225, Rock-away, approach, United, moun-tain, hillsidepatient:people all, 255, 71Table 2: Sample automatically detected modifierheads of different semantic roles.AviationDisaster LaunchEvent* AIRCRAFT * VEHICLE* AIRLINE * VEHICLE TYPEDEPARTURE POINT * VEHICLE OWNERDEPARTURE DATE * PAYLOAD* AIRCRAFT TYPE PAYLOAD TYPE* CRASH DATE PAYLOAD FUNC* CRASH SITE * PAYLOAD OWNERCAUSE INFO PAYLOAD ORIGIN* VICTIMS NUM * LAUNCH DATE* LAUNCH SITEMISSION TYPEMISSION FUNCTIONMISSION STATUSFigure 5: MUC-7 template coverage: asterisksmarking all the slots that could be automaticallygenerated.defined scenario templates quite well.
The twoMUC7 templates are shown as a list of slots in Fig-ure 5, where horizontal lines delimit slots about dif-ferent semantic roles, and asterisks mark all the slotsthat could be automatically generated by our systemonce it has an improved generalizer.
We can seesubstantial amount of overlap, indicating that a STCsystem powered by CSC is able to capture scenarios?important facts.5 ConclusionWe have introduced a new context-sensitive ap-proach to the scenario template creation (STC) prob-lem.
Our method leverages deep NL processing, us-ing semantic role labeler?s structured semantic tu-ples as input.
Despite the use of deeper semantics,we believe that intrinsic semantic similarity by itselfis not sufficient for clustering.
We have shown thisthrough examples and argue that an approach thatconsiders contextual similarity is necessary.
A keyaspect of our work is the incorporation of such con-textual information.
Our approach uses a notion ofcontext that combines two aspects: positional simi-larity (when two tuples are adjacent in the text), andargument similarity (when they have similar argu-ments).
The set of relevant articles are representedas graphs where contextual evidence is encoded.By mapping our problem into a graphical formal-ism, we cast the STC clustering problem as one ofmultiple graph alignment.
Such a graph alignment issolved by an adaptation of EM, which handles con-texts and real-valued similarity by treating both asnoisy and potentially unreliable observations.While scenario template creation (STC) is a dif-ficult problem, its evaluation is arguably more dif-ficult due to the dearth of suitable resources.
Wehave compiled and released a corpus of over 700newswire articles that describe different instances of15 scenarios, as a suitable input dataset for furtherSTC research.
Using this dataset, we have evaluatedand analyzed our context-sensitive approach.
Whileour results are indicative, they show that consideringcontextual evidence improves performance.AcknowledgmentsThe authors are grateful to Kathleen R. McKeownand Elena Filatova at Columbia University for theirstimulating discussions and comments over differentstages of the preparation of this paper.ReferencesRobin Collier.
1998.
Automatic Template Creation forInformation Extraction.
Ph.D. thesis, University ofSheffield, UK.A.P.
Dempster, N.M. Laird, and D.B.
Rubin.
1977.
Max-imum likelihood from incomplete data via the EM al-gorithm.
JRSSB, 39:1?38.Elena Filatova, Vasileios Hatzivassiloglou, and KathleenMcKeown.
2006.
Automatic creation of domain tem-plates.
In Proceedings of the COLING/ACL ?06.Sanda M. Harabagiu and V. Finley Lacatusu.
2005.Topic themes for multi-document summarization.
InProceedings of SIGIR ?05.163Sanda M. Harabagiu and S. J. Maiorano.
2002.
Multi-document summarization with GISTEXTER.
In Pro-ceedings of LREC ?02.Graeme Hirst and Alexander Budanitsky.
2005.
Cor-recting real-word spelling errors by restoring lexicalcohesion.
Natural Language Engineering, 11(1).Andreas Hotho, Steffen Staab, and Gerd Stumme.
2003.WordNet improves text document clustering.
In Pro-ceedings of the SIGIR 2003 Semantic Web Workshop.LDC.
2002.
The aquaint corpus of english news text,catalog no.
LDC2002t31.Yoong Keok Lee and Hwee Tou Ng.
2002.
An empiri-cal evaluation of knowledge sources and learning algo-rithms for word sense disambiguation.
In Proceedingsof EMNLP ?02.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING/ACL ?98.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JamesMartin, and Dan Jurafsky.
2004.
Shallow semanticparsing using support vector machines.
In Proceed-ings of HLT/NAACL ?04.Judita Preiss.
2001.
Local versus global context for wsdof nouns.
In Proceedings of CLUK4.Long Qiu, Min-Yen Kan, and Tat-Seng Chua.
2006.Paraphrase recognition via dissimilarity significanceclassification.
In Proceedings of EMNLP ?06.Ellen Riloff and M. Schmelzenbach.
1998.
An empiri-cal approach to conceptual case frame acquisition.
InProceedings of WVLC ?98.Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.2003.
An improved extraction pattern representationmodel for automatic IE pattern acquisition.
In Pro-ceedings of ACL ?03.Yuen-Hsien Tseng, Chi-Jen Lin, Hsiu-Han Chen, and Yu-I Lin.
2006.
Toward generic title generation for clus-tered documents.
In Proceedings of AIRS ?06.Roman Yangarber, Ralph Grishman, Pasi Tapanainen,and Silja Huttunen.
2000.
Unsupervised discovery ofscenario-level patterns for information extraction.
InProceedings of ANLP ?00.164
