Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 229?233, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsINAOE_UPV-CORE: Extracting Word Associations fromDocument Corpora to estimate Semantic Textual SimilarityFernando S?nchez-VegaManuel Montes-y-G?mezLuis Villase?or-PinedaPaolo RossoLaboratorio de Tecnolog?as del Lenguaje,Instituto Nacional de Astrof?sica, ?ptica yElectr?nica (INAOE), Mexico.Natural Language Engineering Lab., ELiRF,Universitat Polit?cnica de Val?ncia, Spainprosso@dsic.upv.es{fer.callotl,mmontesg,villasen}@inaoep.mxAbstractThis paper presents three methods to evaluatethe Semantic Textual Similarity (STS).
Thefirst two methods do not require labeled train-ing data; instead, they automatically extractsemantic knowledge in the form of word asso-ciations from a given reference corpus.
Twokinds of word associations are considered: co-occurrence statistics and the similarity ofword contexts.
The third method was done incollaboration with groups from the Universi-ties of Paris 13, Matanzas and Alicante.
Ituses several word similarity measures as fea-tures in order to construct an accurate predic-tion model for the STS.1 IntroductionEven with the current progress of the natural lan-guage processing, evaluating the semantic textsimilarity is an extremely challenging task.
Due tothe existence of multiple semantic relations amongwords, the measuring of text similarity is a multi-factorial and highly complex task (Turney, 2006).Despite the difficulty of this task, it remains asone of the most attractive research topics for theNLP community.
This is because the evaluation oftext similarity is commonly used as an internalmodule in many different tasks, such as, informa-tion retrieval, question answering, document sum-marization, etc.
(Resnik, 1999).
Moreover, most ofthese tasks require determining the ?semantic?similarity of texts showing stylistic differences orusing polysemicwords (Hliaoutakis et al 2006).The most popular approach to evaluate the se-mantic similarity of words and texts consists inusing the semantic knowledge expressed in ontolo-gies (Resnik, 1999); commonly, WorldNet is usedfor this purpose (Fellbaum, 2005).
Unfortunately,despite the great effort that has been the creation ofWordNet, it is still far to cover all existing wordsand senses (Curran, 2003).Therefore, the semanticsimilarity methods that use this resource tend toreduce their applicability to a restricted domainand to a specific language.We recognize the necessity of having and usingmanually-constructed semantic-knowledge sourcesin order to get precise assessments of the semanticsimilarity of texts, but, in turn, we also considerthat it is possible to obtain good estimations ofthese similarities using less-expensive, and perhapsbroader, information sources.
In particular ourproposal is to automatically extract the semanticknowledge from large amounts of raw data sam-ples i.e.
document corpora without labels.In this paper we describe two different strategiesto compute the semantic similarity of words from areference corpus.
The first strategy uses word co-occurrence statistics.
It determines that two wordsare associated (in meaning) if they tend to be usedtogether, in the same documents or contexts.
Thesecond strategy measures the similarity of wordsby taking into consideration second order word co-occurrences.
It defines two words as associated ifthey are used in similar contexts (i.e., if they co-occur with similar words).
The following sectiondescribes the implementation of these two strate-gies for our participation at the STS-SEM 2013task, as well as their combination with the meas-ures designed by the groups from the Universitiesof Matanzas, Alicante and Paris 13.2292 Participation in STS-SEM2013The Semantic Textual Similarity (STS) task con-sists of estimated the value of semantic similaritybetween two texts,?1 and ?2 for now on.As we mentioned previously, our participation inthe STS task of SEM 2013 considered two differ-ent approaches that aimed to take advantage of thelanguage knowledge latent in a given referencecorpus.
By applying simple statistics we obtained asemantic similarity measure between words, andthen we used this semantic word similarity (SWS)to get a sentence level similarity estimation.
Weexplored two alternatives for measuring the seman-tic similarity of words, the first one, called????????
, uses the co-occurrence of words in alimited context1,and the second, ??????????
, com-pares the contexts of the words using the vectormodel and cosine similarity to achieve this com-parison.
It is important to point out that using thevector space model directly, without any spatialtransformation as those used by other approaches2,we could get greater control in the selection of thefeatures used for the extraction of knowledge fromthe corpus.
It is also worth mentioning that weapplied a stemming procedure to the sentences tobe compared as well as to all documents from thereference corpus.
We represented the texts ?1 and?2 by bags of tokens, which means that our ap-proaches did not take into account the word order.Following we present our baseline method, then,we introduce the two proposed methods as well asa method done in collaboration with other groups.The idea of this shared-method is to enhance theestimation of the semantic textual similarity bycombining different and diverse strategies forcomputing word similarities.2.1 STS-baseline methodGiven texts?1 and ?2, their textual similarity isgiven by:???
?
????????
= ???(???
?1 ,?2 , ???
(?2 ,?1))where1 In the experiments we considered a window (context) formedof 15 surrounding words.2Such as Latent Semantic Analysis (LSA) (Turney, 2005).???
??
,??
=1|??|1(??
?
??
)????
?This measure is based on a direct matching of to-kens.
It simply counts the number of tokens fromone text ??
that also exist in the other text ??
.
Be-cause STS is a symmetrical attribute, unlike Tex-tual Entailment (Agirre et al 2012), we designedit as a symmetric measure.
We assumed that therelationship between both texts is at least equal totheir smaller asymmetric similarity.2.2 The proposed STS methodsThese methods incorporate semantic knowledgeextracted from a reference corpus.
They aim totake advantage of the latent semantic knowledgefrom a large document collection.
Because theextracted knowledge from the reference corpus isat word level, these methods for STS use the samebasic ?word matching?
strategy for comparing thesentences like the baseline method.
Nevertheless,they allow a soft matching between words by in-corporating information about their semantic simi-larity.The following formula shows the proposedmodification to the SIM function in order to incor-porate information of the semantic word similarity(SWS).
This modification allowed us not only tomatch words with exactly the same stem but alsoto link different but semantically related words.???
??
,??
=  ???
???(??
, ??)?????????
?We propose two different strategies to computethe semantic word similarity (SWS), ????????
and?????????
.
The following subsections describe indetail these two strategies.2.2.1 STS based on word co-occurrence????????
uses a reference corpus to get a numeri-cal approximation of the semantic similarity be-tween two terms ?
?and ??
(when these terms havenot the same stem).
As shown in the followingformula, ????????
takes values between 0 and 1;0 indicates that it does not exist any text sample inthe corpus that contains both terms, whereas, 1indicates that they always occur together.230????????
??
, ??
=??
= ??
1?????#(??
, ??
)???(#(??
), #(??
))where# ??
, ??
is the number of times that ??
and??
co-occur and # ??
and # ??
are the number oftimes that terms ??
and ??
occur in the referencecorpus respectively.2.2.2 STS based on context similarity??????????
is based on the idea that two terms aresemantically closer if they tend to be used in simi-lar contexts.
This measure uses the well-knownvector space model and cosine similarity to com-pare the terms?
contexts.
In a first step, we createda context vector for each term, which captures allthe terms that appear around it in the whole refer-ence corpus.
Then, we computed the semanticsimilarity of two terms by the following formula.??????????
??
, ??
=??
= ??
1?????
??????
?
?
,?
?where the cosine similarity, SIMCOS, is calcu-lated on the vectors ?
?and ?
?
corresponding to thevector space model representation of terms ??
and??
, as indicated in the following equation:??????(?
?
,?
? )
=???
?
????
?
|?||?
?| ?
|?
?
|It is important to point out that SIMCOS is cal-culated on a ?predefined?
vocabulary of interest;the appropriate selection of this vocabulary helpsto get a better representation of terms, and, conse-quently, a more accurate estimation of their seman-tic similarities.2.3 STS based on a combination of measuresIn addition to our main methods we also developeda method that combines our SWS measures withmeasures proposed by other two research groups,namely:?
LIPN (Laboratoire d'Informatique de Paris-Nord, Universit?
Paris 13, France).?
UMCC_DLSI (Universidad de Matanzas Cami-lo Cienfuegos, Cuba, in conjuction with theDepartamento de Lenguajes y Sistemas In-form?ticos, Universidad de Alicante, Spain).The main motivation for this collaboration was toinvestigate the relevance of using diverse strategiesfor computing word similarities and the effective-ness of their combination for estimating the seman-tic similarity of texts.The proposed method used a set of measuresprovided by each one of the groups.
These meas-ures were employed as features to obtained a pre-diction model for the STS.
Table 1 summarizes theused measures.
For the generation and fitting of themodel we used three approaches: linear regression,a Gaussian process and a multilayer neural net-work.Description Team #MeanRankBestRankBased on IR measures LIPN 2 2.0 1Based on distance on WordNet LIPN 2 8.5 2STS-ContextINAOE-UPV1 4.0 4Complexity of the sentencesINAOE-UPV34 27.8 5STS-OccurINAOE-UPV1 7.0 7Based on the alignment ofparticulars POS.UMCC_DLSI12 40.9 18n-gram overlap LIPN 1 20.0 20Based on Edit distanceUMCC_DLSI4 42.6 27Syntactic dependencies overlap LIPN 1 29.0 29Levenshtein?s distance LIPN 1 42.0 42Named entity overlap LIPN 1 57.0 57Table 1.
General description of the features used by the shared me-thod.
The second column indicates the source team for each group offeatures; the third column indicates the number of used features fromeach group; the last two columns show the information gain rank ofeach group of features over the training set.3 Implementation considerationsThe extraction of knowledge for the computationof the SWS was performed over the Reuters-21578collection.
This collection was selected because itis a well-known corpus and also because it in-cludes documents covering a wide range of topics.Due to time and space restrictions we could notconsider all the vocabulary from the reference cor-pus; the vocabulary selection was conducted bytaking the best 20,000 words according to the tran-231sition point method (Pinto et al 2006).
This me-thod selects the terms associated to the main topicsof the corpus, which presumably contain moreinformation for estimating the semantic similarityof words.
We also preserved the vocabulary fromthe evaluation samples, provided they also occur inthe reference corpus.
The size of the vocabularyused in the experiments and the size of the corpusand test set vocabularies are shown in Table 2.Experiment?sVocabularySelectedVocabularyRef.
CorpusVocabularyEvaluationVocabulary26724 20000 31213 11491Table 2.
Number of different stems from each of theconsidered vocabularies4 Evaluation and ResultsThe methods proposed by our group do not requireto be trained, i.e., they do not require tagged data,only a reference corpus, therefore, it was possibleto evaluate them on the whole training set availablethis year.
Table 3 shows their results on this set.Method CorrelationSTS-Baseline 0.455STS-Occur 0.500STS-Contex 0.511Table 3.
Correlation values of the proposed methods andour baseline method with human judgments.Results in Table 3 show that the use of the co-occurrence information improves the correlationwith human judgments.
It also shows that the useof context information further improves the results.One surprising finding was the competitive per-formance of our baseline method; it is considerablybetter than the previous year?s baseline result(0.31).In order to evaluate the method done in collabo-ration with LIPN and UMCC_DLSI, we carriedout several experiments using the features providedby each group independently and in conjunctionwith the others.
The experiments were performedover the whole training set by means of two-foldcross-validation.
The individual and global resultsare shown in Table 4.As shown in Table 4, the result corresponding tothe combination of all features clearly outper-formed the results obtained by using each team?sfeatures independently.
Moreover, the best combi-nation of features, containing selected featuresfrom the three teams, obtained a correlation valuevery close to last year's winner result.Featured by  Group Perdition Model CorrelationLIPN Gaussian Process 0.587LIPN Lineal Regression 0.701LIPN Multilayer-NN 0.756UMCC_DLSI Gaussian Process 0.388UMCC_DLSI Lineal Regression 0.388UMCC_DLSI Multilayer-NN 0.382INAOE-UPV Gaussian Process 0.670INAOE-UPV Lineal Regression 0.674INAOE-UPV Multilayer-NN 0.550ALL Gaussian Process 0.770ALL Lineal Regression 0.777ALL Multilayer-NN 0.633SELECTED-SET Multilayer-NN 0.808LAST YEAR?SWINNERSimplelog-linear regression0.823Table 4.
Results obtained by the different subsets offeatures, from the different participating groups.4.1 Officials RunsFor the official runs (refer to Table 5) we submit-ted the results corresponding to the ????????
and??????????
methods.
We also submitted a resultfrom the method done in collaboration with LIPNand UMCC_DLSI.
Due to time restrictions wewere not able to submit the results from our bestconfiguration; we submitted the results for thelinear regression model using all the features(second best result from Table 4).Table 5 showsthe results in the four evaluation sub-collections;Headlines comes from news headlines, OnWNand FNWN contain pair senses definitions fromWordNet and other resources, finally, SMT aretranslations from automatic machine translationsand from the reference human translations.As shown in Table 5, the performances of thetwo proposed methods by our group were veryclose.
We hypothesize that this result could becaused by the use of a larger vocabulary for thecomputation of co-occurrence statistics than for thecalculation of the context similarities.
We had touse a smaller vocabulary for the later because itshigher computational cost.Finally, Table 5 also shows that the methoddone in collaboration with the other groups ob-232tained our best results, confirming that using moreinformation about the semantic similarity of wordsallows improving the estimation of the semanticsimilarity of texts.
The advantage of this approachover the two proposed methods was especiallyclear on the OnWN and FNWN datasets, whichwere created upon WordNet information.
Some-how this result was predictable since several meas-ures from this ?share-method?
use WordNetinformation to compute the semantic similarity ofwords.
However, this pattern was not the same forthe other two (WordNet unrelated) datasets.
Inthese other two collections, the average perfor-mance of our two proposed methods, without usingany expensive and manually constructed resource,improved by 4% the results from the share-method.Method Headlines OnWN FNWN SMT MEANSTS-Occur 0.639 0.324 0.271 0.349 0.433STS-Contex 0.639 0.326 0.266 0.345 0.431Collaboration 0.646 0.629 0.409 0.304 0.508Table 4.
Correlation values from our official runs over thefour sub-datasets.5 ConclusionsThe main conclusion of this experiment is that it ispossible to extract useful knowledge from rawcorpora for evaluating the semantic similarity oftexts.
Other important conclusion is that the com-bination of methods (or word semantic similaritymeasures) helps improving the accuracy of STS.As future work we plan to carry out a detailedanalysis of the used measures, with the aim of de-termining their complementariness and a betterway for combining them.
We also plan to evaluatethe impact of the size and vocabulary richness ofthe reference corpus on the accuracy of the pro-posed STS methods.AcknowledgmentsThis work was done under partial support ofCONACyT project Grants: 134186, and Scholar-ship 224483.
This work is the result of the collabo-ration in the framework of the WIQEI IRSESproject (Grant No.
269180) within the FP 7 MarieCurie.
The work of the last author was in theframework the DIANA-APPLICATIONS-FindingHidden Knowledge in Texts: Applications(TIN2012-38603-C02-01) project, and theVLC/CAMPUS Microcluster on Multimodal Inte-raction in Intelligent Systems.
We also thank theteams from the Universities of Paris 13, Matanzasand Alicante for their willingness to collaboratewith us in this evalaution exercise.ReferencesAngelosHliaoutakis, GiannisVarelas, EpimeneidisVout-sakis, Euripides G. M. Petrakis, EvangelosMilios,2006, Information Retrieval by Semantic Similarity,Intern.
Journal on Semantic Web and InformationSystems: Special Issue of Multimedia Semantics(IJSWIS), 3(3): 55?73.Carmen Banea, Samer Hassan, Michael Mohler andRadaMihalcea, 2012, UNT: A Supervised SynergisticApproach to Semantic Text Similarity, SEM 2012:The First Joint Conference on Lexical and Computa-tional Semantics, Proceedings of the Sixth Interna-tional Workshop on Semantic Evaluation (SemEval2012), Montreal, Vol.
2: 635-642.Christiane Fellbaum,2005, WordNet and wordnets,Encyclopedia of Language and Linguistics, SecondEd., Oxford, Elsevier: 665-670.David Pinto, Hector Jim?nez H. and Paolo Rosso.
Clus-tering abstracts of scientific texts using the Transi-tion Point technique, Proc.
7th Int.
Conf.
on Comput.Linguistics and Intelligent Text Processing, CICL-ing-2006, Springer-Verlag, LNCS(3878): 536-546.EnekoAgirre, Daniel Cer, Mona Diab and Aitor Gonza-lez-Agirre, SemEval-2012 Task 6: A Pilot on Seman-tic Textual Similarity.
SEM 2012: The First JointConference on Lexical and Computational Seman-tics, Proceedings of the Sixth International Workshopon Semantic Evaluation (SemEval2012), Montreal,Vol.
2: 386-393.James Richard Curran, 2003, Doctoral Thesis: FromDistributional to Semantic Similarity, Institute forCommunicating and Collaborative Systems, Schoolof Informatics, University of Edinburgh.Peter D. Turney, 2005, Measuring semantic similarityby latent relational analysis, IJCAI'05 Proceedings ofthe 19th international joint conference on Artificialintelligence, Edinburgh, Scotland: 1136-1141Peter D. Turney, 2006, Similarity of Semantic Relations,Computational Linguistics, Vol.
32, No.
3: 379-416.Philip Resnik, 1999, Semantic Similarity in a Taxono-my: An Information-Based Measure and its Applica-tion to Problems of Ambiguity in Natural Language,Journal of Artificial Intelligence Research, Vol.
11:95-130.233
