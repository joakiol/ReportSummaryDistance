Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 717?720,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsExtracting Phrase Patterns with Minimum Redundancyfor Unsupervised Speaker Role ClassificationBin Zhang, Brian Hutchinson, Wei Wu and Mari Ostendorf?University of Washington, Seattle, WA 98125AbstractThis paper addresses the problem of learningphrase patterns for unsupervised speaker roleclassification.
Phrase patterns are automati-cally extracted from large corpora, and redun-dant patterns are removed via a graph prun-ing algorithm.
In experiments on English andMandarin talk shows, the use of phrase pat-terns results in an increase of role classifi-cation accuracy over n-gram lexical features,and more compact phrase pattern lists are ob-tained due to the redundancy removal.1 IntroductionThe identification of speaker roles is fundamental tothe analysis of social content and information re-liability in conversational speech.
Previous workhas primarily used supervised learning in automaticrole classification.
Barzilay et al (2000) described aspeaker role classification system for English broad-cast news (BN), where the speakers were catego-rized into three types: anchor, journalist, and guest.The authors used supervised learning to discover n-gram signature phrases for speaker introduction andstructural features such as duration, achieving an ac-curacy of 80% on ASR derived transcripts.
Liu et al(2006) studied speaker role classification on TDT-4Mandarin BN data.
Hidden Markov and maximumentropy models were used to label the sequence ofspeaker turns with the roles anchor, reporter, andother, based on n-gram features, which yielded 80%classification accuracy on human transcripts.Hutchinson et al (2010) extend previous work tothe case of unsupervised learning, with the goal ofportability across languages.
That work explored?
This research was funded by the Office of the Director ofNational Intelligence (ODNI), Intelligence Advanced ResearchProjects Activity (IARPA).
All statements of fact, opinion orconclusions contained herein are those of the authors and shouldnot be construed as representing the official views or policies ofIARPA, the ODNI or the U.S. Government.speaker role classification using structural and n-gram features on talk show (or broadcast conversa-tion (BC)) data.
In this paper, we address a limita-tion of n-grams as features by proposing a methodfor learning phrases with gaps, which is particularlyimportant for conversational speech, since there aremore disfluencies that can cause failure of n-grammatching.
In addition, we want to avoid topic words(e.g., proper nouns) in order to model speaker rolesrather than topics.
For example, for identifying thehost, the phrase pattern ?We?ll be back with * in aminute?
is more general than the n-grams ?We?ll beback with John Smith in a minute.?
To prevent theseproblems with n-grams, one must limit the length oflearned n-grams, making them less discriminative.Phrase patterns have been used in other NLP ap-plications such as (Sun et al, 2007).
To remove theredundancies in the automatically extracted phrasepatterns, we propose a redundancy removal algo-rithm based on graph pruning that does not requirerole-labeled data.
The resulting set of patterns isthen used to extract lists of signature and conversa-tional phrases, from which features are derived thatare used to distinguish between the different roles.Using the phrase pattern-based lexical features inclustering, we obtain 82-89% speaker role classifi-cation accuracy on human transcripts of BC shows.2 MethodPhrase patterns are generalizations of n-grams.
Aphrase pattern p of length n is an ordered list ofwords (w1, w2, .
.
.
, wn).
It is matched by a word se-quence of lengthm ?
n if the sequence contains thewords in the order defined by the pattern.
Becausethe words in a phrase pattern need not appear con-tiguously in the sequence, phrase matching is lesssensitive to disfluencies and topic words.2.1 Phrase Pattern ExtractionPhrase patterns can be extracted by the sequen-tial pattern mining algorithm PrefixSpan (Pei et al,7172001).
This algorithm efficiently extracts frequentphrase patterns in a corpus (i.e., relative frequencygreater than a given threshold).
Prior to the extrac-tion, we perform text preprocessing including split-ting the text into lines based on commas and peri-ods to limit the pattern span, followed by case andpunctuation removal.
The extracted phrase patternshave variable length.
As a result, longer patternsmay contain shorter patterns.
Phrase patterns withthe same length may also be overlapped.
These re-dundancies should be removed; otherwise, the samephrase may match several patterns, resulting in bi-ased counts.2.2 Phrase Pattern Redundancy RemovalDefine a phrase pattern p as contained in anotherphrase pattern q if q contains all the words in p in thesame order.
p is called a parent pattern and q is thecorresponding child pattern.
Instead of constructinga tree as in (Siu et al, 2000) for variable length n-grams, we create a graph of phrase patterns basedon containment, because a pattern can contain andbe contained by multiple patterns.
Our redundancyremoval algorithm involves pruning this graph.
Withthe nodes being the phrase patterns, the edges of thephrase pattern graph are constructed by connectinglength-n phrase pattern p to length-(n + 1) phrasepattern q for all n, if p is contained in q.
We con-nect only phrase patterns that differ by one word inlength for computational efficiency, and this resultsin a multi-layer structure: the phrase patterns in eachlayer have the same length.
For the convenience ofpruning, a virtual node T is created as the ?zeroth?-layer, and it is directly connected to all the nodes inthe layer with the shortest pattern length.Once a phrase pattern graph has been created, weprune the graph in order to remove the redundantnodes.
First, we remove edges based on the ratio ofcounts c(q)/c(p) between child node q and parentnode p. A large ratio implies that the child appears inmost of the cases where the parent appears.
Hence,we keep the edge to indicate that the child can beused as a preferred substitute for the parent.
On theother hand, the edge is removed if the ratio is small(less than a threshold t, see Fig.
1).After this procedure is performed on all the edgesin the graph, we determine whether a node is prunedbased on its connectivity to parents and children.
WeA BA B C A B D A B EA B C D F A B D A B E DXFigure 1: A fragment of an example phrase pattern graph.The letters represent words.
The edge between ?AB?
and?ABD?
is removed because the ratio of counts is less thanthe threshold.define two levels of pruning, which differ in whethera node can be preserved even if its connections toparents are removed:Conservative pruning A node is pruned if it has atleast one child.Aggressive pruning A node is pruned if it has atleast one child or is not on a path connected toT .Both methods were investigated, in case some usefulphrase patterns ended up being pruned with the moreaggressive approach.2.3 Features Based on Phase PatternsAlthough (Hutchinson et al, 2010) uses both lex-ical and structural features, here we use only lexi-cal features to better assess impact.
Once the graphpruning has provided a list of phrase patterns (elimi-nating phrases of length one because of low reliabil-ity), two subsets are extracted to represent signaturephrases as might be used by a host and conversa-tional phrases as might occur more frequently in liveinterviews.
The signature statistic?1 =DFSF+ ?
log(fBC).
(1)is based on the speaker frequency (SF , # speak-ers whose utterances match p), document frequency(DF , # shows that match p), and genre-dependentfrequency fBC (# occurrences of p in BC), all com-puted on the training data.
The ratio DFSF favorsphrases that occur in many documents but fewspeakers, e.g.
one per show, as for a host.
The logBC frequency term is a penalty to eliminate infre-quent patterns.
The conversation statistic?2 =fBCfBN + 11SF>?
.
(2)718uses frequency fBN (# occurrences of p in BN), tolook for phrases that are more frequent in BC datathan BN, ideally live discussion phenomena.
The in-dicator function 1SF>?
eliminates phrases used bya small number of speakers to avoid topic-relatedphrases.
Hyper-parameters ?
and ?
are tuned by in-specting the top phrase patterns after ranking.
Weuse ?
= 10?3, ?
= 500 for English and ?
=10?4, ?
= 1000 for Mandarin.
Phrase patterns areranked by the two statistics to generate lists of sig-nature and conversational patterns, respectively.During speaker-level feature extraction in role de-tection, each phrase pattern in the lists is matchedagainst a speaker?s utterances.
The lexical featureshave two dimensions: the count of matches using thesignature and conversational patterns, each normal-ized by the total number of patterns matched in theshow to account for differences between shows.3 Experiments3.1 Task and DataIn the absence of speaker-role-labeled conversa-tional speech training data, we perform unsuper-vised speaker role classification with three classes:host, expert guest, and soundbite.
We evaluateon two human-labeled evaluation sets (English andMandarin).
The English eval set contains nine BCshows (150 speakers), while the Mandarin eval setcontains 14 shows (140 speakers).
There is an addi-tional labeled Mandarin development set composedof ten shows (71 speakers).
There are on average7.6k words and 7.5k characters per show for Englishand Mandarin, respectively.
The phrase patterns arelearned from much larger corpora with speaker la-bels but without speaker role labels, including webtranscripts for 310 English shows and quick richtranscripts for 4395 Mandarin shows.
Because ofthe larger amount of Mandarin data, we use a lowerthreshold (5 ?
10?5) for phrase pattern extractionthan for English (10?4).3.2 ClassificationSpectral clustering (Shi and Malik, 2000) is usedin this work, since we found it to outperform otherclustering approaches such as k-means and Gaus-sian mixture models.
Given a two-dimensional fea-ture vector for each speaker in a show, we con-struct a speaker graph with edge weights definedby Gaussian similarity exp(??xi?xj?22?2).
The spec-tral clustering is non-deterministic, because it usesk-means as its final step (k = 3), which is ini-tialized by randomly choosing k samples as ini-tial centroids.
We vary ?
as an integer from 1 to100 in combination with different random initial-izations to generate multiple clustering alternatives,and then use a partition selection algorithm to pickthe most common clustering among the candidates.We use domain knowledge to map speaker clustersinto speaker roles: the cluster whose members havethe largest average number of speaker turns is thehost cluster, that with the smallest average numberof turns is the soundbite cluster, and the remainingcluster contains the expert guests.3.3 ResultsThe phase pattern pruning threshold t was tuned onthe Mandarin dev set.
We varied t from 0.1 to 0.9,and measured the classification accuracy.
t = 0.8was found to be optimal (Fig.
2).0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.70.750.80.850.9ConservativeAggressiveNo pruningFigure 2: Accuracy on Man dev vs. pruning threshold tThe list of classification results on all the data setsis shown in Tab.
1.
Aggressive pruning yields thebest classification performance on all the data sets.
Itis also better than using n-gram matching for featureextraction (the last row of the table).Man dev Man eval Eng evalNo pruning 0.83 0.86 0.81Cons.
pruning 0.86 0.83 0.81Aggr.
pruning 0.89 0.89 0.82N-gram 0.86 0.86 0.77Table 1: Classification resultsThe size of phrase pattern lists is given in Tab.2, and the number of redundant phrase patterns (thepatterns that are contained in other patterns) is inTab.
3 for different pruning levels.
Using aggres-sive pruning, the list size and number of redun-719dant phrase patterns are greatly reduced.
However,the classification accuracy does not decrease.
Thisdemonstrates that the redundant phrase patterns arenot helpful and can be harmful for this task.Signature ptn.
Conv.
ptn.Pruning level Eng Man Eng ManNo pruning 2000 2000 1000 1000Cons.
pruning 1605 946 928 998Aggr.
pruning 244 370 465 835Table 2: Phrase pattern list sizeSignature ptn.
Conv.
ptn.Pruning level Eng Man Eng ManNo pruning 396 1331 337 142Cons.
pruning 35 307 334 142Aggr.
pruning 6 59 8 0Table 3: Number of redundant phrase patterns in the listThe unsupervised speaker role classification sys-tem in (Hutchinson et al, 2010) uses both n-gramand structural features, giving classification accu-racy on English and Mandarin eval sets of 0.86and 0.84, respectively.
Adding structural featuresto phrase-pattern-based lexical features improvesthe performance on English but not Mandarin, per-haps because soundbites in English tend to be muchshorter than those in Mandarin.3.4 DiscussionThe experiments reflect differences between the twolanguages.
We observe that the main gain in Man-darin comes from improved classification of hosts,due to the signature phrase patterns.
In English, theimprovement is attributed to improved classificationof expert guests and soundbites, suggesting an im-proved conversational dimension of the lexical fea-tures.
The performance difference of the two lan-guages seems more related to the languages them-selves, rather than the size of data sets on whichphrase patterns are learned, because we were able toobtain similar performance on Mandarin even whenthe training set size is reduced.Anecdotal inspection of the phrase patternslearned for the signature phrases suggests that thecombination of redundancy pruning and the heuris-tic signature statistic is quite effective.
For exam-ple, we observed English signature patterns suchas ?back with after this?
and ?let?s take a lookat.?
The former pattern can be matched by phraseswith names or topics inserted, and the latter can bematched by ?let?s just take a look at?
or ?let?s takea brief look at.?
In the Mandarin signature patterns,we also found patterns such as ????????????
* ?
* ???
(today the guest invited tothe studio is Professor from) and ?????
*????
(thanks to the report from).
These patterns canbe considered to be templates for hosts, where thenamed-entities are skipped.4 ConclusionsWe have presented a method for extracting phrasepatterns with minimum redundancy for speaker roleclassification.
The proposed algorithm removesmost of the redundancies in the phrase patterns,leading to more compact pattern lists and improvedclassification accuracy over n-gram lexical features.We can apply the algorithm to other applicationssuch as text classification, where phrase patternscan be used in place of n-grams.
One way to ex-tend this work is to use the automatically extractedphrase patterns as initial features, and then employsupervised or semi-supervised learning techniquesto learn a more discriminative feature set.ReferencesR.
Barzilay et al 2000.
The Rules Behind Roles: Iden-tifying Speaker Role in Radio Broadcasts Proc.
AAAI,pp.
679?684.Y.
Liu.
2006.
Initial Study on Automatic Identification ofSpeaker Role in Broadcast News Speech.
Proc.
HLT,pp.
81?84.B.
Hutchinson et al 2010.
Unsupervised Broadcast Con-versation Speaker Role Labeling Proc.
ICASSP, pp.5322?5325.G.
Sun et al 2007.
Detecting Erroneous Sentences UsingAutomatically Mined Sequential Patterns.
Proc.
ACL,pp.
81?88.J.
Pei et al 2001.
PrefixSpan: Mining Sequential Pat-terns Efficiently by Prefix-projected Pattern Growth.Proc.
ICDE, pp.
215?224.M.
Siu and M. Ostendorf.
2000.
Variable N-grams andExtensions for Conversational Speech Language Mod-eling.
IEEE Transactions on Speech and Audio Pro-cessing, 8(1):63?75.J.
Shi and J. Malik.
2000.
Normalized Cuts and ImageSegmentation.
IEEE Transactions on Pattern Analysisand Machine Intelligence, 22(8):888?905.720
