Answering Questions in the Genomics DomainFabio Rinaldi, James Dowdall, Gerold SchneiderInstitute of Computational Linguistics,University of Zurich, CH-8057 ZurichSwitzerland{rinaldi, dowdall, gschneid}@cl.unizh.chAndreas PersidisBiovista, 34 Rodopoleos Str.,Ellinikon, GR-16777 Athens,Greeceandreasp@biovista.comAbstractIn this paper we describe current efforts aimed atadapting an existing Question Answering system toa new document set, namely research papers in thegenomics domain.
The system has been originallydeveloped for another restricted domain, however ithas already proved its portability.
Nevertheless, theprocess is not painless, and the specific purpose ofthis paper is to describe the problems encountered.1 IntroductionOne of the core problems in exploiting scientificpapers in research and clinical settings is that theknowledge that they contain is not easily acces-sible.
Although various resources which attemptto consolidate such knowledge are being created(e.g.
UMLS1, SWISS-PROT, OMIM, GeneOntol-ogy, GenBank, LocusLink), the amount of informa-tion available keeps growing exponentially (Stapleyand Benoit, 2000).There is accordingly a pressing need for intelli-gent systems capable of accessing that informationin an efficient and user-friendly way.
Question An-swering systems aim at providing a focused wayto access the information contained in a documentcollection.
Specific research in the area of Ques-tion Answering has been prompted in the last fewyears in particular by the Question Answering trackof the Text REtrieval Conference (TREC-QA) com-petitions (Voorhees, 2001).
The TREC-QA compe-titions focus on open-domain systems, i.e.
systemsthat can (potentially) answer any generic question.As these competitions are based on large volumesof text, the competing systems (normally) resort to arelatively shallow text analysis.2 In contrast a ques-tion answering system working on a restricted do-main can take advantage of the formatting and style1http://www.nlm.nih.gov/research/umls/2With some notable exception, e.g.
(Harabagiu et al, 2001).conventions in the text, can make use of the specificdomain-dependent terminology, and of full parsing.In many restricted domains, including technicaldocumentation and research papers, terminologyplays a pivotal role.
This is in fact one of themajor differences between restricted domains andopen domain texts.
While in open domain systemsNamed Entities play a major role, in technical doc-umentation, as well as in research papers, they havea secondary role, by contrast a far greater role isplayed by domain terminology.
Terminology is amajor obstacle for processing research papers andat the same time a key access path to the knowledgeencoded in those papers.
Terminology provides themeans to name and access domain-specific conceptsand objects.Restricted domains present the additional prob-lem of ?domain navigation?.
Users of the systemcannot always be expected to be completely fa-miliar with the domain terminology.
Unfamiliar-ity with domain terminology might lead to ques-tions which contain imperfect formulations of do-main terms.
It becomes therefore essential to beable to detect terminological variants and exploit therelations between terms (like synonymy, meronymy,antonymy).
The process of variation is well in-vestigated in terminological research (Daille et al,1996).
In the Biomedical domain, an example of asystem that deals with terminological variants (alsocalled ?aliases?)
can be found in (Pustejovsky et al,2002).In the rest of this paper we will first briefly de-scribe our existing Question Answering system, Ex-trAns (section 2).
In the following section (3) wedetail the specific problems encountered in the newdomain and the steps that we have taken to solvethem.
We conclude the paper with an overview ofrelated research (section 4).Figure 1: Example of document to be analyzed2 The original Question Answering systemExtrAns is a Question Answering system aimed atrestricted domains, in particular terminology-richdomains.
While open domain Question Answeringsystems typically are targeted at large text collec-tions and use relatively little linguistic information,ExtrAns answers questions over such domains byexploiting linguistic knowledge from the documentsand terminological knowledge about a specific do-main.
Various applications of the ExtrAns systemhave been developed, from the original prototypeaimed at the Unix documentation files (Molla?
et al,2000) to a version targeting the Aircraft Mainte-nance Manuals (AMM) of the Airbus A320 (Molla?et al, 2003; Rinaldi et al, 2004).
In the present pa-per we describe current work in applying the systemto a different domain and text type: research papersin the genomics area.Our approach to Question Answering is particu-larly computationally intensive; this allows a deeperlinguistic analysis to be performed, at the cost ofhigher processing time.
The documents are an-alyzed in an off-line stage and transformed in asemantic representation (called ?Minimal LogicalForms?
or MLFs), which is stored in a KnowledgeBase (KB).
In an on-line phase (see fig.
2) the userqueries are analyzed using the same basic machin-ery (however the cost of processing them is neg-ligible, so that there is no visible delay) and theirsemantic representation is matched in the KB.
If amatch is encountered, the sentences that gave originto the match are presented as possible answer to thequestion.Documents (and queries) are first tokenized, thenthey go through a terminology-processing module.If a term belonging to a synset in the terminolog-ical knowledge base is detected, then the term isreplaced by a synset identifier in the logical form.This results in a canonical form, where the synsetidentifier denotes the concept that each of the termsin the synset names.
In this way any term containedin a user query is automatically mapped to all itsvariants.
This approach amounts to an implicit ?ter-minological normalization?
for the domain, wherethe synset identifier can be taken as a reference toSemanticMatchingDocumentKBdocument logicalformAnswersinDocumentDocumentLinguisticProcessingQUERY QueryFilteringThesaurusQUERY+SynsetFigure 2: Schematic representation of the core QA enginethe ?concept?
that each of the terms in the synset de-scribes (Kageura, 2002).ExtrAns depends heavily on its use of logicalforms, which are designed so that they are easy tobuild and to use, yet expressive enough for the taskat hand (Molla?, 2001).
The logical forms and asso-ciated semantic interpretation methods are designedto cope with problematic sentences, which includevery long sentences, even sentences with spellingmistakes, and structures that are not recognized bythe syntactic analyzer.
An advantage of ExtrAns?Minimal Logical Forms (MLFs) is that they can beproduced with minimal domain knowledge.
Thismakes our technology easily portable to differentdomains.
The only true impact of the domain isduring the preprocessing stage of the input text andduring the creation of a thesaurus that reflects thespecific terms used in the chosen domain, their lex-ical relations and their word senses.Unlike sentences in documents, user queriesare processed on-line and the resulting MLFs areproved by deduction over the MLFs of documentsentences stored in the KB.
When no direct answerfor a user query can be found, the system is able torelax the proof criteria in a stepwise manner.
First,hyponyms are added to the query terms.
This makesthe query more general but maintains its logical cor-rectness.
If no answers can be found or the userdetermines that they are not good answers, the sys-tem will attempt approximate matching, in whichthe sentence that has the highest overlap of predi-cates with the query is retrieved.
The matching sen-tences are scored and the best matches are returned.The MLFs contain pointers to the original textwhich allow ExtrAns to identify and highlight thosewords in the retrieved sentence that contribute mostto a particular answer.
An example of the output ofExtrAns can be seen in fig.
3.
When the user clickson one of the answers provided, the correspondingdocument will be displayed with the relevant pas-sages highlighted.
Another click displays the an-swer in the context of the document and allows theuser to verify the justification of the answer.3 Moving to the new domainThe first step in adapting the system to a new do-main is identifying the specific set of documents tobe analyzed.
We have experimented with two dif-ferent collections in the genomics domain.
The firstcollection (here called the ?Biovista?
corpus) hasbeen generated from Medline using two seed termlists of genes and pathways (biological process) toextract an initial corpus of research papers (full ar-ticles).
The second collection is constituted by theGENIA corpus (Kim et al, 2003)3, which contains2000 abstracts from Medline (a total of 18546 sen-tences).
The advantage of the latter is that domain-specific terminology is already manually annotated.However focusing only on that case would meandisregarding a number of real-world problems (inparticular terminology detection).3.1 Formatting informationAn XML based filtering tool has been used to selectzones of the documents that need to be processedin a specific fashion.
Consider for instance the caseof bibliography.
The initial structure of the docu-ment allows to identify easily each bibliographicalitem.
Isolating the authors, titles and publication in-formation is then trivial (because it follows a regularstructure).
The name of the authors (together withthe html cross-references) can then be used to iden-tify the citations within the main body of the paper.If a preliminary zone identification (as described) isnot performed, the names of the authors used in thecitations would appear as spurious elements withinsentences, making their analysis very difficult.Another common case is that of titles.
Normallythey are Nominal Phrases rather than sentences.
If3http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/Figure 3: Example of interaction with the systemthe parser was expecting to find a sentence it wouldfail.
However using the knowledge that a title isbeing processed, we can modify the configurationof the parser so that it accepts an NP as a correctparse.3.2 TerminologyThe high frequency of terminology in technical textproduces various problems when locating answers.A primary problem is the increased difficulty ofparsing text in a technical domain due to domain-specific sublanguage.
Various types of multi-wordterms characterize these domains, in particular re-ferring to specific concepts (e.g.
genome sequences,proteins).
These multi-word expressions might in-clude lexical items which are either unknown to ageneric lexicon (e.g.
?argentine methylation?
), havea specific meaning unique to this domain or dever-bal adjectives (and nouns) are often mistagged asverbs (e.g.
?mediated activation?, ?cell killing?
).Abbreviations and acronyms, often complex (e.g.bracketed inside NPs, like ?adenovirus (ad) infec-tion?)
are another common source of inconsisten-cies.
In such cases the parser might either fail toidentify the compound as a phrase and consequentlyfail to parse the sentence including such items.
Al-ternatively a parser might attempt to ?guess?
theirlexical category (in the set of open class categories),leading to an exponential growth of the number ofpossible syntactic parses and often incorrect deci-sions.
Not only the internal structure of the com-pound can be multi-way ambiguous, also the bound-aries of the compounds are difficult to detect and theparsers may try odd combinations of the tokens be-longing to the compounds with neighboring tokens.We have described in (Rinaldi et al, 2002) someapproaches that might be taken towards terminologyextraction for a specific domain.
The GENIA cor-pus removes these problems completely by provid-ing pre-annotated terminological units.
This allowsattention to be focused on other challenges of theQA task, rather than getting ?bogged down?
withterminology extraction and organization.In the case of the Biovista corpus, we had toperform a phase of terminology discovery, whichwas facilitated by the existence of the seed lists ofgenes and pathways.
We first marked up those termswhich appear in the corpus using additional xmltags.
This identified 900 genes and 218 pathwaysthat occur in the corpus - represented as boxed to-kens in fig.
4.
Next the entire corpus is chunked intonominal and verbal chunks using LT Chunk (Finchand Mikheev, 1997).
Ignoring prepositions andgerunds the chunks are a minimal phrasal group -represented as the square braces in fig.
4.
The cor-pus terms are then expanded to the boundary of thephrasal chunk they appear in.
For example, NP3 infig.
4 contains two terms of interest producing thenew term ?IFN-induced transcription?.
The 1118corpus terms were expanded into 6697 new candi-date terms.
1060 involve a pathway in head positionand 1154 a gene.
The remaining 4483 candidateterms involve a novel head with at least one gene orpathway as a modifier.Once the terminology is available, it is necessaryto detect relations among terms in order to exploitArgentine methylation of  STAT1  modulates   IFN -induced  transcriptionNP1 VBZsubjNP2 NP3prepmodppobjFigure 4: An example of syntactic analysisit.
We have focused our attention in particular tothe relations of synonymy and hyponymy, whichare detected as described in (Dowdall et al, 2003)and gathered in a Thesaurus.
The organizing unit isthe WordNet style synset which includes strict syn-onymy as well as three weaker synonymy relations.These sets are further organized into a isa hierarchybased on two definitions of hyponymy.One of the most serious problems that we haveencountered in working in restricted domains isthe syntactic ambiguity generated by multi-wordunits, in particular technical terms.
Any genericparser, unless developed specifically for the do-main at hand, will have serious problems dealingwith those multi-words.
The solution that we haveadopted is to parse multi-word terms as single syn-tactic units.
The tokenizer detects the terms (pre-viously collected in the Thesaurus) as they appear inthe input stream, and packs them into single lexicaltokens prior to syntactical analysis, assigning themthe syntactic properties of their head word.
In previ-ous work this approach has proved to be particularlyeffective, bringing a reduction in the complexity ofparsing of 46% (Rinaldi et al, 2002).3.3 ParsingThe deep syntactic analysis builds upon the chunksto identify sentence level syntactic relations be-tween the heads of the chunks.
The output is ahierarchical structure of syntactic relations - func-tional dependency structures - represented as the di-rected arrows in fig.
4.
The parser (Pro3Gres) useshand-written declarative rules to encode acknowl-edged facts, such as verbs typically take one butnever two subjects, combined with a statistical lan-guage model that calculates lexicalized attachmentprobabilities, similar to (Collins, 1999).
Parsing isseen as a decision process, the probability of a totalparse is the product of probabilities of the individualdecisions at each ambiguous point in the derivation.Probabilistic parsers generally have the advan-tage that they are fast and robust, and that theyresolve syntactic ambiguities with high accuracy.Both of these points are prerequisites for a statisticalanalysis that is feasible over large amounts of textand beneficial to the Q&A system?s performance.In comparison to shallow processing methods,parsing has the advantage that relations spanninglong stretches of text can still be recognized, andthat the parsing context largely contributes to thedisambiguation.
In comparison to deep linguistic,formal grammar-based parsers, however, the outputof probabilistic parsers is relatively shallow, purecontext-free grammar (CFG) constituency output,tree structures that do not include grammatical func-tion annotation nor co-indexation and empty nodesannotation expressing long-distance dependencies(LDD).
In a simple example sentence ?John wantsto leave?, a deep-linguistic syntactic analysis ex-presses the identity of the explicit matrix clausesubject and implicit subordinate clause subject bymeans of co-indexing the explicit and the empty im-plicit subject trace t: ?
[John1 wants [t1 to leave]]?.A parser that fails to recognize these implicit sub-jects, so-called control subjects, misses very impor-tant information, quantitatively about 3 % of all sub-jects.Although LDD annotation is actually provided inTreebanks such as the Penn Treebank (Marcus et al,1993) over which they are typically trained, mostprobabilistic parsers largely or fully ignore this in-formation.
This means that the extraction of LDDsand the mapping to shallow semantic representa-tions such as MLF is not always possible, becausefirst co-indexation information is not available, sec-ond a single parsing error across a tree fragmentcontaining an LDD makes its extraction impossible,third some syntactic relations cannot be recoveredFigure 5: Dependency Tree output of the SWI Prolog graphical implementation of the parseron configurational grounds only.We therefore adapt ExtrAns to use a new statis-tical broad-coverage parser that is as fast as a prob-abilistic parser but more deep-linguistic because itdelivers grammatical relation structures which arecloser to predicate-argument structures and shallowsemantic structures like MLF, and more informativeif non-local dependencies are involved (Schneider,2003).
It has been evaluated and shown to havestate-of-the-art performance.The parser expresses distinctions that are es-pecially important for a predicate-argument basedshallow semantic representation, as far as theyare expressed in the Penn Treebank training data,such as PP-attachment, most LDDs, relative clauseanaphora, participles, gerunds, and the argu-ment/adjunct distinction for NPs.In some cases functional relations distinctionsthat are not expressed in the Penn Treebank aremade.
Commas are e.g.
disambiguated betweenapposition and conjunction, or the Penn tag IN isdisambiguated between preposition and subordinat-ing conjunction.
Other distinctions that are less rel-evant or not clearly expressed in the Treebank areleft underspecified, such as the distinction betweenPP arguments and adjuncts, or a number of types ofsubordinate clauses.
The parser is robust in that itreturns the most promising set of partial structureswhen it fails to find a complete parse for a sentence.For sentences syntactically more complex than thisillustrative example, as many hierarchical relationsare returned as possible.
A screenshot of its graphi-cal interface can be seen in fig.
5.
Its parsing speedis about 300,000 words per hour.Fig.
4 displays the three levels of analysis that areperformed on a simple sentence.
Term expansionyields NP3 as a complete candidate term.
However,NP1 and NP2 form two distinct, fully expandednoun phrase chunks.
Their formation into a nounphrase with an embedded prepositional phrase is re-covered from the parser?s syntactic relations giv-ing the maximally projected noun phrase involv-ing a term: ?Argentine methylation of STAT1?
(orjuxtaposed ?STAT1 Argentine methylation?).
Fi-nally, the highest level syntactic relations (subjand obj) identifies a transitive predicate relationbetween these two candidate terms.3.4 MLFsThe deep-linguistic dependency based parser partlysimplifies the construction of MLF.
First, the map-ping between labeled dependencies and a surfacesemantic representation is often more direct thanacross a complex constituency subtree (Schneider,2003), and often more accurate (Johnson, 2002).Dedicated labels can directly express complex re-lations, the lexical participants needed for the con-struction are more locally available.Let us look at the example sentence ?Aden-ovirus infection and transfection were used to modelchanges in susceptibility to cell killing caused byE1A expression?.
The control relation (infectionis the implicit subject of model) and the PP rela-tion (including the description noun) are availablelocally.
The reduced relative clause killing causedby is expressed by a local dedicated label (modpart).Only the conjunction infection and transfection, ex-pressed here by bracketing, needs to be searchedacross the syntactic hierarchy.This leads to the following MLFs:object(infection, o1, [o1]).object(transfection, o2, [o2]).object(change, o3, [o3]).object(susceptibility, o4, [o4]).object(killing, o5, [o5]).object(expression, o6, [o6]).object(cell, o7, [o7]).evt(cause, e3, [o6]).evt(model, e1, [(o1,o2), o3]).evt(use, e2, [(o1,o2), e1]).by(e3, o6).in(o5, o7).to(o4, o5).in(o3, o4).4 Related WorkQuestion Answering in Biomedicine is surveyed indetail in (Zweigenbaum, 2003), in particular regard-ing clinical questions.
An example of a system ap-plied to such questions is presented in (Niu et al,2003), where it is applied in a setting for Evidence-Based Medicine.
This system identifies specific?roles?
within the document sentences and the ques-tions, determining the answers is then a matter ofcomparing the roles in each.
To this aim, naturallanguage questions are translated into the PICO for-mat (Sackett et al, 2000).Automatic knowledge extraction (or strategies forimproving these methods) over Medline articles arenumerous.
For example, (Craven and Kumlien,1999) identifies possible drug-interaction relations(predicates) between proteins and chemicals usinga ?bag of words?
approach applied to the sentencelevel.
This produces inferences of the type: drug-interactions (protein, pharmacologic-agent) wherean agent has been reported to interact with a pro-tein.
(Sekimizu et al, 1998) uses frequently occurringpredicates and identifies the subject and object ar-guments in the predication, in contrast (Rindfleschet al, 2000) uses named entity recognition tech-niques to identify drugs and genes, then identifiesthe predicates which connect them.
This type of?object-relation-object?
inference may also be im-plied (Cimino and Barnet, 1993).
This methoduses ?if then?
rules to extract semantic relationshipsbetween the medical entities depending on whichMeSH headings these entities appear under.
Forexample, if a citation has ?Electrocardiography?with the subheading ?Methods?
and has ?Myocar-dial Infarction?
with the subheading ?Diagnosis?then ?Electrocardiography?
diagnoses ?MyocardialInfarction?.(Spasic?
et al, 2003) uses domain-relevant verbsto improve on terminology extraction.
The co-occurrence in sentences of selected verbs and can-didate terms reinforces their termhood.
But wheresuch linguistic inferences are stored in a KB as facts,statistical inferences are only used to visualize pos-sible relations between objects for further investiga-tion.
(Stapley and Benoit, 2000) measures statisticalgene name co-occurrence and graphically displaysthe results for an expert to investigate the dominantpatterns.
The PubMed4 system uses the UMLS torelate metathesaurus concepts against a controlledvocabulary used to index the abstracts.
This allowsefficient retrieval of abstracts from medical journals,but it makes use of hyponymy and lexical synonymyto organize the terms.
It collects terminologies fromdiffering sub-domains in a metathesaurus of con-cepts.All such inferences (especially statistical) need tobe verified by an expert to ensure their validity.
Syn-tactic parsing, if any, is reserved to shallow NP iden-tifying strategies (Sekimizu et al, 1998), or possi-bly supplemented with PP information (Rindfleschet al, 2000).
Semantic interpretation of the docu-ments is only attempted through their MeSH head-ings (Mendonca and Cimino, 1999).5 ConclusionThis paper documents our approach towards QA inthe genomics domain.
Although some aspects ofthe work described in this paper are still experimen-tal, we think that the description of the problemsthat we have encountered and the specific solutionsadopted or planned will provide an interesting con-tribution to the workshop.
We conclude by observ-ing that Question Answering is currently seen as an?advanced?
topic in the Genomics Track of TREC5,due to be targeted for the first time in Year 2 (2005).AcknowledgmentsThe authors wish to thank the organizers of the workshopand the anonymous reviewers for their helpful commentsand suggestions.ReferencesJ.J.
Cimino and G.O.
Barnet.
1993.
Automatic Knowl-edge Acquisition from Medline.
Methods of Informa-tion in Medicine, 32(2):120?130.Michael Collins.
1999.
Head-Statistical Models for Nat-ural Language Processing.
Ph.D. thesis, University ofPennsylvania, Philadelphia, USA.M.
Craven and J. Kumlien.
1999.
Constructing biologi-cal knowledge bases by extracting information from4http://www.ncbi.nlm.nih.gov/pubmed/5http://medir.ohsu.edu/?genomics/roadmap.htmltext sources.
Proceedings of the 8th InternationalConference on Intelligent Systems for Molecular Bi-ology (ISMB-99).B.
Daille, B. Habert, C. Jacquemin, and J. Roy-aute?.
1996.
Empirical observation of term varia-tions and principles for their description.
Termino-logy, 3(2):197?258.James Dowdall, Fabio Rinaldi, Fidelia Ibekwe-Sanjuan,and Eric Sanjuan.
2003.
Complex Structuring ofTerm Variants for Question Answering.
In Proc.
of theACL 03, Workshop on Multiword Expression: Analy-sis, Acquisition and Treatment, Sapporo, Japan, July.Steve Finch and Andrei Mikheev.
1997.
A Workbenchfor Finding Structure in Texts.
In Proceedings of Ap-plied Natural Language Processing, Washington, DC,April.Sanda Harabagiu, Dan Moldovan, Marius Pas?ca, RadaMihalcea, Mihai Surdeanu, Razvan Bunescu, Rox-ana G?
?rju, Vasile Rus, and Paul Morarescu.
2001.FALCON: Boosting knowledge for answer engines.In Ellen M. Voorhees and Donna Harman, editors,Proceedings of the Ninth Text REtrieval Conference(TREC-9).Mark Johnson.
2002.
A simple pattern-matching al-gorithm for recovering empty nodes and their an-tecedents.
In Proceedings of the 40th Meeting of theACL, University of Pennsylvania, Philadelphia.Kyo Kageura.
2002.
The Dynamics of Terminology, Adescriptive theory of term formation and terminologi-cal growth.
Terminology and Lexicography, Researchand Practice.
John Benjamins Publishing.J.D.
Kim, T. Ohta, Y. Tateisi, and J. Tsujii.
2003.
GE-NIA corpus - a semantically annotated corpus for bio-textmining.
Bioinformatics, 19(1):180?182.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of english: Thepenn treebank.
Computational Linguistics, 19:313?330.E.
A. Mendonca and J. J. Cimino.
1999.
AutomatedKnowledge Extraction from Medline Citations.
Med-ical Informatics.Diego Molla?, Rolf Schwitter, Michael Hess, and RachelFournier.
2000.
ExtrAns, an answer extraction sys-tem.
T.A.L.
special issue on Information Retrieval ori-ented Natural Language Processing, pages 495?522.Diego Molla?, Fabio Rinaldi, Rolf Schwitter, James Dow-dall, and Michael Hess.
2003.
Answer Extractionfrom Technical Texts.
IEEE Intelligent Systems.Diego Molla?.
2001.
Ontologically promiscuous flat log-ical forms for NLP.
In Harry Bunt, Ielka van der Sluis,and Elias Thijsse, editors, Proceedings of IWCS-4,pages 249?265.
Tilburg University.Yun Niu, Graeme Hirst, Gregory McArthur, and PatriciaRodriguez-Gianolli.
2003.
Answering clinical ques-tions with role identification.
In Sophia Ananiadouand Jun?ichi Tsujii, editors, Proceedings of the ACL2003 Workshop on Natural Language Processing inBiomedicine, pages 73?80.J.
Pustejovsky, J. Castan?o, R. Saur?i, A. Rumshisky,J.
Zhang, and W. Luo.
2002.
Medstract: CreatingLarge-scale Information Servers for Biomedical Li-braries.
In ACL 2002 Workshop on Natural LanguageProcessing in the Biomedical Domain.
Philadel-phia, PA.
Available at http://www.medstract.org/publications.html.Fabio Rinaldi, James Dowdall, Michael Hess, KaarelKaljurand, Mare Koit, Kadri Vider, and NeemeKahusk.
2002.
Terminology as Knowledge in An-swer Extraction.
In Proceedings of the 6th Inter-national Conference on Terminology and KnowledgeEngineering (TKE02), pages 107?113, Nancy, 28?30August.Fabio Rinaldi, Michael Hess, James Dowdall, DiegoMolla?, and Rolf Schwitter.
2004.
Question answeringin terminology-rich technical domains.
In Mark May-bury, editor, New Directions in Question Answering.AAAI Press.T.C.
Rindflesch, L. Tanabe, J. N. Weinstein, andL.
Hunter.
2000.
Edgar: Extraction of drugs, genesand relations from the biomedical literature.
In PacificSymposium on Biocomputing, pages 514?25.D.
L. Sackett, S. E. Straus, W. S. Richardson,W.
Rosenberg, and R. B. Haynes.
2000.
EvidenceBased Medicine: How to Practice and Teach EBM.Churchill Livingstone.Gerold Schneider.
2003.
Extracting and Using Trace-Free Functional Dependencies from the Penn Tree-bank to Reduce Parsing Complexity.
In Proceedingsof The Second Workshop on Treebanks and Linguis-tic Theories (TLT 2003), Va?xjo?, Sweden, November14-15.T.
Sekimizu, H. Park, and J Tsujii.
1998.
Identifying theinteraction between genes and gene products based onfrequently seen verbs in Medline abstracts.
GenomeInformatics, Universal Academy Press.Irena Spasic?, Goran Nenadic?, and Sophia Ananiadou.2003.
Using domain-specific verbs for term classifi-cation.
In Sophia Ananiadou and Jun?ichi Tsujii, edi-tors, Proceedings of the ACL 2003 Workshop on Nat-ural Language Processing in Biomedicine, pages 17?24.B.J.
Stapley and G. Benoit.
2000.
Bibliometrics: infor-mation retrieval and visualization from co-occurrenceof gene names in medline abstracts.
In Proceedingsof the Pacific Symposium on Biocomputing (Oahu,Hawaii), pages 529?540.Ellen M. Voorhees.
2001.
The TREC question answer-ing track.
Natural Language Engineering, 7(4):361?378.Pierre Zweigenbaum.
2003.
Question answering inbiomedicine.
In Proc.
of EACL 03 Workshop: Natu-ral Language Processing for Question Answering, Bu-dapest.
