Evaluating Automatic Dialogue Strategy Adaptation for aSpoken Dialogue SystemJennifer Chu-CarrollLucent  Technologies Bell Laborator ies600 Mounta in  AvenueMurray Hill, NJ 07974, U.S.A.jencc @research.bel l - labs.cornJill Suzanne NickersonHarvard UniversityCambr idge,  MA 02138, U.S.A.nickerso @eecs.harvard.eduAbstractIn this paper, we describe an empirical evaluation of anadaptive mixed initiative spoken dialogue system.
Weconducted two sets of experiments toevaluate the mixedinitiative and automatic adaptation aspects of the system,and analyzed the resulting dialogues along three dimen-sions: performance factors, discourse features, and ini-tiative distribution.
Our results show that 1) both themixed initiative and automatic adaptation aspects ledto better system performance in terms of user satisfac-tion and dialogue fficiency, and 2) the system's adap-tation behavior better matched user expectations, moreefficiently resolved ialogue anomalies, and resulted inhigher overall dialogue quality.1 IntroductionRecent advances in speech technologies have enabledspoken dialogue systems to employ mixed initiative di-alogue strategies (e.g.
(Allen et al, 1996; Sadek et al,1996; Meng et al, 1996)).
Although these systems inter-act with users in a manner more similar to human-humaninteractions than earlier systems employing system ini-tiative strategies, their response strategies are typicallyselected using only local dialogue context, disregardingdialogue history.
Therefore, their gain in naturalness andperformance under optimal conditions i often overshad-owed by their inability to cope with anomalies in dia-logues by automatically adapting dialogue strategies.
Incontrast, Figure 1 shows a dialogue in which the sys-tem automatically adapts dialogue strategies based onthe current user utterance and dialogue history.
1 Af-ter failing to obtain a valid response to an information-seeking query in utterance (4), the system adapted ia-logue strategies to provide additional information i  (6)that assisted the user in responding to the query.
Further-more, after the user esponded toa limited system promptin (10) with a fully-specified query in (11), implicitlyindicating her intention to take charge of the problem-IS and U indicate system and user utterances, respectively.
Thewords appearing in square brackets are the output from the LucentAutomatic Speech Recognizer (Reichl and Chou, 1998; Ortmanns etal., 1999), configured to use class-based probabilistic n-gram languagemodels.
The task and dialogue initiative annotations are explained inSection 2.1.solving process, the system again adapted strategies,hence providing an open-ended prompt in (13).Previous work has shown that dialogue systems inwhich users can explicitly change the system's dia-logue strategies result in better performance than non-adaptable systems (Litman and Pan, 1999).
However,no earlier system allowed for initiative-oriented auto-matic strategy adaptation based on information dynam-ically extracted from the user's spoken input.
In thispaper, we briefly introduce MIMIC, a mixed initiativespoken dialogue system that automatically adapts dia-logue strategies.
We then describe two experiments thatevaluated the effectiveness of MIMIC's mixed initiativeand automatic adaptation capabilities.
Our results howthat, when analyzed along the performance dimension,MIMIC's mixed initiative and automatic adaptation fea-tures lead to more efficient dialogues and higher user sat-isfaction.
Moreover, when analyzed along the discourseand initiative dimensions, MIMIC's adaptation capabil-ities result in dialogues in which system behavior bettermatches user expectations and dialogue anomalies are re-solved more efficiently.2 M IMIC :  An  Adaptive Mixed InitiativeDialogue SystemMIMIC is a voice-enabled, telephone-based dialoguesystem that provides movie showtime information.
Twomain features distinguish MIMIC from other existingspoken dialogue systems.
First, MIMIC automaticallyadapts dialogue strategies based on dialogue character-istics and thus tailors its behavior to the dialogue con-text.
Second, MIMIC's initiative module is decoupledfrom the dialogue manager's goal and strategy selectionprocesses, thereby enabling the system's adaptation be-havior to be easily modified.
We outline characteristicsof MIMIC relevant to our experiments and analyses.
Forfurther details about he system, see (Chu-Carroll, 2000).2.1 Adaptation of Dialogue StrategiesFigure 1 showed an actual interaction between MIMICand a user.
The task and dialogue initiative holdersannotated for each system turn are the factors that af-fect MIMIC's dialogue strategy adaptation.
Following(Chu-Carroll and Brown, 1998), task initiative tracks the202Utterance(1) S: Hello, this is MIMIC, the movie information system.
(2) How can I help you?
(3) U: What time is Analyze This playing in Montclair?\[what time is analyze this playing in montclair\](4) S: What theater would you like?
User(5) U: Montclair.\[montclair\](6) S: Choices for theater are Wellmont Theater and Clearviews Screening Zone.
System(7) What theater would you like?
(8) U: Clearviews Screening Zone.lure movie screening zone\](9) S: Analyze This is playing at Clearviews Screening Zone in Montclair at 1:15pm, 4:10pro, Systemand 6:30pm.
(10) Please say the name of the movie or theater or town you would like information about.
(11) U: What time is Analyze This playing at the Wellmont Theater?\[what time is analyze this playing at the wellmont theater\](12) S: Analyze this is playing at Wellmont Theater in Montclair at 3:15pm, 5:45pm, 8:20pm, Userand 10:10pm.
(13) Can I help you with anything else?Task DialogueInitiative InitiativeUser SystemSystemSystemSystemSystemFigure 1: An Adaptive Mixed Initiative Dialogue with MIMIClead in the process toward achieving the dialogue partici-pants' domain goal, while dialogue initiative models thelead in determining the current discourse focus.
In ourinformation query application domain, MIMIC has task(and thus dialogue) initiative when its utterances are in-tended to provide helpful guidance toward achieving theuser's domain goal, while it has dialogue but not taskinitiative if its utterances only specify the current dis-course goal.
2 For example, as a result of MIMIC takingover task initiative in (6), helpful guidance, in the formof valid response choices, was provided in its attemptto obtain a theater name after the user failed to answeran earlier question intended to solicit this information.In (4), MIMIC specified the current discourse goal (re-questing information about a missing theater) but did notsuggest valid response choices ince it only had dialogueinitiative.MIMIC's ability to automatically adapt dialoguestrategies i achieved by employing an initiative mod-ule that determines initiative distribution based on par-ticipant roles, cues detected uring the current user ut-terance, and dialogue history (Chu-Carroll and Brown,1998).
This initiative framework utilizes the Dempster-Shafer theory (Shafer, 1976; Gordon and Shortliffe,1984), and represents he current initiative distribution astwo basic probability assignments (bpas) that signify theoverall amount of evidence supporting each agent hav-ing task and dialogue initiatives.
The effects that a cuehas on changing the current ask and dialogue initiativedistribution are also represented asbpas, obtained usingan iterative training procedure on a corpus of transcribed21n the dialogues collected inour experiments, which are describedin Section 3, there are system turns in which MIMIC had neither tasknor dialogue initiative.
However, such cases are rare in this domain andwill not be discussed in this paper.and annotated human-human dialogues.
At the end ofeach user turn, the bpas representing the effects of cuesdetected uring that turn are combined with the bpas rep-resenting the current initiative distribution to obtain theinitiative distribution for the system's next turn.In Figure 1, utterance (3) triggered the cue Ambigu-ousAction since the town of Montclair has multiple the-aters.
Although AmbiguousAction contributed to the sys-tem having both initiatives in the next turn, the effect ofthis cue, when combined with the initiative distributionof (3), resulted in MIMIC having dialogue but not taskinitiative in (4).
However, in addition to triggering Am-biguousAction, utterance (5) also triggered NoNewlnfo,since it did not contribute any new information.
The ef-fect of these two cues, combined with the initiative dis-tribution in (5), was enough to lead MIMIC to take overtask initiative in (6).2.2 Modification of Adaptation BehaviorMIMIC employs an initiative module which is decou-pied from its goal selection process, while allowing theoutcome of both processes to guide its response strat-egy selection.
A major advantage of this design is thatchanges in the system's trategy selection behavior, andthus its adaptation behavior, can be achieved by modi-fying parameters in the initiative module, while leavingthe goal and strategy selection processes intact.
The ini-tiative module determines initiative distribution based onparticipant roles, cues observed in the current user ut-terance, and dialogue history.
Although all three factorsmay play a role in modifying MIMIC's adaptation be-havior, we discuss how the effects of participant rolesand dialogue history are modified to create systems usedas bases of comparison i our experiments.Participant roles affect the default initiative distribu-203Utterance(14) S: Hello, this is MIMIC, the movie information system.
(15) Please say the name of the movie or theater or town you would like information about.
(16) U: Analyze This.\[analyze this1(17) S: Choices for task are when and where.
System(18) What would you like to know about Analyze This?
(19) U: When.\[when is\](20) S: Choices for theater are Middlebrook Galeria, Loews East Hanover, Montgomery Cinemas, SystemAtlantic Triplex Cinema, and others.
(21) What theater would you like?
(22) U: Montclair.\[montclair\](23) S: Choices for theater are Wellmont Theater and Clearviews Screening Zone.
System(24) What theater would you like?
(25) U: Wellmont.\[wellmontl(26) S: Analyze This is playing at Wellmont Theater in Montclair at 3:15pm, 5:45pm, 8:20pm, Systemand 10:10pm.
(27) Please say the name of the movie or theater or town you would like information about.
(28) U: Analyze This in Montclair.\[analyze this in montclair\](29) S: Choices for task are location, what, where, and when.
System(30) What would you like to know about Montclair?
(31) U: When.
(32) \[when\](33) S: Choices for theater are Wellmont Theater and Clearviews Screening Zone System(34) What theater would you like?
(35) U: Clearviews Screening Zone.\[screening zone\](36) S: Analyze This is playing at Clearviews Screening Zone in Montclair at 1:15pm, 4:10pm, Systemand 6:30pm.
(37) Please say the name of the movie or theater or town you would like information about.Figure 2: A System Initiative Dialogue with MIMIC-SITask DialogueInitiative InitiativeSystem SystemSystemSystemSystemSystemSystemSystemSys~mtion for MIMIC's initial utterance.
For naive users, itmay be more appropriate for MIMIC to employ system-initiative strategies.
This system, MIMIC-SI, can beachieved by setting the default initiative distribution toprovide definite evidence for the system having both taskand dialogue initiatives) Figure 2 shows a dialoguein which a user interacts with MIMIC-SI to obtain thesame information as in Figure 1.
MIMIC-SI prompts theuser for one piece of information at a time and provides(sometimes unnecessary) assistance, in the form of validdialogue moves, during each turn.Taking into account dialogue history allows MIMICto determine the initiative distribution based on the cu-mulative effect of previously observed cues.
This ef-fect can be disabled so that only local dialogue con-text affects the initiative distribution for the next turn.Based on MIMIC's parameter settings, this adjustmentresults in MIMIC-MI, a system which always has dia-3Based on the Dempster-Shafer theory, if the bpas of the defaultinitiative distribution or those of a detected cue provide definite vi-dence for drawing acertain conclusion, then no subsequent cues haveany effect on changing that conclusion.logue initiative but never has task initiative.
MIMIC-MIbehaves imilarly to many existing non-adaptive mixedinitiative dialogue systems (e.g., (Bennacef et al, 1996;Papineni et' al., 1999)).
Figure 3 shows a dialogue in-volving MIMIC-MI in which the user again attempts toacquire the same information as in the previous two dia-logues.
Notice that (41)-(49) constitute a failed attemptto solicit a theater name from the user.
Instead of provid-ing helpful information as in (6) in Figure 1, MIMIC-MIrelied on the user to change her problem-solving strategyin (50) to obtain the information eeded to answer thesystem's question.3 Experimental  DesignOur main goal in evaluating MIMIC is to determinewhether users find the mixed initiative and automaticadaptation aspects of its dialogue strategies useful.
Wecompared MIMIC to two control systems: MIMIC-SIand MIMIC-MI, since they employ dialogue manage-ment strategies imilar to those in many existing sys-tems.
The comparison between MIMIC and MIMIC-SI204Utterance(38) S:(39)(40) U:(41) S:(42) U:(43) S:(44) U:(45) S:(46) U:(47) S:(48) U:(49) S:(50) U:(51) S:(52) U:(53) S:(54)(55)(56)(57)U:S:Hello, this is MIMIC, the movie information system.How can I help you?When is Analyze This playing in Montclair?\[when analyze this playing in montclair\]What theater would you like?I1What theater would you like?Um.lure\]What theater would you like?Montclair.\[montclairlWhat theater would you like?Any.\[any\]What theater would you like?Where in Montclair is Analyze This playing?\[where in montclair is analyze this playing\]Analyze This is playing at Wellmont Theater and Clearviews Screening Zone in Montclair.When is Analyze This playing at Wellmont Theater?\[when is analyze this playing at wellmont theater in\]Analyze This is playing at Wellmont Theater in Montclair at 3:15pro, 5:45pm, 8:20pro,and 10:10pro.Can I help you with anything else?When is it playing at Clearviews Screening Zone?\[when is it playing at movie screening zone\]Analyze This is playing at Clearviews Screening Zone in Montclair at 1:15pm, 4:10pm,and 6:30pm.Can I help you with anything else?Figure 3: A Non-Adaptive Mixed Initiative Dialogue with MIMIC-MIfocused on the contribution of  mixed-initiative dialoguemanagement, while the comparison between MIMIC andMIMIC-MI emphasized the contribution of  automaticstrategy adaptation.
The following three factors werecontrolled in our experiments:Town Theater(if playing)HobokenTask DialogueInitiative InitiativeUser SystemUser SystemUser SystemUser SystemUser SystemUser SystemUser SystemUser SystemUser SystemMovie Times after 5:10pm(if playing)Antz(a) Easy Task1.
System version: For each experiment, wo systemswere used: MIMIC and a control system.
In the firstexperiment MIMIC was compared with MIMIC-SI,and in the second experiment, with MIMIC-MI.2.
Order:  For each experiment, all subjects were ran-domly divided into two groups.
One group per-formed tasks using MIMIC first, and the other groupused the control system first.Town Theater Movie Two Times(if playing) (if playing)Millbum Analyze ThisBerkeley HgtsMountainsideAnalyze ThisAnalyze ThisMadison True CrimeHoboken True Crime(b) Difficult Task3.
Task difficulty: 3-4 tasks which highlighted iffer-ences between systems were used for each experi-ment.
Based on the amount of information to be ac-quired, we divided the tasks into two groups: easyand difficult; an example of each is shown in Fig-ure 4.Figure 4: Sample Tasks for Evaluation ExperimentsEight subjects 4 participated in each experiment.
Eachof the subjects interacted with both systems to perform4The subjects were Bell Labs researchers, summer students, andtheir friends.
Most of them are computer scientists, electrical engi-205all tasks.
The subjects completed one task per call sothat the dialogue history for one task did not affect thenext task.
Once they had completed all tasks in sequenceusing one system, they filled out a questionnaire to as-sess user satisfaction by rating 8-9 statements, imilarto those in (Walker et al, 1997), on a scale of  1-5, where5 indicated highest satisfaction.
Approximately two dayslater, they attempted the same tasks using the other sys-tem.
5 These experiments resulted in 112 dialogues withapproximately 2,800 dialogue turns.In addition to user satisfaction ratings, we automat-ically logged, derived, and manually annotated a num-ber of  features (shown in boldface below).
For eachtask/subject/system triplet, we computed the task suc-cess rate based on the percentage of slots correctly filledin on the task worksheet, and counted the # of callsneeded to complete ach task.
6 For each call, the user-side of the dialogue was recorded, and the elapsed timeof the call was automatically computed.
All user ut-terances were logged as recognized by our automaticspeech recognizer (ASR) and manually transcribed fromthe recordings.
We computed the ASR word  er ror  rate,ASR reject ion rate, and ASR t imeout rate,  as well as# of user turns and average sentence length for eachtask/subject/system triplet.
Additionally, we recordedthe cues that the system automatically detected fromeach user utterance.
All system utterances were alsologged, along with the init iative d istr ibut ion for eachsystem turn and the dialogue acts selected to generateeach system response.4 Results and DiscussionBased on the features described above, we com-pared MIMIC and the control systems, MIMIC-SI  andMIMIC-MI,  along three dimensions: performance fea-tures, in which comparisons were made using previouslyproposed features relevant o system performance (e.g.,(Price et al, 1992; Simpson and Fraser, 1993; Danieliand Gerbino, 1995; Walker et al, 1997)); discourse fea-tures, in which comparisons were made using character-istics of the resulting dialogues; and initiative distribu-tion, where initiative characteristics of all dialogues in-volving MIMIC from both experiments were examined.4.1 Performance FeaturesFor our performance evaluation, we first applied a three-way analysis of  variance (ANOVA) (Cohen, 1995) toeach feature using three factors: system version, order,neers, or linguists, and none had prior knowledge of MIMIC.SWe used the exact same set of tasks rather than designing tasks ofsimilar difficulty levels because we intended to compare all availablefeatures between the two system versions, including ASR word errorrate, which would have been affected by the choice of movie/theaternames in the tasks.6Although the vast majority of tasks were completed in one call,some subjects, when unable to make progress, did not change strategiesas in (41)-(49) in Figure 3; instead, they hung up and started the taskover .Performance Feature MIMIC# of user turns 10.3Elapsed time (see.)
229.5ASR timeout (%) 12.5User satisfaction (n=8) 21.9ASR rejection (%) 514Task success (%) 100# of calls I 1.028.1 ASR word error (%)Sl13.6277.56.919.88.198.81.131.1(a) MIMIC vs. MIMIC-SI (n=32)P0.00750.01620.02390.04470.19110.32510.5720.8475Performance Feature MIMICASR timeout (%) 5.7# of user turns 10.3User satisfaction (n=8) 29.5Elapsed time (see.)
200.6ASR word error (%) 23.0Task success (%) 100# of calls 1.218.4 ASR rejection (%)MI p15.6 0.00114.3 0.019924.4 0.0364246.4 0.045730.6 0.058898.4 0.16391.21 0.57.7 0.8271(b) MIMIC vs. MIMIC-MI (n=24)Table 1: Comparison of Performance Featuresand task difficulty.
7 If no interaction effects emerged, wecompared system versions using paired sample t-tests.
8Following the PARADISE evaluation scheme (Walkeret al, 1997), we divided performance f atures into fourgroups:?
Task success: task success rate, # of calls.?
Dialogue quality: ASR rejection rate, ASR timeoutrate, ASR word error rate.?
Dialogue efficiency: # of user turns, elapsed time.?
System usability: user satisfaction.For both experiments, the ANOVAs showed no inter-action effects among the controlled factors.
Tables l(a)and l(b) summarize the results of the paired sample t-tests based on performance f atures, where features thatdiffered significantly between systems are shown in ital-ics.
9 These results how that, when compared with either7User satisfaction was a per subject as opposed to a per task per-formance feature; thus, we performed a two-way ANOVA using thefactors ystem version and order.8This paper focuses on evaluating the effect of MIMIC's mixed ini-tiative and automatic adaptation capabilities.
We assess these ffectsbased on comparisons between system version when no interaction ef-fects emerged from the ANOVA tests using the factors ystem version,order, and task difficulty.
Effects based on system order and task diffi-culty alone are beyond the scope of this paper.9Typically p<0.05 is considered statistically significant (Cohen,1995).206control system, users were more satisfied with MIMIC t?and that MIMIC helped users complete tasks more effi-ciently.
Users were able to complete tasks in fewer turnsand in a more timely manner using MIMIC.When comparing MIMIC and MIMIC-MI, dialoguesinvolving MIMIC had a lower timeout rate.
WhenMIMIC detected cues signaling anomalies in the dia-logue, it adapted strategies to provide assistance, whichin addition to leading to fewer timeouts, saved users timeand effort when they did not know what to say.
In con-trast, users interacting with MIMIC-MI had to iterativelyreformulate questions until they obtained the desired in-formation from the system, leading to more timeouts(see (41)-(49) in Figure 3).
However, when comparingMIMIC and MIMIC-SI, even though users accomplishedtasks more efficiently with MIMIC, the resulting dia-logues contained more timeouts.
As opposed to MIMIC-SI, which always prompted users for one piece of infor-mation at a time, MIMIC typically provided more open-ended prompts when the user had task initiative.
Eventhough this required more effort on the user's part in for-mulating utterances and led to more timeouts, MIMICquickly adapted strategies to assist users when recog-nized cues indicated that they were having trouble.To sum up, our experiments show that both MIMIC'smixed initiative and automatic adaptation aspects re-sulted in better performance along the dialogue efficiencyand system usability dimensions.
Moreover, its adap-tation capabilities contributed to better performance interms of dialogue quality.
MIMIC, however, did not con-tribute to higher performance in the task success dimen-sion.
In our movie information domain, the tasks weresufficiently simple; thus, all but one user in each experi-ment achieved a 100% task success rate.4.2 Discourse FeaturesOur second evaluation dimension concerns characteris-tics of resulting dialogues.
We analyzed features of  userutterances in terms of utterance l ngth and cues observedand features of system utterances in terms of dialogueacts.
For each feature, we again applied a three-wayANOVA test, and if no interaction effects emerged, weperformed a paired sample t-test to compare system ver-sions.The cues detected in user utterances provide insightinto both user intentions and system capabilities.
Thecues that MIMIC automatically detects are a subset ofthose discussed in (Chu-Carroll and Brown, 1998): il?
TakeOverTask: triggered when the user providesmore information than expected; an implicit indi-cation that the user wants to take control of thel?The range of user satisfaction scores was 8-40 for experiment oneand 9-45 for experiment two.l t A subset of these cues corresponds loosely to previously proposedevaluation metrics (e.g., (Danieli and Gerbino, 1995)).
However, oursystem automatically detects hese features instead of requiring manualannotation by experts.Discourse Feature MIMICCue: TakeOverTask 1.84Cue: AmbiguousActResolved 1.69'Cue: AmbiguousAction 3Avg sentence l ngth (words) 6.82Cue: InvalidAction 1.16Cue: NoNewInfo 1.28Sl54.596.595.450.941.38(a) MIMIC vs. MIMIC-SI (n=32)P000.00080.00160.17380.766Discourse Feature MIMICCue: TakeOverTask 2.33Cue: InvalidAction 2.04Cue: NoNewlnfo 2.25Cue: AmbiguousActResolved 2.08Avg sentence length (words) 5.26Cue: AmbiguousAction 4.13MI03.754.791.135.634.38(b) MIMIC vs. MIMIC-MI (n=24)P00.00110.01610.02970.17710.8767Table 2: Comparison of User Utterance Featuresproblem-solving process.?
NoNewlnfo: triggered when the user is unable tomake progress toward task completion, either whenthe user does not know what to say or the ASR en-gine fails to recognize the user's utterance.?
lnvalidAction/InvalidActionResolved: triggeredwhen the user utterance makes an invalid as-sumption about the domain and when the invalidassumption is corrected, respectively.?
AmbiguousAction/AmbiguousActionResolved: trig-gered when the user query is ambiguous and whenthe ambiguity is resolved, respectively.Tables 2(a) and (b) summarize the results of the pairedsample t-tests based on user utterance features where fea-tures whose numbers of occurrences were significantlydifferent according to system version used are shown initalics.
12 Table 2(a) shows that users expected the systemto adapt its strategies when they attempted to take controlof the dialogue.
Even though MIMIC-SI did not behaveas expected, the users continued their attempts, resultingin significantly more occurrences of  TakeOverTask in di-alogues with MIMIC-SI than with MIMIC.
Furthermore,the average sentence length in dialogues with MIMICwas only 1.5 words per turn longer than in dialogueswith MIMIC-SI, providing further evidence that users~2Since system dialogue acts are often selected based on cues de-tected in user utterances, we only discuss results of our user utterancefeature analysis, using dialogue act analysis results as additional sup-port for our conclusions.207preferred to provide free-formed queries, regardless ofsystem version used.Table 2(b) shows that MIMIC was more effec-tive at resolving dialogue anomalies than MIMIC-MI.More specifically, there were significantly fewer oc-currences of NoNewlnfo in dialogues with MIMICthan with MIMIC-MI.
In addition, while the numberof occurrences of AmbiguousAction was not signifi-cantly different for the two systems, the number thatwere resolved (AmbiguousActionResolved) was signif-icantly higher in interactions with MIMIC than withMIMIC-MI.
Since NoNewlnfo and AmbiguousActionboth prompted MIMIC to adapt strategies and, as a re-suit, provide additional useful information, the user wasable to quickly resolve the problem at hand.
This is fur-ther supported by the higher frequency of the system dia-logue act GiveOptions in MIMIC (p=0), which provideshelpful information based on dialogue context.In sum, the results of our discourse feature analysisfurther confirm the usefulness of MIMIC's adaptationcapabilities.
Comparisons with MIMIC-SI provide ev-idence that MIMIC's ability to give up initiative bettermatched user expectations.
Moreover, comparisons withMIMIC-MI show that MIMIC's ability to opportunisti-cally take over initiative resulted in dialogues in whichanomalies were more efficiently resolved and progresstoward task completion was more consistently made.4.3 Initiative AnalysisOur final analysis concerns the task initiative distri-bution in our adaptive system in relation to the fea-tures previously discussed.
For each dialogue involvingMIMIC, we computed the percentage of turns in whichMIMIC had task initiative and the correlation coefficient(r) between the initiative percentage and each perfor-mance/discourse feature.
To determine if this correlationwas significant, we performed Fisher' s r to z transform,upon which a conventional Z test was performed (Cohen,1995).Tables 3(a) and (b) summarize the correlation betweenthe performance and discourse features and the percent-age of turns in which MIMIC has task initiative, respec-tively.
13 Again, those correlations which are statisticallysignificant are shown in italics.
Table 3(a) shows a strongpositive correlation between task initiative distributionand the number of user turns as well as the elapsed timeof the dialogues.
Although earlier results (Table l(a))show that dialogues in which the system always had taskinitiative tended to be longer, we believe that this corre-lation also suggests that MIMIC took over task initiativemore often in longer dialogues, those in which the userwas more likely to be having difficulty.
Table 3(a) fur-ther shows moderate correlation between task initiativedistribution and ASR rejection rate as well as ASR worderror rate.
It is possible that such a correlation exists13This test was not performed for user satisfaction, since user saris-faction was a per subject and not a per dialogue f ature.Performance Feature r p# of user turns 0,71 0ASR rejection 0.55 0Elapsed time 0.51 0.00002ASR word error 0.46 0.00012~# of calls 0.15 0.1352!
ASR timeout -0.003 0.4911Task success rate 0 0.5(a) Performance F aturesDiscourse Feature r pCue: AmbiguousActionResolved 0.61 0Cue: NoNewlnfo 0.59 0Cue: TakeOverTask 0.44 0.00028Cue: lnvalidAction 0.42 0.00057Average sentence l ngth -0.40 0.00099Cue: AmbiguousAction 0.38 0.00169(b) Discourse FeaturesTable 3: Correlation Between Task Initiative Distributionand Features (n=56)because ASR performance worsens when MIMIC takesover task initiative.
However, in that case, we would haveexpected the results in Section 4.1 to show that the ASRrejection and word error rates for MIMIC-SI are signif-icantly greater than those for MIMIC, which are in turnsignificantly greater than those for MIMIC-MI, since inMIMIC-SI the system always had task initiative and inMIMIC-MI the system never took over task initiative.To the contrary, Tables l(a) and l(b) showed that thedifferences in ASR rejection rate and ASR word errorrate were not significant between system versions, andTable l(b) showed that ASR word error rate for MIMIC-MI was in fact quite substantially higher than that forMIMIC.
This suggests that the causal relationship is theother way around, i.e., MIMIC's adaptation capabilitiesallowed it to opportunistically take over task initiativewhen ASR performance was poor.Table 3(b) shows that all cues are positively correlatedwith task initiative distribution.
For AmbiguousAction,lnvalidAction, and NoNewlnfo, this correlation exists be-cause observation of these cues contributed to MIMIChaving task initiative.
However, note that AmbiguousAc-tionResolved has a stronger positive correlation with taskinitiative distribution than does AmbiguousAction, againindicating that MIMIC's adaptive strategies contributedto more efficient resolution of ambiguous actions.In brief, our initiative analysis lends additional sup-port to the conclusions drawn in our performance anddiscourse feature analyses and provides new evidencefor the advantages of MIMIC's adaptation capabilities.208In addition to taking over task initiative when previouslyidentified ialogue anomalies were encountered (e.g., de-tection of ambiguous or invalid actions), our analysisshows that MIMIC took over task initiative when ASRperformance was poor, allowing the system to better con-strain user utterances, t45 ConclusionsThis paper described an empirical evaluation of MIMIC,an adaptive mixed initiative spoken dialogue system.
Weconducted two experiments hat focused on evaluatingthe mixed initiative and automatic adaptation aspects ofMIMIC and analyzed the results along three dimensions:performance f atures, discourse features, and initiativedistribution.
Our results showed that both the mixedinitiative and automatic adaptation aspects of the sys-tem led to better performance in terms of user satisfac-tion and dialogue fficiency.
In addition, we found thatMIMIC's adaptation behavior better matched user expec-tations, more efficiently resolved anomalies in dialogues,and led to higher overall dialogue quality.AcknowledgmentsWe would like to thank Bob Carpenter and ChristineNakatani for their help on experimental design, Jan vanSanten for discussion on statistical analysis, and BobCarpenter for his comments on an earlier draft of this pa-per.
Support for the second author is provided by an NSFgraduate fellowship and a Lucent Technologies GRPWgrant.ReferencesJames F. Allen, Bradford W. Miller, Eric K. Ringger,and Teresa Sikorski.
1996.
A robust system for nat-ural spoken dialogue.
In Proceedings of the 34th An-nual Meeting of the Association for ComputationalLinguistics, pages 62-70.S.
Bennacef, L. Devillers, S. Rosset, and L. Lamel.1996.
Dialog in the RAILTEL telephone-based sys-tem.
In Proceedings of the 4th International Confer-ence on Spoken Language Processing.Jennifer Chu-Carroll and Michael K. Brown.
1998.
Anevidential model for tracking initiative in collabora-tive dialogue interactions.
User Modeling and User-Adapted Interaction, 8(3-4):215-253.Jennifer Chu-Carroll.
2000.
MIMIC: An adaptive mixedinitiative spoken dialogue system for informationqueries.
In Proceedings of the 6th ACL Conference onApplied Natural Language Processing.
To appear.Paul R. Cohen.
1995.
Empirical Methods for ArtificialIntelligence.
MIT Press.Morena Danieli and Elisabetta Gerbino.
1995.
Metricsfor evaluating dialogue strategies ina spoken languagelaAlthough not currently utilized, the ability to adapt dialogue strate-gies when ASR performance is poor enables the system to employ dia-logue strategy specific language models for ASR.system.
In Proceedings of the AAAI Spring Sympo-sium on Empirical Methods in Discourse Interpreta-tion and Generation, pages 34-39.Jean Gordon and Edward H. Shortliffe.
1984.
TheDempster-Shafer theory of evidence.
In BruceBuchanan and Edward Shortliffe, editors, Rule-BasedExpert Systems: The MYCIN Experiments of theStanford Heuristic Programming Project, chapter 13,pages 272-292.
Addison-Wesley.Diane J. Litman and Shimei Pan.
1999.
Empiricallyevaluating an adaptable spoken dialogue system.
InProceedings of the 7th International Conference onUser Modeling, pages 55-64.H.
Meng, S. Busayaponchai, J.
Glass, D. Goddeau,L.
Hetherington, E. Hurley, C. Pao, J. Polifroni,S.
Seneff, and V. Zue.
1996.
WHEELS: A conversa-tional system in the automobile classifieds domain.
InProceedings of the International Conference on Spo-ken Language Processing, pages 542-545.Stefan Ortmanns, Wolfgang Reichl, and Wu Chou.
1999.An efficient decoding method for real time speechrecognition.
In Proceedings of the 5th European Con-ference on Speech Communication and Technology.K.A.
Papineni, S. Roukos, and R.T. Ward.
1999.
Free-flow dialog management using forms.
In Proceedingsof the 6th European Conference on Speech Communi-cation and Technology, pages 1411-1414.Patti Price, Lynette Hirschman, Elizabeth Shriberg, andElizabeth Wade.
1992.
Subject-based valuation mea-sures for interactive spoken language systems.
In Pro-ceedings of the DARPA Speech and Natural LanguageWorkshop, pages 34-39.Wolfgang Reichl and Wu Chou.
1998.
Decision treestate tying based on segmental c ustering for acousticmodeling.
In Proceedings of the International Confer-ence on Acoustics, Speech, and Signal Processing.M.D.
Sadek, A. Ferrieux, A. Cozannet, P. Bretier,E Panaget, and J. Simonin.
1996.
Effective human-computer cooperative spoken dialogue: The AGSdemonstrator.
In Proceedings of the InternationalConference on Spoken Language Processing.Glenn Shafer.
1976.
A Mathematical Theory of Evi-dence.
Princeton University Press.Andrew Simpson and Norman M. Fraser.
1993.
Blackbox and glass box evaluation of the SUNDIAL system.In Proceedings of the 3rd European Conference onSpeech Communication a d Technology, pages 1423-1426.Gert Veldhuijzen van Zanten.
1999.
User modelling inadaptive dialogue management.
In Proceedings of the6th European Conference on Speech Communicationand Technology, pages 1183-1186.Marilyn A. Walker, Diane J. Litman, Candance A.Kamm, and Alicia Abella.
1997.
PARADISE: Aframework for evaluating spoken dialogue agents.
InProceedings of the 35th Annual Meeting of the Associ-ation for Computational Linguistics, pages 271-280.209
