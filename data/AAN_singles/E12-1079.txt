Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 777?786,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsBehind the Article: Recognizing Dialog Acts in Wikipedia Talk PagesOliver Ferschke?, Iryna Gurevych??
and Yevgen Chebotar??
Ubiquitous Knowledge Processing Lab (UKP-DIPF)German Institute for Educational Research and Educational Information?
Ubiquitous Knowledge Processing Lab (UKP-TUDA)Department of Computer ScienceTechnische Universita?t Darmstadthttp://www.ukp.tu-darmstadt.deAbstractIn this paper, we propose an annota-tion schema for the discourse analysis ofWikipedia Talk pages aimed at the coor-dination efforts for article improvement.We apply the annotation schema to a cor-pus of 100 Talk pages from the SimpleEnglish Wikipedia and make the resultingdataset freely available for download1.
Fur-thermore, we perform automatic dialog actclassification on Wikipedia discussions andachieve an average F1-score of 0.82 withour classification pipeline.1 IntroductionOver the past decade, the paradigm of informationsharing in the web has shifted towards participa-tory and collaborative content production.
Textsare no longer exclusively prepared by individualsand then shared with the community.
They are in-creasingly created collaboratively by multiple au-thors and iteratively revised by the community.When researchers first conducted surveys onprofessional writers in the 1980s, they found thatthe collaborative writing process differs consider-ably from the way individual writing is done (Pos-ner and Baecker, 1992).
In joint writing, the writ-ers have to externalize processes that are other-wise not made explicit, like the planning and theorganization of the text.
The authors have to com-municate how the text should be written and whatexactly it should contain.Today, many tools are available that supportcollaborative writing.
A tool that has particu-larly taken hold is the Wiki, a web-based, asyn-1http://www.ukp.tu-darmstadt.de/data/wikidiscoursechronous co-authoring tool.
A unique character-istic of Wikis is the documentation of the edithistory which keeps track of every change thatis made to a Wiki page.
With this information,it is possible to reconstruct the writing processfrom the beginning to the end.
Additionally, manyWikis offer their users a communication platform,the Talk pages, where they can discuss the ongo-ing writing process with other users.The most prominent example for a successful,large-scale Wiki is Wikipedia, a collaborativelycreated online encyclopedia, which has grownconsiderably since its launch in 2001, and con-tains a total of almost 20 million articles in 282languages and dialects, as of Sept. 2011.
As thereis no editorial body that manages Wikipedia top-down, it is an open question how the huge on-line community around Wikipedia regulates andenforces standards of behavior and article qual-ity.
The user discussions on the article Talk pagesmight shed light on this issue and give an insightinto the otherwise hidden processes of collabora-tion that, until now, could only be analyzed viainterviews or group observations in experimentalsettings.The main goal of the present paper is to analyzethe content of the discussion pages of the SimpleEnglish Wikipedia with respect to the dialog actsaimed at the coordination efforts for article im-provement.
Dialog acts, according to the classicspeech act theory (Austin, 1962; Searle, 1969),represent the meaning of an utterance at the levelof illocutionary force, i.e.
a dialog act label con-cisely characterizes the intention and the role of acontribution in a dialog.
We chose the Simple En-glish Wikipedia for our initial analysis, becausewe are able to obtain more representative results777by covering almost 15% of all relevant Talk pages,as opposed to the much smaller fraction we couldachieve for the English Wikipedia.
The long-termgoal of this work is to identify relations betweencontributions on the Talk pages and particular arti-cle edits.
We plan to analyze the relation betweenarticle discussions and article content and identifythe edits in the article revision history that react tothe problems discussed on the Talk page.
In com-bination with article quality assessment (Yaari etal., 2011), this opens up the possibility to iden-tify successful patterns of collaboration which in-crease the article quality.
Furthermore, our workwill enable practical applications.
By augment-ing Wikipedia articles with the information de-rived from automatically labeled discussions, arti-cle readers can be made aware of particular prob-lems that are being discussed on the Talk page?behind the article?.Our primary contributions in this paper are: (1)an annotation schema for dialog acts reflectingthe efforts for coordinating the article improve-ment; (2) the Simple English Wikipedia Dis-cussion (SEWD) corpus, consisting of 100 seg-mented and annotated Talk pages which we makefreely available for download; and (3) a dialogact classification pipeline that incorporates sev-eral state of the art machine learning algorithmsand feature selection techniques and achieves anaverage F1-score of .82 on our corpus.2 Related WorkThe analysis of speech and dialog acts has itsroots in the linguistic field of pragmatics.
In1962, John Austin shifted the focus from the meredeclarative use of language as a means for makingfactual statements towards its non-declarative useas a tool for performing actions.
The speech acttheory was further systematized by Searle (1969),whose classification of illocutionary acts (Searle,1976) is still used as a starting point for creatingdialog act classification schemata for natural lan-guage processing.A well known, domain- and task-independentannotation schema is DAMSL (Core and Allen,1997).
It was created as the standard annotationschema for dialog tagging on the utterance levelby the Discourse Resource Initiative.
It uses afour-dimensional tagset that allows arbitrary labelcombinations for each utterance.
Jurafsky et al(1997) augmented the DAMSL schema to fit thepeculiarities of the Switchboard corpus.
The re-sulting SWDB-DAMSL schema contained morethan 220 distinct labels which have been clusteredto 42 coarse grained labels.
Both schemata haveoften been adapted for special purpose annotationtasks.With the rise of the social web, the amount ofresearch analyzing user generated discourse sub-stantially increased.
In addition to analyzing webforums (Kim et al 2010a), chats (Carpenter andFujioka, 2011) and emails (Cohen et al 2004),Wikipedia Talk pages have recently moved intothe center of attention of the research community.Vie?gas et al(2007) manually annotate 25Wikipedia article discussion pages with a set of11 labels in order to analyze how Talk pages areused for planning the work on articles and resolv-ing disputes among the editors.
Schneider et al(2011) extend this schema and manually annotate100 Talk pages with 15 labels.
They confirm thefindings of Vie?gas et althat coordination requestsoccur most frequently in the discussions.Bender et al(2011) describe a corpus of 47Talk pages which have been annotated for author-ity claims and alignment moves.
With this cor-pus, the authors analyze how the participants inWikipedia discussions establish their credibilityand how they express agreement and disagree-ment towards other participants or topics.From a different perspective, Stvilia et al(2008) analyze 60 discussion pages in regard tohow information quality (IQ) in Wikipedia arti-cles is assessed on the Talk pages and which typesof IQ problems are identified by the community.They describe a Wikipedia IQ assessment modeland map it to established frameworks.
Further-more, they provide a list of IQ problems alongwith related causal factors and necessary actionswhich has also inspired the design of our annota-tion schema.Finally, Laniado et al(2011) examineWikipedia discussion networks in order tocapture structural patterns of interaction.
Theyextract the thread structure from all Talk pages inthe English Wikipedia and create tree structuresof the discussion.
The analysis of the graphsreveals patterns that are unique to Wikipediadiscussions and might be used as a means tocharacterize different types of Talk pages.To the best of our knowledge, there is nowork yet that uses machine learning to automati-778Figure 1: Structure of a Talk page: a) Talk page title,b) untitled discussion topic, c) titled discussion topic,d) unsigned turns, e) signed turns, f) topic titlecally classify user contributions in Wikipedia Talkpages.
Furthermore, there is no corpus availablethat reflects the efforts of article improvement inWikipedia discussions.
This is the subject of ourwork.3 Annotation SchemaThe main purpose of Wikipedia Talk pages is thecoordination of the editing process with the goalof improving and sustaining the quality of the re-spective article.
The criteria for article quality inWikipedia are loosely defined in the guidelines for?good articles?2 and ?very good articles?3.
Ac-cording to these guidelines, distinguished articlesmust be well-written in simple English, compre-hensive, neutral, stable, accurate, verifiable andfollow the Wikipedia style guidelines4.
These cri-teria are the main points of reference in the dis-cussions on the Talk pages.Discourse analysis, as it is performed in this pa-per, can be carried out on various levels, depend-ing on what is regarded as the smallest unit of thediscourse.
In this work, we focus on turns, noton individual utterances, as we are interested in acoarse-grained analysis of the discourse-structureas a first step towards a finer-grained discourseanalysis.
We define a turn (or contribution) as thebody of text that is added by an individual contrib-utor in one or more revisions to a single discus-sion topic until another contributor edits the page.Furthermore, a topic (or discussion) is the bodyof turns that revolve around a single matter.
They2http://simple.wikipedia.org/wiki/WP:RGA3http://simple.wikipedia.org/wiki/WP:RVGA4http://simple.wikipedia.org/wiki/WP:STYLEare usually headed by a topic title.
Finally, thethread structure designates the sequence of turnsand their indentation levels on the Talk page.
Astructural overview of a Talk page and its con-stituents can be seen in Figure 1.We composed an annotation schema that re-flects the coordination efforts for article improve-ment.
Therefore, we manually analyzed a setof thirty Talk pages from the Simple EnglishWikipedia to identify the types of article defi-ciencies that are discussed and the way articleimprovement is coordinated.
We furthermoreincorporated the findings from an information-scientific analysis of information quality inWikipedia (Stvilia et al 2008), which identifiestwelve types of quality problems, like e.g.
Accu-racy, Completeness or Relevance.
Our resultingtagset consists of 17 labels (cf.
Table 1) which canbe subdivided into four higher level categories:Article Criticism Denote comments that iden-tify deficiencies in the article.
The criticismcan refer to the article as a whole or to indi-vidual parts of the article.Explicit Performative Announce, report or sug-gest editing activities.Information Content Describe the direction ofthe communication.
A contribution can beused to communicate new information toothers (IP), to request information (IS), orto suggest changes to established facts (IC).The IP label applies to most of the contri-butions as most comments provide a certainamount of new information.Interpersonal Describe the attitude that is ex-pressed towards other participants in the dis-cussion and/or their comments.Since a single turn may consist of several utter-ances, it can consequently comprise multiple di-alog acts.
Therefore, we designed the annotationstudy as a multi-label classification task, i.e.
theannotators can assign one or more labels to eachannotation unit.
Each label is chosen indepen-dently.
Table 1 shows the labels, their respectivedefinitions and an example from our corpus.4 Corpus Creation and AnalysisThe SEWD corpus consists of 100 annotated Talkpages extracted from a snapshot of the Simple En-779Label Description ExampleArticle CriticismCM Content incomplete or lacking detailIt should be added (1) that voters may skip prefer-ences, but (2) that skipping preferences has no impacton the result of the elections.CW Lack of accuracy or correctnessKris Kringle is NOT a Germanic god, but an Englishmispronunciation of Christkind, a German word thatmeans ?the baby Jesus?.CU Unsuitable or unnecessary contentThe references should be removed.
The reason: Thereferences are too complicated for the typical readerof simple Wikipedia.CS Structural problems Also use sectioning, and interlinkingCL Deficiencies in language or styleThis section needs to be simplified further; there are alot of words that are too complex for this wiki.COBJ Objectivity issuesThis article seems to take a clear pro-Christian, anti-commercial view.CO Other kind of criticismI have started an article on Google.
It needs improve-ment though.Explicit PerformativePSR Explicit suggestion, recommendation or request This section needs to be simplified furtherPREF Explicit reference or pointerGot it.
The URL is http://www.dmbeatles.com/history.php?year=1968PFC Commitment to an action in the future Okay, I forgot to add that, I?ll do so later tonight.PPC Report of a performed actionI took and hopefully simplified the ?
[[en:Prehistoricmusic?Prehistoric music]]?
article from EnWPInformation ContentIP Information providing ?Depression?
is the most basic term there is.IS Information seekingSo what kind of theory would you use for your musiccomposing?IC Information correctingIn linguistics and generally speaking, when Talkingabout the lexicon in a language, words are usually cat-egorized as ?nouns?, ?verbs?, ?adjectives?
and so on.The term ?doing word?
does not exist.InterpersonalATT+Positive attitude towards other contributor oracceptanceThank you.ATTP Partial acceptance or partial rejectionOkay, I can understand that, but some citations aregoing to have to be included for [[WP:V]].ATT-Negative attitude towards other contributor orrejectionNow what?
You think you know so much about every-thing, and you are not even helping?
!Table 1: Annotation schema for the dialog act classification in Wikipedia discussion pages with examples fromthe SEWD Corpus.
Some examples have been shortened to fit the table.glish Wikipedia from Apr 4th 2011.5 Technicallyspeaking, a Talk page is a normal Wiki page lo-cated in one of the Talk namespaces.
In this work,we focus on article Talk pages and do not re-gard User Talk pages.
We selected the discussionpages according to the number of turns they con-tain.
First, we discarded all discussion pages withless than four contributions.
We then analyzedthe distribution of turn counts per discussion pagein the remaining set of pages and defined threeclasses: (i) discussion pages with 4-10 turns, (ii)5The snapshot contains 69900 articles and 5783 Talkpages of which 683 contained more than 3 contributions.pages with 11-20 turns, and (iii) pages with morethan 20 turns.
We then randomly extracted 50 dis-cussion pages from class (i), 40 pages from class(ii) and 10 pages from class (iii).
This decision isgrounded in the restricted resources for the humanannotation task.Data Preprocessing Due to a lack of discussionstructure, extracting the discussion threads fromthe Talk pages requires a substantial amount ofpreprocessing.
Laniado et al(2011) tackle thethread extraction by using text indentation and in-serted user signatures as clues.
We found these780attributes to be insufficient for a reliable recon-struction of the thread structure.6Our preprocessing approach consists of threesteps: data retrieval, topic segmentation and turnsegmentation.
For retrieving the discussion pages,we use the Java Wikipedia Library (JWPL) (Zeschet al 2008), which offers efficient, database-driven access to the contents of Wikipedia.
Wesegment the individual Talk pages into discus-sions topics using the MediaWiki parser thatcomes with JWPL.
In our corpus, the parser man-aged to identify all topic boundaries without anyerrors.
The most complex preprocessing step isthe turn segmentation.First, we use the revision history of the Talkpage to identify the author and the creation timeof each paragraph.
We use the Wikipedia Revi-sion Toolkit (Ferschke et al 2011) to examine thechanges between adjacent revisions of the Talkpage in order to identify the exact time a piece oftext was added as well as the author of the con-tribution.
We have to filter out malicious editsfrom the history, as they would negatively affectthe segmentation process.
We therefore disregardall edits that are reverted in later later revisions.In contrast to vandalism on article pages, this ap-proach has proven to be sufficient to detect van-dalism in the Talk page history.Within each discussion topic, we aggregate alladjacent paragraphs with the same author and thesame time stamp to one turn.
In order to accountfor turns that were written in multiple revisions,we regard all time stamps within a window of 10minutes7 as belonging to the same turn, unless thepage was edited by another user in the meantime.Finally, the turn is marked with the indentationlevel of its least indented paragraph.
This infor-mation is used to identify the relationship betweenthe turns, since indentation is used to indicate areply to an existing comment in the discussion.A co-author of this paper evaluated the ac-ceptability of the boundaries of each turn in theSEWD corpus and found that 94% of the 1450turns were correctly segmented.
Turns with seg-mentation errors were not included in the goldstandard.6Vie?gas et al(2007) reported that only 67% of the con-tributions on Wikipedia Talk pages are signed, which makessignatures an unreliable predictor for turn boundaries.7We experimentally tested values between 1 and 60 min-utes.Annotation Process For our annotation study,we used the freely available MMAX2 annotationtool8.
Two annotators were introduced to the an-notation schema by an instructor and trained onan extra set of ten discussion pages.
During theannotation of the corpus, the annotators were al-lowed to discuss difficult cases and could consultthe instructor if in doubt.
They had access to thesegmented discussion pages within the MMAX2tool as well as to the original Wikipedia articlesand discussion pages on the web.The reconciliation of the annotations was car-ried out by an expert annotator.
In order to obtaina consolidated gold standard, the expert decidedall cases in which the annotations of the two an-notators did not match.
Descriptive statistics forthe label assignments of each annotator and forthe gold standard can be seen in Table 2 and willbe further discussed in Section 4.2.Corpus Format We publish our SEWD cor-pus in two formats9, the original MMAX format,and as XMI files for further processing with theApache Unstructured Information ManagementArchitecture10.
For the latter format, we also pro-vide the type system which defines all necessarycorpus specific types needed for using the data inan NLP pipeline.4.1 Inter-Annotator AgreementTo evaluate the reliability of our dataset, we per-form a detailed inter-rater agreement study.
Formeasuring the agreement of the individual labels,we report the observed agreement, Kappa statis-tics (Carletta, 1996), and F1-scores.
The latter arecomputed by treating one annotator as the goldstandard and the other one as predictions (Hripc-sak and Rothschild, 2005).
The scores can be seenin Table 2.The average observed agreement across all la-bels is P?O = .94.
The individual Kappa scoreslargely fall into the range that Landis and Koch(1977) regard as substantial agreement, whilethree labels are above the more strict .8 thresh-old for reliable annotations (Artstein and Poesio,2008).
Furthermore, we obtain an overall pooledKappa (De Vries et al 2008) of ?pool = .67,8http://www.mmax2.net9http://www.ukp.tu-darmstadt.de/data/wikidiscourse10http://uima.apache.org781Annotator 1 Annotator 2 Inter-Annotator Agreement Gold StandardLabel N Percent N Percent NA1?A2 PO ?
F1 N PercentArticle CriticismCM 183 13.4% 105 7.7% 193 .93 .63 .66 116 8.5%CW 106 7.8% 57 4.2% 120 .95 .52 .55 70 5.1%CU 69 5.0% 35 2.6% 83 .95 .38 .40 42 3.1%CS 164 12.0% 101 7.4% 174 .94 .66 .69 136 9.9%CL 195 14.3% 199 14.6% 244 .93 .73 .77 219 16.0%COBJ 27 2.0% 23 1.7% 29 .99 .84 .84 27 2.0%CO 20 1.5% 59 4.3% 71 .95 .18 .20 48 3.5%Explicit PerformativePSR 458 33.5% 351 25.7% 503 .86 .66 .76 406 29.7%PREF 43 3.1% 31 2.3% 51 .98 .61 .62 45 3.3%PFC 73 5.3% 65 4.8% 86 .98 .76 .77 77 5.6%PPC 357 26.1% 340 24.9% 371 .97 .92 .94 358 26.2%Information ContentIP 1084 79.3% 1027 75.1% 1135 .89 .69 .93 1070 78.3%IS 228 16.7% 208 15.2% 256 .95 .80 .83 220 16.1%IC 187 13.7% 109 8.0% 221 .89 .46 .51 130 9.5%InterpersonalATT+ 71 5.2% 140 10.2% 151 .94 .55 .58 144 10.5%ATTP 71 5.2% 30 2.2% 79 .96 .42 .44 33 2.4%ATT- 67 4.9% 74 5.4% 100 .96 .56 .58 87 6.4%Table 2: Label frequencies and inter-annotator agreement.
NA1?A2 denotes the number of turns that have beenlabeled with the given label by at least one annotator.
PO denotes the observed agreement.which is defined as?pool =P?O ?
P?E1?
P?E(1)withP?O =1LL?l=1POl , P?E =1LL?l=1PEl (2)where L denotes the number of labels, PEl theexpected agreement and POl the observed agree-ment of the lth label.
?pool is regarded to be moreaccurate than an averaged Kappa.For assessing the overall inter-rater reliabil-ity of the label set assignments per turn, wechose Krippendorff?s Alpha (Krippendorff, 1980)using MASI, a measure of agreement on set-valued items, as the distance function (Passon-neau, 2006).
MASI accounts for partial agree-ment if the label sets of both annotators overlapin at least one label.
We achieved an Alpha scoreof ?
= .75.
According to Krippendorff, datasetswith this score are considered reliable and allowtentative conclusions to be drawn.The CO label showed the lowest agreement ofonly ?
= .18.
The label was supposed to coverany criticism that is not covered by a dedicatedlabel.
However, the annotators reported that theychose this label when they were unsure whether aparticular criticism label would fit a certain turnor not.Labels in the interpersonal category all showagreement scores below 0.6.
It turned out that theannotators had a different understanding of theselabels.
While one annotator assigned the labelsfor any kind of positive or negative sentiment, theother used the labels to express agreement anddisagreement between the participants of a dis-cussion.A common problem for all labels were contri-butions with a high degree of indirectness and im-plicitness.
Indirect contributions have to be in-terpreted in the light of conversational implica-ture theory (Grice, 1975), which requires contex-tual knowledge for decoding the intentions of aspeaker.
For example, the messageIs population density allowed to be n/a?has the surface form of a question.
However, thecontext of the discussion revealed that the authortried to draw attention to the missing figure in thearticle and requested it to be filled or removed.The annotators rarely made use of the context,which was a major source for disagreement in thestudy.782Another difficulty for the annotators were longdiscussion turns.
While the average turn consistsof 42 tokens, the largest contribution in the cor-pus is 658 tokens long.
Turns of this size cancover multiple aspects and potentially comprisemany different dialog acts, which increases theprobability of disagreement.
This issue can be ad-dressed by going from the turn level to the utter-ance level in future work.A comparison of our results with the agreementreported for other datasets shows that the reliabil-ity of our annotations lies well within the field ofthe related work.
Bender et al(2011) carried outan annotation study of social acts in 365 discus-sions from 47 Wikipedia Talk pages.
They reportKappa scores for thirteen labels in two categoriesranging from .13 to .66 per label.
The overallagreement for each category was .50 and .59, re-spectively, which is considerably lower than our?pool = .67.
Kim et al(2010b) annotate pairs ofposts taken from an online forum.
They use a di-alog act tagset with twelve labels customized formodeling troubleshooting-oriented forum discus-sions.
For their corpus of 1334 posts, they reportan overall Kappa of .59.
Kim et al(2010a) iden-tify unresolved discussions in student online fo-rums by annotating 1135 posts with five differentspeech acts.
They report Kappa scores per speechact between .72 and .94.
Their better results mightbe due to a more coarse grained label set.4.2 Corpus AnalysisThe SEWD corpus contains 313 discussions con-sisting of 1367 turns by 337 users.
The averagelength of a turn is 42 words.
208 of the 337contributors are registered Wikipedia users, 129wrote anonymously.
On average, each contributorwrote 168 words in 4 turns.
However, there was acluster of 16 people with ?
20 contributions.Table 2 shows the frequencies of all labels inthe SEWD corpus.
The most frequent labels areinformation providing (IP), requests (PSR) andreports of performed edits (PPC).
The IP-labelwas assigned to more than 78% of all 1367 turns,because almost every contribution provides a cer-tain amount of information.
The label was onlyomitted if a turn merely consisted of a discussiontemplate but did not contain any text or if it exclu-sively contained questions.More than a quarter of the turns are labeledwith PSR and PPC, respectively.
This indicatesthat edit requests and reports of performed editsare the main subject of discussion.
Generally, it ismore common that edits are reported after theyhave been made than to announce them beforethey are carried out, as can be seen in the ratioof PPC to PFC labels.
The number of turns la-beled with PSR is almost the same as the numberof contributions labeled with either PPC or PFC.This allows the tentative conclusion that nearly allrequests potentially lead to an edit action.
As amatter of fact, the most common label adjacencypair11 in the corpus is PSR?PPC, which substan-tiates this assumption.Article criticism labels have been assigned to39.4% of all turns.
Almost half (241) of the labelsfrom this class are assigned to the first turn of adiscussion.
This shows that it is common to opena discussion in reference to a particular deficiencyof the article.
The large number of CL labels com-pared to other labels from the same category isdue to the fact that the Simple English Wikipediarequires authors to write articles in a way that theyare understandable for non-native speakers of En-glish.
Therefore, the use of adequate language isone of the major concerns of the Simple EnglishWikipedia community.5 Automatic Dialog Act ClassificationFor the automatic classification of dialog acts inWikipedia Talk pages, we transform the multi-label classification problem into a binary classi-fication task (Tsoumakas et al 2010).
We train abinary classifier for each label using the WEKAdata-mining software (Hall et al 2009).
We usethree learners for the classification task, a NaiveBayes classifier, J48, an implementation of theC4.5 decision tree algorithm (Quinlan, 1992) andSMO, an optimization algorithm for training sup-port vector machines (Platt, 1998).
Finally, wecombine the best performing learners for each la-bel in a UIMA-based classification pipeline (Fer-rucci and Lally, 2004).Features for Dialog Act Classification As fea-tures, we use all uni-, bi- and trigrams that oc-curred in at least three different turns.
Further-more, we include the time distance to the previ-ous and the next turn (in seconds), the length ofthe current, previous and next turn (in tokens), the11A label transition A ?
B is recorded if two adjacentturns are labeled with A and B, respectively.783position of the turn within the discussion, the in-dentation level of the turn and two binary featuresindicating whether a turn references or is refer-enced by another turn.12 In order to capture thesequential nature of the discussions, we use then-grams of the previous and the next turn as addi-tional features.Balancing Positive and Negative InstancesSince the number of positive instances for eachlabel is small compared to the number of nega-tive instances, we create a balanced dataset whichcontains an equal amount of positive and nega-tive instances.
Therefore, we randomly select theappropriate number of negative instances and dis-card the rest.
This improves the classification per-formance on every label for all three learners.Feature Selection Using the full set of features,we achieve the following macro/micro averagedF1-scores: 0.29 / 0.57 for Naive Bayes, 0.42 /0.66 for J48 and 0.43 / 0.72 for SMO.
To fur-ther improve the classification performance, wereduce the feature space using two feature selec-tion techniques, the ?2 metric (Yang and Ped-ersen, 1997) and the Information Gain approach(Mitchell, 1997).
For each label, we train separateclassifiers using the top 100, 200 and 300 featuresobtained by each feature selection technique andchoose the best performing set for our final clas-sification pipeline.Indentation and temporal distance to the pre-ceding turn proved to be the best ranked non-lexical features overall.
Additionally, the turn po-sition within the topic was a crucial feature formost labels in the criticism class and for PSR andIS labels.
This is not surprising, because articlecriticism, suggestions and questions tend to oc-cur in the beginning of a discussion.
The tworeference features have not proven to be useful.The relational information was better covered bythe indentation feature.
The subjective quality ofthe lexical features seems to be correlated withthe inter-annotator agreement of the respective la-bels.
Features for labels with low agreement con-tain many n-grams without any recognizable se-mantic connection to the label.
For labels withgood agreement, the feature lists almost exclu-sively contain meaningful lexical cues.12A turn Y references a preceding turn X if the indenta-tion level of Y is one level deeper than of X .Label Human BaseNaiveBayesJ48 SMO BestCM .66 .07 .68 .48 .66 .68CW .55 .01 .70 .20 .56 .70CU .40 .07 .66 .35 .59 .66CS .69 .09 .67 .67 .75 .75CL .77 .11 .70 .66 .73 .73COBJ .84 .04 .78 .51 .63 .78CO .20 .02 .61 .06 .39 .61PSR .76 .30 .72 .70 .76 .76PREF .62 .00 .76 .41 .64 .76PFC .77 .04 .70 .62 .73 .73PPC .94 .25 .74 .82 .85 .85IP .93 .74 .83 .93 .93 .93IS .83 .16 .79 .86 .85 .86IC .51 .06 .67 .32 .59 .67ATT+ .58 .10 .61 .65 .72 .72ATTP .44 .03 .72 .25 .62 .72ATT- .58 .07 .52 .30 .52 .52Macro .65 .13 .70 .52 .68 .73Micro .79 .35 .74 .75 .80 .82Table 3: F1-Scores for the balanced set with featureselection on 10-fold cross-validation.
Base refers tothe baseline performance, Best to our classificationpipeline.Classification Results Table 3 shows the per-formance of all classifiers and our final classi-fication pipeline evaluated on 10-fold cross val-idation.
Naive Bayes performed surprisinglywell and showed the best macro averaged scoresamong the three learners while SMO showed thebest micro averaged performance.
We compareour results to a random baseline and to the per-formance of the human annotators (cf.
Table 3and Figure 2).
The baseline assigns the dialog actlabels at random according to their frequency dis-tribution in the gold standard.
Our classifier out-performed the baseline significantly on all labels.The comparison with the human performanceshows that our system is able to reach the humanperformance.
In most cases, the annotation agree-ment is reliable, and so are the results of the auto-matic classification.
For the labels CU and CO,the inter-annotator agreement is not high.
Thecomparably good performance of the classifierson these labels shows that the instances do haveshared characteristics.
Human raters, however,have difficulties recognizing these labels consis-tently.
Thus, their definitions need to be refined infuture work.To our knowledge, none of the related work ondiscourse analysis of Wikipedia Talk pages per-784CM CW CU CS CLCOBJ CO PSRPREF PFCPPC IP IS ICATT+ATTPATT-00.20.40.60.81F1-scoreBest Human BaselineFigure 2: F1-Scores for our classification pipeline (Best), the human performance and baseline performance.formed automatic dialog act classification.
How-ever, there has been previous work on classify-ing speech acts in other discourse types.
Kim etal.
(2010a) use Support Vector Machines (SVM)and Transformation Based Learning (TBL) forthe automatic assignment of five speech acts toposts taken from student online forums.
They re-port individual F1-scores per label which resultin a macro average of 0.59 for SVM and 0.66for TBL.
Cohen et al(2004) classify speech actsin emails.
They train five binary classifiers us-ing several learners on 1375 emails and report F1scores per speech act between .44 and .85.
De-spite the larger tagset, our classification approachachieves an average F1-score of .82 and thereforelies in the top ranks of the related work.6 ConclusionsIn this paper, we proposed an annotation schemafor the discourse analysis of Wikipedia discus-sions aimed at the coordination efforts for articleimprovement.
We applied the annotation schemato a corpus of 100 Wikipedia Talk pages, whichwe make freely available for download.
A thor-ough analysis of the inter-annotator agreementshowed that the dataset is reliable.
Finally, weperformed automatic dialog act classification onWikipedia Talk pages.
Therefore, we combinedthree machine learning algorithms and two featureselection techniques to a classification pipeline,which we trained on our SEWD corpus.
Weachieve an average F1-score of .82, which is com-parable to the human performance of .79.
Theability to automatically classify discussion pageswill help to investigate the relations between arti-cle discussions and article edits, which is an im-portant step towards understanding the processesof collaboration in large-scale Wikis.
Further-more, it will be the basis for practical applicationsthat bring the hidden content of Talk pages to theattention of article readers.AcknowledgmentsThis work has been supported by the VolkswagenFoundation as part of the Lichtenberg-Professorship Program under grant No.
I/82806,and by the Hessian research excellence program?Landes-Offensive zur Entwicklung Wissen-schaftlich-o?konomischer Exzellenz?
(LOEWE)as part of the research center ?Digital Humani-ties?.ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-CoderAgreement for Computational Linguistics.
Compu-tational Linguistics, 34(4):555?596, December.John L. Austin.
1962.
How to Do Things with Words.Clarendon Press, Cambridge, UK.Emily M. Bender, Jonathan T. Morgan, Meghan Ox-ley, Mark Zachry, Brian Hutchinson, Alex Marin,Bin Zhang, and Mari Ostendorf.
2011.
Annotat-ing Social Acts: Authority Claims and AlignmentMoves in Wikipedia Talk Pages.
In Proceedings ofthe Workshop on Language in Social Media, pages48?57, Portland, Oregon, USA.Jean Carletta.
1996.
Assessing Agreement on Classi-fication Tasks: The Kappa Statistic.
ComputationalLinguistics, 22(2):249?254.Tamitha Carpenter and Emi Fujioka.
2011.
The Roleand Identification of Dialog Acts in Online Chat.
InProceesings of the Workshop on Analyzing Micro-text at the 25th AAAI Conference on Artificial Intel-ligence, San Francisco, CA, USA.William W. Cohen, Vitor R. Carvalho, and Tom M.Mitchell.
2004.
Learning to Classify Email into?Speech Acts?.
In Proceedings of the 2004 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 309?316, Barcelona, ES.785Mark G. Core and James F. Allen.
1997.
Cod-ing dialogs with the DAMSL annotation scheme.In Proceedings of the Working Notes of the AAAIFall Symposium on Communicative Action in Hu-mans and Machines, pages 28?35, Cambridge, MA,USA.Han De Vries, Marc N. Elliott, David E. Kanouse, andStephanie S. Teleki.
2008.
Using Pooled Kappato Summarize Interrater Agreement across ManyItems.
Field Methods, 20(3):272?282.David Ferrucci and Adam Lally.
2004.
UIMA: An Ar-chitectural Approach to Unstructured InformationProcessing in the Corporate Research Environment.Natural Language Engineering, 10:327?348.Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.2011.
Wikipedia Revision Toolkit: EfficientlyAccessing Wikipedia?s Edit History.
In Proceed-ings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human Lan-guage Technologies.
System Demonstrations, pages97?102, Portland, OR, USA.Paul Grice.
1975.
Logic and Conversation.
In Pe-ter Cole and Jerry L. Morgan, editors, Syntax andSemantics, volume 3.
New York: Academic Press.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: An Up-date.
SIGKDD Explorations, 11:10?18.George Hripcsak and Adam S. Rothschild.
2005.Agreement, the f-measure, and reliability in infor-mation retrieval.
Journal of the American MedicalInformatics Association, 12(3):296?298.Dan Jurafsky, Liz Shriberg, and Debbra Biasca.
1997.Switchboard SWBD-DAMSL Shallow-Discourse-Function Annotation Coders Manual.
TechnicalReport Draft 13, University of Colorado, Instituteof Cognitive Science.Jihie Kim, Jia Li, and Taehwan Kim.
2010a.
To-wards Identifying Unresolved Discussions in Stu-dent Online Forums.
In Proceedings of the NAACLHLT 2010 Fifth Workshop on Innovative Use of NLPfor Building Educational Applications, pages 84?91, Los Angeles, CA, USA.Su Nam Kim, Li Wang, and Timothy Baldwin.
2010b.Tagging and linking web forum posts.
In Pro-ceedings of the Fourteenth Conference on Compu-tational Natural Language Learning, CoNLL ?10,pages 192?202, Stroudsburg, PA, USA.Klaus Krippendorff.
1980.
Content Analysis: AnIntroduction to Its Methodology.
Thousand Oaks,CA: Sage Publications.J.
Richard Landis and Gary G. Koch.
1977.
An Appli-cation of Hierarchical Kappa-type Statistics in theAssessment of Majority Agreement among Multi-ple Observers.
Biometrics, 33(2):363?374, June.David Laniado, Riccardo Tasso, Yana Volkovich, andAndreas Kaltenbrunner.
2011.
When the Wikipedi-ans Talk: Network and Tree Structure of WikipediaDiscussion Pages.
In Proceedings of the 5th Inter-national AAAI Conference on Weblogs and SocialMedia, Dublin, IE.Tom Mitchell.
1997.
Machine Learning.
McGraw-Hill Education (ISE Editions), 1st edition.Rebecca Passonneau.
2006.
Measuring Agreement onSet-valued Items (MASI) for Semantic and Prag-matic Annotation.
In Proceedings of the Fifth In-ternational Conference on Language Resources andEvaluation, Genoa, IT.John C. Platt.
1998.
Fast training of support vectormachines using sequential minimal optimization.In Advances in Kernel Methods: Support VectorLearning, pages 185?208, Cambridge, MA, USA.Ilona R. Posner and Ronald M. Baecker.
1992.
HowPeople Write Together.
In Proceedings of the 25thHawaii International Conference on System Sci-ences, pages 127?138, Wailea, Maui, HI, USA.Ross Quinlan.
1992.
C4.5: Programs for MachineLearning.
Morgan Kaufmann, 1st edition.Jodi Schneider, Alexandre Passant, and John G. Bres-lin.
2011.
Understanding and Improving WikipediaArticle Discussion Spaces.
In Proceedings of the26th Symposium on Applied Computing, Taichung,TW.John R. Searle.
1969.
Speech Acts.
Cambridge Uni-versity Press, Cambridge, UK.John R. Searle.
1976.
A classification of illocutionaryacts.
Language in Society, 5:1?23.Besiki Stvilia, Michael B. Twidale, Linda C. Smith,and Les Gasser.
2008.
Information Quality WorkOrganization in Wikipedia.
Journal of the Ameri-can Society for Information Science, 59:983?1001.Grigorios Tsoumakas, Ioannis Katakis, and Ioannis P.Vlahavas.
2010.
Mining multi-label data.
In DataMining and Knowledge Discovery Handbook, pages667?685.
Springer.Fernanda Vie?gas, Martin Wattenberg, Jesse Kriss, andFrank Ham.
2007.
Talk Before You Type: Coor-dination in Wikipedia.
In Proceedings of the 40thAnnual Hawaii International Conference on SystemSciences, Waikoloa, Big Island, HI, USA.Eti Yaari, Shifra Baruchson-Arbib, and Judit Bar-Ilan.2011.
Information quality assessment of commu-nity generated content: A user study of Wikipedia.Journal of Information Science, 37:487?498.Yiming Yang and Jan O. Pedersen.
1997.
A Compara-tive Study on Feature Selection in Text Categoriza-tion.
In Proceedings of the Fourteenth InternationalConference on Machine Learning, pages 412?420,San Francisco, CA, USA.Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.2008.
Extracting Lexical Semantic Knowledgefrom Wikipedia and Wiktionary.
In Proceedings ofthe 6th International Conference on Language Re-sources and Evaluation, Marrakech, MA.786
