Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 94?103,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsHidden Markov tree models for semantic class inductionE?douard GraveInria - Sierra Project-TeamE?cole Normale Supe?rieureParis, FranceEdouard.Grave@inria.frGuillaume ObozinskiUniversite?
Paris-Est, LIGME?cole des Ponts - ParisTechMarne-la-Valle?e, FranceGuillaume.Obozinski@imagine.enpc.frFrancis BachInria - Sierra Project-TeamE?cole Normale Supe?rieureParis, FranceFrancis.Bach@ens.frAbstractIn this paper, we propose a new methodfor semantic class induction.
First, we in-troduce a generative model of sentences,based on dependency trees and whichtakes into account homonymy.
Our modelcan thus be seen as a generalization ofBrown clustering.
Second, we describean efficient algorithm to perform inferenceand learning in this model.
Third, weapply our proposed method on two largedatasets (108 tokens, 105 words types),and demonstrate that classes induced byour algorithm improve performance overBrown clustering on the task of semi-supervised supersense tagging and namedentity recognition.1 IntroductionMost competitive learning methods for compu-tational linguistics are supervised, and thus re-quire labeled examples, which are expensive toobtain.
Moreover, those techniques suffer fromdata scarcity: many words only appear a smallnumber of time, or even not at all, in the trainingdata.
It thus helps a lot to first learn word clus-ters on a large amount of unlabeled data, whichare cheap to obtain, and then to use this clustersas features for the supervised task.
This schemehas proven to be effective for various tasks suchas named entity recognition (Freitag, 2004; Milleret al 2004; Liang, 2005; Faruqui et al 2010),syntactic chunking (Turian et al 2010) or syntac-tic dependency parsing (Koo et al 2008; Haffariet al 2011; Tratz and Hovy, 2011).
It was alsosuccessfully applied for transfer learning of multi-lingual structure by Ta?ckstro?m et al(2012).The most commonly used clustering method forsemi-supervised learning is the one proposed byBrown et al(1992), and known as Brown clus-tering.
While still being one of the most efficientword representation method (Turian et al 2010),Brown clustering has two limitations we want toaddress in this work.
First, since it is a hard clus-tering method, homonymy is ignored.
Second, itdoes not take into account syntactic relations be-tween words, which seems crucial to induce se-mantic classes.
Our goal is thus to propose amethod for semantic class induction which takesinto account both syntax and homonymy, and thento study their effects on semantic class learning.In this paper, we start by introducing a new un-supervised method for semantic classes induction.This is achieved by defining a generative modelof sentences with latent variables, which aims atcapturing semantic roles of words.
We require ourmethod to be scalable, in order to learn models onlarge datasets containing tens of millions of sen-tences.
More precisely, we make the followingcontributions:?
We introduce a generative model of sen-tences, based on dependency trees, which canbe seen as a generalization of Brown cluster-ing,?
We describe a fast approximate inference al-gorithm, based on message passing and on-line EM for scaling to large datasets.
It al-lowed us to learn models with 512 latentstates on a dataset with hundreds of millionsof tokens in less than two days on a singlecore,?
We learn models on two datasets, Wikipediaarticles about musicians and the NYT corpus,94and evaluate them on two semi-supervisedtasks, namely supersense tagging and namedentity recognition.1.1 Related workBrown clustering (Brown et al 1992) is the mostcommonly used method for word cluster induc-tion for semi-supervised learning.
The goal of thisalgorithm is to discover a clustering function Cfrom words to clusters which maximizes the like-lihood of the data, assuming the following sequen-tial model of sentences:?kp(wk | C(wk))p(C(wk) | C(wk?1)).It can be shown that the best clustering is actuallymaximizing the mutual information between adja-cent clusters.
A greedy agglomerative algorithmwas proposed by Brown et al(1992) in order tofind the clustering C, while Clark (2003) proposedto use the exchange clustering algorithm (Kneserand Ney, 1993) to maximize the previous likeli-hood.
One of the limitations of this model is thefact that it neither takes into account homonymyor syntax.Another limitation of this method is the com-plexity of the algorithms proposed to find the bestclustering.
This led Uszkoreit and Brants (2008)to consider a slightly different model, where theclass-to-class transitions are replaced by word-to-class transitions:?kp(wk | C(wk))p(C(wk) | wk?1).Thanks to that modification, Uszkoreit and Brants(2008) designed an efficient variant of the ex-change algorithm, allowing them to train modelson very large datasets.
This model was then ex-tended to the multilingual setting by Ta?ckstro?m etal.
(2012).Semantic space models are another family ofmethods, besides clustering, that can be used asfeatures for semi-supervised learning.
In thosetechniques, words are represented as vectors ina high-dimensional space.
These vectors are ob-tained by representing the unlabeled corpus as aword-document co-occurrence matrix in the caseof latent semantic analysis (LSA) (Deerwester etal., 1990), or word-word co-occurrence matrix inthe case of the hyperspace analog to languagemodel (HAL) (Lund and Burgess, 1996).
Dimen-sion reduction is then performed, by taking thesingular value decomposition of the co-occurrencematrix, in order to obtained the so-called seman-tic space.
Hofmann (1999) proposed a variant ofLSA, which corresponds to a generative model ofdocument.
More recently, Dhillon et al(2011)proposed a method based on canonical correlationanalysis to obtained a such word embeddings.A last approach to word representation is la-tent Dirichlet alcation (LDA), proposed by Bleiet al(2003).
LDA is a generative model whereeach document is viewed as a mixture of topics.The major difference between LDA and our modelis the fact that LDA treats documents as bags ofwords, while we introduce a model of sentences,taking into account the syntax.
Griffiths et al(2005) defined a composite model, using LDA fortopic modeling and an HMM for syntax model-ing.
This model, HMM-LDA, was used by Liand McCallum (2005) for semi-supervised learn-ing and applied to part-of-speech tagging and Chi-nese word segmentation.
Se?aghdha (2010) pro-posed to use topic models, such as LDA, to per-form selectional preference induction.Finally, Boyd-Graber and Blei (2009) proposeda variant of LDA, using parse trees to include thesyntax.
Given that we aim for our classes to cap-ture as much of the word semantics reflected bythe syntax, such as the semantic roles of words,we believe that it is not necessarily useful or evendesirable that the latent variables should be deter-mined, even in part, by topic parameters that aresharing information at the document level.
More-over, our model being significantly simpler, wewere able to design fast and efficient algorithms,making it possible to use our model on muchlarger datasets, and with many more latent classes.2 ModelIn this section, we introduce our probabilistic gen-erative model of sentences.
We start by settingup some notations.
A sentence is representedby a K-tuple w = (w1, ..., wK) where eachwk ?
{1, ..., V } is an integer representing a wordand V is the size of the vocabulary.
Our goal willbe to infer a K-tuple c = (c1, ..., cK) of seman-tic classes, where each ck ?
{1, ..., C} is an in-teger representing a semantic class, correspondingto the word wk.The generation of a sentence can be decom-posed in two steps: first, we generate the seman-tic classes according to a Markov process, and95Opposition political parties have harshly criticized the pactc0 c1 c2 c3 c4 c5 c6 c7 c8w1 w2 w3 w4 w5 w6 w7 w8Figure 1: Example of a dependency tree and its corresponding graphical model.then, given each class ck, we generate the corre-sponding word wk independently of other words.The Markov process used to generate the seman-tic classes will take into account selectional pref-erence.
Since we want to model homonymy, eachword can be generated by multiple classes.We now describe the Markov process we pro-pose to generate the semantic classes.
We assumethat we are given a directed tree defined by thefunction pi : {1, ...,K} 7?
{0, ...,K}, where pi(k)represents the unique parent of the node k and 0is the root of the tree.
Each node, except the root,corresponds to a word of the sentence.
First, wegenerate the semantic class corresponding to theroot of the tree and then generate recursively theclass for the other nodes.
The classes are condi-tionally independent given the classes of their par-ents.
Using the language of probabilistic graphicalmodels, this means that the distribution of the se-mantic classes factorizes in the tree defined by pi(See Fig.
1 for an example).
We obtain the fol-lowing distribution on pairs (w, c) of words andsemantic classes:p(w, c) =K?k=1p(ck | cpi(k))p(wk | ck),with c0 being equal to a special symbol denotingthe root of the tree.In order to fully define our model, we nowneed to specify the observation probability distri-bution p(wk | ck) of a word given the correspond-ing class and the transition probability distributionp(ck | cpi(k)) of a class given the class of the par-ent.
Both these distributions will be categorical(and thus multinomial with one trial).
The cor-responding parameters will be represented by thestochastic matrices O and T (i.e.
matrices withnon-negative elements and unit-sum columns):p(wk = i | ck = j) = Oij ,p(ck = i | cpi(k) = j) = Tij .Finally, we introduce the trees that we consider todefine the distribution on semantic classes.
(Werecall that the trees are assumed given, and not apart of the model.
)2.1 Markov chain modelThe simplest structure we consider on the seman-tic classes is a Markov chain.
In this special case,our model reduces to a hidden Markov model.Each semantic class only depends on the class ofthe previous word in the sentence, thus failing tocapture selectional preference of semantic class.But because of its simplicity, it may be more ro-bust, and does not rely on external tools.
It can beseen as a generalization of the Brown clusteringalgorithm (Brown et al 1992) taking into accounthomonymy.2.2 Dependency tree modelThe second kind of structure we consider to modelinteractions between semantic classes is a syntac-tic dependency tree corresponding to the sentence.A dependency tree is a labeled tree in which nodescorrespond to the words of a sentence, and edgesrepresent the grammatical relations between thosewords, such as nominal subject, direct object ordeterminer.
We use the Stanford typed dependen-cies basic representations, which always form atree (De Marneffe and Manning, 2008).96We believe that a dependency tree is a betterstructure than a Markov chain to learn semanticclasses, with no additional cost for inference andlearning compared to a chain.
First, syntactic de-pendencies can capture long distance interactionsbetween words.
See Fig.
1 and the dependencybetween parties and criticized for an ex-ample.
Second, the syntax is important to modelselectional preference.
Third, we believe that syn-tactic trees could help much for languages whichdo not have a strict word order, such as Czech,Finnish, or Russian.
One drawback of this modelis that all the children of a particular node sharethe same transition probability distribution.
Whilethis is not a big issue for nouns, it is a bigger con-cern for verbs: subject and object should not sharethe same transition probability distribution.A potential solution would be to introduce a dif-ferent transition probability distribution for eachtype of dependency.
This possibility will be ex-plored in future work.2.3 Brown clustering on dependency treesAs for Brown clustering, we can assume thatwords are generated by a single class.
In that case,our model reduces to finding a deterministic clus-tering function C which maximizes the followinglikelihood:?kp(wk | C(wk))p(C(wk) | C(wpi(k))).In that case, we can use the algorithm proposedby Brown et al(1992) to greedily maximize thelikelihood of the data.
This model can be seen asa generalization of Brown clustering taking intoaccount the syntactic relations between words.3 Inference and learningIn this section, we present the approach used toperform learning and inference in our model.
Ourgoal here is to have efficient algorithms, in orderto apply our model to large datasets (108 tokens,105 words types).
The parameters T and O of themodel will be estimated with the maximum likeli-hood estimator:T?, O?
= argmaxT,ON?n=1p(w(n) | T,O),where (w(n))n?
{1,...,N} represents our training setof N sentences.First, we present an online variant of the well-known expectation-maximization (EM) algorithm,proposed by Cappe?
and Moulines (2009), allowingour method to be scalable in term of numbers ofexamples.
Then, we present an approximate mes-sage passing algorithm which has a linear com-plexity in the number of classes, instead of thequadratic complexity of the exact inference algo-rithm.
Finally, we describe a state-splitting strat-egy to speed up the learning.3.1 Online EMIn the batch EM algorithm, the E-step consists incomputing the expected sufficient statistics ?
and?
of the model, sometimes referred as pseudo-counts, corresponding respectively to T and O:?ij =N?n=1Kn?k=1E[?
(c(n)k = i, c(n)pi(k) = j)],?ij =N?n=1Kn?k=1E[?
(w(n)k = i, c(n)k = j)].On large datasets, N which is the number of sen-tences can be very large, and so, EM is inefficientbecause it requires that inference is performed onthe entire dataset at each iteration.
We thereforeconsider the online variant proposed by Cappe?and Moulines (2009): instead of recomputing thepseudocounts on the whole dataset at each itera-tion t, those pseudocounts are updated using onlya small subset Bt of the data, to get?
(t)ij = (1?
?t)?
(t?1)ij +?t?n?BtKn?k=1E[?
(c(n)k = i, c(n)pi(k) = j)],and?
(t)ij = (1?
?t)?
(t?1)ij +?t?n?BtKn?k=1E[?
(w(n)k = i, c(n)k = j)],where the scalars ?t are defined by ?t = 1/(a +t)?
with 0.5 < ?
?
1.
In the experiments,we used a = 4.
We chose ?
in the set{0.5, 0.6, 0.7, 0.8, 0.9, 1.0}.3.2 Approximate inferenceInference is performed on trees using the sum-product message passing algorithm, a.k.a.
belief970 2000 4000 6000 8000 10000Iteration5.955.905.855.80Normalized log-likelihoodk = 128k = 64k = 32k = 16 0 2000 4000 6000 8000 10000Iteration5.955.905.855.80Normalized log-likelihoodepsilon = 0.0epsilon = 0.001epsilon = 0.01epsilon = 0.1 0 100 200 300 400 500Iteration01020304050607080Supportsize epsilon = 0.0001epsilon = 0.001epsilon = 0.01epsilon = 0.1Figure 2: Comparison of the two projection methods for approximating vectors, for a model with 128latent classes.
The first two plots are the log-likelihood on a held-out set as a function of the iterates ofonline EM.
Green curves (k = 128 and ?
= 0) correspond to learning without approximation.propagation, which extends the classical ???
re-cursions used for chains, see e.g.
Wainwright andJordan (2008).
We denote by N (k) the set con-taining the children and the father of node k. Inthe exact message-passing algorithm, the message?k?pi(k) from node k to node pi(k) takes the form:?k?pi(k) = T>u,where u is the vector obtained by taking the ele-mentwise product of all the messages received bynode k except the one from node pi(k), i.e.,ui =?k?
?N (k)\{pi(k)}?k?
?k(i).Similarly, the pseudocounts can be written asE[?
(c(n)k = i, c(n)pi(k) = j)]?
uiTijvj ,where v is the vector obtained by taking the ele-mentwise product of all the messages received bynode pi(k), except the one from node k, i.e.,vj =?k?
?N (pi(k))\{k}?k?
?pi(k)(j).Both these operations thus have quadratic com-plexity in the number of semantic classes.
In or-der to reduce the complexity of those operations,we propose to start by projecting the vectors uand v on a set of sparse vectors, and then, per-form the operations with the sparse approximatevectors.
We consider two kinds of projections:?
k-best projection, where the approximatevector is obtained by keeping the k largestcoefficients,?
?-best projection, where the approximatevector is obtained by keeping the smallest setof larger coefficients such that their sum isgreater than (1?
?)
times the `1-norm of theoriginal vector.This method is similar to the one proposed by Palet al(2006).
The advantage of the k-best projec-tion is that we control the complexity of the op-erations, but not the error, while the advantage ofthe ?-best projection is that we control the errorbut not the complexity.
As shown in Fig.
2, goodchoices for ?
and k are respectively 0.01 and 16.We use these values in the experiments.
We alsonote, on the right plot of Fig.
2, that during thefirst iterations of EM, the sparse vectors obtainedwith the ?-best projection have a large number ofnon-zero elements.
Thus, this projection is notadequate to directly learn large latent class mod-els.
This issue is addressed in the next section,where we present a state splitting strategy in or-der to learn models with a large number of latentclasses.3.3 State splittingA common strategy to speed up the learning oflarge latent state space models, such as ours, isto start with a small number of latent states, andsplit them during learning (Petrov, 2009).
As faras we know, there are still no good heuristics tochoose which states to split, or how to initialize theparameters corresponding to the new states.
Wethus apply the simple, yet effective method, con-sisting in splitting all states into two and in break-ing the symmetry by adding a bit of randomnessto the emission probabilities of the new states.
Asnoted by Petrov (2009), state splitting could alsoimprove the quality of learnt models.3.4 InitializationBecause the negative log-likelihood function is notconvex, initialization can greatly change the qual-ity of the final model.
Initialization for online EMis done by setting the initial pseudocounts, andthen performing an M-step.
We have considered98the following strategies to initialize our model:?
random initialization: the initial pseudo-counts ?ij and ?ij are sampled from a uni-form distribution on [0, 1],?
Brown initialization: the model is initial-ized using the (normalized) pseudocounts ob-tained by the Brown clustering algorithm.Because a parameter equal to zero remainsequal to zero when using the EM algorithm,we replace null pseudocounts by a smallsmoothing value, e.g., for observation i, weuse 10?5 ?maxj ?ij ,4 ExperimentsIn this section, we present the datasets used for theexperiments, and the two semi-supervised taskson which we evaluate our models: named entityrecognition and supersense tagging.4.1 DatasetsWe considered two datasets: the first one, whichwe refer to as the music dataset, corresponds toall the Wikipedia articles refering to a musicalartist.
They were extracted using the Freebasedatabase1.
This dataset comprises 2.22 millionssentences and 56 millions tokens.
We choose thisdataset because it corresponds to a restricted do-main.The second dataset are the articles of the NYTcorpus (Sandhaus, 2008) corresponding to the pe-riod 1987-1997 and labeled as news.
This datasetcomprises 14.7 millions sentences and 310 mil-lions tokens.We parsed both datasets using the Stanfordparser, and converted parse trees to dependencytrees (De Marneffe et al 2006).
We decided todiscard sentences longer than 50 tokens, for pars-ing time reasons, and then lemmatized tokens us-ing Wordnet.
Each word of our vocabulary is thena pair of lemma and its associated part-of-speech.This means that the noun attack and the verb at-tack are two different words.
Finally, we intro-duced a special token, -*-, for infrequent (lemma,part-of-speech) pairs, in order to perform smooth-ing.
For the music dataset, we kept the 25 000most frequent words, while for the NYT corpus,we kept the 100 000 most frequent words.
For themusic dataset we set the number of latent states to256, while we set it to 512 for the NYT corpus.1www.freebase.com4.2 Qualitative resultsBefore moving on to the quantitative evaluation ofour model, we discuss qualitatively the induced se-mantic classes.
Examples of semantic classes arepresented in Tables 1, 2 and 3.
Tree models withrandom initialization were used to obtain those se-mantic classes.
First we observe that most classescan be easily given natural semantic interpretation.For example class 196 of Table 1 contains musicalinstruments, while class 116 contains musical gen-res.Table 2 presents groups of classes that contain agiven homonymous word; it seems that the differ-ent classes capture rather well the different sensesof each word.
For example, the word head belongsto the class 116, which contains body parts and tothe class 127, which contains words referring toleaders.4.3 Semi-supervised learningWe propose to evaluate and compare the differentmodels in the following semi-supervised learningsetting: we start by learning a model on the NYTcorpus in an unsupervised way, and then use it todefine features for a supervised classifier.
We nowintroduce the tasks we considered.4.3.1 Named entity recognitionThe first supervised task on which we evaluate thedifferent models, is named entity recognition.
Wecast it as a sequence tagging problem, and thus, weuse a linear conditional random field (CRF) (Laf-ferty et al 2001) as our supervised classifier.
Foreach sentence, we apply the Viterbi algorithm inorder to obtain the most probable sequence of se-mantic classes, and use this as features for theCRF.
The only other feature we use is a binaryfeature indicating if the word is capitalized or not.Results of experiments performed on the MUC7dataset are reported in table 4.
The baseline forthis task is assigning named entity classes to wordsequences that occur in the training data.4.3.2 Supersense taggingSupersense tagging consists in identifying, foreach word of a sentence, its corresponding su-persense, a.k.a.
lexicographer class, as defined byWordnet (Ciaramita and Altun, 2006).
Becauseeach Wordnet synset belongs to one lexicogra-pher class, supersense tagging can be seen as acoarse disambiguation task for nouns and verbs.We decided to evaluate our models on this task to99# 54 radio BBC television station tv stations channel 1 MTV program network fm music# 52 chart billboard uk top top singles 100 Hot album country 40 10 R&B 200 US song u.s.# 78 bach mozart liszt beethoven wagner chopin brahms stravinsky haydn debussy tchaikovsky# 69 sound style instrument elements influence genre theme form lyric audience direction#215 tour show concert performance appearance gig date tours event debut session set night party#116 rock pop jazz classical folk punk metal roll hip country traditional -*- blues dance#123 win receive sell gain earn award achieve garner give enjoy have get attract bring include#238 reach peak hit chart go debut make top platinum fail enter gold become with certify#203 piano concerto -*- for violin symphony in works sonata string of quartet orchestra no.#196 guitar bass vocal drum keyboard piano saxophone percussion violin player trumpet organ#243 leave join go move form return sign tour begin decide continue start attend meet disband#149 school university college hall conservatory academy center church institute cathedralTable 1: Selected semantic classes corresponding to the music dataset.
Like LDA, our model is a proba-bilistic model which generates words from latent classes.
Unlike LDA though, rather than treating wordsas exchangeable, it accounts for syntax and semantic relations between words.
As a consequence, insteadof grouping words with same topic but various semantic roles or grammatical functions, our model tendsto group words that tend to be syntactically and semantically equivalent.#116 head hand hands foot face shoulder way knee eyes back body finger car arms arm#127 president member director chairman executive head editor professor manager secretary#360 company corporation group industry fund bank association institute trust system#480 street avenue side bank square precinct coast broadway district strip bridge station#87 pay base sell use available buy depend make provide receive get lose spend charge offer#316 charge arrest convict speak tell found accuse release die indict ask responsible suspend#263 system computer machine technology plant product program equipment line network#387 plan agreement contract effort program proposal deal offer bill bid order campaign request#91 have be win score play lead hit make run -*- lose finish pitch start miss come go shoot take#198 kill shoot die wound injure found arrest fire report take dead attack beat leave strike carryTable 2: Semantic classes containing homonymous words.
Different classes capture different senses ofeach word.demonstrate the effect of homonymy.
We cast su-persense tagging as a classification problem anduse posterior distribution of semantic classes asfeatures for a support vector machine with theHellinger kernel, defined byK(p,q) =C?c=1?pcqc,where p and q are posterior distributions.
We trainand test the SVM classifier on the section A, B andC of the Brown corpus, tagged with Wordnet su-persenses (SemCor).
All the considered methodspredict among the possible supersenses accordingto Wordnet, or among all the supersenses if theword does not appear in Wordnet.
We report re-sults in Table 5.
The baseline predicts the mostcommon supersense of the training set.4.4 Discussion of resultsFirst, we observe that hidden Markov models im-prove performances over Brown clustering, onboth chains and trees.
This seems to indicatethat taking into account homonymy leads to richermodels which is beneficial for both tasks.
We alsonote that Brown clustering on dependency trees al-ways outperforms Brown clustering on chains forthe two tasks we consider, confirming that syntac-tic dependencies are a better structure to inducesemantic classes than a linear chain.Hidden Markov tree models also outperformhidden Markov chain models, except for super-sense tagging on verbs.
We believe that this dropin performance on verbs can be explained becausein English the word order (Subject-Verb-Object)is strict, and thus, the chain model is able to dif-100#484 rise fell be close offer drop gain trade price jump slip end decline unchanged sell total lose#352 it have would But be not nt will get may too make So see might can always still probably#115 coach manager bill Joe george don pat Jim bob Lou al general mike Dan tom owner ray#131 San St. santa Notre s Francisco calif. green tampa Diego louis class AP bay &aaa Fla. Jose#350 strong short score good better hit second leave fast close impressive easy high quick enough#274 A Another an new second single free -*- special fair national strong long major political big#47 gogh rushdie pan guardia vega freud Prensa miserable picasso jesus Armani Monde Niro#489 health public medical right care human civil community private social research housing#238 building house home store apartment area space restaurant site neighborhood town park#38 more very too as so much less enough But seem even because if particularly relatively prettyTable 3: Randomly selected semantic classes corresponding to the news dataset.F1 scoreBaseline 71.66Brown clustering 82.57tree Brown clustering 82.93chain HMM, random init 84.66chain HMM, Brown init 84.47tree HMM, random init 84.07tree HMM, Brown init 85.49Table 4: Results of semi-supervised named entityrecognition.ferentiate between subject and object, while thetree model treats subject and object in the sameway (both are children of the verb).
Moreover, inthe tree model, verbs have a lot of children, suchas adverbial clauses and auxiliary verbs, whichshare their transition probability distribution withthe subject and the object.
These two effects makethe disambiguation of verbs more noisy for treesthan for chains.
Another possible explanation ofthis drop of performance is that it is due to errorsmade by the syntactic parser.4.5 On optimization parametersWe briefly discuss the different choices that caninfluence learning efficiency in the proposed mod-els.
In practice, we have not observed noticeabledifferences between ?-best projection and k-bestprojection for the approximate inference, and wethus advise to use the latter as its complexity iscontroled.
By contrast, as illustrated by results intables 4 and 5, initialization can greatly change theperformance in semi-supervised learning, in par-ticular for tree models.
We thus advise to initializewith Brown clusters.
Finally, as noted by Liangand Klein (2009), the step size of online EM alsonouns verbsBaseline 61.9 (0.2) 43.1 (0.2)Brown clustering 73.9 (0.1) 63.7 (0.2)tree Brown clustering 75.0 (0.2) 65.2 (0.2)HMM (random) 76.1 (0.1) 63.0 (0.2)HMM (Brown) 76.8 (0.1) 66.6 (0.3)tree HMM (random) 76.7 (0.1) 61.5 (0.2)tree HMM (Brown) 77.9 (0.1) 66.0 (0.2)Table 5: Results of semi-supervised supersensetagging: prediction accuracies with confidence in-tervals, obtained on 50 random splits of the data.has a significant impact on performance.5 ConclusionIn this paper, we considered an arguably naturalgenerative model of sentences for semantic classinduction.
It can be seen as a generalization ofBrown clustering, taking into account homonymyand syntax, and thus allowed us to study their im-pact on semantic class induction.
We developed anefficient algorithm to perform inference and learn-ing, which makes it possible to learn in this modelon large datasets, such as the New York Timescorpus.
We showed that this model induces rel-evant semantic classes and that it improves perfor-mance over Brown clustering on semi-supervisednamed entity recognition and supersense tagging.We plan to explore in future work better ways tomodel verbs, and in particular how to take into ac-count the type of dependencies between words.AcknowledgmentsFrancis Bach is supported in part by the EuropeanResearch Council (SIERRA ERC-239993).101ReferencesD.
M. Blei, A. Y. Ng, and M. I. Jordan.
2003.
Latentdirichlet alcation.
The Journal of Machine Learn-ing Research.J.
L. Boyd-Graber and D. Blei.
2009.
Syntactic topicmodels.
In Advances in Neural Information Pro-cessing Systems 21.P.
F. Brown, P. V. deSouza, R. L. Mercer, V. J.Della Pietra, and J. C. Lai.
1992.
Class-based n-gram models of natural language.
Computationallinguistics.O.
Cappe?
and E. Moulines.
2009.
On-lineexpectation?maximization algorithm for latent datamodels.
Journal of the Royal Statistical Society: Se-ries B (Statistical Methodology).M.
Ciaramita and Y. Altun.
2006.
Broad-coveragesense disambiguation and information extractionwith a supersense sequence tagger.
In Proceedingsof the 2006 Conference on Empirical Methods inNatural Language Processing.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In Proceedings of the tenth conference of Eu-ropean chapter of the Association for ComputationalLinguistics.M.
C. De Marneffe and C. D. Manning.
2008.
TheStanford typed dependencies representation.
In Col-ing 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation.M.
C. De Marneffe, B. MacCartney, and C. D. Man-ning.
2006.
Generating typed dependency parsesfrom phrase structure parses.
In Proceedings ofLREC.S.
Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-dauer, and R. Harshman.
1990.
Indexing by latentsemantic analysis.
Journal of the American societyfor information science.P.
S. Dhillon, D. Foster, and L. Ungar.
2011.
Multi-view learning of word embeddings via CCA.
Ad-vances in Neural Information Processing Systems.M.
Faruqui, S.
Pado?, and M. Sprachverarbeitung.2010.
Training and evaluating a German named en-tity recognizer with semantic generalization.
Se-mantic Approaches in Natural Language Process-ing.D.
Freitag.
2004.
Trained named entity recognitionusing distributional clusters.
In Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing.T.
L. Griffiths, M. Steyvers, D. M. Blei, and J. B.Tenenbaum.
2005.
Integrating topics and syn-tax.
Advances in Neural Information ProcessingSystems.G.
Haffari, M. Razavi, and A. Sarkar.
2011.
An en-semble model that combines syntactic and semanticclustering for discriminative dependency parsing.
InProceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics.T.
Hofmann.
1999.
Probabilistic latent semantic anal-ysis.
In Proceedings of the Fifteenth conference onUncertainty in artificial intelligence.R.
Kneser and H. Ney.
1993.
Improved clusteringtechniques for class-based statistical language mod-elling.
In Third European Conference on SpeechCommunication and Technology.T.
Koo, X. Carreras, and M. Collins.
2008.
Simplesemi-supervised dependency parsing.
In Proceed-ings of ACL-08: HLT.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
Proceedingsof the 18th International Conference on MachineLearning.W.
Li and A. McCallum.
2005.
Semi-supervised se-quence modeling with syntactic topic models.
InProceedings of the National Conference on ArtificialIntelligence.P.
Liang and D. Klein.
2009.
Online EM for unsuper-vised models.
In Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics.P.
Liang.
2005.
Semi-supervised learning for naturallanguage.
Master?s thesis, Massachusetts Instituteof Technology.K.
Lund and C. Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instru-ments, & Computers.S.
Miller, J. Guinness, and A. Zamanian.
2004.
Nametagging with word clusters and discriminative train-ing.
In Proceedings of HLT-NAACL.C.
Pal, C. Sutton, and A. McCallum.
2006.Sparse forward-backward using minimum diver-gence beams for fast training of conditional randomfields.
In ICASSP 2006 Proceedings.S.
Petrov.
2009.
Coarse-to-Fine Natural LanguageProcessing.
Ph.D. thesis, University of Californiaat Bekeley.E.
Sandhaus.
2008.
The New York Times annotatedcorpus.
Linguistic Data Consortium, Philadelphia.D.
O.
Se?aghdha.
2010.
Latent variable models of se-lectional preference.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics.102O.
Ta?ckstro?m, R. McDonald, and J. Uszkoreit.
2012.Cross-lingual word clusters for direct transfer of lin-guistic structure.
In Proceedings of the 2012 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics.S.
Tratz and E. Hovy.
2011.
A fast, accurate, non-projective, semantically-enriched parser.
In Pro-ceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing.J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word rep-resentations: a simple and general method for semi-supervised learning.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics.J.
Uszkoreit and T. Brants.
2008.
Distributed wordclustering for large scale class-based language mod-eling in machine translation.
Proceedings of ACL-08: HLT.M.
J. Wainwright and M. I. Jordan.
2008.
Graphicalmodels, exponential families, and variational infer-ence.
Foundations and Trends R?
in Machine Learn-ing.103
