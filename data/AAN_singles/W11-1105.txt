Proceedings of the TextGraphs-6 Workshop, pages 29?36,Portland, Oregon, USA, 19-24 June 2011. c?2011 Association for Computational LinguisticsUsing a Wikipedia-based Semantic Relatedness Measure for DocumentClusteringMajid YazdaniIdiap Research Institute and EPFLCentre du Parc, Rue Marconi 191920 Martigny, Switzerlandmajid.yazdani@idiap.chAndrei Popescu-BelisIdiap Research InstituteCentre du Parc, Rue Marconi 191920 Martigny, Switzerlandandrei.popescu-belis@idiap.chAbstractA graph-based distance between Wikipedia ar-ticles is defined using a random walk model,which estimates visiting probability (VP) be-tween articles using two types of links: hy-perlinks and lexical similarity relations.
TheVP to and from a set of articles is then com-puted, and approximations are proposed tomake tractable the computation of semanticrelatedness between every two texts in a largedata set.
The model is applied to documentclustering on the 20 Newsgroups data set.
Pre-cision and recall are improved in comparisonwith previous textual distance algorithms.1 IntroductionMany approaches have been proposed to computesimilarity between texts, from lexical overlap mea-sures to statistical topic models that are learned fromlarge corpora.
In this paper, we propose a method forusing knowledge from a structured, collaborative re-source ?
the Wikipedia hypertext encyclopedia ?
inorder to build a measure of semantic relatedness thatwe test on a text clustering task.The paper first describes the document graph de-rived from Wikipedia (Section 2), and then definesa network-based distance using visiting probability(Section 3), along with algorithms for its applica-tion to text clustering (Section 4).
Results over the20 Newsgroups dataset are shown to be competitive(Section 5), and the relative contributions of cosinelexical similarity and visiting probability are ana-lyzed.
Our proposal is discussed in the light of pre-vious work in Section 6.2 The Document NetworkIn the present proposal, knowledge about seman-tic relatedness is embodied into a document net-work, whose nodes are intended to represent con-cepts, while the links between nodes stand for var-ious relations between concepts.
The nodes of thenetwork correspond to articles from the Wikipediahypertext encyclopedia, and are derived as follows.The network was built from Wikipedia, using theWEX dataset (Metaweb Technologies, 2010).
Allarticles from the following categories were removed,as they do not correspond to proper concepts: Talk,File, Image, Template, Category, Portal, and List.Moreover, disambiguation pages and articles shorterthan 100 non-stopwords were filtered out as well.Out of 4,327,482 articles in WEX, 1,264,611 articleswere kept, forming the nodes of our network.The first type of links in our document networkare the hyperlinks between articles, because, in prin-ciple, each link between two articles indicates someform of relatedness between them.
There are morethan 35 million such links in our network.The second type of links is derived from the sim-ilarity of lexical content between articles.
This iscomputed using cosine similarity between the lexi-cal vectors corresponding to the articles?
texts, afterstopword removal and stemming.
Then, links arecreated by connecting every article to the 10 arti-cles that are most similar to it, each link receivinga weight which is the normalized lexical similarityscore.
The number 10 was chosen to ensure compu-tational tractability, and is in the same range as theaverage number of hyperlinks per node (30).29Computing semantic relatedness between twotexts requires: (1) to estimate relatedness betweentwo sets of nodes in the network, as described inSections 3 and 4; and (2) to project each text ontoa set of nodes, as we briefly explain here.
The pro-jection of a text onto the network is found by com-puting the text?s lexical similarity with all articles,again using cosine distance over stemmed words,without stopwords.
The text is mapped to the 10closest articles, resulting in a probability distributionover the 10 corresponding nodes.
Again, this valuewas chosen to be similar to the number of hyperlinksand content links per node, and to keep computationtractable.
In fact, the numerous Wikipedia articlesare scattered in the space of words, therefore tuningthese values does not seem to bring crucial changes.3 Computing Relatedness in the NetworkUsing Visiting Probability (VP)We have previously defined a random walk model(Yazdani and Popescu-Belis, 2010) to compute re-latedness of sets of nodes as the visiting probability(VP ) of a random walker from one set to anotherone, and we will review the model in this section.
Inthe next section, we will explain how the model wasextended for application to document clustering.3.1 NotationsLet S = {si|1 ?
i ?
n} be the set of n nodesin the graph.
Any two nodes si and sj can be con-nected by one or more directed and weighted links,which can be of L different types (L = 2 in ourcase: hyperlinks and lexical similarity links).
Linksbetween nodes can thus be represented by L matri-ces Al (1 ?
l ?
L) of size n ?
n, where Al(i, j)is the weight of the link of type l between si and sj .The transition matrix Cl gives the probability of adirect transition between nodes si and sj , using onlylinks of type l. This matrix can be built from the Almatrix as follows:Cl(i, j) =Al(i, j)?nk=1Al(i, k).In the random walk process using all link types(1 ?
l ?
L), let the weight wl denote the impor-tance of link type l. Then, the overall transition ma-trix C giving the transition probability Ci,j betweennodes si and sj is : C =?Ll=1wlCl.One of the main parameters in this computationis the relative weight of the two types of links (lex-ical similarity and hyperlinks) in the random walkover the network.
The settings for the experimentson document clustering (0.6 vs. 0.4) are explained inSection 5.1 below.3.2 VP from a Set of Nodes to a NodeLet us consider a probability distribution ~r overnodes, corresponding to the projection of a text frag-ment onto the network of articles (Section 2).
Givena new node sj in the network, our model first esti-mates the probability of visiting sj for the first timewhen a random walker starts from ~r in the graph.The model considers the state St of the randomwalker (its position node) and provides a procedurewhich, executed until termination, yields the valueof VP .
Namely, the initial state is chosen at randomwith probability P (S0 = si|~r) = ri (where the riare the components of ~r).
Then, from state St?1, ei-ther St?1 = sj and the procedure is finished, or thenext node is chosen using the transition matrix C.Moreover, it is also possible to ?fail?
the walk witha small probability, called ?absorption probability?,which makes longer paths less probable.3.3 Differences between VP and PageRank orHitting TimeThe VP of sj starting from the distribution ~r, ascomputed here, is different from the probability as-signed to sj after running Personalized PageRank(Haveliwala, 2003) with a teleport vector equal to ~r.In the computation of VP , the loops starting from sjand ending to the same sj do not have any effect onthe final score, unlike for PPR, for which such loopsboost the probability of sj .
If some pages have thistype of loops (typically, very ?popular?
pages), thenafter using PPR they will have high probability al-though they might not be very close to the teleportvector ~r.The VP of sj is also different from the hittingtime to sj , defined as the average number of stepsa random walker would take to visit sj for the firsttime in the graph starting from ~r.
Hitting time ismore sensitive to long paths in comparison to VP ,a fact that might introduce more noise.The perfor-mance of these three algorithms in computing se-30mantic similarity has been compared in (Yazdaniand Popescu-Belis, 2010).3.4 VP between Sets of NodesGeneralizing now to the computation of VP froma weighted set of nodes ~r1 (a probability distribu-tion) to another set ~r2, the model first constructs avirtual node representing ~r2 in the network, namedby convention sR, and then connects all nodes si tosR according to their weights in ~r2.
The transitionmatrix for the random walk is updated accordingly.To compute relatedness of two texts projectedonto the network as ~r1 and ~r2, the VP of ~r1 given~r2 is averaged with the converse probability, of ~r2given ~r1 ?
a larger probability indicating closer se-mantic relatedness.3.5 Truncated VPThe computation of VP can be done iteratively andcan be truncated after a number of steps, as the im-portance of longer paths grows smaller due to theabsorption probability, leading thus to a T -truncatedvisiting probability noted VPT .
Besides makingcomputation more tractable, truncation reduces theeffect of longer paths, which seem to be less reliableindicators of relatedness.We have computed an upper bound on the trun-cation error, which helps to control and minimizethe number of steps actually computed in a randomwalk.
To compute the upper bound of the truncationerror we compute the probability of returning neithersuccess (reaching sj) nor failure (absorption) in firstt steps, which can be computed as?ni 6=j ?t(~rC ?t)i.This is in fact the probability mass at time t at allnodes except sj , the targeted node.
C ?
is the transi-tion matrix that gives the probability of a transitionbetween two nodes, modified to include the virtualnode sR in the network, and 1?
?
is the absorptionprobability.If pt(success) denotes the probability of success(reaching sj) considering paths of length at most t,and ?t the error made by truncating after step t, thenwe have:?t = p(success)?
pt(success) ?
?ni 6=j ?t(~rC ?t)iSo, if pt(success) is used as an approximation forp(success) then an upper bound for this approxima-tion error ?t is the right term of the above inequality.4 Application of VP to Text ClusteringIn this section, we describe the additional modelingthat was done so that semantic relatedness based onVP could be applied efficiently to text clustering.Indeed, it is not tractable to individually computethe average VP between any two texts in the set ofdocuments to be clustered, because the numbers ofpairs is very large ?
e.g., 20,000 documents in theexperiments in Section 5.
Instead, we propose twosolutions for computing, respectively, VP to a set ofnodes (from all documents in the network), and re-spectively VP from a set of nodes to all documents.4.1 Computing VP from All Nodes to a SubsetTo compute the T -truncated visiting probability(noted VPT ) from all nodes in the network to a nodesR at the same time, the following recursive pro-cedure is defined.
Here, T is the number of stepsbefore truncation, and sR is a virtual node repre-senting a probability distribution ~r from a text.
Theprocedure is based on the definition of VP betweennodes in Section 3 and uses the transition matrix C ?that gives the probability of a transition between twonodes, modified to include the virtual node sR in thenetwork.
If 1?
?
is the absorption probability, thenthe recursive definition of VPT from a node si tothe virtual node sR is:VPT (si, sR) = ?
?k C?
(si, sk)VPT?1(sk, sR)Using dynamic programming, it is possible tocompute VPT from all nodes to sR inO(ET ) steps,where E is the number of links in the network.The initialization of the procedure is done usingVPT (sR, sR) = 1 and VP0(si, sR) = 0 for anyi 6= R.4.2 Computing VP from a Subset to All NodesTo compute the truncated VP from ~r to all nodesin the network, the total computation time usingthe definition of VPT from Section 3 is O(ETN ),where N is the number of nodes in the network, be-cause VPT must be computed for each node sepa-rately.
For a large data set, this is not tractable.The proposed solution is based on a samplingmethod over the random walks to approximateVPT .
The sampling involves running M indepen-dent random walks of length T from ~r.
For a given31node sj and a sample walk m, the first time (if any)when sj is visited on each random walk startingfrom ~r is noted tjm .
Then, VPT can be estimatedby the following average over sample walks, where1?
?
is again the absorption probability:?VPT (~r, sj) = (?m ?tjm )/M.As a result, the estimate of VPT can be computedin O(MT ) steps, where M is the number of samplepaths.Moreover, it is possible to compute a bound on theerror of the estimation, |VPT?
?VPT |, depending onthe number of sample paths M .
It can be shown thatthe error is lower than ?, with a probability largerthan 1 ?
?, on condition that the number of samplepaths is greater than ?2 ln(2/?
)/2?2.To prove this bound, we use inspiration from aproof by Sarkar et al (2008).
If the estimation of avariableX is noted X?
, let us suppose that concept sjhas been visited for the first time at {tj1 , ?
?
?
, tjm}time steps in the M sample walks.
We define therandom variable X l by ?tjl/M , where tjl indicatesthe time step at which sj was visited for the firsttime in lth sampling.
If sj was not visited at all,then X l = 0 by convention.
The l random variablesX l (j1 ?
l ?
jm) are independent and bounded by0 and 1 (0 ?
X l ?
1).
We have:?VPT (~r, sj) =?lXl = (?l ?tjl )/M andE( ?VPT (~r, sj)) = VPT (~r, sj).So, by applying Hoeffding?s inequality, we have:P (| ?VPT ?
E( ?VPT )| ?
) ?
2exp(?2M2?2 ).If the probability of error must be at most ?, thensetting the right side lower than ?
gives the boundfor M that is stated in our theorem.As a consequence, we have the following lowerbound forM if we want ?-approximation for all pos-sible sj with probability at least 1?
?.
We use unionbound and Hoeffding?s inequality:P (?j ?
{1, .
.
.
, n}, | ?VPT ?
E( ?VPT )| ?
) ?
2n?exp(?2M2?2 )which gives the lower bound M ?
?2 ln(2n/?
)22 .5 Document ClusteringThis section describes the experimental setting andthe results of applying the text relatedness measuredefined above to the problem of document cluster-ing over the 20 Newsgroups dataset.1 The datasetcontains about 20,000 postings to 20 news groups,hence 20 document classes, with about 1,000 docu-ments per class.
We aim here at finding these classesautomatically, using for testing the entire data setwithout using any part of it as a training set.
Theknowledge of our system comes entirely from thedocument network and the techniques for comput-ing distances between two texts projected onto it.5.1 Setup of the ExperimentWe first compute a similarity matrix for the entire 20Newsgroups data set, with the relatedness score be-tween any two documents being VPT .
For tractabil-ity, we fixed T = 5 that gives sufficient precision; alarger value only increased computation time.
In-stead of computing VPT between all possible pairsseparately, we fill one row of the matrix at a timeusing the approximations above.We set the absorption probability of the randomwalk 1 ?
?
= 0.2 for this experiment.
Given ?and T by using the formula in section 3.5, it is pos-sible to compute the error bound of the truncation,and noting that for a smaller ?, fewer steps (T ) areneeded to achieve the same approximation precisionbecause of the penalty set to longer paths.
Con-versely, a larger ?
decreases the penalty for longerpaths and requires more computation.2For comparison purposes, four similarity matri-ces were computed.
Indeed, the theoretical appara-tus described above can be applied to various typesof links in the document network.
In Section 2, weintroduced two types of links, namely lexical simi-larity and actual hyperlinks, and these can be usedseparately in the model, or as a weighted combina-tion.
The following similarities will be compared:1.
VP over hyperlinks only (noted VPHyp);2.
VP over lexical similarity links (VPLex);1Distributed at http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html, see also (Mitchell, 1997, Chapter 6).2Note that in the extreme case when ?
= 0, similarity to allnodes except the node itself is zero.323.
VP over a combination of hyperlinks (0.4) andlexical links (0.6) (noted VPComb) ?
these val-ues gave the best results in our previous appli-cations to word and document similarity tasks(Yazdani and Popescu-Belis, 2010);4. no random walk, only cosine similarity be-tween the tf-idf vectors of the documents to beclustered (noted LS , for lexical similarity).5.2 Clustering PerformanceClustering is performed using a k-means algorithmover each of the four similarity matrices.3 The qual-ity of the clustering is first measured using the RandIndex (RI), which counts the proportion of pairs ofdocuments that are similarly grouped, i.e.
either inthe same, or in different clusters, in the referencevs.
candidate clusterings.
Other methods exist (Pan-tel and Lin, 2002), including a Rand Index adjustedfor chance (Vinh et al, 2009), but the RI sufficesfor comparative judgments in this subsection.
How-ever, in Subsection 5.3, we will also look at preci-sion and recall, and in Subsection 5.4 we will usepurity.
As the clustering is performed over the entiredata set, because there is no training vs. test data,confidence intervals are not available, though theycould be computed by splitting the data.
As a result,comparison with other scores on the same test set isabsolute.The scores in terms of Rand Index are, in decreas-ing order:1.
90.8% for VPComb2.
90.6% for VPHyp3.
90.4% for VPLex4.
and only 86.1% for the LS cosine similarity.The random walk model thus clearly outperformsthe baseline LS approach.
If counting only wronglyclustered document pairs, VPComb has 6.6% of suchpairs, while VPLex has 8.4%, confirming the lowerperformance of the model using only lexical similar-ity links, i.e.
the utility of hyperlinks.3The semantic relatedness measure proposed here could beused with other clustering algorithms, such as the committee-based method proposed by Pantel and Lin (2002).5.3 Comparison to Other MethodsTo obtain a better understanding of the performanceof the proposed method, we computed the clusteringprecision and recall of several well-known methodsfor statistical text representation, shown in Table 1.For Latent Dirichlet Allocation (LDA) (Blei et al,2003) and Latent Semantic Analysis (LSA) (Deer-wester et al, 1990), we first mapped the documentsin the latent space and then computed the cosinesimilarity between the documents in the latent space.The number of topics for LSA and LDA is set to 100to make the computation tractable.
Precision and re-call are used, rather than the Rand Index, to show inmore detail the performance of each method.
Theuse of VP over our document network clearly in-creases both precision and recall in comparison toother tested approaches.Similarity method Precision RecallLS 7.50 18.38LSA 8.63 9.99LDA 19.93 31.50VPComb 23.81 35.32Table 1: Precision and Recall for k-means clustering overthe 20 Newsgroups using several well-known methods tocompute text similarity, in comparison to the present pro-posal.5.4 Analysis of the Impact of VP with Respectto Cosine SimilarityTo find out in which cases the proposed method im-proves over a simple cosine similarity measure, weconsidered a linear combination of the cosine simi-larity and VP , noted w?VPComb+(1?w)?LS ,and varied the weight w from 0 to 1.
Consideringthe k-nearest neighbors of every document accord-ing to this combined similarity, we define k-purityas the number of documents with the correct labelover the total number of documents k in the com-puted neighborhood.
The variation of k-purity withw, for several values of k, is shown in Figure 1.The best purity appears to be obtained for a com-bination of the two methods, for all values of k thatwere tested.
This shows that VPComb brings valu-able additional information about document relat-edness that cannot be found in LS only.
Further-33Figure 1: Values of k-purity (vertical axis) averaged over all documents, for neighborhoods of different sizes k. Thehorizontal axis indicates the weightw of visiting probability vs. cosine lexical similarity in the formula: w?VPComb+(1?
w)?
LS .more, when the size of the examined neighborhoodk increases (lower curves in Figure 1), the effect ofVPComb becomes more important, i.e.
its weight inthe optimal combination increases.
For very smallneighborhoods, LS is almost sufficient to ensure op-timal purity, but for larger ones (k = 10 or 15),VPComb used alone (w = 1) outperforms LS usedalone (w = 0).
Their optimal combination leads toscores that are higher than those obtained for eachof them used separately, and, as noted, the weight ofVPComb in the optimal combination increases forlarger neighborhoods.These results can be explained as follows.
Forvery small neighborhoods, the cosine lexical simi-larity score with the nearest 1?5 documents is veryhigh, as they have many words in common, so LS isa good measure of text relatedness.
However, whenlooking at larger neighborhoods, for which related-ness is less based on identical words, then VPCombbecomes more effective, and LS performs poorly.Therefore, we can predict that VPComb will be mostrelevant when looking for larger neighborhoods, orin order to increase recall.
VPComb should also berelevant when there is low diversity among docu-ment words, for instance when all documents arevery short.6 Related WorkMany attempts have been made to improve theoverlap-based lexical similarity distance, for variousapplications to HLT.
One approach is to constructa taxonomy of concepts and relations (manually orautomatically) and to map the text fragments to becompared onto the taxonomy.
For instance, Word-net (Fellbaum, 1998) and Cyc (Lenat, 1995) are twowell-known knowledge bases that can be used forenriching pure lexical matching.
However, buildingand maintaining such resources requires consider-able effort, and they might cover only a fraction ofthe vocabulary of a language, as they usually includefew proper names or technical terminology.Another approach makes use of unsupervisedmethods to construct a semantic representation of34documents by analyzing mainly co-occurrence rela-tionships between words in a corpus.
Latent Seman-tic Analysis (Deerwester et al, 1990), ProbabilisticLSA (Hofmann, 1999) and Latent Dirichlet Alloca-tion (Blei et al, 2003) are unsupervised methods thatconstruct a low-dimensional feature representationor concept space, in which words are no longer sup-posed to be independent.Mihalcea et al (2006) compared knowledge-based and corpus-based methods, using word sim-ilarity and word specificity to define one generalmeasure of text semantic similarity.
Because it com-putes word similarity values between all word pairs,the proposed method appears to be suitable mainlyto compute similarity between short fragments, oth-erwise the computation becomes intractable.WikiRelate!
(Strube and Ponzetto, 2006) com-putes semantic relatedness between two words byusing Wikipedia.
Each word is mapped to the corre-sponding Wikipedia article by using article titles.
Tocompute relatedness, several methods are proposed,namely, using paths in the Wikipedia category struc-ture or the articles?
content.
Our method, by compar-ison, also uses the knowledge embedded in the hy-perlinks between articles, as well as the entire con-tents of articles, but unlike WikiRelate!
it has beenextended to texts of arbitrary lengths.Explicit Semantic Analysis (Gabrilovich andMarkovitch, 2007), instead of mapping a text to anode or a small group of nodes in a taxonomy, mapsthe text to the entire collection of available con-cepts, by computing the degree of affinity of eachconcept to the input text.
Similarity is measuredin the new concept space.
ESA does not use thelink structure or other structured knowledge fromWikipedia.
Moreover, by walking over a contentsimilarity graph, our method benefits from a non-linear distance measure according to the paths con-sisting of small neighborhoods.In the work of Yeh et al (2009), a graph of docu-ments and hyperlinks is computed from Wikipedia,then a Personalized PageRank (Haveliwala, 2003) iscomputed for each text fragment, with the teleportvector being the one resulting from the ESA algo-rithm cited above.
To compute semantic relatednessbetween two texts, Yeh et al (2009) simply comparetheir personalized page rank vectors.
By compari-son, in our method, we also consider in addition tohyperlinks the effect of word co-occurrence betweenarticle contents.
The use of visiting probability alsogives different results over personalized page rank,as it measures different properties of the network.There are many studies on measuring distancesbetween vertices in a graph.
Two measures that areclose to the visiting probability proposed here arehitting time and Personalized PageRank mentionedin Section 3.3.
Hitting time has been used in var-ious studies as a distance measure in graphs, e.g.for dimensionality reduction (Saerens et al, 2004)or for collaborative filtering in a recommender sys-tem (Brand, 2005).
Hitting time was also used forlink prediction in social networks along with otherdistances (Liben-Nowell and Kleinberg, 2003), orfor semantic query suggestion using a query/URLbipartite graph (Mei et al, 2008).
As for Personal-ized PageRank, it was used for word sense disam-biguation (Agirre and Soroa, 2009), and for measur-ing lexical relatedness of words in a graph built fromWordNet (Hughes and Ramage, 2007).7 ConclusionWe proposed a model for measuring text seman-tic relatedness based on knowledge embodied inWikipedia, seen here as document network with twotypes of links ?
hyperlinks and lexical similarityones.
We have used visiting probability to mea-sure proximity between weighted sets of nodes, andhave proposed approximation algorithms to makecomputation efficient for large graphs (more thanone million nodes and 40 million links) and largetext clustering datasets (20,000 documents in 20Newsgroups).
Results on the document clusteringtask showed an improvement using both word co-occurrence information and user-defined hyperlinksbetween articles over other methods for text repre-sentation.AcknowledgmentsThe work presented in this paper has been supportedby the IM2 NCCR (Interactive Multimodal Infor-mation Management) of the Swiss National ScienceFoundation (http://www.im2.ch).35ReferencesEneko Agirre and Aitor Soroa.
2009.
Personalizingpagerank for word sense disambiguation.
In Proceed-ings of EACL 2009 (12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics), pages 33?41, Athens, Greece.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Matthew Brand.
2005.
A random walks perspective onmaximizing satisfaction and profit.
In Proceedings ofthe 2005 SIAM International Conference on Data Min-ing, Newport Beach, CA.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by Latent Semantic Analysis.
Journal of theAmerican Society for Information Science, 41(6):391?407.Christiane Fellbaum, editor.
1998.
WordNet: An elec-tronic lexical database.
MIT Press, Cambridge, MA.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting semantic relatedness using Wikipedia-basedexplicit semantic analysis.
In Proceedings of IJCAI2007 (20th International Joint Conference on Artifi-cial Intelligence), pages 6?12, Hyderabad.Taher H. Haveliwala.
2003.
Topic-sensitive pagerank:A context-sensitive ranking algorithm for web search.IEEE Transactions on Knowledge and Data Engineer-ing, 15:784?796.Thomas Hofmann.
1999.
Probabilistic Latent SemanticIndexing.
In Proceedings of SIGIR 1999 (22nd ACMSIGIR Conference on Research and Development inInformation Retrieval), pages 50?57, Berkeley, CA.Thad Hughes and Daniel Ramage.
2007.
Lexical se-mantic relatedness with random graph walks.
InProceedings of EMNLP-CoNLL 2007 (Conference onEmpirical Methods in Natural Language Processingand Conference on Computational Natural LanguageLearning), pages 581?589, Prague.Douglas B. Lenat.
1995.
CYC: A large-scale investmentin knowledge infrastructure.
Communications of theACM, 38(11):33?38.David Liben-Nowell and Jon Kleinberg.
2003.
The linkprediction problem for social networks.
In Proceed-ings of CIKM 2003 (12th ACM International Confer-ence on Information and Knowledge Management),pages 556?559, New Orleans, LA.Qiaozhu Mei, Dengyong Zhou, and Kenneth Church.2008.
Query suggestion using hitting time.
In Pro-ceeding of CIKM 2008 (17th ACM International Con-ference on Information and Knowledge Management),pages 469?478, Napa Valley, CA.Metaweb Technologies.
2010.
Freebase Wikipedia Ex-traction (WEX).
http://download.freebase.com/wex/.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and knowledge-based measuresof text semantic similarity.
In Proceedings of AAAI2006 (21st National Conference on Artificial Intelli-gence), pages 775?782, Boston, MA.Tom M. Mitchell.
1997.
Machine Learning.
McGraw-Hill, New York.Patrick Pantel and Dekang Lin.
2002.
Document cluster-ing with committees.
In Proceedings of SIGIR 2002(25th Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval),pages 199?206, Tampere.Marco Saerens, Franois Fouss, Luh Yen, and PierreDupont.
2004.
The principal components analysis ofa graph, and its relationships to spectral clustering.
InProceedings of ECML 2004 (15th European Confer-ence on Machine Learning), pages 371?383, Pisa.Purnamrita Sarkar, Andrew W. Moore, and Amit Prakash.2008.
Fast incremental proximity search in largegraphs.
In Proceedings of ICML 2008 (25th Interna-tional Conference on Machine Learning), pages 896?903, Helsinki.Michael Strube and Simone Paolo Ponzetto.
2006.Wikirelate!
Computing semantic relatedness usingWikipedia.
In Proceedings of AAAI 2006 (21st Na-tional Conference on Artificial Intelligence), pages1419?1424.Nguyen Xuan Vinh, Julien Epps, and James Bailey.2009.
Information theoretic measures for clusteringscomparison: Is a correction for chance necessary?
InProceedings of ICML 2009 (26th International Con-ference on Machine Learning), Montreal.Majid Yazdani and Andrei Popescu-Belis.
2010.
A ran-dom walk framework to compute textual semantic sim-ilarity: a unified model for three benchmark tasks.In Proceedings of IEEE ICSC 2010 (4th IEEE Inter-national Conference on Semantic Computing), Pitts-burgh, PA.Eric Yeh, Daniel Ramage, Christopher D. Manning,Eneko Agirre, and Aitor Soroa.
2009.
WikiWalk:random walks on Wikipedia for semantic relatedness.In Proceedings of TextGraphs-4 (4th Workshop onGraph-based Methods for Natural Language Process-ing), pages 41?49, Singapore.36
