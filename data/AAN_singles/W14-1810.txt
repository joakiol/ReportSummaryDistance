Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 79?88,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsAn Explicit Feedback System for Preposition Errorsbased on Wikipedia RevisionsNitin Madnani and Aoife CahillEducational Testing Service660 Rosedale RoadPrinceton, NJ 08541, USA{nmadnani,acahill}@ets.orgAbstractThis paper presents a proof-of-concepttool for providing automated explicit feed-back to language learners based on datamined from Wikipedia revisions.
The tooltakes a sentence with a grammatical er-ror as input and displays a ranked list ofcorrections for that error along with evi-dence to support each correction choice.We use lexical and part-of-speech con-texts, as well as query expansion with athesaurus to automatically match the er-ror with evidence from the Wikipedia revi-sions.
We demonstrate that the tool workswell for the task of preposition selectionerrors, evaluating against a publicly avail-able corpus.1 IntroductionA core feature of learning to write is receivingfeedback and making revisions based on that feed-back (Biber et al., 2011; Lipnevich and Smith,2008; Truscott, 2007; Rock, 2007).
In the field ofsecond language acquisition, the main focus hasbeen on explicit or direct feedback vs. implicitor indirect feedback.
In writing, explicit or directfeedback involves a clear indication of the locationof an error as well as the correction itself, or, morerecently, a meta-linguistic explanation (of the un-derlying grammatical rule).
Implicit or indirectwritten feedback indicates that an error has beenmade at a location, but it does not provide a cor-rection.The work in this paper describes a novel toolfor presenting language learners with explicitfeedback based on human-authored revisions inWikipedia.
Here we describe the proof-of-concepttool that provides explicit feedback on one specificcategory of grammatical errors, preposition selec-tion.
We restrict the scope of the tool in order tobe able to carry out a focused study, but expectthat our findings presented here will also general-ize to other error types.
The task of preposition se-lection errors has been well studied (Tetreault andChodorow, 2008; De Felice and Pulman, 2009;Tetreault et al., 2010; Rozovskaya and Roth, 2010;Dahlmeier and Ng, 2011; Seo et al., 2012; Cahillet al., 2013), and the availability of public, anno-tated corpora containing such errors provides easyaccess to evaluation data.Our tool takes a sentence with a grammaticalerror as input, and returns a ranked list of possi-ble corrections.
The tool makes use of frequencyof correction in edits to Wikipedia articles (asrecorded in the Wikipedia revision history) to cal-culate the rank order.
In addition to the ranked listof suggestions, the tool also provides evidence foreach correction based on the actual changes madebetween different versions of Wikipedia articles.The tool uses the notion of ?context similarity?
todetermine whether a particular edit to a Wikipediaarticle can provide evidence of a correction in agiven context.Specifically, this paper makes the followingcontributions:1.
We build a tool to provide explicit feedbackfor preposition selection errors in the form ofranked lists of suggested corrections.2.
We use evidence from human-authored cor-rections for each suggested correction on alist.3.
We conduct a detailed examination of howthe performance of the tool is affected byvarying the type and size of contextual infor-mation and by the use of query expansion.The remainder of this paper is organized as fol-lows: ?2 describes related work and ?3 outlinespotential approaches for using Wikipedia revisiondata in a feedback tool.
?4 outlines the core system79for generating feedback and ?5 presents an empir-ical evaluation of this system.
In ?6 we describe amethod for enhancing the system using query ex-pansions.
We discuss our findings and some futurework in ?7 and, finally, conclude in ?8.2 Related WorkAttali (2004) examines the general effect of feed-back in the Criterion system (Burstein et al., 2003)and finds that students presented with feedback areable to improve the overall quality of their writ-ing, as measured by an automated scoring system.This study does not investigate different kinds offeedback, but rather looks at the issue of whetherfeedback in general is useful for students.
Shermiset al.
(2004) look at groups of students who usedCriterion and students who did not and comparetheir writing performance as measured by high-stakes state assessment.
They found that, in gen-eral, the students who made use of Criterion andits feedback improved their writing skills.
Theyanalyze the distributions of the individual gram-mar and style error types and found that Criterionhelped reduce the number of repeated errors, par-ticularly for mechanics (e.g.
spelling and punctu-ation errors).
Chodorow et al.
(2010) describe asmall study in which Criterion provided feedbackabout article errors to students writing an essay fora college-level course.
They find, similarly to At-tali (2004), that the number of article errors wasreduced in the final revised version of the essay.Gamon et al.
(2009) describe ESL Assistant ?a web-based proofreading tool designed for lan-guage learners who are native speakers of East-Asian languages.
They used a decision-tree ap-proach to detect and offer suggestions for poten-tial article and preposition errors.
They also al-lowed the user to compare the various suggestionsby showing results of corresponding web searches.Chodorow et al.
(2010) also describe a small studywhere ESL Assistant was used to offer sugges-tions for potential grammatical errors to web userswhile they were composing email messages.
Theyreported that users were able to make effective useof the explicit feedback for that task.
The tool hadbeen offered as a web service but has since beendiscontinued.Our tool is similar to ESL Assistant in that bothproduce a list of possible corrections.
The maindifference between the tools is that ours automat-ically derives the ranked list of correction sugges-tions from a very large corpus of annotated errors,rather than performing a web search on all pos-sible alternatives in the context.
The advantageof using an error-annotated corpus is that it con-tains implicit information about frequent confu-sion pairs (e.g.
?at?
instead of ?in?)
that are in-dependent of the frequency of the preposition andthe current context.Milton and Cheng (2010) describe a toolkit forhelping Chinese learners of English become moreindependent writers.
The toolkit gives the learnersaccess to online resources including web searches,online concordance tools, and dictionaries.
Usersare provided with snapshots of the word or struc-ture in context.
In Milton (2006), 500 revisionsto 323 journal entries were made using an earlierversion of this tool.
Around 70 of these revisionshad misinterpreted the evidence presented or werecareless mistakes; the remaining revisions resultedin more natural sounding sentences.3 Wikipedia RevisionsOur goal is to build a tool that can provide explicitfeedback about errors to writers.
We take advan-tage of the recently released Wikipedia preposi-tion error corpus (Cahill et al., 2013) and designour tool based on this large corpus containing sen-tences annotated for preposition errors and theircorrections.
The corpus was produced automati-cally by mining a total of 288 million revisions for8.8 million articles present in a Wikipedia XMLsnapshot from 2011.
The Wikipedia error corpus,as we refer to in the rest of the paper, contains2 million sentences annotated with preposition er-rors and their respective corrections.There are two possible approaches to buildingan explicit feedback tool for preposition errorsbased on this corpus:1.
Classifier-based.
We could train a classi-fier on the Wikipedia error corpus to predictthe correct preposition in a given context, asCahill et al.
(2013) did.
Although this wouldallow us to suggest corrections for contextsthat are unseen in the Wikipedia data, thesuggestions would likely be quite noisy giventhe inherent difficulty of a classification prob-lem with a large number of classes.1In addi-tion, this approach would not facilitate pro-1Cahill et al.
(2013) used a list of 36 prepositions asclasses.80viding evidence for each correction to theuser.2.
Corpus-based.
We could use the Wikipediaerror corpus directly for feedback.
Al-though this means that suggestions can onlybe generated for contexts occurring in theWikipedia data, it also means that all sug-gestion would be grounded in actual revisionsmade by other humans on Wikipedia.We believe that anchoring suggestions tohuman-authored corrections affords greater util-ity to a language learner, in line with the currentpractice in lexicography that emphasizes authen-tic usage examples (Collins COBUILD learner?sdictionary, Sketch Engine (Kilgarriff et al., 2004)).Therefore, in this paper, we choose the second ap-proach to build our tool.4 MethodologyIn order to use the Wikipedia error corpus directlyfor feedback, we first index the sentences in thecorpus using the following fields:?
The incorrect preposition.?
The correct preposition.?
The words, bigrams, and trigrams before (andafter) the preposition error (indexed sepa-rately).?
The part-of-speech tags, tag bigrams, and tagtrigrams before (and after) the error (indexedseparately).?
The title and URL of the Wikipedia article inwhich the sentence occurred.?
The ID of the article revision containing thepreposition error.?
The ID of the article revision in which thecorrection was made.Once the index is constructed, eliciting explicitfeedback is straightforward.
The input to the sys-tem is a tokenized sentence with a marked uppreposition error (e.g.
from an automated prepo-sition error detection system).
For each input sen-tence, the Wikipedia index is then searched withthe identified preposition error and the words (orn-grams) present in its context.
The index returnsa list of the possible corrections occurring in thegiven context.
The tool then counts how ofteneach possible preposition is returned as a possiblecorrection and orders its suggestions from mostfrequent to least frequent.
In addition, the tool alsodisplays five randomly chosen sentences from theindex as evidence for each correction in order tohelp the learner make a better choice.
The toolcan use either the lexical n-grams (n=1,2,3) or thepart-of-speech n-grams (n=1,2,3) around the errorfor the contextualized search of the Wikipedia in-dex.Figure 1 shows a screenshot of the tool in oper-ation.
The input sentence is entered into the textbox at the top, with the preposition error enclosedin asterisks.
In this case, the tool is using parts-of-speech on either side of the error for context.
Bydefault, the tool shows the top five possible correc-tions as a bar chart, sorted according to how manytimes the erroneous preposition was changed tothe correction in the Wikipedia revision index.
Inthis example, the preposition of with the left con-text of <DT, NNS> and the right context of <DT,NN> was changed to the preposition in 242 timesin the Wikipedia revisions.
When the user clickson a bar, the box on the top shows the sentencewith the change and the gray box on the rightshows 5 (randomly chosen) actual sentences fromWikipedia where the change represented by thebar was made.If parts-of-speech are chosen as context, the tooluses WebSockets to send the sentence to the Stan-ford Tagger (Toutanova et al., 2003) in the back-ground and compute its part-of-speech tags beforesearching the index.5 EvaluationIn order to determine how well the tool performsat suggesting corrections, we used sentences con-taining preposition errors from the CLC FCEdataset.
The CLC FCE Dataset is a collection of1,244 exam scripts written by learners of Englishas part of the Cambridge ESOL First Certificate inEnglish (Yannakoudakis et al., 2011).
Our evalua-tion set consists of 3,134 sentences, each contain-ing a single preposition error.We evaluate the tool on two criteria:?
Coverage.
We define coverage as the pro-portion of errors for which the tool is able tosuggest any corrections.?
Accuracy.
The obvious definition of accu-81Figure1:Ascreenshotofthetoolsuggestingthetop5correctionsforasentenceusingtwoparts-of-speechoneithersideofthemarkederrorascontext.Thecorrectionsaredisplayedinrankedfashionasahistogramandclickingononedisplaysthe?corrected?sentenceaboveandthecorrespondingevidencefromWikipediarevisionsontheleft.82Context Found Missed Blank MRRwords1 889 (28.4%) 356 (11.4%) 1889 (60.3%) .522words2 55 ( 1.8%) 22 ( 0.7%) 3057 (97.5%) .619words3 16 ( 0.5%) 5 ( 0.2%) 3113 (99.3%) .762tags1 2821 (90.0%) 241 ( 7.7%) 72 ( 2.3%) .419tags2 1896 (60.5%) 718 (22.9%) 520 (16.6%) .390tags3 661 (21.1%) 633 (20.2%) 1840 (58.7%) .325Table 1: A detailed breakdown of the Found, Missing and Blank classes along with the Mean ReciprocalRank (MRR) values, for different types (words, tags) and sizes (1, 2, or 3 around the error) ofcontextual information used in the search.racy would be the proportion of errors forwhich the tool?s best suggestion is the cor-rect one.
However, since the tool returnsa ranked list of suggestions, it is importantto award partial credit for errors where thetool made a correct suggestion but it was notranked at the top.
Therefore, we use the MeanReciprocal Rank (MRR), a standard metricused for evaluating ranked retrieval systems(Voorhees, 1999).
MRR is computed as fol-lows:MRR =1|S||S|?i=11Riwhere S denotes the set of sentences forwhich ranked lists of suggestions are gener-ated and Ridenotes the rank of the true cor-rection in the list of suggestions the tool re-turns for sentence i.
A higher MRR is bettersince that means that the tool ranked the truecorrection closer to the top of the list.To conduct the evaluation on the FCE dataset,we run each of the sentences through the tool andextract the top 5 suggestions for each error anno-tated in the sentence.2At this point, each errorinstance input to the tool can be classified as oneof three classes:1.
Found.
The true correction for the error wasfound in the ranked list of suggestions madeby the tool.2.
Missing.
The true correction for the errorwas not found in the ranked list of sugges-tions.3.
Blank.
The tool did not return any sugges-tions for the error.2In this paper, we separate the tasks of error detection andcorrection and use the gold standard as an oracle to detect er-rors and then use our system to propose and rank corrections.First, we examine the distribution of the threeclasses across the types and sizes of the contextualinformation used to conduct the search.
Table 1shows, for each context type and size, a detailedbreakdown of the distribution of the three classesalong with the mean reciprocal rank (MRR) val-ues.3We observe that, with words as contexts, us-ing larger contexts certainly produces more accu-rate results (as indicated by the larger MRR val-ues).
However, we also observe that employinglarger contexts reduces coverage (as indicated bythe decreasing percentage of Found sentences andby the the increasing percentage of the Blank sen-tences).With part-of-speech tags, we observe that al-though using larger tag contexts can find correc-tions for a significantly larger number of sentencesas compared to similar-sized word contexts (as in-dicated by the larger percentages of Found sen-tences), doing so yields overall worse MRR val-ues.
This is primarily due to the fact that withlarger part-of-speech contexts the system producesmore suggestions that never contain the true cor-rection, i.e., an increasing percentage of Missedsentences.
The most likely reason is that signifi-cantly reducing the vocabulary size by using part-of-speech tags introduces a lot of noise.Figure 2 shows the distribution of the rank Rof the true correction in the list of suggestions.4The figure uses a rank of 10+ to denote all ranksgreater than 10 to conserve space.
We observesimilar trends in the figure as in Table 1 ?
us-ing larger word contexts yield higher accuraciesbut significantly lower coverage and using larger3We do not include Blank sentences when computing theMRR values.4Note that in this figure, the bar for R = 0 includes bothsentences where no ranked list was produced (Blank) andthose where the true correction was not produced as a sug-gestion at all (Missing).83words1words2words3tags1tags2tags30100020003000 010002000300001234567891010+01234567891010+01234567891010+Rankof truecorrection(R)Number of FCE sentencesClassBlankMissingFoundFigure2:ThedistributionoftherankthatthetruecorrectionhasinthelistofsuggestionsfortheFCEsentences,acrosseachcontexttypeandsizeused.84tag contexts yield lower accuracies and lower cov-erage, even though the coverage is significantlylarger than that of the correspondingly sized wordcontext.6 Query ExpansionThe results in the previous section indicate that al-though we could use part-of-speech tags as con-texts to improve the coverage of the tool (as indi-cated by the number of Found sentences), doingso leads to a significant reduction in accuracy, asindicated by the lower MRR values.In the field of information retrieval, a commonpractice is to expand the query with words similarto words in the query in order to increase the like-lihood of finding documents relevant to the query(Sp?arck-Jones and Tait, 1984).
In this section, weexamine whether we can use a similar techniqueto improve the coverage of the tool.We employ a simple query expansion techniquefor the cases where no results would otherwise bereturned by the tool.
For these cases, we first ob-tain a list of K words similar to the two wordsaround the error from a distributional thesaurus(Lin, 1998), ranked by similarity.
We then gener-ate a list of additional queries by combining thesetwo ranked lists of similar words.
We then runeach query in the list against the Wikipedia indexuntil one of them yields results.
Note that sincewe are using a word-based thesaurus, this expan-sion technique can only increase coverage whenapplied to the words1 condition, i.e., single wordcontexts.
We investigate K = 1, 2, 5, or 10 expan-sions for each of the context words.Table 2 shows the a detailed breakdown of thedistribution of the three classes and the MRR val-ues with query expansion integrated into the toolfor sentences where it would generally produce nooutput.
Each row corresponds to a different valueof K ?
the number of expansions used per contextword ?
is varied.
Note that K = 0 corresponds tothe condition where query expansion is not used.From the table, we observe that using query ex-pansion indeed seems to increase the coverage ofthe tool as indicated by the increasing percentageof Found sentences and decreasing percentage ofBlank sentences.
However, we also find that usingquery expansion yields worse MRR values, againbecause of the increasing percentage of Missedsentences.
This represents a traditional trade-offscenario where accuracy can be traded off for anincrease in coverage, depending on the desired op-erating characteristics.7 Discussion and Future WorkThere are several issues that merit further discus-sion and possibly provide future extensions to thework described in this paper.?
Need for an extrinsic evaluation.
Althoughour intrinsic evaluation clearly shows that thetool has reasonably good coverage as wellas accuracy on publicly available data con-taining preposition errors, it does not provideany evidence that the explicit feedback pro-vided by the tool is useful to English lan-guage learners in a classroom setting.
In thefuture, we plan to conduct a controlled studyin a classroom setting that measures, for ex-ample, whether the students that see the im-proved feedback from the tool learn moreor better than those who either see no feed-back at all or those who see only implicitfeedback.
Biber et al.
(2011) review sev-eral previously published studies on the ef-fects of feedback on writing development inclassrooms.
Although the number of studiesthat were included in the analysis is small,some patterns did emerge.
In general, stu-dents improve their writing when they re-ceive feedback, however greater gains aremade when they are presented with com-ments rather than direct location and correc-tion of errors.
It is unclear how studentswould react to a ranked list of suggestionsfor a particular error at a given location.
Aninteresting finding was that L2-English stu-dents showed greater improvements in writ-ing when they received either feedback frompeers or computer-generated feedback thanwhen they received feedback from teachers.?
Assuming a single true correction.
Ourevaluation setup assumes that the single cor-rection provided as part of the FCE data set isthe only correct preposition for a given sen-tence.
However, it is well known in the gram-matical error detection community that this isnot always the case.
Most usage errors suchas preposition selection errors are a matter ofdegree rather than simple rule violations suchas number agreement.
As a consequence, itis common for two native English speakers85Context K Found Missed Blank MRRwords1 0 889 (28.4%) 356 (11.4%) 1889 (60.3%) .522words1 1 932 (29.7%) 417 (13.3%) 1785 (57.0%) .513words1 2 1033 (33.0%) 550 (17.6%) 1551 (49.5%) .493words1 5 1118 (35.7%) 691 (22.1%) 1325 (42.3%) .476words1 10 1160 (37.0%) 780 (24.9%) 1194 (38.1%) .465Table 2: A detailed breakdown of the Found, Missing and Blank classes along with the Mean ReciprocalRank (MRR) values, for different number of query expansions (K).to have different judgments of usage.
In fact,this is exactly why the tool is designed to re-turn a ranked list of suggestions rather thana single suggestion.
Therefore, it is possiblethat our intrinsic evaluation is underestimat-ing the performance of the tool.?
Practical considerations for deployment.In this study, we used the gold standard er-ror annotations for detecting preposition er-rors before querying the tool for suggestions.Such a setup allowed us to separate the prob-lems of error detection and the generationof feedback and likely gives an upper boundon performance.
Using a fully automaticerror detection system will likely introduceadditional noise into the pipeline, however,we believe that tuning the detection systemfor higher precision could mitigate that ef-fect.
Another useful idea would be to use theclassifier-based approach (see ?3) as a backupfor the corpus-based approach for providingsuggestions, i.e., using the classifier to pre-dict the suggested corrections when no cor-rections can be found in the Wikipedia revi-sions.?
Using other types of expansions.
In this pa-per, we used a very simple method of gener-ating query expansions ?
a distributional the-saurus.
However, in the future, it may beworth exploring other distributional similar-ity methods such as Brown clusters (Brownet al., 1992; Miller et al., 2004; Liang, 2005)or word2vec (Mikolov et al., 2013).8 ConclusionsIn this paper, we presented our work on build-ing a proof-of-concept tool that can provide au-tomated explicit feedback for preposition errors.We used an existing, error-annotated prepositioncorpus produced by mining Wikipedia revisions(Cahill et al., 2013) to not only provide a rankedlist of suggestions for any given preposition errorbut also to produce human-authored evidence foreach suggested correction.
The tool can use eitherwords or part-of-speech tags around the error ascontext.
We evaluated the tool in terms of bothaccuracy and coverage and found that: (1) usinglarger context window sizes for words increasesaccuracy but reduces coverage due to sparsity (2)using part-of-speech tags leads to increased cov-erage compared to using words as contexts butdecreases accuracy.
We also experimented withquery expansion for single words around the er-ror and found that it led to an increase in cover-age with only a slight decrease in accuracy; usinga larger set of expansions added more noise.
Ingeneral, we find that the approach of using a largeerror-annotated corpus to provide explicit feed-back to writers performs reasonably well in termsof providing ranked lists of alternatives.
It remainsto be seen how useful this tool is in a practical sit-uation.AcknowledgmentsWe would like to thank Beata Beigman Klebanov,Michael Heilman, Jill Burstein, and the anony-mous reviewers for their helpful comments aboutthe paper.
We also thank Ani Nenkova, ChrisCallison-Burch, Lyle Ungar and their students atthe University of Pennsylvania for their feedbackon this work.ReferencesYigal Attali.
2004.
Exploring the Feedback and Re-vision Features of Criterion.
Paper presented atthe National Council on Measurement in Education(NCME), Educational Testing Service, Princeton,NJ.Douglas Biber, Tatiana Nekrasova, and Brad Horn.2011.
The Effectiveness of Feedback for L1-Englishand L2-Writing Development: A Meta-Analysis.86Research Report RR-11-05, Educational TestingService, Princeton, NJ.Peter F. Brown, Vincent J. Della Pietra, Peter V.de Souza, Jennifer C. Lai, and Robert L. Mercer.1992.
Class-Based n-gram Models of Natural Lan-guage.
Computational Linguistics, 18(4):467?479.Jill Burstein, Martin Chodorow, and Claudia Leacock.2003.
Criterion online essay evaluation: An applica-tion for automated evaluation of student essays.
InProceedings of IAAI, pages 3?10, Acapulco, Mex-ico.Aoife Cahill, Nitin Madnani, Joel Tetreault, and Di-ane Napolitano.
2013.
Robust Systems for Prepo-sition Error Correction Using Wikipedia Revisions.In Proceedings of NAACL, pages 507?517, Atlanta,GA, USA.Martin Chodorow, Michael Gamon, and Joel Tetreault.2010.
The Utility of Article and Preposition Er-ror Correction Systems for English Language Learn-ers: Feedback and Assessment.
Language Testing,27(3):419?436.Daniel Dahlmeier and Hwee Tou Ng.
2011.
Gram-matical Error Correction with Alternating StructureOptimization.
In Proceedings of ACL-HLT, pages915?923, Portland, Oregon, USA.Rachele De Felice and Stephen G. Pulman.
2009.Automatic detection of preposition errors in learnerwriting.
CALICO Journal, 26(3):512?528.Michael Gamon, Claudia Leacock, Chris Brockett,William B Dolan, Jianfeng Gao, Dmitriy Belenko,and Alexandre Klementiev.
2009.
Using StatisticalTechniques and Web Search to Correct ESL Errors.CALICO Journal, 26(3):491?511.Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and DavidTugwell.
2004.
The Sketch Engine.
In Proceedingsof EURALEX, pages 105?116.Percy Liang.
2005.
Semi-supervised Learning for Nat-ural Language.
Master?s thesis, Massachusetts Insti-tute of Technology.Dekang Lin.
1998.
Automatic Retrieval andClustering of Similar Words.
In Proceedings ofACL-COLING, pages 768?774, Montreal, Quebec,Canada.Anastasiya A. Lipnevich and Jeffrey K. Smith.
2008.Response to Assessment Feedback: The Effects ofGrades, Praise, and Source of Information.
Re-search Report RR-08-30, Educational Testing Ser-vice, Princeton, NJ.Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.Corrado, and Jeffrey Dean.
2013.
Distributed Rep-resentations of Words and Phrases and their Com-positionality.
In Proceedings of NIPS, pages 3111?3119.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name Tagging with Word Clusters andDiscriminative Training.
In Proceedings of HLT-NAACL, pages 337?342, Boston, MA, USA.John Milton and Vivying SY Cheng.
2010.
A Toolkitto Assist L2 Learners Become Independent Writers.In Proceedings of the NAACL Workshop on Compu-tational Linguistics and Writing: Writing Processesand Authoring Aids, pages 33?41, Los Angeles, CA,USA.John Milton.
2006.
Resource-rich Web-based Feed-back: Helping learners become Independent Writ-ers.
Feedback in second language writing: Contextsand issues, pages 123?139.JoAnn Leah Rock.
2007.
The Impact of Short-Term Use of Criterion on Writing Skills in NinthGrade.
Research Report RR-07-07, EducationalTesting Service, Princeton, NJ.Alla Rozovskaya and Dan Roth.
2010.
TrainingParadigms for Correcting Errors in Grammar andUsage.
In Proceedings of NAACL-HLT, pages 154?162, Los Angeles, California.Hongsuck Seo, Jonghoon Lee, Seokhwan Kim, Kyu-song Lee, Sechun Kang, and Gary Geunbae Lee.2012.
A Meta Learning Approach to GrammaticalError Correction.
In Proceedings of ACL (short pa-pers), pages 328?332, Jeju Island, Korea.Mark D. Shermis, Jill C. Burstein, and Leonard Bliss.2004.
The Impact of Automated Essay Scoring onHigh Stakes Writing Assessments.
In Annual Meet-ing of the National Council on Measurement in Ed-ucation.Karen Sp?arck-Jones and J. I. Tait.
1984.
AutomaticSearch Term Variant Generation.
Journal of Docu-mentation, 40(1):50?66.Joel R. Tetreault and Martin Chodorow.
2008.
TheUps and Downs of Preposition Error Detection inESL Writing.
In Proceedings of COLING, pages865?872, Manchester, UK.Joel Tetreault, Jennifer Foster, and Martin Chodorow.2010.
Using Parse Features for Preposition Selec-tion and Error Detection.
In Proceedings of ACL(short papers), pages 353?358, Uppsala, Sweden.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of NAACL, pages 173?180, Edmon-ton, Canada.John Truscott.
2007.
The Effect of Error Correctionon Learners?
Ability to Write Accurately.
Journalof Second Language Writing, 16(4):255?272.Ellen M. Voorhees.
1999.
The TREC-8 Question An-swering Track Report.
In Proceedings of the TextREtrieval Conference (TREC), volume 99, pages77?82.87Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A New Dataset and Method for AutomaticallyGrading ESOL Texts.
In Proceedings of the ACL:HLT, pages 180?189, Portland, OR, USA.88
