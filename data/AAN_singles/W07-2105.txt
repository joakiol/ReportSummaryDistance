Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 472?475,Prague, June 2007. c?2007 Association for Computational LinguisticsUVAVU: WordNet Similarity and Lexical Patternsfor Semantic Relation ClassicationWillem Robert van HageTNO Science & IndustryStieltjesweg 1, 2628CK Delftthe Netherlandswrvhage@few.vu.nlSophia KatrenkoHCSL, University of AmsterdamKruislaan 419, 1098VA Amsterdamthe Netherlandskatrenko@science.uva.nlAbstractThe system we propose to learning seman-tic relations consists of two parallel com-ponents.
For our final submission we usedcomponents based on the similarity mea-sures defined over WordNet and the patternsextracted from the Web and WMTS.
Othercomponents using syntactic structures wereexplored but not used for the final run.1 Experimental Set-upThe system we used to classify the semantic rela-tions consists of two parallel binary classifiers.
Weran this system for each of the seven semantic re-lations separately.
Each classifier predicts for eachinstance of the relation whether it holds or not.
Thepredictions of all the classifiers are aggregated foreach instance by disjunction.
That is to say, each in-stance is predicted to be false by default unless anyof the classifiers gives evidence against this.To generate the submitted predictions we usedtwo parallel classifiers: (1) a classifier that com-bines eleven WordNet-based similarity measures,see Sec.
2.1, and (2) a classifier that learns lexicalpatterns from Google and the Waterloo Multi-TextSystem (WMTS)(Turney, 2004) snippets and ap-plies these on the same corpora, see Sec.
2.2.Three other classifiers we experimented with, butthat were not used to generate the submitted predic-tions: (3) a classifier that uses string kernel methodson the dependency paths of the training sentences,see Sec.
3.1, (4) a classifier that uses string kernelson the local context of the subject and object nom-inals in the training sentences, see Sec.
3.2 and (5)a classifier that uses hand-made lexical patterns onGoogle and WMTS, see Sec.
3.3.2 Submitted Run2.1 WordNet-based Similarity MeasuresWordNet 3.0 (Fellbaum, 1998) is the most fre-quently used lexical database of English.
As this re-source consists of lexical and semantic relations, itsuse constitutes an appealing option to learning rela-tions.
In particular, we believe that given two men-tions of the same semantic relation, their argumentsshould also be similar.
Or, in analogy learning terms,if R1(X1,Y1) and R2(X2,Y2) are relation mentions ofthe same type, then X1 :: Y1 as X2 :: Y2.
Our prelim-inary experiments with WordNet suggested that fewarguments of each relation are connected by imme-diate hyperonymy or meronymy relations.
As a re-sult, we decided to use similarity measures definedover WordNet (Pedersen et al, 2004).
The Word-Net::Similarity package (Pedersen et al, 2004) in-cludes 11 different measures, which mostly use ei-ther the WordNet glosses (lesk or vector measures)or the paths between a pair of concepts (lch; wup) todetermine their relatedness.To be able to use WordNet::Similarity, wemapped all WordNet sense keys from the trainingand test sets to the earlier WordNet version (2.1).Given a relation R(X ,Y ), we computed the related-ness scores for each pair of arguments X and Y .
Thescores together with the sense keys of argumentswere further used as features for the machine learn-ing method.
As there is no a priori knowledge onwhat measures are the most important for each rela-472tion, all of them were used and no feature selectionstep has been taken.We experimented with a number of machinelearning methods such as k-nearest neighbour al-gorithm, logistic regression, bayesian networks andothers.
For each relation a method performing beston the training set was selected (using 5-fold cross-validation).2.2 Learnt Lexical PatternsThis classifier models the intuition that when a pairof nominals is used in similar phrases as another pairthey share at least one relation, and when no suchphrases can be found they do not share any relation.Applied to the semantic relation classification prob-lem this means that when a pair in the test set can befound in the same patterns as pairs from the trainingset, the classification for the pair will be true.To find the patterns we followed step 1 to 6 de-scribed in (Turney, 2006), with the exception thatwe used both Google and the WMTS to computepattern frequency.First we extracted the pairs of nominals ?X ,Y ?from the training sentences and created one Googlequery and a set of WMTS queries for each pair.The Google queries were of the form "X * Y"OR "Y * X".
Currently, Google performs mor-phological normalization on every query, so wedid not make separate queries for various endingsof the nominals.
For the WMTS we did makeseparate queries for various morphological varia-tions.
We used the following set of suffixes: ?-tion(s|al)?, ?-ly?, ?-ist?, ?-ical?, ?-y?, ?-ing?, ?-ed?,?-ies?, and ?-s?.
For this we used Peter Turney?spairs Perl package.
The WMTS queries lookedlike [n]>([5].."X"..[i].."Y"..[5]) and[n]>([5].."Y"..[i].."X"..[5]) for i =1,2,3 and n = i+12, and for each variation of X andY .
Then we extracted sentences from the Googlesnippets and cut out a context of size 5, so thatwe were left with similar text segments as thosereturned by the WMTS queries.
We merged thelists of text segments and counted all n-grams thatcontained both nominals for n = 1 to 6.
We sub-stituted the nominals by variables in the n-gramswith a count greater than 10 and used these as pat-terns for the classifier.
An example of such a pat-tern for the Cause-Effect relation is "generationof Y by X".
After this we followed step 3 to6 of (Turney, 2006), which left us with a matrixfor each of the seven semantic relations, where eachrow represented a pair of nominals and each columnrepresented the frequency of a pattern, and whereeach pair was classified as either true or false.
Thestraightforward way to find pattern frequencies forthe pairs in the test set would be to fill in these pat-terns with the pairs of nominals from the test set.This was not feasible given the time limitation onthe task.
So instead, for each pair of nominals inthe test set we gathered the top-1000 snippets andcomputed pattern frequencies by counting how of-ten the nominals occur in every pattern on this settext segments.
We constructed a matrix from thesefrequencies in the same way as for the training set,but without classifications for the pairs.
We experi-mented with various machine learning algorithms topredict the classes of the pairs.
We chose to use k-nearest neighbors, because it was the only algorithmthat gave more subtle predictions than true for everypair or false for every pair.
For each semantic rela-tion we used the value of k that produced the highestF1 score on 5-fold cross validation on the trainingdata.3 Additional Runs3.1 String Kernels on Dependency PathsIt has been a long tradition to use syntactic structuresfor relation extraction task.
Some of the methodsas in (Katrenko and Adriaans, 2004) have used in-formation extracted from the dependency trees.
Wefollowed similar approach by considering the pathsbetween each pair of arguments X and Y .
Ideally, ifeach syntactic structure is a tree, there is only onepath from one node to the other.
After we have ex-tracted paths, we used them as input for the stringkernel methods (Hal Daum?
III, 2004).
The advan-tage of using string kernels is that they can handlesequences of different lengths and already proved tobe efficient for a number of tasks.All sentences in the training data were parsedusing MINIPAR (Lin, 1998).
From each depen-dency tree we extracted a dependency path (if any)between the arguments by collecting all lemmas(nodes) and syntactic functions (edges).
The se-quences we obtained were fed into string kernel.473To assess the results, we carried out 5-fold cross-validation.
Even by optimizing the parameters ofthe kernel (such as the length of subsequences) foreach relation, the highest accuracy we obtained wasequal 61,54% (on Origin-Entity relation) and thelowest was accuracy for the Instrument-Agency re-lation (50,48%).3.2 String Kernels on Local ContextAlternatively to syntactic information, we also ex-tracted the snippets of the fixed length from eachsentence.
For each relation mention of R(X ,Y ), alltokens between the relation arguments X and Y werecollected along with at most three tokens to the leftand to the right.
Unfortunately, the results we ob-tained on the training set were comparable to thoseobtained by string kernels on dependency paths andless accurate than the results provided by WordNetsimilarity measures or patterns extracted from theWeb and WMTS.
As a consequence, string kernelmethods were not used for the final submission.3.3 Manually-created Lexical PatternsThe results of the method described in Sec.
2.2 arequite far below what we expected given earlier re-sults in the literature (Turney, 2006; van Hage, Ka-trenko, and Schreiber, 2005; van Hage, Kolb, andSchreiber, 2006; Berland and Charniak, 2006; Et-zioni et al, 2004).
We think this is caused bythe fact that many pairs in the training set are non-stereotypical examples.
So often the most com-monly described relation of such a pair is not the re-lation we try to classify with the pair.
For example,common associations with the pair ?body,parents?are that it is the parents?
body, or that the parentsare member of some organizing body, while it is apositive example for the Product-Producer relation.We wanted to see if this could be the case by testingwhether more intuitive patterns give better results onthe test set.
The patterns we manually created foreach relation are shown in Table 1.
If a pair givesany results for these patterns on Google or WMTS,we classify the pair as true, otherwise we classifyit as false.
The results are shown in Table 2.
Wedid not use these results for the submitted run, be-cause only automatic runs were permitted.
The man-ual patterns did not yield many useful results at all.Apparently intuitive patterns do not capture what isrequired to classify the relations in the test set.
Thepatterns we used for the Part-Whole (6) relation hadan average Precision of .50, which is much lowerthan the average Precision found in (van Hage, Kolb,and Schreiber, 2006), which was around 0.88.
Weconclude that both the sets of training and test ex-amples capture different semantics of the relationsthan the intuitive ones, which causes common sensebackground knowledge, such as Google to producebad results.rel.
patterns1.
X causes Y, X caused by Y, X * cause Y2.
X used Y, X uses Y, X * with a Y3.
X made by Y, X produced by Y, Y makes X,Y produces X4.
Y comes from X, X * source of Y, Y * from * X5.
Y * to * X, Y * for * X, used Y for * X6.
X in Y, Y contains X, X from Y7.
Y contains X, X in Y, X containing Y, X into YTable 1: Hand-written patterns.relation N Prec.
Recall F1 Acc.1.
Cause-Effect 6 1 0.15 0.25 0.562.
Instr.-Agency 2 1 0.05 0.10 0.543.
Prod.-Prod.
4 0.75 0.05 0.09 0.354.
Origin-Ent.
6 0.33 0.05 0.09 0.355.
Theme-Tool 2 0 0 0 0.566.
Part-Whole 16 0.50 0.31 0.38 0.647.
Cont.-Cont.
11 0.54 0.16 0.24 0.50Table 2: Results for hand-written lexical patterns onGoogle and WMTS.4 Results4.1 WordNet-based Similarity MeasuresTable 3 shows the results of the WordNet-based sim-ilarity measure method.
In the ?methods?
column,the abbreviation LR stands for logistic regression,K-NN stands for k-nearest neighbour, and DT standsfor decision trees.relation method Prec.
Recall F1 Acc.1.
Cause-Effect LR 0.48 0.51 0.49 0.452.
Instr.-Agency DT 0.65 0.63 0.64 0.623.
Prod.-Prod.
DT 0.67 0.50 0.57 0.464.
Origin-Ent.
LR 0.50 0.47 0.49 0.495.
Theme-Tool LR 0.54 0.52 0.53 0.626.
Part-Whole DT 0.54 0.73 0.62 0.677.
Cont.-Cont.
2-NN 0.66 0.55 0.60 0.62Table 3: Results for similarity-measure methods.4744.2 Learnt Lexical PatternsTable 4 shows the results of the learnt lexical pat-terns method.
For all relations we used the k-nearestneighbour method.relation method Prec.
Recall F1 Acc.1.
Cause-Effect 3-NN 0.53 0.76 0.63 0.542.
Instr.-Agency 2-NN 0.47 0.89 0.62 0.463.
Prod.-Prod.
2-NN 0 0 0 0.334.
Origin-Ent.
2-NN 0.47 0.22 0.30 0.545.
Theme-Tool 3-NN 0.39 0.93 0.55 0.386.
Part-Whole 2-NN 0.36 1 0.53 0.367.
Cont.-Cont.
2-NN 0.51 0.97 0.67 0.51Table 4: Results for learnt lexical patterns on Googleand WMTS.5 DiscussionOur methods had the most difficulty with classify-ing relation 1, 3 and 4.
We wanted to see if hu-man assessors perform less consistent for those re-lations.
If so, then those relations would simply beharder to classify.
Otherwise, our system performedworse for those relations.
We manually assessed tensample sentences from the test set, five of whichwere positive examples and five were false exam-ples.
The result of a comparison with the test set isshown in Table 5.
The numbers listed there repre-sent the fraction of examples on which we agreedwith the judges of the test set.
There was quite ainter-judge agreementrelation judge 1 judge 21.
Cause-Effect 0.93 0.932.
Instrument-Agency 0.77 0.773.
Product-Producer 0.87 0.804.
Origin-Entity 0.80 0.775.
Theme-Tool 0.80 0.776.
Part-Whole 0.97 1.007.
Content-Container 0.77 0.77Table 5: Inter-judge agreement.large variation in the inter-judge agreement, but forrelation 1 and 3 the consensus was high.
We con-clude that the reason for our low performance onthose relations are not caused by the difficulty ofthe sentences, but due to other reasons.
Our intu-ition is that the sentences, especially those of rela-tion 1 and 3, are easily decidable by humans, butthat they are non-stereotypical examples of the re-lation, and thus hard to learn.
The following ex-ample sentence breaks common-sense domain andrange restrictions: Product-Producer #142 ?And, ofcourse, everyone wants to prove the truth of their be-liefs through experience, but the <e1>belief</e1>begets the <e2>experience</e2>.?
The common-sense domain and range restriction of the Product-Producer relation are respectively something like?Entity?
and ?Agent?.
However, ?belief?
is generallynot considered to be an entity, and ?experience?
notan agent.
The definition of Product-Producer rela-tion used for the Challenge is more flexible and al-lows therefore many examples which are difficult tofind by such common-sense resources as Google orWordNet.ReferencesMatthew Berland and Eugene Charniak.
1999.
FindingParts in Very Large Corpora.
In Proceedings of ACL1999.Christiane Fellbaum (ed.).
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Hal Daum?
III.
2004.
SVMsequel Tutorial Manual.Available at http://www.cs.utah.edu/?hal/SVMsequel/svmsequel.pdfOren Etzioni et al 2004.
Methods for Domain-Independent Information Extraction from the Web: AnExperimental Comparison.
In Proceedings of AAAI2004.Willem Robert van Hage, Sophia Katrenko, and GuusSchreiber.
2005.
A Method to Combine Linguis-tic Ontology-Mapping Techniques.
In Proceedings ofISWC 2005.Willem Robert van Hage, Hap Kolb, and Guus Schreiber.2006.
A Method for Learning Part-Whole Relations.In Proceedings of ISWC 2006.Sophia Katrenko and Pieter Adriaans.
2007.
Learn-ing Relations from Biomedical Corpora Using Depen-dency Trees.
In KDECB, LNBI, vol.
4366.Dekang Lin.
1998.
Dependency-based Evaluation ofMINIPAR.
In Workshop on the Evaluation of ParsingSystems, Granada, Spain.Ted Pedersen, Patwardhan, and Michelizzi.
2004.
Word-Net::Similarity - Measuring the Relatedness of Con-cepts.
In the Proceedings of AAAI-04, San Jose, CA.Peter Turney.
2006.
Expressing Implicit SemanticRelations without Supervision.
In Proceedings ofCOLING-ACL 2006.Peter Turney.
2004.
The MultiText Project Home Page,University of Waterloo, School of Computer Science,http://www.multitext.uwaterloo.ca475
