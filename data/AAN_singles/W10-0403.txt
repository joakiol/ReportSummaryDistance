Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 15?23,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsGrammaticality Judgement in a Word Completion TaskAlfred Renaud2 and Fraser Shein1,2 and Vivian Tsang11Bloorview Kids Rehab 2Quillsoft Ltd.150 Kilgour Road 2416 Queen Street EastToronto, ON  M4G 1R8, Canada Toronto, ON  M2A 1N1, Canada{arenaud,fshein,vtsang}@bloorview.caAbstractIn this paper, we present findings from a hu-man judgement task we conducted on the ef-fectiveness of syntax filtering in a word com-pletion task.
Human participants were askedto review a series of incomplete sentences andidentify which words from accompanying listsextend the expressions in a grammatically ap-propriate way.
The accompanying word listswere generated by two word completion sys-tems (our own plus a third-party commercialsystem) where the ungrammatical items werefiltered out.
Overall, participants agreed more,to a statistically significant degree, with thesyntax-filtered systems than with baseline.However, further analysis suggests that syntaxfiltering alone does not necessarily improvethe overall acceptability and usability of theword completion output.
Given that wordcompletion is typically employed in applica-tions to aid writing, unlike other NLP tasks,accounting for the role of writer vs. reader be-comes critical.
Evaluating word completionand, more generally, applications for alterna-tive and augmentative communication (AAC)will be discussed.1 IntroductionWriters often need help from others to help withspelling and grammar.
For persons with physicalor learning disabilities, writing can be very stress-ful because of a greater reliance on the assistanceof others.
Software tools such as word comple-tion are now commonly used to reduce the physi-cal and cognitive load of completing a word or asentence and thereby reducing a writer?s depend-ence on others.
But can such tools be as effectiveas a human with adequate linguistic knowledge?While it is hardly possible to completely emulatea human tutor or a communication partner, thepurpose of this research is to investigate howmuch linguistic knowledge is necessary to ensurethe usability of word completion.
Here, we willfocus on the grammaticality of word completion.1.1 Word CompletionWord completion facilitates text entry by suggest-ing a list of words that can follow a given linguis-tic context.
If the desired word is in the list, theuser can select that word with a mouse click or akeystroke, thereby saving the effort of typing theremaining letters of the word.
Otherwise, the usercan continue typing while the software continuesto display new lists of words based on that input.For example, consider a user wants to type?That girl by the benches??
After each letter theuser manually enters, a system would return a listof potential next words.
Say, the next letter theuser enters is ?w.?
A system may offer the fol-lowing choices: a) was, b) were, c) with, d) where,e) wrapped.
By suggesting words in any givencontext, word completion can assist in the compo-sition of well-formed text.Typical word completion systems suggestwords by exploiting n-gram Markov statisticalmodels (Bentrup, 1987).
These systems probabil-istically determine the current word in a sentencegiven the previous n?1 words as context, based ona pre-generated n-gram language model derivedfrom a corpus.
With n typically being of low or-15der (two or three, due to sparse data and computa-tional issues), one consequence is that the appli-cability of suggested words beyond a certain worddistance may become somewhat arbitrary.
Fur-ther design improvements for word completiondepend on the user population and the intendeduse.
For example, the demand on the system tohave a sophisticated language model may dependon whether the intent is to primarily reduce thephysical or cognitive load of entering text.Evaluation approaches can elucidate on designand implementation issues for providing meaning-ful word choices.1.2  Evaluation ApproachesA number of studies have been carried out toevaluate the efficacy of word completion systems.Koester (1994) measured time savings, which isthe reduction in time that the user takes to gener-ate a particular text with the aid of a word comple-tion system compared to the time taken without it.The rationale for this measure is that any wordcompletion system imposes a cognitive load on itsusers, whereby they now need to 1) change theirfocus between the target document and the wordlist display, and possibly between the screen andkeyboard; 2) visually scan the word list to decidewhether their intended word is present; and 3) se-lect the intended word with the keyboard ormouse.
Others have also examined similar visual-cognitive issues of using word completion (e.g.,Tam and Wells, 2009).
The overall approach im-plicitly defines a user-centred approach to evalua-tion by having human subjects simulate the actualwriting process (usually in a copying, not writingtask).
Thus, results depend on the abilities andpreferences of individual subjects.System-based evaluation measures exist, themost common of which is keystroke savings.
Thismeasures the reduction in the number of key-strokes needed to produce a given text with theaid of a word completion system.
Keystroke sav-ings is an important factor for users with physicaldisabilities who have difficulty working with akeyboard for which it is desirable to keep thenumber of keystrokes to a minimum.
A comple-mentary measure, completion keystrokes, deter-mines how quickly a given word is predicted bycounting the number of characters required toreach completion.
Completion keystrokes differsfrom keystroke savings in that the latter counts theletters remaining in the word.In contrast to the previous two measures, bothof which measure at the character level, hit ratemeasures at the word level by calculating the ratioof the number of words correctly predicted to thetotal number of words predicted.
Given a suffi-ciently large lexicon, hit rate can be as high as100% if every letter of every word is manuallyentered to its completion.
As this can be mislead-ing, hit rate is more typically measured with refer-ence to the number of characters already typed inorder to assess the system?s demand on the user.These objective measures address motor loadindependent of cognitive load.
With the exceptionof time savings, these measures can be bench-marked automatically by simulating the writingprocess by using existing texts.A shortcoming of these objective measures isthat they focus on the reduction on the user?sphysical demand by simulating the entering of analready written text, and effectively ignore con-sideration of word choices other than the uniqueintended word.
In reality, the actual writing proc-ess depends also on the quality of the entire groupof suggested word choices with respect to the in-tended content.
Renaud (2002) addressed thisshortcoming by arguing that the syntactic and se-mantic relations between words can impact onchoice-making at the target word.
He introducedtwo measures, validity and appropriateness,measuring grammatical consistency and semanticrelevance of all system output, respectively.
Theformer measure calculates the proportion of a sys-tem?s suggested words that is syntactically ac-ceptable.
The latter focuses on the proportion ofrelevant output based on lexical and domain se-mantics.
Renaud compared a number of commer-cial systems and found a positive correlation be-tween the new and existing measures.
This find-ing also lends additional support to Wood?s(1996) finding that offering syntactically and se-mantically appropriate choices improves perform-ance.
(Note that the converse may not hold true.
)For the remainder of this paper, we will put ouremphasis on the impact of linguistic content (here,grammaticality) on the quality of word comple-tion.
The paper is organized as follows.
In thenext section, we will describe the need to incorpo-rate syntactic information in word completion.
Insections 3 and 4, we will describe our human16judgement task evaluating the grammaticality ofword completion.
Based on our analysis, we willreturn to the evaluation issue in section 5 and dis-cuss how grammaticality alone does not addressthe larger usability issue of word completion.Here, we propose that the word completion task,unlike traditional NLP tasks, requires both thereader?s and writer?s perspectives, which impactsthe interpretation of our evaluation, and in turnimpacts design decisions.
In section 6, we willconclude by offering a more inclusive perspectiveon AAC.2 The Demand for Syntactic FilteringAs shown earlier, many evaluation methods havefocused on 1) the proportion of key-presses nor-mally required during a typing session that theuser need not to manually enter and 2) the propor-tion of text words in a typing session that the sys-tem is able correctly to predict.
For a user with alearning disability or language difficulties, agreater concern is that all presented words bevalid, logical, and free of grammatical errors.Current state-of-the-art systems suffer by suggest-ing words that are often syntactically implausiblewhile excluding more justifiable but less probablesuggestions (cf.
our example in section 1).
A usermay be confused by inappropriate suggestions,even if correct suggestions are also present.To quantify the importance of syntax in wordcompletion, we compare the average hit ratescores (over all words) with the hit rate scores atpoints in sentences we consider as syntacticallycritical (see section 3 for their selection).
Nantaiset al (2001) reported an overall hit rate of ap-proximately 56% using bigram word completionafter entering the first letter of a word across alarge document.
However, at the word locationwhere it is crucial to maintain correct syntacticrelation with the existing sentence fragment, hitrates are often much lower.
In our study situation,the hit rate is at best 39%?these syntactic chal-lenges tend to be semantically contentful and thuspresent difficulties to human subjects.
Likewise,the systems are expected to struggle with them.Without a clear understanding of content specificissues during writing, examining time and key-stroke savings alone does not reveal the increaseddifficulty a user faces at these word positions.
Wewill return to these issues in section 5.2.1 Building Syntactic KnowledgeKnowledge of syntax can be obtained by first tag-ging each dictionary word with its part of speech,such as noun or adjective.
This information maythen be used in either a probabilistic or a symbolicmanner.
Systems may reason probabilistically bycombining tag n-gram models, where the part-of-speech tags for the previous n?1 words in a sen-tence are used to predict the tag for the currentword, with word n-gram models that cue the re-sulting part(s) of speech to find words proper(Hunnicutt and Carlberger, 2001).
Fazly andHirst (2003) introduced two algorithms for com-bining tag trigrams with word bigrams.
The firstalgorithm involved conditional independence as-sumptions between word and tag models, and thesecond algorithm involved a weighted linear com-bination of the two models.A fundamental limitation to this approach isthat low-order probabilistic language models canonly account for relationships between closely co-located words.
Symbolic syntactic predictionguided by a grammar, on the other hand, can dealwith long-distance word relationships of arbitrarydepth by applying rules that govern how wordsfrom syntactic categories can be joined, to assignall sentence words to a category.
This approachuses knowledge of English grammar to analyzethe structure of the sentence in progress and de-termine the applicable syntactic categories (e.g.,noun, verb), along with other features (e.g., singu-lar, past participle), to which the currentlytyped/predicted word must belong.
In this way aword completion system is able to suggest wordsthat are grammatically consistent with the activesentence fragment.As such, research closer in nature to our workinvolves parsers that process the input sentenceincrementally as each word is entered.
Wood?s(1996) augmented phrase-structure grammarshowed that symbolic syntactic prediction canimprove overall performance when combinedwith statistical orthographic prediction.
McCoy(1998) used the augmented transition network orATN (Woods, 1970) formalism to find candidateword categories from which to generate wordlists.
Gustavii and Pettersson (2003) used a chartparser to re-rank, or filter, word lists by gram-matical value.
These parsing algorithms manipu-late some data structure that represents, and im-17poses ordering on, syntactic constituents of sen-tences.
Recently, we have been developing a syn-tax module (Renaud et al, 2010) based on anATN-style parser, which can facilitate both in-creasing the level of correctness in parses throughgrammar correction, and modifying the informa-tion collected during parsing for a particular ap-plication (Newman, 2007).
Specifically, thissystem filters words provided by n-gramcompletion such that the word list only showswords that fit an acceptable grammatical structure.It operates on a longer list of the same frequency-ranked words our core predictor generates.
Underthis setup, our syntax module can influence thefinal list shown to the user by demotingimplausible words that otherwise would have beendisplayed and replacing them with plausiblewords that otherwise would not.
Our rationale forusing a symbolic vs. a probabilistic parser in wordcompletion is beyond the scope of the current pa-per.3 Grammaticality Judgement ExperimentTo evaluate the impact of syntactic filtering onword completion, we devised a human judgmenttask where human subjects were asked to judgethe grammatical acceptability of a word offeredby word completion software, with or withoutsyntactic filtering.
Given a partial sentence and aleading prefix for the next word, word completionsoftware presents a number of choices for the po-tential next word.
Although the goal is to assessthe grammaticality of predicted words with orwithout syntactic filtering, the intent is to assesswhether the inclusion of syntactic heuristics in theword completion algorithm improves the qualityof word choices.3.1 Experimental SetupIn our experiment, we compared three differentword completion systems: our baseline comple-tion system (WordQ?
*, henceforth ?baseline?
),our word completion system with syntax filtering(?System B?).
We also included a third-partycommercial word completion system with syntaxfiltering built-in (Co:Writer?
?, ?System C?).
In* http://www.wordq.com; our baseline system uses a bigramlanguage model trained on a corpus of well-edited text.?
http://www.donjohnston.com/products/cowriter/index.htmleach system, we inputted a partial sentence plusthe leading character for the next word.
Each sys-tem returned a list of five choices for the potentialnext word.
Our subjects were asked to judge thegrammatical acceptability of each word (binarydecision: yes or no).It is worth noting that the more letters aremanually inserted, the narrower the search spacebecomes for the next word.
Nantais et al (2001)suggested that after inserting two characters, thehit rate via automatic means can be as high as72%; the hit rate for humans is likely muchhigher.
Given that our goal is to examine thegrammaticality of word choices and not hit rate,providing only one leading letter allows sufficientambiguity on what the potential next word is,which in turn allows for a range of grammaticalchoices for our judgement task.3.2 Sentence SelectionWe selected our test sentences from Canadiannews sources (Toronto Star and the Globe andMail), which are considered reliably grammatical.We chose a total of 138 sentences.?
Each sen-tence was truncated into a fragment containing thefirst x-1 words and the first character of the xthword, where x ranges from three to ten inclusive.The truncation position x was deliberately selectedto include a variety of grammatical challenges.We divided the sentence fragments into ninetypes of grammatical challenges: 1) subject-verbagreement; 2) subject-verb agreement in question-asking; 3) subject-verb agreement within a rela-tive clause; 4) appositives; 5) verb sequence (aux-iliary verb-main verb agreement); 6) case agree-ment; 7) non-finite clauses; 8) number agreement;and 9) others.For example, the sentence ?That girl by thebenches was in my high school?
from section 1.1can be used to test the system?s ability to recog-nize subject-verb agreement if we truncate thesentence to produce the fragment ?That girl by thebenches w___.?
Here, subject-verb agreementshould be decided against the subject ?girl?
andnot the (tempting) subject ?benches.??
We did not pick a larger number of sentences due to thetime constraint in our experimental setup.
The rationale is toavoid over-fatiguing our human subjects (approximately anhour per session).
Based on our pilot study, we were able tofit 140 sentences over three one-hour sessions.18After the initial selection process, we reducedour collection to 123 partial sentences.
Becausethe sentences were not evenly distributed acrossthe nine categories, we divided the sentences intothree sets such that the frequency distribution ofthe sentence types was the same for all three sets(41 sentences per set).
The three word completionsystems were each assigned a different set.
?3.3 Grammaticality JudgementsWe fed each partial sentence into the correspond-ing system to produce a word list for grammaticaljudgement.
Recall our example earlier, given fiveword choices per partial sentence, for each wordchoice, our subjects were asked to judge itsgrammatical acceptability (yes or no).We recruited 14 human subjects, all nativespeakers of English with a university education.Each subject was presented all 123 sentences cov-ering the three systems, in a paper-based task.The sentence order was randomized and the sub-jects were unaware of which system producedwhat list.Given that each system produced a list of fiveoptions for each partial sentence, each subjectproduced 5?41=205 judgements for each system.There were 14 sets of such judgements in total.4 Results and AnalysisOur primary objective is to examine the subjects?agreement with the system, and whether the sub-jects generally agree among themselves.
Our ra-tionale is this.
If the subjects generally agree withone another, then there is an overall agreement onthe perception of grammaticality in word comple-tion.
If this is indeed the case, we then need toexamine how and why our subjects agree or dis-agree with the systems.
Otherwise, if there is lowinter-subject agreement, aside from issues relatedto the experimental setup, we need to reconsiderwhether offering grammatical word completionchoices is indeed practical and possible.We first calculated individual participantagreement with the output of each system (i.e.,?
We initially to used three different sets, i.e., one set persystem, to avoid a sampling ?fluke?
of different grammaticaldifficulties/categories.
However, for exactly the same reason,we also tested our system using the two sets for the other twosystems for ease of comparison.
See section 4.1 for details.averaged over all participants).
The baselinescored 68%.
System B scored 72% and System Cscored 74%.
Thus, an early important result wasthat syntax assistance in general, independent ofparticular approach or algorithm, does appear toimprove subject agreement in a word completiontask.
(Note that we treat system C as a black boxas we are not privy to its algorithms, which arenot published.
)Overall, the grammaticality of a given test word(i.e., averaged over all test words) had an averageagreement of 85%, or by 12 of the 14 participants.The percentage agreement for each system was84% for the baseline, 87% for system B, and 86%for system C.  If at least two-thirds of the partici-pants (10 of 14) agreed on the grammaticality of aparticular test word, we considered the collectiveopinion to be consistent for that word and de-clared a consensus.
Participants reached consen-sus on 77% of the test words for the baseline, 82%of the test words for system B, and 80% of the testwords for system C.Next, we calculated consensus participantagreement for each system.
This measure wasdifferent from the previous in that we consideredonly those cases where 10 or more of the 14 par-ticipants agreed with one another on the gram-maticality of a system?s test word and discardedall other cases.
In 75% of the consensus cases forthe baseline, the subjects agreed with the system(by approving on the grammaticality); in the other25% of the consensus cases the subjects disagreedwith the system.
System B scored 78% on theconsensus agreement and system C scored 81%.A repeated-measures analysis of variance(ANOVA) was performed on the data.
For bothindividual and consensus participant agreement,each of Systems B and C outperformed the base-line system (statistically significant, p<.05), whilethe difference between the two systems with syn-tax awareness was not statistically significant.To summarize our findings, our subjects gener-ally found the output grammatically more accept-able if syntactic assistance was built in (72% and74% over 68% in raw participant agreement; 78%and 81% over 75% in consensus agreement).
Thebehaviour of our System B generally was in linewith the behaviour of the third-party System C.Finally, the agreement among subjects for all sys-tems was quite high (~85%) and is consideredreliable.194.1  Subject Agreement with Other SystemsTo further understand the behaviour of our ownsystem (in contrast to our subjects?
judgements),we create two new systems, A' and C' based onthe output of the baseline system and the third-party System C. Recall that the sentence set usedin each system is mutually exclusive from the setused in another system.
Therefore, this setup in-troduces an additional set of 41 sentences ?
5 pre-dicted words ?
2 systems = 410 judgements.Our setup is simple: we feed into our parsereach of the sentence fragments for the correspond-ing system, along with each predicted word origi-nally produced.
If our parser accepts the word,the analysis remains unchanged.
Otherwise, wecount it as a ?negative?
result, which we explainbelow.Consider again our earlier example, ?The girlby the benches w___.?
Say system C' producesthe following options: a) was, b) were, c) with, d)where, e) wrapped.
We then attempt to generate apartial parse using the partial sentence with eachpredicted word, i.e., ?The girl by the bencheswas,?
?The girl by the benches were,?
and so on.If, for instance, our parser could not generate aparse for ?The girl by the benches where,?
thenwe would treat the word choice ?where?
as notapproved for the purpose of recalculating subjectagreement.
So if any subjects had approved itsgrammaticality (i.e., considered it a grammaticalnext word), then we counted it as a disagreement(between the parser and the human judge), other-wise, we considered it an agreement.Consider the following example.
One partialsentence for System C was ?Japanese farmersimmediately pick the shoots which a[m]??
Only1 of 14 judges agreed with it.
System C' alsoflagged ?am?
as ungrammatical.
Now 13 judgesagreed with it.On the other hand, consider this partial sen-tence originally from the baseline system, ?Thereason we are doing these i[nclude]??
where 10judges said yes but our parser could not generate aparse.
In this case, A' scores 4 on agreement.Overall, A' overrode 10 decisions and scored71% agreement as a result.
That is a 3% im-provement over the baseline 68% score.
Nine ofthe 10 reversed consensus in a positive directionand 1 (example above) reversed consensus in anegative direction.
In comparison, C' overrode 6decisions, and scored 76% (2.0% improvementover the original 74%).
Five of 6 cases reversedconsensus, all in a positive direction.
(The othercase reversed a non-consensus in a positive direc-tion.)
Given that the theoretical maximum agree-ments for the two systems are 84% and 86% (i.e.,regardless of polarity), there is considerable in-crease in the subject agreement.It is worth noting that many subjects made thenumber agreement mistake due to proximity.
Inthe previous example, ?The reason we are doingthese i[nclude]?
?, the subjects made the incorrectagreement linking ?include?
to ?these?
instead oflinking to ?the reason.?
While these cases are notprevalent, this is one reason (among many) thatthe theoretical maximum agreement is not 100%.4.2  System?s vs.
Subjects?
PerspectiveAlthough the agreement between the systems andthe subjects were high, no system achieved perfectagreement?many words were considered un-grammatical extensions of the partial sentences.We see two possible explanations: 1) the dis-agreeable output was erroneous; or 2) the dis-agreeable output was grammatical but judged asungrammatical under certain conditions.We manually examined the parse trees of the?disagreeable?
cases from our system.
Interest-ingly, in most cases, we found there exists a rea-sonable parse tree leading to a grammatical sen-tence.
We thus conclude that grammaticalityjudgements of partial sentences might not com-pletely reflect the underlying improvement of theword completion quality.
That is, discrepanciesbetween human and computer judgement need notpoint to a poor quality syntax filter; instead, itmay indicate that the system is exhibiting correctbehaviour but simply disagrees with subjects onthe particular grammatical cases in question.
Insuch cases, subjects?
disagreement with the sys-tem does not provide sufficient grounds for mak-ing modifications to the system?s behaviour.Rather, it is worth examining the factors leadingto the subjects?
perception of a word as an un-grammatical extension of a partial sentence.5 DiscussionOverall, our results indicate that our subjectsagree with the grammaticality of word completionmore when syntactic filtering is used than not.20That said, in light of the disagreeable cases, webelieve that the quality of word completion maynot be so straightforwardly evaluated.5.1 Selectional RestrictionTake this example, ?The plane carrying the sol-diers a___.?
The next word ?are?
was unani-mously considered ungrammatical by our humanjudges.
Consider the following full sentence ver-sion of it: ?The plane carrying the soldiers arecontemplating is too difficult a task.?
In this case,the subject is ?the plane carrying?
(as an activity),the relative clause is ?the soldiers are contemplat-ing?, and finally, the verb phrase is ?is too diffi-cult a task.?
This sentence may be difficult to in-terpret but a meaningful interpretation is possiblesyntactically and semantically.Consider the following variation, ?The politicalsituation the soldiers a___.?
In this case, it is notdifficult to conceive that ?are?
is a possible nextword, as in ?The political situation the soldiers arediscussing is getting worse.?
The syntactic con-struction is [noun phrase] [relative clause] [verbphrase].
Both partial sentences have a potentialgrammatical parse.
Why then is one consideredgrammatical and the other not?Sentences that induce midpoint reading diffi-culties in humans are well known in psycholin-guistics and are referred to as garden-path sen-tences (Frazier, 1978).
Reading ?the plane carry-ing the soldiers?
induces an expectation in thereader?s mind that the sentence is about the planedoing the carrying, and not about the carrying ofthe plane by the soldiers, leading to a ?short cir-cuit?
at the word ?are.
?In linguistics and CL, one aspect of this phe-nomenon, selectional restriction, has been ex-plored previously (most notably Levin, 1993 andResnik, 1995).
Selectional restriction is definedas the semantics of a verb restricting the type ofwords and phrases that can occur as its arguments.Essentially, the meaning of the verb makes an im-pact on what is possible syntactically and seman-tically.
What we observe here is a generalizedcase where it is no longer only about a verb plac-ing syntactic and semantic restrictions on its sur-rounding words.
Instead, we observe how a wordor a number of words influencing the semanticinterpretation, and in turn impacting on the per-ception of grammaticality of the next word (cf.
hitrate issues in section 2).5.2  Evaluation ApproachAlthough our original intent was to study thegrammaticality of word completion, ultimately thequestion is what impacts on the quality of wordcompletion.
It is without a doubt that the gram-maticality of the next word suggestions impactson the perception of the quality of word comple-tion.
However, we believe the key hinges onwhose perspective of quality is considered, whichthen becomes a usability issue.Recall that word completion is designed to aidthe writing process.
The curious part of ourevaluation was that we devised it as a grammati-cality judgement task via reading.
Is grammati-cality different when one is reading vs. writing?We consider this issue in two ways.Partial Sentences vs. Full SentencesLet us revisit our garden-path example:1a.
The plane carrying the soldiers a[re]?1b.
The plane carrying the soldiers arecontemplating is not that difficult a task.2a.
The political situation the soldiers a[re]?2b.
The political situation the soldiers arelosing sleep over is getting worse.In sentences 1a and 2a, readers have no choice butto judge the grammaticality of ?are?
based on theexisting partial sentence.
Depending on thereader?s creativity, one may or may not anticipatepotential full sentences such as 1b and 2b.
In con-trast, consider an alternative experimental setupwhere the readers were offered full sentences suchas 1b and 2b and were asked to judge the gram-maticality of ?are.?
Given the complexity of thesentences (selectional restriction aside), the read-ers would have no choice but to consider the exis-tence of a relative clause, which should increasethe likelihood of evaluating ?are?
as a grammati-cal component of the sentence.Reading vs. WritingNow we have observed the potential impact ongrammaticality judgements of a potential nextword when reading a partial sentence vs. a fullsentence.
That said, it needs emphasizing that the21key issue is to evaluate the quality of a suggestednext word given a partial sentence, not grammati-cality in complete isolation.
When a user usesword completion, he/she is actively engaged in thewriting process.
No software can truly predict theintent of the writer; the full sentence is waiting tobe written and cannot be written a priori.Consider someone who is in the process ofwriting the sentence ?The plane carrying the sol-diers??
Is this writer likely to be debating inhis/her head whether the sentence is about theplane that does the carrying or ?plane carrying?
asan activity?
Clearly, the writer?s intent is clear tothe writer him/herself.
In contrast, a sentence maybe perfectly grammatical and semantically rea-sonable, yet a reader may still find it ambiguousand/or difficult to read.
In other words, the per-ception of grammaticality of a next word dependson the task (reading vs. writing).
This is not tosay that our evaluation task is compromised as aresult.
Despite that the general grammar rules donot change, our reading judgements depending onthe context (e.g., partial vs. full sentence) suggeststhat the reading perspective only provide a partialpicture on the quality of output that is intended fora writing task.
In our case, higher quality syntac-tic filtering (e.g., our parser here) may not lead togreater usability.6 Concluding RemarksIn this paper, we have shown that the quality ofword completions depends on the perspective onetakes.
Considering that AAC is to aid someone inproducing content for communication, i.e., forthird-party consumption, the reading-writing di-chotomy is too serious an issue to ignore.
Thisissue has received some CL attention (Morris,2004, 2010; Hirst, 2008, 2009) but has not beendiscussed in the AAC literature (Tsang et al,2010).
The question remains, how do we thenevaluate, and more generally, design and use anAAC application?We believe the issue is far from clear.
Take ourcurrent focus?grammaticality of word comple-tion.
If the form of the content produced is un-grammatical or difficult to read from the perspec-tive of a reader, you risk having the reader misun-derstand the writer?s intent.
However, from thewriter?s perspective, unless he/she is perceptive ofthe interpretation problems with his/her potentialreaders, there is no incentive to produce content assuch; the writer can only produce content basedon his/her previous linguistic experience.One may argue that corpus statistics may bestcapture human linguistic behaviour.
For example,hit rate statistics using existing corpora is onesuch way of assessing the quality of word comple-tion.
However, corpora tell only one half of thestory?only the writing half is captured, the inter-pretation issues from the reading side are rarelycaptured, if at all.More important, the design of word completionis setup in a way that the task consists of both areading component and a writing one?the appro-priateness of suggested words is assessed by thewriter via reading during the writing task.
In fact,this is not merely a case of reading vs. writing, butrather, an issue of relevance depending on the lin-guistic context as well as the user?s perception ofit.
Traditionally, researchers in CL and psycho-linguistics have attempted to deal with humanprocessing of linguistic content at various levels(cf.
the CUNY Conference on Human SentenceProcessing, e.g., Merlo and Stevenson, 2002).However, no computational means is truly privyto the content behind the linguistic form.
Content,ultimately, resides in the reader?s or the writer?shead, i.e., intent.
The question remains how bestto design AAC to aid someone to communicatethis content.In summary, in our grammaticality judgementtask, incorporating syntax in word completionimproves the perceived quality of word choices.That said, it is unclear how quality relates to us-ability.
Indeed, the evaluation is far from conclu-sive in that it only captures the reader?s perspec-tive and not the writer?s.
Currently, we are notaware of the existence of a purely writer-basedevaluation for grammaticality of word completion(see Lesher et al, 2002 for one curious attempt).More generally, the reader-writer (or speaker-listener) dichotomy is unexplored in AAC re-search and should be considered more seriouslybecause communication (as text, speech, or oth-erwise) involves multiple people producing andconsuming content, where the perception of con-tent differs considerably.
The challenge of AACmay lie in bridging the gap between productionand consumption where communication is neitheronly about communicating intent nor making in-terpretations.22AcknowledgmentsThis project is funded by Quillsoft Ltd.  We alsowish to thank Jiafei Niu (University of Toronto)for conducting our usability study and FrankRudzicz (University of Toronto) for providinghelpful comments.ReferencesJohn Bentrup.
1987.
Exploiting Word Frequencies andtheir Sequential Dependencies.
Proceedings ofRESNA 10th Annual Conference, 121?122.Afsaneh Fazly and Graeme Hirst.
2003.
Testing theEfficacy of Part-of-Speech Information in WordCompletion.
Proceedings of the 2003 EACL Work-shop on Language Modeling for Text Entry Meth-ods, 9?16.Lyn Frazier.
1978.
On Comprehending Sentences: Syn-tactic Parsing Strategies.
Ph.D. Thesis, Universityof Connecticut.Ebba Gustavii and Eva Pettersson.
2003.
A SwedishGrammar for Word Prediction.
Master?s Thesis,Department of Linguistics, Uppsala University.Graeme Hirst.
2008.
The Future of Text-Meaning inComputational Linguistics.
In Proceedings of the11th International Conference on Text, Speech andDialogue, 1?9.Graeme Hirst.
2009.
Limitations of the Philosophy ofLanguage Understanding Implicit in ComputationalLinguistics.
In Proceedings of the Seventh EuropeanConference on Computing and Philosophy, 108?109.Sheri Hunnicutt and Johan Carlberger.
2001.
Improv-ing Word Prediction Using Markov Models andHeuristic Methods.
Augmentative and AlternativeCommunication, 17(4):255?264.Heidi Koester.
1994.
User Performance with Augmen-tative Communication Systems: Measurements andModels.
Ph.D. thesis, University of Michigan.Gregory W. Lesher, Bryan J. Moulton, D. JefferyHigginbotham, and Brenna Alsofrom.
2002.
Limitsof Human Word Prediction Performance.
In Pro-ceedings of 2002 CSUN Conference.Beth Levin.
1993.
English Verb Classes and Alterna-tions: A Preliminary Investigation.
University ofChicago Press.Kathleen F. McCoy.
1998.
The Intelligent Word Pre-diction Project.
University of Delaware.http://ww.asel.udel.edu/nli/nlp/wpredict.htmlPaola Merlo and Suzanne Stevenson, Eds.
2002.
TheLexical Basis of Sentence Processing: Formal,Computational and Experimental Issues.
John Ben-jamins Publishing Company.Jane Morris.
2004.
Readers?
Interpretations of LexicalCohesion in Text.
Conference of the Canadian As-sociation for Information Science, Winnipeg, Mani-toba.Jane Morris.
2010.
Individual Differences in the Inter-pretation of Text: Implications for Information Sci-ence.
Journal of the American Society for Informa-tion Science and Technology, 61(1):141?149.Tom Nantais, Fraser Shein, and Mattias Johansson.2001.
Efficacy of the word prediction algorithm inWordQ.
In Proceedings of the 2001 RESNA AnnualConference, 77?79.Paula S. Newman.
2007.
RH: A Retro Hybrid Parser.In Proceedings of the 2007 NAACL Conference,Companion, 121?124.Alfred Renaud.
2002.
Diagnostic Evaluation Measuresfor Improving Performance of Word Prediction Sys-tems.
Master?s Thesis, School of Computer Science,University of Waterloo.Alfred Renaud, Fraser Shein, and Vivian Tsang.
2010.A Symbolic Approach to Parsing in the Context ofWord Completion.
In Preparation.Philip Resnik.
1995.
Selectional Constraints: An In-formation-Theoretic Model and its ComputationalRealization.
Cognition, 61:127?125.Cynthia Tam and David Wells.
2009.
Evaluating theBenefits of Displaying Word Prediction Lists on aPersonal Digital Assistant at the Keyboard Level.Assistive Technology, 21:105?114.Vivian Tsang and Kelvin Leung.
2010.
An EcologicalPerspective of Communication With or WithoutAAC Use.
In Preparation.Matthew Wood.
1996.
Syntactic Pre-Processing inSingle-Word Prediction for Disabled People.
Ph.D.Thesis, Department of Computer Science, Univer-sity of Bristol.William Woods.
1970.
Transition Network Grammarsfor Natural Language Analysis.
Communications ofthe ACM, 13(10):591?606.23
