A Language Independent Method for Question ClassificationThamar Solorio1, Manuel Pe?rez-Coutin?o1, Manuel Montes-y-Go?mez1,2Luis Villasen?or-Pineda1 and Aurelio Lo?pez-Lo?pez11Language Technologies Group, Computer Science DepartmentNational Institute of Astrophysics, Optics and Electronics72840 Tonantzintla, Puebla,Mexico2Departamento de Sistemas Informa?ticos y Computacio?nUniversidad Polite?cnica de ValenciaEspan?a{thamy,mapco,mmontesg,villasen,allopez}@inaoep.mxAbstractPrevious works on question classification arebased on complex natural language process-ing techniques: named entity extractors,parsers, chunkers, etc.
While these ap-proaches have proven to be effective theyhave the disadvantage of being targeted to aparticular language.
We present here a sim-ple approach that exploits lexical featuresand the Internet to train a classifier, namelya Support Vector Machine.
The main fea-ture of this method is that it can be appliedto different languages without requiring ma-jor modifications.
Experimental results ofthis method on English, Italian and Span-ish show that this approach can be a prac-tical tool for question answering systems,reaching a classification accuracy as high as88.92%.1 IntroductionOpen-domain Question Answering (QA) sys-tems are concerned with the problem of tryingto answer questions from users posed in nat-ural language.
What makes these systems avery complex and interesting research area isthat the answers they retrieve must be concise,as opposed to traditional search engines that inresponse to a user query retrieve a list of docu-ments believed to contain the answer.
More-over, current evaluation environments of QAsystems, such as TREC QA track (Voorhees,2001) and CLEF (Peters et al, 2003), restrictthe size of the answers to a maximum of 50bytes.
Given the complexity involved in thisproblem, traditional approaches to QA take adivide-and-conquer strategy, where the prob-lem is divided into several less complex subtasksthat combined lead to the resolution of the ques-tions.
An important subtask of a QA systemis question analysis, since it can provide usefulclues for identifying potential answers in a largecollection of texts.
For instance, Question Clas-sification is concerned with assigning semanticclasses to questions.
This semantic classifica-tion can be used to reduce the search space ofpossible answers, i.e.
if we can determine thatthe question Who is the Italian Prime Minis-ter?
belongs to the semantic category PER-SON, then we only need to look for instances oftype PERSON as possible answers.
Clearly, theadvantage of such classification relies on hav-ing the ability of extracting from the documentssuch instances.
In other words, a good questionclassification module may be useless if we lackan accurate named entity extractor for the doc-ument collection.Results of the error analysis of an open-domain QA system showed that 36.4% of theerrors were generated by the question classifi-cation module (Moldovan et al, 2003).
Thusit is not surprising that an increasing interesthas arisen aimed at developing accurate ques-tion classifiers (Zhang and Lee, 2003; Li andRoth, 2002; Suzuki et al, 2003).
However, mostof these approaches are targeted to the Englishlanguage.
Besides, the machine learning algo-rithms used are trained on features extracted bynatural language processing tools that are lan-guage dependent, and for some languages thesetools are not available.
This implies that if wewant to reproduce the results of these methodsin a different language we need first to solve theproblem of making available the appropriate an-alyzers in the given language.We present here a flexible method for ques-tion classification.
We claim that the methodis language-independent since no complex nat-ural language processing tools are needed; weuse plain lexical features that can be extractedautomatically from the questions.
A machinelearning algorithm that has proven to performwell over high dimensional data, is trained onprefixes of words and on additional attribute in-formation gathered automatically from the In-ternet.
The method was evaluated experimen-tally, achieving high accuracy on questions inthree different languages: English, Italian andSpanish.The next section briefly summarizes some ofthe previous approaches for question classifica-tion.
Section 3 presents the learning scenarioof this work, together with a brief introductionto Support Vector Machines (SVM).
Section 4shows our experimental results and we concludewith a discussion of this work and ideas for fu-ture research in Section 5.2 Related WorkMost approaches to question classification arebased on handcrafted rules (Voorhees, 2001).It is not until recently that machine learn-ing techniques are being used to tackle theproblem of question classification.
In (Zhangand Lee, 2003) they present a new methodfor question classification using Support VectorMachines.
They compared accuracy of SVMagainst Nearest Neighbors, Naive Bayes, De-cision Trees and Sparse Network of Winnows(SNoW), with SVM producing the best results.In their work, Zhang and Sun Lee improve accu-racy by introducing a tree kernel function thatallows to represent the syntactic structure ofquestions.
Their experimental results show thatSVM using this tree kernel function achieves anaccuracy of 90%, however, a parser is needed inorder to acquire the syntactic information.Li and Roth reported a hierarchical approachfor question classification based on the SNoWlearning architecture (Li and Roth, 2002).
Thishierarchical classifier discriminates among 5coarse classes, which are then refined into 50more specific classes.
The learners are trainedusing lexical and syntactic features such as postags, chunks and head chunks together with twosemantic features: named entities and seman-tically related words.
They reported questionclassification accuracy of 98.80% for a coarseclassification, using 5,500 instances for training.A different approach, used for Japanese ques-tion classification, is that of Suzuki et al(Suzuki et al, 2003).
They used SVM whitha new kernel function, called Hierarchical Di-rected Acyclic Graph, which allows the use ofstructured data.
They experimented with 68question types and compared performance of us-ing bag-of-words against using more elaboratedcombinations of attributes, namely named en-tities and semantic information.
Their best re-sults, an accuracy of 94.8% at the first level ofthe hierarchy, were obtained when using SVMtrained on bag-of-words together with namedentities and semantic information.The idea of using the Internet in a QA sys-tem is not new.
What is new, however, is thatwe are using the Internet to obtain values forfeatures in our question classification process,as opposed to previous approaches where theredundancy of information available on the In-ternet has been used in the answer extractionprocess (Brill et al, 2002; Lin et al, 2002; Katzet al, 2003).3 Learning Question ClassifiersQuestion classification is very similar to textclassification.
One thing they have in commonis that in both cases we need to assign a class,from a finite set of possible classes, to a naturallanguage text.
Another similarity is attributeinformation; what has been used as attributesfor text classification can also be extracted andused in question classification.
Finally, in bothcases we have high dimensional attributes: if wewant to use the bag-of-words approach, we willface the problem of having very large attributesets.An important difference is that question clas-sification introduces the problem of dealing withshort sentences, compared with text documents,and thus we have less information available oneach question instance.
This is the reason whyquestion classification approaches are trying touse other information (e.g.
chunks and namedentities) besides the words within the questions.However, the main disadvantage of relying onsemantic analyzers, named entity taggers andthe like, is that for some languages these toolsare not yet well developed.
Plus, most of themare very sensitive to changes in the domain ofthe corpus; and even if these tools are accu-rate, in some cases acquiring one for a partic-ular language may be a difficult task.
This isour prime motivation for searching for differ-ent, more easier to gather, information to solvethe question classification problem.
Our learn-ing scenario considers as attribute informationprefixes of words in combination with attributeswhose values are obtained from the Internet.These Internet based attributes are targeted toextract evidence of the possible semantic classof the question.The next subsection will explain how the In-ternet is used to extract attributes for our ques-tion classification problem.
In subsection 3.2we present a brief description of Support Vec-tor Machines, the learning algorithm used onour experiments.3.1 Using InternetAs Kilgarriff and Grefenstette wrote, the In-ternet is a fabulous linguists?
playground (Kil-garriff and Grefenstette, 2003).
It has becomethe greatest information source available world-wide, and although English is the dominantlanguage represented on the Internet it is verylikely that one can find information in almostany desired language.
Considering this, and thefact that the texts are written in natural lan-guage, we believe that new methods that takeadvantage of this large corpus must be devised.In this work we propose using the Internet in or-der to acquire information that can be used asattributes in our classification problem.
This at-tribute information can be extracted automat-ically from the web and the goal is to providean estimate about the possible semantic class ofthe question.The procedure for gathering this informationfrom the web is as follows: we use a set of heuris-tics to extract from the question a word w, orset of words, that will complement the queriessubmitted for the search.
We then go to a searchengine, in this case Google, and submit queriesusing the word w in combination with all thepossible semantic classes for our purpose.
Forinstance, for the question Who is the Presidentof the French Republic?
we extract the wordPresident using our heuristics, and run 5 queriesin the search engine, one for each possible class.These queries take the following form:?
?President is a person??
?President is a place??
?President is a date??
?President is a measure??
?President is an organization?We count the number of results returned byGoogle for each query and normalize them bytheir sum.
The resultant numbers are the val-ues for the attributes used by the learning algo-rithm.
As can be seen, it is a very straightfor-ward approach, but as the experimental resultswill show, this information gathered from theInternet is quite useful.
In Table 1 we presentthe figures obtained from Google for the ques-tion presented above, column Results show thenumber of hits returned by the search engineand in column Normalized we present the num-ber of hits normalized by the total of all resultsreturned for the different queries.An additional advantage of using the Internetis that by approximating the values of attributesin this way, we take into account words or en-tities belonging to more than one class (poly-semy).Now that we have introduced the use of theInternet in this work, we continue describing theset of heuristics that we use in order to performthe web search.3.1.1 HeuristicsWe begin by eliminating from the questionsall words that appear in our stop list.
Thisstop list contains the usual items: articles,prepositions and conjunctions plus all theinterrogative adverbs and all lexical formsof the verb ?to be?.
The remaining wordsare sent to the search engine in combina-tion with the possible semantic classes, asdescribed above.
If no results are returnedfor any of the semantic classes we then starteliminating words from right to left until thesearch engine returns results for at least oneof the semantic categories.
As an exampleconsider the question posed previously: Whois the President of the French Republic?
weeliminate the words from the stop list andthen formulate queries for the remainingwords.
These queries are of the following form:?President French Republic is a si?
where s ?
{Person,Organization, P lace,Date,Measure}.The search engine did not return any resultsfor this query, so we start eliminating wordsfrom right to left.
The query is now like this:?President French is a si?
and given thatagain we have no results returned we finallyformulate the last possible query: ?President isa si?
which returns results for all the semanticclasses except for Date.Being heuristics, we are aware that in somecases they do not work well.
Nevertheless,for the vast majority of the cases they pre-sented surprisingly good results, in the threeQuery Results Normalized?President is a person?
259 0.8662?President is a place?
9 0.0301?President is an organization?
11 0.0368?President is a measure?
20 0.0669?President is a date?
0 0Table 1: Example of using the Internet to extract features for question classificationClass Number of InstancesPerson 91Organization 41Measure 103Date 64Object 12Other 54Place 85Table 2: Distribution of semantic classes for theDISEQuA corpuslanguages, as shown in the experimental eval-uation.3.2 Support Vector MachinesGiven that Support Vector Machines haveproven to perform well over high dimensionalitydata they have been successfully used in manynatural language related applications, such astext classification (Joachims, 1999; Joachims,2002; Tong and Koller, 2001) and named entityrecognition (Mitsumori et al, 2004; Solorio andLo?pez, 2004).
This technique uses geometricalproperties in order to compute the hyperplanethat best separates a set of training examples(Stitson et al, 1996).
When the input spaceis not linearly separable SVM can map, by us-ing a kernel function, the original input spaceto a high-dimensional feature space where theoptimal separable hyperplane can be easily cal-culated.
This is a very powerful feature, be-cause it allows SVM to overcome the limitationsof linear boundaries.
They also can avoid theover-fitting problems of neural networks as theyare based on the structural risk minimizationprinciple.
The foundations of these machineswere developed by Vapnik, for more informa-tion about this algorithm we refer the reader to(Vapnik, 1995; Scho?lkopf and Smola, 2002).4 Experimental Evaluation4.1 Data setsThe data set used in this work consists of thequestions provided in the DISEQuA Corpus(Magnini et al, 2003).
Such corpus was madeup of simple, mostly short, straightforward andfactual queries that sound naturally sponta-neous, and arisen from a real desire to knowsomething about a particular event or situation.The DISEQuA Corpus contains 450 questions,each one formulated in four languages: Dutch,English, Italian and Spanish.
The questionsare classified into seven categories: Person, Or-ganization, Measure, Date, Object, Other andPlace.
The experiments performed in this workused the English, Italian and Spanish versionsof these questions.4.2 ExperimentsIn the experiments performed in this work weused the evaluation technique 10-fold cross-validation which consists of randomly dividingthe data into 10 equally-sized subgroups andperforming 10 different experiments.
We sep-arated nine groups together with their originalclasses as the training set, the remaining groupwas considered the test set.
Each experimentconsists of ten runs of the procedure describedabove, and the overall average are the resultsreported here.In our experiments we used the WEKA imple-mentation of SVM (Witten and Frank, 1999).In this setting multi-class problems are solvedusing pairwise classification.
The optimizationalgorithm used for training the support vec-tor classifier is an implementation of Platt?s se-quential minimal optimization algorithm (Platt,1999).
The kernel function used for mappingthe input space was a polynomial of exponentone.The most common approach to question clas-sification is bag-of-words, so we decided to com-pare results of using bag-of-words against usingjust prefixes of the words in the questions.
InLanguage Words Prefix-5 Prefix-4 InternetENGLISH 81.77% 81.32% 80.21% 67.77%ITALIAN 88.03% 87.59% 88.70% 60.79%SPANISH 79.90% 81.45% 76.97% 68.86%Table 3: Experimental results of accuracy when training SVM with words, prefixes and Internet-based attributesorder to choose an appropriate prefix size wecompute the average length of the words in thethree languages used in this work.
For Englishthe average length of words is 4.62, for Italianis 4.8 while for Spanish the average length is4.75.
So we decided to experiment with pre-fixes of size 4 and 5.
In Table 3 we can see acomparison of classification accuracy of train-ing SVM using all the words in the questions,using prefixes of size 4 and 5 and using onlythe Internet-based attributes.
As we can see forEnglish the best results were obtained when us-ing words as attributes, although the differencebetween using just prefixes and using words isnot so large.
For Spanish however, the best re-sults were achieved when using prefixes of size5.
This can be due to the fact that some ofthe interrogative words, that by themselves candefine the semantic class of questions in thislanguage, such as Cua?ndo (When) and Cua?nto(How much) could be considered as the sameprefix of size 4 i.e.
Cua?n.
But if we considerprefixes of size 5, then these two words will formtwo different prefixes: Cua?nd and Cua?nt, thusreducing the loss of information, as oppose tousing prefixes of size 4.
For Italian languagethe best results were obtained from using pre-fixes of size 4.
And for the three languages theInternet-based attributes had rather low accu-racies, the lowest being for Italian.
When weanalyzed the results computed for Italian, us-ing our Internet-based attributes, we realizedthat in many cases we could not get any re-sults to the queries.
One plausible explanationfor this lack of information, is that the num-ber of Italian documents available on Internetis much smaller than for English and Spanish.Estimates reported in (Kilgarriff and Grefen-stette, 2003) show that for Italian the web sizein words is 1,845,026,000; while for English andSpanish the web sizes are 76,598,718,000 and2,658,631,000 respectively.
Thus our methodwas not able to extract as much information asfor the other two languages.4.3 Combining Internet-basedAttributes with Lexical FeaturesResults presented in the previous subsectionshow how by using just lexical information wecan train SVM and achieve high accuracies inthe three languages.
But our goal is to discoverthe usefulness of using Internet in order to ex-tract attributes for question classification.
Weperformed other experiments combining the lex-ical attributes with the Internet information inorder to discover if we can further improve ac-curacy.
Table 4 show experimental results ofthis attribute combination and Figure 1 showsa graphical representation of these results.It is interesting to note that for English andSpanish we did gain accuracy when using theInternet features in all the cases.
In contrast,for Italian classification accuracy was decreasedwhen incorporating Internet-based attributes towords and prefixes of size 5.
We believe that thisdrop in accuracy for Italian may be due to theweakly supported information extracted fromthe Internet, Table 3 shows that SVM trainedonly on the coefficients from the Internet per-formed worse for Italian.
It is not surprisingthat adding this rather sparse information tothe attributes in the Italian language did notproduce an advantage in the classifiers perfor-mance.5 ConclusionsWe have presented here experimental results ofa language independent question classificationmethod.
The method is claimed to be lan-guage independent since the features used asattributes in the learning task can be extractedfrom the questions in a fully automated manner;we do not use semantic or syntactic informa-tion because otherwise we will be restricted towork on languages for which we do have parsersthat can extract this information.
We believethat this method can be successfully appliedto other languages, such as Romanian, French,Portuguese and Catalan, that share the mor-phologic characteristics of the three languagesLanguage Words+Internet Prefix-5 + Internet Prefix-4 + InternetENGLISH 82.88% 82.66% 83.55%ITALIAN 87.34% 86.93% 88.92%SPANISH 83.43% 84.09% 81.45%Table 4: Experimental results combining Internet-valued attributes with words and prefixesInternetWordsPrefix-5Prefix-4Words+InternetPrefix-5+InternetPrefix-4+Internet60657075808590SpanishEnglishItalianFigure 1: Graphical comparison of question classification accuraciestested here.Comparing our results with those of previousworks we can say that our method is promis-ing.
For instance Zhang and Sun Lee (Zhangand Lee, 2003) reported an accuracy of 90% forEnglish questions, while Li and Roth (Li andRoth, 2002) achieved 98.8% accuracy.
How-ever, they used a training set of 5,500 questionsand a test set of 500 questions, while in our ex-periments we used for training 405 for each 45test questions (10-fold-cross-validation).
WhenZhang and Sun Lee used only 1,000 questionsfor training they achieved an accuracy of 80.2%.It is well known that machine learning algo-rithms perform better when a bigger trainingset is available, so it is expected that experi-ments of our method with a larger training setwill provide improved results.As future work we plan to investigate activelearning with SVM for this problem.
Given thatmanually labelling questions is a very time con-suming task, active learning can provide a fasterapproach to build accurate question classifiers.Instead of randomly selecting question instancesto label manually and then provide them to thelearner, the learner can analyze the unlabeledinstances and select for labelling the instancesthat seem more relevant to the task.Another interesting line for future work is ex-ploring the advantage of using mixed languagescorpora lo learn question classification.
TheRomance languages, for instance, such as Ital-ian, French and Spanish have stems in common.Then it is feasible that questions for several lan-guages may help to train a classifier for a differ-ent language.
The advantage of this idea will bethe availability of larger corpora for languagesfor which a large enough corpus is not available,counting in favor of languages that are under-represented on the Internet.
We could circum-vent this lack of presence on the Internet forsome languages by using information availableon other, more well represented, languages.6 AcknowledgementsWe would like to thank CONACyT for par-tially supporting this work under grants 166934,166876 and U39957-Y, and Secretar?
?a de Estadode Educacio?n y Universidades de Espan?a.ReferencesE.
Brill, S. Dumais, and M. Banko.
2002.
Ananalysis of the AskMSR question-answeringsystem.
In 2002 Conference on EmpiricalMethods in Natural Language Processing.T.
Joachims.
1999.
Transductive inference fortext classification using support vector ma-chines.
In Proceedings of the Sixteenth In-ternational Conference on Machine Learning(ICML), pages 200?209.
Morgan Kaufmann.T.
Joachims.
2002.
Learning to Classify Textusing Support Vector Machines: MethodsTheory and Algorithms, volume 668 of TheKluwer International Series in Engineeringand Computer Science.
Kluwer AcademicPublishers.B.
Katz, J. Lin, D. Loreto, W. Hilde-brandt, M. Bilotti, S. Felshin, A. Fernandes,G.
Marton, and F. Mora.
2003.
Integrat-ing web-based and corpus-based techniquesfor question answering.
In Twelfth Text RE-trieval Conference (TREC 2003), Gaithers-burg, Maryland, November.A.
Kilgarriff and G. Grefenstette.
2003.
Intro-duction to the special issue on the web as cor-pus.
Computational Linguistics, 29(3):333?347.X.
Li and D. Roth.
2002.
Learning questionclassifiers.
In COLING?02.J.
Lin, A. Fernandes, B. Katz, G. Marton,and S. Tellex.
2002.
Extracting answersfrom the web using knowledge annotation andknowledge mining techniques.
In EleventhText REtrieval Conference (TREC 2002),Gaithersburg, Maryland, November.B.
Magnini, S. Romagnoli, A. Vallin, J. Her-rera, A.
Pen?as, V. Peinado, F. Verdejo, andM.
de Rijke.
2003.
Creating the DISEQuAcorpus: a test set for multilingual questionanswering.
In Carol Peters, editor, WorkingNotes for the CLEF 2003 Workshop, Trond-heim, Norway, August.T.
Mitsumori, S. Fation, M. Murata, K. Doi,and H. Doi.
2004.
Boundary correction ofprotein names adapting heuristic rules.
InAlexander Gelbukh, editor, Fifth Interna-tional Conference on Intelligent Text Process-ing and Computational Linguistics, CICLing2004, volume 2945 of Lecture Notes in Com-puter Science, pages 172?175.
Springer.D.
Moldovan, M. Pas?ca, S. Harabagiu, andM.
Surdeanu.
2003.
Performance issues anderror analysis in an open-domain questionanswering system.
ACM Trans.
Inf.
Syst.,21(2):133?154.C.
Peters, M. Braschler, J. Gonzalo, andM.
Kluck, editors.
2003.
Advances inCross-Language Information Retrieval, ThirdWorkshop of the Cross-Language EvaluationForum, CLEF 2002.
Rome, Italy, September19-20, 2002.
Revised Papers, volume 2785 ofLecture Notes in Computer Science.
Springer.J.
Platt.
1999.
Fast training of support vec-tor machines using sequential minimal op-timization.
In Advances in Kernel Meth-ods ?Support Vector Learning, (B. Scho?lkopf,C.J.C.
Burges, A.J.
Smola, eds.
), pages 185?208, Cambridge, Massachusetts.
MIT Press.B.
Scho?lkopf and A. J. Smola.
2002.
Learningwith Kernels: Support Vector Machines, Reg-ularization, Optimization and Beyond.
MITPress.T.
Solorio and A.
Lo?pez Lo?pez.
2004.
Learn-ing named entity classifiers using support vec-tor machines.
In Alexander Gelbukh, editor,Fifth International Conference on IntelligentText Processing and Computational Linguis-tics, CICLing 2004, volume 2945 of LectureNotes in Computer Science, pages 158?167.Springer.M.
O. Stitson, J.
A. E. Wetson, A. Gammer-man, V. Vovk, and V. Vapnik.
1996.
Theoryof support vector machines.
Technical ReportCSD-TR-96-17, Royal Holloway University ofLondon, England, December.J.
Suzuki, H. Taira, Y. Sasaki, and E. Maeda.2003.
Question classification using HDAGkernel.
In Workshop on Multilingual Summa-rization and Question Answering 2003, pages61?68.S.
Tong and D. Koller.
2001.
Support vectormachine active learning with applications totext classification.
Journal of Machine Learn-ing Research, 2:45?66.V.
Vapnik.
1995.
The Nature of Statisti-cal Learning Theory.
Number ISBN 0-387-94559-8.
Springer, N.Y.E.
Voorhees.
2001.
Overview of the TREC2001 question answering track.
In Proceed-ings of the 10th Text REtrieval Conference(TREC01), NIST, pages 157?165, Gaithers-burg, MD.I.
H. Witten and E. Frank.
1999.
Data Mining,Practical Machine Learning Tools and Tech-niques with Java Implementations.
The Mor-gan Kaufmann Series in Data ManagementSystems.
Morgan Kaufmann.D.
Zhang and W. Sun Lee.
2003.
Questionclassification using support vector machines.In Proceedings of the 26th Annual Interna-tional ACM SIGIR Conference on Researchand Development in Information Retrieval,pages 26?32, Toronto, Canada.
ACM Press.
