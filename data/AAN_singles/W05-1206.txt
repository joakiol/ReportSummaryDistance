Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 31?36,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsLocal Textual Inference: can it be defined or circumscribed?Annie ZaenenPalo Alto Research Center3333, Coyote Hill RoadPalo Alto, CA 94304zaenen@parc.comLauri KarttunenPalo Alto Research Center3333, Coyote Hill RoadPalo Alto, CA 94304karttunen@parc.comRichard CrouchPalo Alto Research Center3333, Coyote Hill RoadPalo Alto, CA 94304crouch@parc.comAbstractThis paper argues that local textual in-ferences come in three well-defined vari-eties (entailments, conventional implica-tures/presuppositions, and conversationalimplicatures) and one less clearly definedone, generally available world knowledge.Based on this taxonomy, it discusses someof the examples in the PASCAL text suiteand shows that these examples do not fallinto any of them.
It proposes to enlarge thetest suite with examples that are more di-rectly related to the inference patterns dis-cussed.1 IntroductionThe PASCAL initiative on ?textual entailment?
hadthe excellent idea of proposing a competition testingNLP systems on their ability to understand languageseparate from the ability to cope with world knowl-edge.
This is obviously a welcome endeavor: NLPsystems cannot be held responsible for knowledgeof what goes on in the world but no NLP system canclaim to ?understand?
language if it can?t cope withtextual inferences.
The task also shies away fromcreative metaphorical or metonymic use of languageand makes the assumption that referential assign-ments remain constant for entities that are describedin the same way.
These all seem good features of theproposal as it stands.Looking at the challenge as it was put before thecommunity, however, we feel that it might be usefulto try to circumscribe more precisely what exactlyshould count as linguistic knowledge.
In this paperwe make a stab at this in the hope of getting a discus-sion going.
For reasons that will become clear, weprefer to talk about TEXTUAL INFERENCES ratherthan about textual entailments when referring to thegeneral enterprise.
We first explicitate what we thinkshould be covered by the term textual inferences, wethen look at the PASCAL development suite in thelight of our discussion and we conclude with a shortproposal for extensions to the test suite.Before even starting at this, a point of clarificationneeds to be made: the correspondence of a linguis-tic object to an object in the real world goes beyondwhat can be learned from the text itself.
When some-body says or writes The earth is flat or The king ofFrance is bald because (s)he is a liar or ill-informed,nothing in these linguistic expressions in themselvesalerts us to the fact that they do not correspond to sit-uations in the real world (we leave texts in which theauthor signals consciously or unconsiously that he islying or fibbing out of consideration here.)
What thetext does is give us information about the stance itsauthor takes vis-a`-vis the events or states described.It is thus useful to distinguish between two ingre-dients that go into determining the truth value of anutterance, one is the trustworthiness of the uttererand the other is the stance of the utterer vis-a`-visthe truth of the content.
The latter we will call theveridicity of the content.
When we talk about tex-tual inferences we are only interested in veridicitynot in the truth which lies beyond what can be in-ferred from texts.
Or, maybe more realistically, weassume a trustworthy author so that veridical state-ments are also true.312 Varieties of local textual inferencesUnder this assumption of trustworthiness, semanticsand pragmatics as practiced by philosophers and lin-guists can give us some insights that are of practicalrelevance.
Work done in the last century has led re-searchers to distinguish between entailments, con-ventional implicatures and conversational implica-tures.
We describe these three classes of inferencesand illustrate why the distinctions are important forNLP.2.1 EntailmentsThe most uncontroversional textual inferences arethose that can be made on the basis of what is as-serted in a text.
If the author makes the statementthat Tony Hall arrived in Baghdad on Sunday night,then we can conclude that Tony Hall was in Bagh-dad on Sunday night (keeping referring expressionsconstant, as proposed in the PASCAL task).
The sec-ond sentence is true when the first is true (assum-ing we are talking about the same Tony Hall, thesame Baghdad and the same Sunday) just by virtueof what the words mean.In simple examples such as that in (1)(1) Bill murdered John.Bill killed John.one can go to a resource such as WordNet, look upmurder, discover that it means kill with some fur-ther conditions.
?Ontologies?
or thesauruses typi-cally order terms in a hierarchy that encodes a re-lation from less specific at the top of the hierarchyto more specific at the bottom.
In simple clausesthe replacement of a more specific term with a lessspecific one, ensures an upward monotonic relationbetween these sentences.
As is well known this re-lation is inversed when the sentences are negated.1(2) Bill didn?t murder John.does not entail Bill didn?t kill John.but(3) Bill didn?t kill John.does entail Bill didn?t murder John.Monotonicity relations also hold when adjectivalmodification is introduced as in (4)1A sentence is downward monotonic iff it remains true whenit is narrowed.
A sentence is upward monotonic when it remainstrue when it is broadened.
(4) Ames was a clever spy.entails Ames was a spy.Again negation reverses the entailment:(5) Ames wasn?t a spy.entails Ames wasn?t a clever spy.Quantifiers, easily among the most intensivelystudied lexical items, also exhibit upward or down-ward monotonicity.2 To give just one example:(6) All companies have to file annual reports.entails All Fortune 500 companies have to fileannual reports.but(7) All companies have to file annual reports.does not entail All companies have to file an-nual reports to the SEC.The fact that there are both upwards monotonicand downwards monotonic expressions means thatsimple matching on an inclusion of relevant mate-rial cannot work as a technique to detect entailments.Upward monotone expressions preserve truth byleaving out material whereas downward monotoneexpressions don?t: adding material to them can betruth preserving.3Apart from a more specific/less specific relation,lexical items can establish a part-subpart relation be-tween the events they describe.
If we followed thefirst sentence in (1) by(8) John died.we would still have a lexical inference.
In this caseone in which the event described in the second sen-tence is a subpart of the event described in the first.The investigation of entailments leads one to dis-tinguish several types of lexical items that have pre-dictable effects on meaning that can be exploited todiscover sentences that are inferentially related (byreal entailments in this case).
Other examples arescope bearing elements (an aspect of meaning thatoften leads to ambiguities which are not always eas-ily perceived) and perception reports.2A quantifier Q is downward monotonic with respect to itsrestrictor ?
iff ((Q ?)
?)
remains true when the ?
is narrowed,e.g.
from companies to Fortune 500 companies.
A quantifier Qis upward monotonic with respect to its scope ?
iff ((Q ?)
?
)remains true when ?
is broadened, e.g.
from have to file reportsto the SCE to just have to file reports.3Dagan and Glickman (2004) explore inferencing by syn-tactic pattern matching techniques but consider only upwardmonotonic expressions.
Their proposal ensures loss of recallon downward monotonic expressions.32Two types of relations deserve special mentionhere because they are pervasive and they are at theborderline between linguistic and world knowledge:temporal relations and spatial relations.
Whetherknowing that Tuesday follows Monday or that thereare leap years and non-leap years is linguistic knowl-edge or world knowledge might not be totally clearbut it is clear that one wants this information to bepart of what textual entailment can draw upon.
Theconsequences in a Eucledian space of the place andmovement of objects are similar.
There is a rich setof entailment relations that builds on these temporaland spatial notions.2.2 Conventional Implicatures4Apart from making assertions, however, an authorwill often ?conventionally implicate?
certain things.We use here the term conventional implicature forwhat has been called by that name or labeled as (se-mantic) presupposition.
Some of us have arguedelsewhere there is no need for a distinction betweenthese two notions (Karttunen and Peters, 1979) andthat presupposition is a less felicitous term becauseit tends to be confused with ?old information?.Traditionally these implications are not consid-ered to be part of what makes the sentence true, butthe author is COMMITTED to them and we considerthem part of what textual inferences should be basedon.
We take this position because we think it is rea-sonable, for IE tasks, to assume that material that isconventionally implicated can be used in the sameway as assertions, for instance, to provide answersto questions.
When somebody says Bill acknowl-edges that the earth is round, we know somethingabout the author?s as well as Bill?s beliefs in the mat-ter, namely that the author is committed to the beliefthat the earth is round.If all conventionally implied material were alsodiscourse old information, this might not matter verymuch as the same information would be availableelsewhere in the text, but often conventionally im-plied material is new information that is presentedas not being under discussion.
Conventional impli-catures are a rich source of information for IE tasksbecause the material presented in them is supposed4For more on conventional implicatures, see e.g.
Karttunenand Peters (1979) and Potts (2005)to be non-controversial.
In newspapers and other in-formation sources they are a favorite way to distin-guish background knowledge, that the reader mighthave or not, without confusing it with what is news-worthy in the report at hand.
A very common ex-ample of this, exploited in the PASCAL test suite, isthe use of appositives.
illustrated in the followingexample:(9) The New York Times reported that Hanssen,who sold FBI secrets to the Russians, could facethe death penalty.Did Hanssen sell FBI reports to the Russians?YESFrom the perspective of IE tasks, the way conven-tional implicatures behave under negation is one rea-son to pay close attention to them.
The followingexamples illustrate this:(10) Kerry realized that Bush was right.Bush was right.
(11) Kerry didn?t realize that Bush was right.Bush was right.Other types of embedded clauses that are conven-tionally implicated are temporal adverbials (exceptthose introduced by before or until.
Other types ofmaterial that can introduce a conventional implica-ture are adverbial expressions such as evidently andsimple adverbs such as again or still.It is important to point out that the syntactic struc-ture doesn?t guide the interpretation here.
Considerthe following contrast:(12) As the press reported, Ames was a successfulspy.conventionally implicates that Ames was a success-ful spy, but(13) According to the press, Ames was a successfulspy.does not.2.3 Conversational Implicatures5Authors can be held responsible for more than justassertions and conventional implicatures.
Conversa-tional implicatures are another type of author com-mitment.
A conversational implicature rests on theassumption that, in absence of evidence to the con-trary, a collaborative author will say as much as she5For more on conversational implicatures, see e.g.
Grice(1989) and Horn (2003)33knows.
So if Sue says that she has four children,we tend to conclude that she has no more than four.This type of implicature can be destroyed withoutany contradiction arising: He not only ate some ofthe cake, he ate all of it.
Within the context of a tex-tual inference task such as that defined in the PAS-CAL initiative, it is clear that inferences based onconversational implicatures might be wrong: PAS-CAL doesn?t give the context.
In a more developedtype of inference task, a distinction should be madebetween this type of inference and the ones we dis-cussed earlier, but when inferencing is reduced toone sentence it seems more reasonable to take gen-eralized conversational implicatures into account asbona fide cases of inferences (except of course ifthey are cancelled in the sentence itself, as in theexample above).
(14) I had the time to read your paper.conversationally implies that I read your paper.
Butit could be followed by but I decided to go play ten-nis instead.
(15) Some soldiers were killed.conversationally implies Not all soldiers were killed.But it could be cancelled by In fact we fear that allof them are dead.
(16) He certainly has three children.conversationally implies He doesn?t have more thanthree children but it could be followed by In fact hehas five, three daughters and two sons.Apart from the general conversational implica-tures, implicatures can also arise by virtue of some-thing being said or not said in a particular context.
Ifin a letter of recommendation, one praises the can-didate?s handwriting without saying anything abouthis intellectual abilities, this allows the reader todraw some conclusions.
We assume here that thistype of inference is not part of the PASCAL task, astoo little context is given for it to be reliably calcu-lated.One might agree with the analysis of varioussources of author commitment given above but beof the opinion that it doesn?t matter because, givenenough data, it will come out in the statistical wash.We doubt, however, that this will happen any timesoon without some help: the semantic distinctionsare rather subtle and knowing about them will helpdevelop adequate features for statistical training.It might also be thought that the generalizationsthat we need here can be reduced to syntactic dis-tinctions.
We don?t have the space to show in greatdetail that this is not the case but some reflectionon and experimentation with the examples giventhroughout this paper will convince the reader thatthis is not the cases.
For instance, if one replaces theadjective clever with the equally good adjective al-leged in (4) above, the entailment relation betweenthe sentences doesn?t hold anymore.
Substitutingshow for realize in (11) has the same effect.2.4 Some world knowledge?In our mind this exhausts the ways in which an au-thor can be held responsible for her writings on thebasis of text internal elements.
Textual inferencesare based on textual material that is either an en-tailment of what is explicitly asserted, or materialthat conventionally or conversationally implied bythe author.
These inferences can be made solely onthe basis of the way the meaning of the words andconstruction she uses are related to other words andconstructions in the language.
But even in a task thattries to separate out linguistic knowledge from worldknowledge, it is not possible to avoid the latter com-pletely.
There is world knowledge that underlies justabout everything we say or write: the societies welive in use a common view of time to describe eventsand rely on the assumptions of Euclidean geometry,leading to shared calendars and measurement sys-tems.
It would be impossible to separate these fromlinguistic knowledge.
Then there is knowledge thatis commonly available and static, e.g.
that Baghdadis in Iraq.
It seems pointless to us to exclude theappeal to such knowledge from the test suite but itwould be good to define it more explicitly.3 The PASCAL development suite.We now discuss some of the PASCAL developmentset examples in the light of the discussion above andexplain why we think some of them do not belongin a textual inference task.
First a number of PAS-CAL examples are based on spelling variants or evenspelling mistakes.
While it is clear that coping withthis type of situation is important for NLP applica-tions we think they do not belong in a textual infer-ence test bed.
We first discuss a couple of examples34that we think should not have been in the test suiteand then some that do not confirm to our view oninferencing but which might belong in a textual in-ference test suite.3.1 Errors?A problem arises with an example like the follow-ing:(17) A farmer who was in contact with cows suffer-ing from BSE ?
the so-called mad cow disease?
has died from what is regarded as the humanform of the disease.Bovine spongiform encephalopathy is anothername for the ?mad cow disease?.TRUEIf one googles BSE, one finds that it is an abbre-viation that can stand for many things, includingthe Bombay, Bulgarian, Baku or Bahrain Stock Ex-change, Breast Self-Examination, and Brain Sur-face Extractor.
To select the right alternative, oneneeds the knowledge that ?bovine spongiform en-cephalopathy?
is a name of a disease and the othercompeting BSE expansions are not.The authors of the PASCAL test suite don?t seemto allow for as much world knowledge when theymark the following relation as FALSE.
(18) ?I just hope I don?t become so blissful I be-come boring?
?
Nirvana leader Kurt Cobainsaid, giving meaning to his ?Teen Spirit?
coda,a denial.
?Smells Like Teen Spirit?
is a song by Nirvana.FALSEApparently, it is NOT OK to know that the Nirvanasong ?Smells like Teen Spirit?
is often referred to as?Teen Spirit?.
But why should we then know thatbovine spongiform encephalopathy is a disease?The test suite also contains examples that can onlybe classified as plain errors.
A couple of examplesare the following:(19) Green cards are becoming more difficult to ob-tain.Green card is now difficult to receive.TRUESomething that is becoming more difficult can stillbe easy, if it starts out that way.
(20) Hippos do come into conflict with people quiteoften.Hippopotamus attacks human.TRUEFor somebody who knows a lot about hippos it mightbe reasonable to assume that a conflict is necessarilyan attack but in general there is no inference: conflictis the less general term and attack the more specificone.
(21) A statement said to be from al Qaida claimedthe terror group had killed one American andkidnapped another in Riyadh.A U.S. citizen working in Riyadh has been kid-napped.TRUEThis seems betray a rather implausible belief in theclaims of al Qaida and while we are assuming thatthe author of the text is trustworthy, this assumptiondoes not extend to the sources he invokes.
In thiscase especially, the use of claim can be construed asindication the doubt of the author about the veracityof what the source says.
(22) Wal-Mart is being sued by a number of itsfemale employees who claim they were keptout of jobs in management because they werewomen.Wal-Mart is sued for sexual discrimination.TRUEA minute of reflection will make clear that here therelation between the two sentences involves quite abit of specialized legal knowledge and goes beyondtextual inferencing.
How is sexual discriminationdifferent from sexual harassment?
(23) South Korean?s deputy foreign minister sayshis country won?t change its plan to send 3000soldiers to Iraq.South Korea continues to send troops.TRUEWe assume that in context the second sentencemeans that South Korea continues to plan to sendtroops but normally continue does not mean con-tinue to plan and the first sentence certainly doesn?timply that South Korea has already sent troops.
Herethe way the test suite has been put together leadsto odd results.
A headline is paired up with a fullsentence.
Headlines are not meant to be understoodcompletely out of context and it would be prudent touse them sparingly in inference tasks of the sort pro-posed here.
We discuss other consequences of theway the test suite was constructed in the next sub-section with examples that to our mind need somekind of accommodation.353.2 Not a textual inference as such but .
.
.There are a couple of examples such as the followingin the test suite:(24) The White House failed to act on the domes-tic threat from al Qaida prior to September 11,2001.White House ignored the threat of attack.TRUEHere there is no entailment either way and surelyfail to act is not a synonym of ignore.
The examplesare due to the way the PASCAL test suite was put to-gether.
It was evidently at least in part developed byfinding snippets of text that refer to the same eventin different news sources; this is a fertile method forfinding inferences but it will lead to the inclusion ofsome material that mixes factual description and var-ious APPRECIATIONS of the described facts.
For in-stance in (24) above, two different authors describedwhat the White house did, putting a different spinon it.
While the fact described in both cases wasthe same, the appreciations that the two renderingsgive, while both negative, are not equivalent.
Butalthough there is no legitimate inference for the sen-tences as a whole, they both entail that the WhiteHouse did not act.
Here the test suite is the victim ofits self imposed constraints, namely that the relationhas to be established between two sentences foundin ?real?
text.
We propose to give up this constraint.Another maybe simpler illustration of the sameproblem is (25):(25) The report catalogues 10 missed opportunities.The report lists 10 missed opportunities.Although catalogue and list do not have the samemeaning, they may in some cases be used inter-changeably because, again, there is a common en-tailment:(26) According to the report, there were 10 missedopportunities.One can conceive of a thesaurus where catalogueand list would have a low level common hypernym(in WordNet they don?t) or a statistically inferredword class that would make the common entailmentexplicit, but that relation should not be confusedwith an inference between the two sentences in (25).4 A proposal for some refinementsAs the discussion above has shown, the way the testsuite was put together leads sometimes to the in-clusion of material that should not be there giventhe definition of the task.
Most of the data thatform the basis of PASCAL are extracted from differ-ent newspaper articles about the same event, oftenfrom the same newswire.
This means that the infor-mation packaging is very similar, reducing the con-structional and lexical range that can be used to ex-press a same idea.
This situation will not pertain inthe more general setting of question answering andmany types of paraphrases or inferences that wouldbe useful for question answering in general will notbe found or will be very rare in PASCAL-like suites.We would propose to augment the types of pairsthat one can get through the PASCAL extraction tech-niques with some that take the type of relations thatwe have discussed explicitly into account.
It can beobjected that this introduces a new level of artificial-ity by allowing made-up sentences but the separa-tion of world knowledge from linguistic knowledgeis in any case artificial.
But it is necessary becausewe will not be able to solve the inferencing problemwithout slicing the task into manageable pieces.AcknowledgmentsThis article was supported in part by the AdvancedResearch and Development Agency (ARDA) withinthe program for Advanced Question Answering forIntelligence (AQUAINT).
Thanks to all the membersof PARC?s AQUAINT team.ReferencesIdo Dagan and Oren Glickman.
2004.
Probabilistic tex-tual entailment: Generic applied modeling of languagevariablity.
In Learning Methods for Text Understand-ing and Mining, Grenoble, January.Paul H. Grice.
1989.
Studies in the Way of Words.
Har-vard University, Cambridge, MA.Larry Horn.
2003.
Implicature.
In Horn and Ward, edi-tors, Handbook of Pragmatics.
Blackwell, Oxford.Lauri Karttunen and Stanley Peters.
1979.
Conventionalimplicature.
In Choon-Kyu Oh and David A. Dinneen,editors, Syntax and Semantics, Volume 11: Presuppo-sition, pages 1?56.
Academic Press, New York.Christopher Potts.
2005.
The Logic of Conventional Im-plicatures.
Oxford Studies in Theoretical Linguistics.Oxford University Press, Oxford.36
