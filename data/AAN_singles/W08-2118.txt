CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 135?142Manchester, August 2008Context-based Arabic Morphological Analysis for Machine TranslationThuyLinh NguyenLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAthuylinh@cs.cmu.eduStephan VogelLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAvogel@cs.cmu.eduAbstractIn this paper, we present a novel morphol-ogy preprocessing technique for Arabic-English translation.
We exploit the Arabicmorphology-English alignment to learn amodel removing nonaligned Arabic mor-phemes.
The model is an instance ofthe Conditional Random Field (Lafferty etal., 2001) model; it deletes a morphemebased on the morpheme?s context.
Weachieved around two BLEU points im-provement over the original Arabic trans-lation for both a travel-domain systemtrained on 20K sentence pairs and a newsdomain system trained on 177K sentencepairs, and showed a potential improvementfor a large-scale SMT system trained on 5million sentence pairs.1 IntroductionStatistical machine translation (SMT) relies heav-ily on the word alignment model of the sourceand the target language.
However, there is amismatch between a rich morphology language(e.g Arabic, Czech) and a poor morphology lan-guage (e.g English).
An Arabic source word of-ten corresponds to several English words.
Pre-vious research has focused on attempting to ap-ply morphological analysis to machine translationin order to reduce unknown words of highly in-flected languages.
Nie?en and Ney (2004) rep-resented a word as a vector of morphemes andgained improvement over word-based system forc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.German-English translation.
Goldwater and Mc-closky (2005) improved Czech-English translationby applying different heuristics to increase theequivalence of Czech and English text.Specially for Arabic-English translation, Lee(2004) used the Arabic part of speech and Englishparts of speech (POS) alignment probabilities toretain an Arabic affix, drop it from the corpus ormerge it back to a stem.
The resulting systemoutperformed the original Arabic system trainedon 3.3 million sentence pairs corpora when usingmonotone decoding.
However, an improvementin monotone decoding is no guarantee for an im-provement over the best baseline achievable withfull word forms.
Our experiments showed that anSMT phrase-based translation using 4 words dis-tance reordering could gain four BLEU points overmonotone decoding.
Sadat and Habash (2006) ex-plored a wide range of Arabic word-level prepro-cessing and produced better translation results fora system trained on 5 million Arabic words.What all the above methodologies do not pro-vide is a means to disambiguate morphologi-cal analysis for machine translation based on thewords?
contexts.
That is, for an Arabic word anal-ysis of the form prefix*-stem-suffix* a morphemeonly is either always retained, always dropped offor always merged to the stem regardless of itssurrounding text.
In the example in Figure (1),the Arabic word ?AlnAfi*h?(?window?
in English)was segmented as ?Al nAfi* ap?.
The morpheme?ap?
is removed so that ?Al nAfi*?
aligned to ?thewindow?
of the English sentence.
In the sentence?hl ldyk mqAEd bjwAr AlnAf*h ??
(?do you havewindow tables ??
in English) the word ?AlnAfi*h?is also segmented as ?Al nAfi* ap?.
But in thissentence, morphological preprocessing should re-move both ?Al?
and ?ap?
so that only the remain-135nu  riyd  u  |||  mA}id  ap  |||  bi  jAnib  |||  Al  nAfi* ap  |||  .we  want  to  have  a  table  near  the  window  .nu  riyd  u  mA}id  ap  bi  jAnib  Al  nAfi* .nryd   mA}dh   bjAnb   AlnAf*h   .
(c)(a)(b)(d)Figure 1: (a) Romanization of original Arabic sentence, (b) Output of morphological analysis toolkit?words are separated by ?|||?, (c) English translation and its alignment with full morphological analysis(d) Morphological analysis after removing unaligned morphemes.ing morpheme ?nAfi*?
aligned to the word ?win-dow?
of the English translation.
Thus an appropri-ate preprocessing technique should be guided byEnglish translation and bring the word context intoaccount.In this paper we describe a context-based mor-phological analysis for Arabic-English translationthat take full account morphemes alignment to En-glish text.
The preprocessing uses the Arabic mor-phology disambiguation in (Smith et al, 2005) forfull morphological analysis and learns the remov-ing morphemes model based on the Viterbi align-ment of English to full morphological analysis.
Wetested the model with two training corpora of 5.2millions Arabic words(177K sentences) in newsdomain and 159K Arabic words (20K sentences)in travel conversation domain and gain improve-ment over the original Arabic translation in bothexperiments.
The system that trained on a sub-sample corpora of 5 millions sentence pairs cor-pora also showed one BLEU score improvementover the original Arabic system on unseen test set.We will explain our technique in the next sectionand briefly review the phrase based SMT model insection 3.
The experiment results will be presentedin section 4.2 MethodologyWe first preprocess the Arabic training corpus andsegment words into morpheme sequences of theform prefix* stem suffix*.
Stems are verbs, adjec-tives, nouns, pronouns, etc., carrying the contentof the sentence.
Prefixes and suffixes are func-tional morphemes such as gender and case mark-ers, prepositions, etc.
Because case makers do notexist in English, we remove case marker suffixesfrom the morphology output.
The output of thisprocess is a full morphological analysis corpus.Even after removing case markers, the token countof the full morphology corpus still doubles theoriginal Arabic?s word token count and is approx-imately 1.7 times the number of tokens of the En-glish corpus.
As stated above, using original Ara-bic for translation introduces more unknown wordsin test data and causes multiple English words tomap to one Arabic word.
At the morpheme level,an English word would correspond to a morphemein the full morphology corpus but some prefixesand suffixes in the full morphology corpus may notbe aligned with any English words at all.
For ex-ample, the Arabic article ?Al?
(?the?
in English)prefixes to both adjectives and nouns, while En-glish has only one determiner in a simple nounphrase.
Using the full morphological analysis cor-pus for translation would introduce redundant mor-phemes in the source side.The goal of our morphological analysis methodfor machine translation is removing nonalignedprefixes and suffixes from the full morphology cor-pus using a data-driven approach.
We use the wordalignment output of the full morphology corpus tothe English corpus to delete morphemes in a sen-tence.
If an affix is not aligned to an English wordin the word alignment output, the affix should beremoved from the morphology corpus for betterone-to-one alignment of source and target corpora.However, given an unseen test sentence, the En-glish translation of the sentence is not available toremove affixes based on the word alignment out-put.
We therefore learn a model removing non-aligned morphemes from the full morphology Ara-bic training corpus and its alignment to the Englishcorpus.
To obtain consistency between trainingcorpus and test set, we applied the model to bothArabic training corpus and test set, obtaining pre-processed morphology corpora for the translationtask.In this section, we will explain in detail eachsteps of our preprocessing methodology:136?
Apply word segmentation to the Arabic train-ing corpus to get the full morphological anal-ysis corpus.?
Annotate the full morphological analysis cor-pus based on its word alignment to the En-glish training corpus.
We tag a morpheme as?Deleted?
if it should be removed from thecorpus, and ?Retained?
otherwise.?
Learn the morphology tagger model.?
Apply the model to both Arabic training cor-pus and Arabic test corpus to get prepro-cessed corpus for translation.2.1 Arabic Word SegmentationSmith et al (2005) applies a source-channel modelto the problem of morphology disambiguation.The source model is a uniform model that de-fines the set of analyses.
For Arabic morphologydisambiguation, the source model uses the list ofun-weighted word analyses generated by BAMAtoolkit (Buckwalter, 2004).
The channel modeldisambiguates the morphology alternatives.
It is alog-linear combination of features, which capturethe morphemes?
context including tri-gram mor-pheme histories, tri-gram part-of-speech historiesand combinations of the two.The BAMA toolkit and hence (Smith et al,2005) do not specify if a morpheme is an affix ora stem in the output.
Given a segmentation of anoriginal Arabic word, we considered a morphemeaias a stem if its parts of speech piis either anoun, pronoun, verb, adjective, question, punctua-tion, number or abbreviation.
A morpheme on theleft of its word?s stem is a prefix and it is a suffixif otherwise.
We removed case marker morphemesand got the full morphology corpus.2.2 Annotate MorphemesTo extract the Arabic morphemes that align toEnglish text, we use English as the source cor-pus and aligned to Arabic morpheme corpus us-ing GIZA++ (Och and Ney, 2003) toolkit.
TheIBM3 and IBM4 (Brown et al, 1994) word align-ment models select each word in the source sen-tence, generate fertility and a list of target wordsthat connect to it.
This generative process wouldconstrain source words to find alignments in thetarget sentence.
Using English as source corpus,the alignment models force English words to gen-erate their alignments in the Arabic morphemes.GIZA++ outputs Viterbi alignment for every sen-tence pair in the training corpus as depicted in (b)and (c) of Figure (1).
In our experiment, only 5%of English words are not aligned to any Arabicmorpheme in the Viterbi alignment.
From ViterbiEnglish-morpheme alignment output, we annotatemorphemes either to be deleted or retained as fol-lows:?
Annotate stem morphemes as ?Retained?
(R),in dependant of word alignment output.?
Annotate a prefix or a suffix as ?Retained?
(R)if it is aligned to an English word.?
Annotate a prefix or a suffix as ?Deleted?
(D)if it is not aligned to an English word.Note that the model does not assume thatGIZA++ outputs accurate word alignments.
Welessen the impact of the GIZA++ errors by onlyusing the word alignment output of prefix and suf-fix morphemes.Furthermore, because the full morphology sen-tence is longer, each English word could align to aseparate morpheme.
Our procedure of annotatingmorphemes also constrains morphemes tagged as?Retained?
to be aligned to English words.
Thusif we remove ?Deleted?
morphemes from the mor-phology corpus, the reduced corpus and Englishcorpus have the property of one-to-one mappingwe prefer for source-target corpora in machinetranslation.2.3 Reduced Morphology ModelThe reduced morphology corpus would be thebest choice of morphological analysis for machinetranslation.
Because it is impossible to tag mor-phemes of a test sentence without the English ref-erence based on Viterbi word alignment, we needto learn a morpheme tagging model.
The modelestimates the distributions of tagging sequencesgiven a morphologically analysed sentence usingthe previous step?s annotated training data.The task of tagging morphemes to be either?Deleted?
or ?Retained?
belongs to the set of se-quence labelling problems.
The conditional ran-dom fields (CRF) (Lafferty et al, 2001) model hasshown great benefits in similar applications of nat-ural language processing such as part-of-speechtagging, noun phrase chunking (Sha and Pereira,2003), morphology disambiguation(Smith et al,2005).
We apply the CRF model to our morphemetagging problem.137Let A = {(A,T)} be the full morphology train-ing corpus whereA = a1|p1a2|p2.
.
.
am|pmis amorphology Arabic sentence, aiis a morpheme inthe sentence and piis its POS;T = t1t2.
.
.
tmisthe tag sequence of A, each tiis either ?Deleted?or ?Retained?
.
The CRF model estimates param-eter ?
?maximizing the conditional probability ofthe sequences of tags given the observed data:?
?= argmax??(A,T)?A(1)p?
((A,T)) log p(T|A, ?
)where p?
((A,T)) is the empirical distribution ofthe sentence (A,T) in the training data, ?
are themodel parameters.
The model?s log conditionalprobability log p(T|A, ?
)is the linear combina-tion of feature weights:log p(T|A, ?
)=?k?kfk((Aq,Tq)) (2)The feature functions {fk} are defined on any sub-set of the sentence Aq?
A and Tq?
T. CRFscan accommodate many closely related featuresof the input.
In our morpheme tagging model,we use morpheme features, part-of-speech featuresand combinations of both.
The features capturethe local contexts of morphemes.
The lexical mor-pheme features are the combinations of the currentmorpheme and up to 2 previous and 2 followingmorphemes.
The part-of-speech features are thecombinations of the current part of speech and upto 3 previous part of speeches.
The part of speech,morpheme combination features capture the de-pendencies of current morphemes and up to its 3previous parts of speech.2.4 Preprocessed DataGiven a full morphology sentence A, we use themorpheme tagging model learnt as described in theprevious section to decode A into the most proba-ble sequence of tags T?
= t1t2.
.
.
tm.T?= argmaxTPr(T|A, ??
)(3)If a tiis ?Deleted?, the morpheme aiis removedfrom the morphology sentence A.
The same pro-cedure is applied to both training Arabic corpusand test corpus to get preprocessed data for transla-tion.
We call a morphology sentence after remov-ing ?Deleted?
tag a reduced morphology sentence.In our experiments, we used the freely availableCRF++1 toolkit to train and decode with the mor-pheme tagging model.
The CRF model smoothedthe parameters by assigning them Gaussian priordistributions.3 Phrase-based SMT SystemWe used the open source Moses (Koehn, 2007)phrase-based MT system to test the impact of thepreprocessing technique on translation results.
Wekept the default parameter settings of Moses fortranslation model generation.
The system used the?grow-diag-final?
alignment combination heuris-tic.
The phrase table consisted of phrase pairs up toseven words long.
The system used a tri-gram lan-guage model built from SRI (Stolcke, 2002) toolkitwith modified Kneser-Ney interpolation smooth-ing technique (Chen and Goodman, 1996).
By de-fault, the Moses decoder uses 6 tokens distance re-ordering windows.4 Experiment ResultsIn this section we present experiment results usingour Arabic morphology preprocessing technique.4.1 Data SetsWe tested our morphology technique on a smalldata set of 20K sentence pairs and a medium sizedata set of 177K sentence pairs.4.1.1 BTEC DataAs small training data set we used the BTECcorpus (Takezawa et al, 2002) distributed bythe International Workshop on Spoken LanguageTranslation (IWSLT) (Eck and Hori, 2005).
Thecorpus is a collection of conversation transcriptsfrom the travel domain.
Table 1 gives some de-Arabic EngOri Full ReducedSentences 19972Tokens 159K 258K 183K 183KTypes 17084 8207 8207 7298Table 1: BTEC corpus statisticstails for this corpus, which consists of nearly 20Ksentence pairs with lower case on the English side.There is an imbalance of word types and word to-kens between original Arabic and English.
The1http://crfpp.sourceforge.net/138original Arabic sentences are on average shorterthan the English sentences whereas the Arabic vo-cabulary is more than twice the size of the Englishvocabulary.
The word segmentation reduced thenumber of word types in the corpus to be closedto English side but also increased word tokensquite substantially.
By removing nonaligned mor-phemes, the reduced corpus is well balanced withthe English corpus.The BTEC experiments used the 2004 IWSLTEvaluation Test set as development set and 2005IWSLT Evaluation Test set as unseen test data.Table 2 gives the details of the two test sets.
Bothof them had 16 reference translations per sourcesentence.
The English side of the training corpuswas used to build the language model.
To optimizethe parameters of the decoder, we performed min-imum error rate training on IWSLT04 optimizingfor the IBM-BLEU metric (Papineni et al, 2002).4.1.2 Newswire CorporaWe also tested the impact of our morphologytechnique on parallel corpus in the news domain.The corpora were collected from LDC?s full Ara-bic news translation corpora and a small portionof UN data.
The details of the data are give inTable 3.
The data consists of 177K sentence pairs,5.2M words on the Arabic and 6M words on theEnglish side.Arabic EngOri Full ReducedSentences 177035Tokens 5.2M 9.3M 6.2M 6.2MTypes 155K 47K 47K 68KTable 3: Newswire corpus statisticsWe used two test sets from past NIST evalua-tions as test data.
NIST MT03 was used as devel-opment set for optimizing parameters with respectto the IBM-BLEU metric, NIST MT06 was usedas unseen test set.
Both test sets have 4 referencesper test sentence.
Table 4 describes the data statis-tics of the two test sets.
All Newswire translationexperiments used the same language model esti-mated from 200 million words collected from theXinhua section of the GIGA word corpus.4.2 Translation Results4.2.1 BTECWe evaluated the machine translation accord-ing to the case-insensitive BLEU metric.
Table 5shows the BTEC results when translated with de-fault Moses setting of distance-based reorderingwindow size 6.
The original Arabic word trans-lation was the baseline of the evaluation.
Thesecond row contains translation scores using thefull morphology translation.
Our new technique ofcontext-based morphological analysis is shown inthe last row.IWSLT04 IWSLT05Ori 58.20 54.50Full 58.55 55.87Reduced 60.28 56.03Table 5: BTEC translations results on IBM-BLEUmetrics(Case insensitive and 6 tokens distance re-ordering window).
The boldface marks scores sig-nificantly higher than the original Arabic transla-tion scores.The full morphology translation performed sim-ilar to the baseline on the development set butoutperformed the baseline on the unseen test set.The reduced corpus showed significant improve-ments over the baseline on the development set(IWSLT04) and gave an additional small improve-ment over the full morphology score over the un-seen data (IWSLT05).So why did the reduced morphology translationnot outperform more significantly the full mor-phology translation on unseen set IWSLT05?
Toanalysis this in more detail, we selected goodfull morphology translations and compared themwith the corresponding reduced morphology trans-lations.
Figure 2 shows one of these examples.Typically, the reduced morphology translationsFigure 2: An example of BTEC translation output.are shorter than both the references and the fullmorphology outputs.
Table 2 shows that for theIWSLT05 test set, the ratio of the average En-glish reference sentence length and the source sen-139IWSLT04 (Dev set) IWSLT05 (Unseen set)Arabic English Arabic EnglishOri Full Reduced Ori Full ReducedSentences 500 8000 506 8096Words 3261 5243 3732 64896 3253 5155 3713 66286Avg Sent Length 6.52 10.48 7.46 8.11 6.43 10.19 7.34 8.18Table 2: BTEC test set statisticsMT03 (Dev set) MT06 (Unseen set)Arabic English Arabic EnglishOri Full Reduced Ori Full ReducedSentences 663 2652 1797 7188Words 16268 27888 18888 79163 41059 71497 48716 222750Avg Sent Length 24.53 42.06 28.49 29.85 22.85 39.79 27.1 30.98Table 4: Newswire test set statisticstence length is slightly higher than the correspond-ing ratio for IWSLT04.
Using the parameters op-timised for IWSLT04 to translate IWSLT05 sen-tences would generate hypotheses slightly shorterthan the IWSLT05 references resulting in brevitypenalties in the BLEU metric.
The IWSLT05brevity penalties for original Arabic, reduced mor-phology and full morphology are 0.969, 0.978 and0.988 respectively.
Note that the BTEC corpus andtest sets are in the travel conversation domain, theEnglish reference sentences contain a large num-ber of high frequency words.
The full morpho-logical analysis with additional prefixes and suf-fixes outputs longer translations containing highfrequency words resulting in a high n-gram matchand lower BLEU brevity penalty.
The reducedtranslation method could generate translations thatare comparable but do not have the same effect onBLEU metrics.4.2.2 Newswire resultsTable 6 presents the translation results for theNewswire corpus.
Even though morphology seg-mentation reduced the number of unseen words,the translation results of full morphological anal-ysis are slightly lower than the original Arabicscores in both development set MT03 and unseentest set MT06.
This is consistent with the resultachieved in previous literature (Sadat and Habash,2006).
Morphology preprocessing only helps withsmall corpora, but the advantage decreases forlarger data sets.Our context dependent preprocessing techniqueMT03 MT06Ori 45.55 32.09Full 45.30 31.54Reduced 47.69 34.13Table 6: Newswire translation results on IBM-BLEU metrics(Case insensitive and 6 tokens dis-tance reordering wondow).
The boldface marksscores significantly higher than the original Arabictranslation?s scores.shows significant improvements on both develop-ment and unseen test sets.
Moreover, while the ad-vantage of morphology segmentation diminishesfor the full morphology translation, we achieve animprovement of more than two BLEU points overthe original Arabic translations in both develop-ment set and unseen test set.4.3 Unknown Words ReductionA clear advantage of using morphology basedtranslation over original word translation is thereduction in the number of untranslated words.Table 7 compares the number of unknown Arabictokens for original Arabic translation and reducedmorphology translation.
In all the test sets, mor-phology translations reduced the number of un-known tokens by more than a factor of two.4.4 The Impact of Reordering Distance LimitThe reordering window length is determined basedon the movements of the source phrases.
On anaverage, an original Arabic word has two mor-140Reorder Window 0 2 3 4 5 6 7 8 9IWSLT04Ori 57.21 57.92 58.01 58.31 58.16 58.20 58.20 58.12 58.01Full 56.89 57.54 58.62 58.39 58.32 58.55 58.55 58.55 58.57Reduced 58.36 59.56 60.05 60.70 60.32 60.28 60.46 60.30 60.55MT03Ori 41.75 43.84 45.24 45.61 45.40 45.55 45.21 45.22 45.19Full 41.45 43.12 44.32 44.71 45.30 45.80 45.88 45.82Reduced 44.08 45.28 46.50 47.40 47.41 47.69 47.59 47.75 47.79Table 8: The impact of reordering limits on BTEC ?s development set IWSLT04 and Newswire?s devel-opment set MT03.
The translation scores are IBM-BLEU metricTest Set Ori ReducedIWSLT04 242 100IWSLT05 219 97MT03 1463 553MT06 3734 1342Table 7: Unknown tokens countphemes.
The full morphology translation with a6-word reordering window has the same impactas a 3-word reordering when translating the orig-inal Arabic.
To fully benefit from word reorder-ing, the full morphology translation requires alonger reorder distance limit.
However, in currentphrase based translations, reordering models arenot strong enough to guide long distance source-word movements.
This shows an additional advan-tage of the nonaligned morpheme removal tech-nique.We carried out experiments from monotone de-coding up to 9 word distance reordering limit forthe two development sets IWSLT04 and MT03.The results are given in Table 8.
The BTEC dataset does not benefit from a larger reordering win-dow.
Using only a 2-word reordering windowthe score of the original Arabic translations(57.92)was comparable to the best score (58.31) obtainedby using a 4-word reordering window.
On theother hand, the reordering limit showed a signifi-cant impact on Newswire data.
The MT03 originalArabic translation using a 4-word re-ordering win-dow resulted in an improvement of 4 BLEU pointsover monotone decoding.
Large Arabic corporausually contain data from the news domain.
Thedecoder might not effectively reorder very longdistance morphemes for these data sets.
This ex-plains why machine translation does not benefitfrom word-based morphological segmentation forlarge data sets which adequately cover the vocabu-lary of the test set.4.5 Large Training Corpora ResultsWe wanted to test the impact of our preprocess-ing technique on a system trained on 5 millionsentence pairs (128 million Arabic words).
Un-fortunately, the CRF++ toolkit exceeded memorylimits when executed even on a 24GB server.
Wecreated smaller corpora by sub-sampling the largecorpus for the source side of MT03 and MT06test sets.
The sub-sampled corpus have 500K sen-tence pairs and cover all source phrases of MT03and MT06 which can be found in the large cor-pus.
In these experiments, we used a lexical re-ordering model into translation model.
The lan-guage model was the 5-gram SRI language modelbuilt from the whole GIGA word corpus.
Table 9MT03 MT065M Ori 56.22 42.17Sub-sample Ori 54.54 41.59Sub-sample Full 51.47 40.84Sub-sample Reduced 54.78 43.20Table 9: Translation results of large corpora(Caseinsensitive, IBM-BLEU metric).
The boldfacemarks score significantly higher than the originalArabic translation score.presents the translation result of original Arabicsystem trained on the full 5M sentence pairs cor-pus and the three systems trained on the 500K sen-tence pairs sub-sampled corpus.
The sub-sampledfull morphology system scores degraded for bothdevelopment set and unseen test set.
On devel-opment set, the sub-sampled reduced morphologysystem score was slightly better than baseline.
Onthe unseen test set, it significantly outperformedboth the baseline on sub-sampled training data andeven outperformed the system trained on the entire1415M sentence pairs.5 Conclusion and Future WorkIn this paper, we presented a context-dependentmorphology preprocessing technique for Arabic-English translation.
The model significantly out-performed the original Arabic systems on smalland mid-size corpora and unseen test set on largetraining corpora.
The model treats morphologyprocessing task as a sequence labelling problem.Therefore, other machine learning techniques suchas perceptron (Collins, 2002) could also be appliedfor this problem.The paper also discussed the relation betweenthe size of the reordering window and morphol-ogy processing.
In future investigations, we planto extend the model such that merging morphemesis included.
We also intent to study the impact ofphrase length and phrase extraction heuristics.AcknowledgementWe thank Noah Smith for useful comments andsuggestions and providing us with the morphol-ogy disambiguation toolkit.
We also thank SameerBadaskar for help on editing the paper.
We alsothank anonymous reviewers for helpful comments.The research was supported by the GALE project.ReferencesBrown, Peter F., Stephen Della Pietra, Vincent J. Della Pietra,and Robert L. Mercer.
1994.
The Mathematic of Statisti-cal Machine Translation: Parameter Estimation.
Compu-tational Linguistics, 19(2):263?311.Buckwalter, T. 2004.
Arabic Morphological Analyzer ver-sion 2.0.
LDC2004L02.Chen, Stanley F. and Joshua Goodman.
1996.
An EmpiricalStudy of Smoothing Techniques for Language Modeling.In Proceedings of the ACL, pages 310?318.Collins, Michael.
2002.
Discriminative Training Methodsfor Hidden Markov Models: Theory and Experiments withPerceptron Algorithms.
In Proceedings of EMNLP ?02,pages 1?8.Eck, M. and C. Hori.
2005.
Overview of the IWSLT 2005Evaluation Campaign.
In Proceedings of IWSLT, pages11?17.Goldwater, Sharon and David Mcclosky.
2005.
Improv-ing Statistical MT through Morphological Analysis.
InProceedings of HLT/EMNLP, pages 676?683, Vancouver,British Columbia, Canada.Koehn, et al 2007.
Moses: Open Source Toolkit for Sta-tistical Machine Translation.
In Annual Meeting of ACL,demonstration session.Lafferty, John, Andrew McCallum, and Fernando Pereira.2001.
Conditional Random Fields: Probabilistic Modelsfor Segmenting and Labeling Sequence Data.
In Proceed-ings of 18th ICML, pages 282?289.Lee, Young S. 2004.
Morphological Analysis for StatisticalMachine Translation.
In HLT-NAACL 2004: Short Papers,pages 57?60, Boston, Massachusetts, USA.Nie?en, Sonja and Hermann Ney.
2004.
Statistical Ma-chine Translation with Scarce Resources Using Morpho-Syntactic Information.
Computational Linguistics, 30(2),June.Och, Franz Josef and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51.Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
BLEU: a Method for Automatic Evaluationof Machine Translation.
In Proceedings of the 40th ACL,pages 311?318, Philadelphia.Sadat, Fatiha and Nizar Habash.
2006.
Combination of Ara-bic Preprocessing Schemes for Statistical Machine Trans-lation.
In Proceedings of the ACL, pages 1?8, Sydney,Australia.
Association for Computational Linguistics.Sha, Fei and Fernando Pereira.
2003.
Shallow Parsing withConditional Random Fields.
In Proceedings of NAACL?03, pages 134?141, Morristown, NJ, USA.Smith, Noah A., David A. Smith, and Roy W. Tromble.
2005.Context-Based Morphological Disambiguation with Ran-dom Fields.
In Proceedings of HLT/EMNLP, pages 475?482, Vancouver, British Columbia, Canada, October.
As-sociation for Computational Linguistics.Stolcke, A.
2002.
SRILM ?
an Extensible Language Model-ing Toolkit.
In Intl.
Conf.
on Spoken Language Process-ing.Takezawa, Toshiyuki, Eiichiro Sumita, Fumiaki Sugaya, Hi-rofumi Yamamoto, and Seiichi Yamamoto.
2002.
Towarda Broad-Coverage Bilingual Corpus for Speech Transla-tion of Travel Conversations in the Real World.
In Pro-ceedings of LREC 2002, pages 147?152.142
