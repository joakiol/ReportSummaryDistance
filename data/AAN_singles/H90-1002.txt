Session 1: Spoken Language Systems IWayne Ward, ChairSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, Pa 15213The papers in this session addressed issues in combiningspeech recognition with natural language systems.
The firstthree papers concern the use of grammars.
Speech recog-nizers and Natural Language parsers make different re-quirements of language knowledge.
Recognizers need ef-ficient methods for constraining the search space, whileparsers need detailed analytical knowledge.
One solutionto the problem of integrating speech recognizers with NLprocessors i  to use different language constraints in thetwo modules.
This in effect means using different gram-mars for recognizing and parsing.
The recognizer may useno grammar or simple, efficient grammars, while the parseruses a more complete representation f the language.
Thismeans that the recognizer can overgenerate, or producestrings not acceptable to the parser.
In this case, a recog-nition error can lead to a failure to parse the utterance.
Onesolution to this problem is to use an N-Best recognizer.Such a recognizer produces the N (where N is preset) bestscoring hypotheses for an utterance.
These hypotheses arepassed to the parser which can then pick the overall bestone.Rich Schwartz from BBN and Frank Soong from AT&Tboth presented efficient algorithms for generating the Nbest recognition strings for an utterance.
In contrast to pre-vious N-Best algorithms, both of these algorithms requireonly a small amount of additional computation toproduceN-Best instead of the single best hypothesis.
Both systemsuse a time-synchronous forward search to find the besthypothesis, and a backward pass to generate the N best.Information accumulated in the forward pass is used toscore paths in the backward search.
The BBN algorithmuses a beam search on the backward pass where the AT&Tsystem uses a tree search.In order to be implemented efficiently, recognitiongrammars often overgenerate, but they should not also un-dergenerate.
That is, they should not reject strings accept-able to the parser.
Finite-state approximations of phrasestructure grammars have been described in the past.
Typi-cally the FSA is generated by limiting rule expansions to apreset depth.
This method has the disadvantage that theFSA generated is not a strict superset of the languagegenerated by the PSG.
Some strings are rejected by theFSA that are acceptable to the PSG.
Fernando Pereira ofAT&T presented an algorithm to generate a finite state ap-proximation for any context-free grammar where the ap-proximation is a superset of the language accepted by thegrammar.
This guarantees that no string acceptable to theparser will be precluded uring recognition.
Thus the FSAmay be implemented to provide efficient constraints for arecognizer while the full CFG is used by a parser foranalysis.The final paper in the session concerns a different typeof knowledge, prosodic information.
Mari Ostendorfdescribed a system which uses prosodic phrase breaks todisambiguate parses.
"Break indices" are computed be-tween each pair of words in an utterance.
These indices areautomatically assigned using a Hidden Markov Model ofrelative duration of phonetic segments.
The break indexgives an indication of the relative tightness of couplingbetween adjacent words.
The parser uses this informationto decide between alternative ways of grouping words intophrases.
Results for a small test set of sentences were en-couraging.
Fourteen sentences with prepositional am-biguities were tested.
The use of the break indices sig-nificantly reduced (by about 25%) the number of parsesproduced for these sentences without eliminating any cor-rect parses.
