Analysis and modeling of manual summarization ofJapanese broadcast newsHideki Tanaka, Tadashi Kumano, Masamichi Nishiwaki and Takayuki ItohScience and Techinical Research Laboratories of NHK1-10-11, Kinuta, Setagaya-kuTokyo, 157-8510, Japan{tanaka.h-ja,kumano.t-eq,nishiwaki.m-hk,itou.t-gq}@nhk.or.jpAbstractWe describe our analysis and modelingof the summarization process of Japa-nese broadcast news.
We have studiedthe entire manual summarization proc-ess of the Japan Broadcasting Corpora-tion (NHK).
The staff of NHK has beenmaking manual summarizations ofnews text on a daily basis since De-cember 2000.
We interviewed theseprofessional abstractors and obtained aconsiderable amount of news summa-ries.
We matched the summary with theoriginal text, investigated the news textstructure, and thereby analyzed themanual summarization process.
Wethen developed a summarization modelon which we intend to build a summa-rization system.1  IntroductionAutomatic text summarization research has along history that dates back to the late 50?s(Mani and Maybury, 1999).
It started mainlywith the purpose of information gathering orassimilation, and most of the research has dealtwith extracting the important parts of the texts.The summaries obtained with these techniques,so called extracts, have been used for judgingthe importance of the texts.We have started research on automatic sum-marization for the purpose of informationdissemination, namely summarization of newstexts for broadcast news.
Recently, we havestudied the entire manual summarization processof the Japan Broadcasting Corporation (NHK).NHK has been making manual summariza-tions of news text on a daily basis since Decem-ber 2000, when it started satellite digitalbroadcasting.
The summarized text has beenused for the data service of the digital broadcast-ing and on Web pages accessible by mobilephones.We interviewed NHK?s professional abstrac-tors and analyzed a considerable amount ofnews summaries.
We matched these summarieswith the original news and studied the summari-zation process based on the results of our analy-sis and interviews.In this paper, we report on what we foundduring the interviews with the abstractors andthe results of the automatic text alignment be-tween summaries and the original news togetherwith the word position matching.
We also pro-pose a summarization model for an automatic orsemi-automatic summarization system.2 The manual summarization processMost of the radio and TV news services ofNHK are based on a ?general news manuscript.
?We call such manuscripts the original news inthis paper.
The original news is manually sum-marized into summary news that are made avail-able to the public through Web pages and digitalbroadcasting, as mentioned in section 1.We asked professional abstractors about thesummarization environment and process and inso doing discovered the following.?
AbstractorThe original news is written by NHK report-ers, and the text is summarized by differentwriters, i.e., professional abstractors.
Most pro-fessional abstractors are retired reporters whohave expertise in writing news.?
Compression rate and time allowanceThe original news is compressed to a maxi-mum length of 105 Japanese characters.
We will49show in section 4 that the average compressionrate is about 22.5%.
The upper bound is decidedfrom the display design of the data service ofdigital TV broadcasting.
The abstractors mustwork quickly because the summary news mustbe broadcast promptly.?
TechniquesThe abstractors use only information con-tained in the original news.
They scan the origi-nal news quickly and repeatedly, not tounderstand the full content, but to select theparts to be used in the summary news.
The ab-stractors?
special reading tendency has been re-ported in (Mani, 2001), and we can say the sametendency was observed in our Japanese abstrac-tors.
The abstractors focus on the lead (the open-ing part) of the original news.
They sometimesuse the end part of the original news.3    Corpus constructionWe planned the summary news corpus as aresource to investigate the manual summariza-tion process and to look into the possibility of anautomatic summarization system for broadcastnews.
We obtained 18,777 pieces of summarynews from NHK.
Although each piece is asummary of a particular original news text, thelink between the summary and the original newsis not available.We matched the summary and original newsand constructed a corpus.
There have been sev-eral attempts to construct <summary text, origi-nal text> corpora (Marcu, 1999; Jing andMcKeown, 1999).
We decided to use themethod proposed by Jing and McKeown (1999)for the reasons given below.As our abstractors mentioned that they usedonly information available in the original news,we hypothesize that the summary and the origi-nal news share many surface words.
This indi-cates that the surface-word-based matchingmethods such as (Marcu, 1999; Jing and McKe-own, 1999) will be effective.In particular, the word position matching re-alized in (Jing and McKeown, 1999) seems es-pecially useful.
We thought that we might beable to observe the summarization process pre-cisely by tracing the word position links, and weemployed their work with a little modification.As a result, our corpus takes the form of thetriple: <summary, original, word position corre-spondence>.3.1 Matching algorithmJing and McKeown (1999) treated a wordmatching problem between a summary and itstext, which they called the summary decomposi-tion problem.
They employed a statistical model(briefly described below) and obtained goodresults when they tested their method with theZiff-Davis corpus.
In the following explanation,we use the notion of summary and text insteadof summary news and original news for simplic-ity.
(1) The word position in a summary is repre-sented by <I>.
(2) The word position in the text is repre-sented by a pair of the sentence position (S)and the word position in a sentence (W) as in<S, W>.
(3) Each summary word is checked as towhether it appears in the text.
If it appears,all of the positions in the text are stored inthe form of <S,W> to form a position trellis.
(4) Scan the n summary words from left toright and find the path on the trellis thatmaximizes the score of formula (1).?
?=+ ===1111221 )),(|),((niii WSIWSIPP  (1)This formula is the repeated product of theprobability that the two adjacent words in asummary (Ii and Ii+1) appear at positions (S1, W1)and (S2, W2) in the text, respectively.
This quan-tity represents the goodness of the summary andthe text word matching.
As a result, the path onthe trellis with the maximum probability givesthe overall most likely word position match.Jing and McKeown (1999) assigned six-grade heuristic values to the probability.
Thehighest probability of 1.0 was given when twoadjacent words in a summary appear at adjacentpositions in the same sentence of the text.
Thelowest probability of 0.5 was given when twoadjacent words in a summary appear in differentsentences in the text with a certain distance orgreater.
We fixed the distance at two sentences,considering the average sentence count of theoriginal news texts.50Original news text???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
?bodyleadSummary news textFigure 1.
Summary and original news text matching.Jing and McKeown?s algorithm (1999) is de-signed to treat a fixed summary and text pair andneeds some modification to be applied to ourtwo-fold problem of finding the original news ofa given summary news from a large collectionof news together with the word position match-ing.Their method has a special treatment for asummary word that does not appear in the text.It assumes that such a word does not exist in thesummary and therefore skips the trellis at thisword with a probability of 1.
This unfavorablybiases news text that contains fewer matchingwords.
To alleviate this problem, we experimen-tally found that the probability score of 0.55works well for such a case (This score was thesecond smallest of the original six-grade score).We developed a word match browser to pre-cisely check the words of the summary andoriginal news.3.2 Summary and original news matchingWe matched 18,777 summary news textsfrom November 2003 to June 2004 against thenews database, which mostly covers the originalnews of the period.
We followed the proceduresbelow.?
Numerical expression normalizationNumerical expressions in the original newsare written in Chinese numerals (Kanji) andthose of the summary news are written inArabic numerals.
We normalized the Chinesenumerals into Arabic numerals.?
Morphological analysisThe summary and original news were mor-phologically analyzed.
We used morphemesas a matching unit.
In this paper, we will usemorphemes and words interchangeably.?
Search spanEach summary news was matched against thenews written in the three-day period beforethe summary was written.
This period waschosen experimentally.4  Results and observationWe randomly checked the news matching re-sults and found more than 90% were correct.Some of the summaries were exceptionally long,and we consider that such noisy data was themain reason for incorrect matching.
Figure 1shows a matching example.
The underlined (lineand broken line) sentences show the word posi-tion match.The word matching is not easy to evaluatebecause we do not have the correct matchinganswer.
Although there are some problems inthe matching, most of the results seem to begood enough for approximate analysis.
The fol-lowing discussion assumes that the word match-ing is correct.4.1 Compression rateTable 1 shows the basic statistics of thesummary and its corresponding original news.51We can see that the average compression rate is22.5% in terms of characters.
The average sum-mary news length (109.9 characters per newstext) was longer than what we were told (105,see section 2).We then checked the length of the typicalsummary texts.
We found that the cumulativerelative frequency of the summary text with thesentence count from 1 to 4 was 0.99 and wasquite dominant.
We checked the average lengthof these summaries and obtained 105.4, which isclose to what we were told.
We guess that noisy?long summaries?
skewed the figure.Original Summarytext counts 18,777Ave.
sent.
count/text 5.13 1.63Ave.
text length (char.)
487.7 109.9Ave.
first line length (char.)
94.9 81.301020304050607080%1 2 3 4 5 6 7 8Sent.
No.Figure 2.
Summary word employmentratio of original news4 sent.
5 sent 6 sent 7 sent 8 sentTable 1.
Basic statistics of summary and originalnews4.2 Word match ratioWe measured how many of the summarywords came from original news.
As our match-ing result contains word-to-word correspon-dence, we calculated the ratio of the matchedwords in a summary text.
Table 2 shows a partof the result.
It shows the relative frequency ofthe summary news in which 100% of the wordscame from the original news reached 0.265 andthose that had more than 90% reached 0.970.Word match ratio Rel.
summary  freq.100?
0.265More than 90?
0.970 (cumulative)Table 2.
Word match ratioThis strongly suggests that most of the sum-mary news is the ?extract?
(Mani, 2001), whichis written using only vocabulary appearing in theoriginal news.
This result is in accord with whatthe abstractors told us.4.3 Summary word employment in theoriginal news sentencesThe previous section indicated that our sum-mary likely belongs to the extract type.
Where inthe original news do these words come from?We next measured the word employment ratioof each sentence in the original news and theresult is presented in Figure 2.In this graph, the original news is categorizedinto five cases according to its sentence countfrom 4 to 81 and the average word employmentratio is shown for each sentence.Of this figure, the following observations canbe made:?
Bias toward the first sentenceIn all five cases, the first sentence recordedthe highest word employment ratio.
The per-centages of the second and third sentences in-crease when the news contains many sentences.The opening part of the news text is called thelead.
We will discuss its role in the next section.?
No clear favorite for the final sentenceThere was no employment ratio rise for theclosing sentences in any case even though ourabstractors indicated they often use informationin the last sentence.
This inconsistency may bedue to the word match error.
Final sentencesactually have an important role in news, as wewill see in the next section.5  Summarization modelIn the previous section, we found a quitehigh word overlap between a summary and theopening part of the original news text.
Wechecked with our word match browser the simi-larity of the summary news and lead sentences,and found that most of the summary sentences1 These news texts cover the 88 % of the total news texts.52take exactly the same syntactic pattern of theopening sentence.
Based on this observation andwhat we found in the interviews, we devised anews text summarization model.
The model canexplain our abstractors?
behavior, and we areplanning to develop an automatic or semi-automatic summarization system with it.
Wewill explain the typical news text structure andpresent our model.5.1 News text structureMost of our news texts are written with athree-part structure, i.e., lead, body and supple-ment.
Figure 1 shows the two-fold structure ofthe lead and the body.
Each part has the follow-ing characteristics.?
LeadThe most important information is briefly de-scribed in the opening part of a news text.
Thispart is called the lead.
Proper nouns are oftenavoided in favor of more abstract expressionssuch as ?a young man?
or ?a big insurance com-pany.?
The lead is usually written in one or twosentences.?
BodyThe lead is detailed in the body.
The 5W1Hinformation is mainly elaborated, and the propernames that were vaguely mentioned in the leadappear here.
The statements of people involvedin the news sometimes appear here.
The repeti-tive structure of the lead and the body is rootedin the nature of radio news; listeners cannot goback to the previous part if they missed the in-formation.?
SupplementNecessary information that has not been cov-ered in the lead and the body is placed here.Take for an example of weather news about atyphoon.
A caution from the Meteorologicalagency is sometimes added after the typhoon?smovement has been described.5.2 ModelWe found that most of the summary news iswritten based on the lead sentences.
They arethen shortened or partly modified with the ex-pressions in the body to make them more infor-mative and self-contained.The essential operation, we consider, lies inthe editing of the lead sentences under the sum-mary length constraint.
Based on the observation,we have proposed a two-step summarizationmodel of reading and editing.
The summary inFigure 1 is constructed with the lead sentencewith the insertion of a phrase in the body.?
Reading phase(1) Identify the lead, the body and the sup-plement sentences in the original news.
(2) AnalysisFind the correspondences between the partsin the lead and those in the body.
We can re-gard this process as a co-reference resolution.?
Summary editing phase(3) Set the lead sentence as the base sentenceof the summary.
(4) Apply the following operations until thebase sentence length is close enough to thepredefined length N.(4-1) Delete parts in the base sentence.
(4-2) Substitute parts in the base sentencewith the corresponding parts in the body withthe results of (2).(4-2?)
Add a body part to the base sentence.We may view this as a null part substitutedby a body part.
(4-3) Add supplement sentences.The supplement is often included in a sum-mary; this part contains different informationfrom the other parts.5.3 Related works and discussionOur two-step model essentially belongs tothe same category as the works of (Mani et al,1999) and (Jing and McKeown, 2000).
Mani etal.
(1999) proposed a summarization systembased on the ?draft and revision.?
Jing andMcKeown (2000) proposed a system based on?extraction and cut-and-paste generation.?
Ourabstractors performed the same cut-and-pasteoperations that Jing and McKeown noted in theirwork, and we think that our two-step model willbe a reasonable starting point for our subsequentresearch.
Below are some of our observations.53The lead sentences play a central role in ourmodel since they serve as the base of the finalsummary.
Their identification can be achievedwith the same techniques as used for the impor-tant sentence extraction.
In our case, the sen-tence position information plays an importantrole as was shown by Kato and Uratani (2000).We consider the identification of the body andthe supplement part together with the lead willbe beneficial for the co-reference resolution.The co-reference resolution problem betweenthe lead and the body should be treated in amore general way than usual.
We found that ourproblem ranges from the word level, the corre-spondence between named entities and their ab-stract paraphrases, to the sentence level, anentire statement of a person and its short para-phrase.
We are now investigating the types ofco-reference that we have to cover.We found that the deletion of lead parts didnot occur very often in our summary, unlike thecase of Jing and McKeown (2000).
One reasonis that most of our leads were short enough2 tobe included in the summary and therefore thesubstitution operation became conspicuous.
Thisusually increased the length of summary butcontributed to making it more lively and infor-mative.A supplement part was often included in thesummary.
We consider that this feature corre-sponds to the abstractors?
comments on em-ployment of the final sentence, which was notclearly detected in our statistical investigationdescribed in section 4.3.
We are now investigat-ing the conditions for including the supplement.We have so far listed the basic operations ofediting through the manual checking of samples,and we are currently analyzing the operationswith more examples.
We will then study auto-matic selection of the optimum operation se-quence to achieve the most informative andnatural summary.6  ConclusionsWe have described the manual summaryprocess of NHK?s broadcast news and experi-ments on automatic text alignment betweennews summaries and the original news together2 The present summary length constraint is 105 characters.Meanwhile, the average length of the first sentence (typi-cally the lead) of a  news text is 94.5 as is shown in table 1.with the word position matching.
Through a sta-tistical analysis of the results and interviewswith abstractors, we found that the abstractorssummarize news by taking advantage of itsstructure.
Based on this observation, we pro-posed a summarization model that consists of areading and editing phase.
We are now design-ing an automatic or semi automatic summariza-tion system employing the model.AcknowledgementThe authors would like to thank Mr. IsaoGoto and Dr. Naoto Kato of ATR for valuablediscussion and Mr. Riuzo Waki of EugeneSoftware Inc. for implementing our ideas.ReferencesJing, Hongyan and Kathleen R. McKeown.
1999.The Decomposition of Human-Written SummarySentences.
The 22nd Annual International ACMSIGIR Conference, pages 129-136, Berkeley.Jing, Hongyan and Kathleen R. McKeown.
2000.
Cutand Paste Based Text Summarization.
The 1stMeeting of the North American Chapter of the As-sociation for Computational Linguistics, pages178-185, Seattle.Kato, Naoto and Noriyoshi Uratani.
2000.
ImportantSentence Selection for Broadcast News (in Japa-nese), The 6th Annual convention of the Associa-tion for Natural Language Processing, pages 237-240, Kanazawa, JapanMani, Inderjeet and Mark T. Maybury.
1999.
Ad-vances in Automatic Summarization, The MITpress, Cambridge, MassachusettsMani, Inderjeet, Barbara Gates and Eric Bloedorn.1999.
Improving Summaries by Revising them,The 37th Annual Meeting of the Association forComputational Linguisics, pages 558-565, Mary-land.Mani, Inderjeet.
2001.
Automatic Summarization.John Benjamins, Amsterdam/Philadelphia.Marcu, Daniel.
1999.
The automatic construction oflarge-scale corpora for summarization research.The 22nd Annual International ACM SIGIR Con-ference, pages 137-144, Berkeley.54
