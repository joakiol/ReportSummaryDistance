Natural Language Analysis of Patent ClaimsSvetlana SheremetyevaDepartment of Computational LinguisticsCopenhagen Business School,Bernhard Bangs Alle 17 B,DK-2000, Denmarklanaconsult@mail.dkAbstractWe propose a NLP methodology for ana-lyzing patent claims that combines sym-bolic grammar formalisms with data-intensive methods while enhancing analy-sis robustness.
The output of our analyzeris a shallow interlingual representationthat captures both the structure and con-tent of a claim text.
The methodology canbe used in any patent-related application,such as machine translation, improvingreadability of patent claims, informationretrieval, extraction, summarization, gen-eration, etc.
The methodology should beuniversal in the sense that it could be ap-plied to any language, other parts of pat-ent documentation and text as such.1 IntroductionAn exploding volume of patent applications makesessential the use of adequate patent processingtools that could provide for better results in anyfield of patent related activity.
NLP techniquesassociated with specificity of patent domain havepromise for improving the quality of patent docu-ment processing.Though it is generally recognized that the patentdomain features overwhelmingly long and com-plex sentences and peculiar style (Kando, 2000)only a few researchers really rely on the linguisticspecificity of patent style (vs. technical style) whenprocessing patent documentation  (Shnimory et al,2002; Gnasa and Woch, 2002; Fujii and Ishikawa,2002).Developing natural language analyzers for pat-ents (with at least one or any combination of mor-phological, syntactic and semantic modules) is abasic task.
The ultimate task of such analysis is tobuild a kind of possibly unambiguous content rep-resentation that could further be used to producehigher quality applications.Broad coverage syntactic parsers with goodperformance have recently become available(Charniak, 2000; Collins, 2000), but they are nottrained for patents.
Semantic parsing is considera-bly less developed and shows a trend to rely onontologies rather then semantic primitives.
(Gnasaand Woch, 2002).This paper reports on on-going project whosegoal is to propose a NLP methodology and an ana-lyzer for patent claims.
The claim is the focal pointof a patent disclosure, - it describes essential fea-tures of the invention and is the actual subject oflegal protection.The methodology we suggest combines sym-bolic grammar formalisms with data-intensiveknowledge while enhancing analysis robustness.The output of our analyzer is a shallow interlingualrepresentation that captures both the structure andcontent of a claim text.
It can be used in any pat-ent-related application, such as machine translationimproving readability of patent claims, informationretrieval, extraction, summarization, generation,etc.
The methodology should be universal in thesense that it could be applied to any language,other parts of patent documentation and text assuch.In what follows we first consider the knowledgebase of our model describing in turn a flexibledepth lexicon, grammar formalism, and languageof knowledge representation for the final parse.
Wethen focus on the analysis algorithm as a multi-component procedure.
To illustrate the potential ofthe methodology we further sketch two of its pos-sible applications, namely, machine translation andan application for improving the readability of pat-ent claims.
We conclude with the description of theproject status and future work.2 KnowledgeThe structure and content of the knowledgebase has been designed to a) help solve analysisproblems, ?
different kinds of ambiguity, ?
andb) minimize the knowledge acquisition effort bydrawing heavily on the patent claim linguistic re-strictions.A patent claim shares technical terminologywith the rest of a patent but differs greatly in itscontent and syntax.
It must be formulated accord-ing to a set of precise syntactic, lexical and stylisticguidelines as specified by the German Patent Of-fice at the turn of the last century and commonlyaccepted in the U.S., Japan, and other countries.The claim describes essential features of the inven-tion in the obligatory form of a single extendednominal sentence, which frequently includes longand telescopically embedded predicate phrases.
AUS patent claim that we will further use as an ex-ample in our description is shown in Figure 1.A cassette for holding excess lengths of lightwaveguides in a splice area comprising a coverpart and a pot-shaped bottom part having a bottomdisk and a rim extending perpendicular to saidbottom disk, said cover and bottom parts are su-perimposed to   enclose jointly an area forming amagazine for excess lengths of light waveguides,said cover part being rotatable in said bottom part,two guide slots formed in said cover part, saidslots being approximately radially directed, guidemembers disposed on said cover part, a spliceholder mounted on said cover part to form a ro-tatable splice holder.Figure 1.
A US patent claim text.In our system the knowledge is coded in the sys-tem lexicon, which has been acquired from twokinds of corpora, - a corpus of complete patent dis-closures and a corpus of patent claims.
The lexiconconsists of two parts: a shallow lexicon of lexicalunits and a deep (information-rich) lexicon ofpredicates.
Predicates in our model are words,which are used to describe interrelations betweenthe elements of invention.
They are mainly verbs,but can also be adjectives or prepositions.2.1 Shallow LexiconThe word list for this lexicon was automaticallyacquired from a 5 million-word corpus of a USpatent web site.
A semi-automatic supertaggingprocedure was used to label these lexemes withtheir supertags.Supertagging is a process of tagging lexemeswith labels (or supertags), which code richer in-formation than standard POS tags.
The use of su-pertags, as noted in (Joshi and Srinivas, 1994)localizes some crucial linguistic dependencies, andthus show significant performance gains.
The con-tent of a supertag differs from work to work and istailored for the needs of an application.
For exam-ple, Joshi and Srinivas (1994) who seem to cointhis term use elementary trees of Lexicalized Tree-Adjoining Grammar for supertagging lexical items.In (Gnasa and Woch, 2002) it is grammatical struc-tures of the ontology that are used as supertags.In our model a supertag codes morphologicalinformation (such as POS and inflection type) andsemantic information, an ontological concept, de-fining a word membership in a certain semanticclass (such as object, process, substance, etc.).
Forexample, the supertag Nf shows that a word is anoun in singular (N), means a process (f), and doesnot end in ?ing.
This supertag will be assigned, forexample, to such words as activation or alignment.At present we use 23 supertags that are combina-tions of 1 to 4 features out of a set of 19 semantic,morphological and syntactic features for 14 partsof speech.
For example, the feature structure ofnoun supertags is as follows:Tag [ POS[Noun[object   [plural, singular]process [-ing, other[plural, singular]]substance [plural, singular]other       [plural, singular]]]]]In this lexicon the number of semantic classes(concepts) is domain based.
The ?depth?
of su-pertags is specific for every part of speech andcodes only that amount of the knowledge that isbelieved to be sufficient for our analysis procedure.That means that we do not assign equally ?deep?supertags for every word in this lexicon.
For ex-ample, supertags for verbs include only morpho-logical features such as verb forms (-ing form, -edform, irregular form, finite form).
For finite formswe further code the number feature (plural or sin-gular).
Semantic knowledge about verbs is foundin the predicate lexicon.2.2 Predicate LexiconThis lexicon contains reach and very elaboratedlinguistic knowledge about claim predicates andcovers both the lexical and, crucially for our sys-tem, the syntactic and semantic knowledge.
Ourapproach to syntax is, thus, fully lexicalist.
Below,as an example, we describe the predicate lexiconfor claims on apparatuses.
It was manually ac-quired from the corpus of 1000 US patent claims.Every entry includes the morphological, seman-tic and syntactic knowledge.Morphological knowledge contains a list ofpractically all forms of a predicate that could onlybe found in the claim corpus.Semantic knowledge is coded by associatingevery predicate with a concept of a domain-tunedontology and with a set of case-roles.
The semanticstatus of every case-role is defined as ?agent?,?place?, ?mode?, etc.
The distinguishing feature ofthe case frames in our knowledge base is thatwithin the case frame of every predicate the caseroles are ranked according their weight calculatedon the basis of the frequency of their occurrence inactual corpus together with the predicate.
The setof case-roles is not necessarily the same for everypredicate.Syntactic knowledge includes the knowledgeabout linearization patterns of predicates that codesboth the knowledge about co-occurrences of predi-cates and case-roles and the knowledge about theirliner order in the claim text.
Thus, for example, thefollowing phrase from an actual claim: (1: thesplice holder) *: is arranged (3: on the cover part)(4: to form a rotatable splice holder) (where 1, 3and 4 are case role ranks and ?*?
shows the posi-tion of the predicate), will match the linearizationpattern (1  * 3 4).
Not all case-roles defined for apredicate co-occur every time it appears in theclaim text.
Syntactic knowledge in the predicatedictionary also includes sets of most probable fill-ers of case-roles in terms of types of phrases andlexical preferences.2.3 Grammar and Knowledge RepresentationIn an attempt to bypass weaknesses of differenttypes of grammars the grammar description in ourmodel is a mixture of context free lexicalizedPhrase Structure Grammar and DependencyGrammar formalisms.Our Phrase Structure Grammar consists of anumber of rewriting rules and is specified over aspace of supertags.
The grammar is augmentedwith local information, such as lexical preferenceand some of rhetorical knowledge, - the knowledgeabout claim segments, anchored to tabulations,commas and a period (there can only be one rhet-orically meaningful period in a claim which is justone sentence).
This allows the description of suchphrases as, for example, ?several rotating, spin-ning and twisting elements?.
The head of a phrase(its most important lexical item) is assigned by agrammar rule used to make up this phrase.The second component of our grammar is aversion of Dependency Grammar.
It is specifiedover the space of phrases (NP, PP, etc.)
and a resi-due of ?ungrammatical?
words, i.e., words that donot satisfy any of the rules of our Phrase StructureGrammar.The Dependency Grammar in our model is astrongly lexicalized case-role grammar.
All syntac-tic and semantic knowledge within this grammar isanchored to one type of lexemes, namely predi-cates  (see Section 2.2).
This grammar assigns afinal parse (representation) to a claim sentence inthe form:text::={ template){template}*template::={label predicate-class predicate ((case-role)(case-role)*}case-role::= (rank status value)value::= phrase{(phrase(word supertag)*)}*where label is a unique identifier of the elemen-tary predicate-argument structure (by convention,marked by the number of its predicate as it appearsin the claim sentence, predicate-class is a label ofan ontological concept, predicate is a string corre-sponding to a predicate from the system lexicon,case-roles are ranked according to the frequencyof their cooccurrence with each predicate in thetraining corpus, status is a semantic status of acase-role, such as agent, theme, place, instrument,etc., and value is a string which fills a case-role.Supertag is a tag, which conveys both morphologi-cal information and semantic knowledge as speci-fied in the shallow lexicon (see Section 2.1).
Wordand phrase are a word and phrase (NPs, PPs, etc.
)in a standard understanding.
The representation isthus quite informative and captures to a large ex-tent both morpho-syntactic and semantic propertiesof the claim.For some purposes such set of predicate tem-plates can be used as a final claim representationbut it is also possible to output a unified represen-tation of a patent claim as a tree of predicate-argument templates.3 Analysis algorithmThe analyzer takes a claim text as input and after asequence of analysis procedures produces a set ofinternal knowledge structures in the form of predi-cate-argument templates filled with chunked andsupertagged natural language strings.
The imple-mentation of an experimental version is being car-ried out in C++.
In further description we will usethe example of a claim text shown in Figure 1.The basic analysis scenario for the patent claimconsists of the following sequence of procedures:?
Tokenization?
Supertagging?
Chunking?
Determining dependenciesEvery procedure relies on a certain amount ofstatic knowledge of the model and on the dynamicknowledge collected by the previous analyzingprocedures.The top-level procedure of the claim analyser istokenization.
It detects tabulation and punctuationflagging them with different types of ?border?
tags.Following that runs the supertagging procedure, -a look-up of words in the shallowFigure 2.
A screenshot of the developer tool interface, which shows traces of chunking noun, prepo-sitional, adverbial, gerundial and infinitival phrases in the claim text shown in Figure 1.lexicon (see Section 2.1).
It generates all possibleassignments of supertags to words.Then the supertag disambiguation procedure at-tempts to disambiguate multiple supertags.
It usesconstraint-based hand-crafted rules to eliminateimpossible supertags for a given word in a 5-wordwindow context with the supertag in question inthe middle.
The rules use both lexical, ?supertag?and  ?border?
tags knowledge about the context.The disambiguation rules are of several types, notonly ?reductionistic?
ones.
For example, substitu-tion rules may change the tag ?Present Plural?
into?Infinitive?
(We do not have the ?Infinitive?
fea-ture in the supertag feature space).
If there are stillambiguities pending after this step of disambigua-tion the program outputs the most frequent readingin the multiple supertag.After the supertags are disambiguated the chunk-ing procedure switches on.
Chunking  is carriedout  by  matching  the strings of  supertagsagainst patterns in the right hand side of the rulesin the PG component of our grammar.
?Border?tags are included in the conditioning knowledge.During the chunking procedure we use only asubset of PG rewriting rules.
This subset includesneither the basic rule ?S = NP+VP?, nor any rulesfor rewriting VP.
This means that at this stage ofanalysis we cover only those sentence componentsthat are not predicates of any clause (be it a mainclause or a subordinate/relative clause).
We thus donot consider it the task of the chunking procedureto give any description of syntactic dependencies.The chunking procedure is a succession ofprocessing steps itself starting with the simple-noun-phrase procedure, followed the complex-noun-phrase procedure, which integrates simplenoun phrases into more complex structures (thoseincluding prepositions and conjunctions).
Then theprepositional-, adverbial-, infinitival- and gerun-dial-phrase procedures switch on in turn.Figure 3.
A fragment of the final parse of the sentence in Figure 1.
Fillers of the ?direct-obj?case-role are long distance dependencies of the predicate ?comprising?.The order of the calls to these component proce-dures in the chunking algorithm is established tominimize the processing time and effort.
The or-dering is based on a set of heuristics, such as thefollowing.
Noun phrases are chunked first as theyare the most frequent types of phrases and manyother phrases build around them.
Figure 1 is ascreenshot of the interface of the analysis grammaracquisition tool.
It shows traces of chunking noun,prepositional, adverbial, gerundial and infinitivalphrases in the example of a claim text shown in theleft pane of Figure 3.The next step in claim analysis is the proceduredetermining dependencies.
At this step in additionto PG we start using our DG mechanism.
The pro-cedure determining dependencies falls into twocomponents: determining elementary (one predi-cate) predicate-argument structures and unifyingthese structures into a tree.
In this paper we?ll limitourselves to a detailed description of the first ofthese tasks.The elementary predicate structure procedure,in turn, consists of three components, which aredescribed below.The fist find-predicate component searches forall possible predicate-pattern matches over the?residue?
of ?free?
words in a chunked claim andreturns flagged predicates of elementary predicate-argument structures.
The analyzer is capable toextract distantly located parts of one predicate (e.g.
?is arranged?
from ?A is substantially verticallyarranged on B?
).The second find-case-roles component retrievessemantic (case-roles) and syntactic dependencies(such as syntactic subject), requiring that all andonly dependent elements (chunked phrases in ourcase) be present within the same predicate struc-ture.The rules can use a 5-phrase context with thephrase in question in the middle.
The conditioningknowledge is very rich at this stage.
It includessyntactic and lexical knowledge about phrase con-stituents, knowledge about supertags and ?border?tags, and all the knowledge about the properties ofa predicate as specified in the predicate dictionary.This rich feature space allows quite a good per-formance in solving the most difficult analysisproblems such as, recovery of empty syntacticnodes, long distance dependencies, disambiguationof PP attachment and parallel structures.
There canseveral matches between the set of case-roles asso-ciated with a particular phrase within one predicatestructure.
This type of ambiguity can be resolvedwith the probabilistic knowledge about case-roleweights from the predicate dictionary given themeaning of a predicate.If a predicate is has several meanings then theprocedure disambiguate predicate starts, whichrelies on all the static and dynamic knowledge col-lected so far.
During this procedure, once a predi-cate is disambiguated it is possible to correct acase-role status of a phrase if it does not fit thepredicate description in the lexicon.Figure 3 shows the result of assigning case-roles to the predicates of the claim in Figure 1.
Theset of predicate-arguments structures conforms theformat of knowledge representation given in Sec-tion 2.3.
As we have already mentioned the ana-lyzer might stop at this point.
It can also proceedfurther and unify this set of predicate structuresinto a tree.
We do not describe this rather complexprocedure here and note only that for this purposewe can reuse the planning component of the gen-erator described in (Sheremetyeva and Nirenburg,1996).4 Examples of possible applicationsIn general, the final parse in the format shown inFigure 3 can be used in any patent related applica-tion.
It is impossible to give a detailed descriptionof these applications in one paper.
We thus limitourselves to sketching just two of them, - machinetranslation and improving the readability of patentclaims.Long and complex sentences, of which patentclaims are an ultimate example, are often men-tioned as sentences of extremely low translatability(Gdaniec, 1994).
One strategy currently used tocope with the problem in the MT frame is to auto-matically limit the number of words in a sentenceby cutting it into segments on the basis of thepunctuation only.
In general this results in too fewphrase boundaries (and some incorrect ones, e.g.enumerations).
Another well-known strategy ispre-editing and postediting or/ and using controlledlanguage, which can be problematic for the MTuser.
It is difficult to judgewhether current MT systems use more sophisti-cated parsing strategies to deal with the problemscaused by the length and complexity ofFigure 4.
A screenshot of the user interface of a prototype application for improving the readabilityof patent claims.
The right pane shows an input claim (see Figure 1) chunked into predicates andother phrases (case-role fillers).
The structure of complex phrases can be deployed by clicking onthe ?+?
sign.
The right pane contains the claim text a set of simple sentences.of real life utterances as most system descriptionsare done on the examples of simple sentences.To test our analysis module for its applicabilityfor machine translation we used the generationmodule of our previous application, - AutoPat, - acomputer system for authoring patent claims(Sheremetyeva, 2003), and modeled a translationexperiment within one (English) language, thusavoiding (for now) transfer problems 1  to betterconcentrate on the analysis proper.
Raw claim sen-tences were input into the analyzer, and parsed.The parse was input into the AutoPat generator,which due to its architecture output the ?transla-tion?
in two formats, - as a single sentence, whichis required when a claim is supposed to be in-1 The transfer module (currently under development)transfers every individual SL parse structure into anequivalent TL structure keeping the format of its repre-sentation.
It then ?glues?
the individual structures into atree to output translation as one sentence or generates aset of simple sentences directly from the parse in Figure3.cluded in a patent document, and as a set of simplesentences in TL.
The modules proved to be com-patible and the results of such ?translation?
showeda reasonably small number of failures, mainly dueto the incompleteness of analysis rules.The second type of the translation output (a setof sentences), shows how to use our analyzer in aseparate (unilingual or multilingual) application forimproving the readability of patent claims, whichis relevant, for example, for information dissemi-nation.
Figure 4 is a screenshot of the user inter-face of a prototype of such an application.We are aware of two efforts to deal with theproblem of claim readability.
Shnimory et.
al(2002) investigate NLP technologies to improvereadability of Japanese patent claims concentratingon rhetorical structure analysis.
This approach usesshallow analysis techniques (cue phrases) to seg-ment the claim into more readable parts and visual-izes a patent claim in the form of a rhetoricalstructure tree.
This differs from our final output,which seems to be easier to read.
Shnimory et.
al(cf.)
refer to another NLP research in Japan di-rected towards dependency analysis of patentclaims to support analytical reading of patentclaims.
Unfortunately the author of this paper can-not read in Japanese.
We thus cannot judge our-selves how well the latter approach works.5 Status and Future WorkThe analyzer is in the late stages of implementationas of May 2003.
The static knowledge sourceshave been compiled for the domain of patentsabout apparatuses.
The morphological analysis andsyntactic chunking are operational and well tested.The case-role dependency detection is being cur-rently tested and updated.
The compatibility of theanalyzer and fully operational generator has beenproved and tested.
First experiments have beendone to use the analyzer for such applications asmachine translation and improving claim readabil-ity.
We have not yet made a large-scale evaluationof our analysis module.
This leaves the comparisonbetween other parsers and our approach as a futurework.
The preliminary results show a reasonablysmall number of failures, mainly due to the incom-pleteness of analysis rules that are being improvedand augmented with larger involvement of predi-cate knowledge.We intend to a) add an optional interactivemodule to the analyzer (that would allow for hu-man interference into the process of analysis toimprove its quality), and complete the integrationof the analyzer into a machine translation systemand an application for improving claim readability.Another direction of work is developing applica-tions in a variety of languages (software localiza-tion); b) develop a patent search and extractionfacility on the basis of the patent sublanguage andour parsing strategy.ReferencesAkiro Shnimori, Manabu Okumura,Yuzo Marukawa,and Makoto IwaYama.
2002.
Rethorical StructureAnalysis of Japanese Patent Claims Using CuePhrases.
Proceedings of the Third NTRCIR Work-shop.Aravind K.Joshi and Bangalore Srinivas.
1994.
Disam-biguation of Super Parts of Speech (or Supertags):Almost Parsing.
http://acl.ldc.upenn.edu/C/C94/C94-1024.pdfAtstushi Fujii and Tetsuya Ishikawa.
2002.
NTCIR-3Patent Retrieval Experiments at ULIS.
Proceedingsof the Third NTRCIR Workshop.Claudia Gdaniec.
1994.
The Logos Translatability Indexin Technology  Partnerships for Crossing the Lan-guage Barrier.
Proceedings of the First Conferenceof the Association for Machine Translation in theAmericas (AMTA).Don Blaheta and Eugene Charniak.
2000.
AssigningFunction Tags to Parsed Text.
Proceedings of theNorth American Chapter of the Association of Com-putational Linguistics.Eugene Charniak.
2000.
A Maximum-entropy-inspiredParser.
Proceedings of the North American Chapterof the Association of Computational Linguistics.Michael Collins.
2000.
Discriminative Reranking forNatural Language Parsing.
Machine Learning: Pro-ceedings of the Seventeenth International  Confer-ence (ICML 2000), Stanford California.
USAMelanie Gnasa and Jens Woch.
2002.
Architecture of aknowledge based interactive Information RetrievalSystem.
http://konvens2002.dfki.de/cd/pdf/12P-gnasa.pdfNoriko Kando.
2000.
What Shall we Evaluate?
Prelimi-nary Discussion for the NTCIR Patent IR Challenge(PIC) Based on the Brainstorming with the Special-ized Intermediaries in Patent Searching and PatentAttorneys.
Proceedings of the ACM SIGIR 2000Workshop on Patent Retrieval in conjunction withThe 23rd Annual International ACM SIGIR Confer-ence on Research and Development in Informationretrieval.
Athens.
GreeceSvetlana Sheremetyeva and Sergei Nirenburg.
1996.Generating Patent Claims.
Proceedings of the 8th In-ternational Workshop on Natural Language Genera-tion.
Herstmonceux, Sussex, UK.Svetlana Sheremetyeva 2003.
Towards Designing Natu-ral Language Interfaces.
Proceedings of the 4th Inter-national Conference ?Computational Linguistics andIntelligent Text Processing?
Mexico City, Mexico
