c?
2003 Association for Computational LinguisticsDependency Parsing with an ExtendedFinite-State ApproachKemal Oflazer?Sabanc?
UniversityThis article presents a dependency parsing scheme using an extended finite-state approach.
Theparser augments input representation with ?channels?
so that links representing syntactic depen-dency relations among words can be accommodated and iterates on the input a number of timesto arrive at a fixed point.
Intermediate configurations violating various constraints of projectivedependency representations such as no crossing links and no independent items except senten-tial head are filtered via finite-state filters.
We have applied the parser to dependency parsing ofTurkish.1.
IntroductionFinite-state machines have been used for many tasks in language processing, such astokenization, morphological analysis, and parsing.
Recent advances in the develop-ment of sophisticated tools for building finite-state systems (e.g., XRCE Finite StateTools [Karttunen et al 1996], AT&T Tools [Mohri, Pereira, and Riley 1998], and Fi-nite State Automata Utilities [van Noord 1997]) have fostered the development ofquite complex finite-state systems for natural language processing.
In the last sev-eral years, there have been a number of studies on developing finite-state parsingsystems (Koskenniemi 1990; Koskenniemi, Tapanainen, and Voutilainen 1992; Grefen-stette 1996; Chanod and Tapanainen 1996; Ait-Mokhtar and Chanod 1997; Hobbs etal.
1997).
Another stream of work in using finite-state methods in parsing is basedon approximating context-free grammars with finite-state grammars, which are thenprocessed by efficient methods for such grammars (Black 1989; Pereira and Wright1997; Grimley-Evans 1997; Johnson 1998; Nederhof 1998, 2000).
There have also beena number of approaches to natural language parsing using extended finite-state ap-proaches in which a finite-state engine is applied multiple times to the input, or variousderivatives thereof, until some termination condition is reached (Abney 1996; Roche1997).This article presents an approach to dependency parsing using a finite-state ap-proach.
The approach is similar to those of Roche and Abney in that all three use anextended finite-state scheme to parse the input sentences.
Our contributions can besummarized as follows:?
Our approach differs from Roche?s and Abney?s in that it is based on thedependency grammar approach and at the output produces an encodingof the dependency structure of a sentence.
The lexical items and thedependency relations are encoded in an intertwined manner andmanipulated by grammar rules, as well as structural and linguistic?
Faculty of Engineering and Natural Sciences, Sabanc?
University, Orhanl?, 34956, Tuzla, Istanbul,Turkey.
E-mail: oflazer@sabanciuniv.edu.516Computational Linguistics Volume 29, Number 4constraints implemented as finite-state filters, to arrive at parses.
Theoutput of the parser is a finite-state transducer that compactly packs allthe ambiguities as a lattice.?
As our approach is an all-parses approach with no statistical component,we have used Lin?s (1995) proposal for ranking the parses based on thetotal link length and have obtained promising results.
For over 48% ofthe sentences, the correct parse was among the dependency trees withthe smallest total link length.?
Our approach can employ violable constraints for robust parsing so thatwhen the parser fails to link all dependents to a head, one can uselenient filtering to allow parses with a small number of unlinkeddependents to be output.?
The rules for linking dependents to heads can specify constraints on theintervening material between them, so that, for instance, certain linksmay be prevented from crossing barriers such as punctuation or lexicalitems with certain parts of speech or morphological properties (Collins1996; Giguet and Vergne 1997; Tapanainen and Ja?rvinen 1997).We summarize in Figure 1 the basic idea of our approach.
This figure presents in arather high-level fashion, for a Turkish and an English sentence, the input and outputrepresentation for the approach to be presented.
For the purposes of this summary, weassume that none of the words in the sentences have any morphological ambiguityand that their morphological properties are essentially obvious from the glosses.
Werepresent the input to the parser as a string of symbols encoding the words with someadditional delimiter markers.
Panel (a) of Figure 1 shows this input representation fora Turkish sentence, on the top right, and panel (b) shows it for an English sentence.The parser operates in iterations.
In the first iteration, the parser takes the inputstring encoding the sentence and manipulates it to produce the intermediate stringin which we have three dependency relations encoded by additional symbols (high-lighted with boldface type) injected into the string.
The partial dependency trees en-coded are depicted to the left of the intermediate strings.
It should be noted that thesets of dependency relations captured in the first iteration are different for Turkishand English.
In the Turkish sentence, two determiner links and one object link areencoded in parallel, whereas in the English sentence, two determiner links and onesubject link are encoded in parallel.
The common property of these links is that theydo not ?interfere?
with each other.The second iteration of the parser takes the output of the first iteration and manip-ulates it to produce a slightly longer string in which symbols encoding a new subject(object) link are injected into the Turkish (English) string.
(We again highlight thesesymbols with boldface type.)
Note that in the English string the relative positions ofthe link start and end symbols indicate that this is a right-to-left link.
The dependencystructures encoded by these strings are again on their left.
After the second iteration,there are no further links that can be added, since in each case there is only one wordleft without any outgoing links and it happens to be the head of the sentence.The article is structured as follows: After a brief overview of related work, wesummarize dependency grammars and aspects of Turkish relevant to this work.
Weprovide a summary of concepts from finite-state transducers so that subsequent sec-tions can be self-contained.
We continue by describing the representation that we haveemployed for encoding dependency structures, along with the encoding of depen-dency linking rules operating on these representations and configurational constraints517Oflazer Dependency ParsingFigure 1Dependency parsing by means of iterative manipulations of strings encoding dependencystructures.518Computational Linguistics Volume 29, Number 4for filtering them.
We then describe the parser and its operational aspects, with detailson how linguistically motivated constraints for further filtering are implemented.
Webriefly provide a scheme for a robust-parsing extension of our approach using thelenient composition operation.
We then provide results from a prototype implementa-tion of the parser and its application to dependency parsing of Turkish.
We close withremarks and conclusions.2.
Overview of Related WorkAlthough finite-state methods have been applied to parsing by many researchers,extended finite-state techniques were initially used only by Roche (1997), Abney (1996),and the FASTUS group (Hobbs et al 1997).
In the context of dependency parsing withfinite-state machines, Elworthy (2000) has recently proposed a finite-state parser thatproduces a dependency output.Roche (1997) presents a top-down approach for parsing context-free grammarsimplemented with finite-state transducers.
The transducers are based on a syntacticdictionary comprising patterns of lexical and nonlexical items.
The input is initiallybracketed with sentence markers at both ends and then fed into a transducer forbracketing according to bracketing rules for each of the patterns in the dictionary.The output of the transducer is fed back to the input, and the constituent structure isiteratively refined.
When the output of the transducer reaches a fixed point, that is,when no additional brackets can be inserted, parsing ends.Abney (1996) presents a finite-state parsing approach in which a tagged sentenceis parsed by transducers that progressively transform the input into sequences of sym-bols representing phrasal constituents.
In this approach, the input sentence is assumedto be tagged with a part-of-speech tagger.
The parser consists of a cascade of stages.Each stage is a finite-state transducer implementing rules that bracket and transducethe input to an output containing a mixture of unconsumed terminal symbols andnonterminal symbols for the phrases recognized.
The output of a stage goes to thenext stage in the cascade, which further brackets the input using yet other rules.Each cascade typically corresponds to a level in a standard X-bar grammar.
After acertain (fixed) number of cascades, the input is fully bracketed, with the structuresbeing indicated by labels on the brackets.
Iterations in Roche?s approach roughly cor-respond to cascades in Abney?s approach.
The grammar, however, determines thenumber of levels or cascades in Abney?s approach: that is, structure is fixed.
A workalong the lines of Abney?s is that of Kokkinakis and Kokkinakis (1999) for parsingSwedish.Elworthy (2000) presents a finite-state parser that can produce a dependency struc-ture from the input.
The parser utilizes standard phrase structure rules that are anno-tated with ?instructions?
that associate the components of the phrases recognized withdependency grammar?motivated relations.
A head is annotated with variables associ-ating it with its dependents.
These variables are filled in by the instructions associatedwith the rules.
These variables are copied or percolated ?up?
the rules according tospecial instructions.
The approach resembles a unification-based grammar in whichinstead of unification, dependency relation features are passed from a dependent toits head.
The rules for recognizing phrases and implementing their instructions areimplemented as finite-state transducers.Another notable system for finite-state parsing is FASTUS (Hobbs et al 1997).
FAS-TUS uses a five-stage cascaded system, with each stage consisting of nondeterministicfinite-state machines.
FASTUS is mainly for information extraction applications.
Theearly stages recognize complex multiword units such as proper names and colloca-519Oflazer Dependency Parsingtions and build upon these by grouping them into phrases.
Later stages are gearedtoward recognizing event patterns and building event structures.3.
Dependency SyntaxDependency approaches to syntactic representation use the notion of syntactic relationto associate surface lexical items.
Melc?uk (1988) presents a comprehensive expositionof dependency syntax.
Computational approaches to dependency syntax have recentlybecome quite popular (e.g., a workshop dedicated to computational approaches to de-pendency grammars was held at COLING/ACL?98).
Ja?rvinen and Tapanainen (1998;Tapanainen and Ja?rvinen 1997) have demonstrated an efficient wide-coverage depen-dency parser for English.
The work of Sleator and Temperley (1991) on link grammar,essentially a lexicalized variant of dependency grammar, has also proved to be interest-ing in regard to a number of aspects.
Dependency-based statistical language modelingand parsing have also become quite popular in statistical natural language processing(Lafferty, Sleator, and Temperley 1992; Eisner 1996; Chelba et al 1997; Collins 1996;Collins et al 1999).Robinson (1970) gives four axioms for well-formed dependency structures thathave been assumed in almost all computational approaches.
These state that, in adependency structure of a sentence,1.
one and only one word is independent, that is, not linked to some otherword;2. all others depend directly on some word;3. no word depends on more than one other; and4.
if a word A depends directly on word B, and some word C intervenesbetween them (in linear order), then C depends directly on A or on B, oron some other intervening word.This last condition of projectivity (or various extensions of it; see, e.g., Lai and Huang[1994]) is usually assumed by most computational approaches to dependency gram-mars as a constraint for filtering configurations and has also been used as a simplifyingcondition in statistical approaches for inducing dependencies from corpora (e.g., Yu?ret1998).14.
TurkishTurkish is an agglutinative language in which a sequence of inflectional and deriva-tional morphemes get affixed to a root (Oflazer 1993).
At the syntax level, the un-marked constituent order is Subject-Object-Verb, but constituent order may vary asdemanded by the discourse context.
Essentially all constituent orders are possible,especially at the main sentence level, with very minimal formal constraints.
In writ-ten text, however, the unmarked order is dominant at both the main-sentence andembedded-clause level.Turkish morphophonology is characterized by a number of processes such asvowel harmony (vowels in suffixes, with very minor exceptions, agree with previous1 See section 6 for how projectivity is checked and section 6.5 on the implications of checking forprojectivity during parsing with both right-to-left and left-to-right dependency links.520Computational Linguistics Volume 29, Number 4vowels in certain aspects), consonant agreement, and vowel and consonant ellipsis.The morphotactics are quite complicated: A given word form may involve multiplederivations (as we show shortly).
The number of word forms one can generate froma nominal or verbal root is theoretically infinite (see, e.g., Hankamer, [1989]).Derivations in Turkish are very productive, and the syntactic relations that a wordis involved in as a dependent or head element are determined by the inflectionalproperties of the one or more (possibly intermediate) derived forms.
In this work, weassume that a Turkish word is represented as a sequence of inflectional groups (IGs),separated by ^DBs, denoting derivation boundaries, in the following general form:root+IG1 + ?DB+IG2 + ?DB+?
?
?
+ ?DB+IGn.Here each IGi denotes relevant inflectional features including the part of speech forthe root, for the first IG, and for any of the derived forms.
For instance, the derivedmodifier sag?lamlas?t?rd?g?
?m?zdaki2 would be represented as3sag?lam+Adj+^DB+Verb+Become+^DB+Verb+Caus+Pos+^DB+Noun+PastPart+A3sg+P3sg+Loc+^DB+AdjThe five IGs in this are1.
sag?lam(strong)+Adj2.
+Verb+Become3.
+Verb+Caus+Pos4.
+Noun+PastPart+A3sg+P3sg+Loc5.
+AdjThe first shows the root word along with its part of speech, which is its only inflec-tional feature.
The second IG indicates a derivation into a verb whose semantics is?to become?
the preceding adjective.
The +Become can be thought of as a minor part-of-speech tag.
The third IG indicates that a causative verb with positive polarity isderived from the previous verb.
The fourth IG indicates the derivation of a nominalform, a past participle, with +Noun as the part of speech and +PastPart.
It has hasother inflectional features: +A3sg for third-person singular, +P3sg for third-person sin-gular possessive agreement, and +Loc for locative case.
Finally the fifth IG indicates aderivation into an adjective.A sentence would then be represented as a sequence of the IGs making up thewords.
An interesting observation that we can make about Turkish is that, when aword is considered as a sequence of IGs, syntactic relation links emanate only fromthe last IG of a (dependent) word and land on one of the IGs of a (head)word on the2 Literally, ?
(the thing existing) at the time we caused (something) to become strong?.
Obviously this isnot a word that one would use everyday.
Turkish words found in typical text average three to fourmorphemes including the stem, with an average of about 1.7 derivations per word.3 The morphological features other than the obvious part-of-speech features are +Become: become verb,+Caus: causative verb, PastPart: derived past participle, P3sg: third-person singular possessiveagreement, A3sg: third-person singular number-person agreement, +Zero: zero derivation with no overtmorpheme, +Pnon: no possessive agreement, +Loc: locative case, +Pos: positive polarity.521Oflazer Dependency ParsingFigure 2Links and inflectional groups.Figure 3Dependency links in an example Turkish sentence.right (with minor exceptions), as exemplified in Figure 2.
A second observation is that,with minor exceptions, the dependency links between the IGs, when drawn above theIG sequence, do not cross.4 Figure 3 shows a dependency tree for a Turkish sentencelaid on top of the words segmented along IG boundaries.
It should be noted that allIGs that link to the same head IG comprise a constituent, and the legality of a linkdepends primarily on the inflectional features of the IGs it connects.For the purposes of this article we can summarize aspects of Turkish as follows:?
The IGs are the ?words.?
That is, we treat a chunk of (free and bound)inflectional morphemes as the units that we relate with dependencylinks.?
Within a word, the IGs are linearly dependent on the next IG, if any.
Wewould not, however, show and deal with these explicitly, but rather dealonly with the dependency link emanating from the last IG in each word,which is the syntactic head of the word (whereas the first IG whichcontains the root is the semantic head).4 Such cases would be violating the projectivity constraint.
The only examples of such crossing that weknow are certain discontinuous noun phrases in which an adverbial modifier of the matrix verbintervenes between a specifier and the rest of the noun phrase.
Since the specifier links to the headnoun but the adverbial links to the verb, the links have to cross.522Computational Linguistics Volume 29, Number 4?
For all practical purposes the syntactic dependency links go from left toright, that is, the core structure is subject-object-verb, and modifiersprecede their heads.5.
Finite-State TransducersThe exposition in the subsequent sections will make extensive use of concepts fromfinite-state transducers.
In this section, we provide a brief overview of the main rel-evant concepts; the reader is referred to recent expositions (e.g., Roche and Schabes[1997]; also, Hopcroft and Ullman [1979] provides a detailed exposition of finite-statemachines and regular languages.
)Finite-state transducers are finite-state devices with transitions labeled by pairs ofsymbols (u:l), u denoting the ?upper?
symbol and l denoting the ?lower?
symbol.These symbols come from a finite alphabet.
Additionally, either u or l (but not both)can be the  symbol, denoting the empty string.
A finite-state transducer T mapsbetween two regular languages: U, the ?upper?
language, and L, the ?lower?
language.The mapping is bidirectional, and in general, a string in one of the languages maymap to one or more strings in the other language.
The transductions in both directionsare valid only if the string on the input side takes the finite-state transducer to a finalstate.The behavior of finite-state transducers can also be described using regular ex-pressions over an alphabet of symbols of the form (u:l) (including symbols :l andu:), in complete analogy to regular expressions for finite-state recognizers.
Since thenotational mechanisms provided by the basic definition of regular expressions (con-catenation, union, and Kleene star [Hopcroft and Ullman 1979]) are quite restricted andlow level, developers of finite-state transducer manipulation systems have augmentedthe notational capabilities with operations at a much higher level of abstraction, muchcloser to the operations used by the computational linguistics application (see, e.g.,Karttunen et al, [1996]; see also http://www.xrce.xerox.com/competencies/content-analysis/fsCompiler/fssyntax.html, and also van Noord, [1997]).Finite-state transducers are closed under union, but in contrast to finite-state rec-ognizers, they are not closed under difference and intersection operations (Kaplan andKay 1994).
On the other hand, finite-state transducers are closed under the operationof composition, which is very much an analog of function composition in algebra.
LetT1 be a transducer that maps between regular languages U1 and L1, and let T2 be atransducer that maps between regular languages U2 and L2.
The composition T of T1and T2, denoted by T1 ?
T2, is the transducer that maps between U = T?11 (L1 ?
U2)and L = T2(L1 ?
U2).5 That is, the resulting mapping is defined only for the respectiveimages, in T?11 and T2, of the intersection L1 ?
U2.
A pair of strings (x, y) ?
T1 ?
T2 ifand only if ?z such that (x, z) ?
T1 and (z, y) ?
T2.
Note that the composition operationis order dependent; T1 ?
T2 is not the same mapping as T2 ?
T1.
Figure 4 summarizesthe main points of the composition operation for finite-state transducers.6.
Finite-State Dependency ParsingOur approach is based on constructing a graphic representation of the dependencystructure of a sentence, including the lexical items and the labeled directed arcs en-5 Notationally, for a transducer T, we take T(U) to mean the transduction from the upper to the lowerlanguage and T?1 to mean the transduction from the lower to the upper language.523Oflazer Dependency ParsingFigure 4Composition operation for finite-state transducers.Figure 5Physical representation and logical view of channels and dependency links.coding the dependency relations.
In constructing this dependency graph, these labeledlinks are represented by additional symbols that are laid out within the symbols rep-resenting the lexical items and their morphological features.The approach relies on augmenting the input with ?channels?
that (logically) re-side above the IG sequence and ?laying?
links representing dependency relations inthese channels, as depicted in Figure 5(a).
The input to the parser is a representation ofthe sentence as a sequence (or a lattice, if morphological disambiguation has not beenperformed) of IGs with some additional symbols to delineate certain boundaries.6 The6 The lattice of all morphological analyses of the words in a sentence can be encoded by a finite-stateacceptor.
In fact, one gets such an acceptor when a nondeterministic morphological analysis finite-statetransducer is applied to the input.
Further, finite-state acceptors are assumed to be coerced into identitytransducers that map the input strings they accept to identical output strings.
This coercion is necessaryso that filters defined as acceptors can be used as transducers with the composition operators.524Computational Linguistics Volume 29, Number 4Figure 6Channel symbol slots around an inflectional group.parser operates in a number of iterations: At each iteration of the parser, a new emptychannel is ?stacked?
on ?top?
of the input, and any possible links are established us-ing these channels.7 Parsing terminates when no new links can be established withinthe most recent channel added, that is, when a fixed point is reached.
In this respect,this approach is similar to that of Roche (1997).
An abstract view of this is presentedin panels (a) through (c) of Figure 5.6.1 Representing Channels and Syntactic RelationsThe sequence (or the lattice) of IGs is produced by a morphological analysis transducer,with each IG initially being augmented by two pairs of delimiter symbols, as <(IG)>.The ( and ) pair separates the morphological features from the channel representationsymbols, while < and > separate the representations of consecutive IGs.
Word-final IGs(IGs from which links will emanate) are further augmented with a special marker @.Channels are represented by pairs of matching symbols that are inserted betweenthe <.
.
.
( and the ).
.
.
> delimiter symbols.
Symbols for new channels (upper channelsin Figure 5) are stacked so that the symbols for the topmost channels are those closest tothe (.
.
.
), and in this way dependency links do not cross when drawn (see Figure 6).At any time, the number of channel symbols on both sides of an IG is the same.Multiple dependency links can occupy mutually exclusive segments of a channel aslong as they do not interfere with each other; that is, each channel may accommodatemany dependency links whenever possible.8How a certain segment of channel is used is indicated by various symbols sur-rounding the IGs, within the < and > delimiters:?
The channel symbol 0 indicates that the channel segment is not used byany dependency link and thus is empty.?
The channel symbol 1 indicates that the channel is used by a link thatstarts at some IG on the left and ends at some IG on the right.
That is,the link is just ?crossing over?
this IG.?
When a link starts from a word-final IG, then a link start symbol is usedon the right side of the word-final IG (i.e., between ) and >).?
When a link terminates on an IG, then a link end symbol denoting thesyntactic relation is used on the left side of the IG (i.e., between < and ().7 The beginnings and the ends of the arrows in the figure indicate the dependent and head IGs,respectively.8 The exposition here is for only left-to-right links.
See section 6.5 for a dependency representation forboth left-to-right and right-to-left links.525Oflazer Dependency ParsingThe following syntactic relations are currently encoded in the channels:1.
Subject (s/S)2.
Object (o/O)3.
Modifier (adverbs/adjectives) (m/M) )4.
Possessor (p/P)5.
Classifier (c/C)6.
Determiner (d/D)7.
Dative adjunct (t/T)8.
Ablative adjunct (f/F)9.
Locative adjunct (l/L)10.
Instrumental adjunct (i/I)The lowercase symbol in each case is used to indicate the start of a link, and theuppercase symbols indicate the end of a link.
Both kinds of symbols are used toencode configurational and linguistic constraints on IGs, as we show later.For instance, with three channels, the dependency structure of the IGs of bu eskievdeki gu?lu?n (of the rose at this old house) in Figure 3 would be represented as<000(bu+Det@)0d0><010(eski+Adj@)01m><MD0(ev+Noun+A3sg+Pnon+Loc)000><000(+Adj@)00m><M00(gu?l+Noun+A3sg+Pnon+Gen@)0p0>)The M and the D to the left of the first IG of evdeki (third IG above) indicate theincoming modifier and determiner links from the first two IGs, matching the startsymbols m and d in the second and the first IGs.
The m--M pair encodes the modifierlink from eski (old) to evde (at house), and the d--D pair encodes the determiner linkfrom bu (this) to evde.
The last IG above has an M on the left side matching the m inthe IG to the left.
This m--M pair encodes the modifier relation between +ki and gu?lu?n(of the rose).
The last IG above has an outgoing possessor link marked by the p onits right side, indicating that it is a genitive-marked possessor of some other IG to theright.We should note, however, that the (morphological) relations between IGs thatmake up a single word are not at all a concern here and are not considered to besyntactic dependency relations.
Thus they are never explicitly shown or encoded ex-cept by virtue of their being sequentially placed in the input.
The only links that weexplicitly encode are those links emanating from a word-final IG and landing on someother IG.6.2 Components of the ParserThe basic strategy of a parser iteration is to recognize, by means of a rule (encodedas a regular expression), a dependent IG and a head IG and link them by modifyingthe ?topmost?
channel between the two.
Once we identify the dependent IG and thehead IG (in a manner to be described shortly), we proceed as follows:1.
We create an empty channel by injecting 0s to just outside of the (...)pairs:526Computational Linguistics Volume 29, Number 4<...0(IDdep@)0...>...<...0(IG)0...>...<...0(IGhead)0...>2.
We put temporary braces (of the type for the dependency link to beestablished?we use MOD below for a modifier link for expositorypurposes) to the right of the dependent IG and to the left of the head IG:<...0(IGdep{MOD@)0...>...<...0(IG)0...>...<...0MOD}(IGhead)0...>3.
We mark the start, intermediate, and end IGs of the link with theappropriate symbols encoding the relation thus established by the braces:<...0(IGdep{MOD@)m...>...<...1(IG)1...>...<...MMOD}(IGhead)0...>4.
We remove the temporary braces, and we have the string in item 1, withsome symbols modified:<...0(IGdep@)m...>...<...1(IG)1...>...<...M(IGhead)0...>The second step above deserves some more attention, as it has additional functionsbesides identifying the relevant IGs.
One should be careful to avoid generating stringsthat are either illegal or redundant or cannot lead to a valid parse at the end of theiterations.
Thus, the second step makes sure that1.
The last channel in the segment to be bracketed is free.2.
The dependent is not already linked at one of the lower channels (sincean IG can be the dependent of only one other IG).3.
None of the channels directly underneath the segment have any linkscoming into or going out of the projection, in those channels, of thesegment bracketed.
This makes sure that there are no crossing links.
It isobviously okay to have links that start and terminate in lower channelswithin the projection of the bracketed segment.94.
There are no channels below the current channel that are unused in thesegment to be bracketed (if there are, then this link could have beenmade there.)5.
The link to be established does not trap an unlinked word-final IG.
Ifthere is such an IG, its future link would have to cross the link to beestablished in the current segment.The last three of these constraints are depicted in Figure 7.6.3 Rules for Establishing Dependency LinksThe components of a dependency link are recognized using regular expressions.
Theseregular expressions identify the dependent IG, the head IG, and the IGs in between tobe skipped over, and they temporarily bracket the input segment including these IGs.9 It is actually possible to place a crossing link by laying it in a special channel below the IG sequence sothat it would not interfere with the other links.
This would necessitate additional delimiter symbolsand would unnecessarily further complicate the presentation.527Oflazer Dependency ParsingFigure 7Configurations to be avoided during parsing.These regular expressions correspond to ?grammar rules,?
and a collection of theserules comprise the dependency grammar.A typical rule looks like the following:10[ LR [ML IGMiddle MR]* RL ] (->) "{Rel" ... "Rel}" || IGDep IGHeadThis rule is an example of a XRCE optional-replace rule that nondeterministicallyinserts the curly braces on the right-hand side (the symbols on both sides of theellipsis) into the output string in the lower language, around any part of the inputstring in the upper language that matches its left-hand side, provided the left-handside is contextually constrained on the left by IGDep and on the right by IGHead.
Thisreplace rule can nondeterministically make multiple nonoverlapping replacements.11The left-hand side of this rule (to the left of (->)) has three components: Thefirst part, LR, specifies the constraints on the right-hand side of the dependent IG.The second part, [ML IGMiddle MR]*, defines any middle IGs that will be ignored10 We use the XRCE regular expression language syntax; the [ and ] act like parentheses used asgrouping operators in the language.
See http://www.xrce.xerox.com/competencies/content-analysis/fsCompiler/fssyntax.html for details.11 An earlier implementation of the parser used a slightly different optional-replace rule that did notmake use of the contextual constraint, as the new format was not included in the toolkit available tothe author.
A typical rule there looked like [[LL IGDep LR] [ML IGMiddle MR]* [RL IGHead RR]](->) "{Rel" ... "Rel}".
Although for the purposes of writing the dependency grammar, the old ruleformat was more transparent, its use necessitated some extra complexity in various other componentsof the parser.
The old rule format has been abandoned in favor of the new format.
I thank ananonymous reviewer for suggesting the use of this new rule format.528Computational Linguistics Volume 29, Number 4and skipped over,12 and the third part, RL, specifies the contraints on the left-handside channel symbols of the head IG.
The head and dependent IG patterns IGDep andIGHead are specified as left and right contextual constraints on the pattern of the threecomponents specified on the left-hand side.This rule (optionally) brackets (with {Rel and Rel}) any occurrence of pattern LR[ML IGMiddle MR] RL provided the pattern IGDep is to the left of LR and the patternIGHead is to the right of RL.13 After the (optional) bracketing, the brace {Rel occursbetween IGDep and LR, and the brace Rel} occurs between RL and IGHead.
Each rulehas its own brace symbol depending on the relationship of the dependent and thehead.
The optionality is necessary because a given IG may be related to multiple IGsas a result of syntactic ambiguities, and all such links have to be produced to arriveat the final set of parses.
It should also be noted that there are rules that deviate fromthe template above in that the segment to be skipped may be missing, or may containbarrier patterns that should not be skipped over, etc.The symbols L(eft)R(ight), M(iddle)L, MR, and RL are regular expressions thatencode constraints on the bounding channel symbols that are used to enforce some ofthe configurational constraints described earlier.
LetRightChannelSymbols = [ "1" | "0" | "s" | "o" | "m" | "p" |"c" | "d" | "t" | "l" | "f" | "i" ];andLeftChannelSymbols = [ "1" | "0" | "S" | "O" | "M" | "P" |"C" | "D" | "T" | "L" | "F" | "I" ];These four regular expressions are defined as follows:1.
The regular expression LR = ["@" ")" "0" ["0"]* ">" ] checks thata.
The matching IG is a word-final IG (has a @ marker)b.
The right-side topmost channel is empty (channel symbolnearest to ) is 0)c. The IG is not linked to any other in any of the lower channelsd.
No links in any of the lower channels cross into this segment(that is, there are no 1s in lower channels.
)These conditions imply that the only channel symbol that may appear inthe right side of a dependent IG is 0.2.
The regular expression ML = ["<" LeftChannelSymbols* "0" "(" ]ensures that the topmost channel is empty, but it does not constrain thesymbols in the lower channels, if any, as there may be other links endingat the matching IG.3.
Similarly, the regular expressionMR = [ ")" "0" RightChannelSymbols* ">" ] also ensures that thetopmost channel is empty, but it does not constrain the symbols in thelower channels, if any, as there may be other links starting at thematching IG.12 It is possible that the pattern to be skipped over can be specified by more complex patterns.13 We use the symbols {Rel and Rel} as generic bracketing delimiters.529Oflazer Dependency Parsing4.
The regular expressionRL =[ "<" [LeftChannelSymbols* - $1] "0" "(" ] also ensures thatthe topmost channel is empty.
Note that since the matching IG is the IGof the head, multiple dependency links may end at the matching IG, sothere are no constraints on the symbols in the lower channels, but therecannot be any 1s on the left side, since that would imply a lower linkcrossing to the right side.For instance, the rule[ LR [ ML AnyIG MR ]* RL ] (->) "{SBJ" ... "SBJ}" ||NominativeNominalA3pl _ FiniteVerbA3sgA3pl;is used to bracket a segment starting right after a plural nominative nominal, as subjectof a finite verb somewhere on the right, with either +A3sg or +A3pl number-personagreement (allowed in Turkish).
In this rule, the regular expression Nominative-NominalA3pl is defined as follows:[ (RootWord) ["+Noun" |"+Pron"] (NominalType) "+A3pl"PossessiveAgreement "+Nom"]and it matches any nominal IG (including any derived nominals) with nominativecase and +A3pl agreement.
There are a number of points to note in this expression:1.
(.
.
. )
indicates optionality: The RootWord, a regular expression matchinga sequence of one or more characters in the Turkish alphabet, is optional,since this may be a derived noun for which the root would be in aprevious IG.2.
NominalType, another optional component, is a regular expressionmatching possible minor part-of-speech tags for nouns and pronouns.3.
PossessiveAgreement is a regular expression that matches all possiblepossessive agreement markers.4.
The nominal has third-person plural agreement and nominative case.The order of the components of this regular expression corresponds to the order ofthe morphological feature symbols produced by the morphological analyzer for anominal IG.
The regular expression FiniteVerbA3sgA3pl matches any finite-verb IGwith either +A3sg or +A3pl number-person agreement.
The regular expression AnyIGmatches any IG.All the rules in the dependency grammar written in the form described are groupedtogether into a parallel bracketing regular expression defined as follows:Bracket = [[LR [ML IGMiddle1 MR]* RL] (->) "{Rel1" ... "Rel1}"|| IGDep1 IGHead1,,[LR [ML IGMiddle2 MR]* RL] (->) "{Rel2" ... "Rel2}"|| IGDep2 IGHead2,,.
.
.
[LR [ML IGMiddlen MR]* RL] (->) "{Reln" ... "Reln}"|| IGDepn IGHeadn];530Computational Linguistics Volume 29, Number 4where left-hand-side patterns and dependent and head IGs are specified in accordancewith the rule format given earlier.
{Reli and Reli} are pairs of braces; there is a distinctpair for each syntactic relation to be identified by these rules (and not necessarily aunique one for each rule).
This set of rules will produce all possible bracketings of theinput IG sequence, subject to the constraints specified by the patterns.
This overgen-eration is then filtered (mostly at compile time and not at parse time) by a number ofadditional configurational and linguistic constraints that are discussed shortly.6.4 Constructing the Parsing TransducerIn this section, we describe the components of the parsing transducer.
As stated earlier,links are established in a number of iterations.
Each iteration mainly consists of anapplication of a parsing transducer followed by a filtering transducer that eliminatescertain redundant partial parse configurations.14The parsing transducer consists of a transducer that inserts an empty channelfollowed by transducers that implement steps 2 to 4 described at the beginning ofsection 6.2.We can write the following regular expression for the parser transducer as15Parser = AddChannel.o.Bracket.o.FilterEmptySegments.o.MarkChannels.o.RemoveBraces;The transducer AddChannel is a simple transducer that adds a pair of 0 channel sym-bols around the (...) in the IGs.
It implements step 1 in section 6.2.
The transducerBracket was defined in the previous section.
It implements step 2 described in sec-tion 6.2.Since the bracketing rules are nondeterministic, they will generate many config-urations in which certain segments in the stacked channels will not be used.
A rulemay attempt to establish a link in the topmost channel even though the correspond-ing segment is not utilized in a previous channel (e.g., the corresponding segment ofone of the previous channels may be all 0s).
One needs to eliminate such redundantconfigurations after each iteration to prevent their proliferation at later iterations ofthe parser.
Checking whether the segment just underneath the topmost channel isempty has worked perfectly in our experiments in that none of the parses selectedhad any empty segments that were not detected by this test.
The regular expressionFilterEmptySegments filters these configurations, an example of which is depicted inFigure 7(b).1614 We use the term configuration to denote an encoding of a (partial) dependency parse as a string ofsymbols, as described in section 6.1.15 The .o.
operator denotes the composition operation (denoted using ?
in section 5) for finite-statetransducers in the XRCE regular expression language.16 Incorporating this configurational constraint into the bracketing phase implies that each rule will havea check encoding the fact ?it is not the case that all symbols in the channel immediately below are 0,?which makes all rules a bit awkward.
Incorporating this as a separate postbracketing constraint issimpler.
Since all compositions are done at compile time, no additional penalty is incurred.531Oflazer Dependency ParsingThe transducer MarkChannels implements step 3 in section 6.2.
This transducermodifies the channel symbols to mark a link:?
The new (topmost) right channel symbol in the IG just to the right of theopening brace is modified to a link start symbol (one of the symbols s,o, m, p, c, d, t, l, f, i).?
The new (topmost) right channel symbols on both sides of all the IGsfully bracketed by the braces (all IGs except the dependent and headIGs) are modified to 1; this is necessary so that that segment of thechannel can be claimed and used for detecting crossing links.?
The new (topmost) left channel symbol in the IG just to the left of theclosing brace is modified to a link end symbol (one of the symbols S, O,M, P, C, D, T, L, F, I).Finally, the transducer RemoveBraces removes the braces.17 It should be noted that thetransducer for Parse is computed off-line at compile time, so that no composition isdone at parse time.The parsing scheme described above is bottom up, in that the links between closesthead-dependent pairs are established first, in the lowest channel.
Subsequent longer-distance links are established in later stages as long as they do not conflict with linksestablished in lower channels.
It is also conceivable that one could employ a top-downparsing scheme linking all pairs that are far apart, again checking for configurationalconstraints.
If full nondeterminism is maintained in the bracketing step, it really doesnot matter whether one uses bottom-up or top-down parsing.
Bottom-up parsing,however, offers certain advantages in that various (usually linguistically motivated)constraints that have to hold between nearby pairs or pairs that have to be immediatelysequential can be enforced at an earlier stage by using simpler regular expressions.These constraints help prune the intermediate parse strings.6.5 Dependency Structures with Both Left-to-Right and Right-to-Left LinksAlthough the formulation up until now has been one for dependency structures in-volving left-to-right dependency links, the approach presented above can handle adependency grammar with both left-to-right and right-to-left links.
In this section, wewill outline the details of the changes that would be needed in such a formulationbut will then go ahead with the left-to-right implementation, as that forms the basisof our implementation for Turkish, for which left-to-right links suffice for all practicalpurposes.Incorporating the right-to-left links into the approach would require the followingmodifications to the formulation:1.
The right-to-left links would use the same representation as theleft-to-right links, except they would be distinguished by the symbolsmarking the links at the dependent and head IG sites.
With theleft-to-right links described so far, lowercase link symbols on the rightside of an IG mark the dependent IG and uppercase symbols on the left17 The details of the regular expressions for these transducers are rather uninteresting.
They areessentially upper-side to lower-side contextual-replace regular expressions.
For instance, RemoveBracesmaps all brace symbols on the upper side to  on the lower side.532Computational Linguistics Volume 29, Number 4Figure 8Bidirectional dependency links.side of the IG mark the head IG (which follows the dependent IG inlinear order).
We would still use the same conventions for right-to-leftlinks, except that we could have head and dependent IG markers onboth sides of the channel representation.
This is shown graphically inFigure 8.
So a right-to-left link would have a lowercase link mark on theleft side of the dependent IG and an uppercase link mark on the rightside of the head IG to the left of the dependent IG.2.
With both left-to-right and right-to-left rules, we would need twodifferent rule formats.
The rule format for left-to-right links would beslightly different from the format given earlier:[ LRl [ML IGMiddle MR]* RLl ] (->) "{Rel-left-to-right"... "Rel-left-to-right}"|| LL IGDep IGHead Therule format for right-to-left links would be:[ LRr [ML IGMiddle MR]* RLr ] (->) "{Rel-right-to-left"... "Rel-right-to-left}"|| IGHead IGDep RR3.
Since nothing in the format of the rules indicates the direction of thelink, the direction would need to be indicated by the type of braces thatare used to (temporarily) mark the segment claimed for the link.
Forinstance, for a left-to-right rule to link a subject IG to a verb IG, wewould use braces {SBJ-left-to-right and SBJ-left-to-right} and fora right-to-left rule (for the same kind of relation), we would use symbols{SBJ-right-to-left and SBJ-right-to-left}.
The transducer thatinserts the appropriate markers for links (MarkChannels in section 6.4)would then execute the appropriate action based on the type and thedirection indication of the delimiting braces.
For left-to-right braces itwill insert the (lowercase) link start symbol to the right side of the leftbrace and the (uppercase) link end symbol to the left side of the rightbrace.
For right-to-left braces, it will insert the link start symbol to theleft side of the right brace and the link end symbol to the right side ofthe left brace.4.
The regular expressions checking the channel symbols around thedependent and head IGs would be different for the two types of rules.This is basically necessitated by the fact that since the IGs could nowhave links outgoing from both sides, checks have to be made on bothsides:?
LRl in left-to-right rules would check that the dependent IG is aword-final IG and is not already linked and that no links are533Oflazer Dependency Parsingcrossing in or out.
So it would function like LR, described insection 6.3.?
LL, just to the left of the IGDep pattern, would also make surethat the IG is not linked, via a right-to-left link, to an IG furtherto the left.?
RLl would function just like RL, described in section 6.3.?
LRr, which, for right-to-left rules, would be constraining the leftchannel symbols of the head IG, would need only to ensure thatthe top channel is available for a link and that no other links arecrossing in and out.?
RLr would ensure that the dependent IG is not linked to any IGto the left and that there are no links crossing, and that the topchannel is available.?
RR, just to the right of IGDep in the right-to-left rule, wouldmake sure that the dependent IG is not linked to any IG to theright and would additionally check that the top channel isavailable and that no links are crossing.There is, however, a potential problem for a grammar with both left-to-right andright-to-left links.
Robinson?s axioms (see section 3) do not seem to disallow cyclicdependency links (unless the antisymmetry is interpreted to apply over the transitiveclosure of the ?depends on?
relationship), but configurations involving cycles are notassumed to correspond to legitimate dependency structures.When both left-to-right and right-to-left links exists in a grammar, it is conceivablethat two left-to-right rules may separately posit two left-to-right links, so that IG Alinks to IG B, IG B (or the word-final IG of the word to which B belongs) links to IGC, and later in a subsequent iteration, a right-to-left rule posits a link from IG C (orthe word-final IG of the word to which C belongs) to IG A, where IG A precedes IG B,which precedes IG C in linear order.
An implementation for a grammar would haveto recognize such circular structures and eliminate them.
It is possible to filter some ofthese cyclic configurations using a finite-state filter, but some will have to be checkedlater by a non-finite-state procedure.If all but one of the links forming a cycle are established in the same channel (e.g.,following the example above, the links from IG A to IG B and from IG B to IG Care established in the next-to-the-topmost channel), the cycle-forming link has to beestablished in the (current) topmost channel (that is, the right-to-left link from IG Cto IG A has to be established there; otherwise the configuration will be filtered by therule that says links have to established as the earliest possible channel).
In order fora cycle to form in this case, IGs A, B, and C with have to be in sequential words, andthe cycle-inducing link and the other links in the ?other?
direction will all be side byside.
A set of simple regular expressions can recognize if a series of pairs of link startand link end symbols in one direction all appearing in the next-to-top channel (i.e.,the second symbol to the left and right of ( and ), respectively, are surrounded by alink end?link start pair for the cycle-inducing link in the other direction) and kill anysuch configurations.If, however, cycles are induced by links appearing in more than two different chan-nels, then there is no elegant way of recognizing these in the finite-state framework,and such cases would have to be checked through other means.534Computational Linguistics Volume 29, Number 46.6 Iterative Application of the ParserFull parsing consists of iterative applications of the Parser transducer until a fixedpoint is reached.
It should be noted that in general, different dependency parses of asentence may use different numbers of channels, so all these parses have to be collectedduring each iteration.Let Sentence be a transducer that represents the word sequence.
The pseudocodefor iterative applications of the parser is given as follows:# Map sentence to a transducer representing a lattice of IGsM = [Sentence .o.
MorphologicalAnalyzer];# Initialize ParsesParses = { };i = 0;while (M.l != { } && i < MaxIterations) {# Parse and filter the current MX = M .o.
Parse .o.
SyntacticFilter;# Extract any configurations which correspond to parsesPartial = X .o.
OnlyOneUnlinked;# and union with the Parses transducerParses = [Parses | Partial];# filter any stale configurationsM = [X - Partial] .o.
TopChannelNotEmpty;i = i+1;}Leaving the details of the transducer SyntacticFilter, a filter that eliminates config-urations violating various linguistically motivated constraints, to a later section, thispseudocode works as follows: First, the sentence coded in Sentence is composed withthe MorphologicalAnalyzer, which performs full morphological analysis of the tokensin the sentence along with some very conservative local morphological disambigua-tion.
The resulting transducer encodes the sentence as a lattice representing all relevantmorphological ambiguities.
It is also possible to disambiguate the sentence prior toparsing with a tagger and present the parser with a fully disambiguated sentence.During each iteration, M encodes as a transducer, the valid partial-dependency con-figurations.
First X is computed by applying the Parse and SyntacticFilter trans-ducers, in that order, to M. At this point, there may be some complete parses, that is,configurations that have all except one of their word-final IGs linked (e.g., a parse inwhich every IG is linked to the next IG would use only the first channel, and sucha parse would be generated right after the first iteration.)
The transducer X encodingthe result of one iteration of parsing is filtered by OnlyOneUnlinked, defined asOnlyOneUnlinked = ~[[ $[ "<" LeftChannelSymbols*"(" AnyIG "@" ")"["0" | 1]* ">" ]]^ > 1 ];This would be read as ?It is not the case that there is more than one instance of word-final IGs whose right channel symbols do not contain any outgoing link marker.
?18This filter lets only those configurations that have all their required links established,18 Note that this constraint is for a grammmar with left-to-right links.535Oflazer Dependency Parsingthat is, all word-final IGs, except one, are linked (only one word-final IG has all of itsright channel symbols as 0s and 1s.)
Any such parses in Partial are unioned withParses (initially empty) and removed from X to give the M for the next iteration.
Anyconfigurations among the remaining ones (with no links in the most recently addedchannel, because of optionality in bracketing) are filtered, since these will violate theempty-channel constraint (see Figure 7(b)).
This is achieved by means of composition,with the transducer TopChannelNotEmpty defined as follows:TopChannelNotEmpty = ~[ ["<" LeftChannelSymbols* "0""(" AnyIG ("@") ")" "0"RightChannelSymbols* ">" ] *] ;This filter would be read as ?It is not the case that all topmost channel symbols in aconfiguration are all 0s.?
Thus configurations in which all most recent channel symbolsare 0 are filtered.
If the lower language of M (denoted by M.l) becomes empty at thispoint (or we exceed the number of maximum number of iterations), the iteration exits,with Parses containing the relevant result configurations.
MaxIterations is typicallysmall.
In the worst case, the number of iterations one would need would equal thenumber of word-final IGs, but in our experiments parsing has converged in five or sixiterations, and we have used eight as the maximum.6.7 Handling Coordinating ConjunctionsHeadless constructions such as coordinating conjunctions have been one of the weakerpoints of dependency grammar approaches.
Our implementation of coordinate con-junction constructs essentially follows the formulation of Ja?rvinen and Tapanainen(1998).
For a sequence of IGs likeD1 .
.
.C .
.
.D2 .
.
.C .
.
.
.
.
.Dk .
.
.Hwhere Di are the dependent IGs that are coordinated and C represents the conjunctionIGs (for, (comma), and, and or), and H is the head IG, we effectively thread a ?longlink?
(possibly spanning multiple channels) from D1 to H. If the link between Dk andH is labeled L, then dependent Di links to the following C with link L, and this C linksto Di+1 with L. This is conceptually equivalent to the following: The ?logical?
linkwith label L from conjoined dependent X and Y to their head Z is implemented withthree actual links of type L: X?and, and?Y, and Y?Z.
If there are additional conjunctionsand conjuncts, we continue to add (as required) one link of type L per word: Linkingconjoined dependents (W and X and Y) to Z is implemented with links W?and, and?X,X?and, and?Y, and Y?Z.One feature of Turkish simplifies this threading a bit: The left conjunct IG has toimmediately precede the conjunction IG.
The rules that do not involve conjunctionsestablish the link between Dk and H. For each such rule, we need two simple rules: Thefirst rule links a dependent Di, (i < k), to the conjunction immediately following.
Sincethe link type is almost always determined by the inflectional features of the dependent,this linking can be done (ambiguously in a very few cases in which dependent featuresdo not uniquely determine the link type).
The second rule links the conjunction to theright conjunct.
Note that this applies only to conjunct IGs that have already beenlinked to from their left conjunct.
Since the outermost link symbol on the left side of aconjunction IG identifies the relation (because the left conjunct is immediately to theleft of this IG), the link emanating from the conjunction to the right can be made toland on an IG that agrees with the left conjunct in relevant features.536Computational Linguistics Volume 29, Number 4Figure 9Link configurations for conjunction ambiguity.When we have two groups of conjoined constructsD1 C .
.
.D2 C .
.
.Dk .
.
.H1 C .
.
.H2 C .
.
.Hlthe rightmost conjunct of the first group, Dk, will alternately attach to H1, H2, .
.
.
, Hl.In the first case, the complete first conjunction group links Dk to H1, but not to the rest.In the second case, the complete first conjunction group links to the conjunction of H1and H2, which are then conjoined with H3 through Hl.
In the last case, the completefirst group links to the complete conjunction of H1, H2, .
.
.
, to Hl.
In all cases, thelinks from H1 all the way to Hl are independently threaded.
A number of additionalconstraints also filter situations in which a conjoined head has both conjoined andlocally attached dependents of the same type, by checking that the left channel symbolsfor these are not interleaved with other symbols.
Figure 9 provides an example forthis kind of conjunction ambiguity.
In this implementation we have not attempted tohandle circumscribing conjunctions such as the equivalents of either .
.
.
or.6.8 Enforcing Syntactic ConstraintsThe rules linking the IGs are overgenerating in that they may generate configurationsthat may violate some general or language-specific constraints.
For instance, more thanone subject or one object may attach to a verb, more than one determiner or possessormay attach to a nominal, an object may attach to a verb that is then passivized in thenext IG, or a nominative personal pronoun may be linked as a direct object (which isnot possible in Turkish).Some of the constraints preventing these configurations can be encoded in thebracketing rule patterns.
For instance, a rule for linking a nominal IG to a verb IGas a subject may check, using a suitable regular expression, the left-hand channelsymbols of the verb IG to make sure that it does not already contain an incomingsubject link.
There are also a number of situations in which the determination of a linkdepends on a pattern that is outside the sequence of the IGs from dependent to thehead IG specified in a bracketing rule (but nevertheless in the same word in whichthe head IG is located).
For instance, in Turkish, present participles are considered537Oflazer Dependency Parsingmodifiers derived from verbs.
The verb part is the head of the sentential clause witha subject gap.
Thus if a nominal IG attaches to a verb IG as a subject, but the verbIG is followed by another IG indicating that it is a present participle, then we shouldkill this configuration, since such verbs are not allowed to have subjects.
It is alsopossible to incorporate almost all lexicalized argument structure?related constraintsfor dealing with intransitive and transitive verbs, provided the lexicon component(the morphological analyzer in our case) produces such lexically determined features.We have chosen not to encode such constraints in the general format of the rulesand to implement them instead as filters that eliminate configurations produced bythe parsing.
We have observed that this makes the linking rules more perspicuous andeasier to maintain.Each constraint is implemented as a finite-state filter that operates on the outputsof the Parse transducer by checking the symbols denoting the relations.
For instance,we can define the following regular expression for filtering out configurations in whichtwo determiners are attached to the same IG:AtMostOneDeterminer =[ "<" [ ~[[$"D"]^>1] & LeftChannelSymbols* ] "(" AnyIG ("@") ")"RightChannelSymbols+ ">" ]*;This regular expression constrains the form of the configurations generated by parsing.Note that this transducer lets through a sequence of zero or more IGs, none of whichhave more than one D symbol (indicating an incoming determiner link) among theleft channel symbols.
The crucial portion at the beginning of the regular expressionsays: ?For any IG, it is not the case that there is more than one substring containingD among the left channel symbols of that IG (that is, the intersection of the symbolsbetween < and ( with LeftChannelSymbols does not contain more than one D).
?We can provide the following finite-state filter as an example in which the violat-ing configurations can be found by checking IGs following the head IG.
For instance,the configurations in which subjects are linked to verbs which are then derived intopresent participles would be filtered by a finite-state filter likeNoSubjectForPresentPart = ~$[ "<" $["S"] & LeftChannelSymbols*"(" Verb ")"RightChannelSymbols* ">""<" LeftChannelSymbols*"(" PresentParticipleIG ("@") ")"RightChannelSymbols* ">" ]which says that the configuration does not contain, among the left-side channel sym-bols, a verb IG with a subject marker followed by a present participle IG.The following are examples of the constraints that we have encoded as finite-statefilters:?
At most one subject can link to a verb.?
At most one direct object can link to a verb.?
At most one dative (locative, ablative, instrumental) adjunct can link to averb.?
Finite verbs derived from nouns and adjectives with zero suffixes do nothave objects (as they are the equivalents of be verbs).538Computational Linguistics Volume 29, Number 4?
Reflexive verbs do not get any overt objects.?
A postposition must always have an object to its immediate left (hencesuch a dependent nominal cannot link to some other head.
)19All syntactic constraints can be formulated similar to those given in the list.
All suchconstraints Cons1, Cons2 .
.
.
ConsN can then be composed to give one transducer thatenforces all of these:SyntacticFilter = [ Cons1 .o.
Cons2 .o.
Cons3 .o.
... .o.
ConsN]In the current implementation we use a total of 28 such constraints.6.9 Robust ParsingIt is possible that either because of grammar coverage, or because of ungrammati-cal input, a parse with only one unlinked word-final IG may not be found.
In suchcases, Parses in the pseudocode for parsing presented in section 6.6 would be empty.One might, however, opt to accept parses with k > 1 unlinked word-final IGs whenthere are no parses with < k unlinked word-final IGs (for some small k).
This canbe achieved by using Karttunen?s lenient composition operator (Karttunen 1998).
Le-nient composition, notated as .O., is used with a generator?filter combination.
Whena generator transducer, G, is leniently composed with a filter transducer, F, the result-ing transducer, G .O.
F, has the following behavior when an input is applied: If anyof the outputs of G in response to an input string satisfy the filter F, then G .O.
Fproduces just these as output.
Otherwise, G .O.
F outputs what G outputs.Let Unlinked i denote a regular expression that accepts parse configurations withno more than i unlinked word-final IGs.
For instance, for i = 2, this would be definedas follows:Unlinked_2 = ~[[$[ "<" LeftChannelSymbols* "(" AnyIG "@" ")"["0" | 1]* ">"]]^ > 2 ];which rejects configurations having more than two word-final IGs whose right channelsymbols contain only 0s and 1s (i.e., they do not link to some other IG as a dependent).We can augment the pseudocode given in section 6.6 as follows:if (Parses == { }) {PartialParses = M .O.
Unlinked_1 .O.
Unlinked_2 .O.
Unlinked_3;}This will have the parser produce outputs with up to three unlinked word-final IGswhen there are no outputs with a smaller number of unlinked word-final IGs.
Thus, it ispossible to recover some of the partial-dependency structures when a full-dependencystructure is not available for some reason.
The caveat would be, however, that sinceUnlinked 1 is a very strong constraint, any relaxation would increase the number ofoutputs substantially.
We have used this approach quite productively during the de-velopment of the dependency linking rules to discover coverage gaps in our grammar.19 In fact, the morphological analyzer produces, for each postposition, a marker denoting the case of thepreceding nominal as a subcategorization feature.
This is used in a semilexicalized fashion whilelinking nominals to their head postpositions.539Oflazer Dependency Parsing7.
Experiments with Dependency Parsing of TurkishOur implementation work has mainly consisted of developing and implementing therepresentation and finite-state techniques involved here, along with a nontrivial gram-mar component; we have not attempted to build a wide-coverage parser that is ex-pected to work on an arbitrary test corpus.
Although we have built the grammarcomponent manually using a very small set of sentences, it is conceivable that fu-ture work on inducing (possibly statistical) dependency grammars will exploit de-pendency treebanks, which are slowly becoming available (Hajic?
1998; Oflazer et al2003).The grammar has two major components.
The morphological analyzer is a full-coverage analyzer built using XRCE finite-state tools, slightly modified to generateoutputs as a sequence of IGs for a sequence of words.
When an input sentence (againrepresented as a transducer denoting a sequence of words) is composed with the mor-phological analyzer (see the pseudocode given in section 6.6), a transducer for thelattice representing all IGs for all morphological ambiguities (remaining after a lightmorphological disambiguation) is generated.
The dependency relations are describedby a set of about 60 rules much like the ones exemplified earlier.
These rules weredeveloped using a small set of 30 sentences.
The rules were almost all nonlexical,establishing links of the types listed earlier.
There is an additional set of 28 finite-state constraints that impose various syntactic and structural constraints.
The resultingParser transducer has 13,290 states and 186,270 transitions, and the SyntacticFiltertransducer has 3,800 states and 134,491 transitions.
The combined transducer for mor-phological analysis and (very limited) disambiguation has 100,103 states and 243,533arcs.The dependency grammar and the finite-state dependency parser were tested ona set of 200 Turkish sentences, including the 30 that were used for developing andtesting the grammar.
These sentences had 4 to 43 words, with an average of about18 words.
Table 1 presents our results for parsing this set of 200 sentences.
This tablepresents the minimum, the maximum, and the average of the number of words andIGs per sentence, the number of parser iterations and the number of parses generated.
(The number of iterations includes the last iteration where no new links are added.
)There were 22 sentences among the 200 that had quite a number of verbal adjunctsthat function as modifiers.
These freely attach to any verb IG, creating an analog ofthe PP attachment problem and giving rise to a very large number of parses.
The lastrow in the table gives the minimum, maximum and the average number of parseswhen such sentences were not considered.To impose a ranking on the parses generated based on just structural propertiesof the dependency tree, we employed Lin?s (1996) notion of structural complexity.
Wemeasured the total link length (TLL) in a dependency parse counting the IGs the linkspass over in the linear representation and ordered the dependency parses based onthe TLL of the dependency tree.
We classified the sentences into six groups:1.
Sentences that had a single minimum TLL parse which was correct.
There werea total of 39 sentences (19.5%) in this group.2.
Sentences that had more than one parse with the same minimum TLL and thecorrect parse was among these parses.
There were 58 sentences (29.0%) in thisgroup.
Thus for a total of 97 (48.5%) sentences, the correct parse wasfound among the parses with the minimum TLL.
In these cases theaverage number of parses with the minimum TLL was about 6540Computational Linguistics Volume 29, Number 4(minimum 1 parse and maximum 38 parses with the same minimumTLL).3.
Sentences for which the correct parse was not among the minimum TLL parses butwas among the next-largest TLL group.
There were 29 (14.5%) sentences inthis group.4.
Sentences for which the correct parse was not among the smallest and thenext-smallest TLL groups, but among the next three smallest TLL groups.
Therewere a total of 26 (13%) sentences in this group.5.
Sentences for which the parser generated parses, but the correct parse was notamong the first five groups.
There were 26 (13%) such sentences.
For these,we did not check any further and assumed there were no correct parses.The parses that were generated usually used other (morphological)ambiguities of the lexical item to arrive at a parse.6.
Sentences for which no parses could be found, usually as a result of the lack ofcoverage of the dependency grammar and the morphological analyzer.
Therewere 22 (11%) sentences in this group.It seems that for quite a number of sentences (groups 1?3 in the list), a relativelysmall number of parses have to be processed further with any additional lexical and/orstatistical constraints to extract the correct parse.
Although to obtain the statistics initems 1?6, we had to extract the full set of parse strings from the transducer thatencoded the parses compactly, one does not have to do this.
The parses with theshortest link length can be found by treating the resulting parse lattice transducer as adirected acyclic graph and finding the path with the minimum number of 1 symbols onit from the start state node to the final state node using one of the standard shortest-path algorithms (e.g., Dijsktra?s algorithm [Cormen, Leiserson, and Rivest 1990]).20This is because paths from the start state to the final state are string encodings of thedependency trees.
The 1 symbols in the representation add up to the total link length ofthe encoded dependency tree.
Since the representation of the tree is quite convoluted,the 1s in a block of 1s in the string representation all belong to different links stackedon top of each other.
Thus we ?count?
the length of the links in an ?interleaved?fashion.
On the other hand, Dijkstra?s algorithm may not be very useful, since onemay need to extract the k shortest paths to select from, perhaps, later, with more-informed criteria than link length, such as lexical and statistical information.
For thiswe may use an algorithm which finds the k shortest paths between a source and asink node in a directed graph (e.g., Eppstein 1998).The complete parser, including about 60 linking rules and the 28 syntactic con-straints, is defined using about 240 regular expressions coded using XRCE regularexpression language.
These regular expressions compile in about one minute on Pen-tium III 700 MHz (running Linux) into the Parser and SyntacticFilter transducers.The parser iterations are handled by a script interpreted by the XRCE finite state tool,xfst.Parsing takes about a second per sentence, including lookup in the morphologicalanalyzer, which is performed with a composition.
With manually completely morpho-logically disambiguated input, parsing is essentially instantaneous.2120 I thank an anonymous reviewer for suggesting this.21 We performed a simple experiment with 14 sentences that were manually morphologically541Oflazer Dependency ParsingTable 1Statistics from parsing 200 Turkish sentences.Minimum Maximum AverageWords/sentence 4 43 18.2IGs/sentence 4 59 22.5Parser iterations 3 8 5.0Parses/sentence 1 12,400 408.7Parses/sentence 1 285 55.4excluding the 22 sentenceswith > 1,000 parsesInput Sentence: Du?nya Bankas?
Tu?rkiye Direkto?ru?
hu?ku?metin izledig?i ekonomik pro-gram?n sonucunda o?nemli ad?mlar?n at?ld?g??n?
so?yledi.English: The World Bank Turkey director said that as a result of the economic programfollowed by the government, important steps were taken.Parser output after three iterations:Parse1:<000(dUnya+Noun+A3sg+Pnon+Nom@)00c><C00(banka+Noun+A3sg+P3sg+Nom@)0c0><010(tUrkiye+Noun+Prop+A3sg+Pnon+Nom@)01c><CC0(direktOr+Noun+A3sg+P3sg+Nom@)s00><001(hUkUmet+Noun+A3sg+Pnon+Gen@)10s><S01(izle+Verb+Pos)100><001(+Adj+PastPart+P3sg@)1m0><011(ekonomik+Adj@)11m><MM1(program+Noun+A3sg+Pnon+Gen@)10p><P01(sonuC+Noun+A3sg+P3sg+Loc@)1l0><011(Onem+Noun)110><011(+Adj+With@)11m><M11(adIm+Noun+A3pl+Pnon+Gen@)11s><S11(at+Verb)110><011(+Verb+Pass+Pos)110><011(+Noun+PastPart+A3sg+P3sg+Acc@)11o><OLS(sOyle+Verb+Pos+Past+A3sg@)000>Parse2:<000(dUnya+Noun+A3sg+Pnon+Nom@)00c><C00(banka+Noun+A3sg+P3sg+Nom@)0c0><010(tUrkiye+Noun+Prop+A3sg+Pnon+Nom@)01c><CC0(direktOr+Noun+A3sg+P3sg+Nom@)s00><001(hUkUmet+Noun+A3sg+Pnon+Gen@)10s><S01(izle+Verb+Pos)100><001(+Adj+PastPart+P3sg@)1m0><011(ekonomik+Adj@)11m><MM1(program+Noun+A3sg+Pnon+Gen@)10p><P01(sonuC+Noun+A3sg+P3sg+Loc@)1l0><011(Onem+Noun)110><011(+Adj+With@)11m><M11(adIm+Noun+A3pl+Pnon+Gen@)11s><SL1(at+Verb)100><001(+Verb+Pass+Pos)100><001(+Noun+PastPart+A3sg+P3sg+Acc@)10o><O0S(sOyle+Verb+Pos+Past+A3sg@)000>smallFigure 10Sample input and output of the parser.
The only difference in the two parses is in the locativeadjunct attachment (to verbs at and so?yle).
The IGs that differ in the two parses are<S11(at+Verb)110> versus <SL1(at+Verb)100>, and <OLS(sOyle+Verb+Pos+Past+A3sg@)000>versus <O0S(sOyle+Verb+Pos+Past+A3sg@)000>.Figure 10 presents the input and the output of the parser for a sample Turkishsentence: Du?nya Bankas?
Tu?rkiye Direkto?ru?
hu?ku?metin izledig?i ekonomik program?n sonucundao?nemli ad?mlar?n at?ld?g??n?
so?yledi.
(The World Bank Turkey director said that as a resultof the economic program followed by the government, important steps were taken.
)Figure 11 shows the output of the parser processed with a Perl script to provide amore human-readable presentation:disambiguated.
For this set of sentences, there were about 7 parses per sentence.
The average numberof parses for these sentences when all their morphological ambiguities were considered was 15.
Whenthe two sentences with the highest number of parses were removed from this set, the correspondingnumbers were 3 parses per sentence and 11 parses per sentence.542Computational Linguistics Volume 29, Number 4s----------------------------------------------------c---------------C s m---------------Mc---C c c---CC s s---S m m--MM p-...... ...... ........ ......... ........ ..... ........ ......... ........dUnya banka tUrkiye direktOr hUkUmet izle ekonomik programNoun Noun Noun Noun Noun Verb Adj Adj@ NounA3sg A3sg Prop A3sg A3sg Pos PastPart A3sgPnon P3sg A3sg P3sg Pnon P3sg@ PnonNom@ Nom@ Pnon Nom@ Gen@ Gen@Nom@-----------------------------------------------------------------------------------Sl-----------------------------------------L S--P l m---M s---SL o---O S......... ......... ......... ......... ....... ...... ......... ......sonuC Onem adIm at sOyleNoun Noun Adj Noun Verb Verb Noun VerbA3sg With@ A3pl Pass PastPart PosP3sg Pnon Pos A3sg PastLoc@ Gen@ P3sg A3sg@Acc@Figure 11Dependency tree for the second parse.8.
Discussion and ConclusionsWe have presented the architecture and implementation of a dependency parser usingan extended finite state.
Although the emphasis has been on the description of theapproach, we have developed a dependency grammar for Turkish and have used it toexperiment with a small sample of 200 Turkish sentences.
We have also employed ascheme for ranking dependency parses using the total link length of the dependencytrees, as originally suggested by Lin (1996), with quite promising results.
It is possibleto use algorithms for extracting k shortest paths to extract parses from the transducer,which compactly encodes all dependency parses, and further to rank a much smallerset of parses using lexical and statistical information whenever available.Another interesting point that we have noted, especially during the developmentof the grammar, is that the grammar rules do not have to pay any real attention to thesequence of the IGs that do not have anything to do with the current rule (with a veryfew exceptions in some special cases in which the rules have to check that links donot cross a ?barrier?).
This means that that the grammar of the IG sequence is reallylocalized to the morphological analyzer and that for the most part the dependencygrammar does not have to ?know?
about the sequencing of the IGs within a word.In addition to the reductionistic disambiguator that we have used just prior toparsing, we have implemented a number of heuristics to limit the number of poten-tially spurious configurations that result from optionality in bracketing, mainly byenforcing obligatory bracketing for sequential dependent-head pairs (e.g., the comple-ment of a postposition is immediately before it, or for conjunctions, the left conjunctis always the previous IG).
Such heuristics force such dependencies to appear in thefirst channel and hence prune many potentially useless configurations popping up inlater iterations.
Although we have not performed any significant experiments with therobust parsing technique that we describe in the article, it has been very instrumentalduring the process of debugging the grammar.
During debugging, when the actualparser did not deliver any results after a certain number of iterations, we generatedpartial parses with up to four unlinked word-final IGs to see where we were havingproblems with the coverage and added new linking rules.543Oflazer Dependency ParsingAcknowledgmentsThis work was partially supported by grantEEEAG-199E027 from TU?BI?TAK (TheTurkish Council on Scientific and TechnicalResearch).
A portion of this work was donewhile the author was visiting theComputing Research Laboratory at NewMexico State University.
The author thanksLauri Karttunen of Xerox PARC for makingavailable XRCE finite-state tools.
MercanKarahan, currently of Purdue University,helped substantially with theimplementation of the parser and with theexperimentation.
Comments by anonymousreviewers helped substantially to improvethe article.ReferencesAbney, Steven.
1996.
Partial parsing viafinite-state cascades.
Journal of NaturalLanguage Engineering, 2(4):337?344.Ait-Mokhtar, Salah and Jean-Pierre Chanod.1997.
Incremental finite-state parsing.
InProceedings of ANLP?97, pages 72?79, April.Black, Alan.
1989.
Finite state machinesfrom feature grammars.
In Proceedings ofInternational Workshop on ParsingTechnologies, pages 277?285.Chanod, Jean-Pierre and Pasi Tapanainen.1996.
A robust finite-state grammar forFrench.
In John Carroll and Ted Briscoe,editors, Proceedings of the ESSLLI?96Workshop on Robust Parsing, pages 16?25,August.Chelba, Ciprian, David Engle, FrederickJelinek, Victor Jimenez, SanjeevKhudanpur, Lidia Mangu, Harry Printz,Eric Ristad, Ronald Rosenfeld, AndreasStolcke, and Dekai Wu.
1997.
Structureand estimation of a dependency languagemodel.
In Processings of Eurospeech?97.Collins, Michael.
1996.
A new statisticalparser based on bigram lexicaldependencies.
In Proceedings of the 34thAnnual Meeting of the Association forComputational Linguistics, pages 184?191.Collins, Michael, Jan Hajic?, Lance Ramshaw,and Christoph Tillman.
1999.
A statisticalparser for Czech.
In Proceedings of the 37thAnnual Meeting of the Association forComputational Linguistics, pages 505?512,June.Cormen, Thomas H., Charles E. Leiserson,and Ronald L. Rivest.
1990.
Introduction toAlgorithms.
MIT Press, Cambridge, MA.Eisner, Jason.
1996.
Three new probabilisticmodels for dependency parsing: Anexploration.
In Proceedings of the 16thInternational Conference on ComputationalLinguistics (COLING-96), pages 340?345,August.Elworthy, David.
2000.
A finite state parserwith dependency structure output.
InProceedings of International Workshop onParsing Technologies.Eppstein, David.
1998.
Finding k-shortestpaths.
Siam Journal on Computing,28(2):652?673.Giguet, Emmanuel and Jacques Vergne.1997.
From part-of-speech tagging tomemory-based deep syntactic analysis.
InProceedings of the International Workshop onParsing Technologies, pages 77?88.Grefenstette, Gregory.
1996.
Light parsing asfinite-state filtering.
In ECAI ?96 Workshopon Extended Finite State Models of Language,August.Grimley-Evans, Edmund.
1997.Approximating context-free grammarswith a finite-state calculus.
In Proceedingsof ACL-EACL?97, pages 452?459.Hajic?, Jan. 1998.
Building a syntacticallyannotated corpus: The PragueDependency Treebank.
In Eva Hajicova,editor, Issues in Valency and Meaning:Studies in Honour of Jarmila Panenova.Karolinum?Charles University Press,Prague, pages 106?132.Hankamer, Jorge.
1989.
Morphologicalparsing and the lexicon.
InW.
Marslen-Wilson, editor, LexicalRepresentation and Process.
MIT Press,Cambridge, MA, pages 392?408.Hobbs, Jerry R., Douglas Appelt, John Bear,David Israel, Megumi Kameyama, MarkStickel, and Mabry Tyson.
1997.
FASTUS:A cascaded finite state transducer forextracting information from naturallanguage text.
In Emmanuel Roche andYves Schabes, editors, Finite State LanguageProcessing.
MIT Press, Cambridge, MA,pages 386?406.Hopcroft, John E. and Jeffrey D. Ullman.1979.
Introduction to Automata Theory,Languages, and Computation.Addison-Wesley, Reading, MA.Ja?rvinen, Timo and Pasi Tapanainen.
1998.Towards an implementable dependencygrammar.
In Proceedings ofCOLING/ACL?98 Workshop on ProcessingDependency-Based Grammars, pages 1?10.Johnson, Mark.
1998.
Finite stateapproximation of constraint-basedgrammars using left-corner grammartransforms.
In Proceedings ofCOLING-ACL?98, pages 619?623, August.Kaplan, Ronald M. and Martin Kay.
1994.
Reg-ular models of phonological rule systems.Computational Linguistics, 20(3):331?378.544Computational Linguistics Volume 29, Number 4Karttunen, Lauri.
1998.
The propertreatment of optimality theory incomputational linguistics.
In LauriKarttunen and Kemal Oflazer, editors,Proceedings of the International Workshop onFinite State Methods in Natural LanguageProcessing (FSMNLP), June.Karttunen, Lauri, Jean-Pierre Chanod,Gregory Grefenstette, and Anne Schiller.1996.
Regular expressions for languageengineering.
Natural Language Engineering,2(4):305?328.Kokkinakis, Dimitrios and Sofie JohanssonKokkinakis.
1999.
A cascaded finite stateparser for syntactic analysis of Swedish.In Proceedings of EACL?99.Koskenniemi, Kimmo.
1990.
Finite-stateparsing and disambiguation.
InProceedings of the 13th InternationalConference on Computational Linguistics(COLING?90), pages 229?233.Koskenniemi, Kimmo, Pasi Tapanainen, andAtro Voutilainen.
1992.
Compiling andusing finite-state syntactic rules.
InProceedings of the 14th InternationalConference on Computational Linguistics,COLING-92, pages 156?162.Lafferty, John, Daniel Sleator, and DavyTemperley.
1992.
Grammatical trigrams: Aprobabilistic model of link grammars.
InProceedings of the 1992 AAAI Fall Symposiumon Probablistic Approaches to NaturalLanguage.Lai, Bong Yeung Tom and ChangningHuang.
1994.
Dependency grammar andthe parsing of Chinese sentences.
InProceedings of the 1994 Joint Conference of 8thACLIC and 2nd PaFoCol.Lin, Dekang.
1995.
A dependency-basedmethod for evaluation of broad-coverageparsers.
In Proceedings of IJCAI?95.Lin, Dekang.
1996.
On the structuralcomplexity of natural language sentences.In Proceedings of the 16th InternationalConference on Computational Linguistics(COLING-96).Melc?uk, Igor A.
1988.
Dependency Syntax:Theory and Practice.
State University ofNew York Press, Albany, NY.Mohri, Mehryar, Fernando C. N. Pereira,and Michael Riley.
1998.
A rational designfor a weighted finite-state transducerlibrary.
In Derick Wood and Sheng Yu,editors, Proceedings of the SecondInternational Workshop on ImplementingAutomata (WIA ?97), volume 1436 ofLecture Notes in Computer Science.Springer-Verlag, Berlin, pages 144?158.Nederhof, Mark-Jan. 1998.
Context-freeparsing through regular approximation.In Lauri Karttunen and Kemal Oflazer,editors, Proceedings of InternationalWorkshop on Finite State Methods in NaturalLanguage Processing, pages 13?24, Ankara,Turkey.Nederhof, Mark-Jan. 2000.
Practicalexperiments with regular approximationof context-free languages.
ComputationalLinguistics, 26(1):17?44.Oflazer, Kemal.
1993.
Two-level descriptionof Turkish morphology.
In Proceedings ofthe Sixth Conference of the European Chapterof the Association for ComputationalLinguistics, April.
(A full version appearsin Literary and Linguistic Computing, 9(2),1994).Oflazer, Kemal, Bilge Say, Dilek ZeynepHakkani-Tu?r, and Go?khan Tu?r.
2003.Building a Turkish treebank.
In AnneAbeille?, editor, Treebanks.
KluwerAcademic Publishers, Dordrecht, theNetherlands.Pereira, Fernando C. N., and Rebecca N.Wright.
1997.
Finite state approximationof phrase structure grammars.
InEmmanuel Roche and Yves Schabes,editors, Finite State Language Processing.MIT Press, Cambridge, MA.Robinson, Jane J.
1970.
Dependencystructures and transformational rules.Language, 46(2):259?284.Roche, Emmanuel.
1997.
Parsing with finitestate transducers.
In Emmanuel Rocheand Yves Schabes, editors, Finite?StateLanguage Processing, chap.
8.
MIT Press,Cambrigde, MA.Roche, Emmanuel and Yves Schabes,editors.
1997.
Finite State LanguageProcessing.
MIT Press, Cambridge, MA.Sleator, Daniel and Davy Temperley.
1991.Parsing English with a link grammar.Technical Report CMU-CS-91-196,Computer Science Department, CarnegieMellon University.Tapanainen, Pasi and Timo Ja?rvinen.
1997.A non-projective dependency parser.
InProceedings of ANLP?97, pages 64?71, April.van Noord, Gertjan.
1997.
FSA utilities: Atoolbox to manipulate finite-stateautomata.
In Derick Wood,Darrell Raymond, and Sheng Yu, editors,Automata Implementation, volume 1260 ofLecture Notes in Computer Science.Springer-Verlag, Berlin.Yu?ret, Deniz.
1998.
Discovery of LinguisticRelations Using Lexical Attraction.
Ph.D.thesis, Department of ElectricalEngineering and Computer Science,Massachusetts Institute of Technology,Cambridge, MA.
