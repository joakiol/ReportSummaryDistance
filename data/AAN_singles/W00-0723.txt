In: Proceedings of CoNLL-2000 and LLL-2000, pages 115-118, Lisbon, Portugal, 2000.Learning IE Rules for a Set of Related ConceptsJ.
Turmo and H. Rodr iguezTALP Research Center.
Universitat Polit~cnica de CatalunyaJordi Girona Salgado, 1-3E-08034 Barcelona - Spain1 In t roduct ionThe growing availability of on-line text has ledto an increase in the use of automatic knowledgeacquisition approaches from textual data.
Infact, a number of Information Extraction (IE)systems has emerged in the past few years inrelation to the MUC conferences 1.
The aim ofan IE system consists in automatically extract-ing pieces of information from text, being thisinformation relevant for a set of prescribed con-cepts (scenario).
One of the main drawbacks ofapplying IE systems is the high cost involved inmanually adapting them to new domains andtext styles.In recent years, a variety of Machine Learn-ing (ML) techniques has been used to improvethe portability of IE systems to new domains,as in SRV (Freitag, 1998), RAPIER (Califfand Mooney, 1997), LIEP (Huffman, 1996),CRYSTAL (Soderland et al, 1995) and WHISK(Soderland, 1999) .
However, some drawbacksremain in the portability of these systems: a)existing systems generally depend on the sup-ported text style and learn IE-rules either forstructured texts, semi-structured texts or freetext , b) IE systems are mostly single-conceptlearning systems, c) consequently, an extrac-tor (e.g., a rule set) is learned for each con-cept within the scenario in an independent man-ner, d) the order of execution of the learnersis set manually, and so are the scheduling andway of combination of the resulting extractors,and e) focusing on the training data, the size ofavailable training corpora can be inadequate toaccurately learn extractors for all the conceptswithin the scenario 2.1 http://www.muc.saic.com/~This is so when dealing with some combinations oftext style and domain.This paper describes EVIUS, a multi-conceptlearning system for free text that follows amulti-strategy constructive learning approach(MCL) (Michalshi, 1993) and supports insuffi-cient amounts of training corpora.
EVIUS isa component of a multilingual IE system, M-TURBIO (Turmo et al, 1999).2 EV IUS .
Learn ing  ru le  sets  for aset  o f  re la ted  conceptsThe input of EVIUS is both a partially-parsedsemantically-tagged 3 training corpus and a de-scription of the desired target structure.
Thisdescription is provided as a set of concepts Crelated to a set of asymmetric binary relations,T~.In order to learn set S of IE rule sets for thewhole C, EVIUS uses an MCL approach inte-grating constructive l arning, closed-loop learn-ing and deductive restructuring (Ko, 1998).In this multi-concept situation, the systemdetermines which concepts to learn and, later,incrementally updates S. This can be relativelystraightforward when using knowledge aboutthe target structure in a closed-loop learningapproach.
Starting with C, EVIUS reduces etb/of  unlearned concepts iteratively by selectingsubset P C/g formed by the primitive conceptsin/.4 and learning a rule set for each c E P 4For instance, the single colour scenario 5 in fig-3With EuroWordNet (http://www.hum.uva.nl/-ewn/)synsets.
No attempt has been made to disambiguatesuch tags.4No cyclic scenarios are allowed so that a topologicalsort of C is possible, which starts with a set of primitiveconcepts.5Our testing domain is mycology.
Texts consists ofSpanish descriptions ofspecimens.
There is a rich varietyof colour descriptions including basic colours, intervals,changes, etc.115ure 1 is provided to learn from instances of thefollowing three related concepts: colour, suchas in instance "azul ligeramente claro" (slightlypale blue), colour_interval, as in "entre rosay rojo sangre" (between pink and blood red),and to_change, as in "rojo vira a marr6n" (redchanges to brown).Initially, Lt = C = { colour, colour_interval,to_change}.
Then, EVIUS calculates7 9 ={colour} and once a rule set has beenlearned for colour, the new L/={colour_interval,to_change} is studied identifying 79 = L/.to tofrom fromFigure 1: A single scenario for the colour do-mainIn order to learn a rule set for a concept,EVIUS uses the relational learning method ex-plained in section 3, and defines the learn-ing space by means of a dynamic predicatemodel.
As a pre-process of the system, thetraining corpus is translated into predicatesusing the following initial predicate model:a) attributive meta-predicates: pos_X(A),isa_X(A), has_hypernym_X(A), word_X(A)and lemma_X(A), where X is instantiated withclosed categories, b) relational meta-predicates:distance_le._X(A,B), stating that there are Xterminal nodes, at most, between A and B, andc) relational predicates: ancestor(A,B), where Bis the syntactic ancestor of A, and brother(A,B),where B is the right brother node of A sharingthe syntactic ancestor.Once a rule set for concept c is learned,new examples are added for further learning bymeans of a deductive restructuring approach:training examples are reduced to generate amore compact and useful knowledge of thelearned concept.
This is achieved by usingthe induced rule set and a syntactico-semantictransformational grammar.
Further to all this,a new predicate isa_c is added to the model.For instance, in figure 2 6 , the Spanish sen-tence "su color rojo vira a marrSn oscuro"(its red colour changes to dark brown) has6Which is presented here as a partially-parsed treefor simplicity.S (n12)spec n a v prep/ n asucolorro~vira {lmarrdnloscurc ~ }(nl) (n2) (n3) (n4)(n5)~(n6) .
(n7) /( n ~ e d u c t i o nspec n a v prep/( gnom .
\~ ' r a  a marr6n oscur~ )(nl) (n2) (n3) (n4) (n5)k _ ~  jFigure 2: Restructuring training examplestwo examples of colour, n3 and n6+n7, be-ing these "rojo" (red) and "marr6n'+"oscuro"(dark brown).
No reduction is required by theformer.
However, the latter example is reducedto node n6'.
As a consequence, two new at-tributes are added to the model: isa_colour(n3)and isa_colour(n6').
This new knowledge willbe used to learn the concepts to_change andcolour_interval.3 Ru le  set learn ingEVIUS uses FOIL (First-order Induction Learn-ing) (Quinlan, 1990) to build an initial rule set7~0 from a set of positive and negative xamples.Positive examples C+ can be selected using afriendly environment either as:?
text relations: c(A:,A2) where both A: andA2 are terminal nodes that exactly delimita text value for c. For instance, both textrelations colour(n3,n3) or colour(n6,nT) infigure 2, or as:?
ontology relations: c(A:,A2,...,An) whereall Ai are terminal nodes which are in-stances of already learned concepts relatedto c in the scenario.
For instance, the on-tology relation to_change(n3,n6') 7, in thesame figure, means that the colour repre-sented by instance n3 changes to that rep-resented by n6'.Negative examples $ -  are automatically se-lected as explained in section 3.1.7Note that, after the deductive restructuring step,both n3 and n6' are instances of the concept colour.116If any uncovered examples et, g~-, remainsafter FOIL's performance, this is due to the lackof sufficient examples.
Thus, the system triesto improve recall by growing set g+ with arti-ficial examples (pseudo-examples), as explainedin 3.2.
A new execution of FOIL is done byusing the new g+.
The resulting rule set 7~is combined with T~0 in order to create 7?1 byappending the new rules from T?~ to 7?0.
Conse-quently, the recall value of 7~1 is forced to be atleast equal to that of 7~0, although the accuracycan decrease.
A better method seems to be themerging of rules from 7~ and TO0 by studyingempirical subsumptions.
This last combinationallows to create more compact and accurate rulesets.EVIUS uses an incremental learning approachto learn rule sets for each concept.
This is doneby iterating the process above while uncoveredexamples remain and the F1 score increment(AF1) is greater than pre-defined constant a:select g+ and generate g -7~0 = FOIL(g+,g -)$u + = uncover ed_ f r om ( 7~o )= (7?o)while $u + ~ 0 and AF1 > a dog+ = g+ U pseudo-examples($u +)T?~ = FOIL(E+,g -)T~i+ l = combine_rules(7~i,T?~)gu + = uncovered_f rom( TQ+ l )= E l (h i+ l )  - E l (h i )endwhileif AF1 > a then return "~i+1else return 7~iendi/3.1 Generat ing  re levant  negat iveexamplesNegative examples can be defined as any com-bination of terminal nodes out of g+.
However,this approach produces an extremely large num-ber of examples, out of which only a small sub-set is relevant o learn the concept.
Related tothis, (Freitag, 1998) uses words to learn onlyslot rules (learned from text-relation examples), selecting as negative those non-positive wordpairs that define a string as neither longer thanthe maximum length in positive examples, norshorter than the minimum.A more general approach is adopted to definethe distance between possible examples in thelearning Space, applying a clustering method us-ing positive examples as medoids s. The N near-est non-positive examples to each medoid can beselected as negative ones.
Distance, in our case,must be defined as multidimensional due to thetypology of occurring features.
It is relativelyeasy to define distances between examples forword_X and lemma_X predicates, being 1 whenX values are equal, and 0 otherwise.
For isa_Xpredicates, the minimum of all possible concep-tual distances (Agirre and Rigau, 1995) betweenX values in EWN has been used.
Greater dif-ficulty is encountered when defining a distancefrom a morpho-syntactic point of view (e.g., apronoun seems to be closer to a noun than averb).
In (Turmo et al, 1999), the concept of5-set has been presented as a syntactic relationgeneralization, and a distance measure has beenbased on this concept.3.2 Creat ing  pseudo-examplesA method has been used inspired by the gen-eration of convex pseudo data (Breiman, 1998),in which a similar process to gene-combinationin genetic algorithms is used.For each positive example c(A1,.
.
.
,An) 9 ofconcept c to be dealt with, an attribute vectoris defined as( word--X Bl ,.
.
.
,word._X B~ , lemma-X sl , .
.
.
,lemma_X B~ ,sem-X B1 ,... ,sem_X B~ ,context)where B1, .
.
.
,  Bn are the unrepeated terminalnodes from A1, .
.
.
,  An, context is the set of allpredicates subsumed by the syntactico-semanticstructure between the nearest positive exam-ple on the left and the nearest one on theright, and sem_XB~ is the list of isa_X andhas_hypernym_X predicates for Bi.Then, for each example uncovered by the ruleset learned by FOIL, a set of pseudo-examples isgenerated.
A pseudo-example is built by com-bining both the uncovered example vector anda randomly selected covered one.
This is doneas follows: for each dimension, one of both pos-sible values is randomly selected as value for thepseudo-example.SA medoid is an actual data point representing a clus-ter.9As defined in section 3.117T.
Set* $+150 10525o 20635o 27045o 32855o 398Reca l l \ ]P rec .
F156.86 100 0.72562.74 98.45 0.76673.53 97.40 0.83875.49 98.72 0.85675.49 98.7210.856Table 1: Results for the colour concept for dif-ferent training set sizes (* subscript 0 meansonly one FOIL iteration)4 Eva luat ionEVIUS has been tested on the mycological do-main.
A set of 68 Spanish mycological docu-ments (covering 9800 words corresponding to1360 lemmas) has been used.
13 of them havebeen kept for testing and the others for train-ing.
The target ontology consisted of 14 con-cepts and 24 relations.Several experiments have been carried outwith different raining sets.
Results of the initialrule set for the colour concept 1?
are presentedin table 1.Out of 34 in the 350 initial rule set, one of themost relevant learned rules is11:Col our ( A, B ) :-has_h ypern ym_OOO17586n ( B ) ,has_hypernym_O3464624n (A), brother (A, B).Table 2 shows the results of adding pseudo-examples to the 35012 training set and using thealgorithm in section 3.
This was tested witha = 0.01 (two iterations are enough, 351 and352) and 5 pseudo-examples for each uncoveredcase.
The algorithm returns the rule set pro-duced in the first iteration due to the fact that~F1T13> 0.01 between the first and the sec-ond iterations.
Higher results can be generatedwhen using lower values for a.Although no direct comparison with othersystems is possible due to the domain and lan-guage used, our results can be considered state-1?This concept appears to be the most difficult to belearned.11A chromatic colour (03464624n) that is the left syn-tactic brother of an attribute (00017586n) such as lumi-nosity or another chromatic colour.12This size has been selected to allow a better com-parison with the results in table 1.laF1T means the F1 value for training setsT.
Set E + F i r  Recall Prec.
F1351 415 0.981 76.47 97.50 0.857352 465 0.987 79.41 97.50 0.875Table 2: Results from adding pseudo-examplesto the initial training set with 35 documents.of-the-art regarding similar MUC competitiontasks.Re ferencesEneko Agirre and German Rigau.
1995.
A Proposalfor Word Sense Disambiguation using Concep-tual Distance.
In Proceedings of the InternationalConference RANLP, Tzigov Chark, Bulgaria.L.
Breiman.
1998.
Arcing Classifiers.
The Annalsof Statistics, 26(3):801-849.M.E.
Califf and R. Mooney.
1997.
Relational learn-ing of pattern-match rules for information extrac-tion.
In Workshop on Natural Language Learning,pages 9-15.
ACL.D.
Freitag.
1998.
Machine Learning for Informa-tion Extraction in Informal Domains.
Ph.D. the-sis, Computer Science Department.
Carnegie Mel-lon University.S.
Huffman.
1996.
Learning information extractionpatterns from examples.
In S. Wermter, E. Riloff,and G. Sheller, editors, Connectionist, statisticaland symbolic approaches to learning for naturallanguage processing.
Springer-Verlag.H.
Ko.
1998.
Empirical assembly sequence planning:A multistrategy constructive l arning approach.In I. Bratko R. S. Michalsky and M. Kubat, ed-itors, Machine Learning and Data Mining.
JohnWiley & Sons LTD.R.S.
Michalshi.
1993.
Towards a unified theory oflearning: Multistrategy task-adaptive l arning.In B.G.
Buchanan and D. Wilkins, editors, Read-ings in Knowledge Acquisition and Learning.
Mor-gan Kauffman.J.R.
Quinlan.
1990.
Learning logical definitionsfrom relations.
Machine Learning, 5:239-266.S.
Soderland, D. Fisher, J. Aseltine, and W. Lehn-ert.
1995.
Crystal: Inducing a conceptual dictio-nary.
In XIV International Joint Conference onArtificial Intelligence, pages 1314-1321.S.
Soderland.
1999.
Learning information extractionrules for semi-structured and free text.
MachineLearning, 34:233-272.J.
Turmo, N. Catalk, and H. Rodrlguez.
1999.
Anadaptable i  system to new domains.
Applied In-telligence, 10(2/3):225-246.118
