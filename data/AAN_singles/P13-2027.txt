Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 148?152,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAre Semantically Coherent Topic Models Useful for Ad Hoc InformationRetrieval?Romain Deveaud Eric SanJuanUniversity of Avignon - LIAAvignon, Franceromain.deveaud@univ-avignon.freric.sanjuan@univ-avignon.frPatrice BellotAix-Marseille University - LSISMarseille, Francepatrice.bellot@lsis.orgAbstractThe current topic modeling approaches forInformation Retrieval do not allow to ex-plicitly model query-oriented latent top-ics.
More, the semantic coherence of thetopics has never been considered in thisfield.
We propose a model-based feedbackapproach that learns Latent Dirichlet Al-location topic models on the top-rankedpseudo-relevant feedback, and we mea-sure the semantic coherence of those top-ics.
We perform a first experimental eval-uation using two major TREC test collec-tions.
Results show that retrieval perfor-mances tend to be better when using topicswith higher semantic coherence.1 IntroductionRepresenting documents as mixtures of ?topics?has always been a challenge and an objective forresearchers working in text-related fields.
Basedon the words used within a document, topic mod-els learn topic level relations by assuming that thedocument covers a small set of concepts.
Learn-ing the topics from a document collection can helpto extract high level semantic information, andhelp humans to understand the meaning of doc-uments.
Latent Semantic Indexing (Deerwesteret al, 1990) (LSI), probabilistic Latent Seman-tic Analysis (Hofmann, 2001) (pLSA) and LatentDirichlet Allocation (Blei et al, 2003) (LDA) arethe most famous approaches that tried to tacklethis problem throughout the years.
Topics pro-duced by these methods are generally fancy andappealing, and often correlate well with humanconcepts.
This is one of the reasons of the inten-sive use of topic models (and especially LDA) incurrent research in Natural Language Processing(NLP) related areas.One main problem in ad hoc Information Re-trieval (IR) is the difficulty for users to translate acomplex information need into a keyword query.The most popular and effective approach to over-come this problem is to improve the representa-tion of the query by adding query-related ?con-cepts?.
This approach mostly relies on pseudo-relevance feedback, where these so-called ?con-cepts?
are the most frequent words occurring in thetop-ranked documents retrieved by the retrievalsystem (Lavrenko and Croft, 2001).
From thatperspective, topic models seem attractive in thesense that they can provide a descriptive and intu-itive representation of concepts.
But how can wequantify the usefulness of these topics with respectto an IR system?
Recently, researchers developedmeasures which evaluate the semantic coherenceof topic models (Newman et al, 2010; Mimno etal., 2011; Stevens et al, 2012).
We adopt theirview of semantic coherence and apply one of thesemeasures to query-oriented topics.Several studies concentrated on improving thequality of document ranking using topic models,especially probabilistic ones.
The approach byWei and Croft (2006) was the first to leverageLDA topics to improve the estimate of documentlanguage models and achieved good empirical re-sults.
Following this pioneering work, severalstudies explored the use of pLSA and LDA un-der different experimental settings (Park and Ra-mamohanarao, 2009; Yi and Allan, 2009; Andrze-jewski and Buttler, 2011; Lu et al, 2011).
The re-ported results suggest that the words and the prob-ability distributions learned by probabilistic topicmodels are effective for query expansion.
Themain drawback of these approaches is that topicsare learned on the whole target document collec-tion prior to retrieval, thus leading to a static top-ical representation of the collection.
Dependingon the query and on its specificity, topics may ei-ther be too coarse or too fine to accurately rep-resent the latent concepts of the query.
Recently,Ye et al (2011) proposed a method which uses148LDA and learns topics directly on a limited setof documents.
While this approach is a first steptowards modeling query-oriented topics, it lackssome theoretic principles and only aims to heuris-tically construct a ?best?
topic (from all learnedtopics) before expanding the query with its mostprobable words.
More, none of the aforemen-tioned works studied the semantic coherence ofthose generated topics.
We tackle these issues bymaking the following contributions:?
we introduce Topic-Driven Relevance Mod-els, a model-based feedback approach (Zhaiand Lafferty, 2001) for integrating topic mod-els into relevance models by learning topicson pseudo-relevant feedback documents (asopposed to the entire document collection),?
we explore the coherence of those generatedtopics using the queries of two major andwell-established TREC test collections,?
we evaluate the effects coherent topics haveon ad hoc IR using the same test collections.2 Topic-Driven Relevance Models2.1 Relevance ModelsThe goal of relevance models is to improvethe representation of a query Q by selectingterms from a set of initially retrieved docu-ments (Lavrenko and Croft, 2001).
As the concen-tration of relevant documents is usually higher inthe top ranks of the ranking list, this is constitutedby a number N of top-ranked documents.
Rele-vance models usually perform better when com-bined with the original query model (or maxi-mum likelihood estimate).
Let ?
?Q be this maxi-mum likelihood query estimate and ?
?Q a relevancemodel, the updated new query model is given by:P (w|?Q) = ?
P (w|?
?Q) + (1?
?
)P (w|?
?Q) (1)where ?
?
[0, 1] is a parameter that controls thetradeoff between the original query model and therelevance model.
One of the most robust variantsof the relevance models is computed as follows:P (w|?
?Q) ???D?
?P (?D)P (w|?D)?t?QP (t|?D)(2)where ?
is a set of pseudo-relevant feedback doc-uments and ?D is the language model of documentD.
This notion of estimating a query model isoften referred to as model-based feedback (Zhaiand Lafferty, 2001).
We assume P (?D) to be uni-form, resulting in an estimated relevance modelbased on a sum of document models weightedby the query likelihood score.
The final, inter-polated, estimate expressed in equation (1) is of-ten referred in the literature as RM3.
We tacklethe null probabilities problem by smoothing thedocument language model using the well-knownDirichlet smoothing (Zhai and Lafferty, 2004).2.2 LDA-based Feedback ModelThe estimation of the feedback model ?
?Q consti-tutes the first contribution of this work.
We pro-pose to explicitly model the latent topics (or con-cepts) that exist behind an information need, andto use them to improve the query representation.We consider ?
as the set of pseudo-relevant feed-back documents from which the latent conceptswould be extracted.
The retrieval algorithm usedto obtain these documents can be of any kind, theimportant point is that ?
is a reduced collectionthat contains the top documents ranked by an au-tomatic and state-of-the-art retrieval process.Instead of viewing ?
as a set of document lan-guage models that are likely to contain topical in-formation about the query, we take a probabilistictopic modeling approach.
We specifically focuson Latent Dirichlet Allocation (LDA), since it iscurrently one of the most representative.
In LDA,each topic multinomial distribution ?k is gener-ated by a conjugate Dirichlet prior with parame-ter ?, while each document multinomial distribu-tion ?d is generated by a conjugate Dirichlet priorwith parameter ?.
In other words, ?d,k is the prob-ability of topic k occurring in document D (i.e.P (k|D)).
Respectively, ?k,w is the probability ofwordw belonging to topic k (i.e.
P (w|k)).
We usevariational inference implemented in the LDA-Csoftware1 to overcome intractability issues (Blei etal., 2003; Griffiths and Steyvers, 2004).
Under thissetting, we compute the topic-driven estimation ofthe query model using the following equation:P (w|?
?Q) ???D??
(P (?D)P (w|?D)PTM (w|D)?t?QP (t|?D))(3)where PTM (w|D) is the probability of word woccurring in document D using the previously1www.cs.princeton.edu/?blei/lda-c1495 10 20 30 40 509.49.69.810.010.2CoherenceNumber of feedback documentsNumber of topics3 5 10 15 20WT10g5 10 20 30 40 509.49.69.810.010.2CoherenceNumber of feedback documentsNumber of topics3 5 10 15 20Robust04Figure 1: Semantic coherence of the topic models for different values of K, in function of the numberN of feedback documents.learned multinomial distributions.
Let T?
be atopic model learned on the ?
set of feedback doc-uments, this probability is given by:PTM (w|D) =?k?T?
?k,w ?
?D,k (4)High probabilities are thus given to words that areimportant in topic k, when k is an important topicin document D. In the remainder of this paper, werefer to this general approach as TDRM for Topic-Driven Relevance Models.2.3 Measuring the coherence ofquery-oriented topicsTDRM relies on two important parameters: thenumber of topics K that we want to learn, andthe number of feedback documents N from whichLDA learns the topics.
Varying these two param-eters can help to capture more information and tomodel finer topics, but how about their global se-mantic coherence?Term similarities measured in restricted do-mains was the first step for evaluating seman-tic coherence (Gliozzo et al, 2007), and was afirst basis for the development of several topiccoherence evaluation measures (Newman et al,2010).
Computing the Pointwise Mutual Informa-tion (PMI) of all word pairs over Wikipedia wasfound to be an effective metric using news andbooks corpora.
Recently, Stevens et al (2012)used (among others) an aggregate version of thismetric to evaluate large amounts of topic models.We use this method to evaluate the coherence ofquery-oriented topics.
Specifically, the coherenceof a topic model T K?
composed of K topics is:c(T K? )
=1KK?i=1?(w,w?
)?kilog P (w,w?)
+ P (w)P (w?)
(5)where probabilities of word occurrences and co-occurrences are estimated using an external refer-ence corpus.
Following Newman et al (2010), weuse Wikipedia to compute PMI and set  = 1 asin (Stevens et al, 2012).3 Evaluation3.1 Experimental setupWe performed our evaluation using two mainTREC2 collections: Robust04 and WT10g.
Ro-bust04 is composed 528,155 of news articles com-ing from three newspapers and the FBIS.
It sup-ported the TREC 2004 Robust track, from whichwe used the 250 query topics (numbers: 301-450,601-700).
The WT10g collection is composed of1,692,096 web pages, and supported the TRECWeb track for four years (2001-2004).
We focuson the 2000 and 2001 ad-hoc query topics (num-bers: 451-550).
We used the open-source index-ing and retrieval system Indri3 to run our exper-iments.
We indexed the two collections with theexact same parameters: tokens were stemmed withthe well-known light Krovetz stemmer and stop-words were removed using the standard Englishstoplist embedded with Indri (417 words).3.2 Semantic coherence evaluationMost coherent topics are composed of rare wordsthat do not often occur in the reference corpus, but2trec.nist.gov3lemurproject.org/indri.php1500.2000.2050.2100.2150.2205 10 20 30 40 50MAPNumber of feedback documentslllllllll lllllNumber of topics35101520RM3WT10g0.2600.2650.2700.2750.2800.2850.2905 10 20 30 40 50MAPNumber of feedback documentslllll ll lllllllNumber of topics35101520RM3Robust04Figure 2: Retrieval performance in terms of Mean Average Precision (MAP) of the TDRM approach.Each line represent a different number of topics K, and the performance are reported in function thenumber N of feedback documents.
The black, plain line represents the RM3 baseline.co-occur at lot together.
We see on Figure 1 thatvery coherent topics are identified in the top 5 and10 feedback documents for the WT10g collection,suggesting that closely related documents are re-trieved in the top ranks.
Results are quite differenton the Robust04 collection, where topic modelswith 20 topics on 5 documents are the least co-herent.
However, when looking at the Robust04documents, we see that they are on average almosttwice smaller than the WT10g web pages.
We hy-pothesize that the heterogeneous nature of the weballows to model very different topics covering sev-eral aspects of the query, while news articles arecontributions focused on a single subject.Overall, the more coherent topic models containa reasonable amount of topics (10-15), thus allow-ing to fit with variable amounts of documents.
Theattentive reader will notice that the topic coher-ence scores are very high compared to those pre-viously reported in the literature (Stevens et al,2012).
The TDRM approach captures topics thatare centered around a specific information need,often with a limited vocabulary, which favors wordco-occurrence.
On the other hand, topics learnedon entire collections are coarser than ours, whichleads to lower coherence scores.3.3 Document retrieval resultsSince TDRM is based on Relevance Mod-els (Lavrenko and Croft, 2001), we take the RM3approach presented in Section 2.1 as baseline.
The?
parameter is common between RM3 and TDRMand is determined for each query using leave-one-query-out cross-validation (that is: learn thebest parameter setting for all queries but one, andevaluate the held-out query using the previouslylearned parameter).We report ad hoc document retrieval perfor-mances in Figure 2.
We noticed in the previoussection that the most coherent topic models weremodeled using 5 feedback documents and 20 top-ics for the WT10g collection, and this parame-ter combination also achieves the best retrieval re-sults.
Overall, using 10, 15 or 20 topics allow itto achieve high and similar performance from 5 to20 documents.
We observe than using 20 topicsfor the Robust04 collection consistently achievesthe highest results, with the topic model coherencegrowing as the number of feedback documents in-creases.
Although topics coming from news ar-ticles may be limited, they benefit from the richvocabulary of professional writers who are trainedto avoid repetition.
Their use of synonyms allowsTDRM to model deep topics, with a comprehen-sive description of query aspects.
Since synonymsare less likely to co-occur in encyclopedic articleslike Wikipedia, we think that, in our case, the se-mantic coherence measure could be more accurateusing other textual resources.
This measure seemshowever to be effective when dealing with hetero-geneously structured documents.4 Conclusions & Future WorkOverall, modeling query-oriented topic modelsand estimating the feedback query model usingthese topics greatly improves ad hoc InformationRetrieval, compared to state-of-the-art relevancemodels.
While semantically coherent topic mod-151els do not seem to be effective in the context of anews articles search task, they are a good indica-tor of effectiveness in the context of web search.Measuring the semantic coherence of query top-ics could help predict query effectiveness or evenchoose the best query-representative topic model.AcknowledgmentsThis work was supported by the French Agencyfor Scientific Research (Agence Nationale dela Recherche) under CAAS project (ANR 2010CORD 001 02).ReferencesDavid Andrzejewski and David Buttler.
2011.
LatentTopic Feedback for Information Retrieval.
In Pro-ceedings of the 17th ACM SIGKDD internationalconference on Knowledge discovery and data min-ing, KDD ?11, pages 600?608.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by Latent Semantic Analysis.
Jour-nal of the American Society for Information Science,41(6):391?407.Alfio Massimiliano Gliozzo, Marco Pennacchiotti, andPatrick Pantel.
2007.
The Domain Restriction Hy-pothesis: Relating Term Similarity and SemanticConsistency.
In Human Language Technologies:The 2007 Annual Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, pages 131?138.Thomas L Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences of the United States of Amer-ica, 101 Suppl.Thomas Hofmann.
2001.
Unsupervised Learning byProbabilistic Latent Semantic Analysis.
MachineLearning, 42:177?196.Victor Lavrenko and W. Bruce Croft.
2001.Relevance-Based Language Models.
In Proceedingsof the 24th annual international ACM SIGIR confer-ence on Research and development in informationretrieval, SIGIR ?01, pages 120?127.Yue Lu, Qiaozhu Mei, and ChengXiang Zhai.
2011.Investigating task performance of probabilistic topicmodels: an empirical study of PLSA and LDA.
In-formation Retrieval, 14:178?203.David Mimno, Hanna M. Wallach, Edmund Talley,Miriam Leenders, and Andrew McCallum.
2011.Optimizing Semantic Coherence in Topic Models.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?11, pages 262?272.David Newman, Jey Han Lau, Karl Grieser, and Tim-othy Baldwin.
2010.
Automatic Evaluation ofTopic Coherence.
In Human Language Technolo-gies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 100?108.Laurence A.
Park and Kotagiri Ramamohanarao.
2009.The Sensitivity of Latent Dirichlet Allocation for In-formation Retrieval.
In Proceedings of the Euro-pean Conference on Machine Learning and Knowl-edge Discovery in Databases, ECML PKDD ?09,pages 176?188.Keith Stevens, Philip Kegelmeyer, David Andrzejew-ski, and David Buttler.
2012.
Exploring TopicCoherence over Many Models and Many Topics.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,EMNLP-CoNLL ?12, pages 952?961.Xing Wei and W. Bruce Croft.
2006.
LDA-based Doc-ument Models for Ad-hoc Retrieval.
In Proceedingsof the 29th annual international ACM SIGIR confer-ence on Research and development in informationretrieval, SIGIR ?06, pages 178?185.Zheng Ye, Jimmy Xiangji Huang, and Hongfei Lin.2011.
Finding a Good Query-Related Topic forBoosting Pseudo-Relevance Feedback.
JASIST,62(4):748?760.Xing Yi and James Allan.
2009.
A Comparative Studyof Utilizing Topic Models for Information Retrieval.In Proceedings of the 31th European Conference onIR Research on Advances in Information Retrieval,ECIR ?09, pages 29?41.
Springer-Verlag.Chengxiang Zhai and John Lafferty.
2001.
Model-based Feedback in the Language Modeling Ap-proach to Information Retrieval.
In Proceedingsof the Tenth International Conference on Informa-tion and Knowledge Management, CIKM ?01, pages403?410.Chengxiang Zhai and John Lafferty.
2004.
A Study ofSmoothing Methods for Language Models Appliedto Information Retrieval.
ACM Transactions on In-formation Systems, 22(2):179?214.152
