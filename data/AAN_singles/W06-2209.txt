Active AnnotationAndreas VlachosWilliam Gates BuildingComputer LaboratoryUniversity of Cambridgeav308@cl.cam.ac.ukAbstractThis paper introduces a semi-supervisedlearning framework for creating trainingmaterial, namely active annotation.
Themain intuition is that an unsupervisedmethod is used to initially annotate imper-fectly the data and then the errors madeare detected automatically and correctedby a human annotator.
We applied ac-tive annotation to named entity recogni-tion in the biomedical domain and encour-aging results were obtained.
The mainadvantages over the popular active learn-ing framework are that no seed annotateddata is needed and that the reusability ofthe data is maintained.
In addition to theframework, an efficient uncertainty esti-mation for Hidden Markov Models is pre-sented.1 IntroductionTraining material is always an issue when applyingmachine learning to deal with information extrac-tion tasks.
It is generally accepted that increasingthe amount of training data used improves perfor-mance.
However, training material comes at a cost,since it requires annotation.As a consequence, when adapting existing meth-ods and techniques to a new domain, researchers andusers are faced with the problem of absence of an-notated material that could be used for training.
Agood example is the biomedical domain, which hasattracted the attention of the NLP community rel-atively recently (Kim et al, 2004).
Even thoughthere are plenty of biomedical texts, very little of itis annotated, such as the GENIA corpus (Kim et al,2003).A very popular and well investigated frameworkin order to cope with the lack of training mate-rial is the active learning framework (Cohn et al,1995; Seung et al, 1992).
It has been appliedto various NLP/IE tasks, including named entityrecognition (Shen et al, 2004) and parse selec-tion (Baldridge and Osborne, 2004) with rather im-pressive results in reducing the amount of anno-tated training data.
However, some criticism of ac-tive learning has been expressed recently, concern-ing the reusability of the data (Baldridge and Os-borne, 2004).This paper presents a framework in order to dealwith the lack of training data for NLP tasks.
Theintuition behind it is that annotated training data isproduced by applying an (imperfect) unsupervisedmethod, and then the errors inserted in the annota-tion are detected automatically and reannotated bya human annotator.
The main difference comparedto active learning is that instead of selecting unla-beled instances for annotation, possible erroneousinstances are selected for checking and correctionif they are indeed erroneous.
We will refer to thisframework as ?active annotation?
in the rest of thepaper.The structure of this paper is as follows.
In Sec-tion 2 we describe the software and the dataset used.Section 3 explores the effect of errors in the trainingdata and motivates the active annotation framework.64In Section 4 we describe the framework in detail,while Section 5 presents a method for estimating un-certainty for HMMs.
Section 6 presents results fromapplying the active annotation.
Section 7 comparesthe proposed framework to active learning and Sec-tion 8 attempts an analysis of its performance.
Fi-nally, Section 9 suggests some future work.2 Experimental setupThe data used in the experiments that follow aretaken from the BioNLP 2004 named entity recog-nition shared task (Kim et al, 2004).
The text pas-sages have been annotated with five classes of en-tities, ?DNA?, ?RNA?, ?protein?, ?cell type?
and?cell line?.
In our experiments, following the ex-ample of Dingare et al (2004), we simplified the an-notation to one entity class, namely ?gene?, whichincludes the DNA, RNA and protein classes.
In or-der to evaluate the performance on the task, we usedthe evaluation script supplied with the data, whichcomputes the F-score (F1 = 2?Precision?RecallP recision+Recall ) foreach entity class.
It must be noted that all tokensof an entity must be recognized correctly in order tocount as a correct prediction.
A partially recognizedentity counts both as a precision and recall error.
Inall the experiments that follow, the official split ofthe data in training and testing was maintained.The named entity recognition system used in ourexperiments is the open source NLP toolkit Ling-pipe1.
The named entity recognition module isan HMM model using Witten-Bell smoothing.
Inour experiments, using the data mentioned earlier itachieved 70.06% F-score.3 Effect of errorsNoise in the training data is a common issue in train-ing machine learning for NLP tasks.
It can have sig-nificant effect on the performance, as it was pointedout by Dingare et al (2004), where the performanceof the same system on the same task (named entityrecognition in the biomedical domain) was lowerwhen using noisier material.
The effect of noise inthe data used to train machine learning algorithmsfor NLP tasks has been explored by Osborne (2002),using the task of shallow parsing as the case studyand a variety of learners.
The impact of different1http://www.alias-i.com/lingpipe/types of noise was explored and learner specific ex-tensions were proposed in order to deal with it.In our experiments we explored the effect of noisein training the selected named entity recognition sys-tem, keeping in mind that we are going to use anunsupervised method to create the training material.The kind of noise we expect is mislabelled instances.In order to simulate the behavior of a hypotheticalunsupervised method, we corrupted the training dataartificially using the following models:?
LowRecall: Change tokens labelled as entitiesto non-entities.
It must be noted that in thismodel, due to the presence of multi-token en-tities precision is reduced too, albeit less thanrecall.?
LowRecall WholeEntities: Change the label-ing of whole entities to non-entities.
In thismodel, precision is kept intact.?
LowPrecision: Change tokens labelled as non-entities to entities.?
Random: Entities and non-entities are changedrandomly.
It can be viewed alternatively as arandom tagger which labels the data with someaccuracy.The level of noise inserted is adjusted by specify-ing the probability with which a candidate label ischanged.
In all the experiments in this paper, for aparticular model and level of noise, the corruptionof the dataset was repeated five times in order toproduce more reliable results.
In practice, the be-havior of an unsupervised method is likely to be amixture of the above models.
However, given thatthe method would tag the data with a certain per-formance, we attempted through our experiments toidentify which of these (extreme) behaviors wouldbe less harmful.
In Figure 1, we present graphsshowing the effect of noise inserted with the abovemodels.
The experimental procedure was to addnoise to the training data according to a model, eval-uate the performance of the hypothetical tagger thatproduced it, train Lingpipe on the noisy training dataand evaluate the performance of the latter on the testdata.
The process was repeated for various levelsof noise.
In the top graph, the F-score achieved byLingpipe (F-ling) is plotted against the F-score of65the hypothetical tagger (F-tag), while in the bottomgraph the F-score achieved by Lingpipe (F-ling) isplotted against the number of erroneous classifica-tions made by the hypothetical tagger.00.10.20.30.40.50.60.70.80  0.2  0.4  0.6  0.8  1F-lingF-tagrandomlowrecallwholeentitieslowprecision00.10.20.30.40.50.60.70.80  50  100 150 200 250 300 350 400 450 500F-lingraw errors (in thousands)randomlowrecallwholeentitieslowprecisionFigure 1: F-score achieved by Lingpipe is plottedagainst (a) the F-score of the hypothetical tagger inthe top graph and (b) the number of errors made bythe hypothetical tagger in the bottom graph.A first observation is that limited noise does notaffect the performance significantly, a phenomenonthat can be attributed to the capacity of the machinelearning method to deal with noise.
From the pointof view of correcting mistakes in the training datathis suggests that not all mistakes need to be cor-rected.
Another observation is that while the perfor-mance for all the models follow similar curves whenplotted against the F-score of the hypothetical tag-ger, the same does not hold when plotted against thenumber of errors.
While this can be attributed to theunbalanced nature of the task (very few entity to-kens compared to non-entities), it also suggests thatthe raw number of errors in the training data is nota good indicator for the performance obtained bytraining on it.
However, it represents the effort re-quired to obtain the maximum performance from thedata by correcting it.4 Active AnnotationIn this section we present a detailed description ofthe active annotation framework.
Initially, we havea pool of unlabeled data, D, whose instances are an-notated using an unsupervised method u, which doesnot need training data.
As expected, a significantamount of errors is inserted during this process.
Alist L is created containing the tokens that have notbeen checked by a human annotator.
Then, a super-vised learner s is used to train a model M on thisnoisy training material.
A query module q, whichuses the model created by s decides which instancesof D will be selected to be checked for errors bya human annotator.
The selected instances are re-moved from L so that q does not select them againin future.
The learner s is then trained on this par-tially corrected training data and the sequence is re-peated from the point of applying the querying mod-ule q.
The algorithm written in pseudocode appearsin Figure 2.Data D, unsupervised tagger u,supervised learner s, query module q.Initialization:Apply u to D.Create list of instances L.Loop:Using s train a model M on D.Using q and M select a batch of instances Bto be checked.Correct the instances of B in D.Remove the instances of B from L.Repeat until:L is empty or annotator stops.Figure 2: Active annotation algorithmComparing it with active learning, the similaritiesare apparent.
Both frameworks have a loop in whicha query module q, using a model produced by thelearner, selects instances to be presented to a humanannotator.
The efficiency of active annotation can bemeasured in two ways, both of them used in evalu-ating active learning.
The first is to measure the re-duction in the checked instances needed in order toachieve a certain level of performance.
The secondis the increase in performance for a fixed numberof checked instances.
Following the active learning66paradigm, a baseline for active annotation is randomselection of instances to be checked.There are though some notable differences.
Dur-ing initialization, an unsupervised method u is re-quired to provide an initial tagging on the data D.This is an important restriction which is imposedby the lack of any annotated data.
Even under thisrestriction, there are some options available, espe-cially for tasks which have compiled resources.
Oneoption is to use an unsupervised learning algorithm,such the one presented by Collins & Singer (1999),where a seed set of rules is used to bootstrap a rule-based named entity recognizer.
A different approachcould be the use of a dictionary-based tagger, as inMorgan et al (2003).
It must be noted that the unsu-pervised method used to provide the initial taggingdoes not need to generalize to any data (a commonproblem for such methods), it only needs to performwell on the data used during active annotation.
Gen-eralization on unseen data is an attribute we hopethat the supervised learning method s will have af-ter training on the annotated material created withactive annotation.The query module q is also different from the cor-responding module in active learning.
Instead of se-lecting unlabeled informative instances to be anno-tated and added to the training data, its purpose isto identify likely errors in the imperfectly labelledtraining data, so that they are checked and correctedby the human annotator.In order to perform error-detection, we choseto adapt the approach of Nakagawa and Mat-sumoto (2002) which resembles uncertainty basedsampling for active learning.
According to theirparadigm, likely errors in the training data are in-stances that are ?hard?
for the classifier and incon-sistent with the rest of the data.
In our case, we usedthe uncertainty of the classifier as the measure of the?hardness?
of an instance.
As an indication of in-consistency, we used the disagreement of the labelassigned by the classifier with the current label of theinstance.
Intuitively, if the classifier disagrees withthe label of an instance used in its training, it indi-cates that there have been other similar instances inthe training data that were labelled differently.
Re-turning to the description of active annotation, thequery module q ranks the instances in L first by theirinconsistency and then by decreasing uncertainty ofthe classifier.
As a result, instances that are inconsis-tent with the rest of the data and hard for the classi-fier are selected first, then those that are inconsistentbut easy for the classifier, then the consistent onesbut hard for the classifier and finally the consistentand easy ones.While this method of detecting errors resemblesuncertainty sampling, there are other approachesthat could have been used instead and they can bevery different.
Sjo?bergh and Knutsson (2005) in-serted artificial errors and trained a classifier to rec-ognize them.
Dickinson and Meuers (2003) pro-posed methods based on n-grams occurring with dif-ferent labellings in the corpus.
Therefore, while it isreasonable to expect some correlation between theselections of active annotation and active learning(hard instances are likely to be erroneously anno-tated by the unsupervised tagger), the task of select-ing hard instances is quite different from detectingerrors.
The use of the disagreement between tag-gers for selecting candidates for manual correctionis reminiscent of corrected co-training (Pierce andCardie, 2001).
However, the main difference is cor-rected co-training results in a manually annotatedcorpus, while active annotation allows automaticallyannotated instances to be kept.5 HMM uncertainty estimationIn order to perform error detection according to theprevious section we need to obtain uncertainty es-timations over each token from the named entityrecognition module of Lingpipe.
For each token tand possible label l, Lingpipe estimates the follow-ing Hidden Markov Model from the training data:P (t[n], l[n]|l[n ?
1], t[n ?
1], t[n ?
2]) (1)When annotating a certain text passage, the tokensare fixed and the joint probability of Equation 1 iscomputed for each possible combination of labels.From Bayes?
rule, we obtain:P (l[n]|t[n], l[n ?
1], t[n ?
1], t[n ?
2]) =P (t[n], l[n]|l[n ?
1], t[n ?
1], t[n ?
2])P (t[n]|l[n ?
1], t[n ?
1], t[n ?
2]) (2)For fixed token sequence t[n], t[n ?
1], t[n ?
2]and previous label (l[n ?
1]) the second term of the67left part of Equation 2 is a fixed value.
Therefore,under these conditions, we can write:P (l[n]|t[n], l[n ?
1], t[n ?
1], t[n ?
2]) ?P (t[n], l[n]|l[n ?
1], t[n ?
1], t[n ?
2]) (3)From Equation 3 we obtain an approximation forthe conditional distribution of the current label (l[n])conditioned on the previous label (l[n ?
1]) for afixed sequence of tokens.
It must be stressed thatthe later restriction is very important.
The result-ing distribution from Equation 3 cannot be com-pared across different token sequences.
However,for the purpose of computing the uncertainty overa fixed token sequence it is a reasonable approxi-mation.
One way to estimate the uncertainty of theclassifier is to calculate the conditional entropy ofthis distribution.
The conditional entropy for a dis-tribution P (X|Y ) can be computed as:H[X|Y ] =?yP (Y = y)?xlogP (X = x|Y = y)(4)In our case, X is l[n] and Y is l[n ?
1].
Func-tion 4 can be interpreted as the weighted sum ofthe entropies of P (l[n]|l[n ?
1]) for each valueof l[n ?
1], in our case the weighted sum of en-tropies of the distribution of the current label foreach possible previous label.
The probabilities foreach tag (needed for P (l[n ?
1])) are not calcu-lated directly from the model.
P (l[n]) correspondsto P (l[n]|t[n], t[n ?
1], t[n ?
2]), but since we areconsidering a fixed token sequence, we approxi-mate its distribution using the conditional proba-bility P (l[n]|t[n], l[n ?
1], t[n ?
1], t[n ?
2]), bymarginalizing over l[n ?
1].Again, it must be noted that the above calculationsare to be used in estimating uncertainty over a singleword.
One property of the conditional entropy is thatit estimates the uncertainty of the predictions for thecurrent label given knowledge of the previous tag,which is important in our application because weneed the uncertainty over each label independentlyfrom the rest of the sequence.
This is confirmed bythe theory, from which we know that for a condi-tional distribution of X given Y the following equa-tion holds, H[X|Y ] = H[X,Y ] ?
H[Y ], where Hdenotes the entropy.A different way of obtaining uncertainty estima-tions from HMMs in the framework of active learn-ing has been presented in (Scheffer et al, 2001).There, the uncertainty is estimated by the marginbetween the two most likely predictions that wouldresult in a different current label, explicitly:M = maxi,j {P (t[n] = i|t[n ?
1] = j)} ?maxk,l,k 6=i {P (t[n] = k|t[n ?
1] = l)} (5)Intuitively, the margin M is the difference be-tween the two highest scored predictions that dis-agree.
The lower the margin, the higher the uncer-tainty of the HMM on the token at question.
A draw-back of this method is that it doesn?t take into ac-count the distribution of the previous label.
It is pos-sible that the two highest scored predictions are ob-tained for two different previous labels.
It may alsobe the case that a highly scored label can be obtainedgiven a very improbable previous label.
Finally, analternative that we did not explore in this work isthe Field Confidence Estimation (Culotta and Mc-Callum, 2004), which allows the estimation of con-fidence over sequences of tokens, instead of single-ton tokens only.
However, in this work confidenceestimation over singleton tokens is sufficient.6 Experimental ResultsIn this section we present results from applying ac-tive annotation to biomedical named entity recogni-tion.
Using the noise models described in Section 3,we corrupted the training data and then using Ling-pipe as the supervised learner we applied the algo-rithm of Figure 2.
The batch of tokens selected to bechecked in each round was 2000 tokens.
As a base-line for comparison we used random selection of to-kens to be checked.
The results for various noisemodels and levels are presented in the graphs of Fig-ure 3.
In each of these graphs, the performance ofLingpipe trained on the partially corrected material(F-ling) is plotted against the number of checked in-stances, under the label ?entropy?.In all the experiments, active annotation signifi-cantly outperformed random selection, with the ex-ception of 50% Random, where the high level ofnoise (the F-score of the hypothetical tagger thatprovided the initial data was 0.1) affected the initial680.560.580.60.620.640.660.680.70.720  75  150 225 300 375 450 525F-lingtokens_checked (in thousands)Random 10%randommarginentropy0.6750.680.6850.690.6950.70.7050  75  150 225 300 375 450 525F-lingtokens_checked (in thousands)LowRecall_WholeEntities 20%randommarginentropy0.350.40.450.50.550.60.650.70.750  75  150 225 300 375 450 525F-lingtokens_checked (in thousands)LowRecall 50%randommarginentropy0.60.610.620.630.640.650.660.670.680.690.70.710  75  150 225 300 375 450 525F-lingtokens_checked (in thousands)LowRecall 20%randomuncertaintyentropy0.450.50.550.60.650.70.750  75  150 225 300 375 450 525F-lingtokens_checked (in thousands)LowPrecision 20%randomuncertaintyentropy0.10.20.30.40.50.60.70.80  75  150 225 300 375 450 525F-lingtokens_checked (in thousands)Random 50%randomuncertaintyentropyFigure 3: F-score achieved by Lingpipe is plottedagainst the number of checked instances for variousmodels and levels of noise.judgements of the query module on which instancesshould be checked.
After having checked some por-tion of the dataset though, active annotation startedoutperforming random selection.
In the graphs forthe 10% Random, 20% LowRecall WholeEntitiesand 50% LowRecall noise models, under the la-bel ?margin?, appear the performance curves ob-tained using the uncertainty estimation of Schefferet al (2001).
Even though active annotation us-ing this method performs better than random se-lection, active annotation using conditional entropyperforms significantly better.
These results provideevidence of the theoretical advantages of conditionalentropy described earlier.
We also ran experimentsusing pure uncertainty based sampling (i.e.
with-out checking the consistency of the labels) on se-lecting instances to be checked.
The performancecurves appear under the label ?uncertainty?
for the20% LowRecall, 50% Random and 20% LowPreci-sion noise models.
The uncertainty was estimatedusing the method described in Section 5.
As ex-pected, uncertainty based sampling performed rea-sonably well, better than random selection but worsethan using labelling consistency, except for the ini-tial stage of 20% LowPrecision.7 Active Annotation versus ActiveLearningIn order to compare active annotation to active learn-ing, we run active learning experiments using thesame dataset and software.
The paradigm employedwas uncertainty based sampling, using the uncer-tainty estimation presented in Sec.
5.
HMMs requireannotated sequences of tokens, therefore annotatingwhole sentences seemed as the natural choice, asin (Becker et al, 2005).
While token-level selec-tions could be used in combination with EM, (as in(Scheffer et al, 2001)), constructing a corpus of in-dividual tokens would result in a corpus that wouldbe very difficult to be reused, since it would be par-tially labelled.
We employed the two standard op-tions of selecting sentences, selecting the sentenceswith the highest average uncertainty over the tokensor selecting the sentence containing the most uncer-tain token.
As cost metric we used the number oftokens, which allows more straightforward compar-ison with active annotation.In Figure 4 (left graph), each active learning ex-periment is started by selecting a random sentence asseed data, repeating the seed selection 5 times.
Therandom selection is repeated 5 times for each seedselection.
As in (Becker et al, 2005), selecting thesentences with the highest average uncertainty (ave)performs better than selecting those with the mostuncertain token (max).In the right graph, we compare the best activelearning method with active annotation.
Apparently,the performance of active annotation is highly de-pendent on the performance of the unsupervised tag-ger used to provide us with the initial annotation ofthe data.
In the graph, we include curves for twoof the noise models reported in the previous sec-tion, LowRecall20% and LowRecall50% which cor-respond to tagging performance of 0.66 / 0.69 / 0.67and 0.33 / 0.43 / 0.37 respectively, in terms of Re-call / Precision / F. We consider such tagging per-formances feasible with a dictionary-based tagger,since Morgan et al (2003) report performance of690.10.20.30.40.50.60.70.80  75  150 225 300 375 450 525F-lingtokens_checked (in thousands)Active learningavemaxrandom0.10.20.30.40.50.60.70.80  75  150 225 300 375 450 525F-lingtokens_checked (in thousands)AA vs ALAA-20%AA-50%AL-aveFigure 4: Left, comparison among various activelearning methods.
Right, comparison of activelearning and active annotation.0.88 / 0/78 / 83 with such a method.These results demonstrate that active annotation,given a reasonable starting point, can achieve reduc-tions in the annotation cost comparable to those ofactive learning.
Furthermore, active annotation pro-duces an actual corpus, albeit noisy.
Active learn-ing, as pointed out by Baldridge & Osborne (2004),while it reduces the amount of training materialneeded, it selects data that might not be useful totrain a different learner.
In the active annotationframework, it is likely to preserve correct instancesthat might not be useful to the machine learningmethod used to create it, but maybe beneficial to adifferent method.
Furthermore, producing an actualcorpus can be very important when adding new fea-tures to the model.
In the case of biomedical NER,one could consider adding document-level features,such as whether a token has been seen as part of agene name earlier in the document.
With the cor-pus constructed using active learning this is not fea-sible, since it is unlikely that all the sentences of adocument are selected for annotation.
Also, if oneintended to use the same corpus for a different task,such as anaphora resolution, again the imperfectlyannotated corpus constructed using active annota-tion can be used more efficiently than the partiallyannotated one produced by active learning.8 Selecting errorsIn order to investigate further the behavior of ac-tive annotation, we evaluated the performance of thetrained supervised method against the number of er-rors corrected by the human annotator.
The aim ofthis experiment was to verify whether the improve-ment in performance compared to random selectionis due to selecting ?informative?
errors to correct, ordue to the efficiency of the error detection technique.0.560.580.60.620.640.660.680.70.720  5  10 15 20 25 30 35 40 45 50F-lingerrors corrected (in thousands)randomentropyreverse051015202530354045500  75  150 225 300 375 450 525errorscorrected(inthousands)tokens checked (in thousands)randomentropyreverseFigure 5: Left: F-score achieved by Lingpipeis plotted against the number of corrected errors.Right: Errors corrected plotted against the numberof checked tokens.In Figure 5, we present such graphs for the 10%Random noise model.
Similar results were obtainedwith different noise models.
As can be observed onthe left graph, the errors corrected initially duringrandom selection are far more informative comparedto those corrected at the early stages of active anno-tation (labelled ?entropy?).
The explanation for thisis that using the error detection method described inSection 4, the errors that are detected are those onwhich the supervised method s disagrees with thetraining material, which implies that even if such aninstance is indeed an error then it didn?t affect s.Therefore, correcting such errors will not improvethe performance significantly.
Informative errors arethose that s has learnt to reproduce with high cer-tainty.
However, such errors are hard to detect be-cause similar attributes are exhibited usually by cor-rectly labelled instances.
This can be verified by thecurves labelled ?reverse?
in the graphs of Figure 5,in which the ranking of the instances to be selectedwas reversed, so that instances where the supervisedmethod agrees confidently with the training material70are selected first.
The fact that errors with high un-certainty are less informative than those with low un-certainty suggests that active annotation, while be-ing related to active learning, it is sufficiently differ-ent.
The right graph suggests that the error-detectionperformance during active annotation is much betterthan that of random selection.
Therefore, the per-formance of active annotation could be improved bypreserving the high error-detection performance andselecting more informative errors.9 Future workThis paper described active annotation, a semi-supervised learning framework that reduces the ef-fort needed to create training material, which isvery important in adapting existing trainable meth-ods to new domains.
Future work should investi-gate the applicability of the framework in a varietyof NLP/IE tasks and settings.
We intend to apply thisframework to NER for biomedical literature fromthe FlyBase project for which no annotated datasetsexist.While we have used the number of instanceschecked by a human annotator to measure the costof annotation, this might not be representative of theactual cost.
The task of checking and possibly cor-recting instances differs from annotating them fromscratch.
In this direction, experiments in realisticconditions with human annotators should be carriedout.
We also intend to explore the possibility ofgrouping similar mistakes detected in a round of ac-tive annotation, so that the human annotator can cor-rect them with less effort.
Finally, alternative error-detection methods should be investigated.AcknowledgmentsThe author was funded by BBSRC, grant number38688.
I would like to thank Ted Briscoe and BobCarpenter for their feedback and comments.ReferencesJ.
Baldridge and M. Osborne.
2004.
Active learning andthe total cost of annotation.
In Proceedings of EMNLP2004, Barcelona, Spain.M.
Becker, B. Hachey, B. Alex, and C. Grover.2005.
Optimising selective sampling for bootstrap-ping named entity recognition.
In Proceedings of theWorkshop on Learning with Multiple Views, ICML.D.
A. Cohn, Z. Ghahramani, and M. I. Jordan.
1995.Active learning with statistical models.
In Advancesin Neural Information Processing Systems, volume 7.M.
Collins and Y.
Singer.
1999.
Unsupervised modelsfor named entity classification.
In Proceedings of theJoint SIGDAT Conference on EMNLP and VLC.Aron Culotta and Andrew McCallum.
2004.
Confidenceestimation for information extraction.
In Proceedingsof HLT 2004, Boston, MA.M.
Dickinson andW.
D. Meurers.
2003.
Detecting errorsin part-of-speech annotation.
In Proceedings of EACL2003, pages 107?114, Budapest, Hungary.S.
Dingare, J. Finkel, M. Nissim, C. Manning, andC.
Grover.
2004.
A system for identifying named en-tities in biomedical text: How results from two evalua-tions reflect on both the system and the evaluations.
InThe 2004 BioLink meeting at ISMB.J.
D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii.
2003.
Ge-nia corpus - a semantically annotated corpus for bio-textmining.
In ISMB (Supplement of Bioinformatics).J.
Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier,editors.
2004.
Proceedings of JNLPBA, Geneva.A.
Morgan, L. Hirschman, A. Yeh, and M. Colosimo.2003.
Gene name extraction using FlyBase resources.In Proceedings of the ACL 2003 Workshop on NLP inBiomedicine, pages 1?8.T.
Nakagawa and Y. Matsumoto.
2002.
Detecting errorsin corpora using support vector machines.
In Proceed-ings of COLING 2002.M.
Osborne.
2002.
Shallow parsing using noisy andnon-stationary training material.
J. Mach.
Learn.
Res.,2:695?719.D.
Pierce and C. Cardie.
2001.
Limitations of co-trainingfor natural language learning from large datasets.
InProceedings of EMNLP 2001, pages 1?9.T.
Scheffer, C. Decomain, and S. Wrobel.
2001.
Ac-tive hiddenMarkov models for information extraction.Lecture Notes in Computer Science, 2189:309+.H.
S. Seung, M. Opper, and H. Sompolinsky.
1992.Query by committee.
In Proceedings of COLT 1992.D.
Shen, J. Zhang, J. Su, G. Zhou, and C. L. Tan.
2004.Multi-criteria-based active learning for named entityrecongition.
In Proceedings of ACL 2004, Barcelona.J.
Sjo?bergh and O. Knutsson.
2005.
Faking errors toavoid making errors: Machine learning for error de-tection in writing.
In Proceedings of RANLP 2005.71
