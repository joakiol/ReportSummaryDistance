Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 225?228,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsEnriching spoken language translation with dialog actsVivek Kumar Rangarajan SridharShrikanth NarayananSpeech Analysis and Interpretation LaboratoryUniversity of Southern Californiavrangara@usc.edu,shri@sipi.usc.eduSrinivas BangaloreAT&T Labs - Research180 Park AvenueFlorham Park, NJ 07932, U.S.A.srini@research.att.comAbstractCurrent statistical speech translation ap-proaches predominantly rely on just text tran-scripts and do not adequately utilize therich contextual information such as conveyedthrough prosody and discourse function.
Inthis paper, we explore the role of context char-acterized through dialog acts (DAs) in statis-tical translation.
We demonstrate the integra-tion of the dialog acts in a phrase-based statis-tical translation framework, employing 3 lim-ited domain parallel corpora (Farsi-English,Japanese-English and Chinese-English).
Forall three language pairs, in addition to produc-ing interpretable DA enriched target languagetranslations, we also obtain improvements interms of objective evaluation metrics such aslexical selection accuracy and BLEU score.1 IntroductionRecent approaches to statistical speech translationhave relied on improving translation quality withthe use of phrase translation (Och and Ney, 2003;Koehn, 2004).
The quality of phrase translationis typically measured using n-gram precision basedmetrics such as BLEU (Papineni et al, 2002) andNIST scores.
However, in many dialog based speechtranslation scenarios, vital information beyond whatis robustly captured by words and phrases is car-ried by the communicative act (e.g., question, ac-knowledgement, etc.)
representing the function ofthe utterance.
Our approach for incorporating di-alog act tags in speech translation is motivated bythe fact that it is important to capture and conveynot only what is being communicated (the words)but how something is being communicated (the con-text).
Augmenting current statistical translationframeworks with dialog acts can potentially improvetranslation quality and facilitate successful cross-lingual interactions in terms of improved informa-tion transfer.Dialog act tags have been previously used in theVERBMOBIL statistical speech-to-speech transla-tion system (Reithinger et al, 1996).
In that work,the predicted DA tags were mainly used to improvespeech recognition, semantic evaluation, and infor-mation extraction modules.
Discourse informationin the form of speech acts has also been used in in-terlingua translation systems (Mayfield et al, 1995)to map input text to semantic concepts, which arethen translated to target text.In contrast with previous work, in this paper wedemonstrate how dialog act tags can be directly ex-ploited in phrase based statistical speech translationsystems (Koehn, 2004).
The framework presentedin this paper is particularly suited for human-humanand human-computer interactions in a dialog set-ting, where information loss due to erroneous con-tent may be compensated to some extent through thecorrect transfer of the appropriate dialog act.
Thedialog acts can also be potentially used for impart-ing correct utterance level intonation during speechsynthesis in the target language.
Figure 1 shows anexample where the detection and transfer of dialogact information is beneficial in resolving ambiguousintention associated with the translation output.Figure 1: Example of speech translation output enriched withdialog actThe remainder of this paper is organized as fol-lows: Section 2 describes the dialog act tagger usedin this work, Section 3 formulates the problem, Sec-tion 4 describes the parallel corpora used in our ex-periments, Section 5 summarizes our experimentalresults and Section 6 concludes the paper with abrief discussion and outline for future work.2 Dialog act taggerIn this work, we use a dialog act tagger trained onthe Switchboard DAMSL corpus (Jurafsky et al,2251998) using a maximum entropy (maxent) model.The Switchboard-DAMSL (SWBD-DAMSL) cor-pus consists of 1155 dialogs and 218,898 utterancesfrom the Switchboard corpus of telephone conver-sations, tagged with discourse labels from a shal-low discourse tagset.
The original tagset of 375unique tags was clustered to obtain 42 dialog tagsas in (Jurafsky et al, 1998).
In addition, we alsogrouped the 42 tags into 7 disjoint classes, basedon the frequency of the classes and grouped the re-maining classes into an ?Other?
category constitut-ing less than 3% of the entire data.
The simplifiedtagset consisted of the following classes: statement,acknowledgment, abandoned, agreement, question,appreciation, other.We use a maximum entropy sequence taggingmodel for the automatic DA tagging.
Given a se-quence of utterances U = u1, u2, ?
?
?
, un and adialog act vocabulary (di ?
D, |D| = K), weneed to assign the best dialog act sequence D?
=d1, d2, ?
?
?
, dn.
The classifier is used to assign toeach utterance a dialog act label conditioned on avector of local contextual feature vectors comprisingthe lexical, syntactic and acoustic information.
Weused the machine learning toolkit LLAMA (Haffner,2006) to estimate the conditional distribution usingmaxent.
The performance of the maxent dialog acttagger on a test set comprising 29K utterances ofSWBD-DAMSL is shown in Table 1.Accuracy (%)Cues used (current utterance) 42 tags 7 tagsLexical 69.7 81.9Lexical+Syntactic 70.0 82.4Lexical+Syntactic+Prosodic 70.4 82.9Table 1: Dialog act tagging accuracies for various cues on theSWBD-DAMSL corpus.3 Enriched translation using DAsIf Ss, Ts and St, Tt are the speech signals and equiv-alent textual transcription in the source and targetlanguage, and Ls the enriched representation for thesource speech, we formalize our proposed enrichedS2S translation in the following manner:S?t = argmaxStP (St|Ss) (1)P (St|Ss) =?Tt,Ts,LsP (St, Tt, Ts, Ls|Ss) (2)?
?Tt,Ts,LsP (St|Tt, Ls).P (Tt, Ts, Ls|Ss) (3)where Eq.
(3) is obtained through conditional inde-pendence assumptions.
Even though the recogni-tion and translation can be performed jointly (Ma-tusov et al, 2005), typical S2S translation frame-works compartmentalize the ASR, MT and TTS,with each component maximized for performanceindividually.maxStP (St|Ss) ?
maxSt P (St|T?t , L?s)?maxTtP (Tt|T ?s , L?s) (4)?maxLsP (Ls|T ?s , Ss)?maxTs P (Ts|Ss)where T ?s , T ?t and S?t are the arguments maximiz-ing each of the individual components in the transla-tion engine.
L?s is the rich annotation detected fromthe source speech signal and text, Ss and T ?s respec-tively.
In this work, we do not address the speechsynthesis part and assume that we have access to thereference transcripts or 1-best recognition hypothe-sis of the source utterances.
The rich annotations(Ls) can be syntactic or semantic concepts (Gu etal., 2006), prosody (Agu?ero et al, 2006), or, as inthis work, dialog act tags.3.1 Phrase-based translation with dialog actsOne of the currently popular and predominantschemes for statistical translation is the phrase-based approach (Koehn, 2004).
Typical phrase-based SMT approaches obtain word-level align-ments from a bilingual corpus using tools such asGIZA++ (Och and Ney, 2003) and extract phrasetranslation pairs from the bilingual word alignmentusing heuristics.
Suppose, the SMT had access tosource language dialog acts (Ls), the translationproblem may be reformulated as,T ?t = argmaxTtP (Tt|Ts, Ls)= argmaxTtP (Ts|Tt, Ls).P (Tt|Ls) (5)The first term in Eq.
(5) corresponds to a dialog actspecific MT model and the second term to a dia-log act specific language model.
Given sufficientamount of training data such a system can possiblygenerate hypotheses that are more accurate than thescheme without the use of dialog acts.
However, forsmall scale and limited domain applications, Eq.
(5)leads to an implicit partitioning of the data corpus226Training TestFarsi Eng Jap Eng Chinese Eng Farsi Eng Jap Eng Chinese EngSentences 8066 12239 46311 925 604 506Running words 76321 86756 64096 77959 351060 376615 5442 6073 4619 6028 3826 3897Vocabulary 6140 3908 4271 2079 11178 11232 1487 1103 926 567 931 898Singletons 2819 1508 2749 1156 4348 4866 903 573 638 316 600 931Table 2: Statistics of the training and test data used in the experiments.and might generate inferioir translations in terms oflexical selection accuracy or BLEU score.A natural step to overcome the sparsity issue isto employ an appropriate back-off mechanism thatwould exploit the phrase translation pairs derivedfrom the complete data.
A typical phrase transla-tion table consists of 5 phrase translation scores foreach pair of phrases, source-to-target phrase transla-tion probability (?1), target-to-source phrase transla-tion probability (?2), source-to-target lexical weight(?3), target-to-word lexical weight (?4) and phrasepenalty (?5= 2.718).
The lexical weights are theproduct of word translation probabilities obtainedfrom the word alignments.
To each phrase trans-lation table belonging to a particular DA-specifictranslation model, we append those entries from thebaseline model that are not present in phrase tableof the DA-specific translation model.
The appendedentries are weighted by a factor ?.
(Ts ?
Tt)L?s = (Ts ?
Tt)Ls ?
{?.
(Ts ?
Tt)s.t.
(Ts ?
Tt) 6?
(Ts ?
Tt)Ls} (6)where (Ts ?
Tt) is a short-hand1 notation for aphrase translation table.
(Ts ?
Tt)Ls is the DA-specific phrase translation table, (Ts ?
Tt) is thephrase translation table constructed from entire dataand (Ts ?
Tt)L?s is the newly interpolated phrasetranslation table.
The interpolation factor ?
is usedto weight each of the four translation scores (phrasetranslation and lexical probabilities for the bilan-guage) with the phrase penalty remaining a con-stant.
Such a scheme ensures that phrase translationpairs belonging to a specific DA model are weightedhigher and also ensures better coverage than a parti-tioned data set.4 DataWe report experiments on three different paral-lel corpora: Farsi-English, Japanese-English and1(Ts ?
Tt) represents the mapping between source alpha-bet sequences to target alhabet sequences, where every pair(ts1, ?
?
?
, tsn, tt1, ?
?
?
, ttm) has a weight sequence ?1, ?
?
?
, ?5(five weights).Chinese-English.
The Farsi-English data used inthis paper was collected for human-mediated doctor-patient mediated interactions in which an Englishspeaking doctor interacts with a Persian speakingpatient (Narayanan et al, 2006).
We used a subsetof this corpus consisting of 9315 parallel sentences.The Japanese-English parallel corpus is a partof the ?How May I Help You?
(HMIHY) (Gorinet al, 1997) corpus of operator-customer conversa-tions related to telephone services.
The corpus con-sists of 12239 parallel sentences.
The conversationsare spontaneous even though the domain is lim-ited.
The Chinese-English corpus corresponds to theIWSLT06 training and 2005 development set com-prising 46K and 506 sentences respectively (Paul,2006).5 Experiments and ResultsIn all our experiments we assume that the same di-alog act is shared by a parallel sentence pair.
Thus,even though the dialog act prediction is performedfor English, we use the predicted dialog act as the di-alog act for the source language sentence.
We usedthe Moses2 toolkit for statistical phrase-based trans-lation.
The language models were trigram modelscreated only from the training portion of each cor-pus.
Due to the relatively small size of the corporaused in the experiments, we could not devote a sep-arate development set for tuning the parameters ofthe phrase-based translation scheme.
Hence, the ex-periments are strictly performed on the training andtest sets reported in Table 23.The lexical selection accuracy and BLEU scoresfor the three parallel corpora is presented in Table 3.Lexical selection accuracy is measured in terms ofthe F-measure derived from recall ( |Res?Ref ||Ref | ?
100)and precision ( |Res?Ref ||Res| ?
100), where Ref is theset of words in the reference translation and Res is2http://www.statmt.org/moses3A very small subset of the data was reserved for optimizingthe interpolation factor (?)
described in Section 3.1227F-score (%) BLEU (%)w/o DA tags w/ DA tags w/o DA tags w/ DA tagsLanguage pair 7tags 42tags 7tags 42tagsFarsi-English 56.46 57.32 57.74 22.90 23.50 23.75Japanese-English 79.05 79.40 79.51 54.15 54.21 54.32Chinese-English 65.85 67.24 67.49 48.59 52.12 53.04Table 3: F-measure and BLEU scores with and without use of dialog act tags.the set of words in the translation output.
Adding di-alog act tags (either 7 or 42 tag vocabulary) consis-tently improves both the lexical selection accuracyand BLEU score for all the language pairs.
The im-provements for Farsi-English and Chinese-Englishcorpora are more pronounced than the improve-ments in Japanese-English corpus.
This is due to theskewed distribution of dialog acts in the Japanese-English corpus; 80% of the test data are statementswhile other and questions category make up 16%and 3.5% of the data respectively.
The importantobservation here is that, appending DA tags in theform described in this work, can improve translationperformance even in terms of conventional objectiveevaluation metrics.
However, the performance gainmeasured in terms of objective metrics that are de-signed to reflect only the orthographic accuracy dur-ing translation is not a complete evaluation of thetranslation quality of the proposed framework.
Weare currently planning of adding human evaluationto bring to fore the usefulness of such rich anno-tations in interpreting and supplementing typicallynoisy translations.6 Discussion and Future WorkIt is important to note that the dialog act tags usedin our translation system are predictions from themaxent based DA tagger described in Section 2.
Wedo not have access to the reference tags; thus, someamount of error is to be expected in the DA tagging.Despite the lack of reference DA tags, we are stillable to achieve modest improvements in the trans-lation quality.
Improving the current DA tagger anddeveloping suitable adaptation techniques are part offuture work.While we have demonstrated here that using dia-log act tags can improve translation quality in termsof word based automatic evaluation metrics, the realbenefits of such a scheme would be attested throughfurther human evaluations.
We are currently work-ing on conducting subjective evaluations.ReferencesP.
D. Agu?ero, J. Adell, and A. Bonafonte.
2006.
Prosodygeneration for speech-to-speech translation.
In Proc.of ICASSP, Toulouse, France, May.A.
Gorin, G. Riccardi, and J. Wright.
1997.
How May IHelp You?
Speech Communication, 23:113?127.L.
Gu, Y. Gao, F. H. Liu, and M. Picheny.
2006.Concept-based speech-to-speech translation usingmaximum entropy models for statistical natural con-cept generation.
IEEE Transactions on Audio, Speechand Language Processing, 14(2):377?392, March.P.
Haffner.
2006.
Scaling large margin classifiers for spo-ken language understanding.
Speech Communication,48(iv):239?261.D.
Jurafsky, R. Bates, N. Coccaro, R. Martin, M. Meteer,K.
Ries, E. Shriberg, S. Stolcke, P. Taylor, and C. VanEss-Dykema.
1998.
Switchboard discourse languagemodeling project report.
Technical report researchnote 30, Johns Hopkins University, Baltimore, MD.P.
Koehn.
2004.
Pharaoh: A beam search decoder forphrasebased statistical machine translation models.
InProc.
of AMTA-04, pages 115?124.E.
Matusov, S. Kanthak, and H. Ney.
2005.
On the in-tegration of speech recognition and statistical machinetranslation.
In Proc.
of Eurospeech.L.
Mayfield, M. Gavalda, W. Ward, and A. Waibel.1995.
Concept-based speech translation.
In Proc.
ofICASSP, volume 1, pages 97?100, May.S.
Narayanan et al 2006.
Speech recognition engineer-ing issues in speech to speech translation system de-sign for low resource languages and domains.
In Proc.of ICASSP, Toulose, France, May.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
Technical report, IBM T.J. Watson Re-search Center.M.
Paul.
2006.
Overview of the IWSLT 2006 EvaluationCampaign.
In Proc.
of the IWSLT, pages 1?15, Kyoto,Japan.N.
Reithinger, R. Engel, M. Kipp, and M. Klesen.
1996.Predicting dialogue acts for a speech-to-speech trans-lation system.
In Proc.
of ICSLP, volume 2, pages654?657, Oct.228
