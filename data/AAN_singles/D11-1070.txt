Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 759?770,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsLearning Local Content Shift Detectors from Document-level InformationRicha?rd FarkasInstitute for Natural Language ProcessingUniversity of Stuttgartfarkas@ims.uni-stuttgart.deAbstractInformation-oriented document labeling is aspecial document multi-labeling task wherethe target labels refer to a specific informationinstead of the topic of the whole document.These kind of tasks are usually solved by look-ing up indicator phrases and analyzing theirlocal context to filter false positive matches.Here, we introduce an approach for machinelearning local content shifters which detectsirrelevant local contexts using just the origi-nal document-level training labels.
We handlecontent shifters in general, instead of learn-ing a particular language phenomenon detec-tor (e.g.
negation or hedging) and form a sin-gle system for document labeling and contentshift detection.
Our empirical results achieved24% error reduction ?
compared to supervisedbaseline methods ?
on three document label-ing tasks.1 IntroductionThere are special document multi-labeling taskswhere the target labels refer to a specific piece ofinformation extractable from the document insteadof the overall topic of the document.
In these kindsof tasks the target information is usually an attributeor relation related to the target entity (usually a per-son or an organisation) of the document in question,but the task is to assign class labels at the document(entity) level.
For example, the smoking habits ofthe patients are frequently discussed in the textualparts of clinical notes (Uzuner et al, 2008).
In thiscase the task is to find specific information in thetext ?
i.e.
the patient in question is a smoker, pastsmoker, non-smoker ?
but at the end an applica-tion has to assign labels to the documents(patients).Similarly, the soccer club names where a sportsmanplayed for are document(sportman)-level labels inWikipedia articles expressed by the Wikipedia cat-egories.
The target information in these tasks isusually just mentioned in the document and muchof the document is irrelevant for this informationrequest in contrast to standard document classifi-cation tasks where the goal is to identify the top-ics of the whole document.
On the other hand,they are not a standard information extraction taskas the task is to assign class labels to documents,and the training dataset contains labels just at thislevel.
These special tasks lie somewhere betweeninformation extraction and document classificationand require special approaches to solve them.
Wewill call them Information-oriented document label-ing throughout this paper.
There are several appli-cation areas where information-oriented documentlabels are naturally present in an enormous amountlike clinical records, Wikipedia categories and user-generated tags of news.Previous evaluation campaigns (Uzuner et al,2008; Pestian et al, 2007; Uzuner, 2009) demon-strated that information-oriented document labelingcan be effectively performed by looking up indicatorphrases which can be gathered by hand, by corpusstatistics or in a hybrid way.
However these cam-paigns also highlighted that the analysis of the localcontext of the indicator phrases is crucial.
For in-stance, in the smoking habit detection task there area few indicator words (e.g.
smokes, cigarette) andthe local context of their occurrences in texts should759be analysed to see whether their semantic was rad-ically changed (e.g.
they are negated or in a pasttense), for instance:The patient has a 20 pack-year smokinghistory.The patient denies any smoking history.He has a greater than 100 pack yearsmoking history and quit 9 to 10 yearsago.We propose a simple but efficient approach forinformation-oriented document labeling tasks by ad-dressing the automatic detection of language phe-nomena for a particular task which alters the senseor information content of the indicator phrase?s oc-currences.
For example, they may be logical modi-fiers (e.g.
negation) or modal modifiers (e.g.
auxil-iaries like might and can); they may refer to a subjectwhich differs from the target entity of the task (e.g.clinical notes usually contain information about thefamily history of the patient); or the semantic con-tent of the shifter may change the role of the tar-get span of a text (e.g.
a sportsman can play for oragainst a particular team).
We call these phenom-ena content shifters and the task of identifying themcontent shift detection (CSD).Existing CSD approaches focus on a particularclass of language phenomena (especially negationor hedging) and use hand-crafted rules (Chapman etal., 2007) or a supervised learning approach that ex-ploits corpora manually annotated at the token-levelfor a particular type of content shifter (Morante etal., 2009).
Moreover higher level applications (likedocument labeling and information extraction) use aseparate CSD module which is developed indepen-dently from the target task.
We argue that the natureof content shifters is domain and task dependent, sotraining corpora (at the token-level) are required forcontent shifters which are important for a particulartask but the construction of such training corpora isexpensive.
Here, we propose an alternative approachwhich uses only document-level labels.The input of our system is a training corpus la-beled on the document level (e.g.
a clinical datasetconsisting clinical notes and meta-data about pa-tients).
Our approach extracts indicator phrases andtrains a CSD jointly.
We focus on local contentshifters and we analyse just the sentences of indi-cator phrase occurrences.
Our chief assumption isthat CSD can be learnt by exploiting the false pos-itive occurrences of indicator phrases in the train-ing dataset.
We show that our method performs sig-nificantly better than standard document classifiers(which were designed for a slightly different task).The chief contributions of our work are that (i)we handle the CSD problem in general, so we de-tect all content shifters instead of focusing on oneparticular language phenomenon, (ii) we form a sin-gle framework for joint CSD and document labeling,(iii) moreover our approach does not require a dedi-cated annotated training dataset for content shifters.2 Related WorkInformation-oriented document classification taskswere first highlighted in the clinical domain wheremedical reports contain useful information about thepatient in question, but labels are only available atthe document (patient) level.
The field of clinicalNLP has been studied extensively since the 1990s(Larkey and Croft, 1995), but the most recent resultsare related to the shared task challenges organizedrelatively recently (Pestian et al, 2007; Uzuner etal., 2008; Uzuner, 2009).
For example the firstI2B2 challenge in 2006 (Uzuner et al, 2008) fo-cused on the smoking habits of the patient, the CMCchallenge in 2007 (Pestian et al, 2007) dealt withthe problem of automatically constructing ICD cod-ing systems and the second I2B2 challenge (Uzuner,2009) addressed the classification of discharge sum-maries according to the question ?Who?s obese andwhat co-morbidities do they have??.
These chal-lenges were dominated by entirely or partly rule-based systems that solved the tasks using indicatorphrase lookup and incorporated explicit mechanismsfor detecting speculation and negation.Another domain for information-oriented docu-ment classification might be Wikipedia, which con-tains rich information about entities like persons,places or organisations.
Some items of informationare available about these entities in the form of cate-gories and infoboxes assigned to articles.
Automaticdocument labeling methods can be trained based onthese assignments (Scho?nhofen, 2006), but these la-bels do not refer to the main theme of the article but760to a certain type of information.Existing content shift detection approaches focuson a particular class of language phenomena, espe-cially on negation and hedge recognitions.
Avail-able tools work mainly on clinical and biologicaldomains.
The first systems were fully hand-crafted(Light et al, 2004; Friedman et al, 1994; Chapmanet al, 2007) without any empirical evaluation on adedicated corpus.
Recently, there have been severalcorpora published with manual sentence-, event- ortoken-level annotation for negation, certainty andfactuality in the biological (Medlock and Briscoe,2007; Vincze et al, 2008), newswire (Strassel et al,2008; Sauri and Pustejovsky, 2009) and encyclope-dical (Farkas et al, 2010) domains.Exploiting these corpora, machine learning mod-els were also developed.
Solving the sentence-level task, Medlock and Briscoe (2007) used sin-gle words as input features in order to classify sen-tences from biological articles as speculative or non-speculative.
Szarvas (2008) extended their method-ology to use n-gram features and a semi-supervisedselection of the keyword features.
Ganter and Strube(2009) proposed an approach for the automatic de-tection of sentences containing uncertainty basedon Wikipedia weasel tags and syntactic patterns.For in-sentence negation and speculation detection,Morante et al (2009) developed scope ?
i.e.
con-tent shifted text spans ?
detectors for negation andspeculation following a supervised sequence label-ing approach, while ?Ozgu?r and Radev (2009) devel-oped a rule-based system that exploits syntactic pat-terns.
The goal of the CoNLL 2010 Shared Task(Farkas et al, 2010) was to develop linguistic scopedetectors as well.
The participants usually followeda supervised sequence labeling approach or used arule-based system that exploits syntactic patterns.The approach of classifying identified events intowhether they fall under negation or speculation wasfollowed by Sauri and Pustejovsky (2009) and theparticipants of the BioNLP?09 Shared Task (Kim etal., 2009).
Here the systems investigated the syn-tax path between the event trigger and a cue word(which came from a small lexicon) (Kilicoglu andBergler, 2009; Aramaki et al, 2009).Our approach differs from the previous worksfundamentally.
We deal with the two tasks(information-oriented document classification andcontent shift detection) together and introduce a co-learning approach for them.
Our approach han-dles content shifters in a data-driven and general-ized way i.e.
it is not specialized for a certain classof language phenomena.
Instead it tries to recog-nize task-specific syntactic and semantic patternswhich are responsible for semantic changes or irrel-evance.
In addition, we have no access to a gold-standard sentence-level or in-sentence-level annota-tion but exploit document-level ones.3 Tasks and DatasetsBefore introducing our approach in detail we de-scribe three tasks and datasets which were used inour experiments in order to give an insight into thechallenges of the information-oriented document la-beling tasks.
Table 1 summarizes the key statisticalfigures (the number of documents in the corpora, thesize of the label sets along with the average numberof tokens and label assignments per document) ofthe datasets used for the experimental evaluations.Table 1: The datasets used in our experiments.CMC Obes Soccerdomain clinical clinical encycl.|train| 978 730 4850|eval| 976 507 1736#token/d 25 1387 389#labels 45 16 12#label/d 1.24 4.37 1.23The CMC ICD Coding Dataset was originallyprepared for a shared task challenge organized bythe Computational Medicine Center (CMC) in Cin-cinatti, Ohio in 2007 (Pestian et al, 2007).
It con-tains radiology reports along with document-levelInternational Classification of Diseases (ICD) codesgiven by three human experts.
ICD is a coding ofdiseases, signs, symptoms and abnormal findings.
Inour experiments we used the train/evaluation split ofthe shared task.
The ICD coding guide states thatnegative or uncertain diagnosis should not be codedin any case.The corpus contains very short documents.
Forinstance, the documentHISTORY: Left lower chest pain.
Rule-out761pneumonia.
IMPRESSION: Normal chest.has one label 786.50 (cough) as 486 (pneumonia) isruled out.The main conclusion of the shared task in 2007was that simple rule-based systems generally out-perform bag-of-words-based machine learning mod-els.
The rules were extracted from ICD guidelinesand/or from the training corpus using simple sta-tistical measures, then they were checked or ex-tended manually.
Several systems of the challengeemployed a negation and speculation detection sub-module.
The (manually highly fine-tuned) top sys-tems of the CMC shared task achieved an F-measureof 88-89 (Pestian et al, 2007; Farkas and Szarvas,2008).The I2B2 Obesity Dataset was also the subjectof a clinical natural language processing sharedtask.
The challenge in 2008 focused on analyzingclinical discharge summary texts and addressed thefollowing question: ?Who is obese and what co-morbidities do they have??
(Uzuner, 2009).
Tar-get diseases (document labels) included obesity andits 15 most frequent co-morbidities exhibited bypatients.
In our experiments, we used the sametrain/evaluation split as that of the shared task.
Herea special aspect of the corpus is that the docu-ments are semi-structured, i.e.
they contain head-ings like discharge medications and admit diagno-sis.
By pasting the given heading to the beginningof each sentence, we incorporated it into the localcontext.
The top performing systems of the sharedtask employed mostly hand-crafted rules for indica-tor selection and for negation and uncertainty detec-tion as well.
They achieved an F-measure1 of 96-97(Uzuner, 2009; Solt et al, 2009).Wikipedia Soccer Dataset.
We constructed a cor-pus based on Wikipedia articles and categories2.The categories assigned to Wikipedia articles can beregarded as labels (for example, the labels of DavidBeckham in the Wikipedia are English people, ex-patriate soccer player, male model and A.C. Milanplayer, Manchester United player).
Based on the1Using the definitions of the challenge, the evaluation metricapplied here is the micro F-measure of the textual task on theYES versus every other class.2The dataset is available as the supplementary material.categories of Wikipedia, classifiers can be trained totag unlabeled texts or even add missing category as-signments to Wikipedia (Scho?nhofen, 2006).For a case study we focused on learning En-glish soccer clubs that a given sportsman playedfor.
Note that this task is an information-orienteddocument labeling task as the clubs for which asportsman played are usually just mentioned (espe-cially for smaller clubs) in the article of a player.The Wikipedia category Footballers in England byclub contains 408 subcategories (for the present andpast).
We selected the best known clubs (where thecategory label for the club is assigned to more than500 player pages).
Each article referring to a playerhaving a category assignment to these clubs wasdownloaded and the textual parts were extracted.Then a random 3:1 train:evaluation split of the doc-ument set was used.4 Document-labeling with CSDWe introduce here an iterative solution which selectsindicator phrases and trains a content shift detec-tor at the same time.
Our focus will be on multi-label document classification tasks where multipleclass labels can be assigned to a single document.In this study we will not deal with the modeling ofinter-label dependencies, so binary (positive versusnegative) and multi-class document classifications(where exactly one label has to be assigned to a sin-gle document) can be regarded as special cases ofthis multi-label classification problem.
Our result-ing multi-label model is then a set of binary classi-fiers ?
?assign a label?
classifiers for each class label?
and the final prediction on a document is simplythe union of the labels forecasted by the individualclassifiers.Our key assumption in the multi-label environ-ment is that while indicator phrases have to be se-lected on a per class basis, the content shifters can belearnt in a class-independent (aggregated) way i.e.we can assume that within one task, each class labelbelongs to a given semantic domain (determined bythe task), thus the content shifters for their indicatorphrases are the same.
This approach provides an ad-equate amount of training samples for content shiftdetector learning.762Table 2: Example feature representation of local contexts of Arsenal.
The prefix NP stands for the lemma featuresfrom the deepest noun phrase; D,DR and DEP marks the lemmas, roles and their combination in the dependency path,respectively; SUBJ and SUBJD denote the lemmas and dependency roles on the ?subject path?, respectively.His brother, Paul had a long career at Newcastle.
(sentenceId=1, indicator=Newcastle)bag-of-word features syntax-based featureshe, brother, Paul, have, NP#a, NP#long, NP#career, NP#ata, long, career, at D#career, D#have, DR#prepat, DR#dobj, DEP#career#prepat, DEP#have#dobjSUBJ#brother, SUBJ#Paul, SUBJ#he, SUBJD#he#possHe was born in Gosforth, Newcastle and played for Arsenal.
(sentenceId=2, indicator=Arsenal)bag-of-word features syntax-based featureshe, be, bear, in, Gosforth, D#play, DR#prepfor, DEP#play#prepforNewcastle, and, play, for SUBJ#he4.1 Learning Content Shift DetectorsThe key idea behind our approach is that a trainingcorpus for task-specific content shifter learning canbe automatically generated by exploiting the occur-rences of indicators in various contexts.
The localcontext of an indicator is assumed to have altered ifit yields a false positive document-level prediction.More precisely, a training dataset can be constructedfor learning a content shift detector in a way that theinstances are the local contexts of each occurrence ofindicator phrases in the training document set.
Theinstances of this content shifter training dataset arethen labeled as non-altered when the indicatedlabel is among the gold-standard labels of the doc-ument in question or is labeled as altered other-wise.
On this dataset, arbitrary binary classificationmodels (S) can be trained.As a feature representation of a local context of anindicator phrase, the bag-of-words of the sentenceinstance (excluding the indicator phrase itself) wasused at the beginning.
Our preliminary experimentsshowed that the tokens of the sentence after the indi-cator played a negligible role, hence we representedcontexts just by tokens before the indicator.Features concerning the syntactic context of thegiven indicator were also investigated.
For this, weextended the feature set with features derived fromthe constituent and dependency parses of the sen-tence3.
First, the deepest noun phrase which in-cludes the indicator phrase was identified, then all3We parse only the sentences which contain indicator phrasewhich makes these features computable in reasonable time evenon bigger document sets.lemmas from its subtree were gathered.
From thedependency parse, the lemmas and dependency la-bels on the directed path from the indicator to theroot node (main path) were extracted.
The directedpaths branching from this main path starting withsubject dependency were also used for featureextraction (note that these walk in opposite directionto that of the main path).
The intuition of the latterwas that the subject of the given information ?
as itcan differ from the target entity of extraction ?
is ofgreat importance.
We note that we recognize the in-sentence subject and employing a co-reference mod-ule would probably increase the value of these fea-tures.Table 2 exemplifies the feature representation oflocal contexts of the Newcastle and Arsenal indi-cators for the Wikipedia soccer task.
In both sen-tences, a naive system would extract Newcastle asfalse positives.
We want to learn content shiftersfrom them along with the true positive match ofArsenal in sentence 2.
From the first example theCSD could learn even that the bag-of-word con-tains brother or the SUBJ=brother.
However, inthe second example, the bag-of-word representa-tion is not sufficient to learn that the local contextof Newcastle is altered because it is the sub-set of the bag-of-word representation of Arsenal?snon-altered local context.
In this case the syn-tactic context representation can help and in ourCSD DEP=play#prepfor gets high weight for thenon-altered class.7634.2 Co-learning of Indicator Selection and CSDIf document labels are available at training time,an iterative approach can be used to learn the localcontent shift detector and the indicator phrases aswell.
The training phase of this procedure (see Al-gorithm 1) has two outputs, namely the set of indica-tor phrases for each label I and the content shift de-tector S which is a binary function for determiningwhether the sense of an indicator in a particular localcontext is being altered.
Good indicator phrases arethose that identify the class label in question whenthey are present.
In each step of the iteration we se-lect indicator phrases I[l] for each label l based onthe actual state of the document set D?.
Using theseI[l]s we train a CSD S. Then we apply it to the orig-inal dataset D and we delete each local context fromthe documents which was predicted to be altered byS.Algorithm 1 Co-learning of labels and CSDInput: L class labels, D labeled training documentsD?
?
Drepeatfor all l ?
L doI[l]?
indicatorSelection(D?
, l)end forS ?
learnCSD(D?, I)D?
?
removeAlteredParts(D,S)until convergencereturn I, SThe indicator selection and content shifter learn-ing phases can form an iterative process.
The bet-ter the selected indicators are, the better the contentshift detectors can be learnt.
By applying the contentshift detector to each token of the documents, eachpart of the texts lying within the scope of a contentshifter can be removed4.
By using such a cleanedtraining document set (D?
), better indicators can beselected.
These steps can be repeated until someconvergence criterion is reached.
In our experimentswe simply used a fixed iteration number to gain aninsight into the behavior of the approach.4In our first experiments introduced here, we removed theparts of the documents classified as altered.
Instead of removingthese parts they may be marked and then different features maybe extracted from them.Algorithm 2 Document labeling with CSDInput: d document, I indicator sets, S CSDpred?
?for all l ?
L dofor all o ?
occurrences(d, I[l]) doif not altered(o, S) thenpred?
pred ?
lend ifend forend forreturn predThe prediction procedure of the approach (see Al-gorithm 2) then looks for occurrences of the indica-tor phrases in the text and checks whether they arealtered in a certain local context.
A non-altered indi-cator directly assigns a class label without any globalconsistency check on assigned labels.We note here, that the local relationship amongtokens (i.e.
the local context) may be taken intoaccount by incorporating this information directlyinto the feature space of a document classifier (asan alternative of our co-learning procedure), butthe number of features would exponentially increaseand submodels for each indicator phrases should belearnt which would made such a classification taskintractable.4.3 Indicator Phrase SelectionIndicator phrases are sequences of tokens whosepresence implies the positive class.
We aimed toextract phrases with the length of 1,2 or 3 (and weused exact matching after lemmatisation).
There areseveral possible ways of developing indicator selec-tion algorithms.
One way is to treat it as a specialfeature selection procedure where the goal is to se-lect a set of features (uni-, bi-, trigrams of a bag-of-word model) which achieves high recall alongwith moderate precision as false positives are ex-pected to be eliminated by the local CSD in our two-step approach.
Indicator selectors can be even de-rived from most classifiers which are based on fea-ture weighting (like MaxEnt and AvgPerceptron) orfeature ranking (like rule-based classifiers)5 as well.However indicator selection is not the focus of this5A derivation is more complicated or unfeasible forexample-based classifiers like SVMs.764Table 3: Results obtained for local content shift detection in a precision/recall/F-measure format.CMC Obesity SoccerTrainedBoW 90.7 / 60.7 / 72.7 82.1 / 35.4 / 49.4 75.0 / 70.6 / 72.7BoW+syntactic 88.3 / 60.2 / 71.6 84.4 / 33.3 / 47.8 81.0 / 78.9 / 79.9Hand-craftedCSSDB 94.7 / 53.3 / 68.2 42.0 / 57.9 / 48.7 36.8 / 9.8 / 15.5in-sentence 80.7 / 65.2 / 72.2 70.5 / 40.5 / 51.5 N/Awork.For our experiments, a feature evaluation-basedgreedy algorithm was employed to select the set ofindicators from the pool of token uni- and bigrams.The aim of the the indicator selection here is to covereach positive documents while introducing a rela-tively small amount of false positives.
The greedyalgorithm iteratively selects the 1-best phrase ac-cording to a feature evaluation metric based on theactual state of covered documents and adds it to theindicator phrase set.
The process is iterated whilethe score ?
in terms of the applied feature evaluationmetric ?
of the 1-best phrase is above a thresholdt.
The quality of the selected indicator set is highlydependent on the stopping threshold t, but as indi-vidual feature evaluation functions are very fast andthe number of good indicators is usually low (4-5),the whole greedy indicator selection is fast, hence tcan be fine-tuned without overfitting on the trainingsets employing a cross-validation procedure.
As afeature evaluation metric we employed p(+|f) theprobability of the positive class ?+?
conditioned onthe presence of a feature f because preliminary ex-periments did not show any significant advances formore complex metrics.5 ExperimentsExperiments were carried out on the three datasetsintroduced in Section 3 with local content shift de-tection as an individual task and also to investigateits added value to information-oriented document la-beling.In our experiments, we applied the sentence split-ter and lemmatizer implementation of the Mor-phAdorner package6 and the Stanford tokenizerand lexicalized PCFG parser (Klein and Manning,2003)7.6morphadorner.northwestern.edu/7The JAVA implementation of the entire framework and5.1 Content Shifter Learning ResultsIn order to evaluate content shift detection as an indi-vidual task, a set of indicator phrases have to be fixedas an input to the CSD.
We used manually collectedindicator phrases for each label for each dataset.
Weutilized the terms of Farkas and Szarvas (2008) andFarkas et al (2009) collected for the CMC and Obe-sity datasets, respectively and club names for theSoccer dataset in our first branch of experiments.Note that the clinical term sets here have been man-ually fine-tuned as they were developed for partici-pating systems of the shared tasks of the corpora.Based on the occurrences of these fixed indicatorphrases, CSD training datasets were built from thelocal contexts of the three datasets and binary clas-sification was carried out by using MaxEnt.
Table3 shows the results achieved by the learnt CSDs us-ing the bag-of-word feature representation (row 1)along with the ones obtained by the feature set thatwas extended with syntactic patterns (raw 2).
Here,the precision/recall/F-measure values measure howmany false positive matches of the indicator phrasescan be recognized (the F-measure of the alteredclass), i.e.
here, the true positives are local contextsof an indicator phrase which do not indicate a docu-ment label in the evaluation set and the local contentshift detector predicted it to be altered.For comparison purposes, we employed manu-ally developed CSDs which were fine-tuned for themedical shared task datasets.
Row 3 of Table 3 (werefer to it as content shifted sentence detection base-line (CSSDB) later on) shows the results archivedby the method which predicts every sentence to bealtered which contain any cue phrases for nega-tion, modality and different experiencer.
Note thatoff-the-shelf tools are available just for these typesof content shifters.
We collected cue phrases forsuch a content shifted sentence detection from thedataset adapters can be found as the supplementary material765works of Chapman et al (2007), Light et al (2004)and Vincze et al (2008) and from the experiments ofFarkas and Szarvas (2008) and Farkas et al (2009).For the CMC and Obesity tasks, hand-craftedin-sentence CSDs were also available (Farkas andSzarvas, 2008; Farkas et al, 2009), i.e.
they applyheuristics ?
which usually tries to recognise clauseboundaries ?
for determining the scope of a nega-ton/modality cue.
This CSD is more fine-grainedthan the sentence-level one as here a part of a sen-tence can be detected as alteredwhile other partsas non-altered.
The results of these detectors ?two different CSDs, both highly fine-tuned for thecorresponding shared task ?
are listed in the last rowof Table 3.On the CMC dataset, our machine learning ap-proach identified mostly negation and speculationexpressions as content shifters; the top weightedfeatures for the positive class of the MaxEnt modelwere no, without, may and vs.
They can filter outfalse positive matches likeHyperinflated without focal pneumonia..On the Obesity dataset, similar content shifterswere learnt along with references to family mem-bers (like the terms mother and uncle, and the fam-ily history header).
The significance of these typesof content shifters may be illustrated by the follow-ing sentence:History of hypertension in mother and sis-ter.The soccer task highlighted totally different con-tent shifters which is also the reason for the poor per-formance of CSSDB.
The mention of a club namewhich the person in question did not play for (falsepositives) is usually a rival club, club of an unsuc-cessful negotiation or club which was managed bythe footballer after his retirement.
For example:His last game was against Chelsea atStamford Bridge.He was a coach at United during his son?splaying career.Summing up, the machine learnt CSDs proved tobe competitive with the manually fine-tuned CSDon the three datasets.
Table 3 shows that learntCSDs were able to eliminate a significant amountof false positive indicator phrase matches on eachof the three datasets.
The hand-crafted CSDs de-veloped for the medical texts certainly work poorly(an F-score of 15.5) on the Soccer dataset as contentshifters different from negation, hedge and experi-encer are useful there.
On the other hand, the contentshifters could be learnt on this dataset by our CSDapproach (achieving F-score of 79.9).
In the clinicalcorpora, the features from the syntactic parses justconfused the system, but they proved to be usefulon the Soccer corpus.
Here, the dependency parseachieved improvements in terms of both precisionand recall (the number of true positives increased by137) which can be mainly attributed to the preposi-tions against and over.
The reason why it did notadvance on the clinical corpora is probably the do-main difference between the training corpus of theparsers and the target texts, i.e.
the parsers trainedon the Wall Street Journal could not build adequatedependency parses on clinical notes.As a final comparison we investigated the manu-ally annotated BioScope corpus (Vincze et al, 2008)as a CSD.
The CMC corpus is included in the Bio-Scope corpus where text spans in the in-sentencescope of speculation and negation were annotated.We used this manual annotation as an oracle CSDand got an F-measure of 75.2 (which is significantlyhigher than the scores 72.2 and 72.7 archived by thehand-crafted and trained CSD respectively).
Thisscore can be regarded as an upper bound for theamount of false positive indicator matches that canbe fixed by local speculation and negation detec-tors.
The remaining false positives are not coveredby the linguistically motivated annotations of Bio-Scope, i.e.
false positives recognizable by domainknowledge (e.g.
coding symptoms should be omit-ted when a certain diagnosis that is connected withthe symptom in question is present in the document)are not marked.Our error analysis revealed that most of the er-rors of the learnt CSDs is due to the lack of seman-tic link between lexical units.
For instance, on theSoccer dataset it could learn that the token coachoccuring in the sentence in question indicates analtered content, but it was not able to recognisethis for trainer.
The reason for that is simple, theratio of occurrences of trainer:coach is 5:95 in the766Table 4: Results obtained by document multi-labeling algorithms in a precision/recall/F-measure format.rowID CMC Obesity Soccer1 SVM 87.7 / 76.7 / 81.8 90.0 / 81.3 / 85.4 92.2 / 75.1 / 82.82 Baseline MaxEnt with CSSDB 92.2 / 72.2 / 81.0 91.4 / 87.6 / 89.4 92.2 / 77.4 / 84.23 PART 83.9 / 80.6 / 82.2 87.3 / 86.4 / 86.8 81.2 / 77.0 / 79.04 without CSD 78.0 / 85.1 / 81.4 89.2 / 93.6 / 91.3 84.4 / 83.7 / 84.15 Indicator with CSSDB 79.0 / 84.1 / 81.4 94.8 / 86.6 / 91.1 85.2 / 85.5 / 85.36 Selection with learnt CSD 83.1 / 83.2 / 83.2 91.7 / 92.9 / 92.3 91.7 / 85.2 / 88.37 after 3 iterations 82.4 / 86.8 / 84.6 92.6 / 95.4 / 94.0 92.5 / 84.0 / 88.08 after 10 iterations 82.4 / 86.8 / 84.6 92.7 / 95.4 / 94.0 92.5 / 84.0 / 88.09 Baseline MaxEnt with learnt CSD 89.9 / 77.0 / 83.0 91.9 / 90.4 / 91.1 95.0 / 78.7 / 86.1training corpus.
Increasing the training size may bea simple way to overcome this shortcoming.
Notethat increasing the number of labels (e.g.
introduc-ing more soccer clubs in the Soccer task) would alsodirectly increase the size of training dataset as weuse the occurrences of the indicator phrases belong-ing to each of the labels for training a CSD.
The so-lution for the rare cases would require the explicithandling of semantic relatedness (by utilising ex-isting semantic resources or trying to automaticallyidentify task-specific relations).5.2 Document Labeling ResultsThe second branch of experiments investigated theadded value of CSDs in information-oriented doc-ument labeling tasks.
Table 4 summarizes the re-sults we got on the three datasets using the micro-averaged F?=1 of assigned labels (positive class).As baseline systems we trained binary SVMswith a linear kernel, MaxEnts and PARTs ?
a rule-learner classification algorithm (Frank and Witten,1998) ?
for each label using the bag-of-word rep-resentation of the documents (implementations ofSVMligth (Joachims, 1999), MALLET (McCallum,2002) and WEKA (Witten and Frank, 1999) wereused).
The first two learners are popular choices fordocument classification, while the third is similar toour simple indicator selection procedure.
We did nottuned the parameters of the classifiers, we used thedefault ones everywhere.To have a fair comparison, we applied to pre-processing steps on dataset of these document clas-sifiers.
First, we removed from the training andevaluation raw documents which were predicted tobe altered by CSSDB.
Second, as our indicatorselection phrase can be regarded as a special fea-ture selection method, we carried out an InformationGain-based feature selection (keeping the 500 best-rated features proved to be the best solution) on thebag-of-word representation of the documents.
Theeffect of these two preprocessing steps varied amongdatasets.
It improved the F-score of the MaxEntbaseline document classifier by 20%, 2% and 3% onthe Obesity, CMC and Soccer datasets, respectively(the F-measures of Table 4 are the values we got byemploying pre-processing).The indicator selection results presented in therows 4-8 of Table 4 made use of the p(+|f)-basedindicator selector with a five-fold-cross-validatedstopping threshold t (introduced in Section 4.3).Row 4 contains the results of using the selected in-dicators without any CSD.
Indicator selection withthe CSSDB was applied for the 5th row.
Rows 6-8 of Table 4 show the results obtained after one,three and ten iterations of the full learning algo-rithm (see Algorithm 1).
For training the CSD,we employed MaxEnt as a binary classifier for de-tecting altered local contexts and we used thebasic BoW feature representation for the clinicaltasks while the extended (BoW+syntactic) one forthe Soccer dataset.In the final experiment (the last row of Table4)) we investigated whether the learnt content shiftdetector can be applied as a general ?documentcleaner?
tool.
For this, we trained the baseline Max-Ent document classifier with feature selection ondocuments from which the text spans predicted tobe altered by the learnt CSD in the tenth iter-ation were removed.
This means that the systems767used in row 2 and row 9 differ only in the applieddocument cleaner pre-processing steps (the first oneapplied the CSSDB while the latter one employedthe the learnt CSD).The difference between the best baseline and theindicator selector with learnt CSD and between thebest baseline and the document classifier with learntCSD were statistically significant8 on each dataset.The difference between the predictions after the 1stand 3rd iterations were statistically significant onthe CMC and the Obesity corpora but not signifi-cant on the Soccer dataset.
The difference betweenthe 3th and 10th iterations were not significant in ei-ther case.
Our co-learning method which integratedthe document-labeling and CSD tasks significantlyoutperformed the baseline approaches ?
which useseparate document cleaning and document labelingsteps ?
on the three datasets.On the clinical domains the automatically se-lected indicators were disease names, symptomnames (e.g.
high blood pressure), their spelling vari-ants, synonyms (like hypertension) and their abbre-viations (e.g htn).
On the soccer domain club names,synonyms (like The Saints) and stadium names (e.g.Old Trafford) were selected.
A label was indicatedby 3-4 indicator phrases.Note that in these information-oriented docu-ment multi-labeling tasks simple indicator selection-based document labelers alone achieved resultscomparable to the bag-of-words-based classifiers.The learnt content shift detectors led to an averageimprovement of 3.6% in the F-measure (i.e.
a 24%error reduction).
The effect of further iterations isvarious.
As Table 4 shows, three iterations broughtan increase on the CMC and Obesity datasets but noton the Soccer corpus.
After a few iterations the setof indicator phrases and the content shift detectordid not change substantially.
The results achievedby the MaxEnt document classifier employing the?cleaned?
training documents (last row of Table 4)are significantly better (an average improvement of1.9% in the F-measure and 12% error reduction)than those by the CSSDB (row 2) but the indicatorselector approach performed even better.8According to McNemar?s test with P-value of 0.0016 ConclusionsIn this paper, we dealt with information-orienteddocument labeling tasks and investigated machinelearning approaches for local content shift detectorsfrom document-level labels.
We demonstrated ex-perimentally that a significant amount of false posi-tive matches of indicator phrases can be recognizedby trained content shift detectors.
Our trained CSDdoes not use any task or domain specific knowledgeand exploits the false and true positive matches of in-dicator phrases, i.e.
it uses only document-level an-notation.
This task-independent approach achievedcompetitive results with CSDs which were manuallyfine-tuned for particular datasets.
The empirical re-sults also support the idea of generalized local CSD(false positive removal) opposite to developing in-dependent CSD for particular language phenomena(like negation and speculation).A co-learning framework for training local con-tent shift detectors and indicator selection was in-troduced as well.
Our method integrates documentclassification and CSD learning, which are tradi-tionally used as independent submodules of appli-cations.
Experiments on three information-orienteddocument-labeling datasets ?
from two applicationareas ?
with simple indicator selection and syntacticparse-based content shifter learning were performedand the results show a clear improvement over thebag-of-word-based document classification baselineapproaches.However, the proposed content shift detec-tor learning approach is tailored for information-oriented document labeling tasks, i.e.
it performswell when not too many and reliable indicatorphrases are present.
In the future, we plan to in-vestigate and extend the framework for the generaldocument classification task where many indicatorswith complex relationships among them determinethe labels of a document but local content shifterscan play an important role.AcknowledgementsThis work was partially founded by the ResearchGroup on Artificial Intelligence of the HungarianAcademy of Sciences.
Richa?rd Farkas was alsofunded by Deutsche Forschungsgemeinschaft grantSFB 732.768ReferencesEiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,Tomoko Ohkuma, Hiroshi Mashuichi, and KazuhikoOhe.
2009.
TEXT2TABLE: Medical Text Summa-rization System Based on Named Entity Recognitionand Modality Identification.
In Proceedings of theBioNLP 2009 Workshop, pages 185?192.Wendy W. Chapman, David Chu, and John N. Dowling.2007.
Context: an algorithm for identifying contextualfeatures from clinical text.
In Proceedings of the ACLWorkshop on BioNLP 2007, pages 81?88.Richa?rd Farkas and Gyo?rgy Szarvas.
2008.
Automaticconstruction of rule-based icd-9-cm coding systems.BMC Bioinformatics, 9(Suppl 3):S10.Richa?rd Farkas, Gyo?rgy Szarvas, Istva?n Hegedu?s, At-tila Alma?si, Veronika Vincze, Ro?bert Orma?ndi, andRo?bert Busa-Fekete.
2009.
Semi-automated construc-tion of decision rules to predict morbidities from clini-cal texts.
Journal of the American Medical InformaticsAssociation, 16:601?605.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010 Shared Task: Learning to Detect Hedges andtheir Scope in Natural Language Text.
In Proceedingsof the Fourteenth Conference on Computational Natu-ral Language Learning (CoNLL-2010): Shared Task,pages 1?12.Eibe Frank and Ian H. Witten.
1998.
Generating accu-rate rule sets without global optimization.
In Proc.
ofFifteenth International Conference on Machine Learn-ing, pages 144?151.Carol Friedman, Philip O. Alderson, John H. M. Austin,James J. Cimino, and Stephen B. Johnson.
1994.
AGeneral Natural-language Text Processor for ClinicalRadiology.
Journal of the American Medical Infor-matics Association, 1(2):161?174.Viola Ganter and Michael Strube.
2009.
FindingHedges by Chasing Weasels: Hedge Detection Us-ing Wikipedia Tags and Shallow Linguistic Features.In Proceedings of the ACL-IJCNLP 2009 ConferenceShort Papers, pages 173?176.Thorsten Joachims, 1999.
Making large-scale supportvector machine learning practical, pages 169?184.MIT Press, Cambridge, MA, USA.Halil Kilicoglu and Sabine Bergler.
2009.
Syntactic De-pendency Based Heuristics for Biological Event Ex-traction.
In Proceedings of the BioNLP WorkshopCompanion Volume for Shared Task, pages 119?127.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overview ofBioNLP?09 Shared Task on Event Extraction.
In Pro-ceedings of the BioNLP 2009 Workshop CompanionVolume for Shared Task, pages 1?9.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st ACL,pages 423?430.Leah S. Larkey and W. Bruce Croft.
1995.
Automatic as-signment of icd9 codes to discharge summaries.
Tech-nical report.Marc Light, Xin Ying Qiu, and Padmini Srinivasan.2004.
The language of bioscience: Facts, specula-tions, and statements in between.
In Proc.
of Biolink2004, Linking Biological Literature, Ontologies andDatabases (HLT-NAACL Workshop:), pages 17?24.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Ben Medlock and Ted Briscoe.
2007.
Weakly supervisedlearning for hedge classification in scientific literature.In Proceedings of the ACL, pages 992?999, June.Roser Morante, V. Van Asch, and A. van den Bosch.2009.
Joint memory-based learning of syntactic andsemantic dependencies in multiple languages.
In Pro-ceedings of CoNLL, pages 25?30.Arzucan ?Ozgu?r and Dragomir R. Radev.
2009.
Detect-ing Speculations and their Scopes in Scientific Text.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages1398?1407.John P. Pestian, Chris Brew, Pawel Matykiewicz,DJ Hovermale, Neil Johnson, K. Bretonnel Cohen, andWlodzislaw Duch.
2007.
A shared task involvingmulti-label classification of clinical free text.
In Pro-ceedings of the ACL Workshop on BioNLP 2007, pages97?104.Roser Sauri and James Pustejovsky.
2009.
Factbank:a corpus annotated with event factuality.
LanguageResources and Evaluation, 43(3):227?268.Peter Scho?nhofen.
2006.
Identifying document topicsusing the wikipedia category network.
In Proceedingsof the 2006 IEEE/WIC/ACM International Conferenceon Web Intelligence, pages 456?462.Ille?s Solt, Domonkos Tikk, Viktor Ga?l, and Zsolt TivadarKardkova?cs.
2009.
Semantic classification of diseasesin discharge summaries using a context-aware rule-based classifier.
J.
Am.
Med.
Inform.
Assoc., 16:580?584, jul.Stephanie Strassel, Mark A. Przybocki, Kay Peterson,Zhiyi Song, and Kazuaki Maeda.
2008.
Linguis-tic resources and evaluation techniques for evaluationof cross-document automatic content extraction.
InLREC.Gyo?rgy Szarvas.
2008.
Hedge Classification in Biomed-ical Texts with a Weakly Supervised Selection of Key-words.
In Proceedings of ACL-08, pages 281?289.769O.
Uzuner, Ira Goldstein, Yuan Luo, and Isaac Kohane.2008.
Identifying Patient Smoking Status from Medi-cal Discharge Records.
Journal of American MedicalInformatics Association, 15(1):14?24.Ozlem Uzuner.
2009.
Recognizing obesity and comor-bidities in sparse data.
Journal of American MedicalInformatics Association, 16(4):561?70.Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,Gyo?rgy Mo?ra, and Ja?nos Csirik.
2008.
The Bio-Scope Corpus: Biomedical Texts Annotated for Un-certainty, Negation and their Scopes.
BMC Bioinfor-matics, 9(Suppl 11):S9.Ian H. Witten and Eibe Frank.
1999.
Data Mining: Prac-tical Machine Learning Tools and Techniques withJava Implementations.
Morgan Kaufmann.770
