Proceedings of NAACL HLT 2007, Companion Volume, pages 133?136,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsAn integrated architecture for speech-input multi-target machine translationAlicia Pe?rez, M. Ine?s TorresDep.
of Electricity and ElectronicsUniversity of the Basque Countrymanes@we.lc.ehu.esM.
Teresa Gonza?lez, Francisco CasacubertaDep.
of Information Systems and ComputationTechnical University of Valenciafcn@dsic.upv.esAbstractThe aim of this work is to show the abil-ity of finite-state transducers to simultane-ously translate speech into multiple lan-guages.
Our proposal deals with an ex-tension of stochastic finite-state transduc-ers that can produce more than one out-put at the same time.
These kind of de-vices offer great versatility for the inte-gration with other finite-state devices suchas acoustic models in order to produce aspeech translation system.
This proposalhas been evaluated in a practical situation,and its results have been compared withthose obtained using a standard mono-target speech transducer.1 IntroductionFinite-state models constitute an important frame-work both in syntactic pattern recognition and inlanguage processing.
Specifically, stochastic finite-state transducers (SFSTs) have proved to be usefulfor machine translation tasks within restricted do-mains; they usually offer high speed during the de-coding step and they provide competitive results interms of error rates (Mohri et al, 2002).
Moreover,SFSTs have proved to be versatile models, whichcan be easily integrated with other finite-state mod-els (Pereira and Riley, 1997).The article (Casacuberta and Vidal, 2004) ex-plored an automatic method to learn an SFST from abilingual set of samples for machine translation pur-poses, the so-called GIATI (Grammar Inference andAlignments for Transducers Inference).
It describedhow to learn both the structural and the probabilisticcomponents of an SFST making use of underlyingalignment models.A multi-target SFST is a generalization of stan-dard SFSTs, in such a way that every input stringin the source language results in a tuple of outputstrings each being associated to a different targetlanguage.
An extension of GIATI that allowed to in-fer a multi-target SFST from a multilingual corpuswas proposed in (Gonza?lez and Casacuberta, 2006).A syntactic variant of this method (denoted as GI-AMTI) has been used in this work in order to inferthe models from training samples as it is summa-rized in section 3.On the other hand, speech translation has been al-ready carried out by integrating acoustic models intoa SFST (Casacuberta et al, 2004).
Our main goalin this work is to extend and assess these method-ologies to accomplish spoken language multi-targettranslation.
Section 2 deals with this proposal bypresenting a new integrated architecture for speech-input multi-target translation.
Under this approachspoken language can be simultaneously decoded andtranslated into m languages using a unique network.In section 4, the performance of the system hasbeen experimentally evaluated over a trilingual taskwhich aims to translate TVweather forecast into twolanguages at the same time.2 An integrated architecture forspeech-input multi-target translationThe classical architecture for spoken languagemulti-target translation involves a speech recogni-133tion system in a serial architecture withm decoupledtext-to-text translators.
Thus, the whole process in-volves m + 1 searching stages, a first one for thespeech signal transcription into the source languagetext string, and further m for the source languagetranslation into the m target languages.
If we re-placed the m translators by the multi-target SFST,the problem would be reduced to 2 searching stages.Nevertheless, in this paper we propose a natural wayfor acoustic models to be integrated in the same net-work.
As a result, the input speech-signal can besimultaneously decoded and translated into m targetlanguages just in a single searching stage.Given the acoustic representation (x) of a speechsignal, the goal of multi-target speech translationis to find the most likely m target strings (tm);that is, one string (ti) per target language involved(i ?
{1, .
.
.
,m}).
This approach is summarizedin eq.
(1), where the hidden variable s can be in-terpreted as the transcription of the speech signal:t?m = argmaxtmP (tm|x) = argmaxtm?sP (tm, s|x)(1)Making use of Bayes?
rule, the former expressionturns into:t?m = argmaxtm?sP (tm, s)P (x|tm, s) (2)Empirically, there is no loss of generality if we as-sume that the acoustic signal representation dependsonly on the source string: i.e., that P (x|tm, s) is in-dependent of tm.
In this sense, eq.
(2) can be rewrit-ten as:t?m = argmaxtm?sP (tm, s)P (x|s) (3)Equation (3) combines a standard acoustic model,P (x|s), and a multi-target translation model,P (tm, s), both of whom can be integrated on the flyduring the searching routine.
Nevertheless, the outermaximization is computationally very expensive tosearch for the optimal tuple of target strings tm inan effective way.
Thus we make use of the so calledViterbi approximation, which finds the best path.3 InferenceGiven a multilingual corpus, that is, a finite set ofmultilingual samples (s, t1, .
.
.
, tm) ?
??
?
?
?1 ??
?
?
?
?
?m, where ti denotes the translation of thesource sentence s (formed by words of the input vo-cabulary ?)
into the i-th target language, which, inits turn, has a vocabulary ?i, the GIAMTI methodcan be outlined as follows:1.
Each multilingual sample is transformed into asingle string from an extended vocabulary (?
????
?1 ?
?
?
?
??
?m) using a labelling function(Lm).
This transformation searches an ade-quate monotonous segmentation for each of them source-target language pairs.
A monotonoussegmentation copes with monotonous align-ments, that is, j < k ?
aj < ak followingthe notation of (Brown et al, 1993).
Eachsource word is then joined with a target phraseof each language as the corresponding segmen-tation suggests.
Each extended symbol consistsof a word from the source language plus zeroor more words from each target language.2.
Once the set of multilingual samples has beenconverted into a set of single extended strings(z ?
??
), a stochastic regular grammar can beinferred.3.
The extended symbols associated with thetransitions of the automaton are transformedinto one input word and m output phrases(w/p?1/ .
.
.
/p?m) by the inverse labeling func-tion (L?m), leading to the required transducer.In this work, the first step of the algorithm (asdescribed above), which is the one that handlesthe alignment and segmentation routines, relies onstatistical alignments obtained with GIZA++ (Och,2000).
The second step was implemented us-ing our own language modeling toolkit, whichlearns stochastic k-testable in the string-sense gram-mars (Torres and Varona, 2001), and allows forback-off smoothing.4 Experimental results4.1 Task and corpusWe have implemented a highly practical applicationthat could be used to translate on-line TV weatherforecasts into several languages, taking the speechof the presenter as the input and producing as outputtext-strings, or sub-titles, in several languages.
For134this purpose, we used the corpus METEUS (see Ta-ble 1) which consists of a set of trilingual sentences,in English, Spanish and Basque, as extracted fromweather forecast reports that had been published onthe Internet.
Basque language is a minority lan-guage, spoken in a small area of Europe and alsowithin some small American communities (such asthat in Boise, Idaho).
In the Basque Country it hasan official status along with Spanish.
However bothlanguages differs greatly in syntax and in semantics.The differences in the size of the vocabulary (seeTable 1), for instance, are due to the agglutinativenature of the Basque language.With regard to the speech test, the input consistedof the speech signal recorded by 36 speakers, eachone reading out 50 sentences from the test-set in Ta-ble 1.
That is, each sentence was read out by at leastthree speakers.
The input speech resulted in approx-imately 3.50 hours of audio signal.
Needless to say,the application that we envisage has to be speaker-independent if it is to be realistic.Spanish Basque EnglishTraining Sentences 14,615Different Sent.
7,225 7,523 6,634Words 191,156 187,462 195,627Vocabulary 702 1,147 498Average Length 13.0 12.8 13.3TestDifferent Sent.
500Words 8,706 8,274 9,150Average Length 17.4 16.5 18.3Perplexity (3grams) 4.8 6.7 5.8Table 1: Main features of the METEUS corpus.4.2 System evaluationThe experimental setup was as follows: the multi-target SFST was learned from the training set in Ta-ble 1 using the GIAMTI algorithm described in sec-tion 1; then, the speech test was translated, and theoutput provided by the system in each language wascompared to the corresponding reference sentence.Additionally, two mono-target SFST were inferredfrom the same training set with their outputs for theaforementioned test to be taken as baseline.4.2.1 Computational costThe expected searching time and the amount ofmemory that needs to be allocated for a given modelare two key parameters to bear in mind in speech-input machine translation applications.
These valuescan be objectively measured based on the size and onthe average branching factor of the model displayedin Table 2.multi-target mono-targetS2B S2ENodes 52,074 35,034 20,148Edges 163,146 115,526 69,690Braching factor 3.30 3.13 3.46Table 2: Features of multi-target model and the twodecoupled mono-target models (one for Spanish toBasque translation, referred to as S2B, and the sec-ond for Spanish to English, S2E).Adding the states and the edges up for the twomono-target SFSTs that take part in the decoupledarchitecture (see Table 2), we conclude that the de-coupled model needs a total of 185, 216 edges to beallocated in memory, which represents an incrementof 13% in memory-space with respect to the multi-target model.On the other hand, the multi-target approach of-fers a slightly smaller branching factor than eachmono-target approach.
As a result, fewer paths haveto be explored with the multi-target approach thanwith the decoupled one, which means that searchingfor a translation can be faster.
In fact, experimentalresults in Table 3 show that the mono-target archi-tecture works%11more slowly than the multi-targetone.multi-target mono-targetS2B S2E S2B+S2ETime (s) 30,514 24,398 9,501 33,899Table 3: Time needed to translate the speech-testinto two languages.Summarizing, in terms of computational cost(space and time), a multi-target SFST performs bet-ter than the mono-target decoupled system.4.2.2 PerformanceSo far, the capability of the systems have been as-sessed in terms of time and spatial costs.
However,the quality of the translations they provide is, doubt-less, the most relevant evaluation criterion.
In orderto assess the performance of the system in a quan-titative manner, the following evaluation parameters135were computed for each scenario: bilingual evalua-tion under study (BLEU), position independent er-ror rate (PER) and word error rate (WER).As can be derived from the Speech-input trans-lation results shown in Table 4, slightly better re-sults are obtained with the classical mono-target SF-STs, compared with the multi-target approach.
FromSpanish into English the improvement is around3.4% but from Spanish into Basque, multi-target ap-proach works better with an improvement of a 0.8%.multi-target mono-targetS2B S2E S2B S2EBLEU 39.5 59.0 39.2 61.1PER 42.2 25.3 41.5 23.6WER 51.5 33.9 50.5 31.9Table 4: Speech-input translation results for Spanishinto Basque (S2B) and Spanish into English (S2E)using a multi-target SFST or two mono-target SF-STs.The process of speech signal decoding is itselfintroducing some errors.
In an attempt to measurethese errors, the text transcription of the recognizedinput signal was extracted and compared to the inputreference in terms of WER as shown in Table 5.multi-target mono-targetS2B S2EWER 10.7 9.3 9.1Table 5: Spanish speech decoding results for themulti-target SFST and the two mono target SFSTs.5 Concluding remarks and further workA fully embedded architecture that integrates theacoustic model into the multi-target translationmodel for multiple speech translation has been pro-posed.
Due to the finite-state nature of this model,the speech translation engine is based on a Viterbi-like algorithm.
The most significant feature of thisapproach is its ability to carry out both the recogni-tion and the translation into multiple languages inte-grated in a unique model.In contrast to the classical decoupled systems,multi-target SFSTs enable the translation from onesource language simultaneously into several targetlanguages with lower computational costs (in termsof space and time) and comparable qualitative re-sults.In future work we intend to make a deeper studyon the performance of the multi-target system as theamount of targets increase, since the amount of pa-rameters to be estimated also increases.AcknowledgementsThis work has been partially supported by the Uni-versity of the Basque Country and by the SpanishCICYT under grants 9/UPV 00224.310-15900/2004and TIC2003-08681-C02-02 respectively.ReferencesP.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311.Francisco Casacuberta and Enrique Vidal.
2004.
Ma-chine translation with inferred stochastic finite-statetransducers.
Computational Linguistics, 30(2):205?225.F.
Casacuberta, H. Ney, F. J. Och, E. Vidal, J. M.Vilar, S. Barrachina, I.
Garc?
?a-Varea, D. Llorens,C.
Mart?
?nez, S. Molau, F. Nevado, M. Pastor, D. Pico?,A.
Sanchis, and C. Tillmann.
2004.
Some approachesto statistical and finite-state speech-to-speech transla-tion.
Computer Speech and Language, 18:25?47, Jan-uary.M.
Teresa Gonza?lez and Francisco Casacuberta.
2006.Multi-Target Machine Translation using Finite-StateTransducers.
In Proceedings of TC-Star Speech toSpeech Translation Workshop, pages 105?110.Mehryar Mohri, Fernando Pereira, and Michael Ri-ley.
2002.
Weighted finite-state transducers inspeech recognition.
Computer, Speech and Language,16(1):69?88, January.Franz J. Och.
2000.
GIZA++: Training of statisticaltranslation models.Fernando C.N.
Pereira and Michael D. Riley.
1997.Speech Recognition by Composition of Weighted Fi-nite Automata.
In Emmanuel Roche and Yves Sch-abes, editors, Finite-State Language Processing, Lan-guage, Speech and Communication series, pages 431?453.
The MIT Press, Cambridge, Massachusetts.M.
Ine?s Torres and Amparo Varona.
2001. k-tss lan-guage models in speech recognition systems.
Com-puter Speech and Language, 15(2):127?149.136
