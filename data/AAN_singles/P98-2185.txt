An Interactive Domain Independent Approach to RobustDialogue InterpretationCaro lyn  Penste in  Ros4LRDC 520, Univers i ty of P i t tsburgh3939 Ohara  St.,P i t t sburgh  PA, 15260rosecp?pitt, eduLor i  S. Lev inCarnegie Mellon Univers i tyCenter for Machine Translat ionP i t tsburgh,  PA 15213isl~cs, cmu.
eduAbst ractWe discuss an interactive approach to robust inter-pretation in a large scale speech-to-speech transla-tion system.
Where other interactive approaches torobust interpretation have depended upon domaindependent repair rules, the approach described hereoperates efficiently without any such hand-coded re-pair knowledge and yields a 37% reduction in errorrate over a corpus of noisy sentences.1 In t roduct ionIn this paper we discuss ROSE, an interactive ap-proach to robust interpretation developed in thecontext of the JANUS speech-to-speech translationsystem (Lavie et al, 1996).
Previous interactiveapproaches to robust interpretation have either re-quired excessive amounts of interaction (Ros4 andWaibel, 1994), depended upon domain dependentrepair rules (Vail Noord, 1996; Danieli and Gerbino,1995), or relied on the minimum distance parsingapproach (Hipp, 1992; Smith, 1992; Lehman, 1989)which has been shown to be intractable in a large-scale system (Ros6 and Lavie, 1997).
In contrast,the ROSE approach operates efficiently without anyhand-coded repair knowledge.
An empirical evalu-ation demonstrates the efficacy of this domain in-dependent approach.
A further evaluation demon-strates that the ROSE approach combines easilywith available domain knowledge in order to improvethe quality of the interaction.The ROSE approach is based on a model of hu-man communication between speakers of differentlanguages with a small shared language base.
Hu-mans who share a very small language base areable to communicate when the need arises by sim-plifying their speech patterns and negotiating un-til they manage to transmit their ideas to one an-other (Hatch, 1983).
As the speaker is speaking,the listener "casts his net" in order to catch thosefragments of speech that are comprehensible to him,which he then attempts to fit together semantically.His subsequent negotiation with the speaker buildsupon this partial understanding.
Similarly, ROSErepairs extragramlnatical input in two phases.
Thefirst phase, Repair Hypothesis Formation, is respon-sible for assembling a set of hypotheses about themeaning of the ungrammatical utterance.
In thesecond phase, Interaction with the User, the sys-tem generates a set of queries, negotiating with thespeaker in order to narrow down to a single bestmeaning representation hypothesis.This approach was evaluated in the context ofthe JANUS multi-lingual machine translation sys-tem.
First, the system obtains a meaning represen-tation for a sentence uttered in the source language.Then the resulting meaning representation structureis mapped onto a sentence in the target languageusing GENKIT (Tomita and Nyberg, 1988) with asentence level generation grammar.
Currently, thetranslation system deals with the scheduling domainwhere two speakers attempt o schedule a meetingtogether over the phone.
This paper focuses onthe Interaction phase.
Details about the Hypoth-esis Formation phase are found in (Ros6, 1997).2 In teract ive  Repa i r  In  DepthAs mentioned above, ROSE repairs extragralnmat-ical input in two phases.
The first phase, RepairHypothesis Formation, is responsible for assemblinga ranked set of ten or fewer hypotheses about themeaning of the ungrammatical utterance xpressedin the source language.
This phase is itself dividedinto two stages, Partial Parsing and Combination.The Partial Parsing stage is similar to the conceptof the listener "casting his net" for comprehensi-ble fragments of speech.
A robust skipping parser(Lavie, 1995) is used to obtain an analysis for islandsof the speaker's sentence.
In the Combination stage,the fragments from the partial parse are assembledinto a ranked set of alternative meaning represen-tation hypotheses.
A genetic programming (Koza,1992; Koza, 1994) approach is used to search fordifferent ways to combine the fragments in order toavoid requiring any hand-crafted repair rules.
Ourgenetic programming approach as been shown pre-viously to be orders of magnitude more efficient, thanthe nainimum distance parsing approach (Ros6 andLavie, 1997).
In the second phase, Interaction with1129the user, the system generates a set of queries, nego-tiating with the speaker in order to narrow down toa single best meaning representation hypothesis.
Or,if it determines based on the user's responses to itsqueries that none of its hypotheses are acceptable,it requests a rephrase.Inspired by (Clark and Wilkes-Gibbs, 1986; Clarkand Schaefer, 1989), the goal of the InteractionPhase is to minimize collaborative ffort betweenthe system and the speaker while maintaining a highlevel of interpretation accuracy.
It uses this princi-ple in determining which portions of the speaker'sutterance to question.
Thus, it focuses its interac-tion on those portions of the speaker's meaning thatit is particularly uncertain about.
In its question-ing, it attempts to display the state of the system'sunderstanding, acknowledging information conveyedby the speaker as it becomes clear.
The interactionprocess can be summarized as follows: The systemfirst assesses the state of its understanding of whatthe speaker has said by extracting features that dis-tinguish the top set of hypotheses from one another.It then builds upon this understanding by cyclingthrough the following four step process: selecting afeature; generating a natural anguage query fromthis feature; updating its list of alternative hypothe-ses based on the user's answer; and finally updatingits list of distinguishing features based on the re-maining set of alternative hypotheses.2.1 Extracting Distinguishing FeaturesIn the example in Figure 1, the Hypothesis Forma-tion phase produces three alternative hypotheses.The hypotheses are ranked using a trained evalua-tion function, but the hypothesis ranked first is notguaranteed to be best.
In this case, the hypothe-sis ranked as second is the best hypothesis.
Thehypotheses are expressed in a frame-based featurestructure representation.
Above each hypothesis ithe corresponding text generated by" the system forthe associated feature structure.In order for the system to return the correct hy-pothesis, it must use interaction to narrow down thelist of alternatives to the best single one.
The firsttask of the Interaction Mechanism is to determinewhat the system knows about what the speaker hassaid and what it is not certain about.
It does thisby comparing the top set of repair hypotheses andextracting a set of features that distinguish themfrom one another.
The set of distinguishing featurescorresponding to the example set of alternative hy-potheses can be found in Figure 2.The meaning representation's recursive structureis made up of frames with slots that can be filled ei-ther with other frames or with atomic fillers.
Thesecompositional structures can be thought of as trees,with the top level frame being the root of the treeand branches attached through slots.
The featuresSentence: What did you say 'bout whatwas your schedule for the twenty sixth of May?Alternative Hypotheses:"What will be scheduled for the twenty-sixth of May"((frame *schedule)(what ((frame *what)(wh +)))(when ((frame *simple-time)(day 26)(month 5))))"You will schedule what for the twenty-sixth of May?
"((frame *schedule)(who ((frame *you)))(what ((frame *what)(wh +)))(when ((frame *simple-time)(day 26)(month 5))))"Will schedule for tile twenty-sixthof May"((frame *schedule)(when ((frame *simple-time)(day 26)(month 5))))Figure 1: Example Al ternat ive Hypotheses((f *schedule) (s who))((f *you))((f *schedule) (s what))((f *schedule)(s what)(f *what))((f *what))((f +))((f *what)(s wh)(f +))((f *schedule)(s who)(f *you))((f *schedule) (s what)(f *what)(s wh)(f +))Figure 2: Distinguishing Featuresused in the system to distinguish alternative mean-ing representation structures from one another spec-ify paths down this tree structure.
Thus, the distin-guishing features that are extracted are always an-chored in a frame or atomic filler, marked by an fin Figure 2.
Within a feature, a frame may be fol-lowed by a slot, marked by an s. And a slot maybe followed by a frame or atomic filler, and so on.These features are generated by comparing the setof feature structures returned from the HypothesisFormation phase.
No knowledge about what the fea-tures mean is needed in order to generate or usethese features.
Thus, the feature based approachis completely domain independent.
It can be usedwithout modification with any frame-based meaningrepresentation.1130When a feature is applied to a meaning repre-sentation structure, a value is obtained.
Thus, fea-tures can be used to assign meaning representationstructures to classes according to what value is ob-tained for each when the feature is applied.
Forexample, the feature ( ( f  *schedule)  (s who)(:f*you)),  distinguishes tructures that contain thefiller *you in the ~:ho slot in the *schedule framefrom those that do not.
When it is applied to struc-tures that contain the specified frame in the specifiedslot, it returns t rue .
When it is applied to structuresthat do not, it returns fa l se .
Thus, it groups thefirst and third hypotheses in one class, and the sec-ond hypothesis in another class.
Because the valueof a feature that ends in a frame or atomic fillercan have either t rue  or fa l se  as its value, these arecalled yes/no features.
When a feature that ends ina slot., such as ( ( f  *schedu le ) (s  who)), is appliedto a feature structure, the value is the filler in thespecified slot.
These features are called wh-features.Each feature is associated with a question thatthe system could ask the user.
The purpose of thegenerated question is to determine what the valueof the feature should be.
The system can then keepthose hypotheses that are consistent with that fea-ture value and eliminate from consideration the rest.Generating a natural language question from a fea-ture is discussed in section 2.3.2.2 Se lec t ing  a FeatureOnce a set of features is extracted, the system en-ters a loop in which it selects a feature from thelist, generates a query, and then updates the list ofalternative hypotheses and remaining distinguishingfeatures based on the user's response.
It attemptsto ask the most informative questions first in orderto limit the number of necessary questions.
It usesthe following four criteria in making its selection:?
Askable :  Is it possible to ask a natural ques-tion from it??
Eva luatab le :  Does it ask about a single repairor set of repairs that always occur together??
In  Focus: Does it involve information from thecommon ground??
Most  In fo rmat ive :  Is it likely to result in thegreatest search space reduction?First, the set of features is narrowed own to thosefeatures that represent askable questions.
For exam-ple, it is not natural to ask about the filler of a par-ticular slot in a particular frame if it is not knownwhether the ideal meaning representation structurecontains that frame.
Also, it is awkward to gen-erate a wh-question based on a feature of lengthgreater than two.
For example, a question corre-sponding to ( ( f  *how) (s what) ( f  * in terva l )  (send) ) might be phrased something like "How is thetime ending when?".
So even-lengthed features morethan two elements long are also eliminated at thisstage.The next criterion considered by the Interactionphase is evaluatability.
In order for a Yes/No ques-tion to be evaluatable, it must confirm only a singlerepair action.
Otherwise, if the user responds with"No" it cannot be determined whether the user isrejecting both repair actions or only one of them.Next, the set of features is narrowed own to thosethat can easily be identified as being in focus.
In or-der to do this, the system prefers to use featuresthat overlap with structures that all of the alter-native hypotheses have in common.
Thus, the sys-tem encodes as much COlmnon ground knowledge ineach question as possible.
The structures that allof the alternative hypotheses hare are called non-controversial substructures.
As the negotiation con-tinues, these tend to be structures that have beenconfirmed through interaction.
Including these sub-structures has the effect of having questions tendto follow in a natural succession.
It also has theother desirable effect that the system's tate of un-derstanding the speaker's entence is indicated tothe speaker.The final piece of information used in selectingbetween those remaining features is the expectedsearch reduction.
The expected search reduction in-dicates how much the search space can be expectedto be reduced once the answer to the correspondingquestion is obtained from the user.
Equation 1 isfor calculating S/,  the expected search reduction offeature number f .n ls,  = ?
(L - t , , , l  (iii=IL is the number of alternative hypotheses.
Asmention above, each feature can be used to assignthe hypotheses to equivalence lasses, l{,!
is thenumber of alternative hypotheses in the /th equiv-alence class of feature f. If the value for feature fassociated with the class of length l{,\] is the correctvalue, l{, 1 will be the new size of the search space.In this case, the actual search reduction will be thecurrent number of hypotheses, L, minus the numberof alternative hypotheses in the resulting set, l{,I.Intuitively, the expected search reduction of a fea-ture is the sum over all of a feature's equivalenceclasses of the percentage of hypotheses in that classtimes the reduction i  the search space assuming theassociated value for that feature is correct.The first three criteria select a subset of the cur-rent distinguishing features which the final crite-rion then ranks.
Note that all of these criteria canbe evaluated without he system having any under-standing about what the features actually mean.1131Selected Feature:((f *schedule)(s what)(f *what)(s wh)(f +))Non-controversial Structures if Answerto Question is Yes:((when ((mouth 5)(day 26)(frame *simple-time)))(frame *schedule)(what ((wh +)(frame *what))))Question Structure:((when ((month 5)(day 26)(frame *simple-time)))(frame *schedule)(what ((wh +)(frame *what))))Question Text:Was something like WHAT WILL BESCHEDULED FOR THE TWENTY-SIXTHOF MAY part of what you meant?Figure 3: Query Text Generat ion2.3 Generat ing Query TextThe selected feature is used to generate a query forthe user.
First, a skeleton structure is built fromthe feature, with top level frame equivalent to theframe at the root of the feature.
Then the skeletonstructure isfilled out with the non-controversial sub-structures.
If the question is a Yes/No question, itincludes all of the substructures that would be non-controversial ssuming the answer to the questionis "Yes".
Since information confirmed by the pre-vious question is now considered non-controversial,the result of the previous interaction ismade evidentin how the current question is phrased.
An exam-ple of a question generated with this process can befound in Figure 3.If the selected feature is a wh-feature, i.e., if it isan even lengthed feature, the question is generatedin the form of a wh-question.
Otherwise the textis generated eclaratively and the generated text isinserted into the following formula: "Was somethinglike XXX part of what you meant?
", where XXX isfilled in with the generated text.
The set of alter-native answers based on the set of alternative hy-potheses i presented to the user.
For wh-questions,a final alternative, "None of these alternatives areacceptable", is made available.
Again, no particu-lar domain knowledge is necessary for the purposeof generating query text from features ince the sen-tence level generation component from the systemcan be used as is.2.4 Processing the User's ResponseOnce the user has responded with the correct valuefor the feature, only the alternative hypotheses thathave that value for that feature are kept, and the rest"What will be scheduled for the twenty-sixth of May"((what ((frame *what)(wh +)))(when ((frame *simple-time)(day 26)(month 5)))(frame *schedule))))"You will schedule what for the twenty-sixth of May?
"((what ((frame *what)(wh +)))(frame *schedule)(when ((frame *simple-time)(day 26)(month 5)))(who ((frame *you))))Figure 4: Remaining Hypotheses((f *schedule)(s who))((f *you))((f *schedule) (s who)(f *you))Figure 5: Remaining Distinguishing Featuresare eliminated.
In the case of a wh-question, if theuser selects "None of these alternatives are accept-able", all of the alternative hypothesized structuresare eliminated and a rephrase is requested.
Afterthis step, all of the features that no longer parti-tion the search space into equivalence lasses are alsoeliminated.
In the example, assume the answer tothe generated question in Figure 3 was "Yes".
Thus,the result is that two of the original three hypothe-ses are remaining, displayed in Figure 4, and the re-maining set of features that still partition the searchspace can be found in Figure 5.If one or more distinguishing features remain, thecycle begins again by selecting a feature, generatinga question, and so on until the system narrows downto the final result.
If the user does not answer posi-tively to any of the system's questions by the time itruns out of distinguishing features regarding a par-ticular sentence, the system loses confidence in itsset of hypotheses and requests a rephrase.3 Us ing  D iscourse  In fo rmat ionThough discourse processing is not essential tothe ROSE approach, discourse information hasbeen found to be useful in robust interpretation(Ramshaw, 1994; Smith, 1992).
In this section wediscuss how discourse information can be used forfocusing the interaction between system and user onthe task level rather than oil the literal meaning ofthe user's utterance.A plan-based iscourse processor (Ros6 et al,1995) provides contextual expectations that guidethe system in the manner in which it formulates1132Sentence: What about any time but the ten to twelveslot on Tuesday the thirtieth?Hypothes is  1:"How about from ten o'clock till twelve o'clockTuesday the thirtieth any time"((frame *how)(when (*multiple*((end ((frame *simple-time) (hour 12)))(start ((frame *simple-time) (hour 10)))(incl-excl inclusive)(frame *interval))((frame *simple-time)(day 30)(day-of-week tuesday))((specifier any) (name time)(frame *special-time)))))Hypothesis 2:"From ten o'clock till Tuesday the thirtiethtwelve o'clock"( (frame *interval)(incl-excl inclusive)(start ((frame *simple-time) (hour 10)))(end ("multiple*((frame *simple-time)(day 30)(day-of-week tuesday))((frame *simple-time)(hour 12)))))Selected Feature: ((f *how)(s when)(f *interval))Query Without  discourse: Was something like"how about from ten o'clock till twelve 'clock" partof what you meant?Query With discourse: Are you suggesting thatTuesday November the thirtieth from ten a.m. tilltwelve a.m. is a good time to meet?Figure 6: Mod i f ied  Quest ion  Generat ionqueries to the user.
By computing a structure forthe dialogue, the discourse processor is able to iden-tify the speech act performed by each sentence.
Ad-ditionally, it augments temporal expressions fromcontext.
Based on this information, it computesthe constraints on the speaker's chedule xpressedby each sentence.
Each constraint associates a sta-tus with a particular speaker's chedule for timeslots within the time indicated by the temporalexpression.
There are seven possible statuses, in-eluding accepted,  suggested,  p re fer red ,  neut ra l ,d i spre fer red ,  busy, and rejected.As discussed above, the Interaction Mechanismuses features that distinguish between alternativehypotheses to divide the set of alternative repair hy-potheses into classes.
Each member within the sameclass has the same value for the associated feature.By comparing computed status and augmented tem-poral information for alternative repair hypotheseswithin the same class, it is possible to determinewhat common implications for the task each memberor most of the members in the associated class have.Thus, it is possible to compute what implicationsfor the task are associated with the correspondingvalue for the feature.
By comparing this common in-formation across classes, it is possible to determinewhether the feature makes a consistent distinctionon the task level.
If so, it is possible to take thisdistinguishing information and use it for refocusingthe associated question on the task level rather thanon the level of the sentence's literal meaning.In the example in Figure 6, the parser is not ableto correctly process the "but", causing it to missthe fact that the speaker intended any other timebesides ten to twelve rather than particularly ten totwelve.
Two alternative hypotheses are constructedduring the Hypothesis Formation phase.
However,neither hypothesis correctly represents the meaningof the sentence.
In this case, the purpose of theinteraction is to indicate to the system that neitherof the hypotheses are correct and that a rephrase isneeded.
This will be accomplished when the useranswers negatively to the system's query since theuser will not have responded positively to any of thesystem's queries regarding this sentence.The system selects the feature ( ( f  *how)(swhen) ( f  * in terva l ) )  to distinguish the two hy-potheses from one another.
Its generated query isthus "Was something like HOW ABOUT FROMTEN OCLOCK TILL TWELVE OCLOCK part ofwhat you meant?".
The discourse processor eturnsa different result for each of these two representa-tions.
In particular, only the first hypothesis con-tains enough information for the discourse proces-sor to compute any scheduling constraints ince itcontains both a temporal expression and a top levelsemantic frame.
It would create a constraint associ-ating the status of suggested with a representationfor Tuesday the thirtieth from ten o'clock till twelveo'clock.
The other hypothesis contains date infor-mation but no status information.
Based on thisdifference, the system can generate a query askingwhether or not the user expressed this constraint.
Itsquery is "Are you suggesting that Tuesday, Novem-ber the thirtieth from ten a .m.
till twelve a.m. is agood time to meet?"
The suggested status is as-sociated with a template that looks like "Are yousuggesting that XXX is a good time to meet?"
TheXXX is then filled in with the text generated fromthe temporal expression using the regular systemgeneration grammar.4 EvaluationAn empirical evaluation was conducted in or-der to determine how much improvement can begained with limited amounts of interaction in the1133ParserTop Hypothesis1 Question2 Questions3 QuestionsBad85.0%64.0%61.39%59.41%53.47%Okay12.0%28.0%28.71%28.71%32.67%Perfect3.0%8.0%9.9%11.88%13.86%Total Acceptable15.0%36.0%38,6140.59%46.53%Figure 7: T rans la t ion  Qua l i ty  As Max imum Number  of  Quest ions  Increasesdomain independent ROSE approach.
This evalu-ation is an end-to-end evaluation where a sentenceexpressed in the source language is parsed into alanguage independent meaning representation usingthe ROSE approach.
This meaning representationis then mapped onto a sentence in the target lan-guage.
In this case both the source language andthe target language are English.
An additional eval-uation demonstrates the improvement in interactionquality that can be gained by introducing availabledomain information.4.1 Domain  I ndependent  Repa i rFirst the system automatically selected 100 sen-tences from a previously unseen corpus of 500 sen-tences.
These 100 sentences are the first 100 sen-tences in the set that a parse quality heuristic sim-ilar to that described in (Lavie, 1995) indicated tobe of low quality.
The parse quality heuristic evalu-ates how much skipping was necessary in the parserin order to arrive at a partial parse and how wellthe parser's analysis scores statistically.
It shouldbe kept in mind, then, that this testing corpus iscomposed of 100 of the most difficult sentences fromthe original corpus.The goal of the evaluation was to compute aver-age performance per question asked and to comparethis with the performance with using only the partialparser as well as with using only the Hypothesis For-mation phase.
In each case performance was mea-sured in terms of a translation quality score assignedby an independent human judge to the generatednatural language target text.
Scores of Bad, Okay,and Perfect were assigned.
A score of Bad indicatesthat the translation does not capture the originalmeaning of the input sentence.
Okay indicates thatthe translation captures the meaning of the inputsentence, but not in a completely natural manner.Perfect indicates both that the resulting translationcaptures the meaning of the original sentence andthat it does so in a smooth and fluent manner.Eight native speakers of English who had neverpreviously used the translation system participatedin this evaluation to interact with the system.
Foreach sentence, the participants were presented withthe original sentence and with three or fewer ques-tions to answer.
The parse result, the result of re-pair without interaction, and the result for each userafter each question were recorded in order to begraded later by the independent judge mentionedabove.
Note that this evaluation was conducted onthe nosiest portion of the corpus, not on an aver-age set of naturally occurring utterances.
While thisevaluation indicates that repair without interactionyields an acceptable result in only 36% of these dif-ficult cases, in an evaluation over the entire corpus,it was determined to return an acceptable result in78% of the cases.A global parameter was set such that the systemnever asked more than a maximum of three ques-tions.
This limitation was placed on the system inorder to keep the task from becoming too tediousand time consuming for the users.
It was estimatedthat three questions was approximately the maxi-mum number of questions that users would be will-ing to answer per sentence.The results are presented in Figure 7.
Repairwithout interaction achieves a 25% reduction in er-ror rate.
Since the partial parser only produced suf-ficient chunks for building an acceptable repair lay-pothesis in about.
26% of the cases where it did notproduce an acceptable hypothesis by itself, the max-inmm reduction in error rate was 26%.
Thus, a 25%reduction in error rate without interaction is a verypositive result.
Additionally, interaction increasesthe system's average translation quality above thatof repair without interaction.
With three questions,the system achieves a 37% reduction in error rateover partial parsing alone.4.2 D iscourse  Based  I n teract ionIn a final evaluation, the quality of questions basedonly on feature information was compared with thatof questions focused on the task level using discourseinformation.
The discourse processor was only ableto provide sufficient information for reformulating22% of the questions in terms of the task.
The rea-son is that this discourse processor only provides in-formation for reformulating questions distinguishingbetween meaning representations that differ in termsof status and augmented temporal information.Four independent human judges were asked tograde pairs of questions, assigning a score between1 and 5 for relevance and form and indicating whichquestion they would prefer to answer.
They wereinstructed to think of relevance in terms of how use-1134ful they expected the question would be in helping acomputer understand the sentence the question wasintended to clarify.
For form, they were instructedto evaluate how natural and smooth sounding thegenerated question was.Interaction without discourse received on average2.7 for form and 2.4 for relevance.
Interaction withdiscourse, on the other hand, received 4.1 for formand 3.7 for relevance.
Subjects preferred the dis-course influenced question in 73.6% of the cases, ex-pressed no preference in 14.8% of the cases, and pre-ferred interaction without discourse in 11.6% of thecases.
Though the discourse influenced question wasnot preferred universely, this evaluation supports theclaim that humans prefer to receive clarifications onthe task level and indicates that further explorationin using discourse information i  repair, and partic-ularly in interaction, is a promising avenue for futureresearch.5 Conc lus ions  and  Cur rentD i rec t ionsThis paper presents a domain independent, interac-tive approach to robust interpretation.
Where otherinteractive approaches to robust interpretation havedepended upon domain dependent repair rules, theapproach described here operates efficiently withoutany such hand-coded repair knowledge.
An empir-ical evaluation demonstrates that limited amountsof focused interaction allow this repair approach toachieve a 37% reduction in error rate over a corpus ofnoisy sentences.
A further evaluation demonstratesthat.
this domain independent approach combineseasily with available domain knowledge in order toimprove the quality of the interaction.
Introducingdiscourse information yields a preferable query in74% of the cases where discourse information ap-plies.
Interaction in the current ROSE approachis limited to confirming hypotheses about how thefragments of the partial parse can be combined andrequesting rephrases.
It would be interesting to gen-erate and test hypotheses about information missingfrom the partial parse, perhaps using informationpredicted by the discourse context.ReferencesH.
H. Clark and E. F. Schaefer.
1989.
Contributingto discourse.
Cognitive Science, 13:259-294.H.
H. Clark and D. Wilkes-Gibbs.
1986.
Referringas a collaborative process.
Cognition, 22:1-39.M.
Danieli and E. Gerbino.
1995.
Metrics for evalu-ating dialogue strategies in a spoken language sys-tem.
In Working Notes of the AAAI Spring Sym-posium on Empirical Methods in Discourse Inter-pretation and Generation.E.
Hatch.
1983.
Simplified input and secondlanguage acquisition.
In R. Andersen, editor,Pidginization and Creolization as Language Ac-quisition.
Newbury House Publishers.D.
R. Hipp.
1992.
Design and Development ofSpoken Natural-Language Dialog Parsing Systems.Ph.D.
thesis, Dept.
of Computer Science, DukeUniversity.J.
Koza.
1992.
Genetic Programming: On the Pro-gramming of Computers by Means of Natural Se-lection.
MIT Press.J.
Koza.
1994.
Genetic Programming H. MIT Press.A.
Lavie, D. Gates, M. Gavalda, L. Mayfield, andA.
Waibel L. Levin.
1996.
Multi-lingual transla-tion of spontaneously spoken language in a limiteddomain.
In Proceedings of COLING 96, Kopen-hagen.A.
Lavie.
1995.
A Grammar Based Robust ParserFor Spontaneous Speech.
Ph.D. thesis, School ofComputer Science, Carnegie Mellon University.J.
F. Lehman.
1989.
Adaptive Parsing: Self-Extending Natural Language Interfaces.
Ph.D.thesis, School of Computer Science, Carnegie Mel-lon University.L.
A. Ramshaw.
1994.
Correcting real-worldspelling errors using a model of the problem-solving context.
Computational Intelligence,10(2).C.
P. Ros~ and A. Lavie.
1997.
An efficient, distribu-tion of labor in a two stage robust interpretationprocess.
In Proceedings of the Second Conferenceon Empirical Methods in Natural Language Pro-cessing.C.
P. Ros6 and A. Waibel.
1994.
Recovering fromparser failures: A hybrid statistical/symbolic ap-proach.
In Proceedings of The Balancing Act:Combining Symbolic and Statistical Approaches toLanguage workshop at the 32nd Annual Meeting ofthe A CL.C.
P. Ros~, B.
Di Eugenio, L. S. Levin, and C. VanEss-Dykema.
1995.
Discourse processing of dia-logues with multiple threads.
In Proceedings ofthe A CL.C.
P. Ros~.
1997.
Robust Interactive Dialogue Inter-pretation.
Ph.D. thesis, School of Computer Sci-ence, Carnegie Mellon University.R.
Smith.
1992.
A Computational Model ofExpectation-Driven Mixed-Initiative Dialog Pro-cessing.
Ph.D. thesis, CS Dept., Duke University.M.
Tomita and E. H. Nyberg.
1988.
Generation kitand transformation kit version 3.2: User's man-ual.
Technical Report CMU-CMT-88-MEMO,Carnegie Mellon University, Pittsburgh, PA.G.
Van Noord.
1996.
Robust parsing with the head-corner parser.
In Proceedings of the Eight Euro-pean Summer School In Logic, Language and In-formation, Prague, Czech Republic.1135
