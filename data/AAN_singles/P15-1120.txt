Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1245?1252,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsCan Natural Language Processing Become Natural Language Coaching?Marti A. HearstUC BerkeleyBerkeley, CA 94720hearst@berkeley.eduAbstractHow we teach and learn is undergoing arevolution, due to changes in technologyand connectivity.
Education may be oneof the best application areas for advancedNLP techniques, and NLP researchershave much to contribute to this problem,especially in the areas of learning to write,mastery learning, and peer learning.
Inthis paper I consider what happens whenwe convert natural language processorsinto natural language coaches.1 Why Should You Care, NLPResearcher?There is a revolution in learning underway.
Stu-dents are taking Massive Open Online Courses aswell as online tutorials and paid online courses.Technology and connectivity makes it possible forstudents to learn from anywhere in the world, atany time, to fit their schedules.
And in today?sknowledge-based economy, going to school onlyin one?s early years is no longer enough; in futuremost people are going to need continuous, life-long education.Students are changing too ?
they expect tointeract with information and technology.
For-tunately, pedagogical research shows significantbenefits of active learning over passive methods.The modern view of teaching means students workactively in class, talk with peers, and are coachedmore than graded by their instructors.In this new world of education, there is a greatneed for NLP research to step in and help.
I hopein this paper to excite colleagues about the pos-sibilities and suggest a few new ways of lookingat them.
I do not attempt to cover the field oflanguage and learning comprehensively, nor do Iclaim there is no work in the field.
In fact thereis quite a bit, such as a recent special issue on lan-guage learning resources (Sharoff et al, 2014), thelong running ACL workshops on Building Edu-cational Applications using NLP (Tetreault et al,2015), and a recent shared task competition ongrammatical error detection for second languagelearners (Ng et al, 2014).
But I hope I am cast-ing a few interesting thoughts in this direction forthose colleagues who are not focused on this par-ticular topic.2 How AwkwardPerhaps the least useful feedback that an instructorwrites next to a block of prose on a learner?s essayis ?awkward?.
We know what this means: some-thing about this text does not read fluently.
But thisis not helpful feedback; if the student knew how tomake the wording flow, he or she would have writ-ten it fluently in the first place!
Useful feedback isactionable: it provides steps to take to make im-provements.A challenge for the field of NLP is how to buildwriting tutors or coaches ?
as opposed to gradersor scorers.
There is a vast difference between atool that performs an assessment of writing andone that coaches students to help them as they areattempting to write.Current practice uses the output of scorers togive students a target to aim for: revise your essayto get a higher score.
An alternative is to designa system that watches alongside a learner as theywrite an essay, and coaches their work at all levelsof construction ?
phrase level, clause level, sen-tence level, discourse level, paragraph level, andessay level.Grammar checking technology has been excel-lent for years now (Heidorn, 2000), but instead ofjust showing the right answer as grammar check-ers do, a grammar coach should give hints andscaffolding the way a tutor would ?
not giving theanswer explicitly, but showing the path and lettingthe learner fill in the missing information.
Whenthe learner makes incorrect choices, the parser1245can teach principles and lessons for the concep-tual stage that the learner is currently at.
Differentgrammars could be developed for learners at dif-ferent competency levels, as well as for differentfirst-second language pairings in the case of sec-ond language learning.This suggests a different approach for buildinga parser than what is the current standard.
I amnot claiming that this has not been suggested inthe past; for instance Schwind (1988) designed aparser to explain errors to learners.
However, be-cause of the renewed interest in technology forteaching, this may be a pivotal time to recon-sider how we develop parsing technology: perhapswe should think fundamentally about parsers ascoaches rather than parsers as critics.This inversion can apply to other aspects ofNLP technology as well.
For instance, Dale andKilgarriff (2011) have held a series of workshopto produce algorithms to identify errors introducedinto texts by non-native writers in the warmlynamed ?Helping Our Own?
shared task (Dale etal., 2012).
Using the technology developed fortasks like these, the challenge is to go beyond rec-ognizing and correcting the errors to helping thewriter understand why the choices they are makingare not correct.
Another option is to target practicequestions tailored for learners based on errors in afun manner (as described below).Of course, for decades, the field of IntelligentTutoring Systems (ITS) (VanLehn, 2011) has de-veloped technology for this purpose, so what isnew about what I am suggesting?
First, we knowas NLP researchers that language analysis requiresspecific technology beyond standard algorithms,and so advances in Intelligent Tutoring Systemson language problems most likely requires col-laboration with experts in NLP.
And, apparentlysuch collaborations have not been as robust as theymight be (Borin, 2002; Meurers, 2012).
So thereis an opportunity for new advances at the intersec-tion of these two fields.And second, the newly expanded interest in on-line learning and technology makes possible theaccess of information about student writing be-havior on a large scale that was not possible inthe past.
Imagine thousands of students in cas-caded waves, tasked with writing essays on thesame topic, and receiving real-time suggestionsfrom different algorithms.
The first wave of stu-dent responses to the feedback would be used toFigure 1: Wordcraft user interface showing a farmscene with four characters, a fully formed sen-tence, the word tray with candidate additionalwords colored by part of speech, and tool bar.When the child completes a sentence correctly, thecorresponding action is animated.improve the algorithms and these results would befed into the next wave of student work, and so on.Students and instructors could be encouraged togive feedback via the user interface.
Very rapidcycles of iteration should lead to accelerated im-provements in understanding of how the interfacesand the algorithms could be improved.
A revo-lution in understanding of how to coach studentwriting could result!Algorithms could be designed to give feedbackfor partially completed work: partially writtensentences in the case of a parser; partially com-pleted paragraphs in the case of a discourse writ-ing tool, and so on, rather than only assessingcompleted work after the fact.3 Karaoke Anyone?Beyond learning to write, new technology ischanging other aspects of language learning inways that should excite NLP researchers.
In or-der to write well, a student must have a good vo-cabulary and must know syntax.
Learning wordsand syntax requires exposure to language in many1246contexts, both spoken and written, for a student?sprimary language was well as for learning a sec-ond language.Although computerized vocabulary tools havebeen around for quite some time, the rise of mo-bile, connected applications, the serious gamesmovement, and the idea of ?microtasks?
whichare done during interstices of time while out andabout during the day, opens the door to new waysto expose students to repetitive learning tasks foracquiring language (Edge et al, 2011).
Some ofthe most innovative approaches for teaching lan-guage combine mobile apps with multimedia in-formation.For example, the Tip Tap Tones project (Edgeet al, 2012) attempts to help learners reduce thethe challenge of mastering a foreign phonetic sys-tem by microtasking with minute-long episodes ofmobile gaming.
This work focuses in particularon helping learners acquire the tonal sound systemof Mandarin Chinese and combines gesture swipeswith audio on a smartphone.The ToneWars app (Head et al, 2014) takesthis idea one step farther by linking second lan-guage learners with native speakers in real timeto play a Tretis-like game against one another tobetter learn Chinese pronunciation.
The secondlanguage learner feels especially motivated whenthey are able to beat the native speaker, and thenative speaker contributes their expert tone record-ings to the database, fine-tunes their understandingof their own language, and enjoys the benefits oftutoring others in a fun context.Going beyond phonemes, the DuoLingosecond-language learning application (von Ahn,2013) teaches syntax as well as vocabularythrough a game-based interface.
For instance,one of Duolingo?s games consists of a display ofa sentence in one language, and a jumbled listof words in the opposing language presented ascards to be dragged and dropped onto a tray in thecorrect order to form a sentence.
In some casesthe user must select between two confoundingchoices, such as the articles ?le?
or ?la?
to modifyFrench nouns.Our work on a game for children called Word-Craft takes this idea one step further (Anand et al,2015) (see Figure 1).
Children manipulate wordcards to build sentences which, when grammati-cally well formed, come to life in a storybook-likeanimated world to illustrate their meaning.
Pre-liminary studies of the use of Wordcraft found thatchildren between the ages of 4 and 8 were able toobserve how different sentence constructions re-sulted in different meanings and encouraged chil-dren to engage in metalinguistic discourse, espe-cially when playing the game with another child.A karaoke-style video simulation is used by theEngkoo system to teach English to Chinese speak-ers (Wang et al, 2012).
The interface not onlygenerates audio for the English words, but alsoshows the lip and facial shapes necessary for form-ing English words using a 3D simulated model lip-syncing the words in a highly realistic manner.
Togenerate a large number of sample sentences, thetext was drawn from bilingual sentence pairs fromthe web.These technologies have only become feasiblerecently because of the combination of multime-dia, fast audio and image processing, fast networkconnectivity, and a connected population.
NLP re-searchers may want to let their imaginations con-sider the possibilities that arise from this new andpotent combination.4 Closing the Cheese GapSalman Kahn, the creator of Kahn Academy, talksabout the ?Swiss cheese?
model of learning inwhich students learn something only partly beforethey are forced to move on to the next topic, build-ing knowledge on a foundation filled with holes,like the cheese of the same name (Khan, 2012).This is akin to learning to ride a bicycle withoutperfecting the balancing part.
In standard school-ing, students are made to move one from one les-son to the next even if they only got 70, 80, 90%correct on the test.
By contrast, mastery learn-ing requires a deep understanding, working withknowledge and probing it from every angle, try-ing out the ideas and applying them to solve realproblems.In many cases, mastery learning also requirespracticing with dozens, hundreds, or even thou-sands of different examples, and getting feedbackon those examples.
Automation can help withmastery learning by generating personalized prac-tice examples that challenge and interest students.Automatically generated examples also reduce thecost of creating new questions for instructors whoare concerned about answer sharing among stu-dents from previous runs of a course.Recently, sophisticated techniques developed in1247the programming languages field have begun to beapplied to automate repetitive and structured tasksin education, including problem generation, solu-tion generation, and feedback generation for com-puter science and logic topics (Gulwani, 2014).Closer to the subject at hand is the automatedgeneration of mathematical word problems thatare organized around themes of interest to kids,such as ?School of Wizardry?
(Polozov et al,2015).
The method allows the student to specifypersonal preferences about the world and charac-ters, and then creates mini ?plots?
for each wordproblem by enforcing coherence across the sen-tences using constraints in a logic programmingparadigm combined with hand-crafted discoursetropes (constraints on logical graphs) and a natu-ral language generation step.
A sample generatedword problem isProfessor Alice assigns Elliot to make aluck potion.
He had to spend 9 hoursfirst reading the recipe in the textbook.He spends several hours brewing 11 por-tions of it.
The potion has to be brewedfor 3 hours per portion.
How manyhours did Elliot spend in total?Results are close in terms of comprehensibilityand solubility to those of a textbook.
The project?sultimate goal is to have the word problems actu-ally tell a coherent story, but that challenge is stillan open one.
But the programs can generate aninfinite number of problems with solutions.
Otherwork by the same research team generated person-alized algebraic equation problems in a game en-vironment and showed that students could achievemastery learning in 90 minutes or less during anorganized educational campaign (Liu et al, 2015).Another way that NLP can help with masterylearning is to aid instructors in the providing offeedback on short answer test questions.
Therehas been significant work in this space (Kukich,2000; Hirschman et al, 2000).
The standard ap-proach builds on the classic successful model ofessay scoring which compares the student?s text tomodel essays using a similarity-based techniquesuch as LSA (Landauer et al, 2000; Mohler andMihalcea, 2009) or careful authoring of the answer(Leacock and Chodorow, 2003).Recent techniques pair with learning techniqueslike Inductive Logic Programming with instructorediting to induce logic rules that describe permis-sible answers with high accuracy (Willis, 2015).Unfortunately most approaches require quite alarge number of students?
answers to be markedup manually by the instructor before the feedbackis accurate enough to be reliably used for a givenquestion; a recent study found on the order of 500-800 items per question had to be marked up atminimum in order to obtain acceptable correla-tions with human scorers (Heilman and Madnani,2015).
This high initial cost makes the develop-ment of hundreds of practice questions for a givenconceptual unit a daunting task for instructors.Recent research in Learning at Scale has pro-duced some interesting approaches to improving?feedback at scale.?
One approach (Brooks et al,2014) uses a variation on hierarchical text cluster-ing in tandem with a custom user interface that al-lows instructors to rapidly view clusters and deter-mine which contain correct answers, incorrect an-swers, and partially correct answers.
This greatlyspeeds up the markup time and allows instructorsto assign explanations to a large group of answerswith a click of a button.An entirely different approach to providingfeedback that is becoming heavily used in MassiveOpen Online Courses is peer feedback, in whichstudents assign grades or give feedback to otherstudents on their work (Hicks et al, 2015).
Re-searchers have studied how to refine the processof peer feedback to train students to produce re-views that come within a grade point of that of in-structors, with the aid of carefully designed rubrics(Kulkarni et al, 2013).However, to ensure accurate feedback, severalpeer assessments per assignment are needed in ad-dition to a training exercise, and students some-times complain about workload.
To reduce the ef-fort, Kulkarni et al (2014) experimented with aworkflow that uses machine grading as a first step.After training a machine learning algorithm for agiven assignment, assignments are scored by thealgorithm.
The less confident the algorithm is inits score, the more students are assigned to gradethe assignment, but high-confidence assignmentsmay need only one peer grader.
This step wasfound to successfully reduce the amount of feed-back needed to be done with a moderate decreasein grading performance.
That said, the algorithmdid require the instructors to mark up 500 sam-ple assignments, and there is room for improve-ment in the algorithm in other ways, since onlya first pass at NLP techniques was used to date.1248Nonetheless, mixing machine and peer grading isa promising technique to explore, as it has beenfound to be useful in other contexts (Nguyen andLitman, 2014; Kukich, 2000).5 Are You a FakeBot?Why is the completion rate of MOOCs so low?This question vexes proponents and opponents ofMOOCs alike.
Counting the window shopping en-rollees of a MOOC who do not complete a courseis akin to counting everyone who visits a col-lege campus as a failed graduate of that univer-sity; many people are just checking the course out(Jordan, 2014).
That said, although the anytime,anywhere aspect of online courses works well formany busy professionals who are self-directed, re-search shows that most people need to learn in anenvironment that includes interacting with otherpeople.Learning with others can refer to instructors andtutors, and online tutoring systems have had suc-cess comparable to that of human tutors in somecases (VanLehn, 2011; Aleven et al, 2004).
Butanother important component of learning with oth-ers refers to learning with other students.
Lit-erally hundreds of research papers show that aneffective way to help students learn is to havethem talk together in small groups, called struc-tured peer learning, collaborative learning, or co-operative learning (Johnson et al, 1991; Lord,1998).
In the classroom, this consists of activitiesin which students confer in small groups to discussconceptual questions and to engage in problem-solving.
Studies and meta-analyses show the sig-nificant pedagogical benefit of peer learning in-cluding improved critical thinking skills, retentionof learned information, interest in subject matter,and class morale (Hake, 1998; Millis and Cottell,1998; Springer et al, 1999; Smith et al, 2009;Deslauriers et al, 2011).
Even studies of intelli-gent tutoring systems find it hard to do better thanjust having students discuss homework problemsin a structured setting online (Kumar et al, 2007).The reasons for the success of peer learning in-clude: students are at similar levels of understand-ing that experts can no longer relate to well, peoplelearn material better when they have to explain itto others, and identify the gaps in their current un-derstanding, and the techniques of structured peerlearning introduce activities and incentives to helpstudents help one another.S2 I think E is the right answerS1 Hi, I think E is right, tooS3 Hi!
This seems to be a nurture vs naturequestion.S3 Can scent be learned, or only at birth?S2 Yeah, but answer A supports the author?sconclusionS1 I felt that about A tooS2 But the question was, which statementwould weaken the author?s conclusionS3 So I choose A, showing that scent can belearned at not only AT BIRTH.S2 That?s why I think E is rightS3 Are you real, or fake?S2 realS1 I didn?t think that b or d had anything todo with the statementS3 Actually what you said makes sense.S1 So, do we all agree that E was the correctanswer?S2 I think so, yes.S3 But I?m sticking with A since ?no otherwater could stimulate olfactory sites?
abdI suggests that other water could be de-tected.S3 *andS1 I thought about c for awhile but it didn?treally seem to have anything to do withthe topic of scentS3 It has to be A or E. Other ones don?t haveanything do do with the question.S2 but that ?no other water?
thing appliesequally well to ES3 E is still about spawing ground water, Ithink.
this is a confusing question.S1 I thought E contradicted the statement themostS2 me tooS3 I loving hits with other mturkersTable 1: Transcript of a conversation among threecrowdworkers who discussed the options for amultiple choice question for a GMAT logical rea-soning task.
Note the meta-discussion about theprevalence of robots on the crowdsourcing plat-form.1249In our MOOCChat research, we were inter-ested in bringing structured peer learning into theMOOC setting.
We first tried out the idea ona crowdsourcing platform (Coetzee et al, 2015),showing that when groups of 3 workers discussedchallenging problems together, and especially ifthey were incentivized to help each other arriveat the correct answer, they achieved better resultsthan working alone.
(A sample conversation isshown in Table 1.)
We also found that provid-ing a mini-lesson in which workers consider theprinciples underlying the tested concept and jus-tify their answers leads to further improvements,and combining the mini-lesson with the discussionof the corresponding multiple-choice question in agroup of 3 leads to significant improvements onthat question.
Crowd workers also expressed pos-itive subjective responses to the peer interactions,suggesting that discussions can improve morale inremote work or learning settings.When we tested the synchronous small-groupdiscussions in a live MOOC we found that, forthose students that were successfully placed into agroup of 3 for discussion, they were quite positiveabout the experience (Lim et al, 2014).
However,there are significant challenges in getting studentsto coordinate synchronously in very large low-costcourses (Kotturi et al, 2015).There is much NLP research to be done to en-hance the online dialogues that are associated withstudent discussion text beyond the traditional roleof intelligent tutoring systems.
One idea is to mon-itor discussions in real time and try to shape theway the group works together (Tausczik and Pen-nebaker, 2013).
Another idea is to automaticallyassess if students are discussing content at appro-priate levels on Bloom?s taxonomy of educationalobjectives (Krathwohl, 2002).In our MOOCChat work with triad discussionswe observed that more workers will change theiranswer from an incorrect to a correct one if at leastone member of the group starts out correct thanif no one is correct initially (Hearst et al, 2015).We also noticed that if all group members startout with the same answer ?
right or wrong ?
noone is likely to change their answer in any direc-tion.
This behavior pattern suggests an interestingidea for large scale online group discussions thatare not feasible in in-person environments: dy-namically assign students to groups depending onwhat their initial answers to questions are, and dy-namically regroup students according to the mis-conceptions and correct conceptions they have.Rather than building an intelligent tutoring sys-tem to prompt students with just the right state-ment at just the right time, a more successful strat-egy might be to mix students with other poeplewho for that particular discussion point have thejust the right level of conceptual understanding tomove the group forward.6 ConclusionsIn this paper I am suggesting inverting the stan-dard mode of our field from that of processing,correcting, identifying, and generating aspects oflanguage to one of recognizing what a person isdoing with language: NLP algorithms as coachesrather than critics.
I have outlined a number ofspecific suggestions for research that are currentlyoutside the mainstream of NLP research but whichpose challenges that I think some of my colleagueswill find interesting.
Among these are text ana-lyzers that explain what is wrong with an essay atthe clause, sentence, discourse level as the studentwrites it, promoting mastery learning by generat-ing unlimited practice problems, with answers, ina form that makes practice fun, and using NLP toimprove the manner in which peers learning takesplace online.
The field of learning and educationis being disrupted, and NLP researchers should behelping push the frontiers.Acknowledgements I thank the ACL programchairs Michael Strube and Chengqing Zong forinviting me to write this paper and keynote talk,Lucy Vanderwende for suggested references, andcolleagues at NAACL 2015 for discussing theseideas with me.
This research is supported in partby a Google Social Interactions Grant.ReferencesVincent Aleven, Amy Ogan, Octav Popescu, CristenTorrey, and Kenneth Koedinger.
2004.
Evaluat-ing the effectiveness of a tutorial dialogue systemfor self-explanation.
In Intelligent tutoring systems,pages 443?454.
Springer.Divya Anand, Shreyas, Sonali Sharma, VictorStarostenko, Ashley DeSouza, Kimiko Ryokai, andMarti A. Hearst.
2015.
Wordcraft: Playing withSentence Structure.
Under review.Lars Borin.
2002.
What have you done for me lately?The fickle alignment of NLP and CALL.
Reportsfrom Uppsala Learning Lab.1250Michael Brooks, Sumit Basu, Charles Jacobs, and LucyVanderwende.
2014.
Divide and correct: Usingclusters to grade short answers at scale.
In Pro-ceedings of the first ACM conference on Learn-ing@Scale, pages 89?98.
ACM.D Coetzee, Seongtaek Lim, Armando Fox, Bjorn Hart-mann, and Marti A Hearst.
2015.
Structuring inter-actions for large-scale synchronous peer learning.
InProceedings of the 18th ACM Conference on Com-puter Supported Cooperative Work & Social Com-puting, pages 1139?1152.
ACM.Robert Dale and Adam Kilgarriff.
2011.
Helping ourown: The HOO 2011 pilot shared task.
In Proceed-ings of the 13th European Workshop on Natural Lan-guage Generation, pages 242?249.
Association forComputational Linguistics.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A report on the preposition anddeterminer error correction shared task.
In Proceed-ings of the Seventh Workshop on Building Educa-tional Applications Using NLP, pages 54?62.
Asso-ciation for Computational Linguistics.Louis Deslauriers, Ellen Schelew, and Carl Wieman.2011.
Improved learning in a large-enrollmentphysics class.
Science, 332(6031):862?864.Darren Edge, Elly Searle, Kevin Chiu, Jing Zhao, andJames A Landay.
2011.
Micromandarin: mobilelanguage learning in context.
In Proceedings of theSIGCHI Conference on Human Factors in Comput-ing Systems, pages 3169?3178.
ACM.Darren Edge, Kai-Yin Cheng, Michael Whitney, YaoQian, Zhijie Yan, and Frank Soong.
2012.
Tip taptones: mobile microtraining of mandarin sounds.
InProceedings of the 14th International Conference onHuman-Computer Interaction with Mobile Devicesand Services, pages 427?430.
ACM.Sumit Gulwani.
2014.
Example-based learning incomputer-aided stem education.
Communications ofthe ACM, 57(8):70?80.Richard R Hake.
1998.
Interactive-engagement ver-sus traditional methods: A six-thousand-student sur-vey of mechanics test data for introductory physicscourses.
American Journal of Physics, 66(1):64?74.Andrew Head, Yi Xu, and Jingtao Wang.
2014.Tonewars: Connecting language learners and nativespeakers through collaborative mobile games.
In In-telligent Tutoring Systems, pages 368?377.
Springer.Marti A Hearst, Armando Fox, D Coetzee, and Bjo-ern Hartmann.
2015.
All it takes is one: Evidencefor a strategy for seeding large scale peer learninginteractions.
In Proceedings of the Second (2015)ACM Conference on Learning@Scale, pages 381?383.
ACM.George Heidorn.
2000.
Intelligent writing assistance.Handbook of Natural Language Processing, pages181?207.Michael Heilman and Nitin Madnani.
2015.
The im-pact of training data on automated short answer scor-ing performance.
In Proceedings of the Tenth Work-shop on Building Educational Applications UsingNLP.
Association for Computational Linguistics.Catherine M Hicks, C Ailie Fraser, Purvi Desai, andScott Klemmer.
2015.
Do numeric ratings im-pact peer reviewers?
In Proceedings of the Second(2015) ACM Conference on Learning@ Scale, pages359?362.
ACM.Lynette Hirschman, Eric Breck, Marc Light, John DBurger, and Lisa Ferro.
2000.
Automated gradingof short-answer tests.
Intelligent Systems and theirApplications, IEEE, 15(5):22?37.David W Johnson, Roger T Johnson, and Karl AldrichSmith.
1991.
Active learning: Cooperation in thecollege classroom.
Interaction Book Company Ed-ina, MN.Katy Jordan.
2014.
Initial trends in enrolment andcompletion of massive open online courses.
The In-ternational Review Of Research In Open And Dis-tributed Learning, 15(1).Salman Khan.
2012.
The One World Schoolhouse: Ed-ucation Reimagined.
Twelve.Yasmine Kotturi, Chinmay Kulkarni, Michael S Bern-stein, and Scott Klemmer.
2015.
Structure and mes-saging techniques for online peer learning systemsthat increase stickiness.
In Proceedings of the Sec-ond (2015) ACM Conference on Learning@ Scale,pages 31?38.
ACM.David R Krathwohl.
2002.
A revision of Bloom?staxonomy: An overview.
Theory into practice,41(4):212?218.Karen Kukich.
2000.
Beyond automated essay scor-ing.
IEEE Intelligent Systems and their Applica-tions, IEEE, 15(5):22?27.Chinmay Kulkarni, Koh Pang Wei, Huy Le, DanielChia, Kathryn Papadopoulos, Justin Cheng, DaphneKoller, and Scott R Klemmer.
2013.
Peer andself assessment in massive online classes.
InACM Transactions on Computer Human Interaction(TOCHI), volume 20.
ACM.Chinmay E Kulkarni, Richard Socher, Michael S Bern-stein, and Scott R Klemmer.
2014.
Scaling short-answer grading by combining peer assessment withalgorithmic scoring.
In Proceedings of the firstACM conference on Learning@Scale, pages 99?108.
ACM.Rohit Kumar, Carolyn Penstein Ros?e, Yi-Chia Wang,Mahesh Joshi, and Allen Robinson.
2007.
Tutorialdialogue as adaptive collaborative learning support.Frontiers in Artificial Intelligence and Applications,158:383.1251Thomas K Landauer, Darrell Laham, and Peter WFoltz.
2000.
The intelligent essay assessor.
IEEEIntelligent Systems and their Applications, IEEE,15(5):22?27.Claudia Leacock and Martin Chodorow.
2003.
C-rater:Automated scoring of short-answer questions.
Com-puters and the Humanities, 37(4):389?405.Seongtaek Lim, Derrick Coetzee, Bjoern Hartmann,Armando Fox, and Marti A Hearst.
2014.
Initial ex-periences with small group discussions in moocs.
InProceedings of the first ACM conference on Learn-ing@Scale, pages 151?152.
ACM.Yun-En Liu, Christy Ballweber, Eleanor O?rourke,Eric Butler, Phonraphee Thummaphan, and ZoranPopovi?c.
2015.
Large-scale educational campaigns.ACM Transactions on Computer-Human Interaction(TOCHI), 22(2):8.Thomas Lord.
1998.
Cooperative learning that re-ally works in biology teaching: using constructivist-based activities to challenge student teams.
TheAmerican Biology Teacher, 60(8):580?588.Detmar Meurers.
2012.
Natural language processingand language learning.
In The Encyclopedia of Ap-plied Linguistics.
Wiley Online Library.Barbara J Millis and Philip G Cottell.
1998.
Coop-erative learning for higher education faculty.
OryxPress (Phoenix, Ariz.).Michael Mohler and Rada Mihalcea.
2009.
Text-to-text semantic similarity for automatic short answergrading.
In Proceedings of the 12th Conference ofthe European Chapter of the Association for Com-putational Linguistics, pages 567?575.
Associationfor Computational Linguistics.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Chris-tian Hadiwinoto, and Joel Tetreault.
2014.
TheCoNLL-2013 shared task on grammatical error cor-rection.
In Proceedings of the Eighteenth Confer-ence on Computational Natural Language Learn-ing: Shared Task (CoNLL-2014 Shared Task), pages1?12.Huy V Nguyen and Diane J Litman.
2014.
Improvingpeer feedback prediction: The sentence level is right.ACL 2014, page 99.Oleksandr Polozov, Eleanor ORourke, Adam M Smith,Luke Zettlemoyer, Sumit Gulwani, and ZoranPopovic.
2015.
Personalized mathematical wordproblem generation.
In Proceedings of the 24thInternational Joint Conference on Artificial Intelli-gence (IJCAI 2015).
To appear.Camilla Schwind.
1988.
Sensitive parsing: error anal-ysis and explanation in an intelligent language tutor-ing system.
In Proceedings of the 12th conferenceon Computational Linguistics-Volume 2, pages 608?613.
Association for Computational Linguistics.Serge Sharoff, Stefania Spina, and Sofie JohanssonKokkinakis.
2014.
Introduction to the special issueon resources and tools for language learners.
Lan-guage Resources and Evaluation, 48(1):1?3.Michelle K Smith, William B Wood, Wendy K Adams,Carl Wieman, Jennifer K Knight, Nancy Guild, andTin Tin Su.
2009.
Why peer discussion improvesstudent performance on in-class concept questions.Science, 323(5910):122?124.Leonard Springer, Mary Elizabeth Stanne, andSamuel S Donovan.
1999.
Effects of small-grouplearning on undergraduates in science, mathematics,engineering, and technology: A meta-analysis.
Re-view of educational research, 69(1):21?51.Yla R Tausczik and James W Pennebaker.
2013.
Im-proving teamwork using real-time language feed-back.
In Proceedings of the SIGCHI Conference onHuman Factors in Computing Systems, pages 459?468.
ACM.Joel Tetreault, Jill Burstein, and Claudia Leacock.2015.
Proceedings of the Tenth Workshop on Build-ing Educational Applications Using NLP.
Associa-tion for Computational Linguistics.Kurt VanLehn.
2011.
The relative effectiveness ofhuman tutoring, intelligent tutoring systems, andother tutoring systems.
Educational Psychologist,46(4):197?221.Luis von Ahn.
2013.
Duolingo: learn a language forfree while helping to translate the web.
In Proceed-ings of the 2013 International Conference on Intel-ligent User Interfaces, pages 1?2.
ACM.Lijuan Wang, Yao Qian, Matthew R Scott, Gang Chen,and Frank K Soong.
2012.
Computer-assistedaudiovisual language learning (with online video).Computer, 45(6):38?47.Alistair Willis.
2015.
Using nlp to support scalableassessment of short free text responses.
In Proceed-ings of the Tenth Workshop on Building EducationalApplications Using NLP.
Association for Computa-tional Linguistics.1252
