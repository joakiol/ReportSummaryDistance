NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 39?46,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsUsing Senses in HMM Word AlignmentDouwe Gelling and Trevor CohnDepartment of Computer ScienceUniversity of Sheffield, UK{d.gelling,t.cohn}@sheffield.ac.ukAbstractSome of the most used models for statis-tical word alignment are the IBM models.Although these models generate acceptablealignments, they do not exploit the rich in-formation found in lexical resources, and assuch have no reasonable means to choose bet-ter translations for specific senses.We try to address this issue by extending theIBM HMM model with an extra hidden layerwhich represents the senses a word can take,allowing similar words to share similar outputdistributions.
We test a preliminary version ofthis model on English-French data.
We com-pare different ways of generating senses andassess the quality of the alignments relative tothe IBM HMM model, as well as the gener-ated sense probabilities, in order to gauge theusefulness in Word Sense Disambiguation.1 IntroductionModern machine translation is dominated by statis-tical methods, most of which are trained on word-aligned parallel corpora (Koehn et al, 2007; Koehn,2004), which need to be generated separately.
Oneof the most commonly used methods to generatethese word alignments is to use the IBM models 1-5,which generate one-directional alignments.Although the IBM models perform well, they failto take into account certain situations.
For exam-ple, if an alignment between two words f1 and e1 isconsidered, and f1 is an uncommon translation fore1, the translation probability will be low.
It mighthappen, that an alignment to a different nearby wordis preferred by the model.
Consider for examplethe situation where f1 is ?taal?
(Dutch, meaning lan-guage), and e1 is ?tongue?.
The translation probabil-ity for this may be low, as ?tongue?
usually translatesas ?tong?, meaning the body part.
In this case thepreference of the alignment model may dominate,leading to the wrong alignment.Moreover, the standard tools for word alignmentfail to make use of the lexical resources that alreadyexist, and which could contribute useful informationfor the task.
In particular, the ontology defined inWordNet (Miller, 1995) could be put to good use.Intuitively, the translation of a word should dependon the sense of the word being used.
The currentwork seeks to explore this idea, by explicitly mod-eling the senses in the translation process.
It doesso, by modifying the HMM alignment model to in-clude synsets as an intermediate stage of translation.This would facilitate sharing of translation distribu-tions between words with similar senses that shouldgenerate the correct sense.
In terms of the exampleabove, one of the senses for ?tongue?
will share thetranslation distribution with ?language?, for whichwe will have more relevant translation probabilities.As well as performing word alignment this modelcan be used to generate sense annotations on oneside of a parallel corpus, given an alignment, or evengenerate sense annotations while aligning a corpus.Thus, the model could learn to align a corpus anddo WSD at the same time.
In this paper, the effectthe usage of senses has on alignment is investigated,and the potential usefulness of the model for WSDis explored.
In the next section related work is dis-cussed, after which in section 3 the current model is39discussed.In section 4 the evaluation of the model is dis-cussed, in two parts.
In the first part, the model isevaluated for English-French on gold standard man-ually aligned data and compared to the results of thebase HMM model.
In the second part, the model isqualitatively evaluated by inspecting the senses andassociated output distributions of selected words.2 Previous WorkAlthough most researchers agree that Word SenseDisambiguation (WSD) is a useful field, it hasn?tbeen shown to consistently help in related tasks.
Ma-chine Translation is no exception, and whether ornot WSD systems can improve performance of MTsystems is debated.
Furthermore, it is unclear howparallel corpuses can be exploited for WSD systems.In this section we will present a brief overview of re-lated work.
(Carpuat and Wu, 2007) report an improvementin translation quality by incorporating a WSD sys-tem directly in a phrase-based translation system.This is in response to earlier work done, where in-corporating the output of a traditional WSD systemgave disappointing results (Carpuat and Wu, 2005).The WSD task is redefined, to be similar to choosingthe correct phrasal translation for a word, instead ofchoosing a sense from a sense inventory.
This sys-tem is trained on the same data as the SMT systemis.The output of this model is incorporated into themachine translation system by providing the WSDprobabilities for a phrase translation as extra featuresin a log-linear model (Carpuat and Wu, 2007).
Thissystem consistently outperforms the baseline system(the same system, but without WSD component), onmultiple metrics, which seems to indicate that WSDcan make a useful contribution to machine transla-tion.
However, the way the system is set up, it couldalso be viewed as a way of incorporating translationprobabilities of other systems into the phrase-basedtranslation model.
(Chan and Ng, 2007) introduce a system very sim-ilar to that of (Carpuat and Wu, 2007), but as ap-plied to hierarchical phrase-based translation.
Theydemonstrate modest improvements in BLEU scoreover the unmodified system, as well as some qualita-tive improvements in the output.
Here again, the ar-gument could be made that what is being done is notstrictly word sense disambiguation, but augmentingthe translation system with extra features for someof the phrase translations.In (Tufis?
et al, 2004) parallel corpora and alignedWordNets are exploited for WSD.
This is done, byword aligning the parallel texts, and then for ev-ery aligned pair, generating a set of wordnet sensecodes (ILI codes, or interlingual index codes) for ei-ther word, corresponding to the possible senses thatword can take.
As the wordnets for both languagesare linked, if the ILI code of a sense is the same, thesense should be sufficiently similar.
Thus, the in-tersection of both sets of ILI is taken to find an ILIcode that is common to both pairs.
If such a code isfound, it represents the sense index of both words.Otherwise, the closest ILI code to the two most sim-ilar ILI codes is found, and that is taken as the sensefor the word.
The current work however only usesa lexical resource for one of the languages, and assuch has fewer places to fail, and less demandingrequirements.Other similar work includes that in (Ng et al,2003), where a sense-annotated corpus was automat-ically generated from a parallel corpus.
This is doneby word-aligning the parallel corpus, and then find-ing the senses according to WordNet given a list ofnouns.
Two senses are lumped together if they aretranslated into the same chinese word.
The selec-tion of correct translations is done manually.
Onlythose occurrences of the chosen nouns that translateto one of the chosen chinese words are consideredsense-tagged by the translation.Although similar in approach to what the currentsystem would do, this system uses a much more sim-ple approach to generate sense annotations and it de-pends on a previously word-aligned corpus, whereasthe current approach would integrate alignment andsense-tagging, whis may give a higher accuracy.3 Senses ModelThe current model is based on the HMM alignmentmodel (Vogel et al, 1996), as it is a less complexmodel than IBM models 3 and above, but still findsacceptable alignments.
The HMM alignment modelis defined as a HMM model, where the observed40e a2a1fmf1 f2amFigure 1: Diagram of HMM model.
Arrows indicatedependencies, grey nodes indicate known values, whitenodes indicate hidden variables.variables are the words of a sentence in the Frenchlanguage f, and the hidden variables are alignmentsto words in the English sentence e, or to a null state.See figure 1 for a diagram of the standard HMMmodel.
Under this model, French words can align toat most 1 English word.
The transition probabilityis not dependent on the english words themselves,but on the size of jumps between alignments and thelength of the English sentence.
The probability ofthe French sentence given the English sentence is:Pr(f|e) =?aJ?j=1p(fj |eaj )p(aj |aj?1, I) (1)Here, f and e denote the French and English sen-tences, which have lengths J and I respectively, anda denotes an alignment of these two sentences.
So,the states in the HMM assign a number from therange [0, I] to each of the positions j in the Frenchsentence, effectively assigning one English word eajto each French word fj , or a NULL translation e0.The term p(fj |eaj ) is the translation probability of apair of words, and p(aj |aj?1, I) gives the transitionprobability in the HMM.Here, i is the current state of the HMM, and i?
isthe previous state of the HMM, each being an indexinto the English sentence and p(aj |aj?1, I) is de-fined as the probability of the gap between i and i?.So, if in an alignment French word 2 is aligned to the3rd English word, and the next French Word (3) isaligned to the 5th English word, p(aj |aj?1, I) isn?tmodelled directly as p(5|3, I), but as p(5?
3|I).To implement a dependency on senses in themodel an extra hidden layer is added to the HMMmodel, representing the senses.
The probability of as1 s2e a2a1fmf1 f2amsmFigure 2: Diagram of SHMM model, with senses gener-ated by the English words.
Arrows indicate dependen-cies, grey nodes indicate known values, white nodes in-dicate hidden variables.french word then depends on the generated sense,the probability of which depends on the English.The possible senses for a given English word is con-strained by an external source, such as WordNet.The probability under the model of a french sen-tence f given an English sentence e thus becomes:Pr(f|e) =?aJ?j=1p?
(fj |eaj )p(aj |aj?1, I) (2)wherep?
(fj |eaj ) =K?k=1p(fj |sk)p(sk|eaj ) (3)Here, K is the number of senses that english wordassociated with this translation pair.
The senses willbe constrained either by the English word eaj or bythe French word fj depending on which languagethe sense inventory is taken from.
The first case,with senses constrained by the English, will be de-noted with SHMM1, and the second with SHMM2.In this work, only SHMM1 is used.If the amount of senses defined for each word isexactly 1 and this sense is different for each word,the model reduces to the HMM model (see Figure2).
However, if the sense inventory is defined suchthat for two different words with a sense that is sim-ilar, the same sense can be used, the model is ableto use translation probabilities drawn from observa-tions from both these words together.
For example,41in SHMM1, the words ?small?
and ?little?
may havethe same sense listed in the sense inventory, whichallows the model to learn a translation distribution tothe French words that both these words often alignto.For training this model, as with the IBM models,Expectation-Maximization and initialisation are key.The more complex IBM models are initialised fromsimpler versions, so the complex models can startout with reasonable estimates, which allow it to findgood alignments.
Here, too, the same steps are used.The HMM model is initialised from Model 1, as de-scribed in citevogel:1996.
From this, the SHMMmodels can be initialised.For the SHMM1, given a translation probabilityfor a french word given an english word under theHMM, p(f |e), and a list of valid senses for thatenglish word e, an equal portion of that translationprobability is given to the new translation probabil-ity depending on the sense.
This is done for all trans-lation probabilities, and the translation table is thennormalised.
Probability of a sense given an englishword is initialised to a uniform distribution over thevalid senses.For the SHMM2, the probability of french wordsgiven a sense is set to uniform over the words forwhich the sense is valid, and the probability of thesense given the english word is calculated analogousto the probability of the french word given the sensein the first case.After initialisation, the expectation-maximisationalgorithm can be used for training, as with the HMMmodel, using the forward-backward algorithm tofind the posterior probabilities of the alignments.
Asthe senses can be summed out during this phase, thealgorithm can be used as-is, and afterwards the pro-portion of the partial count that should be assignedto each sense can be found.
By summing out overthe relevant senses and words, the two parts c(fj |qk)and c(qk|ei) can then be found.3.1 Generating Senses for WordsIn order to be able to use this model, an inventoryof senses is needed for every word in the corpus, forone of the languages.
The most obvious source forthis is the English Wordnet (Miller, 1995), as it hasa large inventory of senses.
Note that, in this doc-ument, the words senses and synsets are used inter-changeably.The process of obtaining this inventory is ex-plained from the viewpoint of using English Word-Net, but the same basic conditions apply for anyother lexicon, or language.
The inventory of sensesis obtained through the WordNet corpus in NLTK1, which automatically stems the words that synsetsare sought for.In this model, two senses (synsets) are function-ally equivalent, if the list of words that have themin their senselist is the same for both senses.
Thatis to say, if the partial counts that will be added toeither of the senses will be the same, there is no wayof distinguishing between the two senses under thismodel.
For example, in WordNet 3.0, among thesynsets listed for the word ?small?, there are 3 thathave as constituent words only ?small?
and ?little?.These 3 synsets would be functionally equivalent forour purposes.
When this occurs, the senses that areequivalent are collated under one name, so that it?spossible to find out which senses a particular senseis made up of.At this point, there will be some words with onlya sense that is unique to that word (such as thosewords that were not in the lexicon, which get a newlymade sense), some words with only shared sensesand some with a mix.
We might want to enforce oneof a few distinct options:?
All words have exactly 1 unique sense, and per-haps a few shared ones (?synthesis?
condition)?
Some words have a unique sense, some don?t(?merge?
condition)?
No words have unique senses if they have atleast 1 shared sense (?none?
condition)These conditions are generated by first finding thefiltered list of senses for each word.
At this point,some words have only unique senses, either becausethey didn?t occur in WordNet, or because WordNetonly listed unique senses for that word (the ?merge?condition.
The ?synth?
condition is made, by findingall words that have only shared senses, and adding anew sense, that is unique to that word.
The ?none?1http://www.nltk.org/421 2 3 4 5Number of iterations0.150.200.250.300.350.400.450.50AERModel 1HMMSHMM (none)SHMM (merge)SHMM (synth)Figure 3: AER scores for Model 1, HMM, and 3 SHMM variations trained for 5 iterations each, lower is better.condition then is found by doing the opposite: re-moving all unique senses from words that also haveshared senses.Under each of these 3 conditions, the model mightwork slightly differently.
Under the ?synthesis?
con-dition, it may generate the translation probabilitieseither directly, as in the HMM (which is what hap-pens for any word with only 1 sense, which is uniquefor that word), or from the shared probabilities,through the senses.
In the other models, the modelis increasingly forced to use the shared translationprobabilities.4 EvaluationWe will evaluate the early results of this modelagainst the HMM and Model 1 results, and will doa qualitative analysis of the distribution over sensesand French words that the model obtains, in orderto find out if reasonable predictions for senses aremade.The sense HMM model will be evaluated usingthe three sense inventories suggested in subsection3.1.
The dataset used was a 1 million sentencealigned English-French corpus, taken from the Eu-roparl corpus (Koehn, 2005).
The data was to-kenised, length limited to a maximum length of 50,and lowercased.
The results are evaluated on the testset from the ACL 2005 shared task, using AlignmentError Rate.
The models are all trained for 5 itera-tions, and a pruning threshold is employed that re-moves probabilities from the translation tables if itis below 1.0 ?
10?6.The results of training models based on sensesgenerated in the 3 ways listed above is shown inFigure 3.
The three SHMM models are comparedagainst Model 1, and the standard HMM model,each of which is trained for 5 iterations.
The HMMmodel is initialised from Model 1, and the SHMMmodels initialised from the HMM model.
As the fig-ure shows, the AER score for the last two iterationsof the HMM model is very similar to the scores thatthe three variations of the SHMM model attain.
Thescores for the three HMM models range from 0.185to 0.192A possible reason for this performance is that themodels didn?t have enough sharing going on be-tween the senses.
The corpus contains 70700 uniquewords.
Looking at the amount of senses that arefound in the ?none?
condition, meaning that all of theWordNet senses share output probabilities, there are17194 words that have at least one of these senseslisted, and there are 27120 distinct senses availablein that setting.
For the other 53500 senses, no shar-ing is going on whatsoever.In the ?merge?
and ?synth?
conditions, there aremore senses taken from WordNet (for a total fromWordNet of 33133), but these don?t add any shar-43Sense Definition P (s|e) Most likely French words in ordersevere.s.06 very bad in degree orextent0.4861 graves, se?ve`res, des, se?ve`re, grave, de, grave-ment, une, se?rieuses, lessevere.s.04 unsparing and un-compromising indiscipline or judg-ment0.2358 graves, se?ve`res, des, se?ve`re, grave, de, grave-ment, une, se?rieuses, lesdangerous.s.02 causing fear or anx-iety by threateninggreat harm0.1177 grave, des, graves, les, se?rieux, tre`s,se?rieuses, une, importantes, se?rieuseaustere.s.01 severely simple 0.1148 graves, des, grave, se?ve`re, se?ve`res, tre`s, forte-ment, forte, rigoureuses, situationhard.s.04 very strong or vigor-ous0.035 dur, plus, importants, des, se?ve`res, durement,son, une, difficile, tre`ssevere.s.01 intensely or ex-tremely bad orunpleasant in degreeor quality0.01055 terrible, terribles, des, grave, les, mauvais,dramatique, cette, aussi, terriblementTable 1: Senses for the word ?severe?
in the ?none?
version of the SHMM model, their WordNet definition, the proba-bility of the sense for the word severe, and the most likely French words for the senses given in order of likelihood.ing.
It might be then, that the model has insuffi-cient opportunity to share output distributions, caus-ing it to behave much as the HMM alignment model.Another possibility is, that the senses insufficientlywell-defined, and share probabilities between wordsthat are too dissimilar, negating any positive effectthis may have and possibly pushing the model to-wards less sharing.
We will suggest possibilities fordealing with this in section 5.Regardless of the performance of the model inword alignment, if the model learns probabilities forsenses that are reasonable, it can be used as a wordsense disambiguation system for parallel corpora,with the candidate senses being made up from thesenses out of WordNet.
Those words not listed inWordNet, are treated as being monosemous wordsin this context.
The ?merge?
and ?none?
conditionsare most useful for this: if a WSD system chooses asense that is not linked to a WordNet sense, it is notclearly defined which sense is meant here.In order to find out if the model makes sensi-ble distinctions between different senses, we havepicked a random polysemous word, and looked atthe senses associated with it in the ?none?
condition.The word that was chosen is ?severe?.
It has 6 pos-Sense Associated English wordssevere.s.06 (only has basic 3 senses)severe.s.04 spartandangerous.s.02 dangerous, grave, graver,gravest, grievous, life-threatening, seriousaustere.s.01 austere, stark, starker, starkest,sternhard.s.04 hard, harder, hardestsevere.s.01 terrible, wickedTable 2: Senses for the word ?severe?
in the ?none?
ver-sion of the SHMM model and the English words apartfrom ?severe?, ?severer?
and ?severest?
that have the sensein their senselistsible senses, listed by main word and definition inTable 1, along with the probability of the senses,p(s|e), and the 10 most likely French words for thesenses.As the table shows, the two most likely senses arequite similar.
In fact, because words are stemmedbefore looking up suitable senses, all senses have atleast the following 3 words associated with them:?severe?, ?severer?
and ?severest?.
The words that44Sense Definition P(s?e) Most likely French words in orderrigorous.s.01 rigidly accurate; al-lowing no deviationfrom a standard0.8962 rigoureuse, rigoureux, une, rigueur,rigoureuses, des, un, stricte, strict, strictesrigorous.s.02 demanding strict at-tention to rules andprocedures0.1038 des, strictes, rigoureux, stricte, se?ve`res,rigoureuses, stricts, rigoureuse, une, se?ve`reTable 3: Senses for the word ?rigorous?
in the ?none?
version of the SHMM model, their WordNet definition, theprobability of the senses of the word ?rigorous?, and the most likely French words for the senses given in order oflikelihood.cause the differences between the senses are listedin table 2.
It can be seen that the only differencebetween severe.s.04 and severe.s.06 is the additionof the word ?spartan?
for the first.
As ?spartan?
onlyoccurs 67 times in the corpus, versus 484 for severe,it is possible that they are so similar, because thecounts for ?spartan?
get overshadowed.For the other senses however, the most likelytranslations vary quite a bit.
The sense ?hard.s.04?,meaning very strong or vigorous, also includestranslations to ?plus?
and ?dur?, which seems morelikely given the sense.
Given these translation prob-abilities though, it should at least be possible to dis-tinguish between different senses of the word severe,given that it?s aligned to a different french word.One more example is listed in table 3, showingthe probabilities for two different senses, and theirmost likely translations.
The most likely sense forrigorous under the model is in the sense of ?allowingno deviation from a standard?.
This is the only of thetwo senses that can translate to ?rigueur?
in french,literally rigor.
The other sense, meaning ?demand-ing strict attention to rules and procedures?, is morelikely to translate to ?strictes?, ?stricte?
and ?se?ve`res?,which reflects the WordNet definition.The difference in contributing English words be-tween these two senses can be found in Table 4.
In-terestingly, the three forms of the word strict are as-sociated with the sense rigorous.s.01, even thoughthe naive translations of these words into French aremore likely for rigorous.s.02.
Even so, the resultsmatch the WordNet definitions better.These results show that useful translations arefound, and the corresponding senses can be learnedas well.
For sense discrimination in parallel cor-puses then, this model shows potential, and forSense Associated English wordsrigorous.s.01 rigorous strict stricterstrictestrigorous.s.02 rigorous stringent tighttighter tightestalignment good alignments can be found, even withbetter abstraction in the model.5 ConclusionThe results have shown that this may be a useful wayto incorporate senses in a word alignment system.While the alignment results in themselves weren?tsignificantly better, alignment probabilities to senseshave been shown to be generated, which make it pos-sible to distinguish between different senses.
Thiscould open the door to automatically sense annotat-ing parallel corpora, using a predefined set of senses.At this early point, several options lay open toimprove upon the results so far.
To improve thealignment results, more encompassing senses maybe generated, for example by integrating similarsynsets.
At the same time, the list of synsets foreach word may be improved upon, by filtering outvery unlikely senses for a word.It should also be possible to employ an already ex-isting WSD system to annotate the parallel corpus,and use the counts of the annotated senses to betterinitialise the senses, rather than starting out assum-ing all are equaly likely for a given word.
This maybe used as well to initialise the translation probabil-ities for senses.45ReferencesMarine Carpuat and Dekai Wu.
2005.
Word sense disam-biguation vs. statistical machine translation.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, ACL ?05, pages 387?394, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Marine Carpuat and Dekai Wu.
2007.
Improving statisti-cal machine translation using word sense disambigua-tion.
In In The 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Compu-tational Natural Language Learning (EMNLP-CoNLL2007, pages 61?72.Yee Seng Chan and Hwee Tou Ng.
2007.
Word sensedisambiguation improves statistical machine transla-tion.
In In 45th Annual Meeting of the Associationfor Computational Linguistics (ACL-07, pages 33?40.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, ACL ?07,pages 177?180, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
In Proceedings of AMTA 2004 (Conference of theAssociation for Machine Translation in the Americas),volume 3265, pages 115?124.
Springer.P.
Koehn.
2005.
Europarl: A Parallel Corpus for Sta-tistical Machine Translation.
In Machine TranslationSummit X, pages 79?86, Phuket, Thailand.George A. Miller.
1995.
Wordnet: A lexical database forenglish.
Communications of the ACM, 38:39?41.Hwee Tou Ng, Bin Wang, and Yee Seng Chan.
2003.
Ex-ploiting parallel texts for word sense disambiguation:an empirical study.
In Proceedings of the 41st AnnualMeeting on Association for Computational Linguistics- Volume 1, ACL ?03, pages 455?462, Stroudsburg,PA, USA.
Association for Computational Linguistics.Dan Tufis?, Radu Ion, and Nancy Ide.
2004.
Fine-grainedword sense disambiguation based on parallel corpora,word alignment, word clustering and aligned word-nets.
In Proceedings of the 20th international con-ference on Computational Linguistics, COLING ?04,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
Hmm-based word alignment in statistical trans-lation.
In Proceedings of the 16th conference onComputational linguistics - Volume 2, COLING ?96,pages 836?841, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.46
