Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1140?1149,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsIDEST: Learning a Distributed Representation for Event PatternsSebastian Krause?LT Lab, DFKIAlt-Moabit 91c10559 Berlin, Germanyskrause@dfki.deEnrique Alfonseca Katja Filippova Daniele PighinGoogle ResearchBrandschenkestrasse 1108810 Zurich, Switzerland{ealfonseca,katjaf,biondo}@google.comAbstractThis paper describes IDEST, a new method forlearning paraphrases of event patterns.
It isbased on a new neural network architecturethat only relies on the weak supervision sig-nal that comes from the news published on thesame day and mention the same real-world en-tities.
It can generalize across extractions fromdifferent dates to produce a robust paraphrasemodel for event patterns that can also cap-ture meaningful representations for rare pat-terns.
We compare it with two state-of-the-artsystems and show that it can attain compara-ble quality when trained on a small dataset.Its generalization capabilities also allow it toleverage much more data, leading to substan-tial quality improvements.1 IntroductionMost Open Information Extraction (Open-IE) sys-tems (Banko et al, 2007) extract textual relationalpatterns between entities automatically (Fader et al,2011; Mausam et al, 2012) and optionally organizethem into paraphrase clusters.
These pattern clustershave been found to be useful for Question Answer-ing (Lin & Pantel, 2001; Fader et al, 2013) and re-lation extraction (Moro & Navigli, 2012; Grycner &Weikum, 2014), among other tasks.A related Open-IE problem is that of automati-cally extracting and paraphrasing event patterns:those that describe changes in the state or attributevalues of one or several entities.
An existing ap-proach lo learn paraphrases of event patterns isto build on the following weak supervision signal:?Work performed during an internship at Googlenews articles that were published on the same dayand mention the same entities should contain goodparaphrase candidates.
Two state-of-the-art eventparaphrasing systems that are based on this assump-tion are NEWSSPIKE (Zhang & Weld, 2013) andHEADY (Alfonseca et al, 2013; Pighin et al, 2014).These two systems have a lot in common, yet theyhave never been compared with each other.
Theyhave specific weak and strong points, and there aremany ways in which they are substantially different:?
Scope of generalization.
In NEWSSPIKE theparaphrase clusters are learned separately foreach publication day and entity set, and the sys-tem cannot generalize across events of the sametype involving different entities occurring onthe same or on different days.
For example, ifthe event verbs has married and wed appear innews about two entitiesA andB marrying, andhas married and tied the knot with appear innews involving two different entities C and D,NEWSSPIKE is not able to infer that wed andtied the knot with are also paraphrases, unless apost-processing is done.HEADY overcomes this limitation thanks to aglobal model that learns event representationsacross different days and sets of entities.
How-ever, the global nature of the learning problemcan incur into other drawbacks.
First, training aglobal model is more costly and more difficultto parallelize.
Second, relatively frequent pat-terns that erroneously co-occur with other pat-terns may have a negative impact on the finalmodels, potentially resulting in noisier clusters.Lastly, low-frequency patterns are likely to be1140discarded as noisy in the final model.
Over-all, HEADY is better at capturing paraphrasesfrom the head of the pattern distribution, andis likely to ignore most of the long tail whereuseful paraphrases can still be found.?
Simplifying assumptions.
We already men-tioned that the two systems share a common un-derlying assumption, i.e., that good paraphrasecandidates can be found by looking at newspublished on the same day and mentioning thesame entities.
On top of this, NEWSSPIKE alsoassumes that better paraphrases are reportedaround spiky entities, verb tenses may not dif-fer, there is one event mention per discourse,and others.
These restrictions are not enforcedby HEADY, where the common assumption isindeed even relaxed across days and entity sets.?
Annotated data.
NEWSSPIKE requires hand-annotated data to train the parameters of asupervised model that combines the differentheuristics, whereas HEADY does not need an-notated data.This paper describes IDEST, a new method forlearning paraphrases of event patterns that is de-signed to combine the advantages of these two sys-tems and compensate for their weaknesses.
It isbased on a new neural-network architecture that, likeHEADY, only relies on the weak supervision signalthat comes from the news published on the same day,requiring no additional heuristics or training data.Unlike NEWSSPIKE, it can generalize across differ-ent sets of extracted patterns, and each event patternis mapped into a low-dimensional embedding space.This allows us to define a neighborhood around apattern to find the ones that are closer in meaning.IDEST produces a robust global model that canalso capture meaningful representations for rare pat-terns, thus overcoming one of HEADY?s main lim-itations.
Our evaluation of the potential trade-offbetween local and global paraphrase models showsthat comparably good results to NEWSSPIKE can beattained without relying on supervised training.
Atthe same time, the ability of IDEST to produce aglobal model allows it to benefit from a much largernews corpus.2 Related workRelational Open-IE In an early attempt to moveaway from domain-specific, supervised IE systems,Riloff (1996) attempted to automatically find rela-tional patterns on the web and other unstructured re-sources in an open domain setting.
This idea hasbeen further explored in more recent years by Brin(1999), Agichtein & Gravano (2000), Ravichan-dran & Hovy (2002) and Sekine (2006), amongthe others.
Banko et al (2007) introduced Open-IE and the TEXTRUNNER system, which extractedbinary patterns using a few selection rules ap-plied on the dependency tree.
More recent sys-tems such as REVERB (Fader et al, 2011) and OL-LIE (Mausam et al, 2012) also define linguistically-motivated heuristics to find text fragments or depen-dency structures that can be used as relational pat-terns.A natural extension to the previous work is to au-tomatically identify which of the extracted patternshave the same meaning, by producing either a hardor a soft clustering.
Lin & Pantel (2001) use themutual information between the patterns and theirobserved slot fillers.
Resolver (Yates & Etzioni,2007) introduces a probabilistic model called theExtracted Shared Property (ESP) where the proba-bility that two instances or patterns are paraphrasesis based on how many properties or instances theyshare.
USP (Poon & Domingos, 2009) produces aclustering by greedily merging the extracted rela-tions.
Yao et al (2012) employ topic models to learna probabilistic model that can capture also the am-biguity of polysemous patterns.
More recent workalso organizes patterns in clusters or taxonomiesusing distributional methods on the pattern con-texts or entities extracted (Moro & Navigli, 2012;Nakashole et al, 2012), or implicitly clusters rela-tional text patterns via the learning of latent featurevectors for entity tuples and relations, in a settingsimilar to knowledge-base completion (Riedel et al,2013).A shared difficulty for systems that cluster pat-terns based on the arguments they select is that it isvery hard for them to distinguish between identityand entailment.
If one pattern entails another, bothare likely to be observed in the corpus involving thesame entity sets.
A typical example illustrating thisproblem is the two patterns e1married e2and e11141John Smith married Mary Brown in Baltimore yesterday after a long courtshipperson person locationsubj dobj pobjpreptmodpreppobjamoddetFigure 1: Example sentence, and extraction (the nodes connected through solid dependency edges).?
married ?person personsubj dobjFigure 2: Example pattern that encodes a wedding eventbetween two people.dated e2, which can be observed involving the samepairs of entities, but which carry a different mean-ing.
As discussed below, relying on the temporal di-mension (given by the publication date of the inputdocuments) is one way to overcome this problem.Event patterns and Open-IE Although some ear-lier work uses the temporal dimension of text asfilters to improve precision of relational patternclusters, NEWSSPIKE (Zhang & Weld, 2013) andHEADY (Alfonseca et al, 2013; Pighin et al, 2014)fully rely on it as its main supervision signal.
In or-der to compare the two approaches, we will start bydefining some terms:?
An event pattern encodes an expression thatdescribes an event.
It can be either a linear sur-face pattern or a lexico-syntactic pattern, andcan possibly include entity-type restrictions onthe arguments.
For example, Figure 2 rep-resents a binary pattern that corresponds to awedding event between two people.?
An extraction is a pattern instance obtainedfrom an input sentence, involving specific en-tities.
For example, the subgraph representedwith solid dependency edges in Figure 1 is anextraction corresponding to the pattern in Fig-ure 2.?
An Extracted Event Candidate Set (EEC-Set (Zhang & Weld, 2013), or just EEC forbrevity) is the set of extractions obtained fromnews articles published on the same day, andinvolving the same set of entities.?
Two extractions are co-occurrent if there is atleast one EEC that contains both of them.NEWSSPIKE produces extractions from the in-put documents using REVERB (Fader et al, 2011).The EECs are generated from the titles and all thesentences of the first paragraph of the documentspublished on the same day.
From each EEC, po-tentially one paraphrase cluster may be generated.The model is a factor graph that captures severaladditional heuristics.
Integer Lineal Programming(ILP) is then used to find the Maximum a Posteriori(MAP) solution for each set of patterns, and modelparameters are trained using a labeled corpus thatcontains 500 of these sets.Regarding HEADY, it only considers titles andfirst sentences for pattern extraction and trains atwo-layer Noisy-OR Bayesian Network, in whichthe hidden nodes represent possible event types, andthe observed nodes represent the textual patterns.A maximum-likelihood model is the one in whichhighly co-occurring patterns are generated by thesame latent events.
The output is a global soft clus-tering, in which two patterns may also be clusteredtogether even if they never co-occur in any EEC,as long as there is a chain of co-occurring patternsgenerated by the same hidden node.
HEADY wasevaluated using three different extraction methods:a heuristic-based pattern extractor, a sentence com-pression algorithm and a memory-based method.While this model produces a soft clustering ofpatterns, HEADY was evaluated only on a headlinegeneration task and not intrinsically w.r.t.
the qualityof the clustering itself.Neural networks and distributed representationsAnother related field aims to learn continuous vec-tor representations for various abstraction levels of1142natural language.
In particular the creation of so-called word embeddings has attracted a lot of atten-tion in the past years, often by implementing neural-network language models.
Prominent examples in-clude the works by Bengio et al (2003) and Mikolovet al (2013), with the skip-gram model of the lat-ter providing a basis for the vector representationslearned in our approach.Also closely related to IDEST are approacheswhich employ neural networks capable of handlingword sequences of variable length.
For example,Le & Mikolov (2014) extend the architectures ofMikolov et al (2013) with artificial paragraph to-kens, which accumulate the meaning of words ap-pearing in the respective paragraphs.In contrast to these shallow methods, other ap-proaches employ deep multi-layer networks for theprocessing of sentences.
Examples include Kalch-brenner et al (2014), who employ convolutionalneural networks for analyzing the sentiment of sen-tences, and Socher et al (2013), who present a spe-cial kind of recursive neural network utilizing ten-sors to model the semantics of a sentence in a com-positional way, guided by the parse tree.A frequent issue with the deeper methods de-scribed above is the high computational complex-ity coming with the large numbers of parameters ina multi-layer neural network or in the value prop-agation in unfolded recursive neural networks.
Tocircumvent this problem, our model is inspired byMikolov?s simpler skip-gram model, as describedbelow.3 Proposed modelSimilarly to HEADY and NEWSSPIKE, our model isalso based on the underlying assumption that if sen-tences from two news articles were published on thesame day and mention the same entity set, then theyare good paraphrase candidates.
The main noveltyis in the way we train the paraphrase model fromthe source data.
We propose a new neural-networkarchitecture which is able to learn meaningful dis-tributed representations of full patterns.3.1 Skip-gram neural networkThe original Skip-gram architecture (Mikolov et al,2013) is a feed-forward neural network that istrained on distributional input examples, by assum-ing that each word should be able to predict to someextent the other words in its context.
A skip-gramarchitecture consists of:1.
An input layer, usually represented as a one-of-V or one-hot-spot layer.
This layer type has asmany input nodes as the vocabulary size.
Eachtraining example will activate exactly one inputnode corresponding to the current word wi, andall the other input nodes will be set to zero.2.
A first hidden layer, the embedding or projec-tion layer, that will learn a distributed represen-tation for each possible input word.3.
Zero or more additional hidden layers.4.
An output layer, expected to pre-dict the words in the context of wi:wi?K, ..., wi?1, wi+1, ..., wi+K.In practice, when training based on this architecture,the network converges towards representing wordsthat appear in similar contexts with vectors that areclose to each other, as close vectors will produce asimilar distribution of output labels in the network.3.2 IDEST neural networkFigure 3 shows the network architecture we use fortraining our paraphrase model in IDEST.
In our case,the input vocabulary is the set ofN unique event pat-terns extracted from text, and our supervision signalis the co-occurrence of event patterns in EECs.
Weset the input to be a one-hot-spot layer with a di-mensionality of N , and for each pair of patterns thatbelong to the same EECs, we will have these pat-terns predict each other respectively, in two separatetraining examples.
The output layer is also a one-of-V layer, because for each training example onlyone output node will be set to 1, corresponding to aco-occurring pattern.After training, if two patterns Piand Pjhavea large overlap in the set of entities they co-occurwith, then they should be mapped onto similar in-ternal representations.
Note that the actual entitiesare only used for EEC construction, but they do notplay a role in the training itself, thus allowing thenetwork to generalize over specific entity instantia-tions.
To exemplify, given the two EECs {?
[Alex]married [Leslie]?, ?
[Leslie] tied the knot with1143Input Embedding SoftMaxP0))##O0P1//))##e044//w?0j$$O1...e1::44w?1j**...Piwi0;;wi155wiE))...Oj...eECC?
?w?Ej55))...PVDDAA55OVFigure 3: Model used for training.
V is the total numberof unique patterns, which are used both in the one-of-Vinput and output.
E is the dimensionality of the embed-ding space.[Alex]?}
and {?
[Carl] and [Jane] wed?, ?
[Carl]married [Jane]?
}, IDEST could learn an embeddingspace in which ?
[Per] tied the know with [Per]?
and?
[Per] and [Per] wed?
are relatively close, eventhough the two patterns never co-occur in thesame EEC.
This is possible because both pat-terns have been trained to predict the same pattern{?
[Per] married [Per]?
}.Reported representations of word embeddingstypically use between 50 and 600 dimensions(Mikolov et al, 2013; Levy & Goldberg, 2014).
Forour pattern embeddings we have opted for an em-bedding layer size of 200 nodes.
We also experi-mented with larger sizes and with adding more in-termediate hidden layers, but while the added costin terms of training time was substantial we did notobserve a significant difference in the results.4 Experimental settings4.1 Pattern extraction methods usedIn previous work we can find three different patternextraction methods from a sentence:?
Heuristic-based, where a number of hand-written rules or regular expressions based onpart-of-speech tags or dependency trees areused to select the most likely pattern from thesource sentence (Fader et al, 2011; Mausamet al, 2012; Alfonseca et al, 2013).?
Sentence compression, which takes as input theoriginal sentence and the entities of interest andproduces a shorter version of the sentence thatstill includes the entities (Pighin et al, 2014).?
Memory-based, that tries to find the shortestreduction of the sentence that still includesthe entities, with the constraint that its lexico-syntactic structure has been seen previously asa full sentence in a high-quality corpus (Pighinet al, 2014).It is important to note that the final purpose ofthe system may impact the decision of which ex-traction method to choose.
Pighin et al (2014) usethe event models to generate headlines, and usingthe memory-based method resulted in more gram-matical headlines at the cost of coverage.
If thepurpose of the patterns is information extraction forknowledge base population, then the importance ofhaving well-formed complete sentences as patternsbecomes less obvious, and higher coverage meth-ods become more attractive.
For these reasons, inthis paper we focus on the first two approaches,which are very well-established and can producehigh-coverage output.
More specifically, we useREVERB extractions and a statistical compressionmodel trained on (sentence, compression) pairs im-plemented after Filippova & Altun (2013).4.2 Generating clusters from the embeddingvectorsIDEST does not produce a clustering likeNEWSSPIKE and HEADY, so in order to be able tocompare against them we have used the algorithmdescribed in Figure 4 to build paraphrase clustersfrom the pattern embeddings.
Given a similaritythreshold on the cosine similarity of embedding vec-tors, we start by sorting the patterns by extractionfrequency and proceed in order along the sorted vec-tor by keeping the most similar pattern of each.
Usedpatterns are removed from the original set to makesure that a pattern is not added to two clusters at thesame time.1144function COMPUTECLUSTERS(P , ?
)Result = {}SORTBYFREQUENCY(P )while |P | > 0 dop = POP(P ) .
Take highest-frequency patternCp= {p} .
Initialize cluster around pN = NEIGHBORS(p, P, ?)
.
n ?
P, sim(n, p) > ?for all n ?
N doCp= Cp?
{n}REMOVE(P, n) .
Remember n has been usedResult = Result ?
{Cp}return ResultFigure 4: Pseudocode of the algorithm for producing aclustering from the distributed representation of the ex-tracted patterns.
P is the set of extracted patterns, and ?is the similarity threshold to include two patterns in thesame cluster.5 Evaluation resultsThis section opens with a quantitative look at theclusterings obtained with the different methods tounderstand their implications with respect to the dis-tribution of event clusters and their internal diversity.In 5.2, we will complement these figures with the re-sults of a manual quality evaluation.5.1 Quantitative analysis5.1.1 NEWSSPIKE vs. IDEST-ReV-NSThis section compares the clustering models thatwere output by NEWSSPIKE and IDEST when us-ing the same set of extractions, to evaluate theperformance of the factor graph-based method andthe neural-network method on exactly the sameEECs.
We have used as input the dataset releasedby Zhang & Weld (2013)1, which contains 546,713news articles, from which 2.6 million REVERBextractions were reportedly produced.
84,023 ofthese are grouped into the 23,078 distributed EECs,based on mentions of the same entities on the sameday.
We compare here the released output clustersfrom NEWSSPIKE and a clustering obtained froma IDEST-based distributed representation trained onthe same EECs.Figure 5 shows a comparative analysis of the twosets of clusters.
As can be seen, IDEST generatessomewhat fewer clusters for every cluster size thanNEWSSPIKE.
We have also computed a lexical di-versity ratio, defined as the percentage of root-verb1http://www.cs.washington.edu/node/9473Figure 5: Cluster size (log-scale) and ratio of unique verblemmas in the clusters generated from NEWSSPIKE andIDEST with the REVERB extractions as input.lemmas in a cluster that are unique.
This met-ric captures whether a cluster mainly contains thesame verb with different inflections or modifiers, orwhether it contains different predicates.
The fig-ure shows that IDEST generates clusters with muchmore lexical diversity.
These results make sense in-tuitively, as a global model should be able to pro-duce more aggregated clusters by merging patternsoriginating from different EECs, resulting in fewerclusters with a higher lexical diversity.
A higher lex-ical diversity may be a signal of richer paraphrasesor noisier clusters.
The manual evaluation in Sec-tion 5.2 will address this issue by comparing thequality of the clusterings.5.1.2 NEWSSPIKE vs. IDEST-Comp-NSFigure 6 compares NEWSSPIKE?s clusters againstIDEST clusters obtained using sentence compres-sion instead of REVERB for extracting patterns.Both systems were trained on the same set of inputnews.
Using sentence compression, the total num-ber of extracted patterns was 321,130, organized in41,740 EECs.
We can observe that IDEST producedlarger clusters than NEWSSPIKE.
For cluster sizeslarger or equal to 4, this configuration of IDESTproduced more clusters than NEWSSPIKE.
At thesame time, lexical diversity remained consistentlyon much higher levels, well over 60%.5.1.3 IDEST-Comp-NS vs. IDEST-Comp-AllIn order to evaluate the impact of the size of train-ing data, we produced a clustering from embeddingvectors trained from a much larger dataset.
We used1145Figure 6: Cluster size (log-scale) and ratio of unique verblemmas in the clusters generated from NEWSSPIKE andIDEST with compression-based pattern extraction.Figure 7: Cluster size (log-scale) and ratio of uniqueverb lemmas in the clusters generated from IDEST withcompression-based pattern extraction, using only the500,000 NEWSSPIKE articles, or the large dataset.our own crawl of news collected between 2008 and2014.
Using sentence compression, hundreds ofmillions of extractions have been produced.In order to keep the dataset at a reasonable size,and aiming at producing a model of comparable sizeto the other approaches, we applied a filtering stepin which we removed all the event patterns that werenot extracted at least five times from the dataset.
Af-ter this filtering, 28,014,423 extractions remained,grouped in 8,340,162 non-singleton EECs.Figure 7 compares the resulting clusterings.
Inthe all-data setting, clusters were generally smallerand showed less lexical variability.
We believe thatthis is due to the removal of the long tail of low-frequency and noisy patterns.
Indeed, while highlexical variability is desirable it can also be a signof noisy, unrelated patterns in the clusters.
The co-hesiveness of the clusters, which we will evaluate inSection 5.2, must also be considered to tell construc-tive and destructive lexical variability apart.5.1.4 HEADYHEADY produces a soft-clustering from a gener-ative model, and expects the maximum number ofclusters to be provided beforehand.
The model thentries to approximate this number.
In our experi-ments, 5,496 clusters were finally generated.
Oneweak point of HEADY, mentioned above, is that low-frequency patterns do not have enough evidence andNoisy-OR Bayesian Networks tend to discard them;in our experiments, only 4.3% of the unique ex-tracted patterns actually ended up in the final model.5.2 Qualitative analysisThe clusters obtained with different systems anddataset have been evaluated by five expert raters withrespect to three metrics, according to the followingrating workflow:1.
The rater is shown the cluster, and is asked toannotate which patterns are meaningless or un-readable2.
This provides us with a Readabil-ity score, which measures at the same time thequality of the extraction algorithm and the abil-ity of the method to filter out noise.2.
The rater is asked whether there is a majoritytheme in the cluster, defined as having at leasthalf of the readable patterns refer to the samereal-world event happening.
If the answer isNo, the cluster is annotated as noise.
We callthis metric Cohesiveness.3.
If a cluster is cohesive, the rater is finally askedto indicate which patterns are expressing themain theme, and which ones are unrelated toit.
The third metric, Relatedness, is definedas the percentage of patterns that are related tothe main cluster theme.
All the patterns in anon-cohesive cluster are automatically markedas unrelated.2In the data released by NewsSpike, REVERB patterns arelemmatized, but the original inflected sentences are also pro-vided.
We have restored the original inflection of all the wordsto make those patterns more readable for the raters.1146The inter-annotator agreement on the three metrics,measured as the intraclass correlation (ICC), wasstrong (Cicchetti, 1994; Hallgren, 2012).
More pre-cisely, the observed ICC scores (with 0.95 confi-dence intervals) were 0.71 [0.70, 0.72] for cohe-siveness, 0.71 [0.70, 0.73] for relatedness and 0.66[0.64, 0.67] for readability.For the evaluation, from each model we se-lected enough clusters to achieve an overall size(number of distinct event patterns) comparable toNEWSSPIKE?s.
For HEADY and IDEST, the stop-ping condition in Figure 4 was modified accordingly.Table 1 shows the outcome of the annotation.As expected, using a global model (that can mergepatterns from different EECs into single clusters)and using the whole news dataset both led to largerclusters.
At the same time, we observe that usingREVERB extractions generally led to smaller clus-ters.
This is probably because REVERB producedfewer extractions than sentence compression fromthe same input documents.On REVERB extractions, NEWSSPIKE outper-formed IDEST in terms of cohesiveness and related-ness, but NEWSSPIKE?s lowest cluster size and lex-ical diversity makes it difficult to prefer any of thetwo models only w.r.t.
the quality of the clusters.
Onthe other hand, the patterns retained by IDEST-ReV-NS were generally more readable (65.16 vs. 56.66).On the same original news data, using IDESTwith sentence compression produced comparableresults to IDEST-ReV-NS, Cohesiveness being theonly metric that improved significantly.More generally, in terms of readability all themodels that rely on global optimization (i.e., allbut NEWSSPIKE) showed better readability thanNEWSSPIKE, supporting the intuition that globalmodels are more effective in filtering out noisy ex-tractions.
Also, the more data was available toIDEST, the better the quality across all metrics.IDEST model using all data, i.e, IDEST-Comp-All,was significantly better (with 0.95 confidence) thanall other configurations in terms of cluster size, co-hesiveness and pattern readability.
Pattern related-ness was higher, though not significantly better, thanNEWSSPIKE, whose clusters were on average morethan ten times smaller.We did not evaluate NEWSSPIKE on the wholenews dataset.
Being a local model, extending theSystem Ext Data Size Coh(%) Rel(%) Read(%)HEADY Comp All 12.66bcd34.40!27.70!60.70NEWSSPIKE ReV NS 3.40!56.20ac66.42acd56.66IDEST ReV NS 3.62b40.00 47.10a65.16bIDEST Comp NS 5.54bc50.31ac46.58a66.04bIDEST Comp All 44.09?87.93?68.28acd80.13?Table 1: Results of the manual evaluation, averaged overall the clusters produced by each configuration listed.
Ex-traction algorithms: ReV = REVERB; Comp = Com-pression; Data sets: NS = NewsSpike URLs; All = news2008-2014.
Quality metrics: Size: average cluster size;Coh: cohesiveness; Rel: relatedness; Read: readability.Statistical significance:a: better than HEADY;b: bet-ter than NEWSSPIKE;c: better than IDEST-ReV-NS;d:better than IDEST-Comp-NS;?
: better than all others;!
:worse than all others (0.95 confidence intervals, bootstrapresampling).dataset to cover six years of news would only leadto many more EECs, but it would not affect the re-ported metrics as each final cluster would still begenerated from one single EEC.It is interesting to see that, even though theywere trained on the same data, IDEST outperformedHEADY significantly across all metrics, sometimesby a very large margin.
Given the improvementson cluster quality, it would be interesting to evalu-ate IDEST performance on the headline-generationtask for which HEADY was initially designed, butwe leave this as future work.6 ConclusionsWe described IDEST, a new approach based on neu-ral networks to map event patterns into an embed-ding space.
We show that it can be used to constructhigh quality pattern clusters based on neighborhoodin the embedding space.
On a small dataset, IDESTproduces comparable results to NEWSSPIKE, but itsmain strength is in its ability to generalize extrac-tions into a single global model.
It scales to hun-dreds of millions of news, leading to larger clustersof event patterns with significantly better coherenceand readability.
When compared to HEADY, IDESToutperforms it significantly on all the metrics tried.AcknowledgmentsThe first author was partially supported by the Ger-man Federal Ministry of Education and Research,project ALL SIDES (contract 01IW14002).1147ReferencesAgichtein, E. & L. Gravano (2000).
Snowball: Ex-tracting relations from large plain-text collections.In Proceedings of the fifth ACM conference onDigital libraries, pp.
85?94.Alfonseca, E., D. Pighin & G. Garrido (2013).HEADY: News headline abstraction throughevent pattern clustering.
In Proc.
of ACL-13, pp.1243?1253.Banko, M., M. J. Cafarella, S. Soderland, M. Broad-head & O. Etzioni (2007).
Open information ex-traction from the Web.
In Proc.
of IJCAI-07, pp.2670?2676.Bengio, Y., R. Ducharme & P. Vincent (2003).
Aneural probabilistic language model.
Journal ofMachine Learning Research, 3:1137?1155.Brin, S. (1999).
Extracting patterns and relationsfrom the world wide web.
In The World Wide Weband Databases, pp.
172?183.
Springer.Cicchetti, D. V. (1994).
Guidelines, criteria, andrules of thumb for evaluating normed and stan-dardized assessment instruments in psychology.Psychological Assessment, 6(4):284.Fader, A., S. Soderland & O. Etzioni (2011).
Iden-tifying relations for open information extraction.In Proc.
of EMNLP-11, pp.
1535?1545.Fader, A., L. S. Zettlemoyer & O. Etzioni (2013).Paraphrase-driven learning for open question an-swering.
In Proc.
of ACL-13, pp.
1608?1618.Filippova, K. & Y. Altun (2013).
Overcoming thelack of parallel data in sentence compression.
InProc.
of EMNLP-13, pp.
1481?1491.Grycner, A.
& G. Weikum (2014).
Harpy: Hyper-nyms and alignment of relational paraphrases.
InProc.
of COLING-14, pp.
2195?2204.Hallgren, K. A.
(2012).
Computing inter-rater re-liability for observational data: An overview andtutorial.
Tutorials in quantitative methods for psy-chology, 8(1):23.Kalchbrenner, N., E. Grefenstette & P. Blunsom(2014).
A convolutional neural network for mod-elling sentences.
In Proc.
of ACL-14.Le, Q.
& T. Mikolov (2014).
Distributed represen-tations of sentences and documents.
In Proc.
ofICML-14.Levy, O.
& Y. Goldberg (2014).
Linguistic regular-ities in sparse and explicit word representations.In Proc.
of CoNLL-14.Lin, D. & P. Pantel (2001).
Discovery of inferencerules for question-answering.
Natural LanguageEngineering, 7(4):343?360.Mausam, M. Schmitz, R. Bart, S. Soderland &O. Etzioni (2012).
Open language learning for in-formation extraction.
In Proc.
of EMNLP-12, pp.523?534.Mikolov, T., I. Sutskever, K. Chen, G. S. Corrado& J.
Dean (2013).
Distributed representations ofwords and phrases and their compositionality.
InAdvances in Neural Information Processing Sys-tems, pp.
3111?3119.Moro, A.
& R. Navigli (2012).
Wisenet: Buildinga wikipedia-based semantic network with ontolo-gized relations.
In Proc.
of CIKM-12, pp.
1672?1676.Nakashole, N., G. Weikum & F. Suchanek (2012).Patty: a taxonomy of relational patterns with se-mantic types.
In Proc.
of EMNLP-12, pp.
1135?1145.Pighin, D., M. Colnolti, E. Alfonseca & K. Filippova(2014).
Modelling events through memory-based,Open-IE patterns for abstractive summarization.In Proc.
of ACL-14, pp.
892?901.Poon, H. & P. Domingos (2009).
Unsupervised se-mantic parsing.
In Proc.
of EMNLP-09, pp.
1?10.Ravichandran, D. & E. H. Hovy (2002).
Learningsurface text patterns for a question answering sys-tem.
In Proc.
of ACL-02, pp.
41?47.Riedel, S., L. Yao, B. M. Marlin & A. McCallum(2013).
Relation extraction with matrix factor-ization and universal schemas.
In Proc.
of HLT-NAACL-13.Riloff, E. (1996).
Automatically generating extrac-tion patterns from untagged text.
In Proc.
ofAAAI-96, pp.
1044?1049.1148Sekine, S. (2006).
On-demand information extrac-tion.
In Proc.
of COLING-ACL-06 Poster Session,pp.
731?738.Socher, R., A. Perelygin, J. Wu, J. Chuang, C. Man-ning, A. Ng & C. Potts (2013).
Recursive deepmodels for semantic compositionality over a sen-timent treebank.
In Proc.
of EMNLP-13.Yao, L., S. Riedel & A. McCallum (2012).
Unsuper-vised relation discovery with sense disambigua-tion.
In Proc.
of ACL-12, pp.
712?720.Yates, A.
& O. Etzioni (2007).
Unsupervised resolu-tion of objects and relations on the Web.
In Proc.of NAACL-HLT-07, pp.
121?130.Zhang, C. & D. S. Weld (2013).
Harvesting paral-lel news streams to generate paraphrases of eventrelations.
In Proc.
of EMNLP-13, pp.
1776?1786.1149
