Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 238?243,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsPersonalized Page Rank for Named Entity DisambiguationMaria Pershina Yifan He Ralph GrishmanComputer Science DepartmentNew York UniversityNew York, NY 10003, USA{pershina,yhe,grishman}@cs.nyu.eduAbstractThe task of Named Entity Disambiguationis to map entity mentions in the documentto their correct entries in some knowledgebase.
We present a novel graph-based dis-ambiguation approach based on PersonalizedPageRank (PPR) that combines local andglobal evidence for disambiguation and ef-fectively filters out noise introduced by in-correct candidates.
Experiments show thatour method outperforms state-of-the-art ap-proaches by achieving 91.7% in micro- and89.9% in macroaccuracy on a dataset of 27.8Knamed entity mentions.1 IntroductionName entity disambiguation (NED) is the task inwhich entity mentions in a document are mapped toreal world entities.
NED is both useful on its own,and serves as a valuable component in larger Knowl-edge Base Construction systems (Mayfield, 2014).Since the surge of large, publicly available knowl-edge bases (KB) such as Wikipedia, the most popu-lar approach has been linking text mentions to KBnodes (Bunescu and Pas?ca, 2006).
In this paradigm,the NED system links text mentions to the KB, andquite naturally utilizes information in the KB tosupport the linking process.
Recent NED systems(Cucerzan, 2007; Ratinov et al, 2011; Alhelbawyand Gaizauskas, 2014) usually exploit two types ofKB information: local information, which measuresthe similarity between the text mention and the acandidate KB node; and global information, whichmeasures how well the candidate entities in a docu-ment are connected to each other, with the assump-tion that entities appearing in the same documentshould be coherent.
Both types of features have theirstrengths and drawbacks: local features better en-code similarity between a candidate and a KB node,but overlook the coherence between entities; globalfeatures are able to exploit interlinking informationbetween entities, but can be noisy if they are used bytheir own, without considering information from thetext and the KB (cf.
Section 4).In this paper, we propose to disambiguate NEs us-ing a Personalized PageRank (PPR)-based randomwalk algorithm.
Given a document and a list of en-tity mentions within the document, we first constructa graph whose vertices are linking candidates andwhose edges reflects links in Wikipedia.
We run thePPR algorithm on this graph, with the constraint thatwe only allow the highest scored candidate for eachentity to become the start point of a hop.
As all can-didates but the correct one are erronous and probablymisleading, limiting the random walk to start fromthe most promising candidates effectively filters outpotential noise in the Personalized PageRank pro-cess.Our method has the following properties: 1) asour system is based on a random walk algorithm, itdoes not require training model parameters ; 2) un-like previous PageRank based approaches in NED(Alhelbawy and Gaizauskas, 2014) which mainlyrely on global coherence, our method is able to bet-ter utilize the local similarity between a candidateand a KB node (Section 3); and 3) we tailor thePersonalized PageRank algorithm to only focus onone high-confidence entity at a time to reduce noise(Section 4).2 Related WorkEarly attempts at the NED tasks use local andsurface level information.
Bunescu and Pas?ca238(2006) first utilize information in a knowledge base(Wikipedia) to disambiguate names, by calculatingsimilarity between the context of a name mentionand the taxonomy of a KB node.Later research, such as Cucerzan (2007) andMilne and Witten (2008) extends this line by explor-ing richer feature sets, such as coherence featuresbetween entities.
Global coherence features havetherefore been widely used in NED research (seee.g.
(Ratinov et al, 2011), (Hoffart et al, 2011),and (Cheng and Roth, 2013)) and have been ap-plied successfully in TAC shared tasks (Cucerzan,2011).
These methods often involve optimizing anobjective function that contains both local and globalterms, and thus requires training on an annotated ordistantly annotated dataset.Our system performs collective NED using a ran-dom walk algorithm that does not require supervi-sion.
Random walk algorithms such as PageRank(Page et al, 1999) and Personalized PageRank (Jehand Widom, 2003) have been successfully appliedto NLP tasks, such as Word Sense Disambigua-tion (WSD: (Sinha and Mihalcea, 2007; Agirre andSoroa, 2009)).Alhelbawy and Gaizauskas (2014) successfullyapply the PageRank algorithm to the NED task.Their work is the closest in spirit to ours and per-forms well without supervision.
We try to furtherimprove their model by using a PPR model to bet-ter utilize local features, and by adding constraintsto the random walk to reduce noise.3 The Graph ModelWe construct a graph representation G(V,E) fromthe document D with pre-tagged named entity tex-tual mentions M = {m1, ...,mk}.
For each entitymention mi?
M there is a list of candidates in KBCi= {ci1, ..., cini}.
Vertices V are defined as pairsV = { (mi, cij) | mi?M, cij?
Ci},corresponding to the set of all possible KB candi-dates for different mentions in M .
Edges are undi-rected and exist between two vertices if the two can-didates are directly linked in the knowledge base, butno edge is allowed between candidates for the samenamed entity.
Every vertex (m, c) is associated withan initial similarity score between entity mention mand candidate c (Figure 1).United F.C.
is based in Lincolnshire and participatesin the sixth tier of English football.
The  strikerDevon White joined  this  football club in 1985.Devon_White(baseball), 0.5Lincoln_United_F.C.,0.5 Boston_United_F.C.,0.5Lincolnshire,0.4Boston, _Lincolnshire, 0.3Lincoln,_Lincolnshire, 0.3Devon_White(footballer), 0.5Figure 1: A toy document graph for three entitymentions: United F.C., Lincolnshire, Devon White.Candidates and their initial similarity scores are gen-erated for each entity mention.3.1 VerticesCandidates.
Given named entity mentionsM in thedocument, we need to generate all possible candi-dates for every mention m ?
M .
We first performcoreference resolution on the whole document andexpand m to the longest mention in the coreferencechain.
We then add a Wikipedia entry c to the can-didate set Cifor mention miif 1) the title of c is thesame as the expanded form of mi, or 2) string miredirects to page c, or 3) c appears in a disambigua-tion page with title mi.Initial Similarity.
Initial similarity iSim for ver-tex (m, c) describes how similar entity mention mto candidate c is.
It is independent from other candi-dates in the graph G. We experiment with the localmeasure (localSim), based on the local informationabout the entity in the text, and the global measure(popSim), based on the global importance of the en-tity.
Initial similarity scores of all candidates for asingle named entity mention are normalized to sumto 1.?
localSim: The local similarity score is producedby a MaxEnt model trained on the TAC2014EDL training data (LDC2014E15).
MaxEnt fea-tures include string similarity between the ti-tle of the Wikipedia entry and the entity men-tion, such as edit distance, whether the textmention starts or ends with the Wikipedia title,etc; and whether they have the same type (e.g.person, organization, location, etc).239?
popSim: We use the Freebase popularity as analternative similarity measure.
The Freebase pop-ularity is a function of entity?s incoming and out-going link counts in Wikipedia and Freebase.13.2 EdgesEdges in our graph model represent relations be-tween candidates.
We insert an edge between twocandidates if the Wikipedia entry corresponding toeither of the two candidates contains a link to theother candidate.
We assume that this relation is bidi-rectional and thus this edge is undirected.There is a toy document graph in Figure 1 withthree entity mentions and seven candidates: threecandidates generated for Lincolnshire, and two can-didates generated for United F.C.
and Devon Whiteeach.
Each graph node e(m, c) is a pair of an entitymentionm and a candidate c; every node is assignedan initial score, normalized across all candidates forthe same entity.
An edge is drawn between two can-didates for different entities whenever there is a linkfrom the Wikipedia page for one candidate to theWikipedia page for another.
There is no edge be-tween candidates competing for the same entity.4 The ChallengeA successful entity disambiguation algorithm wouldbenefit from both the initial similarity between can-didate and entity, as well as the coherence amongentities in the same document.
We assume that everyentity can refer to at most one in the list of possiblecandidates, so all candidates except for the correctone for each entity are erroneous and will introducenoise into the document graph.
Based on this ob-servation, we contend that the typical random walkapproach, which computes coherence of one candi-date to the whole graph, is not suitable for our sce-nario.
To address this problem, we propose to con-sider pairwise relations between every two nodes,given by PPR scores, compute the contribution ofevery node to the coherence of the other, and imposeaggregation constraints to avoid redundant contribu-tions.4.1 Personalized PageRankThe PageRank algorithm considers random walk ona graph, where at each step with probability  (tele-1https://developers.google.com/freebase/v1/searchport probability) we jump to a randomly selectednode on a graph, and with probability 1 ?
 we fol-low a random outgoing edge of the current node.Stationary distribution of this walk gives PageR-ank weights associated with each node.
Personal-ized PageRank is the same as PageRank, except thatall teleports are made to the same source node, forwhich we are personalizing the PageRank.4.2 Coherence and ConstraintsThe coherence of the node e to the graph G quan-tifies how well node e ?fits?
into this graph.
Intu-itively, pairwise weights PPR(s?
e) represent re-lationships between nodes in the graph: the higherthe weight is, the more relevant endpoint e is forthe source s. Candidate nodes in the graph havedifferent quality, measured by their initial similarityiSim.
Thus, coherence of the node e to the graph Gdue to the presence of node s is given bycohs(e) = PPR(s?
e) ?
iSim(s), (1)where relevance e for s is weighted by the iSim(s),which is the similarity between entity e and candi-date s. We experiment with a MaxEnt-trained lo-cal score and the Freebase popularity as the iSim inSection 5.We observe that summing the contributionscohs(e) for all nodes s?V would accumulate noise,and therefore impose two aggregation constraints totake into account this nature of document graph G.Namely, to compute coherence coh(e) of the nodee(m, c), corresponding to the entity mention m andthe candidate c, to the graph G we enforce:(c1) ignore contributions from candidate nodes com-peting for an entity m;(c2) take only one, highest contribution from candi-date nodes, competing for an entity m?6= m;The first constraint (c1) means that alternative candi-dates e?
(m, c?
), generated for the same entity mentionm, should not contribute to the coherence of e(m, c),as only one candidate per entity can be correct.
Forthe same reason the second constraint (c2) picks thesingle candidate node s(m?, c?)
for entity m?6= mwith the highest contribution cohs(e) towards e. Sothese constraints guarantee that exactly one and themost relevant candidate per entity will contribute240to the coherence of the node e. Thus, the set of con-tributors towards coh(e) is defined asCONTRe(m,c)={ (m?, argmaxccoh(m?,c)(e) ) ?V, m?6=m }(2)Then coherence of the node e to graph G is given bycoh(e) =?s?CONTRe(m,c)cohs(e)(3)Consider the example in Figure 1, whichhas two connected components.
CandidateDevon White (baseball) is disconnected from therest of the graph and can neither contribute towardsany other candidate nor get contributions from othernodes.
So its coherence is zero.
All other candidatesare connected, i.e.
belong to the same connectedcomponent.
Thus, the random walker, started fromany node in this component, will land at any othernode in this component with some positive likeli-hood.Let us consider the CONTRe(m,c)for en-tity mention m = Lincolnshire and candidatec = Lincolnshire, 0.4,.
Without our con-straints, nodes Devon White (footballer), 0.5,Lincoln United F.C., 0.5, Boston United F.C., 0.5,Lincoln Lincolnshire, 0.3,Boston Lincolnshire, 0.3can all potentially contribute towards coherence ofLincolnshire, 0.4.However, (c1) and (c2) will eliminate contri-bution from some of the candidates: Constraint(c1) does not allow Lincoln Lincolnshire, 0.3 andBoston Lincolnshire, 0.3 to contribute, because theycompete for the same entity mention as candidateLincolnshire, 0.4; constraint (c2) will allow only onecontribution from either Lincoln United F.C., 0.5or Boston United F.C., 0.5 whichever is bigger,since they compete for the same entity mentionUnited F.C..
Therefore, set CONTRe(m,c)for en-tity mention m = Lincolnshire and candidate c =Lincolnshire, 0.4,will contain only two contributors:candidate Devon White (footballer), 0.5, for entitymention Devon White, and exactly one of the candi-dates for entity mention United F.C.4.3 PPRSimOur goal is to find the best candidate for every entitygiven a candidate?s coherence and its initial similar-ity to the entity.
To combine the coherence scorecoh(e) with iSim(e), we weight the latter with anaverage value of PPR weights used in coherencecomputation (3) across all nodes in the documentgraph G(V,E):PPRavg=?e?V?s?CONTRePPR(s?
e)|V |(4)Thus, the final score for node e is a linear combina-tionscore(e) = coh(e) + PPRavg?
iSim(e) (5)If the document graph has no edges then PPRavgiszero and for any node e its coherence coh(e) is zeroas well.
In this case we set score(e) to its initialsimilarity iSim(e) for all nodes e in the graph G.Finally, PPRSim disambiguates entity mention mwith the highest scored candidate c ?
Cm:disambiguate(m) = argmaxc?Cmscore(m, c) (6)To resolve ties in (6) we pick a candidate with themost incoming wikipedia links.Thus, candidate Devon White (footballer), 0.5in Figure 1 will get higher overall score than its com-petitor, Devon White (baseball), 0.5.
Their initialscores are the same, 0.5, but the latter one is discon-nected from other nodes in the graph and thus hasa zero coherence.
So, entity mention Devon Whitewill be correctly disambiguated with the candidateDevon White (footballer), 0.5.
This candidate isdirectly connected to Boston United F.C., 0.5and has a shortest path of length 3 toLincolnshire United F.C., 0.5, and thereforecontributes more towards Boston United F.C., 0.5,and boosts its coherence to make it the cor-rect disambiguation for United F.C.
Similarly,Lincolnshire is correctly disambiguated withBoston Lincolnshire F.C., 0.3.5 Experiments and Results.Data.
For our experiments we use dataset AIDA2.All textual entity mentions are manually disam-biguated against Wikipedia links (Hoffart et al,2http://www.mpi-inf.mpg.de/yago-naga/aida/241Models Cucerzan Kulkarni Hoffart Shirakawa Alhelbawy iSim PPR PPRSimMicro 51.03 72.87 81.82 82.29 87.59 62.61 85.56 91.77Macro 43.74 76.74 81.91 83.02 84.19 72.21 85.86 89.89Table 1: Performance of PPRSim compared to baselines and state-of-the-art models on AIDA dataset.Baselines iSim and PPR choose a candidate with the highest initial similarity or coherence correspondingly.2011).
There are 34,965 annotated mentions in 1393documents.
Only mentions with a valid entry in theWikipedia KB are considered (Hoffart et al, 2011),resulting in a total of 27,816 mentions.
We use aWikipedia dump from June 14, 2014, as the refer-ence KB.
Our set of candidates is publicly availablefor experiments3.Evaluation.
We use two evaluation metrics: (1)Microaccuracy is the fraction of correctly disam-biguated entities; (2) Macroaccuracy is the propor-tion of textual mentions, correctly disambiguatedper entity, averaged over all entities.PPR.
We adopt the Monte Carlo approach (Fogarasand Racz, 2004) for computing Personalized PageR-ank.
It performs a number of independent randomwalks for every source node and takes an empiricaldistribution of ending nodes to obtain PPR weightswith respect to the source.
We initialized 2,000 ran-dom walks for every source node, performed 5 stepsof PPR, and computed PPR weights from all itera-tions dropping walks from the first one.
The teleportprobability is set to 0.2.Baselines.
We performed a set of experimentsusing initial similarity and Personalized PageRankweights.
Model iSim uses only Freebase scoresand achieves microaccuracy of 62.61% (Table 1).PPR model picks a candidate with highest coher-ence, computed in (3), where no initial similarity isused (iSim ?
1.0) and no constraints are applied.It has microaccuracy of 85.56%.
This is a strongbaseline, proving that coherence (3), solely basedon PPR weights, is very accurate.
We also reimple-mented the most recent state-of-the-art approach byAlhelbawy (2014) based on the PageRank.
We ranit on our set of candidates with freebase scores andgot 82.2% and 80.2% in micro- and macroaccuracycorrespondingy.3https://github.com/masha-p/PPRforNEDPPRSim Micro MacroiSim ?
1.0 85.56 85.86iSim = localSim 87.01 86.65iSim = popSim 90.26 88.98+(c1) 90.52 89.21+(c2) 91.68 89.78+(c1),(c2) 91.77 89.89Table 2: Performance of PPRSim with different ini-tial similarities and constraints.Results.
We observe that PPR combined withglobal similarity popSim achieves a microaccuracyof 90.2% (Table 2).
Adding constraints into thecoherence computation further improves the perfor-mance to 91.7%.
Interestingly, (c2) is more ac-curate than (c1).
When put together, (c1)+(c2)performs better than each individual constraint (Ta-ble 2).
Thus, combining coherence and initial sim-ilarity via (5) improves both micro- and macroac-curacy, outperforming state-of-the-art models (Ta-ble 1).6 Conclusion and Future WorkIn this paper we devise a new algorithm for collec-tive named entity disambiguation based on Person-alized PageRank.
We show how to incorporate pair-wise constraints between candidate entities by us-ing PPR scores and propose a new robust scheme tocompute coherence of a candidate entity to a doc-ument.
Our approach outperforms state-of-the-artmodels and opens up many opportunities to employpairwise information in NED.
For future work, weplan to explore other strategies and constraints fornoise reduction in the document graph.242ReferencesEneko Agirre and Aitor Soroa.
2009.
PersonalizingPageRank for word sense disambiguation.
In Proceed-ings of the 12th Conference of the European Chapterof the ACL, pages 33?41, Athens, Greece.Ayman Alhelbawy and Robert Gaizauskas.
2014.
GraphRanking for Collective Named Entity Disambiguation.In Proceedings of the 52nd Annual Meeting of theAssociation for Computational Linguistics (Volume 2:Short Papers), pages 75?80.Razvan Bunescu and Marius Pas?ca.
2006.
Using ency-clopedic knowledge for named entity disambiguation.In Proceedings of the 11th Conference of the EuropeanChapter of the Association for Computational Linguis-tics.Xiao Cheng and Dan Roth.
2013.
Relational inferencefor wikification.
In Proceedings of the 2013 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 1787?1796, Seattle, WA.Silviu Cucerzan.
2007.
Large-scale named entity dis-ambiguation based on Wikipedia data.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 708?716.Silviu Cucerzan.
2011.
Tac entity linking by perform-ing full-document entity extraction and disambigua-tion.
In Proceedings of the 2011 TAC Workshop, pages708?716.Fogaras and Racz.
2004.
Towards scaling fully person-alized page rank.
In Proceedings of the 3rd Workshopon Algorithms and Models for the Web-Graph (WAW),pages 105?117.Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino,Hagen F?urstenau, Manfred Pinkal, Marc Spaniol,Bilyana Taneva, Stefan Thater, and Gerhard Weikum.2011.
Robust disambiguation of named entities in text.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 782?792.Glen Jeh and Jennifer Widom.
2003.
Scaling personal-ized web search.
In Proceedings of the 12th Interna-tional Conference onWorld Wide Web, pages 271?279.James Mayfield.
2014.
Cold start knowledge base pop-ulation at tac 2014.
In Proceedings of the 2014 TACWorkshop.David Milne and Ian H. Witten.
2008.
Learning to linkwith wikipedia.
In Proceedings of the 17th ACM Con-ference on Information and Knowledge Management,pages 509?518.Lawrence Page, Sergey Brin, Rajeev Motwani, and TerryWinograd.
1999.
The pagerank citation ranking:Bringing order to the web.
Technical Report 1999-66,Stanford InfoLab.Lev Ratinov, Dan Roth, Doug Downey, and Mike An-derson.
2011.
Local and global algorithms for dis-ambiguation to wikipedia.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages1375?1384, Portland, OR.Ravi Sinha and Rada Mihalcea.
2007.
Unsupervisedgraph-basedword sense disambiguation using mea-sures of word semantic similarity.
In Proceedings ofthe International Conference on Semantic Computing,pages 363?369.243
