Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 22?25,Athens, Greece, 30 March, 2009. c?2009 Association for Computational LinguisticsLinguistics in Computational Linguistics:Observations and PredictionsHans UszkoreitLanguage Technology Lab, DFKI GmbHStuhlsatzenhausweg 3, D-66123 Saarbrueckenuszkoreit@dfki.deAbstractAs my title suggests, this position paper focuseson the relevance of linguistics in NLP instead ofasking the inverse question.
Although the ques-tion about the role of computational linguisticsin the study of language may theoretically bemuch more interesting than the selected topic, Ifeel that my choice is more appropriate for thepurpose and context of this workshop.This position paper starts with some retrospec-tive observations clarifying my view on the am-bivalent and multi-facetted relationship betweenlinguistics and computational linguistics as ithas evolved from both applied and theoreticalresearch on language processing.
In four briefpoints I will then strongly advocate a strength-ened relationship from which both sides benefit.First, I will observe that recent developments inboth deep linguistic processing and statisticalNLP suggest a certain plausible division of laborbetween the two paradigms.Second, I want to propose a systematic approachto research on hybrid systems which determinesoptimal combinations of the paradigms and con-tinuously monitors the division of labor as bothparadigm progress.
Concrete examples illustrat-ing the proposal are taken from our own re-search.Third, I will argue that a central vision ofcomputational linguistics is still alive, the dreamof a formalized reusable linguistic knowledgesource embodying the core competence of alanguage that can be utilized for wide range ofapplications.1 IntroductionComputational linguistics did not organicallygrow out of linguistics as a new branch of mathe-matical or applied linguistics.
Although the termsuggests the association with linguistics, in prac-tice much of CL has rather been purely engineer-ing-driven natural language processing.
Even ifcomputational linguistics has become a recognizedsubfield of linguistics, most of the action in CLdoes not address linguistic research questions.For most practitioners, the term was never morethan a sexy sounding synonym for natural lan-guage processing.
Many others, however, fortu-nately including many of the most creative andsuccessful scientists in CL, shared the ambition ofcontributing to the scientific study of human lan-guage.Already in the eighties Lauri Karttunen ob-served that there is a coexistence and mutual fer-tilization of applied computational linguistics andtheoretical computational linguistics, and that thelatter subarea can provide important insights intothe structure and use of human language.When we look into the actual relationship be-tween linguistics and CL, we can easily perceive anumber of changes that have happened over time.We can distinguish five major paradigms in com-putational linguistics, each of which has assigned aslightly different role to linguistic research.
Thefirst paradigm was the direct procedural implemen-tation of language processing.
NLP systems of thisparadigm were programs in languages such asFORTRAN, COBOL or assembler in which therewas no systematic division between linguisticknowledge and processing.
Linguistics was onlyimportant because it had educated some of thepractitioners on relevant properties of human lan-guage.The second paradigm was the development ofspecialized algorithms and methods for language22processing.
This paradigm includes for instanceparsing algorithms, finite-state parsers, ATNs,RTNs and augmented phrase-structure grammars.Although we find a separation between linguisticknowledge and processing components, none ofthe developed methods were imports from linguis-tics, nor were they adopted in linguistics.
(A nota-ble exception may have been two-level finite-statemorphology which at least caused some discussionin linguistic morphology.)
Nevertheless, some ofthe approaches required a certain level of linguisticsophistication.The third paradigm was the emergence of lin-guistic formalisms.
In the eighties a variety of newdeclarative grammatical formalisms such as HPSG,LFG, CCG, CUG had quite some influence on CL.These formal grammar models were accompaniedby semantic formalisms such as DRT.
A number ofthese formal models were tightly connected to lin-guistic theories and therefore also taught in linguis-tics curricula.
Several attempts to turn current ver-sions of the linguistic mainstream theory ofGB/P&P/minimalism into such a declarative for-malism were not very successful in NLP but stilldiscussed and used in linguistic classrooms.When these linguistic formalisms failed to meetthe performance criteria needed for realistic appli-cations, most of applied computational linguisticsresearch fell back on specialized methods for NLPsuch as finite state methods for information extrac-tion.
Other colleagues moved on to methods of thefourth paradigm in CL, i.e., statistical methods.Inspired by the rapid success of these statisticaltechniques, the new paradigm soon ruled most ofNLP research.
Not surprisingly, the distance be-tween linguistics and mainstream CL increased, asresearchers in most subareas researchers did nothave to know much about language and linguisticsin order to be successful in statistical NLP.Only when the success curve of statistical NLPstarted to flatten in several application areas, inter-est in linguistic methods and knowledge sourcesreawakened.
Hard core statistical NLP specialistsconsulted lexicons or tried to develop statisticalmodels on phrase structures.
Many statistical ap-proaches now exploit structured linguistic descrip-tions as obtained from treebanks and other linguis-tically annotated corpora.In the meantime, proponents of linguistic meth-ods had discovered the power of statistical modelsfor overcoming some of the performance limita-tions of deep NLP.
Statistical models trained ontreebanks have become the preferred method forsolving the massive ambiguity problem of deeplinguistic parsing.All these pragmatic mixes of statistical and lin-guistic methods marked the birth of the fifth para-digm in CL, the creative combination of statisticaland non-statistical machine learning approacheswith linguistic methods.2 Division of Labor between Linguisticsand StatisticsTo illustrate my view on the complementarycontributions of statistical and linguistic methods Iwant to start with three observations.
The first ob-servation stems from parser evaluations.
A CCGparser was successfully applied to the standardWall Street Journal test data within the Penn Tree-bank (refs).
Although the C&C parser did notquite get the same coverage as the best statisticalsystems, it produced very impressive results.
AsMark Steedman demonstrated in a talk at the Com-putational Linguistics session of the 2008 Interna-tional Congress of Linguists, the C&C parsermoreover found many dependencies needed forsemantic interpretation that are not even annotatedin the Penn Treebank.Observation two stems from our work on hybridmachine translation.
Within the EU project Euro-Matrix we are organizing open evaluation cam-paigns of MT systems by shared tasks whose re-sults are reported in the annual WMT workshops.The first large campaign combining automatic andintellectual evaluation took place in 2008.
Partici-pants could contribute translations of two test datasets for a range of language pairs.
One test set wasin a specific domain for which training data hadbeen provided.
The other test set contained newstexts on a variety of topics.
Although a training setof news texts had been provided as well, the cov-ered domains exhibited much more diversity thanthe closed domain texts.
It turned out that in gen-eral the best systems for the closed domain taskwere statistical MT systems, whereas the open do-main task was best solved by seasoned rule-basedMT commercial products.23A careful comparative study of errors made bysome of the best SMT and RBMT systems re-vealed that the errors of the two systems werelargely complementary.
As SMT can acquire fre-quently used expressions from training data, theoutput generally appears rather fluent, at least forshort sentences and short portions of sentences.SMT is also superior in lexical and phrasal disam-biguation and the optimal lexical choice in the tar-get language.
However, the translations exhibitmany syntactic problems such as missing verbs oragreement violations, especially if the target lan-guage has a complex morphology.
RBMT systems,on the other hand, usually get the syntactic struc-ture right?unless they fail in attachment ambigui-ties?but on the word and phrase level they often donot select the correct or stylistically optimal trans-lations.Today's machine learning methods for acquiringthe statistical translation models from parallel textsfail on many syntactic phenomena that can be ana-lyzed correctly by a linguistic grammar.
Inducing acorrect treatment of long distance phenomena suchas topicalization or "easy"-adjectives, ellipsis andcontrol phenomena from unannotated texts seemsquite impossible.
Learning complex rules fromsyntactically and semantically annoted texts maybe possible if linguists have already understoodand formalized the underlying analysis of the phe-nomenon.The third observation comes from supervisingwork in grammar development and attempts to en-large the coverage of existing grammars automati-cally through the exploitation of corpus data.When he tried to extend the coverage of the ERG,Zhang Yi could show that almost all of the cover-age gaps could be attributed to missing lexicalknowledge.
Even if the words in the unanalyzablesentence were all in the lexicon, usually somereading of words, i.e.
their membership in someadditional word class, was missing.
The few re-maining coverage deficits result from specific in-frequent constructions not yet covered by thegrammar plus missing treatments for a few notori-ously tricky syntactic problems such as certaintypes of ellipsis.These three observations together with numer-ous others strongly suggest the following insight.Every grammar of a human language consists of asmall set of highly complex regularities and of ahuge set of much less complex phenomena.
Thesmall set of highly complex phenomena occursmuch more often than most of the phenomena oflittle complexity.
This slanted distribution makeslanguage learnable.
So far we have no automaticlearning methods that could correctly induce thecomplex phenomena.
It is highly questionablewhether these regularities could ever be inducedwithout full access to the syntax-semantics map-pings that the human language learner exploits.On the other hand, the lexicon or simple selec-tional restrictions can easily be learned because thecomplexity lies in the structure of the lexicalclasses and not in the simple mapping from wordsto these classes.3 Hybrid Systems ResearchIn several areas of language processing, first ap-proaches of designing hybrid systems containingboth linguistic and statistical components havedemonstrated promising results.However, much of this research is based onrather opportunistic selections.
Readily availablecomponents are connected in a pure trial and errorfashion.
In our hybrid MT research we are sys-tematically searching for optimal combinations ofthe best statistical and the best rule-based systemsfor a given language pair.
The approach is system-atic, because we use a detailed error analysis byskilled linguists to find out which classes ofphrases are usually better translated by the beststatistical systems.
We then insert the translationsfor such kinds of phrases into the syntactic skele-tons of the translated sentences provided by therule-base system.
One of the translations we sub-mitted to this year?s EuroMatrix evaluation cam-paign was obtained in this way.24The technique to merge sentence parts from thetwo systems into one translation is only a crudefirst approximation of a truly hybrid processingsystem, i.e., a system in which the statistical phrasetranslation is fully integrated into the rule-basedsystem.
Our goal is to test the usefulness of statis-tical methods in analysis, especially for disam-biguation, in transfer, especially for selecting thebest translation for words and smaller phrases andin generation, for the selection among paraphrasesaccording to monolingual language models.Another systematic approach to hybrid systemsdesign was investigated in the Norwegian LOGONproject, in which deep linguistic processing byHPSG and LFG was complemented by statisticalmethods.Another example for a systematic approach tohybrid systems building is our work on an architec-ture for the combination of components for theanalysis of texts.
The DFKI platform Heart-of-Gold (HoG) was especially designed for this pur-pose.
In HoG several components can be combinedin multiple ways.
All processing components writetheir analysis results into a multi-layer XML stand-off annotation of the analyzed text.
The actual in-terface language is RMRS (Robust Minimal Re-cursion Semantics, ref.)
XML is just used as thesyntactic carrier language for RMRS.4 Computational Models of LinguisticCompetenceAlthough the competence-performance distinc-tion is a complex and highly controversial issue,the theoretical dichotomy is useful for the argu-ment I want to make.
When children acquire a lan-guage, they first learn to comprehend and producespoken utterances.
Much later they learn to readand to write, and much later again they may learnhow to sing and rhyme and how to summarize,translate and proofread texts.All of the acquired types of performance utilizetheir underlying linguistic competence.
New typesof performance are relatively easy to learn.
Theshared knowledge base ensures a useful level ofconsistency across the performance skills.
Ofcourse, each type of performance may use differentparts of the shared competence.
Certain types ofperformance may also extend the shared base intodifferent directions.The child could not acquire the complex map-ping between sound and meaning without havingaccess to both spoken (and later also written) formand the corresponding semantics.
Therefore thechild cannot learn a language from a radio besideher crib, nor can the older child acquire Chinese bybeing locked up in a library of Chinese books.Thus the basic competence cannot be obtained out-side performance or successful communication.The first approaches to linguistic computationalgrammars may have been too simplistic by notproviding the connection between competence andperformance needed for exploiting the competencebase in realistic applications.
However, in gradu-ally solving the problems of efficiency, robustnessand coverage researchers have arrived at more so-phisticated views of deep linguistic processing.After several decades of experience in workingon competence and performance modeling for bothgeneric grammatical resources and many special-ized applications, I am fully convinced that thegoal of a reusable shared competence model forevery surviving language in our global digital in-formation and communication structure is still aworthwhile and central goal of computational lin-guistics.
I am also certain that the goal will be ob-tained in many steps.
We already witness a reuseof large computational grammar resources such asthe HPSG ERG, the LFG ParGram Grammar andthe English CCG in many different applications.These applications are still experimental but whendeep linguistic processing keeps improving in effi-ciency, specificity (ability to select among read-ings), robustness and coverage at current speed ofprogress, we will soon see first cases of real lifeapplications.I am not able to predict the respective propor-tions of the intellectually designed core compo-nents, the components learned automatically fromlinguistically annotated data and the componentsautomatically learned from unannotated data but Iam convinced that the systematic search for thebest combinations will be central to partially real-izing the dream of computational linguistics stillwithin our life times.If such solutions can be found and gradually im-proved, the insights gained through this systematicinvestigation may certainly also have a strong im-pact in the other direction, i.e.
from computationallinguistics into linguistics.25
