Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 391?398,New York, June 2006. c?2006 Association for Computational LinguisticsCreating a Test Collection for Citation-based IR ExperimentsAnna RitchieUniversity of CambridgeComputer Laboratory15 J J Thompson AvenueCambridge, CB3 0FD, U.K.ar283@cl.cam.ac.ukSimone TeufelUniversity of CambridgeComputer Laboratory15 J J Thompson AvenueCambridge, CB3 0FD, U.K.sht25@cl.cam.ac.ukStephen RobertsonMicrosoft Research LtdRoger Needham House7 J J Thomson AvenueCambridge, CB3 0FB, U.K.ser@microsoft.comAbstractWe present an approach to building a testcollection of research papers.
The ap-proach is based on the Cranfield 2 tests butuses as its vehicle a current conference;research questions and relevance judge-ments of all cited papers are elicited fromconference authors.
The resultant test col-lection is different from TREC?s in thatit comprises scientific articles rather thannewspaper text and, thus, allows for IRexperiments that include citation informa-tion.
The test collection currently con-sists of 170 queries with relevance judge-ments; the document collection is the ACLAnthology.
We describe properties ofour queries and relevance judgements, anddemonstrate the use of the test collectionin an experimental setup.
One potentiallyproblematic property of our collection isthat queries have a low number of relevantdocuments; we discuss ways of alleviatingthis.1 IntroductionWe present a methodology for creating a test collec-tion of scientific papers that is based on the Cran-field 2 methodology but uses a current conference asthe main vehicle for eliciting relevance judgementsfrom users, i.e., the authors.Building a test collection is a long and expensiveprocess but was necessary as no ready-made test col-lection existed on which the kinds of experimentswith citation information that we envisage could berun.
We aim to improve term-based IR on scien-tific articles with citation information, by using in-dex terms from the citing article to additionally de-scribe the cited document.
Exactly how to do this isthe research question that our test collection shouldhelp to address.This paper is structured as follows: Section 2 mo-tivates our proposed experiments and, thereby, ourtest collection.
Section 3 discusses the how test col-lections are built and, in particular, our own.
Sec-tion 4 briefly describes the practicalities of compil-ing the document collection and the processing weperform to prepare the documents for our experi-ments.
In Section 5, we show that our test collectioncan be used with standard IR tools.
Finally, Sec-tion 6 discusses the problem of the low number ofrelevant documents judged so far and two ways ofalleviating this problem.2 MotivationThe idea of using terms external to a document,coming from a ?citing?
document, has been bor-rowed from web-based IR.
When one paper citesanother, a link is made between them and this linkstructure is analogous to that of the web: ?hyper-links ... provide semantic linkages between ob-jects, much in the same manner that citations linkdocuments to other related documents?
(Pitkow andPirolli, 1997).
Link structure, particularly anchortext, has been used to advantage in web-based IR.While web pages are often poorly self-descriptive(Brin and Page, 1998) anchor text is often a higher-level description of the pointed-to page.
(Davison,3912000) provides a good discussion of how well an-chor text does this and provides experimental resultsin support.
Thus, beginning with (McBryan, 1994),there is a trend of propagating anchor text along itshyperlink to associate it with the linked page, as wellas the page in which it is found.
Google, for ex-ample, includes anchor text as index terms for thelinked page (Brin and Page, 1998).
The TREC Webtracks have also shown that using anchor text im-proves retrieval effectiveness for some search tasks(Hawking and Craswell, 2005).This idea has already been applied to citations andscientific articles (Bradshaw, 2003).
In Bradshaw?sexperiment, scientific documents are indexed by thetext that refers to them in documents that cite them.However, unlike in experiments with previous col-lections, we need both the citing and the cited articleas full documents in our collection.
The question ofhow to identify citation ?anchor text?
and its extentis a matter for research; this requires the full text ofthe citing article.
Previous experiments and test col-lections have had only limited access to the contentof the citing article: Bradshaw had access only to afixed window of text around the citation, as providedby CiteSeer?s ?citation context?
; in the GIRT collec-tions (Kluck, 2003), a dozen or so content-bearinginformation fields (e.g., title, abstract, methodologi-cal descriptors) represent each document and the fulltext is not available.
Additionally, in Bradshaw?s ex-periment, no access is given to the text of the citedarticle itself so that the influence of a term-based IRmodel cannot be studied and so that documents canonly be indexed if they have been cited at least once.A test collection containing full text for many cit-ing and cited documents, thus, has advantages froma methodological point of view.2.1 Choosing a GenreWhen choosing a scientific field to study, we lookedfor one that is practicable for us to compile the doc-ument collection (freely available machine-readabledocuments; as few as possible document styles),while still ensuring good coverage of research top-ics in an entire field.
Had we chosen the medicalfield or bioinformatics, the prolific number of jour-nals would have been a problem for the practicaldocument preparation.We also looked for a relatively self-containedfield.
As we aim to propagate referential text to citedpapers as index terms, references from documentsin the collection to other documents within the col-lection will be most useful.
We call these internalreferences.
While it is impossible to find or createa collection of documents with only internal refer-ences, we aim for as high a proportion of internalreferences as possible.We chose the ACL (Association for Computa-tional Linguistics) Anthology1 , a freely availabledigital archive of computational linguistics researchpapers.
Computational linguistics is a small, ho-mogenous research field and the Anthology containsthe most prominent publications since the beginningof the field in 1960, consists of only 2 journals, 7conferences and 5 less important publications, suchas discontinued conferences and a series of work-shops, resulting in only 7000 papers2.With the ACL Anthology, we expect a high pro-portion of internal references within a relativelycompact document collection.
We empirically mea-sured the proportion of collection-internal refer-ences.
We found a proportion of internal refer-ences to all references of 0.33 (the in-factor).
Wewanted to compare this number to a situation inanother, larger field (genetics) but no straightfor-ward comparison is possible, as there are very manygenetics journals and quality of journals probablyplays a larger role in a bigger field.
We tried tosimulate a similar collection to the 9 main jour-nals+conferences in the Anthology, by considering10 journals in genetics with a range of impact fac-tors3, resulting in an in-factor of 0.17 (dropping to0.14 if only 5 journals are considered).
Thus, ourhypothesis that the Anthology is reasonably self-contained, at least in comparison with other possiblecollections, was confirmed.The choice of computational linguistics has theadded benefit that we are familiar with the domain;we can interpret the subject matter better than wewould be able to in the medical domain.
This shouldbe of use to us in our eventual experiments.1http://www.aclweb.org/anthology/2This is our estimate, after substracting non-papers such asletters to the editor, tables of contents etc.
The Anthology isgrowing by ?500 papers per year.3Journal impact factor is a measure of the frequency withwhich its average article is cited and is a measure of the relativeimportance of journals within a field (Garfield, 1972).3923 Building Test CollectionsTo turn our document collection into a test col-lection, a parallel set of search queries and rele-vance judgements is needed.
There are a numberof alternative methods for building a test collec-tion.
For TREC, humans devise queries specificallyfor a given set of documents and make relevancejudgements on pooled retrieved documents from thatset (Harman, 2005).
Theirs is an extremely labour-intensive and expensive process and an unrealisticoption in the context of our project.The Cranfield 2 tests (Cleverdon et al, 1966) in-troduced an alternative method for creating a testcollection, specifically for scientific texts.
Themethod was subject to criticism and has not beenemployed much since.
Nevertheless, we believe thismethod to be worth revisiting for our current situa-tion.
In this section, we describe in turn the Cran-field 2 method and our adapted method.
We discusssome of the original criticisms and their bearing onour own work, then describe our returns thus far.3.1 The Cranfield 2 Test CollectionThe Cranfield 2 tests (Cleverdon et al, 1966) werea comparative evaluation of indexing language de-vices.
From a base collection of 182 (high speedaerodynamics and aircraft structures) papers, theCranfield test collection was built by asking the au-thors to formulate the research question(s) behindtheir work and to judge how relevant each referencein their paper was to each of their research questions,on a 5-point scale.
Referenced documents were ob-tained and added to the base set.
Authors were alsoasked to list additional relevant papers not cited intheir paper.
The collection was further expandedin a second stage, using bibliographic coupling tosearch for similar papers to the referenced ones andemploying humans to search the collection for otherrelevant papers.
The resultant collection comprised1400 documents and 221 queries (Cleverdon, 1997).The principles behind the Cranfield technique are:?
Queries: Each paper has an underlying researchquestion or questions; these constitute validsearch queries.?
Relevant documents: A paper?s reference list isa good starting point for finding papers relevantto its research questions.?
Judges: The paper author is the person bestqualified to judge relevance.3.2 Our Anthology Test CollectionWe altered the Cranfield design to fit to a fixed,existing document collection.
We designed ourmethodology around an upcoming conference andapproached the paper authors at around the time ofthe conference, to maximize their willingness to par-ticipate and to minimise possible changes in theirperception of relevance since they wrote the paper.Due to the relatively high in-factor of the collection,we expected a significant proportion of the relevancejudgements gathered in this way to be about Anthol-ogy documents and, thus, useful as evaluation data.Hence, the authors of accepted papers for ACL-2005 and HLT-EMNLP-2005 were asked, by email,for their research questions and relevance judge-ments for their references.
We defined a 4-pointrelevance scale, c.f.
Table 1, since we felt that thedistinctions between the Cranfield grades were notclear enough to warrant 5.
Our guidelines also in-cluded examples of referencing situations that mightfit each category.
Personalized materials for partic-ipation were sent, including a reproduction of theirpaper?s reference list in their response form.
Thismeant that invitations could only be sent once thepaper had been made available online.We further deviated from the Cranfield methodol-ogy by deciding not to ask the authors to try to listadditional references that could have been includedin their reference list.
An author?s willingness toname such references will differ more from authorto author than their naming of original references, asreferencing is part of a standardized writing process.By asking for this data, the consistency of the dataacross papers will be degraded and the status of anyadditional references will be unclear.
Furthermore,feedback from an informal pilot study conducted onten paper authors confirmed that some authors foundthis task particularly difficult.Each co-author of the papers was invited individu-ally to participate, rather than inviting the first authoralone.
This increased the number of invitations thatneeded to be prepared and sent (by a factor of around2.5) but also increased the likelihood of getting a re-turn for a given paper.
Furthermore, data from mul-tiple co-authors of the same paper can be used to393Grade Description4 The reference is crucially relevant to the problem.
Knowledge of the contents of the referred work will be fun-damental to the reader?s understanding of your paper.
Often, such relevant references are afforded a substantialamount of text in a paper e.g., a thorough summary.3 The reference is relevant to the problem.
It may be helpful for the reader to know the contents of the referred work,but not crucial.
The reference could not have been substituted or dropped without making significant additions tothe text.
A few sentences may be associated with the reference.2 The reference is somewhat (perhaps indirectly) relevant to the problem.
Following up the reference probably wouldnot improve the reader?s understanding of your paper.
Alternative references may have been equally appropriate(e.g., the reference was chosen as a representative example from a number of similar references or included in alist of similar references).
Or the reference could have been dropped without damaging the informativeness of yourpaper.
Minimal text will be associated with the reference.1 The reference is irrelevant to this particular problem.Table 1: Relevance Scalemeasure co-author agreement on the relevance task.This is an interesting research question, as it is notat all clear how much even close collaborators wouldagree on relevance, but we do not address this here.We plan to expand the collection in a secondstage, in line with the Cranfield 2 design.
We willreapproach contributing authors after obtaining re-trieval results on our collection (e.g., with a stan-dard IR engine) and ask them to make additional rel-evance judgements on these papers.3.3 Criticisms of Cranfield 2Both Cranfield 1 (Cleverdon, 1960) and 2 were sub-ject to various criticisms; (Spa?rck Jones, 1981) givesan excellent account of the tests and their criticisms.The majority were criticisms of the test collectionparadigm itself and are not pertinent here.
How-ever, the source-document principle (i.e., the use ofqueries created from documents in the collection) at-tracted particular criticisms.
The fundamental con-cern was that the way in which the queries were cre-ated led to ?an unnaturally close relation?
betweenthe terms in the queries and those used to indexthe documents in the colection (Vickery, 1967); anysuch relationship might have created a bias towardsa particular indexing language, distorting the com-parisons that were the goal of the project.In Cranfield 1, system success was measuredby retrieval of source documents alone, criticizedfor being an over-simplification and a distortion of?real-life?
searching.
The evaluation procedure waschanged for Cranfield 2 so that source documentswere excluded from searches and, instead, retrievalof other relevant documents was used to measuresuccess.
This removed the problem that, usually,when a user searches, there is no source documentfor their query.
Despite this, Vickery notes that therewere ?still verbal links between sought documentand question?
in the new method: each query authorwas asked to judge the relevance of the source doc-ument?s references and ?the questions ... were for-mulated after the cited papers had been read and haspossibly influenced the wording of his question?.While adapting the Cranfield 2 method to ourneeds, we have tried to address some of the crit-icisms, e.g., that authors?
relevance judgementschange over time.
Nevertheless, we still havesource-document queries and must consider the as-sociated criticisms.
Firstly, our test collection isnot intended for comparisons of indexing languages.Rather, we aim to compare the effect of adding ex-tra index terms to a base indexing of the documents.The source documents will have no influence onthe base indexing of a document above that of theother documents.
The additional index terms, com-ing from citations to that document, will generallybe ?chosen?
by someone other than the query author,with no knowledge of the query terms4.
Also, ourdocuments will be indexed fully automatically, fur-ther diminishing the scope of any subconscious hu-man influence.Thus, we believe that the suspect relationship be-tween queries and indexing is negligible in the con-4The exception to this is self-citation.
This (very indirectly)allows the query author to influence the indexing but it seemshighly improbable that an author would be thinking about theirquery whilst citing a previous work.394text of our work, as opposed to the Cranfield tests,and that the source-document principle is sound.3.4 Returns and AnalysisOut of around 500 invitations sent to conference au-thors, 85 resulted in research questions with rele-vance judgements being returned; 235 queries in to-tal.
Example queries are:?
Do standard probabilistic parsing techniques,developed for English, fare well for French anddoes lexicalistion help improve parsing results??
Analyze the lexical differences between gendersengaging in telephone conversations.Of the 235 queries, 18 were from authors whoseco-authors had also returned data and were dis-carded (for retrieval purposes); we treat co-authordata on the same paper as ?the same?
and keeponly the first authors?.
47 queries had no relevantAnthology-internal references and were discarded.Another 15 had only relevant Anthology referencesnot yet included in the archive5; we keep these forthe time being.
This leaves 170 unique queries withat least 1 relevant Anthology reference and an aver-age of 3.8 relevant Anthology references each.
Theaverage in-factor across queries is 0.42 (similar toour previously estimated Anthology in-factor)6 .Our average number of judged relevant docu-ments per query is lower than for Cranfield, whichhad an average of 7.2 (Spa?rck Jones et al, 2000).However, this is the final number for the Cran-field collection, arrived at after the second stageof relevance judging, which we have not yet car-ried out.
Nevertheless, we must anticipate a po-tentially low number of relevant documents perquery, particularly in comparison to, e.g., the TRECad hoc track (Voorhees and Harman, 1999), with86.8 judged relevant documents per query.4 Document Collection and ProcessingThe Anthology documents are distributed in PDF, aformat designed to visually render printable docu-ments, not to preserve editable text.
So the PDF col-lection must be converted into a fully textual format.5HLT-NAACL-2004 papers, e.g., are listed as ?in process?.6We cannot directly compare this to Cranfield?s in-factor aswe do not have access to the documents.IXML XML XMLPDF XMLStructurePresentational StructureLogicalPre?ProcesssorPTX TemplatePTX List ParserReference ProcessorCitationXML+ ReferenceList + CitationsOmniPageFigure 1: Document Processing PipelineA pipeline of processing stages has been developedin the framework of a wider project, illustrated inFigure 1.Firstly, OmniPage Pro 147, a commercial PDFprocessing software package, scans the PDFs andproduces an XML encoding of character-level pagelayout information.
AI algorithms for heuristicallyextracting character information (similar to OCR)are necessary since many of the PDFs were createdfrom scanned paper-copies and others do not containcharacter information in an accessible format.The OmniPage output describes a paper as textblocks with typesetting information such as font andpositional information.
A pre-processor (Lewin etal., 2005) filters and summarizes the OmniPage out-put into Intermediate XML (IXML), as well as cor-recting certain characteristic errors from that stage.A journal-specific template converts the IXML to alogical XML-based document structure (Teufel andElhadad, 2002), by exploiting low-level, presenta-tional, journal-specific information such as font sizeand positioning of text blocks.Subsequent stages incrementally add more de-tailed information to the logical representation.
Thepaper?s reference list is annotated in more detail,marking up individual references, author names, ti-tles and years of publication.
Finally, a citation pro-cessor identifies and marks up citations in the doc-ument body and their constituent parts, e.g., authornames and years.5 Preliminary ExperimentationWe expect that our test collection, built for our cita-tion experiments, will be of wider value and we in-tend to make it publicly available.
As a sanity checkon our data so far, we carried out some preliminaryexperimentation, using standard IR tools: the LemurToolkit8, specifically Indri (Strohman et al, 2005),7http://www.scansoft.com/omnipage/8http://www.lemurproject.org/395its integrated language-model based search engine,and the TREC evaluation software, trec eval9.5.1 Experimental Set-upWe indexed around 4200 Anthology documents.This is the total number of documents that have, atthe time of writing, been processed by our pipeline(24 years of CL journal, 25 years of ACL proceed-ings, 14 years of assorted workshops), plus another?90 documents for which we have relevance judge-ments that are not currently available through theAnthology website but should be incorporated intothe archive in the future.
The indexed documents donot yet contain annotation of the reference list or ci-tations in text.
19 of our 170 queries have no relevantreferences in the indexed documents and were notincluded in these experiments.
Thus, Figure 2 showsthe distribution of queries over number of relevantAnthology references, for a total of 151 queries.Our Indri index was built using default parameterswith no optional processing, e.g., stopping or stem-ming, resulting in a total of 20117410 terms, 218977unique terms and 2263 ?frequent?10 terms.We then prepared an Indri-style query file fromthe conference research questions.
The Indri querylanguage is designed to handle highly complexqueries but, for our very basic purposes, we createdsimple bag-of-words queries by stripping all punctu-ation from the natural language questions and usingIndri?s #combine operator over all the terms.
Thismeans Indri ranks documents in accordance withquery likelihood.
Again, no stopping or stemmingwas applied.Next, the query file was run against the Anthologyindex using IndriRunQuery with default parametersand, thus, retrieving 1000 documents for each query.Finally, for evaluation, we converted the Indri?sranked document lists to TREC-style top results fileand the conference relevance judgements compiledinto a TREC-style qrels file, including only judge-ments corresponding to references within the in-dexed documents.
These files were then input totrec eval, to calculate precision and recall metrics.9http://trec.nist.gov/trec eval/trec eval.8.0.tar.gz10Terms that occur in over 1000 documents.2 3 4 5 6 7 8 9 10 11 12 13 14Threshhold (# Relevant References in Index)00.050.10.150.2Precisionat5 DocumentsFigure 3: Effect of Thresholding on P at 5 Docs5.2 Results and DiscussionOut of 489 relevant documents, 329 were retrievedwithin 1000 (per query) documents.
The mean av-erage precision (MAP) was 0.1014 over the 151queries.
This is the precision calculated at each rele-vant document retrieved (0.0, if that document is notretrieved), averaged over all relevant documents forall queries, i.e., non-interpolated.
R-precision, theprecision after R (the number of relevant documentsfor a query) documents are returned, was 0.0965.The average precision at 5 documents was 0.0728.We investigated the effect of excluding querieswith lower than a threshold number of judged rel-evant documents.
Figure 3 shows that precision at5 documents increases as greater threshold valuesare applied.
Similar trends were observed with otherevaluation measures, e.g., MAP and R-precision in-creased to 0.2018 and 0.1528, respectively, whenonly queries with 13 or more relevant documentswere run, though such stringent thresholding doesresult in very few queries.
Nevertheless, these trendsdo suggest that the present low number of relevantdocuments has an adverse effect on retrieval resultsand is a potential problem for our test collection.We also investigated the effect of including onlyauthors?
main queries, as another potential way ofobjectively constructing a ?higher quality?
query set.Although, this decreased the average in-factor of rel-evant references, it did, in fact, increase the averageabsolute number of relevant references in the index.Thus, MAP increased to 0.1165, precision at 5 doc-uments to 0.1016 and R-precision to 0.1201.These numbers look poor in comparison to theperformance of IR systems at TREC but, impor-tantly, they are not intended as performance results.Their purpose is to demonstrate that such numberscan be produced using the data we have collected,396(a) (b)0 20 40 60 80 100 120 140Query0102030# Relevant ReferencesTotalAnthology Index1 2 3 4 5 6 7 8 9 10 11 12 13 14 15# Relevant References in Index0102030# Queries(a) (b)Figure 2: (a) Relevant References Per Query and (b) Distribution of Queries over Number of Relevant Referencesrather than to evaluate the performance of some newretrieval system or strategy.A second point for consideration follows directlyfrom the first: our experiments were carried outon a new test collection and ?different test collec-tions have different intrinsic difficulty?
(Buckleyand Voorhees, 2004).
Thus, it is meaningless tocompare statistics from this data (from a differentdomain) to those from the TREC collections, wherequeries and relevance judgements were collected ina different way, and where there are very many rele-vant documents.Thirdly, our experiments used only the most basictechniques and the results could undoubtedly be im-proved by, e.g., applying a simple stop-list.
Never-theless, this notion of intrinsic difficulty means thatit may be the case that evaluations carried out on thiscollection will produce characteristically low preci-sion values.Low numbers do not necessarily preclude ourdata?s usefulness as a test collection, whose purposeis to facilitate comparative evaluations.
(Voorhees,1998) states that ?To be viable as a laboratory tool,a [test] collection must reliably rank different re-trieval variants according to their true effectiveness?and defends the Cranfield paradigm (from criticismsbased on relevance subjectivity) by demonstratingthat the relative performance of retrieval runs is sta-ble despite differences in relevance judgements.
Theunderlying principle is that it is not the absolute pre-cision values that matter but the ability to comparethese values for different retrieval techniques or sys-tems, to investigate their relative benefits.
A test col-lection with low precision values will still allow this.It is known that all evaluation measures are un-stable for very small numbers of relevant documents(Buckley and Voorhees, 2000) and there are issuesarising from incomplete relevance information in atest collection (Buckley and Voorhees, 2004).
Thismakes the second stage of our test collection com-pilation even more indispensable (asking subjects tojudge retrieved documents), as this will increase thenumber of judged relevant documents, as well asbridging the completeness gap.There are further possibilities of how the prob-lem could be countered.
We could exclude querieswith lower than a threshold number of relevant docu-ments (after the second stage).
Given the respectablenumber of queries we have, we might be able to af-ford this luxury.
We could add relevant documentsfrom outside the Anthology to our collection.
Thisis least preferable methodologically: using the An-thology has the advantage that it has a real identityand was created for real reasons outside our experi-ments.
Furthermore, the collection ?covers a field?,i.e., it includes all important publications and onlythose.
By adding external documents to the collec-tion, it would lose both these properties.6 Conclusions and Future WorkWe have presented an approach to building a testcollection from an existing collection of research pa-pers and described the application of our methodto the ACL Anthology.
We have collected 170queries with relevance data, centered around theACL-2005 and HLT-EMNLP-2005 conferences.
We397have sanity-checked the usability of our data byrunning the queries through a retrieval system andevaluating the results using standard software.
Thecollection currently has a low number of judgedrelevant documents and further experimentation isneeded to determine if this poses a real problem.We plan a second stage of collecting relevancejudgements, in line with the original Cranfield de-sign, whereby authors who have contributed querieswill be asked to judge the relevance of documents inretrieval rankings from standard IR models and, ide-ally, from our eventual citation-based experiments.Nevertheless, our test collection is likely to sufferfrom incomplete relevance information.
The bprefmeasure (Buckley and Voorhees, 2004) gauges re-trieval effectiveness solely on the basis of judgeddocuments and is more stable to differing levelsof completeness than measures such as MAP, R-precision or precision at fixed document cutoffs.Thus, bpref may offer a solution to the incomplete-ness problem and we intend to investigate its poten-tial use in our future evaluations.When finished, we hope our test collection willbe a generally useful IR resource.
In particular, weexpect the collection to be useful for experimenta-tion with citation information, for which there is cur-rently no existing test collection with the propertiesthat ours offers.Acknowledgements Thanks to the reviewers fortheir useful comments and to Karen Spa?rck Jones formany instructive discussions.ReferencesShannon Bradshaw.
2003.
Reference directed indexing:Redeeming relevance for subject search in citation in-dexes.
In Research and Advanced Technology for Dig-ital Libraries (ECDL), pages 499?510.Sergey Brin and Lawrence Page.
1998.
The anatomy ofa large-scale hypertextual Web search engine.
Com-puter Networks and ISDN Systems, 30(1?7):107?117.Chris Buckley and Ellen Voorhees.
2000.
Evaluatingevaluation measure stability.
In Research and Devel-opment in Information Retrieval (SIGIR).Chris Buckley and Ellen Voorhees.
2004.
Retrieval eval-uation with incomplete information.
In Research anddevelopment in information retrieval (SIGIR).Cyril Cleverdon, Jack Mills, and Michael Keen.
1966.Factors determining the performance of indexingsytems, volume 1. design.
Technical report, ASLIBCranfield Project.Cyril Cleverdon.
1960.
Report on the first stage of an in-vestigation into the comparative efficiency of indexingsystems.
Technical report, ASLIB Cranfield Project.Cyril Cleverdon.
1997.
The Cranfield tests on index lan-guage devices.
In Readings in information retrieval,pages 47?59.
Morgan Kaufmann Publishers Inc.Brian D. Davison.
2000.
Topical locality in the web.In Research and Development in Information Retrieval(SIGIR), pages 272?279.Eugene Garfield.
1972.
Citation analysis as a tool injournal evaluation.
Science, 178 (4060):471?479.Donna Harman.
2005.
The TREC test collections.
InEllen Voorhees and Donna Harman, editors, TRECExperiment and Evaluation in Information Retrieval,chapter 2.
MIT Press.David Hawking and Nick Craswell.
2005.
The verylarge collection and web tracks.
In Ellen Voorhees andDonna Harman, editors, TREC: Experiment and Eval-uation in Information Retrieval, chapter 9.
MIT Press.Michael Kluck.
2003.
The GIRT data in the evaluationof CLIR systems - from 1997 until 2003.
In CLEF,pages 376?390.Ian Lewin, Bill Hollingsworth, and Dan Tidhar.
2005.Retrieving hierarchical text structure from typeset sci-entific articles - a prerequisite for e-science text min-ing.
In UK e-Science All Hands Meeting.Oliver McBryan.
1994.
GENVL and WWWW: Toolsfor taming the web.
In World Wide Web Conference.James Pitkow and Peter Pirolli.
1997.
Life, death, andlawfulness on the electronic frontier.
In Human Fac-tors in Computing Systems.Karen Spa?rck Jones, Steve Walker, and Stephen Robert-son.
2000.
A probabilistic model of information re-trieval: development and comparative experiments -parts 1 and 2.
Information Processing and Manage-ment, 36(6):779?840.Karen Spa?rck Jones.
1981.
The Cranfield tests.
InKaren Spa?rck Jones, editor, Information Retrieval Ex-periment, chapter 13, pages 256?284.
Butterworths.Trevor Strohman, Donald Metzler, Howard Turtle, andW.
Bruce Croft.
2005.
Indri: a language-model basedsearch engine for complex queries.
Technical report,University of Massachusetts.Simone Teufel and Noemie Elhadad.
2002.
Collectionand linguistic processing of a large-scale corpus ofmedical articles.
In Language Resources and Evalu-ation Conference (LREC).B.
C. Vickery.
1967.
Reviews of CLEVERDON, C. W.,MILLS, J. and KEEN, E. M. the Cranfield 2 report.Journal of Documentation, 22:247?249.Ellen Voorhees and Donna Harman.
1999.
Overview ofthe eighth Text REtrieval Conference (TREC 8).
InText REtrieval Conference (TREC).Ellen Voorhees.
1998.
Variations in relevance judgmentsand the measurement of retrieval effectiveness.
In Re-search and Development in Information Retrieval (SI-GIR), pages 315?323.398
