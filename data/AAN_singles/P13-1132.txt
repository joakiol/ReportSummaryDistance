Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1341?1351,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAlign, Disambiguate and Walk: A Unified Approach forMeasuring Semantic SimilarityMohammad Taher Pilehvar, David Jurgens and Roberto NavigliDepartment of Computer ScienceSapienza University of Rome{pilehvar,jurgens,navigli}@di.uniroma1.itAbstractSemantic similarity is an essential com-ponent of many Natural Language Pro-cessing applications.
However, prior meth-ods for computing semantic similarity of-ten operate at different levels, e.g., sin-gle words or entire documents, which re-quires adapting the method for each datatype.
We present a unified approach to se-mantic similarity that operates at multiplelevels, all the way from comparing wordsenses to comparing text documents.
Ourmethod leverages a common probabilisticrepresentation over word senses in order tocompare different types of linguistic data.This unified representation shows state-of-the-art performance on three tasks: seman-tic textual similarity, word similarity, andword sense coarsening.1 IntroductionSemantic similarity is a core technique for manytopics in Natural Language Processing such asTextual Entailment (Berant et al, 2012), Seman-tic Role Labeling (Fu?rstenau and Lapata, 2012),and Question Answering (Surdeanu et al, 2011).For example, textual similarity enables relevantdocuments to be identified for information re-trieval (Hliaoutakis et al, 2006), while identifyingsimilar words enables tasks such as paraphrasing(Glickman and Dagan, 2003), lexical substitution(McCarthy and Navigli, 2009), lexical simplifica-tion (Biran et al, 2011), and Web search resultclustering (Di Marco and Navigli, 2013).Approaches to semantic similarity have oftenoperated at separate levels: methods for word sim-ilarity are rarely applied to documents or even sin-gle sentences (Budanitsky and Hirst, 2006; Radin-sky et al, 2011; Halawi et al, 2012), whiledocument-based similarity methods require morelinguistic features, which often makes them in-applicable at the word or microtext level (Saltonet al, 1975; Maguitman et al, 2005; Elsayed etal., 2008; Turney and Pantel, 2010).
Despite thepotential advantages, few approaches to semanticsimilarity operate at the sense level due to the chal-lenge in sense-tagging text (Navigli, 2009); for ex-ample, none of the top four systems in the recentSemEval-2012 task on textual similarity comparedsemantic representations that incorporated senseinformation (Agirre et al, 2012).We propose a unified approach to semantic sim-ilarity across multiple representation levels fromsenses to documents, which offers two signifi-cant advantages.
First, the method is applicableindependently of the input type, which enablesmeaningful similarity comparisons across differ-ent scales of text or lexical levels.
Second, by op-erating at the sense level, a unified approach is ableto identify the semantic similarities that exist in-dependently of the text?s lexical forms and any se-mantic ambiguity therein.
For example, considerthe sentences:t1.
A manager fired the worker.t2.
An employee was terminated from work byhis boss.A surface-based approach would label the sen-tences as dissimilar due to the minimal lexicaloverlap.
However, a sense-based representationenables detection of the similarity between themeanings of the words, e.g., fire and terminate.Indeed, an accurate, sense-based representation isessential for cases where different words are usedto convey the same meaning.The contributions of this paper are threefold.First, we propose a new unified representation ofthe meaning of an arbitrarily-sized piece of text,referred to as a lexical item, using a sense-basedprobability distribution.
Second, we propose anovel alignment-based method for word sense dis-1341ambiguation during semantic comparison.
Third,we demonstrate that this single representation canachieve state-of-the-art performance on three sim-ilarity tasks, each operating at a different lexicallevel: (1) surpassing the highest scores on theSemEval-2012 task on textual similarity (Agirreet al, 2012) that compares sentences, (2) achiev-ing a near-perfect performance on the TOEFL syn-onym selection task proposed by Landauer andDumais (1997), which measures word pair sim-ilarity, and also obtaining state-of-the-art perfor-mance in terms of the correlation with humanjudgments on the RG-65 dataset (Rubenstein andGoodenough, 1965), and finally (3) surpassing theperformance of Snow et al (2007) in a sense-coarsening task that measures sense similarity.2 A Unified Semantic RepresentationWe propose a representation of any lexical item asa distribution over a set of word senses, referredto as the item?s semantic signature.
We beginwith a formal description of the representation atthe sense level (Section 2.1).
Following this, wedescribe our alignment-based disambiguation al-gorithm which enables us to produce sense-basedsemantic signatures for those lexical items (e.g.,words or sentences) which are not sense annotated(Section 2.2).
Finally, we propose three methodsfor comparing these signatures (Section 2.3).
Asour sense inventory, we use WordNet 3.0 (Fell-baum, 1998).2.1 Semantic SignaturesThe WordNet ontology provides a rich net-work structure of semantic relatedness, connect-ing senses directly with their hypernyms, and pro-viding information on semantically similar sensesby virtue of their nearby locality in the network.Given a particular node (sense) in the network, re-peated random walks beginning at that node willproduce a frequency distribution over the nodesin the graph visited during the walk.
To ex-tend beyond a single sense, the random walk maybe initialized and restarted from a set of senses(seed nodes), rather than just one; this multi-seedwalk produces a multinomial distribution over allthe senses in WordNet with higher probability as-signed to senses that are frequently visited fromthe seeds.
Prior work has demonstrated that multi-nomials generated from random walks over Word-Net can be successfully applied to linguistic taskssuch as word similarity (Hughes and Ramage,2007; Agirre et al, 2009), paraphrase recogni-tion, textual entailment (Ramage et al, 2009),and pseudoword generation (Pilehvar and Navigli,2013).Formally, we define the semantic signature ofa lexical item as the multinomial distribution gen-erated from the random walks over WordNet 3.0where the set of seed nodes is the set of sensespresent in the item.
This representation encom-passes both when the item is itself a single senseand when the item is a sense-tagged sentence.To construct each semantic signature, we usethe iterative method for calculating topic-sensitivePageRank (Haveliwala, 2002).
Let M be the ad-jacency matrix for the WordNet network, whereedges connect senses according to the rela-tions defined in WordNet (e.g., hypernymy andmeronymy).
We further enrich M by connectinga sense with all the other senses that appear in itsdisambiguated gloss.1 Let ~v(0) denote the prob-ability distribution for the starting location of therandom walker in the network.
Given the set ofsenses S in a lexical item, the probability massof ~v(0) is uniformly distributed across the sensessi ?
S, with the mass for all sj /?
S set to zero.The PageRank may then be computed using:~v (t) = (1?
?
)M~v (t?1) + ?
~v (0) (1)where at each iteration the random walker mayjump to any node si ?
S with probability ?/|S|.We follow standard convention and set ?
to 0.15.We repeat the operation in Eq.
1 for 30 itera-tions, which is sufficient for the distribution toconverge.
The resulting probability vector ~v(t) isthe semantic signature of the lexical item, as ithas aggregated its senses?
similarities over the en-tire graph.
For our semantic signatures we usedthe UKB2 off-the-shelf implementation of topic-sensitive PageRank.2.2 Alignment-Based DisambiguationCommonly, semantic comparisons are betweenword pairs or sentence pairs that do not have theirlexical content sense-annotated, despite the poten-tial utility of sense annotation in making seman-tic comparisons.
However, traditional forms ofword sense disambiguation are difficult for shorttexts and single words because little or no con-textual information is present to perform the dis-ambiguation task.
Therefore, we propose a novel1http://wordnet.princeton.edu2http://ixa2.si.ehu.es/ukb/1342Figure 1: (a) Example alignments of the first sense of term manager (in sentence t1) to the two firstsenses of the word types in sentence t2, along with the similarity of the two senses?
semantic signatures;(b) Alignments which maximize the similarities across words in t1 and t2 (the source side of an alignmentis taken as the disambiguated sense of its corresponding word).alignment-based sense disambiguation that lever-ages the content of the paired item in order to dis-ambiguate each element.
Leveraging the paireditem enables our approach to disambiguate wheretraditional sense disambiguation methods can notdue to insufficient context.We view sense disambiguation as an alignmentproblem.
Given two arbitrarily ordered texts, weseek the semantic alignment that maximizes thesimilarity of the senses of the context words inboth texts.
To find this maximum we use an align-ment procedure which, for each word type wi initem T1, assigns wi to the sense that has the max-imal similarity to any sense of the word types inthe compared text T2.
Algorithm 1 formalizes thealignment process, which produces a sense dis-ambiguated representation as a result.
Senses arecompared in terms of their semantic signatures,denoted as function R. We consider multiple def-initions ofR, defined later in Section 2.3.As a part of the disambiguation procedure, weleverage the one sense per discourse heuristic ofYarowsky (1995); given all the word types in twocompared lexical items, each type is assigned asingle sense, even if it is used multiple times.
Ad-ditionally, if the same word type appears in bothsentences, both will always be mapped to the samesense.
Although such a sense assignment is poten-tially incorrect, assigning both types to the samesense results in a representation that does no worsethan a surface-level comparison.We illustrate the alignment-based disambigua-tion procedure using the two example sentences t1and t2 given in Section 1.
Figure 1(a) illustratesexample alignments of the first sense of managerto the first two senses of the word types in sentencet2 along with the similarity of the two senses?Algorithm 1 Alignment-based Sense DisambiguationInput: T1 and T2, the sets of word types being comparedOutput: P , the set of disambiguated senses for T11: P ?
?2: for each token ti ?
T13: max sim?
04: best si?
null5: for each token tj ?
T26: for each si ?
Senses(ti), sj ?
Senses(tj)7: sim?R(si, sj)8: if sim > max sim then9: max sim = sim10: best si = si11: P ?
P ?
{best si}12: return Psemantic signatures.
For the senses of manager,sense manager1n obtains the maximal similarityvalue to boss1n among all the possible pairings ofthe senses for the word types in sentence t2, and asa result is selected as the sense labeling for man-ager in sentence t1.3 Figure 1(b) shows the final,maximally-similar sense alignment of the wordtypes in t1 and t2.
The resulting alignment pro-duces the following sets of senses:Pt1 = {manager1n, fire4v, worker1n}Pt2 = {employee1n, terminate4v, work3n, boss2n}where Px denotes the corresponding set of sensesof sentence x.2.3 Semantic Signature SimilarityCosine Similarity.
In order to compare seman-tic signatures, we adopt the Cosine similarity mea-sure as a baseline method.
The measure is com-puted by treating each multinomial as a vector andthen calculating the normalized dot product of thetwo signatures?
vectors.3We follow Navigli (2009) and denote with wip the i-thsense of w in WordNet with part of speech p.1343However, a semantic signature is, in essence,a weighted ranking of the importance of Word-Net senses for each lexical item.
Given that theWordNet graph has a non-uniform structure, andalso given that different lexical items may be ofdifferent sizes, the magnitudes of the probabilitiesobtained may differ significantly between the twomultinomial distributions.
Therefore, for com-puting the similarity of two signatures, we alsoconsider two nonparametric methods that use theranking of the senses, rather than their probabilityvalues, in the multinomial.Weighted Overlap.
Our first measure providesa nonparametric similarity by comparing the simi-larity of the rankings for intersection of the sensesin both semantic signatures.
However, we addi-tionally weight the similarity such that differencesin the highest ranks are penalized more than differ-ences in lower ranks.
We refer to this measure asthe Weighted Overlap.
Let S denote the intersec-tion of all senses with non-zero probability in bothsignatures and rji denote the rank of sense si ?
Sin signature j, where rank 1 denotes the highestrank.
The sum of the two ranks r1i and r2i for asense is then inverted, which (1) weights higherranks more and (2) when summed, provides themaximal value when a sense has the same rank inboth signatures.
The unnormalized weighted over-lap is then calculated as?|S|i=1(r1i + r2i )?1.
Then,to bound the similarity value in [0, 1], we normal-ize the sum by its maximum value, ?|S|i=1(2i)?1,which occurs when each sense has the same rankin both signatures.Top-k Jaccard.
Our second measure uses theranking to identify the top-k senses in a signa-ture, which are treated as the best representativesof the conceptual associates.
We hypothesize thata specific rank ordering may be attributed to smalldifferences in the multinomial probabilities, whichcan lower rank-based similarities when one of thecompared orderings is perturbed due to slightlydifferent probability values.
Therefore, we con-sider the top-k senses as an unordered set, withequal importance in the signature.
To compare twosignatures, we compute the Jaccard Index of thetwo signatures?
sets:RJac(Uk, Vk) =|Uk ?
Vk||Uk ?
Vk|(2)whereUk denotes the set of k senses with the high-est probability in the semantic signature U .Dataset MSRvid MSRpar SMTeuroparl OnWN SMTnewsTraining 750 750 734 - -Test 750 750 459 750 399Table 1: Statistics of the provided datasets for theSemEval-2012 Semantic Textual Similarity task.3 Experiment 1: Textual SimilarityMeasuring semantic similarity of textual items hasapplications in a wide variety of NLP tasks.
Asour benchmark, we selected the recent SemEval-2012 task on Semantic Textual Similarity (STS),which was concerned with measuring the seman-tic similarity of sentence pairs.
The task receivedconsiderable interest by facilitating a meaningfulcomparison between approaches.3.1 Experimental SetupData.
We follow the experimental setup used inthe STS task (Agirre et al, 2012), which providedfive test sets, two of which had accompanyingtraining data sets for tuning system performance.Each sentence pair in the datasets was given ascore from 0 to 5 (low to high similarity) by hu-man judges, with a high inter-annotator agreementof around 0.90 when measured using the Pearsoncorrelation coefficient.
Table 1 lists the number ofsentence pairs in training and test portions of eachdataset.Comparison Systems.
The top-ranking partic-ipating systems in the SemEval-2012 task weregenerally supervised systems utilizing a variety oflexical resources and similarity measurement tech-niques.
We compare our results against the topthree systems of the 88 submissions: TLsim andTLsyn, the two systems of S?aric?
et al (2012), andthe UKP2 system (Ba?r et al, 2012).
UKP2 utilizesextensive resources among which are a Distribu-tional Thesaurus computed on 10M dependency-parsed English sentences.
In addition, the sys-tem utilizes techniques such as Explicit SemanticAnalysis (Gabrilovich and Markovitch, 2007) andmakes use of resources such as Wiktionary andWikipedia, a lexical substitution system based onsupervised word sense disambiguation (Biemann,2013), and a statistical machine translation sys-tem.
The TLsim system uses the New York TimesAnnotated Corpus, Wikipedia, and Google BookNgrams.
The TLsyn system also uses GoogleBook Ngrams, as well as dependency parsing andnamed entity recognition.1344Ranking System Overall Dataset-specificALL ALLnrm Mean ALL ALLnrm Mean Mpar Mvid SMTe OnWN SMTn1 1 1 ADW 0.866 0.871 0.711 0.694 0.887 0.555 0.706 0.6042 3 2 UKP2 0.824 0.858 0.677 0.683 0.873 0.528 0.664 0.4933 4 6 TLsyn 0.814 0.857 0.660 0.698 0.862 0.361 0.704 0.4684 2 3 TLsim 0.813 0.864 0.675 0.734 0.880 0.477 0.679 0.398Table 2: Performance of our system (ADW) and the 3 top-ranking participating systems (out of 88) inthe SemEval-2012 Semantic Textual Similarity task.
Rightmost columns report the corresponding Pear-son correlation r for individual datasets, i.e., MSRpar (Mpar), MSRvid (Mvid), SMTeuroparl (SMTe),OnWN (OnWN) and SMTnews (SMTn).
We also provide scores according to the three official evalua-tion metrics (i.e., ALL, ALLnrm, and Mean).
Rankings are also presented based on the three metrics.System Configuration.
Here we describe theconfiguration of our approach, which we havecalled Align, Disambiguate and Walk (ADW).
TheSTS task uses human similarity judgments on anordinal scale from 0 to 5.
Therefore, in ADW weadopted a similar approach to generating similar-ity values to that adopted by other participatingsystems, whereby a supervised system is trainedto combine multiple similarity judgments to pro-duce a final rating consistent with the human an-notators.
We utilized the WEKA toolkit (Hall etal., 2009) to train a Gaussian Processes regressionmodel for each of the three training sets (cf.
Table1).
The features discussed hereafter were consid-ered in our regression model.Main features.
We used the scores calculatedusing all three of our semantic signature compar-ison methods as individual features.
Although theJaccard comparison is parameterized, we avoidedtuning and instead used four features for distinctvalues of k: 250, 500, 1000, and 2500.String-based features.
Additionally, becausethe texts often contain named entities which arenot present in WordNet, we incorporated the sim-ilarity values produced by four string-based mea-sures, which were used by other teams in the STStask: (1) longest common substring which takesinto account the length of the longest overlap-ping contiguous sequence of characters (substring)across two strings (Gusfield, 1997), (2) longestcommon subsequence which, instead, finds thelongest overlapping subsequence of two strings(Allison and Dix, 1986), (3) Greedy String Tilingwhich allows reordering in strings (Wise, 1993),and (4) the character/word n-gram similarity pro-posed by Barro?n-Ceden?o et al (2010).We followed S?aric?
et al (2012) and used themodels trained on the SMTeuroparl and MSRpardatasets for testing on the SMTnews and OnWNtest sets, respectively.3.2 STS ResultsThree evaluation metrics are provided by the or-ganizers of the SemEval-2012 STS task, all ofwhich are based on Pearson correlation r of humanjudgments with system outputs: (1) the correla-tion value for the concatenation of all five datasets(ALL), (2) a correlation value obtained on a con-catenation of the outputs, separately normalizedby least square (ALLnrm), and (3) the weightedaverage of Pearson correlations across datasets(Mean).
Table 2 shows the scores obtained byADW for the three evaluation metrics, as well asthe Pearson correlation values obtained on eachof the five test sets (rightmost columns).
We alsoshow the results obtained by the three top-rankingparticipating systems (i.e., UKP2, TLsim, and TL-syn).
The leftmost three columns show the systemrankings according to the three metrics.As can be seen from Table 2, our system (ADW)outperforms all the 88 participating systems ac-cording to all the evaluation metrics.
Our sys-tem shows a statistically significant improvementon the SMTnews dataset, with an increase in thePearson correlation of over 0.10.
MSRpar (MPar)is the only dataset in which TLsim (S?aric?
et al,2012) achieves a higher correlation with humanjudgments.
Named entity features used by the TL-sim system could be the reason for its better per-formance on the MSRpar dataset, which containsa large number of named entities.3.3 Similarity Measure AnalysisTo gain more insight into the impact of ouralignment-based disambiguation approach, wecarried out a 10-fold cross-validation on the threetraining datasets (cf.
Table 1) using the systemsdescribed hereafter.ADW-MF.
To build this system, we utilized ourmain features only; i.e., we did not make use ofadditional string-based features.1345DW.
Similarly to ADW-MF, this system utilizedthe main features only.
In DW, however, we re-placed our alignment-based disambiguation phasewith a random walk-based WSD system that dis-ambiguated the sentences separately, without per-forming any alignment.
As our WSD system,we used UKB, a state-of-the-art knowledge-basedWSD system that is based on the same topic-sensitive PageRank algorithm used by our ap-proach.
UKB initializes the algorithm from allsenses of the words in the context of a word tobe disambiguated.
It then picks the most relevantsense of the word according to the resulting prob-ability vector.
As the lexical knowledge base ofUKB, we used the same semantic network as thatutilized by our approach for calculating semanticsignatures.Table 3 lists the performance values of the twoabove-mentioned systems on the three trainingsets in terms of Pearson correlation.
In addition,we present in the table correlation scores for fourother similarity measures reported by Ba?r et al(2012):?
Pairwise Word Similarity that comprises ofa set of WordNet-based similarity measuresproposed by Resnik (1995), Jiang and Con-rath (1997), and Lin (1998b).
The aggre-gation strategy proposed by Corley and Mi-halcea (2005) has been utilized for extend-ing these word-to-word similarity measuresfor calculating text-to-text similarities.?
Explicit Semantic Analysis (Gabrilovichand Markovitch, 2007) where the high-dimensional vectors are obtained on Word-Net, Wikipedia and Wiktionary.?
Distributional Thesaurus where a similarityscore is computed similarly to that of Lin(1998a) using a distributional thesaurus ob-tained from a 10M dependency-parsed sen-tences of English newswire.?
Character n-grams which were also used asone of our additional features.As can be seen from Table 3, our alignment-based disambiguation approach (ADW-MF) isbetter suited to the task than a conventional WSDapproach (DW).
Another interesting point is thehigh scores achieved by the Character n-gramsSimilarity measure DatasetMpar Mvid SMTeDW 0.448 0.820 0.660ADW-MF 0.485 0.842 0.721Explicit Semantic Analysis 0.427 0.781 0.619Pairwise Word Similarity 0.564 0.835 0.527Distributional Thesaurus 0.494 0.481 0.365Character n-grams 0.658 0.771 0.554Table 3: Performance of our main-feature sys-tem with conventional WSD (DW) and with thealignment-based disambiguation approach (ADW-MF) vs. four other similarity measures, using 10-fold cross validation on the training datasets MSR-par (Mpar), MSRvid (Mvid), and SMTeuroparl(SMTe).measure.
This confirms that string-based meth-ods are strong baselines for semantic textual sim-ilarity.
Except for the MSRpar (Mpar) dataset,our system (ADW-MF) outperforms all other sim-ilarity measures.
The scores obtained by ExplicitSemantic Analysis and Distributional Thesaurusare not competitive on any dataset.
On the otherhand, Pairwise Word Similarity achieves a highperformance on MSRpar and MSRvid datasets,but performs surprisingly low on the SMTeuroparldataset.4 Experiment 2: Word SimilarityWe now proceed from the sentence level to theword level.
Word similarity has been a key prob-lem for lexical semantics, with significant effortsbeing made by approaches in distributional se-mantics to accurately identify synonymous words(Turney and Pantel, 2010).
Different evaluationmethods exist in the literature for evaluating theperformance of a word-level semantic similaritymeasure; we adopted two well-established bench-marks: synonym recognition and correlating wordsimilarity judgments with those from human an-notators.For synonym recognition, we used the TOEFLdataset created by Landauer and Dumais (1997).The dataset consists of 80 multiple-choice syn-onym questions from the TOEFL test; a word ispaired with four options, one of which is a validsynonym.
Test takers with English as a secondlanguage averaged 64.5% correct.
Despite multi-ple approaches, only recently has the test been an-swered perfectly (Bullinaria and Levy, 2012), un-derscoring the challenge of synonym recognition.1346Approach AccuracyPPMIC (Bullinaria and Levy, 2007) 85.00%GLSA (Matveeva et al, 2005) 86.25%LSA (Rapp, 2003) 92.50%ADWJac 93.75?2.5%ADWWO 95.00%ADWCos 96.25%PR (Turney et al, 2003) 97.50%PCCP (Bullinaria and Levy, 2012) 100.00%Table 4: Accuracy on the 80-question TOEFLSynonym test.
ADWJac, ADWWO, and ADWCoscorrespond to results with the Jaccard, WeightedOverlap and Cosine signature comparison mea-sures, respectively.For the similarity judgment evaluation, weused as benchmark the RG-65 dataset created byRubenstein and Goodenough (1965).
The datasetcontains 65 word pairs judged by 51 human sub-jects on a scale of 0 to 4 according to their seman-tic similarity.
Ideally, a measure?s similarity judg-ments are expected to be highly correlated withthose of humans.
To be consistent with the previ-ous literature (Hughes and Ramage, 2007; Agirreet al, 2009), we used Spearman?s rank correlationin our experiment.4.1 Experimental SetupOur alignment-based sense disambiguation trans-forms the task of comparing individual wordsinto that of calculating the similarity of the best-matching sense pair across the two words.
Asthere is no training data we do not optimize the kvalue for computing signature similarity with theJaccard index; instead, we report, for the synonymrecognition and the similarity judgment evalua-tions, the respective range of accuracies and theaverage correlation obtained upon using five val-ues of k randomly selected in the range [50, 2500]:678, 1412, 1692, 2358, 2387.4.2 Word Similarity Results: TOEFL datasetTable 4 lists the accuracy performance of the sys-tem in comparison to the existing state of theart on the TOEFL test.
ADWWO, ADWCos,and ADWJac correspond to our approach whenWeighted Overlap, Cosine, and Jaccard signa-ture comparison measures are used, respectively.Despite not being tuned for the task, our modelachieves near-perfect performance, answering allbut three questions correctly with the Cosine mea-sure.
Among the top-performing approaches, onlyWord Synonym choices (correct in bold)fanciful familiar apparent?
imaginative?
logicalverbal oral?
overt fitting verbose?resolved settled?
forgotten?
publicized examinedpercentage volume sample proportion profit?
?figure list solve?
divide?
expresshighlight alter?
imitate accentuate?
restoreTable 5: Questions answered incorrectly by ourapproach.
Symbols ?
and ?
correspond to thechoices of our approach with the Weighted Over-lap and Cosine signature comparisons respec-tively.
We do not include the mistakes made whenthe Jaccard measure was used as they vary withthe k value.that of Rapp (2003) uses word senses, an approachthat is outperformed by our method.The errors produced by our system were largelythe result of sense locality in the WordNet net-work.
Table 5 highlights the incorrect responses.The synonym mistakes reveal cases where sensesof the two words are close in WordNet, indicatingsome relatedness.
For example, percentage maybe interpreted colloquially as monetary value (e.g.,?give me my percentage?)
and elicits the synonymof profit in the economic domain, which ADW in-correctly selects as a synonym.4.3 Word Similarity Results: RG-65 datasetTable 6 shows the Spearman?s ?
rank correlationcoefficients with human judgments on the RG-65dataset.
As can be seen from the Table, our ap-proach with the Weighted Overlap signature com-parison improves over the similar approach ofHughes and Ramage (2007) which, however, doesnot involve the disambiguation step and considersa word as a whole unit as represented by the set ofits senses.5 Experiment 3: Sense SimilarityWordNet is known to be a fine-grained sense in-ventory with many related word senses (Palmer etal., 2007).
Accordingly, multiple approaches haveattempted to identify highly similar senses in or-der to produce a coarse-grained sense inventory.We adopt this task as a way of evaluating our sim-ilarity measure at the sense level.5.1 Coarse-graining BackgroundEarlier work on reducing the polysemy of senseinventories has considered WordNet-based senserelatedness measures (Mihalcea and Moldovan,2001) and corpus-based vector representations of1347Approach CorrelationADWCos 0.825Agirre et al (2009) 0.830Hughes and Ramage (2007) 0.838Zesch et al (2008) 0.840ADWJac 0.841ADWWO 0.868Table 6: Spearman?s ?
correlation coefficientswith human judgments on the RG-65 dataset.ADWJac, ADWWO, and ADWCos correspond toresults with the Jaccard, Weighted Overlap andCosine signature comparison measures respec-tively.word senses (Agirre and Lopez, 2003; McCarthy,2006).
Navigli (2006) proposed an automatic ap-proach for mapping WordNet senses to the coarse-grained sense distinctions of the Oxford Dictio-nary of English (ODE).
The approach leveragessemantic similarities in gloss definitions and thehierarchical relations between senses in the ODEto cluster WordNet senses.
As current state ofthe art, Snow et al (2007) developed a super-vised SVM classifier that utilized, as its features,several earlier sense relatedness techniques suchas those implemented in the WordNet::Similaritypackage (Pedersen et al, 2004).
The classifieralso made use of resources such as topic signaturesdata (Agirre and de Lacalle, 2004), the WordNetdomain dataset (Magnini and Cavaglia`, 2000), andthe mappings of WordNet senses to ODE sensesproduced by Navigli (2006).5.2 Experimental SetupWe benchmark the accuracy of our similarity mea-sure in grouping word senses against those of Nav-igli (2006) and Snow et al (2007) on two datasetsof manually-labeled sense groupings of WordNetsenses: (1) sense groupings provided as a part ofthe Senseval-2 English Lexical Sample WSD task(Kilgarriff, 2001) which includes nouns, verbs andadjectives; (2) sense groupings included in theOntoNotes project4 (Hovy et al, 2006) for nounsand verbs.
Following the evaluation methodologyof Snow et al (2007), we combine the Senseval-2and OntoNotes datasets into a third dataset.Snow et al (2007) considered sense grouping asa binary classification task whereby for each wordevery possible pairing of senses has to be classified4Sense groupings belong to a pre-version 1.0: http://cemantix.org/download/sense/ontonotes-sense-groups.tar.gzOnto SE-2 Onto + SE-2Method Noun Verb Noun Verb Adj Noun VerbRCos 0.406 0.522 0.450 0.465 0.484 0.441 0.485RWO 0.421 0.544 0.483 0.482 0.531 0.470 0.503RJac 0.418 0.531 0.478 0.473 0.501 0.465 0.493SVM 0.370 0.455 NA NA 0.473 0.423 0.432ODE 0.218 0.396 NA NA 0.371 0.331 0.288Table 7: F-score sense merging evaluation onthree hand-labeled datasets: OntoNotes (Onto),Senseval-2 (SE-2), and combined (Onto+SE-2).Results are reported for all three of our signaturecomparison measures and also for two previousworks (last two rows).as either merged or not-merged.
We constructeda simple threshold-based classifier to perform thesame binary classification.
To this end, we cal-culated the semantic similarity of each sense pairand then used a threshold value t to classify thepair as merged if similarity ?
t and not-mergedotherwise.
We sampled out 10% of the dataset fortuning the value of t, thus adapting our classifierto the fine granularity of the dataset.
We used thesame held-out instances to perform a tuning of thek value used for Jaccard index, over the same val-ues of k as in Experiment 1 (cf.
Section 3).5.3 Sense Merging ResultsFor a binary classification task, we can directlycalculate precision, recall and F-score by con-structing a contingency table.
We show in Ta-ble 7 the F-score performance of our classifier asobtained by an averaged 10-fold cross-validation.Results are presented for all three of the mea-sures of semantic signature comparison and forthe three datasets: OntoNotes, Senseval-2, andthe two combined.
In addition, we show in Ta-ble 7 the F-score results provided by Snow et al(2007) for their SVM-based system and for themapping-based approach of Navigli (2006), de-noted by ODE.Table 7 shows that our methodology yields im-provements over previous work on both datasetsand for all parts of speech, irrespective ofthe semantic signature comparison method used.Among the three methods, Weighted Overlapachieves the best performance, which demon-strates that our transformation of semantic signa-tures into ordered lists of concepts and calculatingsimilarity by rank comparison has been helpful.13486 Related WorkDue to the wide applicability of semantic similar-ity, significant efforts have been made at differentlexical levels.
Early work on document-level sim-ilarity was driven by information retrieval.
Vectorspace methods provided initial successes (Saltonet al, 1975), but often suffer from data spar-sity when using small documents, or when doc-uments use different word types, as in the caseof paraphrases.
Later efforts such as LSI (Deer-wester et al, 1990), PLSA (Hofmann, 2001) andTopic Models (Blei et al, 2003; Steyvers and Grif-fiths, 2007) overcame these sparsity issues usingdimensionality reduction techniques or modelingthe document using latent variables.
However,such methods were still most suitable for compar-ing longer texts.
Complementary approaches havebeen developed specifically for comparing shortertexts, such as those used in the SemEval-2012STS task (Agirre et al, 2012).
Most similar toour approach are the methods of Islam and Inkpen(2008) and Corley and Mihalcea (2005), who per-formed a word-to-word similarity alignment; how-ever, they did not operate at the sense level.
Ram-age et al (2009) used a similar semantic represen-tation of short texts from random walks on Word-Net, which was applied to paraphrase recognitionand textual entailment.
However, unlike our ap-proach, their method does not perform sense dis-ambiguation prior to building the representationand therefore potentially suffers from ambiguity.A significant amount of effort has also been putinto measuring similarity at the word level, fre-quently by approaches that use distributional se-mantics (Turney and Pantel, 2010).
These meth-ods use contextual features to represent semanticsat the word level, whereas our approach representsword semantics at the sense level.
Most similar toour approach are those of Agirre et al (2009) andHughes and Ramage (2007), which represent wordmeaning as the multinomials produced from ran-dom walks on the WordNet graph.
However, un-like our approach, neither of these disambiguatesthe two words being compared, which potentiallyconflates the meanings and lowers the similarityjudgment.Measures of sense relatedness have frequentlyleveraged the structural properties of WordNet(e.g., path lengths) to compare senses.
Budanit-sky and Hirst (2006) provided a survey of suchWordNet-based measures.
The main drawbackwith these approaches lies in the WordNet struc-ture itself, where frequently two semantically sim-ilar senses are distant in the WordNet hierar-chy.
Possible solutions include relying on wider-coverage networks such as WikiNet (Nastase andStrube, 2013) or multilingual ones such as Babel-Net (Navigli and Ponzetto, 2012b).
Fewer workshave focused on measuring the similarity ?
as op-posed to relatedness ?
between senses.
The topicsignatures method of Agirre and Lopez (2003)represents each sense as a vector over corpus-derived features in order to build comparable senserepresentations.
However, topic signatures oftenproduce lower quality representations due to spar-sity in the local structure of WordNet, especiallyfor rare senses.
In contrast, the random walkused in our approach provides a denser, and thusmore comparable, representation for all WordNetsenses.7 ConclusionsThis paper presents a unified approach for comput-ing semantic similarity at multiple lexical levels,from word senses to texts.
Our method leveragesa common probabilistic representation at the senselevel for all types of linguistic data.
We demon-strate that our semantic representation achievesstate-of-the-art performance in three experimentsusing semantic similarity at different lexical levels(i.e., sense, word, and text), surpassing the per-formance of previous similarity measures that areoften specifically targeted for each level.In future work, we plan to explore the impact ofthe sense inventory-based network used in our se-mantic signatures.
Specifically, we plan to investi-gate higher coverage inventories such as BabelNet(Navigli and Ponzetto, 2012a), which will handletexts with named entities and rare senses that arenot in WordNet, and will also enable cross-lingualsemantic similarity.
Second, we plan to evaluateour method on larger units of text and formalizecomparison methods between different lexical lev-els.AcknowledgmentsThe authors gratefully acknowledgethe support of the ERC StartingGrant MultiJEDI No.
259234.We would like to thank Sameer S. Pradhanfor providing us with an earlier version of theOntoNotes dataset.1349ReferencesEneko Agirre and Oier Lopez de Lacalle.
2004.
Publiclyavailable topic signatures for all WordNet nominal senses.In Proceedings of LREC, pages 1123?1126, Lisbon, Por-tugal.Eneko Agirre and Oier Lopez.
2003.
Clustering WordNetword senses.
In Proceedings of RANLP, pages 121?130,Borovets, Bulgaria.Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kraval-ova, Marius Pas?ca, and Aitor Soroa.
2009.
A studyon similarity and relatedness using distributional andWordNet-based approaches.
In Proceedings of NAACL,pages 19?27, Boulder, Colorado.Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre.
2012.
SemEval-2012 task 6: A pilot on semantictextual similarity.
In Proceedings of SemEval-2012, pages385?393, Montreal, Canada.Lloyd Allison and Trevor I. Dix.
1986.
A bit-string longest-common-subsequence algorithm.
Information ProcessingLetters, 23(6):305?310.Daniel Ba?r, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
UKP: Computing semantic textual similar-ity by combining multiple content similarity measures.
InProceedings of SemEval-2012, pages 435?440, Montreal,Canada.Alberto Barro?n-Ceden?o, Paolo Rosso, Eneko Agirre, andGorka Labaka.
2010.
Plagiarism detection across distantlanguage pairs.
In Proceedings of COLING, pages 37?45,Beijing, China.Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012.Learning entailment relations by global graph structureoptimization.
Computational Linguistics, 38(1):73?111.Chris Biemann.
2013.
Creating a system for lexical sub-stitutions from scratch using crowdsourcing.
LanguageResources and Evaluation, 47(1):97?122.Or Biran, Samuel Brody, and Noe?mie Elhadad.
2011.Putting it simply: a context-aware approach to lexical sim-plification.
In Proceedings of ACL, pages 496?501, Port-land, Oregon.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003.Latent Dirichlet Allocation.
The Journal of MachineLearning Research, 3:993?1022.Alexander Budanitsky and Graeme Hirst.
2006.
Evaluat-ing WordNet-based measures of Lexical Semantic Relat-edness.
Computational Linguistics, 32(1):13?47.John A. Bullinaria and Joseph.
P. Levy.
2007.
Extractingsemantic representations from word co-occurrence statis-tics: A computational study.
Behavior Research Methods,(3):510.John A. Bullinaria and Joseph P. Levy.
2012.
Extractingsemantic representations from word co-occurrence statis-tics: stop-lists, stemming, and SVD.
Behavior ResearchMethods, 44:890?907.Courtney Corley and Rada Mihalcea.
2005.
Measuring thesemantic similarity of texts.
In Proceedings of the ACLWorkshop on Empirical Modeling of Semantic Equiva-lence and Entailment, pages 13?18, Ann Arbor, Michigan.Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer,George W. Furnas, and Richard A. Harshman.
1990.
In-dexing by Latent Semantic Analysis.
Journal of AmericanSociety for Information Science, 41(6):391?407.Antonio Di Marco and Roberto Navigli.
2013.
Cluster-ing and diversifying Web search results with graph-basedWord Sense Induction.
Computational Linguistics, 39(3).Tamer Elsayed, Jimmy Lin, and Douglas W. Oard.
2008.Pairwise document similarity in large collections withMapReduce.
In Proceedings of ACL-HLT, pages 265?268, Columbus, Ohio.Christiane Fellbaum, editor.
1998.
WordNet: An ElectronicDatabase.
MIT Press, Cambridge, MA.Hagen Fu?rstenau and Mirella Lapata.
2012.
Semi-supervisedSemantic Role Labeling via structural alignment.
Compu-tational Linguistics, 38(1):135?171.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Comput-ing semantic relatedness using Wikipedia-based explicitsemantic analysis.
In Proceedings of IJCAI, pages 1606?1611, Hyderabad, India.Oren Glickman and Ido Dagan.
2003.
Acquiring lexi-cal paraphrases from a single corpus.
In Proceedings ofRANLP, pages 81?90, Borovets, Bulgaria.Dan Gusfield.
1997.
Algorithms on strings, trees, and se-quences: computer science and computational biology.Cambridge University Press.Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and YehudaKoren.
2012.
Large-scale learning of word relatednesswith constraints.
In Proceedings of KDD, pages 1406?1414, Beijing, China.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.
2009.The WEKA data mining software: an update.
ACMSIGKDD Explorations Newsletter, 11(1):10?18.Taher H. Haveliwala.
2002.
Topic-sensitive PageRank.
InProceedings of WWW, pages 517?526, Hawaii, USA.Angelos Hliaoutakis, Giannis Varelas, Epimenidis Voutsakis,Euripides GM Petrakis, and Evangelos Milios.
2006.Information retrieval by semantic similarity.
Interna-tional Journal on Semantic Web and Information Systems,2(3):55?73.Thomas Hofmann.
2001.
Unsupervised Learning by Prob-abilistic Latent Semantic Analysis.
Machine Learning,42(1):177?196.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes: The90% solution.
In Proceedings of NAACL, pages 57?60,NY, USA.Thad Hughes and Daniel Ramage.
2007.
Lexical semanticrelatedness with random graph walks.
In Proceedings ofEMNLP-CoNLL, pages 581?589, Prague, Czech Repub-lic.Aminul Islam and Diana Inkpen.
2008.
Semantic text sim-ilarity using corpus-based word similarity and string sim-ilarity.
ACM Transactions on Knowledge Discovery fromData, 2(2):10:1?10:25.Jay J. Jiang and David W. Conrath.
1997.
Semantic simi-larity based on corpus statistics and lexical taxonomy.
InProceedings of ROCLING X, pages 19?30, Taiwan.1350Adam Kilgarriff.
2001.
English lexical sample task descrip-tion.
In Proceedings of Senseval, pages 17?20, Toulouse,France.Thomas K. Landauer and Susan T. Dumais.
1997.
A solutionto Plato?s problem: The latent semantic analysis theory ofacquisition, induction, and representation of knowledge.Psychological Review; Psychological Review, 104(2):211.Dekang Lin.
1998a.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING, pages 768?774, Montreal, Quebec, Canada.Dekang Lin.
1998b.
An information-theoretic definition ofsimilarity.
In Proceedings of ICML, pages 296?304, SanFrancisco, CA.Bernardo Magnini and Gabriela Cavaglia`.
2000.
Integrat-ing subject field codes into WordNet.
In Proceedings ofLREC, pages 1413?1418, Athens, Greece.Ana G. Maguitman, Filippo Menczer, Heather Roinestad, andAlessandro Vespignani.
2005.
Algorithmic detection ofsemantic similarity.
In Proceedings of WWW, pages 107?116, Chiba, Japan.Irina Matveeva, Gina-Anne Levow, Ayman Farahat, andChristiaan Royer.
2005.
Terms representation with gener-alized latent semantic analysis.
In Proceedings of RANLP,Borovets, Bulgaria.Diana McCarthy and Roberto Navigli.
2009.
The Englishlexical substitution task.
Language Resources and Evalu-ation, 43(2):139?159.Diana McCarthy.
2006.
Relating WordNet senses for wordsense disambiguation.
In Proceedings of the Workshop onMaking Sense of Sense at EACL-06, pages 17?24, Trento,Italy.Rada Mihalcea and Dan Moldovan.
2001.
Automatic gen-eration of a coarse grained WordNet.
In Proceedingsof NAACL Workshop on WordNet and Other Lexical Re-sources, Pittsburgh, USA.Vivi Nastase and Michael Strube.
2013.
TransformingWikipedia into a large scale multilingual concept network.Artificial Intelligence, 194:62?85.Roberto Navigli and Simone Paolo Ponzetto.
2012a.
Ba-belNet: The automatic construction, evaluation and appli-cation of a wide-coverage multilingual semantic network.Artificial Intelligence, 193:217?250.Roberto Navigli and Simone Paolo Ponzetto.
2012b.
Babel-Relate!
a joint multilingual approach to computing seman-tic relatedness.
In Proceedings of AAAI, pages 108?114,Toronto, Canada.Roberto Navigli.
2006.
Meaningful clustering of senseshelps boost Word Sense Disambiguation performance.
InProceedings of COLING-ACL, pages 105?112, Sydney,Australia.Roberto Navigli.
2009.
Word Sense Disambiguation: A sur-vey.
ACM Computing Surveys, 41(2):1?69.Martha Palmer, Hoa Dang, and Christiane Fellbaum.
2007.Making fine-grained and coarse-grained sense distinc-tions, both manually and automatically.
Natural Lan-guage Engineering, 13(2):137?163.Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.2004.
WordNet::Similarity - measuring the relatedness ofconcepts.
In Proceedings of AAAI, pages 144?152, SanJose, CA.Mohammad Taher Pilehvar and Roberto Navigli.
2013.Paving the way to a large-scale pseudosense-annotateddataset.
In Proceedings of NAACL-HLT, pages 1100?1109, Atlanta, USA.Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, andShaul Markovitch.
2011.
A word at a time: comput-ing word relatedness using temporal semantic analysis.
InProceedings of WWW, pages 337?346, Hyderabad, India.Daniel Ramage, Anna N. Rafferty, and Christopher D. Man-ning.
2009.
Random walks for text semantic similarity.
InProceedings of the 2009 Workshop on Graph-based Meth-ods for Natural Language Processing, pages 23?31, Sun-tec, Singapore.Reinhard Rapp.
2003.
Word sense discovery based on sensedescriptor dissimilarity.
In Proceedings of the Ninth Ma-chine Translation Summit, pages 315?322, New Orleans,LA.Philip Resnik.
1995.
Using information content to evalu-ate semantic similarity in a taxonomy.
In Proceedings ofIJCAI, pages 448?453, Montreal, Canada.Herbert Rubenstein and John B. Goodenough.
1965.
Con-textual correlates of synonymy.
Communications of theACM, 8(10):627?633.Gerard Salton, A. Wong, and C. S. Yang.
1975.
A vectorspace model for automatic indexing.
Communications ofthe ACM, 18(11):613?620.Rion Snow, Sushant Prakash, Daniel Jurafsky, and Andrew Y.Ng.
2007.
Learning to merge word senses.
In EMNLP-CoNLL, pages 1005?1014, Prague, Czech Republic.Mark Steyvers and Tom Griffiths.
2007.
Probabilistictopic models.
Handbook of Latent Semantic Analysis,427(7):424?440.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2011.
Learning to rank answers to non-factoidquestions from Web collections.
Computational Linguis-tics, 37(2):351?383.Peter D. Turney and Patrick Pantel.
2010.
From frequencyto meaning: Vector space models of semantics.
Journal ofArtificial Intelligence Research, 37:141?188.Peter D. Turney, Michael L. Littman, Jeffrey Bigham, andVictor Shnayder.
2003.
Combining independent modulesto solve multiple-choice synonym and analogy problems.In Proceedings of RANLP, pages 482?489, Borovets, Bul-garia.Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder, andBojana Dalbelo Bas?ic?.
2012.
Takelab: Systems formeasuring semantic text similarity.
In Proceedings ofSemEval-2012, pages 441?448, Montreal, Canada.Michael J.
Wise.
1993.
String similarity via greedy stringtiling and running Karp-Rabin matching.
In Departmentof Computer Science Technical Report, Sydney.David Yarowsky.
1995.
Unsupervised Word Sense Disam-biguation rivaling supervised methods.
In Proceedings ofACL, pages 189?196, Cambridge, Massachusetts.Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.
2008.Using Wiktionary for computing semantic relatedness.
InProceedings of AAAI, pages 861?866, Chicago, Illinois.1351
