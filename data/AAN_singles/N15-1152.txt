Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1357?1361,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsLearning to parse with IAA-weighted lossH?ector Mart?
?nez Alonso?Barbara Plank?Arne Skj?rholt?Anders S?gaard?
?Njalsgade 140, Copenhagen (Denmark), University of Copenhagen?Gaustadall?een 23B, Oslo (Norway), University of Osloalonso@hum.ku.dk,bplank@cst.dk,arnskj@ifi.uio.no,soegaard@hum.ku.dkAbstractNatural language processing (NLP) annota-tion projects employ guidelines to maximizeinter-annotator agreement (IAA), and modelsare estimated assuming that there is one singleground truth.
However, not all disagreement isnoise, and in fact some of it may contain valu-able linguistic information.
We integrate suchinformation in the training of a cost-sensitivedependency parser.
We introduce five differ-ent factorizations of IAA and the correspond-ing loss functions, and evaluate these acrosssix different languages.
We obtain robust im-provements across the board using a factoriza-tion that considers dependency labels and di-rectionality.
The best method-dataset combi-nation reaches an average overall error reduc-tion of 6.4% in labeled attachment score.1 IntroductionTypically, NLP annotation projects employ guide-lines to maximize inter-annotator agreement.
Pos-sible inconsistencies are resolved by adjudication,and models are induced assuming there is one sin-gle ground truth.
However, there exist linguisticallyhard cases where there is no clear answer (Zeman,2010; Manning, 2011), and incorporating such dis-agreements into the training of a model has provenhelpful for POS tagging (Plank et al, 2014a; Planket al, 2014b).Inter-annotator agreement (IAA) is straight-forward to calculate for POS, but not for depen-dency trees.
There is no well-established standardfor computing agreement on trees (Skj?rholt, 2014).For a dependency tree, annotators can disagree inattachment, labeling, or both.
We implement dif-ferent strategies, i.e., factorizations (?2), to capturedisagreement on specific syntactic phenomena.Our hypothesis is that a dependency parser can beinformed of disagreements to regularize over anno-tators?
biases.
Testing our hypothesis requires theavailability of doubly-annotated data, and involvestwo steps: i) how to factorize attachment or labelingdisagreements; and ii) how to inform the parser ofthem during learning (?3).2 FactorizationsAssume a sample of sentences annotated by annota-torsA1andA2.
With such a sample we can estimateprobabilities of the two annotators?
disagreeing onthe annotation of a word or span, relative to some de-pendency tree factorization.
We factorize disagree-ment on dependency tree annotations relative to fourproperties of the annotated dependency edges: thePOS of the dependent, the POS of the head, the la-bel of the edge and the direction (left or right) ofthe head with regards to the dependent.
This sectiondescribes the different factorizations.We present five factorizations, depicted in Fig-ure 1.
With artificial root notes, all words in a depen-dency tree have one incoming edge.
This means thatin our sample, any word wihas two ?headId, label?annotations, i.e., ?h1, l1?
and ?h2, l2?
given by A1and A2, respectively, with POS(?)
being a functionfrom word indices to POS.
The five factorizationsare as follows:1357a)wjwiobjsbjb)wjwiwksub-L obj-Rc)wjwiwkNL Rd)wjwiwkN Vobje)wjwiwkV VL RFigure 1: Factorizations: a) LABEL, b) LABELD; c)CHILDPOSD, d) HEADPOS and e) HEADPOSD.
Red andgreen depict different choices by annotators A1and A2.a) LABEL: disagreement over label pairs, regard-less of attachment (h1,h2).
That is, ?h1, l1?
and?h2, l2?
count as disagreement, iff l16= l2.b) LABELD, same as LABEL, but incorporatingedge direction.
That is, ?h1, l1?
and ?h2, l2?count as disagreement, for any j, k ?
h1, h2,iff hj< i < hkor l16= l2.c) CHILDPOSD, i.e., disagreement on attachmentdirection given POS(i).
That is, for POS(i),?h1, l1?
and ?h2, l2?
count as disagreement, iffhj< i < hk.d) HEADPOS: disagreement on head POS.
Thatis, ?h1, l1?
and ?h2, l2?
count as disagreement,iff POS(h1) 6=POS(h2).e) HEADPOSD, i.e., HEADPOS, plus direction.That is, ?h1, l1?
and ?h2, l2?
count as disagree-ment, iff POS(h1) 6=POS(h2) or hj< i < hk.Each factorization yields a symmetric confusion ma-trix.
In our Norwegian data (?4), for instance, forLABEL there are 834 words that have been labeledas ATR (attribute) by both annotators, while thereare 44 cases where one annotator has given the ATRlabel and the other has given the ADV (adverbial)label.
For LABELD, there are 968 words that havebeen labeled as ADV where both annotators agreeon the head being on the left side of the word,whereas there are 9 cases where the annotators agreeon ADV label but not on the direction of the head.These 9 cases count as disagreements for LABELDbut not for LABEL.lang train test l pNO 13.7k/209k 5.8k/96.7k 29 19EN 3.6k/70k ?1.0k/20.3k 30 44DA 4.2k/74k ?1.2k/23.4k 31 25CA 3.9k/73k 1.7k/34.4k 27 11HR 3.1k/79k 1.3k/35.5k 26 27FI 9.1k/123k 3.9k/54.4k 45 12Table 1: Data statistics: number of sentences/tokens, de-pendency labels l, POS tags p for NO (Norwegian), EN(English), DA (Danish), CA (Catalan), Croatian (HR)and Finnish (FI); ?=canonical test split available.3 Cost-sensitive updatesWe use the cost-sensitive perceptron classifier, fol-lowing Plank et al (2014a), but extend it totransition-based dependency parsing, where the pre-dicted values are transitions (Goldberg and Nivre,2012).
Given a gold yiand predicted label y?i(POStags or transitions), the loss is weighted by ?
(y?i, yi):Lw(y?i, yi) = ?
(y?i, yi)max(0,?yiw ?
xi)Whenever a transition has been wrongly predicted,we retrieve the predicted edge and compare it to thegold dependency to calculate ?.
?
(yi, yj) is then theinverse of the confusion probability estimated fromour sample of doubly-annotated data.
For example,using the factorization LABEL, if the parser predictswito be SUBJECT and the gold annotation is OB-JECT, the confusion probability is the number oftimes one annotator said SUBJECT while the othersaid OBJECT out of the times one annotator said oneof them.
In LABELD, A1and A2can disagree evenif both say the grammatical function of some wordwiis SUBJECT, namely if one says the subject is leftof wi, and the other says it is right of wi.
The con-fusion probability is then the count of disagreementsover the total number of cases where both annotatorssaid a word was SUBJECT.In our baseline model, ?
(y?i, yi) = 1.
The valuesfor our cost-sensitive systems (LABEL, LABELD,CHILDPOSD, HEADPOS, HEADPOSD) are neverabove 1, which means that we are selectively under-fitting the parser for specific syntactic phenomena.In other words, we use the doubly-annotated data toregularize our model, hopefully preventing overfit-ting to annotators?
biases.13584 DataWe use six treebanks (Buch-Kromann et al, 2003;Buch-Kromann et al, 2007; Arias et al, 2014; Sol-berg et al, 2014; Agi?c and Merkler, 2013; Haveri-nen et al, 2010) for which we could get a sampleof doubly-annotated data.
All these treebanks aredirectly developed as dependency treebanks, insteadof being converted from constituent treebanks.
Ta-ble 1 gives overview statistics of the treebanks, Ta-ble 2 lists the sizes of the doubly-annotated samples,as well as F1 scores between annotators and ?
val-ues (Skj?rholt, 2014).
The doubly-annotated sam-ples are solely used to estimate confusion probabili-ties, and not for training or testing.
When a treebankhad no canonical train/test split, we took the final30% for testing.between annotator:lang sents tokens LAS UAS LA ?
plainNO 400 5.3k 94.70 96.47 96.62 0.984EN 264 5.5k 88.44 93.83 91.95 0.925DA 162 2.4k 90.43 96.12 92.40 0.957CA 63 1.3k 94.48 98.26 95.64 0.978HR 100 2.4k 78.89 89.16 84.07 0.939FI 400 5.1k 83.45 88.77 89.83 0.950Table 2: Statistics of the doubly-annotated data.5 ExperimentsIn our experiments, we use redshift,1atransition-based arc-eager dependency parser thatimplements the dynamic oracle (Goldberg andNivre, 2012) with averaged perceptron training.
Wemodified the parser2to read confusion matrices andweigh the updates with the respective ?.
We com-pare the five (?2) factorized systems to a baselinesystem that does not take confusion probabilitiesinto account, i.e., standard redshift.
Through-out the experiments, we fix the number of iterationsto 5, and we use pseudo-projectivization (Nivre andNilsson, 2005).3The parser does not include mor-phological features, which lowers performance formorphological rich languages like FI.
We report la-beled attachment scores (LAS) incl.
punctuation.1https://github.com/syllog1sm/redshift2The modified code, as well as the confusion matrices for allfactorizations, is available at https://bitbucket.org/lowlands/iaa-parsing315?33% of the sentences contain non-projectivities.We use bootstrap sampling in all our experimentsin order to get more reliable results.
This methodallows abstracting away from biases?in samplingand annotation?of training and test splits.
Weuse two complementary evaluation methods: cross-validation within the training data, and learningcurves against the test set.
We calculate significanceusing the approximate randomization test (Noreen,1989) with 10k iterations.Cross-validation In this setup, we perform 50runs of 5-fold cross validation on bootstrap-basedsamples of the training data.
This allows us to gaugethe effect of our factorization without committing toa certain test set.
We report on the average of thetotal of 250 runs.Learning curve To calculate the learning curves,we train the parser on increasing amounts of train-ing data, bootstrap-sampled in steps of 10%, andevaluate against the test set.
Each 10% incrementis repeated k = 50 times.
We finally report averageoverall error reduction over the baseline.6 ResultsCross-validation The results for cross-validationare shown in Table 3.
For 5 out of the 6 languageswe get significant improvements over the baselinewith some factorization.
We obtain improvementson all treebanks using LABELD, and on five out ofsix using CHILDPOSD.
For CA, with the smallestdoubly-annotated sample, results are not as consis-tent across the two evaluation methods.Learning curve Table 4 summarizes the overallaverage error reduction over the 10-step bootstrap-based learning curve (with 50 runs at each step).We get consistent improvements for languages forwhich we have a sample of 100+ sentences (Ta-ble 2).
Again, the most robust factorization is LA-BELD.
Figure 2 shows the learning curves for thesystem with the highest error reduction (NO withCHILDPOSD).Additional studies In order to evaluate whetherour results are meaningful and not just artifactsof random regularization, we performed a sanitycheck for the best performing system and factoriza-tion (i.e., NO with CHILDPOSD factorization).
We1359BASELINE CHILDPOSD LABEL LABELD HEADPOS HEADPOSDNO 90.98 92.67?
91.16 91.34 92.08?
90.48EN 81.72 83.48?
80.35 83.05?
85.89?
85.91?DA 80.56 83.67?
82.90?
82.47?
83.23?
84.11?CA 83.78 83.26 84.21?
83.79 82.84 82.61HR 76.94 78.07 78.22 77.52 79.49* 78.71*FI 66.19 66.74 64.88 67.18 65.63 65.27Table 3: Crossvalidation results (in LAS incl.
punctuation).
Gray: below baseline.
Best factorization per language inboldface.
Significance at p < 0.01 (computed over runs and wrt baseline) is indicated by ?
.788286child pos% of dataattachmentscore10 30 50 70 90Figure 2: Bootstrap learning curve (k=50) for NO withCHILDPOSD.
Black: LAS, green: UAS; solid line: base-line; dashed line: IAA-weighted model.CHILDPOSD LABEL LABELD HEADPOS HEADPOSDNO 6.4% 0.6% 0.7% 3.3% 1.2%EN 2.0% 2.6% 2.9% 5.3% 3.8%DA 0.7% 1.6% 1.0% 2.0% 1.0%CA -2.0% -0.1% -0.1% -2.9% -2.8%HR -0.2% 0.3% 0.7% 0.1% 0.1%FI 0.4% -0.4% 0.1% -0.1% -0.70%Table 4: Overall avg.
error red.
across learning curves.shuffled the confusion matrix and ran the bootstraplearning curve with k = 50 repetitions, for five dif-ferent shufflings.
The mean over the five runs for theoverall average error reductions is negative (-0.38%,compared to the 2.4% mean for the original, non-shuffled version).
We thus conclude that our factor-izations capture linguistically plausible informationrather than random noise.7 Related WorkPlank et al (2014a) propose IAA-weighted cost-sensitive learning for POS tagging.
We extend theirline of work to dependency parsing.A single sentence can have more than one plausi-ble dependency annotation.
Some researchers haveproposed evaluation metrics that do not penalize dis-agreements (Schwartz et al, 2011; Tsarfaty et al,2011), while others have argued that we should in-stead ensure the consistency of treebanks (Dickin-son, 2010; Manning, 2011; McDonald et al, 2013).Others have claimed that because of these ambigu-ities, only downstream evaluations are meaningful(Elming et al, 2013).Syntactic annotation disagreement has typicallybeen studied in the context of treebank develop-ment.
Haverinen et al (2012), for example, ana-lyze annotator disagreement for Finnish dependencysyntax, and compare it against parser performance.Skj?rholt (2014) use doubly-annotated data to eval-uate various agreement metrics.
Our paper differsfrom both lines of research in that we leverage dis-agreements from doubly-annotated data to obtainmore robust models.
While we agree that evaluationmetrics should probably reflect disagreements, weshow that our learning algorithms can indeed bene-fit from information about disagreement, also usingstandard performance metrics.8 ConclusionsWe have evaluated five different factorizations on sixtreebanks to evaluate the impact of IAA-weightedlearning for dependency parsing, obtaining promis-ing results.
The findings support our hypothesis thatannotator disagreement is informative for parsing.The LABELD factorization?which takes both la-beling and word order into account?is the overallmost robust factorization across all languages.
How-ever, the best factorization for each language varies.This variation can be a result of the morphosyntax ofthe language, but also of the dependency annotationformalisms, annotation method, training corpus andsize of the doubly-annotated sample.1360AcknowledgementsWe thank Jorge Vivaldi, Filip Ginter and?Zeljko Agi?cfor providing doubly-annotated data.
This researchis partially funded by the ERC Starting Grant LOW-LANDS No.
313695.References?Zeljko Agi?c and Danijela Merkler.
2013.
Three syntac-tic formalisms for data-driven dependency parsing ofcroatian.
In Text, Speech, and Dialogue.
Springer.Blanca Arias, Nuria Bel, Merc`e Lorente, MontserratMarim?on, Alba Mil`a, Jorge Vivaldi, Muntsa Padr?o,Marina Fomicheva, and Imanol Larrea.
2014.
Boost-ing the creation of a treebank.
In LREC.Matthias Buch-Kromann, Line Mikkelsen, andStine Kern Lynge.
2003.
Danish dependencytreebank.
In TLT.Matthias Buch-Kromann, J?urgen Wedekind, and JakobElming.
2007.
The Copenhagen Danish-English Dependency Treebank v.2.0.
http://buch-kromann.dk/matthias/cdt2.0.Markus Dickinson.
2010.
Detecting errors inautomatically-parsed dependency relations.
In ACL.Jakob Elming, Anders Johannsen, Sigrid Klerke,Emanuele Lapponi, Hector Martinez, and AndersS?gaard.
2013.
Down-stream effects of tree-to-dependency conversions.
In NAACL.Yoav Goldberg and Joakim Nivre.
2012.
A dynamic or-acle for arc-eager dependency parsing.
In COLING.Katri Haverinen, Timo Viljanen, Veronika Laippala,Samuel Kohonen, Filip Ginter, and Tapio Salakoski.2010.
Treebanking finnish.
In TLT.Katri Haverinen, Filip Ginter, Samuel Kohonen, TimoViljanen, Jenna Nyblom, and Tapio Salakoski.
2012.A dependency-based analysis of treebank annotationerrors.
In Computational Dependency Theory.
IOSPress.Christopher D Manning.
2011.
Part-of-speech taggingfrom 97% to 100%: is it time for some linguistics?In Computational Linguistics and Intelligent Text Pro-cessing.
Springer.Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, KuzmanGanchev, Keith Hall, Slav Petrov, Hao Zhang, OscarT?ackstr?om, Claudia Bedini, N?uria Bertomeu Castell?o,and Jungmee Lee.
2013.
Universal dependency anno-tation for multilingual parsing.
In ACL.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projectivedependency parsing.
In ACL.Eriw W. Noreen.
1989.
Computer-intensive methods fortesting hypotheses: an introduction.
Wiley.Barbara Plank, Dirk Hovy, and Anders S?gaard.
2014a.Learning part-of-speech taggers with inter-annotatoragreement loss.
In EACL.Barbara Plank, Dirk Hovy, and Anders S?gaard.
2014b.Linguistically debatable or just plain wrong?
In ACL.Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rap-poport.
2011.
Neutralizing linguistically problematicannotations in unsupervised dependency parsing eval-uation.
In ACL.Arne Skj?rholt.
2014.
A chance-corrected measure ofinter-annotator agreement for syntax.
In ACL.Per Erik Solberg, Arne Skj?rholt, Lilja ?vrelid, KristinHagen, and Janne Bondi Johannessen.
2014.
The Nor-wegian Dependency Treebank.
In LREC.Reut Tsarfaty, Joakim Nivre, and Evelina Ndersson.2011.
Evaluating dependency parsing: robust andheuristics-free cross-nnotation evaluation.
In EMNLP.Daniel Zeman.
2010.
Hard problems of tagset conver-sion.
In Proceedings of the Second International Con-ference on Global Interoperability for Language Re-sources.1361
