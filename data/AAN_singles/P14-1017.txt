Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 175?185,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsThe effect of wording on message propagation:Topic- and author-controlled natural experiments on TwitterChenhao TanDept.
of Computer ScienceCornell Universitychenhao@cs.cornell.eduLillian LeeDept.
of Computer ScienceCornell Universityllee@cs.cornell.eduBo PangGoogle Inc.bopang42@gmail.comAbstractConsider a person trying to spread animportant message on a social network.He/she can spend hours trying to craft themessage.
Does it actually matter?
Whilethere has been extensive prior work look-ing into predicting popularity of social-media content, the effect of wording perse has rarely been studied since it is of-ten confounded with the popularity of theauthor and the topic.
To control for theseconfounding factors, we take advantageof the surprising fact that there are manypairs of tweets containing the same url andwritten by the same user but employingdifferent wording.
Given such pairs, weask: which version attracts more retweets?This turns out to be a more difficult taskthan predicting popular topics.
Still, hu-mans can answer this question better thanchance (but far from perfectly), and thecomputational methods we develop can dobetter than both an average human and astrong competing method trained on non-controlled data.1 IntroductionHow does one make a message ?successful??
Thisquestion is of interest to many entities, includingpolitical parties trying to frame an issue (Chongand Druckman, 2007), and individuals attemptingto make a point in a group meeting.
In the firstcase, an important type of success is achieved ifthe national conversation adopts the rhetoric of theparty; in the latter case, if other group membersrepeat the originating individual?s point.The massive availability of online messages,such as posts to social media, now affords re-searchers new means to investigate at a very largescale the factors affecting message propagation,also known as adoption, sharing, spread, or vi-rality.
According to prior research, important fea-tures include characteristics of the originating au-thor (e.g., verified Twitter user or not, author?smessages?
past success rate), the author?s socialnetwork (e.g., number of followers), message tim-ing, and message content or topic (Artzi et al,2012; Bakshy et al, 2011; Borghol et al, 2012;Guerini et al, 2011; Guerini et al, 2012; Hansenet al, 2011; Hong et al, 2011; Lakkaraju et al,2013; Milkman and Berger, 2012; Ma et al, 2012;Petrovi?c et al, 2011; Romero et al, 2013; Suh etal., 2010; Sun et al, 2013; Tsur and Rappoport,2012).
Indeed, it?s not surprising that one of themost retweeted tweets of all time was from userBarackObama, with 40M followers, on November6, 2012: ?Four more years.
[link to photo]?.Our interest in this paper is the effect of alterna-tive message wording, meaning how the messageis said, rather than what the message is about.
Incontrast to the identity/social/timing/topic featuresmentioned above, wording is one of the few fac-tors directly under an author?s control when he orshe seeks to convey a fixed piece of content.
Forexample, consider a speaker at the ACL businessmeeting who has been tasked with proposing thatParis be the next ACL location.
This person can-not on the spot become ACL president, change theshape of his/her social network, wait until the nextmorning to speak, or campaign for Rome instead;but he/she can craft the message to be more hu-morous, more informative, emphasize certain as-pects instead of others, and so on.
In other words,we investigate whether a different choice of wordsaffects message propagation, controlling for userand topic: would user BarackObama have gottensignificantly more (or fewer) retweets if he hadused some alternate wording to announce his re-election?Although we cannot create a parallel universe175Table 1: Topic- and author-controlled (TAC) pairs.
Topic control = inclusion of the same URL.author tweets #retweetsnatlsecuritycnn t1: FIRST ON CNN: After Petraeus scandal, Paula Broadwell looks to recapture ?normal life.?
http://t.co/qy7GGuYW n1= 5t2: First on CNN: Broadwell photos shared with Security Clearance as she and her family fight media portrayal of her [same URL] n2= 29ABC t1: Workers, families take stand against Thanksgiving hours: http://t.co/J9mQHiIEqv n1= 46t2: Staples, Medieval Times Workers Say Opening Thanksgiving Day Crosses the Line [same URL] n2= 27cactus music t1: I know at some point you?ve have been saved from hunger by our rolling food trucks friends.
Let?s help support them!http://t.co/zg9jwA5jn1= 2t2: Food trucks are the epitome of small independently owned LOCAL businesses!
Help keep them going!
Sign the petition [sameURL]n2= 13in which BarackObama tweeted something else1,fortunately, a surprising characteristic of Twitterallows us to run a fairly analogous natural exper-iment: external forces serendipitously provide anenvironment that resembles the desired controlledsetting (DiNardo, 2008).
Specifically, it turns outto be unexpectedly common for the same user topost different tweets regarding the same URL ?a good proxy for fine-grained topic2?
within arelatively short period of time.3Some examplepairs are shown in Table 1; we see that the pairedtweets may differ dramatically, going far beyondword-for-word substitutions, so that quite interest-ing changes can be studied.Looking at these examples, can one in fact tellfrom the wording which tweet in a topic- andauthor-controlled pair will be more successful?The answer may not be a priori clear.
For example,for the first pair in the table, one person we askedfound t1?s invocation of a ?scandal?
to be moreattention-grabbing; but another person preferredt2because it is more informative about the URL?scontent and includes ?fight media portrayal?.
Inan Amazon Mechanical Turk (AMT) experiment(?4), we found that humans achieved an averageaccuracy of 61.3%: not that high, but better thanchance, indicating that it is somewhat possible forhumans to predict greater message spread fromdifferent deliveries of the same information.Buoyed by the evidence of our AMT study thatwording effects exist, we then performed a batteryof experiments to seek generally-applicable, non-1Cf.
the Music Lab ?multiple universes?
experiment totest the randomness of popularity (Salganik et al, 2006).2Although hashtags have been used as coarse-grainedtopic labels in prior work, for our purposes, we have no assur-ance that two tweets both using, say, ?#Tahrir?
would be at-tempting to express the same message but in different words.In contrast, see the same-URL examples in Table 1.3Moreover, Twitter presents tweets to a reader in strictchronological order, so that there are no algorithmic-rankingeffects to compensate for in determining whether readers sawa tweet.
And, Twitter accumulates retweet counts for the en-tire retweet cascade and displays them for the original tweetat the root of the propagation tree, so we can directly useTwitter?s retweet counts to compare the entire reach of thedifferent versions.Twitter-specific features of more successful phras-ings.
?5.1 applies hypothesis testing (with Bonfer-roni correction to ameliorate issues with multiplecomparisons) to investigate the utility of featureslike informativeness, resemblance to headlines,and conformity to the community norm in lan-guage use.
?5.2 further validates our findings viaprediction experiments, including on completelyfresh held-out data, used only once and after anarray of standard cross-validation experiments.4We achieved 66.5% cross-validation accuracy and65.6% held-out accuracy with a combination ofour custom features and bag-of-words.
Our clas-sifier fared significantly better than a number ofbaselines, including a strong classifier trained onthe most- and least-retweeted tweets that was evengranted access to author and timing metadata.2 Related workThe idea of using carefully controlled experimentsto study effective communication strategies datesback at least to Hovland et al (1953).
Recentstudies range from examining what characteris-tics of New York Times articles correlate with highre-sharing rates (Milkman and Berger, 2012) tolooking at how differences in description affectthe spread of content-controlled videos or images(Borghol et al, 2012; Lakkaraju et al, 2013).Simmons et al (2011) examined the variation ofquotes from different sources to examine how tex-tual memes mutate as people pass them along, butdid not control for author.
Predicting the ?success?of various texts such as novels and movie quoteshas been the aim of additional prior work not al-ready mentioned in ?1 (Ashok et al, 2013; Louisand Nenkova, 2013; Danescu-Niculescu-Mizil etal., 2012; Pitler and Nenkova, 2008; McIntyre andLapata, 2009).
To our knowledge, there have beenno large-scale studies exploring wording effects ina both topic- and author-controlled setting.
Em-ploying such controls, we find that predicting themore effective alternative wording is much harderthan the previously well-studied problem of pre-4And after crossing our fingers.176dicting popular content when author or topic canfreely vary.Related work regarding the features we consid-ered is deferred to ?5.1 (features description).3 DataOur main dataset was constructed by first gath-ering 1.77M topic- and author-controlled (hence-forth TAC) tweet pairs5differing in more than justspacing.6We accomplished this by crawling time-lines of 236K user ids that appear in prior work(Kwak et al, 2010; Yang and Leskovec, 2011)via the Twitter API.
This crawling process alsoyielded 632K TAC pairs whose only differencewas spacing, and an additional 558M ?unpaired?tweets; as shown later in this paper, we used theseextra corpora for computing language models andother auxiliary information.
We applied non-obvious but important filtering ?
described laterin this section ?
to control for other external fac-tors and to reduce ambiguous cases.
This broughtus to a set of 11,404 pairs, with the gold-standardlabels determined by which tweet in each pair wasthe one that received more retweets according tothe Twitter API.
We then did a second crawl toget an additional 1,770 pairs to serve as a held-outdataset.
The corresponding tweet IDs are availableonline at http://chenhaot.com/pages/wording-for-propagation.html.
(Twit-ter?s terms of service prohibit sharing the actualtweets.
)Throughout, we refer to the textual content ofthe earlier tweet within a TAC pair as t1, and of thelater one as t2.
We denote the number of retweetsreceived by each tweet by n1and n2, respectively.We refer to the tweet with higher (lower) nias the?better (worse)?
tweet.Using ?identical?
pairs to determine how tocompensate for follower-count and timing ef-fects.
In an ideal setting, differences betweenn1and n2would be determined solely by dif-ferences in wording.
But even with a TAC pair,retweets might exhibit a temporal bias because ofthe chronological order of tweet presentation (t1might enjoy a first-mover advantage (Borghol etal., 2012) because it is the ?original?
; alternatively,5No data collection/processing was conducted at Google.6The total excludes: tweets containing multiple URLs;tweets from users posting about the same URL more than fivetimes (since such users might be spammers); the third, fourth,or fifth version for users posting between three and five tweetsfor the same URL; retweets (as identified by Twitter?s API orby beginning with ?RT @?
); non-English tweets.3 6 12 18 24 36 48time lag (hours)246810121416D >1K f?ers>2.5K f?ers>5K f?ers>10K f?ers(a) For identical TAC pairs,retweet-count deviation vs.time lag between t1andt2, for the author follower-counts given in the legend.0 2 4 6 8n10246810E?
(n 2|n 1)>5K f?ers,<12hrsotherwise(b) Avg.
n2vs.
n1for iden-tical TAC pairs, highlightingour chosen time-lag and fol-lower thresholds.
Bars: stan-dard error.
Diagonal line:pEpn2|n1q ?
n1.Figure 1: (a): The ideal case where n2?
n1when t1?
t2is best approximated when t2oc-curs within 12 hours of t1and the author has atleast 10,000 or 5,000 followers.
(b): in our chosensetting (blue circles), n2indeed tends to track n1,whereas otherwise (black squares), there?s a biastowards retweeting t1.t2might be preferred because retweeters considert1to be ?stale?).
Also, the number of followers anauthor has can have complicated indirect effectson which tweets are read (space limits precludediscussion).We use the 632K TAC pairs wherein t1andt2are identical7to check for such confoundingeffects: we see how much n2deviates from n1in such settings, since if wording were the onlyexplanatory factor, the retweet rates for identicaltweets ought to be equal.
Figure 1(a) plots howthe time lag between t1and t2and the author?sfollower-count affect the following deviation esti-mate:D ?
?0?n1?10| pEpn2|n1q ?
n1|,wherepEpn2|n1q is the average value of n2overpairs whose t1is retweeted n1times.
(Note thatthe number of pairs whose t1is retweeted n1timesdecays exponentially with n1; hence, we condi-tion on n1to keep the estimate from being domi-nated by pairs with n1?
0, and do not considern1?
10 because there are too few such pairs to es-timatepEpn2|n1q reliably.)
Figure 1(a) shows thatthe setting where we (i) minimize the confound-ing effects of time lag and author?s follower-countand (ii) maximize the amount of data to work with7Identical up to spacing: Twitter prevents exact copies bythe same author appearing within a short amount of time, butsome authors work around this by inserting spaces.177is: when t2occurs within 12 hours after t1andthe author has more than 5,000 followers.
Figure1(b) confirms that for identical TAC pairs, our cho-sen setting indeed results in n2being on averageclose to n1, which corresponds to the desired set-ting where wording is the dominant differentiatingfactor.8Focus on meaningful and general changes.Even after follower-count and time-lapse filtering,we still want to focus on TAC pairs that (i) ex-hibit significant/interesting textual changes (as ex-emplified in Table 1, and as opposed to typo cor-rections and the like), and (ii) have n2and n1suf-ficiently different so that we are confident in whichtiis better at attracting retweets.
To take care of(i), we discarded the 50% of pairs whose similar-ity was above the median, where similarity wastf-based cosine.9For (ii), we sorted the remain-ing pairs by n2?
n1and retained only the top andbottom 5%.10Moreover, to ensure that we do notoverfit to the idiosyncrasies of particular authors,we cap the number of pairs contributed by eachauthor to 50 before we deal with (ii).4 Human accuracy on TAC pairsWe first ran a pilot study on Amazon Mechan-ical Turk (AMT) to determine whether humanscan identify, based on wording differences alone,which of two topic- and author- controlled tweetsis spread more widely.
Each of our 5 AMT tasksinvolved a disjoint set of 20 randomly-sampledTAC pairs (with t1and t2randomly reordered);subjects indicated ?which tweet would other peo-ple be more likely to retweet?
?, provided a shortjustification for their binary response, and clickeda checkbox if they found that their choice was a?close call?.
We received 39 judgments per pair inaggregate from 106 subjects total (9 people com-pleted all 5 tasks).
The subjects?
justificationswere of very high quality, convincing us that theyall did the task in good faith11.
Two examples for8We also computed the Pearson correlation between n1and n2, even though it can be dominated by pairs with smallern1.
The correlation is 0.853 for ??
5K f?ers, ?12hrs?,clearly higher than the 0.305 correlation for ?otherwise?.9Idf weighting was not employed because changes to fre-quent words are of potential interest.
Urls, hashtags, @-mentions and numbers were normalized to [url], [hashtag],[at], and [num] before computing similarity.10For our data, this meant n2?
n1?
10 or ?
?15.
Cf.our median number of retweets: 30.11We also note that the feedback we got was quite pos-itive, including: ?...It?s fun to make choices between closetweets and use our subjective opinion.
Thanks and best ofthe third TAC pair in Table 1 were: ?
[t1makes] thecause relate-able to some people, therefore show-ing more of an appeal as to why should they clickthe link and support?
and, expressing the oppositeview, ?I like [t2] more because [t1] starts out witha generalization that doesn?t affect me and try tomake me look like I had that experience before?.If we view the set of 3900 binary judgmentsfor our 100-TAC-pair sample as constituting in-dependent responses, then the accuracy for thisset is 62.4% (rising to 63.8% if we exclude the587 judgments deemed ?close calls?).
However, ifwe evaluate the accuracy of the majority responseamong the 39 judgments per pair, the number risesto 73%.
The accuracy of the majority responsegenerally increases with the dominance of the ma-jority, going above 90% when at least 80% of thejudgments agree (although less than a third of thepairs satisfied this criterion).Alternatively, we can consider the average ac-curacy of the 106 subjects: 61.3%, which is bet-ter than chance but far from 100%.
(Variance washigh: one subject achieved 85% accuracy out of20 pairs, but eight scored below 50%.)
This re-sult is noticeably lower than the 73.8%-81.2% re-ported by Petrovi?c et al (2011), who ran a sim-ilar experiment involving two subjects and 202tweet pairs, but where the pairs were not topic- orauthor-controlled.12We conclude that even though propagation pre-diction becomes more challenging when topicand author controls are applied, humans canstill to some degree tell which wording attractsmore retweets.
Interested readers can try thisout themselves at http://chenhaot.com/retweetedmore/quiz.5 ExperimentsWe now investigate computationally what word-ing features correspond to messages achieving abroader reach.
We start (?5.1) by introducing a setof generally-applicable and (mostly) non-Twitter-specific features to capture our intuitions aboutwhat might be better ways to phrase a message.We then use hypothesis testing (?5.1) to evaluatethe importance of each feature for message prop-luck with your research?
and ?This was very interesting andreally made me think about how I word my own tweets.
Greatjob on this survey!?.
We only had to exclude one person (notcounted among the 106 subjects), doing so because he or shegave the same uninformative justification for all pairs.12The accuracy range stems from whether author?s socialfeatures were supplied and which subject was considered.178Table 2: Notational conventions for tables in ?5.1.One-sided paired t-test for feature efficacy????
: p?1e-20 ????
: p?1-1e-20???
: p?0.001 ???
: p?0.999??
: p?0.01 ??
: p?0.99?
: p?0.05 ?
: p?0.95?
: passes our Bonferroni correctionOne-sided binomial test for feature increase(Do authors prefer to ?raise?
the feature in t2?
)YES : t2has a higher feature score than t1, ?
?
.05NO : t2has a lower feature score than t1, ?
?
.05(x%): %pf2?
f1q, if sig.
larger or smaller than 50%agation and the extent to which authors employit, followed by experiments on a prediction task(?5.2) to further examine the utility of these fea-tures.5.1 Features: efficacy and author preferenceWhat kind of phrasing helps message propaga-tion?
Does it work to explicitly ask people to sharethe message?
Is it better to be short and concise orlong and informative?
We define an array of fea-tures to capture these and other messaging aspects.We then examine (i) how effective each feature isfor attracting more retweets; and (ii) whether au-thors prefer applying a given feature when issuinga second version of a tweet.First, for each feature, we use a one-sided pairedt-test to test whether, on our 11K TAC pairs, ourscore function for that feature is larger in the bet-ter tweet versions than in the worse tweet versions,for significance levels ?
?
.05, .01, .001, 1e-20.Given that we did 39 tests in total, there is a riskof obtaining false positives due to multiple test-ing (Dunn, 1961; Benjamini and Hochberg, 1995).To account for this, we also report significance re-sults for the conservatively Bonferroni-corrected(?BC?)
significance level ?
= 0.05/39=1.28e-3.Second, we examine author preference for ap-plying a feature.
We do so because one (but by nomeans the only) reason authors post t2after havingalready advertised the same URL in t1is that theseauthors were dissatisfied with the amount of atten-tion t1got; in such cases, the changes may havebeen specifically intended to attract more retweets.We measure author preference for a feature by thepercentage of our TAC pairs13where t2has more?occurrences?
of the feature than t1, which we de-note by ?%pf2?
f1q?.
We use the one-sided bi-nomial test to see whether %pf2?
f1q is signifi-cantly larger (or smaller) than 50%.13For our preference experiments, we added in pairs wheren2?
n1was not in the top or bottom 5% (cf.
?3, meaningfulchanges), since to measure author preference it?s not neces-sary that the retweet counts differ significantly.Table 3: Explicit requests for sharing (where onlyoccurrences POS-tagged as verbs count, accordingto the Gimpel et al (2011) tagger).effective?
author-preferred?rt ????
* ?
?retweet ????
* YES (59%)spread ???
?
* YES (56%)please ???
?
* ?
?pls ?
???
?
?plz ??
??
?
?Table 4: Informativeness.effective?
author-preferred?length (chars) ????
* YES (54%)verb ????
* YES (56%)noun ????
* ?
?adjective ???
?
* YES (51%)adverb ???
?
* YES (55%)proper noun ???
?
* NO?
(45%)number ????
* NO?
(48%)hashtag ?
???
??
@-mention ???
?
* YES (53%)Not surprisingly, it helps to ask people to share.
(See Table 3; the notation for all tables is ex-plained in Table 2.)
The basic sanity check weperformed here was to take as features the numberof occurrences of the verbs ?rt?, ?retweet?, ?please?,?spread?, ?pls?, and ?plz?
to capture explicit re-quests (e.g.
?please retweet?
).Informativeness helps.
(Table 4) Messages thatare more informative have increased social ex-change value (Homans, 1958), and so may bemore worth propagating.
One crude approxima-tion of informativeness is length, and we see thatlength helps.14In contrast, Simmons et al (2011)found that shorter versions of memes are morelikely to be popular.
The difference may resultfrom TAC-pair changes being more drastic thanthe variations that memes undergo.A more refined informativeness measure iscounts of the parts of speech that correspondto content.
Our POS results, gathered using aTwitter-specific tagger (Gimpel et al, 2011), echothose of Ashok et al (2013) who looked at predict-14Of course, simply inserting garbage isn?t going to leadto more retweets, but adding more information generally in-volves longer text.179Table 5: Conformity to the community and one?sown past, measured via scores assigned by variouslanguage models.effective?
author-preferred?twitter unigram ???
?
* YES (54%)twitter bigram ???
?
* YES (52%)personal unigram ???
?
* YES (52%)personal bigram ???
NO?
(48%)ing the success of books.
The diminished effect ofhashtag inclusion with respect to what has been re-ported previously (Suh et al, 2010; Petrovi?c et al,2011) presumably stems from our topic and authorcontrols.Be like the community, and be true to yourself(in the words you pick, but not necessarily inhow you combine them).
(Table 5) Although dis-tinctive messages may attract attention, messagesthat conform to expectations might be more eas-ily accepted and therefore shared.
Prior work hasexplored this tension: Lakkaraju et al (2013), in acontent-controlled study, found that the more up-voted Reddit image titles balance novelty and fa-miliarity; Danescu-Niculescu-Mizil et al (2012)(henceforth DCKL?12) showed that the memora-bility of movie quotes corresponds to higher lexi-cal distinctiveness but lower POS distinctiveness;and Sun et al (2013) observed that deviating fromone?s own past language patterns correlates withmore retweets.Keeping in mind that the authors in our datahave at least 5000 followers15, we consider twotypes of language-conformity constraints an au-thor might try to satisfy: to be similar to whatis normal in the Twitter community, and to besimilar to what his or her followers expect.
Wemeasure a tweet?s similarity to expectations by itsscore according to the relevant language model,1|T |?xPT logpppxqq, where T refers to either allthe unigrams (unigram model) or all and only bi-grams (bigram model).16We trained a Twitter-community language model from our 558M un-paired tweets, and personal language models fromeach author?s tweet history.Imitate headlines.
(Table 6) News headlines areoften intentionally written to be both informativeand attention-getting, so we introduce the idea of15This is not an artificial restriction on our set of authors; alarge follower count means (in principle) that our results drawon a large sample of decisions whether to retweet or not.16The tokens [at], [hashtag], [url] were ignored in theunigram-model case to prevent their undue influence, but re-tained in the bigram model to capture longer-range usage(?combination?)
patterns.Table 6: LM-based resemblance to headlines.effective?
author-preferred?headline unigram ??
??
YES (53%)headline bigram ????
* YES (52%)Table 7: Retweet score.effective?
author-preferred?rt score ??
??
* NO?
(49%)verb rt score ????
* ?
?noun rt score ???
?
* ?
?adjective rt score ?
???
YES (50%)adverb rt score ?
???
YES (51%)proper noun rt score ???
NO?
(48%)scoring by a language model built from New YorkTimes headlines.17Use words associated with (non-paired)retweeted tweets.
(Table 7) We expect thatprovocative or sensationalistic tweets are likelyto make people react.
We found it difficult tomodel provocativeness directly.
As a roughapproximation, we check whether the changes int2with respect to t1(which share the same topicand author) involve words or parts-of-speech thatare associated with high retweet rate in a verylarge separate sample of unpaired tweets (retweetsand replies discarded).
Specifically, for each wordw that appears more than 10 times, we computethe probability that tweets containing w areretweeted more than once, denoted by rspwq.
Wedefine the rt score of a tweet as maxwPT rspwq,where T is all the words in the tweet, and thert score of a particular POS tag z in a tweet asmaxwPT&tagpwq?zrspwq.Include positive and/or negative words.
(Ta-ble 8) Prior work has found that including posi-tive or negative sentiment increases message prop-agation (Milkman and Berger, 2012; Godes et al,2005; Heath et al, 2001; Hansen et al, 2011).
Wemeasured the occurrence of positive and negativewords as determined by the connotation lexiconof Feng et al (2013) (better coverage than LIWC).Measuring the occurrence of both simultaneouslywas inspired by Riloff et al (2013).Refer to other people (but not your audience).
(Table 9) First-person has been found useful forsuccess before, but in the different domains of sci-entific abstracts (Guerini et al, 2012) and books(Ashok et al, 2013).17To test whether the results stem from similarity to newsrather than headlines per se, we constructed a NYT-text LM,which proved less effective.
We also tried using Gawkerheadlines (often said to be attention-getting) but pilot studiesrevealed insufficient vocabulary overlap with our TAC pairs.180Table 8: Sentiment (contrast is measured by pres-ence of both positive and negative sentiments).effective?
author-preferred?positive ???
?
* ?
?negative ???
?
* ?
?contrast ???
?
* ?
?Table 9: Pronouns.effective?
author-preferred?1st person singular ???
YES (51%)1st person plural ???
YES (52%)2nd person ???
YES (57%)3rd person singular ??
??
YES (55%)3rd person plural ?
???
YES (58%)Generality helps.
(Table 10) DCKL?12 positedthat movie quotes are more shared in the culturewhen they are general enough to be used in multi-ple contexts.
We hence measured the presence ofindefinite articles vs. definite articles.The easier to read, the better.
(Table 11) Wemeasure readability by using Flesch reading ease(Flesch, 1948) and Flesch-Kincaid grade level(Kincaid et al, 1975), though they are not de-signed for short texts.
We use negative grade levelso that a larger value indicates easier texts to read.Final question: Do authors prefer to do whatis effective?
Recall that we use binomial tests todetermine author preference for applying a featuremore in t2.
Our preference statistics show that au-thor preferences in many cases are aligned withfeature efficacy.
But there are several notable ex-ceptions: for example, authors tend to increase theuse of @-mentions and 2nd person pronouns eventhough they are ineffective.
On the other hand,they did not increase the use of effective oneslike proper nouns and numbers; nor did they tendto increase their rate of sentiment-bearing words.Bearing in mind that changes in t2may not alwaysbe intended as an effort to improve t1, it is still in-teresting to observe that there are some contrastsbetween feature efficacy and author preferences.5.2 Predicting the ?better?
wordingHere, we further examine the collective efficacyof the features introduced in ?5.1 via their perfor-mance on a binary prediction task: given a TACpair (t1, t2), did t2receive more retweets?Our approach.
We group the features introducedin ?5.1 into 16 lexicon-based features (Table 3,8, 9, 10), 9 informativeness features (Table 4), 6language model features (Table 5, 6), 6 rt scorefeatures (Table 7), and 2 readability features (Ta-ble 11).
We refer to all 39 of them together asTable 10: Generality.effective?
author-preferred?indefinite articles (a,an) ???
?
* ?
?definite articles (the) ???
YES (52%)Table 11: Readability.effective?
author-preferred?reading ease ??
??
YES (52%)negative grade level ?
???
YES (52%)custom features.
We also consider tagged bag-of-words (?BOW?)
features, which includes all theunigram (word:POS pair) and bigram features thatappear more than 10 times in the cross-validationdata.
This yields 3,568 unigram features and 4,095bigram features, for a total of 7,663 so-called1,2-gram features.
Values for each feature are nor-malized by linear transformation across all tweetsin the training data to lie in the range r0, 1s.18For a given TAC pair, we construct its featurevector as follows.
For each feature being consid-ered, we compute its normalized value for eachtweet in the pair and take the difference as the fea-ture value for this pair.
We use L2-regularized lo-gistic regression as our classifier, with parameterschosen by cross validation on the training data.
(We also experimented with SVMs.
The perfor-mance was very close, but mostly slightly lower.
)A strong non-TAC alternative, with social infor-mation and timing thrown in.
One baseline re-sult we would like to establish is whether the topicand author controls we have argued for, whileintuitively compelling for the purposes of tryingto determine the best way for a given author topresent some fixed content, are really necessaryin practice.
To test this, we consider an alterna-tive binary L2-regularized logistic-regression clas-sifier that is trained on unpaired data, specifically,on the collection of 10,000 most retweeted tweets(gold-standard label: positive) plus the 10,000least retweeted tweets (gold-standard label: neg-ative) that are neither retweets nor replies.
Notethat this alternative thus is granted, by design,roughly twice the training instances that our clas-sifiers have, as a result of having roughly the samenumber of tweets, since our instances are pairs.Moreover, we additionally include the tweet au-thor?s follower count, and the day and hour ofposting, as features.
We refer to this alternativeclassifier as  TAC+ff+time.
(Mnemonic: ?ff?
isused in bibliographic contexts as an abbreviation18We also tried normalization by whitening, but it did notlead to further improvements.181(a) Cross-validation and heldout accuracy for various feature sets.
Blue lines insidebars: performance when custom features are restricted to those that pass our Bon-ferroni correction (no line for readability because no readability features passed).Dashed vertical line:  TAC+ff+time performance.1000 3000 5000 7000 900058%60%62%64%66%68%70% custom+1,2-gramcustom1,2-gramhuman(b) Cross-validation accuracy vs data size.Human performance was estimated from adisjoint set of 100 pairs (see ?4).Figure 2: Accuracy results.
Pertinent significance results are as follows.
In cross-validation, custom+1,2-gram is significantly better than  TAC+ff+time (p=0) and 1,2-gram (p=3.8e-7).
In heldout validation,custom+1,2-gram is significantly better than  TAC+ff+time (p=3.4e-12) and 1,2-gram (p=0.01) but notunigram (p=0.08), perhaps due to the small size of the heldout set.for ?and the following?.)
We apply it to a tweetpair by computing whether it gives a higher scoreto t2or not.Baselines.
To sanity-check whether our classifierprovides any improvement over the simplest meth-ods one could try, we also report the performanceof the majority baseline, our request-for-sharingfeatures, and our character-length feature.Performance comparison.
We compare the ac-curacy (percentage of pairs whose labels were cor-rectly predicted) of our approach against the com-peting methods.
We report 5-fold cross validationresults on our balanced set of 11,404 TAC pairsand on our completely disjoint heldout data19of1,770 TAC pairs; this set was never examined dur-ing development, and there are no authors in com-mon between the two testing sets.Figure 2(a) summarizes the main results.
WhileTAC+ff+time outperforms the majority base-line, using all the features we proposed beatsTAC+ff+time by more than 10% in both cross-validation (66.5% vs 55.9%) and heldout valida-tion (65.6% vs 55.3%).
We outperform the aver-age human accuracy of 61% reported in our Ama-zon Mechanical Turk experiments (for a differentdata sample);  TAC+ff+time fails to do so.The importance of topic and author con-trol can be seen by further investigation ofTAC+ff+time?s performance.
First, note that19To construct this data, we used the same criteria as in?3: written by authors with more than 5000 followers, postedwithin 12 hours, n2?
n1?
10 or ?
?15, and cosine simi-larity threshold value the same as in ?3, cap of 50 on numberof pairs from any individual author.it yields an accuracy of around 55% on ouralternate-version-selection task,20even though itscross-validation accuracy on the larger most- andleast-retweeted unpaired tweets averages out to ahigh 98.8%.
Furthermore, note the superior per-formance of unigrams trained on TAC data vsTAC+ff+time ?
which is similar to our uni-grams but trained on a larger but non-TAC datasetthat included metadata.
Thus, TAC pairs are a use-ful data source even for non-custom features.
(Wealso include individual feature comparisons later.
)Informativeness is the best-performing customfeature group when run in isolation, and outper-forms all baselines, as well as  TAC+ff+time;and we can see from Figure 2(a) that this is notdue just to length.
The combination of all our 39custom features yields approximately 63% accu-racy in both testing settings, significantly outper-forming informativeness alone (p?0.001 in bothcases).
Again, this is higher than our estimate ofaverage human performance.Not surprisingly, the TAC-trained BOW fea-tures (unigram and 1,2-gram) show impressivepredictive power in this task: many of our customfeatures can be captured by bag-of-word features,in a way.
Still, the best performance is achieved20One might suspect that the problem is thatTAC+ff+time learns from its training data to over-rely on follower-count, since that is presumably a goodfeature for non-TAC tweets, and for this reason suffers whenrun on TAC data where follower-counts are by constructionnon-informative.
But in fact, we found that removing thefollower-count feature from  TAC+ff+time and re-trainingdid not lead to improved performance.
Hence, it seems thatit is the non-controlled nature of the alternate training datathat explains the drop in performance.182by combining our custom and 1,2-gram featurestogether, to a degree statistically significantly bet-ter than using 1,2-gram features alone.Finally, we remark on our Bonferroni correc-tion.
Recall that the intent of applying it is toavoid false positives.
However, in our case, Fig-ure 2(a) shows that our potentially ?false?
posi-tives ?
features whose effectiveness did not passthe Bonferroni correction test ?
actually do raiseperformance in our prediction tests.Size of training data.
Another interesting obser-vation is how performance varies with data size.For n ?
1000, 2000, .
.
.
, 10000, we randomlysampled n pairs from our 11,404 pairs, and com-puted the average cross-validation accuracy on thesampled data.
Figure 2(b) shows the averages over50 runs of the aforementioned procedure.
Our cus-tom features can achieve good performance withlittle data, in the sense that for sample size 1000,they outperform BOW features; on the other hand,BOW features quickly surpass them.
Across theboard, the custom+1,2-gram features are consis-tently better than the 1,2-gram features alone.Top features.
Finally, we examine some ofthe top-weighted individual features from our ap-proach and from the competing  TAC+ff+timeclassifier.
The top three rows of Table 12 show thebest custom and best and worst unigram featuresfor our method; the bottom two rows show the bestand worst unigrams for  TAC+ff+time.
Amongcustom features, we see that community and per-sonal language models, informativeness, retweetscores, sentiment, and generality are represented.As for unigram features, not surprisingly, ?rt?
and?retweet?
are top features for both our approachand  TAC+ff+time.
However, the other unigramsfor the two methods seem to be a bit different inspirit.
Some of the unigrams determined to bemost poor only by our method appear to be bothsurprising and yet plausible in retrospect: ?icymi?
(abbreviation for ?in case you missed it?)
tends toindicate a direct repetition of older information,so people might prefer to retweet the earlier ver-sion; ?thanks?
and ?sorry?
could correspond topersonal thank-yous and apologies not meant tobe shared with a broader audience, and similarly@-mentioning another user may indicate a tweetintended only for that person.
The appearance of[hashtag] in the best  TAC+ff+time unigrams isconsistent with prior research in non-TAC settings(Suh et al, 2010; Petrovi?c et al, 2011).Table 12: Features with largest coefficients, de-limited by commas.
POS tags omitted for clarity.Our approachbest 15 custom twitter bigram, length (chars), rt(the word), retweet (the word), verb, verb retweet score,personal unigram, proper noun, number, noun, positivewords, please (the word), proper noun retweet score,indefinite articles (a,an), adjectivebest 20 unigrams rt, retweet, [num], breaking,is, win, never, ., people, need, official, officially, are,please, november, world, girl, !!
!, god, newworst 20 unigrams :, [at], icymi, also, comments,half, ?, earlier, thanks, sorry, highlights, bit, point, up-date, last, helping, peek, what, haven?t, debateTAC+ff+timebest 20 unigrams [hashtag], teen, fans, retweet,sale, usa, women, butt, caught, visit, background, up-coming, rt, this, bieber, these, each, chat, houston, bookworst 20 unigrams :, ..., boss, foundation, ?, ?,others, john, roll, ride, appreciate, page, drive, correct,full, ?, looks, @ (not as [at]), sales, hurts6 ConclusionIn this work, we conducted the first large-scaletopic- and author-controlled experiment to studythe effects of wording on information propagation.The features we developed to choose the bet-ter of two alternative wordings posted better per-formance than that of all our comparison algo-rithms, including one given access to author andtiming features but trained on non-TAC data, andalso bested our estimate of average human perfor-mance.
According to our hypothesis tests, help-ful wording heuristics include adding more infor-mation, making one?s language align with bothcommunity norms and with one?s prior messages,and mimicking news headlines.
Readers maytry out their own alternate phrasings at http://chenhaot.com/retweetedmore/ to seewhat a simplified version of our classifier predicts.In future work, it will be interesting to examinehow these features generalize to longer and moreextensive arguments.
Moreover, understandingthe underlying psychological and cultural mecha-nisms that establish the effectiveness of these fea-tures is a fundamental problem of interest.Acknowledgments.
We thank C. Callison-Burch,C.
Danescu-Niculescu-Mizil, J. Kleinberg, P.Mahdabi, S. Mullainathan, F. Pereira, K. Raman,A.
Swaminathan, the Cornell NLP seminar par-ticipants and the reviewers for their comments; J.Leskovec for providing some initial data; and theanonymous annotators for all their labeling help.This work was supported in part by NSF grant IIS-0910664 and a Google Research Grant.183ReferencesYoav Artzi, Patrick Pantel, and Michael Gamon.
2012.Predicting responses to microblog posts.
In Pro-ceedings of NAACL (short paper).Vikas Ganjigunte Ashok, Song Feng, and Yejin Choi.2013.
Success with style: Using writing style topredict the success of novels.
In Proceedings ofEMNLP.Eitan Bakshy, Jake M. Hofman, Winter A. Mason, andDuncan J. Watts.
2011.
Everyone?s an influencer:Quantifying influence on twitter.
In Proceedings ofWSDM.Yoav Benjamini and Yosef Hochberg.
1995.
Control-ling the false discovery rate: A practical and pow-erful approach to multiple testing.
Journal of theRoyal Statistical Society.
Series B (Methodological),pages 289?300.Youmna Borghol, Sebastien Ardon, Niklas Carlsson,Derek Eager, and Anirban Mahanti.
2012.
Theuntold story of the clones: Content-agnostic factorsthat impact YouTube video popularity.
In Proceed-ings of KDD.Dennis Chong and James N. Druckman.
2007.
Fram-ing theory.
Annual Review of Political Science,10:103?126.Cristian Danescu-Niculescu-Mizil, Justin Cheng, JonKleinberg, and Lillian Lee.
2012.
You had me athello: How phrasing affects memorability.
In Pro-ceedings of ACL.John DiNardo.
2008.
Natural experiments and quasi-natural experiments.
In The New Palgrave Dictio-nary of Economics.
Palgrave Macmillan.Olive Jean Dunn.
1961.
Multiple comparisons amongmeans.
Journal of the American Statistical Associa-tion, 56(293):52?64.Song Feng, Jun Seok Kang, Polina Kuznetsova, andYejin Choi.
2013.
Connotation lexicon: A dash ofsentiment beneath the surface meaning.
In Proceed-ings of ACL.Rudolph Flesch.
1948.
A new readability yardstick.Journal of applied psychology, 32(3):221.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech Taggingfor Twitter: Annotation, Features, and Experiments.In Proceedings of NAACL (short paper).David Godes, Dina Mayzlin, Yubo Chen, SanjivDas, Chrysanthos Dellarocas, Bruce Pfeiffer, BarakLibai, Subrata Sen, Mengze Shi, and Peeter Verlegh.2005.
The firm?s management of social interactions.Marketing Letters, 16(3-4):415?428.Marco Guerini, Carlo Strapparava, and G?ozde?Ozbal.2011.
Exploring text virality in social networks.
InProceedings of ICWSM (poster).Marco Guerini, Alberto Pepe, and Bruno Lepri.
2012.Do linguistic style and readability of scientific ab-stracts affect their virality?
In Proceedings ofICWSM (poster).Lars Kai Hansen, Adam Arvidsson, Finn?Arup Nielsen,Elanor Colleoni, and Michael Etter.
2011.
Goodfriends, bad news-affect and virality in Twitter.Communications in Computer and Information Sci-ence, 185:34?43.Chip Heath, Chris Bell, and Emily Sternberg.
2001.Emotional selection in memes: The case of urbanlegends.
Journal of personality and social psychol-ogy, 81(6):1028.George C. Homans.
1958.
Social Behavior as Ex-change.
American Journal of Sociology, 63(6):597?606.Liangjie Hong, Ovidiu Dan, and Brian D. Davison.2011.
Predicting popular messages in Twitter.
InProceedings of WWW.Carl I. Hovland, Irving L. Janis, and Harold H. Kelley.1953.
Communication and Persuasion: Psycholog-ical Studies of Opinion Change, volume 19.
YaleUniversity Press.J.
Peter Kincaid, Robert P. Fishburne Jr., Richard L.Rogers, and Brad S. Chissom.
1975.
Derivationof new readability formulas (automated readabilityindex, fog count and flesch reading ease formula)for navy enlisted personnel.
Technical report, DTICDocument.Haewoon Kwak, Changhyun Lee, Hosung Park, andSue Moon.
2010.
What is Twitter, a social networkor a news media?
In Proceedings of WWW.Himabindu Lakkaraju, Julian McAuley, and JureLeskovec.
2013.
What?s in a name?
Understandingthe interplay between titles, content, and communi-ties in social media.
In Proceedings of ICWSM.Annie Louis and Ani Nenkova.
2013.
What makeswriting great?
First experiments on article qualityprediction in the science journalism domain.
Trans-actions of ACL.Zongyang Ma, Aixin Sun, and Gao Cong.
2012.
Willthis #hashtag be popular tomorrow?
In Proceedingsof SIGIR.Neil McIntyre and Mirella Lapata.
2009.
Learning totell tales: A data-driven approach to story genera-tion.
In Proceedings of ACL-IJCNLP.Katherine L Milkman and Jonah Berger.
2012.
Whatmakes online content viral?
Journal of MarketingResearch, 49(2):192?205.184Sa?sa Petrovi?c, Miles Osborne, and Victor Lavrenko.2011.
RT to win!
Predicting message propagationin Twitter.
In Proceedings of ICWSM.Emily Pitler and Ani Nenkova.
2008.
Revisitingreadability: A unified framework for predicting textquality.
In Proceedings of EMNLP.Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-dra De Silva, Nathan Gilbert, and Ruihong Huang.2013.
Sarcasm as contrast between a positive sen-timent and negative situation.
In Proceedings ofEMNLP.Daniel M. Romero, Chenhao Tan, and Johan Ugander.2013.
On the interplay between social and topicalstructure.
In Proceedings of ICWSM.Matthew J. Salganik, Peter Sheridan Dodds, and Dun-can J. Watts.
2006.
Experimental study of inequal-ity and unpredictability in an artificial cultural mar-ket.
Science, 311(5762):854?856.Matthew P. Simmons, Lada A Adamic, and Eytan Adar.2011.
Memes online: Extracted, subtracted, in-jected, and recollected.
In Proceedings of ICWSM.Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed H.Chi.
2010.
Want to be retweeted?
Large scale an-alytics on factors impacting retweet in Twitter net-work.
In Proceedings of SocialCom.Tao Sun, Ming Zhang, and Qiaozhu Mei.
2013.
Unex-pected relevance: An empirical study of serendipityin retweets.
In Proceedings of ICWSM.Oren Tsur and Ari Rappoport.
2012.
What?s in a hash-tag?
: Content based prediction of the spread of ideasin microblogging communities.
In Proceedings ofWSDM.Jaewon Yang and Jure Leskovec.
2011.
Patterns oftemporal variation in online media.
In Proceedingsof WSDM.185
