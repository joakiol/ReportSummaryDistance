Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp.
1170?1174,Prague, June 2007. c?2007 Association for Computational LinguisticsAdapting the RASP System for the CoNLL07 Domain-Adaptation TaskRebecca Watson and Ted BriscoeComputer LaboratoryUniversity of CambridgeFirstName.LastName@cl.cam.ac.ukAbstractWe describe our submission to the domainadaptation track of the CoNLL07 sharedtask in the open class for systems using ex-ternal resources.
Our main finding was thatit was very difficult to map from the annota-tion scheme used to prepare training and de-velopment data to one that could be used toeffectively train and adapt the RASP systemunlexicalized parse ranking model.
Never-theless, we were able to demonstrate a sig-nificant improvement in performance utiliz-ing bootstrapping over the PBIOTB data.1 IntroductionThe CoNLL07 domain adaptation task was createdto explore how a parser trained in one domain mightbe adapted to a new one.
The training data weredrawn from the PTB (Marcus et al, 1993) rean-notated with dependency relations (Johansson andNugues, 2007, hereafter DRs).
The test data weretaken from a corpus of biomedical articles (Kulicket al, 2004) and the CHILDES database (Brown,1973; MacWhinney, 2000) also reannotated withDRs (see Nivre et al, 2007) for further details ofthe task, annotation format, and evaluation scheme.The development data consisted of a small amountof annotated and unannotated biomedical and con-versational data.The RASP system (Briscoe et al, 2006) utilizesa manually-developed grammar and outputs gram-matical bilexical dependency relations (see Briscoe,2006 for a detailed description, hereafter GRs).
Wat-son et al (2007) describe a semi-supervised, boot-strapping approach to training the parser which uti-lizes unlabelled partially-bracketed input with re-spect to the system derivations.
For the domainadaptation task we retrained RASP by mapping ourGR scheme to the DR scheme and annotation for-mat, and used this mapping to select a derivationto train the unlexicalized parse ranking model fromthe annotated PTB training data.
We also performedsimilar partially-supervised bootstrapping over the200 annotated biomedical sentences in the develop-ment data.
We then tried unsupervised bootstrap-ping from the unannotated development data basedon these initial models.As the parser requires input to consist of a se-quence of one of 150 CLAWS PoS tags, we also uti-lize a first-order HMM PoS tagger which has beentrained on manually-annotated data from the LOB,BNC and Susanne Corpora (see Briscoe, 2006 forfurther details).
Accordingly, we submitted our re-sults in the open class.2 Training and AdaptationThe RASP parser is a generalized LR parser whichbuilds a non-deterministic generalized LALR(1)parse table from the grammar (Tomita, 1987).
Acontext-free ?backbone?
is automatically derivedfrom a unification grammar.
The residue of fea-tures not incorporated into the backbone are unifiedon each reduce action and if unification fails the as-sociated derivation paths also fail.
The parser cre-ates a packed parse forest represented as a graph-structured stack.Inui et al (1997) describe the probability model1170utilized in the system where a transition is repre-sented by the probability of moving from one stackstate, ?i?1, (an instance of the graph structuredstack) to another, ?i.
They estimate this probabilityusing the stack-top state si?1, next input symbol liand next action ai.
This probability is conditionedon the type of state si?1.
Ss and Sr are mutuallyexclusive sets of states which represent those statesreached after shift or reduce actions, respectively.The probability of an action is estimated as:P (li, ai, ?i|?i?1) ?
{P (li, ai|si?1) si?1 ?
SsP (ai|si?1, li) si?1 ?
Sr}Therefore, normalization is performed over alllookaheads for a state or over each lookahead for thestate depending on whether the state is a member ofSs or Sr, respectively.
In addition, Laplace estima-tion can be used to ensure that all actions in the tableare assigned a non-zero probability.These probabilities are estimated from countsof actions which yield derivations compatible withtraining data.
We use a confidence-based self-training approach to select derivations compatiblewith the annotation of the training and developmentdata to train the model.
In Watson et al (2007), weutilized unlabelled partially-bracketed training dataas the starting point for this semi-supervised train-ing process.
Here we start from the DR-annotatedtraining data, map it to GRs, and then find the oneor more derivations in our grammar which yield GRoutput consistent with the GRs recovered from theDR scheme.
Following Watson et al (2007), weutilize the subset of sentences in the training datafor which there is a single derivation consistent withthis mapping to build an initial trained parse rankingmodel.
Then we use this model to rank the deriva-tions consistent with the mapping in the portion ofthe training data which remains ambiguous giventhe mapping.
We then train a new model based oncounts from these consistent derivations which areweighted in somemanner by our confidence in them,given both the degree of remaining ambiguity andalso the ranking and/or derivation probabilities pro-vided by the initial model.Thus, the first and hardest step was to map theDR scheme to our GR scheme.
Issues concerningthis mapping are discussed in section 4.
Given thismapping, we determined the subset of sentences inthe (PTB) training data for which there was a sin-gle derivation in the grammar compatible with theset of mapped GRs.
These derivations were usedto create the initial trained model (B) from the uni-form model (A).
To evaluate the performance ofthese and subsequent models, we tested them usingour own GR-based evaluation scheme over 560 sen-tences from our reannotated version of DepBank, asubset of section 23 of the WSJ PTB (see Briscoe& Carroll, 2006).
Table 1 gives the unlabelled pre-cision, recall and microaveraged F1 score of thesemodels over this data.
Model B was used to rerankderivations compatible with the mapped GRs recov-ered for the PTB training data.
Model C was builtfrom the weighted counts of actions in the initial setof unambiguous data and from the highest-rankedderivations over the training data (i.e.
we do not in-clude duplicate counts from the unambiguous data).Counts were weighted with scores ranging [0 ?
1]corresponding to the overall probability of the rel-evant derivation.
The evaluation shows a steadyincrease in performance for these successive mod-els.
We also explored other variants of this boot-strapping approach involving use of weighted countsfrom the top n ranked parses derived from the initialmodel (see Watson et al, 2007, for details), but noneperformed better than simply selecting the highest-ranked derivation.To adapt the trained parser, we used the sametechnique for the 200 in-domain biomedical sen-tences (PBIOTB), using Model C to find the highest-ranked parse compatible with the annotation, andderived Model D from the combined counts fromthis and the previous training data.
We then usedModel D to rank the parses for the unannotatedin-domain data (PBIOTB unsupervised), and de-rived Model E from the combined counts from thehighest-ranked parses for all of the training and de-velopment data.
We then iterated this process twomore times over the unannotated datasets (each withan increasing number of examples though increas-ingly less relevant to the test data).
The performanceover our out-of-domain PTB-derived test data re-mains approximately the same for all these models.Therefore, we chose to use Model G for the blindtest as it incorporates most information from the in-1171Mdl.
Data Init.
Prec.
Rec.
F1A Uniform - 71.06 69.00 70.01PTBB Unambig.
A 75.94 73.16 74.53C Ambig.
B 77.88 75.11 76.47PBIOTBD Supervised C 77.86 75.09 76.45E Unsup.
1 D 77.98 75.25 76.59F Unsup.
2 E 77.85 75.19 76.50G Unsup.
3 F 77.76 75.09 76.41CHILDESH Unsup.
1 C 78.34 75.59 76.94Table 1: Performance of Successive BootstrappingModelsScore Avg.
StdPCHEMTB - labelled 55.47 65.11 09.64PCHEMTB - unlab.ed 62.79 70.24 08.14CHILDES - unlab.ed 45.61 56.12 09.17Table 2: Official Scoresdomain data.
For the CHILDES data we performedone iteration of unsupervised adaptation in the samemanner starting from Model C.3 EvaluationFor the blind test submission we used Models G andH to parse the PCHEMTB and CHILDES data, re-spectively.
We then mapped our GR output fromthe highest-ranked parses to the DR scheme and an-notation format required by the CoNLL evaluationscript.
Our reported results are given in Table 2.We used the annotated versions of the blind testdata supplied after the official evaluation to assessthe degree of adaptation of the parser to the in-domain data.
We mapped from the DR scheme andannotation format to our GR format and used ourevaluation script to calculate the precision, recalland microaveraged F1 score for the unadapted mod-els and their adapted counterparts on the blind testdata, given in Table 3.
The results for CHILDESshow no evidence of adaptation to the domain.
How-ever, those for PCHEMTB show a statistically sig-nificant (Wilcoxin Signed Ranks) improvement overthe initial model.
The generally higher scores inModel Test Data Prec.
Rec.
F1C PCHEMTB 71.58 73.69 72.62G PCHEMTB 72.32 74.56 73.42C CHILDES 82.64 65.18 72.88H CHILDES 81.71 64.58 72.14Table 3: Performance of (Un)Adapted ModelsTable 3, as compared to Table 2, reflect the differ-ences between the task annotation scheme and ourGR representation as well as those of the evaluationschemes, which we discuss in the next section.4 DiscussionThe biggest issue for us participating in the sharedtask was the difficulty of reconciling the DR an-notation scheme with our GR scheme, given theoften implicit and sometimes radical underlyingdifferences in linguistic assumptions between theschemes.Firstly, the PoS tagsets are different and ours con-tains three times the number of tags.
Given that thegrammar uses these tags as preterminal categories,this puts us at a disadvantage in mapping the anno-tated training and development data to optimal inputto train the (semi-)supervised models.Secondly, there are 17 main types of GR rela-tion and a total of 46 distinctions when GR sub-types are taken into account ?
for instance the GRncsubj has two subtypes depending on whether thesurface subject is the underlying object of a passiveclause.
The DR scheme has far fewer distinctionscreating similar difficulties when creating optimal(semi-)supervised training data.Thirdly, the topology of the dependency graphsis often significantly different because of reversedhead-dependent bilexical relations and their knock-on effects ?
for instance, the DR AUX relation treatsthe (leftmost) auxiliary as head and modifiers of theverb group attach to the leftmost auxiliary, while theGR scheme treats the main verb as (semantic) headand modifiers of the verb group attach to it.Fourthly, the treatment of punctuation is very dif-ferent.
The DR scheme includes punctuation mark-ers in DRs which attach to the root of the subgraphover which they have scope.
By contrast, the GRscheme does not output punctuation marks directly1172but follows Nunberg?s (1990) linguistic analysis ofpunctuation as delimiting and typing text units oradjuncts (at constituent boundaries).
Thus the GRscheme includes text (adjunct) relations and treatspunctuation marks as indicators of such relations ?for instance, for the example The subject GRs ?
nc-subj, xsubj and csubj ?
all have subtypes., RASPoutputs the GR (ta dash GRs and) indicating thatthe dash-delimited parenthetical is a text adjunct ofGRs with head and, while the DR scheme gives(DEP GRs and), and two (P and ?)
relations cor-responding to each dash mark.Although we attempted to derive an optimal anderror-free mapping between the schemes, this washampered by the lack of information concerning theDR scheme, lack of time, and the very different ap-proaches to punctuation.
This undoubtedly limitedour ability to train effectively from the PTB data andto adapt the trained parser using the in-domain data.For instance, the mean average unlabelled F1 scorebetween the GRs mapped from the annotated PTBtraining data and closest matching set of GRs outputby RASP for this data is 84.56 with a standard de-viation of 12.41.
This means that the closest match-ing derivation which is used for training the initialmodel is on average only around 85% similar evenby the unlabelled measure.
Thus, the mapping pro-cedure required to relate the annotated data to RASPderivations is introducing considerable noise into thetraining process.Mapping difficulties also depressed our officialscores very significantly.
In training and adaptingwe found that bootstrapping based on unlabelled de-pendencies worked better in all cases than utilizingthe labelled mapping we derived.
For the officialsubmission, we removed all ta, quote and passiveGRs and mapped all punctuation marks to the P re-lation with head 0.
Furthermore, we do not generatea root relation, though we assumed any word thatwas not a dependent in other GRs to have the depen-dent ROOT.
In our own evaluations based on map-ping the annotated training and development data toour GR scheme, we remove all P relations and mapROOT relations to the type root which we addedto our GR hierarchy.
We determined the semantichead of each parse during training so as to compareagainst the root GR and better utilize this additionalinformation.
In the results given in Table 1 over ourDepBank test set, the effect of removing the P de-pendencies is to depress the F1 scores by over 20%.For the CHILDES and PCHEMTB blind test data,our F1 scores improve by over 7% and just under 9%respectively when we factor out the effect of P rela-tions.
These figures give an indication of the scaleof the problem caused by these representional differ-ences.5 ConclusionsThe main conclusion that we draw from this experi-ence is that it is very difficult to effectively relate lin-guistic annotations even when these are inspired bya similar (dependency-based) theoretical tradition.The scores we achieved were undoubtedly furtherdepressed by the need to use a partially-supervisedboostrapping approach to training because the DRscheme is less informative than the GR one, and byour decision to use an entirely unlexicalized parseranking model for these experiments.
Despite thesedifficulties, performance on the PCHEMTB datasetusing the adapted model improved significantly overthat of the unadapted model, suggesting that boot-strapping using confidence-based self-training is aviable technique.AcknowledgementsThis research has been partially supported by theEPSRC via the RASP project (grants GR/N36462and GR/N36493) and the ACLEX project (grantGR/T19919).
The first author is funded by theOverseas Research Students Awards Scheme and thePoynton Scholarship appointed by the CambridgeAustralia Trust in collaboration with the CambridgeCommonwealth Trust.ReferencesE.
Briscoe (2006) An introduction to tag sequencegrammars and the RASP system parser, Univer-sity of Cambridge, Computer Laboratory Techni-cal Report, UCAM-CL-TR-662.E.
Briscoe and J. Carroll (2006) ?Evaluating the Ac-curacy of an UnlexicalizedStatistical Parser on the PARC DepBank?, Pro-ceedings of the ACL-Coling?06, Sydney, Aus-tralia.1173Briscoe, E.J., J. Carroll and R. Watson (2006) ?TheSecond Release of the RASP System?, Proceed-ings of the ACL-Coling?06, Sydney, Australia.R.
Brown (1973) A First Language: The EarlyStages, Harvard University Press.Inui, K., V. Sornlertlamvanich, H. Tanaka andT.
Tokunaga (1997) ?A new formalization of prob-abilistic GLR parsing?, Proceedings of the 5thInternational Workshop on Parsing Technologies,MIT, Cambridge, Massachusetts, pp.
123?134.R.
Johansson and P. Nugues (2007) ExtendedConstituent-to-Dependency Conversion for En-glish, NODALIDA16.S.
Kulick, A. Bies, M. Liberman, M. Mandel, R.McDonald, M. Palmer, A. Schein and L. Ungar(2004) ?Integrated Annotation for Biomedical In-formation Extraction?, Proceedings of the HLT-NAACL2004, Boston, MA..B. MacWhinney (2000) The CHILDES Project:Tools for Analyzing Talk, Lawrence Erlbaum.M.
Marcus, B. Santorini and M. Marcinkiewicz(1993) ?Building a Large Annotated Corpus ofEnglish: the Penn Treebank?, Computational Lin-guistics, vol.19.2, 313?330.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nils-son, S. Riedel and D. Yuret (2007) ?The CoNLL2007 Shared Task on Dependency Parsing?, Pro-ceedings of the EMNLP-CoNLL2007, Prague.G.
Nunberg (1990) The Linguistics of Punctuation,CSLI Publications.Tomita, M. (1987) ?An Efficient AugmentedContext-Free Parsing Algorithm?, ComputationalLinguistics, vol.13(1?2), 31?46.R.
Watson, E. Briscoe and J. Carroll (2007) ?Semi-supervised Training of a Statistical Parser fromUnlabeled Partially-bracketed Data?, Proceedingsof the IWPT07, Prague.1174
