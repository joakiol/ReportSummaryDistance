Are You Sure That This Happened?Assessing the Factuality Degree ofEvents in TextRoser Saur??
?Barcelona Media - Innovation CenterJames Pustejovsky?
?Brandeis UniversityIdentifying the veracity, or factuality, of event mentions in text is fundamental for reasoningabout eventualities in discourse.
Inferences derived from events judged as not having happened,or as being only possible, are different from those derived from events evaluated as factual.
Eventfactuality involves two separate levels of information.
On the one hand, it deals with polarity,which distinguishes between positive and negative instantiations of events.
On the other, ithas to do with degrees of certainty (e.g., possible, probable), an information level generallysubsumed under the category of epistemic modality.
This article aims at contributing to a betterunderstanding of how event factuality is articulated in natural language.
For that purpose,we put forward a linguistic-oriented computational model which has at its core an algorithmarticulating the effect of factuality relations across levels of syntactic embedding.
As a proofof concept, this model has been implemented in De Facto, a factuality profiler for eventualitiesmentioned in text, and tested against a corpus built specifically for the task, yielding an F1 of0.70 (macro-averaging) and 0.80 (micro-averaging).
These two measures mutually compensatefor an over-emphasis present in the other (either on the lesser or greater populated categories),and can therefore be interpreted as the lower and upper bounds of the De Facto?s performance.1.
IntroductionWhen we talk about situations in the world, we often leave pieces of information vagueor try to complete the story with approximations, either because we do not know allthe details or we are not sure about what we know.
With a lesser or greater degree, thisvagueness is pervasive in all types of accounts, regardless of the topic and the degree ofproximity of the speaker with the facts being reported: our last family gathering, whatwe read about the tsunami and its aftermath in Japan, our perspective on a particulartopic, or how we feel today.
Even in scientific discourse, findings tend to be expressedwith degrees of cautiousness.?
Voice and Language Group, Barcelona Media - Innovation Center, Diagonal 177, 08018 Barcelona,Catalonia.
E-mail: roser.sauri@barcelonamedia.org.??
Computer Science Department, Brandeis University, 415 South Street, Waltham MA 02454, USA.E-mail: jamesp@cs.brandeis.edu.Submission received: 4 April 2011; revised submission received: 1 October 2011; accepted for publication:30 November 2011.?
2012 Association for Computational LinguisticsComputational Linguistics Volume 38, Number 2The linguistic mechanisms for coping with the vagueness and fuzziness in ourknowledge are commonly referred to as speculative language.
This involves differentlevels of grammatical manifestation, most significantly quantification over entities andevents, modality, and hedging devices of a varied nature.
We can be vague or approx-imate with the temporal and spatial references of situations in the world, when quan-tifying the frequency of usual events, assessing the number of participants involved,and describing or adscribing them into a class.
We also qualify our statements withapproximative language when giving an opinion, or when we are not certain about thedegree of veracity of what we are telling.The present article focuses on a particular kind of speculation in language, specif-ically, that concerning the factuality status of eventualities mentioned in discourse.Whenever we talk about situations, we express our degree of certainty about theirfactual status.
We can characterize them as an unquestionable fact, or qualify them withsome degree of uncertainty if we are not sure whether the situation holds, or will hold,in the world.Identifying the factuality status of event mentions is fundamental for reasoningabout eventualities in discourse.
Inferences derived from events judged as not havinghappened, or as being only possible, are different from those derived from eventsevaluated as factual.
Event factuality is also essential for any task involving temporalordering, because the plotting of event mentions into a timeline requires differentactions depending on their veracity.
Karttunen and Zaenen (2005) discuss its relevancefor information extraction, and in the area of textual entailment, factuality-related infor-mation (modality, intensional contexts, etc.)
has been taken as a basic feature in somesystems participating in the PASCAL RTE challenges (e.g., Hickl and Bensley 2007).The need for this type of information is also acknowledged in the annotation schemesof corpora devoted to event information, such as the ACE corpus for the Event andRelation recognition task (e.g., ACE 2008), or TimeBank, a corpus annotated with eventand temporal information (Pustejovsky et al 2006).Significantly, in the past few years this level of information has been at the focus ofmuch research within the NLP area dedicated to the biomedical domain.
Distinguishingbetweenwhat is reported as a fact versus a possibility in experiment reports or in patienthealth records is a crucial capability for any robust information extraction tool operatingon that domain.
This interest has resulted in the compilation of domain-specific corporadevoted particularly to that level of information, such as BioScope (Vincze et al 2008),and others that include event factivity as a further attribute in the annotation of biomed-ical events, such as GENIA (Kim, Ohta, and Tsujii 2008).
Furthermore, factuality-relatedinformation was the main focus in the CoNLL-2010 shared task on Learning to DetectHedges and their Scope in Natural Language Text (Farkas et al 2010), and the topicin a subtask of the BioNLP?09 and BioNLP?11 shared task editions on Event Extraction(Kim et al 2009),1 dedicated to predict whether the biological event is under negationor speculation.The overall goal of this article is to contribute to a better understanding of thisparticular aspect of speculation.
We analyze all the ingredients involved in comput-ing the factuality nature of event mentions in text, and put forward a computationalmodel based on that.
As a proof of concept, the model is implemented into De Facto,a factuality profiler, and its performance tested against FactBank, a corpus annotated1 For the 2011 edition, refer to: https://sites.google.com/site/bionlpst/.262Saur??
and Pustejovsky Assessing the Factuality Degree of Events in Textwith factuality information built specifically for the task and currently available to thecommunity through the Linguistic Data Consortium (Saur??
and Pustejovsky 2009a).The article begins by defining event factuality and its place in speculative language(Section 2).
The basic components for the model on event factuality are presented in Sec-tion 3, and the algorithm integrating these is introduced in Section 4.
Section 5 reportson the experiment resulting from implementing the proposed model into De Facto, andSection 6 relates the present work to other research in the field.2.
Event Factuality and Speculative Language2.1 Defining Event FactualityEvent factuality (or factivity) is understood here as the level of information expressingthe factual nature of eventualities mentioned in text.
That is, expressing whether theycorrespond to a fact in the world (Example (1a)), a possibility (Examples 1b, 1c), or asituation that does not hold (Example 1d), as is the case with the events denoted by thefollowing underlined expressions:2(1) a. Har-Shefi regretted calling the prime minister a traitor.b.
Results indicate that Pb2+ may inhibit neurite initiation.c.
Noah?s flood may have not been as biblical in proportion as previouslythought.d.
Albert Einstein did not win a Nobel prize for his theories of Relativity.The fact that an eventuality is depicted as holding or not does not mean that this isthe case in the world, but that this is how it is characterized by its informant.
Similarly,it does not mean that this is the real knowledge that informant has (his true cognitivestate regarding that event) but what he wants us to believe it is.Event factuality rests upon distinctions along two different parameters: the notionsof certainty (what is certain vs. what is only possible) and polarity (positive vs. nega-tive).
In some contexts, the factual status of events is presented with absolute certainty.Then, depending on the polarity, events are depicted as either situations that have takenor will take place in the world (here referred to as facts; see Example (1a)), or situationsthat do not hold in the world (here called counterfacts; see Example (1d)).
In othercontexts, events are qualified with different shades of uncertainty.
Combining that withpolarity, events are seen as possibly factual (Example (1b)) or possibly counterfactual(Example (1c)).32 In this article, the terms event and eventuality will be used in a very broad sense to refer to both processesand states, but also other abstract objects such as situations, propositions, facts, possibilities, and so on.Furthermore, events in the examples will be identified by marking only their verb, noun, or adjectivehead, together with their modal and negation particles when deemed necessary.
This follows theconvention assumed in TimeML, a specification language for representing event and time information(Pustejovsky et al 2005).3 The term counterfactual has a long tradition in philosophy of language and linguistics, where it refers toconditional (or if?then) statements expressing what would be the case if their antecedent was true,although it is not.
For example: If Gandhi had survived the fatal gun attack, he would have continued workingfor a better world.
Here, however, we extend its use to refer to negated events in general.
One can arguethat negated events are facts as well.
For example, it is a fact that Gandhi did not survive the fatal gunattack.
The term counterfactmust be understood here as negative fact.263Computational Linguistics Volume 38, Number 2Factuality is expressed through a complex interaction of many different aspects ofthe overall linguistic expression.
It involves explicit polarity and modality markers,but also lexical items, morphological elements, syntactic constructions, and discourserelations between clauses or sentences.Polarity particles, which convey the positive or negative factuality of events, in-clude elements of a varied nature: adverbs (not, neither, never), determiners (no, non),pronouns (none, nobody), and so forth.
At another level, modality particles contributedifferent degrees of certainty.
In English, they can be realized as verbal auxiliaries(must, may), adverbials (probably, presumably), and adjectives (likely, possible).
All thesecategories display an equivalent gradation of modality (Givo?n 1993).In many cases, the factuality of events is conveyed by what we refer to as event-selecting predicates (ESPs), that is, predicates (either verbs, nouns, or adjectives) thatselect for an argument denoting an event of some sort.
ESPs are of interest here becausethey qualify the degree of factuality of their embedded event, which can be presentedas a fact in the world (Example (2)), a counterfact (Example (3)), or a possibility (Ex-ample (4)).
In these examples, the ESPs are in boldface and their embedded events areunderlined.
(2) a.
Some of the Panamaniansmanaged [to escape with their weapons].b.
The defendant knew that [he had been in possession of narcotics].
(3) a.
1,200 voters were prevented from [casting ballots on election night].b.
The manager avoided [returning the phone calls].
(4) a. I think [they voted last weekend].b.
Hawking speculated that [most extraterrestrial life would be similarto microbes].Absolute factuality is conveyed by ESPs belonging to classes fairly well studiedin the literature, such as: implicative (Example (2a)) (Karttunen 1970); factive (Exam-ple (2b)) (Kiparsky and Kiparsky 1970); perception (e.g., see a car explode); aspectual(e.g., finish reading), and change-of-state predicates (e.g., increase its exports).
Coun-terfactuality is brought about by other implicative predicates, like avoid and prevent(Example (3)) (Karttunen 1970), whereas predicates such as think, speculate, and suspectqualify their complements as not totally certain (Example (4)) (Hooper 1975; Bach andHarnish 1979; Dor 1995).
The group of ESPs that leave the factuality of their eventcomplement underspecified is also significant.
The event is mentioned in discourse,but no information is provided concerning its factual status.
Several predicate classescreate this effect, for example: volition (e.g., want, wish, hope), commitment (commit,offer, propose), and inclination predicates (willing, ready, eager, reluctant), among others(cf.
Asher 1993).Other information at play is evidentiality (e.g., a seen event is presented with afactuality degree stronger than that of an event reported by someone else), and mood(e.g., indicative vs. subjunctive).
Factuality information is also introduced by certainsyntactic constructions involving subordination.
In some cases, the embedded event ispresupposed as a fact, as in non-restrictive relative clauses (Example (5a)) or participialclauses (Example (5b)).
In others, like purpose clauses, the event is intensional and thuspresented as underspecified (Example (5c)).264Saur??
and Pustejovsky Assessing the Factuality Degree of Events in Text(5) a. Obama, [who took office in January], inherited a budget deficit of $1.3 trillion.b.
[Having revolutionized linguistics], Chomsky moved to political activism.c.
Stronach resigned as CEO of Magna [to seek a seat in Canada?s Parliament].Finally, a further means for conveying factuality information is available at thediscourse level.
Some events may first have their factual status characterized in oneway, but then be presented differently in a subsequent sentence.2.2 Notions Connected to Event FactualityEvent factuality results from the interaction between polarity and certainty.
Here wereview the connections of these two notions with other ones in the study of language.Certainty.
The axis of certainty is related to epistemic modality, a category dealing withthe degree of certainty of situations in the world.
Epistemic modality has been studiedfrom both the logical and linguistic traditions.
Within linguistics, authors from differ-ent traditions converge in analyzing modality as a subjective component of discourse(e.g., Lyons 1977; Chafe 1986; Palmer 1986; Kiefer 1987), a view that is adopted in thepresent analysis.4 Traditionally, the study of epistemic modality in linguistics has beenconfined to modal auxiliaries (e.g., Palmer 1986), but more recently a wider view hasbeen adopted which includes other parts of speech as well, such as epistemic adverbs,adjectives, nouns, and lexical verbs (e.g., Rizomilioti 2006).In a more secondary way, the axis of certainty is also related to the system ofevidentiality, concerned with the way in which information about situations in theworld is acquired, such as directly experienced, witnessed, heard-about, inferred, andso on (van Valin and LaPolla 1997; Aikhenvald 2004).
Different types of evidence havean effect on the way the factuality of an event is evaluated.
For instance, somethingreported as seen can more easily be assessed as a fact than something reported asinferred.Certainty touches as well on the notion of epistemic stance, developed from amore cognitivist perspective and which is defined as the pragmatic relation betweenspeakers and their knowledge regarding the things they talk about (Biber and Finegan1989; Mushin 2001).
Similarly, within Systemic Functional Linguistics, the AppraisalFramework develops a taxonomy of the mechanisms employed for expressing sub-jective information such as attitude, its polarity, graduation, and so forth (Martin andWhite 2005).Within NLP, most work on uncertainty and speculative information has been ap-proached from a hedging-based perspective.
The notion of hedging is initially definedby Lakoff (1973, page 471) as ?words whose job is making things fuzzier or less fuzzy.
?In particular, he uses this term to analyze linguistic constructions that express degreesof the is a relationship (e.g., is a sort of, in essence/strickly speaking... is...).
Due to thefuzziness aspect of hedges, subsequent work extends the notion to include expres-sions for qualifying the degree of commitment of the writer with respect to what isasserted (Hyland [1996], among others).
By this definition, hedging and event factualityseem to be overlapping concepts.
They differ on the extent of the phenomena they4 This is different, however, from most of the work within truth-conditional semantics, which conceivesmodality as independent from the speaker?s perspective (e.g., Kratzer 1991).265Computational Linguistics Volume 38, Number 2each cover, however.
First, hedging is confined only to partial degrees of uncertainty,whereas factuality includes also the levels of absolute certainty.
Second, in additionto degrees of writer?s commitment towards the veridicity of her statements, hedging(but not factuality) encompasses speculative expressions belonging to other scales, mostsignificantly, expressions of usuality (to quantify the frequency of events: often, barely,tends to, etc.
), expressions of category membership (i.e., is a downgraders, such as is asort of, presented by Lakoff [1973]), as well as lack of knowledge (e.g, little is known).Polarity.
The second axis configuring event factuality is the system of polarity, so calledbecause it articulates the polar opposition between positive and negative contexts.Due to its recent adoption in the NLP area of sentiment analysis, the term polarity isoften taken to express only the direction of an opinion.
Here, we use the term in itsoriginal grammatical sense, that is, as conveying the distinction between affirmative andnegative contexts (e.g., Horn 1989).
Beingmore abstract, this definition encompasses thedifferent facets of the positive/negative opposition, and not only the one that is relevantin opinion mining.2.3 Key Elements in the Factuality SystemIdentifying event factuality in text poses challenges at different levels of analysis.
Weexplore them in the current section.A scale of factuality degrees.
Concerning distinctions at the level of both polarity andcertainty (or modality, as is more commonly referred to within linguistics), the factualityof events can be characterized as a double-axis scale.
Figure 1 illustrates the system.The axis of polarity defines a binary distinction (positive vs. negative), and theaxis of modality conveys certainty as a continuous scale that ranges from truly certainto completely uncertain, passing through a whole spectrum of shades that languagesaccommodate in different ways, depending on the grammatical resources they haveavailable.
For example, assuming only a limited number of words in English, onecan create the following distinctions: improbable, slightly possible, possible, fairly possible,probable, very probable, most probable, most certain, certain.Figure 1The double range of factuality.266Saur??
and Pustejovsky Assessing the Factuality Degree of Events in TextThis continuum poses a challenge in the setting of a model of factuality withpotential cross-linguistic validity.
Many linguists agree, however, that speakers are ableto map areas of the modality axis into discrete values (Lyons 1977; Horn 1989; deHaan 1997).
The goal is therefore identifying the factuality distinctions that reflect ourlinguistic intuitions as speakers, and that can also help define a set of sound and stablecriteria for differentiating among them.
The factual value of markers such as possiblyand probably is fairly transparent.
What, however, is the contribution of elements likethink, predict, suggest, or seem?Interactions among factuality markers.
The factuality status of a given event cannot bedetermined from the strictly local modality and polarity operators scoping over thatevent alone; rather, if present, other non-local markers must be considered as well toobtain the adequate interpretation.
Consider:(6) a.
Several EUmember states will continue to allow passengers to carry duty-freedrinks in hand luggage.b.
Several EU member states will continue to refuse to allow passengers to carryduty-free drinks in hand luggage.c.
Several EU member states may refuse to allow passengers to carry duty-freedrinks in hand luggage.5In all three examples above the event carry is directly embedded under the verballow, but receives a different interpretation depending on the elements scoping overthat.
In Example (6a), where allow is embedded under the factive predicate continue,carry is characterized as a fact in the world.
Example (6b), on the other hand, depicts it asa counterfact because of the effect of the predicate refuse scoping over allow, and finally,Example (6c) presents it as uncertain due to the modal auxiliary may qualifying refuse.6Any treatment aiming at adequately handling the contents of sentences like theseneeds to incorporate the notion of scope in its model, but scope is not enough.
As thesedata show, the factuality value of an event does not depend on the element immediatelyscoping over it.
Neither does it rely on the meaning resulting from some sort of additive(or concatenative) operation among all the markers.
In Example (6b), for example, twoof the factuality markers that include the event carry in their scope (continue and refuse)typically mark contradictory information.
The first one presupposes the factuality of theevent it scopes over, and the second negates it.
Which should be the resulting factualityvalue for carry if only scope information is used?Factuality as a property qualifying events and not the whole sentence.
Factuality is a propertythat qualifies the nature of events, hence operating at a level of units smaller thansentences.
Frequently sentences express more than one event (or proposition), each ofthem qualified with a different degree of certainty.
Consider Example (7),7 where themain event have an easier time (e3) is depicted as a possibility in the world, event crossover5 The original sentence in this set is (6b) (http://www.irishtimes.com/newspaper/ireland/2011/0502/1224295867753.html).
The other two have been adapted for the argument?s sake.6 The verb allow is generally used as a two-way implicative predicate, that is, as a predicate that holds adirect relation between its truth (or falsity) and that of its embedded event (Karttunen 1970).7 Extracted from Rubin (2006, page 59).267Computational Linguistics Volume 38, Number 2voting being barred (e2) is asserted as a fact, and event crossover voting (e1) is uncertain?that is, the fact that it is barred does not mean that it does not take place.
(7) In future primaries, where crossover votinge1 is barrede2 , Bush may well havee3an easier time.Facts and their sources.
Certain event components, such as the temporal reference or theparticipants taking part in it, are inherent elements of any given event.
For example,the visit to the zoo with Max in April, Ivet in August, and Arlet in December are threeseparate events, given the difference in participants and temporal location.
By contrast,factuality is a matter of perspective.
Different sources can have divergent views aboutthe factuality of the very same event.
Recognizing this is crucial for any task involvingtext entailment.
Event e in Example (8), for instance (i.e., Ruby being the niece of theEgyptian president), will be inferred as a fact in the world if it cannot be qualified ashaving been asserted by a specific source, here Berlusconi (underlined).
(8) Berlusconi said that Rubywase the niece of Egyptian President Hosni Mubarak.By default, events mentioned in discourse always have an implicit source, namely,the author of the text.
Additional sources are introduced in discourse by means of ESPssuch as say or pretend:(9) Nelles saide1 that Germany has been pretendinge2 for long that nuclear poweris safee3 .In some cases, the different sources relevant for a given event may coincide withrespect to its factual status, but in others they may be in disagreement.
In Example (9),for instance, event e3 (nuclear power being safe) is assessed as a fact according to Germanybut as a counterfact according to Nelles, whereas the text author remains uncommitted.The time variable.
It is not only the case that two participants can present differentviews about the same event, but also that the same (or different) participant presentsa diverging view at different points in time.
Consider:(10) a.
In mid-2001, Colin Powell and Condoleezza Rice both publicly denied that Iraqhad weapons of mass destruction.b.
Secretary of State Colin Powell Thursday defended the Bush administration?sposition that Iraq had weapons of mass destruction.
(CNN, 8 January 2004)A model of event factuality needs therefore to be sensitive to the distinctions in per-spective brought about by sources and temporal references.
Only under this assumptionis it possible to account for the potential divergence of opinions on the factual status ofevents, as is common in news reports.3.
Towards a Model for Event FactualityHaving identified themain aspects involved in event factuality, we explore the interplayamong these elements, and subsequently build a model that can explain these interac-tions.
Based on the structure of linguistic expressions, this model will assume an event-268Saur??
and Pustejovsky Assessing the Factuality Degree of Events in Textcentered approach in order to tackle the factuality nature of each event independentlyof the others mentioned in the same sentence.
Factuality distinctions are establishedat a fine-grained level, and multiple perspectives on the same event are accounted forby means of the notion of source as a participant introduced by predicates of report,knowledge, belief, and so on.
We begin by introducing the notion of a factuality profile(Section 3.1), and then formalize the basic components that have a role in it, namely:factuality values (Section 3.2), sources (Section 3.3), and factuality markers (Section 3.4).The algorithm putting all these ingredients together will be presented in Section 4.3.1 The Factuality Profile of EventsWhenever speakers talk about events, they qualify them with a degree of factuality.Here, we refer to this act of assigning a factuality value to a given event performedby a particular source at a specific point in time as a factuality commitment act.
Thisinvolves four components: The event in focus, e. The factuality value assigned to that event, f, which touches on bothpolarity and epistemic modality distinctions as encoded in factualitymarkers. The source assigning the factuality value to that event, s. The time when the factuality value assignment takes place, t.For instance, in Example (9) Germany is presented as defending that nuclear poweris safe (event e3).
This corresponds to the factuality commitment act that assesses evente3 as a fact in the world, performed by source Germany at an underspecified point intime t1.Given that events in discourse can be evaluated by more than one source and atseveral points in time, the factuality of each event can be characterized through morethan one factuality commitment act.
We define the set of factuality commitment actsassociated to an event as its factuality profile.
Formally, the factuality profile of a givenevent e, pe, can be represented as follows:pe = {?s, t, f ?
| s is a relevant source for event e & t is a point in time & f is the factualityvalue assigned by source s to event e at point in time t} (1)Using example (9) again, the factuality profile of event e3 (nuclear power being safe)contains three factuality commitment acts: one by source Germany, who commits to theveradicity of the event, another for source Nelles, who disagrees, and finally another forthe author, who keeps an underspecified position.pe3 = {?germany, t1, fact?, ?nelles, t2,negative fact?, ?author, t3,underspecified?}
(2)The model that will be presented here for determining the factuality profiles ofevents in text will disregard the temporal component and focus only on identifyingrelevant sources and factuality values.269Computational Linguistics Volume 38, Number 23.2 How Certain Are You: Factuality ValuesThe values for characterizing event factuality must account for distinctions along boththe polarity and the modality axes.
Whereas polarity is a binary system with the val-ues positive and negative, epistemic modality constitutes a continuum ranging fromuncertain to absolutely certain.
In order to obtain consistent annotation for informingand evaluating automatic systems, a discrete categorization of modality that effectivelyreflects the main distinctions applied in natural languages is desirable.Within modal logic two operators are typically used to express modal contexts:necessity () and possibility (?).
Most linguists, however, agree that this is inadequateto capture the richness of cross-linguistic data.
It has generally been observed that, eventhough modality is a continuous system, a three-fold distinction is commonly adoptedby speakers (e.g., Lyons 1977; Palmer 1986; Halliday andMatthiessen 2004).
Horn (1989)analyzes modality and its interaction with polarity based on both linguistic tests andthe logical relations holding at the basis of the Aristotelian Square of Opposition (inparticular, the Law of Excluded Middle and the Law of Contradiction).
In Horn?s work,the system of epistemic modality is analyzed as a particular instantiation of scalarpredication, that is, as a collection of predicates Pn such as ?Pj, Pj?1, ..., P2, P1?, where Pnoutranks (i.e., is stronger than) Pn?1 on the relevant scale.
The relations holding amongpredicates of the same scalar predication are manifested in syntactic contexts like thefollowing (Horn 1972): Contexts with the possibility open that a higher value on the relevant scaleobtains:?
(at least) Pn?1, if not (downright) Pn.?
Pn?1, {or/ and possibly} even Pn. Contexts by which a higher value in the scale is known to obtain:?
Pn?1, {indeed/ in fact/ and what is more} Pn.?
not only Pn?1 but Pn.This set of contexts allows him to conclude the existence of two independent epis-temic scales that differ in quality (positive vs. negative polarity):8(11) a.
?certain, likely (probable), possible?b.
?impossible, unlikely (improbable), uncertain?Based on Horn?s distinctions, we divide the modality axis into the values certain(CT), probable (PR), and possible (PS), and the polarity axis into positive (+) and negative(?).
Moreover, we add an underspecified value in both axes to account for cases of non-commitment of the source or in which the value is not known.
A degree of factuality isthen characterized as a pair ?MOD, pol?, containing a modality and a polarity value (e.g.,?CT,+?).
For the sake of simplicity, these will be represented in the abbreviated form of:MODpol (e.g., CT+).
Table 1 presents the full set of factuality values.8 The beauty of the system can be appreciated when mapped to the traditional Square of Opposition,used to account for the interaction between negation and quantifiers or modal operators (Horn [1989],following Aristotle).
For a detailed account of that within the current framework, see Saur??
andPustejovsky (2009b).270Saur??
and Pustejovsky Assessing the Factuality Degree of Events in TextTable 1Factuality values.Positive Negative UnderspecifiedCertain CT+ (factual) CT?
(counterfactual) CTu (certain but unknown output)Probable PR+ (probable) PR?
(not probable) [NA]Possible PS+ (possible) PS?
(not certain) [NA]Underspecified [NA] [NA] Uu (unknown or uncommitted)The table includes six fully committed (or specified) values (CT+, CT?, PR+, PR?,PS+, PS?
), and two underspecified ones: the partially underspecified CTu, and the fullyunderspecified Uu.
The use of the fully committed values should be clear from theparaphrases in the table, but uncommitted values deserve further explanation.
Thepartially underspecified value CTu is for cases where the source has total certainty aboutthe factual nature of the event but it does not commit to its polarity.
This is the case ofsource John regarding event e in: John knows whetherMary camee.
The fully underspecifiedvalue Uu, on the other hand, is used when any of the following situations applies:(i) The source does not know the factual status of the event (e.g., John does notknow whether Mary camee).
(ii) The source is not aware of the possibility of the event (e.g., John does notknow that Mary camee).
(iii) The source does not overtly commit to the event (e.g., John didn?t say thatMary camee).93.3 Who Said What: Factuality SourcesSources are understood here as the cognitive individuals that hold a specific stanceregarding the factuality status of events in text.
They correspond to one of the followingactor types:Text author.
Events mentioned in discourse always have a default source, whichcorresponds to the author of the text (speaker or writer).Other sources.
Contexts of report, belief, knowledge, inference, and so forth (cre-ated by predicates like say, think, know, see) introduce additional explicit sources,generally expressed by the logical subject of the predicate.
Similarly, impersonalconstructions (e.g., it seems, it is clear, ...) or passive constructions with no agentiveargument (e.g., it is expected) introduce an implicit source which can be rephrasedas everybody or somebody, among similar expressions.
The factuality of the embed-ded event is assessed relative to this new (explicit or implicit) source, as well as toany source already present in the discourse, such as the text author.In the current framework, these sources will be formally represented as: s0 (authorsource), sn for n > 0 (explicit source), and GEN (for implicit, generic source).9 The value Uu could be seen as equivalent to others, such as PS?
and PR?.
Note, however, that inthese two, but not in Uu, the source commits to a specific degree of uncertainty (possible or probable,respectively), as in John said that Mary [may have not come], and John said that [Mary has probably not come].271Computational Linguistics Volume 38, Number 2?Source?
as a technical term.
Although the term source is generally used as a synonym ofinformant, in the scope of the current work it is used in a very specific, technical sense.First, it not only refers to the typical informants, that is, those participants activelycommitting to the factuality of an event by means of a speech act or a writing event ofsome sort (e.g.,Mary says/claims/wrote...), but also to those that are presented as holding(or being able to hold) a position about the factuality of that event?be it because theyhold a mental attitude about the situation (Mary knows/learned/thinks/suspects that...),because they are the experiencers of a psychological reaction generated by the eventin question (Mary regrets/is sad that...), or because they are presented as witnesses orperceivers of the situation (Mary saw/heard that...).Second, the notion of source as used here includes participants that are presentedas unaware of the relevant event as well.
Consider:(12) Galbraith is claiming that President Bush was unaware that there were twomajor sects of Islam just two months before the President ordered troops toinvade Iraq.A complete analysis of the facts, causes, and consequences regarding the war inIraq needs to include the existence of two major sects of Islam, and what this means interms of the potential stability of the area.
But it should also include that President Bushdid not know this piece of information beforehand, as claimed by the political actorGalbraith.
Thus, the factuality analysis of the sentence must include President Bush asa source who at some point in time held an uncommitted factuality stance with regardto the existence of these two Islamic sects.Nested sources.
The status of the author is, however, different from that of the additionalsources.
The reader does not have direct access to the factual assessments made by thesenew sources, but only according to what the author asserts.
Thus, we need to appeal tothe notion of nested source as presented in Wiebe, Wilson, and Cardie (2005).
That is,Nelles in Example (13) is not a licensed source of the factuality of event e2, but Nellesaccording to the author, represented here as nelles author.10 Similarly, the source referredto as Germany corresponds to the chain: germany nelles author.
(13) Nelles saide1 that Germany has been pretendinge2 for long that nuclearpower is safee3 .Source roles.
We distinguish between two different source roles.
Sources most imme-diately committed (or uncommitted, in the case of unaware sources) to the factualitystatus of an event perform the role of cognizers of that event.
This is typically thecase of sources introduced in contexts of report, witnessing, belief, and so forth.
On theother hand, sources that present (or anchor) the factuality commitment of the cognizertowards an event are referred to as the anchors.
The roles of cognizer and anchor arerelative to each event.
For instance, in Example (13) the cognizer of event e2 (Germanypretending) is Nelles (according to the author, hence: nelles author) and its anchor is thetext author.
On the other hand, the cognizer of event e3 (nuclear power being safe) is10 This is equivalent to the notation ?author, nelles?
in Wiebe?s work.
Here, we adopt a reversedrepresentation of the nesting (i.e., the non-embedded source last) because it positions the most directsource of the event at the outmost layer, thus facilitating its reading.272Saur??
and Pustejovsky Assessing the Factuality Degree of Events in TextTable 2Polarity value given contextual polarity.Context polarityMarker value + ?
u+ + ?
u?
?
+ uGermany (based onwhat the author claims that Nelles says, thus: germany nelles author),and its anchor is Nelles (nelles author).11 Event e1 (Nelles saying) is directly affirmed bythe author, and so the distinction between cognizer and anchor at this level is irrelevant.3.4 Expressing Factuality in Text: Factuality MarkersEvent factuality is conveyed by means of explicit polarity and modality-denoting ex-pressions of a wide variety.
Section 2.1 gave a brief introduction to the main types(namely, polarity and modality particles, the ESPs and syntactic constructions), andSection 2.3 illustrated the natural interplay that takes place among them in the contextof a sentence.
In the current section we organize the factuality-relevant informationpresent in lexical and syntactic structures so that it can be used by a model capableof accounting for the interaction of information across levels of embedding.
The focusis on English data, but the information is easily applicable to other languages, such asthose in the Romance and Germanic families.12Here and in the following sections, we understand the notion of context of afactuality marker as the level of scope most immediately embedding it.
For instance, thecontext of the polarity particle never in Example (14) (subsequent paragraph) is set bythe main clause.3.4.1 Polarity Particles.
Polarity particles of negation (from the adverb not to pronounslike nobody) switch the original polarity of its context (cf.
Polanyi and Zaenen 2006): Ifit is positive, the presence of a marker of negative polarity switches it to negative, andvice versa.
Nothing changes if the original context is underspecified.
For instance, inExample (14a) the context of the polarity particle never is positive, and so the resultingpolarity for event train is negative, as opposed to what happens in Example (14b).
InExample (14c) the contextual polarity is underspecified, and so is the factuality valuefor event train.
(14) a.
It is the case that [context:CT+ John never trainse].
(traine: CT?)b.
It is not the case that [context:CT?
John never trainse].
(traine: CT+)c. It is unknown whether [context:Uu John never trainse].
(traine: Uu)Table 2 models the interaction between contextual polarity (columns) and thepolarity value contributed by a new marker (rows).11 Therefore, a source performing the cognizer role for one event can be the anchor source of another.12 As a matter of fact, we plan to port it to Catalan and Spanish in the near future.273Computational Linguistics Volume 38, Number 2Table 3Modality value given contextual factuality.Contextual factualityPolarity = + Polarity = ?
Polarity = uMarker CT PR PS U CT PR PS U CT PR PS UCT CT PR PS Uu PS PR PS Uu CT PR PS UuPR PR PR PS Uu PR PR PS Uu PR PR PS UuPS PS PS PS Uu CT PR PS Uu PS PS PS UuU Uu Uu Uu Uu Uu Uu Uu Uu Uu Uu Uu Uu3.4.2 Particles of Epistemic Modality.
The following are some of the most common modal-ity particles, paired with the factuality value that they express.13(15) Value Adverbs Adjectives AuxiliariesCT+ : certainly, necessarily certain, necessary, surePR+ : apparently, probably apparent, likely, probablePS+ : possibly, presumably, seemingly possible, presumed, hypothetical can, could; may, mightCT?
: impossiblePR?
: improbable, unlikelyPS?
: uncertain, unsureUu : reportedly, supposedly reported, supposed must, shouldA modality particle, however, does not necessarily color the event it scopes overwith its inherent modal value.
The factuality value projected to that event depends onthe interaction between the particle on the one hand, and the modality and polarity ofits context, on the other.
Consider:(16) a. Koenig denies [context:CT?
that Freidin may have lefte the country].
(lefte: CT?)b.
Koenig suspects [context:PR+ that Freidin may have lefte the country].
(lefte: PS+)In Example (16a),may is used in a context of negative polarity and absolute certainty(CT?)
set by deny, whereas in Example (16b), it is used in a context of positive polarityand probable modality (PR+) set by suspect.
As a result, in the first example, event eis presented as a counterfact according to Koenig (CT?
), but as a possibility in thesecond (PS+).Table 3 illustrates the interaction between the polarity and modality values fromthe context (columns) and the modal value contributed by the marker (rows).14 Notethat the resulting values do not specify polarity information, except for the contextswhere contextual modality or polarity is underspecified (columns 4, 8, and 12, and lastrow), where the resulting polarity is u (underspecified).
In all other cases, the polaritycontributed by the marker will interact with that from the context as specified in Table 2.That is, positive contextual polarity will respect the original polarity denoted by themarker, whereas negative polarity will switch it.
For instance, the marker impossible,which has an inherent value of CT?, in a negative context will express PS+ (e.g., it is not13 Modal auxiliaries in English can express different types of modality (e.g., epistemic or deontic).Disambiguating among the possible interpretations of the same auxiliary is a goal beyond thescope of the current research.14 It has been compiled by exploring corpus data as well as made-up examples.
Combinations with midvalues (probability) are highly unusual; the resulting values are only estimated.274Saur??
and Pustejovsky Assessing the Factuality Degree of Events in Textimpossible that...).
The reader can use Table 3 to verify the interactions between deny andmay in Example (16a) (corresponding to the value in column 5, row 3), and suspect andmay in Example (16b) (column 3, row 2).3.4.3 Event-Selecting Predicates (ESPs).
As presented earlier, ESPs are predicates withan event-denoting argument (for instance, predicates of report, knowledge, belief, orvolition).
As part of their meaning, they qualify the factuality nature of that event.
Here,we distinguish between two kinds of ESPs: those introducing a new source in discourse,referred to as Source Introducing Predicates (SIPs), and those that do not, called Non-Source Introducing Predicates (NSIPs).Source Introducing Predicates (SIPs).
The additional source they contribute tends to cor-respond to their logical subject.
They typically belong to one of the following classes:(a) Predicates of report; for example, say, add, claim, write, publish.
(b) Predicates of knowledge: know, remember, learn, discover, forget, admit.
(c) Predicates of belief and opinion: think, consider, guess, predict, suggest.
(d) Predicates of doubt: doubt, wonder, ask.
(e) Predicates of perception: see, hear, feel.
(f) Predicates expressing proof: prove, show, support, explain.
(g) Predicates expressing some kind of inferencing process: infer, conclude,seem (as in: it seems that).
(h) Predicates expressing some psychological reaction as a result of an eventor situation taking place: regret, be glad (that).As part of their lexical semantics, SIPs express the factuality value that both the newsource they introduce (that is, the cognizer) as well as the anchor, assign to their event-denoting complement.
Compare the following examples built with two different SIPs:know and say.
For each sentence, the columns anchor and cognizer display the factualvalues that these two sources assign to the embedded event e (underlined).
(17) anchor cognizera.
The clients knew that his father had been killede.
CT+ CT+b.
... said ... Uu CT+By using the SIP know (Example (17a)), the anchor (here the text author) is posi-tioning himself as agreeing with the client (the cognizer) in considering that his fatherhad been killed.
On the other hand, by using the SIP say (Example (17b)) the anchorremains uncommitted.
Distinctions of this kind are fundamental for any task requiringperspective identification.
SIPs can therefore be characterized and grouped accordingto the configuration in the factuality assignments performed by anchor and cognizer.Notice that none of the SIPs in the following list has the same factual configuration.
(18) anchor cognizer anchor cognizersay: Uu CT+ know: CT+ CT+deny: Uu CT?
forget: CT+ Uusuppose: Uu PR+ pretend: CT?
CT+275Computational Linguistics Volume 38, Number 2Table 4Lexicon fragment for SIPs.
Entries: know and say.Contextual factualitymod=CT mod<CT mod=U+ ?
u + ?
u + ?
uknow (a) CT+ CT+ CT+ CT+ CT+ CT+ CT+ CT+ CT+(c) CT+ Uu Uu Uu Uu Uu Uu Uu Uusay (a) Uu Uu Uu Uu Uu Uu Uu Uu Uu(c) CT+ Uu Uu Uu Uu Uu Uu Uu UuMoreover, the factuality assessments made by anchor and cognizer will vary de-pending on the polarity and modality in the SIP context.
Compare the factuality assign-ments for sentences a in the following examples with those for sentences b, where theSIP is in a context of negative polarity.
(19) Context Example anchor cognizera.
CT+ Hes knows his father diede of a heart attack.
CT+ CT+b.
CT?
Hes does not know his father diede... CT+ Uua?.
CT+ Hes said his father diede of a heart attack.
Uu CT+b?.
CT?
Hes did not say his father diede... Uu UuThese data can be systematized into a lexicon for SIPs, with each entry specifyingthe factual value assigned to the embedded event by both the anchor and the cognizer,relative to the polarity and modality values of the SIP context.
The structure of lexicalentries is as shown in Table 4, where each predicate has the information distributed intwo different rows: one for the anchor (a), and another for the cognizer (c).
For instance,the factuality value of event die in Example (19a) can be found in the 1st column of therows for know, whereas the value for die in Example (19b) is in the 2nd column of thesame rows.Non-source Introducing Predicates (NSIPs).
For convenience, all ESPs that do not con-tribute any additional source in discourse are grouped under the term of NSIPs.
Theseinclude a varied set of predicate classes, such as:(a) Implicative and semi-implicative predicates: fail, manage, or allow.
(b) Predicates introducing a future event as their complement, like volition(want), commissive (offer), and command (require) predicates.1515 These predicates are considered as introducing a new source in Wiebe, Wilson, and Cardie (2005).
Here,however, they are treated as NSIPs due to semantic considerations.
Whereas SIPs express the epistemicattitude of their (logical) subject concerning the degree of certainty of the embedded event, predicateslike want or offer denote the role of their subjects as either having some degree of responsibility on theembedded event (e.g., promise/offer to go; force somebody to go), or being in a greater or lesser favorable statetowards its accomplishment (e.g., need/want to go).
In other words, they express distinctions within thespace of deontic modality.
Nothing precludes us from treating them as SIPs if preferred, however.276Saur??
and Pustejovsky Assessing the Factuality Degree of Events in TextTable 5Lexicon fragment for NSIPs.
Entries: manage and fail.Contextual factualityCT PR PS U+ ?
u + ?
u + ?
u + ?
umanage (a) CT+ CT- CTu PR+ PR- PRu PS+ PS- PSu Uu Uu Uufail (a) CT- CT+ CTu PR- PR+ PRu PS- PS+ PSu Uu Uu Uu(c) Change of state predicates: increase, change, or improve.
(d) Aspectual predicates: begin, continue, and terminate.By contrast to SIPs, NSIPs express a unique factuality assignment, attributed to theanchor source.
Table 5 illustrates this with the lexical entries for NSIPs manage and fail.We invite the reader to verify the factuality values of the embedded event as providedby the table, given different factuality contexts of the NSIP (manage/didn?t manage/mayhave managed to go, etc.
).3.4.4 Syntactic Constructions.
Factuality information can be also conveyed through syn-tactic constructions involving subordination.
Here we focus only on three of thesestructures: restrictive relative clauses, participial clauses, and purpose clauses.16Purpose clauses.
The main event denoted by a purpose clause is intensional in nature.Thus, all its relevant sources will assess it as underspecified (Uu), as is the case of seek inthe following example, where the ?b?
part shows the factual assessment:(20) a. Stronach resigned as CEO of Magna [to seeke a seat in Canada?s Parliament].b.
f (e, s0)=UuRelative and participial clauses.
Three different situations apply.
We illustrate them focus-ing on relative clauses, but assume the same treatment for participial clauses as well.First, in generic contexts, the event denoted by the relative clause is presupposed ascorresponding to a fact in the world (CT+), regardless of the modality and polarity ofthe event in the main clause.
In the following sentence, for example, the main event e1is characterized as a counterfact (CT?)
but the event working in the relative clause ispresented as a fact (CT+).
(21) a.
After World War II, industrial companies could not firee1 the women [relative cl.that had beenworkinge2 in their plants during the war period].b.
f (e1, s0)=CT?
f (e2, s0)=CT+16 Our decision is motivated by practical reasons.
These are the only constructions recognized by thedependency parser on which De Facto, the implementation of our model, relies.277Computational Linguistics Volume 38, Number 2Second, in quoted contexts the anchor remains uncommitted with respect to theevent in the relative clause:(22) a.
?
[quoted After World War II, industrial companies could not firee2 the women[rel cl.
that had been workinge3 in their plants during the war period]],?arguede1 Prof. Poes1 .b.
anchor : f (e3, author)=Uu cognizer : f (e3, prof.poe author)=CT+Third, in reported speech and attitudinal contexts, both the cognizer and the anchorcommit to the event in the relative clause as a fact (CT+).17(23) a. Prof. Poes1 thinks/saide1 [attit./rep.
that after World War II, industrial companiescould not firee2 the women [rel cl.
that had beenworkinge3 in their plants duringthe war period]].b.
anchor : f (e3, author)=CT+ cognizer : f (e3, prof.poe author)=CT+The last two interpretations have for long been a matter of discussion in the litera-ture.
Here, we embrace the analyses defended by Geurts (1998) and Glanzberg (2003),among others.
As will be shown in Section 5.4, this area turned out to be a source ofboth disagreement among annotators and error from our system.4.
Computing the Factuality Profiles of EventsThe current section puts forward an algorithm for a factuality profiler, that is, a toolfor computing the factuality profiles of events in text.
As such, it integrates all thecomponents presented so far: the scalar system of factuality degrees, an organized viewof factuality informants, as well as the structuring of the linguistic devices employed byspeakers to convey distinctions of factuality.
The details of the system presented hereare further elaborated in Saur??
(2008).4.1 Computational ApproachThe core procedure of the factuality profiler applies top?down, traversing a dependencytree.
Two reasons motivate a top?down approach.
The first one is of empirical nature.As seen, syntactic subordination is directly involved in the factual characterization ofevents (mainly through ESPs), and due to the recursive character of natural language,the factuality of a given event may depend on non-local information located severallevels higher in the tree (cf.
the set of sentences in Example (6)).The second reason for a top?down approach is methodological.
We conceive thefactuality profiler as a neutral and naive decoder; neutral in that it takes all sources asequally reliable; and naive, because it assumes that sources are trustworthy, based onthe Griceanmaxim of quality.
That is, ourmodel assumes that the information presentedin the text is true, without questioning anyone?s view or adopting a particular side.18 Inour model, the naive decoder assumption is applied by initiating the tree top of each17 Technically speaking, the presupposition is blocked at the quoted level in Example (22), whereas it isprojected up to the embedding level in Example (23).18 We can then consider a later postprocessing using different weights in order to favor one source as morereliable than another.278Saur??
and Pustejovsky Assessing the Factuality Degree of Events in TextFigure 2Computing event factuality inMia may not be aware that Joe knows (Paul is the father).sentence with a default factuality value of CT+; that is, all sentences are assumed tobe true according to their author.
This initial value will be potentially modified by thefactuality markers available at subsequent levels of the tree.
Consider the sentence:(24) Mia may not be awaree1 that Joe knowse2 Paul ise3 the father.Figure 2 exemplifies the initial steps of the procedure computing the factualityprofiles of its events (the full-fledged algorithm will be presented in Section 4.3, afterintroducing the relevant technical details).The computation proceeds as follows.
At the top level of the sentence, there is onlyone source involved, namely, the author of the text (s0).
She is the one uttering thesentence, and thus the one assessing the factuality of the event placed at its top level (i.e.,Mia not being aware of something, e1).
By the naive decoder assumption, the factualityat the top level is set to CT+ (Step 1 in Figure 2).As the algorithm proceeds down the tree, this value is updated to PS+ by themodal auxiliary may (Step 2) and to PS?
by the polarity marker not (Step 3).19 Thisis the factuality value available when the parser reaches event e1 (be aware), whichis consequently characterized as PS?
according to source s0, the text author.
In otherwords, the factuality profile of event e1, pbe awaree1, is the set of factuality values relativeto the relevant sources at its level: pbe awaree1={?PS?,s0?}
(Step 4).
In the figure, this isindicated by the dotted line.The computation continues.
Being a SIP, the predicate be aware contributes a newsource in the situation.
In addition to the author (s0), now there is also the source Mia(sm s0).
Mia is the cognizer of event e2 (she is in an ?unaware?
epistemic stance concern-ing Joe?s knowledge), whereas the author is the source anchoring that epistemic stance.Determining these roles is crucial, because nowwe can appeal to the lexical informationin Table 4 in order to set the perspective of each of these sources.
In accordance with theinformation there, the anchor of an epistemic state introduced by the SIP be aware (whichbehaves like the SIP know) in a context of factuality PS?
is characterized with a factuality19 For convenience, the contribution of the marker is signaled with mod if it affects the modality value,and pol if it impacts the polarity.
Some lexical elements (e.g., the complementizer that) are left off therepresentation when not relevant for the computation.279Computational Linguistics Volume 38, Number 2stance of certainty (CT+), whereas the cognizer, being unaware, remains uncommitted(Uu) (Step 5).
Because there are no other factuality markers affecting these values, whenthe parser reaches event e2 (Joe knowing something) these are the factuality assign-ments constituting the factuality profile of that event: pknowe2={?CT+,s0?,?Uu,sm s0?
}(Step 6).Thus, the factuality of every event corresponds to the factuality information avail-able at its context, as computed from the interaction of the different factuality markersscoping over it.
SIPs are crucial inflection points throughout this computation, giventhat they reset the evaluation situation by introducing additional sources and character-izing the factuality perspective these take.
Computationally, this is modeled bymeans ofthe concept of evaluation level.
Every time a new source is incorporated in the discoursebymeans of a SIP, a new evaluation level is created.
The next section details the technicalspecificities of this notion.4.2 Evaluation LevelsConsider each sentence, S, as consisting of one or more evaluation levels, l. By default,sentences have a root evaluation level, l0.
Sentences with SIPs havemore, correspondingto the levels of embedding created by these predicates.
For example, a sentence withtwo SIPs, in boldface in Example (25b), has three evaluation levels.
We identify eachevaluation level by its embedding depth, expressed in the bracket subindices.20(25) a.
[l0 Paul is the father].b.
[l0 Mia may not be aware that [l1 Joe knows [l2 Paul is the father]].Each evaluation level ln has:A set Sn of relevant sources.
At the root level l0, S0 contains only one relevant source,s0, corresponding to the author of the text.
At each higher level ln>0, a new sourceis introduced by the SIP triggering it.A set En of events (one or more), the factuality of which is evaluated relative to eachrelevant source s ?
Sn.A set Fn of contextual factuality values.
At the beginning of each new level, one ormore factuality values are set (cf.
the value CT+ applying the naive decoderassumption at the top level).
These values are relative to the relevant sources inSn, because each source may assess the same event differently.The task of event identification can be carried out by already existing event recog-nizers.
The next sections define the operations for identifying the set of relevant sourcesSn and the factuality values these assign to each event in any evaluation level ln.4.2.1 Identifying Relevant Sources and Their Roles.
The process for identifying the set ofrelevant sources Sn at each evaluation level ln can be defined inductively.20 Note that, because evaluation levels are only triggered by SIPs, a sentence can contain several levels ofsyntactic embedding and yet only one evaluation level, corresponding to the top one, l0.
The followingexample contains three embedded clauses (signaled with curly brackets) but only one evaluation level.
[l0 {After four years there}, Freidin managed {to return to the country {where she was originally from}} ].280Saur??
and Pustejovsky Assessing the Factuality Degree of Events in TextDefinition 1Relevant Sourcesi.
The set of relevant sources at level l0 contains only a (non-nested) source, whichcorresponds to the text author: S0 = {s0}.ii.
The set of relevant sources at level ln, where n > 0, is:Sn = Sn?1 ?
{sn z | sn is the new source introduced at level ln & z ?
Sn?1}Clause (i) needs no additional comment.
Clause (ii) states that the set of relevantsources Sn at level ln contains (a) the set of relevant sources at the previous level ln?1,that is, Sn?1 (this is expressed as the first part of the union); and (b) the set of all sourcechains composed of the new source sn introduced at that level by the corresponding SIP,and a relevant source from the preceding level, z ?
Sn?1 (second part of the union).We use the sentence Mia is not aware that Joe knows Paul is the father to illustrate theset of relevant sources Sn identified at each level ln by the previous definition:(26)[l0Miasm is not awaree1 that [l1 Joesj knowse2 [l2Paul ise3 the father]]]s0 s0 s0sm s0 sm s0sj s0sj sm s0Definition 1 seems to return an excessive number of sources at level l2.
In particular,the source chains sj s0 and sj sm s0 appear to be redundant, because both of them referto the same person, Joe.
Notwithstanding, the analysis is adequate if we want to accountfor Joe?s epistemic stance relative to the other sources involved in the situation.
Sourceexpressions sj s0 and sj sm s0 represent in fact two different perspectives.
Expressionsj sm s0 includes a reference to Mia, that is, it presents Joe?s epistemic stance accordingto Mia, based on what the author says.
On the other hand, expression sj s0 refers to Joe?sperspective only according to the author.As asserted in the sentence, Mia is clueless about Joe?s knowledge concerning Paul?spaternity, whereas according to the author, Joe knows the fact.
Strictly speaking, then,the event Paul being the father (e3) is evaluated by sj s0 as a fact in the world (CT+), butwill be presented with an uncommitted value (Uu) from the perspective expressed bysj sm s0.The next step now is determining the roles for each of these sources.
In Section 3.4on factuality markers, we saw that this distinction is crucial for identifying the factualitystance of each involved source.
Themechanism for finding the anchorsAn and cognizersCn at each evaluation level ln can be stated as follows:Definition 2Source Rolesi.
At level l0: A0 = {s0} and C0 = {s0}.ii.
At level ln, for n > 0:An = {s | s ?
Sn?1 & f (en?1, s) = Uu} andCn = {sn sa | sn is the new source introduced at level ln & sa ?
An}.Clause (i) defines the sets of anchors and cognizers at the evaluation level l0, whichcontains only the relevant source s0 (the text author).
At this level, the distinction281Computational Linguistics Volume 38, Number 2between anchor and cognizer is irrelevant, and so we arbitrarily establish s0 as per-forming both roles.Clause (ii) defines anchors and cognizers for higher evaluation levels, ln>0.
Inparticular, anchors are defined as those sources from the previous evaluation level, s ?Sn?1, that are not uncommited (Uu) towards the factuality of en?1, which is the SIP eventembedding ln (in the definition, the notation f (e, s) expresses the factuality assessmentmade by source s over event e).
Returning to Example (26), this restriction preventsselecting source Mary (sm s0) as the anchor of event e3, because she is presented ashaving an uncommitted perspective (she doesn?t know) on event e2.
Given that morethan one source in a level can commit to the same event, an event can have more thanone anchor, hence the notion of anchor set.Last, clause (ii) defines cognizers as those sources composed of the new sourceintroduced at level ln, sn, nested relative to any anchor source at that level, sa ?
An.Computationally the notion of cognizer is therefore dependent on that of anchor, andgiven that more than one anchor is possible at each level, the cognizer role can be per-formed by several source chains as well.
All other sources not satisfying the definitionof either anchor or cognizer are assigned the role of none, expressed as ( ).We apply Definition 2 to the earlier sentence, as well as to a second one, structurallyidentical but with different SIPs setting each evaluation level:(27)[l0Miasm is not awaree1 that [l1 Joesj knowse2 [l2Paul ise3 the father]]](a,c) s0 (a) s0 (a) s0(c) sm s0 ( ) sm s0(c) sj s0( ) sj sm s0(28)[l0Miasm sayse1 that [l1 Joesj tolde2 her [l2Paul ise3 the father]]](a,c) s0 (a) s0 ( ) s0(c) sm s0 (a) sm s0( ) sj s0(c) sj sm s0The roles for sources at level l0 and l1 are the same in both sentences: The roleassignment at level l0 is trivial, while at level l1, Mia is the cognizer of event e2 (Joetelling/knowing something) because she is the one cognitively aware, or unaware, ofthe fact that Joe is telling/knows something.
Nonetheless, source roles are different atlevel l2.
In Example (27) Mia cannot be the anchor of Joe?s epistemic stance becauseshe is presented as unaware of that (Uu).
Instead, the source anchoring Joe?s epistemicstance concerning event e3 is the author of the sentence, that is, s0 (as opposed to sm s0).Because of this, in Example (27) the cognizer role is performed by the source chain sj s0,whereas in Example (28) is performed by sj sm s0.4.2.2 Identifying Contextual Factuality Values.
In order to compute the factuality valuesassigned by the relevant sources to the events at each level, we start by associatinga contextual factuality value f to each relevant source s ?
Sn every time a new levelln is opened.
We represent this mapping as ?
f, s?, and subsequently define the set ofcontextual factuality values at level ln as: Fn = {?
f, s?| f is a factuality value & s ?
Sn }.The set of contextual factuality values Fn can be obtained as follows.282Saur??
and Pustejovsky Assessing the Factuality Degree of Events in TextDefinition 3Contextual Factuality Valuesi.
At level l0: Fn={?CT+, s0?}ii.
At level ln, for n>0: Fn={ ?
f, s?
| s ?
Sn & f=Lex(en?1, cen?1 , rs)}Clause (i) sets the contextual factuality for evaluation level l0.
By default, at level l0the set Fn contains only the value CT+ relative to the text author: ?CT+, s0?.
This appliesthe naive decoder assumption.In clause (ii), the contextual factuality value f associated to each source s is deter-mined by function Lex, which performs a lookup into the SIPs lexical base (Table 4)given the following parameters:rs: The role performed by the source s ?
Sn (anchor, cognizer, or none).en?1: The SIP in the previous evaluation level ln?1 that is embedding the current level,ln.
The information in its lexical entry will provide the contextual factuality valuesfor the relevant sources at the current evaluation level (cf.
Table 4).cen?1 : The committed factuality value that was assigned to SIP en?1 in the previous levelln?1.
All factuality values, except for the fully underspecified Uu, are consideredcommitted values.
For instance, in Example (29), the factuality value to be used forsetting the contextual factuality values for level l2 is CT+, the only committed valueassigned to event knows (e1) in level l1.
(29)[l0Miasm is not awaree1 that [l1 Joesj knowse2 [l2 Paul ise3 the father ]]](a,c) f (e1, s0)=CT?
(a) f (e2, s0)=CT+ (a) f (e3, s0)=CT+(c) f (e2, sm s0)=Uu ( ) f (e3, sm s0)=Uu(c) f (e3, sj s0)=CT+( ) f (e3, sj sm s0)=UuWe illustrate how clause (ii) works with the operation of setting the contextual fac-tuality values when opening the evaluation level l1 in Example (29).
The SIP embeddingthis level (corresponding to parameter en?1 in function Lex) is be aware, which receivesthe committed factuality value of CT?
(parameter cen?1 ).
Furthermore, at level l1 thereare two relevant sources, s0 and sm s0, the first one performing the role of anchor, andthe second the role of cognizer (parameters rs).
With all that information at hand, thecontextual factuality values for level l1 will be obtained by means of a dictionary lookupperformed by function Lex(en?1, cen?1 , rs).
Using the lexical information in Table 4, thisoperation can establish the following contextual factuality values for the anchor andcognizer sources at the new level l1:a. Lex(be aware, CT?, anchors0 ) = CT+(3)b. Lex(be aware, CT?, cognizersm s0 ) = UuIf the role is none, there is no need to perform the lexical look-up.
The contextualfactuality value will be set to underspecified (Uu).4.3 AlgorithmThe factuality profiler algorithm is provided in Algorithm 1, which further developsthat presented in Saur??
and Pustejovsky (2007) by incorporating syntactic constructions.283Computational Linguistics Volume 38, Number 2Algorithm 1 De Facto: the Factuality Profiler.1: n ?
02: set level ln3: for all i in TREE do4: #PART 1: CHECK FOR SYNTACTIC MARKER5: if i is head of relative, participle, or purpose clause then6: update contextual factuality, Fn7: end if8: #PART 2: CHECK FOR EVENT9: if i is an event then10: obtain the factuality profile of i, pi11: end if12: #PART 3: CHECK FOR LEXICAL MARKER13: if i is a SIP then14: n ?
n+ 115: set level ln16: else if i is another type of marker then17: update contextual factuality, Fn18: end if19: end forIts core procedure (lines 3?19) consists of three main components.
Part 1 implements theeffect of syntactic-based factualitymarkers (specifically, relative, participle, and purposeclauses), Part 2 is in charge of assigning the factuality value to every found event, andPart 3 implements the effect of lexical markers on the contextual factuality values.Part 3 (checking whether the node found is a lexical marker of any sort andsubsequently updating the contextual factuality values) needs to be performed afterPart 2 (obtaining the factuality profile of any found event) due to the double nature ofESPs, which are both event-denoting expressions and, at the same time, lexical markers.As markers, they affect the contextual factuality of their embedded events.
Hence,their factuality profile (Part 2) needs to be obtained before they update the contextvalues (Part 3).
This is illustrated in Figure 2.
When the algorithm index i is at nodebe aware, it must first obtain the factuality profile of that event (Step 4) before updatingthe contextual factuality according to the semantics of the verb be aware (Step 5).
Bycontrast, Part 1 needs to be run before evaluating the factuality of the event given that itimplements the effect of syntactic constructions imposing a specific factuality value toits main event.The functionality of the algorithm splits into three main components, which are incharge of: (i) setting each new evaluation level ln; (ii) updating the set of contextualfactuality values, Fn, every time a newmarker is found; and (iii) obtaining the factualityprofile of events.
We discuss them in what follows.
(i) Set Level ln (lines 1?2 and 14?15).
This function is called every time a new level isopened, be it at the top of the tree (lines 1?2) or when a SIP is found (lines 14?15).
Itexecutes the following steps:1.
Identify the set of relevant sources at the current level, Sn.
This procedureis carried out applying Definition 1.284Saur??
and Pustejovsky Assessing the Factuality Degree of Events in TextAlgorithm 2 Syntactic Markers in De Facto.1: if i is head of a participle or a relative clause then2: if i is in a quoted area then3: anchor=Uu ; cognizer=CT+ #APPLY PLUG4: else5: anchor=CT+ ; cognizer=CT+ #PROJECT PRESUPPOSITION6: end if7: else if i is head of a purpose clause then8: anchor=Uu ; cognizer=Uu #APPLY UNCOMMITMENT9: end if2.
For each s ?
Sn, identify its role (anchor, cognizer, or none).
Computedapplying Definition 2.3.
Set the contextual factuality values, Fn.
This is performed applyingDefinition 3, based on lexicon look-up.
(ii) Update the contextual factuality, Fn (lines 5?6 and 16?17).
The update may betriggered by either a syntactic or a lexical marker.
Lexical markers that are appropriatehere are polarity particles, modality particles, or NSIPs.21 Any time one of them isfound in ln, the profiler updates the contextual factuality values v ?
Fn according tothe information it conveys (lines 16?17).
Syntactic constructions, on the other hand,reset the contextual factuality values according to Algorithm 2, which articulates thelinguistic analysis concerning participle, relative, and purpose clauses, as presented inSection 3.4.
(iii) Obtain the factuality profile of e, Pe (lines 9?10).
Applied when an event is found.Due to the on-the-fly updating of the contextual factuality values in Fn whenever a newlevel is set (i) or a newmarker is found (ii), the event profile is in fact already computed.The factuality profile for event en, pen , corresponds to the set of contextual factualityvalues Fn available at that point.5.
Experiments and Evaluation5.1 ImplementationThe modeling of the factuality profiler put forward here has been implemented andevaluated against a corpus annotated for that purpose.
The resulting tool, calledDe Facto, integrates the algorithm in the previous section, along with the linguisticresources with lexical and syntactic information structured as presented in Section 3.4,and articulated around the scalar definition of factuality values developed in Section 3.2.The approach is therefore entirely symbolic, involving lexical look-up while top?down traversing the dependency tree of each sentence.
The lexical resources informingDe Facto include those listed here.
They will be made available to the community in thenear future.21 Recall that SIPs affect the contextual factuality as they set a new evaluation level.285Computational Linguistics Volume 38, Number 2Table 6Distribution of ESPs in De Facto.Part of Speech SIPs NSIPs TotalVerbs 204 189 393Nouns 58 107 165Adjectives 27 61 88Total 289 357 646Polarity particles: A total of 11 negation particles distributed among adverbs (such asnot, neither), determiners (no, non), and pronouns (none, nobody), together with thetable on contextual polarity interactions (Table 2).Modality particles: The set of 31 particles presented in Example (15), each accom-panied with their default modality interpretation, as well as their interactiontable (Table 3).ESPs: The lexical entries for a total of 646 ESPs, distributed as shown in Table 6.
Lexicalentries structure their factuality information as illustrated in Tables 4 and 5 (forSIPs and NSIPs, respectively).
The information in each lexical entry was compiledmanually in a data-driven fashion by exploring its use in our corpora of reference,TimeBank and the American National Corpus (Slate and NYTimes fragments).22De Facto takes as input a document (or a set of them) and returns the factualityprofiles of each event.
Input documents have been tokenized, POS-tagged, and parsedinto dependency trees with the Stanford Parser (version 1.6; de Marneffe, MacCartney,and Manning 2006).
In the current implementation, De Facto does not incorporateany component for recognizing events nor identifying source mentions in text.
Thisinformation was generated frommanual annotation and fed to the tool.
The chaining ofdifferent source mentions into relevant sources is computed automatically, however, bymeans of Definition 1.As output, De Facto returns the factuality profile of each event in the input text.Example (31) shows the factuality profiles for the events in (30).
(30) Analystss1 saide1 the governments2 knewe2 a peaceful solution wase3 in reach.
(31) e1: said fp(e1) = { ?s0, CT+? }
(anchor)e2: knew fp(e2) = { ?s0, Uu?, (anchor)?s1 s0, CT+? }
(cognizer)e3: was fp(e3) = { ?s0, Uu?,?s1 s0, CT+?, (anchor)?s2 s0, Uu?,?s2 s1 s0, CT+? }
(cognizer)5.2 Development and Evaluation CorpusFor developing and evaluating De Facto, we compiled FactBank, a corpus annotatedwith information concerning the factuality of events (Saur??
and Pustejovsky 2009a).22 Documented, respectively, at: http://www.timeml.org/site/timebank/timebank.html, andhttp://americannationalcorpus.org.286Saur??
and Pustejovsky Assessing the Factuality Degree of Events in TextTable 7Confusion matrix: Gold standard (rows) vs. De Facto output (columns).CT+ CT?
Ctu PR+ PR?
PS+ PS?
Uu NA TotalCT+ 1,131 0 0 0 0 2 0 84 59 1,276CT?
13 33 0 0 0 0 0 1 4 51CTu 1 0 0 0 0 0 0 0 0 1PR+ 12 0 0 8 0 0 0 3 2 25PR?
0 0 0 0 0 0 0 0 0 0PS+ 7 0 0 0 0 22 0 2 2 33PS?
0 0 0 0 0 0 2 0 0 2Uu 226 4 1 2 0 17 0 532 22 804Total 1,390 37 1 10 0 41 2 622 89 2,192FactBank consists of 208 documents, which include all those in TimeBank (Pustejovskyet al 2006) and a subset of those in the AQUAINT TimeML Corpus.23 The TimeBankpart was used for developing De Facto and its associated linguistic resources, and theAQUAINT TimeML part was set as the gold standard for evaluating its performance.TimeBank contains 183 documents (amounting to 88% of the documents in FactBank)and 7,935 events (83.6% of the events), and the AQUAINT part has 25 documents (12%)and 1,553 events (16.4%).Overall, FactBank contains a total of 9,488 events.
Given that each event can havemore than one relevant source, FactBank has a total of 13,506 event/source pairsmanually annotated with the set of factuality distinctions introduced in Table 1.
Theannotation has applied a battery of discriminatory tests grounded on the linguistic andlogical relations at the core of Horn?s analysis (refer to Section 3.2).
The inter-annotationagreement from that exercise is ?
= 0.81 (over 30% of events in the corpus).
In termsof pairwise F1-score (that is, taking one of the annotators as the gold standard), theagreement between annotators yielded: CT+: 0.93, CT?
: 0.83, PR+: 0.57, PR?
: 0.46, PS+:0.56, PS?
: 0.75, and Uu: 0.88.
Overall, these results are highly satisfying consideringthe difficulty of the task and thus validate the approach on the annotation.
See furtherdetails in Saur??
and Pustejovsky (2009b).5.3 PerformanceThe confusion matrix resulting from mapping the subset of FactBank used as goldstandard against De Facto output is shown in Table 7.
The total number at the bottom-right corner corresponds to the number of event/source pairs in the gold standard, thatis, the number of instances to be classified with a factuality value.
Classes PRu and PSuare not shown because they have no instance in the gold standard.Instances classified in the NA column correspond to event/source pairs for whichDe Facto did not return a factuality judgment.
An analysis of this pointed to errors in thedependency trees as the possible cause of this behavior.
In other words, they seemed tobe pairs involving sources mentioned in subordinated clauses that had not been parsedproperly and, as a consequence, De Facto could not pair with their correspondingevents.
Because subordination structures are fundamental in De Facto?s algorithm, we23 http://www.timeml.org/site/timebank/timebank.html.287Computational Linguistics Volume 38, Number 2decided to evaluate the system on two different versions of the gold standard: a firstone with the dependency trees originally returned by the parser (corresponding to thedata in Table 7), and a second one where dependency errors on subordination had beenmanually corrected.
In total, we corrected an estimated 2% (at the lowest bound) of thedependencies involving subordination structures.Table 8 shows the results from running De Facto against both versions of the goldstandard.
De Facto?s performance is evaluated in terms of precision and recall (P&R)and their harmonic mean, F1 score.
We considered only those categories for whichthere exist more than 10 instances classified as such in the gold standard; that is: CT+,CT?, PR+, PS+, Uu.
Furthermore, P&R for the whole corpus is obtained by applyingthe measures of macro- and micro-averaging (last two columns in the table).
Macro-averaging averages the result obtained in each class, and micro-averaging applies overthe set of instances, regardless of class distribution.
The first measure gives equal weightto each class and hence over-emphasizes the performance of the less populated ones,and the second one over-emphasizes the performance of the largest classes becauseit assigns equal weight to each instance.
Given the uneven class distribution in ourgold standard, we take the combination of both measures as indicative of the lowerand upper bounds of the result.As can be seen from Table 8, the corrected version of the gold standard attains muchhigher recall than the original one (especially for the classes CT?, PR+ and Uu).
Thereason for that is the absence of event/source pairs tagged as NA by our system (asopposed to what was appreciated in the confusion matrix on Table 7).
In the correctedversion, De Facto was able to follow the dependency tree, appropriately pair all theevents with their sources, and return a factuality value for each pair.The results obtained in all the categories for the corrected version of the goldstandard are equivalent to or higher than those in the original one, except for the veryparticular case of PR+ precision.
The fact that increasing the quality of the parsing resultsin better performance of the system validates the linguistic model in De Facto.The results for CT?, PR+, and PS+ must be interpreted cautiously, given the sparsityof data in these classes.
Nevertheless, the high precision achieved for CT?
is encour-aging, especially considering that polarity here is not only determined locally but bymeans of subordinating predicates as well.
Similarly, the distinction between the twomodal degrees PR and PS seems pertinent and possible to determine by the system.
Noinstance was misclassified between the two, as shown in the confusion matrix (Table 7).Table 8P&R for each relevant category and for the whole corpus (macro- and micro-average).CT+ CT?
PR+ PS+ Uu Macro-A Micro-AOriginal parsesPrecision 0.81 0.89 0.80 0.54 0.86 0.78 0.82Recall 0.89 0.65 0.32 0.67 0.66 0.64 0.79F1 0.85 0.75 0.46 0.59 0.75 0.70 0.80Corrected parsesPrecision 0.86 0.90 0.73 0.56 0.86 0.78 0.85Recall 0.92 0.75 0.44 0.67 0.77 0.71 0.85F-1 0.89 0.82 0.55 0.61 0.81 0.74 0.85288Saur??
and Pustejovsky Assessing the Factuality Degree of Events in TextEvaluating De Facto?s performance on both versions of the gold standard providesa look into two different aspects of the system.
Whereas the original version shows itsimpact on a standard NLP pipeline, the corrected version puts the proposed algorithmto test by exposing it to complex sentences with several levels of embedding.
In order toassess De Facto?s results regarding these two aspects, we generated a baseline from a su-pervised learning approach, by means of support vector machines (SVM).
We followedPrabhakaran, Rambow, and Diab (2010), which is state-of-the-art on automatic taggingof committed belief (cf.
Diab et al 2009b), a notion equivalent to modality and whichdistinguishes between certain vs. uncertain events.
The classification that they proposeis less fine-grained than ours (certain vs. probable vs. possible), but the informationsupporting the distinctions is exactly the same, and therefore we adopted the featuresemployed in their best classifier (listed from 1 to 12 in the following example).
Inaddition, we added feature 13 given that our classifier was not aiming at identifyingevent mentions in the text (contrary to Prabhakaran, Rambow, and Diab?s model), andfeatures 14 and 15 to cope with distinctions along the axis of polarity (not addressed bythat system).
(32) 1. isNumeric Word is Alphabet or Numeric?2.
POS Word?s POS tag3.
verbType Modal, auxiliary or regular (nil if not a verb)4. whichModalAmI If I am a modal, what am I?
(nil if not a modal)5. amVBwithDaughterTo Am I a VB (base verb) with a daughter to?6.
haveDaughterPerfect Do I have a have form daughter?
(only for verbs)7. haveDaughterShould Do I have a should daughter?
(only for verbs)8. haveDaughterWh Do I have a daughter which is: where, when, while, who, why?9.
haveReportingAncestor Am I an event with an ancestor whose lemma is: believe, accuse,insist, seem, tell, say, find, conclude, claim, trust, think, suspect, doubt,suppose?10.
parentPOS What is my parent POS tag?11.
whichAuxIsMyDaughter If my daughter is an auxiliary, what is it?
(nil if not an auxiliary)12. whichModalIsMyDaughter If my daughter is a modal, what is it?
(nil if not a modal)13. amEvent Am I an event?14.
whichPolarAmI If I am a polar marker, am I a conjunction (nor), a pronoun (none)or other?15.
whichPolarIsMyDaughter If my daughter is a polar particle, what type is it?16.
amSource Am I a source?17.
whichSIPtypeAreMyAncest.
If I am a source, what SIP type are my ancestors?
(based on theSIP classification in Section 3.4.3)18. whichDepRelWithMyParent If I am a source, what is my dependency relation with my parent?19.
whichSIPtypeAmI If I am a SIP, which type am I?Prabhakaran, Rambow, and Diab?s work assesses the committed belief of onlythe author source, but in our case an event can receive several factuality values fromdifferent sources.
Hence, we decided to generate two different models: the author levelmodel, in which the factuality of events is assessed relative to the author of the text (i.e., atthe level of source s0), and the top source level model, in which event factuality is assignedaccording to the source with a higher level of nesting in the set of relevant sources forthat event (e.g., sm sj s0).
Thus, features 16?19 were added to convey information on thetop-level sources as well.Following Prabhakaran, Rambow, and Diab?s work, we trained our SVM classifiersusing YAMCHA (Kudo andMatsumoto 2000) and used the same parameters applied totheir best classifier: context width of 2 (i.e., the feature vector of any token includes thetwo tokens before and after), and the One versus allmethod for multiclass classificationon a quadratic kernel with a c value of 0.5.
For evaluation, we performed a 10-foldcross-validation.289Computational Linguistics Volume 38, Number 2Table 9 shows the results (F1 measure) of the two SVM classifiers (author and topsource levels, as well as their average) running on both the original and the correctedversions of the gold standard.
For a more meaningful comparison with our system,we also computed De Facto?s performance on these two source levels.
The results areshown in Table 10, where we also added, as a reference point, the figures obtained fromevaluating De Facto on all source levels (corresponding to the F1 rows in Table 8).Furthermore, we assessed whether De Facto?s improvement over the baseline isstatistically significant applying a one-sample two-tailed t-test over the results for everycategory at each source level.
We applied the one-sample version of the t-test because DeFacto?s performance results do not conform a distribution, because they were obtainedfrom running the system once over the evaluation subcorpus.
In the test, the sampledata corresponds to the results from the 10 runs of the SVM classifier, whereas theDe Facto?s value is taken as the expected (or null) hypothesis.
For the top and authorlevels, the degree of freedom is df = 9 (from 10 runs ?
1), while for their average it isdf = 19 (10 + 10 runs ?
1).As seen in Table 9, there is no significant difference between the baseline generatedfrom the original and the corrected versions of the corpus, which is explained by thefact that the SVM models are based on fairly local linguistic features and use very littleinformation on subordination structures.
What is in fact most noticeable in the baselinesis the difference between the results on the author and the top source levels for the lesspopulated classes (CT?, PR+, and PS+).
The top source level reachesmuch higher results,which could be explained by the greater use of dependency-based features providinginformation on the top source (features 15?18).
This hypothesis underlines the role ofdeep linguistic features for identifying the factuality of event mentions in text.By contrast to the baseline, De Facto shows a significant improvement when run-ning on the corrected version of the gold standard, which proves the adequacy ofits model to the linguistic information it targets.
The downside of that is potentiallytoo much dependency on high quality linguistic data in order to obtain acceptableperformance degrees.
Nevertheless, the results in the two tables show that De Factois performing equal or better than the SVM classifiers when fed with original (notcorrected) output from a standard NLP pipeline, especially in the case of less populatedclasses, which happen to be the ones with marked polarity and modality values, that is,which feature negative polarity, or probable and possible modality values.Table 9Baseline performance (F1 measures).CT+ CT?
PR+ PS+ Uu Macro-A Micro-AOriginal parsesAuthor 0.88 0.53 0.07 0.29 0.75 0.53 0.83Top sources 0.92 0.69 0.51 0.50 0.57 0.66 0.86Average 0.90 0.61 0.29 0.39 0.66 0.59 0.84Corrected parsesAuthor 0.88 0.54 0.07 0.27 0.77 0.53 0.83Top sources 0.92 0.67 0.50 0.50 0.51 0.64 0.85Average 0.90 0.61 0.28 0.38 0.64 0.58 0.84290Saur??
and Pustejovsky Assessing the Factuality Degree of Events in TextTable 10De Facto performance (F1 measures).CT+ CT?
PR+ PS+ Uu Macro-A Micro-AOriginal parsesAll sources 0.85 0.75 0.46 0.59 0.75 0.70 0.80Author 0.88 0.88 *** 0.67 *** 0.33 0.78 0.73 *** 0.84 *Top sources 0.90 0.79 * 0.33 * 0.66 ** 0.58 0.67 0.84Average 0.89 0.84 *** 0.50 ** 0.50 * 0.68 0.70 *** 0.84Corrected parsesAll sources 0.89 0.82 0.55 0.61 0.81 0.74 0.85Author 0.90 0.91 *** 0.67 *** 0.35 0.84 ** 0.75 *** 0.88 *Top sources 0.93 0.85 ** 0.53 0.67 ** 0.65 * 0.74 * 0.88Average 0.92 0.88 *** 0.60 *** 0.51 * 0.75 * 0.75 *** 0.88 *** p ?
0.05** p ?
0.01*** p ?
0.001The low performance of the SVMmodels is due to the small sample of these classesin the corpus, and so it can be expected that with more training data the classifierswill learn to perform better, a fact that makes them dependent on the availability ofsignificantly larger annotated corpora.
De Facto, on the other hand, is grounded onthe linguistic expression that articulates factuality distinctions in natural language, andtherefore does not depend as much on corpus size but on a good modeling of theinteraction among the relevant linguistic structures.
In this sense, the results shownhere are quite promising regarding the capabilities of our system, even though it suffersfrom some limitations, as will be seen next.5.4 Error AnalysisWe analyzed the errors returned by De Facto when run on the manually correctedversion of the corpus.
With this choice, we wanted to avoid error from the parser andhence obtain a more precise assessment on the adequacy of our computational model.This version of the corpus has 320 event/source pairs wrongly classified (14.6% on thetotal number of pairs), whereas the original version has 464 pairs (21.2%).Most disagreements between De Facto?s output and the gold standard are dueto limitations in our system (84.4%), which mainly classify into insufficient coverageof factuality markers, either lexical or syntactic, and structural and lexical ambiguity.Other disagreements are due to some inaccuracy in the gold standard annotation(7.5%), or to an incorrect analysis from the dependency parser which escaped ourmanual correction (8.1%).
Table 11 shows the error type distribution, distinguishingbetween lexical and syntactic error when relevant.Insufficient coverage.
There are a number of syntactic constructions crucially involvedin determining the factuality nature of events and which, nevertheless, have not been291Computational Linguistics Volume 38, Number 2accounted for here, most commonly: copulative phrases, cleft structures (e.g., But it?s nottonight we?re worriede about), and conditional constructions (of the form if... then..., andequivalent).
This amounts to 32.5% of the total error.
De Facto also suffers from gapsat the lexical level, even though in a much lesser degree (1.9%).
It lacks, for example,ESPs such as conspiracy (as in: a conspiracy to commit murder) or easy (e.g., it is easierto do it).Ambiguity.
De Facto does not cope with lexical polysemy of any type (18.1% ofthe total error).
For example, the modal auxiliary would is employed in embeddedcontexts to express future (and hence CT+, which is how De Facto models this tense),but there are certain constructions in which it expresses some degree of uncertainty.
Afurther interesting case involves ambiguity regarding the temporal reference of events.De Facto assumes that aspectual predicates of termination (e.g., stop, finish) qualifytheir embedded event as a fact (that is, it is a fact that they took place in the world),whereas the gold standard treats them as counterfactual (the event does not holdanymore).At the syntactic level, there are cases of truly ambiguous constructions, such asrelative and participial clauses, as well as event-denoting nouns, when embedded undercontexts of report, propositional attitudes, or uncertainty (28.1% of the total error).Some of these ambiguities have long been discussed in the linguistics literature, andhappened to be a source of remarkable disagreement among the FactBank annotatorsas well (cf.
Saur??
and Pustejovsky 2009b).
The high error rate in this area seemedto suggest that the approach assumed in De Facto for these constructions (followingGeurts [1998] and Glanzberg [2003]; see Section 3.4.4) was not completely adequate.Thus, we experimented running De Facto without the part of the algorithm dealingwiththem (Algorithm 2, lines 1?9).
The results, however, are inconclusive.
Although there isa slight improvement of 1 or 2 points over the F1 of categories PS+ (from 0.59/0.61to 0.61/0.63 when running on the original/corrected parses), and Uu (from 0.75/0.81to 0.77/0.82 on the original/corrected parses), there is a decrease in other categories,such as PR+ (from 0.46 to 0.43, original parses) and CT?
(from 0.82 to 0.80, correctedparses).Overall, the main limitations observed here are shared with other work also ap-proaching tasks of sub-sentential interpretation by means of linguistically heavy andresource-intensive models, such as Moilanen and Pulman (2007) or Neviarouskaya,Prendinger, and Ishizuka (2009), which address sentiment analysis based on the princi-ple of compositionality.
Moilanen, Pulman, and Zhang (2010) successfully explore thefeasibility of combining this approach with a machine learning-based classifier.Table 11Error classification.Error source % % Lexical % SyntacticInsufficient coverage 34.4 1.9 32.5De Facto Ambiguity 46.2 18.1 28.1limitations Other 3.8 ?
?Subtotal 84.4 20 60.6Gold standard 7.5 ?
?Other errorWrong dependency trees 8.1 ?
?sourcesSubtotal 15.6 ?
?292Saur??
and Pustejovsky Assessing the Factuality Degree of Events in Text6.
Related WorkThe last decade has seen a growing interest on speculative language and its treatmentwithin NLP.
This has crystallized into research from a variety of perspectives, includinggeneral but also domain-specific (mainly biomedical), and reflects not only in the build-ing of processing systems, but also in the area of corpus creation, where most of theconception and structuring of factuality-related information takes place, thus providingthe support for more applied investigations.6.1 Factuality Information in CorporaIn some corpora, factuality-related information is annotated as information comple-mentary to the main phenomenon they target.
It is, for instance, contemplated indifferent versions of the ACE corpus for the Event and Relation recognition task (see,e.g., ACE 2008), in the Penn Discourse TreeBank (Prasad et al 2007), and in TimeBank(Pustejovsky et al 2006).
In other corpora, factuality information becomes the epicenterof their annotations.
For example, Rubin (2007, 2010) is concerned with the notion ofcertainty, the Language Understanding Annotation Corpus (Diab et al 2009a) focuseson the author?s committed belief towards what is reported (a notion comparable tothe modality axis in event factuality), and the small knowledge-intensive corpus byHenriksson and Velupillai (2010) targets degrees of certainty.In the bioNLP area, factuality and related information is lately becoming a no-table area of research and has led to the creation of remarkable corpus resources.
TheBioScope corpus (Vincze et al 2008) contains more than 20,000 sentences annotatedwith speculative and negative key words and their scope.
Based on this experience,Dalianis and Skeppstedt (2010) compiled a corpus of Swedish electronic health recordswith speculation and negative cues marked up, together with the values resulting fromtheir interaction.
The corpus presented in Wilbur, Rzhetsky, and Shatkay (2006) tagsthe polarity and certainty degree of clauses, along with other dimensions.
The GENIAEvent corpus (Kim, Ohta, and Tsujii 2008) contains 1,000 abstracts with biological eventsannotated with polarity and degrees of certainty, in addition to other information suchas the lexical cues leading to these values (Ohta, Kim, and Tsuji 2007).
Such an approachis followed by the currently on-going large scale annotation effort (Nawaz, Thompson,and Ananiadou 2010), with an event-centered annotation that includes polarity, degreesof certainty, and sources.6.2 Systems for Identifying Factuality and Related InformationSystems devoted to identifying factuality-related information can be generally classifiedinto two groups: (a) those prioritizing the identification of linguistic structure (that is,speculative cues and their scope); and (b) those focusing on the factuality values thatresult from these cues and their interaction.
The first approach mostly revolves aroundthe BioScope corpus, which has become a good catalyzer for research on this topic in thebiomedicine domain.
Part of it was used for the CoNLL-2010 shared task on Learning ToDetect Hedges and their scope in Natural Language Text (Farkas et al 2010).
Moreover,it is at the basis of explorations on hedging and negation cues scope identification, suchas Morante and Daelemans (2009a, 2009b), which apply a supervised sequence labelingapproach, or O?zgu?r and Radev (2009) and Velldal, Ovrelid, and Oepen (2010), which293Computational Linguistics Volume 38, Number 2combine supervised learning techniques with rule-based systems exploiting syntacticpatterns.Identifying modality and polarity cues and their scope is certainly a key aspect fordetermining the degree of factuality of events, but not sufficient if the values result-ing from these cues and their interactions are not provided.
Complementary to thisperspective, the second approach to factuality-related information puts the emphasison identifying speculative degrees (along the lines assumed in this article).
Pioneeringwork within this view is Light, Qiu, and Srinivasan (2004), a paper exploring the use ofspeculative language in sentences from Medline abstracts.
It experiments with a hand-crafted list of hedge cues as well as a supervised SVM in order to classify sentences aseither certain, high, or low speculative.
Drawing on this, Medlock and Briscoe (2007)address the classification of sentences into speculative or non-speculative as a weaklysupervised machine learning task and perform experiments with SVMs, achieving aprecision-recall breakeven point of 0.76.
This line of research is further explored bySzarvas (2008).
On the other hand, Shatkay et al (2008) use the corpus developed byWilbur, Rzhetsky, and Shatkay (2006) to explore machine learning classifiers for taggingdata along the five dimensions in which it is marked up, including polarity and degreesof certainty.
It is a challenging task in that it involves simultaneous multi-dimensionalclassification and, in some dimensions also, multi-label tagging.
They experiment withSVMs andMaximum Entropy classifiers, and report very good results (macro-averagedF1 of 0.71 for degrees of certainty and 0.97 for polarity).Resourcing to rich linguistic information.
As argued throughout the article, subordinationstructures play a crucial role in determining the factuality values of events as well astheir relevant sources, but most of the work presented so far addresses the problem ofevent factuality identification by means of classifiers fed with linguistic features that arenot fully sensitive to sentences?
structural depth and the complex interactions amongtheir constituents.
Previous work using subordination syntax to model factuality is thetool for identifying polarity and modality using lexical information and subordinatingcontexts by Saur?
?, Verhagen, and Pustejovsky (2006).
Similarly, Kilicoglu and Bergler(2008) use the data from Medlock and Briscoe (2007) to show the effectiveness oflexically centered syntactic patterns for distinguishing between speculative and non-speculative sentences.These systems are, however, limited in that they neither account for the effect ofmultiple embeddings, nor distinguish between different sources.
To our knowledge, thefirst system in which factuality-related information is computed applying top?downon a dependency tree, and hence potentially overcoming these limitations, is Nairn,Condoravdi, and Karttunen (2006), who model the percolation of the polarity featuredown the syntactic structure.
A somewhat comparable perspective is adopted in thework on sentiment analysis addressing the problem from a compositional perspective.For example, in Moilanen and Pulman (2007) and Moilanen, Pulman, and Zhang (2010)the well-known semantics principle of compositionality is applied for sentiment po-larity classification at the (sub)sentence level, and in Neviarouskaya, Prendinger, andIshizuka (2009), for recognizing emotions such as anger, guilt, or joy.
All these casesinvolve the use of deep parsing and rich lexicons in a way very similar to the modelpresented here for event factuality.
The main difference with respect to our approach,however, is that De Facto applies top?down, whereas these systems follow a bottom?up processing of the data, as determined by the principle of compositionality.
Suchdifference is not trivial.
A top?down approach allows to keep track and compute the294Saur??
and Pustejovsky Assessing the Factuality Degree of Events in Textnesting of the different sources involved in the factuality assessment, a computationthat does not follow naturally from processing bottom?up.Factuality information according to their sources.
A common feature in all the approachesmentioned so far is the lack of awareness of the role of information sources.
The funda-mental role of source participants is already acknowledged in previous work on opinionand perspective (most significantly, Wiebe, Wilson, and Cardie [2005]).
Concerningfactuality-related information, the work incorporating the parameter of sources in thecomputation is pretty recent.
It is acknowledged in Diab et al (2009b) and Prabhakaran,Rambow, and Diab (2010), who nevertheless explore only the feasibility of identifyingthe committed beliefs of the text author, as annotated in the Language UnderstandingAnnotation Corpus (Diab et al 2009a), by means of SVM classifiers, in the first casewith basic linguistic features whereas in the second one incorporating dependency-based features, reaching a maximum overall F1 of 53.97 and 64.0, respectively.
Thedistinction of event factuality depending on sources is also present in the corpuspresented by Nawaz, Thompson, and Ananiadou (2010), who differentiate betweencurrent (i.e., the author) or other.
Nevertheless, no system has yet been built based onthese data.Factuality distinctions in the different systems.
Determining the factuality value has gen-erally been approached as a classification problem, but there is no agreement in theliterature on what the classes should be.
In assuming a three-fold distinction of valuesalong the certainty axis (certain, probable, possible), our model takes a middle pathbetween other proposals in the NLP literature that only differentiate between certainand uncertain (e.g., Medlock and Briscoe [2007] and its subsequent work, or Diab et al[2009b]) and approaches that distinguish among four (e.g., Henriksson and Velupillai2010) or even five degrees (Rubin 2007, 2010).
As a matter of fact, our linguistic-baseddistinctions are shared with the approach in Wilbur, Rzhetsky, and Shatkay (2006),the GENIA corpus (Kim, Ohta, and Tsujii 2008) and, in particular, that in Nawaz,Thompson, and Ananiadou (2010).7.
Final ConsiderationsKnowing the factuality status of event mentions in discourse is important for anyNLP task involving some degree of text understanding, but its identification presentschallenges at different levels of analysis.
First, we conceive event factuality as a con-tinuum, but a discrete scale appears to be a better approach for its automatic iden-tification.
Second, the way language expresses the factuality of situations is complexbecause it involves multiple contributing and interrelating factors.
And finally, thefactuality of an event is always relative to the author but often involves other sourcesas well.In this article, we put forward a computational model of event factuality with theaim of contributing to a better understanding of this level of speculation in language.The model is based on the grammatical structuring of factuality in languages suchas English, and addresses the three aforementioned challenges.
Specifically, it restsupon a three-fold distinction of the factuality scale, it acknowledges the possibility ofdifferent sources (with potentially contradictory views), and it is strongly groundedon the information provided by linguistic operators (including polarity and modality295Computational Linguistics Volume 38, Number 2particles, predicates of different types, and subordination constructions) together withtheir cross-level interactions.The model has been implemented into De Facto, a tool that takes dependencytrees as input and returns the factuality profiles of events in text.
To the best of ourknowledge, it is the only system capable of identifying event factuality degrees pairedto all the relevant sources for each event.
In order to better assess its results, we built abaseline with SVMs following the state of the art in the area.
We run De Facto on twoversions of the dependency parses: one with the dependency trees originally returnedby the parser, and another where dependency errors in subordination constructions hadbeen manually corrected.
De Facto?s performance increases significantly when run onthe second one, thus proving that event factuality as modeled in our work is linguisti-cally well-grounded.
De Facto is not completely dependent on high-quality linguisticdata, however.
Its performance even when run on the original dependency trees isnotably better than the baseline regarding the classes that are harder to identify, namely,those involving negative polarity or some degree of uncertainty, therefore showing theadequacy of De Facto as a component in a standard NLP pipeline as well.De Facto has been implemented for English, and so the set of linguistic resourcesinforming it are specific to this language.
Porting it to other close languages, however,such as Romance or Germanic ones, is a feasible task.
The conceptual distinctions ofcertainty and polarity are shared across these languages, as well as the main linguisticstructures encoding factuality information and which are handled by De Facto, includ-ing specific lexical types (e.g., reporting, presuppositional, or implicative predicates ofdifferent kinds) and syntactic constructions (different structures of evidentiality suchas hearsay, perception or inference, conditional structures, etc.).
Hence, the porting toother languages would mainly involve a mapping of lexical entries.Furthermore, given that most of these linguistic expressions are not domain-specificbut belong to the general structure of any given language, it seems plausible to believethat the model can be applied to data from other domains, such as biomedicine, withoutthe burden of having to compile large amounts of annotated corpus for every new areaof knowledge.
At most, it would involve enriching the set of hedging markers for eachdomain.
More support is needed, however, in order to confirm this claim.On the other hand, such a highly linguistically based approach has its drawbacks aswell, because it suffers from limitations regarding its linguistic coverage (mainly syntac-tic constructions), and its incapability to deal with ambiguity in natural language.
Theseare problems commonly shared with other work approaching tasks of sub-sententialinterpretation by means of linguistically heavy and resource intensive models.All in all, De Facto can provide valuable information for different NLP tasks.
Forexample, it can be of great help in systems dedicated to identifying facts or trackingrumors on news reports, detecting degrees of uncertainty in medical records, or recog-nizing the different sources involved in reported situations.
Similarly, event factualityinformation can contribute, together with other semantic layers (e.g., dependency rela-tions, semantic role labeling, or event and entity coreference), to the challenging taskof identifying textual entailment relations.
In addition, any machine learning effortstowards event factuality identification can both train over De Facto?s output, as well asbenefit from the lexical types and syntactic features it uses when considering options formachine learning algorithm choice and feature engineering decisions.
In other words,we believe that the linguistically motivated model we propose here can, in additionto provide actual information on natural language text, help us understand the phe-nomenon of event factuality and complement data-driven approaches commonly usedin the field.296Saur??
and Pustejovsky Assessing the Factuality Degree of Events in TextAcknowledgmentsWe are very grateful to CarlosRodr?
?guez-Penagos, Bernat Saur?
?,Jordi Atserias, Guillem Masso?, AndreasKaltenbrunner, Toni Badia, Sabine Bergler,and Marc Verhagen for their valuablecomments and helpful discussions.We also want to thank our anonymousreviewers for helping make this a muchbetter piece of work.
All errors and mistakesare responsibility of the authors.
This workwas supported by an EU Marie Curie grantto R.
Saur?
?, PIRG04-GA-2008-239414.ReferencesACE, 2008.
Automatic Content Extraction.English Annotation Guidelines for Relations.Linguistic Data Consortium, version 6.0?2008.01.07 edition.
Available at http://www.ldc.upenn.edu/Projects/ACE/.Aikhenvald, Alexandra Y.
2004.
Evidentiality.Oxford University Press, Oxford.Asher, Nicholas.
1993.
Reference to AbstractObjects in English.
Kluwer Academic Press,Dordrecht.Bach, Kent and Robert M. Harnish.
1979.Linguistic Communication and Speech Acts.The MIT Press, Cambridge, MA.Biber, Douglas and Edward Finegan.
1989.Styles of stance in English: Lexical andgrammatical marking of evidentialityand affect.
Text, 9(1):93?124.Chafe, Wallace.
1986.
Evidentiality in Englishconversation and academic writing.
In W.Chafe and J. Nichols, editors, Evidentiality:The Linguistic Coding of Epistemology.
AblexPublishing Corporation, Norwood, NJ.Dalianis, Hercules and Maria Skeppstedt.2010.
Creating and evaluating a consensusfor negated and speculative words in aSwedish clinical corpus.
In Proceedings ofthe Workshop on Negation and Speculation inNatural Language Processing, pages 5?13,Uppsala, Sweden.de Haan, Ferdinand.
1997.
The Interaction ofModality and Negation: a Typological Study.Garland, New York.de Marneffe, Marie-Catherine, BillMacCartney, and Christopher D. Manning.2006.
Generating typed dependency parsesfrom phrase structure parses.
In Proceedingsof LREC 2006, pages 449?454, Genoa, Italy.Diab, Mona, Bonnie Dorr, Lori Levin, TerukoMitamura, Rebecca Passonneau, OwenRambow, and Lance Ramshaw.
2009a.Language Understanding Annotation Corpus.Linguistic Data Consortium, Philadelphia,PA.
LDC2009T10.Diab, Mona T., Lori Levin, TerukoMitamura, Owen Rambow, VinodkumarPrabhakaran, and Weiwei Guo.
2009b.Committed belief annotation and tagging.In Proceedings of the Third LinguisticAnnotation Workshop, ACL-IJNLP?09,pages 68?73, Suntec, Singapore.Dor, Daniel.
1995.
Representations,Attitudes and Factivity Evaluations.
AnEpistemically-based Analysis of LexicalSelection.
Ph.D. thesis, Stanford University.Farkas, Richa?rd, Veronika Vincze, Gyo?rgyMo?ra, Ja?nos Csirik, and Gyo?rgy Szarvas.2010.
The CoNLL-2010 shared task:Learning to detect hedges and their scopein natural language text.
In Proceedings ofthe 14th CoNLL Conference ?
Shared Task,pages 1?12, Uppsala, Sweden.Geurts, Bart.
1998.
Presuppositions andanaphors in attitude contexts.
Linguisticsand Philosophy, 21:545?601.Givo?n, Talmy.
1993.
English Grammar.A Function-Based Introduction.John Benjamins, Amsterdam.Glanzberg, Michael.
2003.
Felicity andpresupposition triggers.
In Universityof Michigan Workshop in Philosophy andLinguistics, Michigan.Halliday, M. A. K. and Christian M. I. M.Matthiessen.
2004.
An Introduction toFunctional Grammar.
Hodder Arnold,London.Henriksson, Aron and Sumithra Velupillai.2010.
Levels of certainty in knowledge-intensive corpora: An initial annotationstudy.
In Proceedings of the Workshop onNegation and Speculation in NaturalLanguage Processing, pages 41?45,Uppsala, Sweden.Hickl, Andrew and Jeremy Bensley.2007.
A discourse commitment-basedframework for recognizing textualentailment.
In Proceedings of the Workshopon Textual Entailment and Paraphrasing,pages 171?176, Prague.Hooper, Joan B.
1975.
On assertivepredicates.
In J. Kimball, editor, Syntax andSemantics, IV.
Academic Press, New York,pages 91?124.Horn, Laurence R. 1972.
On the SemanticProperties of Logical Operators in English.Ph.D.
thesis, UCLA.
Distributed by theIndiana University Linguistics Club, 1976.Horn, Laurence R. 1989.
A Natural History ofNegation.
University of Chicago Press,Chicago, IL.Hyland, Ken.
1996.
Writing withoutconviction?
Hedging in science researcharticles.
Applied Linguistics, 14(4):433?454.297Computational Linguistics Volume 38, Number 2Karttunen, Lauri.
1970.
Implicative verbs.Language, 47:340?358.Karttunen, Lauri and Annie Zaenen.
2005.Veridicity.
In G. Katz, J. Pustejovsky,and F. Schilder, editors, Dagstuhl SeminarProceedings, Schloss Dagstuhl, link:http://drops.dagstuhl.de/opus/volltexte/2005/314/pdf/05151.KarttunenLauri.Paper.314.pdf.Kiefer, Ferenc.
1987.
On defining modality.Folia Linguistica, XXI:67?94.Kilicoglu, Halil and Sabine Bergler.
2008.Recognizing speculative languagein biomedical research articles: Alinguistically motivated perspective.BMC Bioinformatics, 9(Suppl 11):S10.Kim, Jin-Dong, Tomoko Ohta, SampoPyysalo, Yoshinobu Kano, and Jun?ichiTsujii.
2009.
Overview of BioNLP?09shared task on event extraction.
InProceedings of the Workshop on CurrentTrends in Biomedical Natural LanguageProcessing: Shared Task, pages 1?9.Boulder, Colorado, USA.Kim, Jin-Dong, Tomoko Ohta, and Jun?ichiTsujii.
2008.
Corpus annotation for miningbiomedical events from literature.
BMCBioinformatics, 9(1):10.Kiparsky, Paul and Carol Kiparsky.
1970.Fact.
In M. Bierwisch and K. E. Heidolph,editors, Progress in Linguistics.
A Collectionof Papers.
Mouton, The Hague, Paris,pages 143?173.Kratzer, Angelika.
1991.
Modality.
InA.
van Stechow and D. Wunderlich,editors, Semantik: Ein internationalesHandbuch der zeitgenoessischenForschung.
Walter de Gruyter, Berlin,pages 639?650.Kudo, Taku and Yuji Matsumoto.
2000.Use of support vector learning for chunkidentification.
In Proceedings of CoNLL-2000and LLL-2000, pages 142?144, Lisbon,Portugal.Lakoff, George.
1973.
Hedges: A study inmeaning criteria and the logic of fuzzyconcepts.
Journal of Philosophical Logic,2(4):458?508.Light, Marc, Xin Ying Qiu, and PadminiSrinivasan.
2004.
The language ofbioscience: Facts, speculations, andstatements in between.
In BioLINK 2004:Linking Biological Literature, Ontologies,and Databases, pages 17?24, Boston,Massachusetts, USA.Lyons, John.
1977.
Semantics.
CambridgeUniversity Press, Cambridge.Martin, James R. and Peter R. R. White.
2005.Language of Evaluation: Appraisal in English.London and New York: PalgraveMacmillan.Medlock, Ben and Ted Briscoe.
2007.Weakly supervised learning for hedgeclassification in scientific literature.In Proceedings of the 45th ACL,pages 992?999, Prague, Czech Republic.Moilanen, Karo and Stephen Pulman.
2007.Sentiment composition.
In Proceedings ofthe RANLP, pages 27?29, Borovets,Bulgaria.Moilanen, Karo, Stephen Pulman, and YueZhang.
2010.
Packed feelings and orderedsentiments: Sentiment parsing withquasi-compositional polarity sequencingand compression.
In Proceedings of the 1stWorkshop on Computational Approaches toSubjectivity and Sentiment Analysis (WASSA2010), pages 36?43, Alacant, Spain.Morante, Roser and Walter Daelemans.2009a.
Learning the scope of hedge cuesin biomedical texts.
In Proceedings of theWorkshop on Current Trends in BiomedicalNatural Language Processing, pages 28?36,Boulder, Colorado, USA.Morante, Roser and Walter Daelemans.2009b.
A metalearning approach toprocessing the scope of negation.
InProceedings of the Thirteenth Conference onComputational Natural Language Learning,pages 21?29, Boulder, Colorado, USA.Mushin, Ilana.
2001.
Evidentiality andEpistemological Stance.
John Benjamin,Philadelphia, PA.Nairn, Rowan, Cleo Condoravdi, andLauri Karttunen.
2006.
Computingrelative polarity for textual inference.In Inference in Computational Semantics,ICoS-5, pages 67?76, Buxton, England.Nawaz, Raheel, Paul Thompson, andSophia Ananiadou.
2010.
Evaluating ameta-knowledge annotation scheme forbio-events.
In Proceedings of the Workshopon Negation and Speculation in NaturalLanguage Processing, pages 69?77,Uppsala, Sweden.Neviarouskaya, Alena, HelmutPrendinger, and Mitsuru Ishizuka.
2009.Compositionality principle in recognitionof fine-grained emotions from text.In Proceedings of the 3rd InternationalICWSM Conference, pages 278?281,San Jose, California, USA.Ohta, Tomoko, Jin-Dong Kim, andJun?ichi Tsuji.
2007.
Guidelines forevent annotation.
University of Tokyo,link: http://www-tsujii.is.s.u-tokyo.ac.jp/?genia/release/Genia event annotation guidelines.pdf.298Saur??
and Pustejovsky Assessing the Factuality Degree of Events in TextO?zgu?r, Arzucan and Dragomir Radev.
2009.Detecting speculations and their scopes inscientific text.
In Proceedings of the 2009EMNLP Conference, pages 1398?1407,Suntec, Singapore.Palmer, Frank R. 1986.Mood and Modality.Cambridge University Press, Cambridge.Polanyi, Livia and Annie Zaenen.
2006.Contextual valence shifters.
In W. B. Croft,J.
Shanahan, Y. Qu, and J. Wiebe, editors,Computing Attitude and Affect in Text:Theory and Applications, volume 20.Springer-Verlag, New York, pages 1?10.Prabhakaran, Vinodkumar, Owen Rambow,and Mona Diab.
2010.
Automaticcommitted belief tagging.
In CoLing2010.
Poster Volume, pages 1014?1022,Beijing, China.Prasad, Rashmi, Nikhil Dinesh, Alan Lee,Aravind Joshi, and Bonnie Webber.
2007.Attribution and its annotation in thePenn Discourse Treebank.
TraitementAutomatique des Langues, 47(2):43?64.Pustejovsky, James, Bob Knippen, JessicaLittman, and Roser Saur??.
2005.
Temporaland event information in natural languagetext.
Language Resources and Evaluation,39(2):123?164.Pustejovsky, James, Marc Verhagen, RoserSaur?
?, Jessica Littman, Robert Gaizauskas,Graham Katz, Inderjeet Mani, RobertKnippen, and Andrea Setzer.
2006.TimeBank 1.2.
Linguistic Data Consortium,Philadelphia, PA. LDC2006T08.Rizomilioti, Vassiliki.
2006.
Exploringepistemic modality in academic discourseusing corpora.
In E. Arno?
Macia`, A. SolerCervera, and C. Rueda Ramos, editors,Information Technology in Languages forSpecific Purposes, volume 7.
Springer,Berlin, pages 53?71.Rubin, Victoria L. 2006.
Identifying Certaintyin Texts.
Ph.D. thesis, Syracuse University.Rubin, Victoria L. 2007.
Stating withcertainty or stating with doubt: Intercoderreliability results for manual annotationof epistemically modalized statements.In Proceedings of the NAACL-HLT 2007,pages 141?144, Rochester, NY.Rubin, Victoria L. 2010.
Epistemicmodality: From uncertainty to certaintyin the context of information seekingas interactions with texts.
InformationProcessing and Management, 46:533?540.Saur?
?, Roser.
2008.
A Factuality Profiler forEventualities in Text.
Ph.D. thesis,Brandeis University.Saur?
?, Roser and James Pustejovsky.
2007.Determining modality and factuality fortext entailment.
In Proceedings of the FirstIEEE International Conference on SemanticComputing, pages 509?516, Irvine,California, USA.Saur?
?, Roser and James Pustejovsky.
2009a.FactBank 1.0.
Linguistic Data Consortium,Philadelphia, PA.
LDC2009T23.Saur?
?, Roser and James Pustejovsky.
2009b.FactBank.
A corpus annotated with eventfactuality.
Language Resources andEvaluation, 43:227?268.Saur?
?, Roser, Marc Verhagen, and JamesPustejovsky.
2006.
SlinkET: A partialmodal parser for events.
In Proceedingsof LREC 2006, pages 1332?1337,Genoa, Italy.Shatkay, Hagit, Fengxia Pang, AndreyRzhetsky, and W. John Wilbur.
2008.Multi-dimensional classification ofbiomedical text: Toward automated,practical provision of high-utility textto diverse users.
Bioinformatics,24:2086?2093.Szarvas, Gyorgy.
2008.
Hedge classificationin biomedical texts with a weaklysupervised selection of keywords.In ACL 08: HLT, pages 281?289,Columbus, Ohio, USA.van Valin, Robert D. and Randy J. LaPolla.1997.
Syntax.
Structure, Meaning andFunction.
Cambridge University Press,Cambridge.Velldal, Erik, Lilja Ovrelid, and StephanOepen.
2010.
Resolving speculation:Maxent cue classification anddependency-based scope rules.In Proceedings of the 14th CoNLL:Shared Task, pages 48?55, Uppsala,Sweden.Vincze, Veronika, Gyo?rgy Szarvas, Richa?rdFarkas, Gyo?rgy Mo?ra, and Ja?nos Csirik.2008.
The BioScope corpus: Biomedicaltexts annotated for uncertainty, negationand their scopes.
BMC Bioinformatics,9(Suppl 11):S9.Wiebe, Janyce, Theresa Wilson, and ClaireCardie.
2005.
Annotating expressions ofopinions and emotions in language.Language Resources and Evaluation,39(2):165?210.Wilbur, W. John, Andrey Rzhetsky, andHagit Shatkay.
2006.
New directions inbiomedical text annotation: definitions,guidelines and corpus construction.BMC Bioinformatics, 7(1):356?365.299
