Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 85?90,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsMultilingual Semantic Role Labelling with Markov LogicIvan Meza-Ruiz?
Sebastian Riedel??
?School of Informatics, University of Edinburgh, UK?Department of Computer Science, University of Tokyo, Japan?Database Center for Life Science, Research Organization of Information and System, Japan?I.V.Meza-Ruiz@sms.ed.ac.uk ?
sebastian.riedel@gmail.comAbstractThis paper presents our system for the CoNLL2009 Shared Task on Syntactic and SemanticDependencies in Multiple Languages (Hajic?et al, 2009).
In this work we focus only on theSemantic Role Labelling (SRL) task.
We useMarkov Logic to define a joint SRL model andachieve the third best average performance inthe closed Track for SRLOnly systems and thesixth including for both SRLOnly and Jointsystems.1 Markov LogicMarkov Logic (ML, Richardson and Domingos,2006) is a Statistical Relational Learning languagebased on First Order Logic and Markov Networks.It can be seen as a formalism that extends First Or-der Logic to allow formulae that can be violated withsome penalty.
From an alternative point of view, it isan expressive template language that uses First Or-der Logic formulae to instantiate Markov Networksof repetitive structure.In the ML framework, we model the SRL taskby first introducing a set of logical predicates1 suchas word(Token,Ortho) or role(Token,Token,Role).
Inthe case of word/2 the predicate represents a wordof a sentence, the type Token identifies the positionof the word and the type Ortho its orthography.
Inthe case of role/3, the predicate represents a seman-tic role.
The first token identifies the position of thepredicate, the second the syntactic head of the argu-ment and finally the type Role signals the semanticrole label.
We will refer to predicates such as word/21In the cases were is not obvious whether we refer to SRLor ML predicates we add the prefix SRL or ML, respectively.as observed because they are known in advance.
Incontrast, role/3 is hidden because we need to infer itat test time.With the ML predicates we specify a set ofweighted first order formulae that define a distribu-tion over sets of ground atoms of these predicates (orso-called possible worlds).
A set of weighted formu-lae is called a Markov Logic Network (MLN).
For-mally speaking, an MLN M is a set of pairs (?,w)where ?
is a first order formula and w a real weight.M assigns the probabilityp (y) = 1Z exp??
?
(?,w)?Mw?c?C?f?c (y)??
(1)to the possible world y.
Here C?
is the set of allpossible bindings of the free variables in ?
with theconstants of our domain.
f?c is a feature functionthat returns 1 if in the possible world y the groundformula we get by replacing the free variables in ?by the constants in c is true and 0 otherwise.
Zis a normalisation constant.
Note that this distri-bution corresponds to a Markov Network (the so-called Ground Markov Network) where nodes repre-sent ground atoms and factors represent ground for-mulae.In this work we use 1-best MIRA (Crammer andSinger, 2003) Online Learning in order to train theweights of an MLN.
To find the SRL assignmentwith maximal a posteriori probability according toan MLN and observed sentence, we use CuttingPlane Inference (CPI, Riedel, 2008) with ILP basesolver.
This method is used during both test timeand the MIRA online learning process.852 ModelIn order to model the SRL task in the ML frame-work, we propose four hidden predicates.
Considerthe example of the previous section:argument/1 indicates the phrase for which its headis a specific position is an SRL argument.In our example argument(2) signals that thephrase for which the word in position 2 is itshead is an argument (i.e., Ms. Haag).hasRole/2 relates a SRL predicate to a SRL argu-ment.
For example, hasRole(3,2) relates thepredicate in position 3 (i.e., play) to the phrasewhich head is in position 2 (i.e., Ms. Haag).role/3 identifies the role for a predicate-argumentpair.
For example, role(3,2,ARG0) denotes therole ARG0 for the SRL predicate in the posi-tion 2 and the SRL argument in position 3.sense/2 denotes the sense of a predicate at a specificposition.
For example, sense(3,02) signals thatthe predicate in position 3 has the sense 02.We also define three sets of observable predicates.The first set represents information about each tokenas provided in the shared task corpora for the closedtrack: word for the word form (e.g.
word(3,plays));plemma/2 for the lemma; ppos/2 for the POS tag;feat/3 for each feature-value pair; dependency/3 forthe head dependency and relation; predicate/1 fortokens that are predicates according to the ?FILL-PRED?
column.
We will refer to these predicates asthe token predicates.The second set extends the information providedin the closed track corpus: cpos/2 is a coarse POStag (first letter of actual POS tag); possibleArg/1 istrue if the POS tag the token is a potential SRL argu-ment POS tag (e.g., PUNC is not); voice/2 denotesthe voice for verbal tokens based on heuristics thatuse syntactic information, or based on features in theFEAT column of the data.
We will refer to thesepredicates as the extended predicates.Finally, the third set represents dependency infor-mation inspired by the features proposed by Xue andPalmer (2004).
There are two types of predicatesin this set: paths and frames.
Paths capture the de-pendency path between two tokens, and frames thesubcategorisation frame for a token or a pair of to-kens.
There are directed and undirected versions ofpaths, and labelled (with dependency relations) andunlabelled versions of paths and frames.
Finally, wehave a frame predicate with the distance from thepredicate to its head.
We will refer to the paths andmost of the frames predicates as the path predicates,while we will consider the frame predicates for aunique token part token predicates.The ML predicates here presented are used withinthe formulae of our MLN.
We distinguish betweentwo types of formula: local and global.2.1 Local formulaeA formula is local if its groundings relate any num-ber of observed ground atoms to exactly one hiddenground atom.
For example, a grounding of the localformulalemma(p,+l1)?lemma(a,+l2) ?
hasRole(p, a)connects a hidden hasRole/2 ground atom to two ob-served plemma/2 ground atoms.
This formula can beinterpreted as the feature for the predicate and argu-ment lemmas in the argument identification stage ofa pipeline SRL system.
Note that the ?+?
prefix indi-cates that there is a different weight for each possiblepair of lemmas (l1, l2).We divide our local formulae into four sets, onefor each hidden predicate.
For instance, the set forargument/1 only contains formulae in which the hid-den predicate is argument/1.The sets for argument/1 and sense/2 predicateshave similar formulae since each predicate only in-volves one token at time: the SRL argument or theSRL predicate token.
The formulae in these sets aredefined using only token or extended observed pred-icates.There are two differences between the argument/1and sense/2 formulae.
First, the argument/1 for-mulae use the possibleArg/1 predicate as precondi-tion, while the sense formulae are conditioned onthe predicate/1 predicate.
For instance, consider theargument/1 formula based on word forms:word(a,+w) ?
possibleArg(a) ?
argument(a),and the equivalent version for the sense/2 predicate:word(p,+w) ?
predicate(p) ?
sense(p,+s).This means we only apply the argument/1 formulaeif the token is a potential SRL argument, and thesense/2 formulae if the token is a SRL predicate.86The second difference is the fact that for thesense/2 formulae we have different weights for eachpossible sense (as indicated by the +s term in thesecond formula above), while for the argument/1formulae this is not the case.
This follows naturallyfrom the fact that argument/1 do not explicitly con-sider senses.Table 1 presents templates for the local formualeof argument/1 and sense/2.
Templates allow us tocompactly describe the FOL clauses of a ML.
Thetemplate column shows the body of a clause.
Thelast two columns of the table indicate if there is aclause with the given body and argument(i) (I) orsense(i,+s) (S) head, respectively.
For example,consider the first row: since the last two columnsof the row are marked, this template expands intotwo formulae: word(i,+w) ?
argument(i) andword(i,+w) ?
sense(i,+s).
Including the pre-conditions for each hidden predicate we obtain thefollowing formulae:possibleArg(i) ?
word(i,+w) ?
argument(i)andpredicate(i) ?
word(i,+w) ?
sense(i,+s).In the case of the template marked with a ?
*?sign, the parameters P and I, where P ?
{ppos, plemma} and I ?
{?2,?1, 0, 1, 2}, have tobe replaced by any combination of possible values.Since we generate argument and sense formulaefor this template, the row corresponds to 20 formu-lae in total.Table 2 shows the local formuale for hasRole/2and role/3 predicates, for these formulae we use to-ken, extended and path predicates.
In this case,these templates have as precondition the formulapredicate(p) ?
possibleArg(a).
This ensures thatthe formulae are only applied for SRL predicatesand potential SRL arguments.
In the table we in-clude the values to replace the template parame-ters with.
Some of these formulae capture a no-tion of distance between SRL predicate and SRLargument and are implicitely conjoined with adistance(p, a,+d) atom.
If a formulae exists bothwith and without distance atom, we write Both inthe ?Dist?
column; if it only exists with the distanceatom, we write Only, otherwise No.Note that Tables 1 and 2 do not mentionthe feature information provided in the cor-Template I Sword(i,+w) X XP(i+ I,+v)* X Xcpos(i+ 1,+c1) ?
cpos(i?
1,+c2) X Xcpos(i+ 1,+c1) ?
cpos(i?
1,+c2) ?cpos(i+ 2,+c3) ?
cpos(i?
2,+c4)X Xdep(i, ,+d) X Xdep( , i,+d) X Xppos(i,+o) ?
dep(i, j,+d) X Xppos(i,+o1) ?
ppos(j,+o2) ?dep(i, j,+d)X Xppos(j,+o1) ?
ppos(k,+o2) ?dep(j, k, ) ?
dep(k, i,+d)X Xplemma(i,+l) ?
dep(j, i,+d) X Xframe(i,+f) X X(Empty Body) XTable 1: Templates of the local formulae for argument/1and sense/2.
I: head of clause is argument(i), S: head ofclause is sense(i,+s)pora because this information was not avail-able for every language.
We therefore groupthe formulae which consider the feature/3 pred-icate into another a set we call feature formu-lae.
This is the summary of these formulae:feat(p,+f,+v) ?
sense(p,+s)feat(p,+f,+v) ?
argument(a)feat(p,+f,+v1) ?
feat(p, f,+v2) ?hasRole(p, a)feat(p,+f,+v1) ?
feat(p, f,+v2) ?role(p, a,+r)Additionally, we define a set of language spe-cific formulae.
They are aimed to capture the re-lations between argument and its siblings for thehasRole/2 and role/3 predicates.
In practice inturned out that these formulae were only beneficialfor the Japanese language.
This is a summary ofsuch formulae which we called argument siblings:dep(a, h, ) ?
dep(h, c, ) ?
ppos(a,+p1)?ppos(c,+p2) ?
hasRole(p, a)dep(a, h, ) ?
dep(h, c, ) ?
ppos(a,+p1)?ppos(c,+p2) ?
role(p, a,+r)dep(a, h, ) ?
dep(h, c, ) ?
plemma(a,+p1)?ppos(c,+p2) ?
hasRole(p, a)dep(a, h, ) ?
dep(h, c, ) ?
plemma(a,+p1)?ppos(c,+p2) ?
role(p, a,+r)With these sets of formulae we can build specificMLNs for each language in the shared task.
Wegroup the formulae into the modules: argument/1,87Template Parameters Dist.
H RP(p,+v) P ?
S1 Both X Xplemma(p,+l) ?
ppos(a,+o) No Xppos(p,+o) ?
plemma(a,+l) No Xplemma(p,+l1) ?
plemma(a,+l2) Only X Xppos(p,+o1) ?
ppos(a,+o2) Only Xppos(p,+o1) ?
ppos(a+ I,+o2) I ?
{?1, 0, 1} Only Xplemma(p,+l) Only Xvoice(p,+e) ?
lemma(a,+l) Only Xcpos(p,+c1) ?
cpos(p+ I,+c2) ?
cpos(a,+c3) ?
cpos(a+ J, c4) I,J ?
{?1, 1}2 No X Xppos(p,+v1) ?
ppos(a, IN) ?
dep(a,m, ) ?P(m,+v2) P ?
S1 No X Xplemma(p,+v1) ?
ppos(a, IN) ?
dep(a,m, ) ?
ppos(m,+v2) No X XP(p, a,+v) P ?
S2 No X XP(p, a,+v) ?
plemma(p,+l) P ?
S3 No X XP(p, a,+v) ?
plemma(p,+l1) ?
plemma(a,+l2) P ?
S4 No X XpathFrame(p, a,+t) ?
plemma(p,+l) ?
voice(p,+e) No X XpathFrameDist(p, a,+t) Only X XpathFrameDist(p, a,+t) ?
voice(p,+e) Only X XpathFrameDist(p, a,+t) ?
plemma(p,+l) Only X XP(p, a,+v) ?
plemma(a,+l) P ?
S5 Only X XP(p, a,+v) ?
ppos(p,+o) P ?
S5 Only X XpathFrameDist(p, a,+t) ?
ppos(p,+o1) ?
ppos(a,+o2) Only X Xpath(p, a,+t) ?
plemma(p,+l) ?
cpos(a,+c) Only X Xdep( , a,+d) Only X Xdep( , a,+) ?
voice(p,+e) Only X Xdep( , a,+d1) ?
dep( , p,+d2) Only X X(EmptyBody) No X XTable 2: Templates of the local formulae for hasRole/2 and role/3.
H: head of clause is hasRole(p, a), R:head of clause is role(p, a,+r) and S1 = {ppos, plemma}, S2 = {frame, unlabelFrame, path}, S3 ={frame, pathFrame}, S4 = {frame, pathFrame, path}, S5 = {pathFrameDist, path}hasRole/2, role/3, sense/3, feature and argument sib-lings.
Table 3 shows the different configurations ofsuch modules that we used for the individual lan-guages.
We omit to mention the argument/1, has-Role/2 and role/3 modules because they are presentfor all languages.A more detailed description of the formulae canbe found in our MLN model files.2 They can beused both as a reference and as input to our MarkovLogic Engine,3 and thus allow the reader to easilyreproduce our results.2.2 Global formulaeGlobal formulae relate several hidden ground atoms.We use them for two purposes: to ensure consis-2http://thebeast.googlecode.com/svn/mlns/conll093http://thebeast.googlecode.comSet Feature sense/2 ArgumentsiblingsCatalan Yes Yes NoChinese No Yes NoCzech Yes No NoEnglish No Yes NoGerman Yes Yes NoJapanese Yes No YesSpanish Yes Yes NoTable 3: Different configuration of the modules for theformulae of the languages.88tency between the decisions of all SRL stages andto capture some of our intuition about the task.
Wewill refer to formulae that serve the first purposeas structural constraints.
For example, a structuralconstraint is given by the (deterministic) formularole(p, a, r) ?
hasRole(p, a)which ensures that, whenever the argument a isgiven a label r with respect to the predicate p, thisargument must be an argument of a as denoted byhasRole(p,a).The global formulae that capture our intuitionabout the task itself can be further divided into twoclasses.
The first one uses deterministic or hard con-straints such asrole(p, a, r1) ?
r1 6= r2 ?
?role(p, a, r2)which forbids cases where distinct arguments of apredicate have the same role unless the role de-scribes a modifier.The second class of global formulae is soft or non-deterministic.
For instance, the formulalemma(p,+l) ?
ppos(a,+p)?hasRole(p, a) ?
sense(p,+f)is a soft global formula.
It captures the observationthat the sense of a verb or noun depends on the typeof its arguments.
Here the type of an argument tokenis represented by its POS tag.Table 4 presents the global formulae used in thismodel.3 ResultsFor our experiments we use the corpora provided inthe SRLonly track of the shared task.
Our MLNis tested on the following languages: Catalan andSpanish (Taule?
et al, 2008) , Chinese (Palmer andXue, 2009), Czech (Hajic?
et al, 2006),4 English(Surdeanu et al, 2008), German (Burchardt et al,2006), Japanese (Kawahara et al, 2002).Table 5 presents the F1-scores and training/testtimes for the development and in-domain corpora.Clearly, our model does better for English.
This is4For training we use only sentences shorter than 40 words inthis corpus.Structural constraintshasRole(p, a) ?
argument(a)role(p, a, r) ?
hasRole(p, a)argument(a) ?
?p.hasRole(p, a)hasRole(p, a) ?
?r.role(p, a, r)Hard constraintsrole(p, a, r1) ?
r1 6= r2 ?
?role(p, a, r2)sense(p, s1) ?
s1 6= s2 ?
?sense(p, r2)role (p, a1, r) ?
?mod (r) ?
a1 6= a2 ?
?role (p, a2, r)Soft constraintsrole (p, a1, r) ?
?mod (r) ?
a1 6= a2 ?
?role (p, a2, r)plemma(p,+l)?ppos(a,+p)?hasRole(p, a) ?sense(p,+f)plemma(p,+l)?
role(p, a,+r) ?
sense(p,+f)Table 4: Global formulae for ML modelLanguage Devel Test Train Testtime timeAverage 77.25% 77.46% 11h 29m 23mCatalan 78.10% 78.00% 6h 11m 14mChinese 77.97% 77.73% 36h 30m 34mCzech 75.98% 75.75% 14h 21m 1h 7mEnglish 82.28% 83.34% 12h 26m 16mGerman 72.05% 73.52% 2h 28m 7mJapanese 76.34% 76.00% 2h 17m 4mSpanish 78.03% 77.91% 6h 9m 16mTable 5: F-scores for in-domain in corpora for each lan-guage.in part because the original model was developed forEnglish.To put these results into context: our SRL systemis the third best in the SRLOnly track of the SharedTask, and it is the sixth best on both Joint and SR-LOnly tracks.
For five of the languages the differ-ence to the F1 scores of the best system is 3%.
How-ever, for German it is 6.19% and for Czech 10.76%.One possible explanation for the poor performanceon Czech data will be given below.
Note that in com-parison our system does slightly better in terms ofprecision than in terms of recall (we have the fifthbest average precision and the eighth average recall).Table 6 presents the F1 scores of our system forthe out of domain test corpora.
We observe a similartendency: our system is the sixth best for both Jointand SRLOnly tracks.
We also observe similar largedifferences between our scores and the best scoresfor German and Czech (i.e., > 7.5%), while for En-glish the difference is relatively small (i.e., < 3%).89Language Czech English GermanF-score 77.34% 71.86% 62.37%Table 6: F-scores for out-domain in corpora for each lan-guage.Finally, we evaluated the effect of the argumentsiblings set of formulae introduced for the JapaneseMLN.
Without this set the F-score is 69.52% for theJapanese test set.
Hence argument siblings formulaeimprove performance by more than 6%.We found that the MLN for Czech was the onewith the largest difference in performance whencompared to the best system.
By inspecting ourresults for the development set, we found that forCzech many of the errors were of a rather techni-cal nature.
Our system would usually extract frameIDs (such as ?play.02?)
by concatenating the lemmaof the token and outcome of the sense/2 prediction(for the ?02?
part).
However, in the case of Czechsome frame IDs are not based on the lemma of thetoken, but on an abstract ID in a vocabulary (e.g.,?v-w1757f1?).
In these cases our heuristic failed,leading to poor results for frame ID extraction.4 ConclusionWe presented a Markov Logic Network that per-forms joint multi-lingual Semantic Role Labelling.This network achieves the third best semantic F-scores in the closed track among the SRLOnly sys-tems of the CoNLL-09 Shared Task, and sixth bestsemantic scores among SRLOnly and Joint systemsfor the closed task.We observed that the inclusion of features whichtake into account information about the siblings ofthe argument were beneficial for SRL performanceon the Japanese dataset.
We also noticed that ourpoor performance with Czech are caused by ourframe ID heuristic.
Further work has to be done inorder to overcome this problem.ReferencesAljoscha Burchardt, Katrin Erk, Anette Frank, An-drea Kowalski, Sebastian Pado?, and ManfredPinkal.
The SALSA corpus: a German corpusresource for lexical semantics.
In Proceedings ofLREC-2006, Genoa, Italy, 2006.Koby Crammer and Yoram Singer.
Ultraconserva-tive online algorithms for multiclass problems.Journal of Machine Learning Research, 3:951?991, 2003.
ISSN 1533-7928.Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, PetrSgall, Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka,Marie Mikulova?, and Zdene?k Z?abokrtsky?.
Praguedependency treebank 2.0, 2006.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart??,Llu?
?s Ma`rquez, Adam Meyers, Joakim Nivre, Se-bastian Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mi-ahi Surdeanu, Nianwen Xue, and Yi Zhang.
TheCoNLL-2009 shared task: Syntactic and semanticdependencies in multiple languages.
In Proceed-ings of CoNLL-2009), Boulder, Colorado, USA,2009.Daisuke Kawahara, Sadao Kurohashi, and Ko?itiHasida.
Construction of a Japanese relevance-tagged corpus.
In Proceedings of the LREC-2002,pages 2008?2013, Las Palmas, Canary Islands,2002.Martha Palmer and Nianwen Xue.
Adding semanticroles to the Chinese Treebank.
Natural LanguageEngineering, 15(1):143?172, 2009.Matt Richardson and Pedro Domingos.
Markovlogic networks.
Machine Learning, 62:107?136,2006.Sebastian Riedel.
Improving the accuracy and ef-ficiency of map inference for markov logic.
InUAI ?08: Proceedings of the Annual Conferenceon Uncertainty in AI, 2008.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
The CoNLL-2008 shared task on joint parsing of syntacticand semantic dependencies.
In Proceedings ofCoNLL-2008, 2008.Mariona Taule?, Maria Anto`nia Mart?
?, and MartaRecasens.
AnCora: Multilevel Annotated Cor-pora for Catalan and Spanish.
In Proceedings ofLREC-2008, Marrakesh, Morroco, 2008.Nianwen Xue and Martha Palmer.
Calibrating fea-tures for semantic role labeling.
In EMNLP ?04:Proceedings of the Annual Conference on Em-pirical Methods in Natural Language Processing,2004.90
