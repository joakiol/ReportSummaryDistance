Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1897?1907, Dublin, Ireland, August 23-29 2014.A Neural Reordering Model for Phrase-based TranslationPeng Li?Yang Liu?Maosong Sun?Tatsuya Izuha?Dakun Zhang?
?State Key Laboratory of Intelligent Technology and SystemsTsinghua National Laboratory for Information Science and TechnologyDepartment of Computer Sci.
and Tech., Tsinghua University, Beijing, Chinapengli09@gmail.com, {liuyang2011,sms}@tsinghua.edu.cn?Toshiba Corporation Corporate Research & Development Centertatsuya.izuha@toshiba.co.jp?Toshiba (China) R&D Centerzhangdakun@toshiba.com.cnAbstractWhile lexicalized reordering models have been widely used in phrase-based translation systems,they suffer from three drawbacks: context insensitivity, ambiguity, and sparsity.
We propose aneural reordering model that conditions reordering probabilities on the words of both the currentand previous phrase pairs.
Including the words of previous phrase pairs significantly improvescontext sensitivity and reduces reordering ambiguity.
To alleviate the data sparsity problem, webuild one classifier for all phrase pairs, which are represented as continuous space vectors.
Ex-periments on the NIST Chinese-English datasets show that our neural reordering model achievessignificant improvements over state-of-the-art lexicalized reordering models.1 IntroductionReordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004).While local reordering can be directly memorized in phrases, modeling reordering at a phrase level stillremains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete(Knight, 1999; Zaslavskiy et al., 2009).The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al.,2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn etal., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012;Cherry, 2013), just to name a few).
Among them, lexicalized reordering models (Tillman, 2004; Koehnet al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems.
Un-like the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacementsin terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabil-ities conditioned on the words of each phrase pair.
They often distinguish between three orientationswith respect to the previous phrase pair: monotone, swap, and discontinuous.
As lexicalized reorderingmodels capture the phenomenon that some words are far more likely to be displaced than others, theyoutperform unlexicalized reordering models substantially.Despite their apparent success in statistical machine translation, lexicalized reordering models sufferfrom the following three drawbacks:1.
Context insensitivity.
Lexicalized reordering models determine the orientations only depending onthe words of current phrase pairs.
In fact, a phrase pair usually has different orientations in differentcontexts.
It is important to include more contexts to improve the expressive power of reorderingmodels.2.
Ambiguity.
Short phrase pairs, which are observed in the training data more frequently, usually havemultiple orientations.
We observe that about 92.4% of one-word Chinese-English phrase pairs areambiguous.
This makes it hard to decide which orientation should be properly used in decoding.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1897Figure 1: Ambiguity in phrase reordering.
The phrase pair ?
?yingyun?, ?business??
is labeled withdifferent orientations in different contexts: (a) monotone, (b) swap, and (c) discontinuous.
Lexicalizedreordering models use fixed probability distributions (e.g., 17.50% for M, 1.59% for S, and 80.92% forD) in decoding even though the surrounding contexts keep changing.3.
Sparsity.
Lexicalized reordering models maintain a reordering probability distribution for eachphrase pair.
As most long phrase pairs that are capable of memorizing local word selection andreordering only occur once in the training data, maximum likelihood estimation can hardly train themodels accurately.In this work, we propose a neural reordering model for phrase-based translation.
The contribution istwofold.
Firstly, unlike conventional lexicalized reordering models, the neural reordering model condi-tions reordering probabilities on the words of both the current and previous phrase pairs.
Including thewords of previous phrase pairs significantly improves context sensitivity and reduces reordering ambi-guity.
Secondly, to alleviate the data sparsity problem, we build a neural classifier for all phrase pairs,which are represented as continuous space vectors.
Experiments on the NIST Chinese-English datasetsshow that our neural reordering model achieves significant improvements over state-of-the-art lexicalizedmodels.2 Lexicalized Reordering ModelsThe lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) havebecome the de facto standard in modern phrase-based systems.
These models are called lexicalizedbecause they condition reordering probabilities on the words of each phrase pair.
Depending on therelationship between the current and previous phrase pairs, lexicalized reordering models often defineorientations to classify different reordering patterns.More formally, we use f = {?f1, .
.
.
,?fn} to denote a sequence of source phrases, e = {e?1, .
.
.
, e?n}to denote the phrase sequence on the target side, and a = {a1, .
.
.
, an} to denote the alignment be-tween source and target phrases.
A source phrase?faiand a target phrase e?iform a phrase pair.
Lex-icalized reordering models aim to estimate the conditional probability of a sequence of orientationso = {o1, .
.
.
, on}:P (o|f , e,a) =n?i=1P (oi|f , e?1, .
.
.
, e?i, a1, .
.
.
, ai) (1)where each oitakes values over a set of predefined orientations.
For simplicity, current lexicalized1898modelsource phrase length1 2 3 4 5 6 7P (oi|?fai, e?i, ai?1, ai) 92.74 54.01 24.09 14.40 10.78 8.47 6.95P (oi|?fai, e?i,?fai?1, e?ai?1, ai?1, ai) 21.72 5.22 2.63 1.48 0.98 0.67 0.54Table 1: Percentages of phrase pairs that have multiple orientations.
Including previous phrase pairs inmodeling significantly reduces the reordering ambiguity for the M/S/D orientations.
For example, while92.74% of 1-word Chinese-English phrase pairs have multiple orientations observed in the training data,the ratio dramatically drops to 21.72% if the orientations are conditioned on both the current and previousphrase pairs.reordering models use orientations conditioned only on ai?1and ai:P (o|f , e,a) ?n?i=1P (oi|?fai, e?i, ai?1, ai) (2)The most widely used orientations are monotone (M), swap (S), and discontinuous (D):1oi=??
?M if ai?
ai?1= 1S if ai?
ai?1= ?1D if |ai?
ai?1| =?
1(3)As lexicalized reordering models maintain a reordering probability distribution for each phrase pair,it is hard to accurately learn reordering probabilities for long phrase pairs that are usually observed onlyonce in the training data.
On the contrary, short phrase pairs that occur in the training data for many timestend to be ambiguous.
For example, as shown in Figure 1, a Chinese-English phrase pair ??yingyun?,?business??
is observed to have different orientations in different contexts.It is unreasonable to use fixed reordering probability distributions in decoding as the surroundingcontexts keep changing.
Previous study shows that considering more contexts into reordering modelingimproves translation performance (Khalilov and Simaan, 2010).
Therefore, we need a more powerfulmechanism to include more contexts, resolve the reordering ambiguity, and reduce the data sparsity.3 A Neural Reordering Model3.1 The ModelIntuitively, conditioning reordering probabilities on the words of both the current and previous phrasepairs will significantly reduce both reordering ambiguity and context insensitivity.
The new reorderingmodel is given byP (o|f , e,a) ?n?i=1P (oi|?fai, e?i,?fai?1, e?i?1, ai?1, ai) (4)where ?
?fai?1, e?i?1?
is the previous phrase pair.Including the previous phrase pairs improves the context sensitivity.
For example, given a phrase pair?
?yingyun?, ?business?
?, its orientation is more likely to be monotone if it is preceded by a noun phrasepair such as ?
?xinyongka?, ?credit card??.
On the contrary, the probability of the discontinuous orienta-tion is higher if the previous phrase pairs contain verbs such as ?
?gaishan?, ?improve??.
Therefore, thenew model is capable of capturing the phenomenon that the orientation of a phrase pair depends on itssurrounding contexts.Another advantage of including previous phrase pairs is the reduction of reordering ambiguity.
Asshown in Table 1, 92.74% of 1-word Chinese-English phrase pairs have multiple orientations (i.e., M, S,1There are many variants of lexicalized reordering models depending on the model type, orientation, directionality, lan-guage, and collapsing.
See http://www.statmt.org/moses/?n=FactoredTraining.BuildReorderingModel for more details.1899and D) observed in the training data.
The ratio decreases with the increase of phrase length.
In contrast,the new model is much less ambiguous (e.g., the ratio of ambiguous one-word phrase pairs dramaticallydrops to 21.72%) as it is conditioned on both the current and previous phrase pairs.Unfortunately, including more contexts in modeling also increases the data sparsity.
We observe thatabout 90% of reordering examples (i.e., the current and previous phrase pairs) are observed only once inthe training data.
As a result, it is more difficult to train lexicalized reordering models accurately usingmaximum likelihood estimation.To alleviate the data sparsity problem, we use the following two strategies:1.
Reordering as classification.
Instead of maintaining a reordering probability distribution for eachphrase pair, we build a reordering classifier for all phrase pairs (Xiong et al., 2006; Li et al., 2013).This significantly reduces data sparsity by considering all occurrences of extracted phrase pairs astraining examples.
We find that 500, 000 reordering examples suffice to train a robust classifier(Section 4.5).2.
Continuous space representation.
Instead of using a symbolic representation of phrases, we usea continuous space representation that treats a phrase as a dense real-valued vector (Socher et al.,2011b; Li et al., 2013).
Consider two phrases ?in London?
and ?in Centara Grand?.
It is usuallyeasy to predict the orientations of ?in London?
because it might be observed in the training data formany times.
This is not the case for ?in Centara Grand?
as it might occur only once.
However, ifthe two phrases happen to have very similar continuous space representations, ?in Centara Grand?is likely to have a similar reordering probability distribution with ?in London?.To generate vector space representation for phrases, we follow Socher et al.
(2011a) to use recursiveautoencoders.
Given two words w1and w2, suppose their vector space representations are c1and c2.The vector space representation p of the two-word phrase {w1, w2} can be computed using a two-layerneural network:p = g(1)(W(1)[c1; c2] + b(1)) (5)where [c1; c2] ?
R2nis the concatenation of c1and c2, W(1)?
Rn?2nis a weight matrix, b(1)is a biasvector, and g(1)is an element-wise activation function.In order to measure how well p represents c1and c2, they can be reconstructed using another two-layerneural network:[c?1; c?2] = g(2)(W(2)p + b(2)) (6)where c?1?
Rnand c?2?
Rnare reconstructed vectors of c1and c2, W(2)?
R2n?nis a weight matrix,b(2)?
Rnis a bias vector, and g(2)is an element-wise activation function.
The reconstruction error canbe measured by comparing c1and c2with c?1and c?2.
This process runs recursively in a bottom-up styleto obtain the vector space representation of a multi-word phrase (Socher et al., 2011a).
Socher et al.
(2011a) find that minimizing the norms of hidden layers leads to the reduction of reconstruction error inan undesirable way.
Therefore, we normalize p such that ||p||2= 1.Treating phrase reordering as a classification problem, we propose a neural reordering classifier thattakes the current and previous phrase pairs as input.
The neural network consists of four recursiveautoencoders and a softmax layer.
The input of the classifier are the previous phrase pair and the currentphrase pair.
Four recursive autoencoders are used to transform the four phrases (i.e.,?fai, e?i,?fai?1, e?i?1)into vectors.
Then, these vectors are fed to the softmax layer to predict reordering orientations.
Note thatthe recursive autoencoders for the same language share with the same parameters.
Our neural network issimilar to that of Li et al.
(2013).
The major difference is that Li et al.
(2013) need to compute vectorspace representation for variable-sized blocks ranging from words to sentences on the fly both in trainingand decoding.
In contrast, we only need to compute vectors for phrases with up to 7 words in the trainingphase, which makes our approach simpler and more scalable to large data.1900Formally, given the previous phrase pair ?
?fai?1, e?i?1?, the current phrase pair ?
?fi, e?i?
and the orienta-tion oi, the reordering probability is computed asP (oi|?fai, e?i,?fai?1, e?i?1, ai?1, ai) = g(Woc(?fai, e?i,?fai?1, e?i?1) + bo), (7)where Wois a weight matrix, bois a bias vector, c(?fai, e?i,?fai?1, e?i?1) is the concatenation of the vectorsof the four phrases.2Following Och (2003), we use a linear model in our decoder with conventional features (e.g., trans-lation probabilities and n-gram language model).
The neural reordering model is incorporated into thediscriminative framework as an additional feature.3.2 TrainingTraining the neural reordering model involves minimizing the following two kinds of errors:?
Reconstruction error: It measures how well the computed vector space representations representthe input vectors.
It is defined as the average reconstruction error of all the parent nodes in the treesformed during computing the vector space representation for all the phrases in the training data.?
Classification error: It measures how well the resulting classifier predicts the reordering orienta-tions.
It is defined as the average cross-entropy errors of all the training examples.In our experiments, the objective function is a linear interpolation of the reconstruction error and theclassification error.Following Socher et al.
(2011b), we use L-BFGS (Liu and Nocedal, 1989) to optimize the parameters.At the beginning of each iteration, a binary tree for each phrase is constructed using a greedy algorithm(Socher et al., 2011b).3With these trees fixed, the partial derivatives with respect to parameters arecomputed via the backpropagation through structures algorithm (Goller and Kuchler, 1996).When optimizing the parameters of the softmax layer, the training procedure keeps the parameters ofthe recursive autoencoders and word embedding matrices fixed.
The corresponding error function is theclassification error as described above.
We also use L-BFGS to optimize the parameters and the standarderror backpropagation algorithm (Rumelhart et al., 1986) to compute the derivatives.3.3 DecodingAs the vector space representation of a phrase is calculated based on all the words in the phrase, usingthe neural reordering model complicates the conditions for risk-free hypothesis recombination (Koehn etal., 2003).
Therefore, many hypotheses are not likely to be recombined if the neural reordering modelis directly integrated in decoding, making the decoder to only explore in a much smaller search space.4Therefore, we use Moses to generate search graphs and then use hypergraph reranking (Huang andChiang, 2007; Huang, 2008) to find most probable derivations using the neural reordering model.4 Experiments4.1 Data PreparationWe evaluate our reordering model on Chinese-English translation.
The training corpus consists of 1.23Msentence pairs with 32.1M Chinese words and 35.4M English words.
A 4-gram language model wastrained on the Xinhua portion of the English GIGAWORD corpus using KenLM (Heafield, 2011), whichcontains 398.6M words.
We used the NIST 2006 MT Chinese-English dataset as the development set,and NIST 2002-2005, 2008 MT Chinese-English datasets as the test sets.
Case-insensitive BLEU is used2In practice, as suggested by Socher et al.
(2011b), we feed the four average vectors of the vectors present in each recursiveautoencoders to the softmax layer.
Taking ?resident population?
as an example, there are three vectors in the binary tree usedby the corresponding recursive autoencoder, denoted as x?1, x?2and x?3.
The average vector is computed as x?
=13?3i=1x?i.3As phrases in phrase-based translation are not necessarily syntactic constituents, we do not use parse trees in this work.4Experimental results show that we can only achieve comparable performance with Moses by integrating neural reorderingmodel directly in decoding.1901Model Orientation MT06 MT02 MT03 MT04 MT05 MT08distance N/A 29.56 31.40 31.27 31.34 29.98 23.87wordM/S/D 30.19 32.03 31.86 32.09 30.55 24.20left/right 30.17 31.98 31.52 31.98 30.19 24.30phraseM/S/D 30.24 32.35 31.85 32.00 30.78 24.33left/right 29.57 32.64 31.53 31.90 30.70 24.28hierarchicalM/S/D 30.46 32.52 31.89 32.09 30.39 24.11left/right 30.03 32.13 31.59 31.91 30.21 24.41neuralM/S/D 30.68 32.19 31.94 32.20 30.81 24.71left/right 31.03** 33.03** 32.48** 32.52** 31.11* 25.20**Table 2: Comparison of distance-based, lexicalized, and neural reordering models in terms of case-insensitive BLEU-4 scores.
?distance?
denotes the distance-based reordering model (Koehn et al., 2003),?word?
denotes the word-based lexicalized model (Tillman, 2004), ?phrase?
denotes the phrase-basedlexicalized model (Koehn et al., 2007), ?hierarchical?
denotes the hierarchical phrase-based reorderingmodel (Galley and Manning, 2008), and ?neural?
denotes our model.
The ?left?
and ?right?
orientationsonly considers whether the current source phrase is on the left of the previous source phrase or not.
Weuse ?*?
to highlight the result that is significantly better than the best baseline (highlighted in italic)at p < 0.05 level and ?**?
at p < 0.01 level.
The neural model does not work well for the M/S/Dorientations due to the non-separability problem (Section 4.3).as the evaluation metric.
As a trade-off between expressive power and computational cost, we set thedimension of the word embedding vectors to 25.5Both g(1)and g(2)are set to tanh(?).
The otherhyperparameters are optimized via random search (Bergstra and Bengio, 2012).4.2 Comparison of Distance-based, Lexicalized, and Neural Reordering ModelsWe compare three kinds of reordering models with increasing expressive power:1. distance-based model: penalizing phrase displacements proportionally to the amount of nonmono-tonicity (Koehn et al., 2003);2. lexicalized models: conditioning the reordering probabilities on the current phrase pairs.
The ori-entations can be determined with respect to words (Tillman, 2004), phrases (Koehn et al., 2007), orhierarchical phrases (Galley and Manning, 2008);3. neural model: conditioning the reordering probabilities on both the current and previous phrasepairs.For lexicalized and neural models, we further distinguish between two kinds of orientation sets:{monotone, swap, discontinuous} and {left, right}.
The left/right orientations only consider whetherthe current source phrase is on the left of the previous source phrase or not.
Therefore, swap anddiscontinuous-left are merged into left while monotone and discontinuous-right into right.All these reordering models are tested using Moses (Koehn et al., 2007), except that the neural modelneeds an additional hypergraph reranking procedure (Section 3.3).
Implemented using Java, it takes thereranker 0.748 second to rerank a hypergraph on average.Table 2 shows the case-insensitive BLEU-scores of distance-based, lexicalized, and neural reorderingmodels on the NIST Chinese-English datasets.
?distance?
denotes the distance-based reordering model(Koehn et al., 2003), ?word?
denotes the word-based lexicalized model (Tillman, 2004), ?phrase?
denotesthe phrase-based lexicalized model (Koehn et al., 2007), ?hierarchical?
denotes the hierarchical phrase-based reordering model (Galley and Manning, 2008), and ?neural?
denotes our model.5We find that the dimensions of vectors do not have a significant impact on translation performance.
For efficiency, we setthe dimension to 25.1902Figure 2: The non-separability problem for the neural reordering model.
Given an aligned Chinese-English sentence pair, the unaligned Chinese word ?de?
makes a big difference in determining M/S/Dorientations.
In (a), ?de?
is included in the previous source phrase and thus the orientation is monotone.In (b), however, it is not included in the previous source phrase and the orientation is discontinuous.
Inour neural reordering model, ?liu wan de?
and ?liu wan?
have very similar vector space representationsyet different orientations (i.e., M and D).
In other words, training examples labeled with M, S, D areprone to be mixed with each other in the vector space.
Therefore, it is difficult to find a hyperplane toseparate M, S and D examples in the high-dimensional space.We find that lexicalized reordering models obtain significant improvements over the distance-basedmodel, which indicates that conditioning reordering probabilities on the words of the current phrasepairs does improve the expressive power.
Our neural model using left/right orientations significantlyoutperforms all variants of lexicalized models.
We use ?*?
to highlight the result that is significantlybetter than the best baseline (highlighted in italic) at p < 0.05 level and ?**?
at p < 0.01 level.
Thissuggests that conditioning reordering probabilities on the words of current and previous phrase pairs ishelpful for resolving reordering ambiguities and reducing context insensitivity.4.3 The Non-Separability ProblemIn Table 2, the neural model using the M/S/D orientations fails to outperform lexicalized models signifi-cantly.
One possible reason is that the neural model suffers from the non-separability problem due to theM/S/D orientations.As shown in Figure 2, given an aligned Chinese-English sentence pair, the unaligned Chinese functionword ?de?
makes a big difference in determining M/S/D orientations.
In (a), ?de?
is included in theprevious source phrase and thus the orientation is monotone.
In (b), however, ?de?
is not included in theprevious source phrase and the orientation is discontinuous.
In our neural reordering model, ?liu wande?
and ?liu wan?
have very similar vector space representations yet different orientations (i.e., M andD).
In other words, training examples labeled with M, S, D are prone to be mixed with each other inthe vector space.
Therefore, it is difficult to find a hyperplane to separate M, S and D examples in thehigh-dimensional space.Fortunately, we find that using the left/right orientations can alleviate this problem.
As the left/rightorientations only consider whether the current source phrase is on the left of the previous source phraseor not, unaligned source words will not change orientations.
For example, both Figure 2(a) and 2(b) areidentified as the right orientation.As a result, using left/right orientations in the neural reordering model not only has a higher classifi-cation accuracy (85%) over using the M/S/D orientations (69%), but also achieves higher BLEU scoreson all NIST datasets systematically.4.4 The Effect of Distortion LimitFigure 3 shows the performance of the lexicalized model and our neural model with various distortionlimits.
The lexicalized model is the word-based model with M/S/D orientations.
The neural model usesleft/right orientations.
The neural model consistently outperforms the lexicalized model, especially forlarge distortion limits.
This finding suggests that the neural model is superior to lexicalized models inpredicting long-distance reordering.19032 4 6 822232425Distortion LimitBLEUneurallexicalizedFigure 3: BLEU with various distortion limits.# examples Accuracy BLEU100,000 83.55 30.92200,000 84.40 31.03300,000 84.55 31.01400,000 84.95 30.93500,000 85.25 31.273,000,000 85.55 31.03Table 3: Effect of training corpus size.Vectors MT06 MT02 MT03 MT04 MT05 MT08ours 31.03 33.03 32.48 32.52 31.11 25.20word2vec 30.44 32.28 32.00 32.07 30.24 24.54Table 4: Comparison of neural reordering models trained based on word vectors produced by our model(ours) and word2vec (Mikolov et al., 2013).4.5 The Effect of Training Corpus SizeTable 3 shows the classification accuracy and translation performance with various number of randomlysampled reordering examples for training the neural classifier.
The classification accuracy and transla-tion performance generally rise as the number of reordering example increases.6Surprisingly, both theclassification accuracy and translation performance of using 500,000 reordering examples are close tousing 3,000,000 reordering examples, suggesting that a relatively small amount of reordering examplesare enough for training a robust classifier.4.6 Learned Vector Space RepresentationsWe randomly sampled 200,000 English phrases and found 999 clusters according to the vector spacerepresentations computed by recursive autoencoders using the k-means algorithm (MacQueen, 1967).The distance between two phrases is calculated by the Euclidean distance between their vector spacerepresentations.Figure 4 shows 10 of the 999 clusters.
An interesting finding is that phrase pairs that are close in thevector space share with similar reordering patterns rather than semantic similarity.
For example, ?byjune 1?
and ?within the agencies?
have similar distributions on the left/right orientations but are totallyunrelated in terms of meaning.
As a result, the vector representations of words trained using unlabeleddata hardly helps in training the neural reordering model.
Table 4 shows the results when we replacethe word vectors of our model with those trained using word2vec (Mikolov et al., 2013).
The recursiveautoencoders and the classifier are retrained.
The performance of the neural reordering model trained inthis way drops significantly, which confirms our analysis.5 Related WorkReordering as classification is a common way to alleviate the data sparsity problem.
Xiong et al.
(2006)use a maximum entropy model to predict whether to merge two blocks in a straight or an inverted orderin their ITG decoder.
Nguyen et al.
(2009) build a similar model for hierarchical phrase reorderingmodels (Galley and Manning, 2008).
Green et al.
(2010) and Yahyaei and Monz (2010) predict finer-grained distance bins instead.
Another direction is to learn sparse reordering features and create moreflexible distributions (Cherry, 2013).
Although these models are effective, feature engineering is a majorchallenge.
In contrast, our neural reordering model is capable of learning features automatically.6The reason why the BLEU scores oscillate slightly on the training set is that classification accuracy is not directly correlatedwith BLEU scores.
Optimizing the neural reordering model directly with respect to BLEU score may further improve theperformance.
We leave this for future work.1904but is willing toeconomy is required torange of services to said his visit is tois making use ofjune 18, 2001late 2011as detention centergroup all togethertake care of oldby june 1and complete by end 1998or other economicand for otherwithin the agenciesFigure 4: Phrase clusters as calculated by the Euclidean distance in the vector space.
English phrasesthat have similar reordering probability distributions rather than similar semantic similarity fall into onecluster.Along another line, n-gram-based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al.,2013) treat translation as Markov chains over minimal translation units (Mari`no et al., 2006; Durrani etal., 2013) or operations (Durrani et al., 2011) directly.
Although naturally leveraging both the source andtarget side contexts, these approaches still face the data sparsity problem.Our work is closely related to Li et al.
(2013).
The major difference is that Li et al.
(2013) need tocompute vector space representation for variable-sized blocks ranging from words to sentences on thefly both in training and decoding.
In contrast, we only need to compute vectors for phrases with up to 7words in the training phase, which makes our approach simpler and more scalable to large data.6 ConclusionWe have shown that surrounding context is effective for resolving reordering ambiguities in phrase-basedmodels.
As the data sparseness problem is the major challenge for using context in reordering models,we propose to use a single classifier based on recursive autoencoders to predict reordering orientations.Experimental results show that our neural reordering model outperforms the state-of-the-art lexicalizedreordering models significantly and consistently across all the NIST datasets under various settings.There are a few future directions we plan to explore.
First, as the machine translation system and neu-ral classifier are trained separately, the neural network training only has an indirect effect on translationquality.
Jointly training the machine translation system and neural classifier is an interesting topic.
Sec-ond, it is interesting to develop more efficient models to leverage larger contexts to resolve reorderingambiguities.
Third, we plan to extend our work to other translation models such as syntax-based andn-gram based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al., 2013).
Finally, as we castphrase reordering as two-category classification problem (i.e, left vs. right), it is interesting to intersectstructured SVM (Tsochantaridis et al., 2005) with neural networks to develop a large margin trainingalgorithm for our neural reordering model.AcknowledgementsThis research is supported by the 973 Program (No.
2014CB340501), the National Natural ScienceFoundation of China (No.
61331013), the 863 Program (No.
2012AA011102), Toshiba CorporationCorporate Research & Development Center, and the Singapore National Research Foundation under itsInternational Research Centre @ Singapore Funding Initiative and administered by the IDM Programme.1905ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Distortion models for statistical machine translation.
In Pro-ceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, pages 529?536.James Bergstra and Yoshua Bengio.
2012.
Random search for hyper-parameter optimization.
Journal of MachineLearning Research, 13(1):281?305.Arianna Bisazza and Marcello Federico.
2012.
Modified distortion matrices for phrase-based statistical machinetranslation.
In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 478?487.Colin Cherry.
2013.
Improved reordering for phrase-based translation using sparse features.
In Proceedings ofthe 2013 Conference of the North American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies, pages 22?31.Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011.
A joint sequence translation model with integrat-ed reordering.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:Human Language Technologies, pages 1045?1054.Nadir Durrani, Alexander Fraser, Helmut Schmid, Hieu Hoang, and Philipp Koehn.
2013.
Can Markov modelsover minimal translation units help phrase-based SMT?
In Proceedings of the 51st Annual Meeting of theAssociation for Computational Linguistics (Volume 2: Short Papers), pages 399?405.Yang Feng, Haitao Mi, Yang Liu, and Qun Liu.
2010.
An efficient shift-reduce decoding algorithm for phrased-based machine translation.
In Proceedings of the 23rd International Conference on Computational Linguistics:Posters, pages 285?293.Michel Galley and Christopher D. Manning.
2008.
A simple and effective hierarchical phrase reordering model.In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 848?856.Christoph Goller and Andreas Kuchler.
1996.
Learning task-dependent distributed representations by backprop-agation through structure.
In Proceedings of 1996 IEEE International Conference on Neural Networks (Vol-ume:1), volume 1, pages 347?352.Spence Green, Michel Galley, and Christopher D. Manning.
2010.
Improved models of distortion cost for statis-tical machine translation.
In Proceedings of Human Language Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association for Computational Linguistics, pages 867?875.Kenneth Heafield.
2011.
KenLM: faster and smaller language model queries.
In Proceedings of the EMNLP 2011Sixth Workshop on Statistical Machine Translation, pages 187?197.Liang Huang and David Chiang.
2007.
Forest rescoring: Faster decoding with integrated language models.
InProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144?151.Liang Huang.
2008.
Forest reranking: Discriminative parsing with non-local features.
In Proceedings of the46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages586?594.Maxim Khalilov and Khalil Simaan.
2010.
Source reordering using maxent classifiers and supertags.
In Proceed-ings of The 14th Annual Conference of the European Association for Machine Translation, pages 292?299.Kevin Knight.
1999.
Decoding complexity in word-replacement translation models.
Computational Linguistics,25(4):607?615.Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003.
Statistical phrase-based translation.
In Proceedingsof the 2003 Conference of the North American Chapter of the Association for Computational Linguistics onHuman Language Technology-Volume 1, pages 48?54.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, BrookeCowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and EvanHerbst.
2007.
Moses: Open source toolkit for statistical machine translation.
In Proceedings of the 45thAnnual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demoand Poster Sessions, pages 177?180.Peng Li, Yang Liu, and Maosong Sun.
2013.
Recursive autoencoders for ITG-based translation.
In Proceedingsof the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567?577.1906DongC.
Liu and Jorge Nocedal.
1989.
On the limited memory BFGS method for large scale optimization.
Math-ematical Programming, 45(1-3):503?528.James MacQueen.
1967.
Some methods for classification and analysis of multivariate observations.
In Proceed-ings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 281?297.Jos?e B. Mari`no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrik Lambert, Jos?e A. R. Fonollosa, andMarta R. Costa-juss`a.
2006.
N-gram-based machine translation.
Computational Linguistics, 32(4):527?549.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013.
Linguistic regularities in continuous space word rep-resentations.
In Proceedings of the 2013 Conference of the North American Chapter of the Association forComputational Linguistics: Human Language Technologies, pages 746?751.Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen, and Thai Phuong Nguyen.
2009.
Improving a lexicalizedhierarchical reordering model using maximum entropy.
In Proceedings of The twelfth Machine TranslationSummit (MT Summit XII).Franz Josef Och and Hermann Ney.
2004.
The alignment template approach to statistical machine translation.Computational Linguistics, 30(4):417?449.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev.
2004.
A smorgasbordof features for statistical machine translation.
In Proceedings of the Human Language Technology Conferenceof the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004: MainProceedings, pages 161?168.Franz Josef Och.
2003.
Minimum error rate training in statistical machine translation.
In Proceedings of the 41stAnnual Meeting of the Association for Computational Linguistics, pages 160?167.David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
1986.
Learning internal representations byerror propagation.
Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1:Foundations, pages 318?362.Richard Socher, Eric H. Huang, Jeffrey Pennin, Andrew Y. Ng, and Christopher D. Manning.
2011a.
Dynamicpooling and unfolding recursive autoencoders for paraphrase detection.
In Proceedings of Advances in NeuralInformation Processing Systems 24, pages 801?809.Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning.
2011b.
Semi-supervised recursive autoencoders for predicting sentiment distributions.
In Proceedings of the 2011 Conferenceon Empirical Methods in Natural Language Processing, pages 151?161.Christoph Tillman.
2004.
A unigram orientation model for statistical machine translation.
In Proceedings of theHuman Language Technology Conference of the North American Chapter of the Association for ComputationalLinguistics: HLT-NAACL 2004: Short Papers, pages 101?104.Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun.
2005.
Large margin methodsfor structured and interdependent output variables.
Journal of Machine Learning Research, 6:1453?1484.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maximum entropy based phrase reordering model for statisticalmachine translation.
In Proceedings of the 21st International Conference on Computational Linguistics and44th Annual Meeting of the Association for Computational Linguistics, pages 521?528.Sirvan Yahyaei and Christof Monz.
2010.
Dynamic distortion in a discriminative reordering model for statisticalmachine translation.
In Proceedings of the 7th International Workshop on Spoken Language Translation, pages353?360.Mikhail Zaslavskiy, Marc Dymetman, and Nicola Cancedda.
2009.
Phrase-based statistical machine translationas a traveling salesman problem.
In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 333?341.Richard Zens, Hermann Ney, Taro Watanabe, and Eiichiro Sumita.
2004.
Reordering constraints for phrase-based statistical machine translation.
In Proceedings of the 20th International Conference on ComputationalLinguistics, pages 205?211.1907
