Answer ExtractionTowards better Evaluations of NLP  SystemsRo l f  Schwi t te r  and D iego  Mo l l~  and Rache l  Fourn ie r  and Michae lHessDepar tment  of Informat ion TechnologyComputat iona l  Linguistics GroupUniversity of ZurichCH-8057 Zurich\[schwitter, molla, fournier, hess\] @ifi.
unizh, chAbst rac tWe argue that reading comprehension tests arenot particularly suited for the evaluation ofNLP systems.
Reading comprehension tests arespecifically designed to evaluate human readingskills, and these require vast amounts of worldknowledge and common-sense r asoning capa-bilities.
Experience has shown that this kind offull-fledged question answering (QA) over textsfrom a wide range of domains is so difficult formachines as to be far beyond the present stateof the art of NLP.
To advance the field we pro-pose a much more modest evaluation set:up, viz.Answer Extraction (AE) over texts from highlyrestricted omains.
AE aims at retrieving thosesentences from documents that contain the ex-plicit answer to a user query.
AE is less ambi-tious than full-fledged QA but has a number ofimportant advantages over QA.
It relies mainlyon linguistic knowledge and needs only a verylimited amount of world knowledge and few in-ference rules.
However, it requires the solutionof a number of key linguistic problems.
Thismakes AE a suitable task to advance NLP tech-niques in a measurable way.
Finally, there is areal demand for working AE systems in techni:cal domains.
We outline how evaluation proce-dures for AE systems over real world domainsmight look like and discuss their feasibility.1 On  the  Des ign  o f  Eva luat ionMethods  for  NLP  SystemsThe idea that the systematic and principledevaluation of document processing systems iscrucial for the development of the field as awhole has gained wide acceptance in the com-munity during the last decade.
In a num-ber of large-scale projects (among them TREC(Voorhees and Harman, 1998) and MUC (MUC-7, 1998)), evaluation procedures for specifictypes of systems have been used extensively, andrefined over the years.
Three things were com-mon to these evaluations: First, the systems tobe evaluated were each very closely tied to a par-ticular task (document retrieval and informationextraction, respectively).
Second, the evalua-tion was of the black box type, i.e.
it consideredonly system input-output relations without re-gard to the specific mechanisms by which theoutputs were obtained.
Third, the amount ofdata to be processed was enormous (several gi-gabytes for TREC).There is general agreement that these com-petitive evaluations had a striking and bene-ficial effect on the performance of the varioussystems tested over the years.
However, it isalso recognized (albeit less generally) that theseevaluation experiments also had the, less ben-eficial, effect that the participating systems fo-cussed increasingly more narrowly on those fewparameters that were measured in the evalua-tion, to the detriment of more general prop-erties.
In some cases this meant that power-ful and linguistically interesting but slow sys-tems were dropped in favour of shallow but fastsystems with precious little linguistic content.Thus the system with which SRI participatedin the MUC-3 evaluation in 1991, TACITUS(Hobbs et al, 1991), a true text-understandingsystem, was later replaced by FASTUS (Appeltet al, 1995; Hobbs et al, 1996), a much sim-pler, and vastly faster, information extractionsystem.
The reason was that TACITUS wasspending so much of its time attempting to makesense of portions of the text that were irrelevantto the task that recall was mediocre.
We ar-gue that the set-up of these competitive valu-ations, and in particular the three parametersmentioned above, drove the development of theparticipating systems towards becoming impres-20sive feats of engineering, fine-tuned to one veryspecific task, but with limited relevance outsidethis task and with little linguistically relevantcontent.
We argue that these evaluations there-fore did not drive progress in ComputationalLinguistics very much.We therefore think it a timely idea to con-ceive of evaluation methodologies which mea-sure the linguistically relevant functions of NLPsystems and thus advance Computational Lin-guistics as a science rather than as an engineer-ing discipline.
The suggestion made by the or-ganizers of this workshop on how this could beachieved has-four comPonents.
First, they sug-gest to use full-fledged text-based question an-swering (QA) as task.
Second, they suggest arelatively small amount off text (compared withthe volumes of text used in TREC) as test data.Third they (seem to) suggest o .use texts froma wide range off domains.
Finally they suggestto use pre-existing question/answer pairs, de-veloped for and tested on humans, as evaluationbenchmark (Hirschman et al, 1999).However, our experience in the field leads usto believe that this evaluation set-up will nothelp Computational Linguistics as much as itwould be needed, mainly because it is way tooambitious.
We fear that this fact will force de-velopers, again, to design all kinds of ad-hoc so-lutions and efficiency hacks which will severelylimit the scientific relevance of the resulting sys-tems.
We argue that three of the four compo-nents of the suggested set-up must be reducedconsiderably in scope to make the test-bed help-ful.First, we think the task is too difficult.
Full-fledged QA on the basis of natural languagetexts is far beyond the present state of theart.
The example of the text-based QA sys-tem LILOG (Herzog and Rollinger, 1991) hasshown that the analysis of texts to the depthrequired for real QA over their contents is so re-source intensive as to be unaffordable in any realworld context.
After an investment of around 65person-years of work the LILOG system couldanswer questions over a few (reputedly merelythree) texts of around one page length each froman extremely narrow domain (city guides andthe like).
We think it is fair to say that the situ-ation in our field has not changed enough in themeantime to invalidate this finding.Second, we agree that the volume off data tobe used should be relatively small.
We mustavoid that the sheer pressure of the volumes oftexts to be processed forces system developersto use shallow methods.Third, we think it is very important o restrictthe domain of the task.
We certainly do not ar-gue in favour of some toy domain but we getthe impression that the reading comprehensiontexts under consideration cover a far too widerange of topics.
We think that technical man-uals are a better choice.
They cover a narrowdomain (such as computer operating systems,or airplanes), and they also use a relatively re-stricted type of language with a reasonably clearsemantic foundation.Fourth, we think that tests that are specif-ically designed to evaluate to what extent ahuman being understands a text are intrinsi-cally unsuitable for our present purposes.
Al-though it would admittedly be very convenientto have "well written" texts, "good" questionsabout them and the "correct" answers all in onepackage, the texts are not "real world" language(in that they were written specifically for thesetests), and the questions are:just far too difficult,primarily because they rely on exactly thosecomponents of language understanding wherehumans excel and computers are abominablypoor (inferences over world knowledge).In Section 2 we outline what kinds of prob-lems would have to be solved by a QA sys-tem if it were to answer the test questionsgiven in (WRC, 2000).
Most of the prob-lems would require enormous amounts of worldknowledge and vast numbers of lexical inferencerules for a solution, on top of all the "classi-cal" linguistic problems our field has been strug-gling with (ambiguities, anaphoric references,synonymy/hyponymy).
We will then argue inSection 3 that a more restricted kind of task,Answer Extraction, is better suited as experi-mental set-up as it would focus our forces onthese unsolved but reasonably well-understoodproblems, rather than divert them to the ill-understood and fathomless black' hole of worldknowledge.
In Section 4, we will finally outlinehow evaluation procedures in this context mightlook like.21...k2 Why Read ing  Comprehens ionTests  v ia QA are  Too :DifficultReading comprehension tests are designed tomeasure how well human readers understandwhat they read.
Each story comes with a setof questions about information that is statedor implied in the text.
The readers demon-strate their understanding of the story by an-swering the questions about it.
Thus, read-ing comprehension tests assume a cognitive pro-cess of human beings.
This process involves ex-panding the mental model of a text by usingits implications and presuppositions, retrievingthe stored information, performing inferences tomake implicit information explicit, and generat-ing the surface strings that express this infor-mation.
Many different forms of knowledge takepart in this process: linguistic, procedural andworld knowledge.
All these forms coalesce inthe memory of the reader and it is very difficultto clearly distinguish and reconstruct them in aQA system.
At first sight the story published in(WRC, 2000) is easy to understand because thesentences are short and cohesive.
But it turnsout that a classic QA system would need vastamounts of knowledge and inference rules in or-der to understand the text and to give sensibleanswers.Let us investigate what kind of informationa full-fledged QA system needs in order to an-swer the questions that come with the readingcomprehension test (Figure 1) and discuss howdifficult it is to provide this information.To answer the first question(1) Who collects maple sap?the system needs to know that the mass nounsap in the text sentenceFarmers collect the sap.is indeed the maple sap mentioned in thequestion.
The compound noun maple sap is a se-mantically narrower term than the noun sap andencodes an implicit relation between the first el-ement maple and the head noun sap.
This rela-tion names the origin of the material.
Since noexplicit information about the relation betweenthe two objects is available in the text an idealQA system would have to assume such a relationby a form of abductive reasoning.How.Maple  Syrup is MadeMaple syrup comes from sugar maple trees.
Atone time, maple syrup was used to make sugar.This is why the tree is called a "sugar" mapletree.Sugar maple trees make sap.
Farmers collect hesap.
The best time to collect sap is in Februaryland March.
The nights must be cold and thedays warm.The framer drills a few small holes in each tree.He puts a spout in each hole.
Then he hangsa bucket on the end of each spout.
The buckethas a cover to keep rain and snow out.
The sapdrips into the bucket.
About 10 gallons of sapcome from each hole.1.
Who collects maple sap?(Farmers)2.
What does the farmer hang from a spout?
(A bucket)3.
When is sap collected?
(February and March)4.
Where does the maple sap come from?
(Sugar maple trees)5.
Why is the bucket covered?
(to keep rain and snow out)Figure 1: Reading comprehension testTo answer the second question(2) What does the farmer hang from a spout?successfully the system would need at leastthree different kinds of knowledge:First, it would need discourse knowledge toresolve the intersentential co-reference betweenthe anaphor he and the antecedent the farmerin the following text sequence:The farmer drills- a few small holes in eachtree.
\[...\] Then he hangs a bucket ...Although locating antecedents has proved tobe one of the hard problems of natural lan-guage processing, the anaphoric reference reso-lution can be done easily in this case because theantecedent is the most recent preceding nounphrase thgt agrees in gender, number and per-son.22Second, the system would require linguisticknowledge to deal with the synonymy relationbetween hang on and hang .from, and the at-tachment ambiguity of the prepositional phraseused in the text sentence and the query.Third, the system needs an inference rule thatmakes somehow clear that the noun phrase aspout expressed in the query is entailed in themore complex noun phrase the end of each spoutin the text sentence.
Additionally, to processthis relation the system would require an infer-ence rule of the form:IF X does Y to EACH ZTHEN X does Y to A Z.The third question(3) When is sap collected?asks for the time point when' ~ap is collectedbut the text gives only a rule-like recommenda-tionThe best time to collect sap is in Februaryand March.with an additional constraintThe nights must be cold and the days warm.and does not say that the sap is in fact col-lected in February and March.
The bridginginference that the system would need to modelhere is not founded on linguistic knowledge buton world knowledge.
Solving this problem isvery hard.
It could be argued that default rulesmay solve such problems but it is not clearwhether formal methods are able to handle thesort of default reasoning required for represent-ing common-sense reasoning.To give an answer for the fourth question(4) Where does the maple sap come .from?the system needs to know that maple sapcomes from sugar maple trees.
This informa-tion is not explicitly available in the text.
In-stead of saying where maple sap comes from thetext says where maple syrup comes from:Maple syrup comes .from sugar maple trees.23There exists a metonymy relation betweenthese two compound nouns.
The compoundnoun maple syrup (i.e.
product) can only besubstituted by maple sap (i.e.
material), if thesystem is able to deal with metonymy.
Togetherwith the information in the sentenceSugar maple trees make sap.and an additional exical inference rule inform of a meaning postulateIF X makes Y THEN Y comes from X.the system could deduce (in theory) first sapand then by abductive reasoning assume thatthe sap found is maple sap.
Meaning postulatesare true by virtue of the meaning they link.
Ob-servation cannot prove them false.To answer the fifth question(5) Why is the bucket covered?the system needs to know that the syntac-tically different expressions has a cover and iscovered have the same propositional content.The system needs an explicit lexical inferencerule in form of a conditional equivalenceIF ConditionsTHEN X has a cover ~-> X is covered.that converts the verbal phrase with the nom-inal expression i to a the corresponding passiveconstruction (and vice versa) taking the presentcontext into consideration.As these concrete xamples how, the task ofQA over this simple piece of text is frighten-ingly difficult.
Finding the correct answers tothe questions requires far more information thatone would think at first.
Apart from linguisticknowledge a vast amount of world knowledgeand a number of bridging inferences are nec-essary to answer these seemingly simple ques-tions.
For human beings bridging inferencesare automatic and for the most part uncon-scious.
The hard task consists in reconstructingall this information coming from different knowl-edge sources and modeling the suitable inferencerules in a general way so that the system scalesup.3 Answer  Ext ract ion  as anA l te rnat ive  TaskAn alternative to QA is answer extraction (AE).The general goal of AE is the same as that ofQA, to find answers to user queries in textualdocuments.
But the way to achieve this is differ-ent.
Instead of generating the answer from theinformation given in the text (possibly in im-plicit form only), an AE system will retrieve thespecific sentence(s) in the text that contain(s)the explicit answer to the query.
In addition,those phrases in the sentence that represent theexplicit a_nswer to the query may be highlighted.For example, let us assume that the followingsentence is in the text (and we are going to useexamples from a technical domain, that of theUnix user's manual):(1) cp copies the contents of filenamel ontofilename2.If the user asks the queryWhich command copies files?a QA system will return:cpHowever, an AE system will return all thesentences in the text that directly answer thequestion, among them (1).Obviously, an AE system is far less power-ful than a real QA system.
Information thatis not explicit in a text will not be found, letalone information that must be derived fromtextual information together with world knowl-edge.
But AE has a number of important ad-vantages over QA as a test paradigm.
First, anobvious advantage of this approach is that theuser receives first-hand information, right fromthe text, rather than system-generated replies.It is therefore much easier for the user to de-termine whether the result is reliable.
Second,it is a realistic task (as the systems we are de-scribing below proves) as there is no need togenerate natural language output, and there isless need to perform complex inferences becauseit merely looks up things in the texts which axeexplicitly there.
It need not use world knowl-edge.
Third, it requires the solution of a num-ber of well-defined and truly important linguisticproblems and is therefore well suited to measure,and advance, progress in these respects.
We willcome to this later.
And finally, there is a realdemand for working AE systems in technical do-mains since the standard IR approaches just donot work in a satisfactory manner in many appli-cations where the user is in pressure to quicklyfind a specific answer to a specific question, andnot just (potentially long) lists of pointers to(potentially large) documents that may (or maynot) be relevant o the query.
Examples of ap-plications are on-line software help systems, in-terfaces to machine-readable technical manuals,help desk systems in large organizations, andpublic enquiry systems accessible over the Web.The basic procedure we use in our approachto AE is as follows: In an off-line stage, thedocuments are processed and the core mean-ing of each sentence is extracted and stored asso-called minimal logical forms.
In an on-linestage, the user query is also processed to pro-duce a minimal ogical form.
In order to retrieveanswer sentences from the document collection,the minimal logical form of the query is proved,by a theorem prover, over the minimal logicalforms of the entire document collection (Moll~tet al, 1998).
Note that this method will not re-trieve patently wrong answer sentences like bkupfiles all copies on the hard disk in response toqueries like Which command copies files?
Thisis the kind of response we inevitably get if weuse some variation of the bag-of-words approachadopted by IR based systems not performingany kind of content analysis.We are currently developing two AE sys-tems.
The first, ExtrAns, uses deep linguis-tic analysis to perform AE over the Unix man-pages.
The prototype of this system uses 500Unix manpages, and it can be tested over theWeb \[http://www.ifi.unizh.ch/cl/extrans\].
Inthe second (new) project, WebExtrAns, we in-tend to perform AE-over the "Aircraft Main-tenance Manual" of the Airbus 320 (ADRES,1996).
The larger volume of data (about 900 kgof printed paper!)
will represent an opportunityto test the scalability of an AE system that usesdeep linguistic analysis.There is a number of important areas of re-search that ExtrAns and WebExtrAns, and byextension any AE system, has to focus on.
Firstof all, in order to generate the logical form of the24sentences, the following must be tackled: Find-ing the verb arguments, performing disambigua-tion, anaphora resolution, and coping with nom-inalizations, passives, ditransitives, compoundnouns, synonymy, and hyponymy (Moll~t et al,1998; Mollh and Hess, 2000).
Second, the veryidea of producing the logical forms of real-worldtext requires the formalization of the logicalform notation so that it is expressive nough butstill remaining usable (Schwitter et al, 1999).Finally, the goal of producing a practical systemfor a real-world application eeds to address theissue of robustness and scalability (Moll~t andHess, 1999).--Note that the fact that AE and QA share thesame goal makes it possible to start a projectthat initially performs AE, and gradually en-hance and extend it with inference and gener-ation modules, until we get a full-fledged QAsystem.
This is the long-time g0al of our cur-rent series of projects on AE.4 Eva luat ing  the  Resu l tsInstead of using reading comprehension teststhat are meant for humans, not machines, weshould produce the specific tests that wouldevaluate the AE capability of machines.
Hereis our proposal.Concerning test queries, it is always better touse real world queries than queries that were ar-tificially constructed to match a portion of text.Experience has shown time and again that realpeople tend to come up with questions differentfrom those the test designers could think of.
Byusing, as we suggest, manuals of real world sys-tems, it is possible to tap the interaction of realusers with this system as a source of real ques-tions (we do this by logging the questions ub-mitted to our system over the Web).
Anotherway of finding queries is to consult he FAQ listsconcerning a given system sometimes availableon the Web.
In both cases you will have to fil-ter out those queries that have no answers in thedocument collection or that are clearly beyondthe scope of the system to evaluate (for exam-ple, if the inference needed to answer a query istoo complex, even for a human judge).Concerning answers, the principal measuresfor the AE task must be recall and precision,applied to individual answer sentences.
Recallis the number of correct answer sentences thesystem retrieved divided by the total numberof correct answers in the entire document col-lection.
Precis ion is the number of correct an-swer sentences the system retrieved ivided bythe total number of answers it returned.
As isknown all too well, recall is nearly impossible todetermine in an exact fashion for all but toy ap-plications ince the totality of correct answers inthe entire document collection has to be foundmainly by hand.
Almost certainly one will haveto resort to (hopefully) representative samplesof documents to arrive at a reasonable approxi-mation to this value.
Precision is easier to deter-mine although even this step can become verytime consuming in real world applications.If, on the other hand, one only needs to doan approximate evaluation of the AE system, itwould be possible to find a representative s t ofcorrect answers by making a person write theideal answers, and then automatically findingthe sentences in the documents that are seman-tically close to these ideal answers.
Semanticcloseness between a sentence and the ideal an-swer can be computed by combining the suc-c inctness and correctness of the sentence withrespect to the ideal answer.
Succinctness andcorrectness are the counterparts ofprecision andrecall, but on the sentence level.
These mea-sures can be computed by checking the overlapof words between the sentence and the ideal an-swer (Hirschman et al, 1999), but we suggest amore content-based approach.Our proposal is to compare not words in asentence, but their logical forms.
Of course, thiscomparison can be done only if it is possible toagree on how logical forms should look like, tocompute them, and to perform comparisons be-tween them.
The second and third conditionscan be fulfilled if the logical forms are simplelists of predicates that contain some minimal se-mantic information, as it is the case in ExtrAns(Schwitter et al, 1999).
In this paper we willuse a simplification of the minimal ogical formsused by ExtrAns.
Below are two sentences withtheir logical forms:(1) rm removes one or more files.remove(x ,y ) ,  rm(x) ,  f i le(y)(2) csplit pr ints  the character counts .for eachfile created, and removes any files it createsi f  an error occurs.25print(x,y), csplit(x), character-count(y),remove(x ,z ) ,  fi le(z), create(x,z), oc-cur(e), error(e)As an example of how to compute succinct-ness and correctness, take the following ques-tion:Which command removes files?The ideal answer is a full sentence that con-tains the information given by the question andthe information requested.
Since rm is the com-mand used to remove files, the ideal answer is:rm removes  f i les.remove(x,y), rm(x), file(y)Instead of computing the overlap of words,succinctness and correctness ofa sentence can bedetermined by computing the overlap of predi-cates.
The overlap of the predicates (overlaphenceforth) of two sentences is the maximumset of predicates that can be used as part of thelogical form in both sentences.
The predicatesin boldface in the two examples above indicatethe overlap with the ideal answer: 3 for (1), and2 for (2).Succinctness of a sentence with respect o anideal answer (precision on the sentence level) isthe ratio between the overlap and the total num-ber of predicates in the sentence.
Succinctnessis, therefore, 3/3=1 for (1), and 2/8=0.25 for(2).Correctness of a sentence with respect o anideal answer (recall on the sentence level) is theratio between the overlap and the number ofpredicates in the ideal answer.
In the exam-ples above, correctness i 3/3=1 for (1), and2/3=0.66 for (2).A combined measure of succinctness and cor-rectness could be used to determine the seman-tic closeness of the sentences to the ideal an-swer.
By establishing a threshold to the seman-tic closeness, one can find the sentences in thedocuments that are answers to the user's query.The advantage of using overlap of predicatesagainst overlap of words is that the relations be-tween the words also affect the measure for suc-cinctness and correctness.
We can see this inthe following artificial example.
Let us supposethat the ideal answer to a query is:Madrid defeated Barcelona.defeat(x,y), madrid(x), barcelona(y)The following candidate sentence producesthe same predicates:Barcelona defeated Madrid.defeat(x,y), madr id (y ) ,  barce lona(x)However, at most two predicates only can bechosen at the same time (in boldface), becauseof the restrictions of the arguments.
In theideal answer, the first argument of "defeat" isMadrid and the second argument is Barcelona.In the candidate sentence, however, the argu-ments are reversed (the name of the variableshave no effect on this).
The overlap is, therefore,2.
Succinctness and correctness are 2/3=0.66and 2/3=0.66, respectively.5 Conc lus ionWe are convinced that reading comprehensiontests are too difficult for the current state ofart in natural language processing.
Our anal-ysis of the Maple Syrup story shows how muchworld knowledge and inference rules are neededto actually answer the test questions correctly.Therefore, we think that a more restricted kindof task that focuses rather on tractable problemsthan on AI-hard problems of question-answering(QA) is better suited to take our field a stepfurther.
Answer Extraction (AE) is an alter-native to QA that relies mainly O n linguisticknowledge.
AE aims at retrieving those exactpassages of a document hat directly answer agiven user query.
AE is less ambitious than full-fledged QA since the answers are not generatedfrom a knowledge base but looked up in the doc-uments.
These documents come from a well-defined (technical) domain and consist of a rela-tively small volume of data.
Our test queries arereal world queries that express a concrete infor-mation need.
To evaluate our AE systems, wepropose besides precision and recall two addi-tional measures: succinctness and correctness.They measure the quality of answer sentenceson the sentence level and are computed on thebasis of the overlap of logical predicates.To round out the picture, we address the ques-tions in (WRC, 2000) in the view of what we saidin this paper:26Q: Can such exams \[reading comprehensiontests\] be used to evaluate computer-based lan-guage understanding effectively and e~ciently?A: We think that no language unders tand-ing system will currently be able to answer a sig-nificant proportion of such questions, which willmake evaluation results difficult at best, mean-ingless at worst.Q: Would they provide an impetus and testbed for interesting and useful research?A: We think that the impetus they might pro-vide would drive development in the wrong di-rection, viz.
towards the creation of (possiblyimpressive) engineering feats without much lin-guistically interestingcontent.Q: Are they too hard for current technology?A: Definitely, and by a long shot.Q: Or are they too easy, such that simplehacks can score high, although there is clearlyno understanding involved?
.,A: "Simple hacks" would almost certainlyscore higher than linguistically interesting meth-ods but not because the task is too simple butbecause it is far too difficult.ReferencesADRES, 1996.
A319/A320/A321 AircraftMaintenance Manual.
Airbus Industrie,Blagnac Cedex, France.
Rev.
May 1.Douglas E. Appelt, Jerry R. Hobbs, John Bear,David Israel, Megumi Kameyama, AndyKehler, David Martin, Karen Myers, andMabry Tyson.
1995.
SRI International FAS-TUS system MUC-6 test results and analysis.In Proc.
Sixth Message Understanding Con-\]erence (MUC-6), Columbia, Maryland.Otthein Herzog and Claus-Rainer Rollinger, ed-itors.
1991.
Text Understanding in LILOG:Integrating Computational Linguistics andArtificial Intelligence - final report on theIBM Germany LILOG project, volume 546 ofLecture Notes in Computer Science.
Springer-Verlag, Berlin.Lynette Hirschman, Marc Light, Eric Breck, andJohn D. Burger.
1999.
Deep Read: A read-ing comprehension system.
In Proc.
A CL '99.University of Maryland.Jerry Hobbs, Douglas E. Appelt, John S. Bear,Mabry Tyson, and David Magerman.
1991.The TACITUS system: The MUC-3 experi-ence.
Technical report, AI Center, SRI Inter-national, Menlo Park, CA.Jerry R. Hobbs, Douglas E. Appelt, John Bear,David Israel, Megumi Kameyama, MarkStickel, and Mabry Tyson.
1996.
FASTUS:A cascaded finite-state transducer for extract-ing information from natural-language t xt.In E. Roche and Y. Schabes, editors, FiniteState Devices for Natural Language Process-ing.
MIT Press, Cambridge, MA.Diego Moll~ and Michael Hess.
1999.
Onthe scalability of the answer extraction sys-tem "ExtrAns".
In Proc.
Applications ofNatural Language to Information Systems(NLDB'99), pages 219-224, Klagenfurt, Aus-tria.Diego Moll~ and Michael Hess.
2000.
Deal-ing with ambiguities in an answer extrac-tion system.
In Representation and Treatmentof Syntactic Ambiguity in Natural LanguageProcessing, Paris.
ATALA.Diego Moll~, Jawad Berri, and Michael Hess.1998.
A real world implementation f answerextraction.
In Proc.
of the 9th InternationalConference and Workshop on Database andExpert Systems.
Workshop "Natural Languageand Information Systems" (NLIS'98), pages143-148, Vienna, August.MUC-7.
1998.
Proc.
of the seventh mes-sage understanding conference (MUC-7).http://www.muc.saic.com.Rolf Schwitter, Diego Moll~, and Michael Hess.1999.
Extrans - -  answer extraction fromtechnical documents by minimal ogical formsand selective highlighting.
In Proc.
Third In-ternational Tbilisi Symposium on Language,Logic and Computation, Batumi, Georgia.http://www.ifi.unizh.ch/cl/.Ellen M. Voorhees and Donna Harman.
1998.Overview of the seventh Text REtrieval Con-ference (TREC-7).
In Ellen M. Voorhees andDonna Harman, editors, The Seventh TextREtrieval Conference (TREC-7), number500-242 in NIST Special Publication, pages 1-24.
NIST-DARPA, Government Printing Of-rice.WRC.
2000.
Workshop on reading compre-hension texts as evaluation for computer-based language understanding systems.http://www.gte.com/AboutGTE/gto/anlp-naacl2000/comprehension.html.27
