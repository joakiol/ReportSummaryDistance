Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 467?473,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsRanking Human and Machine Summarization SystemsPeter RankelUniversity of MarylandCollege Park, Marylandrankel@math.umd.eduJohn M. ConroyIDA/Center for Computing SciencesBowie, Marylandconroyjohnm@gmail.comEric V. SludUniversity of MarylandCollege Park, Marylandevs@math.umd.eduDianne P. O?LearyUniversity of MarylandCollege Park, Marylandoleary@cs.umd.eduAbstractThe Text Analysis Conference (TAC) rankssummarization systems by their average scoreover a collection of document sets.
We in-vestigate the statistical appropriateness of thisscore and propose an alternative that betterdistinguishes between human and machineevaluation systems.1 IntroductionFor the past several years, the National Institute ofStandards and Technology (NIST) has hosted theText Analysis Conference (TAC) (previously calledthe Document Understanding Conference (DUC))(Nat, 2010).
A major theme of this conference ismulti-document summarization: machine summa-rization of sets of related documents, sometimesquery-focused and sometimes generic.
The sum-marizers are judged by how well the summariesmatch human-generated summaries in either auto-matic metrics such as ROUGE (Lin and Hovy, 2003)or manual metrics such as responsiveness or pyra-mid evaluation (Nenkova et al, 2007).
Typically thesystems are ranked by their average score over alldocument sets.Ranking by average score is quite appropriate un-der certain statistical hypotheses, for example, wheneach sample is drawn from a distribution whichdiffers from the distribution of other samples onlythrough a location shift (Randles and Wolfe, 1979).However, a non-parametric (rank-based) analysis ofvariance on the summarizers?
scores on each docu-ment set revealed an impossibly small p-value (lessFigure 1: Confidence Intervals from a non-parametricTukey?s honestly significant difference test for 46 TAC2010 update document sets.
The blue confidence interval(for document set d1032) does not overlap any of the 30red intervals.
Hence, the test concludes that 30 documentsets have mean significantly different from the mean ofd1032.467Figure 2: Overall Responsiveness scores.Figure 3: Linguistic scores.Figure 4: Pyramid scores.Figure 5: ROUGE-2 scores for the TAC 2010 updatesummary task, organized by document set (y-axis) andsummarizer (x-axis).
The 51 summarizers fall into twodistinct groups: machine systems (first 43 columns) andhumans (last 8 columns).
Note that each human onlysummarized half of the document sets, thus creating 23missing values in each of the last 8 columns.
Black isused to indicate missing values in the last 8 columns andlow scores in the first 43 columns.than 10?12 using Matlab?s kruskalwallis 1),providing evidence that a summary?s score is notindependent of the document set.
This effect canbe seen in Figure 1, showing the confidence bands,as computed by a Tukey honestly significant differ-ence test for each document set?s difficulty as mea-sured by the mean rank responsiveness score forTAC 2010.
The test clearly shows that the summa-rizer performances on different document sets havedifferent averages.We further illustrate this in Figures 2 ?
5, whichshow the scores of various summarizers on vari-ous document sets using standard human and au-tomatic evaluation methods (Dang and Owczarzak,2008) of overall responsiveness, linguistic quality,pyramid scores, and ROUGE-2 using color to indi-cate the value of the score.
Some rows are clearlydarker, indicating overall lower scores for the sum-1The Kruskal-Wallis test performs a one-way analysis ofvariance of document-set differences after first converting thesummary scores for each sample to their ranks within the pooledsample.
Computed from the converted scores, the Kruskal-Wallis test statistic is essentially the ratio of the between-groupsum of squares to the combined within-group sum of squares.468maries of these documents, and the variances of thescores differ row-by-row.
These plots show qualita-tively what the non-parametric analysis of variancedemonstrates statistically.
While the data presentedwas for the TAC 2010 update document sets, similarresults hold for all the TAC 2008, 2009, and 2010data.
Hence, it may be advantageous to measuresummarizer quality by accounting for heterogeneityof documents within each test set.
A non-parametricpaired test like the Wilcoxon signed-rank is one wayto do this.
Another way would be paired t-tests.In the paper (Conroy and Dang, 2008) the authorsnoted that while there is a significant gap in perfor-mance between machine systems and human sum-marizers when measured by average manual met-rics, this gap is not present when measured by theaverages of the best automatic metric (ROUGE).
Inparticular, in the DUC 2005-2007 data some systemshave ROUGE performance within the 95% confi-dence intervals of several human summarizers, buttheir pyramid, linguistic, and responsiveness scoresdo not achieve this level of performance.
Thus,the inexpensive automatic metrics, as currently em-ployed, do not predict well how machine summariescompare to human summaries.In this work we explore the use of document-paired testing for summarizer comparison.
Our mainapproach is to consider each pair of two summa-rizers?
sets of scores (over all documents) as a bal-anced two-sample dataset, and to assess that pair?smean difference in scores through a two-sample Tor Wilcoxon test, paired or unpaired.
Our goal hasbeen to confirm that human summarizer scores areuniformly different and better on average than ma-chine summarizer scores, and to rate the quality ofthe statistical method (T or W, paired or unpaired)by the consistency with which the human versusmachine scores show superior human performance.Our hope is that paired testing, using either the stan-dard paired two-sample t-test or the distribution-free Wilcoxon signed-rank test, can provide greaterpower in the statistical analysis of automatic metricssuch as ROUGE.2 Size and Power of TestsStatistical tests are generally compared by choosingrejection thresholds to achieve a certain small prob-ability of Type I error (usually as ?
= .05).
Givenmultiple tests with the same Type I error, one prefersthe test with the smallest probability of Type II error.Since power is defined to be one minus the Type IIerror probability, we prefer the test with the mostpower.
Recall that a test-statistic S depending onavailable data-samples gives rise to a rejection re-gion by defining rejection of the null hypothesis H0as the event {S ?
c} for a cutoff or rejection thresh-old c chosen so thatP (S ?
c) ?
?for all probability laws compatible with the null hy-pothesis where the (nominal) significance level ?is chosen in advance by the statistician, usually as?
= .05.
However, in many settings, the null hy-pothesis comprises many possible probability laws,as here where the null hypothesis is that the under-lying probability laws for the score-samples of twoseparate summarizers are equal, without specifyingexactly what that probability distribution is.
In thiscase, the significance level is an upper bound for theattained size of the test, defined as supP?H0 P (S ?c), the largest rejection probability P (S ?
c)achieved by any probability law compatible with thenull hypothesis.
The power of the test then dependson the specific probability law Q from the consid-ered alternatives in HA.
For each such Q, and givena threshold c, the power for the test at Q is the re-jection probability Q(S ?
c).
These definitions re-flect the fact that the null and alternative hypothe-ses are composite, that is, each consists of multipleprobability laws for the data.
One of the advan-tages of considering a distribution-free two-sampletest statistic such as the Wilcoxon is that the proba-bility distribution for the statistic S is then the samefor all (continuous, or non-discrete) probability lawsP ?
H0, so that one cutoff c serves for all of H0with all rejection probabilities equal to ?.
2Two test statistics, say S and S?, are generallycompared in terms of their powers at fixed alterna-tives Q in the alternative hypothesis HA, when theirrespective thresholds c, c?
have been defined so thatthe sizes of the respective tests, supP?H0 P (S ?2The Wilcoxon test is not distribution-free for discrete data.However, the discrete TAC data can be thought of as roundedcontinuous data, rather than as truly discrete data.469c) and supP?H0 P (S?
?
c?
), are approximatelyequal.
In this paper, the test statistics under consid-eration are ?
in one-sided testing ?
the (unpaired)two-sample t test with pooled sample variance (T ),the paired two-sample t test (T p), and the (paired)signed-rank Wilcoxon test (W ); and for two-sidedtesting, S is defined by the absolute value of oneof these statistics.
The thresholds c for the testscan be defined either by theoretical distributions, bylarge-sample approximations, or by data-resampling(bootstrap) techniques, and (only) in the last caseare these thresholds data-dependent, or random.
Weexplain these notions with respect to the two-sampledata-structure in which the scores from the first sum-marizer are denoted X1, .
.
.
, Xn, where n is thenumber of documents with non-missing scores forboth summarizers, and the scores from the secondsummarizer are Y1, .
.
.
, Yn.
Let Zk = Xk ?
Ykdenote the document-wise differences between thesummarizers?
scores, and Z?
= n?1?nk=1 Zk betheir average.
Then the paired statistics are definedasT p =?n(n?
1) Z?/(n?k=1(Zk ?
Z?
)2)1/2andW =n?k=1sgn(Zk)R+kwhere R+k is the rank of |Zk| among|Z1|, .
.
.
, |Zn|.
Note that under both null and alter-native hypotheses, the variates Zk are assumed in-dependent identically distributed (iid), while underH0, the random variables Zk are symmetric about 0.The t-statistic T p is ?parametric?
in the sense thatexact theoretical calculations of probabilities P (a <T p < b) depend on the assumption of normality ofthe differences Zk, and when that holds, the two-sided cutoff c = c(T p) is defined as the 1 ?
?/2quantile of the tn?1 distribution with n ?
1 degreesof freedom.
However, when n is moderately orvery large, the cutoff is well approximated by thestandard-normal 1 ?
?/2 quantile z?/2, and T p be-comes approximately nonparametrically valid withthis cutoff, by the Central Limit Theorem.
TheWilcoxon signed-rank statistic W has theoreticalcutoff c = c(W ) which depends only on n, when-ever the data Zk are continuously distributed; but forlarge n, the cutoff is given simply as ?n3/12 ?
z?/2.When there are ties (as might be common in discretedata), the calculation of cutoffs and p-values forWilcoxon becomes slightly more complicated andis no longer fully nonparametric except in a large-sample approximate sense.The situation for the two-sample unpaired t-statistic T currently used in TAC evaluation is notso neat.
Even when the two samplesX = {Xk}nk=1and Y = {Yk}nk=1 are independent, exact theoret-ical distribution of cutoffs is known only under theparametric assumption that the scores are normallydistributed (and in the case of the pooled-sample-variance statistic, that Var(Xk) = Var(Yk).)
How-ever, an essential element of the summarization datais the heterogeneity of documents.
This means thatwhile {Xk}nk=1 can be viewed as iid scores whendocuments are selected randomly ?
and not neces-sarily equiprobably ?
from the ensemble of all pos-sible documents, the Yk and Xk samples are de-pendent.
Still, the pairs {(Xk, Yk)}nk=1, and there-fore the differences {Zk}nk=1, are iid which is whatmakes paired testing valid.
However, there is no the-oretical distribution for T from which to calculatevalid quantiles c for cutoffs, and therefore the use ofthe unpaired t-statistic cannot be recommended forTAC evaluation.What can be done in a particular dataset, like theTAC summarization score datsets we consider, toascertain the approximate validity of theoreticallyderived large-sample cutoffs for test statistics?
Inthe age of plentiful and fast computers, quite a lot,through the powerful computational machinery ofthe bootstrap (Efron and Tibshirani, 1993).The idea of bootstrap hypothesis testing (Efronand Tibshirani, 1993), (Bickel and Ren, 2001) is torandomly sample with replacement (the rows withnon-missing data in) the dataset {(Xk, Yk)}nk=1 insuch a way as to generate representative data thatplausibly would have been seen if two-sample scoredata had been generated from two equally effec-tive summarizers with score distributional charac-teristics like the pooled scores from the two ob-served summarizers.
We have done this in two dis-tinct ways, each creating 2000 datasets with n pairedscores:MC Monte Carlo Method.
For each of many it-470erations (in our case 2000), define a newdataset {(X ?k, Y ?k)}nk=1 by independently swap-ping Xk and Yk with probability 1/2.
Hence,(X ?k, Y ?k) = (Xk, Yk) with probability 1/2 and(Yk, Xk) with probability 1/2.HB Hybrid MC/Bootstrap.
For each of 2000iterations, create a re-sampled dataset{(X ?
?k , Y ?
?k )}nk=1 in the following way.
First,sample n pairs (Xk, Yk) with replacementfrom the original dataset.
Then, as above,randomly swap the components of each pair,each with 1/2 probability.Both of these two methods can be seen to gener-ate two-sample data satisfying H0, with each score-sample?s distribution obtained as a mixture of thedistributions actually generating the X and Y sam-ples.
The empirical qth quantiles for a statisticS = S(X,Y) such as |W | or |T p| are estimatedfrom the resampled data as F?
?1S (q), where F?S(t) issimply the fraction of times (out of 2000) that thestatistic S applied to the constructed dataset had avalue less than or equal to t. The upshot is that the1 ?
?
empirical quantile for S based on either ofthese simulation methods serves as a data-dependentcutoff c attaining approximate size ?
for all H0-generated data.
The MC and HB methods will beemployed in Section 4 to check the theoretical p-values.3 Relative Efficiency ofW versus T pStatistical theory does have something to say aboutthe comparative powers of paired W versus T pstatistics.
These statistics have been studied (Ran-dles and Wolfe, 1979), in terms of their asymp-totic relative efficiency for location-shift alternativesbased on symmetric densities (f(z??)
is a location-shift of f(z)).
For many pairs of parametric andrank-based statistics S, S?, including W and T p, thefollowing assertion has been proved for testing H0at significance level ?.First assume the Zk are distributed according tosome density f(z ?
?
), where f(z) is a symmet-ric function (f(?z) = f(z)).
Next assume ?
= 0under H0.
When n gets large the powers at any al-ternatives with very small ?
= ?/?n, ?
6= 0, canbe made asymptotically equal by using samples ofsize n with statistic S and of size ?
?
n with statisticS?.
Here ?
= ARE(S, S?)
is a constant not depend-ing on n or ?
but definitely depending on f , calledasymptotic relative efficiency of S with respect to S?.
(The smaller ?
< 1 is, the more statistic S?
is pre-ferred among the two.
)Using this definition, it is known (Randles andWolfe 1979, Sec.
5.4 leading up to Table 5.4.7 onp.
167) that the Wilcoxon signed-rank statistic Wprovides greater robustness and often much greaterefficiency than the paired T, with ARE which is 0.95with f a standard normal density, and which is neverless than 0.864 for any symmmetric density f .
How-ever, in our context, continuous scores such as pyra-mid exhibit document-specific score differences be-tween summarizers which often have approximatelynormal-looking histograms, and although the alter-natives perhaps cannot be viewed as pure locationshifts, it is unsurprising in view of the ARE theorycited above that the W and T paired tests have verysimilar performance.
Nevertheless, as we found bystatistical analysis of the TAC data, both are far su-perior to the unpaired T-statistic, with either theoret-ical or empirical bootstrapped p-values.4 Testing Setup and ResultsTo evaluate our ideas, we used the TAC data from2008-2010 and focused on three manual metrics(overall responsiveness, pyramid score, and lin-guistic quality score) and two automatic metrics(ROUGE-2 and ROUGE-SU4).
We make the as-sumption, backed by both the scores given and com-ments made by NIST summary assessors 3, that au-tomatic summarization systems do not perform atthe human level of performance.
As such, if a statis-tic based on an automatic metric, such as ROUGE-2, were to show fewer systems performing at humanlevel of performance than the statistic of averagingscores, such a statistic would be preferable because3Assessors have commented privately at the Text AnalysisConference 2008, that while the origin of the summary is hid-den from them, ?we know which ones are machine generated.
?Thus, automatic summarization fails the Turing test of machineintelligence (Turing, 1950).
This belief is also supported by(Conroy and Dang, 2008) and (Dang and Owczarzak, 2008).
Fi-nally, our own results show no matter how you compare humanand machine scores all machines systems score significantlyworse than humans.4712008: 2145 = (662) pairs 2009: 1830 = (612) pairs 2010: 1275 = (512) pairsMetric Unpair-T Pair-T Wilc.
Unpair-T Pair-T Wilc.
Unpair-T Pair-T Wilc.Linguistic 1234 1416 1410 1000 1182 1173 841 939 934Overall 1202 1353 1342 982 1149 1146 845 894 889Pyramid 1263 1417 1418 1075 1238 1216 875 933 926ROUGE-2 1243 1453 1459 1016 1182 1193 812 938 939ROUGE-SU4 1333 1493 1507 1059 1241 1254 894 983 976Table 1: Number of significant differences found when testing for the difference of all pairs of summarization systems(including humans).2008: 464 = 58?
8 pairs 2009: 424 = 53?
8 pairs 2010: 344 = 43?
8 pairsMetric Unpair-T Pair-T Wilc.
Unpair-T Pair-T Wilc.
Unpair-T Pair-T Wilc.Linguistic 464 464 464 424 424 424 344 344 344Overall 464 464 464 424 424 424 344 344 344Pyramid 464 464 464 424 424 424 344 344 344ROUGE-2 375 409 402 323 350 341 275 309 305ROUGE-SU4 391 418 414 354 378 373 324 331 328Table 2: Number of significant differences resulting from 8 ?
(N ?
8) tests for human-machine system means orsigned-rank comparisons.of its greater power in the machine vs. human sum-marization domain.For each of these metrics, we first created a scorematrix whose (i, j)-entry represents the score forsummarizer j on document set i (these matrices gen-erated the colorplots in Figures 2 ?
5).
We then per-formed a Wilcoxon signed-rank test on certain pairsof columns of this matrix (any pair consisting of onemachine system and one human summarizer).
As abaseline, we did the same testing with a paired andan unpaired t-test.
Each of these tests resulted in ap-value, and we counted how many were less than.05 and called these the significant differences.The results of these tests (shown in Table 2),were somewhat surprising.
Although we expectedthe nonparametric signed-rank test to perform betterthan an unpaired t-test, we were surprised to see thata paired t-test performed even better.
All three testsalways reject the null hypotheses when human met-rics are used.
This is what we?d like to happen withautomatic metrics as well.
As seen from the table,the paired t-test and Wilcoxon signed-rank test offera good improvement over the unpaired t-test.The results in Table 1 are less clear, but still posi-tive.
In this case, we are comparing pairs of machinesummarization systems.
In contrast to the human vs.machine case, we do not know the truth here.
How-ever, since the number of significant differences in-creases with paired testing here as well, we believethis also reflects the greater discriminatory power ofpaired testing.We now apply the Monte Carlo and Hybrid MonteCarlo to check the theoretical p-values reported inTables 1 and 2.
The empirical quantiles foundby these methods generally confirm the theoreti-cal p-value test results reported there, especiallyin Table 2.
In the overall tallies of all compar-isons (Table 1), it seems that the bootstrap results(comparing only W and the un-paired T ) makeW look still stronger for linguistic and overall re-sponsiveness versus the T ; but for the pyramidand ROUGE scores, the bootstrap p-values bring Tslightly closer to W although it still remains clearlyinferior, achieving roughly 10% fewer rejections.5 Conclusions and Future WorkIn this paper we observed that summarization sys-tems?
performance varied significantly across doc-ument sets on the Text Analysis Conference (TAC)data.
This variance in performance suggested thatpaired testing may be more appropriate than thet-test currently employed at TAC to compare the472performance of summarization systems.
We pro-posed a non-parametric test, the Wilcoxon signed-rank test, as a robust more powerful alternative tothe t-test.
We estimated the statistical power of thet-test and the Wilcoxon signed-rank test by calcu-lating the number of machine systems whose per-formance was significantly different than that of hu-man summarizers.
As human assessors score ma-chine systems as not achieving human performancein either content or responsiveness, automatic met-rics such as ROUGE should ideally indicate this dis-tinction.
We found that the paired Wilcoxon testsignificantly increases the number of machine sys-tems that score significantly different than humanswhen the pairwise test is performed on ROUGE-2and ROUGE-SU4 scores.
Thus, we demonstratedthat the Wilcoxon paired test shows more statisticalpower than the t-test for comparing summarizationsystems.Consequently, the use of paired testing should notonly be used in formal evaluations such as TAC, butalso should be employed by summarization devel-opers to more accurately assess whether changes toan automatic system give rise to improved perfor-mance.Further study needs to analyze more summariza-tion metrics such as those proposed at the recentNIST evaluation of automatic metrics, Automati-cally Evaluating Summaries of Peers (AESOP) (Nat,2010).
As metrics become more sophisticated andaim to more accurately predict human judgementssuch as overall responsiveness and linguistic qual-ity, paired testing seems likely to be a more power-ful statistical procedure than the unpaired t-test forhead-to-head summarizer comparisons.Throughout our research in this paper, we treatedeach separate kind of scores on a document set asdata for one summarizer to be compared with thesame kind of scores for other summarizers.
How-ever, it might be more fruitful to treat all the scoresas multivariate data and compare the summarizersthat way.
Multivariate statistical techniques such asPrincipal Component Analysis may play a construc-tive role in suggesting highly discriminating newcomposite scores, perhaps leading to statistics witheven more power to measure a summary?s quality.ROUGE was inspired by the success of theBLEU (BiLingual Evaluation Understudy), an n-gram based evaluation for machine translation (Pa-pineni et al, 2002).
It is likely that paired testingmay also be appropriate for BLEU as well and willgive additional discriminating power between ma-chine translations and human translations.ReferencesPeter J. Bickel and Jian-Jian Ren.
2001.
The Bootstrapin Hypothesis Testing.
In State of the Art in Statisticsand Probability Theory, Festschrift for Willem R. vanZwet, volume 36 of Lecture Notes?
Monograph Series,pages 91?112.
Institute of Mathematical Statistics.John M. Conroy and Hoa Trang Dang.
2008.
Mind theGap: Dangers of Divorcing Evaluations of SummaryContent from Linguistic Quality.
In Proceedings ofthe 22nd International Conference on ComputationalLinguistics - Volume 1, COLING ?08, pages 145?152,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Hoa T. Dang and Karolina Owczarzak.
2008.
Overviewof the tac 2008 update summarization task.
In Pro-ceedings of the 1st Text Analysis Conference (TAC),Gaithersburg, Maryland, USA.B.
Efron and R. J. Tibshirani.
1993.
An Introduction tothe Bootstrap.
Chapman & Hall, New York.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic Eval-uation of Summaries Using N-gram Co-OccurrencesStatistics.
In Proceedings of the Conference of theNorth American Chapter of the Association for Com-putational Linguistics, Edmonton, Alberta.National Institute of Standards and Technology.
2010.Text Analysis Conference, http://www.nist.gov/tac.Ani Nenkova, Rebecca Passonneau, and Kathleen McK-eown.
2007.
The Pyramid Method: IncorporatingHuman Content Selection Variation in SummarizationEvaluation.
ACM Transactions on Speech and Lan-guage Processing, 4(2).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedings ofthe 40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.R.H.
Randles and D.A.
Wolfe.
1979.
Introduction tothe Theory of Nonparametric Statistics.
Wiley seriesin probability and mathematical statistics.
Probabilityand mathematical statistics.
Wiley.Alan Turing.
1950.
Computing Machinery and Intelli-gence.
Mind, 59(236):433?460.473
