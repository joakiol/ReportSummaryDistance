Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 105?108,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPQuery-Focused Summaries or Query-Biased Summaries ?Rahul KatragaddaLanguage Technologies Research CenterIIIT Hyderabadrahul k@research.iiit.ac.inVasudeva VarmaLanguage Technologies Research CenterIIIT Hyderabadvv@iiit.ac.inAbstractIn the context of the Document Understand-ing Conferences, the task of Query-FocusedMulti-Document Summarization is intended toimprove agreement in content among human-generated model summaries.
Query-focus alsoaids the automated summarizers in directingthe summary at specific topics, which may re-sult in better agreement with these model sum-maries.
However, while query focus corre-lates with performance, we show that high-performing automatic systems produce sum-maries with disproportionally higher queryterm density than human summarizers do.
Ex-perimental evidence suggests that automaticsystems heavily rely on query term occurrenceand repetition to achieve good performance.1 IntroductionThe problem of automatically summarizing text doc-uments has received a lot of attention since the earlywork by Luhn (Luhn, 1958).
Most of the current auto-matic summarization systems rely on a sentence extrac-tive paradigm, where key sentences in the original textare selected to form the summary based on the clues (orheuristics), or learning based approaches.Common approaches for identifying key sentencesinclude: training a binary classifier (Kupiec et al,1995), training a Markov model or CRF (Conroy et al,2004; Shen et al, 2007) or directly assigning weightsto sentences based on a variety of features and heuris-tically determined feature weights (Toutanova et al,2007).
But, the question of which components and fea-tures of automatic summarizers contribute most to theirperformance has largely remained unanswered (Marcuand Gerber, 2001), until Nenkova et al (Nenkova etal., 2006) explored the contribution of frequency basedmeasures.
In this paper, we examine the role a queryplays in automated multi-document summarization ofnewswire.One of the issues studied since the inception of auto-matic summarization is that of human agreement: dif-ferent people choose different content for their sum-maries (Rath et al, 1961; van Halteren and Teufel,2003; Nenkova et al, 2007).
Later, it was as-sumed (Dang, 2005) that having a question/query toprovide focus would improve agreement between anytwo human-generated model summaries, as well as be-tween a model summary and an automated summary.Starting in 2005 until 2007, a query-focused multi-document summarization task was conducted as part ofthe annual Document Understanding Conference.
Thistask models a real-world complex question answeringscenario, where systems need to synthesize from a setof 25 documents, a brief (250 words), well organizedfluent answer to an information need.Query-focused summarization is a topic of ongoingimportance within the summarization and question an-swering communities.
Most of the work in this areahas been conducted under the guise of ?query-focusedmulti-document summarization?, ?descriptive questionanswering?, or even ?complex question answering?.In this paper, based on structured empirical evalu-ations, we show that most of the systems participat-ing in DUC?s Query-Focused Multi-Document Sum-marization (QF-MDS) task have been query-biased inbuilding extractive summaries.
Throughout our discus-sion, the term ?query-bias?, with respect to a sentence,is precisely defined to mean that the sentence has atleast one query term within it.
The term ?query-focus?is less precisely defined, but is related to the cognitivetask of focusing a summary on the query, which we as-sume humans do naturally.
In other words, the humangenerated model summaries are assumed to be query-focused.Here we first discuss query-biased content in Sum-mary Content Units (SCUs) in Section 2 and then inSection 3 by building formal models on query-bias wediscuss why/how automated systems are query-biasedrather than being query-focused.2 Query-biased content inSummary Content Units (SCUs)Summary content units, referred as SCUs hereafter, aresemantically motivated subsentential units that are vari-able in length but not bigger than a sentential clause.SCUs are constructed from annotation of a collectionof human summaries on a given document collection.They are identified by noting information that is re-peated across summaries.
The repetition is as smallas a modifier of a noun phrase or as large as a clause.The evaluation method that is based on overlappingSCUs in human and automatic summaries is called the105Figure 1: SCU annotation of a source document.pyramid method (Nenkova et al, 2007).The University of Ottawa has organized the pyramidannotation data such that for some of the sentences inthe original document collection, a list of correspond-ing content units is known (Copeck et al, 2006).
Asample of an SCU mapping from topic D0701A ofthe DUC 2007 QF-MDS corpus is shown in Figure 1.Three sentences are seen in the figure among whichtwo have been annotated with system IDs and SCUweights wherever applicable.
The first sentence has notbeen picked by any of the summarizers participating inPyramid Evaluations, hence it is unknown if the sen-tence would have contributed to any SCU.
The secondsentence was picked by 8 summarizers and that sen-tence contributed to an SCU of weight 3.
The thirdsentence in the example was picked by one summa-rizer, however, it did not contribute to any SCU.
Thisexample shows all the three types of sentences avail-able in the corpus: unknown samples, positive samplesand negative samples.We extracted the positive and negative samples in thesource documents from these annotations; types of sec-ond and third sentences shown in Figure 1.
A totalof 14.8% sentences were annotated to be either posi-tive or negative.
When we analyzed the positive set,we found that 84.63% sentences in this set were query-biased.
Also, on the negative sample set, we found that69.12% sentences were query-biased.
That is, on anaverage, 76.67% of the sentences picked by any au-tomated summarizer are query-biased.
On the otherhand, for human summaries only 58% sentences werequery-biased.
All the above numbers are based on theDUC 2007 dataset shown in boldface in Table 11.There is one caveat: The annotated sentences comeonly from the summaries of systems that participated inthe pyramid evaluations.
Since only 13 among a total32 participating systems were evaluated using pyramidevaluations, the dataset is limited.
However, despitethis small issue, it is very clear that at least those sys-tems that participated in pyramid evaluations have beenbiased towards query-terms, or at least, they have beenbetter at correctly identifying important sentences fromthe query-biased sentences than from query-unbiasedsentences.1We used DUC 2007 dataset for all experiments reported.3 Formalizing query-biasOur search for a formal method to capture the relationbetween occurrence of query-biased sentences in theinput and in summaries resulted in building binomialand multinomial model distributions.
The distributionsestimated were then used to obtain the likelihood of aquery-biased sentence being emitted into a summary byeach system.For the DUC 2007 data, there were 45 summariesfor each of the 32 systems (labeled 1-32) among which2 were baselines (labeled 1 and 2), and 18 summariesfrom each of 10 human summarizers (labeled A-J).
Wecomputed the log-likelihood, log(L[summary;p(Ci)]),of all human and machine summaries from DUC?07query focused multi-document summarization task,based on both distributions described below (see Sec-tions 3.1, 3.2).3.1 The binomial modelWe represent the set of sentences as a binomial distribu-tion over type of sentences.
Let C0and C1denote thesets of sentences without and with query-bias respec-tively.
Let p(Ci) be the probability of emitting a sen-tence from a specified set.
It is also obvious that query-biased sentences will be assigned lower emission prob-abilities, because the occurrence of query-biased sen-tences in the input is less likely.
On average each topichas 549 sentences, among which 196 contain a queryterm; which means only 35.6% sentences in the inputwere query-biased.
Hence, the likelihood function heredenotes the likelihood of a summary to contain nonquery-biased sentences.
Humans?
and systems?
sum-maries must now constitute low likelihood to show thatthey rely on query-bias.The likelihood of a summary then is :L[summary; p(Ci)] =N !n0!n1!p(C0)n0p(C1)n1(1)Where N is the number of sentences in the sum-mary, and n0+ n1= N; n0and n1are the cardinali-ties of C0and C1in the summary.
Table 2 shows var-ious systems with their ranks based on ROUGE-2 andthe average log-likelihood scores.
The ROUGE (Lin,2004) suite of metrics are n-gram overlap based met-rics that have been shown to highly correlate with hu-man evaluations on content responsiveness.
ROUGE-2and ROUGE-SU4 are the official ROUGE metrics forevaluating query-focused multi-document summariza-tion task since DUC 2005.3.2 The multinomial modelIn the previous section (Section 3.1), we describedthe binomial model where we classified each sentenceas being query-biased or not.
However, if we wereto quantify the amount of query-bias in a sentence,we associate each sentence to one among k possibleclasses leading to a multinomial distribution.
Let Ci?106Dataset total positive biased positive negative biased negative % bias in positive % bias in negativeDUC 2005 24831 1480 1127 1912 1063 76.15 55.60DUC 2006 14747 1047 902 1407 908 86.15 71.64DUC 2007 12832 924 782 975 674 84.63 69.12Table 1: Statistical information on counts of query-biased sentences.ID rank LL ROUGE-2 ID rank LL ROUGE-2 ID rank LL ROUGE-21 31 -1.9842 0.06039 J -3.9465 0.13904 24 4 -5.8451 0.11793C -2.1387 0.15055 E -3.9485 0.13850 9 12 -5.9049 0.1037016 32 -2.2906 0.03813 10 28 -4.0723 0.07908 14 14 -5.9860 0.1027727 30 -2.4012 0.06238 21 22 -4.2460 0.08989 5 23 -6.0464 0.087846 29 -2.5536 0.07135 G -4.3143 0.13390 4 3 -6.2347 0.1188712 25 -2.9415 0.08505 25 27 -4.4542 0.08039 20 6 -6.3923 0.10879I -3.0196 0.13621 B -4.4655 0.13992 29 2 -6.4076 0.1202811 24 -3.0495 0.08678 19 26 -4.6785 0.08453 3 9 -7.1720 0.1066028 16 -3.1932 0.09858 26 21 -4.7658 0.08989 8 11 -7.4125 0.104082 18 -3.2058 0.09382 23 7 -5.3418 0.10810 17 15 -7.4458 0.10212D -3.2357 0.17528 30 10 -5.4039 0.10614 13 5 -7.7504 0.11172H -3.4494 0.13001 7 8 -5.6291 0.10795 32 17 -8.0117 0.09750A -3.6481 0.13254 18 19 -5.6397 0.09170 22 13 -8.9843 0.10329F -3.8316 0.13395 15 1 -5.7938 0.12448 31 20 -9.0806 0.09126Table 2: Rank, Averaged log-likelihood score based on binomial model, true ROUGE-2 score for the summariesof various systems in DUC?07 query-focused multi-document summarization task.ID rank LL ROUGE-2 ID rank LL ROUGE-2 ID rank LL ROUGE-21 31 -4.6770 0.06039 10 28 -8.5004 0.07908 5 23 -14.3259 0.0878416 32 -4.7390 0.03813 G -9.5593 0.13390 9 12 -14.4732 0.103706 29 -5.4809 0.07135 E -9.6831 0.13850 22 13 -14.8557 0.1032927 30 -5.5110 0.06238 26 21 -9.7163 0.08989 4 3 -14.9307 0.11887I -6.7662 0.13621 J -9.8386 0.13904 18 19 -15.0114 0.0917012 25 -6.8631 0.08505 19 26 -10.3226 0.08453 14 14 -15.4863 0.102772 18 -6.9363 0.09382 B -10.4152 0.13992 20 6 -15.8697 0.10879C -7.2497 0.15055 25 27 -10.7693 0.08039 32 17 -15.9318 0.09750H -7.6657 0.13001 29 2 -12.7595 0.12028 7 8 -15.9927 0.1079511 24 -7.8048 0.08678 21 22 -13.1686 0.08989 17 15 -17.3737 0.10212A -7.8690 0.13254 24 4 -13.2842 0.11793 8 11 -17.4454 0.10408D -8.0266 0.17528 30 10 -13.3632 0.10614 31 20 -17.5615 0.0912628 16 -8.0307 0.09858 23 7 -13.7781 0.10810 3 9 -19.0495 0.10660F -8.2633 0.13395 15 1 -14.2832 0.12448 13 5 -19.3089 0.11172Table 3: Rank, Averaged log-likelihood score based on multinomial model, true ROUGE-2 score for the sum-maries of various systems in DUC?07 query-focused multi-document summarization task.
{C0, C1, C2, .
.
.
, Ck} denote the k levels of query-bias.
Ciis the set of sentences, each having i queryterms.The number of sentences participating in each classvaries highly, with C0bagging a high percentage ofsentences (64.4%) and the rest {C1, C2, .
.
.
, Ck} dis-tributing among themselves the rest 35.6% sentences.Since the distribution is highly-skewed, distinguish-ing systems based on log-likelihood scores using thismodel is easier and perhaps more accurate.
Like be-fore, Humans?
and systems?
summaries must now con-stitute low likelihood to show that they rely on query-bias.The likelihood of a summary then is :L[summary; p(Ci)] =N !n0!n1!
?
?
?nk!p(C0)n0p(C1)n1?
?
?
p(Ck)nk(2)Where N is the number of sentences in the sum-mary, and n0+ n1+ ?
?
?
+ nk= N; n0, n1,?
?
?
,nkare respectively the cardinalities of C0, C1, ?
?
?
,Ck,in the summary.
Table 3 shows various systems withtheir ranks based on ROUGE-2 and the average log-likelihood scores.3.3 Correlation of ROUGE and log-likelihoodscoresTables 2 and 3 display log-likelihood scores of vari-ous systems in the descending order of log-likelihoodscores along with their respective ROUGE-2 scores.We computed the pearson correlation coefficient (?)
of?ROUGE-2 and log-likelihood?
and ?ROUGE-SU4 andlog-likelihood?.
This was computed for systems (ID: 1-32) (r1) and for humans (ID: A-J) (r2) separately, andfor both distributions.For the binomial model, r1 = -0.66 and r2 = 0.39 wasobtained.
This clearly indicates that there is a strongnegative correlation between likelihood of occurrenceof a non-query-term and ROUGE-2 score.
That is, astrong positive correlation between likelihood of occur-107rence of a query-term and ROUGE-2 score.
Similarly,for human summarizers there is a weak negative cor-relation between likelihood of occurrence of a query-term and ROUGE-2 score.
The same correlation anal-ysis applies to ROUGE-SU4 scores: r1 = -0.66 and r2= 0.38.Similar analysis with the multinomial model havebeen reported in Tables 4 and 5.
Tables 4 and 5 showthe correlation among ROUGE-2 and log-likelihoodscores for systems2and humans3.?
ROUGE-2 ROUGE-SU4binomial -0.66 -0.66multinomial -0.73 -0.73Table 4: Correlation of ROUGE measures with log-likelihood scores for automated systems?
ROUGE-2 ROUGE-SU4binomial 0.39 0.38multinomial 0.15 0.09Table 5: Correlation of ROUGE measures with log-likelihood scores for humans4 Conclusions and DiscussionOur results underscore the differences between humanand machine generated summaries.
Based on Sum-mary Content Unit (SCU) level analysis of query-biaswe argue that most systems are better at finding impor-tant sentences only from query-biased sentences.
Moreimportantly, we show that on an average, 76.67% ofthe sentences picked by any automated summarizer arequery-biased.
When asked to produce query-focusedsummaries, humans do not rely to the same extent onthe repetition of query terms.We further confirm based on the likelihood of emit-ting non query-biased sentence, that there is a strong(negative) correlation among systems?
likelihood scoreand ROUGE score, which suggests that systems aretrying to improve performance based on ROUGE met-rics by being biased towards the query terms.
On theother hand, humans do not rely on query-bias, thoughwe do not have statistically significant evidence to sug-gest it.
We have also speculated that the multinomialmodel helps in better capturing the variance across thesystems since it distinguishes among query-biased sen-tences by quantifying the amount of query-bias.From our point of view, most of the extractive sum-marization algorithms are formalized based on a bag-of-words query model.
The innovation with individ-ual approaches has been in formulating the actual algo-rithm on top of the query model.
We speculate that2All the results in Table 4 are statistically significant withp-value (p < 0.00004, N=32)3None of the results in Table 5 are statistically significantwith p-value (p > 0.265, N=10)the real difference in human summarizers and auto-mated summarizers could be in the way a query (or rel-evance) is represented.
Traditional query models fromIR literature have been used in summarization researchthus far, and though some previous work (Amini andUsunier, 2007) tries to address this issue using con-textual query expansion, new models to represent thequery is perhaps one way to induce topic-focus on thesummary.
IR-like query models, which are designedto handle ?short keyword queries?, are perhaps not ca-pable of handling ?an elaborate query?
in case of sum-marization.
Since the notion of query-focus is appar-ently missing in any or all of the algorithms, the futuresummarization algorithms must try to incorporate thiswhile designing new algorithms.AcknowledgementsWe thank Dr Charles L A Clarke at the University ofWaterloo for his deep reviews and discussions on ear-lier versions of the paper.
We are also grateful to all theanonymous reviewers for their valuable comments.ReferencesMassih R. Amini and Nicolas Usunier.
2007.
A contextual query expansionapproach by term clustering for robust text summarization.
In the proceed-ings of Document Understanding Conference.John M. Conroy, Judith D. Schlesinger, Jade Goldstein, and Dianne P. O?leary.2004.
Left-brain/right-brain multi-document summarization.
In the pro-ceedings of Document Understanding Conference (DUC) 2004.Terry Copeck, D Inkpen, Anna Kazantseva, A Kennedy, D Kipp, Vivi Nastase,and Stan Szpakowicz.
2006.
Leveraging duc.
In proceedings of DUC2006.Hoa Trang Dang.
2005.
Overview of duc 2005.
In proceedings of DocumentUnderstanding Conference.Julian Kupiec, Jan Pedersen, and Francine Chen.
1995.
A trainable documentsummarizer.
In the proceedings of ACM SIGIR?95, pages 68?73.
ACM.Chin-Yew Lin.
2004.
Rouge: A package for automatic evaluation of sum-maries.
In the proceedings of ACL Workshop on Text SummarizationBranches Out.
ACL.H.P.
Luhn.
1958.
The automatic creation of literature abstracts.
In IBM Jour-nal of Research and Development, Vol.
2, No.
2, pp.
159-165, April 1958.Daniel Marcu and Laurie Gerber.
2001.
An inquiry into the nature of mul-tidocument abstracts, extracts, and their evaluation.
In Proceedings of theNAACL-2001 Workshop on Automatic Summarization.Ani Nenkova, Lucy Vanderwende, and Kathleen McKeown.
2006.
A compo-sitional context sensitive multi-document summarizer: exploring the fac-tors that influence summarization.
In SIGIR ?06: Proceedings of the 29thannual international ACM SIGIR conference on Research and developmentin information retrieval, pages 573?580, New York, NY, USA.
ACM.Ani Nenkova, Rebecca Passonneau, and Kathleen McKeown.
2007.
Thepyramid method: Incorporating human content selection variation in sum-marization evaluation.
In ACM Trans.
Speech Lang.
Process., volume 4,New York, NY, USA.
ACM.G.J.
Rath, A. Resnick, and R. Savage.
1961.
The formation of abstracts by theselection of sentences: Part 1: Sentence selection by man and machines.
InJournal of American Documentation., pages 139?208.Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng Chen.
2007.
Doc-ument summarization using conditional random fields.
In the proceedingsof IJCAI ?07., pages 2862?2867.
IJCAI.Kristina Toutanova, Chris Brockett, Michael Gamon, Jagadeesh Jagarlamundi,Hisami Suzuki, and Lucy Vanderwende.
2007.
The pythy summarizationsystem: Microsoft research at duc 2007.
In the proceedings of DocumentUnderstanding Conference.Hans van Halteren and Simone Teufel.
2003.
Examining the consensus be-tween human summaries: initial experiments with factoid analysis.
InHLT-NAACL 03 Text summarization workshop, pages 57?64, Morristown,NJ, USA.
Association for Computational Linguistics.108
