Proceedings of the 12th European Workshop on Natural Language Generation, pages 16?24,Athens, Greece, 30 ?
31 March 2009. c?2009 Association for Computational LinguisticsSystem Building Cost vs. Output Quality in Data-To-Text GenerationAnja Belz Eric KowNatural Language Technology GroupUniversity of BrightonBrighton BN2 4GJ, UK{asb,eykk10}@bton.ac.ukAbstractData-to-text generation systems tend tobe knowledge-based and manually built,which limits their reusability and makesthem time and cost-intensive to createand maintain.
Methods for automating(part of) the system building process ex-ist, but do such methods risk a loss inoutput quality?
In this paper, we inves-tigate the cost/quality trade-off in gen-eration system building.
We comparefour new data-to-text systems which werecreated by predominantly automatic tech-niques against six existing systems for thesame domain which were created by pre-dominantly manual techniques.
We eval-uate the ten systems using intrinsic au-tomatic metrics and human quality rat-ings.
We find that increasing the degree towhich system building is automated doesnot necessarily result in a reduction in out-put quality.
We find furthermore that stan-dard automatic evaluation metrics under-estimate the quality of handcrafted sys-tems and over-estimate the quality of au-tomatically created systems.1 IntroductionTraditional Natural Language Generation (NLG)systems tend to be handcrafted knowledge-basedsystems.
Such systems tend to be brittle, expen-sive to create and hard to adapt to new domainsor applications.
Over the last decade or so, inparticular following Knight and Langkilde?s workon n-gram-based generate-and-select surface real-isation (Knight and Langkilde, 1998; Langkilde,2000), NLG researchers have become increasinglyinterested in systems that are automatically train-able from data.
Systems that have a trainable com-ponent tend to be easier to adapt to new domainsand applications, and increased automation is of-ten taken as self-evidently a good thing.
The ques-tion is, however, whether reduced system buildingcost and increased adaptability are achieved at theprice of a reduction in output quality, and if so,how great the price is.
This in turn raises the ques-tion of how to evaluate output quality so that a po-tential decrease can be detected and quantified.In this paper we set about trying to find answersto these questions.
We start, in the following sec-tion, we briefly describing the SUMTIME corpusof weather forecasts which we used in our experi-ments.
In the next section (Section 2), we outlinefour different approaches to building data-to-textgeneration systems which involve different combi-nations of manual and automatic techniques.
Next(Section 4) we describe ten systems in the four cat-egories that generate weather forecast texts in theSUMTIME domain.
In Section 5 we describe thehuman-assessed and automatically computed eval-uation methods we used to comparatively evalu-ate the quality of the outputs of the ten systems.We then present the evaluation results and discussimplications of discrepancies we found betweenthe results of the human and automatic evaluations(Section 6).2 DataThe SUMTIME-METEO corpus was created by theSUMTIME project team in collaboration with WNIOceanroutes (Sripada et al, 2002).
The corpuswas collected by WNI Oceanroutes from the com-mercial output of five different (human) forecast-ers, and each instance in the corpus consists of nu-merical data files paired with a weather forecast.The experiments in this paper focussed on the partof the forecasts that predicts wind characteristicsfor the next 15 hours.Figure 1 shows an example data file and Fig-ure 2 shows the corresponding wind forecast writ-ten by one of the meteorologists.
In Figure 1, the16Oil1/Oil2/Oil3_FIELDS05-10-0005/06 SSW 18 22 27 3.0 4.8 SSW 2.5905/09 S 16 20 25 2.7 4.3 SSW 2.3905/12 S 14 17 21 2.5 4.0 SSW 2.2905/15 S 14 17 21 2.3 3.7 SSW 2.2805/18 SSE 12 15 18 2.4 3.8 SSW 2.3805/21 SSE 10 12 15 2.4 3.8 SSW 2.4806/00 VAR 6 7 8 2.4 3.8 SSW 2.48...Figure 1: Meteorological data file for 05-10-2000,a.m.
(names of oil fields anonymised).FORECAST FOR:-Oil1/Oil2/Oil3 FIELDS...2.
FORECAST 06-24 GMT, THURSDAY, 05-Oct 2000=====WARNINGS: RISK THUNDERSTORM.
=======WIND(KTS) CONFIDENCE: HIGH10M: SSW 16-20 GRADUALLY BACKING SSE THENFALLING VARIABLE 04-08 BY LATE EVENING50M: SSW 20-26 GRADUALLY BACKING SSE THENFALLING VARIABLE 08-12 BY LATE EVENING...Figure 2: Wind forecast for 05-10-2000, a.m.(names of oil fields anonymised).first column is the day/hour time stamp, the secondthe wind direction predicted for the correspondingtime period; the third the wind speed at 10m abovethe ground; the fourth the gust speed at 10m; andthe fifth the gust speed at 50m.
The remainingcolumns contain wave data.We used a version of the corpus reported pre-viously (Belz, 2008) which contains pairs of windstatements and the wind data that is actually in-cluded in the statement, e.g.
:Data: 1 SSW 16 20 - - 0600 2 SSE - - - -NOTIME 3 VAR 04 08 - - 2400Text: SSW 16-20 GRADUALLY BACKING SSE THENFALLING VARIABLE 4-8 BY LATE EVENINGThe input vector represents a sequence of 7-tuples ?i, d, smin, smax, gmin, gmax, t?
where i isthe tuple?s ID, d is the wind direction, sminand smax are the minimum and maximum windspeeds, gmin and gmax are the minimum and max-imum gust speeds, and t is a time stamp (indicat-ing for what time of the day the data is valid).
Thecorpus consists of 2,123 instances, correspondingto a total of 22,985 words.3 Four Ways to Build an NLG SystemsIn this section, we describe four approachesto building language generators involving differ-ent combinations of automatic and manual tech-niques: traditional handcrafted systems (Sec-tion 3.1); handcrafted but trainable probabilis-tic context-free grammar (PCFG) generators (Sec-tion 3.2); partly automatically constructed andtrainable probabilistic synchronous context-freegrammar (PSCFG) generators; and generators au-tomatically built with phrase-based statistical ma-chine translation (PBSMT) methods (Section 3.4).In Section 4 we explain how we used these tech-niques to build the ten systems in our evaluation.3.1 Rule-based NLGTraditional NLG systems are handcrafted as rule-based deterministic decision-makers that make de-cisions locally, at each step in the generation pro-cess.
Decisions are encoded as generation ruleswith conditions for rule application (often in theform of if-then rules or rules with parameters to bematched), usually on the basis of corpus analysisand expert consultation.
Reiter and Dale?s influen-tial paper (1997) recommended that NLG systemsbe built largely ?by careful analysis of the targettext corpus, and by talking to domain experts?
(p.74, and reiterated on pp.
58, 61, 72 and 73).Handcrafted generation tools have alwaysformed the mainstay of NLG research, a situationvirtually unchanged by the statistical revolutionthat swept through other NLP fields in the 1990s.Well-known examples include the surface realis-ers Penman, FUF/SURGE and RealPro, the re-ferring expression generation components createdby Dale, Reiter, Horacek and van Deemter, andcontent-to-text generators built in the PLANDocand M-PIRO projects, to name but a very few.3.2 PCFG generationContext-free grammars are non-directional, andcan be used for generation as well as for analy-sis (parsing).
One approach that uses CFGs forgeneration is Probabilistic Context-free Represen-tationally Underspecified (pCRU) language gener-ation (Belz, 2008).
As mentioned above, tradi-tional NLG systems tend to be composed of gen-eration rules that apply transformations to rep-resentations.
The basic idea in pCRU is that aslong as the generation rules are all of the formrelation(arg1, ...argn) ?
relation1(arg1, ...argp) ...relationm(arg1, ...argq), m ?
1, n, p, q ?
0, then theset of all generation rules can be seen as defininga context-free language and a single probabilisticmodel can be estimated from raw or annotated textto guide generation processes.In this approach, a CFG is created by hand thatencodes the space of all possible generation pro-17Input [[1,SSW,16,20,-,-,0600],[2,SSE,-,-,-,-,NOTIME],[3,VAR,04,08,-,-,2400]]Corpus SSW 16-20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 4-8 BY LATE EVENINGSUMTIME-Hybrid SSW 16-20 GRADUALLY BACKING SSE THEN BECOMING VARIABLE 10 OR LESS BY MIDNIGHTPCFG-greedy SSW 16-20 BACKING SSE FOR A TIME THEN FALLING VARIABLE 4-8 BY LATE EVENINGPCFG-roulette SSW 16-20 GRADUALLY BACKING SSE AND VARIABLE 4-8PCFG-viterbi SSW 16-20 BACKING SSE VARIABLE 4-8 LATERPCFG-2gram SSW 16-20 BACKING SSE VARIABLE 4-8 LATERPCFG-random SSW 16-20 AT FIRST FROM MIDDAY BECOMING SSE DURING THE AFTERNOON THEN VARIABLE 4-8PSCFG-semantic SSW 16-20 BACKING SSE THEN FALLING VARIABLE 04-08 BY LATE EVENINGPSCFG-unstructured SSW 16-20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 04-08 BY LATE EVENINGPBSMT-unstructured LESS SSW 16-20 SOON BACKING SSE BY END OF THEN FALLING VARIABLE 04-08 BY LATE EVENINGPBSMT-structured GUSTS SSW 16-20 BY EVENING STEADILY LESS GUSTS GRADUALLY BACKING SSE BY LATE EVENINGMINONE BY MIDDAY THEN AND FALLING UNKNOWN VARIABLE 04-08 LATER GUSTSTable 1: Example input with corresponding outputs by all systems and from the corpus (for 5 Oct 2000).cesses from inputs to outputs, and has no decision-making ability.
A probability distribution over thisbase CFG is estimated from a corpus, and this iswhat enables decisions between alternative gener-ation rules to be made.
The pCRU package permitsthis distribution to be used in one of the follow-ing three modes to drive generation processes: (i)greedy ?
apply only the most likely rule at eachchoice point; (ii) Viterbi ?
apply all expansionrules to each nonterminal to create the generationforest for the input, then do a Viterbi search of thegeneration forest; (iii) greedy roulette-wheel ?
se-lect a rule to expand a nonterminal according toa non-uniform random distribution proportional tothe likelihoods of expansion rules.In addition there are two baseline modes: (i)random ?
where generation rules are randomlyselected at each choice point; and (ii) n-gram ?where all alternatives are generated and the mostlikely is selected according to an n-gram languagemodel (as in HALOGEN).For the simple SUMTIME domain, pCRU gen-erators trained on raw corpora have been shownto perform well (Belz, 2008), but for more com-plex domains it is likely that manually annotatedcorpora will be needed for training the CFG basegenerator.
As this is in addition to the manuallyconstructed CFG base generator, the manual com-ponent in PCFG generator building is potentiallysubstantial.3.3 PSCFG generationSynchronous context-free grammars (SCFGs) areused in machine translation (Chiang, 2006), buthave also been used for simple concept-to-textgeneration (Wong and Mooney, 2007).
The sim-plest form of SCFG can be viewed as a pair of CFGsG1, G2 with paired production rules such that foreach rule in G1 there is a rule in G2 with the sameleft-hand side, and the same non-terminals in theright-hand side.
The order of non-terminals on theRHSs may differ, and each RHS may additionallycontain any terminals in any order.
SCFGs canbe trained from aligned corpora to produce proba-bilistic (or ?weighted?)
SCFGs.An SCFG can equivalently be seen as a singlegrammar G encoding a set of pairs of strings.
Aprobabilistic SCFG is defined by the 6-tuple G =?N ,Te,Tf , L, S, ?
?, where N is a finite set of non-terminals, Te, Tf are finite sets of terminal sym-bols, L is a set of paired production rules, S is astart symbol ?
N , and ?
is a set of parameters thatdefine a probability distribution of derivations un-der G. Each rule in L has the form A ?
??;?
?,where A ?
N , ?
?
N ?
Te+, ?
?
N ?
Tf+, andN ?
N .In MT the two CFGs that make up an SCFG areused to encode (the structure of) the two languageswhich the MT system translates between.
Trans-lation with an SCFG then consists of (i) parsingthe input string with the source language CFG toproduce a derivation tree, and then (ii) generatingalong the same derivation tree, but using the targetlanguage CFG to produce the output string.When using SCFGs for content-to-text genera-tion one of the paired CFGs encodes the meaningrepresentation language, and the other the (natu-ral) language in which text is supposed to be gen-erated.
A generation process then consists in (i)?parsing?
the meaning representation (MR) into itsconstituent structure, and, in the opposite direc-tion, (ii) assembling strings of words correspond-ing to constituent parts of the input MR into a sen-tence or text that realises the entire MR.We used the WASP?1 method (Wong andMooney, 2006; Wong and Mooney, 2007) which18provides a way in which a probabilistic SCFG canbe constructed for the most part automatically.The training process requires two resources as in-put: a CFG of MRs and a set of sentences pairedwith their MRs. As output, it produces a proba-bilistic SCFG.
The training process works in twophases, producing a (non-probabilistic) SCFG inthe ?lexical acquisition phase?, and associating therules with probabilities in the ?parameter estima-tion phase?.The lexical acquisition phase uses the GIZA++word-alignment tool, an implementation (Och andNey, 2003) of IBM Model 5 (Brown et al, 1993)to construct an alignment of MRs with NL strings.An SCFG is then constructed by using the MR CFGas a skeleton and inferring the NL grammar fromthe alignment.For the parameter estimation phase, WASP?1uses a log-linear model from Koehn et al (2003)which defines a conditional probability distribu-tion over derivations d given an input MR f asPr?
(d|f) ?
Pr(e(d))?1?d?dw?
(r(d))where w?
(r(d)) is the weight an individual ruleused in a derivation, defined asw?
(A ?
?e, f?)
=P (f |e)?2P (e|f)?3Pw(f |e)?4Pw(e|f)?5exp(?|?|)?6where P (?|?)
and P (?|?)
are the relative fre-quencies of ?
and ?, Pw(?|?)
and Pw(?|?)
arelexical weights, and exp(?|?|) is a word penaltyto control output sentence length.
The model pa-rameters ?i are trained using minimum error ratetraining.Compared to probabilistic CFGs, WASP?1-trained probabilistic SCFGs have a much reducedmanual component in system building.
In the lat-ter, the NL grammar for the output language, themapping from MRs to word strings and the ruleprobabilities are all created automatically, more-over from raw corpora, whereas in PCFGs, onlythe rule probabilities are created automatically.3.4 SMT methodsA Statistical Machine Translation (SMT) system isessentially composed of a translation model anda language model, where the former translatessource language substrings into target languagesubstrings, and the language model determinesthe most likely linearisation of the translated sub-strings.
The currently most popular phrase-basedSMT (PBSMT) approach translates phrases (an ar-bitrary sequence of words, rather than the lin-guistic sense), whereas the original ?IBM models?translated words.
Different PBSMT methods differin how they construct the phrase translation table.We used the phrase-based translation modelproposed by Koehn et al (2003) and implementedin the MOSES toolkit (Koehn et al, 2007) which isbased on the noisy channel model, where Bayes?srule is used to reformulate the task of translat-ing a source language string f into a target lan-guage string e as finding the sentence e?
such thate?
= argmaxe Pr(e) Pr(f |e).The translation model (which gives Pr(f |e)) isobtained from a parallel corpus of source and tar-get language texts, where the first step is automaticalignment using the GIZA++ word-level aligner.Word-level alignments are used to obtain phrasetranslation pairs using a set of heuristics.A 3-gram language model (which gives Pr(e))for the target language is trained either on thesame or a different corpus.
For full details referto Koehn et al (2003; 2007).PBSMT offers a completely automatic methodfor constructing generators, where all that is re-quired as input to the system building process is acorpus of paired MRs and realisations, on the basisof which the PBSMT approach constructs a map-ping from MSRs to realisations.4 Ten Weather Forecast Text Generators4.1 SUMTIME-HybridWe included the original SUMTIME system (Re-iter et al, 2005) in our evaluations.
Thisrule-based system has two modules: a content-determination module and a microplanning andrealisation module.
It can be run without thecontent-determination module, taking content rep-resentations (tuple sequence as described in Sec-tion 2) as inputs, and is then called SUMTIME-Hybrid.
SUMTIME-Hybrid is a traditional deter-ministic rule-based generation system, and tookabout one year to build.1 Table 1 shows an ex-ample forecast from the SUMTIME system (andcorresponding outputs from the other systems, de-scribed below).1Belz (2008), estimated on the basis of personal commu-nication with E. Reiter and S. Sripada.194.2 PCFG generatorsWe also included five pCRU generators forthe SUMTIME domain created previously (Belz,2008).
The pCRU base generator for SUMTIMEis a set of generation rules with atomic argumentsthat convert an input into a set of NL forecasts.To create inputs to the pCRU generators, the in-put vectors as they appear in the corpus (see Sec-tion 2) are augmented and converted into sequenceof nonterminals: First, information is added toeach of the 7-tuples in an automatic preprocessingphase encoding whether the change in wind direc-tion compared to the preceding 7-tuple was clock-wise or anti-clockwise; whether change in windspeed was an increase or a decrease; and whethera 7-tuple was the last in the vector.
Then, the aug-mented tuples are converted into a representationof nonterminals with 7 arguments.A probability distribution over the base genera-tor was obtained by the multi-treebanking method(Belz, 2008) from the un-annotated SUMTIMEcorpus.
This method first parses the corpus withthe base CFG and then obtains rule-application fre-quency counts from the parsed corpus which areused to obtain a probability distribution by straigh-forward maximum likelihood estimation.
If thereis more than one parse for a sentence then the fre-quency count increment is equally split over rulesin alternative parses.4.3 PSCFG generatorsWe created two probabilistic synchronous CFG(PSCFG) generators for the SUMTIME domain us-ing WASP?1.
The main task here was to createa CFG for wind data representations.
We usedtwo different grammars (resulting in two differentgenerators).
The ?unstructured?
grammar encodesraw corpus input vectors augmented as describedin Section 4.2, whereas the ?semantic?
grammarencodes representations with recursive predicate-argument structure that more resemble semanticforms.
These were produced automatically fromthe raw input vectors.Both the PSCFG-unstructured and the PSCFG-semantic generators were built in the same way,by feeding the CFG for wind data representationsand the corpus of paired wind data representationsand forecasts to WASP?1 which then created prob-abilistic SCFGs from it.System BLEU Homogeneous subsetscorpus 1.00 APCFG-greedy .65 BPSCFG-sem .637 BPSCFG-unstr .617 B CPCFG-viterbi .57 C DPCFG-2gram .561 DPCFG-roule .516 D EPBSMT-unstr .500 ESUMTIME .437 FPBSMT-struc .338 GPCFG-rand .269 HTable 2: Mean forecast-level BLEU scores and ho-mogeneous subsets (Tukey HSD, alpha = .05) forSUMTIME test sets.4.4 PBSMT generatorsWe also created two SUMTIME generators withthe MOSES toolkit.
The main question here washow to represent the ?source language?
inputs.While SMT methods are often applied with no lin-guistic knowledge at all (and are therefore blind asto whether paired inputs and outputs are NL stringsor something else), it was not clear how wellthey would cope with the task of mapping fromnumber/symbol vectors to NL strings.
We testedtwo different input representations, one of whichwas simply the augmented corpus input vectorsas described above (PBSMT-unstructured), and an-other in which the individual 7-tuples of whichthe vectors are composed are explicitly marked bypredicate-argument structure (PBSMT-structured).For comparability with Wong & Mooney (2007)the structure markers were treated as tokens.We built two different generators by feedingthe two different versions of the paired corpus toMOSES.
We did not use a factored translationmodel (the words used in weather forecasts did notvary sufficiently), or tuning.5 Evaluation Methods5.1 Automatic evaluation methodsThe two automatic metrics used in the evaluations,NIST2 and BLEU3, have been shown to correlatewell with expert judgments (Pearson?s r = 0.82and 0.79 respectively) in the SUMTIME domain(Belz and Reiter, 2006).2http://cio.nist.gov/esd/emaildir/lists/mt_list/bin00000.bin3ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl20BLEU-x is an n-gram based string comparisonmeasure, originally proposed by Papineni et al(2001) for evaluation of MT systems.
It computesthe proportion of word n-grams of length x andless that a system output shares with several refer-ence outputs.
Setting x = 4 (i.e.
considering all n-grams of length ?
4) is standard.
NIST (Dodding-ton, 2002) is a version of BLEU, but where BLEUgives equal weight to all n-grams, NIST gives moreimportance to less frequent (hence more informa-tive) n-grams, and the range of NIST scores de-pends on the size of the test set.
Some research hasshown NIST to correlate with human judgmentsmore highly than BLEU (Doddington, 2002; Rie-zler and Maxwell, 2005; Belz and Reiter, 2006).5.2 Human evaluationWe designed an experiment in which participantswere asked to rate forecast texts for Clarity andReadability on scales of 1?7.
Clarity was ex-plained as indicating how understandable a fore-cast was, and Readability as indicating how flu-ent and readable it was.
After an introduction anddetailed explanations, participants carried out theevaluations over the web.
They were able to inter-rupt and resume the evaluation at any time.We randomly selected 22 forecast dates andused outputs from all 10 systems for those dates(as well as the corresponding forecasts in the cor-pus) in the evaluation, i.e.
a total of 242 forecasttexts.
We used a repeated Latin squares designwhere each combination of forecast date and sys-tem is assigned two trials.
As there were 2 eval-uation criteria, there were 968 individual ratingsin this experiment.
An evaluation session startedwith three training examples; the real trials werethen presented in random order.We recruited 22 participants from among ouruniversity colleagues whose first language wasEnglish and who had no experience of NLP.
Wedid not try to recruit master mariners as in earlierexperiments reported by Reiter and Belz (2006),because these experiments also demonstrated thatthe correlation between the ratings by such ex-pert evaluators and lay-people is very strong in theSUMTIME domain (Pearson?s r = 0.845).6 ResultsFor each evaluation method, we carried out a one-way ANOVA with ?System?
as the fixed factor, andthe evaluation measure as the dependent variable.System NIST Homogeneous subsetscorpus 4.062 APCFG-greedy 3.361 BPSCFG-sem 3.303 BPSCFG-unstr 3.191 B CPCFG-roule 3.033 C DPBSMT-unstr 2.924 DPCFG-viterbi 2.854 D EPCFG-2gram 2.854 D ESUMTIME 2.707 E FPCFG-rand 2.540 FPBSMT-struc 2.331 GTable 3: Mean forecast-level NIST scores and ho-mogeneous subsets (Tukey HSD, alpha = .05) forSUMTIME test sets.In each case we report the main effect of Systemon the measure and (if it is significant) we alsoreport significant differences between pairs of sys-tems in the form of homogeneous subsets obtainedwith a post-hoc Tukey HSD analysis.Tables 2 and 3 display the results for the BLEUand NIST evaluations, where scores were cal-culated on test data sets, using a 5-fold cross-validation set-up.
System names (in abbrevi-ated form) are shown in the first column, meanforecast-level scores in the second, and the re-maining columns indicate significant differencesbetween systems.
The way to read the homoge-neous subsets is that two systems which do nothave a letter in common are significantly differentwith p < .05.For the BLEU evaluation, the main effect of Sys-tem on BLEU score was F = 248.274, at p <.001.
PCFG-greedy, PSCFG-semantic and PSCFG-unstructured come out top, although only the firsttwo are significantly better than all other systems.SUMTIME-Hybrid, PBSMT-structured and PCFG-random bring up the rear, with the remaining sys-tems distributed over the middle ground.
A strik-ing result is that the handcrafted SUMTIME sys-tem comes out near the bottom, being signifi-cantly worse than all other systems except PCFG-structured and PBSMT-random.For the NIST evaluation, the main effect of Sys-tem on BLEU score was F = 108.086, at p <.001.
The systems were ranked in the same wayas in the BLEU evaluation except for the systemsin the D subset.
The correlation between the NISTand BLEU scores is Pearson?s r = .739, p < .001,Spearman?s ?
= .748, p < .001.21Scores on data from human evaluationClarity Readability NIST BLEUSUMTIME 6.06 6.18 5.71 0.52PSCFG-semantic 5.79 5.70 6.76 0.65corpus 5.79 5.93 8.45 1PCFG-greedy 5.79 5.63 6.73 0.67PSCFG-unstruc 5.72 5.84 6.61 0.64PCFG-roulette 5.29 5.56 6.07 0.52PCFG-2gram 5.29 5.29 5.23 0.52PCFG-viterbi 4.90 5.34 5.15 0.51PCFG-random 4.43 4.52 4.52 0.25PBSMT-unstruc 3.70 3.93 5.38 0.49PBSMT-struc 2.79 2.77 4.21 0.33Table 4: Mean Clarity and Readability ratingsfrom human evaluation; NIST and BLEU scoreson same 22 forecasts as used in human evaluation.The main results from the automatic evalua-tions are that the two PSCFG systems and the PCFGsystem with the greedy generation algorithm arebest overall.
However, the human evaluations pro-duced rather different results.Figure 3 is a series of bar charts representingthe results of the human evaluation for Clarity.
Foreach system (indicated by the labels on the x-axis),there are 7 bars, showing how many ratings of 1,2, 3, 4, 5, 6 and 7 (7 being the best) a system wasgiven.
So the left-most bar for a system showshow many ratings of 1 a system was given, thesecond bar how many ratings of 2, etc.
Systemsare shown in descending order of mode (the valueof the most frequently assigned rating, e.g.
7 forPSCFG-unstructured on the left, and 1 for PBSMT-structured on the right).
The PSCFG-unstructuredand SUMTIME systems come out top in this eval-uation, with PSCFG-semantic, PCFG-roulette andPCFG-greedy close behind.
Conversely, PBSMT-structured clearly came out worst, with no ratingsof 7 and a mode of 1 (=completely unclear).Figure 4 consists of the same kind of bar charts,for the Readability ratings.
Here the SUMTIMEsystem is the clear winner, with no ratings of 1and 2 and 22 ratings of 7 (=excellent, all partsread well).
It is closely followed by PSCFG-unstructured, the corpus forecasts and PSCFG-semantic.
Again, PBSMT-structured is clearlyworst with no ratings of 7, although this time themode is 3 (=fairly bad).We also looked at the means of the ratings, andthese are shown in the second and third columnsof Table 4.
The means have to be treated withsome caution, because ratings are ordinal dataand it is not clear how meaningful it is to com-pute means.
However, it is a simple way of ob-taining a system ranking for comparison with thetwo automatic scores (shown in the remaining twocolumns of Table 4, for the 22 dates in the humanevaluation only).
In terms of means, SUMTIMEcomes out top for both Clarity and Readability.In Clarity, it is followed by the two PSCFG sys-tems, the corpus files (the only forecasts actuallywritten by humans), and PCFG-greedy which havevirtually the same means.
For Readability, cor-pus and PSCFG-unstructured are ahead of PSCFG-semantic and PCFG-greedy (in this order).
Bring-ing up the rear for both Clarity and Readability, asin the NIST evaluations, is PBSMT-structured, withPCFG-random and and PBSMT-unstructured faringsomewhat better.There are some striking differences betweenthe automatic and human evaluations.
For one,the human evaluators rank the SUMTIME systemvery high, whereas both automatic metrics rankit very low, just above PCFG-random and PBSMT-structured.
Furthermore, the metrics rank PBSMT-unstructured more highly than the human evalua-tors, placing it above the SUMTIME system andin the case of NIST, also above two of the PCFGsystems (Table 3).
The human and the automaticevaluations agree only that the PSCFG systems andPCFG-greedy are equally good.7 ConclusionsReports of research on automating (part of) systembuilding often take it as read that such automationis a good thing.
The resulting systems are not of-ten compared to handcrafted alternatives in termsof output quality or other quality criteria, and littleis therefore known about the loss of system qual-ity that results from automation.
The existence ofseveral independently developed systems for theSUMTIME domain of weather forecasts, to whichwe have added four new systems in the researchreported in this paper, provides a unique opportu-nity to examine the system building cost vs. sys-tem quality trade-off in data-to-text generation.We investigated 10 systems which fall into fourcategories in terms of the manual work involved increating them, ranging from completely manual tocompletely automatic system building.
We foundthat increasing the automatic component in systembuilding from a handcrafted system to an automat-22Figure 3: Clarity ratings: Number of times each system was rated 1, 2, 3, 4, 5, 6, and 7 on Clarity.Systems in descending order of mode (most frequent rating).ically trained but manually crafted generator led toa loss of acceptability to human readers, but an im-provement in terms of n-gram similarity to corpustexts.
Further increasing the automatic componentto the point where only a CFG for meaning repre-sentations is created manually did not result in afurther reduction in quality in either acceptabilityto humans or corpus similarity.
However, com-pletely removing the manual component resultedin a reduction in quality in both human acceptabil-ity and corpus similarity (although this is more ap-parent in the former).We found striking differences between the re-sults from tests of human acceptability and mea-surements of corpus similarity.
Compared to thehuman ratings, the automatic metrics severely un-derestimated the quality of the handcrafted SUM-TIME system, but overestimated the quality ofthe automatically constructed SMT systems.
Thiswill not come as a surprise to those familiar withthe machine translation evaluation literature wherethis is a major complaint about BLEU (Callison-Burch et al, 2006).
From our results it seems clearthat when the quality of diverse types of systems iscompared, automatic metrics such as BLEU do notgive a complete and reliable picture, and carryingout additional evaluations is crucial.Increased reusability and adaptability of sys-tems and components have cost and time bene-fits, and methods for automatically training sys-tems from data offer advantages in both these re-spects.
However, careful evaluation is needed toensure that these advantages are not achieved atthe price of a reduction in system quality that ren-ders systems unacceptable to human users.AcknowledgmentsThe research reported in this paper was supportedunder EPSRC grant EP/E029116/1 (the ProdigyProject).
We thank the anonymous reviewers fortheir helpful comments.ReferencesA.
Belz and E. Reiter.
2006.
Comparing automaticand human evaluation of NLG systems.
In Proceed-ings of the 11th Conference of the European Chap-ter of the Association for Computational Linguistics(EACL?06), pages 313?320.A.
Belz.
2008.
Automatic generation of weatherforecast texts using comprehensive probabilisticgeneration-space models.
Natural Language Engi-neering, 14(4):431?455.P.
F. Brown, V. J. Della Pietra, S. A. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: parameter estimation.
Compu-tational Linguistics, 19(2):263?311.D.
Chiang.
2006.
An introduction to synchronousgrammars (part of the course materials for theACL?06 tutorial on synchronous grammars).G.
Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-23Figure 4: Readability ratings: Number of times each system was rated 1, 2, 3, 4, 5, 6, and 7 on Readabil-ity.
Systems in descending order of mode (most frequent rating).occurrence statistics.
In Proceedings of the ARPAWorkshop on Human Language Technology.K.
Knight and I. Langkilde.
1998.
Generation thatexploits corpus-based statistical knowledge.
In Pro-ceedings of the 36th Annual Meeting of the Associ-ation for Computational Linguistics and 17th Inter-national Conference on Computational Linguistics(COLING-ACL?98), pages 704?710.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisti-cal phrase-based translation.
In Proceedings of Hu-man Language Technologies: The Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics on Human Lan-guage Technology (HLT-NAACL?03), pages 48?54.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Proceedings ofthe 45th Annual Meeting of the Association for Com-putational Linguistics (ACL?07), pages 177?180.I.
Langkilde.
2000.
Forest-based statistical sen-tence generation.
In Proceedings of the 6th AppliedNatural Language Processing Conference and the1st Meeting of the North American Chapter of theAssociation of Computational Linguistics (ANLP-NAACL ?00), pages 170?177.F.
J. Och and H. Ney.
2003.
A Systematic Comparisonof Various Statistical Alignment Models.
Computa-tional Linguistics, 29(1):19?51.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2001.BLEU: A method for automatic evaluation of ma-chine translation.
IBM research report, IBM Re-search Division.E.
Reiter and R. Dale.
1997.
Building applied natu-ral language generation systems.
Natural LangaugeEngineering, 3(1):57?87.E.
Reiter, S. Sripada, J.
Hunter, and J. Yu.
2005.Choosing words in computer-generated weatherforecasts.
Artificial Intelligence, 167:137?169.S.
Riezler and J. T. Maxwell.
2005.
On some pitfallsin automatic evaluation and significance testing forMT.
In Proceedings of the ACL?05 Workshop onIntrinsic and Extrinsic Evaluation Measures for MTand/or Summarization, pages 57?64.S.
Sripada, E. Reiter, J.
Hunter, and J. Yu.
2002.SUMTIME-METEO: A parallel corpus of naturallyoccurring forecast texts and weather data.
TechnicalReport AUCS/TR0201, Computing Science Depart-ment, University of Aberdeen.Y.
W. Wong and R. Mooney.
2006.
Learning forsemantic parsing with statistical machine transla-tion.
In Proceedings of Human Language Technolo-gies: The Annual Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics on Human Language Technology (HLT-NAACL?06), pages 439?446.Y.
W. Wong and R.J. Mooney.
2007.
Generationby inverting a semantic parser that uses statisti-cal machine translation.
In Proceedings of HumanLanguage Technologies: The Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology (HLT-NAACL?07), pages 172?179.24
