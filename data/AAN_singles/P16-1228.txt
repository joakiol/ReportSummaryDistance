Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2410?2420,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsHarnessing Deep Neural Networks with Logic RulesZhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, Eric P. XingSchool of Computer ScienceCarnegie Mellon University{zhitingh,xuezhem,liu,epxing}@cs.cmu.edu, hovy@cmu.eduAbstractCombining deep neural networks withstructured logic rules is desirable to harnessflexibility and reduce uninterpretability ofthe neural models.
We propose a generalframework capable of enhancing varioustypes of neural networks (e.g., CNNs andRNNs) with declarative first-order logicrules.
Specifically, we develop an iterativedistillation method that transfers the struc-tured information of logic rules into theweights of neural networks.
We deploy theframework on a CNN for sentiment anal-ysis, and an RNN for named entity recog-nition.
With a few highly intuitive rules,we obtain substantial improvements andachieve state-of-the-art or comparable re-sults to previous best-performing systems.1 IntroductionDeep neural networks provide a powerful mech-anism for learning patterns from massive data,achieving new levels of performance on imageclassification (Krizhevsky et al, 2012), speechrecognition (Hinton et al, 2012), machine trans-lation (Bahdanau et al, 2014), playing strategicboard games (Silver et al, 2016), and so forth.Despite the impressive advances, the widely-used DNN methods still have limitations.
Thehigh predictive accuracy has heavily relied on largeamounts of labeled data; and the purely data-drivenlearning can lead to uninterpretable and some-times counter-intuitive results (Szegedy et al, 2014;Nguyen et al, 2015).
It is also difficult to encodehuman intention to guide the models to capture de-sired patterns, without expensive direct supervisionor ad-hoc initialization.On the other hand, the cognitive process of hu-man beings have indicated that people learn notonly from concrete examples (as DNNs do) butalso from different forms of general knowledgeand rich experiences (Minksy, 1980; Lake et al,2015).
Logic rules provide a flexible declarativelanguage for communicating high-level cognitionand expressing structured knowledge.
It is there-fore desirable to integrate logic rules into DNNs, totransfer human intention and domain knowledge toneural models, and regulate the learning process.In this paper, we present a framework capable ofenhancing general types of neural networks, suchas convolutional networks (CNNs) and recurrentnetworks (RNNs), on various tasks, with logic ruleknowledge.
Combining symbolic representationswith neural methods have been considered in dif-ferent contexts.
Neural-symbolic systems (Garcezet al, 2012) construct a network from a given ruleset to execute reasoning.
To exploit a priori knowl-edge in general neural architectures, recent workaugments each raw data instance with useful fea-tures (Collobert et al, 2011), while network train-ing, however, is still limited to instance-label super-vision and suffers from the same issues mentionedabove.
Besides, a large variety of structural knowl-edge cannot be naturally encoded in the feature-label form.Our framework enables a neural network to learnsimultaneously from labeled instances as well aslogic rules, through an iterative rule knowledgedistillation procedure that transfers the structuredinformation encoded in the logic rules into the net-work parameters.
Since the general logic rulesare complementary to the specific data labels, anatural ?side-product?
of the integration is the sup-port for semi-supervised learning where unlabeleddata is used to better absorb the logical knowledge.Methodologically, our approach can be seen as acombination of the knowledge distillation (Hintonet al, 2015; Bucilu et al, 2006) and the posteriorregularization (PR) method (Ganchev et al, 2010).2410In particular, at each iteration we adapt the pos-terior constraint principle from PR to construct arule-regularized teacher, and train the student net-work of interest to imitate the predictions of theteacher network.
We leverage soft logic to supportflexible rule encoding.We apply the proposed framework on both CNNand RNN, and deploy on the task of sentimentanalysis (SA) and named entity recognition (NER),respectively.
With only a few (one or two) veryintuitive rules, both the distilled networks and thejoint teacher networks strongly improve over theirbasic forms (without rules), and achieve better orcomparable performance to state-of-the-art modelswhich typically have more parameters and compli-cated architectures.To the best of our knowledge, this is thefirst work to integrate logic rules with generalworkhorse types of deep neural networks in a prin-cipled framework.
The encouraging results indi-cate our method can be potentially useful for in-corporating richer types of human knowledge, andimproving other application domains.2 Related WorkCombination of logic rules and neural networkshas been considered in different contexts.
Neural-symbolic systems (Garcez et al, 2012), such asKBANN (Towell et al, 1990) and CILP++ (Franc?aet al, 2014), construct network architectures fromgiven rules to perform reasoning and knowledgeacquisition.
A related line of research, such asMarkov logic networks (Richardson and Domin-gos, 2006), derives probabilistic graphical models(rather than neural networks) from the rule set.With the recent success of deep neural networksin a vast variety of application domains, it is in-creasingly desirable to incorporate structured logicknowledge into general types of networks to har-ness flexibility and reduce uninterpretability.
Re-cent work that trains on extra features from do-main knowledge (Collobert et al, 2011), whileproducing improved results, does not go beyondthe data-label paradigm.
Kulkarni et al (2015) usesa specialized training procedure with careful order-ing of training instances to obtain an interpretableneural layer of an image network.
Karaletsos etal.
(2016) develops a generative model jointly overdata-labels and similarity knowledge expressed intriplet format to learn improved disentangled repre-sentations.Though there do exist general frameworks thatallow encoding various structured constraints onlatent variable models (Ganchev et al, 2010; Zhuet al, 2014; Liang et al, 2009), they either arenot directly applicable to the NN case, or couldyield inferior performance as in our empirical study.Liang et al (2008) transfers predictive power ofpre-trained structured models to unstructured onesin a pipelined fashion.Our proposed approach is distinct in that we usean iterative rule distillation process to effectivelytransfer rich structured knowledge, expressed inthe declarative first-order logic language, into pa-rameters of general neural networks.
We showthat the proposed approach strongly outperformsan extensive array of other either ad-hoc or generalintegration methods.3 MethodIn this section we present our framework which en-capsulates the logical structured knowledge intoa neural network.
This is achieved by forcingthe network to emulate the predictions of a rule-regularized teacher, and evolving both models it-eratively throughout training (section 3.2).
Theprocess is agnostic to the network architecture, andthus applicable to general types of neural models in-cluding CNNs and RNNs.
We construct the teachernetwork in each iteration by adapting the posteriorregularization principle in our logical constraintsetting (section 3.3), where our formulation pro-vides a closed-form solution.
Figure 1 shows anoverview of the proposed framework.losslabeled datalogic rules?(?|?)??
(?|?)
projectionunlabeled datateacher network construction rule knowledge distillationbackpropagationteacher?(?|?)
student ??
(?|?
)Figure 1: Framework Overview.
At each iteration,the teacher network is obtained by projecting thestudent network to a rule-regularized subspace (reddashed arrow); and the student network is updatedto balance between emulating the teacher?s outputand predicting the true labels (black/blue solid ar-rows).24113.1 Learning Resources: Instances and RulesOur approach allows neural networks to learn fromboth specific examples and general rules.
Here wegive the settings of these ?learning resources?.Assume we have input variable x ?
X andtarget variable y ?
Y .
For clarity, we focuson K-way classification, where Y = ?Kisthe K-dimensional probability simplex and y ?
{0, 1}K?
Y is a one-hot encoding of the classlabel.
However, our method specification canstraightforwardly be applied to other contexts suchas regression and sequence learning (e.g., NERtagging, which is a sequence of classification deci-sions).
The training data D = {(xn,yn)}Nn=1is aset of instantiations of (x,y).Further consider a set of first-order logic(FOL) rules with confidences, denoted as R ={(Rl, ?l)}Ll=1, where Rlis the lth rule over theinput-target space (X ,Y), and ?l?
[0,?]
is theconfidence level with ?l= ?
indicating a hardrule, i.e., all groundings are required to be true(=1).
Here a grounding is the logic expressionwith all variables being instantiated.
Given a setof examples (X,Y ) ?
(X ,Y) (e.g., a minibatchfrom D), the set of groundings of Rlare denotedas {rlg(X,Y )}Glg=1.
In practice a rule groundingis typically relevant to only a single or subset ofexamples, though here we give the most generalform on the entire set.We encode the FOL rules using soft logic (Bachet al, 2015) for flexible encoding and stable opti-mization.
Specifically, soft logic allows continu-ous truth values from the interval [0, 1] instead of{0, 1}, and the Boolean logic operators are refor-mulated as:A&B = max{A+B ?
1, 0}A ?B = min{A+B, 1}A1?
?
?
?
?AN=?iAi/N?A = 1?A(1)Here & and ?
are two different approximationsto logical conjunction (Foulds et al, 2015): & isuseful as a selection operator (e.g., A&B = Bwhen A = 1, and A&B = 0 when A = 0), while?
is an averaging operator.3.2 Rule Knowledge DistillationA neural network defines a conditional probabil-ity p?
(y|x) by using a softmax output layer thatproduces a K-dimensional soft prediction vectordenoted as ??(x).
The network is parameterizedby weights ?.
Standard neural network traininghas been to iteratively update ?
to produce thecorrect labels of training instances.
To integratethe information encoded in the rules, we proposeto train the network to also imitate the outputsof a rule-regularized projection of p?
(y|x), de-noted as q(y|x), which explicitly includes rule con-straints as regularization terms.
In each iterationq is constructed by projecting p?into a subspaceconstrained by the rules, and thus has desirableproperties.
We present the construction in the nextsection.
The prediction behavior of q reveals theinformation of the regularized subspace and struc-tured rules.
Emulating the q outputs serves to trans-fer this knowledge into p?.
The new objective isthen formulated as a balancing between imitatingthe soft predictions of q and predicting the true hardlabels:?
(t+1) = argmin???1NN?n=1(1?
pi)`(yn,??
(xn))+ pi`(s(t)n,??
(xn)),(2)where ` denotes the loss function selected accord-ing to specific applications (e.g., the cross entropyloss for classification); s(t)nis the soft predictionvector of q on xnat iteration t; and pi is the imita-tion parameter calibrating the relative importanceof the two objectives.A similar imitation procedure has been used inother settings such as model compression (Buciluet al, 2006; Hinton et al, 2015) where the pro-cess is termed distillation.
Following them we callp?
(y|x) the ?student?
and q(y|x) the ?teacher?,which can be intuitively explained in analogousto human education where a teacher is aware ofsystematic general rules and she instructs studentsby providing her solutions to particular questions(i.e., the soft predictions).
An important differ-ence from previous distillation work, where theteacher is obtained beforehand and the student istrained thereafter, is that our teacher and studentare learned simultaneously during training.Though it is possible to combine a neural net-work with rule constraints by projecting the net-work to the rule-regularized subspace after it isfully trained as before with only data-label in-stances, or by optimizing projected network di-rectly, we found our iterative teacher-student dis-tillation approach provides a much superior per-formance, as shown in the experiments.
More-over, since p?distills the rule information into the2412weights ?
instead of relying on explicit rule rep-resentations, we can use p?for predicting new ex-amples at test time when the rule assessment isexpensive or even unavailable (i.e., the privilegedinformation setting (Lopez-Paz et al, 2016)) whilestill enjoying the benefit of integration.
Besides,the second loss term in Eq.
(2) can be augmentedwith rich unlabeled data in addition to the labeledexamples, which enables semi-supervised learningfor better absorbing the rule knowledge.3.3 Teacher Network ConstructionWe now proceed to construct the teacher networkq(y|x) at each iteration from p?(y|x).
The itera-tion index t is omitted for clarity.
We adapt theposterior regularization principle in our logic con-straint setting.
Our formulation ensures a closed-form solution for q and thus avoids any significantincreases in computational overhead.Recall the set of FOL rules R = {(Rl, ?l)}Ll=1.Our goal is to find the optimal q that fits the ruleswhile at the same time staying close to p?.
For thefirst property, we apply a commonly-used strategythat imposes the rule constraints on q through anexpectation operator.
That is, for each rule (indexedby l) and each of its groundings (indexed by g)on (X,Y ), we expect Eq(Y |X)[rlg(X,Y )] = 1,with confidence ?l.
The constraints define a rule-regularized space of all valid distributions.
For thesecond property, we measure the closeness betweenq and p?with KL-divergence, and wish to minimizeit.
Combining the two factors together and furtherallowing slackness for the constraints, we finallyget the following optimization problem:minq,?
?0KL(q(Y |X)?p?
(Y |X)) + C?l,gl?l,gls.t.
?l(1?
Eq[rl,gl(X,Y )]) ?
?l,glgl= 1, .
.
.
, Gl, l = 1, .
.
.
, L,(3)where ?l,gl?
0 is the slack variable for respec-tive logic constraint; and C is the regularizationparameter.
The problem can be seen as project-ing p?into the constrained subspace.
The problemis convex and can be efficiently solved in its dualform with closed-form solutions.
We provide thedetailed derivation in the supplementary materialsand directly give the solution here:q?
(Y |X) ?
p?
(Y |X) exp?????l,glC?l(1?
rl,gl(X,Y ))???
(4)Intuitively, a strong rule with large ?lwill lead tolow probabilities of predictions that fail to meetthe constraints.
We discuss the computation of thenormalization factor in section 3.4.Our framework is related to the posterior regular-ization (PR) method (Ganchev et al, 2010) whichplaces constraints over model posterior in unsuper-vised setting.
In classification, our optimizationprocedure is analogous to the modified EM algo-rithm for PR, by using cross-entropy loss in Eq.
(2)and evaluating the second loss term on unlabeleddata differing from D, so that Eq.
(4) correspondsto the E-step and Eq.
(2) is analogous to the M-step.This sheds light from another perspective on whyour framework would work.
However, we found inour experiments (section 5) that to produce strongperformance it is crucial to use the same labeleddata xnin the two losses of Eq.
(2) so as to form adirect trade-off between imitating soft predictionsand predicting correct hard labels.3.4 ImplementationsThe procedure of iterative distilling optimizationof our framework is summarized in Algorithm 1.During training we need to compute the softpredictions of q at each iteration, which is straight-forward through direct enumeration if the rule con-straints in Eq.
(4) are factored in the same way asthe base neural model p?
(e.g., the ?but?-rule ofsentiment classification in section 4.1).
If the con-straints introduce additional dependencies, e.g., bi-gram dependency as the transition rule in the NERtask (section 4.2), we can use dynamic program-ming for efficient computation.
For higher-orderconstraints (e.g., the listing rule in NER), we ap-proximate through Gibbs sampling that iterativelysamples from q(yi|y?i,x) for each position i. Ifthe constraints span multiple instances, we groupthe relevant instances in minibatches for joint in-ference (and randomly break some dependencieswhen a group is too large).
Note that calculatingthe soft predictions is efficient since only one NNforward pass is required to compute the base dis-tribution p?
(y|x) (and few more, if needed, forcalculating the truth values of relevant rules).p v.s.
q at Test Time At test time we can useeither the distilled student network p, or the teachernetwork q after a final projection.
Our empirical re-sults show that both models substantially improveover the base network that is trained with only data-label instances.
In general q performs better thanp.
Particularly, q is more suitable when the logicrules introduce additional dependencies (e.g., span-2413Algorithm 1 Harnessing NN with RulesInput: The training data D = {(xn,yn)}Nn=1,The rule setR = {(Rl, ?l)}Ll=1,Parameters: pi ?
imitation parameterC ?
regularization strength1: Initialize neural network parameter ?2: repeat3: Sample a minibatch (X,Y ) ?
D4: Construct teacher network q with Eq.
(4)5: Transfer knowledge into p?by updating ?
with Eq.
(2)6: until convergenceOutput: Distill student network p?and teacher network qning over multiple examples), requiring joint infer-ence.
In contrast, as mentioned above, p is morelightweight and efficient, and useful when rule eval-uation is expensive or impossible at prediction time.Our experiments compare the performance of p andq extensively.Imitation Strength pi The imitation parameter piin Eq.
(2) balances between emulating the teachersoft predictions and predicting the true hard la-bels.
Since the teacher network is constructed fromp?, which, at the beginning of training, would pro-duce low-quality predictions, we thus favor pre-dicting the true labels more at initial stage.
Astraining goes on, we gradually bias towards emu-lating the teacher predictions to effectively distillthe structured knowledge.
Specifically, we definepi(t)= min{pi0, 1 ?
?t} at iteration t ?
0, where?
?
1 specifies the speed of decay and pi0< 1 is alower bound.4 ApplicationsWe have presented our framework that is generalenough to improve various types of neural networkswith rules, and easy to use in that users are allowedto impose their knowledge and intentions throughthe declarative first-order logic.
In this sectionwe illustrate the versatility of our approach by ap-plying it on two workhorse network architectures,i.e., convolutional network and recurrent network,on two representative applications, i.e., sentence-level sentiment analysis which is a classificationproblem, and named entity recognition which is asequence learning problem.For each task, we first briefly describe the baseneural network.
Since we are not focusing ontuning network architectures, we largely use thesame or similar networks to previous successfulneural models.
We then design the linguistically-motivated rules to be integrated.I like this book store a lot PaddingPaddingWordEmbeddingConvolutionMax PoolingSentenceRepresentationFigure 2: The CNN architecture for sentence-levelsentiment analysis.
The sentence representationvector is followed by a fully-connected layer withsoftmax output activation, to output sentiment pre-dictions.4.1 Sentiment ClassificationSentence-level sentiment analysis is to identify thesentiment (e.g., positive or negative) underlyingan individual sentence.
The task is crucial formany opinion mining applications.
One challeng-ing point of the task is to capture the contrastivesense (e.g., by conjunction ?but?)
within a sen-tence.Base Network We use the single-channel convo-lutional network proposed in (Kim, 2014).
The sim-ple model has achieved compelling performanceon various sentiment classification benchmarks.The network contains a convolutional layer on topof word vectors of a given sentence, followed bya max-over-time pooling layer and then a fully-connected layer with softmax output activation.
Aconvolution operation is to apply a filter to wordwindows.
Multiple filters with varying windowsizes are used to obtain multiple features.
Figure 2shows the network architecture.Logic Rules One difficulty for the plain neuralnetwork is to identify contrastive sense in order tocapture the dominant sentiment precisely.
The con-junction word ?but?
is one of the strong indicatorsfor such sentiment changes in a sentence, wherethe sentiment of clauses following ?but?
generallydominates.
We thus consider sentences S with an?A-but-B?
structure, and expect the sentiment of thewhole sentence to be consistent with the sentimentof clause B.
The logic rule is written as:has-?A-but-B?-structure(S)?
(1(y = +)?
??(B)+?
??(B)+?
1(y = +)) ,(5)2414where 1(?)
is an indicator function that takes 1when its argument is true, and 0 otherwise; class ?+?represents ?positive?
; and ??
(B)+is the element of??
(B) for class ?+?.
By Eq.
(1), when S has the ?A-but-B?
structure, the truth value of the above logicrule equals to (1 + ??
(B)+)/2 when y = +, and(2 ?
??
(B)+)/2 otherwise1.
Note that here weassume two-way classification (i.e., positive andnegative), though it is straightforward to designrules for finer grained sentiment classification.4.2 Named Entity RecognitionNER is to locate and classify elements in text intoentity categories such as ?persons?
and ?organiza-tions?.
It is an essential first step for downstreamlanguage understanding applications.
The task as-signs to each word a named entity tag in an ?X-Y?format where X is one of BIEOS (Beginning, In-side, End, Outside, and Singleton) and Y is theentity category.
A valid tag sequence has to followcertain constraints by the definition of the taggingscheme.
Besides, text with structures (e.g., lists)within or across sentences can usually expose someconsistency patterns.Base Network The base network has a similararchitecture with the bi-directional LSTM recur-rent network (called BLSTM-CNN) proposed in(Chiu and Nichols, 2015) for NER which has out-performed most of previous neural models.
Themodel uses a CNN and pre-trained word vectorsto capture character- and word-level information,respectively.
These features are then fed into abi-directional RNN with LSTM units for sequencetagging.
Compared to (Chiu and Nichols, 2015) weomit the character type and capitalization features,as well as the additive transition matrix in the out-put layer.
Figure 3 shows the network architecture.Logic Rules The base network largely makes in-dependent tagging decisions at each position, ignor-ing the constraints on successive labels for a validtag sequence (e.g., I-ORG cannot follow B-PER).In contrast to recent work (Lample et al, 2016)which adds a conditional random field (CRF) tocapture bi-gram dependencies between outputs, weinstead apply logic rules which does not introduceextra parameters to learn.
An example rule is:equal(yi?1, I-ORG)?
?
equal(yi,B-PER) (6)1Replacing ?
with & in Eq.
(5) leads to a probably moreintuitive rule which takes the value ??
(B)+when y = +,and 1?
??
(B)+otherwise.Char+ WordRepresentationB ackw ardL ST MF orw ardL ST ML ST M L ST M L ST M L ST ML ST M L ST M L ST M L ST MO utputRepresentationN Y C locates in U SAFigure 3: The architecture of the bidirectionalLSTM recurrent network for NER.
The CNN forextracting character representation is omitted.The confidence levels are set to?
to prevent anyviolation.We further leverage the list structures within andacross sentences of the same documents.
Specifi-cally, named entities at corresponding positions ina list are likely to be in the same categories.
Forinstance, in ?1.
Juventus, 2.
Barcelona, 3.
...?
weknow ?Barcelona?
must be an organization ratherthan a location, since its counterpart entity ?Juven-tus?
is an organization.
We describe our simpleprocedure for identifying lists and counterparts inthe supplementary materials.
The logic rule is en-coded as:is-counterpart(X,A)?
1?
?c(ey)?
c(??
(A))?2, (7)where eyis the one-hot encoding of y (the class pre-diction of X); c(?)
collapses the probability masson the labels with the same categories into a singleprobability, yielding a vector with length equalingto the number of categories.
We use `2distanceas a measure for the closeness between predictionsof X and its counterpart A.
Note that the distancetakes value in [0, 1] which is a proper soft truthvalue.
The list rule can span multiple sentences(within the same document).
We found the teachernetwork q that enables explicit joint inference pro-vides much better performance over the distilledstudent network p (section 5).5 ExperimentsWe validate our framework by evaluating its appli-cations of sentiment classification and named en-tity recognition on a variety of public benchmarks.By integrating the simple yet effective rules with2415Model SST2 MR CR1 CNN (Kim, 2014) 87.2 81.3?0.1 84.3?0.22 CNN-Rule-p 88.8 81.6?0.1 85.0?0.33 CNN-Rule-q 89.3 81.7?0.1 85.3?0.34 MGNC-CNN (Zhang et al, 2016) 88.4 ?
?5 MVCNN (Yin and Schutze, 2015) 89.4 ?
?6 CNN-multichannel (Kim, 2014) 88.1 81.1 85.07 Paragraph-Vec (Le and Mikolov, 2014) 87.8 ?
?8 CRF-PR (Yang and Cardie, 2014) ?
?
82.79 RNTN (Socher et al, 2013) 85.4 ?
?10 G-Dropout (Wang and Manning, 2013) ?
79.0 82.1Table 1: Accuracy (%) of Sentiment Classification.
Row 1, CNN (Kim, 2014) is the base networkcorresponding to the ?CNN-non-static?
model in (Kim, 2014).
Rows 2-3 are the networks enhanced byour framework: CNN-Rule-p is the student network and CNN-Rule-q is the teacher network.
For MR andCR, we report the average accuracy?one standard deviation using 10-fold cross validation.the base networks, we obtain substantial improve-ments on both tasks and achieve state-of-the-artor comparable results to previous best-performingsystems.
Comparison with a diverse set of otherrule integration methods demonstrates the uniqueeffectiveness of our framework.
Our approach alsoshows promising potentials in the semi-supervisedlearning and sparse data context.Throughout the experiments we set the regular-ization parameter to C = 400.
In sentiment clas-sification we set the imitation parameter to pi(t)=1?
0.9t, while in NER pi(t)= min{0.9, 1?
0.9t}to downplay the noisy listing rule.
The confidencelevels of rules are set to ?l= 1, except for hardconstraints whose confidence is ?.
For neuralnetwork configuration, we largely followed the ref-erence work, as specified in the following respec-tive sections.
All experiments were performed ona Linux machine with eight 4.0GHz CPU cores,one Tesla K40c GPU, and 32GB RAM.
We imple-mented neural networks using Theano2, a populardeep learning platform.5.1 Sentiment Classification5.1.1 SetupWe test our method on a number of commonlyused benchmarks, including 1) SST2, StanfordSentiment Treebank (Socher et al, 2013) whichcontains 2 classes (negative and positive), and6920/872/1821 sentences in the train/dev/test setsrespectively.
Following (Kim, 2014) we train mod-els on both sentences and phrases since all labelsare provided.
2) MR (Pang and Lee, 2005), a set of10,662 one-sentence movie reviews with negative2http://deeplearning.net/software/theanoor positive sentiment.
3) CR (Hu and Liu, 2004),customer reviews of various products, containing 2classes and 3,775 instances.
For MR and CR, weuse 10-fold cross validation as in previous work.
Ineach of the three datasets, around 15% sentencescontains the word ?but?.For the base neural network we use the ?non-static?
version in (Kim, 2014) with the exact sameconfigurations.
Specifically, word vectors are ini-tialized using word2vec (Mikolov et al, 2013) andfine-tuned throughout training, and the neural pa-rameters are trained using SGD with the Adadeltaupdate rule (Zeiler, 2012).5.1.2 ResultsTable 1 shows the sentiment classification per-formance.
Rows 1-3 compare the base neuralmodel with the models enhanced by our frame-work with the ?but?-rule (Eq.(5)).
We see thatour method provides a strong boost on accuracyover all three datasets.
The teacher network q fur-ther improves over the student network p, thoughthe student network is more widely applicablein certain contexts as discussed in sections 3.2and 3.4.
Rows 4-10 show the accuracy of re-cent top-performing methods.
On the MR and CRdatasets, our model outperforms all the baselines.On SST2, MVCNN (Yin and Schutze, 2015) (Row5) is the only system that shows a slightly better re-sult than ours.
Their neural network has combineddiverse sets of pre-trained word embeddings (whilewe use only word2vec) and contained more neurallayers and parameters than our model.To further investigate the effectiveness of ourframework in integrating structured rule knowl-edge, we compare with an extensive array of other2416Model Accuracy (%)1 CNN (Kim, 2014) 87.22 -but-clause 87.33 -`2-reg 87.54 -project 87.95 -opt-project 88.36 -pipeline 87.97 -Rule-p 88.88 -Rule-q 89.3Table 2: Performance of different rule integrationmethods on SST2.
1) CNN is the base network; 2)?-but-clause?
takes the clause after ?but?
as input; 3)?-`2-reg?
imposes a regularization term ????
(S) ???
(Y )?2to the CNN objective, with the strength?
selected on dev set; 4) ?-project?
projects thetrained base CNN to the rule-regularized subspacewith Eq.
(3); 5) ?-opt-project?
directly optimizes theprojected CNN; 6) ?-pipeline?
distills the pre-trained?-opt-project?
to a plain CNN; 7-8) ?-Rule-p?
and ?-Rule-q?
are our models with p being the distilled stu-dent network and q the teacher network.
Note that?-but-clause?
and ?-`2-reg?
are ad-hoc methods ap-plicable specifically to the ?but?-rule.possible integration approaches.
Table 2 lists thesemethods and their performance on the SST2 task.We see that: 1) Although all methods lead to differ-ent degrees of improvement, our framework outper-forms all other competitors with a large margin.
2)In particular, compared to the pipelined method inRow 6 which is in analogous to the structure com-pilation work (Liang et al, 2008), our iterative dis-tillation (section 3.2) provides better performance.Another advantage of our method is that we onlytrain one set of neural parameters, as opposed totwo separate sets as in the pipelined approach.
3)The distilled student network ?-Rule-p?
achievesmuch superior accuracy compared to the base CNN,as well as ?-project?
and ?-opt-project?
which ex-plicitly project CNN to the rule-constrained sub-space.
This validates that our distillation proceduretransfers the structured knowledge into the neu-ral parameters effectively.
The inferior accuracyof ?-opt-project?
can be partially attributed to thepoor performance of its neural network part whichachieves only 85.1% accuracy and leads to inaccu-rate evaluation of the ?but?-rule in Eq.
(5).We next explore the performance of our frame-work with varying numbers of labeled instances aswell as the effect of exploiting unlabeled data.
In-tuitively, with less labeled examples we expect theData size 5% 10% 30% 100%1 CNN 79.9 81.6 83.6 87.22 -Rule-p 81.5 83.2 84.5 88.83 -Rule-q 82.5 83.9 85.6 89.34 -semi-PR 81.5 83.1 84.6 ?5 -semi-Rule-p 81.7 83.3 84.7 ?6 -semi-Rule-q 82.7 84.2 85.7 ?Table 3: Accuracy (%) on SST2 with varying sizesof labeled data and semi-supervised learning.
Theheader row is the percentage of labeled examplesfor training.
Rows 1-3 use only the supervised data.Rows 4-6 use semi-supervised learning where the re-maining training data are used as unlabeled exam-ples.
For ?-semi-PR?
we only report its projectedsolution (in analogous to q) which performs betterthan the non-projected one (in analogous to p).general rules would contribute more to the perfor-mance, and unlabeled data should help better learnfrom the rules.
This can be a useful property espe-cially when data are sparse and labels are expensiveto obtain.
Table 3 shows the results.
The subsam-pling is conducted on the sentence level.
That is,for instance, in ?5%?
we first selected 5% trainingsentences uniformly at random, then trained themodels on these sentences as well as their phrases.The results verify our expectations.
1) Rows 1-3give the accuracy of using only data-label subsetsfor training.
In every setting our methods consis-tently outperform the base CNN.
2) ?-Rule-q?
pro-vides higher improvement on 5% data (with margin2.6%) than on larger data (e.g., 2.3% on 10% data,and 2.0% on 30% data), showing promising po-tential in the sparse data context.
3) By addingunlabeled instances for semi-supervised learningas in Rows 5-6, we get further improved accuracy.4) Row 4, ?-semi-PR?
is the posterior regulariza-tion (Ganchev et al, 2010) which imposes the ruleconstraint through only unlabeled data during train-ing.
Our distillation framework consistently pro-vides substantially better results.5.2 Named Entity Recognition5.2.1 SetupWe evaluate on the well-established CoNLL-2003NER benchmark (Tjong Kim Sang and De Meul-der, 2003), which contains 14,987/3,466/3,684sentences and 204,567/51,578/46,666 tokens intrain/dev/test sets, respectively.
The dataset in-cludes 4 categories, i.e., person, location, orga-nization, and misc.
BIOES tagging scheme is used.2417Model F11 BLSTM 89.552 BLSTM-Rule-trans p: 89.80, q: 91.113 BLSTM-Rules p: 89.93, q: 91.184 NN-lex (Collobert et al, 2011) 89.595 S-LSTM (Lample et al, 2016) 90.336 BLSTM-lex (Chiu and Nichols, 2015) 90.777 BLSTM-CRF1(Lample et al, 2016) 90.948 Joint-NER-EL (Luo et al, 2015) 91.209 BLSTM-CRF2(Ma and Hovy, 2016) 91.21Table 4: Performance of NER on CoNLL-2003.Row 2, BLSTM-Rule-trans imposes the transitionrules (Eq.
(6)) on the base BLSTM.
Row 3, BLSTM-Rules further incorporates the list rule (Eq.(7)).
Wereport the performance of both the student model pand the teacher model q.Around 1.7% named entities occur in lists.We use the mostly same configurations for thebase BLSTM network as in (Chiu and Nichols,2015), except that, besides the slight architecturedifference (section 4.2), we apply Adadelta for pa-rameter updating.
GloVe (Pennington et al, 2014)word vectors are used to initialize word features.5.2.2 ResultsTable 4 presents the performance on the NER task.By incorporating the bi-gram transition rules (Row2), the joint teacher model q achieves 1.56 improve-ment in F1 score that outperforms most previousneural based methods (Rows 4-7), including theBLSTM-CRF model (Lample et al, 2016) whichapplies a conditional random field (CRF) on topof a BLSTM in order to capture the transition pat-terns and encourage valid sequences.
In contrast,our method implements the desired constraints in amore straightforward way by using the declarativelogic rule language, and at the same time does notintroduce extra model parameters to learn.
Furtherintegration of the list rule (Row 3) provides a sec-ond boost in performance, achieving an F1 scorevery close to the best-performing systems includingJoint-NER-EL (Luo et al, 2015) (Row 8), a proba-bilistic graphical model optimizing NER and entitylinking jointly with massive external resources, andBLSTM-CRF (Ma and Hovy, 2016), a combinationof BLSTM and CRF with more parameters thanour rule-enhanced neural networks.From the table we see that the accuracy gap be-tween the joint teacher model q and the distilledstudent p is relatively larger than in the sentimentclassification task (Table 1).
This is because in theNER task we have used logic rules that introduceextra dependencies between adjacent tag positionsas well as multiple instances, making the explicitjoint inference of q useful for fulfilling these struc-tured constraints.6 Discussion and Future WorkWe have developed a framework which combinesdeep neural networks with first-order logic rulesto allow integrating human knowledge and inten-tions into the neural models.
In particular, we pro-posed an iterative distillation procedure that trans-fers the structured information of logic rules intothe weights of neural networks.
The transferring isdone via a teacher network constructed using theposterior regularization principle.
Our frameworkis general and applicable to various types of neu-ral architectures.
With a few intuitive rules, ourframework significantly improves base networkson sentiment analysis and named entity recogni-tion, demonstrating the practical significance ofour approach.Though we have focused on first-order logicrules, we leveraged soft logic formulation whichcan be easily extended to general probabilistic mod-els for expressing structured distributions and per-forming inference and reasoning (Lake et al, 2015).We plan to explore these diverse knowledge rep-resentations to guide the DNN learning.
The pro-posed iterative distillation procedure also revealsconnections to recent neural autoencoders (Kingmaand Welling, 2014; Rezende et al, 2014) wheregenerative models encode probabilistic structuresand neural recognition models distill the informa-tion through iterative optimization (Rezende et al,2016; Johnson et al, 2016; Karaletsos et al, 2016).The encouraging empirical results indicate astrong potential of our approach for improvingother application domains such as vision tasks,which we plan to explore in the future.
Finally,we also would like to generalize our frameworkto automatically learn the confidence of differentrules, and derive new rules from data.AcknowledgmentsWe thank the anonymous reviewers for their valu-able comments.
This work is supported by NSFIIS1218282, NSF IIS1447676, Air Force FA8721-05-C-0003, and FA8750-12-2-0342.2418ReferencesStephen H Bach, Matthias Broecheler, Bert Huang, and LiseGetoor.
2015.
Hinge-loss Markov random fields and prob-abilistic soft logic.
arXiv preprint arXiv:1505.04406.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.2014.
Neural machine translation by jointly learning toalign and translate.
Proc.
of ICLR.Cristian Bucilu, Rich Caruana, and Alexandru Niculescu-Mizil.
2006.
Model compression.
In Proc.
of KDD, pages535?541.
ACM.Jason PC Chiu and Eric Nichols.
2015.
Named entity recog-nition with bidirectional LSTM-CNNs.
arXiv preprintarXiv:1511.08308.Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen,Koray Kavukcuoglu, and Pavel Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
JMLR, 12:2493?2537.James Foulds, Shachi Kumar, and Lise Getoor.
2015.
La-tent topic networks: A versatile probabilistic programmingframework for topic models.
In Proc.
of ICML, pages777?786.Manoel VM Franc?a, Gerson Zaverucha, and Artur S dAvilaGarcez.
2014.
Fast relational learning using bottom clausepropositionalization with artificial neural networks.
Ma-chine learning, 94(1):81?104.Kuzman Ganchev, Joao Grac?a, Jennifer Gillenwater, and BenTaskar.
2010.
Posterior regularization for structured latentvariable models.
JMLR, 11:2001?2049.Artur S d?Avila Garcez, Krysia Broda, and Dov M Gabbay.2012.
Neural-symbolic learning systems: foundations andapplications.
Springer Science & Business Media.Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, VincentVanhoucke, Patrick Nguyen, Tara N Sainath, et al 2012.Deep neural networks for acoustic modeling in speechrecognition: The shared views of four research groups.Signal Processing Magazine, IEEE, 29(6):82?97.Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
2015.
Dis-tilling the knowledge in a neural network.
arXiv preprintarXiv:1503.02531.Minqing Hu and Bing Liu.
2004.
Mining and summarizingcustomer reviews.
In Proc.
of KDD, pages 168?177.
ACM.Matthew J Johnson, David Duvenaud, Alexander B Wiltschko,Sandeep R Datta, and Ryan P Adams.
2016.
StructuredVAEs: Composing probabilistic graphical models and vari-ational autoencoders.
arXiv preprint arXiv:1603.06277.Theofanis Karaletsos, Serge Belongie, Cornell Tech, and Gun-nar R?atsch.
2016.
Bayesian representation learning withoracle constraints.
In Proc.
of ICLR.Yoon Kim.
2014.
Convolutional neural networks for sentenceclassification.
Proc.
of EMNLP.Diederik P Kingma and Max Welling.
2014.
Auto-encodingvariational Bayes.
In Proc.
of ICLR.Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
2012.Imagenet classification with deep convolutional neural net-works.
In Proc.
of NIPS, pages 1097?1105.Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, andJosh Tenenbaum.
2015.
Deep convolutional inverse graph-ics network.
In Proc.
of NIPS, pages 2530?2538.Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenen-baum.
2015.
Human-level concept learning through prob-abilistic program induction.
Science, 350(6266):1332?1338.Guillaume Lample, Miguel Ballesteros, Sandeep Subrama-nian, Kazuya Kawakami, and Chris Dyer.
2016.
Neuralarchitectures for named entity recognition.
In Proc.
ofNAACL.Quoc V Le and Tomas Mikolov.
2014.
Distributed represen-tations of sentences and documents.
Proc.
of ICML.Percy Liang, Hal Daum?e III, and Dan Klein.
2008.
Structurecompilation: trading structure for features.
In Proc.
ofICML, pages 592?599.
ACM.Percy Liang, Michael I Jordan, and Dan Klein.
2009.
Learn-ing from measurements in exponential families.
In Proc.of ICML, pages 641?648.
ACM.David Lopez-Paz, L?eon Bottou, Bernhard Sch?olkopf, andVladimir Vapnik.
2016.
Unifying distillation and privi-leged information.
Prof. of ICLR.Gang Luo, Xiaojiang Huang, Chin-Yew Lin, and Zaiqing Nie.2015.
Joint named entity recognition and disambiguation.In Proc.
of EMNLP.Xuezhe Ma and Eduard Hovy.
2016.
End-to-end sequencelabeling via bi-directional LSTM-CNNs-CRF.
In Proc.
ofACL.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,and Jeff Dean.
2013.
Distributed representations of wordsand phrases and their compositionality.
In Proc.
of NIPS,pages 3111?3119.Marvin Minksy.
1980.
Learning meaning.
Technical ReportAI Lab Memo.
Project MAC.
MIT.Anh Nguyen, Jason Yosinski, and Jeff Clune.
2015.
Deepneural networks are easily fooled: High confidence predic-tions for unrecognizable images.
In Proc.
of CVPR, pages427?436.
IEEE.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploiting classrelationships for sentiment categorization with respect torating scales.
In Proc.
of ACL, pages 115?124.Jeffrey Pennington, Richard Socher, and Christopher D Man-ning.
2014.
Glove: Global vectors for word representation.In Proc.
of EMNLP, volume 14, pages 1532?1543.Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wier-stra.
2014.
Stochastic backpropagation and approximateinference in deep generative models.
Proc.
of ICML.Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka,Karol Gregor, and Daan Wierstra.
2016.
One-shot gen-eralization in deep generative models.
arXiv preprintarXiv:1603.05106.Matthew Richardson and Pedro Domingos.
2006.
Markovlogic networks.
Machine learning, 62(1-2):107?136.2419David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Lau-rent Sifre, George van den Driessche, Julian Schrittwieser,Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot,et al 2016.
Mastering the game of go with deep neuralnetworks and tree search.
Nature, 529(7587):484?489.Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang,Christopher D Manning, Andrew Y Ng, and ChristopherPotts.
2013.
Recursive deep models for semantic compo-sitionality over a sentiment treebank.
In Proc.
of EMNLP,volume 1631, page 1642.
Citeseer.Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, JoanBruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus.2014.
Intriguing properties of neural networks.
Proc.
ofICLR.Erik F Tjong Kim Sang and Fien De Meulder.
2003.
Introduc-tion to the conll-2003 shared task: Language-independentnamed entity recognition.
In Proc.
of CoNLL, pages 142?147.
Association for Computational Linguistics.Geoffrey G Towell, Jude W Shavlik, and Michiel O No-ordewier.
1990.
Refinement of approximate domain theo-ries by knowledge-based neural networks.
In Proceedingsof the eighth National conference on Artificial intelligence,pages 861?866.
Boston, MA.Sida Wang and Christopher Manning.
2013.
Fast dropouttraining.
In Proc.
of ICML, pages 118?126.Bishan Yang and Claire Cardie.
2014.
Context-aware learn-ing for sentence-level sentiment analysis with posteriorregularization.
In Proc.
of ACL, pages 325?335.Wenpeng Yin and Hinrich Schutze.
2015.
Multichannelvariable-size convolution for sentence classification.
Proc.of CONLL.Matthew D Zeiler.
2012.
Adadelta: an adaptive learning ratemethod.
arXiv preprint arXiv:1212.5701.Ye Zhang, Stephen Roller, and Byron Wallace.
2016.
MGNC-CNN: A simple approach to exploiting multiple word em-beddings for sentence classification.
Proc.
of NAACL.Jun Zhu, Ning Chen, and Eric P Xing.
2014.
Bayesianinference with posterior regularization and applications toinfinite latent SVMs.
JMLR, 15(1):1799?1847.2420
