Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 132?134,October 25, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsWord?s Vector Representations meet Machine TranslationEva Mart?
?nez GarciaCristina Espan?a-BonetTALP Research CenterUnivesitat Polite`cnica de Catalunyaemartinez@lsi.upc.educristinae@lsi.upc.eduJo?rg TiedemannUppsala UniversityDepartment of Linguisticsand Philologyjorg.tiedemann@lingfil.uu.seLlu?
?s Ma`rquezQatar Computing Research InstituteQatar Foundationlluism@lsi.upc.eduAbstractDistributed vector representations ofwords are useful in various NLP tasks.We briefly review the CBOW approachand propose a bilingual application ofthis architecture with the aim to improveconsistency and coherence of MachineTranslation.
The primary goal of the bilin-gual extension is to handle ambiguouswords for which the different senses areconflated in the monolingual setup.1 IntroductionMachine Translation (MT) systems are nowadaysachieving a high-quality performance.
However,they are typically developed at sentence levelusing only local information and ignoring thedocument-level one.
Recent work claims thatdiscourse-wide context can help to translate indi-vidual words in a way that leads to more coherenttranslations (Hardmeier et al., 2013; Hardmeier etal., 2012; Gong et al., 2011; Xiao et al., 2011).Standard SMT systems use n-gram models torepresent words in the target language.
How-ever, there are other word representation tech-niques that use vectors of contextual information.Recently, several distributed word representationmodels have been introduced that have interestingproperties regarding to the semantic informationthat they capture.
In particular, we are interestedin the word2vec package available in (Mikolov etal., 2013a).
These models proved to be robustand powerful for predicting semantic relations be-tween words and even across languages.
However,they are not able to handle lexical ambiguity asthey conflate word senses of polysemous wordsinto one common representation.
This limitation isalready discussed in (Mikolov et al., 2013b) and in(Wolf et al., 2014), in which bilingual extensionsof the word2vec architecture are proposed.
In con-trast to their approach, we are not interested inmonolingual applications but instead like to con-centrate directly on the bilingual case in connec-tion with MT.We built bilingual word representation mod-els based on word-aligned parallel corpora byan application of the Continuous Bag-of-Words(CBOW) algorithm to the bilingual case (Sec-tion 2).
We made a twofold preliminary evalua-tion of the acquired word-pair representations ontwo different tasks (Section 3): predicting seman-tically related words (3.1) and cross-lingual lexicalsubstitution (3.2).
Section 4 draws the conclusionsand sets the future work in a direct application ofthese models to MT.2 Semantic Models using CBOWThe basic architecture that we use to build ourmodels is CBOW (Mikolov et al., 2013a).
Thealgorithm uses a neural network (NN) to predicta word taking into account its context, but withoutconsidering word order.
Despite its drawbacks, wechose to use it since we presume that the transla-tion task applies the same strategy as the CBOWarchitecture, i.e., from a set of context words try topredict a translation of a specific given word.In the monolingual case, the NN is trained usinga monolingual corpus to obtain the correspondingprojection matrix that encloses the vector repre-sentations of the words.
In order to introduce thesemantic information in a bilingual scenario, weuse a parallel corpus and automatic word align-ment to extract a training corpus of word pairs:(wi,S|wi,T).
This approach is different from (Wolfet al., 2014) who build an independent model foreach language.
With our method, we try to cap-ture simultaneously the semantic information as-sociated to the source word and the informationin the target side of the translation.
In this way,we hope to better capture the semantic informa-tion that is implicitly given by translating a text.132Model Accuracy Known wordsmono en 32.47 % 64.67 %mono es 10.24 % 44.96 %bi en-es 23.68 % 13.74 %Table 1: Accuracy on the Word Relationship set.3 ExperimentsThe semantic models are built using a combinationof freely available corpora for English and Span-ish (EuropalV7, United Nations and MultilingualUnited Nations, and Subtitles2012).
They canbe found in the Opus site (Tiedemann, 2012).Wetrained vectors to represent word pairs forms us-ing this corpora with the word2vec CBOW imple-mentation.
We built a training set of almost 600million words and used 600-dimension vectors inthe training.
Regarding to the alignments, we onlyused word-to-word ones to avoid noise.3.1 Accuracy of the Semantic ModelWe first evaluate the quality of the models basedon the task of predicting semantically relatedwords.
A Spanish native speaker built the bilin-gual test set similarly to the process done to thetraining data from a list of 19, 544 questions intro-duced by (Mikolov et al., 2013c).
In our bilingualscenario, the task is to predict a pair of words giventwo pairs of related words.
For instance, given thepair Athens|Atenas Greece|Grecia andthe question London|Londres, the task is topredict England|Inglaterra.Table 1 shows the results, both overall accuracyand accuracy over the known words for the mod-els.
Using the first 30, 000 entries of the model(the most frequent ones), we obtain 32% of ac-curacy for English (mono en) and 10% for Span-ish (mono es).
We chose these parameters for oursystem to obtain comparable results to the onesin (Mikolov et al., 2013a) for a CBOW architec-ture but trained with 783 million words (50.4%).Decay for the model in Spanish can be due to thefact that it was built from automatic translations.In the bilingual case (bi en-es), the accuracy islower than for English probably due to the noisein translations and word alignment.3.2 Cross-Lingual Lexical SubstitutionAnother way to evaluate the semantic models isthrough the effect they have in translation.
We im-plemented the Cross-Lingual Lexical Substitutiontask carried out in SemEval-2010 (Task2, 2010)and applied it to a test set of news data from theNews Commentary corpus of 2011.We identify those content words which aretranslated in more than one way by a baselinetranslation system (Moses trained with Europarlv7).
Given one of these content words, we take thetwo previous and two following words and lookfor their vector representations using our bilingualmodels.
We compute a linear combination of thesevectors to obtain a context vector.
Then, to chosethe best translation option, we calculate a scorebased on the similarity among the vector of everypossible translation option seen in the documentand the context vector.In average there are 615 words per documentwithin the test set and 7% are translated in morethan one way by the baseline system.
Our bilin-gual models know in average 87.5% of the wordsand 83.9% of the ambiguous ones, so althoughthere is a good coverage for this test set, still, someof the candidates cannot be retranslated or someof the options cannot be used because they aremissing in the models.
The accuracy obtained af-ter retranslation of the known ambiguous wordsis 62.4% and this score is slightly better than theresult obtained by using the most frequent transla-tion for ambiguous words (59.8%).
Even thoughthis improvement is rather modest, it shows poten-tial benefits of our model in MT.4 ConclusionsWe implemented a new application of word vec-tor representations for MT.
The system uses wordalignments to build bilingual models with the finalaim to improve the lexical selection for words thatcan be translated in more than one sense.The models have been evaluated regarding theiraccuracy when trying to predict related words(Section 3.1) and also regarding its possible effectwithin a translation system (Section 3.2).
In bothcases one observes that the quality of the transla-tion and alignments previous to building the se-mantic models are bottlenecks for the final perfor-mance: part of the vocabulary, and therefore trans-lation pairs, are lost in the training process.Future work includes studying different kindsof alignment heuristics.
We plan to developnew features based on the semantic models touse them inside state-of-the-art SMT systems likeMoses (Koehn et al., 2007) or discourse-orienteddecoders like Docent (Hardmeier et al., 2013).133ReferencesZ.
Gong, M. Zhang, and G. Zhou.
2011.
Cache-baseddocument-level statistical machine translation.
InProc.
of the 2011 Conference on Empirical Methodsin NLP, pages 909?919, UK.C.
Hardmeier, J. Nivre, and J. Tiedemann.
2012.Document-wide decoding for phrase-based statisti-cal machine translation.
In Proc.
of the Joint Con-ference on Empirical Methods in NLP and Compu-tational Natural Language Learning, pages 1179?1190, Korea.C.
Hardmeier, S. Stymne, J. Tiedemann, and J. Nivre.2013.
Docent: A document-level decoder forphrase-based statistical machine translation.
InProc.
of the 51st ACL Conference, pages 193?198,Bulgaria.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: open source toolkit forstatistical machine translation.
In Proc.
of the 45thACL Conference, pages 177?180, Czech Republic.T.
Mikolov, K. Chen, G. Corrado, and J.
Dean.
2013a.Efficient estimation of word representations in vec-tor space.
In Proceedings of Workshop at ICLR.http://code.google.com/p/word2vec.T.
Mikolov, Q. V. Le, and I. Sutskever.
2013b.
Ex-ploiting similarities among languages for machinetranslation.
In arXiv.T.
Mikolov, I. Sutskever, G. Corrado, and J. Dean.2013c.
Distributed representations of words andphrases and their compositionality.
In Proceedingsof NIPS.Task2.
2010.
Cross-lingual lexi-cal substitution task, semeval-2010.http://semeval2.fbk.eu/semeval2.php?location=tasksT24.J.
Tiedemann.
2009.
News from opus - a collectionof multilingual parallel corpora with tools and in-terfaces.
In N. Nicolov and K. Bontcheva and G.Angelova and R. Mitkov (eds.)
Recent Advances inNatural Language Processing (vol V), pages 237?248, Amsterdam/Philadelphia.
John Benjamins.J.
Tiedemann.
2012.
Parallel data, tools and interfacesin opus.
In Proceedings of the 8th InternationalConference on Language Resources and Evaluation(LREC?2012).
http://opus.lingfil.uu.se/.L.
Wolf, Y. Hanani, K. Bar, and N. Derschowitz.
2014.Joint word2vec networks for bilingual semantic rep-resentations.
In Poster sessions at CICLING.T.
Xiao, J. Zhu, S. Yao, and H. Zhang.
2011.Document-level consistency verification in machinetranslation.
In Proc.
of Machine Translation SummitXIII, pages 131?138, China.134
