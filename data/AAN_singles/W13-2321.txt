Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 168?177,Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational LinguisticsAutomatic Named Entity Pre-Annotationfor Out-of-Domain Human AnnotationSophie Rosset?, Cyril Grouin?, Thomas Lavergne?,?
, Mohamed Ben Jannet?,?,?,?Je?re?my Leixa, Olivier Galibert?
, Pierre Zweigenbaum?.
?LIMSI?CNRS ?Universite?
Paris-Sud ?LNE?LPP, Universite?
Sorbonne Nouvelle ELDA{rosset,grouin,lavergne,ben-jannet,pz}@limsi.frleixa@elda.org, olivier.galibert@lne.frAbstractAutomatic pre-annotation is often used toimprove human annotation speed and ac-curacy.
We address here out-of-domainnamed entity annotation, and examinewhether automatic pre-annotation is stillbeneficial in this setting.
Our study de-sign includes two different corpora, threepre-annotation schemes linked to two an-notation levels, both expert and novice an-notators, a questionnaire-based subjectiveassessment and a corpus-based quantita-tive assessment.
We observe that pre-annotation helps in all cases, both forspeed and for accuracy, and that the sub-jective assessment of the annotators doesnot always match the actual benefits mea-sured in the annotation outcome.1 IntroductionHuman corpus annotation is a difficult, time-consuming, and hence costly process.
This mo-tivates research into methods which reduce thiscost (Leech, 1997).
One such method consists ofautomatically pre-annotating the corpus (Marcuset al 1993; Dandapat et al 2009) using an ex-isting system, e.g., a POS tagger, syntactic parser,named entity recognizer, according to the task forwhich the annotations aim to provide a gold stan-dard.
The pre-annotations are then corrected bythe human annotators.
The underlying hypothe-sis is that this should reduce annotation time whilepossibly at the same time increasing annotationcompleteness and consistency.We study here corpus pre-annotation in a spe-cific setting, out-of-domain named entity annota-tion, in which we examine specific questions thatwe present below.
We produced corpora and an-notation guidelines for named entities which areboth hierarchical and compositional (Grouin et al2011),1 and which we used in contrastive stud-ies of news texts in French (Rosset et al 2012).We want to rely on the same named entity def-initions for studies on two types of data we didnot cover: parliament debates (Europarl corpus)and regional, contemporary written news (L?EstRe?publicain), both in French.
To help the annota-tion process we could reuse our system (Dinarelliand Rosset, 2011), but needed first to examinewhether a system trained on one type of text (ourfirst Broadcast News data) could be used to pro-duce a useful pre-annotation for different types oftext (our two corpora).We therefore set up the present study in whichwe aim to answer the following questions linkedto this point and to related annotation issues:?
can a system trained on data from one spe-cific domain be useful on data from anotherdomain in a pre-annotation task??
does this pre-annotation help human annota-tors or bias them??
what importance can we give to the annota-tors?
subjective assessment of the usefulnessof the pre-annotation??
can we observe differences in the use of pre-annotation depending on the level of exper-tise of human annotators?Moreover, as the aforementioned annotationscheme is based on two annotation levels (entitiesand components), we want to answer these ques-tions taking into account these two levels.We first examine related work on pre-annotation(Section 2), then present our corpora and annota-tion task (Section 3).
We describe and discuss ex-periments in Section 4, and make subjective and1Corpora, guidelines and tools are available throughELRA under references ELRA-S0349 and ELRA-W0073.168quantitative observations in Sections 5 and 6.
Fi-nally, we conclude and present some perspectivesin Section 7.2 Related WorkFacilitating human annotations has been the topicof a large amount of research.
Two differentapproaches can be distinguished: active learn-ing (Ringger et al 2007; Settles et al 2008) andpre-annotation (Marcus et al 1993; Dandapat etal., 2009).
Our work falls into the latter type.Pre-annotation can be used in several ways.
Thefirst is to provide annotations to be corrected byhuman annotators (Fort and Sagot, 2010).
A vari-ant consists of merging multiple automatic anno-tations before having them corrected by humancurators to produce a gold-standard (Rebholz-Schuhmann et al 2011).
The second type con-sists of providing clues to help human annotatorsperform the annotation task (Mihaila et al 2013).This work addresses the first type, a single-system pre-annotation with human correction.
Anobjective is to examine whether a system trainedon one type of text can be useful to pre-annotatetexts of a different type.
Most previous studieshave been performed on well-behaved tasks suchas part-of-speech tagging on in-domain data, i.e.,the model used for pre-annotating the target datahad been trained on similar data.
For instance, Fortand Sagot (2010) provide a precise evaluation ofthe usefulness of pre-annotation and compare theimpact of different quality levels in POS taggerson the Penn TreeBank corpus.
They first traineddifferent models on the training part of the cor-pus and applied them to the test corpus.
The pre-annotated test corpus was then corrected by hu-mans.
They reported gains in accuracy and inter-annotator agreement.
The study focused on theminimal quality (accuracy threshold) of automaticannotation that would prove useful for human an-notation.
They reported a gain for human annota-tion when accuracy ranged from 66.5% to 81.6%.On the contrary, for a semantic-frame annotationtask, Rehbein et al(2009) observed no significantgain in quality and speed of annotation even whenusing a state-of-the-art system.Generally speaking, annotators find the pre-annotation stage useful (Rehbein et al 2009;South et al 2011; Huang et al 2011).
Anno-tation managers consider that a bias may occurdepending on how much human annotators trustthe pre-annotation (Rehbein et al 2009; Fort andSagot, 2010; South et al 2011).
In their frame-semantic argument structure annotation, Rehbeinet al(2009) addressed a specific question consid-ering a two-level annotation scheme: is the pre-annotation of frame assignment (low-level anno-tation) useful for annotating semantic roles (high-level annotation)?
Although for the low-level an-notation task they observed a significant differencein quality of final annotation, for the high-leveltask they found no difference.Most of these studies used a pre-annotation sys-tem trained on the same kind of data as thosewhich were to be annotated manually.
Neverthe-less some system-oriented studies have focused onthe results obtained by systems trained on one typeof corpus and applied to another type of corpus,e.g., for a Latin POS tagger (Poudat and Longre?e,2009; Skj?rholt, 2011) or for a CoNLL named en-tity tagger for German (Faruqui and Pado?, 2010)for which the authors noticed noticed a reduc-tion of the F-measure when going from in-domain(newswire data, F=0.782 for their best system) toout-of-domain (Europarl data, F=0.656).One of our objectives is then to examinewhether a system trained on one type of text canbe useful to pre-annotate texts of a different type.We set up experiments to study precisely the pos-sible induced bias and whether the level of experi-ence of the annotators would make a difference insuch a context.
In this study, we used two differentkinds of corpora, which were both different fromthe corpus used to train the pre-annotation system.3 Task and corpus description3.1 TaskIn this work, we used the structured named entitydefinition we proposed in a previous study (Grouinet al 2011): entities are both hierarchical (typeshave subtypes) and compositional (types and com-ponents are included in entities) as in Figure 1.func.collorg.entnameBEIde lakindanalystes financierslesFigure 1: Multi-level annotation of entity sub-types (red tags) and components (blue tags): thefinancial analysts of the EBI169This taxonomy of entity types is composed of7 types (person, location, organization, amount,time, production and function) and 32 sub-types(individual person pers.ind vs. group of personspers.coll; administrative organization org.adm vs.services org.ent; etc.).
Types and subtypes consti-tute the first level of annotation.Within these categories, components aresecond-level elements (kind, name, first.name,etc.
), and can never be used outside the scope of atype or subtype element.3.2 CorporaTwo French corpora were sampled from largerones:Europarl: Prepared speech (ParliamentDebates?Europarl): 15,306 word extract;Press: Local, contemporary written news (L?EstRe?publicain): 11,146 word extract.These corpora were automatically annotated us-ing the system described in (Dinarelli and Rosset,2011).
This system relies on a Conditional Ran-dom Field (CRF) model for the detection of com-ponents and on a probabilistic context-free gram-mar (PCFG) model for types and sub-types.
Thesemodels have been trained on Broadcast News data.This system achieved a Slot Error Rate (Makhoulet al 1999) of 37.0% on Broadcast conversationand 29.7% on Broadcast news, and ranked first inthe Quaero evaluation campaign (Galibert et al2011).4 ExperimentsIn this section we present the protocol we designedto study the usefulness of pre-annotation underdifferent conditions, and its overall results.4.1 ProtocolWe defined the following protocol, similar to theone used in Rehbein et al(2009).Corpora.
Four versions of our two corpora wereprepared: (i) raw text, (ii) pre-annotation oftypes, (iii) pre-annotation of components, and(iv) full pre-annotation of both types and compo-nents.
Each of these four versions was split intofour quarters.Annotators.
Eight human annotators were in-volved in this task.
Among them, four are con-sidered as expert annotators (they annotated cor-pora in the previous years) while the four re-maining ones are novice annotators (this was thefirst time they annotated such corpora; they weregiven training sessions before starting actual anno-tation).
We defined four pairs of annotators, whereeach pair was composed of an expert and a noviceannotator.Quarter allocation.
We allocated each corpusquarter in such a way that each pair of annotatorsprocessed, in each corpus, material from each oneof the four pre-annotated versions (see Table 3).The same allocation was made in both corpora.4.2 ResultsFor each corpus part, a reference was built basedon a majority vote by confronting all annotations.The resulting reference corpus is presented in Ta-ble 1.Corpus # comp.
# types # entities # wordsPressQ1 481 310 791 3047Q2 367 246 673 2628Q3 495 327 822 2971Q4 413 282 695 2600Europarl Q1 362 259 621 3926Q2 309 221 530 3809Q3 378 247 625 3604Q4 413 299 712 3967Table 1: General description of the reference an-notations: number of components, types, entities(the sum of components and types), and wordsTable 2 presents the performance of the au-tomatic pre-annotation system against the refer-ence corpus.
We used the well known F-measureand in addition the Slot Error Rate as it allowsto weight different error classes (deletions, inser-tions, type or frontier errors).
Fort and Sagot(2010) reported a gain in human annotation whenpre-annotation accuracy ranged from 66.5% to81.6%.
Given their results we can hope for a gainin both accuracy and annotation time when usingpre-annotation.Table 3 presents all results obtained by each an-notators given each pre-annotation condition (raw,components, types and full) in terms of precision,recall and F-measure.170Corpus #Raw text Components Types FullR P F R P F R P F R P FPressQ10.874 0.777 0.823 0.876 0.741 0.803 0.824 0.870 0.846 0.852 0.800 0.8250.810 0.766 0.788 0.815 0.777 0.796 0.645 0.724 0.683 0.844 0.785 0.813Q20.765 0.796 0.780 0.870 0.773 0.819 0.822 0.801 0.812 0.917 0.773 0.8390.558 0.654 0.602 0.826 0.775 0.800 0.815 0.777 0.795 0.816 0.752 0.783Q30.835 0.715 0.771 0.888 0.809 0.847 0.884 0.796 0.837 0.887 0.859 0.8730.792 0.689 0.736 0.904 0.780 0.837 0.876 0.771 0.820 0.780 0.827 0.803Q40.802 0.757 0.779 0.845 0.876 0.860 0.900 0.702 0.789 0.914 0.840 0.8760.794 0.727 0.759 0.696 0.715 0.705 0.812 0.701 0.752 0.802 0.757 0.779EuroparlQ10.809 0.728 0.766 0.800 0.568 0.665 0.776 0.862 0.817 0.754 0.720 0.7360.754 0.720 0.736 0.720 0.609 0.660 0.687 0.607 0.644 0.736 0.638 0.683Q20.776 0.792 0.784 0.782 0.617 0.690 0.797 0.645 0.713 0.821 0.526 0.6410.563 0.498 0.529 0.802 0.619 0.699 0.698 0.553 0.617 0.769 0.566 0.652Q30.747 0.459 0.569 0.749 0.624 0.681 0.805 0.800 0.803 0.735 0.744 0.7390.732 0.598 0.658 0.736 0.717 0.726 0.822 0.738 0.777 0.808 0.734 0.769Q40.742 0.624 0.678 0.874 0.760 0.813 0.732 0.480 0.580 0.743 0.608 0.6690.721 0.566 0.634 0.695 0.652 0.672 0.707 0.600 0.649 0.738 0.603 0.664Table 3: Overall recall, precision and F-measure for each pair of annotators (blue: pair #1, ocre: pair#2, green: pair #3, white: pair #4) on each corpus quarter (Q1, Q2, Q3, Q4), depending on the kind ofpre-annotation (raw text, only components, only types, full pre-annotation).
Expert annotator is on theupper line of each quarter, novice annotator is on the lower line.
Boldface indicates the best F-measurefor each novice and expert annotator among all pre-annotation tasks in a given corpus quarterCorpusComponents Types FullF SER F SER F SERPressQ1 72.4 37.9 63.5 46.3 68.9 41.0Q2 77.2 32.2 66.8 43.5 73.1 36.6Q3 76.1 34.1 68.3 41.7 73.1 36.9Q4 76.1 33.3 63.3 45.7 71.0 38.2Europarl Q1 61.9 49.9 57.5 55.4 60.1 52.2Q2 61.2 51.3 54.6 54.3 58.5 52.5Q3 61.6 50.1 53.3 55.7 58.2 52.2Q4 57.1 57.0 48.1 59.7 53.3 58.1Broad.
88.3 29.1 73.1 39.1 73.2 33.1Table 2: F-measure and Slot Error Rate achievedby the automatic system on each kind of annota-tion and on in-domain broadcast dataWe also computed inter-annotator agreement(IAA) for each corpus considering two groups ofannotators, experts and novices.
We consider thatthe inter-annotator agreement is somewhere be-tween the F-measure and the standard IAA con-sidering as markables all the units annotated by atleast one of the annotators (Grouin et al 2011).We computed Scott?s Pi (Scott, 1955), and Co-hen?s Kappa (Cohen, 1960).
The former considersone model for all annotators while the latter con-siders one model per annotator.
In our case, thesetwo values are almost the same, which means thatthe proportions and kinds of annotations are verysimilar across experts and novices.
Figure 2 showsthe IAA (Cohen?s Kappa and F-measure) obtainedon the two corpora given the four pre-annotationconditions (no pre-annotation, components, types,and full pre-annotation).
As we can see, IAA issystematically higher for the Press corpus thanfor the Europarl corpus, which can be linkedto the higher performance of the automatic pre-annotation system on this corpus.
We also can seethat pre-annotation always improves agreementand that full pre-annotation yields the best result.We observe that, as expected, pre-annotation leadshuman annotators to obtain higher consistency.5 Subjective assessmentAn important piece of information in any anno-tation campaign is the feelings of the annotatorsabout the task.
This can give interesting cluesabout the expected quality of their work and on theusefulness of the pre-annotation step.
We askedthe annotators a few questions concerning sev-eral features of this project, such as the annotation1710.50.60.70.80.91raw comp types fullPress: Cohen's kappaPress: F-measureEuroparl: Cohen's kappaEuroparl: F-measureFigure 2: Cohen?s Kappa (red and blue) and F-measure (green and pink) measuring agreement ofexperts and novices on Press and Europarl corporain four pre-annotation conditions.
Each measurecompares the concatenated annotations of the fourexperts with the four novices.manual, or how they assessed the benefits of pre-annotation in the different corpora (Section 5.1).Another important point is the experience of theannotators, which we also examine in the light oftheirs answers to the questionnaire (Section 5.2).5.1 QuestionnaireThe questionnaire submitted to the annotators con-tained 4 questions, dealing with their feedback onthe annotation process:1.
According to you, which level of pre-annotation has been the most helpful duringthe annotation process?
Types, components,or both?2.
To what extent would you say that pre-annotation helped you in terms of precisionand speed?
Did it produce many errors youhad to correct?3.
If you had to choose between the Europarlcorpus and the Press corpus, could you saythat one has been easier to annotate than theother?4.
Concerning the annotation manual, are theretopics that you would like to change, or cor-rect?
In the same way, which named entitiescaused you the most difficulties to deal with?All 8 annotators answered these questions.
Wesummarize below what we found in their answers.5.1.1 Level of pre-annotationMost of the annotators preferred the corpora thatwere pre-annotated with types only.
The reason,for the most part, is that a pre-annotation of typesallows the annotator to work faster on their files,because guessing the components from the typesis easier than guessing types from components.2Indeed, the different types of entities defined inthe manual always imply the same components,be they specific (to one entity type) or transverse(common to several entity types).
On the contrary,a transverse component, such as <kind>, can bepart of any type of named entity.
The other rea-son for this choice of pre-annotation concerns thereadability brought to the corpora.
An annotationwith types only is easier to read than an annotationwith components, and less exhausting after manyhours of work on the texts.5.1.2 Gain in precision and speedWhat motivated the answers to the second ques-tion mainly concerns the accuracy of the differentpre-annotation methods.
While all of them pre-sented errors that needed to be corrected, the pre-annotation of types was the one that they felt pre-sented the smaller number of errors.
Thus, annota-tors spent less time reviewing the corpora in searchof errors, compared to the other pre-annotated cor-pora (with components, and with both types andcomponents), where more errors had to be spot-ted and corrected.
This search for incorrect pre-annotations impacted the time spent on each cor-pus.
Indeed, most annotators declared that pre-annotation with types was quicker to deal withthan other pre-annotation schemes.5.1.3 Corpus differencesAbout one half of the annotators agreed that theEuroparl corpus had been more difficult to anno-tate.
Despite obvious differences in register, sen-tence structure and vocabulary between the twocorpora, Europarl seemed more redundant andcomplex than the other corpus.
For instance, oneof the annotators declared:The Europarl corpus is more difficultto annotate in the sense that the exist-ing types and components do not al-ways match the realities found in thecorpus, either because their definitions2This feeling is supported by results about ambiguity pre-sented in Fort et al(2012).172cannot apply exactly, or because the re-quired types and components are miss-ing (mainly for frequencies: ?five timesper year?
).The other half of the annotators did not feel anyspecific difficulties in annotating one corpus or theother.
According to them, both corpora are thesame in terms of register and sentence structure.5.1.4 Improvements in guidelinesAll of the annotators were unanimous in think-ing that two points need to be modified in themanual.
First of all, the distinction between the<org.adm> and <org.ent> subtypes is too diffi-cult to apprehend, above all in the Europarl corpuswhere these entities are too ambiguous to be anno-tated correctly.
Secondly, the distinction betweenthe <pers> and <func> types has also been diffi-cult to deal with.
The other remarks about poten-tial changes mainly concerned the introduction ofexplicit rules for frequencies, which are recurrentin the Europarl corpus.5.2 ExperienceAs mentioned earlier in Section 4.1, we will nowsee if the differences in experience between an-notators impacted their difficulty in annotating thecorpora.
First of all, when we look at the answersgiven to question 3, we notice that both novice andexpert annotators consider the Europarl corpus themost difficult to annotate.
Most of their answersdeal with the redundancy and the formal registerof the data.
Moreover, as everyone answered inquestion 4, both <func> and <org> entities haveto be modified to be easier to understand and touse.
This unanimous opinion about what needsto be reviewed in the manual allows us to thinkthat the annotators?
level of experience has a lowimpact on their apprehension of the corpora, bothEuroparl and Press.
To confirm this, we can lookat the answers given to questions 1 and 2, as indi-cated in the previous paragraph.
As has been ex-plained, every annotator correctly pointed at themany errors found in pre-annotation, regardlessof their experience.
Besides, the assessment ofthe benefits of pre-annotation is the same for al-most everyone, regardless of their experience too:both novice and expert annotators agree that pre-annotation with type adds efficiency and speed toannotation.To conclude, according to our observationsbased on the questionnaire, we cannot assert thatthere has been a difference between novice and ex-pert annotators.
Both groups agreed on the samedifficulties, pointed at the same errors, and crit-icized the same entities, saying that their defini-tions needed to be clarified.6 Quantitative observationsIn this section we provide results of quantitativeobservations in order to support, or not, the anno-tators?
subjective assessment.6.1 Corpus statisticsThe annotators reported different feelings depend-ing on the corpora.
Some of them reported thatthe Europarl corpus was more difficult to annotate,with more complex sentence structures, or usageof fewer proper nouns.To explore these differences, we computedsome statistics over the two original, un-annotatedcorpora (which are much larger than the samplesannotated in this experiment) as well as over theoriginal broadcast news corpus used to train thepre-annotation system.
Each of these corpora con-tains several million words.Table 4 reports simple statistics about sentencesin the three corpora.
Based on these statistics,while the Europarl (Euro) corpus is very similar tothe original Broadcast News (BN), the Press cor-pus shows differences: sentences are 20% shorter,with fewer but larger chunks, confirming the im-pression of simpler, less convoluted sentences.BN Press EuroMean sentence length 30.2 23.9 29.7Mean chunk count 10.9 6.7 10.4Mean chunk length 2.7 3.6 2.8Table 4: Sentence summary of the three corporaLooking more closely at the contents of thesesentences, Figure 3 summarizes the proportions ofgrammatical word classes.
The sentiment of ex-tensive naming of entities in the Press corpus isconfirmed by the four times higher rate of propernouns.
On the other hand, entities are more oftenreferred to using nouns with an optional adjectivein the Europarl corpus, leading to a more frequentusage of the latter.173Figure 3: Frequency of word classes in the threecorpora (BN = Broadcast News, Est = Press, Euro= Europarl).
TOOL = grammatical words, PCT/NB= punctuation and numbers, ADJ/ADV = adjectivesand adverbs, NAM = proper name, NOM = noun,VER = verb.6.2 Influence of pre-annotation on thebehaviour of annotatorsAs already mentioned, it is often reported that abias may occur depending on human confidencein the pre-annotation (Fort and Sagot, 2010; Re-hbein et al 2009; South et al 2011).
An im-portant unknown is always the influence of pre-annotation on the behaviour of annotators, and atwhich point pre-annotation induces more errorsthan it helps.
This may obviously depend on pre-annotation quality.
Table 5 summarizes the er-ror rates of the automatic annotator in the stud-ied data (Press + Europarl) and in comparison toin-domain data.
Insertions (Ins) are extra anno-tations, deletions (Del) missing annotations, andsubstitutions (Subs) are annotations that are incor-rect in type, boundaries, or both.
We can see thatDomain Pre-annotation Ins Del SubsComponents 4.4% 33.6% 7.8%Out Types 7.0% 36.2% 12.7%Full 5.5% 34.6% 9.7%In Full 3.7% 23.4% 10.6%Table 5: Pre-annotation errors and comparisonwith in-domain (Broadcast News) datagoing out-of-domain increased deletions, proba-bly through a lack of knowledge of domain vo-cabulary.
But it did not influence the other errorrates significantly.
It is also noticeable that dele-tion is the type of error most produced by the sys-tem, with every third entity missed.
Automatic,full pre-annotation of Press + Europarl obtains aprecision of 0.79 and a recall of 0.56.Human annotator performance can then be mea-sured over the same three error types (Table 6).
WePre-annotation Ins Del SubsRaw 8.9% 18.9% 12.8%Components 5.9% 16.7% 11.3%Types 7.1% 16.5% 12.0%Full 7.1% 16.5% 10.1%Table 6: Mean human annotation error levels foreach pre-annotation schemecan see that annotation quality was systematicallyimproved by pre-annotation, with the best globalresult obtained by full pre-annotation.
In additionthere was no increase in deletions (had the humanstopped looking at the unannotated text) or inser-tions (had the human always trusted the system) asmight have been feared.
This may be a side effectof the high deletion rate, making it obvious to thehuman that the system was missing things.
In anycase, the annotation was clearly beneficial in ourexperiment with no ill effects seen in error ratescompared to the gold standard.6.3 Is pre-annotation useful and to whom?All annotators asserted that pre-annotation is use-ful, specifically with types.
In this section, we pro-vide observations concerning variations in annota-tion both in terms of accuracy (F-measure is used)and duration.Raw Comp.
Types FullExperts 0.748 0.786 0.778 0.791Novices 0.682 0.737 0.721 0.742Table 7: Mean F-measure of experts and novices,for each pre-annotation schemeRaw Comp.
Types FullExperts 109.0 52.5 64.0 39.13Novices 151.7 135.5 117.9 103.88Table 8: Mean duration (in minutes) of annotationfor experts and novices, for each pre-annotationscheme (two corpus quarters)Tables 7 and 8 confirm the hypothesis that auto-matic pre-annotation helps annotators to annotate174faster and to be more efficient.
All pre-annotationlevels (components, types and both) seem to behelpful for both experts and novices.
Expertsreached a higher accuracy (F=0.791) and theywere more than twice faster with components orfull pre-annotation.
Similarly, novices performedbetter when working on a full pre-annotation(F=0.742) and reached a faster working time(48mn less than with no pre-annotation).
This lastobservation contradicts the annotators?
reportedexperience: the annotators felt more comfortableand faster with a types-only pre-annotation thanwith full pre-annotation (see Section 5.1.2).
Theresults show that full pre-annotation was the bestchoice for both quality and speed.These results confirm that pre-annotation is use-ful, even with a moderate level of performance ofthe system.
Does it help to annotate componentsand types equally?
To answer this question, wecomputed the F-measure of novices and expertsfor both components and types separately (see Fig-ure 4).60657075808590raw comp types fulltypes/novicescomponents/novicestypes/expertscomponents/expertsFigure 4: Mean F-measure on each pre-annotationlevel for expert and novice annotatorsFor experts we can see that all pre-annotationlevels allow them to improve their performance onboth types and components.
However for novices,pre-annotation with types does not improve theirperformance in labeling components.
We also no-tice that pre-annotation in both types and compo-nents allows experts and novices to reach their bestperformance for both types and components.7 Conclusion and PerspectivesConclusion.
In this paper, we studied the inter-est of a pre-annotation process for a complex an-notation task with only an out-of-domain annota-tion system available.
We also designed our exper-iments to check whether the level of experience ofthe annotators made a difference in such a context.The experiment produced in the end a high-qualitygold standard (8-way merge including 2 versionswithout pre-annotation) which enabled us to mea-sure quantitatively the performance of every pre-annotation scheme.We noticed that the pre-annotation systemproved relatively precise for such a complex task,with 79% correct pre-annotations, but with a poorrecall at 56%.
This may be a good operating pointfor a pre-annotation system to reduce bias though.In our quantitative experiments we found thatthe fullest pre-annotation helped most, both interms of quality and annotation speed, even thoughthe quality of the pre-annotation system varied de-pending on the annotation layer.
This contradictedthe feelings of the annotators who thought that atype-only pre-annotation was the most efficient.This shows that in such a setting self-evaluationcannot be trusted.
On the other hand their remarksabout the problems in the annotation guide itselfseemed rather pertinent.When it comes to experts vs. novices, we notedthat their behaviour and remarks were essentiallyidentical.
Experts were both better and fasterat annotating, but had similar reactions to pre-annotation and essentially the same feelings.In conclusion, even with an out-of-domain sys-tem, a pre-annotation step proves extremely usefulin both annotation speed and annotation quality,and at least in our setting, with a reasonably pre-cise system (at the expense of recall) no bias wasdetectable.
In addition, no matter what the anno-tators feel, as long as precision is good enough,the more pre-annotations the better.
Pre-filteringeither of our two levels did not help.Perspectives.
Based upon this conclusion, weplan to use automatic pre-annotation in further an-notation work, beginning with the present corpora.As a first use, we plan to propose a few changesto the annotation principles in the guidelines weused.
To annotate existing corpora with thesechanges, automatic pre-annotation will be useful.As a second piece of future work, we plan toannotate new corpora with the existing annotationframework.
We also plan to add new types ofnamed entities (e.g., events) to extend the anno-tation of existing annotated corpora, using the pre-annotation process to reduce the overall workload.175AcknowledgmentsThis work has been partially funded by OSEO un-der the Quaero program and by the French ANRVERA project.ReferencesJacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, 20(1):37?46.Sandipan Dandapat, Priyanka Biswas, Monojit Choud-hury, and Kalika Bali.
2009.
Complex linguistic an-notation ?
no easy way out!
A case from Bangla andHindi POS labeling tasks.
In Proc of 3rd LinguisticAnnotation Workshop (LAW-III), pages 10?18, Sun-tec, Singapore, August.
ACL.Marco Dinarelli and Sophie Rosset.
2011.
Modelscascade for tree-structured named entity detection.In Proc of IJCNLP, pages 1269?1278, Chiang Mai,Thailand.Manaal Faruqui and Sebastian Pado?.
2010.
Trainingand evaluating a German named entity recognizerwith semantic generalization.
In Proc of Konvens,Saarbru?cken, Germany.Kare?n Fort and Beno?
?t Sagot.
2010.
Influence of pre-annotation on POS-tagged corpus development.
InProc of 4th Linguistic Annotation Workshop (LAW-IV), pages 56?63, Uppsala, Sweden.
ACL.Kare?n Fort, Adeline Nazarenko, and Sophie Rosset.2012.
Modeling the complexity of manual anno-tation tasks: a grid of analysis.
In Proceedings ofCOLING 2012, pages 895?910, Mumbai, India, De-cember.
The COLING 2012 Organizing Committee.Olivier Galibert, Sophie Rosset, Cyril Grouin, PierreZweigenbaum, and Ludovic Quintard.
2011.
Struc-tured and extended named entity evaluation in au-tomatic speech transcriptions.
In Proc of IJCNLP,Chiang Mai, Thailand.Cyril Grouin, Sophie Rosset, Pierre Zweigenbaum,Kare?n Fort, Olivier Galibert, and Ludovic Quin-tard.
2011.
Proposal for an extension of tradi-tional named entities: From guidelines to evalua-tion, an overview.
In Proc of 5th Linguistic Anno-tation Workshop (LAW-V), pages 92?100, Portland,OR.
ACL.Minlie Huang, Aure?lie Ne?ve?ol, and Zhiyong Lu.2011.
Recommending MeSH terms for annotatingbiomedical articles.
Journal of the American Medi-cal Informatics Association, 18(5):660?7.Geoffrey Leech.
1997.
Introducing corpus annota-tion.
In Roger Garside, Geoffrey Leech, and TonyMcEnery, editors, Corpus annotation: Linguistic in-formation from computer text corpora, pages 1?18.Longman, London.John Makhoul, Francis Kubala, Richard Schwartz, andRalph Weischedel.
1999.
Performance measures forinformation extraction.
In Proc.
of DARPA Broad-cast News Workshop, pages 249?252.Mitchell P Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: the Penn TreeBank.
Com-putational Linguistics, 19(2):313?330.Claudiu Mihaila, Tomoko Ohta, Sampo Pyysalo, andSophia Ananiadou.
2013.
Biocause: Annotatingand analysing causality in the biomedical domain.BMC Bioinformatics, 14:2.Ce?line Poudat and Doninique Longre?e.
2009.
Vari-ations langagie`res et annotation morphosyntaxiquedu latin classique.
Traitement Automatique desLangues, 50(2):129?148.Dietrich Rebholz-Schuhmann, Antonio Jimeno, ChenLi, Senay Kafkas, Ian Lewin, Ning Kang, Peter Cor-bett, David Milward, Ekaterina Buyko, Elena Beiss-wanger, Kerstin Hornbostel, Alexandre Kouznetsov,Rene?
Witte, Jonas B Laurila, Christopher JO Baker,Cheng-Ju Kuo, Simone Clematide, Fabio Rinaldi,Richa?rd Farkas, Gyo?rgy Mo?ra, Kazuo Hara, Laura IFurlong, Michael Rautschka, Mariana Lara Neves,Alberto Pascual-Montano, Qi Wei, Nigel Collier,Md Faisal Mahbub Chowdhury, Alberto Lavelli,Rafael Berlanga, Roser Morante, Vincent Van Asch,Walter Daelemans, Jose?
L Marina, Erik van Mulli-gen, Jan Kors, and Udo Hahn.
2011.
Assessment ofNER solutions against the first and second CALBCsilver standard corpus.
J Biomed Semantics, 2.Ines Rehbein, Josef Ruppenhofer, and CarolineSporleder.
2009.
Assessing the benefits of partialautomatic pre-labeling for frame-semantic annota-tion.
In Proc of 3rd Linguistic Annotation Workshop(LAW-III), pages 19?26, Suntec, Singapore.
ACL.Eric Ringger, Peter McClanahan, Robbie Haertel,George Busby, Marc Carmen, James Carroll, KevinSeppi, and Deryle Lonsdale.
2007.
Active learningfor part-of-speech tagging: Accelerating corpus an-notation.
In Proc of Linguistic Annotation Workshop(LAW), pages 101?108.
ACL.Sophie Rosset, Cyril Grouin, Kare?n Fort, Olivier Gal-ibert, Juliette Kahn, and Pierre Zweigenbaum.
2012.Structured named entities in two distinct press cor-pora: Contemporary broadcast news and old news-papers.
In Proc of 6th Linguistic Annotation Work-shop (LAW-VI), pages 40?48, Jeju, South Korea.ACL.William A Scott.
1955.
Reliability of content analysis:The case of nominal scale coding.
Public OpinionQuaterly, 19(3):321?325.Burr Settles, Mark Craven, and Lewis Friedland.
2008.Active learning with real annotation costs.
In Procof the NIPS Workshop on Cost-Sensitive Learning.176Arne Skj?rholt.
2011.
More, faster: Accelerated cor-pus annotation with statistical taggers.
Journal forLanguage Technology and Computational Linguis-tics, 26(2):151?163.Brett R South, Shuying Shen, Robyn Barrus, Scott LDuVall, O?zlem Uzuner, and Charlene Weir.
2011.Qualitative analysis of workflow modifications usedto generate the reference standard for the 2010i2b2/VA challenge.
In Proc of AMIA.177
