Proceedings of the ACL 2010 Conference Short Papers, pages 156?161,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsCoreference Resolution with ReconcileVeselin StoyanovCenter for Languageand Speech ProcessingJohns Hopkins Univ.Baltimore, MDves@cs.jhu.eduClaire CardieDepartment ofComputer ScienceCornell UniversityIthaca, NYcardie@cs.cornell.eduNathan GilbertEllen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UTngilbert@cs.utah.eduriloff@cs.utah.eduDavid ButtlerDavid HysomLawrence LivermoreNational LaboratoryLivermore, CAbuttler1@llnl.govhysom1@llnl.govAbstractDespite the existence of several noun phrase coref-erence resolution data sets as well as several for-mal evaluations on the task, it remains frustratinglydifficult to compare results across different corefer-ence resolution systems.
This is due to the high costof implementing a complete end-to-end coreferenceresolution system, which often forces researchersto substitute available gold-standard information inlieu of implementing a module that would computethat information.
Unfortunately, this leads to incon-sistent and often unrealistic evaluation scenarios.With the aim to facilitate consistent and realis-tic experimental evaluations in coreference resolu-tion, we present Reconcile, an infrastructure for thedevelopment of learning-based noun phrase (NP)coreference resolution systems.
Reconcile is de-signed to facilitate the rapid creation of corefer-ence resolution systems, easy implementation ofnew feature sets and approaches to coreference res-olution, and empirical evaluation of coreference re-solvers across a variety of benchmark data sets andstandard scoring metrics.
We describe Reconcileand present experimental results showing that Rec-oncile can be used to create a coreference resolverthat achieves performance comparable to state-of-the-art systems on six benchmark data sets.1 IntroductionNoun phrase coreference resolution (or simplycoreference resolution) is the problem of identi-fying all noun phrases (NPs) that refer to the sameentity in a text.
The problem of coreference res-olution is fundamental in the field of natural lan-guage processing (NLP) because of its usefulnessfor other NLP tasks, as well as the theoretical in-terest in understanding the computational mech-anisms involved in government, binding and lin-guistic reference.Several formal evaluations have been conductedfor the coreference resolution task (e.g., MUC-6(1995), ACE NIST (2004)), and the data sets cre-ated for these evaluations have become standardbenchmarks in the field (e.g., MUC and ACE datasets).
However, it is still frustratingly difficult tocompare results across different coreference res-olution systems.
Reported coreference resolu-tion scores vary wildly across data sets, evaluationmetrics, and system configurations.We believe that one root cause of these dispar-ities is the high cost of implementing an end-to-end coreference resolution system.
Coreferenceresolution is a complex problem, and successfulsystems must tackle a variety of non-trivial sub-problems that are central to the coreference task ?e.g., mention/markable detection, anaphor identi-fication ?
and that require substantial implemen-tation efforts.
As a result, many researchers ex-ploit gold-standard annotations, when available, asa substitute for component technologies to solvethese subproblems.
For example, many publishedresearch results use gold standard annotations toidentify NPs (substituting for mention/markabledetection), to distinguish anaphoric NPs from non-anaphoric NPs (substituting for anaphoricity de-termination), to identify named entities (substitut-ing for named entity recognition), and to identifythe semantic types of NPs (substituting for seman-tic class identification).
Unfortunately, the use ofgold standard annotations for key/critical compo-nent technologies leads to an unrealistic evalua-tion setting, and makes it impossible to directlycompare results against coreference resolvers thatsolve all of these subproblems from scratch.Comparison of coreference resolvers is furtherhindered by the use of several competing (andnon-trivial) evaluation measures, and data sets thathave substantially different task definitions andannotation formats.
Additionally, coreference res-olution is a pervasive problem in NLP and manyNLP applications could benefit from an effectivecoreference resolver that can be easily configuredand customized.To address these issues, we have created a plat-form for coreference resolution, called Reconcile,that can serve as a software infrastructure to sup-port the creation of, experimentation with, andevaluation of coreference resolvers.
Reconcilewas designed with the following seven desideratain mind:?
implement the basic underlying software ar-156chitecture of contemporary state-of-the-artlearning-based coreference resolution sys-tems;?
support experimentation on most of the stan-dard coreference resolution data sets;?
implement most popular coreference resolu-tion scoring metrics;?
exhibit state-of-the-art coreference resolutionperformance (i.e., it can be configured to cre-ate a resolver that achieves performance closeto the best reported results);?
can be easily extended with new methods andfeatures;?
is relatively fast and easy to configure andrun;?
has a set of pre-built resolvers that can beused as black-box coreference resolution sys-tems.While several other coreference resolution sys-tems are publicly available (e.g., Poesio andKabadjov (2004), Qiu et al (2004) and Versley etal.
(2008)), none meets all seven of these desider-ata (see Related Work).
Reconcile is a modularsoftware platform that abstracts the basic archi-tecture of most contemporary supervised learning-based coreference resolution systems (e.g., Soonet al (2001), Ng and Cardie (2002), Bengtson andRoth (2008)) and achieves performance compara-ble to the state-of-the-art on several benchmarkdata sets.
Additionally, Reconcile can be eas-ily reconfigured to use different algorithms, fea-tures, preprocessing elements, evaluation settingsand metrics.In the rest of this paper, we review related work(Section 2), describe Reconcile?s organization andcomponents (Section 3) and show experimental re-sults for Reconcile on six data sets and two evalu-ation metrics (Section 4).2 Related WorkSeveral coreference resolution systems are cur-rently publicly available.
JavaRap (Qiu et al,2004) is an implementation of the Lappin andLeass?
(1994) Resolution of Anaphora Procedure(RAP).
JavaRap resolves only pronouns and, thus,it is not directly comparable to Reconcile.
GuiTaR(Poesio and Kabadjov, 2004) and BART (Versleyet al, 2008) (which can be considered a succes-sor of GuiTaR) are both modular systems that tar-get the full coreference resolution task.
As such,both systems come close to meeting the majorityof the desiderata set forth in Section 1.
BART,in particular, can be considered an alternative toReconcile, although we believe that Reconcile?sapproach is more flexible than BART?s.
In addi-tion, the architecture and system components ofReconcile (including a comprehensive set of fea-tures that draw on the expertise of state-of-the-artsupervised learning approaches, such as Bengtsonand Roth (2008)) result in performance closer tothe state-of-the-art.Coreference resolution has received much re-search attention, resulting in an array of ap-proaches, algorithms and features.
Reconcileis modeled after typical supervised learning ap-proaches to coreference resolution (e.g.
the archi-tecture introduced by Soon et al (2001)) becauseof the popularity and relatively good performanceof these systems.However, there have been other approachesto coreference resolution, including unsupervisedand semi-supervised approaches (e.g.
Haghighiand Klein (2007)), structured approaches (e.g.McCallum and Wellner (2004) and Finley andJoachims (2005)), competition approaches (e.g.Yang et al (2003)) and a bell-tree search approach(Luo et al (2004)).
Most of these approaches relyon some notion of pairwise feature-based similar-ity and can be directly implemented in Reconcile.3 System DescriptionReconcile was designed to be a research testbedcapable of implementing most current approachesto coreference resolution.
Reconcile is written inJava, to be portable across platforms, and was de-signed to be easily reconfigurable with respect tosubcomponents, feature sets, parameter settings,etc.Reconcile?s architecture is illustrated in Figure1.
For simplicity, Figure 1 shows Reconcile?s op-eration during the classification phase (i.e., assum-ing that a trained classifier is present).The basic architecture of the system includesfive major steps.
Starting with a corpus of docu-ments together with a manually annotated corefer-ence resolution answer key1, Reconcile performs1Only required during training.157Figure 1: The Reconcile classification architecture.the following steps, in order:1.
Preprocessing.
All documents are passedthrough a series of (external) linguistic pro-cessors such as tokenizers, part-of-speechtaggers, syntactic parsers, etc.
These com-ponents produce annotations of the text.
Ta-ble 1 lists the preprocessors currently inter-faced in Reconcile.
Note that Reconcile in-cludes several in-house NP detectors, thatconform to the different data sets?
defini-tions of what constitutes a NP (e.g., MUCvs.
ACE).
All of the extractors utilize a syn-tactic parse of the text and the output of aNamed Entity (NE) extractor, but extract dif-ferent constructs as specialized in the corre-sponding definition.
The NP extractors suc-cessfully recognize about 95% of the NPs inthe MUC and ACE gold standards.2.
Feature generation.
Using annotations pro-duced during preprocessing, Reconcile pro-duces feature vectors for pairs of NPs.
Forexample, a feature might denote whether thetwo NPs agree in number, or whether theyhave any words in common.
Reconcile in-cludes over 80 features, inspired by other suc-cessful coreference resolution systems suchas Soon et al (2001) and Ng and Cardie(2002).3.
Classification.
Reconcile learns a classifierthat operates on feature vectors representingTask SystemsSentence UIUC (CC Group, 2009)splitter OpenNLP (Baldridge, J., 2005)Tokenizer OpenNLP (Baldridge, J., 2005)POS OpenNLP (Baldridge, J., 2005)Tagger + the two parsers belowParser Stanford (Klein and Manning, 2003)Berkeley (Petrov and Klein, 2007)Dep.
parser Stanford (Klein and Manning, 2003)NE OpenNLP (Baldridge, J., 2005)Recognizer Stanford (Finkel et al, 2005)NP Detector In-houseTable 1: Preprocessing components available inReconcile.pairs of NPs and it is trained to assign a scoreindicating the likelihood that the NPs in thepair are coreferent.4.
Clustering.
A clustering algorithm consoli-dates the predictions output by the classifierand forms the final set of coreference clusters(chains).25.
Scoring.
Finally, during testing Reconcileruns scoring algorithms that compare thechains produced by the system to the gold-standard chains in the answer key.Each of the five steps above can invoke differ-ent components.
Reconcile?s modularity makes it2Some structured coreference resolution algorithms (e.g.,McCallum and Wellner (2004) and Finley and Joachims(2005)) combine the classification and clustering steps above.Reconcile can easily accommodate this modification.158Step Available modulesClassification various learners in the Weka toolkitlibSVM (Chang and Lin, 2001)SVMlight (Joachims, 2002)Clustering Single-linkBest-FirstMost Recent FirstScoring MUC score (Vilain et al, 1995)B3 score (Bagga and Baldwin, 1998)CEAF score (Luo, 2005)Table 2: Available implementations for differentmodules available in Reconcile.easy for new components to be implemented andexisting ones to be removed or replaced.
Recon-cile?s standard distribution comes with a compre-hensive set of implemented components ?
thoseavailable for steps 2?5 are shown in Table 2.
Rec-oncile contains over 38,000 lines of original Javacode.
Only about 15% of the code is concernedwith running existing components in the prepro-cessing step, while the rest deals with NP extrac-tion, implementations of features, clustering algo-rithms and scorers.
More details about Recon-cile?s architecture and available components andfeatures can be found in Stoyanov et al (2010).4 Evaluation4.1 Data SetsReconcile incorporates the six most commonlyused coreference resolution data sets, two from theMUC conferences (MUC-6, 1995; MUC-7, 1997)and four from the ACE Program (NIST, 2004).For ACE, we incorporate only the newswire por-tion.
When available, Reconcile employs the stan-dard test/train split.
Otherwise, we randomly splitthe data into a training and test set following a70/30 ratio.
Performance is evaluated accordingto the B3 and MUC scoring metrics.4.2 The Reconcile2010 ConfigurationReconcile can be easily configured with differ-ent algorithms for markable detection, anaphoric-ity determination, feature extraction, etc., and runagainst several scoring metrics.
For the purpose ofthis sample evaluation, we create only one partic-ular instantiation of Reconcile, which we will callReconcile2010 to differentiate it from the generalplatform.
Reconcile2010 is configured using thefollowing components:1.
Preprocessing(a) Sentence Splitter: OpenNLP(b) Tokenizer: OpenNLP(c) POS Tagger: OpenNLP(d) Parser: Berkeley(e) Named Entity Recognizer: Stanford2.
Feature Set - A hand-selected subset of 60 out of themore than 80 features available.
The features were se-lected to include most of the features from Soon et alSoon et al (2001), Ng and Cardie (2002) and Bengtsonand Roth (2008).3.
Classifier - Averaged Perceptron4.
Clustering - Single-link - Positive decision thresholdwas tuned by cross validation of the training set.4.3 Experimental ResultsThe first two rows of Table 3 show the perfor-mance of Reconcile2010.
For all data sets, B3scores are higher than MUC scores.
The MUCscore is highest for the MUC6 data set, while B3scores are higher for the ACE data sets as com-pared to the MUC data sets.Due to the difficulties outlined in Section 1,results for Reconcile presented here are directlycomparable only to a limited number of scoresreported in the literature.
The bottom threerows of Table 3 list these comparable scores,which show that Reconcile2010 exhibits state-of-the-art performance for supervised learning-basedcoreference resolvers.
A more detailed study ofReconcile-based coreference resolution systemsin different evaluation scenarios can be found inStoyanov et al (2009).5 ConclusionsReconcile is a general architecture for coreferenceresolution that can be used to easily create variouscoreference resolvers.
Reconcile provides broadsupport for experimentation in coreference reso-lution, including implementation of the basic ar-chitecture of contemporary state-of-the-art coref-erence systems and a variety of individual mod-ules employed in these systems.
Additionally,Reconcile handles all of the formatting and scor-ing peculiarities of the most widely used coref-erence resolution data sets (those created as partof the MUC and ACE conferences) and, thus,allows for easy implementation and evaluationacross these data sets.
We hope that Reconcilewill support experimental research in coreferenceresolution and provide a state-of-the-art corefer-ence resolver for both researchers and applicationdevelopers.
We believe that in this way Recon-cile will facilitate meaningful and consistent com-parisons of coreference resolution systems.
Thefull Reconcile release is available for download athttp://www.cs.utah.edu/nlp/reconcile/.159System Score Data setsMUC6 MUC7 ACE-2 ACE03 ACE04 ACE05Reconcile2010MUC 68.50 62.80 65.99 67.87 62.03 67.41B3 70.88 65.86 78.29 79.39 76.50 73.71Soon et al (2001) MUC 62.6 60.4 ?
?
?
?Ng and Cardie (2002) MUC 70.4 63.4 ?
?
?
?Yang et al (2003) MUC 71.3 60.2 ?
?
?
?Table 3: Scores for Reconcile on six data sets and scores for comparable coreference systems.AcknowledgmentsThis research was supported in part by the Na-tional Science Foundation under Grant # 0937060to the Computing Research Association for theCIFellows Project, Lawrence Livermore NationalLaboratory subcontract B573245, Department ofHomeland Security Grant N0014-07-1-0152, andAir Force Contract FA8750-09-C-0172 under theDARPA Machine Reading Program.The authors would like to thank the anonymousreviewers for their useful comments.ReferencesA.
Bagga and B. Baldwin.
1998.
Algorithms for scoringcoreference chains.
In Linguistic Coreference Workshopat the Language Resources and Evaluation Conference.Baldridge, J.
2005.
The OpenNLP project.http://opennlp.sourceforge.net/.E.
Bengtson and D. Roth.
2008.
Understanding the value offeatures for coreference resolution.
In Proceedings of the2008 Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP).CC Group.
2009.
Sentence Segmentation Tool.http://l2r.cs.uiuc.edu/ cogcomp/atool.php?tkey=SS.C.
Chang and C. Lin.
2001.
LIBSVM: a Li-brary for Support Vector Machines.
Available athttp://www.csie.ntu.edu.tw/cjlin/libsvm.J.
Finkel, T. Grenager, and C. Manning.
2005.
IncorporatingNon-local Information into Information Extraction Sys-tems by Gibbs Sampling.
In Proceedings of the 21st In-ternational Conference on Computational Linguistics and44th Annual Meeting of the ACL.T.
Finley and T. Joachims.
2005.
Supervised clustering withsupport vector machines.
In Proceedings of the Twenty-second International Conference on Machine Learning(ICML 2005).A.
Haghighi and D. Klein.
2007.
Unsupervised CoreferenceResolution in a Nonparametric Bayesian Model.
In Pro-ceedings of the 45th Annual Meeting of the ACL.T.
Joachims.
2002.
SVMLight, http://svmlight.joachims.org.D.
Klein and C. Manning.
2003.
Fast Exact Inference witha Factored Model for Natural Language Parsing.
In Ad-vances in Neural Information Processing (NIPS 2003).S.
Lappin and H. Leass.
1994.
An algorithm for pronom-inal anaphora resolution.
Computational Linguistics,20(4):535?561.X.
Luo, A. Ittycheriah, H. Jing, N. Kambhatla, andS.
Roukos.
2004.
A mention-synchronous coreferenceresolution algorithm based on the bell tree.
In Proceed-ings of the 42nd Annual Meeting of the ACL.X.
Luo.
2005.
On Coreference Resolution PerformanceMetrics.
In Proceedings of Human Language TechnologyConference and Conference on Empirical Methods in Nat-ural Language Processing (HLT/EMNLP).A.
McCallum and B. Wellner.
2004.
Conditional Modelsof Identity Uncertainty with Application to Noun Coref-erence.
In Advances in Neural Information Processing(NIPS 2004).MUC-6.
1995.
Coreference Task Definition.
In Proceedingsof the Sixth Message Understanding Conference (MUC-6).MUC-7.
1997.
Coreference Task Definition.
In Proceed-ings of the Seventh Message Understanding Conference(MUC-7).V.
Ng and C. Cardie.
2002.
Improving Machine LearningApproaches to Coreference Resolution.
In Proceedings ofthe 40th Annual Meeting of the ACL.NIST.
2004.
The ACE Evaluation Plan.
NIST.S.
Petrov and D. Klein.
2007.
Improved Inference for Un-lexicalized Parsing.
In Proceedings of the Joint Meetingof the Human Language Technology Conference and theNorth American Chapter of the Association for Computa-tional Linguistics (HLT-NAACL 2007).M.
Poesio and M. Kabadjov.
2004.
A general-purpose,off-the-shelf anaphora resolution module: implementationand preliminary evaluation.
In Proceedings of the Lan-guage Resources and Evaluation Conference.L.
Qiu, M.-Y.
Kan, and T.-S. Chua.
2004.
A public referenceimplementation of the rap anaphora resolution algorithm.In Proceedings of the Language Resources and EvaluationConference.W.
Soon, H. Ng, and D. Lim.
2001.
A Machine Learning Ap-proach to Coreference of Noun Phrases.
ComputationalLinguistics, 27(4):521?541.V.
Stoyanov, N. Gilbert, C. Cardie, and E. Riloff.
2009.
Co-nundrums in noun phrase coreference resolution: Mak-ing sense of the state-of-the-art.
In Proceedings ofACL/IJCNLP.160V.
Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler, andD.
Hysom.
2010.
Reconcile: A coreference resolutionresearch platform.
Technical report, Cornell University.Y.
Versley, S. Ponzetto, M. Poesio, V. Eidelman, A. Jern,J.
Smith, X. Yang, and A. Moschitti.
2008.
BART: Amodular toolkit for coreference resolution.
In Proceed-ings of the Language Resources and Evaluation Confer-ence.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A Model-Theoretic CoreferenceScoring Theme.
In Proceedings of the Sixth Message Un-derstanding Conference (MUC-6).X.
Yang, G. Zhou, J. Su, and C. Tan.
2003.
Coreferenceresolution using competition learning approach.
In Pro-ceedings of the 41st Annual Meeting of the ACL.161
