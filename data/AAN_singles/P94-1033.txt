A Corpus-based Approach to AutomaticCompound ExtractionKeh-Y ih  Su  Ming-Wen Wu J ing -Sh in  ChangDept.
of Electr ical Engineering Behavior Design Corporat ion  Dept.
of Electrical EngineeringNat ional  Ts ing-Hua University No.
28, 2F, R&D Road II National  Ts ing-Hua UniversityHsinchu, Taiwan 30043, R.O.C.
Science-Based Industr ial  Park Hsinchu, Talwan 30043, R.O.C.kysu?bdc,  com.
tw Hsinchu, Taiwan 30077, R.O.C.
sh in?hera ,  ee .n thu ,  edu.
1;wmingwen~bdc, com.
twAbst ractAn automatic ompound retrieval method is pro-posed to extract compounds within a text mes-sage.
It uses n-gram mutual information, relativefrequency count and parts of speech as the featuresfor compound extraction.
The problem is mod-eled as a two-class classification problem basedon the distributional characteristics of n-gram to-kens in the compound and the non-compound clus-ters.
The recall and precision using the proposedapproach are 96.2% and 48.2% for bigram com-pounds and 96.6% and 39.6% for trigram com-pounds for a testing corpus of 49,314 words.
Asignificant cutdown in processing time has beenobserved.In t roduct ionIn technical manuals, technical compounds\[Levi 1978\] are very common.
Therefore, the qual-ity of their translations greatly affects the per-formance of a machine translation system.
If acompound is not in the dictionary, it would betranslated incorrectly in many cases; the reasonis: many compounds are not compositional, whichmeans that the translation of a compound is notthe composite of the respective translations of theindividual words \[Chen and Su 1988\].
For exam-ple, the translation of 'green house' into Chineseis not the composite of the Chinese ~anslations of'green' and 'house'.
Under such circumstances,the number of parsing ambiguities will also in-crease due to the large number of possible partsof speech combinations for the individual words.It will then reduce the accuracy rate in disam-biguation and also increase translation time.In practical operations, a computer-translated?
manual is usually concurrently processed by sev-eral posteditors; thus, to maintain the consistencyof translated terminologies among different poste-ditors is very important, because terminologicalconsistency is a major advaatage of machine trans-lation over human translation.
If all the termi-nologies can be entered into the dictionary beforetranslation, the consistency can be automaticallymaintained, the translation quality can be greatlyimproved, and lots of postediting time and consis-tency maintenance cost can be saved.Since compounds are rather productive andnew compounds are created from day to day, itis impossible to exhaustively store all compoundsin a dictionary.
Also, it is too costly and time-consuming to inspect the manual by people forthe compound candidates and update the dictio-nary beforehand.
Therefore, it is important hatthe compounds be found and entered into the dic-tionary before translation without much humaneffort; an automatic and quantitative tool for ex-tracting compounds from the text is thus seriouslyrequired.Several compound extracting approaches havebeen proposed in the literature \[Bourigault 1992,Calzolari and Bindi 1990\].
Traditional rule-basedsystems are to encode some sets of rules to ex-tract likely compounds from the text.
However, alot of compounds obtained with such approachesmay not be desirable since they are not assignedobjective preferences.
Thus, it is not clear howlikely one candidate is considered a compound.In LEXTER, for example, a text corpus is ana-lyzed and parsed to produce a list of likely ter-minological units to be validated by an expert\[Bourigault 1992\].
While it allows the test to bedone very quickly due to the use of simple anal-ysis and parsing rules, instead of complete syn-tactic analysis, it does not suggest quantitativelyto what extent a unit is considered a terminologyand how often such a unit is used in real text.
Itmight therefore xtract many inappropriate t rmi-nologies with high false alarm.
In another statis-tical approach by \[Calzolari and Bindi 1990\], theassociation ratio of a word pair and the disper-sion of the second word are used to decide if it.is a fixed phrase (a compound).
The drawback isthat it does not take the number of occurrencesof the word pair into account; therefore, it is not.242known if the word pair is commonly or rarely used.Since there is no performance evaluation reportedin both frameworks, it is not clear how well theywork.A previous framework by \[Wu and Su 1993\]shows that the mutual information measure andthe relative frequency information are discrimi-native for extracting highly associated and fre-quently encountered n-gram as compound.
How-ever, many non-compound n-grams, like 'is a',which have high mutual information and high rel-ative frequency of occurrence are also recognizedas compounds.
Such n-grams can be rejected ifsyntactic constraints are applied.
In this paper,we thus incorporate parts of speech of the wordsas a third feature for compound extraction.
Anautomatic ompound retrieval method combiningthe joint features of n-gram mutual information,relative frequency count and parts of speech is pro-posed.
A likelihood ratio test method, designedfor a two-class classification task, is used to checkwhether an n-gram is a compound.
Those n-gramsthat pass the test are then listed in the order ofsignificance for the lexicographers to build theseentries into the dictionary.
It is found that, byincorporating parts of speech information, boththe recall and precision for compound extractionis improved.
The simulation result shows that theproposed approach works well.
A significant cut-down of the postediting time has been observedwhen using this tool in an MT system, and thetranslation quality is greatly improved.A Two Cluster Classification Modelfor Compound Extract ionThe first step to extract compounds is to findthe candidate list for compounds.
According toour experience in machine translation, most com-pounds are of length 2 or 3.
Hence, only bigramsand trigrams compounds are of interest o us.
Thecorpus is first processed by a morphological ana-lyzer to normalize very word into its stem form,instead of surface form, to reduce the number' ofpossible alternatives.
Then, the corpus is scannedfrom left to right with the window sizes 2 and 3.The lists of bigrams and trigrams thus acquiredthen form the lists of compound candidates of in-terest.
Since the part of speech pattern for the n-grams (n=2 or 3) is used as a compound extractionfeature, the text is tagged by a discrimination ori-ented probabilistic lexical tagger \[Lin et al 1992\].The n-gram candidates are associated with anumber of features o that they can be judged asbeing compound or non-compound.
In particular,we use the mutual information among the wordsin an n-gram, the relative frequency count of then-gram, and the part of speech patterns associated243with the word n-grams for the extraction task.Such features form an 'observation vector' ?
(to bedescribed later) in the feature space for an inputn-gram.
Given the input features, we can modelthe compound extraction problem as a two-classclassification problem, in which an n-gram is ei-ther classified as a compound or a non-compound,using a likelihood ratio )t for decision making:,x = P ( ,~ IM?)
x P(M?)P(~IMn?)
x P(M,~)where Mc stands for the event that 'the n-gramis produced by a compound model', Mnc standsfor the alternative vent that 'the n-gram is pro-duced by a non-compound model', and ?
is theobservation associated with the n-gram consistingof the joint features of mutual information, rela-tive frequency and part of speech patterns.
Thetest is a kind of likelihood ratio test commonlyused in statistics \[Papoulis 1990\].
If A > 1, it ismore likely that the n-gram belongs to the com-pound cluster.
Otherwise, it is assigned to thenon-compound cluster.
Alternatively, we coulduse the logarithmic likelihood ratio In A for testing:if In A > O, the n-gram is considered a compound;it is, otherwise, considered a non-compound.Features  fo r  Compound Ret r ieva lThe statistics of mutual information among thewords in the n-grams, the relative frequency countfor each n-gram and the transition probabilitiesof the parts of speech of the words are adoptedas the discriminative features for classification asdescribed in the following subsections.Mutua l  In fo rmat ion  Mutual information is ameasure of word association.
It compares theprobability of a group of words to occur together(joint probability) to their probabilities of occur-ring independently.
The bigram mutual informa-tion is known as \[Church and Hanks 1990\]:P(x, y)I(x; y) = log2 P(x) x P(y)where x and y are two words in the corpus, andI (x;y) is the mutual information of these twowords (in this order).
The mutual information ofa trigram is defined as \[Su et al 1991\]:PD(X,y,z)I(x; y; z) = log 2 Pz(x, y, z)where PD(X,y,z) -- P (x ,y ,z )  is the probabilityfor x, y and z to occur jointly (Dependently), andPi(x, y, z) is the probability for x, y and z to oc-cur by chance (Independently), i.e., Pz(x, y, z) =_P(x) x P(y) x P (z )+P(x)  x P(y, z )+P(x,  y) x P(z).In general, I(.)
>> 0 implies that the words in theu-gram are strongly associated.
Ot.herwise, theirappearance may be simply by chance.Relat ive Frequency  Count  The relative fre-quency count for the i th n-gram is defined as:f~Kwhere fi is the total number of occurrences of thei th n-gram in the corpus, and K is the averagenumber of occurrence of all the entries.
In otherwords, f~ is normalized with respect o K to getthe relative frequency.
Intuitively, a frequently en-countered word n-gram is more likely to be a com-pound than a rarely used n-gram.
Furthermore, itmay not worth the cost of entering the compoundinto the dictionary if it occurs very few times.
Therelative frequency count is therefore used as a fea-ture for compound extraction.Using both the mutual information and rel-ative frequency count as the extraction featuresis desirable since using either of these two fea-tures alone cannot provide enough information forcompound finding.
By using relative frequencycount alone, it is likely to choose the n-gramwith high relative frequency count but low as-sociation {mutual information) among the wordscomprising the n-gram.
For example, if P(x)and P(y) are very large, it may cause a largeP(z,y) even though they are not related.
How-ever, P(x, y)/P(z) ?
P(y) would be small for thiscase .On the other hand, by using mutual informa-tion alone it may be highly unreliable if P(x) andP(y) are too small.
An n-gram may have highmutual information ot because the words withinit are highly correlated but due to a large estima-tion error.
Actually, the relative frequency countand mutual information supplement each other.A group of words of both high relative frequencyand mutual information is most likely to be com-posed of words which are highly correlated, andvery commonly used.
Hence, such an n-gram is apreferred compound candidate.The distribution statistics of the training cor-pus, excluding those n-grams that appear onlyonce or twice, is shown in Table 1 and 2 (MI: mu-tual information, RFC: relative frequency count,cc: correlation coefficient, sd: standard devia-tion).
Note that the means of mutual informa-tion and relative frequency count of the compoundcluster are, in general, larger than those in thenon-compound cluster.
The only exception is themeans of relative frequencies for trigrams.
Sincealmost 86.5% of the non-compound trigrams oc-cur only once or twice, which are not consideredin estimation, the average number of occurrenceof such trigrams are smaller, and hence a largerIn?
?f I mean?f I sd?f I tokens MI MIbigram I 862 I 7.49 I 3.08 Itrigram 245 7.88 2.51I I I RFC I covariance ccbigram I 3.18 I -0.71 I-0.0721trigram 2.18 -0.41 -0.074Table 1: D is t r ibut ion  stat ist ics ofpoundsmean ofRFC2.432.92corn -inoof I mo nof I sdof Itokens MI MItrigram 8057 3.55 2.24I RFC I covariance ccbigram I 3.50 -0.45 l-0.0511trigram 2.99 -0.33 -0.049mean ofRFC2.283.14Table 2: D is t r ibut ion  stat ist ics of  non-compoundsrelative frequency than the compound cluster, inwhich only about 30.6% are excluded from consid-eration.Note also that mutual information and rel-ative frequency count are almost uncorrelated inboth clusters ince the correlation coefficients areclose to 0.
Therefore, it is appropriate to takethe mutual information measure and relative fre-quency count as two supplementary features forcompound extraction.Par ts  of  Speech Part of speech is a very impor-tant feature for extracting compounds.
In mostcases, part of speech of compounds has the forms:\[noun, noun\] or \[adjective, noun\] (for bigrams)and \[noun, noun, noun\], \[noun, preposition, noun\]or \[adjective, noun, noun\] (for trigrams).
There-fore, n-gram entries which violate such syntacticconstraints should be filtered out even with highmutual information and relative frequency count.The precision rate of compound extraction willthen be greatly improved.Parameter  Es t imat ion  andSmooth ingThe parameters for the compound model Mr andnon-compound model M,c can be evaluated forma training corpus that is tagged with parts ofspeech and normalized into stem forms.
The cor-244pus is divided into two parts, one as the trainingcorpus, and the other as the testing set.
The n-grams in the training corpus are further dividedinto two clusters.
The compound cluster com-prises the n-grams already in a compound ictio-nary, and the non-compound cluster consists of then-grams which are not in the dictionary.
How-ever, n-grams that occur only once or twice areexcluded from consideration because such n-gramsrarely introduce inconsistency and the estimationof their mutual information and relative frequencyare highly unreliable.Since each n-gram may have different partof speech (POS) patterns Li in a corpus (e.g.,Li = \[n n\] for a bigram) the mutual informationand relative frequency counts will be estimated foreach of such POS patterns.
Furthermore, a partic-ular POS pattern for an n-gram may have severaltypes of contextual POS's surrounding it.
For ex-ample, a left context of 'adj' category and a rightcontext of 'n' together with the above examplePOS pattern can form an extended POS pattern,such as Lij = \[adj (n n) n\], for the n-gram.
Byconsidering all these features, the numerator fac-tor for the log-likelihood ratio test is simplified inthe following way to make parameter estimationfeasible:P(aT\]Mc) x P(Mc)Hi:I \ [P( i t , ,  RL \[Mc) n, P(Mc) " , I-Ij=l P(Lij IMc)\] xwhere n is the number of POS patterns occuringin the text for the n-gram, rt i is the number ofextended POS patterns corresponding to the i thPOS pattern, Li, Lij is the jth extended POS pat-tern for Li, and MLI and RL~ represent the meansof the mutual information and relative frequencycount, respectively, for n-grams with POS patternLi.
The denominator factor for the non-compoundcluster can be evaluated in the same way.For simplicity, a subscript c (/nc) is usedfor the parameters of the compound (/non-compound) model, e.g., P(~.IMc) ~- Pc(Z).
As-sume that ML.
and RL~ are of Gaussian distribu-tion, then the bivariate probability density func-tion Pc(ML,,RL,) for MLi and RL~ can be evalu-ated from their estimated means and standard e-viations \[Papoulis 1990\].
Further simplification onthe factor Pc(Lij) is also possible.
Take a bigramfor example, and assume that the probability den-sity function depends only on the part of speechpattern of the bigram (C1, C2) (in this order), oneleft context POS Co and one right lookahead POSC3, the above formula can be decomposed as:P(Lo \[Me)= Pc(CO, C1, C2, C3)Pc(CaJC=) x Pc(C2\[C,) x Pc(C, lCo) x &(Co)A similar formulation for trigrams with one leftcontext POS and one right context POS, i.e.,Pc(Co, C1, C2, C3, C4), can be derived in a similarway.The n-gram entries with frequency count _ < 2are excluded from consideration before estimatingparameters, because they introduce little inconsis-tency problem and may introduce large estimationerror.
After the distribution statistics of the twoclusters are first estimated, we calculate the meansand standard deviations of the mutual informa-tion and relative frequency counts.
The entrieswith outlier values (outside the range of 3 stan-dard deviations of the mean) are discarded for es-timating a robust set of parameters.
The factors,like Pc(C2\[C1), are smoothed by adding a flatten-ing constant 1/2 \[Fienberg and Holland 1972\] tothe frequency counts before the probability is es-timated.Simulat ion ResultsAfter all the required parameters are estimated,both for the compound and non-compound clus-ters, each input text is tagged with appropriateparts of speech, and the log-likelihood functionIn$ for each word n-gram is evaluated.
If it turnsout that In ~ is greater than zero, then the n-gramis included in the compound list.
The entries inthe compound list are later sorted in the descend-ing order of A for use by the lexicographers.The training set consists of 12,971 sentences(192,440 words), and the testing set has 3,243sentences (49,314 words) from computer manu-als.
There are totally 2,517 distinct bigrams and1,774 trigrams in the testing set, excluding n-grams which occur less than or equal to twice.The performance of the extraction approach forbigrams and trigrams is shown in Table 3 and 4.The recall and precision for the bigrams are 96.2%and 48.2%, respectively, and they become 96.6%and 39.6% for the trigrams.
The high recall ratesshow that most compounds can be captured to thecandidate list with the proposed approach.
Theprecision rates, on the other hand, indicate that areal compound can be found approximately every2 or 3 entries in the candidate list.
The methodtherefore provides substantial help for updatingthe dictionary with little human efforts.Note that the testing set precision of bigramsis a little higher than the training set.
This sit-uation is unusual; it is due to the deletion of thelow frequency n-grams from consideration.
For in-stance, the number of compounds in the testing setoccupies only a very small portion (about 2.8%)after low frequency bigrams are deleted from con-sideration.
The recall for the testing set is there-fore higher than for the training set.245To make better trade-off between the preci-sion rate and recall, we could adjust he thresholdfor ln~.
For instance, when ln~ = -4  is usedfor separating the two clusters, the recall will beraised with- a lower precision.
On the contrary, byraising the threshold for In ~ to positive numbers,the precision will be raised at the cost of a smallerrecall.training set testing set Irecall rate (%) 97.7 96.2precision rate (%) 44.5 48.2Table 3: Performance for bigrams\[ training set testing setrecall rate (%) I 97.6 96.6precision rate (%) I 40.2 39.6Table 4: Performance for tr igramsTable 5 shows the first five bigrams and tri-grams with the largest ,~ for the testing set.Among them, all five bigrams and four out of fivetrigrams are plausible compounds.-------~ram I tr~gram \]dialog boxmail labelWord User's guideMicrosoft Word User'smain documentdata fileFile menuTemplate option buttonnew document baseFile Name boxTable 5: The first five bigrams and tr igramswith the largest A for the testing set.Concluding RemarksIn machine translation systems, information ofthe source compounds should be available beforeany translation process can begin.
However, sincecompounds are very productive, new compoundsare created from day to day.
It is obviously im-possible to build a dictionary to contain all com-pounds.
To guarantee correct parsing and transla-tion, new compounds must be extracted from theinput text and entered into the dictionary.
How-ever, it is too costly and time-consuming for thehuman to inspect he entire text to find the com-pounds.
Therefore, an automatic method to ex-tract compounds from the input text is required.The method proposed in this paper uses mu-tual information, relative frequency count andpart of speech as the features for discriminatingcompounds and non-compounds.
The compoundextraction problem is formulated as a two clusterclassification problem in which an n-gram is as-signed to one of those two clusters using the like-lihood test method.
With this method, the timefor updating missing compounds can be greatlyreduced, and the consistency between differentposteditors can be maintained automatically.
Thetesting set performance for the bigram compoundsis 96.2% recall rate and 48.2% precision rate.
Fortrigrams, the recall and precision are 96.6% and39.6%, respectively.Re ferences\[Bourigault 1992\] D. Bouriganlt, 1992.
"SurfaceGrammar Analysis for the Extraction of Ter-minological Noun Phrases," In Proceedings ofCOLING-92, vol.
4, pp.
977-981, 14th Inter-national Conference on Computational Linguis-tics, Nantes, France, Aug. 23-28, 1992.\[Calzolari and Bindi 1990\] N. Calzolari and R.Bindi, 1990.
"Acquisition of Lexical Infor-mation from a Large Textual Italian Corpus,"In Proceedings of COLING-90, vol.
3, pp.
54-59, 13th International Conference on Computa-tional Linguistics, Helsinki, Finland, Aug. 20-25, 1990.\[Chen and Su 1988\] S.-C. Chen and K.-Y.
Su,1988.
"The Processing of English Compoundand Complex Words in an English-Chinese Ma-chine Translation System," In Proceedings ofROCLING L Nantou, Taiwan, pp.
87-98, Oct.21-23, 1988.\[Church and Hanks 1990\] K. W. Church and P.Hanks, 1990.
"Word Association Norms, Mu-tual Information, and Lexicography," Compu-tational Linguistics, pp.
22-29, vol.
16, Mar.1990.\[Fienberg and Holland 1972\] S. E. Fienberg andP.
W. Holland, 1972.
"On the Choice of Flat-tening Constants for Estimating MultinominalProbabilities," Journal of Multivariate Analy-sis, vol.
2, pp.
127-134, 1972.\[Levi 1978\] J.-N. Levi, 1978 The Syntax and Se-mantics of Complex Nominals, Academic Press,Inc., New York, NY, USA, 1978.\[Linet al 1992\] Y.-C. Lin, T.-H. Chiang and K.-Y.
Su, 1992.
"Discrimination Oriented Proba-bilistic Tagging," In Proceedings of ROCLINGV, Taipei, Taiwan, pp.
85-96, Sep. 18-20, 1992.\[Papoulis 1990\] A. Papoulis, 1990.
Probability ~'Statistics, Prentice Hall, Inc., Englewood Cliffs,N J, USA, 1990.\[Su et al 1991\] K.-Y.
Su, Y.-L. Hsu and C. Sail-lard, 1991.
"Constructing a Phrase Structure246Grammar by Incorporating Linguistic Knowl-edge and Statistical Log-Likelihood Ratio," InProceedings of ROCLING IV, Kenting, Taiwan,pp.
257-275, Aug. 18-20, 1991.\[Wu and Su 1993\] Ming-Wen Wu and Keh-YihSu, 1993.
"Corpus-based Automatic Com-pound Extraction with Mutual Information andRelative Frequency Count", In Proceedings ofROCLING VI, Nantou, Taiwan, ROC Compu-tational Linguistics Conference VI, pp.
207-216,Sep.
2-4, 1993.247
