Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 130?136,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsNatural Language Inference by Tree-Based Convolutionand Heuristic MatchingLili Mou?,1Rui Men,?1Ge Li,?1Yan Xu,1Lu Zhang,1Rui Yan,2Zhi Jin?11Key Laboratory of High Confidence Software Technologies (Peking University),Ministry of Education, China; Software Institute, Peking University, China{doublepower.mou,menruimr}@gmail.com{lige,xuyan14,zhanglu,zhijin}@sei.pku.edu.cn2Baidu Inc., China yanrui02@baidu.comAbstractIn this paper, we propose the TBCNN-pair model to recognize entailment andcontradiction between two sentences.
Inour model, a tree-based convolutional neu-ral network (TBCNN) captures sentence-level semantics; then heuristic matchinglayers like concatenation, element-wiseproduct/difference combine the informa-tion in individual sentences.
Experimen-tal results show that our model outper-forms existing sentence encoding-basedapproaches by a large margin.1 IntroductionRecognizing entailment and contradiction be-tween two sentences (called a premise and a hy-pothesis) is known as natural language inference(NLI) in MacCartney (2009).
Provided with apremise sentence, the task is to judge whether thehypothesis can be inferred (entailment), or thehypothesis cannot be true (contradiction).Several examples are illustrated in Table 1.NLI is in the core of natural language under-standing and has wide applications in NLP, e.g.,question answering (Harabagiu and Hickl, 2006)and automatic summarization (Lacatusu et al,2006; Yan et al, 2011a; Yan et al, 2011b).
More-over, NLI is also related to other tasks of sen-tence pair modeling, including paraphrase detec-tion (Hu et al, 2014), relation recognition of dis-course units (Liu et al, 2016), etc.Traditional approaches to NLI mainly fall intotwo groups: feature-rich models and formal rea-soning methods.
Feature-based approaches typ-ically leverage machine learning models, but re-quire intensive human engineering to representlexical and syntactic information in two sentences?Equal contribution.
?Corresponding authors.Premise Two men on bicycles competing in a race.People are riding bikes.
EHypothesis Men are riding bicycles on the streets.
CA few people are catching fish.
NTable 1: Examples of relations between a premiseand a hypothesis: Entailment, Contradiction, andNeutral (irrelevant).
(MacCartney et al, 2006; Harabagiu et al, 2006).Formal reasoning, on the other hand, converts asentence into a formal logical representation anduses interpreters to search for a proof.
However,such approaches are limited in terms of scope andaccuracy (Bos and Markert, 2005).The renewed prosperity of neural networks hasmade significant achievements in various NLP ap-plications, including individual sentence modeling(Kalchbrenner et al, 2014; Mou et al, 2015) aswell as sentence matching (Hu et al, 2014; Yinand Sch?utze, 2015).
A typical neural architectureto model sentence pairs is the ?Siamese?
structure(Bromley et al, 1993), which involves an underly-ing sentence model and a matching layer to de-termine the relationship between two sentences.Prevailing sentence models include convolutionalnetworks (Kalchbrenner et al, 2014) and recur-rent/recursive networks (Socher et al, 2011b).
Al-though they have achieved high performance, theymay either fail to fully make use of the syntacti-cal information in sentences or be difficult to traindue to the long propagation path.
Recently, wepropose a novel tree-based convolutional neuralnetwork (TBCNN) to alleviate the aforementionedproblems and have achieved higher performancein two sentence classification tasks (Mou et al,2015).
However, it is less clear whether TBCNNcan be harnessed to model sentence pairs for im-plicit logical inference, as is in the NLI task.In this paper, we propose the TBCNN-pairneural model to recognize entailment and con-tradiction between two sentences.
We lever-130age our newly proposed TBCNN model to cap-ture structural information in sentences, whichis important to NLI.
For example, the phrase?riding bicycles on the streets?
in Table 1 canbe well recognized by TBCNN via the depen-dency relations dobj(riding,bicycles)and prep on(riding,street).
As we cansee, TBCNN is more robust than sequential con-volution in terms of word order distortion, whichmay be introduced by determinators, modifiers,etc.
A pooling layer then aggregates informationalong the tree, serving as a way of semantic com-positonality.
Finally, two sentences?
informationis combined by several heuristic matching lay-ers, including concatenation, element-wise prod-uct and difference; they are effective in capturingrelationships between two sentences, but remainlow complexity.To sum up, the main contributions of this pa-per are two-fold: (1) We are the first to introducetree-based convolution to sentence pair modelingtasks like NLI; (2) Leveraging additional heuris-tics further improves the accuracy while remaininglow complexity, outperforming existing sentenceencoding-based approaches to a large extent, in-cluding feature-rich methods and long short termmemory (LSTM)-based recurrent networks.12 Related WorkEntailment recognition can be viewed as a task ofsentence pair modeling.
Most neural networks inthis field involve a sentence-level model, followedby one or a few matching layers.
They are some-times called ?Siamese?
architectures (Bromley etal., 1993).Hu et al (2014) and Yin and Sch?utze (2015) ap-ply convolutional neural networks (CNNs) as theindividual sentence model, where a set of featuredetectors over successive words are designed toextract local features.
Wan et al (2015) build sen-tence pair models upon recurrent neural networks(RNNs) to iteratively integrate information alonga sentence.
Socher et al (2011a) dynamically con-struct tree structures (analogous to parse trees) byrecursive autoencoders to detect paraphrase be-tween two sentences.
As shown, inherent struc-tural information in sentences is oftentimes impor-tant to natural language understanding.The simplest approach to match two sentences,1Code is released on:https://sites.google.com/site/tbcnninference/perhaps, is to concatenate their vector representa-tions (Zhang et al, 2015; Hu et al, 2014, Arc-I).Concatenation is also applied in our previous workof matching the subject and object in relation clas-sification (Xu et al, 2015; Xu et al, 2016).
Heet al (2015) apply additional heuristics, namelyEuclidean distance, cosine measure, and element-wise absolute difference.
The above methods op-erate on a fixed-size vector representation of a sen-tence, categorized as sentence encoding-based ap-proaches.
Thus the matching complexity is O(1),i.e., independent of the sentence length.
Word-by-word similarity matrices are introduced to enhanceinteraction.
To obtain the similarity matrix, Hu etal.
(2014) (Arc-II) concatenate two words?
vectors(after convolution), Socher et al (2011a) computeEuclidean distance, and Wan et al (2015) applytensor product.
In this way, the complexity is ofO(n2), where n is the length of a sentence; hencesimilarity matrices are difficult to scale and lessefficient for large datasets.Recently, Rockt?aschel et al (2016) intro-duce several context-aware methods for sentencematching.
They report that RNNs over a singlechain of two sentences are more informative thanseparate RNNs; a static attention over the first sen-tence is also useful when modeling the second one.Such context-awareness interweaves the sentencemodeling and matching steps.
In some scenarioslike sentence pair re-ranking (Yan et al, 2016), itis not feasible to pre-calculate the vector represen-tations of sentences, so the matching complexity isofO(n).
Rockt?aschel et al (2016) further developa word-by-word attention mechanism and obtain ahigher accuracy with a complexity order ofO(n2).3 Our ApproachWe follow the ?Siamese?
architecture (like mostwork in Section 2) and adopt a two-step strategy toclassify the relation between two sentences.
Con-cretely, our model comprises two parts:?
A tree-based convolutional neural networkmodels each individual sentence (Figure 1a).Notice that, the two sentences, premise and hy-pothesis, share a same TBCNN model (withsame parameters), because this part aims tocapture general semantics of sentences.?
A matching layer combines two sentences?
in-formation by heuristics (Figure 1b).
After in-dividual sentence models, we design a sen-tence matching layer to aggregate information.We use simple heuristics, including concate-131Poolingh1???????h1????????????????????h2??????h1??h2????????h1???h2(b)?Combining?two?sentences?by?heuristics(a)?Individual?sentence??modeling?by?TBCNNPremise?
?Hypothesish2Extracted?features?by?TBCNNWord?embeddings?along?dependency?treesSentenceembeddings?SoftmaxPoolingFigure 1: TBCNN-pair model.
(a) Individ-ual sentence modeling via tree-based convolution.
(b) Sentence pair modeling with heuristics, afterwhich a softmax layer is applied for output.nation, element-wise product and difference,which are effective and efficient.Finally, we add a softmax layer for output.The training objective is cross-entropy loss, andwe adopt mini-batch stochastic gradient descent,computed by back-propagation.3.1 Tree-Based ConvolutionThe tree-based convolutoinal neural network(TBCNN) is first proposed in our previous work(Mou et al, 2016)2to classify program sourcecode; later, we further propose TBCNN variantsto model sentences (Mou et al, 2015).
This sub-section details the tree-based convolution process.The basic idea of TBCNN is to design a set ofsubtree feature detectors sliding over the parse treeof a sentence; either a constituency tree or a depen-dency tree applies.
In this paper, we prefer the de-pendency tree-based convolution for its efficiencyand compact expressiveness.Concretely, a sentence is first converted to adependency parse tree.3Each node in the de-pendency tree corresponds to a word in the sen-tence; an edge a?b indicates a is governed by b.Edges are labeled with grammatical relations (e.g.,nsubj) between the parent node and its children(de Marneffe et al, 2006).
Words are representedby pretrained vector representations, also knownas word embeddings (Mikolov et al, 2013a).2Preprinted on arXiv on September 2014(http://arxiv.org/abs/1409.5718v1)3Parsed by the Stanford parser(http://nlp.stanford.edu/software/lex-parser.shtml)Now, we consider a set of two-layer subtree fea-ture detectors sliding over the dependency tree.
Ata position where the parent node is p with childnodes c1, ?
?
?
, cn, the output of the feature detec-tor, y, isy = f(Wpp+n?i=1Wr[ci]ci+ b)Let us assume word embeddings (p and ci) areof nedimensions; that the convolutional layer y isnc-dimensional.
W ?
Rnc?neis the weight ma-trix; b ?
Rncis the bias vector.
r[ci] denotes thedependency relation between p and ci.
f is thenon-linear activation function, and we apply ReLUin our experiments.After tree-based convolution, we obtain a set offeature maps, which are one-one corresponding tooriginal words in the sentence.
Therefore, theymay vary in size and length.
A dynamic poolinglayer is applied to aggregate information along dif-ferent parts of the tree, serving as a way of seman-tic compositionality (Hu et al, 2014).
We use themax pooling operation, which takes the maximumvalue in each dimension.Then we add a fully-connected hidden layer tofurther mix the information in a sentence.
The ob-tained vector representation of a sentence is de-noted as h (also called a sentence embedding).Notice that the same tree-based convolution ap-plies to both the premise and hypothesis.Tree-based convolution along with pooling en-ables structural features to reach the output layerwith short propagation paths, as opposed to therecursive network (Socher et al, 2011b), whichis also structure-sensitive but may suffer from theproblem of long propagation path.
By contrast,TBCNN is effective and efficient in learning suchstructural information (Mou et al, 2015).3.2 Matching HeuristicsIn this part, we introduce how vector represen-tations of individual sentences are combined tocapture the relation between the premise and hy-pothesis.
As the dataset is large, we prefer O(1)matching operations because of efficiency con-cerns.
Concretely, we have three matching heuris-tics:?
Concatenation of the two sentence vectors,?
Element-wise product, and?
Element-wise difference.The first heuristic follows the most standard pro-cedure of the ?Siamese?
architectures, while thelatter two are certain measures of ?similarity?
or132?closeness.?
These matching layers are furtherconcatenated (Figure 1b), given bym = [h1;h2;h1?
h2;h1?
h2]where h1?
Rncand h2?
Rncare the sentencevectors of the premise and hypothesis, respec-tively; ???
denotes element-wise product; semi-colons refer to column vector concatenation.
m ?R4ncis the output of the matching layer.We would like to point out that, with subse-quent linear transformation, element-wise differ-ence is a special case of concatenation.
If weassume the subsequent transformation takes theform of W [h1h2]>, where W = [W1W2] isthe weights for concatenated sentence representa-tions, then element-wise difference can be viewedas such that W0(h1?h2) = [W0?W0][h1h2]>.
(W0is the weights corresponding to element-wisedifference.)
Thus, our third heuristic can be ab-sorbed into the first one in terms of model ca-pacity.
However, as will be shown in the exper-iment, explicitly specifying this heuristic signifi-cantly improves the performance, indicating thatoptimization differs, despite the same model ca-pacity.
Moreover, word embedding studies showthat linear offset of vectors can capture relation-ships between two words (Mikolov et al, 2013b),but it has not been exploited in sentence-pair rela-tion recognition.
Although element-wise distanceis used to detect paraphrase in He et al (2015),it mainly reflects ?similarity?
information.
Ourstudy verifies that vector offset is useful in cap-turing generic sentence relationships, akin to theword analogy task.4 Evaluation4.1 DatasetTo evaluate our TBCNN-pair model, we used thenewly published Stanford Natural Language In-ference (SNLI) dataset (Bowman et al, 2015).4The dataset is constructed by crowdsourced ef-forts, each sentence written by humans.
More-over, the SNLI dataset is magnitudes of largerthan previous resources, and hence is particularlysuitable for comparing neural models.
The tar-get labels comprise three classes: Entailment,Contradiction, and Neutral (two irrel-evant sentences).
We applied the standardtrain/validation/test split, contraining 550k, 10k,and 10k samples, respectively.
Figure 2 presents4http://nlp.stanford.edu/projects/snli/Statistics Mean Std# nodes 8.59 4.14Max depth 3.93 1.13Avg leaf depth 3.13 0.65Avg node depth 2.60 0.54Table 2: Statistics of the Stanford Natural Lan-guage Inference dataset where each sentence isparsed into a dependency parse tree.0 0.1 0.2 0.3Dropout rate7678808284Validation acc.
(%)Figure 2: Validation accuracy versus dropout rate(full TBCNN-pair model).additional dataset statistics, especially those rele-vant to dependency parse trees.54.2 Hyperparameter SettingsAll our neural layers, including embeddings, wereset to 300 dimensions.
The model is mostly robustwhen the dimension is large, e.g., several hundred(Collobert and Weston, 2008).
Word embeddingswere pretrained ourselves by word2vec on theEnglish Wikipedia corpus and fined tuned duringtraining as a part of model parameters.
We applied`2penalty of 3?10?4; dropout was chosen by val-idation with a granularity of 0.1 (Figure 2).
We seethat a large dropout rate (?
0.3) hurts the perfor-mance (and also makes training slow) for such alarge dataset as opposed to small datasets in othertasks (Peng et al, 2015).
Initial learning rate wasset to 1, and a power decay was applied.
We usedstochastic gradient descent with a batch size of 50.4.3 PerformanceTable 3 compares our model with previous re-sults.
As seen, the TBCNN sentence pairmodel, followed by simple concatenation alone,outperforms existing sentence encoding-basedapproaches (without pretraining), including afeature-rich method using 6 groups of human-engineered features, long short term memory5We applied collapsed dependency trees, where preposi-tions and conjunctions are annotated on the dependency rela-tions, but these auxiliary words themselves are removed.133ModelTest acc.
Matching(%) complexityUnlexicalized featuresb50.4O(1)Lexicalized featuresb78.2Vector sum + MLPb75.3Vanilla RNN + MLPb72.2LSTM RNN + MLPb77.6CNN + cat 77.0GRU w/ skip-thought pretrainingv81.4TBCNN-pair + cat 79.3TBCNN-pair + cat,?,- 82.1Single-chain LSTM RNNsr81.4O(n)+ static attentionr82.4LSTM + word-by-word attentionr83.5 O(n2)Table 3: Accuracy of the TBCNN-pair model incomparison with previous results (bBowman et al,2015;vVendrov et al, 2015;rRockt?aschel et al,2015).
?cat?
refers to concatenation; ?-?
and ??
?denote element-wise difference and product, resp.Model Variant Valid Acc.
Test Acc.TBCNN+?
73.8 72.5TBCNN+- 79.9 79.3TBCNN+cat 80.8 79.3TBCNN+cat,?
81.6 80.7TBCNN+cat,- 81.7 81.6TBCNN+cat,?,- 82.4 82.1Table 4: Validation and test accuracies ofTBCNN-pair variants (in percentage).
(LSTM)-based RNNs, and traditional CNNs.
Thisverifies the rationale for using tree-based convolu-tion as the sentence-level neural model for NLI.Table 4 compares different heuristics of match-ing.
We first analyze each heuristic separately:using element-wise product alone is significantlyworse than concatenation or element-wise differ-ence; the latter two are comparable to each other.Combining different matching heuristics im-proves the result: the TBCNN-pair model withconcatenation, element-wise product and differ-ence yields the highest performance of 82.1%.
Asanalyzed in Section 3.2, the element-wise differ-ence matching layer does not add to model com-plexity and can be absorbed as a special case intosimple concatenation.
However, explicitly usingsuch heuristic yields an accuracy boost of 1?2%.Further applying element-wise product improvesthe accuracy by another 0.5%.The full TBCNN-pair model outperforms allexisting sentence encoding-based approaches, in-cluding a 1024d gated recurrent unit (GRU)-basedRNN with ?skip-thought?
pretraining (Vendrov etal., 2015).
The results obtained by our modelare also comparable to several attention-basedLSTMs, which are more computationally inten-sive than ours in terms of complexity order.4.4 Complexity ConcernsFor most sentence models including TBCNN, theoverall complexity is at least O(n).
However, anefficient matching approach is still important, es-pecially to retrieval-and-reranking systems (Yanet al, 2016; Li et al, 2016).
For example, ina retrieval-based question-answering or conversa-tion system, we can largely reduce response timeby performing sentence matching based on pre-computed candidates?
embeddings.
By contrast,context-aware matching approaches as describedin Section 2 involve processing each candidategiven a new user-issued query, which is time-consuming in terms of most industrial products.In our experiments, the matching part (Fig-ure 1b) counts 1.71% of the total time during pre-diction (single-CPU, C++ implementation), show-ing the potential applications of our approachin efficient retrieval of semantically related sen-tences.5 ConclusionIn this paper, we proposed the TBCNN-pair modelfor natural language inference.
Our model re-lies on the tree-based convolutional neural net-work (TBCNN) to capture sentence-level seman-tics; then two sentences?
information is com-bined by several heuristics including concatena-tion, element-wise product and difference.
Ex-perimental results on a large dataset show a highperformance of our TBCNN-pair model while re-maining a low complexity order.AcknowledgmentsWe thank all anonymous reviewers for their con-structive comments, especially those on complex-ity issues.
We also thank Sam Bowman, Ed-ward Grefenstette, and Tim Rockt?aschel for theirdiscussion.
This research was supported by theNational Basic Research Program of China (the973 Program) under Grant No.
2015CB352201and the National Natural Science Foundation ofChina under Grant Nos.
61232015, 61421091, and61502014.134ReferencesJohan Bos and Katja Markert.
2005.
Combining shal-low and deep NLP methods for recognizing textualentailment.
In Proceedings of the First PASCALChallenges Workshop on Recognising Textual En-tailment, pages 65?68.Samuel R. Bowman, Gabor Angeli, Christopher Potts,and Christopher D. Manning.
2015.
A large anno-tated corpus for learning natural language inference.In Proceedings of the 2015 Conference on Empiri-cal Methods in Natural Language Processing, pages632?642.Jane Bromley, James W Bentz, L?eon Bottou, Is-abelle Guyon, Yann LeCun, Cliff Moore, EduardS?ackinger, and Roopak Shah.
1993.
Signature ver-ification using a ?Siamese?
time delay neural net-work.
International Journal of Pattern Recognitionand Artificial Intelligence, 7(04):669?688.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th International Conference onMachine learning, pages 160?167.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the Language Resource and Evalua-tion Conference, pages 449?454.Sanda Harabagiu and Andrew Hickl.
2006.
Methodsfor using textual entailment in open-domain ques-tion answering.
In Proceedings of the 21st Inter-national Conference on Computational Linguisticsand the 44th Annual Meeting of the Association forComputational Linguistics, pages 905?912.Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.2006.
Negation, contrast and contradiction in textprocessing.
In Proceedings of AAAI Conference onArtificial Intelligence, pages 755?762.Hua He, Kevin Gimpel, and Jimmy Lin.
2015.
Multi-perspective sentence similarity modeling with con-volutional neural networks.
In Proceedings of the2015 Conference on Empirical Methods in NaturalLanguage Processing, pages 17?21.Baotian Hu, Zhengdong Lu, Hang Li, and QingcaiChen.
2014.
Convolutional neural network archi-tectures for matching natural language sentences.In Advances in Neural Information Processing Sys-tems, pages 2042?2050.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network formodelling sentences.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics, pages 655?665.Finley Lacatusu, Andrew Hickl, Kirk Roberts, YingShi, Jeremy Bensley, Bryan Rink, Patrick Wang, andLara Taylor.
2006.
LCCs GISTexter at DUC 2006:Multi-strategy multi-document summarization.
InProceedings of DUC 2006.Xiang Li, Lili Mou, Rui Yan, and Ming Zhang.
2016.StalemateBreaker: A proactive content-introducingapproach to automatic human-computer conversa-tion.
In Proceedings of the 25th International JointConference on Artificial Intelligence.Yang Liu, Sujian Li, Xiaodong Zhang, and Zhifang Sui.2016.
Implicit discourse relation classification viamulti-task neural networks.
In Proceedings of theThirtieth AAAI Conference on Artificial Intelligence.Bill MacCartney, Trond Grenager, Marie-Catherinede Marneffe, Daniel Cer, and Christopher D. Man-ning.
2006.
Learning to recognize features of validtextual entailments.
In Proceedings of the HumanLanguage Technology Conference of the NAACL,pages 41?48.Bill MacCartney.
2009.
Natural Language Inference.Ph.D.
thesis, Stanford University.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013a.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In NAACL-HLT, pages 746?751.Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, andZhi Jin.
2015.
Discriminative neural sentence mod-eling by tree-based convolution.
In Proceedings ofthe 2015 Conference on Empirical Methods in Nat-ural Language Processing, pages 2315?2325.Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin.2016.
Convolutional neural networks over treestructures for programming language processing.
InProceedings of the Thirtieth AAAI Conference onArtificial Intelligence.Hao Peng, Lili Mou, Ge Li, Yunchuan Chen, YangyangLu, and Zhi Jin.
2015.
A comparative study onregularization strategies for embedding-based neuralnetworks.
In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Process-ing, pages 2106?2111.Tim Rockt?aschel, Edward Grefenstette, Karl MoritzHermann, Tom?a?s Ko?cisk`y, and Phil Blunsom.
2016.Reasoning about entailment with neural attention.In Proceedings of the International Conference onLearning Representations.Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-pher D Manning, and Andrew Y Ng.
2011a.
Dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
In Advances in Neural In-formation Processing Systems, pages 801?809.135Richard Socher, Jeffrey Pennington, Eric H Huang,Andrew Y Ng, and Christopher D Manning.
2011b.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 151?161.Ivan Vendrov, Ryan Kiros, Sanja Fidler, and RaquelUrtasun.
2015.
Order-embeddings of images andlanguage.
arXiv preprint arXiv:1511.06361.Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu,Liang Pang, and Xueqi Cheng.
2015.
A deep ar-chitecture for semantic matching with multiple po-sitional sentence representations.
arXiv preprintarXiv:1511.08277.Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,and Zhi Jin.
2015.
Classifying relations via longshort term memory networks along shortest depen-dency paths.
In Proceedings of Conference on Em-pirical Methods in Natural Language Processing,pages 1785?1794.Yan Xu, Ran Jia, Lili Mou, Ge Li, Yunchuan Chen,Yangyang Lu, and Zhi Jin.
2016.
Improved re-lation classification by deep recurrent neural net-works with data augmentation.
arXiv preprintarXiv:1601.03651.Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,Xiaoming Li, and Yan Zhang.
2011a.
Timeline gen-eration through evolutionary trans-temporal summa-rization.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 433?443.Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,Xiaoming Li, and Yan Zhang.
2011b.
Evolutionarytimeline summarization: A balanced optimizationframework via iterative substitution.
In Proceedingsof the 34th international ACM SIGIR conference onResearch and development in Information Retrieval,pages 745?754.Rui Yan, Yiping Song, and Hua Wu.
2016.
Learn-ing to respond with deep neural networks for re-trieval based human-computer conversation system.In Proceedings of the 39th International ACM SIGIRConference on Research and Development in Infor-mation Retrieval.Wenpeng Yin and Hinrich Sch?utze.
2015.
Convolu-tional neural network for paraphrase identification.In Proceedings of the 2015 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 901?911.Biao Zhang, Jinsong Su, Deyi Xiong, Yaojie Lu, HongDuan, and Junfeng Yao.
2015.
Shallow convo-lutional neural network for implicit discourse rela-tion recognition.
In Proceedings of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 2230?2235.136
