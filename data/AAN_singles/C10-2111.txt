Coling 2010: Poster Volume, pages 963?971,Beijing, August 2010Incremental Chinese Lexicon Extractionwith Minimal Resources on a Domain-Specific CorpusGae?l Patin(1) Texts, Computer Science and Multilingualism Research Center (Ertim)National Institute of Oriental Languages and Civilizations (Inalco)(2) Arisem, Thales Companygael.patin@arisem.comAbstractThis article presents an original lexicalunit extraction system for Chinese.
Themethod is based on an incremental pro-cess driven by an association score featur-ing a minimal resources statistically aidedlinguistic approach.
We also introducea linguistics-based lexical unit definitionand use it to describe an evaluation pro-tocol dedicated to the task.
The experi-mental results on a domain specific cor-pus show that the method performs betterthan other approaches.
The extraction re-sults, evaluated on a random sample of theworking corpus, show a recall of 68.4 %and precision of 37.1 %.1 IntroductionLexical resources are all the more fundamental toNLP systems since domain specific corpora aremultiple and various.
The performance of com-mon tasks, such as Information Retrieval or In-formation Extraction, can be improved by com-prehensive and updated domain specific lexicon(i.e.
terminology).
However the constitution oflexicons raises pragmatic issues, such as develop-ment cost or re-usability, which have a great im-portance in an industrial context ; and also theoret-ical issues, such as the definition of the lexical unitor evaluation protocol, which are crucial for therelevance of the results.
In Chinese text process-ing context, lexicons are particularly important fordictionary-based word segmentation techniques inwhich out-of-vocabulary words are an importantcause of errors (Sproat and Emerson, 2003).In this paper we consider the lexicon extractiontask independent of the word segmentation, thisposition differs from Zhao and Kit?s (2004) pointof view.
Generally speaking, word segmentationaims at delimiting units in a sequence of charac-ters.
The delimited units are usually morpholog-ical lexical units (i.e.
words) and internal com-position of the unit is not considered.
The eval-uation process checks whether each word occur-rence is well delimited.
On the opposite, lexiconextraction aims at extracting lexicon entries froma corpus.
The extracted units are morphologicalor syntactic units and the internal components arealso considered.
The evaluation process checksthe extracted candidates list considering the cor-pus global scope.Many approaches for Chinese lexicon extrac-tion rely on a supervised word segmenter (Wu andJiang, 2003; Li et al, 2004) or a morpho-syntactictagger (Piao et al, 2006) to extract unknownwords.
These techniques perform well but sufferfrom a major drawback, they cannot be appliedefficiently to corpora that cover different domainsthan the calibration corpus.
Some approaches arenested in an unsupervised word segmentation pro-cess and aim at improving its effectiveness.
Fungand Wu (1994) try to select segments using mu-tual information on bigram.
Chang and Su (1997)present an iterative unsupervised lexicon extrac-tion system driven by the quality of segmentationobtained with the discovered lexicon.
This ap-proach, although efficient, imposes an arbitrarily4-character length restriction on candidates.
Otherworks, like this approach, focus on the lexicon orterminology extraction as standalone task.
Feng etal.
(2004) introduce a lexicon extraction unsuper-963vised method based on context variation with veryconvincing results.
Yang et al (2008) focus onterminology extraction using delimiters extractedfrom a training corpus with good results.This study proposes an original answer to theChinese lexicon extraction task using an incre-mental minimal resources method to extract andrank lexical unit candidates.
An annotated refer-ence corpus is required to extract a common-worddictionary and to prepare the data.
The methodhas the advantage of proposing structured candi-dates, which allow interactive candidate filtering.In addition the candidate maximum length is de-termined by the number of associations that allowthe detection of the longer lexical units.
We ex-tend the association measure method introducedby Sun et al (1998) for word segmentation with-out lexical resources.
This paper starts with a lin-guistic definition of the lexical unit which drivesthe method.
We also build on it to propose an im-provement of the evaluation protocol for the Chi-nese lexicon extraction task.2 Lexical Unit DefinitionAlthough defining the Chinese lexical unit is nota trivial task, we think that it is absolutely neces-sary for the understanding of the kind of linguis-tic phenomena we are dealing with.
Without thisknowledge we may miss important features andmay not be able to efficiently evaluate the extrac-tion process.
We introduce two linguistic conceptsto define the lexical units focusing on contempo-rary written Chinese: the morpho-syntactic unitand the lexical content.
These definitions use con-cepts introduced by Polgue`re (2003) applied to theChinese case by Nguyen (2008).2.1 Morpho-syntactic UnitA graphy is the Chinese minimal autonomous or-thographic unit and it approximatively matchesthe glyph concept in computer science.
The fol-lowing glyphs are different Chinese graphies: ?,?, ?, ?, ?.
A morph (noted |m |) is the small-est meaningful unit representable by a sequence ofgraphies.
Morphs are atomic so that they cannotbe representable by a smaller sequence of morphs.The following sequences of graphies are differentmorphs : |longevity?
|, | grape?
?|, | aspirin???
?|, | buy?
|.
Note thatthe graphy ?
does not carry any meaning and isnot a morph.
A morpheme (noted ||M||1) is a set ofmorphs sharing the same lexical content ignoringgrammatical inflection or variants (Table 1).
Chi-nese morphs cannot be inflected, unlike Europeanlanguages, but some graphies have variants.Morpheme Morph||protect?
|| |protect?
||| aspirin????
|| | aspirin????
||| cat?
|| | cat?
| | cat?
|Table 1: Morphemes and related morphsA word-form (noted ( w )) is an autonomousand inseparable sequence of morphs.
Autonomymeans that it can be enunciated individually andcan take place in a syntactic paradigm.
Insepa-rability means that breaking the sequence causesthe loss of the relationship between elements.
Alexeme (noted ( W) ) is a set of word-forms shar-ing the same lexical content ignoring inflection orvariants (Table 2).Lexeme Word-form( aspirin????)
( aspirin|???
?| )( take? )
( take|?|) ( take /prefect/|?|?|) ( take /progressive/|?|?| ) ( take /experience/|?|?| )( insurance?? )
( insurance|?|?|)( panda|?|?|) ( panda|?|?|) ( panda|?|?|)Table 2: Lexemes and associated word-formsA phrase (noted [ s ]) is a syntactic combina-tion of word-forms.
The syntactic nature of thecombination implies that the phrase componentsare relatively free.
A locution (noted [[ S ]]) is aset of lexicalized phrases sharing the same lexicalcontent ignoring inflection or variants (Table 3).Locution Phrase[[ shoot?? ]]
[ shoot(?)(?)]
[ shoot /prefect/(??)(?)
] ...[[ be jealous?? ]]
[ be jealous(?)(?
)][[ insurance company????]]
[ insurance company(??)(??)
]Table 3: Locutions and associated phrases1The standard simplified form is used to represent mor-phemes.964The morphs, word-forms and phrases are themorpho-syntactic units, they describe the compo-sition of lexemes and locutions.2.2 Lexical ContentThe lexical units we look for are lexemes and lo-cutions.
Finding lexical units means identifyingwords-forms and phrases having a lexical content.We use two criteria to define the lexical content:the compositionality criterion and the referential-ity criterion (Table 4).
Units which fulfill at leastone of these criteria are said to have a lexical con-tent.The compositionality criterion (or lexicaliza-tion criterion) is relative to the relationship be-tween the sense of the unit and the sense of itscomponents.
The question is whether or not thesense of the unit can be deduced from the combi-nation of its components.
The referentiality crite-rion is related to the relationship between the unitand the referent concept or object.
The question iswhether or not the referent has specific propertiesfor the speakers.
This criterion is strongly depen-dent on human judgment and the working domain.Referential No-ReferentialCompositional (Chinese food?? )
( anticization???
)[[ insurance company???? ]]
[ African car????
]Lexicalized ( disinfect??)
[[ everyone??
]][[ dividend product???? ]]
[[ selling vinegar as wine??????
]]Table 4: Referential and Compositional unitsThe Table 4 presents examples of four criterioncombinations.
Referentiality and compositional-ity criteria are always applied at the highest as-sociation level, thus [[ insurance company???? ]])
is compositional,although ( insurance??)
and ( company??)
are not compositional.Word-forms are not necessarily compositional orreferential, thus the unit ( anticization???)
does not refer toany concept and we can use the combination of itscomponents to interpret it: ( antiquity?? )
+ || -ation?
||.
Ref-erentiality does not imply lexicalization, thus thecompositional unit [[ German car???? ]]
is referential be-cause it refers to the German car brands or char-acteristics in the automobile context.Morph detectionSegment detectionLexical unit candidate selectionMax levelreached ?Candidates reorganizationPresentation & user interaction useryesnoFigure 1: Method overview3 MethodologyThe method (Figure 1) follows the linguistic in-tuitions developed in the previous section.
Weidentify morpho-syntactic units and select thosethat are likely to have a lexical content to obtainlexical unit candidates (LUCs).
The word-formsand phrases are respectively generated by associ-ations of morphs or word-forms and associationof word-forms or phrases.
We consequently usean incremental process, which associates LUCsas they are selected.
The incremental process isinitiated by detecting every morph and splitingthe corpus into segments.
Then we enumerate allthe morpho-syntactic unit couples and use lexicalcontent criteria to select the couples to associate.This process is repeated until the maximum num-ber of associations is reached.
At the end, theLUCs are reorganized and submitted to the user.The user?s answers are used to filter the remainingLUCs.3.1 Morphs DetectionAs stated in Section 2.1, we consider that themorph is the minimal morpho-syntactic unit.
Ev-ery glyph is considered as a morph unless itcan be included in an ancient loanword morph(( butterfly??)
( garbage?? )
) or a foreign transcription morph(( Italy???)
, ( microphone???)
).
In an ambiguous case thelongest possibility is accepted.
Foreign transcrip-tions are phonetic interpretations of foreign wordsusing the pronunciation of the Chinese graphies.The set of graphies used for transcription is well-965known and closed.
We trained a CRF tagger2using simple features based on current, next andprevious graphies to extract foreign transcriptions(the training corpus is described in Section 4.1).Ancient loanwords importation process is not pro-ductive anymore, thus they are detected using aloanword list.3.2 Segment DetectionThe aim of the segment detection step is to splitthe corpus into segments (i.e.
a succession ofChinese graphies).
Chinese texts contain twokinds of delimiters which are not likely to becomponents of a lexical unit, delimiter-words anddelimiter-expressions.
Delimiter-words are enu-merable with a common word dictionary3 andinclude prepositions (?, ?
), adverbs (?, ?,?
), pronouns (?, ?
?, ??
), interrogativepronouns (?
?, ?
), conjunctions (?
?, ?,??
), discourse structure words (?
?, ??,?
), tonal particles (?, ?)
and tool-words (?
).Delimiter-expressions include numerical expres-sions (???
?, ??
), temporal expressions(???
?, ????
), circumstantial expres-sions (?...?
?, ?...?)
, which are easily de-scribable using shallow context-free grammars.Delimiters are removed from the corpus and usedto delimit the segments.
The inflexions (?, ?,?
), which introduce inflectional variations, arealso removed from the corpus but do not delimitthe segments.
The delimiters identification is con-trolled by rules.
For instance tonal particles areremoved only if they are the end of a segment, dis-course structure words are removed only if theyare the beginning of a segment.
Delimiters andinflexions are not removed if they are inside a se-quence of graphies which is present in a common-word dictionary.3.3 Selection of Lexical Unit CandidatesIn this step, lexical unit candidates (LUCs) areextracted by selecting morpho-syntactic unit cou-ples, which are likely to have a lexical content.The first assumption is that lexical units can al-ways be decomposed into binary trees.
Only asmall number of lexical units do not satisfy this2CRF++ implementation of Conditional Random Fields3We assert that this kind of dictionary is easily availableSentence with delimiters noted {delimiter}:???????????A?????C?????????{?}??????{?}???????????????????{?}??{?}??????{?}??{?}???????{?}??{?}?????{?}??{?}??{?}{?}?{?}???
?Obtained segments noted [segment]:[????????]
[??]
[??]
[??????????????]
[??]
[????]
[????]
[??????]
[??]
[??????]
[??]
[??]
[??????][????]
[??]
[??]
[?]
[???
]Figure 2: Segment detection exampleassumption (e.g.
????
), in such case it ispossible to select a non-linguistically motivatedway to decompose the unit into binary associa-tions.
Thus, every couples of contiguous morpho-syntactic units are iteratively enumerated for eachsegment.
The second assumption is that asso-ciation measures are good statistical evidence todetect lexical content.
Thereby, the associationstrength of morpho-syntactic couples is used as amain criterion to identify relevant candidates.Consider G the alphabet of all Chinese gra-phies, M = G+ the language describing themorpho-syntactic units, Sn a set of segments atstep n, sin = m1,m2, ...,mn the ith segment ofSn where ?m ?
sin | m ?
M and S?n the setof all morpho-syntactic unit couples in Sn seg-ments.
Given the morpho-syntactic unit couplemi, mi+1 ?
S?n (denoted as mi,i+1), the lexical con-tent criteria (LCC(mi,i+1)) matches if the follow-ing conditions are fulfilled:1.
Neither mi nor mi+1 has not been associatedat the current step n.2.
Nb(mi) 6= 1 or Nb(mi+1) 6= 1.3.
AS(mi,i+1) > T .4.
AS(mi,i+1) > AS(mi?1,i)or not LCC(mi?1,i)5.
AS(mi,i+1) > AS(mi+1,i+2)or not LCC(mi+1,i+2)where Nb(x) is the number of occurrences of x,AS(x, y) returns the association score of the cou-966ple x, y computed with a given association mea-sure, and T is the association threshold relative tothe association measure (cf.
4.1).Let S0 the initial set of segments where ?si0 ?
S0,si0 is a segment (cf.
3.2) such that ?m ?
si0, m isa morph (cf.
3.1).
The LUC list is composed ofmorpho-syntactic couples produced by the asso-ciation operator ?
to compute Smax (algorithm 1)with max the maximum number of iteration.S ?
Sn?1while ?mi,i+1 ?
S?| LCC(mi,i+1)S ?
S[mi ?mi+1]endSn ?
S(1)with ?
the association operator whose result is amorpho-syntactic unit, Sn[m1 ?m2] the replace-ment of m1 and m2 by the morpho-syntactic unitm1 ?m2 in the corresponding segment.
See theSection 5 for more details about the maximumnumber of iteration setting.3.4 Candidates ReorganizationOnce LUCs are extracted, we map every LUC tothe couple of morpho-syntactic units it is com-posed of.
These units are called components.Some LUCs are generated from two different cou-ples at the candidate selection step.
For instance,????
is discovered in two ways: ????
?or ?????.
We always choose the most fre-quent option.
When the ?LUC/couple?
map iscreated, we sort the LUCs by their correspond-ing couple association scores.
Finally, if a LUC isranked in the list before its components we movethe components to the position just before it inthe list and use the same rule to recursively checkthe moved components.
The candidates list is ex-pected to be ordered by likelihood deduced by anassociation measure and compositional order.3.5 Presentation and User InteractionThe lexicon extraction task aims at submitting aranked list of candidates to the user in order tohelp him produce lexical resources.
The user isexpected to check the list in this order and themethod uses the user answers to discard not yetverified candidates.
To do so, the user is askedto answer the following questions for each LUCaccording to the definition given in the Section 2:1.
Does the unit have a lexical content ?2.
Is the unit a part of a lexical unit ?If answers to both these questions are ?no?
thenall the candidates having this component are re-moved from the remaining list.4 EvaluationSince the submitted candidates are progressivelymodified according to the user answers, the eval-uated candidates are only the ones submitted tothe user.
We used three measures to evaluate themethod: recall, precision and precision at rank n.Since producing large annotated corpora is costly,we perform the evaluation using a sample of textsfrom the evaluation corpus.
Therefore the scoresobtained are an estimation of the true scores.
Theinter-human variation is not considered here andshould be integrated in further works.4.1 Evaluation parametersThe morphs and the segment detection step usedata from a reference corpus: The Lancaster Cor-pus of Mandarin Chinese (McEnery and Xiao,2004).
The corpus is composed of text sampleschoose in various domain and genre corpora, itcontains two millions of glyphs and it is anno-tated according to the Beijing University anno-tation guideline4.
This corpus is mainly usedto extract delimiter-words, to produce the gram-mar for delimiter-expressions and to extract acommon-word dictionary.
All foreign transcrip-tions are also annotated for the CRF tagger train-ing (cf.
3.1).The lexical unit detection step is evaluated us-ing four well-known association measures: Point-wise Mutual Information (PMI), Poisson-Striling(PS) (Quasthoff and Wolff, 2002), Log-likelihood(LL), Pointwise Mutual Information Cube (PMI3)(Daille, 1994).
These measures are detailed in ta-ble 5.
The significant association threshold is in-tuitively given by the statistical interpretation of4http://icl.pku.edu.cn/icl groups/corpus/coprus-annotation.htm967AM Formulas VariablesPMI log pxypx?p?y x, y : wordsx : all words but x?
: all wordspx : x probabilityfx : x frequencyN : nb.
of bigram?
= N ?
px ?
pyk = fxyf?xy =fx?f?yNLL 2{x,x},{y,y}Xi,jfij logfijf?ijPS k(log k ?
log ??
1)logNPMI3 log Nfxy3fx?f?yTable 5: Association score calculationthe formulas for MI and PS.
Thus, these measuresare used for LCC?s selection criterion 2 and T isset to 0 (cf.
3.3).
A threshold can not be deducedfrom PS and PMI3, therefore they are only usedfor LCC?s comparison criteria 3 & 4.4.2 Evaluation ProcessTo prepare the evaluation we randomly selectedtwenty texts in an evaluation corpus and anno-tated lexical units according to the linguistic de-scription given in Section 2.
For each sampletext, we obtained a set of lexical unit trees (Ta-ble 3) corresponding to all the encountered lex-ical units.
N-trees are used for units which cannot be transformed into binary tree.
Two evalua-tion sets are defined, the shallow set which con-tains the root nodes of the lexical unit trees andthe deep set which contains the inner nodes5 ofthe lexical unit trees.
Given the four examplesof Figure 3, the shallow set contains [????],[????
], [?????]
and (???)
; and the deepset contains [????
], (??
), (??
), [????],[?????
], (??
), (???)
and (??
).Experiments with different parameters producedifferent candidate lists and an expert interven-tion is required to evaluate each candidate list.
Toavoid this problem, all the repeated sequences ofnon-inflectional graphies are generated from theannotated sample texts and intersected with theLUC list.
The obtained list is a projection of thecandidate list on the sample texts.
This trick al-lows us to extract all LUCs appearing in the sam-5All nodes excluding leaves.[????](??
)|?| |?|(??
)|?| |?|[????
]|?| |?| |?| |?|[?????](???)
(??)(?)
(?)(???)(??
)|?| |?||?|Figure 3: Lexical unit treesple texts and evaluate them automatically.5 ExperimentsThe experiments are conducted on insurance do-main corpus containing ten million graphies.
Thisevaluation corpus is composed of news and ar-ticles collected automatically from Chinese in-surance companies websites.
The text fields areextracted with an xhtml parser.
Several textfields, such as menus or buttons, are repeatedand duplicates are removed to avoid noise.
Thepresented method, referred as ILex (Incremen-tal Lexicon Extractor), is applied using the pre-viously mentioned 4 measures (cf.
4.1).
Theevaluation is based on couple of measures, thefirst measure is dedicated to candidates selection(LCC 2.)
and the second to candidates compar-ison (LCC 3.
& 4.).
The comparison measure isalso used to sort the candidates (cf.
3.4).
The max-imal number of iterations is set to 3 (for a maxi-mal depth of 4), which is the maximum numberof associations required to compose the majorityof lexical units in the reference corpus.
The preci-sion and recall are computed on the deep set in or-der to consider all valid lexical units, the recall onthe shallow set is given to see the results on widerlexical units (Table 6).
The results show that PMI-LL couple performs better overall than the othermeasures.
It can be noticed that the scores are rel-atively close (?1.8% for precision and ?7.0% fordeep recall) meaning that the choice of the associ-ation measure has a low influence over the results.For the further experiments are conducted withPMI-LL, which achieves the best recall score.968Selection PMI PSComparison LL PMI3 LL PMI3Precision 37.1 38.9 37.3 38.1Deep recall 68.4 65.6 62.3 61.4Shallow recall 75.1 74.2 70.6 70.6Table 6: Measure combinations resultsThe method extracted 585,794 LUCs from thewhole corpus using the PMI-LL couple before ap-plying the user interaction step.
The candidatelist projection (cf.
4.2) contains 4,539 LUCs.
Theuser decisions are simulated with the lexical unittrees obtained from sample texts.
In total 312LUCs were removed in consequence of the userinteraction (cf.
3.5), without this step the preci-sion decreases to 33.7%.
The 1,246 LUCs presentin the common-word dictionary are ignored.
Fi-nally 1,886 invalid candidates and 1,105 valid lex-ical units are submitted to the user, the evaluationis based on these 3,059 LUCs.Lexical unit Rank Nb.
[[ policy agricultural insurance??????? ]]
155 1798[[Tai Kang Life Insurance???? ]]
453 1,854( insurer??? )
1,048 4,999[[ Nan Kai University???? ]]
2,828 111[[ Los Angeles tourism professionals??????? ]]
9,647 3[[ life insurance???? ]]
11,647 871(Wang Enshao (person)??? )
14,617 2[[ compensated use???? ]]
34,596 8( Taihu Lake Basin????)
102,612 2(wait an opportunity?? )
126,044 31[[ The People?s Republic of China labor contract law????????????]]
387,235 1Table 7: Sample of extracted lexical unitsA sample of extracted lexical units is presentedin Table 7.
In this list, the lower number ofoccurrences is 1 and the longest unit length is12 graphies.
Most of the extracted lexical unitare terms, a significant number of people names,common words and larger named entities are ex-tracted too.
The most part of the very frequentlexical units are ranked at the top of the list butsome low frequency LUCs are ranked over thehigh frequency candidates.
The Figure 4 presentscandidate list decileprecision (sorted by LL)precision (sorted by frequency)precision (no user interaction)shallow recalldeep recall01020304050607080901001 2 3 4 5 6 7 8 9 10Figure 4: ILex results using PMI-LLthe results as a function of the LUC list deciles.The LL sorting is compared to frequency sort-ing for the precision at rank n. The LL sort-ing curve is above the frequency sorting curve,this fact shows that LL is more efficient at sort-ing valid LUCs.
The majority of the missed can-didates have a low number of occurrences (?3)and 57.8% of the longest lexical unit (>7) arealso missed.
Most of extraction errors have alow number of occurrences, 40.1% of the er-rors are caused by lexical unit composition errors(e.g.
*(insurance??
)?|study?| in [ insurance institute(??)(??)]
or *( reform??
)|commission?| in[reform & development commission(??)(??
)|?| ]) and 59.9% by association errors(e.g.
( extend??)?
[agricultural Insurance???? ]
or ( standard??)?(development??
)).The AccessVar method (Feng et al, 2004),an unsupervised lexicon extraction method hav-ing the best performance, was reimplemented andused as a reference.
This method uses the corpussubstrings?
number of distinct contexts, noted AV(accessor variety), to extract candidates.
Access-Var is configured by an accessors variety thresh-old (AVT), which is the minimal AV required tohold a candidate, the number of occurrences ofcandidates is consequently greater or equal to theAVT.
For the experiments, the candidate maximallength is set to 7 graphies6 and AVT to 3.
Sim-ilarly, ILex candidates appearing less than threetimes and having a length greater than 7 are dis-carded.
The ILex user interaction is not appliedfor this comparison.
In order unify the input data,AccessVar handles the segments detected by ILex6Higher values cause space complexity issues.969candidate list projection rankILex-MI3 Precision (no user interaction)ILex-MI3 RecallAccessVar (with ILex segments) precisionAccessVar (with ILex segments) recall0102030405060708090100500 1000 1500 2000 2500 3000Figure 5: ILex & AccessVar resultsinstead of the corpus full text.AccessVar and ILex extract respectively125,467 and 116,412 LUCs and the candidate listprojection contains 2,190 and 1,876 LUCs.
Theresults are computed on the deep set (figure 5).AccessVar and ILex achieve respectively recall of43.7% and 49.0%.
A total of 667 of the lexicalunits extracted are common to both methods, 161lexical units are extracted exclusively by ILexand 74 lexical units are extracted exclusivelyby AccessVar.
This means that both methodshave close covering capacities.
From rank 100to rank 700, the results are close but the curvesbegin to diverge after this rank, this trend meansthat the performance are similar for the 700best candidates.
However, ILex achieves 44.4%precision which is 10.6% higher than AccessVar(33.8%), this difference, in view of the close re-call score, shows that ILex generates less invalidcandidates.
The errors specific to AccessVar aredue to context adhesion errors (e.g.
*(company??
)?|specialty?| in[insurance industry(??)(??
)], [ insurance product(??)(??
)], [ insurance produce(??)(??)]
etc.
), orassociation errors (e.g.
*|country?|?|East?|, *( industry??)?
( group??
)).ILex avoids these errors because of three mech-anisms.
First, the statistical likelihood betweenthe couple components is tested (e.g.
*|country?|?|East?|PMI score is negative).
Second, the methodchecks association likelihood of the contextsbefore associating two morpho-syntactic units,(e.g.
(aeronautic??
)( industry??)
score is over *( industry??)?
( group??
)score in [ Aviation Industries Corporation of China??????????]).
Third, the in-cremental association process determine smallerunit before trying associating bigger couples(e.g.
( insurance?? )
and ( industry?? )
are associated before[insurance industry???? ]
).6 Conclusion and Further WorksThe presented method features incremental lexi-cal unit extraction with interactive candidate fil-tering capability.
The maximal candidate length isnot imposed directly, but instead is determined bythe maximal number of associations.
The lexicalresources required are re-usable and non-domainspecific, which significantly reduce their cost forlong-term deployment.
The method achievesdecent performance and improves the referencemethod?s precision for this task.
Furthermore, theextracted results include low-frequency and longcandidates which are known to be difficult to ex-tract.
Finally, the binary association process al-lows us to sort the candidates by association mea-sure, which is more relevant than frequency.This paper also introduced the beginning ofa linguistically consistent lexical unit definition.This definition draws the outlines of a corpus an-notation guide dedicated to the lexicon extractiontask.
The evaluation process is improved by thelexical unit trees annotations and a candidate listprojection technique, which allows full-automaticestimation of extraction system performance.The first upcoming objective is the develop-ment of a robust evaluation protocol for the lex-ical extraction task.
This is crucial for further im-provements and means that the variation betweenannotators of the evaluation corpus, and the sta-bility of the method over different corpora need tobe considered.
Finally we will try to solve the notyet managed lexicon extraction issues, Latin char-acters tokens which cause the method miss someextractions (e.g.
( [product name]?
?A ) ), and the discontinuouslocutions (e.g.
[[ reach on the phone???? ]]
in ( call / through???)???
( phone??
)or [[bear responsibility??? ]]
in ???(bear?)?????(responsibility??
)).AcknowledgementsOur sincere thanks to the anonymous reviewers.Special thanks to Pierre Zweigenbaum, to all mycolleagues from Arisem and Ertim and to the cor-pus annotators without which this work would notbe possible.970ReferencesChang, Jing-Shin and Keh-Yih Su.
1997.
An un-supervised iterative method for Chinese new lexi-con extraction.
International Journal of Computa-tional Linguistics & Chinese Language Processing,vol.
1(1), pp.
101?157.Daille, Be?atrice.
1994.
Approche mixte pourl?extraction automatique de terminologie: statis-tiques lexicales et filtres linguistiques.
PhD thesis,Universite?
Paris 7.Feng, Haodi, Kang Chen, Xiaotie Deng and WeiminZheng.
2004.
Accessor variety criteria for Chi-nese word extraction.
Computational Linguistics,vol.
30:1, pp.
75-93.Fung, Pascale and Dekai Wu.
1994.
Statistical aug-mentation of a Chinese machine-readable dictio-nary.
In WVLC-2, Second Annual Workshop onVery Large Corpora (COLING-94), Kyoto, Japan,pp.
69-85.Hai, Zhao and Chunyu Kit.
2008.
An Empirical Com-parison of Goodness Measures for UnsupervisedChinese Word Segmentation with a Unified Frame-work.
In Proceedings of the Third InternationalJoint Conference on Natural Language Processing(IJCNLP-08), Hyderabad, India, Vol.
1, pp.
9-16.McEnery, Tony and Richard Xiao.
2004.
The Lan-caster Corpus of Mandarin Chinese: A corpus formonolingual and contrastive language study.
Pro-ceedings of the Fourth International Conference onLanguage Resources and Evaluation (LREC-04),Lisbon, Portugal, pp.
1175-1178.Li, Hongqiao, Changning Huang, Jiangfen Gao andXiaozhong Fan.
2004.
The use of SVM for Chi-nese new word identification.
First InternationalJoint Conference on Natural Language Processing(IJCNLP-04), Sanya, China, pp.
497-504.Nguyen, Etienne Van Tien.
2008.
Unite?
lexi-cale et morphologie en chinois mandarin ?
versl?e?laboration d?un DEC du chinois.
Phd thesis,Montre?al University.Piao, Scott S. L., Guangfan Sun, Paul Rayson and QiYuan.
2006.
Automatic extraction of Chinese mul-tiword expressions with a statistical tool.
Workshopon Multi-word-expressions in a Multilingual Con-text held in conjunction with the 11th EACL, Trento,Italy, pp.
17-24.Polgue`re, Alain.
2003.
Lexicologie et se?mantiquelexicale.
Notions fondamentales.
Presses del?Universite?
de Montre?al, Coll.
Parame`tres.Quasthoff, Uwe and Christian Wolff.
2003.
The Pois-son collocation measure and its application.
InSecond International Workshop on ComputationalApproaches to Collocations, Vienna, Austria.Sproat, Richard and Tom Emerson.
2003.
Thefirst international Chinese word segmentation bake-off.
Proceedings of the Second SIGHAN Workshopon Chinese Language Processing, Japan, vol.
17,pp.
133-143.Sun, Maosong, Danyang Shen and Benjamin K Tsou.1998.
Chinese Word segmentation without lex-icon and hand-crafted training data.
Proceed-ings of the 17th international conference on Com-putational linguistics, Montreal, Canada, Vol.
2,pp.
1265-1271.Wu, Andi and Zixin Jiang.
2000.
Statistically-enhanced new word identification in a rule-basedChinese system.
Proceedings of the 2nd Chi-nese Language Processing Workshop, Hong-Kong,vol.
12, pp.
45-51.Yang, Yuhang, Qin Lu and Tiejun Zhao.
2008.
Chi-nese Term Extraction Using Minimal Resources.Proceedings of the 22nd International Conferenceon Computational Linguistics, Manchester, UnitedKingdom, Vol.
1, pp.1033-1040.971
