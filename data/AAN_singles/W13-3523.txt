Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 212?221,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsTopic Models + Word Alignment = A Flexible Framework for ExtractingBilingual Dictionary from Comparable CorpusXiaodong Liu, Kevin Duh and Yuji MatsumotoGraduate School of Information ScienceNara Institute of Science and Technology8916-5 Takayama, Ikoma, Nara 630-0192, Japan{xiaodong-l,kevinduh,matsu}@is.naist.jpAbstractWe propose a flexible and effective frame-work for extracting a bilingual dictionaryfrom comparable corpora.
Our approachis based on a novel combination of topicmodeling and word alignment techniques.Intuitively, our approach works by con-verting a comparable document-alignedcorpus into a parallel topic-aligned cor-pus, then learning word alignments us-ing co-occurrence statistics.
This topic-aligned corpus is similar in structure to thesentence-aligned corpus frequently used instatistical machine translation, enabling usto exploit advances in word alignment re-search.
Unlike many previous work, ourframework does not require any language-specific knowledge for initialization.
Fur-thermore, our framework attempts to han-dle polysemy by allowing multiple trans-lation probability models for each word.On a large-scale Wikipedia corpus, wedemonstrate that our framework reliablyextracts high-precision translation pairs ona wide variety of comparable data condi-tions.1 IntroductionA machine-readable bilingual dictionary plays avery important role in many natural language pro-cessing tasks.
In machine translation (MT), dic-tionaries can help in the domain adaptation set-ting (Daume III and Jagarlamudi, 2011).
Incross-lingual information retrieval (CLIR), dictio-naries serve as efficient means for query trans-lation (Resnik et al 2011).
Many other multi-lingual applications also rely on bilingual dictio-naries as integral components.One approach for building a bilingual dictio-nary resource uses parallel sentence-aligned cor-pora.
This is often done in the context of Statis-tical MT, using word alignment algorithms suchas the IBM models (Brown et al 1993; Och andNey, 2003).
Unfortunately, parallel corpora maybe scarce for certain language-pairs or domains ofinterest (e.g., medical and microblog).Thus, the use of comparable corpora for bilin-gual dictionary extraction has become an activeresearch topic (Haghighi et al 2008; Vulic?
etal., 2011).
Here, a comparable corpus is definedas collections of document pairs written in dif-ferent languages but talking about the same topic(Koehn, 2010), such as interconnected Wikipediaarticles.
The challenge with bilingual dictionaryextraction from comparable corpus is that exist-ing word alignment methods developed for paral-lel corpus cannot be directly applied.We believe there are several desiderata for bilin-gual dictionary extraction algorithms:1.
Low Resource Requirement: The approachshould not rely on language-specific knowl-edge or a large scale seed lexicon.2.
Polysemy Handling: One should handle thefact that a word form may have multiplemeanings, and such meanings may be trans-lated differently.3.
Scalability: The approach should run effi-ciently an massively large-scale datasets.Our framework addresses the above desiredpoints by exploiting a novel combination of topicmodels and word alignment, as shown in Figure 1.Intuitively, our approach works by first convertinga comparable document-aligned corpus into a par-212Figure 1: Proposed Frameworkallel topic-aligned corpus, then apply word align-ment methods to model co-occurence within top-ics.
By employing topic models, we avoid theneed for seed lexicon and operate purely in therealm of unsupervised learning.
By using wordalignment on topic model results, we can easilymodel polysemy and extract topic-dependent lexi-cons.Specifically, let we be an English word andwf be a French word.
One can think of tradi-tional bilingual dictionary extraction as obtaining(we, wf ) pairs in which the probability p(we|wf )or p(wf |we) is high.
Our approach differsby modeling p(we|wf , t) or p(wf |we, t) instead,where t is a topic.
The key intuition is that it iseasier to tease out the translation of a polysemousword e given p(wf |we, t) rather than p(wf |we).A word may be polysemous, but given a topic,there is likely a one-to-one correspondence for themost appropriate translation.
For example, un-der the simple model p(wf |we), the English word?free?
may be translated into the Japanese word??
(as in free speech) or ??
(as in freebeer) with equal 0.5 probability; this low proba-bility may cause both translation pairs to be re-jected by the dictionary extraction algorithm.
Onthe other hand, given p(wf |we, t), where t is ?pol-itics?
or ?shopping?, we can allow high probabili-ties for both words depending on context.Our contribution is summarized as follows:?
We propose a bilingual dictionary extrac-tion framework that simultaneously achievesall three of the desiderata: low resource re-quirement, polysemy handling, and scalabil-ity.
We are not aware of any previous worksthat address all three.?
Our framework is extremely flexible andsimple-to-implement, consisting of a novelcombination of existing topic modeling toolsfrom machine learning and word alignmenttools from machine translation.2 Related WorkThere is a plethora of research on bilingual lexi-con extraction from comparable corpora, startingwith seminal works of (Rapp, 1995; Fung and Lo,1998).
The main idea is to assume that translationpairs have similar contexts, i.e.
the distributionalhypothesis, so extraction consists of 3 steps: (1)identify context windows around words, (2) trans-late context words using a seed bilingual dictio-nary, and (3) extract pairs that have high result-ing similarity.
Methods differ in how the seeddictionary is acquired (Koehn and Knight, 2002;De?jean et al 2002) and how similarity is defined(Fung and Cheung, 2004; Tamura et al 2012).Projection-based approaches have also been pro-posed, though they can be shown to be relatedto the aforementioned distributional approaches(Gaussier et al 2004); for example, Haghighi(2008) uses CCA to map vectors in different lan-guages into the same latent space.
Laroche (2010)presents a good summary.Vulic?
et al(2011) pioneered a new approachto bilingual dictionary extraction based on topicmodeling approach which requires no seed dictio-nary.
While our approach is motivated by (Vulic?et al 2011), we exploit the topic model in a verydifferent way (explained in Section 4.2).
They donot use word alignments like we do and thus can-not model polysemy.
Further, their approach re-quires training topic models with a large numberof topics, which may limit the scalability of theapproach.Recently, there has been much interest in mul-tilingual topic models (MLTM) (Jagarlamudi andDaume, 2010; Mimno et al 2009; Ni et al 2009;Boyd-Graber and Blei, 2009).
Many of these mod-els give p(t|e) and p(t|f), but stop short of extract-ing a bilingual lexicon.
Although topic models cangroup related e and f in the same topic cluster, theextraction of a high-precision dictionary requiresadditional effort.
One of our contributions here isan effective way to do this extraction using wordalignment methods.3 System Components: BackgroundThis section reviews MLTMs and Word Align-ment, the main components of our framework.The knowledgeable readers may wish to skim thissection for notation and move to Section 4, whichdescribes our contribution.3.1 Multilingual Topic ModelAny multilingual topic model may be used withour framework.
We use the one by Mimno et213al.
(2009), which extends the monolingual La-tent Dirichlet Allocation model (Blei et al 2003).Given a comparable corpus E in English and Fin a foreign language, we assume that the docu-ment pair boundaries are known.
For each doc-ument pair di = [dei , dfi ] consisting of Englishdocument dei and Foreign document dfi (wherei ?
{1, .
.
.
, D}, D is number of document pairs),we know that dei and dfi talk about the sametopics.
While the monolingual topic model letseach document have its own so-called document-specific distribution over topics, the multilingualtopic model assumes that documents in each tu-ple share the same topic prior (thus the compara-ble corpora assumption) and each topic consists ofseveral language-specific word distributions.
Thegenerative story is shown in Algorithm 1.for each topic k dofor l ?
{e, f} dosample ?lk ?
Dirichlet(?l);endendfor each document pair di dosample ?i ?
Dirichlet(?
);for l ?
{e, f} dosample zl ?Multinomial(?i);for each word wl in dli dosample wl ?
p(wl|zl, ?l);endendendAlgorithm 1: Generative story for (Mimno et al2009).
?i is the topic proportion of documentpair di.
Words wl are drawn from language-specific distributions p(wl|zl, ?l), where lan-guage l indexes English e or Foreign f .
Herepairs of language-specific topics ?l are drawnfrom Dirichlet distributions with prior ?l.3.2 Statistical Word AlignmentFor a sentence-pair (e,f), let e =[we1, we2, .
.
.
we|e|] be the English sentence with |e|words and f = [wf1 , wf2 , .
.
.
wf|f |] be the foreignsentence with |f | words.
For notation, we willindex English words by i and foreign wordsby j.
The goal of word alignment is to find analignment function a : i ?
j mapping words in eto words in f (and vice versa).We will be using IBM Model 1 (Brown et al1993; Och and Ney, 2003), which proposes thefollowing probabilistic model for alignment:p(e, a, |f) ?|e|?i=1p(wei |wfa(i)) (1)Here, p(wei |wfa(i)) captures the translation prob-ability of the English word at position i from theforeign word at position j = a(i), where the ac-tual alignment a is a hidden variable, and trainingcan be done via EM.
Although this model does notincorporate much linguistic knowledge, it enablesus to find correspondence between distinct objectsfrom paired sets.
In machine translation, the dis-tinct objects are words from different languageswhile the paired sets are sentence-aligned corpora.In our case, our distinct objects are also wordsfrom distinct languages but our pair sets will betopic-aligned corpora.4 Proposed Framework for BilingualDictionary ExtractionThe general idea of our proposed framework issketched in Figure 1: First, we run a multilin-gual topic model to convert the comparable cor-pora to topic-aligned corpora.
Second, we runa word alignment algorithm on the topic-alignedcorpora in order to extract translation pairs.
Theinnovation is in how this topic-aligned corpora isdefined and constructed, the link between the twostages.
We describe how this is done in Section 4.1and show how existing approaches are subsumedin our general framework in Section 4.2.4.1 Topic-Aligned CorporaSuppose the original comparable corpus has Ddocument pairs [dei , dfi ]i=1,...,D. We run a mul-tilingual topic model with K topics, where Kis user-defined (Section 3.1).
The topic-alignedcorpora is defined hierarchically as a set of sets:On the first level, we have a set of K topics,{t1, .
.
.
, tk, .
.
.
, tK}.
On the second level, foreach topic tk, we have a set of D ?word col-lections?
{Ck,1, .
.
.
, Ck,i, .
.
.
, Ck,D}.
Each wordcollection Ck,i represents the English and foreignwords that occur simultaneously in topic tk anddocument di.For clarity, let us describe the topic-alignedcorpora construction process step-by-step togetherwith a flow chart in Figure 2:1.
Train a multilingual topic model.214Figure 2: Construction of topic-aligned corpora.2.
Infer a topic assignment for each token in thecomparable corpora, and generate a list of wordcollections Ck,i occurring under a given topic.3.
Re-arrange the word collections such that Ck,ibelonging to the same topic are grouped together.This resulting set of sets is called topic-alignedcorpora, since it represents word collections linkedby the same topics.4.
For each topic tk, we run IBM Model 1 on{Ck,1, .
.
.
, Ck,i, .
.
.
, Ck,D}.
In analogy to statis-tical machine translation, we can think of thisdataset as a parallel corpus of D ?sentence pairs?,where each ?sentence pair?
contains the Englishand foreign word tokens that co-occur under thesame topic and the same document.
Note thatword alignment is run independently for eachtopic, resulting in K topic-dependent lexiconsp(we|wf , tk).5.
To extract a bilingual dictionary, we find pairs(we, wf ) with high probability under the model:p(we|wf ) =?kp(we|wf , tk)p(tk|wf ) (2)The first term is the topic-dependent bilingual lex-icon from Step 4; the second term is the topic pos-terior from the topic model in Step 1.In practice, we will compute the probabilitiesof Equation 2 in both directions: p(we|, wf ) as inEq.
2 and p(wf |we) =?k p(wf |we, tk)p(tk|we).The bilingual dictionary can then be extractedbased on a probabilities threshold or some bidirec-tional constraint.
We choose to use a bidirectionalconstraint because it gives very high-precisiondictionaries and avoid the need to tune probabilitythresholds.
A pair (e?, f?)
is extracted if thefollowing holds:e?
= argmaxep(e|f = f?
); f?
= argmaxfp(f |e = e?
)(3)To summarize, the main innovation of our ap-proach is that we allow for polysemy as topic-dependent translation explicitly in Equation 2, anduse a novel combination of topic modeling andword alignment techniques to compute the termp(we|wf , tk) in an unsupervised fashion.4.2 Alternative ApproachesTo the best of our knowledge, (Vulic?
et al 2011)is the only work focuses on using topic modelsfor bilingual lexicon extraction like ours, but theyexploit the topic model results in a different way.Their ?Cue Method?
computes:p(we|wf ) =?kp(we|tk)p(tk|wf ) (4)This can be seen as a simplification ofour Eq.
2, where Eq.
4 replaces p(we|tk, wf )with the simpler p(we|tk).
Another vari-ant is the so-called Kullback-Liebler (KL)method, which scores translation pairs by?
?k p(tk|we) log p(tk|we)/p(tk|wf ).
In eithercase, their contribution is the use of topic-worddistributions like p(tk|wf ) or p(wf |tk) to computetranslation probabilities.1 Our formulation can beconsidered more general because we do not havethe strong assumption that we is independent of1A third variant uses TF-IDF weighting, but is conceptu-ally similar and have similar results.215wf given tk, and focus on estimating p(we|wf , tk)directly with word alignment methods.5 Experimental Setup5.1 Data SetWe perform experiments on the Kyoto Wiki Cor-pus2.
We chose this corpus because it is a parallelcorpus, where the Japanese edition of Wikipediais translated manually into English sentence-by-sentence.
This enables us to use standard wordalignment methods to create a gold-standard lexi-con for large-scale automatic evaluation.3From this parallel data, we prepared severaldatasets at successively lower levels of compara-bility.
As shown in Table 1, Comp100% is a com-parable version of original parallel data, deletingall the sentence alignments but otherwise keepingall content on both Japanese and English sides.Comp50% and Comp20% are harder datasetsthat keep only 50% and 20% (respectively) of ran-dom English sentences per documents.
We furtheruse a real comparable corpus (Wiki)4, which isprepared by crawling the online English editionsof the corresponding Japanese articles in the Ky-oto Wiki Corpus.
The Comp datasets are con-trolled scenarios where all English content is guar-anteed to have Japanese translations; no such guar-antee exists in our Wiki data.5.2 Experimental Results1.
How does the proposed framework compareto previous work?We focus on comparing with previous topic-modeling approaches to bilingual lexicon extrac-tion, namely (Vulic?
et al 2011).
The methods are:?
Proposed: The proposed method whichexploits a combination of topic modelingand word alignment to incorporate topic-dependent translation probabilities (Eq.
2).?
Cue: From (Vulic?
et al 2011), i.e.
Eq.
4.2http://alaginrc.nict.go.jp/WikiCorpus/index E.html3We trained IBM Model 4 using GIZA++ for both direc-tions p(e|f) and p(f |e).
Then, we extract word pair (e?, f?)
asa ?gold standard?
bilingual lexicon if it satisfies Eq.
3.
Dueto the large data size and the strict bidirectional requirementimposed by Eq.
3, these ?gold standard?
bilingual dictionaryitems are of high quality (94% precision by a manual checkon 100 random items).
Note sentence alignments are usedonly for creating this gold-standard.4The English corresponding dataset, gold-standard andML-LDA software used in our experiments are available athttps://sites.google.com/site/buptxiaodong/home/resourceDataset #doc #sent(e/j) #voc(e/j)Comp100% 14k 472k/472k 152k/116kComp50% 14k 236k/472k 100k/116kComp20% 14k 94k/472k 62k/116kWiki 3.6k 127k/163k 88k/61kTable 1: Datasets: the number of documentpairs (#doc), sentences (#sent) and vocabularysize (#voc) in English (e) and Japanese (j).
Forpre-processing, we did word segmentation onJapanese using Kytea (Neubig et al 2011) andPorter stemming on English.
A TF-IDF basedstop-word lists of 1200 in each language is ap-plied.
#doc is smaller for Wiki because not allJapanese articles in Comp100% have English ver-sions in Wikipedia during the crawl.?
JS: From (Vulic?
et al 2011).
SymmetrizingKL by Jensen-Shannon (JS) divergence im-proves results, so we report this variant.5We also have a baseline that uses no topic models:IBM-1 runs IBM Model 1 directly on the compa-rable dataset, assuming each document pair is a?sentence pair?.Figure 3 shows the ROC (Receiver Operat-ing Characteristic) Curve on the Wiki dataset.The ROC curve lets us observe the change inRecall as we gradually accept more translationpairs as dictionary candidates.
In particular,it measures the true positive rate (i.e.
recall =|{Gold(e, f)}?
{Extracted(e, f)}|/#Gold)and false positive rate (fraction of false extractionsover total number of extractions) at varying levelsof thresholds.
This is generated by first computingp(e|f) + p(f |e) as the score for pair (e, f) foreach method, then sorting the pairs by this scoreand successive try different thresholds.The curve of the Proposed method dominatesthose of all other methods.
It is also the bestin Area-Under-Curve scores (Davis and Goadrich,2006), which are 0.96, 0.90, 0.85 and 0.71, forProposed, IBM-1, Cue, and JS, respectively.6ROC is insightful if we are interested in com-paring methods for all possible thresholds, but inpractice we may desire a fixed operating point.Thus we apply the bidirectional heuristic of Eq.5Topic model hyperparameters for Proposed, Cue, andJS are ?
= 50/K and ?
= 0.1 following (Vulic?
et al 2011).6The Precision-Recall curve gives a similar conclusion.We do not show it here since the extremely low precision ofJS makes the graph hard to visualize.
Instead see Table 2.216Figure 3: ROC curve on the Wiki dataset.
Curveson upper-left is better.
Cue, JS, Proposed all useK=400 topics.
Note that Proposed is best.K Method Prec ManP #Extracted100Cue 0.027 0.02 3800JS 0.013 0.01 3800Proposed 0.412 0.36 3800400Cue 0.059 0.02 2310JS 0.075 0.02 2310Proposed 0.631 0.56 2310- IBM-1 0.514 0.42 2310- IBM-1* 0.493 0.39 3714Table 2: Precision on the Wiki dataset.K=number of topics.
Precision (Prec) is definedas |{Gold(e,f)}?
{Extracted(e,f)}|#Extracted .
ManP is preci-sion evaluated manually on 100 random items.3 to extract a fixed set of lexicon for Proposed.For the other methods, we calibrated the thresh-olds to get the same number of extractions.
Thenwe compare the precision, as shown in Table 2.1.
Proposed outperforms other methods,achieving 63% (automatic) precision and56% (manual) precision.2.
The JS and Cue methods suffer from ex-tremely poor precision.
We found that thisis due to insufficient number of topics, andis consistent with the results by (Vulic?
et al2011) which showed best results with K >2000.
However, we could not train JS/Cueon such a large number of topics since it iscomputationally-demanding for a corpus aslarge as ours.7 In this regard, the Proposed7The experiments in (Vulic?
et al 2011) has vocabularyFigure 4: Robustness of method under differentdata conditions.method is much more scalable, achievinggood results with low K, satisfying one oforiginal desiderata.83.
IBM-1 is doing surprisingly well, consider-ing that it simply treats document pairs assentence pairs.
This may be due to someextent to the structure of the Kyoto Wikidataset, which contains specialized topics(about Kyoto history, architecture, etc.
), lead-ing to a vocabulary-document co-occurrencematrix with sparse block-diagonal structure.Thus there may be enough statistics trainIBM-1 on documents.2.
How does the proposed method perform un-der different degrees of ?comparability?
?We next examined how our methods perform un-der different data conditions.
Figure 4 plots the re-sults in terms of Precision evaluated automatically.We observe that Proposed (K=400) is relativelystable, with a decrease of 14% Precision goingfrom fully-comparable to real Wikipedia compa-rable corpora.
The degradation for K=100 is muchlarger (31%) and therefore not recommended.
Webelieve that robustness depends on K, because thesize of 10k, compared to 150k in our experiments.
We haveattempted large K ?
1000 but Cue did not finish after days.8We have a hypothesis as to why Cue and JS depend onlargeK.
Eq.
2 is a valid expression for p(we|wf ) that makeslittle assumptions.
We can view Eq.
4 as simplifying the firstterm of Eq.
2 from p(we|tk, wf ) to p(we|tk).
Both prob-ability tables have the same output-space (we), so the samenumber of parameters is needed in reality to describe this dis-tribution.
By throwing outwf , which has large cardinality, tkneeds to grow in cardinality to compensate for the loss of ex-pressiveness.2175 10 15 200100003000001000030000WordCountTopic CountenjpFigure 5: Power-law distribution of number ofword types with X number of topics.topic model of (Mimno et al 2009) assumes onetopic distribution per document pair.
For low-levels of comparability, a small number of topicsmay not sufficiently model the differences in top-ical content.
This suggests the use of hierarchicaltopic models (Haffari and Teh, 2009) or other vari-ants in future work.3.
What are the statistical characteristics oftopic-aligned corpora?First, we show the word-topic distribution frommultilingual topic modeling in the K = 400 sce-nario (first step of Proposed, Cue, and JS).
Foreach word type w, we count the number of topicsit may appear in, i.e.
nonzero probabilities accord-ing to p(w|t).
Fig.
5 shows the number of wordtypes that have x number of topics.
This power-law is expected since we are modeling all words.9Next we compute the statistics after construct-ing the topic-aligned corpora (Step 3 of Fig.
2).For each part of the topic-aligned corpora, wecompute the ratio of distinct English word typesvs.
distinct Japanese word types.
If the ratio isclose to one, that means the partition into topic-aligned corpora effectively separates the skewedword-topic distribution of Fig 5.
We found thatthe mean ratio averaged across topics is low at1.721 (variance is 1.316), implying that withineach topic, word alignment is relatively easy.4.
What kinds of errors are made?We found that the proposed method makes sev-eral types of incorrect lexicon extractions.
First,Word Segmentation ?errors?
on Japanese could9This means that it is not possible to directly extract lexi-con by taking the cross-product (wf , we) of the top-n wordsin p(wf |tk) and p(we|tk) for the same topic tk, as suggestedby (Mimno et al 2009).
When we attempted to do this, us-ing top-2 words per p(wf |tk) and p(we|tk), we could onlyobtain precision of 0.37 for 1600 extractions.
This skeweddistribution similarly explains the poor performance of Cue.make it impossible to find a proper English trans-lation (e.g., ????
should translate to ?Prince-Takechi?
but system proposes ?Takechi?).
Sec-ond, an unrelated word pair (we, wf ) may be in-correctly placed in the same topic, leading to anIncorrect Topic error.
Third, even if (we, wf ) in-tuitively belong to the same topic, they may not bedirect translations; an extraction in this case wouldbe a Correct Topic, Incorrect Alignment error(e.g.
?????
?, a particular panfried snack,is incorrectly translated as ?panfry?
).Table 3 shows the distribution of error types bya manual classification.
Incorrect Alignment er-rors are most frequent, implying the topic modelsare doing a reasonable job of generating the topic-aligned corpus.
The amount of Incorrect Topic isnot trivial, though, so we would still imagine moreadvanced topic models to help.
Segmentation er-rors are in general hard to solve, even with a betterword segmenter, since in general one-to-one cross-lingual word correspondence is not consistent?webelieve the solution is a system that naturally han-dles multi-word expressions (Baldwin, 2011).Word Segmentation Error 14Incorrect Topic 29Correct Topic, Incorrect Alignment 40Reason Unknown 7Table 3: Counts of various error types.5.
What is the computation cost?Timing results on a 2.4GHz Opteron CPU for var-ious steps of Proposed and Cue are shown in Ta-ble 5.
The proposed method is 5-8 times fasterthan Cue.
For Proposed, computation time isdominated by topic modeling while GIZA++ ontopic-aligned corpora is extremely fast.
Cue addi-tionally suffers from computational complexity incalculating Eq.4, especially when both p(we|tk)and p(tk|wf ) have high cardinality.
In compari-son, calculating Eq.2 is fast since p(we|wf , tk) isin practice quite sparse.6.
What topic-dependent lexicons are learnedand do they capture polysemy?In our evaluation so far, we have only produced anone-to-one bilingual dictionary (due to the bidirec-tionality constraint of Eq.3).
We have seen howtopic-dependent translation models p(wf |we, tk)is important in achieving good results.
However,Eq.2 marginalizes over the topics so we do notknow what topic-dependent lexicons are learned.218English Japanese1(gloss), Japanese2(gloss)interest ??
(a sense of concern),??
(a charge of money borrowing)count ??
(act of reciting numbers),??
(nobleman)free ??
(as in ?free?
speech),??
(as in ?free?
beer)blood ??
(line of descent),?
(the red fluid)demand ??
(as noun),??
(as verb)draft ??
(as verb),??
(as noun)page ???
(one leaf of e.g.
a book),??
(youthful attendant)staff ????
(general personel),??
(as in political ?chief of staff?
)director ??
(someone who controls),??
(board of directors)??
(movie director)beach ?
(area of sand near water),???
(leisure spot at beach)actor ??
(theatrical performer),??
(movie actor)Table 4: Examples of topic-dependent translations given by p(wf |we, tk).
The top portion shows ex-amples of polysemous English words.
The bottom shows examples where English is not decisivelypolysemous, but indeed has distinct translations in Japanese based on topic.K topic giza Eq.2 Eq.4 Prp Cue100 180 3 20 1440 203 1620200 300 3 33 2310 336 2610400 780 5 42 3320 827 4100Table 5: Wall-clock times in minutes for TopicModeling (topic), Word Alignment (giza), andp(we|wf ) calculation.
Overall time for Pro-posed (Prp) is topic+giza+Eq.2 and for Cue istopic+Eq.4.Here, we explore the model p(wf |we, tk) learnedat Step 4 of Figure 2 to see whether it capturessome of the polysemy phenomenon mentioned inthe desiderata.
It is not feasible to automaticallyevaluate topic-dependent dictionaries, since thisrequires ?gold standard?
of the form (e, f, t).
Thuswe cannot claim whether our method successfullyextracts polysemous translations.
Instead we willpresent some interesting examples found by ourmethod.
In Table 4, we look at potentially pol-ysemous English words we, and list the highest-probability Japanese translations wf conditionedon different tk.
We found many promising caseswhere the topic identification helps divide the dif-ferent senses of the English word, leading to thecorrect Japanese translation achieving the highestprobability.6 ConclusionWe proposed an effective way to extract bilin-gual dictionaries by a novel combination of topicmodeling and word alignment techniques.
Thekey innovation is the conversion of a compara-ble document-aligned corpus into a parallel topic-aligned corpus, which allows word alignmenttechniques to learn topic-dependent translationmodels of the form p(we|wf , tk).
While this kindof topic-dependent translation has been proposedfor the parallel corpus (Zhao and Xing, 2007),we are the first to enable it for comparable cor-pora.
Our large-scale experiments demonstratedthat the proposed framework outperforms existingbaselines under both automatic metrics and man-ual evaluation.
We further show that our topic-dependent translation models can capture some ofthe polysemy phenomenon important in dictionaryconstruction.
Future work includes:1.
Exploring other topic models (Haffari and Teh,2009) and word alignment techniques (DeNeroand Macherey, 2011; Mermer and Saraclar, 2011;Moore, 2004) in our framework.2.
Extract lexicon from massive multilingual col-lections.
Mausum (2009) and Shezaf (2010) showthat language pivots significantly improve the pre-cision of distribution-based approaches.
Sincemultilingual topic models can easily be trained onmore than 3 languages, we expect it will give a bigboost to our approach.AcknowledgmentsWe thank Mamoru Komachi, Shuhei Kondo andthe anonymous reviewers for valuable discussionsand comments.
Part of this research was executedunder the Commissioned Research of National In-stitute of Information and Communications Tech-nology (NICT), Japan.219ReferencesTimothy Baldwin.
2011.
Mwes and topic mod-elling: enhancing machine learning with linguis-tics.
In Proceedings of the Workshop on MultiwordExpressions: from Parsing and Generation to theReal World, MWE ?11, pages 1?1, Stroudsburg, PA,USA.
Association for Computational Linguistics.D.
Blei, A. Ng, and M. Jordan.
2003.
Latent dirichletallocation.
Journal of Machine Learning Research.Jordan Boyd-Graber and David M. Blei.
2009.
Multi-lingual topic models for unaligned text.
In UAI.P.
Brown, S. Della Pietra, V. Della Pietra, and R. Mer-cer.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
ComputationalLinguistics, 19(2).Hal Daume III and Jagadeesh Jagarlamudi.
2011.
Do-main adaptation for machine translation by min-ing unseen words.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages407?412, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Jesse Davis and Mark Goadrich.
2006.
The relation-ship between precision-recall and ROC curves.
InICML.Herve?
De?jean, E?ric Gaussier, and Fatia Sadat.
2002.An approach based on multilingual thesauri andmodel combination for bilingual lexicon extraction.In Proceedings of the 19th international conferenceon Computational linguistics - Volume 1, COLING?02, pages 1?7.John DeNero and Klaus Macherey.
2011.
Model-based aligner combination using dual decomposi-tion.
In Proceedings of the Association for Com-putational Linguistics (ACL).Pascale Fung and Percy Cheung.
2004.
Miningverynon-parallel corpora: Parallel sentence and lex-icon extraction via bootstrapping and em.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing.Pascale Fung and Yuen Yee Lo.
1998.
Translating un-known words using nonparallel, comparable texts.In COLING-ACL.Eric Gaussier, J.M.
Renders, I. Matveeva, C. Goutte,and H. Dejean.
2004.
A geometric view on bilin-gual lexicon extraction from comparable corpora.
InProceedings of the 42nd Meeting of the Associationfor Computational Linguistics (ACL?04), Main Vol-ume, pages 526?533, Barcelona, Spain, July.Ghloamreza Haffari and Yee Whye Teh.
2009.
Hier-archical dirichlet trees for information retrieval.
InNAACL.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In Proceedings of ACL-08: HLT, pages 771?779, Columbus, Ohio, June.Association for Computational Linguistics.Jagadeesh Jagarlamudi and Hal Daume.
2010.
Ex-tracting multilingual topics from unaligned compa-rable corpora.
In ECIR.Philipp Koehn and Kevin Knight.
2002.
Learn-ing a translation lexicon from monolingual corpora.In Proceedings of ACL Workshop on UnsupervisedLexical Acquisition.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press, New York, NY, USA,1st edition.Audrey Laroche and Philippe Langlais.
2010.
Re-visiting context-based projection methods for term-translation spotting in comparable corpora.
InProceedings of the 23rd International Conferenceon Computational Linguistics (Coling 2010), pages617?625, Beijing, China, August.
Coling 2010 Or-ganizing Committee.Mausam, Stephen Soderland, Oren Etzioni, Daniel S.Weld, Michael Skinner, and Jeff Bilmes.
2009.Compiling a massive, multilingual dictionary viaprobabilistic inference.
In ACL.Coskun Mermer and Murat Saraclar.
2011.
Bayesianword alignment for statistical machine translation.In ACL.David Mimno, Hanna Wallach, Jason Naradowsky,David A. Smith, and Andrew McCallum.
2009.Polylingual topic models.
In EMNLP.Robert Moore.
2004.
Improving IBM word alignmentmodel 1.
In ACL.Graham Neubig, Yosuke Nakata, and Shinsuke Mori.2011.
Pointwise prediction for robust, adaptablejapanese morphological analysis.
In The 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies (ACL-HLT) Short Paper Track, pages 529?533, Portland,Oregon, USA, 6.Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.2009.
Mining multilingual topics from wikipedia.In WWW.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Comput.
Linguist., 29(1):19?51, March.Reinhard Rapp.
1995.
Identifying word translations innon-parallel texts.
In Proceedings of the 33rd An-nual Meeting of the Association for ComputationalLinguistics.220Philip Resnik, Douglas Oard, and Gina Levow.
2011.Improved cross-language retrieval using backofftranslation.
In Proceedings of the First InternationalConference on Human Language Technology.Daphna Shezaf and Ari Rappoport.
2010.
Bilinguallexicon generation using non-aligned signatures.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, ACL ?10,pages 98?107.
Association for Computational Lin-guistics.Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.2012.
Bilingual lexicon extraction from compara-ble corpora using label propagation.
In Proceedingsof the 2012 Joint Conference on Empirical Meth-ods in Natural Language Processing and Compu-tational Natural Language Learning, pages 24?36,Jeju Island, Korea, July.
Association for Computa-tional Linguistics.Ivan Vulic?, Wim De Smet, and Marie-Francine Moens.2011.
Identifying word translations from compa-rable corpora using latent topic models.
In Pro-ceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 479?484, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Bing Zhao and Eric P. Xing.
2007.
HM-BiTAM:Bilingual Topic Exploration, Word Alignment, andTranslation.
In NIPS.221
