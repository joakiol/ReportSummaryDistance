Proceedings of ACL-08: HLT, pages 950?958,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsIntegrating Graph-Based and Transition-Based Dependency ParsersJoakim NivreVa?xjo?
University Uppsala UniversityComputer Science Linguistics and PhilologySE-35195 Va?xjo?
SE-75126 Uppsalanivre@msi.vxu.seRyan McDonaldGoogle Inc.76 Ninth AvenueNew York, NY 10011ryanmcd@google.comAbstractPrevious studies of data-driven dependencyparsing have shown that the distribution ofparsing errors are correlated with theoreticalproperties of the models used for learning andinference.
In this paper, we show how theseresults can be exploited to improve parsingaccuracy by integrating a graph-based and atransition-based model.
By letting one modelgenerate features for the other, we consistentlyimprove accuracy for both models, resultingin a significant improvement of the state ofthe art when evaluated on data sets from theCoNLL-X shared task.1 IntroductionSyntactic dependency graphs have recently gaineda wide interest in the natural language processingcommunity and have been used for many problemsranging from machine translation (Ding and Palmer,2004) to ontology construction (Snow et al, 2005).A dependency graph for a sentence represents eachword and its syntactic dependents through labeleddirected arcs, as shown in figure 1.
One advantageof this representation is that it extends naturally todiscontinuous constructions, which arise due to longdistance dependencies or in languages where syntac-tic structure is encoded in morphology rather than inword order.
This is undoubtedly one of the reasonsfor the emergence of dependency parsers for a widerange of languages.
Many of these parsers are basedon data-driven parsing models, which learn to pro-duce dependency graphs for sentences solely froman annotated corpus and can be easily ported to anyFigure 1: Dependency graph for an English sentence.language or domain in which annotated resourcesexist.Practically all data-driven models that have beenproposed for dependency parsing in recent years canbe described as either graph-based or transition-based (McDonald and Nivre, 2007).
In graph-basedparsing, we learn a model for scoring possible de-pendency graphs for a given sentence, typically byfactoring the graphs into their component arcs, andperform parsing by searching for the highest-scoringgraph.
This type of model has been used by, amongothers, Eisner (1996), McDonald et al (2005a), andNakagawa (2007).
In transition-based parsing, weinstead learn a model for scoring transitions fromone parser state to the next, conditioned on the parsehistory, and perform parsing by greedily taking thehighest-scoring transition out of every parser stateuntil we have derived a complete dependency graph.This approach is represented, for example, by themodels of Yamada and Matsumoto (2003), Nivre etal.
(2004), and Attardi (2006).Theoretically, these approaches are very different.The graph-based models are globally trained and useexact inference algorithms, but define features over alimited history of parsing decisions.
The transition-based models are essentially the opposite.
They uselocal training and greedy inference algorithms, but950define features over a rich history of parsing deci-sions.
This is a fundamental trade-off that is hardto overcome by tractable means.
Both models havebeen used to achieve state-of-the-art accuracy for awide range of languages, as shown in the CoNLLshared tasks on dependency parsing (Buchholz andMarsi, 2006; Nivre et al, 2007), but McDonald andNivre (2007) showed that a detailed error analysisreveals important differences in the distribution oferrors associated with the two models.In this paper, we consider a simple way of inte-grating graph-based and transition-based models inorder to exploit their complementary strengths andthereby improve parsing accuracy beyond what ispossible by either model in isolation.
The methodintegrates the two models by allowing the outputof one model to define features for the other.
Thismethod is simple ?
requiring only the definition ofnew features ?
and robust by allowing a model tolearn relative to the predictions of the other.2 Two Models for Dependency Parsing2.1 PreliminariesGiven a set L = {l1, .
.
.
, l|L|} of arc labels (depen-dency relations), a dependency graph for an inputsentence x = w0, w1, .
.
.
, wn (where w0 = ROOT) isa labeled directed graph G = (V,A) consisting of aset of nodes V = {0, 1, .
.
.
, n}1 and a set of labeleddirected arcs A ?
V ?V ?L, i.e., if (i, j, l) ?
A fori, j ?
V and l ?
L, then there is an arc from nodei to node j with label l in the graph.
A dependencygraphG for a sentence xmust be a directed tree orig-inating out of the root node 0 and spanning all nodesin V , as exemplified by the graph in figure 1.
Thisis a common constraint in many dependency parsingtheories and their implementations.2.2 Graph-Based ModelsGraph-based dependency parsers parameterize amodel over smaller substructures in order to searchthe space of valid dependency graphs and producethe most likely one.
The simplest parameterizationis the arc-factored model that defines a real-valuedscore function for arcs s(i, j, l) and further definesthe score of a dependency graph as the sum of the1We use the common convention of representing words bytheir index in the sentence.score of all the arcs it contains.
As a result, the de-pendency parsing problem is written:G = argmaxG=(V,A)?
(i,j,l)?As(i, j, l)This problem is equivalent to finding the highestscoring directed spanning tree in the complete graphover the input sentence, which can be solved inO(n2) time (McDonald et al, 2005b).
Additionalparameterizations are possible that take more thanone arc into account, but have varying effects oncomplexity (McDonald and Satta, 2007).
An advan-tage of graph-based methods is that tractable infer-ence enables the use of standard structured learningtechniques that globally set parameters to maximizeparsing performance on the training set (McDonaldet al, 2005a).
The primary disadvantage of thesemodels is that scores ?
and as a result any featurerepresentations ?
are restricted to a single arc or asmall number of arcs in the graph.The specific graph-based model studied in thiswork is that presented by McDonald et al (2006),which factors scores over pairs of arcs (instead ofjust single arcs) and uses near exhaustive search forunlabeled parsing coupled with a separate classifierto label each arc.
We call this system MSTParser, orsimply MST for short, which is also the name of thefreely available implementation.22.3 Transition-Based ModelsTransition-based dependency parsing systems use amodel parameterized over transitions of an abstractmachine for deriving dependency graphs, such thatevery transition sequence from the designated initialconfiguration to some terminal configuration derivesa valid dependency graph.
Given a real-valued scorefunction s(c, t) (for transition t out of configurationc), parsing can be performed by starting from the ini-tial configuration and taking the optimal transitiont?
= argmaxt?T s(c, t) out of every configurationc until a terminal configuration is reached.
This canbe seen as a greedy search for the optimal depen-dency graph, based on a sequence of locally optimaldecisions in terms of the transition system.Many transition systems for data-driven depen-dency parsing are inspired by shift-reduce parsing,2http://mstparser.sourceforge.net951where each configuration c contains a stack ?c forstoring partially processed nodes and a buffer ?ccontaining the remaining input.
Transitions in such asystem add arcs to the dependency graph and mani-pulate the stack and buffer.
One example is the tran-sition system defined by Nivre (2003), which parsesa sentence x = w0, w1, .
.
.
, wn in O(n) time.To learn a scoring function on transitions, thesesystems rely on discriminative learning methods,such as memory-based learning or support vectormachines, using a strictly local learning procedurewhere only single transitions are scored (not com-plete transition sequences).
The main advantage ofthese models is that features are not restricted to alimited number of graph arcs but can take into ac-count the entire dependency graph built so far.
Themajor disadvantage is that the greedy parsing strat-egy may lead to error propagation.The specific transition-based model studied inthis work is that presented by Nivre et al (2006),which uses support vector machines to learn transi-tion scores.
We call this system MaltParser, or Maltfor short, which is also the name of the freely avail-able implementation.32.4 Comparison and AnalysisThese models differ primarily with respect to threeproperties: inference, learning, and feature repre-sentation.
MaltParser uses an inference algorithmthat greedily chooses the best parsing decision basedon the current parser history whereas MSTParseruses exhaustive search algorithms over the space ofall valid dependency graphs to find the graph thatmaximizes the score.
MaltParser trains a modelto make a single classification decision (choose thenext transition) whereas MSTParser trains a modelto maximize the global score of correct graphs.MaltParser can introduce a rich feature history basedon previous parser decisions, whereas MSTParser isforced to restrict features to a single decision or apair of nearby decisions in order to retain efficiency.These differences highlight an inherent trade-offbetween global inference/learning and expressive-ness of feature representations.
MSTParser favorsthe former at the expense of the latter andMaltParserthe opposite.
This difference was highlighted in the3http://w3.msi.vxu.se/?jha/maltparser/study of McDonald and Nivre (2007), which showedthat the difference is reflected directly in the errordistributions of the parsers.
Thus, MaltParser is lessaccurate than MSTParser for long dependencies andthose closer to the root of the graph, but more accu-rate for short dependencies and those farthest awayfrom the root.
Furthermore, MaltParser is more ac-curate for dependents that are nouns and pronouns,whereas MSTParser is more accurate for verbs, ad-jectives, adverbs, adpositions, and conjunctions.Given that there is a strong negative correlationbetween dependency length and tree depth, andgiven that nouns and pronouns tend to be moredeeply embedded than (at least) verbs and conjunc-tions, these patterns can all be explained by the sameunderlying factors.
Simply put, MaltParser has anadvantage in its richer feature representations, butthis advantage is gradually diminished by the nega-tive effect of error propagation due to the greedy in-ference strategy as sentences and dependencies getlonger.
MSTParser has a more even distribution oferrors, which is expected given that the inference al-gorithm and feature representation should not preferone type of arc over another.
This naturally leadsone to ask: Is it possible to integrate the two modelsin order to exploit their complementary strengths?This is the topic of the remainder of this paper.3 Integrated ModelsThere are many conceivable ways of combining thetwo parsers, including more or less complex en-semble systems and voting schemes, which onlyperform the integration at parsing time.
However,given that we are dealing with data-driven models,it should be possible to integrate at learning time, sothat the two complementary models can learn fromone another.
In this paper, we propose to do this byletting one model generate features for the other.3.1 Feature-Based IntegrationAs explained in section 2, both models essentiallylearn a scoring function s : X ?
R, where thedomain X is different for the two models.
For thegraph-based model, X is the set of possible depen-dency arcs (i, j, l); for the transition-based model,X is the set of possible configuration-transition pairs(c, t).
But in both cases, the input is represented952MSTMalt ?
defined over (i, j, l) (?
= any label/node)Is (i, j, ?)
in GMaltx ?Is (i, j, l) in GMaltx ?Is (i, j, ?)
not in GMaltx ?Is (i, j, l) not in GMaltx ?Identity of l?
such that (?, j, l?)
is in GMaltx ?Identity of l?
such that (i, j, l?)
is in GMaltx ?MaltMST ?
defined over (c, t) (?
= any label/node)Is (?0c , ?0c , ?)
in GMSTx ?Is (?0c , ?0c , ?)
in GMSTx ?Head direction for ?0c in GMSTx (left/right/ROOT)Head direction for ?0c in GMSTx (left/right/ROOT)Identity of l such that (?, ?0c , l) is in GMSTx ?Identity of l such that (?, ?0c , l) is in GMSTx ?Table 1: Guide features for MSTMalt and MaltMST.by a k-dimensional feature vector f : X ?
Rk.In the feature-based integration we simply extendthe feature vector for one model, called the basemodel, with a certain number of features generatedby the other model, which we call the guide modelin this context.
The additional features will be re-ferred to as guide features, and the version of thebase model trained with the extended feature vectorwill be called the guided model.
The idea is that theguided model should be able to learn in which situ-ations to trust the guide features, in order to exploitthe complementary strength of the guide model, sothat performance can be improved with respect tothe base parser.
This method of combining classi-fiers is sometimes referred to as classifier stacking.The exact form of the guide features depend onproperties of the base model and will be discussedin sections 3.2?3.3 below, but the overall scheme forthe feature-based integration can be described as fol-lows.
To train a guided version BC of base model Bwith guide model C and training set T , the guidedmodel is trained, not on the original training set T ,but on a version of T that has been parsed with theguide model C under a cross-validation scheme (toavoid overlap with training data for C).
This meansthat, for every sentence x ?
T , BC has access attraining time to both the gold standard dependencygraph Gx and the graph GCx predicted by C, and it isthe latter that forms the basis for the additional guidefeatures.
When parsing a new sentence x?
with BC ,x?
is first parsed with model C (this time trained onthe entire training set T ) to derive GCx?
, so that theguide features can be extracted also at parsing time.3.2 The Guided Graph-Based ModelThe graph-based model, MSTParser, learns a scor-ing function s(i, j, l) ?
R over labeled dependen-cies.
More precisely, dependency arcs (or pairs ofarcs) are first represented by a high dimensional fea-ture vector f(i, j, l) ?
Rk, where f is typically a bi-nary feature vector over properties of the arc as wellas the surrounding input (McDonald et al, 2005a;McDonald et al, 2006).
The score of an arc is de-fined as a linear classifier s(i, j, l) = w ?
f(i, j, l),where w is a vector of feature weights to be learnedby the model.For the guided graph-based model, which we callMSTMalt, this feature representation is modified toinclude an additional argument GMaltx , which is thedependency graph predicted by MaltParser on theinput sentence x.
Thus, the new feature represen-tation will map an arc and the entire predicted Malt-Parser graph to a high dimensional feature repre-sentation, f(i, j, l, GMaltx ) ?
Rk+m.
These m ad-ditional features account for the guide features overthe MaltParser output.
The specific features used byMSTMalt are given in table 1.
All features are con-joined with the part-of-speech tags of the words in-volved in the dependency to allow the guided parserto learn weights relative to different surface syntac-tic environments.
Though MSTParser is capable ofdefining features over pairs of arcs, we restrict theguide features over single arcs as this resulted inhigher accuracies during preliminary experiments.3.3 The Guided Transition-Based ModelThe transition-based model, MaltParser, learns ascoring function s(c, t) ?
R over configurations andtransitions.
The set of training instances for thislearning problem is the set of pairs (c, t) such thatt is the correct transition out of c in the transitionsequence that derives the correct dependency graphGx for some sentence x in the training set T .
Eachtraining instance (c, t) is represented by a featurevector f(c, t) ?
Rk, where features are defined interms of arbitrary properties of the configuration c,including the state of the stack ?c, the input buffer?c, and the partially built dependency graph Gc.
Inparticular, many features involve properties of thetwo target tokens, the token on top of the stack ?c(?0c ) and the first token in the input buffer ?c (?0c ),953which are the two tokens that may become con-nected by a dependency arc through the transitionout of c. The full set of features used by the basemodel MaltParser is described in Nivre et al (2006).For the guided transition-based model, which wecall MaltMST, training instances are extended totriples (c, t, GMSTx ), where GMSTx is the dependencygraph predicted by the graph-based MSTParser forthe sentence x to which the configuration c belongs.We define m additional guide features, based onproperties of GMSTx , and extend the feature vectoraccordingly to f(c, t, GMSTx ) ?
Rk+m.
The specificfeatures used by MaltMST are given in table 1.
Un-like MSTParser, features are not explicitly definedto conjoin guide features with part-of-speech fea-tures.
These features are implicitly added throughthe polynomial kernel used to train the SVM.4 ExperimentsIn this section, we present an experimental evalua-tion of the two guided models based on data fromthe CoNLL-X shared task, followed by a compar-ative error analysis including both the base modelsand the guided models.
The data for the experimentsare training and test sets for all thirteen languagesfrom the CoNLL-X shared task on multilingual de-pendency parsing with training sets ranging in sizefrom from 29,000 tokens (Slovene) to 1,249,000 to-kens (Czech).
The test sets are all standardized toabout 5,000 tokens each.
For more information onthe data sets, see Buchholz and Marsi (2006).The guided models were trained according to thescheme explained in section 3, with two-fold cross-validation when parsing the training data with theguide parsers.
Preliminary experiments suggestedthat cross-validation with more folds had a negli-gible impact on the results.
Models are evaluatedby their labeled attachment score (LAS) on the testset, i.e., the percentage of tokens that are assignedboth the correct head and the correct label, usingthe evaluation software from the CoNLL-X sharedtask with default settings.4 Statistical significancewas assessed using Dan Bikel?s randomized pars-ing evaluation comparator with the default setting of10,000 iterations.54http://nextens.uvt.nl/?conll/software.html5http://www.cis.upenn.edu/?dbikel/software.htmlLanguage MST MSTMalt Malt MaltMSTArabic 66.91 68.64 (+1.73) 66.71 67.80 (+1.09)Bulgarian 87.57 89.05 (+1.48) 87.41 88.59 (+1.18)Chinese 85.90 88.43 (+2.53) 86.92 87.44 (+0.52)Czech 80.18 82.26 (+2.08) 78.42 81.18 (+2.76)Danish 84.79 86.67 (+1.88) 84.77 85.43 (+0.66)Dutch 79.19 81.63 (+2.44) 78.59 79.91 (+1.32)German 87.34 88.46 (+1.12) 85.82 87.66 (+1.84)Japanese 90.71 91.43 (+0.72) 91.65 92.20 (+0.55)Portuguese 86.82 87.50 (+0.68) 87.60 88.64 (+1.04)Slovene 73.44 75.94 (+2.50) 70.30 74.24 (+3.94)Spanish 82.25 83.99 (+1.74) 81.29 82.41 (+1.12)Swedish 82.55 84.66 (+2.11) 84.58 84.31 (?0.27)Turkish 63.19 64.29 (+1.10) 65.58 66.28 (+0.70)Average 80.83 82.53 (+1.70) 80.74 82.01 (+1.27)Table 2: Labeled attachment scores for base parsers andguided parsers (improvement in percentage points).10 20 30 40 50 60Sentence Length0.70.750.80.850.9AccuracyMaltMSTMalt+MSTMST+MaltFigure 2: Accuracy relative to sentence length.4.1 ResultsTable 2 shows the results, for each language and onaverage, for the two base models (MST, Malt) andfor the two guided models (MSTMalt, MaltMST).First of all, we see that both guided models showa very consistent increase in accuracy compared totheir base model, even though the extent of the im-provement varies across languages from about halfa percentage point (MaltMST on Chinese) up to al-most four percentage points (MaltMST on Slovene).6It is thus quite clear that both models have the capa-city to learn from features generated by the othermodel.
However, it is also clear that the graph-basedMST model shows a somewhat larger improvement,both on average and for all languages except Czech,6The only exception to this pattern is the result for MaltMSTon Swedish, where we see an unexpected drop in accuracy com-pared to the base model.9542 4 6 8 10 12             14      15+Dependency Length0.60.650.70.750.80.850.90.95RecallMaltMSTMalt+MSTMST+Malt2 4 6 8 10 12             14      15+Dependency Length0.550.60.650.70.750.80.85PrecisionMaltMSTMalt+MSTMST+Malt1 2 3 4 5 6 7+Distance to Root0.80.820.840.860.880.9RecallMaltMSTMalt+MSTMST+Malt1 2 3 4 5 6 7+Distance to Root0.780.80.820.840.860.880.90.92PrecisionMaltMSTMalt+MSTMST+Malt(a) (b)Figure 3: Dependency arc precision/recall relative to predicted/gold for (a) dependency length and (b) distance to root.German, Portuguese and Slovene.
Finally, giventhat the two base models had the previously bestperformance for these data sets, the guided modelsachieve a substantial improvement of the state of theart.
While there is no statistically significant differ-ence between the two base models, they are bothoutperformed by MaltMST (p < 0.0001), which inturn has significantly lower accuracy than MSTMalt(p < 0.0005).An extension to the models described so far wouldbe to iteratively integrate the two parsers in thespirit of pipeline iteration (Hollingshead and Roark,2007).
For example, one could start with a Maltmodel, use it to train a guided MSTMalt model, thenuse that as the guide to train a MaltMSTMalt model,etc.
We ran such experiments, but found that accu-racy did not increase significantly and in some casesdecreased slightly.
This was true regardless of whichparser began the iterative process.
In retrospect, thisresult is not surprising.
Since the initial integrationeffectively incorporates knowledge from both pars-ing systems, there is little to be gained by addingadditional parsers in the chain.4.2 AnalysisThe experimental results presented so far show thatfeature-based integration is a viable approach forimproving the accuracy of both graph-based andtransition-based models for dependency parsing, butthey say very little about how the integration benefitsthe two models and what aspects of the parsing pro-cess are improved as a result.
In order to get a betterunderstanding of these matters, we replicate parts ofthe error analysis presented by McDonald and Nivre(2007), where parsing errors are related to differentstructural properties of sentences and their depen-dency graphs.
For each of the four models evalu-ated, we compute error statistics for labeled attach-ment over all twelve languages together.Figure 2 shows accuracy in relation to sentencelength, binned into ten-word intervals (1?10, 11-20,etc.).
As expected, Malt and MST have very simi-lar accuracy for short sentences but Malt degradesmore rapidly with increasing sentence length be-cause of error propagation (McDonald and Nivre,2007).
The guided models, MaltMST and MSTMalt,behave in a very similar fashion with respect to eachother but both outperform their base parser over theentire range of sentence lengths.
However, exceptfor the two extreme data points (0?10 and 51?60)there is also a slight tendency for MaltMST to im-prove more for longer sentences and for MSTMalt toimprove more for short sentences, which indicatesthat the feature-based integration allows one parserto exploit the strength of the other.Figure 3(a) plots precision (top) and recall (bot-tom) for dependency arcs of different lengths (pre-dicted arcs for precision, gold standard arcs for re-call).
With respect to recall, the guided models ap-pear to have a slight advantage over the base mod-955Part of Speech MST MSTMalt Malt MaltMSTVerb 82.6 85.1 (2.5) 81.9 84.3 (2.4)Noun 80.0 81.7 (1.7) 80.7 81.9 (1.2)Pronoun 88.4 89.4 (1.0) 89.2 89.3 (0.1)Adjective 89.1 89.6 (0.5) 87.9 89.0 (1.1)Adverb 78.3 79.6 (1.3) 77.4 78.1 (0.7)Adposition 69.9 71.5 (1.6) 68.8 70.7 (1.9)Conjunction 73.1 74.9 (1.8) 69.8 72.5 (2.7)Table 3: Accuracy relative to dependent part of speech(improvement in percentage points).els for short and medium distance arcs.
With re-spect to precision, however, there are two clear pat-terns.
First, the graph-based models have better pre-cision than the transition-based models when pre-dicting long arcs, which is compatible with the re-sults of McDonald and Nivre (2007).
Secondly, boththe guided models have better precision than theirbase model and, for the most part, also their guidemodel.
In particular MSTMalt outperformsMST andis comparable to Malt for short arcs.
More inter-estingly, MaltMST outperforms both Malt and MSTfor arcs up to length 9, which provides evidence thatMaltMST has learned specifically to trust the guidefeatures from MST for longer dependencies.
Thereason that accuracy does not improve for dependen-cies of length greater than 9 is probably that thesedependencies are too rare for MaltMST to learn fromthe guide parser in these situations.Figure 3(b) shows precision (top) and recall (bot-tom) for dependency arcs at different distances fromthe root (predicted arcs for precision, gold standardarcs for recall).
Again, we find the clearest pat-terns in the graphs for precision, where Malt hasvery low precision near the root but improves withincreasing depth, while MST shows the oppositetrend (McDonald and Nivre, 2007).
Consideringthe guided models, it is clear that MaltMST im-proves in the direction of its guide model, with a5-point increase in precision for dependents of theroot and smaller improvements for longer distances.Similarly, MSTMalt improves precision in the rangewhere its base parser is inferior to Malt and for dis-tances up to 4 has an accuracy comparable to orhigher than its guide parser Malt.
This again pro-vides evidence that the guided parsers are learningfrom their guide models.Table 3 gives the accuracy for arcs relative to de-pendent part-of-speech.
As expected, we see thatMST does better than Malt for all categories exceptnouns and pronouns (McDonald and Nivre, 2007).But we also see that the guided models in all casesimprove over their base parser and, in most cases,also over their guide parser.
The general trend is thatMST improves more thanMalt, except for adjectivesand conjunctions, where Malt has a greater disad-vantage from the start and therefore benefits morefrom the guide features.Considering the results for parts of speech, as wellas those for dependency length and root distance, itis interesting to note that the guided models oftenimprove even in situations where their base parsersare more accurate than their guide models.
This sug-gests that the improvement is not a simple functionof the raw accuracy of the guide model but dependson the fact that labeled dependency decisions inter-act in inference algorithms for both graph-based andtransition-based parsing systems.
Thus, if a parsercan improve its accuracy on one class of dependen-cies, e.g., longer ones, then we can expect to see im-provements on all types of dependencies ?
as we do.The interaction between different decisions mayalso be part of the explanation why MST benefitsmore from the feature-based integration than Malt,with significantly higher accuracy for MSTMalt thanfor MaltMST as a result.
Since inference is global(or practically global) in the graph-based model,an improvement in one type of dependency has agood chance of influencing the accuracy of other de-pendencies, whereas in the transition-based model,where inference is greedy, some of these additionalbenefits will be lost because of error propagation.This is reflected in the error analysis in the followingrecurrent pattern: Where Malt does well, MaltMSTdoes only slightly better.
But where MST is good,MSTMalt is often significantly better.Another part of the explanation may have to dowith the learning algorithms used by the systems.Although both Malt and MST use discriminativealgorithms, Malt uses a batch learning algorithm(SVM) and MST uses an online learning algorithm(MIRA).
If the original rich feature representationof Malt is sufficient to separate the training data,regularization may force the weights of the guidedfeatures to be small (since they are not needed attraining time).
On the other hand, an online learn-956ing algorithm will recognize the guided features asstrong indicators early in training and give them ahigh weight as a result.
Features with high weightearly in training tend to have the most impact on thefinal classifier due to both weight regularization andaveraging.
This is in fact observed when inspectingthe weights of MSTMalt.5 Related WorkCombinations of graph-based and transition-basedmodels for data-driven dependency parsing havepreviously been explored by Sagae and Lavie(2006), who report improvements of up to 1.7 per-centage points over the best single parser whencombining three transition-based models and onegraph-based model for unlabeled dependency pars-ing, evaluated on data from the Penn Treebank.
Thecombined parsing model is essentially an instance ofthe graph-based model, where arc scores are derivedfrom the output of the different component parsers.Unlike the models presented here, integration takesplace only at parsing time, not at learning time, andrequires at least three different base parsers.
Thesame technique was used by Hall et al (2007) tocombine six transition-based parsers in the best per-forming system in the CoNLL 2007 shared task.Feature-based integration in the sense of letting asubset of the features for one model be derived fromthe output of a different model has been exploitedfor dependency parsing by McDonald (2006), whotrained an instance of MSTParser using featuresgenerated by the parsers of Collins (1999) and Char-niak (2000), which improved unlabeled accuracy by1.7 percentage points, again on data from the PennTreebank.
In addition, feature-based integration hasbeen used by Taskar et al (2005), who trained adiscriminative word alignment model using featuresderived from the IBM models, and by Florian et al(2004), who trained classifiers on auxiliary data toguide named entity classifiers.Feature-based integration also has points in com-mon with co-training, which have been applied tosyntactic parsing by Sarkar (2001) and Steedman etal.
(2003), among others.
The difference, of course,is that standard co-training is a weakly supervisedmethod, where guide features replace, rather thancomplement, the gold standard annotation duringtraining.
Feature-based integration is also similar toparse re-ranking (Collins, 2000), where one parserproduces a set of candidate parses and a second-stage classifier chooses the most likely one.
How-ever, feature-based integration is not explicitly con-strained to any parse decisions that the guide modelmight make and only the single most likely parse isused from the guide model, making it significantlymore efficient than re-ranking.Finally, there are several recent developments indata-driven dependency parsing, which can be seenas targeting the specific weaknesses of graph-basedand transition-based models, respectively, thoughwithout integrating the two models.
Thus, Naka-gawa (2007) and Hall (2007) both try to overcomethe limited feature scope of graph-based models byadding global features, in the former case usingGibbs sampling to deal with the intractable infer-ence problem, in the latter case using a re-rankingscheme.
For transition-based models, the trend isto alleviate error propagation by abandoning greedy,deterministic inference in favor of beam search withglobally normalized models for scoring transitionsequences, either generative (Titov and Henderson,2007a; Titov and Henderson, 2007b) or conditional(Duan et al, 2007; Johansson and Nugues, 2007).6 ConclusionIn this paper, we have demonstrated how the twodominant approaches to data-driven dependencyparsing, graph-based models and transition-basedmodels, can be integrated by letting one model learnfrom features generated by the other.
Our experi-mental results show that both models consistentlyimprove their accuracy when given access to fea-tures generated by the other model, which leads toa significant advancement of the state of the art indata-driven dependency parsing.
Moreover, a com-parative error analysis reveals that the improvementsare largely predictable from theoretical properties ofthe two models, in particular the tradeoff betweenglobal learning and inference, on the one hand, andrich feature representations, on the other.
Directionsfor future research include a more detailed analysisof the effect of feature-based integration, as well asthe exploration of other strategies for integrating dif-ferent parsing models.957ReferencesGiuseppe Attardi.
2006.
Experiments with a multilan-guage non-projective dependency parser.
In Proceed-ings of CoNLL, pages 166?170.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of CoNLL, pages 149?164.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of NAACL, pages 132?139.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In Proceedings of ICML, pages175?182.Yuan Ding and Martha Palmer.
2004.
Synchronous de-pendency insertion grammars: A grammar formalismfor syntax based statistical MT.
In Proceedings of theWorkshop on Recent Advances in Dependency Gram-mar, pages 90?97.Xiangyu Duan, Jun Zhao, and Bo Xu.
2007.
Probabilis-tic parsing action models for multi-lingual dependencyparsing.
In Proceedings of EMNLP-CoNLL, pages940?946.Jason M. Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Proceed-ings of COLING, pages 340?345.Radu Florian, Hany Hassan, Abraham Ittycheriah,Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,Nicolas Nicolov, and Salim Roukos.
2004.
A statisti-cal model for multilingual entity detection and track-ing.
In Proceedings of NAACL/HLT.Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryig?it,Bea?ta Megyesi, Mattias Nilsson, and Markus Saers.2007.
Single malt or blended?
A study in multilin-gual parser optimization.
In Proceedings of EMNLP-CoNLL.Keith Hall.
2007.
K-best spanning tree parsing.
In Pro-ceedings of ACL, pages 392?399.Kristy Hollingshead and Brian Roark.
2007.
Pipelineiteration.
In Proceedings of ACL, pages 952?959.Richard Johansson and Pierre Nugues.
2007.
Incremen-tal dependency parsing using online learning.
In Pro-ceedings of EMNLP-CoNLL, pages 1134?1138.Ryan McDonald and Joakim Nivre.
2007.
Characteriz-ing the errors of data-driven dependency parsing mod-els.
In Proceedings of EMNLP-CoNLL, pages 122?131.Ryan McDonald and Giorgio Satta.
2007.
On the com-plexity of non-projective data-driven dependency pars-ing.
In Proceedings of IWPT, pages 122?131.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005a.
Online large-margin training of dependencyparsers.
In Proceedings of ACL, pages 91?98.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005b.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedings ofHLT/EMNLP, pages 523?530.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with a two-stage discriminative parser.
In Proceedings of CoNLL,pages 216?220.Ryan McDonald.
2006.
Discriminative Learning andSpanning Tree Algorithms for Dependency Parsing.Ph.D.
thesis, University of Pennsylvania.Tetsuji Nakagawa.
2007.
Multilingual dependency pars-ing using global features.
In Proceedings of EMNLP-CoNLL, pages 952?956.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.Memory-based dependency parsing.
In Proceedingsof CoNLL, pages 49?56.Joakim Nivre, Johan Hall, Jens Nilsson, Gu?lsen Eryig?it,and Svetoslav Marinov.
2006.
Labeled pseudo-projective dependency parsing with support vector ma-chines.
In Proceedings of CoNLL, pages 221?225.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007.
The CoNLL 2007 shared task on dependencyparsing.
In Proceedings of EMNLP-CoNLL, pages915?932.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of IWPT,pages 149?160.Kenji Sagae and Alon Lavie.
2006.
Parser combinationby reparsing.
In Proceedings of NAACL: Short Papers,pages 129?132.Anoop Sarkar.
2001.
Applying co-training methods tostatistical parsing.
In Proceedings of NAACL, pages175?182.Rion Snow, Dan Jurafsky, and Andrew Y. Ng.
2005.Learning syntactic patterns for automatic hypernymdiscovery.
In Proceedings of NIPS.Mark Steedman, Rebecca Hwa, Miles Osborne, andAnoop Sarkar.
2003.
Corrected co-training for statis-tical parsers.
In Proceedings of ICML, pages 95?102.Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005.A discriminative matching approach to word align-ment.
In Proceedings of HLT/EMNLP, pages 73?80.Ivan Titov and James Henderson.
2007a.
Fast and ro-bust multilingual dependency parsing with a genera-tive latent variable model.
In Proceedings of EMNLP-CoNLL, pages 947?951.Ivan Titov and James Henderson.
2007b.
A latent vari-able model for generative dependency parsing.
In Pro-ceedings of IWPT, pages 144?155.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisticaldependency analysis with support vector machines.
InProceedings of IWPT, pages 195?206.958
