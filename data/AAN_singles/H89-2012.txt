Parsing,Word AssociationsandTypical Predicate-Argument Relations 1Kenneth ChurchWilliam GalePatrick HanksDonald HindleBell Laboratories and CollinsAbstractThere are a number of coUocational constraints in natural anguages that ought to play a more importantrole in natural anguage parsers.
Thus, for example, it is hard for most parsers to take advantage of thefact that wine is typically drunk, produced, and sold, but (probably) not pruned.
So too, it is hard for aparser to know which verbs go with which prepositions (e.g., set up) and which nouns fit together toform compound noun phrases (e.g., computer programmer).
This paper will attempt to show that manyof these types of concerns can be addressed with syntactic methods (symbol pushing), and need notrequire explicit semantic interpretation.
We have found that it is possible to identify many of theseinteresting co-occurrence r lations by computing simple summary statistics over millions of words oftext.
This paper will summarize a number of experiments carried out by various subsets of the authorsover the last few years.
The term collocation will be used quite broadly to include constraints on SVO(subject verb object) triples, phrasal verbs, compound noun phrases, and psycholinguistic notions ofword association (e.g., doctor~nurse).1.
Mutual InformationChurch and Hanks (1989) discussed the use of the mutual information statistic in order to identify avariety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type(content word/content word) to lexico-syntacfic co-occurrence onstraints between verbs and prepositions(content word/function word).
Mutual information, l(x;y), compares the probability of observing word xand word y together (the joint probability) with the probabilities of observing x and y independently(chance).l(x;y) -= log 2 P(x,y)e(x) e(y)If there is a genuine association between x and y, then the joint probability P(x,y) will he much largerthan chance P(x) P(y), and consequently l(x;y) >> 0, as illustrated in the table below.
If there is nointeresting relationship between x and y, then P(x,y) = P(x) P(y), and thus, I(x;y) = 0.
If x and yare in complementary distribution, then P(x,y) will be much less than P(x) P(y), forcing l(x;y) << 0.Word probabilities, P(x) and P(y), are estimated by counting the number of observations of x and y in acorpus, f(x) and f(y), and normalizing by N, the size of the corpus.
Joint probabilities, P(x,y), areestimated by counting the number of times that x is followed by y in a window of w words, fw(x,y),and normalizing by N (w - 1).
21.
The paper was previously presented at International Workshop on Parsing Technologies, CMU, 1989.2.
The window size parameter allows us to look at different scales.
Smaller window sizes will identify fixed expressions (idioms),noun phrases, and other relations that hold over short ranges; larger window sizes will highlight semantic oncepts and otherrelationships that hold over larger scales.75Some Interesting Associations with "Doctor"in the 1987 AP Corpus (N = 15 million; w = 6)I(x; y) f(x~ y) f(x) x fly) Y2.
Phrasal Verbs8.0 2.4 111 honorary 621 doctor8.0 1.6 1105 doctors 44 dentists8.4 6.0 1105 doctors 241 nurses7.1 1.6 1105 doctors 154 treating6.7 1.2 275 examined 621 doctor6.6 1.2 1105 doctors 317 treat6.4 5.0 621 doctor 1407 bills6.4 1.2 621 doctor 350 visits6.3 3.8 1105 doctors 676 hospitals6.1 1.2 241 nurses 1105 doctorsAssociations with "Doctor" Some Less Interesting-1.3 1.2 621 doctor 73785 with-1.4 8.2 284690 a 1105 doctors- 1.4 2.4 84716 is 1105 doctorsChurch and Hanks (1989) also used the mutual information statistic in order to identify phrasal verbs,following up a remark by Sinclair:"How common are the phrasal verbs with set?
Set is particularly rich in making combinationswith words like about, in, up, out, on, off, and these words are themselves very common.
Howlikely is set off to occur?
Both are frequent words; \[set occurs approximately 250 times in amillion words and\] off occurs approximately 556 dmes in a million words... \[T\]he question weare asking can be roughly rephrased as follows: how likely is off to occur immediately after set?...
This is 0.00025x0.00055 \[P(x) P(y)\], which gives us the tiny figure of 0.0000001375 ...The assumption behind this calculation is that the words are distributed at random in a text \[atchance, in our terminology\].
It is obvious to a linguist that this is not so, and a rough measureof how much set and off attract each other is to compare the probability with what actuallyhappens... Set off occurs nearly 70 times in the 7.3 million word corpus\[P(x,y)=70/(7.3 106) >> P(X) P(y)\].
That is enough to show its main patterning and itsuggests that in currently-held corpora there will be found sufficient evidence for the descriptionof a substantial collection of phrases... (Sinclair 1987b, pp.
151-152)It happens that set ... off was found 177 times in the 1987 AP Corpus of approximately 15 millionwords, about the same number of occurrences per million as Sinclair found in his (mainly British)corpus.
Quantitatively, I (set ;of f )  = 3.7, indicating that the probability of set ... off is 23.7 = 13 timesgreater than chance.
This association is relatively strong; the other particles that Sinclair mentions havescores of: about (-0.9), in (0.6), up (4.6), out (2.2), on (1.0) in the 1987 AP Corpus of 15 million words.3.
Preprocessing the Corpus with a Part of Speech TaggerPhrasal verbs involving the preposition to raise an interesting problem because of the possible confusionwith the infinitive marker to.
We have found that if we first tag every word in the corpus with a part ofspeech using a method such as Church (1988) or DeRose (1988), and then measure associations betweentagged words, we can identify interesting contrasts between verbs associated with a followingpreposition to~in and verbs associated with a following infinitive marker to~to.
(Part of speech notationis borrowed from Francis and Kucera (1982); m = preposition; to = infinitive marker; vb = bare verb;vbg = verb + ing; vbd = verb + ed; vbz = verb + s; vbn = verb + en.)
The score identifies quite anumber of verbs associated in an interesting way with to; restricting our attention to pairs with a scoreof 3.0 or more, there are 768 verbs associated with the preposition to~in and 551 verbs with the infinitivemarker to~to.
The ten verbs found to be most associated before to~in are:76?
to~in: alluding/vbg, adhere/vb, amounted/vbn, relating/vbg, amounting/vbg, revert/vb, re-verted/vbn, resorting/vbg, relegated/vbn?
to/to: obligated/vbn, trying/vbg, compelled/vbn, enables/vbz, supposed/vbn, intends/vbz, vow-ing/vbg, tried\]vbd, enabling/vbg, tends/vbz, tend\]vb, intend\]vb, tries/vbzThus, we see there is considerable verage to be gained by preprocessing the corpus and manipulatingthe inventory of tokens.4.
Preprocessing with a Syntactic ParserHindle has found it useful to preprocess the input with the Fidditch parser (Hindle 1983) in order to askabout the typical arguments of verbs.
Thus, for any of verb in the sample, we can ask what nouns ittakes as subjects and objects.
The following table shows the objects of the verb drink that appeared atleast two times in a sample of six million words of AP text, in effect giving the answer to the question"what can you drink?"
Calculating the co-occurrence weight for drink, shown in the third column,gives us a reasonable ranking of terms, with it near the bottom.
This list of drinkable things isintuitively quite good.Object Frequency Mutual Information<quantity> beer 2 12.34tea 4 11.75Pepsi 2 11.75champagne 4 11.75liquid 2 10.53beer 5 10.20wine 2 9.34water 7 7.65anything 3 5.15much 3 2.54it 3 1.25<quantity> 2 1.22A standard alternative approach to the classification of entities is in terms of a hierarchy of types.
Thebiological taxonomy is the canonical example: a penguin is a bird is a vertebrate and so on.
Such "is-a" hierarchies have found a prominent place in natural language processing and knowledgerepresentation because they allow generalized representation f semantic features and of rules.
There isa wide range of problems and issues in using "is-a" hierarchies in natural anguage processing, but twoespecially recommend that we investigate alternative classification schemes like the one reported here.First, "is-a" hierarchies are large and complicated and expensive to acquire by hand.
Attempts toautomatically derive these hierarchies for words from existing dictionaries have been only partiallysuccessful (Chodorow, Byrd, and Heidom 1985).
Yet without a comprehensive hierarchy, it is difficultto use such classifications in the processing of unrestricted text.
Secondly, for many purposes, evenknowing the subclass-superclass relations is insufficient; it is difficult to predict which properties areinherited from a superclass and which aren't, and what properties are relevant in a particular linguisticusage.
So for example, as noted above, despite the fact that both potatoes and peanuts are edible foodsthat grow underground, we typically bake potatoes, but roast peanuts.
A distribution-basedclassification, if successful, promises to do better at least on these two problems.5.
Significance LevelsIf the frequency counts are very small, the mutual information statistic becomes unstable.
This is thereason for not reporting objects that appeared only once with the verb drink.
Although these objectshave very large mutual information scores, there is also a very large chance that they resulted from somequirk in the corpus, or a bug in the parser.
For some purposes, it is desirable to measure confidencerather than likelihood.
Gale and Church have investigated the use of a t-score instead of the mutualintbrmation score, as a way of identifying "significant" bigrams.77The following table shows a few significant bigrams ending with potatoes, computed from 44 millionwords of AP news wire from 2/12/88 until 12/31/88.
The numbers in the first column indicate theconfidence in standard deviations that the word sequence is interesting, and cannot be attributed tochance.t x y4.6 sweet potatoes4.3 mashed potatoes4.3 , potatoes4.0 and potatoes3.8 couch potatoes3.3 of potatoes3.3 frozen potatoes2.8 fresh potatoes2.8 small potatoes2.1 baked potatoesThese numbers were computed by the following formulaE(er(x y)) - E(Pr(x) er(y)) t=qeZ(er(x y)) + ~2(Pr(x) Pr(y))where E(Pr(x y)) and t~Z(Pr(x y)) are the mean and variance of the probability of seeing word xfollowed by word y.
The means and variances are computed by the Good-Turing method (Good 1953).Let r be the number of times that the bigram x y was found in a corpus of N words, and let N r be thefrequencies of frequences (the number of bigrams with count r).
Then r*, the estimated expected valueNr+lof r  in similar corpus of the same size, is r* = NxE(Pr(x y)) = ( r+ l )  ~ and the variance o f ri s~2(r )  = N2a2(Pr(x y)) = r* (1 + ( r+ l ) *  - r*)6.
Just a Powerfid ToolAlthough it is clear that the statistics discussed above can be extremely powerful aids to a lexicographer,they should not be overrated.
We do not aim to replace lexicographers with self-organizing statistics;we merely hope to provide a set of tools that could greatly improve their productivity.
Suppose, forexample, that a lexicographer wanted to find a set of words that take sentential complements.
Then itmight be helpful to start with a table of t-scores uch as:t x y74.0 said that50.9 noted that43.3 fact that41.9 believe that40.7 found that40.1 is that40.0 reported that39.5 adding that38.6 Tuesday that38.4 Wednesday thatIt might be much quicker for a lexicographer to edit down this list than to construct he list fromintuition alone.
It doesn't take very much time to decide that Tuesday and Wednesday are lessinteresting than the others.
Of course, it might be possible to automate some of these decisions byappropriately preprocessing the corpus with a part of speech tagger or a parser, but it will probablyalways be necessary to exercise some editorial judgment.787.
Practical ApplicationsThe proposed statistical description has a large number of potentially important appficadons, including:?
enhancing the productivity of lexicographers in identifying normal and conventional usage,?
enhancing the productivity of computational linguists in compifing lexicons of lexico-syntacticfacts,?
providing disambiguation cues for parsing highly ambiguous yntactic structures uch as nouncompounds, conjunctions, and prepositional phrases,?
retrieving texts from large databases (e.g., newspapers, patents), and?
constraining the language model both for speech recognition and optical character recognition(OCR).Consider the optical character recognizer (OCR) application.
Suppose that we have an OCR device suchas (Kahan, Pavfidis, Baird 1987), and it has assigned about equal probability to having recognized"farm" and "form," where the context is either: (1) "federal credit" or (2) "some of."
Wedoubt that the reader has any trouble specifying which alternative is more likely.
By using the followingprobabilities for the eight bigrams in this sequence, a computer program can rely on an estimatedlikelihood to make the same distinction.x y Observations per million wordsfederal farm 0.50federal form 0.039farm credit 0.13form credit 0.026some form 4.1some farm 0.63form of 34.0farm of 0.81The probability of the tri-grams can be approximated by multiplying the probabilities of the the twoconstituent bigrams.
Thus, the probability of federal farm credit can be approximated as(0.5x10-6)x(0.13x 10 -6) = 0.065?
10 -12.
Similarly, the probability for federal form credit can beapproximiated as (0.039x10-6)x(0.026x10 -6) = 0.0010?10 -12.
The ratio of these likelihoodsshows that "farm" is (0.065x10-12)/(0.0010x10-12) = 65 times more likely than "form" in thiscontext.
In the other context, "some of," it turns out that " form" is 273 times more likely than"farm."
This example shows how likelihood ratios can be used in an optical character recognitionsystem to disambiguate among optically confusable words.
Note that alternative disambiguationmethods based on syntactic onstraints such as part of speech are unlikely to help in this case since both" form" and "farm" are commonly used as nouns.8.
Alternatives to Collocation for Recognition ApplicationsThere have been quite a number of attempts to use syntactic methods in speech recognition, beginningwith the A.RPA speech project and continuing on to the present.
It might be noted, however, that therehas not been very much success, perhaps because syntax alone is not a strong enough constraint onlanguage use (performance).
We believe that collocational constraints should play an important role inrecognition applications, and attempts to ignore coUocafional constraints and use purely syntacticmethods will probably run into difficulties.Syntactic onstraints, by themselves, though are probably not very important.
Any psycholinguist knowsthat the influence of syntax on lexical retrieval is so subtle that you have to control very carefully for allthe factors that really matter (e.g., word frequency, word association orms, etc.).
On the other hand,collocational factors (word associations) dominate syntactic ones so much that you can easily measure79the influence of word frequency and word association orms on lexical retrieval without careful controlsfor syntax.There are many ways to demonstrate he relative lack of constraint imposed by syntax.
Recall the oldtelevision game show, "The Match Game," where a team of players was given a sentence with amissing word, e.g, "Byzantine icons couM murder the divine BLANK," and asked to fill in the blankthe same way that the studio audience did.
The game was 'interesting' because there are enoughconstraints in natural language so that there is a reasonably large probability of a match.
Suppose,however, that we make our speech recognition device play the match game with a handicap; instead ofgiving the speech recognition device the word string, "Byzantine icons couM murder the divineBLANK," we give the speech recognition device just the syntactic parse tree, \[S \[NP nn nns\] \[VP\[AUX md\ ]  v \[NP at jj BLANK \]\]\], and ask it to guess the missing word.
This is effectively what weare doing by limiting the language model to syntactic considerations alone.
Of course, with this thehandicap, the match game isn't much of a game; the recognition device doesn't have a fair chance toguess the missing word.We believe that syntax will ultimately be a very important source of constraint, but in a more indirectway.
As we have been suggesting, the real constraints will come from word frequencies andcollocational constraints, but these questions will probably need to be broken out by syntactic ontext.How likely is it for this noun to conjoin with that noun?
Is this noun a typical subject of that verb?And so on.
In this way, syntax plays a crucial role in providing the relevant representation forexpressing these very important constraints, but crucially, it does not provide very much usefulconstraint (in the information theoretic sense) all by itself.
39.
ConclusionIn any natural language there are restrictions on what words can appear together in the sameconstruction, and in particular, on what can be arguments of what predicates.
It is common practice inlinguistics to classify words not only on the basis of their meanings but also on the basis of their co-occurrence with other words.
Running through the whole Fiahian tradition, for example, is the themethat "You shall know a word by the company it keeps" (Firth, 1957).
"On the one hand, bank co-occurs with words and expressions uch as money, notes, loan,account, investment, clerk, official, manager, robbery, vaults, working in a, its actions, FirstNational, of England, and so forth.
On the other hand, we find bank co-occurring with river,swim, boat, east (and of course West and South, which have acquired special meanings of theirown), on top of the, and of the Rhine."
(Hanks 1987, p. 127)Harris (1968) makes this "distributional hypothesis" central to his linguistic theory.
His claim is that:"the meaning of entities, and the meaning of grammatical relations among them, is related to therestriction of combinations of these entities relative to other entities," (Harris 1968:12).
Granting thatthere must be some relationship between distribution and meaning, the exact nature of such arelationship to our received notions of meaning is nevertheless not without its complications.
Forexample, there are some purely collocational restrictions in English that seem to enforce no semanticdistinction.
Thus, one can roast chicken and peanuts in an oven, but typically fish and beans are bakedrather than roasted: this fact seems to be a quirk of the history of English.
Polysemy provides a secondkind of complication.
A sentence can be parsed and a sentence can be commuted, but these are twodistinct senses of the word sentence; we should not be misled into positing a class of things that can beboth parsed and commuted.3.
Much of the work on language modeling for speech recognition has tended to concentrate on search questions.
Should we stillbe using Bates' island riving approach (Bates 1975), or should we try something ewer such as Tomita's o-called generalizedLR(k) parser (Tomita 1986)7 We suggest that he discussion should concentrate more on describing the facts, and less on howthey are enforced.80Given these complicating factors, it is by no means obvious that the distribution of words will directlyprovide a useful semantic lassification, at least in the absence of considerable human intervention.
Thework that has been done based on Harris' distributional hypothesis (most notably, the work of theassociates of the Linguistic String Project (see for example, Hirschman, Grishman, and Sager 1975))unfortunately does not provide a direct answer, since the corpora used have been small (tens ofthousands of words rather than millions) and the analysis has typically involved considerableintervention by the researchers.
However, with much larger corpora (10-100 million words) and robustparsers and taggers, the early results reported here and elsewhere appear extremely promising.ReferencesBates, M., "Syntactic Analysis in a Speech Understanding System," BBN Report No.
3116, 1975.Chodorow, M, Byrd, R., and Heidorn, G., (1985) "Extracting semantic hierarchies from a large on-linedictionary," ACL Proceedings.Church, K., (1988), "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text,"Second Conference on Applied Natural Language Processing, Austin, Texas.Church, K., and Hanks, P., (1989), "Word Association Norms, Mutual Information, and Lexicography,"ACL Proceedings.DeRose, S., "Grammatical Category Disambiguation by Statistical Optimization," ComputationalLinguistics, Vol.
14, No.
1, 1988.Firth, J., (1957), "A Synopsis of Linguistic Theory 1930-1955" in Studies in Linguistic Analysis,Philological Society, Oxford; reprinted in Palmer, F., (ed.
1968), Selected Papers of J.R. Firth,Longman, Harlow.Francis, W., and Kucera, H., (1982), Frequency Analysis of English Usage, Houghton Mifflin Company,Boston.Good, I. J., (1953), The Population Frequencies of Species and the Estimation of PopulationParameters, Biometrika, Vol.
40, pp.
237-264.Hanks, P., (1987), "Definitions and Explanations," in Sinclair (1987a).Harris, Z., (1968), "Mathematical Structures of Language," New York: Wiley.Hirschman, L., Gnshman, R., and Sager, N., (1975) "Grammatically-based automatic word classformation," Information Processing and Management, 11, 39-57.Hindle, D., (1983), "User manual for Fidditch, a deterministic parser," Naval Research LaboratoryTechnical Memorandum #7590-142Kahan, S., Pavl.idis, T., and Baird, H., (1987) "On the Recognition of Printed Characters of any Font orSize," IEEE Transactions PAMI, pp.
274-287.Sinclair, J., Hanks, P., Fox, G., Moon, R., Stock, P. (eds), (1987a), Collins Cobuild English LanguageDictionary, Collins, London and Glasgow.Sinclair, J., (1987b), "The Nature of the Evidence," in Sinclair, J.
(ed.
), Looking Up: an account of theCOBUILD Project in lexical computing, Collins, London and Glasgow.Tomita, M., (1986), Efficient Parsing for Natural Language, Kluwer Academic Press.81
