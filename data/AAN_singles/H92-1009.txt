Human-Machine Problem Solving Using Spoken Language Systems (SLS): FactorsAffecting Performance and User SatisfactionElizabeth Shriberg 1,3, Elizabeth Wade 2'3, Patti Price 31University of California at Berkeley, Department of Psychology, Berkeley, CA 947202Stanford University, Department of Psychology, Stanford, CA 943053SRI International, 333 Ravenswood Avenue, Menlo Park, CA 94303ABSTRACTWe have analyzed three factors affecting user satisfactionand system performance using an SLS implemented in theATIS domain.
We have found that: (I) trade-offs betweenspeed and accuracy have different implications for user sat-isfaction; (2) recognition performance improves over time,at least in part because of a reduction in sentence perplex-ity; and (3) hyperarticulation increases recognition errors,and while instructions can reduce this behavior, they do notresult in improved recognition performance.
We concludethat while users may adapt o some aspects of an SLS, cer-tain types of user behavior may require technological solu-tions.1.
INTRODUCTIONData collection is a critical component of the DARPA Spo-ken Language Systems (SLS) program.
Data are crucial notonly for system training, development and evaluation, butalso for analyses that can provide insight o guide futureresearch and development.
By observing users interactingwith an SLS under different conditions, we can assesswhich issues may best be addressed by human factors andwhich will require technological solutions.
System devel-opers can benefit from considering not only initial use of anSLS, but also the experience of a user over time.Systems based on current echnology work best whenspeech and language closely resemble the training dataused to develop the system.
However, there is considerablevariability in the degree to which the speech and languageof new users match that of the training data.
The currentpaper examines the importance of this initial match.
It ispossible that users whose speech does not conform to thesystem may be able to adapt heir behavior over time (e.g.,Stem and Rudnicky \[11\]).
In order to evaluate technologyin terms of the demands of the application, we need tounderstand the extent and the nature of such adaptation andthe conditions that affect it.
Although system performancecan be measured in a number of ways, in this paper, wefocus on (1) self-reports of user satisfaction, and (2) recog-nition performance.
Further studies could include addi-tional measures.SRI has been collecting data in the air travel planningdomain using a number of different systems (see Bly et al\[1\]; Kowtko and Price \[5\]).
In moving from wizard-baseddata collection to the use of SRI's SLS, we observedchanges in user behavior that were associated with systemerrors.
Some of these behaviors were adaptive; for exam-ple, learning to avoid out-of-vocabulary words or unusualsyntax should facilitate successful interaction.
Otherbehaviors, however, were non-adaptive and could actuallyimpede the interaction.
For example, speaking more loudlyor in a hyperarticulate style may be detrimental to systemperformance insofar as these styles differ from thoseobserved in training material dominated by wizard-medi-ated data in which system errors are minimal.It is difficult o predict how well an SLS will need to per-form in order to be acceptable to users.
Both speed andaccuracy are crucial to system acceptability; we have there-fore collected ata using versions of the system that priori-tize one of these parameters atthe expense of the other.
Thepresent study first addresses the issue of user satisfactionwith different levels of system speed and accuracy and thenfocuses on an example of an adaptive behavior and anotherthat is maladaptive.
These behaviors represent a subset ofpotential factors influencing human-machine interaction.Because these issues are not restricted to any particular sys-tem, they should be of general interest o developers ofSLS technology.In the first study, we compared three points in the speed-accuracy space for this application: (1) an extremely slowbut very accurate wizard-mediated system (described inBly et al \[1\]) with a 2-3 minute response time and a mini-mal error rate; (2) a software version of the DECIPHERrecognizer with a response time of several times real timeand a fairly low word error rate; and (3) a version of theDECIPHER recognizer implemented in special-purposehardware using older word models, which has a very fastresponse time but currently has a higher word error rate.49We compared user satisfaction based on responses to apost-session questionnaire.The second study investigated the effect of user experienceon syntax and word choice.
We hypothesized that one wayusers might adapt would be to conform to the languagemode~s constraining recognition.
We therefore measuredrecognition performance in subjects' first and second sce-narios, and compared sentence perplexities in order todetermine whether any changes in recognition performancecould be attributed to a change in perplexity.The third study examined the effect of hyperarticulatespeech on recognition and tested whether instructions tousers could reduce this potentially maladaptive behavior.We coded each utterance for hyperarticulation a d com-pared recognizer performance for normal and hyperarticu-late utterances.
We also compared rates of hyperarticulationfor subjects who were either given or not given the instruc-tions.2.
DATA COLLECT ION METHODS2.1.
SubjectsData from a total of 145 subjects were included in the anal-yses.
Subsets of these data were chosen for inclusion ineach analysis in order to counterbalance forgender and sce-nario.
The majority of subjects were SRI employeesrecruited from an advertisement i  an intemal newsletter; asmall number were students from a nearby university,employees in a local research corporation, or members of avolunteer organization.
Subjects were native speakers ofEnglish, ranged in age from 22 to 71 and had varyingdegrees of experience with travel planning and computers.2.2.
MaterialsFour different ravel-planning scenarios were used.
Oneentailed arranging flights to two cities in three days; a sec-ond entailed finding two fares for the price of a first classfare; a third required coordinating the arrival times of threeflights from different cities; and a fourth involved weighingfactors such as fares and meals in order to choose betweentwo flight times.
Because the task demands of the scenarioswere different, we controlled for scenario in the analyses.2.3.
ApparatusThe data were collected using two versions of SRI's SLS(with no human in the loop); the first study also includeddata collected in a Wizard of Oz setting (Bly et a.l.
\[1\]).
Thebasic characteristics of the DECIPHER speech recognitioncomponent are described in Murveit et al\[7,9\], and thebasic characteristics of the natural anguage understandingcomponent are described in Jackson et al \[4\].
Some sub-jects used the real-time hardware version of the DECIPHERsystem (Murveit and Weintraub \[8\]; Weintraub et al \[12\]);others used the software version of the system, which was amodified version of SRI's benchmark system (as describedin the references above) tuned using the pnming thresholdto improve speed at the cost of introducing a small numberof recognition errors.SRI's SLS technology was implemented in the air travelplanning domain, a domain with which many people arefamiliar (see Price \[10\]).
The underlying database was arelational version of an 11-city subset of the Official AirlineGuide.
Two DARPA/NIST standard microphones wereused: the Sennheiser HMD-410 close-talking microphoneand the Crown PCC-160 table-top microphone.
Most datawere collected with two channels; some of the early datawere collected using only the Sennheiser microphone.When both microphones were used, recognition was basedon the Sennheiser input.The interface presented the user with a screen showing alarge button labeled "Click Here to Talk."
A mouse click onthis button caused the system to capture speech starting ahalf second before the click; the system automatically deter-mined when the speaker finished speaking based on silenceduration set at a threshold of two seconds.
The user couldmove to the context of previous questions via mouse clicks.Once the speech was processed, the screen displayed therecognized string of words, a "paraphrase" of the system'sunderstanding of the request, and, where appropriate, a for-matted table of data containing the answer to the query.
Incases where the natural anguage component could notarrive at a reasonable answer, a message window appeareddisplaying one of a small number of error messages.
A logfile was automatically created, containing time-stampsmarking each action by the user and by the system.2.4.
ProcedureSubjects were seated in a quiet room in front of a colormonitor, and had use of a mouse and microphone(s) but nokeyboard.
They were given a short demonstration how touse the system.
Some of the subjects were given additionalinstructions explaining that, while they might have a ten-dency to enunciate more clearly in the face of recognitionerrors, they should try to speak naturally, since the systemwas not trained on overenunciated or separated speech.Once subjects were comfortable with the system, they wereleft alone in the room while they solved travel planning sce-narios.
After they finished as many scenarios as possiblewithin an hour, they were asked to fill out a questionnaireand were given a choice of gift certificate for use at a localbookstore or a contribution to a charitable institution.503.
EXPERIMENTS3.1.
The Effects of Speed and Accuracy Trade-offs on User SatisfactionSince in general, speech understanding systems can tradeaccuracy for speed, we first assessed how these parametersmight affect user behavior and acceptance of the system.The software version of the recognizer was slower than thehardware version (2.5 compared to 0.42 times the utteranceduration), but was substantially more accurate (with a worderror rate of 16.1% as compared with 24.8% on the samesound files).~ I00  io 80 Q.cD6Og: 40q~~ 2O0B WizardSoftwareI ~ Hardware//h7//// v////\] /wQuestion Question Question Question Question 51.
Were the answers provided quickly enough?2.
Did the system understand your requests the first time?3.
I focused most of my attention on solving the problems,rather than trying to make the system understand me.4.
Do you think a person unfamiliar with computers coulduse the system easily?5.
Would you prefer this method to looking up the informa-tion in a book?Figure 1: User SatisfactionTo assess user satisfaction, we compared questionnaireresponses for 46 subjects who used the hardware, 23 whoused the software, and 46 who used the earlier wizard-mediated system.
Mean responses are shown in Figure 1.
Ingeneral, user satisfaction with the speed of the system cor-related with the response time of the system they used;when asked, "Were the answers provided quickly enough?
"69.6% of the hardware users responded "Yes."
In contrast,only 34.8% of the software users and a mere 11.1% of thewizard-system users gave "Yes" responses, a significant dif-ference from the hardware result, ~2 (df=4) = 35.6, p < .001.Although ardware users were pleased with the speed of thesystem; they were less likely than wizard system and soft-ware users to say they focused their attention on solving theproblem rather than on trying to make the system under-stand them (33.3% as compared with 61.4% and 56.5%,respectively), a marginally significant effect, ~2 (df=4) =7.8, p <.10.On several other measures users found the wizard-basedsystem preferable to either the software or the hardware.More wizard-system users said that the system usuallyunderstood them the first time (47.8% as compared with13.0% and 8.7% for the software and hardware users,respectively), ~2(df=4) =22.5, p < .001.
Overall, the wizardsystem users were more likely to say the system could beeasily used by a person who was unfamiliar with computers(78% compared with 43.5% and 35.6% for the software andhardware, respectively) Z2 (df=4) = 20.5, p < .001.
How-ever, in terms of general satisfaction, as expressed inwhether the subjects aid they would prefer using the sys-tem to looking the information up in a book, there was nosignificant difference between the groups, with 52.3%,60.9% and 55.6% "Yes" answers for the three groupsrespectively.Because the hardware system was least satisfying to usersin terms of recognition accuracy, we concluded that thehardware would provide the greatest potential for user adap-tation to the system.
For this reason, we used the hardwaresystem to collect data on the effects of user experience andinstructions regarding hyperarticulation.3.2.
Effect of User Experience on RecognitionUser experience was evaluated in a within-subjects design,counterbalanced forscenario, that compared 24 users' firstand second sessions.
As a global measure of adaptation, welooked at how long it took subjects to complete their twoscenarios.
Although subjects were not told to solve the sce-narios as quickly as possible, they nevertheless took lesstime (10.5 compared to 13.0 minutes) to complete their sec-ond scenarios, F(1,23) = 5.78, p < .05.
This difference waspartially but not completely attributable toa lower numberof total utterances in the second scenario.The users also elicited fewer recognition errors in the sec-ond scenario.
The mean word error rate was 20.4% for thefirst scenario but fell to 16.1% for the second, F(1,22) =5.60, p < .05.
However, not all users decreased their recog-nition error rate.
There was a significant interaction betweeninitial error rate and change in error rate from the first sce-nario to the second, F(1,22) = 10.98, p < .01.
Subjects whohad recognition error rates of 20% or worse in the first sce-nario (N=I 1) tended to improve recognition performance,while subjects who had better initial performance (N=13)did not (Figure 2).
Subjects with initial error rates of 20% or51higher went from an average of 31.3% errors down to19.6%, while subjects with initially lower error ratesshowed no statistically significant change.
For those sub-jects who did improve recognition performance, theimprovement could only be due to user adaptation, sincethe same SLS version was used for both scenarios.500 40W~ 3oo?
200I1.
10High Initial ErrorLow Initial ErrorC,-0 I IScenario 1 Scenario 2Figure 2: Recognition accuracy over time.The improvement in recognition may be due in part to useradaptation to the language models used.
As a measure ofdeviation from the system's language models, we used test-set perplexity, which was based on the bigram probabilitiesof the observed word sequences.
As would be expected,there was a significant, positive average correlationbetween utterance word error and perplexity: mean r = .28,t = 4.55, p < .001.
Thus, one way for subjects to improverecognition accuracy would be to change their language toconform to that of the system model.
Perplexity may there-fore play a role in the decrease in recognition error ratesobserved over time for those subjects who had an error rateof 20% or worse in their first scenario.
For this group ofsubjects, there was a tendency to produce queries withlower sentence perplexity in the second scenario (Figure 3).Using the median as a measure of central tendency (a morestable measure due to the inherent positive skew of perplex-ity), we found that the average median sentence perplexitywas 25.3 for the first scenario and 19.4 for the second, areliable difference, F(1,10) = 7.44, p < .05.~ 35X~,.
30a.c: 25= mqD20o~10High Initial Error" " "0""  Low Initial Errori iScenar io  1 Scenar io  2Figure 3: Median perplexity over time.In addition to decreasing perplexity, subjects who had initialerror rates of greater than 20% also tended to decrease theuse of out-of-vocabulary words in the second scenario,whereas ubjects who had lower error rates did not, a signif-icant interaction, F(1,22) = 6.10, p < .05.
Overall, however,the use of out-of-vocabulary words was rare.These findings indicate that at least o some degree, subjectsadapted to the language models of the system and, in doingso, managed to improve the recognizer's performance.Quite possibly, subjects were finding ways to phrase theirqueries that produced successful answers, and then repro-ducing these phrases in subsequent queries.
In future work,further analyses (for example, looking at dialogue) willaddress this issue in greater detail.3.3.
Effect of Instructions on Speech StyleAnother potential source of recognition errors arises whenthe speech of the user deviates from the acoustic models ofthe system.
Since the vast majority of the data used to trainthe DECIPHER recognizer came from wizard-mediateddata collection \[6\], where recognition performance wasnearly perfect, examples of "frustrated" speech were rare.
Inhuman-human i teraction, when an addressee (such as aforeigner) has difficulty understanding, speakers changetheir speech style to enunciate more clearly than usual (Fer-guson \[3\]).
We suspected that a similar effect might occurfor people speaking to a machine that displayed feedbackshowing less than perfect understanding.
We noticed that,when using an SLS as opposed to a wizard-mediated sys-tem, subjects tended to hyperarticulate: releasing stops,emphasizing initial word segments, pausing between words,and increasing vocal effort.52Although hyperarticulation is a multifaceted behavior, itwas nevertheless possible to make global judgments aboutindividual utterances.
Hyperarticulation was coded for eachutterance on a three-point scale by listening to the utter-ances.
Utterances were coded as (1) clearly natural sound-ing, (2) strongly hyperarticulated, or (3) somewhathyperarticulated.
The coding was done blindly without ref-erence to session context or system performance.Using a within-subjects design, so that any differences inrecognition performance ould be attributed to a change inspeech style, rather than speaker effects, we analyzed thespeech style of 24 subjects' first scenarios (future analyseswill also examine repeat scenarios).
These subjects (ofwhom 20 were also included in the previous analysis ofuser experience) all used the hardware system.
The subjectsaveraged about 10 natural sounding, 4 somewhat hyperar-ticulate, and 5 strongly hyperarticulate utterances ach.
Forthe 13 subjects who had at least three natural and threestrongly hyperarticulated utterances, we compared recogni-tion performance within subjects and found that thestrongly hyperarticulate utterances resulted in higher worderror rates, F(1,12) = 5.19, p < .05.Hyperarticulation was reduced, however, by giving usersinstructions not to "overenunciate" and by explaining thatthe system was trained on "normal" speech.
We calculated ahyperarticulation score for each subject by weighting"strongly hyperarticulated" utterances as 1, "somewhathyperarticulated" utterances as 0.5, and "nonhyperarticu-lated" utterances a  0, and taking the mean weight across allutterances in the scenario.
The 12 subjects who heard theinstructions (the "instruction group") had lower meanhyperarticulation scores, 0.22 as compared with 0.60 for the12 subjects who received no special instructions (the "noinstruction group"), asignificant difference F(1,22) = 11.97,p < .01.Given that the instruction group had significantly fewerhyperarticulated utterances, and given that hyperarticula-tion is associated with lower recognition accuracy, wewould expect he instruction group to have better ecogni-tion performance overall.
However, although the trend wasin that direction (18.1% word error for the instruction groupversus 22.5% for the no-instruction group.
), the differencewas not reliable.
One possible explanation is a lack ofpower in the analysis, as a result of the small number ofsubjects and large individual differences in error rates.
Asecond, not necessarily conflicting explanation is that thesubjects given the instructions to "speak naturally" usedsomewhat less planned and less formal speech.
We noticedthat these subjects tended to have more spontaneous speecheffects, such as verbal deletions, word fragments, lengthen-ings and filled pauses.
Overall, spontaneous speech effectsoccurred in 15% of the 232 utterances for the instructiongroup, compared with 10% for the 229 utterances for theno-instruction group.
Although these baseline rates are low,they may nevertheless have contnbuted topoorer ecogni-tion rates (see Butzberger et al \[2\]).
They may also beindicative of subtle speech style differences between thetwo groups not captured by the coding of hyperarticulation.4.
CONCLUSIONApplication development can benefit from analyses of fac-tors affecting system performance and user satisfaction.
Wehave presented examples of ways in which the behavior andsatisfaction of subjects interacting with an SLS may beaffected.
We have described ways in which parameters ofthe system itself, such as speed and accuracy, affect differ-ent aspects of user satisfaction.
We have examined theeffect of user experience on recognition performance andfound a decrease inword error rate over repeated scenarios.Adaptation was relatively greater for those subjects whohad more than 20% errors on the first scenario.
Thedecrease in errors could be attributed at least in part to adecrease in sentence perplexity and to a reduction in the useof out-of-vocabulary words.
We have also shown a signifi-cant relationship between word error rates and hyperarticu-lation, a speech style that occurs relatively frequently withan imperfect recognizer.
We have shown that instructionsnot to hyperarticulate r duced this maladaptive speechstyle, but that instructions did not result in improved recog-nition performance overall.Our studies have shown that along some dimensions,humans are flexible and can adapt in ways that improve sys-tem performance.
However, hyperarticulation may be amaladaptive behavior for which a technological solutionshould be investigated.
In particular we have found thatstrategies people use to try to improve normal human com-munication (e.g., hyperarticulation) can have the reverseeffect in the context of our current models.
While hyperar-ticulation is an "exaggerated" speech style that mightimprove comprehension for humans, it can cause poor rec-ognition for automatic systems in which "exaggeration" isnot adequately modeled.AcknowledgmentsWe gratefully acknowledge support for this work fromDARPA through the Office of Naval Research contractN00014-90-C-0085.
The govemment has certain rights inthis material.
Any opinions, findings, and conclusions orrecommendations expressed in this material are those of theauthors and do not necessarily reflect he views of the gov-eminent funding agencies.
We also gratefully acknowledgeSteven Tepper for software development.532.3.4.5.6.7.8.9.10.11.12.REFERENCESBly, B., E Price, S. Tepper, E. Jackson, and V. Abrash,"'Designing the Human Machine Interface in the ATISDomain," Prec.
Third DARPA Speech and LanguageWorkshop, pp.
136-140, Hidden Valley, PA, June 1990.Butzberger, J. W., H. Murveit, E. Shriberg, E Price, "Spon-taneous Effects in Large Vocabulary Speech RecognitionApplications," Prec.
DARPA Speech and Natural Lan-guage Workshop, M. Marcus (ed.
), Morgan Kaufmann,1992.Ferguson, C. "'Towards aCharacterization f English For-eigner TaLk," Anthropological Linguistics, 17, pp 1-14,1975.Jackson, E., D. Appelt, J.
Bear, R. Moore, A. Podlozny,"A Template Matcher for Robust NL Interpretation,"Prec.
DARPA Speech and Natural Language Workshop, P.Price (ed.
), Morgan Kaufmann, 1991.Kowtko, J. C. and E J.
Price, "Data Collection and Analy-sis in the Air Travel Planning Domain," Proe.
SecondDARPA Speech and Language Workshop, pp.
119-125,Harwichport, MA, October 1989.MADCOW, "Multi-Site Data Collection for a Spoken Lan-guage System," Prec.
DARPA Speech and Natural Lan-guage Workshop, M. Marcus (ed.
), Morgan Kaufmaun,1992.Murveit, H., J. Butzberger, and M. Weintraub,"SpeechRecognition i  SRI's Resource Management and ATISSystems," Prec.
DARPA Speech and Natural LanguageWorkshop, P. Price (ed.
), Morgan Kaufmann, 1991.Murveit, H. and M. Weintraub, "Real-Time Speech Rec-ognition System," Prec.
DARPA Speech and Natural Lan-guage Workshop, E Price (ed.
), Morgan Kaufmann, 1991.Murveit, H., J. Butzberger, and M. Weintraub, "Perfor-mance of SRI's Decipher Speech Recognition System onDARPA's ATIS Task," Prec.
DARPA Speech and NaturalLanguage Workshop, M. Marcus (ed.
), Morgan Kaufmann,1992.Price E, "Evaluation of Spoken Language Systems: TheATIS Domain,'" Prec.
Third DARPA Speech and LanguageWorkshop, pp.
91-95, Hidden Valley, PA, June 1990.Stern, R. M. and A. I. Rudnicky, "'Spoken-LanguageWorkstations in the Office Environment," Prec.
SpeechTech' 90, Media Dimensions, 1990.Weintraub, M., O. Chen, P. Mankoski, H. Murveit, A. Stol-zle, S. Narayanaswamy, R. Yu, B. Richards, M. Srivastava,J.
Rabay, R. Broderson, "The SRI/UCB Real-Time SpeechRecognition System," Prec.
DARPA Speech and NaturalLanguage Workshop, M. Marcus (ed.
), Morgan Kanfmenn,1992.54
