AUTOMATIC  NOUN CLASS IF ICAT ION BY  US INGJAPANESE-ENGL ISH WORD PA IRS*Naomi InoueKDD R & D Laboratories2-1-50hara, Kamifukuoka-shi Saitama 356, Japaninoue@kddlab.kddlabs.cp.jpABSTRACTThis paper descr ibes a method ofclassifying semantically similar nouns.
Theapproach is based on the "distr ibutionalhypothesis".
Our approach is characterizedby distinguishing among senses of the sameword in order to resolve the "polysemy" issue.The classification result demonstrates thatour approach is successful.1.
INTRODUCTIONSets of semantically similar words arevery useful in natural anguage processing.The general approach toward classifyingwords is to use semantic categories, forexample the thesaurus.
The "is-a" relation isconnected between words and categories.However, it is not easy to acquire the "is-a"connect ion by hand,  and it becomesexpensive.Approaches toward automat ica l lyclassifying words using existing dictionarieswere there fore  a t tempted\ [Chodorow\ ]\[Tsurumaru\] \[Nakamura\].
These approachesare partially successful.
However, there is afatal problem in these approaches, namely,existing dictionaries, particularly Japanesedictionaries, are not assembled on the basisof semantic hierarchy.On the other hand, approaches towardautomatically classifying words by using ala rge-sca le  corpus  have also beenattempted\[Shirai\]\[Hindle\].
They seem to bebased on the idea that semantically similarwords appear in similar environments.
Thisidea is derived from Harris's "distributionalhypothesis"\[Harris\] in linguistics.
Focusingon nouns, the idea claims that each noun ischaracterized by Verbs with which it occurs,and also that nouns are similar to the extentthat they share verbs.
These automaticclassification approaches are also partial lysuccessful.
However, Hindle says that thereis a number of issues to be confronted.
Themost important issue is that of "polysemy".In Hindle's experiment, two senses of"table",that is to say "table under which one canhide" and "table which can be commuted ormemorized", are conflated in the set of wordssimilar to "table".
His result shows thatsenses of the word must be distinguishedbefore classification.
(1)I sit on the table.
(2)I sit on the chair.
(3)I fill in the table.
(4)I fill in the list.For example, the above sentences mayappear in the corpus.
In sentences (1) and (2),"table" and "chair" share the same verb "siton".
In sentences (3) and (4), "table" and"list" share the same verb "fill in".
However,"table" is used in two different senses.
Unlessthey are distinguished before classification,"table", "chair" and "list" may be put into thesame category because "chair" and "list"share the same verbs which are associatedwith "table".
It is thus  necessary  todistinguish the senses of "table" beforeautomatic classification.
Moreover, when thecorpus is not sufficiently large, this must beperformed for verbs as well as nouns.
In thefollowing Japanese sentences, the Japaneseverb "r~ < "is used in different senses.
One is* This study was done during the author's tayat ATR Interpreting Telephony Research Laboratories.201' l-' '1 t"space at objectEl:Please ~ in the reply :form ahd su~rmiE the summary to you.A.
.Figure 1 An example of deep semantic relations and the correspondence"to request information from someone".
Theother is "to give attention in hearing".Japanese words " ~ ~l-~ (name)" and " ~ "~(music)" share the same verb" ~ < ".
Usingthe small corpus, " ~ Hl~ (name)" and" ~(music)" may be classified into the samecategory because they share the same verb,though not the same sense, on relativelyfrequent.
(5):~ ~ t" M <(6 ) :~ ~" ~JThis paper describes an approach toautomatically classify the Japanese nouns.Our approach  is character i zed  bydistinguishing among senses of the sameword by using Japanese-English word pairsextracted from a bilingual database.
Wesuppose here that some senses of Japanesewords are dist inguished when Japanesesentences are t rans la ted  into anotherlanguage.
For example,  The fol lowingJapanese sentences (7),(8) are translated intoEnglish sentences (9),(10), respectively.
(7)~ ~J~ ~: ~(8)~ ~ ~ ~ ~ ~-(9)He sends a letter.
(t0)He publishes a book.The Japanese word " ~ T"  has at leasttwo senses.
One is "to cause to go or be takento a place" and the other is "to have printedand put on sale".
In the above example, theJapanese word" ~ ~-" corresponds to "send"from sentences (7) and (9).
The Japaneseword " ~ -~" also corresponds to "publish"from sentences (8) and (10).
That is to say,the Japanese word" ~ T"  is translated into202different English words according to thesense.
This example shows that it may bepossible to distinguish among senses of thesame word by using words from anotherlanguage.
We used Japanese-English wordpairs, for example," ~ ~-send" and" ~ ~-publish", as senses of Japanese words.In this paper, these word pairs areacquired from ATR's large scale database.2.
CONTENT OF THE DATABASEATR has constructed a large-scaledatabase which is collected from simulatedtelephone and keyboard  conversat ions\[Ehara\].
The sentences collected in Japaneseare manually translated into English.
Weobtain a bilingual database.
The database iscalled the ATR Dialogue Database(ADD).ATR aims to build ADD to one million wordscovering two tasks.
One task is dialoguesbetween secretar ies and part ic ipants ofinternat ional  conferences.
The other  isd ia logues between t rave l  agents  andcustomers.
Collected Japanese and Englishsentences are morphological ly analyzed.Japanese sentences are also dependencyanalyzed and given deep semantic relations.We use 63 deep semantic cases\[ Inoue\].Correspondences of Japanese and Englishare made by several linguistic units, forexample words, sentences and so on.Figure 1 shows an example of deepsemantic relations and correspondences ofJapanese and English words.
The sentence isalready morphologically analyzed.
The solidline shows deep semantic relations.
TheJapanese nouns" ') 7" ~ 4 7 ~r -- ~"  and "~~'~" modify the Japanese verbs "~ v~" and "~", respectively.
The semantic relations are"space at" and "object", which are almostequal to " locat ive" and "objective" ofFillmore's deep case\[Fillmore\].
The dottedline shows the word correspondence b tweenJapanese and English.
The Japanese words"V 7"~ 4 7 ~- - - .
/~" , "~" , "~,~)"and"~L" correspond to the English words "replyform", "fill out", "summary" and "submit",respectively.
Here, " ~ v," and " ~i \[~" areconjugations o f"  ~ < " and " ~ -?",respectively.
However, it is possible toextract  semant ic  re la t ions  and wordcorrespondence in dictionary form, becauseADD includes the dictionary forms.3.
CLASSIFICATION OF NOUNS3.1 Using DataWe automatically extracted from ADDnot only deep semantic relations betweenJapanese nouns and verbs but also theEnglish word which corresponds to theJapanese word.
We used telephone dialoguesbetween secretaries and participants becausethe scale of analyzed words was largest.Table 1 shows the cur rent  number  ofanalyzed words.Table I Analyzed words counts of ADDMedia Task WordsConference 139,774 TelephoneTravel 11,709Conference 64,059 KeyboardTravel 0Figure 2 shows an example of the dataextracted from ADD.
Each field is delimitedby the delimiter "1"- The first field is thedialogue identification umber in which thesemantic relation appears.
The second andthe third fields are Japanese nouns and theircorresponding English words.
The next 2f ie lds are Japanese  verbs and the i rcorresponding English words.
The last is thesemantic relations between ouns and verbs.Moreover, we automatically acquiredword pairs from the data shown in Figure 2.Different senses of nouns appear far lessfrequently than those of verbs because thedatabase is restricted to a specific task.
Inthis experiment, only word pairs of verbs areused.
Figure 3 shows deep semantic relationsbetween ouns and word pairs of verbs.
Thelast field is raw frequency of co-occurrence.We used the data shown in Figure 3 for nounclassification.1\[ ~J $,~  \[registration feel ?
?
Ipay\[object151~ ?.
'~ Isummaryl ~ ~-Isend\]object15717" ~ ~/ -  ~" 4 ~ ~f\ [proceedingl~lissuelobject41~ ~lconferencel~ ;5 Ibe heldlobject8\] ~ r~9 Iquestionl~ ;5Ihavelobject31J~ ~ Ibusl~ ~ Itakelobject1801~: ~ Inewspaperl~!
;5 Iseelspace atFigure 2 An example of data extractedfrom ADDThe experiment is done for a sample of138 nouns which are included in the 500most frequent words.
The 500 most frequentwords cover 90% of words accumulated in thetelephone dialogue.
Those nouns appearmore frequently than 9 in ADD.~ l~ ~-paylobjectll~,'~ I~ T -sendlobjectl27" ~ ":/- -7" ~ :~/f l~-issue~objectl2~ ~\ ]~ ;5 -be heldlobject 16~o9 I~ $ -havelobjectl7/< ;1, \]!~!
;5 -take\]objectll~ I~ $ -seelspace atl 1Figure 3 - An example of semantic rela-tions of nouns and word pairs3.2 Semantic Distance of NounsOur classification approach is based onthe "distributional hypothesis".
Based onthis semantic theory, nouns are similar tothe extent hat they share verb senses.
Theaim of this paper is to show the efficiency ofusing the word pair as the word sense.
Wetherefore used the following expression(l),which was already defined by Shirai\[Shirai\]as the distance between two words.
The203d(a,b)~(M(a,v,r) ,M(b,v,r))v( V,rE R~(M(a ,v , r )  + M(b,v,r))v(V , r (R(1)Here, a,b : noun (a,b (N)r : semant ic  relat ionv : verb sensesN : the set of  nounsV : the set of  verb sensesR : the set of  semant ic  relat ionsM(a,v,r) : the frequency of  the semant ic  relat ion rbetween a and v?P(x,y) = f i  + y (x > 0, y > 0)(x=0ory=0)second term of the expression can show thesemant ic  simi lar i ty between two nouns,because it is the ratio of the verb senses withwhich both nouns (a and b) occur and all theverb senses with which each noun (a or b)occurs.
The distance is normalized from 0.0 to1.0.
If one noun (a) shares all verb senseswith the other noun (b) and the frequency isalso same, the distance is 0.0.
If one noun (a)shares no verb senses with the other noun(b), the distance is 1.0.3.3 C lass i f i ca t ion  MethodFor the classification, we adopted clusteranalysis which is one of the approaches fnmultivariant analysis.
Cluster analysis isgenerally used in various fields, for examplebiology, ps.ychology, etc..
Some hierarchicalclustering methods, for example the nearestneighbor method, the centroid method, etc.,have been studied.
It has been proved thatthe centroid method can avoid the chaineffect.
The chain effect is an undesirablephenomenon i  which the nearest unit is notalways classified into a cluster and moredistant units are chained into a cluster.
Thecentroid method is a method in which thecluster is characterized by the centroid ofcategorized units.
In the following section,the result obtained by the centroid method isshown.4.EXPERIMENT4.1 Clustering Resu l tAll 138 nouns are h ie rarch ica l lyclassified.
However, only some subsets of thewhole hierarchy are shown, as space isl imited.
In F igure 4, we can see thatsemantically similar nouns, which may bedefined as "things made from paper", aregrouped together.
The X-axis is the semanticdistance defined before.
Figure 5 showsanother subset.
All nouns in Figure 5, "~ ~_(decision)", ~  ~(presentation)", ";~~" - ~"(speech)" and " ~(talk)" ,  have an activeconcept like verbs.
Subsets of nouns shown inFigures 4 and 5 are fairly coherent.
However,all subsets of nouns are not coherent.
InFigure 6, " ~ ~ 4 b ?
(slide)", "~, ~ (draft)"," ~" ~ (conference site)", "8 E (8th)" and" ~R(stat ion)"  are grouped together .
Thesemantic distances are 0.67, 0.6, 0.7 and 0.8.The distance is upset when "~ ~(conferencesite)" is attached to the cluster containing":~ ~ 4 b'(slide)" and "~ ~(draft)".
This isone characteristic of the centroid method.However ,  th is  seems to resu l t  in asemantically ess similar cluster.
The wordpairs of verbs, the deep semantic relationsand the frequency are shown in Table 2.After "~ ~ 4 b ~ (slide)" and "~ ~(draft)" aregrouped into a cluster, the cluster and " ~(conference site)" share two word pairs, " fE") -use" and "~ ~ -be".
"~ Yo -be" contributesmore largely to attach " ~ ~(conferencesite)" to the cluster than "tE ~) -use" becausethe frequency of co-occurrence is greater.
Inthis sample, " ~ ~-be" occurs with morenouns than "f~ ") -use".
It shows that "~J~ Yo -be" is less important in characterizing nouns204though the raw frequency of co-occurrence isgreater.
It is therefore necessary to develop ameans of not relying on the raw frequency ofco-occurrence,  in order to make theclustering result more accurate.
This is leftto further study.4.2 Estimation of the ResultAll nouns are hierarchically classified,but some semantically separated clusters areacquired if the threshold is used.It is possible to compare clusters derivedfrom this exper iment  with semant iccategories which are used in our automaticinterpreting telephony system.
We usedexpression (2), wh ich  was def ined byGoodman and Kruskal\[Goodman\], i  order toobjectively compare them.0.0I~J :~ b (list) ....~( fo rm)~=~(material) '~T ~_-~ (hope)~(document )7" 7.~ b ~ ~ ~ (abstract)7" ~ ~ ~ ~ (program)Figure 40.2 0.4 0.6 0.8 1.0I I I I It-- iAn example of the classification ofnouns0.0 0.2 0.4 0.6 0.8 1.0I I I I I Ii i~(decision)~ (presentation)~" - ~- (speech)~8(talk)Figure 50.0~ -1" b" (slide)~, ~ (draft)~ (conference site)8 E (Sth)~(station)Figure 6Another example of the classification ofnouns0.2 0.4 0.6I J J0.8JAnother example of the classification ofnouns1.0l205Table 2noun~ d" b" (slide)/~,~ (draft)~ J~ (conference site)8 \[3 (8th)~(stat ion)A subset of semantical ly similar nounsword pairs of verb deep case frequencyT ~-make goal 1{~ 7~ -make object 15 -use object 1f~ & -make object 1?
-be object 1o_look forward to object 1~J~ ?
-take condition 1~ ") -get space to 1") -use object 1~ 7o -can space at 1"~ -say space at 1/~ & -be object 2~.
7~ -end time 2/~ 7o -be object 1~\] < -guess content 1~ 7~ -take condition 11~ ~ ~ ~- there  be space from 1p --Here ,P1 "P2(2)PlP1 - 1- f-mpP2 =  .ri.
(1 - fi.
Jfi.
)i l lf.= = max(f.1, f.2, "", f,~}farn a -- max{fa l ,  fa2, "" ,  faq}% = n /nf.j = n lnA : a set of clusters which are automatical ly obtained.B : a set ofclusters which are used in our interpret ingtelephony system.p ?
the number of clusters of a set Aq : the number of clusters of a set Bnij : the number of nouns which are included in both the ithcluster of A and the jth cluster of Bn.j : the number ofnouns which are included in the jth clusterof Bn : all nouns which are included in A or B206They proposed that one set of clusters, called'A', can be estimated to the extent that 'A'associates with the other set of clusters,called 'B'.
In figure 7, two results are shown.One (solid line) is the result of using the wordpair to distinguish among senses of the sameverb.
The other (dotted line} is the result ofusing the verb form itself.
The X-axis is thenumber of classified nouns and the Y-axis isthe va lue der ived  f rom the aboveexpression.Figure 7 shows that it is better touse word pairs of verbs than not use them,when fewer than about  30 nouns areclassified.
However, both are almost thesame, when more than about 30 nouns areclassif ied.
The resu l t  proves that  thedistinction of senses of verbs is successfulwhen only a few nouns are classified.I Word Pain of VerbsB .......... Verb Form0.30.2| h"0.1 ' = ' ; ~J0.0z...:.L.'!/?
~~./ ,  .
~,in50 100Number  of  Nou,usFigure 7 Estimation result5.
CONCLUSIONUsing word pairs  of Japanese  andEnglish to distinguish among senses of thesame verb, we have shown that using wordpairs to classify nouns is better than notusing word pairs, when only a few nouns areclassified.
However, this experiment did notsucceed for a sufficient number of nouns fortwo reasons.
One is that the raw co-occurrentfrequency is used to calculate the semanticdistance.
The other is that the sample size istoo small.
It is thus necessary to resolve thefollowing issues to make the classificationresult more accurate.
(1)to develop a means of using thefrequency normalized by expected wordpairs.
(2)to estimate an adequate sample size.In this experiment, we acquired wordpairs and semantic re lat ions  from ourdatabase.
However, they are made by hand.It is also preferable to develop a method ofautomatical ly  acquir ing them from thebilingual text database.Moreover ,  we want  to app ly  theh ierarch ica l ly  c lass i f ied resu l t  to thet rans la ted  word se lect ion prob lem inMachine translation.ACKNOWLEDGEMENTSThe author is deeply grateful toDr.
Akira Kurematsu, President of ATRInterpreting Telephony ResearchLaboratories, Dr. Toshiyuki Takezawa andother members of the Knowledge & DataBase Department for their encouragement,during the author's StaY at ATR InterpretingTelephony Research Laboratories.REFERENCES\ [Chodorow\]  Chodorow,  M. S., et al"Extracting Semantic Hierarchies from aLarge On-line Dictionary.
", Proceedings ofthe 23rd Annual Meeting of the ACL, 1985.\[Ehara\] Ehara, T., et al "ATR DialogueDatabase", Proceedings of ICSLP, 1990.\[Fillmore\] Fillmore, C. J.
"The case for case",in E. Bach & Harms (Eds.)
Universals inlinguistic theory, 1968.\[Goodman\] Goodman, L. A., and KruskalW.H.
"Measures of Association for CrossClassifications", J. Amer.
Statist.
Assoc.
49,1954.\ [Harr is \ ]  Har r i s ,  Z. S. "Mathemat ica lS t ructures  of Language" ,  a Wi ley -Interscience Publication.207\[Hindle\] Hindle, D. "Noun Classificationfrom Pred icate-Argument  S ructures",Proceedings of 28th Annual Meeting of theACL, 1990.\[Inoue\].
Inoue, N., et al "Semantic Relationsin ATR Linguistic Database" (in Japanese),ATR Technical Report TR-I-0029, 1988.\[Nakamura\] Nakamura, J., et al "AutomaticAnalysis of Semantic Relation betweenEnglish Nouns by an Ordinal Engl ishDictionary" (in Japanese), the Institute ofE lect ron ics ,  In fo rmat ion  andCommunicat ion Eng ineers ,  Technica lReport, NLC-86, 1986.\ [Shirai \ ]  Sh i ra i  K., et al "DatabaseFormulation and Learning Procedure forKakar iuke Dependency Analys is"  (inJapanese), Transactions of InformationProcessing Society of Japan, Vol.26, No.4,1985.\ [Tsurumaru \ ]  Tsurumaru  H., et al"Automatic Extract ion of HierarchicalS t ruc ture  of Words from Def in i t ionSentences" (in Japanese), the InformationProcessing Society of Japan, Sig.
Notes, 87-NL-64, 1987.208
