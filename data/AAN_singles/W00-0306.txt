Stochastic Language Generation for Spoken Dialogue SystemsAlice H. OhCarnegie Mellon University5000 Forbes Ave.Pittsburgh, PA 15213aliceo+@cs.cmu.eduAlexander I. RudnickyCarnegie Mellon University5000 Forbes Ave.Pittsburgh, PA 15213air+@cs.cmu.eduAbstractThe two current approaches to languagegeneration, Template-based and rule-based(linguistic) NLG, have limitations whenapplied to spoken dialogue systems, in partbecause they were developed for textgeneration.
In this paper, we propose a newcorpus-based approach to natural anguagegeneration, specifically designed for spokendialogue systems.Introduct ionSeveral general-purpose rule-based generationsystems have been developed, some of whichare available publicly (cf.
Elhadad, 1992).Unfortunately these systems, because of theirgenerality, can be difficult to adapt to small,task-oriented applications.
Bateman andHenschel (1999) have described a lower cost andmore efficient generation system for a specificapplication using an automatically customizedsubgrammar.
Busemann and Horacek (1998)describe a system that mixes templates and rule-based generation.
This approach takesadvantages of templates and rule-basedgeneration as needed by specific sentences orutterances.
Stent (1999) has proposed a similarapproach for a spoken dialogue system.However, there is still the burden of writing andmaintaining rammar rules, and processing timeis probably too slow for sentences usinggrammar rules (only the average time fortemplates and rule-based sentences combined isreported in Busemann and Horacek, 1998), foruse in spoken dialogue systems.Because comparatively less effort is needed,many current dialogue systems use template-based generation.
But there is one obviousdisadvantage: the quality of the output dependsentirely on the set of templates.
Even in arelatively simple domain, such as travelreservations, the number of templates necessaryfor reasonable quality can become quite largethat maintenance becomes a serious problem.There is an unavoidfible trade-off between theamount of time and effort in creating andmaintaining templates and the variety andquality of the output utterances.Given these shortcomings of the aboveapproaches, we developed a corpus-basedgeneration system, in which we model anguagespoken by domain experts performing the task ofinterest, and use that model to stochasticallygenerate system utterances.
We have applied thistechnique to sentence realization and contentplanning, and have incorporated the resultinggeneration component into a working naturaldialogue system (see Figure 1).
In this paper, wedescribe the technique and report the results oftwo evaluations.We used two corpora in the travelreservations domain to build n-gram languagemodels.
One corpus (henceforth, the CMUcorpus) consists of 39 dialogues between a travelagent and clients (Eskenazi, et al 1999).SurfaceReal izat ionContentPlanningSentencePlanningDialogue Manager Generation EngineFigure 1 : Overall Architecture27query_arrive_cityquery_arrive_timequery_confirmquery_depart_datequery_depart_timequery_pay_by_cardquery_preferred_airportquery_returndatequery_return_timehotel car infohotel_hotel_chainhotel_hotel_infoinform_airportinform_confirm_utteranceinform_flightinform_flight_anotherinform_flight_earliern form_flight_earliestinform_flight_laterinform_flight_latestinform_not_availinform_num_flightsinform_priceotherFigure 2 : utterance classesairlinearriveairportarriveSci:ty -arrive_date -arrive_timecar companycar pricedepart airportdepart citydepart_datedepart_timeflight_humhotel_cityhotel_pricenamenum_flightspmpriceFigure 3 : word classesAnother corpus (henceforth, the SRI corpus)consists of 68 dialogues between a travel agentand users in the SRI community (Kowtko andPrice 1989).The utterances in the two corpora weretagged with utterance classes and word classes(see Figure 2 and Figure 3).
The CMU corpuswas manually tagged, and back-off trigrammodels built (using Clarkson and Rosenfeld,1997).
These language models were used toautomatically tag the SRI corpus; the tags weremanually checked.1 Content P lanningIn content planning we decide which attributes(represented as word classes, see Figure 3)should be included in an utterance.
In a task-oriented dialogue, the number of attributesgenerally increases during the course of thedialogue.
Therefore, as the dialogue progresses,we need to decide which ones to include at eachsystem turn.
If we include all of them every time(indirect echoing, see Hayes and Reddy, 1983),the utterances become overly lengthy, but if weremove all unnecessary attributes, the user mayget confused.
With a fairly high recognitionerror rate, this becomes an even more importantissue.The problem, then, is to find a compromisebetween the two.
We compared two ways tosystematically generate system utterances withonly selected attributes, such that the user hearsrepetition of some of the constraints he/she hasspecified, at appropriate points in the dialogue,without sacrificing naturalness and efficiency.The specific problems, then, are deciding whatshould be repeated, and when.
We first describea simple heuristic of old versus new information.Then we present a statistical approach, based onbigram models.1.1 First  approach:  old versus newAs a simple solution, we can use the previousdialogue history, by tagging the attribute-valuepairs as old (previously said by the system)information or new (not said by the system yet)information.
The generation module wouldselect only new information to be included in thesystem utterances.
Consequently, information?
given by the user is repeated only once in thedialogue, usually in the utterance immediatelyfollowing the user utterance in which the newinformation was given 1.Although this approach seems to work fairlywell, echoing user's constraints only once maynot be the right thing to do.
Looking at human-human dialogues, we observe that this is notvery natural for a conversation; humans oftenrepeat mutually known information, and theyalso often do not repeat some information at all.Also, this model does not capture the closerelationship between two consecutive utteranceswithin a dialogue.
The second approach tries toaddress these issues.1.2 Second approach:  statist ical  mode lFor this approach, we adopt the first of the twosub-maxims in (Oberlander, 1998) '?
'Do thehuman thing".
Oberlander (1998) talks aboutgeneration of referring expressions, but it isuniversally valid, at least within naturallanguage generation, to say the best we can do isWhen the system utterance uses a template that doesnot contain the slots for the new information given inthe previous user utterance, then that newinformation will be confirmed in the next availablesystem utterance in which the template contains thoseslots.28to mimic human behavior.
Hence, we built atwo-stage statistical model of human-humandialogues using the CMU corpus.
The modelfirst predicts the number of attributes in thesystem utterance given the utterance class, thenpredicts the attributes given the attributes in theprevious user utterance.1.2.1 The number of attributes modelThe first model will predict the number ofattributes in a system utterance given theutterance class.
The model is the probabilitydistribution P(nk) = P(nklck), where nk is thenumber of attributes and Ck is the utterance classfor system utte~anee k.1.2.2 The bigram model of the attributesThis model will predict which attributes to usein a system utterance.
Using a statistical model,what we need to do is find the set of attributesA* = {al, az .
.
.
.
.
an } such thatA * = arg max F I  P(al, a2 ..... an)We assume that the distributions of the ai'sare dependent on the attributes in the previousutterances.
As a simple model, we look only atthe utterance immediately preceding the currentutterance and build a bigram model of theattributes.
In other words, A* = arg max P(AIB),where B = {b l ,  b2 .
.
.
.
.
b in},  the set of mattributes in the preceding user utterance.If we took the above model and tried toapply it directly, we would run into a seriousdata sparseness problem, so we make twoindependence assumptions.
The first assumptionis that the attributes in the user utterancecontribute independently to the probabilities ofthe attributes in the system utterance followingit.
Applying this assumption to the model above,we get the following:mA * = arg max ~ P(bk)P(A I bk)k=lThe second independence assumption is thatthe attributes in the system utterance areindependent of each other.
This gives the finalmodel that we used for selecting the attributes.m tlA*.
= arg max ~ P(bk ) \ [ ' I  P(al I bk)k=l i=129Although this independence assumption isan oversimplification, this simple model is agood starting point for our initialimplementation f this approach.2 Stochastic Surface RealizationWe follow Busemann and Horacek (1998) indesigning our generation engine with "differentlevels of granularity."
The different levelscontribute to the specific needs of the variousutterance classes.
For example, at the beginningof the dialogue, a system greeting can be simplygenerated by a "canned" expression.
Other short,simple utterances can be generated efficiently bytemplates.
In Busemann and Horacek (1998), theremaining output is generated by grammar rules.We replace the gefieration grammar with asimple statistical anguage model to generatemore complex utterances.There are four aspects to our stochasticsurface realizer: building language models,generating candidate utterances, scoring theutterances, and filling in the slots.
We explaineach of these below.2.1 Building Language ModelsUsing the tagged utterances as described inthe introduction, we built an unsmoothed n-gramlanguage model for each utterance class.
Tokensthat belong in word classes (e.g., "U.S.Airways" in class "airline") were replaced by theword classes before building the languagemodels.
We selected 5 as the n in n-gram tointroduce some variability in the outpututterances while preventing nonsense utterances.Note that language models are not used herein the same way as in speech recognition.
Inspeech recognition, the language modelprobability acts as a 'prior' in determining themost probable sequence of words given theacoustics.
In other words,W* = arg max P(WIA)= arg max P(AI W)Pr(W)where W is the string of words, wl, ..., wn, andA is the acoustic evidence (Jelinek 1998).Although we use the same statistical tool,we compute and use the language modelprobability directly to predict he next word.
Inother words, the most likely utterance is W* =arg max P(WIu), where u is the utterance class.We do not, however, look for the most likelyhypothesis, but rather generate each wordrandomly according to the distribution, asillustrated in the next section.2.2 Generat ing  Ut terancesThe input to NLG from the dialoguemanager is a frame of attribute-value pairs.
Thefirst two attribute-value pairs specify theutterance class.
The rest of the frame containsword classes and their values.
Figure 4 is anexample of an input frame to NLG.- act-querycontent depart_timedepart_city New Yorkarrive_city San Franciscodepart_date 19991117}Figure 4 : an input frame to NLGThe generation engine uses the appropriatelanguage model for the utterance class andgenerates word sequences randomly accordingto the language model distributions.
As inspeech recognition, the probability of a wordusing the n-gram language model isP(wi) = P(wilwi.1, wi.2 .
.
.
.
Wi.
(n.1) , U)where u is the utterance class.
Since we havebuilt separate models for each of the utteranceclasses, we can ignore u, and say thatP(wi) = P(wilw|.l, wi-2 .
.
.
.
Wi.
(n.1))using the language model for u.Since we use unsmoothed 5,grams, we willnot generate any unseen 5-grams (or smaller n-grams at the beginning and end of an utterance).This precludes generation of nonsenseutterances, at least within the 5-word window.Using a smoothed n-gram would result in morerandomness, but using the conventional back-offmethods (Jelinek 1998), the probability massassigned to unseen 5-grams would be verysmall, and those rare occurrences of unseen n-grams may not make sense anyway.
There is theproblem, as in speech recognition using n-gramlanguage models, that long-distance dependencycannot be captured.=2.3 Scoring UtterancesFor each randomly generated utterance, wecompute a penalty score.
The score is based onthe heuristics we've empirically selected.Various penalty scores are assigned for anutterance that 1. is too short or too long(determined by utterance-class dependentthresholds), 2. contains repetitions of any of theslots, 3. contains lots for which there is no validvalue in the frame, or 4. does not have somerequired slots (see section 2 for deciding whichslots are required).The generation engine generates a candidateutterance, scores it, keeping only the best-scoredutterance up to that point.
It stops and returns thebest utterance when it finds an utterance with azero penalty scoreTor uns out of time.2.4 Fil l ing SlotsThe last step is filling slots with the appropriatevalues.
For example, the utterance "What timewould you like to leave {depart_city}?
"becomes "What time would you like to leaveNew York?
".3 EvaluationIt is generally difficult to empirically evaluate ageneration system.
In the context of spokendialogue systems, evaluation of NLG becomesan even more difficult problem.
One reason issimply that there has been very little effort inbuilding generation engines for spoken dialoguesystems.
Another reason is that it is hard toseparate NLG from the rest of the system.
It isespecially hard to separate evaluation oflanguage generation and speech synthesis.As a simple solution, we have conducted acomparative evaluation by running two identicalsystems varying only the generation component.In this section we present results from twopreliminary evaluations of our generationalgorithms described in the previous ections.3.1 Content Planning: ExperimentFor the content planning part of the generation-system, we conducted a comparative evaluationof the two different generation algorithms:old/new and bigrams.
Twelve subjects had twodialogues each, one with the old/new generationsystem, and another with the bigrams generation30system (in counterbalanced order); all othermodules were held fixed.
Afterwards, eachsubject answered seven questions on a usabilitysurvey.
Immediately after, each subject wasgiven transcribed logs of his/her dialogues andasked to rate each system utterance on a scale of1 to 3 (1 = good; 2 = okay; 3 = bad).3.2 Content Planning: ResultsFor the usability survey, the results seem toindicate subjects' preference for the old/newsystem, but the difference is not statisticallysignificant (p - 0.06).
However, six out of thetwelve subjects chose the bigram system to thequestion "Durqng-the session, which system'sresponses were easier to understand?"
comparedto three subjects choosing the old/new system.3.3 Surface Realization: ExperimentFor surface realization, we conducted a batch-mode evaluation.
We picked six recent calls toour system and ran two generation algorithms(template-based generation and stochasticgeneration) on the input frames.
We thenpresented to seven subjects the generateddialogues, consisting of decoder output of theuser utterances and corresponding systemresponses, for each of the two generationalgorithms.
Subjects then selected the outpututterance they would prefer, for each of theutterances that differ between the two systems.The results show a trend that subjects preferredstochastic generation over template-basedgeneration, but a t-test shows no significantdifference (p = 0.18).
We are in the process ofdesigning a larger evaluation.4 ConclusionWe have presented a new approach to languagegeneration for spoken dialogue systems.
Forcontent planning, we built a simple bigrammodel of attributes, and found that, in our firstimplementation, it performs as well as aheuristic of old vs. new information.
For surfacerealization, we used an n-gram language modelto stochastically generate ach utterance andfound that the stochastic system performs atleast as well as the template-based system.Our stochastic generation system has severaladvantages.
One of those, an important issue forspoken dialogue systems, is the response time.With stochastic surface realization, the averagegeneration time for the longest utterance class(10 - 20 words long) is about 200 milliseconds,which is much faster than any rule-basedsystems.
Another advantage is that by using acorpus-based approach, we are directlymimicking the language of a real domain expert,rather than attempting to model it by rule.Corpus collection is usually the first step inbuilding a dialogue system, so we are leveragingthe effort rather than creating more work.
Thisalso means adapting this approach to newdomains and even new languages will berelatively simple.The approach we present does require someamount of knowledge ngineering, though thisappears to overlap with work needed for otherparts of the dialogue system.
First, defining theclass of utterance and the attribute-value pairsrequires care.
Second, tagging the human-humancorpus with the right classes and attributesrequires effort.
However, we believe the taggingeffort is much less difficult than knowledgeacquisition for most rule-based systems or eventemplate-based systems.
Finally, what maysound right for a human speaker may soundawkward for a computer, but we believe thatmimicking a human, especially a domain expert,is the best we can do, at least for now.AcknowledgementsWe are thankful for significant contribution byother members of the CMU CommunicatorProject, especially Eric Thayer, Wei Xu, andRande Shern.
We would like to thank thesubjects who participated in our evaluations.
Wealso extend our thanks to two anonymousreviewers.ReferencesBateman, J. and Henschel, R. (1999) From fullgeneration to 'near-templates' without losinggenerality.
In Proceedings of the KI'99 workshop,"May I Speak Freely?
"Busemann, S. and Horacek, H. (1998) A flexibleshallow approach to text generation.
InProceedings of the International Natural LanguageGeneration Workshop.
Niagara-on-the-Lake,Canada..31
