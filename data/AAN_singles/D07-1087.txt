Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
827?836, Prague, June 2007. c?2007 Association for Computational LinguisticsBootstrapping Information Extraction from Field BooksSander Canisius and Caroline SporlederILK / Communication and Information SciencesTilburg University, P.O.
Box 90153, 5000 LE Tilburg, The Netherlands{S.V.M.Canisius,C.Sporleder}@uvt.nlAbstractWe present two machine learning ap-proaches to information extraction fromsemi-structured documents that can be usedif no annotated training data are available,but there does exist a database filled withinformation derived from the type of docu-ments to be processed.
One approach em-ploys standard supervised learning for infor-mation extraction by artificially constructinglabelled training data from the contents ofthe database.
The second approach com-bines unsupervised Hidden Markov mod-elling with language models.
Empiricalevaluation of both systems suggests that it ispossible to bootstrap a field segmenter froma database alone.
The combination of Hid-den Markov and language modelling wasfound to perform best at this task.1 IntroductionOver the past decades much textual data has be-come available in electronic form.
Many text typesare inherently more or less structured, for example,classified advertisements for appartments, medicalrecords, or logs of archaeological finds or zoologicalspecimens collected during expeditions.
Such doc-uments consist of a number of shorter texts (or en-tries), each describing an individual object (e.g., anappartment, or an archaeological find) or event (e.g.,a patient presenting to a health care provider).
Thesedescriptions in turn typically consist of different seg-ments (or fields) which contain information of a spe-cific type drawn from a more or less given inven-tory.
Example (1), for instance, shows two descrip-tions of zoological specimens (a snake and threefrogs) collected during an expedition.
The descrip-tions contain different segments giving informationabout the specimens and the circumstances of theircollection.
For example, in the first description, Lep-tophis and ahaetulla refer, respectively, to the genusand species of the specimen, road to Overtoom men-tions the place of collection, in bush above water en-codes information about the biotope, in the processof eating Hyla minuta is a remark about the circum-stances of collection, 16-V-1968 gives the collectiondate and RMNH 15100 the registration number.
(1) Leptophis ahaetulla, road to Overtoom, in bushabove water in the process of eating Hyla minuta16-V-1968.
RMNH 15100Hyla minuta 1 ?
2 ?
Las Claritas, 9-VI-1978 quak-ing near water 50 cm above water surface, near sec-ondary vegetation, 200 m, M.S.
Hoogmoed, RMNH27217 27219Unfortunately, this inherent structure is rarelymade explicit.
While the different object or eventdescriptions might be indicated by additional white-space or other formatting means, as in the exampleabove, the individual fields within a description aretypically not marked in any way.
However, knowl-edge of the inherent structure would be very bene-ficial for information extraction and retrieval.
Forinstance, texts in their raw form only allow key wordsearch.
To retrieve all entries describing specimensof type Hyla minuta from a zoological field report,one can only search for occurrences of that stringanywhere in the document.
This can return false827positives, such as the first description in (1) above,which does contain the string but is not about a Hylaminuta specimen but about a specimen of type Lep-tophis ahaetulla (the string Hyla minuta just hap-pens to occur in the SPECIAL REMARKS field).
Onthe other hand, if the genus and species informationin an entry was explicitly marked, it would be pos-sible to query specifically for entries whose GENUSis Hyla and whose SPECIES is minuta, thus avoidingthe retrieval of entries in which this string occurs inanother field.The task of automatically finding and labellingsegments in object or event descriptions has beenreferred to as field segmentation (Grenager et al,2005).1 It can be seen as a sequence labelling prob-lem, where each text is viewed as a sequence oftokens and the aim is to assign each token a labelindicating to what segment the token belongs (e.g.,BIOTOPE or LOCATION).
If training data in the formof texts annotated with segment information wasreadily available, the problem could be approachedby training a sequence labeller in a supervised ma-chine learning set-up.
However, manually annotateddata is rarely available.
Creating it from scratch isnot only time consuming but usually also requiresa certain amount of expert knowledge.
Moreover,the sequence labeller has to be re-trained for eachnew domain (e.g., natural history vs. archaeology)and possibly also each sub-domain (e.g., insects vs.mammals) due to the fact that the inventory of fieldsvaries.Thus, fully supervised machine learning is notfeasible for this task.
In this paper, we explore twoapproaches which require no or only a very smallamount of manually labelled training data.
Bothapproaches exploit the fact that there are often re-sources derived from the original documents thatcan potentially be utilised to bootstrap a sequencelabeller in the absence of labelled training data.
Itis common practice, for example, that informationcontained in (semi-structured) field reports or medi-cal records is manually entered into a database, usu-ally in an attempt to make the data more accessi-1The task differs from many other information extractionproblems in which the aim is to extract short pieces of relevantinformation from larger text of largely irrelevant information.In field segmentation, all or most of the information in the inputdocument is assumed to be relevant and the task is to segment itinto fields containing different types of information.ble and easier to search.
In such databases, eachrow corresponds to an entry in the original docu-ment (e.g., a zoological specimen) and the databasecolumns correspond to the fields one would like todiscern in the original document.
Manually convert-ing raw text documents into databases is a labori-ous task though, and it is rather common that thedatabase covers only a small fraction of the objectsdescribed in the original texts.
The research questionwe address in this paper is whether it is possible tobootstrap a domain-specific field segmentation sys-tem from an existing, manually created database forthat domain.
Such a system could then be applied tothe remaining texts in that domain, which could thenbe segmented (semi-)automatically and possibly beadded to the original database.A database does not make perfect training mate-rial for a field segmenter though, as it is only de-rived from the original document and there are typ-ically significant (and sometimes systematic) differ-ences between the two data sources: First, whilethe ordering of the segments in a semi-structuredtext document is often not entirely fixed, some or-derings are more likely than others.
This informa-tion is lost in the derived databases.
Second, thedatabases may contain information that is not nor-mally present in the underlying text documents, forexample information relating to the storage of anobject in a collection.
Conversely, some of the de-tails present in the texts might be omitted from thedatabase, e.g., the SPECIAL REMARKS field mightbe significantly shortened in the database.
Third,pieces of information are frequently re-written whenentered in the database, in some cases these differ-ences may be systematic, e.g., dates, person names,or registration numbers might be written in a differ-ent format.
Also, field boundaries in the text docu-ments are sometimes indicated by punctuation, suchas commas, and fields sometimes start with explicitkey words, such as collector.
Both of these featuresare missing from the database.Despite of this, these databases will provide cer-tain clues about the structure and content of differ-ent segments in the text documents.
We exploit thisin two different ways: (i) by concatenating databasefields to artificially create annotated training data fora supervised machine learner, and (ii) by using thedatabase to build language models for the field seg-828mentation task.2 Related WorkMost approaches to field segmentation and relatedinformation extraction tasks, such as filling tem-plates with information about specific events, havebeen supervised.
Freitag and Kushmerick (2000)combine a pattern learner with boosting to performfield segmentation in raw texts and in highly struc-tured texts such as web pages and test this approachon a variety of field segmentation and template fill-ing tasks.
Kushmerick et al (2001) address the prob-lem of extracting contact information from busi-ness cards.
They mainly focus on field labelling,bypassing the segmentation step by assuming thateach line on a business card only contains one field(though a field like ADDRESS may span severallines).
Their method combines a text classifier, forassigning likely labels to each field, with a trainedHidden Markov Model (HMM) for learning order-ing constraints between fields.
Borkar et al (2001)identify fields in international postal addresses andbibliographic records by nesting HMMs: an outerHMM for modelling field transitions and a num-ber of inner HMMs for modelling token transitionswithin fields.
Viola and Narasimhand (2005) alsodeal with address segmentation but employ a trainedcontext-free grammar.One of the few unsupervised approaches is pro-vided by Grenager et al (2005), who perform fieldsegmentation on bibliographic records and classifiedadvertisements, using EM to fit an HMM to the data.They show that an unconstrained model does notlearn the field structure very well and propose aug-menting the model with a limited amount of domain-unspecific background knowledge, for example, bymodifying the transition model to bias it towardsrecognising larger-scale patterns.3 Learning Field Segmentation fromDatabases3.1 DataWe tested our approach on two datasets providedby Naturalis, the Dutch National Museum of Nat-ural History2 Each dataset consists of (i) a number2http://www.naturalis.nlof field book entries describing the circumstancesunder which animal specimens were collected, and(ii) a database containing similar information aboutthe same group of animals but in a more structuredform.
The latter were used for training, the formerfor testing.
While the databases were manually cre-ated from the corresponding field books, we madesure that the field book entries we selected for test-ing did not overlap with the database entries.
Thetwo data sets are described below.
Table 1 lists themain properties of the data.Reptiles and Amphibians (RA) This dataset de-scribes a number of reptile and amphibian speci-mens.
The database consists of 16,670 entries and41 columns.
The columns relate, for example, to thecircumstances of a specimen?s collection, its taxo-nomic classification, how and where it is stored, whoentered the entry into the database and when.
Manydatabase cells are empty.
Those that are filled comein a variety of format, i.e., numbers, dates, individ-ual words, and free text of various lengths.
22 ofthe columns contained information that was miss-ing from the field books, e.g., information relatingto the storage of the specimens; these columns wereexcluded from the experiments.From the corresponding field books, 210 entrieswere selected randomly and manually annotatedwith segment information.
To test the reliability ofthe manual annotation, 50 entries were labelled bytwo annotators.
The inter-annotator accuracy on thetoken level was 92.84% and the kappa .92.
The num-ber of distinct field types found in the entries was 19,some of which only occurred in two entries, othersoccurred in virtually every entry.
The average fieldlength was four tokens, with a maximum average of21 for the SPECIAL REMARKS field, and a minimumof one for fields such as SPECIES.
The average num-ber of tokens per entry was 60.
Punctuation marksthat did not clearly belong to any field were labelledas OTHER.
In the experiments, 200 entries were usedfor testing and 10 for parameter tuning.Pisces The second dataset contains informationabout the stations where fish specimens were caught.The database consists of 1,375 entries and fourcolumns which provide information on the locationof the stations.
From the corresponding field books,we manually labelled 100 entries.
Compared to the829RA Pisces# entries in DB 16,670 1,375# fields 19 4entry length (avg.)
60.17 39.79segment length (avg.)
4.08 4.75Table 1: Properties of the two datasetsfirst data set, this set is much more regular, with lessvariation in the number of segments per entry and inthe average segment length.
The field book entriesare also much shorter and there are fewer segments(see Table 1).3.2 BaselinesIn order to get a sense of the difficulty of the task, weimplemented five baseline approaches.
For the first,Majority (MajB), we always assign the field labelthat occurs most frequently in the manually labelledtest data, namely SPECIAL REMARKS.
The otherfour baselines implement different look-up strate-gies, using the database to determine which labelshould be assigned to a token or token sequence.Exact (ExactB) looks for substrings in a fieldbook entry which exactly match the content of adatabase cell and then assigns each token in thematched string the corresponding column label fromthe database.
There are normally several ways tomatch a field book entry to the database cells; weemployed a greedy search, labelling the longestmatching substrings first.
All tokens that couldnot be matched in this way were assigned the labelOTHER.Unigram (UniB) assigns each token the columnlabel of the database cell in which it occurs mostfrequently.
If a token is not found in the database, itis assigned the label OTHER.Trigram (TriB) assigns each token the most fre-quent column label of the trigram centred on it.
Ifa trigram is not found in the database, the baselinebacks off to the two bigrams covering the token andthen to the unigram.
If the token is not found in thedatabase, OTHER is assigned.Trigram+Voting (TriB+Vote) is based on a tech-nique proposed by Van den Bosch and Daelemans(2005) for sequence labelling tasks.
The main ideais to assign labels to trigrams in the sequence usinga sliding window.
Because each token, except theboundary tokens, is contained in three different tri-grams (i.e., the one centred on the token to its left,the one centred on itself, and the one centred on thetoken to its right), each token gets three labels as-signed to it, over which voting can be performed.
Inour case the labels are assigned by database look-up.If a trigram is not found in the database, no label isassigned to it.
If the labels assigned to a given tokendiffer, majority voting is used to resolve the conflict.If this does not break the tie (i.e., because all threetrigrams assign different labels), the label of the tri-gram that occurs most frequently in the database isassigned.
We also implemented two post-processingrules: (i) turning the label OTHER between two iden-tical neighbouring labels into the surrounding labels,and (ii) labelling commas as OTHER if the neigh-bouring labels are not identical.3.3 Supervised Learning from AutomaticallyGenerated Training DataOur first strategy was to automatically generatetraining data for a supervised machine learner fromthe database.
Since the rows in the database corre-spond to field book entries and the columns corre-sponds to the fields that we want to identify, train-ing data can be obtained by concatenating the cellsin each database row.
The order of the fields in thefield book entries is not fixed and this should also bereflected in the artificially generated training data.However, the field sequence is not entirely random,i.e., not all sequences are equally likely.
If a smallamount of manually annotated data is available, thefield transition probabilities can be estimated fromthis, otherwise the best one can do is to assume uni-form probabilities for all possible orderings.
Weexperimented with both strategies, creating two dif-ferent training sets, one in which the database cellswere concatenated randomly with uniform probabil-ities, and another in which the cells were concate-nated to reflect the field ordering probabilities esti-mated from ten entries in the manually labelled de-velopment set.3 When estimating the field transition3We found that 10 annotated entries are enough for this pur-pose; the field segmentation results we obtained by estimatingthe sequence probabilities for the training set from 100 entrieswere not significantly different.
This is probably because theprobabilities are only used indirectly, i.e.
to bias the field order-ings for the generated training data.
If the probabilities wereused directly in the model, the amount of manually annotateddata would probably matter much more.830probabilities, we computed a probability distributionover the initial fields of an entry as well as the condi-tional probability distributions of a field x followinga field y for all seen segment pairs in the ten entries.To account for unobserved events, we used Laplacesmoothing.The artificially created training data were thenconverted to a token-based representation in whicheach token corresponds to an instance to be labelledwith the field to which it belongs.
On the whole, wehad just under 700,000 instances (i.e., tokens) in ourtraining data.
We implemented 107 features, fallingin three classes:?
the neighbouring tokens (in a window of 5 cen-tering on the token in focus)?
the typographic properties of the focus token(word vs. number, capitalisation, number ofcharacters in the token etc.)?
the tfidf weight of the focus token in its con-text with respect to each of the columns in thedatabase (i.e., the fields)The tfidf-based features were computed for a win-dow of three, centering on the token in focus.
For alln-grams in this window covering the token in focus(i.e., the trigram, the two bigrams, and the unigramof the focus token), we calculated the tfidf simi-larity with the columns in the database, where thesimilarity between an n-gram ti and a column colxis defined as:tfidfti,colx = tfti,colx log idftiThe term frequency, tfti,colx is the number of oc-currences of ti in colx divided by the number of oc-currences of all n-grams of length n in colx (0 ifthe n-gram does not occur in the column).
The in-verse document frequency, idfti , is the number ofall columns in the database divided by the numberof columns containing ti.
A high tfidf weight for agiven n-gram in a given column means that it fre-quently occurs in that column but rarely in othercolumns, thus it is a good indicator for that column.The training data was then used to train amemory-based machine learner (TiMBL (Daele-mans et al, 2004), default settings, k = 3, numericfeatures declared) to determine which field each to-ken belongs to.44We chose TiMBL because it has been applied successfully3.4 Hidden Markov ModelsOur second approach combines language modellingand Hidden Markov Models (HMMs) (Rabiner,1989).
Hidden Markov Models have been in use forinformation extraction tasks for a long time.
A prob-abilistic model is trained to assign a label, or stateto each of a sequence of observations, where bothlabels and observations are expected to be sequen-tially correlated; hence the popularity of HMMs innatural language processing and information extrac-tion.
Recently, a large number of more sophisticatedlearning techniques have largely replaced HMMsfor information extraction; however unlike most ofthose newer techniques, HMMs offer the advantageof having a well-established unsupervised trainingprocedure: the Baum-Welch algorithm (Baum et al,1970).Training a Hidden Markov Model, whether su-pervised or unsupervised, comes down to estimatingthree probability distributions.1.
An initial state distribution pi, which modelsthe probability of the first observation of a se-quence to have a certain label.2.
A state-transition distribution A, modelling theconditional probability of being in a certainstate s, given that the previous state was s?.3.
A state-emission distribution B, which modelsthe conditional probability of observing a cer-tain object o given some state s.For information extraction tasks, the typical in-terpretation of an observation as referred to above,is that of a token, where the entire observation se-quence commonly corresponds to one sentence.
Inthe current study, we chose to apply HMMs on asomewhat higher level, where an observation corre-sponds to a segment of the field book entry.
Ideally,one such segment maps one-to-one to a cell in thespecimen database, though we leave open the possi-bility of merging several segments into one databasecell.Provided that a field book entry can be segmentedreliably, we have turned one part of the learn-ing problem, that of estimating the state-emissionto sequence labelling tasks (Van den Bosch and Canisius, 2006;Van den Bosch and Daelemans, 2005).831distribution, into one for which we have (almost)perfect supervised training data: the contents ofthe database cells.
The general form of a Hid-den Markov Model?s state-emission distribution isP (o|s), where s is the state, i.e.
a field type in ourcase, and o is the observation.
As mentioned be-fore, we treat a segment of tokens as one observa-tion, therefore our state-emission distribution willlook like P (o = t1, t2, ..., tn|s).
Essentially, whatwe have here is a language model, conditioned onthe current state.
Since the specimen database pro-vides a large amount of labelled segment sequences,any probabilistic language modelling method can beused to estimate the state-emission distribution.Whereas the specimen database provides suffi-cient information to estimate the state-emission dis-tribution in a fully supervised way, the initial-stateand state-transition distributions cannot be derivedfrom the database alone.
Columns in a databaseare either unordered or ordered in a way that doesnot necessarily reflect the order they had in the fieldbook entries they were extracted from.
However, theoriginal field book entries do show a rather system-atic structure.
Often, using information about theorder fields typically occur in, seems to be the onlyway to distinguish certain field types from one an-other.
To estimate the two missing probability dis-tributions, the Baum-Welch algorithm was used, up-dating the initial-state and state-transition distribu-tions, while keeping the state-emission distributionsunchanged.3.4.1 Segmentation of Field Book EntriesIn our setup, the Hidden Markov Model expectsthe input texts to be pre-segmented.
To come upwith a good initial segmentation of an input entry,we again chose a language-modelling approach.
It isexpected that segment boundaries can best be recog-nised by looking for unusual token subsequences;that is, token sequences that are highly unlikely tooccur within a field according to the information weobtained from the specimen database about what atypical segment does look like.
A bigram languagemodel has been trained on the contents of all thecolumns of the specimen database.
Using this lan-guage model and the Viterbi algorithm, the globallymost-likely segmentation of the input text is pre-dicted.3.4.2 The State-emission ModelThe state-emission model is constructed by train-ing a separate bigram language model for each col-umn of the specimen database.
Combining thosegives us the conditional distribution required for aHidden Markov Model.
However, in a database, notevery column has necessarily been filled for everyrecord.
For example, in the Reptiles and Amphib-ians database, there are columns that only containactual data as infrequently as in 5% of the records.Relative to columns that contain data more often,these sparsely-filled columns tend to be overesti-mated when simply computing a likelihood accord-ing to the language model.
For this reason, a penaltyterm is added to the state-emission distribution cor-responding to the probability that a record containsdata for the given column.
The likelihood com-puted by the language model and the correspondingpenalty term are then simply multiplied.3.4.3 Language ModellingFor building both types of language model pre-sented in the two previous sections, we used n-gram language modelling as implemented by theSRI Language Modelling Toolkit (Stolcke, 2002).With this toolkit, high-order n-gram models can bebuilt, where the sparsity problem often encounteredwith such models is tackled by various smoothingmethods.
We supplemented this built-in n-gramsmoothing, with our own smoothing on the tokenlevel by replacing low-frequent words with symbolsreflecting certain orthographic features of the origi-nal word, and numbers with a symbol only encodingthe number of digits in the original number.In addition to these general measures to dealwith sparsity, we also applied a small number ofknowledge-driven modifications to the training datafor the language models.
The need for those iscaused by the fact that the contents of the specimendatabase are almost, but not entirely extracted liter-ally from the original field book entries.
For exam-ple, for the second entry of Example 1, the followinginformation is stored in the database.Genus HylaSpecies minutaGender 1 f + 2 mPlace Las Claritas832Collection date 9-6-1978Biotope quaking near water 50 cm above water sur-face, near secondary vegetationHeight 200 mCollector M.S.
HoogmoedRegistration number 27217 27219Comparing just this single field book entry withits corresponding database record, one can alreadysee several mismatches.
The gender symbols ?
and?
in the field book texts are stored as m and f inthe database.
The collection date 9-VI-1978, hasbeen converted to 9-6-1978 before adding it to thedatabase, i.e.
the Roman numeral for the monthnumber has been mapped to the corresponding Ara-bic numeral.
As a final example, in the field book en-try the registration number for the specimen is pre-ceded by the symbol RMNH; in the database this stan-dard symbol is stripped of and only the number isstored.
Each of these differences, while only small,will hinder the performance of a language modeltrained on the contents of the database and appliedto field book texts.As a simple illustration of this, when encounter-ing the symbol RMNH in a field book entry, this mostlikely indicates the start of a new (registration num-ber) segment.
However, in the database, on whichall language models are trained, RMNH never occursas a symbol in the registration number column; itdoes occur a few times in the column for special re-marks but never at the start of the text.
As a result,a segmentation model trained on the contents of thedatabase, on encountering the symbol RMNH will al-ways opt for continuing the existing segment as op-posed to starting a new one, which is most likely thebetter choice.Fortunately, many such mismatches between thetext in field books and the database are systematicand can easily be covered by a small number of man-ually constructed rules that modify the training datafor the language models.
Among others, we addedthe RMNH symbol in front of registration numbers,and randomly changed some month numbers fromArabic numerals to Roman numerals.Another difference between the field books andthe database that turned out to be rather crucial isthe fact that many segments in the field book en-tries are separated by commas.
Such commas usedToken SegmentAcc.
Prec.
Rec.
F?=1 WDiffMajB 24.8 0.0 0.0 0.0 .346ExactB 16.0 25.7 23.1 24.3 .425UniB 27.0 8.9 22.8 12.8 .818TriB 43.8 12.9 24.8 16.9 .582TriB+Vote 45.1 14.9 27.8 19.4 .536MBL rand.
44.6 7.1 19.2 10.4 .568MBL bias 53.4 12.1 32.0 17.6 .533HMM 56.9 62.7 58.1 60.3 .177Table 2: Performance of all baseline and learningapproaches on the Reptiles and Amphibians data,expressed in token accuracy, precision, recall, F-score, and WindowDiff.
For WindowDiff, lowerscores are better.as delimiters between fields do not appear in thedatabase, where fields correspond to columns andboundaries between fields consequently do not haveto be explicitly marked by punctuation.
For exam-ple, the comma between Las Claritas and 9-VI-1978only serves to separate the Place segment from theCollection date segment; the comma is not copiedto the database.
However, commas do occur field-internally in the database, especially in longer fieldssuch as SPECIAL REMARKS.
Hence a languagemodel trained on the database in its original formwill never have encountered a comma functioning asa segment boundary marker and thus will not recog-nise that commas may be used for this purpose in thefield book entries.
To deal with this, we modified thetraining data for the segment model by randomly in-serting commas at the end of some segments.
Exper-imental results point out that this modification has alarge impact on the performance of the segmentationmodel.3.5 Results and DiscussionTo evaluate the performance of the two approaches,we applied them to the Reptiles and Amphibiansdatabase.
First we computed baseline scores usingthe approaches described in Section 3.2.
All result-ing scores are listed in Table 2.Performance of the systems was measured us-ing a number of different metrics, each reflectingdifferent qualities of the output.
The most basicone, token accuracy, simply measures the percent-age of tokens that were assigned the correct field833type.
It has the disadvantage that it does not reflectthe quality of the segments that were found.
For amore segment-oriented evaluation, we used preci-sion, recall and F-score on correctly identified andlabelled segments.
As a last measure for segmen-tation quality we used WindowDiff (Pevzner andHearst, 2002), which only evaluates segment bound-aries not the labels assigned to them.
In comparisonwith F-score, it is more forgiving with respect to seg-ment boundaries that are slightly off.The baseline performance scores support our as-sumption that the contents of the database can beused to learn how to segment and label field bookentries, i.e.
the increasingly more sophisticateddatabase matching strategies each cause a substan-tial performance improvement up to 45.1 token ac-curacy for the trigram lookup with voting strategy.The biggest problem of all baseline approaches isthat their performance with respect to the segment-oriented measures is disappointing.
Even trigramlookup with voting only reaches an F-score of 19.4.Looking at the performance of the two memory-based learners in Table 2 (MBL rand.
was trained onrandomly concatenated training data, MBL bias ondata modelled after 10 training sequences), we seethat the small amount of prior knowledge used forgenerating the artificial training data results in a sub-stantial improvement compared with the memory-based learner that was trained on randomly concate-nated training data with uniform probabilities.As can be seen in the last row of Table 2, the Hid-den Markov Model outperforms all other approachesin all aspects; it attains both the best token accuracy(56.9), and by far the best F-score (60.3).
The mostprobable explanation for the superior performanceof the HMM-based approach is that this approachmodels sequential constraints between different seg-ments, whereas the baselines and the memory-basedmodels are predominantly local.In Table 3, we consider the effect that theknowledge-based rewriting rules discussed in Sub-section 3.4.3 have on the performance of both thesegmentation and the labelling step.
We evaluateboth (i) the performance of the two processing stepsseparately?for labelling this presupposes perfectlysegmented input?
and (ii) the performance of thecascade of segmentation and labelling.
As before,the performance of the labelling and the cascade isexpressed in F-score on segments.
Performance ofthe segmentation is measured in F-score on insertedsegment boundaries.The first row of the table shows the scores if nomodification rules are used.
This proves detrimen-tal for the segmentation, only attaining an F-scoreof 28.4.
With 62.3, the F-score for labelling is rea-sonable; however, the weakness of the segmentationcauses the output of the entire cascade to be use-less.
Modifying the training data for the segmen-tation model by randomly inserting a comma at theend of segments gives a substantial improvement insegmentation performance, and as a result the qual-ity of the cascade improves with it, as can be seen inthe second row.
All remaining rows list the scoresof a single modification rule applied to the trainingdata in addition to the comma rule.
Each of the rulesgives a slight performance increase.
Using all rulestogether makes a big difference: the F-score of thecascade increases from 44.7 with the comma ruleonly, to 60.3.Rule Boundaries Labels CascadeNone 28.4 62.3 17.2Comma 69.7 62.3 44.7Comma+Collection Date 69.7 64.4 46.8Comma+Reg.
Number 72.9 68.6 50.9Comma+Gender 71.5 65.0 49.5Comma+Collector 70.4 65.8 45.3All 72.0 78.3 60.3Table 3: The effect of systematically modifying thetraining data for both the segmentation and labellingmodels.
The comma rule is used only in the train-ing of the segmentation model.
The other rules arenamed after the database field they are applied to.The scores reflect their performance when appliedin conjunction with the comma rule.To confirm that the HMM-based approach carriesover to other datasets, we also tested it on the Piscesdata.
The results of this experiment, as well as allbaseline scores are presented in Table 4.5 The factthat this data set is more regular and contains fewersegments is reflected by the relatively high token ac-curacies attained by the baseline approaches.
With5We did not test the memory-based approaches as these ledto significantly worse results than the HMM-based model on theReptiles and Amphibians data set.834the simple database lookup strategies, however, al-most no entire segments are predicted correctly.
At-taining a token accuracy of 94.4 and an F-score of86.9, the performance of the Hidden Markov Modelis again more than satisfactory, confirming the re-sults observed with the Reptiles and Amphibiansdata.Token SegmentAcc.
Prec.
Rec.
F?=1 WDiffMajB 50.0 0.0 0.0 0.0 .279ExactB 9.3 0.0 0.0 0.0 .565UniB 53.7 1.5 3.6 2.1 .588TriB 68.6 0.2 0.2 0.2 .359TriB+Vote 67.1 2.2 2.8 2.4 .384HMM 94.4 87.6 86.3 86.9 .049Table 4: Performance of Hidden Markov Model andall baseline approaches on the Pisces data, expressedin token accuracy, precision, recall, F-score, andWindowDiff.
For WindowDiff, lower scores are bet-ter.4 ConclusionInformation extraction is often used to automate theprocess of filling a structured database with contentextracted from written texts.
Supervised machinelearning approaches have been successfully appliedfor creating systems capable of performing this task.However, the supervised nature of these approachesrequires large amounts of annotated training data;the acquisition of which is often a laborious andtime-consuming process.
In this study, we experi-mented with two machine learning techniques thatdo not require such annotated training data, but canbe trained on a database containing information de-rived from the type of documents targeted by the ap-plication.The first approach is an attempt to employ a stan-dard supervised machine learning algorithm, train-ing it on artificial labelled training data.
These dataare created by concatenating the contents of the cellsof the database records in random order.
Experi-ments with this approach pointed out that truly ran-dom concatenation of database fields results in weakperformance; a rather simple baseline approach,which only matches substrings of a field book en-try with the contents of the database, leads to betterresults.
However, if a small amount of annotatedfield book entries is available ?in this study, 10 en-tries turned out to be sufficient?
one can estimatefield ordering probabilities that can be used to gener-ate more realistic training data from the database.
Amachine learner trained on these data labelled 10%more tokens correctly than the system trained on therandomly generated data.Our second approach is based on unsupervisedHidden Markov modelling.
First, an n-gram lan-guage model is used to divide the field book en-tries into unlabelled segments.
Then, a HiddenMarkov Model is trained on these segmented entriesusing the Baum-Welch algorithm to estimate state-transition probabilities.
The resulting HMM labelsthe segments found in the preceding segmentationstep.
The HMM state-emission distributions are es-timated by training n-gram language models on thecontents of the database columns.The performance of the HMM proved to be supe-rior to the other approaches, outperforming the su-pervised learner by labelling 56.9% of the tokenscorrectly, as well as attaining good results in termsof segment-level F-score (60.3).
Experiments withthe HMM approach on a second, independent dataset confirmed its generality.AcknowledgementsThe research reported in this paper was fundedby the Netherlands Organisation for Scientific Re-search (NWO) as part of the IMIX and CATCH pro-grammes.
We are grateful to Antal van den Bosch,Marieke van Erp, Steve Hunt, and the staff at Natu-ralis, the Dutch National Museum for Natural His-tory, for interesting discussions and help in prepar-ing the data.ReferencesLeonard E. Baum, Ted Petrie, George Soules, and Nor-man Weiss.
1970.
A Maximization Technique Occur-ring in the Statistical Analysis of Probabilistic Func-tions of Markov Chains.
The Annals of MathematicalStatistics, 41(1):164?171.Vinayak Borkar, Kaustubh Deshmukh, and SunitaSarawagi.
2001.
Automatic segmentation of text intostructured records.
In Proceedings of the 2001 ACMSIGMOD International Conference on Management ofData, pages 175?186.835Walter Daelemans, Jakub Zavrel, Ko Van der Sloot, andAntal Van den Bosch, 2004.
TiMBL: Tilburg MemoryBased Learner, version 5.1, Reference Guide.
ILK Re-search Group Technical Report Series no.
04-02.Dayne Freitag and Nicholas Kushmerick.
2000.Boosted wrapper induction.
In Proceedings of the17th National Conference on Artificial Intelligence(AAAI/IAAI-2000), pages 577?583.Trond Grenager, Dan Klein, and Christopher D. Man-ning.
2005.
Unsupervised learning of field segmen-tation models for information extraction.
In Proceed-ings of the 43nd Annual Meeting of the Association forComputational Linguistics (ACL 2005), pages 371?378.Nicholas Kushmerick, Edward Johnston, and StephenMcGuinness.
2001.
Information extraction by textclassification.
In Proceedings of the IJCAI-01 Work-shop on Adaptive Text Extraction and Mining.Lev Pevzner and Marti A. Hearst.
2002.
A critique andimprovement of an evaluation metric for text segmen-tation.
Computational Linguistics, 28(1):19?36.Lawrence R. Rabiner.
1989.
A tutorial on HiddenMarkov Models and selected applications in speechrecognition.
Proceedings of the IEEE, 77(2):257?286.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
Proceedings of the InternationalConference on Spoken Language Processing (ICSLP2002), 2:901?904.Antal Van den Bosch and Sander Canisius.
2006.Improved morpho-phonological sequence processingwith constraint satisfaction inference.
In Proceed-ings of the Eighth Meeting of the ACL Special InterestGroup in Computational Phonology (SIGPHON ?06).Antal Van den Bosch and Walter Daelemans.
2005.
Im-proving sequence segmentation learning by predictingtrigrams.
In Proceedings of the Ninth Conference onNatural Language Learning, CoNLL-2005, pages 80?87.Paul Viola and Mukund Narasimhand.
2005.
Learningto extract information from semi-structured text usinga discriminative context free grammar.
In Proceedingsof the 28th Annual International ACM SIGIR Confer-ence on Research and Development in Information Re-trieval, pages 330?337.836
