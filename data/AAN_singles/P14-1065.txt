Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687?698,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsUsing Discourse Structure Improves Machine Translation EvaluationFrancisco Guzm?an Shafiq Joty Llu?
?s M`arquez and Preslav NakovALT Research GroupQatar Computing Research Institute ?
Qatar Foundation{fguzman,sjoty,lmarquez,pnakov}@qf.org.qaAbstractWe present experiments in using dis-course structure for improving machinetranslation evaluation.
We first designtwo discourse-aware similarity measures,which use all-subtree kernels to comparediscourse parse trees in accordance withthe Rhetorical Structure Theory.
Then,we show that these measures can helpimprove a number of existing machinetranslation evaluation metrics both at thesegment- and at the system-level.
Ratherthan proposing a single new metric, weshow that discourse information is com-plementary to the state-of-the-art evalu-ation metrics, and thus should be takeninto account in the development of futurericher evaluation metrics.1 IntroductionFrom its foundations, Statistical Machine Transla-tion (SMT) had two defining characteristics: first,translation was modeled as a generative process atthe sentence-level.
Second, it was purely statisti-cal over words or word sequences and made lit-tle to no use of linguistic information.
Althoughmodern SMT systems have switched to a discrim-inative log-linear framework, which allows for ad-ditional sources as features, it is generally hard toincorporate dependencies beyond a small windowof adjacent words, thus making it difficult to uselinguistically-rich models.Recently, there have been two promising re-search directions for improving SMT and its eval-uation: (a) by using more structured linguisticinformation, such as syntax (Galley et al, 2004;Quirk et al, 2005), hierarchical structures (Chi-ang, 2005), and semantic roles (Wu and Fung,2009; Lo et al, 2012), and (b) by going beyondthe sentence-level, e.g., translating at the docu-ment level (Hardmeier et al, 2012).Going beyond the sentence-level is importantsince sentences rarely stand on their own in awell-written text.
Rather, each sentence followssmoothly from the ones before it, and leads intothe ones that come afterwards.
The logical rela-tionship between sentences carries important in-formation that allows the text to express a meaningas a whole beyond the sum of its separate parts.Note that sentences can be made of severalclauses, which in turn can be interrelated throughthe same logical relations.
Thus, in a coherent text,discourse units (sentences or clauses) are logicallyconnected: the meaning of a unit relates to that ofthe previous and the following units.Discourse analysis seeks to uncover this coher-ence structure underneath the text.
Several formaltheories of discourse have been proposed to de-scribe the coherence structure (Mann and Thomp-son, 1988; Asher and Lascarides, 2003; Webber,2004).
For example, the Rhetorical Structure The-ory (Mann and Thompson, 1988), or RST, repre-sents text by labeled hierarchical structures calledDiscourse Trees (DTs), which can incorporate sev-eral layers of other linguistic information, e.g.,syntax, predicate-argument structure, etc.Modeling discourse brings together the aboveresearch directions (a) and (b), which makes it anattractive goal for MT.
This is demonstrated by theestablishment of a recent workshop dedicated toDiscourse in Machine Translation (Webber et al,2013), collocated with the 2013 annual meeting ofthe Association of Computational Linguistics.The area of discourse analysis for SMT is stillnascent and, to the best of our knowledge, noprevious research has attempted to use rhetoricalstructure for SMT or machine translation evalua-tion.
One possible reason could be the unavailabil-ity of accurate discourse parsers.
However, thissituation is likely to change given the most recentadvances in automatic discourse analysis (Joty etal., 2012; Joty et al, 2013).687We believe that the semantic and pragmatic in-formation captured in the form of DTs (i) can helpdevelop discourse-aware SMT systems that pro-duce coherent translations, and (ii) can yield bet-ter MT evaluation metrics.
While in this work wefocus on the latter, we think that the former is alsowithin reach, and that SMT systems would bene-fit from preserving the coherence relations in thesource language when generating target-languagetranslations.In this paper, rather than proposing yet anotherMT evaluation metric, we show that discourseinformation is complementary to many existingevaluation metrics, and thus should not be ignored.We first design two discourse-aware similaritymeasures, which use DTs generated by a publicly-available discourse parser (Joty et al, 2012); then,we show that they can help improve a number ofMT evaluation metrics at the segment- and at thesystem-level in the context of the WMT11 and theWMT12 metrics shared tasks (Callison-Burch etal., 2011; Callison-Burch et al, 2012).These metrics tasks are based on sentence-levelevaluation, which arguably can limit the benefitsof using global discourse properties.
Fortunately,several sentences are long and complex enough topresent rich discourse structures connecting theirbasic clauses.
Thus, although limited, this settingis able to demonstrate the potential of discourse-level information for MT evaluation.
Furthermore,sentence-level scoring (i) is compatible with mosttranslation systems, which work on a sentence-by-sentence basis, (ii) could be beneficial to mod-ern MT tuning mechanisms such as PRO (Hop-kins and May, 2011) and MIRA (Watanabe et al,2007; Chiang et al, 2008), which also work atthe sentence-level, and (iii) could be used for re-ranking n-best lists of translation hypotheses.2 Related WorkAddressing discourse-level phenomena in ma-chine translation is relatively new as a research di-rection.
Some recent work has looked at anaphoraresolution (Hardmeier and Federico, 2010) anddiscourse connectives (Cartoni et al, 2011; Meyer,2011), to mention two examples.1However, sofar the attempts to incorporate discourse-relatedknowledge in MT have been only moderately suc-cessful, at best.1We refer the reader to (Hardmeier, 2012) for an in-depthoverview of discourse-related research for MT.A common argument, is that current automaticevaluation metrics such as BLEU are inadequateto capture discourse-related aspects of translationquality (Hardmeier and Federico, 2010; Meyer etal., 2012).
Thus, there is consensus that discourse-informed MT evaluation metrics are needed in or-der to advance research in this direction.
Here wesuggest some simple ways to create such metrics,and we also show that they yield better correlationwith human judgments.The field of automatic evaluation metrics forMT is very active, and new metrics are contin-uously being proposed, especially in the contextof the evaluation campaigns that run as part ofthe Workshops on Statistical Machine Transla-tion (WMT 2008-2012), and NIST Metrics forMachine Translation Challenge (MetricsMATR),among others.
For example, at WMT12, 12 met-rics were compared (Callison-Burch et al, 2012),most of them new.There have been several attempts to incorpo-rate syntactic and semantic linguistic knowledgeinto MT evaluation.
For instance, at the syn-tactic level, we find metrics that measure thestructural similarity between shallow syntactic se-quences (Gim?enez and M`arquez, 2007; Popovicand Ney, 2007) or between constituency trees (Liuand Gildea, 2005).
In the semantic case, there aremetrics that exploit the similarity over named en-tities and predicate-argument structures (Gim?enezand M`arquez, 2007; Lo et al, 2012).In this work, instead of proposing a new metric,we focus on enriching current MT evaluation met-rics with discourse information.
Our experimentsshow that many existing metrics can benefit fromadditional knowledge about discourse structure.In comparison to the syntactic and semantic ex-tensions of MT metrics, there have been very fewattempts to incorporate discourse information sofar.
One example are the semantics-aware metricsof Gim?enez and M`arquez (2009) and Comelles etal.
(2010), which use the Discourse Representa-tion Theory (Kamp and Reyle, 1993) and tree-based discourse representation structures (DRS)produced by a semantic parser.
They calculate thesimilarity between the MT output and referencesbased on DRS subtree matching, as defined in (Liuand Gildea, 2005), DRS lexical overlap, and DRSmorpho-syntactic overlap.
However, they couldnot improve correlation with human judgments, asevaluated on the MetricsMATR dataset.688Compared to the previous work, (i) we use adifferent discourse representation (RST), (ii) wecompare discourse parses using all-subtree ker-nels (Collins and Duffy, 2001), (iii) we evaluateon much larger datasets, for several language pairsand for multiple metrics, and (iv) we do demon-strate better correlation with human judgments.Wong and Kit (2012) recently proposed anextension of MT metrics with a measure ofdocument-level lexical cohesion (Halliday andHasan, 1976).
Lexical cohesion is achieved usingword repetitions and semantically similar wordssuch as synonyms, hypernyms, and hyponyms.For BLEU and TER, they observed improvedcorrelation with human judgments on the MTC4dataset when linearly interpolating these metricswith their lexical cohesion score.
Unlike theirwork, which measures lexical cohesion at thedocument-level, here we are concerned with co-herence (rhetorical) structure, primarily at thesentence-level.3 Our Discourse-Based MeasuresOur working hypothesis is that the similarity be-tween the discourse structures of an automatic andof a reference translation provides additional in-formation that can be valuable for evaluating MTsystems.
In particular, we believe that good trans-lations should tend to preserve discourse relations.As an example, consider the three discoursetrees (DTs) shown in Figure 1: (a) for a reference(human) translation, and (b) and (c) for transla-tions of two different systems on the WMT12 testdataset.
The leaves of a DT correspond to con-tiguous atomic text spans, called Elementary Dis-course Units or EDUs (three in Figure 1a).
Ad-jacent spans are connected by certain coherencerelations (e.g., Elaboration, Attribution), forminglarger discourse units, which in turn are also sub-ject to this relation linking.
Discourse units linkedby a relation are further distinguished based ontheir relative importance in the text: nuclei arethe core parts of the relation while satellites aresupportive ones.
Note that the nuclearity and re-lation labels in the reference translation are alsorealized in the system translation in (b), but notin (c), which makes (b) a better translation com-pared to (c), according to our hypothesis.
We ar-gue that existing metrics that only use lexical andsyntactic information cannot distinguish well be-tween (b) and (c).In order to develop a discourse-aware evalua-tion metric, we first generate discourse trees forthe reference and the system-translated sentencesusing a discourse parser, and then we measure thesimilarity between the two discourse trees.
We de-scribe these two steps below.3.1 Generating Discourse TreesIn Rhetorical Structure Theory, discourse analysisinvolves two subtasks: (i) discourse segmentation,or breaking the text into a sequence of EDUs, and(ii) discourse parsing, or the task of linking theunits (EDUs and larger discourse units) into la-beled discourse trees.
Recently, Joty et al (2012)proposed discriminative models for both discoursesegmentation and discourse parsing at the sen-tence level.
The segmenter uses a maximum en-tropy model that achieves state-of-the-art accuracyon this task, having an F1-score of 90.5%, whilehuman agreement is 98.3%.The discourse parser uses a dynamic Condi-tional Random Field (Sutton et al, 2007) as a pars-ing model in order to infer the probability of allpossible discourse tree constituents.
The inferred(posterior) probabilities are then used in a proba-bilistic CKY-like bottom-up parsing algorithm tofind the most likely DT.
Using the standard setof 18 coarse-grained relations defined in (Carlsonand Marcu, 2001), the parser achieved an F1-scoreof 79.8%, which is very close to the human agree-ment of 83%.
These high scores allowed us to de-velop successful discourse similarity metrics.23.2 Measuring SimilarityA number of metrics have been proposed to mea-sure the similarity between two labeled trees, e.g.,Tree Edit Distance (Tai, 1979) and Tree Kernels(Collins and Duffy, 2001; Moschitti and Basili,2006).
Tree kernels (TKs) provide an effectiveway to integrate arbitrary tree structures in kernel-based machine learning algorithms like SVMs.In the present work, we use the convolution TKdefined in (Collins and Duffy, 2001), which effi-ciently calculates the number of common subtreesin two trees.
Note that this kernel was originallydesigned for syntactic parsing, where the subtreesare subject to the constraint that their nodes aretaken with either all or none of the children.
Thisconstraint of the TK imposes some limitations onthe type of substructures that can be compared.2The discourse parser is freely available fromhttp://alt.qcri.org/tools/689ElaborationROOTSPAN NucleusAttributionSatelliteVoices are coming from Germany , SPANSatellite SPANNucleussuggesting that ECB be the last resort creditor .
(a) A reference (human) translation. 			  	 !"(b) A higher quality system translation.SPANROOTIn Germany the ECB should be for the creditors of last resort .
(c) A lower quality system translation.Figure 1: Example of three different discourse trees for the translations of a source sentence.
(a) Thereference, (b) A higher quality translation, (c) A lower quality translation.One way to cope with the limitations of the TKis to change the representation of the trees to aform that is suitable to capture the relevant infor-mation for our task.
We experiment with TKs ap-plied to two different representations of the dis-course tree: non-lexicalized (DR), and lexicalized(DR-LEX).
In Figure 2 we show the two represen-tations for the subtree that spans the text: ?sug-gest the ECB should be the lender of last resort?,which is highlighted in Figure 1b.As shown in Figure 2a, DR does not include anylexical item, and therefore measures the similar-ity between two translations in terms of their dis-course structures only.
On the contrary, DR-LEXincludes the lexical items to account for lexicalmatching; moreover, it separates the structure (theskeleton) of the tree from its labels, i.e.
the nucle-arity and the relations, in order to allow the treekernel to give partial credit to subtrees that differin labels but match in their skeletons.
More specif-ically, it uses the tags SPAN and EDU to build theskeleton of the tree, and considers the nuclearityand/or the relation labels as properties, added aschildren, of these tags.For example, a SPAN has two properties (itsnuclearity and its relation), and an EDU has oneproperty (its nuclearity).
The words of an EDUare placed under the predefined children NGRAM.In order to allow the tree kernel to find subtreematches at the word level, we include an additionallayer of dummy leaves as was done in (Moschittiet al, 2007); not shown in Figure 2, for simplicity.4 Experimental SetupIn our experiments, we used the data available forthe WMT12 and the WMT11 metrics shared tasksfor translations into English.3This included theoutput from the systems that participated in theWMT12 and the WMT11 MT evaluation cam-paigns, both consisting of 3,003 sentences, forfour different language pairs: Czech-English (CS-EN), French-English (FR-EN), German-English(DE-EN), and Spanish-English (ES-EN); as well asa dataset with the English references.We measured the correlation of the metrics withthe human judgments provided by the organizers.The judgments represent rankings of the outputof five systems chosen at random, for a particu-lar sentence, also chosen at random.
Note thateach judgment effectively constitutes 10 pairwisesystem rankings.
The overall coverage, i.e.
thenumber of unique sentences that were evaluated,was only a fraction of the total; the total numberof judgments, along with other information of thedatasets are shown in Table 1.4.1 MT Evaluation MetricsIn this study, we evaluate to what extent existingevaluation metrics can benefit from additional dis-course information.
To do so, we contrast differentMT evaluation metrics with and without discourseinformation.
The evaluation metrics we used aredescribed below.3http://www.statmt.org/wmt{11,12}/results.html690AttributionSPANSatellite SPANNucleusNucleus(a) DT for DRSPANEDU EDUNUC NGRAM NUC NGRAMSatellite suggest Nucleus the ECB should be lender of the last resort .NUCNucleusRELAttribution(b) DT for DR-LEXFigure 2: Two different DT representations for the highlighted subtree shown in Figure 1b.WMT12 WMT11systs ranks sents judges systs ranks sents judgesCS-EN 6 1,294 951 45 8 498 171 20DE-EN 16 1,427 975 47 20 924 303 31ES-EN 12 1,141 923 45 15 570 207 18FR-EN 15 1,395 949 44 18 708 249 32Table 1: Number of systems (systs), judgments(ranks), unique sentences (sents), and differentjudges (judges) for the different language pairs, forthe human evaluation of the WMT12 and WMT11shared tasks.Metrics from WMT12.
We used the publiclyavailable scores for all metrics that participatedin the WMT12 metrics task (Callison-Burch etal., 2012): SPEDE07PP, AMBER, METEOR,TERRORCAT, SIMPBLEU, XENERRCATS,WORDBLOCKEC, BLOCKERRCATS, and POSF.Metrics from ASIYA.
We used the freely avail-able version of the ASIYA toolkit4in order to ex-tend the set of evaluation measures contrasted inthis study beyond those from the WMT12 metricstask.
ASIYA (Gim?enez and M`arquez, 2010a) is asuite for MT evaluation that provides a large set ofmetrics that use different levels of linguistic infor-mation.
For reproducibility, below we explain theindividual metrics with the exact names requiredby the toolkit to calculate them.First, we used ASIYA?s ULC (Gim?enez andM`arquez, 2010b), which was the best performingmetric at the system and the segment levels at theWMT08 and WMT09 metrics tasks.
This is a uni-form linear combination of 12 individual metrics.From the original ULC, we only replaced TER andMeteor individual metrics by newer versions tak-ing into account synonymy lookup and paraphras-ing: TERp-A and METEOR-pa in ASIYA?s termi-nology.
We will call this combined metric Asiya-0809 in our experiments.4http://nlp.lsi.upc.edu/asiya/To complement the set of individual metricsthat participated at the WMT12 metrics task, wealso computed the scores of other commonly-used evaluation metrics: BLEU (Papineni et al,2002), NIST (Doddington, 2002), TER (Snoveret al, 2006), ROUGE-W (Lin, 2004), and threeMETEOR variants (Denkowski and Lavie, 2011):METEOR-ex (exact match), METEOR-st (+stem-ming) and METEOR-sy (+synonyms).
The uni-form linear combination of the previous 7 indi-vidual metrics plus the 12 from Asiya-0809 is re-ported as Asiya-ALL in the experimental section.The individual metrics combined in Asiya-ALLcan be naturally categorized according to the typeof linguistic information they use to compute thequality scores.
We grouped them in the follow-ing four families and calculated the uniform linearcombination of the metrics in each group:51.
Asiya-LEX.
Combination of five metricsbased on lexical similarity: BLEU, NIST,METEOR-ex, ROUGE-W, and TERp-A.2.
Asiya-SYN.
Combination of four met-rics ba-sed on syntactic information fromconstituency and dependency parse trees:?CP-STM-4?, ?DP-HWCM c-4?, ?DP-HWCM r-4?, and ?DP-Or(*)?.3.
Asiya-SRL.
Combination of three metricvariants based on predicate argument struc-tures (semantic role labeling): ?SR-Mr(*)?,?SR-Or(*)?, and ?SR-Or?.4.
Asiya-SEM.
Combination of two metricsvariants based on semantic parsing:6?DR-Or(*)?
and ?DR-Orp(*)?.5A detailed description of every individual metric can befound at (Gim?enez and M`arquez, 2010b).
For a more up-to-date description, see the User Manual from ASIYA?s website.6In ASIYA the metrics from this family are referred to as?Discourse Representation?
metrics.
However, the structuresthey consider are actually very different from the discoursestructures exploited in this paper.
See the discussion in Sec-tion 2.
For clarity, we will refer to them as semantic parsingmetrics.691All uniform linear combinations are calculatedoutside ASIYA.
In order to make the scores ofthe different metrics comparable, we performed amin?max normalization, for each metric, and foreach language pair combination.4.2 Human Judgements and LearningThe human-annotated data from the WMT cam-paigns encompasses series of rankings on the out-put of different MT systems for every source sen-tence.
Annotators rank the output of five systemsaccording to perceived translation quality.
The or-ganizers relied on a random selection of systems,and a large number of comparisons between pairsof them, to make comparisons across systems fea-sible (Callison-Burch et al, 2012).
As a result,for each source sentence, only relative rankingswere available.
As in the WMT12 experimen-tal setup, we use these rankings to calculate cor-relation with human judgments at the sentence-level, i.e.
Kendall?s Tau; see (Callison-Burch etal., 2012) for details.For the experiments reported in Section 5.4, weused pairwise rankings to discriminatively learnthe weights of the linear combinations of indi-vidual metrics.
In order to use the WMT12 datafor training a learning-to-rank model, we trans-formed the five-way relative rankings into tenpairwise comparisons.
For instance, if a judgeranked the output of systems A, B, C, D, Eas A > B > C > D > E, this would entail thatA > B, A > C, A > D and A > E, etc.To determine the relative weights for the tunedcombinations, we followed a similar approach tothe one used by PRO to tune the relative weights ofthe components of a log-linear SMT model (Hop-kins and May, 2011), also using Maximum En-tropy as the base learning algorithm.
UnlikePRO, (i) we use human judgments, not automaticscores, and (ii) we train on all pairs, not on a sub-sample.5 Experimental ResultsIn this section, we explore how discourse informa-tion can be used to improve machine translationevaluation metrics.
Below we present the evalua-tion results at the system- and segment-level, usingour two basic metrics on discourse trees (Section3.1), which are referred to as DR and DR-LEX.5.1 EvaluationIn our experiments, we only consider translationinto English, and use the data described in Table 1.For evaluation, we follow the setup of the metricstask of WMT12 (Callison-Burch et al, 2012): atthe system-level, we use the official script fromWMT12 to calculate the Spearman?s correlation,where higher absolute values indicate better met-rics performance; at the segment-level, we useKendall?s Tau for measuring correlation, wherenegative values are worse than positive ones.7In our experiments, we combine DR andDR-LEX to other metrics in two different ways:using uniform linear interpolation (at system- andsegment-level), and using a tuned linear interpo-lation for the segment-level.
We only present theaverage results over all four language pairs.
Forsimplicity, in our tables we show results dividedinto evaluation groups:1.
Group I: contains our evaluation metrics, DRand DR-LEX.2.
Group II: includes the metrics that partici-pated in the WMT12 metrics task, excludingmetrics which did not have results for all lan-guage pairs.3.
Group III: contains other important evalu-ation metrics, which were not consideredin the WMT12 metrics task: NIST andROUGE for both system- and segment-level,and BLEU and TER at segment-level.4.
Group IV: includes the metric combinationscalculated with ASIYA and described in Sec-tion 4.For each metric in groups II, III and IV, wepresent the results for the original metric as wellfor the linear interpolation of that metric with DRand with DR-LEX.
The combinations with DRand DR-LEX that improve over the original met-rics are shown in bold, and those that degrade arein italic.
Furthermore, we also present overall re-sults for: (i) the average score over all metrics, ex-cluding DR and DR-LEX, and (ii) the differencesin the correlations for the DR/DR-LEX-combinedand the original metrics.7We have fixed a bug in the scoring tool from WMT12,which was making all scores positive.
This madeTERRORCAT?s score negative, as we present it in Table 3.692Metrics +DR +DR-LEXIDR .807 ?
?DR-LEX .876 ?
?IISEMPOS .902 .853 .903AMBER .857 .829 .869METEOR .834 .861 .888TERRORCAT .831 .854 .889SIMPBLEU .823 .826 .859TER .812 .836 .848BLEU .810 .830 .846POSF .754 .841 .857BLOCKERRCATS .751 .859 .855WORDBLOCKEC .738 .822 .843XENERRCATS .735 .819 .843IIINIST .817 .842 .875ROUGE .884 .899 .869IVAsiya-LEX .879 .881 .882Asiya-SYN .891 .913 .883Asiya-SRL .917 .911 .909Asiya-SEM .891 .889 .886Asiya-0809 .905 .914 .905Asiya-ALL .899 .907 .896average .839 .862 .874diff.
+.024 +.035Table 2: Results on WMT12 at the system-level.Spearman?s correlation with human judgments.5.2 System-level ResultsTable 2 shows the system-level experimental re-sults for WMT12.
We can see that DR is alreadycompetitive by itself: on average, it has a cor-relation of .807, very close to BLEU and TERscores (.810 and .812, respectively).
Moreover,DR yields improvements when combined with 15of the 19 metrics; worsening only four of the met-rics.
Overall, we observe an average improvementof +.024, in the correlation with the human judg-ments.
This suggests that DR contains informationthat is complementary to that used by the othermetrics.
Note that this is true both for the indi-vidual metrics from groups II and III, as well asfor the metric combinations in group IV.
Combi-nations in the last group involve several metricsthat already use linguistic information at differentlevels and are hard to improve over; yet, addingDR does improve, which shows that it has somecomplementary information to offer.As expected, DR-LEX performs better than DRsince it is lexicalized (at the unigram level), andalso gives partial credit to correct structures.
Indi-vidually, DR-LEX outperforms most of the metricsfrom group II, and ranks as the second best metricin that group.
Furthermore, when combined withindividual metrics in group II, DR-LEX is able toimprove consistently over each one of them.Metrics +DR +DR-LEXIDR -.433 ?
?DR-LEX .133 ?
?IISPEDE07PP .254 .190 .223METEOR .247 .178 .217AMBER .229 .180 .216SIMPBLEU .172 .141 .191XENERRCATS .165 .132 .185POSF .154 .125 .201WORDBLOCKEC .153 .122 .181BLOCKERRCATS .074 .068 .151TERRORCAT -.186 -.111 -.104IIINIST .214 .172 .206ROUGE .185 .144 .201TER .217 .179 .229BLEU .185 .154 .190IVAsiya-LEX .254 .237 .253Asiya-SYN .177 .169 .191Asiya-SRL -.023 .015 .161Asiya-SEM .134 .152 .197Asiya-0809 .254 .250 .258Asiya-ALL .268 .265 .270average .165 .145 .190diff.
-.019 +.026Table 3: Results on WMT12 at the segment-level.Kendall?s Tau with human judgments.Note that, even though DR-LEX has better indi-vidual performance than DR, it does not yield im-provements when combined with most of the met-rics in group IV.8However, over all metrics and alllanguage pairs, DR-LEX is able to obtain an aver-age improvement in correlation of +.035, which isremarkably higher than that of DR.
Thus, we canconclude that at the system-level, adding discourseinformation to a metric, even using the simplest ofthe combination schemes, is a good idea for mostof the metrics, and can help to significantly im-prove the correlation with human judgments.5.3 Segment-level Results: Non-tunedTable 3 shows the results for WMT12 at thesegment-level.
We can see that DR performsbadly, with a high negative Kendall?s Tau of -.433.This should not be surprising: (a) the discoursetree structure alone does not contain enough infor-mation for a good evaluation at the segment-level,and (b) this metric is more sensitive to the qualityof the DT, which can be wrong or void.8In this work, we have not investigated the reasons behindthis phenomenon.
We speculate that this might be caused bythe fact that the lexical information in DR-LEX is incorpo-rated only in the form of unigram matching at the sentence-level, while the metrics in group IV are already complex com-bined metrics, which take into account stronger lexical mod-els.
Note, however, that the variations are very small andmight not be significant.693TunedMetrics Orig.
+DR +DR-LEXIDR -.433 ?
?
?DR-LEX .133 ?
?
?IISPEDE07PP .254 ?
.253 .254METEOR .247 ?
.250 .251AMBER .229 ?
.230 .232SIMPBLEU .172 ?
.181 .199TERRORCAT -.186 ?
.181 .196XENERRCATS .165 ?
.175 .194POSF .154 ?
.160 .201WORDBLOCKEC .153 ?
.161 .189BLOCKERRCATS .074 ?
.087 .150IIINIST .214 ?
.222 .224ROUGE .185 ?
.196 .218TER .217 ?
.229 .246BLEU .185 ?
.189 .194IVAsiya-LEX .254 .266 .269 .270Asiya-SYN .177 .229 .228 .232Asiya-SRL -.023 -.004 .039 .181Asiya-SEM .134 .146 .179 .202Asiya-0809 .254 .295 .295 .295Asiya-ALL .268 .296 .295 .295average .165 .201 .222diff.
+.036 +.057Table 4: Results on WMT12 at the segment-level: tuning with cross-validation on WMT12.Kendall?s Tau with human judgments.Additionally, DR is more likely to produce ahigh number of ties, which is harshly penalizedby WMT12?s definition of Kendall?s Tau.
Con-versely, ties and incomplete discourse analysiswere not a problem at the system-level, where ev-idence from all 3,003 test sentences is aggregated,and allows to rank systems more precisely.
Due tothe low score of DR as an individual metric, it failsto yield improvements when uniformly combinedwith other metrics.Again, DR-LEX is better than DR; with a pos-itive Tau of +.133, yet as an individual metric, itranks poorly compared to other metrics in groupII.
However, when linearly combined with othermetrics, DR-LEX outperforms 14 of the 19 met-rics in Table 3.
Across all metrics, DR-LEX yieldsan average Tau improvement of +.026, i.e.
from.165 to .190.
This is a large improvement, takinginto account that the combinations are just uniformlinear combinations.
In subsection 5.4, we presentthe results of tuning the linear combination in adiscriminative way.5.4 Segment-level Results: TunedWe experimented with tuning the weights of theindividual metrics in the metric combinations, us-ing the learning method described in Section 4.2.First, we did this using cross-validation to tuneand test on WMT12.
Later we tuned on WMT12and evaluated on WMT11.
For cross-validationin WMT12, we used ten folds of approximatelyequal sizes, each containing about 300 sentences:we constructed the folds by putting together en-tire documents, thus not allowing sentences froma document to be split over two different folds.During each cross-validation run, we trained ourpairwise ranker using the human judgments cor-responding to nine of the ten folds.
We aggre-gated the data for different language pairs, andproduced a single set of tuning weights for all lan-guage pairs.9We then used the remaining fold forevaluationThe results are shown in Table 4.
As in previ-ous sections we present the average results overall four language pairs.
We can see that the tunedcombinations with DR-LEX improve over most ofthe individual metrics in groups II and III.
Inter-estingly, the tuned combinations that include themuch weaker metric DR now improve over 12 outof 13 of the individual metrics in groups II and III,and only slightly degrades the score of the 13thone (SPEDE07PP).Note that the ASIYA metrics are combinationsof several metrics, and these combinations (whichexclude DR and DR-LEX) can be also tuned; thisyields sizable improvements over the untuned ver-sions as column three in the table shows.
Com-pared to this baseline, DR improves for three ofthe six ASIYA metrics, while DR-LEX improvesfor four of them.
Note that improving over thelast two ASIYA metrics is very hard: they havevery high scores of .296 and .295; for compar-ison, the best segment-level system at WMT12(SPEDE07PP) achieved a Tau of .254.On average, DR improves Tau from .165 to.201, which is +.036, while DR-LEX improves to.222, or +.057.
These much larger improvementshighlight the importance of tuning the linear com-bination when working at the segment-level.5.4.1 Testing on WMT11In order to rule out the possibility that the im-provement of the tuned metrics on WMT12 comesfrom over-fitting, and to verify that the tuned met-rics do generalize when applied to other sentences,we also tested on a new test set: WMT11.9Tuning separately for each language pair yielded slightlylower results.694Therefore, we tuned the weights on all WMT12pairwise judgments (no cross-validation), and weevaluated on WMT11.
Since the metrics that par-ticipated in WMT11 and WMT12 are different(and even when they have the same name, thereis no guarantee that they have not changed from2011 to 2012), we only report results for the ver-sions of NIST, ROUGE, TER, and BLEU availablein ASIYA, as well as for the ASIYA metrics, thusensuring that the metrics in the experiments areconsistent for 2011 and 2012.The results are shown in Table 5.
Once again,tuning yields sizable improvements over the sim-ple combination for the ASIYA metrics (third col-umn in Table 5).
Adding DR and DR-LEX to thecombinations manages to improve over five andfour of the six tuned ASIYA metrics, respectively.However, some of the differences are very small.On the contrary, DR and DR-LEX significantly im-prove over NIST, ROUGE, TER, and BLEU.
Over-all, DR improves the average Tau from .207 to.244, which is +.037, while DR-LEX improves to.267 or +.061.
These improvements are very closeto those for the WMT12 cross-validation.
Thisshows that the weights learned on WMT12 gen-eralize well, as they are also good for WMT11.What is also interesting to note is that when tun-ing is used, DR helps achieve sizeable improve-ments, even if not as strong as for DR-LEX.
Thisis remarkable given that DR has a strong negativeTau as an individual metric at the sentence-level.This suggests that both DR and DR-LEX containinformation that is complementary to that of theindividual metrics that we experimented with.Overall, from the experimental results in thissection, we can conclude that discourse structureis an important information source to be taken intoaccount in the automatic evaluation of machinetranslation output.6 Conclusions and Future WorkIn this paper we have shown that discourse struc-ture can be used to improve automatic MT evalua-tion.
First, we defined two simple discourse-awaresimilarity metrics (lexicalized and un-lexicalized),which use the all-subtree kernel to compute sim-ilarity between discourse parse trees in accor-dance with the Rhetorical Structure Theory.
Then,after extensive experimentation on WMT12 andWMT11 data, we showed that a variety of ex-isting evaluation metrics can benefit from ourTunedMetrics Orig.
+DR +DR-LEXIDR -.447 ?
?
?DR-LEX .146 ?
?
?IIINIST .219 ?
.226 .232ROUGE .205 ?
.218 .242TER .262 ?
.274 .296BLEU .186 ?
.192 .207IVAsiya-LEX .282 .301 .302 .303Asiya-SYN .216 .259 .260 .260Asiya-SRL -.004 .017 .051 .200Asiya-SEM .189 .194 .220 .239Asiya-0809 .300 .348 .349 .348Asiya-ALL .313 .347 .347 .347average .207 .244 .267diff.
+.037 +.061Table 5: Results on WMT11 at the segment-level:tuning on the entire WMT12.
Kendall?s Tau withhuman judgments.discourse-based metrics, both at the segment- andthe system-level, especially when the discourse in-formation is incorporated in an informed way (i.e.using supervised tuning).
Our results show thatdiscourse-based metrics can improve the state-of-the-art MT metrics, by increasing correlation withhuman judgments, even when only sentence-leveldiscourse information is used.Addressing discourse-level phenomena in MTis a relatively new research direction.
Yet, manyof the ongoing efforts have been moderately suc-cessful according to traditional evaluation met-rics.
There is a consensus in the MT communitythat more discourse-aware metrics need to be pro-posed for this area to move forward.
We believethis work is a valuable contribution towards thislonger-term goal.The tuned combined metrics tested in this pa-per are just an initial proposal, i.e.
a simple ad-justment of the relative weights for the individ-ual metrics in a linear combination.
In the fu-ture, we plan to work on integrated representationsof syntactic, semantic and discourse-based struc-tures, which would allow us to train evaluationmetrics based on more fine-grained features.
Ad-ditionally, we propose to use the discourse infor-mation for MT in two different ways.
First, at thesentence-level, we can use discourse informationto re-rank alternative MT hypotheses; this couldbe applied either for MT parameter tuning, or as apost-processing step for the MT output.
Second,we propose to move in the direction of using dis-course information beyond the sentence-level.695ReferencesNicholas Asher and Alex Lascarides, 2003.
Logics ofConversation.
Cambridge University Press.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011workshop on statistical machine translation.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 22?64, Edinburgh, Scot-land, July.
ACL.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montr?eal, Canada, June.
ACL.Lynn Carlson and Daniel Marcu.
2001.
Discourse Tag-ging Reference Manual.
Technical Report ISI-TR-545, University of Southern California InformationSciences Institute.Bruno Cartoni, Sandrine Zufferey, Thomas Meyer, andAndrei Popescu-Belis.
2011.
How comparable areparallel corpora?
measuring the distribution of gen-eral vocabulary and connectives.
In Proceedings ofthe 4th Workshop on Building and Using Compa-rable Corpora: Comparable Corpora and the Web,pages 78?86, Portland, Oregon, June.
ACL.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP?08), Honolulu, Hawaii,USA.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, ACL ?05, pages 263?270, Ann Arbor, Michigan.Michael Collins and Nigel Duffy.
2001.
ConvolutionKernels for Natural Language.
In Neural Informa-tion Processing Systems, NIPS?01, pages 625?632,Vancouver, Canada.Elisabet Comelles, Jes?us Gim?enez, Llu?
?s M`arquez,Irene Castell?on, and Victoria Arranz.
2010.Document-level automatic mt evaluation based ondiscourse representations.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 333?338, Uppsala,Sweden, July.
ACL.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic metric for reliable optimization andevaluation of machine translation systems.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 85?91, Edinburgh, Scot-land, July.
ACL.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the Sec-ond International Conference on Human LanguageTechnology Research, HLT ?02, pages 138?145, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of the 2004 Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics: Human Language Technology,HLT-NAACL, pages 273?280.Jes?us Gim?enez and Llu?
?s M`arquez.
2007.
Linguis-tic features for automatic evaluation of heterogenousMT systems.
In Proceedings of the Second Work-shop on Statistical Machine Translation, pages 256?264, Prague, Czech Republic, June.
ACL.Jes?us Gim?enez and Llu?
?s M`arquez.
2009.
On the ro-bustness of syntactic and semantic features for auto-matic MT evaluation.
In Proceedings of the FourthWorkshop on Statistical Machine Translation, pages250?258, Athens, Greece, March.
ACL.Jes?us Gim?enez and Llu?
?s M`arquez.
2010a.
Asiya:an Open Toolkit for Automatic Machine Translation(Meta-)Evaluation.
The Prague Bulletin of Mathe-matical Linguistics, 94:77?86.Jes?us Gim?enez and Llu?
?s M`arquez.
2010b.
LinguisticMeasures for Automatic Machine Translation Eval-uation.
Machine Translation, 24(3?4):77?86.Michael Halliday and Ruqaiya Hasan, 1976.
Cohesionin English.
Longman, London.Christian Hardmeier and Marcello Federico.
2010.Modelling pronominal anaphora in statistical ma-chine translation.
In Proceedings of the Interna-tional Workshop on Spoken Language Translation,pages 283?289.Christian Hardmeier, Joakim Nivre, and J?org Tiede-mann.
2012.
Document-wide decoding for phrase-based statistical machine translation.
In Proceed-ings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, EMNLP-CoNLL ?12, pages 1179?1190, Jeju Island, Korea.ACL.Christian Hardmeier.
2012.
Discourse in statisticalmachine translation.
a survey and a case study.
Dis-cours.
Revue de linguistique, psycholinguistique etinformatique, 11(8726).Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?11.696Shafiq Joty, Giuseppe Carenini, and Raymond T. Ng.2012.
A Novel Discriminative Framework forSentence-Level Discourse Analysis.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, EMNLP-CoNLL ?12,pages 904?915, Jeju Island, Korea.
ACL.Shafiq Joty, Giuseppe Carenini, Raymond T. Ng, andYashar Mehdad.
2013.
Combining Intra- andMulti-sentential Rhetorical Parsing for Document-level Discourse Analysis.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics, ACL ?13, pages 486?496, Sofia,Bulgaria.
ACL.Hans Kamp and Uwe Reyle.
1993.
From Discourseto Logic: Introduction to Model theoretic Semanticsof Natural Language, Formal Logic and DiscourseRepresentation Theory.
Number 42 in Studies inLinguistics and Philosophy.
Kluwer Academic Pub-lishers.Chin-Yew Lin.
2004.
ROUGE: A Package for Au-tomatic Evaluation of Summaries.
In Proceedingsof Workshop on Text Summarization Branches Out,pages 74?81, Barcelona.Ding Liu and Daniel Gildea.
2005.
Syntactic fea-tures for evaluation of machine translation.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pages 25?32, Ann Ar-bor, Michigan, June.
ACL.Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.2012.
Fully automatic semantic mt evaluation.
InProceedings of the Seventh Workshop on Statisti-cal Machine Translation, pages 243?252, Montr?eal,Canada, June.
ACL.William Mann and Sandra Thompson.
1988.
Rhetor-ical Structure Theory: Toward a Functional Theoryof Text Organization.
Text, 8(3):243?281.Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui,and Andrea Gesmundo.
2012.
Machine translationof labeled discourse connectives.
In Proceedings ofthe Tenth Biennial Conference of the Association forMachine Translation in the Americas (AMTA).Thomas Meyer.
2011.
Disambiguating temporal-contrastive connectives for machine translation.
InProceedings of the ACL 2011 Student Session, pages46?51, Portland, OR, USA, June.
ACL.Alessandro Moschitti and Roberto Basili.
2006.
ATree Kernel approach to Question and Answer Clas-sification in Question Answering Systems.
In Pro-ceedings of the 5th international conference on Lan-guage Resources and Evaluation, Genoa, Italy.Alessandro Moschitti, Silvia Quarteroni, RobertoBasili, and Suresh Manandhar.
2007.
ExploitingSyntactic and Shallow Semantic Kernels for Ques-tion/Answer Classification.
In Proceedings of theACL-2007, pages 776?783, Prague, Czech Repub-lic.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the Association for Computational Linguistics(ACL?02), Philadelphia, PA, USA.Maja Popovic and Hermann Ney.
2007.
Word errorrates: Decomposition over POS classes and applica-tions for error analysis.
In Proceedings of the Sec-ond Workshop on Statistical Machine Translation,pages 48?55, Prague, Czech Republic, June.
ACL.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal smt.
In Proceedings of the 43rdAnnual Meeting on Association for ComputationalLinguistics, ACL ?05, pages 271?279, Ann Arbor,Michigan.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the 7th Biennial Conference of theAssociation for Machine Translation in the Ameri-cas, AMTA ?06, Cambridge, MA, USA.Charles Sutton, Andrew McCallum, and KhashayarRohanimanesh.
2007.
Dynamic Conditional Ran-dom Fields: Factorized Probabilistic Models for La-beling and Segmenting Sequence Data.
Journal ofMachine Learning Research (JMLR), 8:693?723.Kuo-Chung Tai.
1979.
The tree-to-tree correctionproblem.
Journal of the ACM, 26(3):422?433, July.Taro Watanabe, Jun Suzuki, Hajime Tsukada, andHideki Isozaki.
2007.
Online large-margin trainingfor statistical machine translation.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, EMNLP-CoNLL ?07,Prague, Czech Republic.Bonnie Webber, Andrei Popescu-Belis, Katja Markert,and J?org Tiedemann, editors.
2013.
Proceedings ofthe Workshop on Discourse in Machine Translation.ACL, Sofia, Bulgaria, August.Bonnie Webber.
2004.
D-LTAG: Extending Lex-icalized TAG to Discourse.
Cognitive Science,28(5):751?779.Billy T. M. Wong and Chunyu Kit.
2012.
Ex-tending machine translation evaluation metrics withlexical cohesion to document level.
In Proceed-ings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, EMNLP-CoNLL, pages 1060?1068, Jeju Island, Korea, July.ACL.697Dekai Wu and Pascale Fung.
2009.
Semantic rolesfor smt: A hybrid two-pass model.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguis-tics, Companion Volume: Short Papers, pages 13?16, Boulder, Colorado, June.698
