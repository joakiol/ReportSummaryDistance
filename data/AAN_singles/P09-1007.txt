Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 55?63,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPCross Language Dependency Parsing using a Bilingual Lexicon?Hai Zhao(??)?
?, Yan Song(??
)?, Chunyu Kit?, Guodong Zhou?
?Department of Chinese, Translation and LinguisticsCity University of Hong Kong83 Tat Chee Avenue, Kowloon, Hong Kong, China?School of Computer Science and TechnologySoochow University, Suzhou, China 215006{haizhao,yansong,ctckit}@cityu.edu.hk, gdzhou@suda.edu.cnAbstractThis paper proposes an approach to en-hance dependency parsing in a languageby using a translated treebank from an-other language.
A simple statistical ma-chine translation method, word-by-worddecoding, where not a parallel corpus buta bilingual lexicon is necessary, is adoptedfor the treebank translation.
Using an en-semble method, the key information ex-tracted from word pairs with dependencyrelations in the translated text is effectivelyintegrated into the parser for the target lan-guage.
The proposed method is evaluatedin English and Chinese treebanks.
It isshown that a translated English treebankhelps a Chinese parser obtain a state-of-the-art result.1 IntroductionAlthough supervised learning methods bring state-of-the-art outcome for dependency parser infer-ring (McDonald et al, 2005; Hall et al, 2007), alarge enough data set is often required for specificparsing accuracy according to this type of meth-ods.
However, to annotate syntactic structure, ei-ther phrase- or dependency-based, is a costly job.Until now, the largest treebanks1 in various lan-guages for syntax learning are with around onemillion words (or some other similar units).
Lim-ited data stand in the way of further performanceenhancement.
This is the case for each individuallanguage at least.
But, this is not the case as weobserve all treebanks in different languages as awhole.
For example, of ten treebanks for CoNLL-2007 shared task, none includes more than 500K?The study is partially supported by City University ofHong Kong through the Strategic Research Grant 7002037and 7002388.
The first author is sponsored by a research fel-lowship from CTL, City University of Hong Kong.1It is a tradition to call an annotated syntactic corpus astreebank in parsing community.tokens, while the sum of tokens from all treebanksis about two million (Nivre et al, 2007).As different human languages or treebanksshould share something common, this makes itpossible to let dependency parsing in multiple lan-guages be beneficial with each other.
In this pa-per, we study how to improve dependency parsingby using (automatically) translated texts attachedwith transformed dependency information.
As acase study, we consider how to enhance a Chinesedependency parser by using a translated Englishtreebank.
What our method relies on is not theclose relation of the chosen language pair but thesimilarity of two treebanks, this is the most differ-ent from the previous work.Two main obstacles are supposed to confront ina cross-language dependency parsing task.
Thefirst is the cost of translation.
Machine translationhas been shown one of the most expensive lan-guage processing tasks, as a great deal of time andspace is required to perform this task.
In addition,a standard statistical machine translation methodbased on a parallel corpus will not work effec-tively if it is not able to find a parallel corpus thatright covers source and target treebanks.
How-ever, dependency parsing focuses on the relationsof word pairs, this allows us to use a dictionary-based translation without assuming a parallel cor-pus available, and the training stage of translationmay be ignored and the decoding will be quite fastin this case.
The second difficulty is that the out-puts of translation are hardly qualified for the pars-ing purpose.
The most challenge in this aspect ismorphological preprocessing.
We regard that themorphological issue should be handled aiming atthe specific language, our solution here is to usecharacter-level features for a target language likeChinese.The rest of the paper is organized as follows.The next section presents some related existingwork.
Section 3 describes the procedure on tree-55bank translation and dependency transformation.Section 4 describes a dependency parser for Chi-nese as a baseline.
Section 5 describes how aparser can be strengthened from the translatedtreebank.
The experimental results are reported inSection 6.
Section 7 looks into a few issues con-cerning the conditions that the proposed approachis suitable for.
Section 8 concludes the paper.2 The Related WorkAs this work is about exploiting extra resources toenhance an existing parser, it is related to domainadaption for parsing that has been draw some in-terests in recent years.
Typical domain adaptationtasks often assume annotated data in new domainabsent or insufficient and a large scale unlabeleddata available.
As unlabeled data are concerned,semi-supervised or unsupervised methods will benaturally adopted.
In previous works, two basictypes of methods can be identified to enhance anexisting parser from additional resources.
The firstis usually focus on exploiting automatic generatedlabeled data from the unlabeled data (Steedmanet al, 2003; McClosky et al, 2006; Reichart andRappoport, 2007; Sagae and Tsujii, 2007; Chenet al, 2008), the second is on combining super-vised and unsupervised methods, and only unla-beled data are considered (Smith and Eisner, 2006;Wang and Schuurmans, 2008; Koo et al, 2008).Our purpose in this study is to obtain a furtherperformance enhancement by exploiting treebanksin other languages.
This is similar to the abovefirst type of methods, some assistant data shouldbe automatically generated for the subsequent pro-cessing.
The differences are what type of data areconcerned with and how they are produced.
In ourmethod, a machine translation method is appliedto tackle golden-standard treebank, while all theprevious works focus on the unlabeled data.Although cross-language technique has beenused in other natural language processing tasks,it is basically new for syntactic parsing as fewworks were concerned with this issue.
The rea-son is straightforward, syntactic structure is toocomplicated to be properly translated and the costof translation cannot be afforded in many cases.However, we empirically find this difficulty maybe dramatically alleviated as dependencies ratherthan phrases are used for syntactic structure repre-sentation.
Even the translation outputs are not sogood as the expected, a dependency parser for thetarget language can effectively make use of themby only considering the most related informationextracted from the translated text.The basic idea to support this work is to makeuse of the semantic connection between differentlanguages.
In this sense, it is related to the work of(Merlo et al, 2002) and (Burkett and Klein, 2008).The former showed that complementary informa-tion about English verbs can be extracted fromtheir translations in a second language (Chinese)and the use of multilingual features improves clas-sification performance of the English verbs.
Thelatter iteratively trained a model to maximize themarginal likelihood of tree pairs, with alignmentstreated as latent variables, and then jointly parsingbilingual sentences in a translation pair.
The pro-posed parser using features from monolingual andmutual constraints helped its log-linear model toachieve better performance for both monolingualparsers and machine translation system.
In thiswork, cross-language features will be also adoptedas the latter work.
However, although it is not es-sentially different, we only focus on dependencyparsing itself, while the parsing scheme in (Bur-kett and Klein, 2008) based on a constituent rep-resentation.Among of existing works that we are aware of,we regard that the most similar one to ours is (Ze-man and Resnik, 2008), who adapted a parser to anew language that is much poorer in linguistic re-sources than the source language.
However, thereare two main differences between their work andours.
The first is that they considered a pair of suf-ficiently related languages, Danish and Swedish,and made full use of the similar characteristics oftwo languages.
Here we consider two quite dif-ferent languages, English and Chinese.
As fewerlanguage properties are concerned, our approachholds the more possibility to be extended to otherlanguage pairs than theirs.
The second is that aparallel corpus is required for their work and astrict statistical machine translation procedure wasperformed, while our approach holds a merit ofsimplicity as only a bilingual lexicon is required.3 Treebank Translation and DependencyTransformation3.1 DataAs a case study, this work will be conducted be-tween the source language, English, and the tar-get language, Chinese, namely, we will investigate56how a translated English treebank enhances a Chi-nese dependency parser.For English data, the Penn Treebank (PTB) 3is used.
The constituency structures is convertedto dependency trees by using the same rules as(Yamada and Matsumoto, 2003) and the standardtraining/development/test split is used.
However,only training corpus (sections 2-21) is used forthis study.
For Chinese data, the Chinese Treebank(CTB) version 4.0 is used in our experiments.
Thesame rules for conversion and the same data splitis adopted as (Wang et al, 2007): files 1-270 and400-931 as training, 271-300 as testing and files301-325 as development.
We use the gold stan-dard segmentation and part-of-speech (POS) tagsin both treebanks.As a bilingual lexicon is required for our taskand none of existing lexicons are suitable for trans-lating PTB, two lexicons, LDC Chinese-EnglishTranslation Lexicon Version 2.0 (LDC2002L27),and an English to Chinese lexicon in StarDict2,are conflated, with some necessary manual exten-sions, to cover 99% words appearing in the PTB(the most part of the untranslated words are namedentities.).
This lexicon includes 123K entries.3.2 TranslationA word-by-word statistical machine translationstrategy is adopted to translate words attachedwith the respective dependency information fromthe source language to the target one.
In detail, aword-based decoding is used, which adopts a log-linear framework as in (Och and Ney, 2002) withonly two features, translation model and languagemodel,P (c|e) = exp[?2i=1 ?ihi(c, e)]?c exp[?2i=1 ?ihi(c, e)]Whereh1(c, e) = log(p?
(c|e))is the translation model, which is converted fromthe bilingual lexicon, andh2(c, e) = log(p?
(c))is the language model, a word trigram modeltrained from the CTB.
In our experiment, we settwo weights ?1 = ?2 = 1.2StarDict is an open source dictionary software, availableat http://stardict.sourceforge.net/.The conversion process of the source treebankis completed by three steps as the following:1.
Bind POS tag and dependency relation of aword with itself;2.
Translate the PTB text into Chinese word byword.
Since we use a lexicon rather than a parallelcorpus to estimate the translation probabilities, wesimply assign uniform probabilities to all transla-tion options.
Thus the decoding process is actu-ally only determined by the language model.
Sim-ilar to the ?bag translation?
experiment in (Brownet al, 1990), the candidate target sentences madeup by a sequence of the optional target words areranked by the trigram language model.
The outputsentence will be generated only if it is with maxi-mum probability as follows,c = argmax{p?(c)p?
(c|e)}= argmax p?
(c)= argmax?p?
(wc)A beam search algorithm is used for this processto find the best path from all the translation op-tions; As the training stage, especially, the mosttime-consuming alignment sub-stage, is skipped,the translation only includes a decoding procedurethat takes about 4.5 hours for about one millionwords of the PTB in a 2.8GHz PC.3.
After the target sentence is generated, the at-tached POS tags and dependency information ofeach English word will also be transferred to eachcorresponding Chinese word.
As word order is of-ten changed after translation, the pointer of eachdependency relationship, represented by a serialnumber, should be re-calculated.Although we try to perform an exact word-by-word translation, this aim cannot be fully reachedin fact, as the following case is frequently encoun-tered, multiple English words have to be translatedinto one Chinese word.
To solve this problem,we use a policy that lets the output Chinese wordonly inherits the attached information of the high-est syntactic head in the original multiple Englishwords.4 Dependency Parsing: Baseline4.1 Learning Model and FeaturesAccording to (McDonald and Nivre, 2007), alldata-driven models for dependency parsing thathave been proposed in recent years can be de-scribed as either graph-based or transition-based.57Table 1: Feature NotationsNotation Meanings The word in the top of stacks?
The first word below the top of stack.s?1,s1...
The first word before(after) the wordin the top of stack.i, i+1,...
The first (second) word in theunprocessed sequence, etc.dir Dependent directionh Headlm Leftmost childrm Rightmost childrn Right nearest childform word formpos POS tag of wordcpos1 coarse POS: the first letter of POS tag of wordcpos2 coarse POS: the first two POS tags of wordlnverb the left nearest verbchar1 The first character of a wordchar2 The first two characters of a wordchar?1 The last character of a wordchar?2 The last two characters of a word.
?s, i.e., ?s.dprel?
means dependent labelof character in the top of stack+ Feature combination, i.e., ?s.char+i.char?means both s.char and i.char work as afeature function.Although the former will be also used as compari-son, the latter is chosen as the main parsing frame-work by this study for the sake of efficiency.
In de-tail, a shift-reduce method is adopted as in (Nivre,2003), where a classifier is used to make a parsingdecision step by step.
In each step, the classifierchecks a word pair, namely, s, the top of a stackthat consists of the processed words, and, i, thefirst word in the (input) unprocessed sequence, todetermine if a dependent relation should be estab-lished between them.
Besides two dependency arcbuilding actions, a shift action and a reduce ac-tion are also defined to maintain the stack and theunprocessed sequence.
In this work, we adopt aleft-to-right arc-eager parsing model, that meansthat the parser scans the input sequence from leftto right and right dependents are attached to theirheads as soon as possible (Hall et al, 2007).While memory-based and margin-based learn-ing approaches such as support vector machinesare popularly applied to shift-reduce parsing, weapply maximum entropy model as the learningmodel for efficient training and adopting over-lapped features as our work in (Zhao and Kit,2008), especially, those character-level ones forChinese parsing.
Our implementation of maxi-mum entropy adopts L-BFGS algorithm for pa-rameter optimization as usual.With notations defined in Table 1, a feature setas shown in Table 2 is adopted.
Here, we explainsome terms in Tables 1 and 2.
We used a largescale feature selection approach as in (Zhao et al,2009) to obtain the feature set in Table 2.
Somefeature notations in this paper are also borrowedfrom that work.The feature curroot returns the root of a par-tial parsing tree that includes a specified node.The feature charseq returns a character sequencewhose members are collected from all identifiedchildren for a specified word.In Table 2, as for concatenating multiple sub-strings into a feature string, there are two ways,seq and bag.
The former is to concatenate all sub-strings without do something special.
The latterwill remove all duplicated substrings, sort the restand concatenate all at last.Note that we systemically use a group ofcharacter-level features.
Surprisingly, as to ourbest knowledge, this is the first report on using thistype of features in Chinese dependency parsing.Although (McDonald et al, 2005) used the pre-fix of each word form instead of word form itselfas features, character-level features here for Chi-nese is essentially different from that.
As Chineseis basically a character-based written language.Character plays an important role in many means,most characters can be formed as single-characterwords, and Chinese itself is character-order freerather than word-order free to some extent.
In ad-dition, there is often a close connection betweenthe meaning of a Chinese word and its first or lastcharacter.4.2 Parsing using a Beam Search AlgorithmIn Table 2, the feature preactn returns the previousparsing action type, and the subscript n stands forthe action order before the current action.
Theseare a group of Markovian features.
Without thistype of features, a shift-reduce parser may directlyscan through an input sequence in linear time.Otherwise, following the work of (Duan et al,2007) and (Zhao, 2009), the parsing algorithm isto search a parsing action sequence with the max-imal probability.Sdi = argmax?ip(di|di?1di?2...),where Sdi is the object parsing action sequence,p(di|di?1...) is the conditional probability, and di58Figure 1: A comparison before and after translationTable 2: Features for Parsingin.form, n = 0, 1i.form + i1.formin.char2 + in+1.char2, n = ?1, 0i.char?1 + i1.char?1in.char?2 n = 0, 3i1.char?2 + i2.char?2 +i3.char?2i.lnverb.char?2i3.posin.pos + in+1.pos, n = 0, 1i?2.cpos1 + i?1.cpos1i1.cpos1 + i2.cpos1 + i3.cpos1s?2.char1s?.char?2 + s?1.char?2s??2.cpos2s?
?1.cpos2 + s?1.cpos2s?.cpos2 + s?1.cpos2s?.children.cpos2.seqs?.children.dprel.seqs?.subtree.depths?.h.form + s?.rm.cpos1s?.lm.char2 + s?.char2s.h.children.dprel.seqs.lm.dprels.char?2 + i1.char?2s.charn + i.charn, n = ?1, 1s?1.pos + i1.poss.pos + in.pos, n = ?1, 0, 1s : i|linePath.form.bags?.form + i.forms?.char2 + in.char2, n = ?1, 0, 1s.curroot.pos + i.poss.curroot.char2 + i.char2s.children.cpos2.seq + i.children.cpos2.seqs.children.cpos2.seq + i.children.cpos2.seq+ s.cpos2 + i.cpos2s?.children.dprel.seq + i.children.dprel.seqpreact?1preact?2preact?2+preact?1is i-th parsing action.
We use a beam search algo-rithm to find the object parsing action sequence.5 Exploiting the Translated TreebankAs we cannot expect too much for a word-by-wordtranslation, only word pairs with dependency rela-tion in translated text are extracted as useful andreliable information.
Then some features basedon a query in these word pairs according to thecurrent parsing state (namely, words in the cur-rent stack and input) will be derived to enhancethe Chinese parser.A translation sample can be seen in Figure 1.Although most words are satisfactorily translated,to generate effective features, what we still have toconsider at first is the inconsistence between thetranslated text and the target text.In Chinese, word lemma is always its word formitself, this is a convenient characteristic in com-putational linguistics and makes lemma featuresunnecessary for Chinese parsing at all.
However,Chinese has a special primary processing task, i.e.,word segmentation.
Unfortunately, word defini-tions for Chinese are not consistent in various lin-guistical views, for example, seven segmentationconventions for computational purpose are for-mally proposed since the first Bakeoff3.Note that CTB or any other Chinese treebankhas its own word segmentation guideline.
Chi-nese word should be strictly segmented accordingto the guideline before POS tags and dependencyrelations are annotated.
However, as we say the3Bakeoff is a Chinese processing share task held bySIGHAN.59English treebank is translated into Chinese wordby word, Chinese words in the translated text areexactly some entries from the bilingual lexicon,they are actually irregular phrases, short sentencesor something else rather than words that followsany existing word segmentation convention.
If thebilingual lexicon is not carefully selected or re-fined according to the treebank where the Chineseparser is trained from, then there will be a seriousinconsistence on word segmentation conventionsbetween the translated and the target treebanks.As all concerned feature values here are calcu-lated from the searching result in the translatedword pair list according to the current parsingstate, and a complete and exact match cannot bealways expected, our solution to the above seg-mentation issue is using a partial matching strat-egy based on characters that the words include.Above all, a translated word pair list, L, is ex-tracted from the translated treebank.
Each item inthe list consists of three elements, dependant word(dp), head word (hd) and the frequency of this pairin the translated treebank, f .There are two basic strategies to organize thefeatures derived from the translated word pair list.The first is to find the most matching word pairin the list and extract some properties from it,such as the matched length, part-of-speech tagsand so on, to generate features.
Note that amatching priority serial should be defined afore-hand in this case.
The second is to check everymatching models between the current parsing stateand the partially matched word pair.
In an earlyversion of our approach, the former was imple-mented.
However, It is proven to be quite inef-ficient in computation.
Thus we adopt the sec-ond strategy at last.
Two matching model fea-ture functions, ?(?)
and ?(?
), are correspondinglydefined as follows.
The return value of ?(?)
or?(?)
is the logarithmic frequency of the matcheditem.
There are four input parameters requiredby the function ?(?).
Two parameters of themare about which part of the stack(input) words ischosen, and other two are about which part ofeach item in the translated word pair is chosen.These parameters could be set to full or charn asshown in Table 1, where n = ...,?2,?1, 1, 2, ....For example, a possible feature could be?
(s.full, i.char1, dp.full, hd.char1), it tries tofind a match in L by comparing stack word anddp word, and the first character of input wordTable 3: Features based on the translated treebank?
(i.char3, s?.full, dp.char3, hd.full)+i.char3+s?.form?
(i.char3, s.char2, dp.char3, hd.char2)+s.char2?
(i.char3, s.full, dp.char3, hd.char2)+s.form?
(s?.char?2, hd.char?2, head)+i.pos+s?.pos?
(i.char3, s.full, dp.char3, hd.char2)+s.full?
(s?.full, i.char4, dp.full, hd.char4)+s?.pos+i.pos?
(i.full, hd.char2, root)+i.pos+s.pos?
(i.full, hd.char2, root)+i.pos+s?.pos?
(s.full, dp.full, dependant)+i.pospairscore(s?.pos, i.pos)+s?.form+i.formrootscore(s?.pos)+s?.form+i.formrootscore(s?.pos)+i.posand the first character of hd word.
If sucha match item in L is found, then ?(?)
returnslog(f).
There are three input parameters requiredby the function ?(?).
One parameter is aboutwhich part of the stack(input) words is chosen,and the other is about which part of each itemin the translated word pair is chosen.
The thirdis about the matching type that may be set todependant, head, or root.
For example, thefunction ?
(i.char1, hd.full, root) tries to find amatch in L by comparing the first character of in-put word and the whole dp word.
If such a matchitem in L is found, then ?(?)
returns log(f) as hdoccurs as ROOT f times.As having observed that CTB and PTB share asimilar POS guideline.
A POS pair list from PTBis also extract.
Two types of features, rootscoreand pairscore are used to make use of such infor-mation.
Both of them returns the logarithmic valueof the frequency for a given dependent event.
Thedifference is, rootscore counts for the given POStag occurring as ROOT, and pairscore counts fortwo POS tag combination occurring for a depen-dent relationship.A full adapted feature list that is derived fromthe translated word pairs is in Table 3.6 Evaluation ResultsThe quality of the parser is measured by the pars-ing accuracy or the unlabeled attachment score(UAS), i.e., the percentage of tokens with correcthead.
Two types of scores are reported for compar-ison: ?UAS without p?
is the UAS score withoutall punctuation tokens and ?UAS with p?
is the onewith all punctuation tokens.The results with different feature sets are in Ta-ble 4.
As the features preactn are involved, a60beam search algorithm with width 5 is used forparsing, otherwise, a simple shift-reduce decodingis used.
It is observed that the features derivedfrom the translated text bring a significant perfor-mance improvement as high as 1.3%.Table 4: The results with different feature setsfeatures with p without pbaseline -d 0.846 0.858+da 0.848 0.860+Tb -d 0.859 0.869+d 0.861 0.870a+d: using three Markovian features preact andbeam search decoding.b+T: using features derived from the translated textas in Table 3.To compare our parser to the state-of-the-artcounterparts, we use the same testing data as(Wang et al, 2005) did, selecting the sentenceslength up to 40.
Table 5 shows the results achievedby other researchers and ours (UAS with p), whichindicates that our parser outperforms any otherones 4.
However, our results is only slightly betterthan that of (Chen et al, 2008) as only sentenceswhose lengths are less than 40 are considered.
Asour full result is much better than the latter, thiscomparison indicates that our approach improvesthe performance for those longer sentences.Table 5: Comparison against the state-of-the-artfull up to 40(McDonald and Pereira, 2006)a - 0.825(Wang et al, 2007) - 0.866(Chen et al, 2008) 0.852 0.884Ours 0.861 0.889aThis results was reported in (Wang et al, 2007).The experimental results in (McDonald andNivre, 2007) show a negative impact on the pars-ing accuracy from too long dependency relation.For the proposed method, the improvement rela-tive to dependency length is shown in Figure 2.From the figure, it is seen that our method givesobservable better performance when dependencylengths are larger than 4.
Although word order ischanged, the results here show that the useful in-formation from the translated treebank still helpthose long distance dependencies.4There is a slight exception: using the same data splitting,(Yu et al, 2008) reported UAS without p as 0.873 versus ours,0.870.1 4 7 10 13 16 190.40.50.60.70.80.91Dependency LengthF1basline: +d+T: +dFigure 2: Performance vs. dependency length7 DiscussionIf a treebank in the source language can help im-prove parsing in the target language, then theremust be something common between these twolanguages, or more precisely, these two corre-sponding treebanks.
(Zeman and Resnik, 2008)assumed that the morphology and syntax in thelanguage pair should be very similar, and that isso for the language pair that they considered, Dan-ish and Swedish, two very close north Europeanlanguages.
Thus it is somewhat surprising thatwe show a translated English treebank may helpChinese parsing, as English and Chinese even be-long to two different language systems.
However,it will not be so strange if we recognize that PTBand CTB share very similar guidelines on POS andsyntactics annotation.
Since it will be too abstractin discussing the details of the annotation guide-lines, we look into the similarities of two treebanksfrom the matching degree of two word pair lists.The reason is that the effectiveness of the proposedmethod actually relies on how many word pairs atevery parsing states can find their full or partialmatched partners in the translated word pair list.Table 6 shows such a statistics on the matchingdegree distribution from all training samples forChinese parsing.
The statistics in the table suggestthat most to-be-check word pairs during parsinghave a full or partial hitting in the translated wordpair list.
The latter then obtains an opportunity toprovide a great deal of useful guideline informa-tion to help determine how the former should betackled.
Therefore we have cause for attributingthe effectiveness of the proposed method to thesimilarity of these two treebanks.
From Table 6,61we also find that the partial matching strategy de-fined in Section 5 plays a very important role inimproving the whole matching degree.
Note thatour approach is not too related to the characteris-tics of two languages.
Our discussion here bringsan interesting issue, which difference is more im-portant in cross language processing, between twolanguages themselves or the corresponding anno-tated corpora?
This may be extensively discussedin the future work.Table 6: Matching degree distributiondependant-match head-match Percent (%)None None 9.6None Partial 16.2None Full 9.9Partial None 12.4Partial Partial 42.6Partial Full 7.3Full None 3.7Full Partial 7.0Full Full 0.2Note that only a bilingual lexicon is adopted inour approach.
We regard it one of the most mer-its for our approach.
A lexicon is much easier tobe obtained than an annotated corpus.
One of theremained question about this work is if the bilin-gual lexicon should be very specific for this kindof tasks.
According to our experiences, actually, itis not so sensitive to choose a highly refined lexi-con or not.
We once found many words, mostlynamed entities, were outside the lexicon.
Thuswe managed to collect a named entity translationdictionary to enhance the original one.
However,this extra effort did not receive an observable per-formance improvement in return.
Finally we re-alize that a lexicon that can guarantee two wordpair lists highly matched is sufficient for this work,and this requirement may be conveniently satis-fied only if the lexicon consists of adequate high-frequent words from the source treebank.8 Conclusion and Future WorkWe propose a method to enhance dependencyparsing in one language by using a translated tree-bank from another language.
A simple statisti-cal machine translation technique, word-by-worddecoding, where only a bilingual lexicon is nec-essary, is used to translate the source treebank.As dependency parsing is concerned with the re-lations of word pairs, only those word pairs withdependency relations in the translated treebank arechosen to generate some additional features to en-hance the parser for the target language.
The ex-perimental results in English and Chinese tree-banks show the proposed method is effective andhelps the Chinese parser in this work achieve astate-of-the-art result.Note that our method is evaluated in two tree-banks with a similar annotation style and it avoidsusing too many linguistic properties.
Thus themethod is in the hope of being used in other simi-larly annotated treebanks 5.
For an immediate ex-ample, we may adopt a translated Chinese tree-bank to improve English parsing.
Although thereare still something to do, the remained key workhas been as simple as considering how to deter-mine the matching strategy for searching the trans-lated word pair list in English according to theframework of our method.
.AcknowledgementsWe?d like to give our thanks to three anonymousreviewers for their insightful comments, Dr. ChenWenliang for for helpful discussions and Mr. LiuJun for helping us fix a bug in our scoring pro-gram.ReferencesPeter F. Brown, John Cocke, Stephen A. Della Pietra,Vincent J. Della Pietra, Fredrick Jelinek, John D.Lafferty, Robert L. Mercer, and Paul S. Roossin.1990.
A statistical approach to machine translation.Computational Linguistics, 16(2):79?85.David Burkett and Dan Klein.
2008.
Two lan-guages are better than one (for syntactic parsing).
InEMNLP-2008, pages 877?886, Honolulu, Hawaii,USA.Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchi-moto, Yujie Zhang, and Hitoshi Isahara.
2008.
De-pendency parsing with short dependency relationsin unlabeled data.
In Proceedings of IJCNLP-2008,Hyderabad, India, January 8-10.Xiangyu Duan, Jun Zhao, and Bo Xu.
2007.
Proba-bilistic parsing action models for multi-lingual de-pendency parsing.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages940?946, Prague, Czech, June 28-30.Johan Hall, Jens Nilsson, Joakim Nivre,Gu?lsen Eryig?it, Bea?ta Megyesi, Mattias Nils-son, and Markus Saers.
2007.
Single malt or5For example, Catalan and Spanish treebanks from theAnCora(-Es/Ca) Multilevel Annotated Corpus that are an-notated by the Universitat de Barcelona (CLiC-UB) and theUniversitat Polit?cnica de Catalunya (UPC).62blended?
a study in multilingual parser optimiza-tion.
In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL 2007, pages 933?939,Prague, Czech, June.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In Proceedings of ACL-08: HLT, pages 595?603,Columbus, Ohio, USA, June.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Reranking and self-training for parseradaptation.
In Proceedings of ACL-COLING 2006,pages 337?344, Sydney, Australia, July.Ryan McDonald and Joakim Nivre.
2007.
Charac-terizing the errors of data-driven dependency pars-ing models.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL 2007), pages 122?131,Prague, Czech, June 28-30.Ryan McDonald and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proceedings of EACL-2006, pages 81?88,Trento, Italy, April.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of ACL-2005,pages 91?98, Ann Arbor, Michigan, USA, June 25-30.Paola Merlo, Suzanne Stevenson, Vivian Tsang, andGianluca Allaria.
2002.
A multilingual paradigmfor automatic verb classification.
In ACL-2002,pages 207?214, Philadelphia, Pennsylvania, USA.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The conll 2007 shared task on de-pendency parsing.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, page915?932, Prague, Czech, June.Joakim Nivre.
2003.
An efficient algorithm for projec-tive dependency parsing.
In Proceedings of IWPT-2003), pages 149?160, Nancy, France, April 23-25.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In Proceedings of ACL-2002, pages 295?302, Philadelphia, USA, July.Roi Reichart and Ari Rappoport.
2007.
Self-trainingfor enhancement and domain adaptation of statisticalparsers trained on small datasets.
In Proceedings ofACL-2007, pages 616?623, Prague, Czech Republic,June.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependencyparsing and domain adaptation with lr models andparser ensembles.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, page1044?1050, Prague, Czech, June 28-30.Noah A. Smith and Jason Eisner.
2006.
Annealingstructural bias in multilingual weighted grammar in-duction.
In Proceedings of ACL-COLING 2006,page 569?576, Sydney, Australia, July.Mark Steedman, Miles Osborne, Anoop Sarkar,Stephen Clark, Rebecca Hwa, Julia Hockenmaier,Paul Ruhlen, Steven Baker, and Jeremiah Crim.2003.
Bootstrapping statistical parsers from smalldatasets.
In Proceedings of EACL-2003, page331?338, Budapest, Hungary, April.Qin Iris Wang and Dale Schuurmans.
2008.
Semi-supervised convex training for dependency parsing.In Proceedings of ACL-08: HLT, pages 532?540,Columbus, Ohio, USA, June.Qin Iris Wang, Dale Schuurmans, and Dekang Lin.2005.
Strictly lexical dependency parsing.
In Pro-ceedings of IWPT-2005, pages 152?159, Vancouver,BC, Canada, October.Qin Iris Wang, Dekang Lin, and Dale Schuurmans.2007.
Simple training of dependency parsers viastructured boosting.
In Proceedings of IJCAI 2007,pages 1756?1762, Hyderabad, India, January.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Sta-tistical dependency analysis with support vectormachines.
In Proceedings of IWPT-2003), page195?206, Nancy, France, April.Kun Yu, Daisuke Kawahara, and Sadao Kurohashi.2008.
Chinese dependency parsing with largescale automatically constructed case structures.
InProceedings of COLING-2008, pages 1049?1056,Manchester, UK, August.Daniel Zeman and Philip Resnik.
2008.
Cross-language parser adaptation between related lan-guages.
In Proceedings of IJCNLP 2008 Workshopon NLP for Less Privileged Languages, pages 35?42, Hyderabad, India, January.Hai Zhao and Chunyu Kit.
2008.
Parsing syntactic andsemantic dependencies with two single-stage max-imum entropy models.
In Proceeding of CoNLL-2008, pages 203?207, Manchester, UK.Hai Zhao, Wenliang Chen, Chunyu Kit, and GuodongZhou.
2009.
Multilingual dependency learning:A huge feature engineering method to semantic de-pendency parsing.
In Proceedings of CoNLL-2009,Boulder, Colorado, USA.Hai Zhao.
2009.
Character-level dependencies inchinese: Usefulness and learning.
In EACL-2009,pages 879?887, Athens, Greece.63
