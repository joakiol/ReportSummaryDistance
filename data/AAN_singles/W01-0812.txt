Reusing a Statistical Language Model for GenerationKevin Humphreys, Mike Calcagno, David WeiseNatural Language Group, Microsoft CorporationOne Microsoft WayRedmond, WA 98052, USA{kevinhum,mikecalc,davidw}@microsoft.comAbstractA relatively self-contained subtask ofnatural language generation is sentencerealization: the process of generating agrammatically correct sentence from anabstract semantic / logicalrepresentation.
We propose a methodwhere sentence realization is carriedout using a simplified (context free)version of a large analysis grammar,combined with a statistical languagemodel from the full (context sensitive)version of the same grammar.
Thestatistical model provides a measure ofthe probability of syntacticsubstructures, derived from the analysisof a corpus with the full grammar, andis used to guide both subsequentanalysis and generation.1 IntroductionTo date, only limited use of statistically-derivedresources has been made for realization innatural language generation, notably Knight &Hatzivassiloglou (1995), Langkilde & Knight(1998) and Bangalore & Rambow (2000).
Thispaper reports on new work in that direction, butwith an emphasis on reusing resources originallyproduced for analysis purposes.
In particular, ageneration grammar is derived from anextensive analysis grammar in such a way as toretain the statistical language model built usingthe analysis grammar.2 Statistically-Driven GenerationWork to date on using statistical knowledge forgeneration has mainly focused on the sub-task ofsurface (in fact, sentence) realization: theproduction of a grammatically correct stringfrom an abstract semantic/logical representationof linguistic content.
This assumes the existenceof a separate higher-level process to producesuch a representation, following the canonicalpipeline architecture of a full generation system(Reiter, 1994).
The approach described here hasthe same focus, but attempts to more tightlyintegrate the statistical knowledge in thegeneration process, and also to avoid the need tocreate generation-specific resources.2.1 NitrogenThe Nitrogen system (Knight &Hatzivassiloglou, 1995; Langkilde & Knight,1998) made the first significant attempt tointegrate statistical knowledge for surfacerealization.
It uses an extremely simplegeneration-specific grammar and generates alattice representing all possible strings that thegrammar allows for a particular semantic input.Then, in a separate stage, simple bigramstatistics are used to rank the alternatives interms of ?fluency?, determined by similarity toword pairs in the training corpus.
The languagemodel represented by the bigrams is not usedwithin the generation algorithm itself, rather itacts as a filter on the proposed output of anindependent generation system.
The simplifiedgrammar is so unconstrained that typicallyhundreds of thousands of alternative strings aregenerated for a single input, including manyungrammatical forms.
The bigram model thenselects the most probable pairwise combinationof words to select a sentence, considering non-adjacent words for a fixed set of syntacticrelations, but not representing any context toallow for true long-distance dependencies or toavoid multiple expression of the sameconstituents.2.2 FergusBangalore & Rambow (2000) build on theapproach of the Nitrogen system but use alanguage model which does encode somestructural information.
They use an XTAGgrammar (XTAG-Group, 1999), which is notgeneration-specific, with statistically rankedsubtree structures associated with lexical entries.An initial set of subtrees is chosen for aparticular input, using the model, then a latticeof all possible combinations licensed by thegrammar is constructed, where eachcombination represents an alternative outputstring.
Then, in the same way as Nitrogen, aseparate trigram model is used to rank thealternative strings.
The subtree combinationphase allows the handling of long-distancedependencies, and can more accurately controlconstraints such as agreement, which Nitrogenmust leave entirely to its bigram model.3 A Reusable Language ModelThe approach presented in this paper places astrong emphasis on reusing resources originallydeveloped for analysis applications.
Withoutrequiring a fully reversible analysis system (e.g.Neumann & van Noord, 1994), it has provedpossible to successfully reuse a language modeldeveloped for analysis in a related generationsystem.3.1 AnalysisThe analysis system is built around a broad-coverage, manually-constructed grammar (adescendant of that described in Jensen et al,1993).
The grammar can be viewed as acontext-free backbone of binary phrase-structurerules, together with an extensive set of detailed,potentially context-sensitive, conditions on eachrule, referring to lexical, morphological,syntactic and semantic features.A statistical language model ?
a lexicalizedPCFG (similar to that of Collins, 1997) ?
isderived from the analysis grammar byprocessing a corpus using the same grammarwith no statistical model and recordingfrequencies of substructures built by each rule.The sensitivity of the model can be tuned toinclude any of the features referred to by ruleconditions, including neighboring or descendantnodes.The training phase for the model requires nomanual annotation of the corpus, although somemanual filtering was done to attempt to excludeany particularly bad parses.
For theapproximately 200 rule grammar, a corpus of25,000 sentences was used for training, selectedfrom a variety of sources and genres.The model is then used in subsequentanalysis with the same grammar to guidebottom-up rule applications to build the mostprobable substructures first, acting to direct thesearch through the structures licensed by thegrammar.3.2 GenerationAn equivalent guidance is also required ingeneration, although here structures are builttop-down.
The analysis grammar cannot beused directly for generation, though thestatistical model depends on the rules in thisgrammar.
For generation, then, a simplifiedgrammar is derived from the full form,effectively retaining only the context-freebackbone and discarding almost all of thedetailed rule conditions.
The statistical model istherefore still applicable to the derivedgeneration grammar.
Probabilities can bedetermined for substructures exactly as in theanalysis grammar, thus retaining the effects ofthe rule conditions without requiring reversingand explicit testing when generating.The derived, simplified, generation grammar,because almost all rule conditions have beendiscarded, will massively overgenerate.1However, the statistical language model,because of the one-to-one correspondence ofrules in the full and simplified grammars,provides an immediate way to constrain theovergeneration.
Structures allowed by thegeneration grammar but which are excluded in1 This is in addition to the overgeneration known to be partof the original analysis grammar to allow for certain classesof ungrammatical inputanalysis will simply be assigned extremely lowprobabilities, and similarly for ungrammaticalstructures, which may be allowed by the analysisgrammar but which occur only rarely in thetraining corpus.At every choice point in the application ofthe grammar rules, the statistical model isavailable to indicate a preference.4 The Generation SystemThe current implementation of the generationsystem operates in three distinct stages:1. the semantic representation (basically, arepresentation of argument structure) ismapped to an unordered set of syntacticnodes,2.
the generation grammar is used to create atree structure to order the syntactic nodesand insert any additional, syntacticallyrequired, nodes,3.
an inflected form of each leaf node in thefinal tree is produced, and a final stringgenerated.The input semantic representation is roughlyequivalent to a quasi-logical form (QLF)(Alshawi, 1992), abstracting away fromstructural syntactic dependencies but makingexplicit many surface features.
In particular,each input is known to be a sentence unit, andall lexical choices, including prepositions anddeterminers, are fully specified.
The remaininggeneration tasks are therefore the linear orderingof the lexical units, and their inflections, eachhandled separately by the second and thirdstages.The current application context for thegeneration system is in the restatement ofnatural language database queries, forclarification and disambiguation.
Analysis ofthe queries produces a QLF representationwhich is interpreted with respect to a semanticmodel of the current database.
Interpretationresults in one or more revised QLFs which arethen passed to the realizer for confirmation orselection by the user before translating to thedatabase query language.4.1 Semantic to syntactic mappingThe first stage translates logical form relationsand features to corresponding syntactic terms,referring to lexical entries where necessary.
Forexample, the semantic representation:run (+past)actor: Johnmanner: quicklyis translated unit-by-unit to a verb phrase, asubject noun phrase, and an adverbial phrasemodifier.
The syntactic units are linked in agraph structure, but with no ordering constraintsbetween them.In the current prototype system, this stage infact enforces a one-to-one mapping of semanticto syntactic features, though there is clear scopefor extending the use of the statistical languagemodel to direct the translation here, and to allowfor a one-to-many mapping to be rankedsubsequently.
The analysis grammar buildslogical forms compositionally, with individualgrammar rules fully specifying their semanticcontributions, but at present these specificationsare not automatically extracted for the simplifiedgeneration grammar, and the language model isnot yet fully sensitive to the semantic features inthe rules.
The extent to which this mappingstage can be automated is currently beinginvestigated.4.2 Linear orderingThe second stage of the system, to determine alinear ordering for the syntactic nodes using thegeneration grammar, integrates the statisticallanguage model directly with no adaptationrequired from its use in analysis.A root node is selected from the set producedin the previous stage, currently the nodeproduced from the root of the logical formgraph, though the selection could be made moreflexible.
The simplified generation grammar isthen checked for all rules which apply to thenode, testing features and relations, such assubject, derived from the semantic relations.
Toaccess the rule probabilities represented in thelanguage model requires that a substructure isgenerated from each applicable rule, and thelanguage model then assigns a probability toeach substructure.
The highest scoringsubstructure is not immediately selected though?
a ?look-ahead?
is carried out to evaluate anyalternative structures which express the samefeatures.First the effect of the rule which producedthe initial structure is determined, in terms ofwhich features or relations the rule ?expresses?or ?consumes?
from the input, e.g.
a VP  NPVP rule, which builds a VP with a subject inanalysis, will ?consume?
the subject from a rootVP in generation.
Alternative generation pathswhich consume the same features or relationsare then considered, and the language modelagain used to determine a probability for eachsubstructure expressing the same.
If analternative path receives a higher ranking thanthe substructure produced initially, the initialsubstructure is discarded, and the remainderconsidered.For the example input given above, threealternative rules in the generation grammarapply to express the adverbial modifier:S  AVP S  (Quickly, John ran)VP  AVP VP (John quickly ran)VP  VP AVP (John ran quickly)The first rule applies to the initial root node,and the substructure it describes is generatedimmediately.
The other two rules apply at lowerlevels of the tree on alternative paths, and therespective probabilities obtained from thelanguage model for the substructures generatedwith these particular lexical items are:2S  AVP S  (0.073)VP  AVP VP (0.061)VP  VP AVP (0.087)The best ranked structure is therefore thatproduced by the third rule, and so the generationpaths including the first two are discarded.
Theremaining paths are then considered for theexpression of the subject relation, selecting ahighly probable VP  NP VP structure, withunary rules applying at the leaves (VP  Verb,NP  Noun and AVP  Adverb), until nofurther expansion of nodes can be made with the2 Probabilities may be different for other lexical items, e.g.with an unambiguously intransitive verb such as ?fall?, theVP  AVP VP rule (John quickly fell) receives the highestranking, based on the current training set.grammar.
For any given node, ungrammaticalsubstructures may be produced, but the languagemodel will always be able to rank them.The generation algorithm for the linearordering stage can be sketched as follows:1.
Make the syntactic node mapped from theroot node of the logical form, the root nodeof the new syntactic tree.2.
For each non-terminal leaf node in the tree:a.
For each generation grammar rule thatapplies to the selected node, testingconditions on the semantically-derivedrelations and features (e.g.
subject):i.
Generate the substructure describedby the rule.ii.
Determine the probability for thesubstructure.iii.
For each generation grammar rulethat applies to the selected node at alower level in the tree, andexpresses the same semanticrelations/features as the rule at thecurrent level:1.
Generate the substructuredescribed by the rule.2.
Determine the probability forthe substructure.iv.
If a substructure generated at alower level has a higher probabilitythan the substructure generated atthe current level, discard thesubstructure at the current level.b.
Add the substructure generated at thecurrent level with the highest probabilityto the current syntactic tree.
If nosubstructures exist at the current level(no applicable rules or all discarded),step down one level (apply a null rule)and repeat from 2.a.The algorithm in fact follows a head-drivennode expansion, or search through the grammar,(as in Shieber et al, 1990), with the head of themost recently expanded node being selected forthe next expansion (in step 2 of the algorithmabove), until a leaf node is produced.
However,the nature of the grammar is such that no ruleexpansion will have side effects on any nodeother than the head of its substructure, and soany search strategy will produce the same finaltree, though more alternative paths may beconsidered.The ?look-ahead?
in the search (step 2.a.iii),to find other rules expressing the same featuresas a current rule, means that, although the ruleprobabilities obtained from the language modelare based entirely on local rule substructures, theoverall path chosen through the grammar isglobally optimal.
The current implementation ofthe look-ahead is not optimal, however, withduplicate substructures being created andevaluated for the same rules along equivalentpaths, and an obvious extension would be theaddition of a simple caching mechanism forsubstructures and their probabilities.One adaptation of the method is to use thelanguage model to produce an overall score forthe final tree (a simple product of thesubstructure scores), and then ?backtrack?
toconsider alternative derivations.
This allows anexhaustive search through the grammar, insteadof finding only the best path, as described above.For the example input given above, the grammarlicenses 12 alternative tree structures, with manyproducing the same final strings but via ruleswhich would be excluded by the conditions inthe analysis grammar (e.g.
AVP  Pronoun)and which are assigned extremely lowprobabilities by the language model, causing theoverall tree scores to differ typically by severalorders of magnitude.4.3 InflectionOnce a complete tree has been produced, eachleaf node is passed to the final stage of therealization process to be inflected.
Featuresaffecting inflection, mainly Person and Numberfor agreement in English, are initially obtainedfrom the logical form input or from the lexiconin the initial semantic to syntactic mappingstage, and then passed through to the appropriatesyntactic nodes by the rules selected to expandthe tree structure.
The rules may also introduceadditional syntactic features such as Case, whichare then also passed through.
Inflected formsfor the final feature set of each leaf node arethen either retrieved from the lexicon orgenerated by rule.
A single final string is thenread from the completed tree.5 PerformanceThe current implementation of the generationsystem is as a prototype subsystem within anexisting (C++) analysis system.
This allowsdirect access to the statistical language modelbuilt for analysis, and which is used unchangedfor generation.
The simplified generationgrammar is derived automatically from theanalysis grammar, and much of the ruleapplication mechanism and representation oftree structures is reused.The system, including the analysis grammarand the statistical model, is under continueddevelopment, but, as is typical of generationsystems, the most significant omission is thelack of any formal evaluation methodology.Bangalore & Rambow (2000) propose someinteresting initial metrics, but we have not yetattempted any comparative experiments.Informal evaluation on a non-blind trainingcorpus of 200 logical forms (processed atapproximately 40 per second on a 1GHz PC)currently shows a roughly 4% error rate(ungrammatical output) for output sentenceswith an average length of 7 words (maximum14).
However the vast majority of these errors(85%) are from the bad placement of adjectivephrase modifiers, for example ?Show [NP the [AJPlarger than Monaco] countries]?, due to a highprobability being assigned for structures with amodifier phrase on the left.
This suggests arevision of the statistical model to make it moresensitive to particular modifier types, rather thana revision of the generation algorithm.
Indeed, asignificant benefit of the reversible approachrepresented by sharing resources betweenanalysis and generation, is to driveimprovements to the resources by identifyingweaknesses not apparent in a single processingmode.6 Future WorkPlanned extensions include the use of anadditional bigram model to assist in cases whereconstituent orderings are not constrained by thegrammar, such as sequences of adjectives andother modifiers.
Such information can beintegrated in the initial semantic to syntacticmapping stage where constituent head-wordscan be compared directly, or during ruleselection in the linear ordering stage to indicatea precedence among otherwise equivalentconstituents.A further area of investigation is the effect ofretraining the statistical model on specific genre,rather than general, corpora.
This has thepotential to bias selections in the grammartowards constructions typical of a certain style,such as prepositional phrase fronting in formalwriting, etc.The current generation grammar alsoexcludes punctuation rules, though these arepresent in the original analysis grammar, andexperimentation to determine the ability of thelanguage model to select and place punctuationis planned.Of course, the input representation used forthe realization stage assumes that most of thechallenging higher level issues in text andsentence planning have already been dealt with,but this is not yet the case.7 ConclusionThe tight integration of the statistical languagemodel into the generation process described hereallows a ?best first?
search through the possibleexpansions licensed by a simplified andovergenerating grammar.
This contrasts withthe exhaustive searches through the grammars inNitrogen (Langkilde & Knight, 1998) andFergus (Bangalore & Rambow, 2000), where thegeneration algorithm operates independently ofthe statistical resources.
Such integration hasthe potential to produce easily tunablegeneration systems based around a stablecomprehensive grammar, as well as indicateprecisely which statistical language models aremost suited to generation requirements, andwhether these requirements differ at all fromthose of analysis.ReferencesAlshawi, H., ed., (1992) The Core Language Engine,MIT Press, Cambridge, Massachusetts, USA.Bangalore S. and Rambow O.
(2000) Exploiting aprobabilistic hierarchical model for generation.
In?Proceedings of COLING-2000?, Saarbr?cken,Germany.Collins, M.J. (1997) Three generative lexicalisedmodels for statistical parsing.
In ?Proceedings ofACL-EACL?97?, Madrid, Spain.Jensen K., Heidorn G.E.
and Richardson S.D., eds.,(1993) Natural language processing: the PLNLPapproach, Kluwer Academic Publishers, Boston,Massachusetts, USA.Knight K. And Hatzivassiloglou V. (1995) Two-level,Many-Paths Generation.
In ?Proceedings ofACL?95?, Cambridge, Massachusetts, USA.Langkilde I. and Knight K. (1998) Generation thatexploits corpus-based statistical knowledge.
In?Proceedings of COLING-ACL?98?, Montreal,Canada.Nuemann G. and van Noord G. (1994) Reversibilityand self-monitoring in natural languagegeneration.
In ?Reversible Grammar in NaturalLanguage Processing?, Strzalkowski T., ed.,Kluwer Academic Publishers, Dordrecht, TheNetherlands.Reiter E. (1994) Has a consensus NL generationarchitecture appeared, and is it psychologicallyplausible?
In ?Proceedings of the 7th InternationalWorkshop on Natural Language Generation?,Maine, USA.Shieber S.M., van Noord G., Moore R.C.
and PereiraC.N.
(1990) Semantic-head-driven generation.Computational Linguistics, 16(1).XTAG-Group The (1999) A lexicalised TreeAdjoining Grammar for English.
Technicalreport, Institute for Research in Cognitive Science,University of Pennsylvania, USA.
