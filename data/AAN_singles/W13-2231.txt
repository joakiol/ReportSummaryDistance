Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 240?251,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsCoping with the Subjectivity of Human Judgementsin MT Quality EstimationMarco Turchi Matteo Negri Marcello FedericoFondazione Bruno Kessler, FBK-irstTrento , Italy{turchi|negri|federico}@fbk.euAbstractSupervised approaches to NLP tasks relyon high-quality data annotations, whichtypically result from expensive manual la-belling procedures.
For some tasks, how-ever, the subjectivity of human judgementsmight reduce the usefulness of the an-notation for real-world applications.
InMachine Translation (MT) Quality Esti-mation (QE), for instance, using human-annotated data to train a binary classifierthat discriminates between good (usefulfor a post-editor) and bad translations isnot trivial.
Focusing on this binary task,we show that subjective human judge-ments can be effectively replaced with anautomatic annotation procedure.
To thisaim, we compare binary classifiers trainedon different data: the human-annotateddataset from the 7th Workshop on Statis-tical Machine Translation (WMT-12), andan automatically labelled version of thesame corpus.
Our results show that humanlabels are less suitable for the task.1 IntroductionWith the steady progress in the field of StatisticalMachine Translation (SMT), the translation indus-try is now faced with the possibility of significantproductivity increases (i.e.
amount of publishableoutput per unit of time).
One way to achieve thisgoal, in Computer Assisted Translation (CAT) en-vironments, is the integration of (precise, but of-ten partial) suggestions obtained through ?fuzzymatches?
from a Translation Memory (TM), with(complete, but potentially less precise) translationsproduced by an MT system.
Such integration canloosely consist in presenting translators with un-ranked suggestions obtained from the MT and theTM, or rely on tighter combination strategies.
Forinstance, MT and TM translations can be automat-ically ranked to ease the selection of the most suit-able one for post-editing (He et al 2010), or theTM can be used to constrain and improve MT sug-gestions (Ma et al 2011).
In all cases, the ef-fectiveness of the integration is conditioned by:i) the quality of MT, and ii) the accuracy in au-tomatically predicting such quality.
Higher pro-ductivity increases depend on the capability of theMT system to output useful material that is closeto be publishable ?as is?
(Denkowski and Lavie,2012), and the capability to automatically identifyand present to human translators only such sug-gestions.Recognizing good translations falls in the scopeof research on automatic MT Quality Estimation(QE), which addresses the problem of estimatingthe quality of a translated sentence at run-time,without access to reference translations (Specia etal., 2009; Soricut and Echihabi, 2010; Bach et al2011; Specia, 2011; Mehdad et al 2012b).
Inrecent years QE gained increasing interest in theMT community, resulting in several datasets avail-able for training and evaluation (Callison-Burch etal., 2012), the definition of features showing goodcorrelation with human judgements (Soricut et al2012), and the release of open-source software.1The proposed solutions to the QE problem relyon supervised methods that strongly depend on theavailability of labelled data.
While early works(Blatz et al 2003) exploited annotations obtainedwith automatic MT evaluation metrics like BLEU(Papineni et al 2002), the current trend is torely on human annotations, which seem to leadto more accurate models (Quirk, 2004; Specia etal., 2009).
Along this direction, the QE task con-sists in predicting scores that reflect human qualityjudgements, by learning from manually annotateddatasets (e.g.
collections of source-target pairs la-1http://www.quest.dcs.shef.ac.uk/240belled according to an n-point Likert scale or withreal numbers in a given interval).
Within this dom-inant supervised framework, we explore differentways to obtain labelled data for training a bi-nary QE classifier suitable for integration in aCAT tool.
Since, to the best of our knowledge,labelled data with binary judgements are currentlynot available, we consider two alternative options.The first option is to adapt an existing dataset,checking whether it can be partitioned in a waythat reflects the distinction between good (use-ful for the translator, suitable for post editing)and bad translations (that need complete rewrit-ing).2 To this aim we experiment with the QEdata released within the 7th Workshop on Ma-chine Translation (WMT-12).
The corpus con-sists of source-target pairs annotated with manualQE labels (1-5 scores) indicating the post-editingneeded to correct the translations.
Besides explicithuman judgements, the availability of post-editedtranslations makes also possible to calculate theactual HTER values (Snover et al 2009), indicat-ing the minimum edit distance between the ma-chine translation and its manually post-edited ver-sion in the [0,1] interval.The second option is to automatically re-annotate the same dataset, trying to produce labelsthat reflect an objective and more reliable binarydistinction based on empirical observations.Our analysis aims to answer the following ques-tions:1.
Are human labels reliable and coherentenough to train accurate binary models?2.
Are arbitrarily-set thresholds useful to parti-tion QE data for this task?3.
Is it possible to obtain reliable binary annota-tions from an automatic procedure?Negative answers to the first two questions wouldrespectively call into question: i) the intuitive ideathat human labels are the most reliable for a super-vised approach to binary QE, and ii) the possibilitythat thresholds on a single metric (e.g.
the HTER)can be set to capture the subtle differences separat-ing useful from useless translations.
A positive an-swer to the third question would open to the possi-bility to create training datasets in a more coherent2In the remainder of the paper we will consider as ?good?translations those for which post-editing requires a smallereffort than translation from scratch.
Conversely, we will labelas ?bad?
the translations that need complete rewriting.and replicable way compared to current data anno-tation methods.
By answering these questions, thispaper provides the following main contributions:?
We show that training a binary classifier onarbitrary partitions of an existing dataset isdifficult.
Our experiments with the WMT-12 corpus demonstrate that neither followingstandard indications (e.g.
?if more than 70%of the MT output needs to be edited, a trans-lation from scratch is necessary?
)3, nor con-sidering arbitrary HTER thresholds, it is pos-sible to obtain accurate binary classifiers suit-able for integration in a CAT environment;?
We propose a replicable automatic (hencenon subjective) method to re-annotate an ex-isting dataset in a way that the resulting bi-nary classifier outperforms those trained withhuman labels.?
We show that, with our method, a smalleramount of training data is sufficient to ob-tain similar or better performance comparedto that of the human-annotated dataset usedfor comparison.2 Binary QE for CAT environmentsQE has been mainly addressed as a classificationor regression task, where a quality score (respec-tively an integer or a real value) has to be automat-ically assigned to MT output sentences given theirsource (Specia et al 2010).
Casting the problemin this way, the integration of a QE componentin a CAT environment makes possible to presenttranslators with estimates of the expected qualityof each MT suggestion.
Such intuitive solution,however, disregards the fact that even precise QEscores would not alleviate translators from the ef-fort of reading useless MT output (or at least theassociated score).A more effective alternative is to use the esti-mated QE scores to filter out poor MT suggestions,presenting only those worth for post-editing.
Bi-nary classification, however, has to confront withthe problem of setting reasonable cut-off criteria.The arbitrary thresholds, used in several previousworks (Quirk, 2004; Specia et al 2010; Speciaet al 2011) are in fact hard to justify, and evenharder to learn from human-labelled training data.3This was a guideline for the professional trans-lators involved in the annotation of a previous ver-sion of the dataset used for the WMT-12 evalua-tion (see http://www.statmt.org/wmt12/quality-estimation-task.html).241On one side, for instance, there is no evi-dence that the 70% HTER threshold used in somedatasets yields the optimal separation between ac-ceptable and totally useless suggestions.
Such ar-bitrary criterion, based on the raw count of post-editing operations, is likely to reflect a partial viewon a complex problem, disregarding important as-pects such as the distribution of the corrections inthe MT output.
However, in some cases, havingthe first 30% of words correctly translated mighttake less post-editing effort than having 50% ofcorrectly translated terms scattered throughout thewhole sentence.
In these cases, a 70% HTERthreshold would wrongly consider useless trans-lations as positive instances and vice-versa.On the other side, when arbitrary thresholds areused as annotation guidelines (Callison-Burch etal., 2012), the moderate agreement between hu-man judges might make manual labels ill-suited tolearn accurate models.Under the constraints posed by a CAT envi-ronment, where only useful suggestions can leadto a significant productivity increase, the idealmodel should maximize the number of true posi-tives (useful translations recognized as good) min-imizing, at the same time, the number of false pos-itives (useless translations recognized as good).
Tothis aim, the more the training data are partitionedaccording to objective criteria, the higher the ex-pected reliability of the corresponding cut-off and,in turn, the higher the expected performance of thebinary classifier.Focusing on these issues, the following sectionsdiscuss various methods to obtain training data forbinary QE geared to the integration in a CAT en-vironment.
Partitions based on human judgementsfrom the WMT-12 dataset will be compared withan automatic method to re-annotate the same cor-pus.
The suitability of the resulting training setsfor binary classification will be assessed by mea-suring the performance of classifiers built fromeach training set.
Metrics sensitive to the numberof false positives will be used for this purpose.3 Partitioning the WMT-12 datasetDue to the lack of datasets annotated with ex-plicit binary (good, bad) judgements about transla-tion quality, the most intuitive way to obtain train-ing data for our QE classifier is to adapt exist-ing manually-labelled data.
The reasonable sizeof the WMT-12 dataset makes it a good candidatefor our purposes.
The corpus consists of 2,254English-Spanish news sentences (1,832 for train-ing, 422 for test) produced by the Moses phrase-based SMT system (Koehn et al 2007) trainedon Europarl (Koehn, 2005) and News Commen-taries corpora,4 along with their source sentences,reference translations and post-edited translations.Training and test instances have been annotated byprofessional translators with scores (1 to 5) indi-cating the estimated post-editing effort (percent-age of MT output that has to be corrected).
Ac-cording to the proposed scheme, the highest scoreindicates lowest effort (MT output requires little orno editing), while the lowest score indicates thatthe MT output needs to be translated from scratch.To cope with systematic biases among the anno-tators,5 the judgements were combined in a finalscore obtained from their weighted average, re-sulting in a labelled dataset with real numbers inthe [1, 5] interval as effort scores.In order to obtain suitable data for binary QE,the WMT-12 training set (1,832 instances) hasbeen partitioned in different ways, leaving the testset for evaluation (see Section 5).
The goal, foreach partition strategy, was to label as bad (the as-signed label is -1) only the translations that needcomplete rewriting, keeping all the other transla-tions as good instances (labelled with +1).
Consid-ering the averaged effort scores, the actual humanjudgements, and the HTER values calculated be-tween the translations and the corresponding post-edited version, we experimented with the follow-ing three partition criteria.Average effort scores (AES).
Three partitionshave been generated based on the effort scoresof 2, 2.5, and 3, labelling the WMT-12 train-ing instances with scores below or equal to eachthreshold as negative examples (-1), and the in-stances with scores above the threshold as posi-tive examples (+1).
Partitions with thresholds be-low 2 were also considered, including the mostintuitive partition with cut-off set to 1.
However,the resulting number of negative instances, if any,was too scarce, and the overall dataset too unbal-anced, to make standard supervised learning meth-ods effective The creation of highly unbalanceddata is a recurring issue for all the partition meth-4http://www.statmt.org/wmt11/translation-task.html#download5Such biases support the idea that labelling translationswith quality scores is per se a highly subjective task.242ods we applied to the WMT-12 corpus.
Togetherwith the low homogeneity of human labels (evenfor very poor translations the three judges do notagree in assigning the lowest score), in most ofthe cases the small number of low-quality transla-tions in the dataset makes the negative class con-siderably smaller than the positive one.
This canbe observed in Table 1, which provides the to-tal number of positive and negative instances foreach partition method.
For instance, with our low-est AES threshold (2) the total number of nega-tive instances is 113, while the positive ones are1,719.
Although considering different cut-off cri-teria aims to make our investigation more com-plete, it?s also worth remarking that the higher thethreshold, the higher the distance of the result-ing experimental setting from our target scenario.While 2, as an effort score threshold, is likelyto reflect a reasonable separation between uselessand post-editable translations, higher values are inprinciple more appropriate for ?soft?
separationsinto worse versus better translations.Human scores (HS).
Five partitions have beengenerated using the actual labels assigned by thethree annotators to each translation instead of theaverage effort scores.
In particular, we consideredthe following score combinations (?X?
stands forany integer between 1 and 5): 1-X-X, 2-2-2, 2-2-X, 2-3-3, 3-3-3.
Also in this case, as shownin Table 1, partitions based on lower scores leadto highly unbalanced datasets of limited usability,while those based on higher scores are increas-ingly more distant to our application scenario.6HTER scores (HTER).
Seven partitions havebeen generated considering the following HTERthresholds: 0.75, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45.In this case, being the HTER an error measure,training instances with scores above or equal tothe threshold were labelled as negative examples(-1), while instances with lower scores were la-belled as positive examples (+1).
Similar to theother partition criteria, some of our threshold val-ues reflect our task more closely than others, butresult in more unbalanced datasets.
In particular,thresholds around 0.7 substantially adhere to theWMT-12 annotation guidelines (as far as transla-tions that need complete rewriting are concerned)6The partition most closely related to our task (i.e.
1-1-1)was impossible to produce since none of the examples waslabelled with 1 by all the annotators.
Even for 1-1-X, thenegative class contains only one example.and produce training data with fewer negative in-stances.
Other thresholds, which is still worth ex-ploring since we do not know the optimal cut-offvalue, are in principle less suitable to our task butproduce more balanced training data.Training instancesAverage effort scores (AES) Positive Negative2 1,719 1132.5 1,475 3573 1,194 638Human scores (HS) Positive Negative1-X-X 1,736 962-2-2 1,719 1132-2-X 1,612 2202-3-3 1,457 3753-3-3 1,360 472HTER scores (HTER) Positive Negative0.75 1,798 340.7 1,786 460.65 1,756 760.6 1,708 1240.55 1,653 1790.5 1,531 3010.45 1,420 412Table 1: Number of positive/negative instances foreach partition of the WMT-12 training set.4 Re-annotating the WMT-12 datasetAs an alternative to partitioning methods, we in-vestigated the possibility to re-annotate the WMT-12 training set with an automatic procedure.4.1 ApproachOur approach, which does not involve subjec-tive human judgements, is based on the observa-tion of similarities and dissimilarities between anautomatic translation (TGT), its post-edited ver-sion (PE) and the corresponding reference trans-lation (RT).
Such comparisons provide useful in-dications about the behaviour of a post-editorwhen correcting automatic translations and, inturn, about MT output quality.Typically, the PE version of a good-quality TGTpreserves some characteristics (e.g.
lexical, struc-tural) that indicate a moderate correction activityby the post editor.
Conversely, in the PE ver-sion of a low-quality TGT, such characteristicsare more difficult to observe, indicating an in-tense correction activity.
At the two extremes, thePE of a perfect TGT preserves all its characteris-tics, while the PE of a useless TGT looses mostof them.
In the first case TGT and PE are iden-243tical, and their similarity is the highest possible(i.e.
sim(TGT, PE) = 1).
In the second case,TGT and PE show a degree of similarity close tothat of TGT and a completely rewritten transla-tion featuring different lexical choices and struc-ture.
This is where reference translations comeinto play: considering RT as a good example ofrewritten sentence,7 for low-quality TGT we willhave sim(TGT, PE) ?
sim(TGT,RT ).In light of these considerations, we hypothe-size that the automatic re-annotation of WMT-12training data can take advantage of a classifier thatlearns a similarity threshold T such that:?
a PE sentence with sim(TGT, PE) ?
Twill be considered as a rewritten translation(hence TGT is useless, and the correspond-ing source-TGT pair a negative example tobe labelled as ?-1?);?
a PE sentence with sim(TGT, PE) > Twill be considered as a real post-edition(hence TGT is useful for the post-editor, andthe corresponding source-TGT pair a positiveexample to be labelled as ?+1?
).Based on this hypothesis, to perform our au-tomatic re-annotation procedure we: 1) create atraining set Z of positive and negative examples(i.e.
[TGT, correct translation] pairs, where cor-rect translation is either a post-editing or a rewrit-ten translation); 2) design a feature set capableto capture different aspects of the similarity be-tween TGT and correct translation; 3) build a bi-nary classifier using Z; 4) use the classifier to labelthe [TGT, PE] pairs as instances of post-editingsor rewritings; 5) assess the quality of the resultingannotation.4.2 Building the classifierTraining corpus.
To build a classifier capableof labelling PE sentences as rewritten/post-editedmaterial, we first created a set of positive and neg-ative instances from the WMT-12 training set.
Foreach tuple [source, TGT, PE, RT] of the dataset,one positive and one negative instance have beenrespectively obtained as the combination of [TGT,PE] and [TGT, RT].
Figure 1, which plots the dis-tribution of positive and negative instances againstHTER, shows a fairly good separation between the7Such assumption is supported by the fact that referencesentences are, by definition, free translations manually pro-duced without any influence from the target.0 500 1000 1500 2000 2500 3000 3500 400000.10.20.30.40.50.60.70.80.91SentencesHTERTGT?PE sentencesTGT?RT sentencesFigure 1: Distribution of [TGT, PE] and [TGT,RT] pairs plotted against the HTER.two classes.
This indicates that our use of thereferences as examples of rewritten translationsbuilds on a reasonable assumption.Features.
Crucial to our classification task, anumber of features can be used to estimate sen-tence similarity.
Differently from the binary QEtask, where the possibility to catch common char-acteristics between two sentences is limited bylanguage barriers, in our re-annotation task all thefeatures are extracted by comparing two monolin-gual sentences (i.e.
TGT and a correct translation,either a PE or a RT).
Although the problem ofmeasuring sentence similarity can be addressedin many ways, the solutions should not overlookthe specificities of the task.
In our case, for in-stance, the scarce importance of the semantic as-pect (TGT, PE and RT typically show a high se-mantic similarity) makes features used for othertasks (e.g.
based on distributional similarity) lesseffective than shallow features looking at the sur-face form of the input sentences.
Our problempresents some similarities with the plagiarism de-tection task, where subtle lexical and structuralsimilarities have to be identified to spot suspiciousplagiarized texts (Potthast et al 2010).
For thisreason, part of our features (e.g.
ROUGE scores)are inspired by research in such field (Chen et al2010), while others have been designed ad-hoc,based on the specific requirements of our task.
Theresulting feature set aims to capture text similar-ity by measuring word/n-gram matches, as well asthe level of sparsity and density of the commonwords as a shallow indicator of structural similar-ity.
In total, from each [TGT, correct translation]244pair, the following 22 features are extracted:?
Human-targeted Translation Error Rate ?HTER.
The editing operations consideredare: shift, insertion, substitution and deletion.?
Number of words in common.?
Number of words in common, normalized byTGT length and correct translation length (2features).?
Number of words in TGT and in the cor-rect translation (2 features).?
Size of the longest common subsequence.?
Size of the longest common subsequence,normalized by TGT length.?
Aligned word density: total number ofaligned words,8 divided by the number ofaligned blocks (more than 1 aligned word).?
Unaligned word density: total number of un-aligned words, divided by the number of un-aligned blocks (more than 1 unaligned word).?
Normalized number of aligned blocks: totalnumber of aligned blocks, divided by TGTlength.?
Normalized number of unaligned blocks: to-tal number of unaligned blocks, divided byTGT length.?
Normalized density difference: differencebetween aligned word density and unalignedword density, divided by TGT length.?
Modified Lesk score (Lesk, 1986): sum ofthe squares of the length of n-gram matches,normalized by the product of the sentencelengths.?
ROUGE-1/2/3/4: n-gram recall with n=1,...,4(4 features).9?
ROUGE-L: size of longest commonsubsequence, normalized by the cor-rect translation length.?
ROUGE-W: the ROUGE-L using differentweights for consecutive matches of length L(default weight = 1.2).?
ROUGE-S: the ROUGE-L allowing for thepresence of skip-bigrams (pairs of words,even not adjacent, in their sentence order).?
ROUGE-SU: the extension of ROUGE-Sadding unigrams as counting unit.8Monolingual stem-to-stem exact matches between TGTand correct translation are inferred by computing the HTER,as in (Blain et al 2012).9All ROUGE scores, described in (Lin, 2004), have beencalculated using the software available at http://www.berouge.com.To increase the capability of identifying simi-lar sentences, all sentences are tokenized, lower-cased and stemmed using the Snowball algorithm(Porter, 2001).Classifier.
On the resulting corpus, an SVMclassifier has been trained using the LIBSVM tool-box (Chang and Lin, 2011).
The selection of thekernel (linear) and the optimization of the param-eters (C=0.8) were carried out through grid searchin 5-fold cross-validation.Labelling the dataset.
Using the best parametersetting obtained, [TGT, PE] and [TGT, RT] pairshave been re-labelled as post-editings or rewrit-ings through 5 rounds of cross-validation.
The fi-nal label of each instance was set to the mode ofthe predictions produced by each cross-validationround.
Since we assume that the quality of the tar-get sentence can be inferred from the amount ofcorrection activity done by the post-editor, the la-bels assigned to the [TGT, PE] pairs represent theresult of our re-annotation of the corpus into posi-tive and negative instances.At the end of the process, of the 1,832 [TGT,PE] pairs of the WMT 2012 training set, 1.394 arelabelled as examples of post-editing (TGT is use-ful), and 438 as examples of complete rewriting(TGT is useless).
Compared to the distributionof positive and negative instances obtained withmost of the partition methods described in Section3, our automatic annotation produces a fairly bal-anced dataset.
The resulting proportion of nega-tive examples (?1:3) is similar to what could bereached only by partitions reflecting a ?soft?
sep-aration into worse versus better translations ratherthan a strict separation into useless versus usefultranslations.10 In Figure 2, the labelling resultsplotted against the HTER show that there is a quiteclear separation between [TGT, PE] pairs markedas post-editings (lower HTER values) and pairsmarked as rewritings (higher HTER values).
Suchseparation corresponds to an HTER value around0.4, which is significantly lower than the thresh-old of 0.7 proposed by the WMT-12 guidelines asa criterion to label sentences for which ?a trans-lation from scratch is necessary?.
This confirmsthat our separation differs from those produced bypartition methods based on human annotations orarbitrary HTER thresholds.
Furthermore, our au-10Such partitions are: average effort scores = 3, humanscores = 3-3-3, HTER score = 0.45.2450 200 400 600 800 1000 1200 1400 1600 1800 200000.10.20.30.40.50.60.70.80.91SentencesHTERTGT?PE labelled as PETGT?PE labelled as RTFigure 2: TGT-PE classification in post-editingsand rewritings.tomatic annotation procedure relies on the contri-bution of features designed to capture different as-pects of the similarity between the TGT and a cor-rect translation, while some of the partition meth-ods discussed in Section 3 rely on thresholds set ona single score (e.g.
HTER).
Considering the manyfacets of the binary QE problem, we expect thatour features are more effective to deal with latentaspects disregarded by such thresholds.5 Experiments and resultsAt this point, the question is: are the automaticallylabelled data more suitable than partitions basedon human labels to train a binary QE classifier?To answer this question, all the proposed separa-tions of the WMT-12 training set have been eval-uated on different test sets.
For each separationwe trained a binary classifier able to assign a label(good or bad) to unseen source-target pairs.
Sincethe classifiers use the same algorithm and featureset, differences in performance will mainly dependon the quality of the training data on which theyare built.
Using task-oriented metrics sensitive tothe number of false positives, results highlightingsuch differences will indicate the best separation.5.1 Experimental SettingBinary QE classifier.
Each separation of theWMT-12 training data was used to train a binarySVM classifier.
Different kernels and parameterswere optimized through a grid search in 5-foldcross-validation on each training set.
Being thenumber of positive and negative training instanceshighly unbalanced, the best models were selectedoptimizing a metric that takes into account thenumber of true and false positives (see below).Seventeen features proposed in (Specia et al2009) were extracted from each source-target pair.This feature set, fully described in (Callison-Burch et al 2012), mainly takes into account thecomplexity of the source sentence (e.g.
numberof tokens, number of translations per source word)and the fluency of the target translation (e.g.
lan-guage model probabilities).
Results of the WMT2012 QE task shown that these ?baseline?
featuresare particularly competitive in the regression task,with only few systems able to beat them.
All thefeatures are extracted using the Quest software11and the model files released by the organizers ofthe WMT 2013 workshop.Test sets.
To obtain different separations be-tween good and bad translations, artificial test setshave been created using arbitrary thresholds onthe HTER (the same used to partition the train-ing set on a HTER basis) and the post-editing time(PET).12 Two different datasets were split: i) theWMT-12 test (422 source, target, post-edited andreference sentences); ii) the WMT-13 training setfor Task 1.3 (800 source, target and post-editedsentences labelled with PET).
The first dataset, themost similar to the WMT-12 training set, shouldbetter reflect (and reward) the HTER-based parti-tions proposed in Section 3.
The WMT-13 datasetcontains sentences translated with a different con-figuration (data and parameters) of the SMT en-gine.
This can result in different HTER-based par-titions in good and bad, useful to test the portabil-ity of our automatic re-annotation method acrossdifferent datasets.
Finally, testing on data parti-tions based on PET allows us to check the stabilityof the automatic re-annotation method when eval-uated on a test set divided according to a differentconcept of translation quality.
In the end, the com-bination of different partition methods, thresholdsand datasets results in 21 different test sets (seeTable 2).Evaluation metrics.
F-score and accuracy arethe classic evaluation metrics used in classifica-tion.
In our evaluation, however, they would al-ways result in high uninformative values due tothe unbalanced nature of the test sets (positive in-stances  negative instances).
In order to bet-11http://www.quest.dcs.shef.ac.uk/12PET is the time spent by a post-editor to transform thetarget into a publishable sentence.246Test instancesWMT-12 HTER Positive Negative0.45 289 1330.5 319 1030.55 352 700.6 371 510.65 386 360.70 398 240.75 406 16WMT-13 Task 1.3 HTER Positive Negative0.45 582 2180.5 622 1780.55 695 1050.6 724 760.65 748 520.70 763 370.75 773 27WMT-13 Task 1.3 PET Positive Negative4 499 3014.16?
517 2834.50 554 2465 594 2066 659 1417 698 1028 727 73Table 2: Number of positive and negative in-stances for each partition of the WMT-12 test setand WMT-13 training set.
?*?
: Average PET com-puted on all the instances in the WMT-13 dataset.ter understand the real quality of the classifica-tion, we hence opted for two task-oriented evalua-tion metrics sensitive to the number of false posi-tives (the main issue in a CAT environment, wherefalse positives and true positives should be re-spectively minimized and maximized).
These are:i) the weighted combination of the false positiverate (FPR) and false discovery rate (FDR) (Ben-jamini and Hochberg, 1995), and ii) the weighedaverage of sensitivity and specificity (also calledbalanced/weighted accuracy).
FPR measures thelevel of false positives, but does not provide infor-mation about the number of true positives.
For thisreason, we combined it with FDR (1-precision),which indirectly controls the level of true posi-tives.
FPR and FDR were equally weighted inthe average; lower values indicate good perfor-mance.
Furthermore, in our scenario it is desir-able to have a classifier with high prediction ac-curacy over the minority class (specificity), whilemaintaining reasonable accuracy for the majorityclass (sensitivity).
Weighted accuracy is useful insuch situations.
To better asses the performance onthe minority (negative) class, we hence gave moreimportance to specificity (0.7 vs 0.3).
As regardsweighted accuracy higher values in indicate bet-ter performance.
Penalizing majority voting clas-sifiers, both metrics are particularly appropriate inour framework.
Besides evaluation, the weightedaverage of FPR and FDR was also used to tune theparameters of the SVM classifier.5.2 ResultsTable 3 presents the results achieved by classifierstrained on different datasets, on the 21 splits pro-duced from the test sets used for evaluation.Although the total number of classifiers testedis 16 (15 resulting from partitions based on humanlabels, and 1 obtained with our automatic annota-tion method), most of them are not present in thetable since they predict the majority class for allthe test points.
These are, in general, trained onhighly unbalanced training sets where the numberof negative samples is really small.
However, itis interesting to note that increasing the numberof instances in the negative class does not alwaysresult in a better classifier.
For instance, the classi-fier built on an HTER separation with threshold at0.55 performs majority voting even if it is built ona more balanced (but probably more noisy) train-ing set than the classifier obtained with thresholdat 0.6.
This suggests that the quality of the sep-aration is as important as the actual proportion ofpositive and negative instances.On all test sets, and for both the evaluation met-rics used, the results achieved by the classifier builtfrom the automatically annotated training set (AA)produces lower error rates (Weighted FPR-FDR)and higher accuracy (Weighted Accuracy), outper-forming all the other classifiers.
The effective-ness of the automatic annotation is confirmed bythe fact that classifiers 3 (based on the averageof effort scores - AES) and 3-3-3 (based on theactual human scores - HS), which are trained onmore balanced training sets, achieve worse perfor-mances than the AA classifier.13Results on the WMT-13 PET test set are not asgood as in the other two test sets.
This shows thattest data labelled in terms of time are more dif-ficult to be correctly classified compared to thosebased on the HTER.
This can be explained consid-ering the intrinsic differences between the HTERand the PET as approximations of the post-editing13The distribution of positive/negative instances in thetraining sets is: 1194/638 for classifier 3, 1360/472 for clas-sifier 3-3-3, 1394/438 for classifier AA.247Weighted Training: WMT-12 SeparationsFPR-FDR 3 2-2-X 2-3-3 3-3-3 0.5 0.6 AAAES HS HS HS HTER HTERTest:WMT-12HTER 0.45 0.61 0.66 0.66 0.66 0.66 0.66 0.550.5 0.57 0.62 0.62 0.62 0.62 0.62 0.490.55 0.52 0.58 0.58 0.58 0.58 0.58 0.420.6 0.5 0.56 0.56 0.56 0.56 0.56 0.40.65 0.5 0.54 0.54 0.54 0.54 0.54 0.390.7 0.49 0.53 0.53 0.53 0.53 0.53 0.390.75 0.49 0.52 0.52 0.52 0.52 0.52 0.35Test:WMT-13HTER 0.45 0.59 0.63 0.63 0.64 0.64 0.63 0.540.5 0.57 0.6 0.6 0.61 0.61 0.6 0.50.55 0.51 0.56 0.56 0.57 0.57 0.56 0.410.6 0.49 0.54 0.54 0.55 0.55 0.54 0.370.65 0.47 0.53 0.53 0.53 0.53 0.53 0.330.7 0.44 0.52 0.52 0.52 0.52 0.52 0.290.75 0.44 0.52 0.52 0.52 0.52 0.52 0.28Test:WMT-13PET4 0.61 0.68 0.68 0.69 0.69 0.68 0.584.16 0.61 0.67 0.67 0.67 0.67 0.67 0.564.5 0.58 0.65 0.64 0.65 0.65 0.65 0.545 0.55 0.63 0.62 0.63 0.63 0.62 0.516 0.49 0.58 0.58 0.58 0.58 0.58 0.457 0.45 0.55 0.55 0.56 0.56 0.55 0.438 0.45 0.54 0.54 0.54 0.54 0.54 0.41Weighted Training: WMT-12 SeparationsAccuracy 3 2-2-X 2-3-3 3-3-3 0.5 0.6 AAAES HS HS HS HTER HTERTest:WMT-12HTER 0.45 0.35 0.3 0.3 0.3 0.3 0.3 0.410.5 0.35 0.3 0.3 0.3 0.3 0.3 0.440.55 0.37 0.3 0.3 0.3 0.3 0.3 0.480.6 0.37 0.3 0.3 0.3 0.3 0.3 0.490.65 0.35 0.3 0.3 0.3 0.3 0.3 0.470.7 0.35 0.3 0.3 0.3 0.3 0.3 0.450.75 0.33 0.3 0.3 0.3 0.3 0.3 0.49Test:WMT-13HTER 0.45 0.33 0.31 0.31 0.3 0.3 0.31 0.40.5 0.34 0.31 0.31 0.3 0.3 0.31 0.420.55 0.35 0.31 0.31 0.3 0.3 0.31 0.480.6 0.35 0.31 0.31 0.3 0.3 0.31 0.510.65 0.36 0.3 0.3 0.3 0.3 0.3 0.540.7 0.39 0.3 0.3 0.3 0.3 0.3 0.560.75 0.38 0.3 0.3 0.3 0.3 0.3 0.59Test:WMT-13PET4 0.37 0.3 0.31 0.3 0.3 0.3 0.44.16 0.37 0.3 0.31 0.3 0.3 0.3 0.44.5 0.37 0.3 0.31 0.3 0.3 0.3 0.45 0.38 0.31 0.31 0.3 0.3 0.31 0.416 0.41 0.31 0.31 0.3 0.3 0.31 0.437 0.42 0.31 0.31 0.3 0.3 0.31 0.448 0.4 0.31 0.31 0.3 0.3 0.31 0.43Table 3: Weighted FPR-FDR (left table) and weighted Accuracy (right table) obtained by the binary QEclassifiers trained on different separations of the WMT-12 training set.
Several arbitrary partitions of theWMT-12 Test set and WMT-13 Training set are considered.effort, as pointed out by several recent works (Spe-cia, 2011; Koponen, 2012).Comparing the results calculated with the twometrics, we note that weighted accuracy seems tobe less sensible to small variations in terms of trueand false negatives returned by the classifier, evenif the specificity (accuracy on our minority class)is weighted more than sensitivity (accuracy on ourmajority class).
This often results in scores veryclose (differences ?
10?3) to the accuracy ob-tained by majority voting classification (0.3).Overall, our experiments demonstrate that theproposed automatic separation method is more ef-fective than arbitrary partitions of datasets anno-tated with subjective human judgements.5.3 Learning CurveOur automatic re-annotation approach requirespost-edited and reference sentences.
Although allthe datasets annotated for QE include post-editedsentences, this is not always true for the refer-ences.
The cost of having both resources is infact not negligible.
For this reason, we investi-gated the minimal number of training data neededto re-annotate the WMT-12 training set withoutaltering performance on binary classification.
Tothis aim, we selected two of the test sets on whichour re-annotation method produces classifiers withhigh performance results (WMT-13 HTER 0.6 and0.75), and measured score variations with increas-ing amounts of data.Nine subsets of the WMT-12 training set cor-pus were created (with 10%, 20%,..., 100% of thedataset) by sub-sampling sentences from a uni-form distribution.
The process was iterated 10times.
Then, for each subset, a new re-annotationprocess was run, the resulting training set was usedto build the relative binary QE classifier, whichwas eventually evaluated on the test set in terms ofweighted FPR-FDR.
Figures 3 and 4 show the ob-tained learning curves.
Each point is the averageresult of the 10 runs; the error bars show ?1std.As can be seen from both curves, performanceresults with 60% of the training data are alreadycomparable with those obtained using the wholetraining data.
Similar trends have been observedfor several learning curves created with differenttest sets.
This shows that, besides avoiding theuse of human labelled data, our approach allowsto drastically reduce the amount of training in-stances.
Considering the high costs of collectingpost-editions, and the fact that reference transla-tions can be taken from parallel corpora, our solu-tion represents a viable way to overcome the lackof training data for binary QE geared towards in-tegration in a CAT environment.2480 0.2 0.4 0.6 0.8 10.360.380.40.420.440.460.480.5Training Set SizeWeighted FPR?FDRFigure 3: Learning curve for WMT-13 HTER 0.60.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.320.370.420.470.52Training Set SizeWeighted FPR?FDRFigure 4: Learning curve for WMT-13 HTER 0.75.6 ConclusionWe presented a task-oriented analysis of the use-fulness of human-labelled data for binary qual-ity estimation.
Our target scenario is computer-assisted translation, which calls for solutions topresent human translators with useful MT sugges-tions (i.e.
easier to correct than to rewrite fromscratch).
Within this framework, the integrationof binary classifiers capable to distinguish ?good?
(useful) from ?bad?
(useless) suggestions wouldmake possible to significantly increase translators?productivity.
Such binary classifiers, however,need labelled training data (possibly of good qual-ity) that are currently not available.An intuitive solution to fill this gap is to takeadvantage of an existing dataset, adapting its man-ual annotations to our task.
Exploring this solu-tion (the first contribution of this paper) has toface problems related to the subjectivity of humanjudgements about translation quality, and the re-sulting variability in the annotation.
In particular,our experiments with the WMT-12 dataset showthat any adaptation (either based on human judge-ments or arbitrarily-set HTER thresholds) collideswith the problem of setting reasonable partitioncriteria.
Our results suggest that the subtle dif-ferences between useful and useless translationsmake subjective human judgements inadequate tolearn effective models.Instead of relying on manually-assigned qual-ity labels, an alternative solution to the problemis to re-annotate an existing dataset.
Proposingan automatic way to do that (the second contri-bution of this paper), we argue that reliable dataseparations into positive and negative examplescan be obtained by measuring the similarities be-tween: i) automatic translations and post-editings,and ii) automatic translations and their references.Our results demonstrate that binary classifiers builtfrom training data produced with our supervisedmethod are less prone to the misclassification ofbad suggestions.As in any supervised learning framework, theamount of data needed to obtain good results is ofcrucial importance.
By analysing the demand ofour automatic annotation method in terms of train-ing data (the third contribution of this paper), weshow that competitive results can be obtained witha fraction of the data needed by methods based onhuman labels.
Our results indicate that a good-quality training set for binary classification canbe obtained with 40% less instances of [training,post edited sentence, reference sentence], totallyavoiding manually-assigned quality judgements.Our future works will address the improvementof the automatic annotation procedure using super-vised methods suitable to learn from unbalancedtraining sets (e.g.
one-class SVM, weighted ran-dom forests), and the integration of new features(e.g.
GTM, meteor) to refine our classification of acorrect sentence into rewritten/post-edited.
Then,to boost binary QE results on the resulting corpora,the ?baseline?
features used for experiments in thispaper will be extended with new features exploredin recent works (Mehdad et al 2012a; de Souzaet al 2013; Turchi and Negri, 2013).AcknowledgmentsThis work has been partially supported by the EC-funded project MateCat (ICT-2011.4.2-287688).249ReferencesNguyen Bach, Fei Huang, and Yaser Al-Onaizan.2011.
Goodness: a Method for Measuring Ma-chine Translation Confidence.
In The 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies, Proceed-ings of the Conference, 19-24 June, 2011, Portland,Oregon, USA, pages 211?219.
The Association forComputer Linguistics.Yoav Benjamini and Yosef Hochberg.
1995.
Control-ling the False Discovery Rate: a Practical and Pow-erful Approach to Multiple Testing.
Journal of theRoyal Statistical Society.
Series B (Methodological),pages 289?300.Fre?de?ric Blain, Holger Schwenk, and Jean Senellart.2012.
Incremental Adaptation Using Translation In-formation and Post-Editing Analysis.
In Interna-tional Workshop on Spoken Language Translation,pages 234?241, Hong-Kong (China).John Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, AlbertoSanchis, and Nicola Ueffing.
2003.
Confidence Es-timation for Machine Translation.
Summer work-shop final report, JHU/CLSP.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Proceedings of the Sev-enth Workshop on Statistical Machine Translation(WMT?12), pages 10?51, Montre?al, Canada.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: a Library for Support Vector Machines.
ACMTrans.
Intell.
Syst.
Technol., 2(3):27:1?27:27, May.Chien-Ying Chen, Jen-Yuan Yeh, and Hao-Ren Ke.2010.
Plagiarism Detection using ROUGE andWordNet.
Journal of Computing, 2(3).Jose?
G. C. de Souza, Miquel Espla`-Gomis, MarcoTurchi, and Matteo Negri.
2013.
Exploiting Quali-tative Information from Automatic Word Alignmentfor Cross-lingual NLP Tasks.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (ACL 2013).Michael Denkowski and Alon Lavie.
2012.
Chal-lenges in Predicting Machine Translation Utilityfor Human Post-Editors.
In Proceedings of AMTA2012.Yifan He, Yanjun Ma, Josef van Genabith, and AndyWay.
2010.
Bridging SMT and TM with TranslationRecommendation.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Pro-ceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 177?180.
Association for Computational Lin-guistics.Philip Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In Proceedingsof Machine Translation Summit X, pages 79?86,Phuket, Thailand.Maarit Koponen.
2012.
Comparing Human Percep-tions of Post-editing Effort with Post-editing Oper-ations.
In Proceedings of the Seventh Workshop onStatistical Machine Translation, pages 181?190.
As-sociation for Computational Linguistics.Michael Lesk.
1986.
Automated Sense Disambigua-tion Using Machine-readable Dictionaries: How toTell a Pine Cone from an Ice Cream Cone.
In Pro-ceedings of the 5th annual international conferenceon Systems documentation (SIGDOC86).Chin-Yew Lin.
2004.
ROUGE: A Package for Auto-matic Evaluation of Summaries.
In Proceedings ofthe ACL workshop on Text Summarization BranchesOut., pages 74?81, Barcelona, Spain.Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-abith.
2011.
Consistent Translation using Discrim-inative Learning: a Translation Memory-inspiredApproach.
Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 1239?1248.Yashar Mehdad, Matteo Negri, and Marcello Federico.2012a.
Detecting semantic equivalence and infor-mation disparity in cross?lingual documents.
InProceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics (ACL?12),pages 120?124, Jeju Island, Korea.Yashar Mehdad, Matteo Negri, and Marcello Fed-erico.
2012b.
Match without a Referee: Evaluat-ing MT Adequacy without Reference Translations.In Proceedings of the Seventh Workshop on Statis-tical Machine Translation, WMT ?12, pages 171?180, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.M.
Porter.
2001.
Snowball: A language for stemmingalgorithms.Martin Potthast, Alberto Barro?n-Ceden?o, AndreasEiselt, Benno Stein, and Paolo Rosso.
2010.Overview of the 2nd International Competition onPlagiarism Detection.
Notebook Papers of CLEF,10.250Christopher B. Quirk.
2004.
Training a Sentence-Level Machine Translation Confidence Measure.
InIn Proceedings of LREC.Matthew Snover, Nitin Madnani, Bonnie J. Dorr, andRichard Schwartz.
2009.
Fluency, Adequacy,or HTER?
: Exploring Different Human Judgmentswith a Tunable MT Metric.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, StatMT ?09, pages 259?268, Stroudsburg, PA,USA.
Association for Computational Linguistics.Radu Soricut and Abdessamad Echihabi.
2010.TrustRank: Inducing Trust in Automatic Transla-tions via Ranking.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 612?621, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Radu Soricut, Nguyen Bach, and Ziyuan Wang.
2012.The SDL language weaver systems in the WMT12quality estimation shared task.
In Proceedings of theSeventh Workshop on Statistical Machine Transla-tion (WMT?12), pages 145?151, Montre?al, Canada.Lucia Specia, Nicola Cancedda, Marc Dymetman,Marco Turchi, and Nello Cristianini.
2009.
Es-timating the Sentence-Level Quality of MachineTranslation Systems.
In Proceedings of the 13thAnnual Conference of the European Associationfor Machine Translation (EAMT?09), pages 28?35,Barcelona, Spain.Lucia Specia, Dhwaj Raj, and Marco Turchi.
2010.Machine Translation Evaluation versus Quality Es-timation.
Machine translation, 24(1):39?50.Lucia Specia, Najeh Hajlaoui, Catalina Hallett, andWilker Aziz.
2011.
Predicting machine transla-tion adequacy.
In Proceedings of the 13th Ma-chine Translation Summit, pages 513?520, Xiamen,China, September.Lucia Specia.
2011.
Exploiting Objective Annota-tions for Measuring Translation Post-editing Effort.pages 73?80.Marco Turchi and Matteo Negri.
2013.
ALTN: WordAlignment Features for Cross-Lingual Textual En-tailment.
Proceedings of the 7th International Work-shop on Semantic Evaluation (SemEval 2013).251
