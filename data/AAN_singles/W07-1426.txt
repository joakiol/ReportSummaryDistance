Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 159?164,Prague, June 2007. c?2007 Association for Computational LinguisticsMachine Learning Based Semantic Inference: Experiments and Ob-servations at RTE-3Baoli Li1, Joseph Irwin1, Ernest V. Garcia2, and Ashwin Ram11 College of ComputingGeorgia Institute of TechnologyAtlanta, GA 30332, USAbaoli@gatech.edugtg519g@mail.gatech.eduashwin@cc.gatech.edu2 Department of RadiologySchool of Medicine, Emory UniversityAtlanta, GA 30322, USAErnest.Garcia@emoryhealthcare.orgAbstractTextual Entailment Recognition is a se-mantic inference task that is required inmany natural language processing (NLP)applications.
In this paper, we present oursystem for the third PASCAL recognizingtextual entailment (RTE-3) challenge.
Thesystem is built on a machine learningframework with the following features de-rived by state-of-the-art NLP techniques:lexical semantic similarity (LSS), namedentities (NE), dependent content word pairs(DEP), average distance (DIST), negation(NG), task (TK), and text length (LEN).
Onthe RTE-3 test dataset, our system achievesthe accuracy of 0.64 and 0.6488 for the twoofficial submissions, respectively.
Experi-mental results show that LSS and NE arethe most effective features.
Further analy-ses indicate that a baseline dummy systemcan achieve accuracy 0.545 on the RTE-3test dataset, which makes RTE-3 relativelyeasier than RTE-2 and RTE-1.
In addition,we demonstrate with examples that the cur-rent Average Precision measure and itsevaluation process need to be changed.1 IntroductionTextual entailment is a relation between two textsnippets in which the meaning of one snippet,called the hypothesis (H), can be inferred from theother snippet, called the text (T).
Textualentailment recognition is the task of decidingwhether a given T entails a given H. An examplepair (pair id 5) from the RTE-3 developmentdataset is as follows:T: A bus collision with a truck in Uganda has resultedin at least 30 fatalities and has left a further 21 injured.H: 30 die in a bus collision in Uganda.Given such a pair, a recognizing textual entail-ment (RTE) system should output its judgementabout whether or not an entailment relation holdsbetween them.
For the above example pair, H isentailed by T.The PASCAL Recognizing Textual EntailmentChallenge is an annual challenge on this taskwhich has been held since 2005 (Dagan et al,2006; Bar-Haim et al 2006).
As textual entailmentrecognition is thought to be a common underlyingsemantic inference task for many natural languageprocessing applications, such as Information Ex-traction (IE), Information Retrieval (IR), QuestionAnswering (QA), and Document Summarization(SUM), the PASCAL RTE Challenge has beengaining more and more attention in the NLP com-munity.
In the past challenges, various approachesto recognizing textual entailment have been pro-posed, from syntactic analysis to logical inference(Bar-Haim et al 2006).As a new participant, we have two goals by at-tending the RTE-3 Challenge: first, we would liketo explore how state-of-the-art language techniqueshelp to deal with this semantic inference problem;second, we try to obtain a more thorough knowl-edge of this research and its state-of-the-art.Inspired by the success of machine learningtechniques in RTE-2, we employ the same strategyin our RTE-3 system.
Several lexical, syntactical,and semantical language analysis techniques are159explored to derive effective features for determin-ing textual entailment relation.
Then, a generalmachine learning algorithm is applied on the trans-formed data for training and prediction.
Our twoofficial submissions achieve accuracy 0.64 and0.6488, respectively.In the rest of this paper we describe the detail ofour system and analyze the results.
Section 2 givesthe overview of our system, while Section 3 dis-cusses the various features in-depth.
We presentour experiments and discussions in Section 4, andconclude in Section 5.2 System DescriptionFigure 1 gives the architecture of our RTE-3 sys-tem, which finishes the process of both trainingand prediction in two stages.
At the first stage, a T-H pair goes through language processing and fea-ture extraction modules, and is finally converted toa set of feature-values.
At the second stage, a ma-chine learning algorithm is applied to obtain aninference/prediction model when training or outputits decision when predicting.In the language processing module, we try toanalyze T-H pairs with the state-of-the-art NLPtechniques, including lexical, syntactical, and se-mantical analyses.
We first split text into sentences,and tag the Part of Speech (POS) of each word.The text with POS information is then fed intothree separate modules: a named entities recog-nizer, a word sense disambiguation (WSD) module,and a dependency parser.
These language analyz-ers output their own intermediate representationsfor the feature extraction module.We produce seven features for each T-H pair:lexical semantic similarity (LSS), named entities(NE), dependent content word pairs (DEP), aver-age distance (DIST), negation (NG), task (TK),and text length (LEN).
The last two features areextracted from each pair itself, while others arebased on the results of language analyzers.The resources that we used in our RTE-3 systeminclude:OAK: a general English analysis tool (Sekine2002).
It is used for sentence splitting, POS tag-ging, and named entities recognition.WordNet::SenseRelate::Allwords package: aword sense disambiguation (WSD) module for as-signing each content word a sense from WordNet(Pedersen et al, 2005).
It is used in WSD module.Figure 1.
System Architecture.WordNet::Similarity package: a Perl modulethat implements a variety of semantic similarityand relatedness measures based on WordNet (Pe-dersen et al, 2005).
This package is used for deriv-ing LSS and DIST features in feature extractionmodule.C&C parser: a powerful CCG parser (Clarkand Curran 2004).
We use C&C parser to obtaindependent content word pairs in dependency pars-ing module.WEKA: the widely used data mining software(Witten&Frank 2005).
We have experimented withseveral machine learning algorithms implementedin WEKA at the second stage.3 FeaturesIn this section, we explain the seven features thatwe employ in our RTE-3 system.3.1 Lexical Semantic Similarity (LSS)Let H={HW 1, HW 2, ?, HW m} be the set of words ina hypothesis, and T={TW 1, TW 2, ?, TW n} the set ofwords in a text, then the lexical semantic similarityfeature LSS for a T-H pair is calculated as the fol-lowing equation:?
?=iiiiiijijHWIDFHWIDFHWHWSSimTWHWSSimMAXTHLSS )())(*)),(),(((),( .
(1)where IDF(w) return the Inverse Document Fre-quency (IDF) value of word w, and SSim is anyfunction for calculating the semantic relatednessbetween two words.
We use WordNet::Similarity160package to calculate the semantic similarity of twocontent words in WordNet (Fellbaum 1998).
Thispackage provides many different semantic related-ness measures.
In our system, we use the Lesk re-latedness measure for function SSim, as it can beused to make comparisons between concepts ofdifferent parts of speech (POS) (Baner-jee&Pedersen, 2002).
Because the value of SSimmay be larger than 1, we normalize the originalvalue from the WordNet::Similarity package toguarantee it fall between 0 and 1.For the words out of WordNet, e.g.
new propernouns, we use the following strategy: if two wordsmatch exactly, the similarity between them is 1;otherwise, the similarity is 0.It needs to be pointed out that Equation (1) is avariant of the text semantic similarity proposed in(Mihalcea et al 2006).
However, in Equation (1),we take into account out of vocabulary words andnormalization for some word-to-word similaritymetrics that may be larger than 1.In addition, we use an IDF dictionary fromMEAD (Radev et al 2001; http://www.summari-zation.com/mead/) for retrieving the IDF value foreach word.
For the words out of the IDF diction-ary, we assign a default value 3.0.3.2 Named Entities (NE)Named Entities are important semantic informationcarriers, which convey more specific informationthan individual component words.
Intuitively, wecan assume that all named entities in a hypothesiswould appear in a textual snippet which entails thehypothesis.
Otherwise, it is very likely that the en-tailment relation in a T-H pair doesn?t hold.
Basedon this assumption, we derive a NE feature foreach T-H pair as follows:????
?>?==.0|)(_|,|)(_||)(_)(_|,0|)(_|,                       1),( HSNEifHSNETSNEHSNEHSNEifTHNEFunction NE_S derives the set of named entitiesfrom a textual snippet.
When we search in T thecounterpart of a named entity in H, we use a loosermatching strategy: if a named entity neA in H isconsumed by a named entity neB in T, neA andneB are thought to be matched.
We use the Englishanalysis tool OAK (Sekine 2002) to recognizenamed entities in textual snippets.3.3 Dependent Content Word Pairs (DEP)With the NE feature, we can capture some localdependency relations between words, but we maymiss many dependency relations expressed in along distance.
These missed long distance depend-ency relations may be helpful for determiningwhether entailment holds between H and T. So, wedesign a DEP feature as follows:????
?>?==.0|)(_|,|)(_||)(_)(_|,0|)(_|,                       1),( HSDEPifHSDEPTSDEPHSDEPHSDEPifTHDEPFunction DEP_S derives the set of dependentcontent word pairs from a textual snippet.
We re-quire that the two content words of each pairshould be dependent directly or linked with at mostone function word.
We use C&C parser (Clark andCurran 2004) to parse the dependency structure ofa textual snippet and then derive the dependentcontent word pairs.
We don?t consider the type ofdependency relation between two linked words.3.4 Average Distance (DIST)The DIST feature measures the distance betweenunmapped tokens in the text.
Adams (2006) uses asimple count of the number of unmapped tokens inthe text that occur between two mapped tokens,scaled to the length of the hypothesis.
Our systemuses a different approach, i.e.
measuring the aver-age length of the gaps between mapped tokens.The number of tokens in the text between eachconsecutive pair of mapped tokens is summed up,and this sum is divided by the number of gaps(equivalent to the number of tokens ?
1).
In thisformula, consecutive mapped tokens in the textcount as gaps of 0, so a prevalence of consecutivemapped tokens lowers the value for this feature.The purpose of this approach is to reduce the effectof long appositives, which may not be mapped tothe hypothesis but should not rule out entailment.3.5 Negation (NG)The Negation feature is very simple.
We simplycount the occurrences of negative words from a listin both the hypothesis (nh) and the text (nt).
The listincludes some common negating affixes.
Then thevalue is:????
?=otherwise 0,parity  samethe have n and n if 1,T)NEG(H, th1613.6 Task (TK)The Task feature is simply the task domain fromwhich the text-hypothesis pair was drawn.
Thevalues are Question Answering (QA), InformationRetrieval (IR), Information Extraction (IE), andMulti-Document Summarization (SUM).3.7 Text Length (LEN)The Text Length feature is drawn directly from thelength attribute of each T-H pair.
Based on thelength of T, its value is either ?short?
or ?long?.4 Experiments and DiscussionsWe run several experiments using various datasetsto train and test models, as well as different com-binations of features.
We also experiment withseveral different machine learning algorithms, in-cluding support vector machine, decision tree, k-nearest neighbor, na?ve bayes, and so on.
Decisiontree algorithm achieves the best results in all ex-periments during development.
Therefore, wechoose to use decision tree algorithm (J48 inWEKA) at the machine learning stage.4.1 RTE-3 DatasetsRTE-3 organizers provide two datasets, i.e.
a de-velopment set and a test set, each consisting of 800T-H pairs.
In both sets pairs are annotated accord-ing to the task the example was drawn from and itslength.
The length annotation is introduced in thisyear?s competition, and has a value of either?long?
or ?short.?
In addition, the development setis annotated as to whether each pair is in an en-tailment relation or not.In order to aid our analysis, we compile somestatistics on the datasets of RTE-3.
Statistics on thedevelopment dataset are given in Table 1, whilethose on the test dataset appear in Table 2.From these two tables, we found the distributionof different kinds of pairs is not balanced in boththe RTE-3 development dataset and the RTE-3 testdataset.
412 entailed pairs appear in the develop-ment dataset, where 410 pairs in the test dataset aremarked as ?YES?.
Thus, the first baseline systemthat outputs all ?YES?
achieves accuracy 0.5125.If we consider task information (IE, IR, QA, andSUM) and assume the two datasets have the same?YES?
and ?NO?
distribution for each task, wewill derive the second baseline system, which canget accuracy 0.5450.
Similarly, if we further con-sider length information (short and long) and as-sume the two datasets have the same ?YES?
and?NO?
distribution for each task with length infor-mation, we will derive the third baseline system,which can also get accuracy 0.5450.Table 1.
Statistical Information of the RTE-3 De-velopment Dataset.Table 2.
Statistical Information of the RTE-3 TestDataset.As different kinds of pairs are evenly distributedin RTE-1 and RTE-2 datasets, the baseline systemfor RTE-1 and RTE-2 that assumes all ?YES?
orall ?NO?
can only achieve accuracy 0.5.
The rela-tively higher baseline performance for RTE-3 data-sets (0.545 vs. 0.5) makes us expect that the aver-age accuracy may be higher than those in previousRTE Challenges.Another observation is that the numbers of longpairs in both datasets are very limited.
OnlyNO 11 1.38% IEYES 17 2.13%NO 22 2.75% IRYES 21 2.63%NO 20 2.50% QAYES 27 3.38%NO 4 0.50%Long(135)SUMYES 13 1.63%NO 80 10.00% IEYES 92 11.50%NO 89 11.13% IRYES 68 8.50%NO 73 9.13% QAYES 80 10.00%NO 89 11.13%Short(665)SUMYES 94 11.75%NO 11 1.38% IEYES 8 1.00%NO 31 3.88% IRYES 23 2.88%NO 13 1.63% QAYES 22 2.75%NO 4 0.50%Long(117)SUMYES 5 0.63%NO 84 10.50% IEYES 97 12.13%NO 82 10.25% IRYES 64 8.00%NO 81 10.13% QAYES 84 10.50%NO 84 10.50%Short(683)SUMYES 107 13.38%16216.88% and 14.63% pairs are long in the develop-ment dataset and the test dataset respectively.4.2 Evaluation MeasuresSystems are evaluated by simple accuracy as inEquation (2); that is, the number of pairs (C) clas-sified correctly over the total number of pairs (N).This score can be further broken down accordingto task.NCAccuracy = .
(2)There is another scoring available for ranked re-sults, Average Precision, which aims to evaluatethe ability of systems to rank all the T-H pairs inthe test set according to their entailment confi-dence (in decreasing order from the most certainentailment to the least certain).
It is calculated as inEquation (3).
?==Ni iiNepiERAvgP1)(*)(1.
(3)Where R is the total number of positive pairs inthe test set, E(i) is 1 if the i-th pair is positive and 0otherwise, and Nep(i) returns the number of posi-tive pairs in the top i pairs.Table 3.
Our Official RTE-3 Run Results.4.3 Official RTE-3 ResultsThe official results for our system are shown inTable 3.
For our first run, the model was trained onall the datasets from the two previous challenges aswell as the RTE-3 development set, using only theLSS, NE, and TK features.
This feature combina-tion achieves the best performance on the RTE-3development dataset in our experiments.
For thesecond run, the model was trained only on theRTE-3 development dataset, but adding other twofeatures LEN and DIST.
We hope these two fea-tures may be helpful for differentiating pairs withdifferent length.RUN2 with five features achieves better resultsthan RUN1.
It performs better on IE, QA and SUMtasks than RUN1, but poorer on IR task.
Both runsobtain the best performance on QA task, and per-form very poor on IE task.
For the IE task itself, abaseline system can get accuracy 0.525.
RUN1cannot beat this baseline system on IE task, whileRUN2 only has a trivial advantage over it.
In fur-ther analysis on the detailed results, we found thatour system tends to label all IE pairs as entailedones, because most of the IE pairs exhibit higherlexical overlapping between T and H. In our opin-ion, word order and long syntactic structures maybe helpful for dealing with IE pairs.
We will ex-plore this idea and other methods to improve RTEsystems on IE pairs in our future research.Table 4.
Accuracy by task and selected feature seton the RTE-3 Test dataset (Trained on the RTE-3development dataset).4.4 Discussions4.4.1 Feature AnalysisTable 4 lays out the results of using various featurecombinations to train the classifier.
All of themodels were trained on the RTE 3 developmentdataset only.It is obvious that the LSS and NE features havethe most utility.
The DIST and LEN features seemuseless for this dataset, as these features them-selves can not beat the baseline system with accu-racy 0.545.
Systems with individual features per-form similarly on SUM pairs except NG, and on IEpairs except NG and DEP features.
However, onIR and QA pairs, they behave quite differently.
Forexample, system with NE feature achieves accu-racy 0.78 on QA pairs, while system with DEPfeature obtains 0.575.
NE and LSS features havesimilar effects, but NE is more useful for QA pairs.Accuracy by TaskRUN Overall Accuracy IE IR QA SUM1 0.6400 0.5100 0.6600 0.7950 0.59502 0.6488 0.5300 0.6350 0.8050 0.6250Accuracy by TaskFeature SetIE IR QA SUMAcc.LSS 0.530 0.660 0.720 0.595 0.6263NE 0.520 0.620 0.780 0.580 0.6250DEP 0.495 0.625 0.575 0.570 0.5663TK 0.525 0.565 0.530 0.560 0.5450DIST 0.525 0.435 0.530 0.560 0.5125NG 0.555 0.505 0.590 0.535 0.5463LEN 0.525 0.435 0.530 0.560 0.5125LSS+NE 0.525 0.645 0.805 0.585 0.6400LSS+NE+DEP 0.520 0.650 0.810 0.580 0.6400LSS+NE+TK 0.530 0.625 0.805 0.595 0.6388LSS+NE+TK+LEN 0.530 0.630 0.805 0.625 0.6475LSS+NE+TK+DEP 0.530 0.625 0.805 0.620 0.6450LSS+NE+TK+DEP+NG 0.460 0.625 0.785 0.655 0.6313LSS+NE+TK+LEN+DEP 0.525 0.615 0.790  0.600 0.6325LSS+NE+TK+LEN+DIST(run2) 0.530 0.635 0.805 0.625 0.6488All Features 0.500 0.590 0.790 0.630 0.6275163It is interesting to note that some features im-prove the score in some combinations, but in oth-ers they decrease it.
For instance, although DEPscores above the baseline at 0.5663, when added tothe combination of LSS, NE, TK, and LEN it low-ers the overall accuracy by 1.5%.4.4.2 About Average Precision MeasureAs we mentioned in section 4.2, Average Precision(AvgP) is expected to evaluate the ranking abilityof a system according to confidence values.
How-ever, we found that the current evaluation processand the measure itself have some problems andneed to be modified for RTE evaluation.On one hand, the current evaluation processdoesn?t consider tied cases where many pairs mayhave the same confidence value.
It is reasonable toassume that the order of tied pairs will be random.Accordingly, the derived Average Precision willvary.Let?s look at a simple example: suppose wehave two pairs c and d, and c is the only one posi-tive entailment pair.
Here, R=1, N=2 for Equation(3).
Two systems X and Y output ranked results as{c, d} and {d,c} respectively.
According to Equa-tion (3), the AvgP value of system X is 1, wherethat of system Y is 0.5.
If these two systems assignsame confidence value for both pairs, we can notconclude that system X is better than system Y.To avoid this problem, we suggest requiring thateach system for ranked submission output its con-fidence for each pair.
Then, when calculating Av-erage Precision measure, we first re-rank the listwith these confidence values and true answers foreach pair.
For tied pairs, we rank pairs with trueanswer ?NO?
before those with positive entailmentrelation.
By this way, we can produce a stable andmore reasonable Average Precision value.
For ex-ample, in the above example, the modified averageprecisions for both systems will be 0.5.On the other hand, from the Equation (3), weknow that the upper bound of Average Precision is1.
At the same time, we can also derive a lowerbound for this measure as in Equation (4).
It corre-sponds to the worst system which places all thenegative pairs before all the positive pairs.
Thelower bound of Average Precision for RTE-3 testdataset is 0.3172.??
?=?=101_Rj jNjRRAvgPLB .
(4)As the values of N and R change, the lowerbound of Average Precision will vary.
Therefore,the original Average Precision measure as in Equa-tion (3) is not an ideal one for comparison acrossdatasets.To solve this problem, we propose a normalizedAverage Precision measure as in Equation (5).AvgPLBAvgPLBAvgPAvgPNorm_1__?
?= .
(5)5 Conclusion and Future WorkIn this paper, we report our RTE-3 system.
Thesystem was built on a machine learning frameworkwith features produced by state-of-the-art NLPtechniques.
Lexical semantic similarity and Namedentities are the two most effective features.
Dataanalysis shows a higher baseline performance forRTE-3 than RTE-1 and RTE-2, and the currentAverage Precision measure needs to be changed.As T-H pairs from IE task are the most difficultones, we will focus on these pairs in our future re-search.ReferencesRod Adams.
2006.
Textual Entailment Through Extended LexicalOverlap.
In Proceedings of RTE-2 Workshop.Satanjeev Banerjee and Ted Pedersen.
2002.
An Adapted Lesk Algo-rithm for Word Sense Disambiguation Using WordNet.
In Pro-ceedings of CICLING-02.Roy Bar-Haim et al 2006.
The Second PASCAL Recognising TextualEntailment Challenge.
In Proceedings of RTE-2 Workshop.Stephen Clark and James R. Curran.
2004.
Parsing the WSJ usingCCG and Log-Linear Models.
In Proceedings of ACL-04.Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006.
The PAS-CAL Recognising Textual Entailment Challenge.
In Qui?onero-Candela et al (editors.
), MLCW 2005, LNAI Volume 3944.Christiane Fellbaum.
1998.
WordNet: an Electronic Lexical Database.MIT Press.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006.
Cor-pus-based and Knowledge-based Measures of Text Semantic Simi-larity.
In Proceedings of AAAI-06.Ted Pedersen et al 2005.
Maximizing Semantic Relatedness to Per-form Word Sense Disambiguation.
Research Report UMSI2005/25, Supercomputing Institute, University of Minnesota.Dragomir Radev, Sasha Blair-Goldensohn, and ZhuZhang.
2001.Experiments in single and multidocument summarization usingMEAD.
In Proceedings of DUC 2001.Satoshi Sekine.
2002.
Manual of Oak System (version 0.1).
ComputerScience Department, New York University,http://nlp.cs.nyu.edu/oak.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Practical machinelearning tools and techniques.
Morgan Kaufmann, San Francisco.164
