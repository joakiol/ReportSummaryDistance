Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 304?311,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsBootstrapping Word Alignment via Word PackingYanjun Ma, Nicolas Stroppa, Andy WaySchool of ComputingDublin City UniversityGlasnevin, Dublin 9, Ireland{yma,nstroppa,away}@computing.dcu.ieAbstractWe introduce a simple method to pack wordsfor statistical word alignment.
Our goal is tosimplify the task of automatic word align-ment by packing several consecutive wordstogether when we believe they correspondto a single word in the opposite language.This is done using the word aligner itself,i.e.
by bootstrapping on its output.
Weevaluate the performance of our approachon a Chinese-to-English machine translationtask, and report a 12.2% relative increase inBLEU score over a state-of-the art phrase-based SMT system.1 IntroductionAutomatic word alignment can be defined as theproblem of determining a translational correspon-dence at word level given a parallel corpus of alignedsentences.
Most current statistical models (Brownet al, 1993; Vogel et al, 1996; Deng and Byrne,2005) treat the aligned sentences in the corpus as se-quences of tokens that are meant to be words; thegoal of the alignment process is to find links be-tween source and target words.
Before applyingsuch aligners, we thus need to segment the sentencesinto words ?
a task which can be quite hard for lan-guages such as Chinese for which word boundariesare not orthographically marked.
More importantly,however, this segmentation is often performed in amonolingual context, which makes the word align-ment task more difficult since different languagesmay realize the same concept using varying num-bers of words (see e.g.
(Wu, 1997)).
Moreover, asegmentation considered to be ?good?
from a mono-lingual point of view may be unadapted for trainingalignment models.Although some statistical alignment models al-low for 1-to-n word alignments for those reasons,they rarely question the monolingual tokenizationand the basic unit of the alignment process remainsthe word.
In this paper, we focus on 1-to-n align-ments with the goal of simplifying the task of auto-matic word aligners by packing several consecutivewords together when we believe they correspond to asingle word in the opposite language; by identifyingenough such cases, we reduce the number of 1-to-nalignments, thus making the task of word alignmentboth easier and more natural.Our approach consists of using the output froman existing statistical word aligner to obtain a set ofcandidates for word packing.
We evaluate the re-liability of these candidates, using simple metricsbased on co-occurence frequencies, similar to thoseused in associative approaches to word alignment(Kitamura and Matsumoto, 1996; Melamed, 2000;Tiedemann, 2003).
We then modify the segmenta-tion of the sentences in the parallel corpus accord-ing to this packing of words; these modified sen-tences are then given back to the word aligner, whichproduces new alignments.
We evaluate the validityof our approach by measuring the influence of thealignment process on a Chinese-to-English MachineTranslation (MT) task.The remainder of this paper is organized as fol-lows.
In Section 2, we study the case of 1-to-n word alignment.
Section 3 introduces an auto-matic method to pack together groups of consecutive3041: 0 1: 1 1: 2 1: 3 1:n (n > 3)IWSLT Chinese?English 21.64 63.76 9.49 3.36 1.75IWSLT English?Chinese 29.77 57.47 10.03 1.65 1.08IWSLT Italian?English 13.71 72.87 9.77 3.23 0.42IWSLT English?Italian 20.45 71.08 7.02 0.9 0.55Europarl Dutch?English 24.71 67.04 5.35 1.4 1.5Europarl English?Dutch 23.76 69.07 4.85 1.2 1.12Table 1: Distribution of alignment types for different language pairs (%)words based on the output from a word aligner.
InSection 4, the experimental setting is described.
InSection 5, we evaluate the influence of our methodon the alignment process on a Chinese to EnglishMT task, and experimental results are presented.Section 6 concludes the paper and gives avenues forfuture work.2 The Case of 1-to-n AlignmentThe same concept can be expressed in different lan-guages using varying numbers of words; for exam-ple, a single Chinese word may surface as a com-pound or a collocation in English.
This is fre-quent for languages as different as Chinese and En-glish.
To quickly (and approximately) evaluate thisphenomenon, we trained the statistical IBM word-alignment model 4 (Brown et al, 1993),1 using theGIZA++ software (Och and Ney, 2003) for the fol-lowing language pairs: Chinese?English, Italian?English, and Dutch?English, using the IWSLT-2006corpus (Takezawa et al, 2002; Paul, 2006) for thefirst two language pairs, and the Europarl corpus(Koehn, 2005) for the last one.
These asymmet-ric models produce 1-to-n alignments, with n ?
0,in both directions.
Here, it is important to mentionthat the segmentation of sentences is performed to-tally independently of the bilingual alignment pro-cess, i.e.
it is done in a monolingual context.
For Eu-ropean languages, we apply the maximum-entropybased tokenizer of OpenNLP2; the Chinese sen-tences were human segmented (Paul, 2006).In Table 1, we report the frequencies of the dif-ferent types of alignments for the various languagesand directions.
As expected, the number of 1:n1More specifically, we performed 5 iterations of Model 1, 5iterations of HMM, 5 iterations of Model 3, and 5 iterations ofModel 4.2http://opennlp.sourceforge.net/.alignments with n 6= 1 is high for Chinese?English(' 40%), and significantly higher than for the Eu-ropean languages.
The case of 1-to-n alignments is,therefore, obviously an important issue when deal-ing with Chinese?English word alignment.32.1 The Treatment of 1-to-n AlignmentsFertility-based models such as IBM models 3, 4, and5 allow for alignments between one word and sev-eral words (1-to-n or 1:n alignments in what fol-lows), in particular for the reasons specified above.They can be seen as extensions of the simpler IBMmodels 1 and 2 (Brown et al, 1993).
Similarly,Deng and Byrne (2005) propose an HMM frame-work capable of dealing with 1-to-n alignment,which is an extension of the original model of (Vogelet al, 1996).However, these models rarely question the mono-lingual tokenization, i.e.
the basic unit of the align-ment process is the word.4 One alternative to ex-tending the expressivity of one model (and usuallyits complexity) is to focus on the input representa-tion; in particular, we argue that the alignment pro-cess can benefit from a simplification of the input,which consists of trying to reduce the number of1-to-n alignments to consider.
Note that the needto consider segmentation and alignment at the sametime is also mentioned in (Tiedemann, 2003), andrelated issues are reported in (Wu, 1997).2.2 NotationWhile in this paper, we focus on Chinese?English,the method proposed is applicable to any language3Note that a 1: 0 alignment may denote a failure to capturea 1:n alignment with n > 1.4Interestingly, this is actually even the case for approachesthat directly model alignments between phrases (Marcu andWong, 2002; Birch et al, 2006).305pair ?
even for closely related languages, we ex-pect improvements to be seen.
The notation how-ever assume Chinese?English MT.
Given a Chi-nese sentence cJ1 consisting of J words {c1, .
.
.
, cJ}and an English sentence eI1 consisting of I words{e1, .
.
.
, eI}, AC?E (resp.
AE?C) will denote aChinese-to-English (resp.
an English-to-Chinese)word alignment between cJ1 and eI1.
Since we areprimarily interested in 1-to-n alignments, AC?Ecan be represented as a set of pairs aj = ?cj , Ej?denoting a link between one single Chinese wordcj and a few English words Ej (and similarly forAE?C).
The set Ej is empty if the word cj is notaligned to any word in eI1.3 Automatic Word RepackingOur approach consists of packing consecutive wordstogether when we believe they correspond to a sin-gle word in the other language.
This bilinguallymotivated packing of words changes the basic unitof the alignment process, and simplifies the task ofautomatic word alignment.
We thus minimize thenumber of 1-to-n alignments in order to obtain morecomparable segmentations in the two languages.
Inthis section, we present an automatic method thatbuilds upon the output from an existing automaticword aligner.
More specifically, we (i) use a wordaligner to obtain 1-to-n alignments, (ii) extract can-didates for word packing, (iii) estimate the reliabilityof these candidates, (iv) replace the groups of wordsto pack by a single token in the parallel corpus, and(v) re-iterate the alignment process using the up-dated corpus.
The first three steps are performedin both directions, and produce two bilingual dic-tionaries (source-target and target-source) of groupsof words to pack.3.1 Candidate ExtractionIn the following, we assume the availability of anautomatic word aligner that can output alignmentsAC?E and AE?C for any sentence pair (cJ1 , eI1)in a parallel corpus.
We also assume that AC?Eand AE?C contain 1:n alignments.
Our method forrepacking words is very simple: whenever a singleword is aligned with several consecutive words, theyare considered candidates for repacking.
Formally,given an alignment AC?E between cJ1 and eI1, ifaj = ?cj , Ej?
?
AC?E , with Ej = {ej1 , .
.
.
, ejm}and ?k ?
J1,m?
1K, jk+1 ?
jk = 1, then the align-ment aj between cj and the sequence of words Ejis considered a candidate for word repacking.
Thesame goes for AE?C .
Some examples of such 1-to-n alignments between Chinese and English (inboth directions) we can derive automatically are dis-played in Figure 1.????
: white wine????
: department store??
: excuse me??
: call the police?
: cup of??
: have toclosest: ?
?fifteen: ?
?fine: ?
?flight: ?
?
?get: ?
?here:  ?
?
?Figure 1: Example of 1-to-n word alignments be-tween Chinese and English3.2 Candidate Reliability EstimationOf course, the process described above is error-prone and if we want to change the input to give tothe word aligner, we need to make sure that we arenot making harmful modifications.5 We thus addi-tionally evaluate the reliability of the candidates weextract and filter them before inclusion in our bilin-gual dictionary.
To perform this filtering, we usetwo simple statistical measures.
In the following,aj = ?cj , Ej?
denotes a candidate.The first measure we consider is co-occurrencefrequency (COOC(cj , Ej)), i.e.
the number oftimes cj and Ej co-occur in the bilingual corpus.This very simple measure is frequently used in as-sociative approaches (Melamed, 1997; Tiedemann,2003).
The second measure is the alignment confi-dence, defined asAC(aj) =C(aj)COOC(cj , Ej),where C(aj) denotes the number of alignments pro-posed by the word aligner that are identical to aj .In other words, AC(aj) measures how often the5Consequently, if we compare our approach to the problemof collocation identification, we may say that we are more in-terested in precision than recall (Smadja et al, 1996).
However,note that our goal is not recognizing specific sequences of wordssuch as compounds or collocations; it is making (bilinguallymotivated) changes that simplify the alignment process.306aligner aligns cj and Ej when they co-occur.
Wealso impose that |Ej | ?
k, where k is a fixed inte-ger that may depend on the language pair (between3 and 5 in practice).
The rationale behind this is thatit is very rare to get reliable alignment between oneword and k consecutive words when k is high.The candidates are included in our bilingual dic-tionary if and only if their measures are above somefixed thresholds tcooc and tac, which allow for thecontrol of the size of the dictionary and the qualityof its contents.
Some other measures (including theDice coefficient) could be considered; however, ithas to be noted that we are more interested here inthe filtering than in the discovery of alignment, sinceour method builds upon an existing aligner.
More-over, we will see that even these simple measurescan lead to an improvement of the alignment pro-cess in a MT context (cf.
Section 5).3.3 Bootstrapped Word RepackingOnce the candidates are extracted, we repack thewords in the bilingual dictionaries constructed usingthe method described above; this provides us withan updated training corpus, in which some word se-quences have been replaced by a single token.
Thisupdate is totally naive: if an entry aj = ?cj , Ej?
ispresent in the dictionary and matches one sentencepair (cJ1 , eI1) (i.e.
cj and Ej are respectively con-tained in cJ1 and eI1), then we replace the sequenceof words Ej with a single token which becomes anew lexical unit.6 Note that this replacement occurseven if no alignment was found between cj and Ejfor the pair (cJ1 , eI1).
This is motivated by the factthat the filtering described above is quite conserva-tive; we trust the entry ai to be correct.
This updateis performed in both directions.
It is then possible torun the word aligner using the updated (simplified)parallel corpus, in order to get new alignments.
Byperforming a deterministic word packing, we avoidthe computation of the fertility parameters associ-ated with fertility-based models.Word packing can be applied several times: oncewe have grouped some words together, they becomethe new basic unit to consider, and we can re-runthe same method to get additional groupings.
How-6In case of overlap between several groups of words to re-place, we select the one with highest confidence (according totac).ever, we have not seen in practice much benefit fromrunning it more than twice (few new candidates areextracted after two iterations).It is also important to note that this process isbilingually motivated and strongly depends on thelanguage pair.
For example, white wine, excuse me,call the police, and cup of (cf.
Figure 1) translate re-spectively as vin blanc, excusez-moi, appellez la po-lice, and tasse de in French.
Those groupings wouldnot be found for a language pair such as French?English, which is consistent with the fact that theyare less useful for French?English than for Chinese?English in a MT perspective.3.4 Using Manually Developed DictionariesWe wanted to compare this automatic approach tomanually developed resources.
For this purpose,we used a dictionary built by the MT group ofHarbin Institute of Technology, as a preprocessingstep to Chinese?English word alignment, and moti-vated by several years of Chinese?English MT prac-tice.
Some examples extracted from this resourceare displayed in Figure 2.?
: whiti en??
:?dp aw wr??
: aiim arw??
: ea str aw rs?
: pn nrra  pn?
: orrx pwFigure 2: Examples of entries from the manually de-veloped dictionary4 Experimental Setting4.1 EvaluationThe intrinsic quality of word alignment can be as-sessed using the Alignment Error Rate (AER) met-ric (Och and Ney, 2003), that compares a system?salignment output to a set of gold-standard align-ment.
While this method gives a direct evaluation ofthe quality of word alignment, it is faced with sev-eral limitations.
First, it is really difficult to builda reliable and objective gold-standard set, especiallyfor languages as different as Chinese and English.Second, an increase in AER does not necessarily im-ply an improvement in translation quality (Liang etal., 2006) and vice-versa (Vilar et al, 2006).
The307relationship between word alignments and their im-pact on MT is also investigated in (Ayan and Dorr,2006; Lopez and Resnik, 2006; Fraser and Marcu,2006).
Consequently, we chose to extrinsically eval-uate the performance of our approach via the transla-tion task, i.e.
we measure the influence of the align-ment process on the final translation output.
Thequality of the translation output is evaluated usingBLEU (Papineni et al, 2002).4.2 DataThe experiments were carried out using theChinese?English datasets provided within theIWSLT 2006 evaluation campaign (Paul, 2006), ex-tracted from the Basic Travel Expression Corpus(BTEC) (Takezawa et al, 2002).
This multilingualspeech corpus contains sentences similar to thosethat are usually found in phrase-books for touristsgoing abroad.
Training was performed using the de-fault training set, to which we added the sets de-vset1, devset2, and devset3.7 The English side ofthe test set was not available at the time we con-ducted our experiments, so we split the developmentset (devset 4) into two parts: one was kept for testing(200 aligned sentences) with the rest (289 alignedsentences) used for development purposes.As a pre-processing step, the English sentenceswere tokenized using the maximum-entropy basedtokenizer of the OpenNLP toolkit, and case infor-mation was removed.
For Chinese, the data pro-vided were tokenized according to the output formatof ASR systems, and human-corrected (Paul, 2006).Since segmentations are human-corrected, we aresure that they are good from a monolingual point ofview.
Table 2 contains the various corpus statistics.4.3 BaselineWe use a standard log-linear phrase-based statisticalmachine translation system as a baseline: GIZA++implementation of IBM word alignment model 4(Brown et al, 1993; Och and Ney, 2003),8 the re-finement and phrase-extraction heuristics describedin (Koehn et al, 2003), minimum-error-rate training7More specifically, we choose the first English referencefrom the 7 references and the Chinese sentence to construct newsentence pairs.8Training is performed using the same number of iterationsas in Section 2.Chinese EnglishTrain Sentences 41,465Running words 361,780 375,938Vocabulary size 11,427 9,851Dev.
Sentences 289 (7 refs.
)Running words 3,350 26,223Vocabulary size 897 1,331Eval.
Sentences 200 (7 refs.
)Running words 1,864 14,437Vocabulary size 569 1,081Table 2: Chinese?English corpus statistics(Och, 2003) using Phramer (Olteanu et al, 2006),a 3-gram language model with Kneser-Ney smooth-ing trained with SRILM (Stolcke, 2002) on the En-glish side of the training data and Pharaoh (Koehn,2004) with default settings to decode.
The log-linearmodel is also based on standard features: condi-tional probabilities and lexical smoothing of phrasesin both directions, and phrase penalty (Zens andNey, 2004).5 Experimental Results5.1 ResultsThe initial word alignments are obtained using thebaseline configuration described above.
From these,we build two bilingual 1-to-n dictionaries (one foreach direction), and the training corpus is updatedby repacking the words in the dictionaries, using themethod presented in Section 2.
As previously men-tioned, this process can be repeated several times; ateach step, we can also choose to exploit only one ofthe two available dictionaries, if so desired.
We thenextract aligned phrases using the same procedure asfor the baseline system; the only difference is the ba-sic unit we are considering.
Once the phrases are ex-tracted, we perform the estimation of the features ofthe log-linear model and unpack the grouped wordsto recover the initial words.
Finally, minimum-error-rate training and decoding are performed.The various parameters of the method (k, tcooc,tac, cf.
Section 2) have been optimized on the devel-opment set.
We found out that it was enough to per-form two iterations of repacking: the optimal set ofvalues was found to be k = 3, tac = 0.5, tcooc = 20for the first iteration, and tcooc = 10 for the second308BLEU[%]Baseline 15.14n=1.
with C-E dict.
15.92n=1.
with E-C dict.
15.77n=1.
with both 16.59n=2.
with C-E dict.
16.99n=2.
with E-C dict.
16.59n=2.
with both 16.88Table 3: Influence of word repacking on Chinese-to-English MTiteration, for both directions.9 In Table 3, we reportthe results obtained on the test set, where n denotesthe iteration.
We first considered the inclusion ofonly the Chinese?English dictionary, then only theEnglish?Chinese dictionary, and then both.After the first step, we can already see an im-provement over the baseline when considering oneof the two dictionaries.
When using both, we ob-serve an increase of 1.45 BLEU points, which cor-responds to a 9.6% relative increase.
Moreover, wecan gain from performing another step.
However,the inclusion of the English?Chinese dictionary isharmful in this case, probably because 1-to-n align-ments are less frequent for this direction, and havebeen captured during the first step.
By including theChinese?English dictionary only, we can achieve anincrease of 1.85 absolute BLEU points (12.2% rela-tive) over the initial baseline.10Quality of the Dictionaries To assess the qual-ity of the extraction procedure, we simply manu-ally evaluated the ratio of incorrect entries in thedictionaries.
After one step of word packing, theChinese?English and the English?Chinese dictio-naries respectively contain 7.4% and 13.5% incor-rect entries.
After two steps of packing, they onlycontain 5.9% and 10.3% incorrect entries.5.2 Alignment TypesIntuitively, the word alignments obtained after wordpacking are more likely to be 1-to-1 than before.
In-9The parameters k, tac, and tcooc are optimized for eachstep, and the alignment obtained using the best set of parametersfor a given step are used as input for the following step.10Note that this setting (using both dictionaries for the firststep and only the Chinese dictionary for the second step) is alsothe best setting on the development set.deed, the word sequences in one language that usu-ally align to one single word in the other languagehave been grouped together to form one single to-ken.
Table 4 shows the detail of the distribution ofalignment types after one and two steps of automaticrepacking.
In particular, we can observe that the 1: 11: 0 1: 1 1: 2 1: 3 1:n(n > 3)C-E Base.
21.64 63.76 9.49 3.36 1.75n=1 19.69 69.43 6.32 2.79 1.78n=2 19.67 71.57 4.87 2.12 1.76E-C Base.
29.77 57.47 10.03 1.65 1.08n=1 26.59 61.95 8.82 1.55 1.09n=2 25.10 62.73 9.38 1.68 1.12Table 4: Distribution of alignment types (%)alignments are more frequent after the applicationof repacking: the ratio of this type of alignment hasincreased by 7.81% for Chinese?English and 5.26%for English?Chinese.5.3 Influence of Word SegmentationTo test the influence of the initial word segmenta-tion on the process of word packing, we consideredan additional segmentation configuration, based onan automatic segmenter combining rule-based andstatistical techniques (Zhao et al, 2001).BLEU[%]Original segmentation 15.14Original segmentation + Word packing 16.99Automatic segmentation 14.91Automatic segmentation + Word packing 17.51Table 5: Influence of Chinese segmentationThe results obtained are displayed in Table 5.
Asexpected, the automatic segmenter leads to slightlylower results than the human-corrected segmenta-tion.
However, the proposed method seems to bebeneficial irrespective of the choice of segmentation.Indeed, we can also observe an improvement in thenew setting: 2.6 points absolute increase in BLEU(17.4% relative).1111We could actually consider an extreme case, which wouldconsist of splitting the sentences into characters, i.e.
each char-acter would be blindly treated as one word.
The segmentation3095.4 Exploiting Manually Developed ResourcesWe also compared our technique for automatic pack-ing of words with the exploitation of manuallydeveloped resources.
More specifically, we useda 1-to-n Chinese?English bilingual dictionary, de-scribed in Section 3.4, and used it in place of theautomatically acquired dictionary.
Words are thusgrouped according to this dictionary, and we thenapply the same word aligner as for previous experi-ments.
In this case, since we are not bootstrappingfrom the output of a word aligner, this can actuallybe seen as a pre-processing step prior to alignment.These resources follow more or less the same for-mat as the output of the word segmenter mentionedin Section 5.1.2 (Zhao et al, 2001), so the experi-ments are carried out using this segmentation.BLEU[%]Baseline 14.91Automatic word packing 17.51Packing with ?manual?
dictionary 16.15Table 6: Exploiting manually developed resourcesThe results obtained are displayed in Table 6.Wecan observe that the use of the manually developeddictionary provides us with an improvement in trans-lation quality: 1.24 BLEU points absolute (8.3% rel-ative).
However, there does not seem to be a cleargain when compared with the automatic method.Even if those manual resources were extended, wedo not believe the improvement is sufficient enoughto justify this additional effort.6 Conclusion and Future WorkIn this paper, we have introduced a simple yet effec-tive method to pack words together in order to givea different and simplified input to automatic wordaligners.
We use a bootstrap approach in which wefirst extract 1-to-n word alignments using an exist-ing word aligner, and then estimate the confidenceof those alignments to decide whether or not the nwords have to be grouped; if so, this group is con-would thus be completely driven by the bilingual alignment pro-cess (see also (Wu, 1997; Tiedemann, 2003) for related consid-erations).
In this case, our approach would be similar to theapproach of (Xu et al, 2004), except for the estimation of can-didates.sidered a new basic unit to consider.
We can finallyre-apply the word aligner to the updated sentences.We have evaluated the performance of our ap-proach by measuring the influence of this processon a Chinese-to-English MT task, based on theIWSLT 2006 evaluation campaign.
We report a12.2% relative increase in BLEU score over a stan-dard phrase-based SMT system.
We have verifiedthat this process actually reduces the number of 1:nalignments with n 6= 1, and that it is rather indepen-dent from the (Chinese) segmentation strategy.As for future work, we first plan to consider dif-ferent confidence measures for the filtering of thealignment candidates.
We also want to bootstrap ondifferent word aligners; in particular, one possibilityis to use the flexible HMM word-to-phrase model ofDeng and Byrne (2005) in place of IBM model 4.Finally, we would like to apply this method to othercorpora and language pairs.AcknowledgmentThis work is supported by Science Foundation Ire-land (grant number OS/IN/1732).
Prof. Tiejun Zhaoand Dr. Muyun Yang from the MT group of HarbinInstitute of Technology, and Yajuan Lv from the In-stitute of Computing Technology, Chinese Academyof Sciences, are kindly acknowledged for provid-ing us with the Chinese segmenter and the manuallydeveloped bilingual dictionary used in our experi-ments.ReferencesNecip Fazil Ayan and Bonnie J. Dorr.
2006.
Going be-yond aer: An extensive analysis of word alignmentsand their impact on mt.
In Proceedings of COLING-ACL 2006, pages 9?16, Sydney, Australia.Alexandra Birch, Chris Callison-Burch, and Miles Os-borne.
2006.
Constraining the phrase-based, jointprobability statistical translation model.
In Proceed-ings of AMTA 2006, pages 10?18, Boston, MA.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Yonggang Deng and William Byrne.
2005.
HMM wordand phrase alignment for statistical machine transla-tion.
In Proceedings of HLT-EMNLP 2005, pages169?176, Vancouver, Canada.310Alexander Fraser and Daniel Marcu.
2006.
Measuringword alignment quality for statistical machine transla-tion.
Technical Report ISI-TR-616, ISI/University ofSouthern California.Mihoko Kitamura and Yuji Matsumoto.
1996.
Auto-matic extraction of word sequence correspondences inparallel corpora.
In Proceedings of the 4th Workshopon Very Large Corpora, pages 79?87, Copenhagen,Denmark.Philip Koehn, Franz Och, and Daniel Marcu.
2003.
Sta-tistical phrase-based translation.
In Proceedings ofHLT-NAACL 2003, pages 48?54, Edmonton, Canada.Philip Koehn.
2004.
Pharaoh: A beam search decoderfor phrase-based statistical machine translation mod-els.
In Proceedings of AMTA 2004, pages 115?124,Washington, District of Columbia.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Machine Transla-tion Summit X, pages 79?86, Phuket, Thailand.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of HLT-NAACL2006, pages 104?111, New York, NY.Adam Lopez and Philip Resnik.
2006.
Word-basedalignment, phrase-based translation: What?s the link?In Proceedings of AMTA 2006, pages 90?99, Cam-bridge, MA.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In Proceedings of EMNLP 2002, pages 133?139,Morristown, NJ.I.
Dan Melamed.
1997.
Automatic discovery of non-compositional compounds in parallel data.
In Pro-ceedings of EMNLP 1997, pages 97?108, Somerset,New Jersey.I.
Dan Melamed.
2000.
Models of translational equiv-alence among words.
Computational Linguistics,26(2):221?249.Franz Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Com-putational Linguistics, 29(1):19?51.Franz Och.
2003.
Minimum error rate training in statisti-cal machine translation.
In Proceedings of ACL 2003,pages 160?167, Sapporo, Japan.Marian Olteanu, Chris Davis, Ionut Volosen, and DanMoldovan.
2006.
Phramer - an open source statis-tical phrase-based translator.
In Proceedings of theNAACL 2006 Workshop on Statistical Machine Trans-lation, pages 146?149, New York, NY.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of ACL2002, pages 311?318, Philadelphia, PA.Michael Paul.
2006.
Overview of the IWSLT 2006 Eval-uation Campaign.
In Proceedings of IWSLT 2006,pages 1?15, Kyoto, Japan.Frank Smadja, Kathleen R. McKeown, and VasileiosHatzivassiloglou.
1996.
Translating collocations forbilingual lexicons: A statistical approach.
Computa-tional Linguistics, 22(1):1?38.Andrea Stolcke.
2002.
SRILM ?
An extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,pages 901?904, Denver, Colorado.T.
Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, andS.
Yamamoto.
2002.
Toward a broad-coverage bilin-gual corpus for speech translation of travel conversa-tions in the real world.
In Proceedings of LREC 2002,pages 147?152, Las Palmas, Spain.Jo?rg Tiedemann.
2003.
Combining clues for word align-ment.
In Proceedings of EACL 2003, pages 339?346,Budapest, Hungary.David Vilar, Maja Popovic, and Hermann Ney.
2006.AER: Do we need to ?improve?
our alignments?
InProceedings of IWSLT 2006, pages 205?212, Kyoto,Japan.Stefan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proceedings of COLING 1996, pages 836?841, Copenhagen, Denmark.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Jia Xu, Richard Zens, and Hermann Ney.
2004.
Dowe need chinese word segmentation for statisticalmachine translation?
In Proceedings of the ThirdSIGHAN Workshop on Chinese Language Learning,pages 122?128, Barcelona, Spain.Richard Zens and Hermann Ney.
2004.
Improvementsin phrase-based statistical machine translation.
InProceedings of HLT-NAACL 2004, pages 257?264,Boston, MA.Tiejun Zhao, Yajuan Lu?, and Hao Yu.
2001.
Increas-ing accuracy of chinese segmentation with strategy ofmulti-step processing.
Journal of Chinese InformationProcessing, 15(1):13?18.311
