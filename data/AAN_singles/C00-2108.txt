Chlstering Verbs SemanticallyAccording to their Alternation BehaviourSab ine  Schu l te  im WaldeIns t i tu t  f / i t  Masch ine l le  S1) rachvera , r lw .
i tungUnivcrsitSA: S tut tgar tAzenbergst ra l~e 12, 70174 Stut tgar t ,  ~ermm Wschult  e@ims, un i - s tu t tgar t ,  deAbstractVerbs were clustered seInantically on the basis oftheir alternation behaviom:, as characterised I)y theirsyut;actic sul)caI;e.gorisation franms extrael:ed fromlllllXillllllll proba.bili(;y parses of a robu,st sI;atisl;icalpa.rser, aald eOml~leted by assigning \'VordNe(; classesas se\]ecl, ional preferences (;o the fl:ame arl~uments.The clustering was achieved (a) iteratively by mea-su):ing the lelal;ive enl;rol)y b(,tween (;he verbs' l)rOl)-ability dis(;ribut, ions ()vet' the.
franle (,yl)eS, and (1))l)y ul;ilising a lateni; (:lass m/a.lysis t)ased on the jointfrequencies of verbs and flmne.
(,yl)eS.1 IntroductionThis paper eml)irieally investiga(;es the proposit ionthat ve, rl)s can 1)e seman(;ically classilied accordingto their synl;a(:(;ic alterna.tion 1)ehaviour (:()n(:ernint,;subca.
(;(%orisation frames ;rod their seleetional i)ref-er(;nces tbr the, arguments within the frames.
Theidea is l(;lal;ed Ix) (lx'.vin, 1993) who de.lined verl>classes on the basis of verl) atterl)al;ion beh~aqour.For exmnl)le , (;he seman(;ic (:lass of l&h, icle Namesconi, ains verbs like lmlloo'n, bicycle, ca.
'n, oe, skate, skiwhich agree in (;he prol)erties (1)-(4) below.
(1) 1NTltAN.qlTIVI.
; IJSI,:, possibly followed by ;tpath:a.
'lPhey skated.1).
They skated a\ ] ( )ng Llle cana l /over  t:}lel>ri(lge.
(2) INI)UCI'H) ACTION AI:rEIuqA'rIoN (~omeverl>s):a. Ite skated Penny around the rink.
(causing the aetiolt nanled 1)y the verb;tyl)ieal eausee is m~ alfimal;e volitionalentity)b. Penny skated a.round the )'inl?.
(3) LO(3A'rivI,; PI{.1,;I'OSITION \])l{Ol > A~:r),m~A-TION (some verbs):a.
They skated along the canals.b.
They skated the canals.
(~) I{.I,;S 1.11 "IWI'I VI,; Iq l I IASE:Pem~y skated her skate blades bhmt(an XI ) describing (,he sl:a~e achieved by lhereferent of (;l)(~ nora) l)hrase as a resu\]l; of theacl,iou mmw.,.l by the verb)Levin's work rel)resenCs th(~ basis foc a tani,;e <)f ,e-cent invesl;ig;d,ions veril)qng (\])orr :m(1 .lone.% \] 996),evaluating (SI;e.\,ensoll aud Merl% 199!
0 or utJi;::.ing (\]mpata., 1999) the proposed elas.qitlcation-,~well as transferring i(, 1.o other lanw, ta.ge',~ tha:l }~;u-glish (Jones et al, 7199.\[)~Generally, the definition of a verl /s selllaIii.i(: (:\];/:~.qcan be considered as part  of its lexic.al entry, next ioidiosyncratic intinmation: the Sellla,lll;ic CI;ts~; gell-eralises as a. l.ype definition over ~t range o\[ s.yn-l,acl.ic nnd s(mmnl:ic \])tOl)ertie:; , Lo .qUl)por(; Nai.u.
':dl~a.nguat,;e Processinp; in V;/\['\].
()lI~4 .~II'(~;/S like.
h',xicor;c~t-phy (llapl)ai>ort l\]ov'av and I,evin, t99~;), wo){I :;(:it~:edisaml)igual:i(m (l)orr aml .\]oneea, ?l!)!
)i;), or ()'.~+-nlelll; cla,~silh:al.ion (Klavm~.~; and Kaa,  199,~0oI al;l;enlpLed l;o aul;omaLica\]ly ch,sicr verb>~ h~i;oselna.lti, ie elas.
'-;c~; Oll l.be b;.tsi'A Of i,}\]e vel\]),-; ~ ~ll{;(~P,;I-tion I)ehavioltr.
'l)lm iulm 1; into i:he at/i;(ii\[Nti;it; i IJ(!!!
(:tion pr()(:e,~q:4 w;m characi:eri:;ed 1)y (J~e \;erl)',4' di>:l.~;Im-(,ion eve\]" ,qyJil,;tc{,\](; ~;ul>ca.l;egori>;;tl;i(m ' .
:~; I!
~1.U.
'A ~!\]K-Lrael;ed f rom )u : tx inn lm pvol>alfil}t 5' (Vil.crlfi) !
:, ~c~e:-',()f  ~/ l'O})ll,ql; 'r;I;t, ll;i'At,\]C~!\] l);/l':;?;!
'~ ; l l!
(t c:()i(l\]~i(;i,,.,d }.,\- ;t!
;-siff lt i l l~ \Vor(\[)~,:;(; (:l:is',',(z:; ;~.~; ;(~\]('~(:{;i.o~l,lJ l~i:eI'ei'~!
; ,:c:; {:othe frame a.rg~m~e**i;:',.
'Fbe, clmd:eti~i~ a~;~:', ;,el,}: ,4d(a) iteratively 1)y me;mminig Lhe relai,ive ,;~{,~,')p3 '.m-l,ween t,he verbs' probabil ity dist):it)c,i,io,~:~ ; ,,,(:, theframe tyl)e, % m~d (I)) l) 3, uLi/i'.-;ing a. Ld:e,t cD;;;.i c,:tal.-3'sis ba,q(:d on (,l)e joi .i; J'requen(:ies of verl>~s a:,>d ~,;::,1~'~?'~tyl)eS.
U,~;itu~ Ii,evin':; \,c)'b elas,',ific:d;\]o~) ;',.~'~ c,,ai~;~;ionbasi,q, 61% of the \,ert)~; were classified (;o1;(;<5;13 ~>!
;o~;emm~tic cla:;5;e; l>y met,l:,ut (a), ;rod 540/0 b3 :~.~i.
!..~d(1,).Section 2 de:;cribe',; (;l)e three.
:de,!,:~ h~ i!w, ~:~i.,')--marie aC(luisit;ion of ,qen)an(;i< verb cla::;sc~;; i.l)e ~ .,h>ation takes l)\]a(:e, in ,~eel;ioJl 2{, ; lAd .~;oel.ioil d (tJ:,,.l~>;~;{~;(,he re:mll:s.7472 Automatic Acquisition ofSemant ic  Verb  C lassesTile first step was the induction of purely syntac-tic subcategorisation fi'ames for verbs from the het-erogeneous British National CoTpus (BNC).
I usedthe robust statistical head-entity parser as describedin (Carroll and Rooth, 1998) which utilises an En-glish context-free grammar and a lexicalised prob-ability model to produce parse forests, and ex-tracted the maximum probability (Viterbi) parses,for a total of 5.5 million sentences.
The trees weremapped to subcategorisation frame tokens consist-ing of a inain verb and its argmnents.
Each syntac-tic category was accompanied by the lexical head,the pret)ositional phrase by the lexical prepositionalhead plus the head noun of the subordinated nounphrase.
Proper names were accompanied by theidentifier pn.
The head information in the frameswas lemmatised.
For example, the sentence Sam-rout handled the plaudits during the awards cere-mony would be represented by the frame tokenhandle subj*pn*sammut obj*p laudi t  pp*during*ceremony.To generalise over the verbs' usage of subcategori-sation frames, I defined as 88 frame types the mostfrequent frames which appeared at least 2,000 timesin total in the BNC sentence parses, disregardingthe lexical head information.
On the basis of theframe types I collected information about the jointfrequencies of the verbs in the BNC and the subcat-egorisation frame types they appeared with.
Thesefrequency counts then represented the syntactic de-scription of the verbs.Tim next step was to refine the subcategorisationframe types by a preferential ordering on conceptualclasses for the argument slots in the fl'ames.
Thebasis I could use for the selectional preferences wasprovided by the lexical heads ill the fi'anm tokens.For example, the nouns appearing in the direct ob-ject slot of the transitive frame for the verb drinkincluded coffee, milk, beer, indicating a conceptualclass like beverage tbr this argument slot.I followed (Resnik, 1993)/(Resnik, 1997) who de-fined selectional preference as the amount of infor-mation a verb provides about its semantic argumentclasses.
He utilised the WordNet taxonomy (Beck-with et al, 1991) for a probabilistic model captur-ing the co-occurrence behaviour of verbs and con-ceptual classes, where the conceptual classes wereidentified by WordNet synsets, sets of synonymousnouns within a semantic hierarchy.
Referring to theabove example, the three nouns coffee, milk, beerare in three different synsets -since they are notsynonyms-, but are all subordinated to the synset{beverage, drink, potable}.
The goal in this examplewould therefore be to determine the relevant synsetas the most selectionally preferred synset for the di-rect object slot of the verb drink.Redefined fbr iny usage, the selectional preferenceof a verb v tbr a certain semantic lass c within asubcategorisation franm slot s was deternfined bythe association ass between verb and semantic lass:=des Pl, C, lV~pOg ~ (5)with the probabilities estimated by maxinmnl likeli-hood:f(v,,P(C*lVs) - f(vs) (6)p(Cs) = f(c.,) _ f(cs) (7)f(c's) /(8)and the following interpretation:1. f(v,, c,): number of times a semantic lass ap-peared in a fi'ame slot of a verb's fi'ame type2.
f (v,) :  frequency of a verb regarding a specificfi'ame type, i.e.
the joint Dequency of verb andframe type3.
f(Cs): numl)er of times a semantic class ap-peared in a fi'ame slot of a frame type disre-garding tim verb4.
~?
'c,~'**,,s f(c'~) equals f(s), the frequency of theargument slot within a certain frame type, sincesumming over all possible classes within a sub-categorisation fl'ame slot equals the lmlnber oftinms the slot; appeared5.
f(s): uulnber of times the franle type appeared,since the frequency of a. frame type equals thefrequency of that frame with a certain slotmarkedThe fi'equencies of a semantic class concerning anargument slot, of a frame type (dependent or inde-pendent of a verb) were calculated by all approachslightly difl'erent to Resnik's, originally proposedby (Ribas, 1994)/(Ribas, 1995).
For each noun ap-pearing in a certain argument position its fi'equencywas divided by the nmnber of senses the noun wasassigned by the WordNet hierarchy, t to take accountof the uncertainty about the sense of the noun.
Thefi'action was allocated to each conceptual class in thehierarchy to which the noun belonged and accumu-lated upwards until a top node was reached.
Tileresult was a numerical distribution over the Word-Net classes:/(noun) (8) s(c,/-- E1For example, when considering the noun coffee isolatedfrom its context, we do not know whether we are talking aboutthe beverage coffee, the plant coffee or a coffee bean.
Thero.-fore, a third of the frequency of the noun was assigned to eachof the three classes.748I restricted tlm possible (:onceptual classes within1;he fl'ames' argmnent slots to 23 Wor(tNet nodes, 2 1;ofacilitate generalisation a d comI)arison of the verbs'seleetional preference behaviour.On the basis of the inforlnation al)out subcategori-sation frame types and their arguments' concet)tualclasses I clustered 153 verbs from Levin's classitica-(;ion.
I chose (i) some l)olysemous verbs to investi-gate how this l)henomenon could be handled 1)y theclustering algorithms, and (ii) high and low frequentverbs to see the intluence of frequency on th(; al-gorithms: the 1~3 verbs had 226 verb senses whichbelonged to 30 different semantic lasses.
D)ur of theverbs were low-Dequeney verbs with a total corpusfrequency below 100.To cluster the verbs I applied two different al-gorithms, and each algorithm clustered the verl)sbot, h (h) according to only the syntactic informa-tion about tlm subcategorisation frames, and (B)according to the intbrmation at)out the subcategori-sation ti'ames including their selectional 1)referelmes.,.
lterative clustering based on a dcfinitionby (Ilugh, es, 109/,):In the l)eginning, each vert) represent;ed a single-ton cluster.
Iteratively, the distances betweentim clusters were lneasure(l and the closest chls-ters merged togel;her.For the rel)resentation of the.
verbs, each verl)v was assigned a distribution over the ditfere.nttyl)es of subcategorisatioll fl'anms i, according1;o the.
maximum likelihood estimate, of (k) the.verb apl)earing with the frame tyl)e:f(v,/,)f(,,,) (9)with f (v , t )  the joint fi'equency of verb andfrmne type, and f(v) the fl'e(tuency of the verb,and (B) the verb appearing with the frame tyt)emid a selectionally t)refe.rred (:lass coml)inationC for the m'gmnent t)osil;ions .s in t:i,(~,, e ly  ) =,,ef p(tl v) * J ,(Clv , t) (10)with p(/,lv) defined as in equation (9), andp(C\]v, t) =&/  Ec:6,:l,,.~, \[Isct a.s.s'(v.~, c') (11)which intuitively estimates the probability of acertain class combination by comparing its as-sociation value with the sum over all possibleclass combinations, concerning the respectiveverb and frame.2I chose l.he 11 tel) level nodes of the 11 WordNet l,ierar-chies as conceptual classes.
'Phe top level node Ent i ty  seemedtoo general as concel)tual class, so it was replaced by its 13sulml'dinal, ed synsets.Starting out with each verb representing a sin-gleton cluster, I iteratively determined the twoclosest chlsters by applying tim information-theoretic measure relative cutropy :~ (Kulll)ackmid Leibler, 1951) to comi)are the distributions.The nearest clusters were merged into one clus-ter, and their distributions were merged 1)y cal-culating a weighted average.
Based on test runsI defined lleuristics about how many elusl, eriugiteral;ions were pertbrmed.
In addition, i lira-ire(1 the maximum mnuber of verbs within one(:luster to four elements because otherwise the.verbs showed the tendency to cluster togetherin a few large clusters only; so after the over-all clustering process was finished, each clusterwith more tlmn four members initialised a fllr-ther clustering pass on itself.Unsupervised latent, class aualysis as describedin (l~ooth, 1998), based on the cxpcetation-'maximisation al.qorithm:The algorithm identified categori(:al typesamong indirect, ly observed multinomial distri-butions 1) 3, apl)lying the EM-algorithm (\])elnp-s teret  al., 1977) to maximise the joint prol)a-bility of (h) t;he verb and frmne tyl)e: p(v, t),and (B) the verl) and frame type consideringthe selectional I)referenees: p(v, t, C).\]TUl)Ut to the algorithm were absolute, frequen-cies of the verl)s at)l)earing with the sul)categori-sation frames.
Test runs showed that 80 clustersmodelled the semantic verl) classes best.
To 1)eable to comI)a.re the analysis wit;h the iterativeclustering al)proach , I also limited tim numb(~rof verbs wit;hin a (:lus|;er 1;o four consideringthat; generally all verbs ai)l)ear within each (:lus-l;er when using this apl)roach , the verbs wil;h l:hehighest l)rol)abilities where chosen.D)r version (h) the frequencies were provide.dby the joint frequencies of verbs and frametyI)es, for version (B) I used the associationva.lues of the verbs with tile frame tyl)eS con-sidering seleetional preferences, as described 1)yequation (10).The unsupervised algorithm then classified jointevents of verbs and subeategoris~tion frmncswith 200 iterations of the EM-algorithm into 80clusters r, based on the iteratively estimatedvahlesv(v, 0 = v, l,) =T T(12)aConcerning the two typical prol)lems one has with thismeasure, (i) zero frequencies were smoothed 1)y adding 0.5 toall frequencies, and (ii) since the measure is not symmetr ic ,the resl)ective smaller vahm was used as distance.749InformationSFsSFs + PretlsClusters VerbsTotal Correct Total Correct Recall Precision31 20 90 55 36% 61%30 14 81 31 20% 38%Figure 1: Evaluation based on Iterative Clusteringhfformation Clusters~lbtal CorrectSFs 80 36SFs ~1-Prefs 80 22Verbs(Senses)Total Precision107(159)153(226)Correct Recall58(9O) 38(4O)%47(56) 31(25)0/o,54(57)%31(25)%Figure 2: Evaluation based on Latent ClassesI,(,,, t, c )  = v, t, c )  = Cl )T T(13)for versions (h) and (B), respectively.3 Eva luat ionThe evaluation of the resulting clusters was basedon Levin's classification.
Figures 1 and 2 present hesuccess of the two clustering algorithms, consideringtim two difl'erent informational versions (/~) and (B).They contain the total mnnber of clusters the algo-rithms had formed (clusters containing between twoand four verbs in the iterative algorithm, and thefixed immber of 80 clusters in the l&l;ent (:lass rarely-sis), the prol)ortion of correct clusters (non-singletonclusters which were subsets of a Levin (:lass, for ex-ample the cluster conl;aining the verl)s need, like,,want, desire is a subset of the Levin (:lass Des i re) ,and the numl)er of verbs wMlin those clusters.
Infigure, 2 the nulnl)er of verbs in brackets rethrs to therespective number of Lheir senses, since a verb cou ldbe clustered several times according to its senses.For examl)le, the verl) want  could t)e meml)er of the(:lasses Desi re  and Declarat ion.Recall was define(l by the I)ercentage of verbs(verb senses) within the correct clusters comparedto the total munber of verbs (verb senses) to be clus-tered:I,,e,'bs ......... , ,.,,.,, .....
I?
*C'C = 153(Iv ,.b .
.
.
.
.
.
.
.
.
.
, ........... l ).
226and precision was defined by the percentage of verbs(verb  senses)  apl)earing in the correct clusters com-pared to the numl)er of verbs (verb senses) apl)earingin any cluster:\[ ve.rbs..o,..~,.,t ~.t,~t,~,.~ \[wee = Ive,.r,s,,, ~,,.,,~,., I( i v -+ .................. ,...........
I)Concerning t)recision, the assignntent of verbs intosemantic lasses was most successfifl when using theil;erative distance clustering method; 61% of all verbswere clustered into correct classes.
Clustering theverbs into latent classes was with 54% less success-tiff.
With both clustering methods the results be-came worse when adding information about the se-lectional preferences tbr the arguments in the sub-categorisation fl'ames.A baseline ext)eriment was performed in order todetermine how hard the task of verb clustering was:each verb was randomly assigned another verb as"closest neighbour", which resulted in only 5% el the,verl)s being paired with a verb Don1 the same Levin(:lass.
Performing the same experiment by assign-ing the closest neighbour on the basis of moasm'ingthe relative entropy between two verbs' distributionsover subcategorisation fl'ames resulted in 61% of theverbs pointing to a verb flom the same Levin class.4 D iscuss ion  dThe classitications of both clustering approaches il-lustrate the close relationship between alternationbehaviour and semantic classes, lYor exalnple, thecommon preferences of verbs (see the tlve mostprobable frames) ill the iteratively crea.ted Des i re(:lass were towards a sul)ject followed by an infini-tival phrase (subj :to).
Alternatively a l;ransitivesubj :obj flame was used, partly followed by an ad-ditional infinitival phrase indicated by to: s4For a more detailed discussion see tile originalwork (Schulte im Walde, 1998).Note that  the (wrongly chosen) intransit ive fl:ame is listedas well.
This  is {Ill('.
t,o underlying sentences containing an NPellipsis, parsing mistakes and Dame extraction.750Ver l )needdesireF rame l )rol )abi l i tysub j : to  0.38subj:ol) j  (I.32subj  0.10sub j :ob j : to  0.05sub j :ob j :pp .
fo r  0.02sul) j : to 0.34subj:ol) j  0.34subj  0.14su l ) j :ob j :adv  0.(14sub.
i :obj :obj  0.03sub j : to  0.53sub j  :obj 0.15subj 0.11sul)j :ol)j :to (1.10subj  : to :adv 0.02subj  :obj 0.25sub j  0.24sul) j : to 0.20sul~j:obj:to 0.
(17sul) j :sent (I.02Adding ilfformation about the selectional prefer-enees of the verbs' argmnents hell)s to gel; a deeperidea about their lexical semantics.
D:)r exalnple,mar~,'n, er of Motion verbs 1)referably appeared witha subject only, sometimes with a following adverl).The subject was an inanimate ol)ject, for move itmight also be a part (such as a body part like fin_ger) or a grout), roll and fly alternatively used thetransitive frmne type subj :obj,  preferal)ly with aliving entity as subject, followed by an inanimateob.iecl;:ro l lflyFl ' i t l l lesub.i ( l ' hysOb ject )subj  ( l 'hysOb ject ) :advsub j (Agent ) :ob j  ( lq~ysObject )sub j  ( I J  fel, 'orm) :ol)j ( lq C,'sObject)sub j (Agent ) :ob j  ( lhu't)sub j ( l 'hysOI ) jec t )subj  ( l 'hysOI ) j cc t ) :advsub.i(Lifel , 'orm) :obj ( l 'hysOb jcc t )subj  (l , i l laForm) :pp.to (1A fel"or n 0sub j (L i fe leorm): l )p .
to  (Agent )sul).i(l 'hysObject)subj (l)hysOl~ject):advsul)j (1 're'i,)sul~j(Groul)):advsubj(Part) :advl ' rob~df i l i ty0 .240 .100 .070 .070 .050 .3d0 .120.
(170 .050.0,10 .200.110 .090.0,1(1.0,1Parallel examples created by the latent class analy-sis present he clusters with the most probable verbsand frmnes, according to cluster membershi I) (firstcolumn).
The dot indicates whether the verb-fi'mnecombination was seen in the data, the mmtber nextto the verb frame gives the probability of the verb-frmne combination.Some verbs of Telling were clustered mainly accord-ing to their similar transitive use combined with aninfiifitival phrase:~?_ g o'?
gClus i ;e r  d o cb oo =,, oo 9.
(}.17 adv ise  ?
?
?
?0 .12  te l tch  ?
?
?
?0.12 ins t ruct  ?
?
?
?The verl)s of Aspect alternate between a subjectonly, realised by an action, an inanimate subject fol-lowed by an infinitiwfl phrase, and a living subjectfollowed by a gerund:g', ~ g gClHster  o d o o?b0< ;~ ;5 <0.3,1 s tar t  ?
?
?
?0.19 finish ?
?
?0.18  s top  ?
?
?0.16 begin  ?
?Both approaches established a relationship be-tween alternation behaviour and semantic lass byonly considering information about the syntactic us-age of the subcategorisation Dames.
The refinementby the frames' selectional preferences allowed fllrtherdemarcations by the identifying (:onceptual restric-tions on tile use of the frames.Since tim latent class analysis is a soft; clusteringmethod, it additionally distinguishes between thedith;rent verbs' senses and the resl)ective uses ofsubcategorisation Dames.
For example, the verbplay was clustered with meet 1)ecause of tile com-mon strong tendency towards a transitive ti"ame il-lustrating a gen(;ral meeting, and it, was clusteredwith figh, t t)eemlse of their colnmon preference foran intransitive fi'ame together with a prepositionalphrase headed 1)y against, illustrating a more aggres-six'(; me.eting like a fight:Cluster0.49 meet0.20 l)layC lus terI~ g g g5 5 o obO~L0.22 f ight ?
?
?
?0.20 p lay  ?
?
?
?An extensive investigation of tile linguistic relia-bility of the clustered verbs and frames showed thatl;he character(sing usages could be under\](ned by cor-pus data, for example the above cited transitive use751of the verb fly concerning the subj : obj frame typewith a living subject and ml inanimate object can beillustrated by the BNC-sentence In March the man-ufacturer's test pilot flew the aircraft for its annualinspection check flight.
The clusters were thereforecreated on a reliable linguistic basis representing (aselective part of) the verbs' properties.Comparing the two informational versions, however,showed that refining the fralnes with selectional pref-erences points to a problem caused by data sparse-ness in the verb description.
Investigating the au-tomatically created distribution of the verbs overthe enriched fl'ame types revealed that, for exam-ple, even the high fl'equent, alternating verb movecontains 97% (smoothed) zeroes within its distribu-tion.
In accordance with this fiuding even subtlesimilarities, e.g.
the sole fact that two verbs havenon-zero wflues for certain fl'ame types, highly cor-relates the two verbs.
For example, a semantic lus-ter contained the two verbs promise and love, be-cause both have non-zero attribute values for thesubj : to  frame, demmlding an agent for the subjectslot; in their alternation behaviour (including selec-tional preferences) the two verbs differ, however, sothey should not be packed into one cluster.
A possi-ble suggestion to handle the problem of data sparse-ness could be to formulate the conceptual class typesin a way which ensures an increased ata potentialfor each type.Concerning the polysemy of verbs, the (hard) iter-ative distance clustering failed to model verb senses;a polysemous verb was either not at all assigned toany cluster, or assigned to a cluster describing oneof the verb's senses.
The (soft) latent (:lass analy-sis was able to filter the multiple senses and assignthem to distinct (:lusters, but tended to split senses.Low-frequency verbs presented another problem, be-cause the verbs' distributions contained mostly ze-roes.
They were assigned to clusters nearly ran-domly.An investigation of selected WordNet concep-tual classes revealed that the selectional preferenceswithin the subcategorisation frames were donfinatedby a few WordNet classes, mainly Li feForm andAgent.
The demarcation between these two con-cepts was not obvious when referring to actually ap-pearing nouns within the frames, since both containa large number of common subordinated nouns.
Incontrast, some WordNet classes were not chosen atall, e.g.
Unit or Ant i c ipat ion .
Since the WordNethierarchy in general had turned out to define intu-itively correct seleetional preferences, an improvedclassification utilised for my conceptual classifica-tion should be substituted by finer synsets, i.e.
oneshould consider using a different cut through theWordNet hierarchy.5 Conclus ionI proposed two algorithms for automatically classi-f~,ing verbs semantically, based on their alternationbehaviour.
Taking Levin's classification as a stan-dard for 153 manually chosen verbs with 226 verbsenses and their assignment into 30 semantic lasses,the iterative distance clustering succeeded for 61%of the verbs considering the syntactic usage of thefl'ames only, and for 38% when adding informationabout the frmne arguments' electional preferences.The latent class analysis ucceeded for 54% and 31%,respectively.An investigation of the resulting clusters showedthat the assignment of the verbs was actually basedon their shared linguistic properties: the verbs ina cluster presented common alternation behaviour,refined by adding selectional preferences to the syn-tactic description of the subcategorisation frmnes.It is impressive that as little lexical idiosyncraticverb information as the syntactic use of subcategori-sation fl'ames like subj : to or subj : pp.
aga ins t  suf-fices as a basis for a semantic lass distinction to-wards Levin's narrow classification system includingfine concepts as Desire or Manner of Motion.
Thepotential is partly characterised by specific frames,but in the majority of cases by successflflly com-bining the frames in order to define the syntacticalternation, hnproving the definition and demarca-tion of conceptual classes hould provide further po-tential concerning the inclusion of selectional prefer-ences into the syntactic description.ReferencesRichard Beckwith, Christiane Fellbaum, DerekGross, and George A. Miller.
1991.
Wordnet: ALexical Database Organized on PsycholinguisticPrinciples.
In Uri Zernik, editor, Lcxical Acqui-sition - Exploiting On-Line Resources to Bnild aLczicon, chapter 9, pages 211 232.
Lawrence Erl-baron Associates, Hillsdale - New Jersey.Glenn Carroll and Mats Rooth.
1998.
Valence In-duction with a Head-Lexicalized PCFG.
In Pro-ceedings of the 3rd Confcrcncc on Empirical Meth-ods in Natu~nl Language Processing, Granada,Spain.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum Likelihood from Incomplete Data viathe EM algorithm.
Journal of the Royal StatisticalSociety, 39(B):1-38.Bonnie J. Dorr and Doug Jones.
1996.
Role of WordSense Dismnbiguation i Lexical Acquisition: Pre-dicting Semantics from Syntactic Cues.
In Pro-ceedings of the 16th International Conference onComp'utational Linguistics, Copenhagen.John Hughes.
1994.
Automatically Acquiring Clas-sification of Words.
Ph.D. thesis, University ofLeeds, School of Computer Studies.752Douglas A. Jones, Robert C. Berwick, FranklinCho, Zeeshan Khan, Karen T. Kohl, Naoyuki No-mura, Anand Radhakrislman, Ulri('h Sauerlan(1,and Brian Ulicny.
1994.
Verb (,'lasses and Al-ternations ill Bangla, German, English, and Ko-rean.
Technical {el)ort MIT AI MEMO 1517,Massachusetts Institute of Technology.Judith L. Kla.vans and Min-Yen Kan. 1998.
TheRole of Verbs in DOeulnent Analysis.
In Pwceed-ings of thc 17th Intcrnational Co~@rcncc on Com-putational Linguistics, Montreal, Canada.S.
Kullback and R. A. Leibler.
1951.
On Infl)rmationand Sufficiency.
Annals of Mathematical Statis-tics, 22:79-86.Maria Lapata.
1999.
Acquiring Lcxical Generaliza-tions from Corpora: A Case Study for Diathe-sis Alternations.
In Proceedings of the 37th An-nual Mccting of the Association for Computa-tional Linguistics, pages 397 404:.Beth Levin.
1993.
English Verb Classes and Al-ternations.
The University of Chi(:ago Press,Chicago, 1st edition.Malka Rat)i)al)ort Hovav and Beth Levin.
1998.Building Verb Meanings.
In M. Butt andW.
Geuder, editors, Lcxical and CompositionalFactors, pages 97-134.
CSLI Publications, Stan-ford, CA.Philip Resnik.
1993.
Selection and Information:A Class-Based AppTvach to Lexical Relationsh, ips.Ph.D.
thesis, University of Pennsylvania.Philip Resnik.
1997.
Selectional Preference andSense Disambiguation.
I  Proceedings of the ACLSIGLEX Workshop on ~hflginfl ~::ct with, LcxicalSemantics: Wh, y, Wh, at, and llow?l~5"ancesc Ribas.
1994.
An Experiment on Learn-ing Appropriate SelectionM Restrictions fi'om aParsed Corpus.
In Procecdings of the 15th Inter-national Conference on Computational Linguis-tics, pages 769 774.Francesc Ribas.
1995.
On Learning Mot'e Appropri-ate Selcctional Restrictions.
In Pwcccdings of the7th Conference of the Eurot)ean Chaptcr of the As-sociation for Computational Linguistics, Dublin,Ireland.Mats Rooth.
1998.
Two-Dimensional Clusters inGrammatical Relations.
In Inducing Lexiconswith th, c EM Algorithm, AIMS Report 4(3).
Insti-tut ffir Maschinelle Si)raehverarl)eitung, Univer-sitgt Stuttgart.Sabine Schulte im Walde.
1998.
Automatic Se-nmntic Classification of Verbs According to TheirAlternation Behaviour.
Master's thesis, Institutffir Maschinelle Sprachverarbeitung, UniversitStStuttgart.Suzamm Stevenson and Paola Merlo.
1999.
Auto-Inatic Verb Classification Using Distributions ofGrammatical Features.
hi P~vcccdings of the 9thConference of thc European Chaptcr of the Associ-ation for Computational Linguistics, pages 45-52.753
