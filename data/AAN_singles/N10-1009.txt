Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 73?81,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsMinimally-Supervised Extraction of Entities from Text AdvertisementsSameer SinghDept.
of Computer ScienceUniversity of MassachusettsAmherst, MA 01003sameer@cs.umass.eduDustin HillardAdvertising SciencesYahoo!
Labs Silicon ValleySanta Clara, CA 95054dhillard@yahoo-inc.comChris LeggetterAdvertising SciencesYahoo!
Labs Silicon ValleySanta Clara, CA 95054cjl@yahoo-inc.comAbstractExtraction of entities from ad creatives is animportant problem that can benefit many com-putational advertising tasks.
Supervised andsemi-supervised solutions rely on labeled datawhich is expensive, time consuming, and dif-ficult to procure for ad creatives.
A smallset of manually derived constraints on fea-ture expectations over unlabeled data can beused to partially and probabilistically labellarge amounts of data.
Utilizing recent workin constraint-based semi-supervised learning,this paper injects light weight supervisionspecified as these ?constraints?
into a semi-Markov conditional random field model of en-tity extraction in ad creatives.
Relying solelyon the constraints, the model is trained on a setof unlabeled ads using an online learning al-gorithm.
We demonstrate significant accuracyimprovements on a manually labeled test setas compared to a baseline dictionary approach.We also achieve accuracy that approaches afully supervised classifier.1 IntroductionGrowth and competition in web search in recentyears has created an increasing need for improve-ments in organic and sponsored search.
While foun-dational approaches still focus on matching the exactwords of a search to potential results, there is emerg-ing need to better understand the underlying intent inqueries and documents.
The implicit intent is partic-ularly important when little text is available, such asfor user queries and advertiser creatives.This work specifically explores the extraction ofnamed-entities, i.e.
discovering and labeling phrasesin ad creatives.
For example, for an ad ?Move toSan Francisco!
?, we would like to extract the entitysan francisco and label it a CITY.
Similarly, for anad ?Find DVD players at Amazon?, we would ex-tract dvd players as a PRODUCT and amazon as aORGNAME.
The named-entities provide importantfeatures to downstream tasks about what words andphrases are important, as well as information on theintent.
Much recent research has focused on extract-ing useful information from text advertisement cre-atives that can be used for better retrieval and rank-ing of ads.
Semantic annotation of queries and adcreatives allows for more powerful retrieval models.Structured representations of semantics, like the onestudied in our task, can be directly framed as infor-mation extraction tasks, such as segmentation andnamed-entity recognition.Information extraction methods commonly relyon labeled data for training the models.
The hu-man labeling of ad creatives would have to pro-vide the complete segmentation and entity labels forthe ads, which the information extraction algorithmwould then rely on as the truth.
For entity extractionfrom advertisements this involves familiarity witha large number of different domains, such as elec-tronics, transportation, apparel, lodging, sports, din-ing, services, etc.
This leads to an arduous and timeconsuming labeling process that can result in noisyand error-prone data.
The problem is further com-pounded by the inherent ambiguity of the task, lead-ing to the human editors often presenting conflictingand incorrect labeling.Similar problems, to a certain degree, are alsofaced by a number of other machine learning taskswhere completely relying on the labeled data leadsto unsatisfactory results.
To counter the noisyand sparse labels, semi-supervised learning meth-73ods utilize unlabeled data to improve the model(see (Chapelle et al, 2006) for an overview).
Fur-thermore, recent work on constraint-based semi-supervised learning allows domain experts to eas-ily provide additional light supervision, enabling thelearning algorithm to learn using the prior domainknowledge, labeled and unlabeled data (Chang etal., 2007; Mann and McCallum, 2008; Bellare et al,2009; Singh et al, 2010).Prior domain knowledge, if it can be easily ex-pressed and incorporated into the learning algo-rithm, can often be a high-quality and cheap sub-stitute for labeled data.
For example, previouswork has often used dictionaries or lexicons (listsof phrases of a particular label) to bootstrap themodel (Agichtein and Ganti, 2004; Canisius andSporleder, 2007), leading to a partial labeling of thedata.
Domain knowledge can also be more proba-bilistic in nature, representing the probability of cer-tain token taking on a certain label.
For most tasks,labeled data is a convenient representation of the do-main knowledge, but for complex domains such asstructured information extraction from ads, these al-ternative easily expressible representations may beas effective as labeled data.Our approach to solving the the named entity ex-traction problem for ads relies completely on do-main knowledge not expressed as labeled data, anapproach that is termed minimally supervised.
Eachad creative is represented as a semi-Markov condi-tional random field that probabilistically representsthe segmentation and labeling of the creative.
Exter-nal domain knowledge is expressed as a set of targetsfor the expectations of a small subset of the featuresof the model.
We use alternating projections (Bel-lare et al, 2009) to train our model using this knowl-edge, relying on the rest of the features of the modelto ?dissipate?
the knowledge.
Topic model and co-occurrence based features help this propagation bygeneralizing the supervision to a large number ofsimilar ads.This method is applied to a large dataset of textadvertisements sampled from a variety of differentdomains.
The minimally supervised model performssignificantly better than a model that incorporatesthe domain knowledge as hard constraints.
Ourmodel also performs competitively when comparedto a supervised model trained on labeled data from asimilar domain (web search queries).Background material on semi-CRFs and con-straint based semi-supervised learning is summa-rized in Section 2.
In Section 3, we describe theproblem of named entity recognition in ad creativesas a semi-CRF, and describe the features in Sec-tion 4.
The constraints that we use to inject super-vision into our model are listed in Section 5.
Wedemonstrate the success of our approach in Sec-tion 6.
This work is compared with related literaturein Section 7.2 BackgroundThis section covers introductory material onthe probabilistic representation of our model(semi-Markov conditional random fields) and theconstraint-driven semi-supervised method that weuse to inject supervision into the model.2.1 Semi-Markov Conditional Random FieldsConditional Random Fields (CRFs) (Lafferty etal., 2001) use a Markov random field to model theconditional probability P (y|x).
CRFs are com-monly used to learn sequential models, where theMarkov field is a linear-chain, and y is a linear se-quence of labels and each label yi ?
Y .
Let f be avector of local feature functions f = ?f1, .
.
.
, fK?,each of which maps a pair (x,y) and an index i toa measurement fk(i,x,y) ?
<.
Let f(i,x,y) bethe vector of these measurements, and let F(x,y) =?|x|i f(i,x,y).
CRFs use these feature functions inconjunction with the parameters ?
to represent theconditional probability as follows:P (y|x, ?)
=1Z(x)e?
?F(x,y)where Z(x) =?y?
e??F(x,y?
).For sequential models where the same labels ap-pear within a sequence as contiguous blocks (e.g.,named entity recognition) it is more convenient torepresent these blocks directly as segments.
Thisrepresentation was formulated as semi-Markov con-ditional random fields (Semi-CRFs) in (Sarawagiand Cohen, 2004).
The segmentation of a sequenceis represented by s = ?s1, .
.
.
, sp?
where each seg-ment sj = ?tj , uj , yj?
consists of a start positiontj , an end position uj , and a label yj ?
Y .
Similarto the CRF, let g be the vector of segment feature74functions g = ?g1, .
.
.
, gK?, each of which mapsthe pair (x, s) and an index j to a measurementgk(j,x, s) ?
<, and G(x, s) =?|s|j g(j,x, s).
Theconditional probability is represented as:P (s|x, ?)
=1Z(x)e?
?G(x,s)where Z(x) =?s?
e??G(x,s?).
To assert the Marko-vian assumption, each gk(j,x, s) only computesfeatures based on x, sj , and yj?11.An exact inference algorithm was described in(Sarawagi and Cohen, 2004), and was later im-proved to be more efficient (Sarawagi, 2006).2.2 Constraint Driven Learning UsingAlternating ProjectionsRecent work in semi-supervised learning usesconstraints as external supervision (Chang et al,2007; Mann and McCallum, 2008; Bellare et al,2009; Singh et al, 2010).
These external constraintsare specified as constraints on the expectations of aset of auxiliary features g?
= {g?1, .
.
.
, g?k} over theunlabeled data.
In particular, given the targets u ={u1, .
.
.
, uk} corresponding to the auxiliary featuresg?, the constraints can take different forms, for ex-ampleL2 penalty ( 12??ui?
?j Ep[g?i(xj , s)]?22 = 0),L1 box constraints (|ui ?
?j Ep[g?i(xj , s)]| ?
?
)and Affine constraints2 (Ep[g?i(x, s)] ?
ui).
In thiswork, we only use the affine form of the constraints.For an example, using domain knowledge, wemay know that token ?arizona?
should get the labelSTATE in at least half of the occurrences in our data.To capture this, we introduce an auxiliary feature g?
:[[Label=STATE given Token=?arizona?]].
Theaffine constraint is written as Ep[g?
(x, y)] ?
0.5.These constraints have been incorporated intolearning using Alternating Projections (Bellare etal., 2009).
Instead of directly optimizing an ob-jective function that includes the constraints, thismethod considers two distributions, p?
and q?,?,where p?
(s|x) = 1Z(x)e?
?G(x,s) is the usual semi-Markov model, and q?,?
= 1Z(x)e(??G(x,s)+??G?
(x,s))is an auxiliary distribution that satisfies the con-straints and has low divergence with the model p?.1i.e.
gk(j,x, s) can be written as gk(yj?1,x, sj)2where Ep[g] represents the expectation of g over the unla-beled data using the model p.In the batch setting, parameters ?
and ?
arelearned using an EM-like algorithm, where ?
is fixedwhile optimizing ?
and vice versa.
Each of the up-dates in these steps decomposes according to the in-stances, leading to a stochastic gradient based onlinealgorithm, as follows:1.
For t = 1, .
.
.
, T , let ?
= 1t+t0 where t0 =1/?0, ?0 the initial learning rate.
Let labeledand unlabeled data set sizes be m and n ?
mrespectively.
Let the initial parameters be ?0and ?0, and ?
be the weight of L2 regulariza-tion on ?.2.
For a new labeled instance xt with segmen-tation st, set ?t = ?t?1 and ?t = ?t?1 +?
[g(xt, st)?
Ep?t?1 [g(xt, s)]???t?1n].3.
For a new unlabeled instance xt, ?t =?t?1 + ?
[u(n?m) ?
Eq?t?1,?t?1 [g?
(xt, s)]]and ?t = ?t?1 +?
[Eq?t?1,?t?1 [g(xt, s)]?
Ep?t?1 [g(xt, s)]??
?t?1n].Online training enables scaling the approach tolarge data sets, as is the case with ads.
In our ap-proach we rely only on unlabeled data (m = 0, andstep 2 of the above algorithm does not apply).3 ModelMost text ads consist of a brief title and an ac-companying abstract that provides additional infor-mation.
The objective of our paper is to extractthe named-entity phrases within these titles and ab-stracts, then label them with a type from a pre-determined taxonomy.
An example of such an ex-traction is shown in Fig 1.We represent the ad creatives as a sequence ofindividual tokens, with a special token inserted be-tween the title and the abstract of the ad.
The dis-tribution over possible phrases and labels of the adis expressed as a semi-Markov conditional randomfield, as described earlier in Section 2.1.3.1 Label TaxonomyIn most applications of CRFs and semi-CRFs, thedomain of labels is a fixed set Y , where each labelindexes into one value.
Instead, in our approach, werepresent our set of labels as a taxonomy (tree).
Thelabels higher in the taxonomy are more generic (for75Ad Title: Bradley International Airport HotelAd Abstract: Marriott Hartford, CT Airport hotel - free shuttle service & parking.Output: Bradley International Airport HotelMarriott Hartford, CT Airport hotel free shuttle service & parking.Label SegmentPLACE: AIRPORT Bradley InternationalBUSINESS: TRAVEL HotelORGNAME: LODGING MarriottPLACE: CITY HartfordPLACE: STATE CTBUSINESS: TRAVEL hotelPRODUCT: TRAVEL shuttle service & parking.Figure 1: Example Prediction: An example of an ad creative (title and abstract), along with a set of probable ex-tracted entities.
Note that even in this relatively simple example, there is some ambiguity about what is the correctsegmentation and labeling.instance, PLACE) and the labels lower in the taxon-omy are more specific (for instance, STATE may bea child of PLACE).
The taxonomy of labels that weuse for tagging phrases is shown in Figure 2.When the model predicts a label for a segment,it can be from any of the levels in the tree.
Thebenefits of this is multi-fold.
First, this allows themodel to be flexible in predicting labels at a lower(or higher) level based on its confidence.
For ex-ample, the model may have enough evidence to la-bel ?san francisco?
a CITY, however, for ?georgia?it may not have enough context to discriminate be-tween STATE or COUNTRY, but could confidentlylabel it a PLACE.
Secondly, this also allows us todesign the features over multiple levels of label gran-ularity, which leads to a more expressive model.
Ex-pectation constraints can be specified over this ex-panded set of features, at any level of the taxonomy.In order to incorporate the nested labels into ourmodel, we observe that every feature that fires fora non-leaf label should also fire for all descendantsof that label, e.g.
every feature that is active for la-bel PLACE should also be active for a label CITY,COUNTRY, etc 3.
Following the observation, for ev-ery feature gk(x, ?tj , uj , yj?)
that is active, we also3Note that this argument works similarly for the taxonomyrepresented as a DAG, where the descendants are of a node areall nodes reachable from it.
We do not explore this structure ofthe taxonomy in this paper.fire ?y?
?
desc(yj), gk(x, ?tj , uj , y??)4.
The sameprocedure is applied to the constraints.4 FeaturesOur learning algorithm relies on constraints g?
assupervision to extract entities, but even though con-straints are designed to be generic they do not coverthe whole dataset.
The learning algorithm needsto propagate the supervision to instances where theconstraints are not applicable, guided by the setof feature functions g. More expressive and rele-vant features will provide better propagation.
Eventhough these feature functions represent the ?unsu-pervised?
part of the model (in that they are onlydependent on the unlabeled sequences), they playan important role in propagating the supervisionthroughout the dataset.4.1 Sequence and Segment FeaturesOur first set of features are the commonly usedfeatures employed in linear-chain sequence modelssuch as CRFs and HMMs.
These consist of factorsbetween each token and its corresponding label, andneighboring labels.
They also include transition fac-tors between the labels.
These are local feature func-tions that are defined only over pairs of token-wise4This example describes when gk(yj?1,x, sj) ignoresyj?1.
For the usual case gk(yj?1,x, sj), features between allpairs of descendants of yj?1 and yj are enabled.76Proper Nouns Common NounsPLACECITY STATECOUNTRY CONTINENTAIRPORT ZIPCODEPERSONMANUFACTURERPRODUCTNAMEMEDIATITLEEVENTPRODUCT and BUSINESSFINANCE MEDIAEDUCATION APPARELTRAVEL AUTOTECHNOLOGY RESTAURANTORGNAMEAIRLINE SPORTSLEAGUE APPAREL AUTOMEDIA TECHNOLOGY FINANCE LODGINGEDUCATION SPORTSTEAM RESTAURANTOCCASIONFigure 2: Label Taxonomy: The set of labels that are used are shown grouped by the parent label.
PRODUCT andBUSINESS labels have been merged for brevity, i.e.
there are two labels of each child label shown (e.g.
PRODUCT:AUTO and BUSINESS: AUTO).
An additional label OTHER is used for the tokens that do not belong to any entities.labels yj and yj?1.
To utilize the semi-Markov rep-resentation that allows features over the predictedsegmentation, we add the segment length and pre-fix/suffix tokens of the segment as features.4.2 Segment ClustersAlthough the sequence and segment features cap-ture a lot of useful information, they are not suffi-cient for propagation.
For example, if we have aconstraint about the token ?london?
being a CITY,but not about ?boston?, the model can only rely onsimilar contexts between ?london?
and ?boston?
topropagate the information.
To allow more compli-cated propagation to occur, we use features basedon a clustering of segments.The segment cluster features are based on simi-larity between segments from English sentences.
Alarge corpus of English documents were taken fromweb, from which 5.1 billion unique sentences wereextracted.
Using the co-occurrence of segments inthe sentences as a distance measure, K-Means isused to identify clusters of segments as described in(Pantel et al, 2009).
The cluster identity of each seg-ment is added as a feature to the model, capturingthe intuition that segments that appear in the samecluster should get the same label.4.3 Topic ModelMost of the ads lie in separate domains withvery little overlap, for example travel and electron-ics.
Additional information about the domain canbe very useful for identifying entities in the ad.
Forexample, consider the token ?amazon?.
It may bedifficult to discern whether the token refers to thegeographical region or the website from just the fea-tures in the model, however given that the domainof the ad is travel (or conversely, electronics), thechoice becomes easier.The problem of domain identification is oftenposed as a document classification task, which re-quires labeled data to train and thus is not applica-ble for our task.
Additionally, we are not concernedwith accurately specifying the exact domain of eachad, instead any information about similarity betweenads according to their domains is helpful.
This kindof representation can be obtained in an unsupervisedfashion by using topic cluster models (Steyvers andGriffiths, 2007; Blei et al, 2003).
Given a largeset of unlabeled documents, topic models define adistribution of topics over each document, such thatdocuments that are similar to each other have similartopic distributions.The LDA (Blei et al, 2003) implementation oftopic models in the Mallet toolkit (McCallum, 2002)was used to construct a model with 1000 topics fora dataset containing 3 million ads.
For each ad, thediscrete distribution over the topics, in conjunctionwith each possible label, was added as a feature.This captures a potential for each label given an ap-proximation of the ad?s domain captured as topics.775 ConstraintsConstraints are used to inject light supervisioninto the learning algorithm and are defined as tar-gets u for expectations of features G?
over the data.Any feature that can be included in the model can beused as a constraint.
This allows us to capture a va-riety of different forms of domain knowledge, someof which we shall explore in this section.Labeled data xl, sl can be incorporated as a spe-cial case when constraints have a target expectationof 1.0 for the features that are defined only for thesequence xl and with segmentation sl.
This allowsus to easily use labeled data in form of constraints,but in this work we do not include any labeled data.A more interesting case is that of partial labeling,where the domain expert may have prior knowledgeabout the probability that certain tokens and/or con-texts result in a specific label.
These constraintscan cover more instances than labeled data, howeverthey only provide partial and stochastic labels.
Allof the constraints described in this section are alsoincluded as simple features.Many different methods have been suggested inrecent work for finding the correct target values forthe feature expectations.
First, if ample labeled datais available, features expectations can be calculated,and assumptions can be made that the same expec-tations hold for the unlabeled data.
This methodcannot be applied to our work due to lack of la-beled data.
Second, for certain constraints, the priorknowledge can be used directly to specify these val-ues.
Third, if the constraints are an output of aprevious machine learning model, we can use thatmodel?s confidence in the prediction as the targetexpectation of the constraint.
Finally, a search forthe ideal values of the target expectations can beperformed by evaluating on small evaluation data.Our target values for feature expectations were setbased on domain knowledge, then adjusted manu-ally based on minimal manual examination of ex-amples on a small held-out data set.5.1 Dictionary-BasedDictionary constraints are the form of constraintsthat apply to the feature between an individual tokenand its label.
For a set of tokens in the dictionary, theconstraints specify which label they are likely to be.Dictionaries can be easily constructed using varioussources, for example product databases, lexicons,manual collections, or predictions from other mod-els.
These dictionary constraints are often used tobootstrap models (Agichtein and Ganti, 2004; Cani-sius and Sporleder, 2007) and have also been used inthe ads domain (Li et al, 2009).
For our application,we rely on dictionary constraints from two sources.First, the predictions of a previous model are usedto construct a dictionary.
A model for entity extrac-tion is trained on a large amount of labeled searchquery data.
The domain and style of web queriesdiffers from advertisements, but the set of labels isessentially the same.
The supervised query entityextraction model is used to infer segments and la-bels for the ads domain, and each of the predictedsegments are added to the dictionary of the corre-sponding predicted label.
Even though the predic-tions of the model are not perfect (see Section 6.1)the predictions of some of the labels are of high pre-cision, and thus can be used for supervision in formof noisy dictionary constraints.The second source of prior information for dictio-nary constraints are external databases.
Lists of vari-ous types of places can be obtained easily, for exam-ple CITY, COUNTRY, STATE, AIRPORT, etc.
Ad-ditionally, product databases available internally toour research group are used for MANUFACTURERS,BRANDS, PRODUCTS, MEDIATITLE, etc.
Some ofthese databases are noisy, and the constraints basedon them are given lower target expectations.5.2 Pattern-BasedPrior knowledge can often be easily expressed aspatterns that appear for a specific domain.
Patternbased matching has been used to express supervisionfor information extraction tasks (Califf and Mooney,1999; Muslea, 1999).
The usual use case involvesa domain expert specifying a number of ?prototyp-ical?
patterns, while additional patterns are discov-ered based on these initial patterns.We incorporate noisy forms of patterns as con-straints.
Simple regular expression based patternswere used to identify and label segments for a fewdomains (e.g.
?flights to {PLACE}?
and ?lookingfor {PRODUCT}??).
We do not employ a pattern-discovery algorithm for finding other contexts; themodel propagates these labels, as before, using the78features of the rest of the model.
However if theoutput of a pattern-discovery algorithm is available,it can be directly incorporated into the model as ad-ditional constraints.5.3 Domain-BasedA number of label-independent constraints arealso added to avoid unrealistic segmentation predic-tions.
For example, an expectation over segmentlengths was included, which denotes that the seg-ment length is usually 1 or 2, and almost never morethan 6.
A constraint is also added to avoid segmentsthat overlap the separator token between title andabstract by ensuring that the segment that includesthe separator token is always of length 1 and of la-bel OTHER.
Finally, an additional constraint ensuresthat the label OTHER is the most common label.6 ResultsThe feature expectations of the model are cal-culated with modifications to an open sourcesemi-CRF package5.
We collect two datasets ofad creatives randomly sampled from Yahoo!
?s adsdatabase: a smaller dataset contains 14k ads and alarger dataset of 42k ads.
The ads were not restrictedto any particular domain (such as travel, electronics,etc.).
The average length of the complete ad textwas ?14 tokens.
Preprocessing of the text involvedlower-casing, basic cleaning, and stemming.The training time for each iteration through thedata was ?90 minutes for the smaller dataset and?360 minutes for the larger dataset.
Inference overthe dataset, using Viterbi decoding for semi-CRFs,took a total of ?8 and ?32 minutes.
The initiallearning rate ?
is set to 10.0.6.1 DiscussionWe compare our approach to a baseline ?Dictio-nary?
system that deterministically selects a labelbased on the dictionaries described in Section 5.1.A segment is given a label corresponding to the dic-tionary it appears in, or OTHER if it does not ap-pear in any dictionary.
In addition, we compare toan external supervised system that has been trainedon tens-of-thousands of manually-annotated searchqueries that use the same taxonomy (the same sys-tem as used in Section 5.1 to derive dictionaries).5Available on http://crf.sourceforge.net/This CRF-based model contains mostly the samefeatures as our unsupervised system, and approxi-mates what a fully supervised system might achieve,although it is trained on search queries.
Results forour approach and these two systems are presentedin Table 1.
Our evaluation data consists of 2,157randomly sampled ads that were manually labeledby professional editors.
This labeled data size wastoo small to sufficiently train a supervised semi-CRFmodel that out-performed the dictionary baseline forour task (which consists of 45 potential labels).We measure the token-wise accuracy and macroF-score over the manually labeled dataset.
Typi-cally, these metrics measure only exact matches be-tween the true and the predicted label, but this leadsto cases where the model may predict PLACE for atrue CITY.
To allow a ?partial credit?
for these cases,we introduce ?weighted?
version of these measures,where a predicted label is given 0.5 credit if the truelabel is its direct child or parent, and 0.25 credit ifthe true label is a sibling.
Our F-score measures therecall of all true labels except OTHER and similarlythe precision of all predicted labels except OTHER.We focus on these labels because the OTHER la-bel is mostly uninformative for downstream tasks.The token-wise accuracy over all labels (includingOTHER) is included as ?Overall Accuracy?.Our method significantly outperforms the base-line dictionary method while approaching the resultsobtained with the sophisticated supervised model.Overall accuracy is 50% greater than the dictionarybaseline, and comes within 10% of the supervisedmodel6.
Increasing unlabeled data from 14k to 42kads provides an increase in overall accuracy andnon-OTHER precision, but somewhat reduces recallfor the remaining labels.
We also include the F2-score which gives more weight to recall, becausewe are interested in extracting informative labels fordownstream models (which may be able to com-pensate for a lower precision in label prediction).Our model trained on 14k samples out-performs thequery-based supervised model in terms of F2, whichis promising for future work that will incorporatepredicted labels in ad retrieval and ranking systems.6Comparisons and trends for normal and weighted measuresare consistent throughout the results.79Table 1: Evaluation: Token-wise accuracy and F-score for the methods evaluated on labeled data (Normal / Weighted)Metric Dictionary Our Method (14k) Our Method (42k) Query-based Sup.
ModelOverall Accuracy 0.454 / 0.466 0.596 / 0.627 0.629 / 0.649 0.665 / 0.685non-OTHER Recall 0.170 / 0.205 0.329 / 0.412 0.271 / 0.325 0.286 / 0.342non-OTHER Precision 0.136 / 0.163 0.265 / 0.333 0.297 / 0.357 0.392 / 0.469F1-score 0.151 / 0.182 0.293 / 0.368 0.283 / 0.340 0.331 / 0.395F2-score 0.162 / 0.195 0.313 / 0.393 0.276 / 0.331 0.303 / 0.3617 Related WorkExtraction of structured information from text isof interest to a large number of communities.
How-ever, in the ads domain, the task has usually beensimplified to that of classification or ranking.
Pre-vious work has focused on retrieval (Raghavan andIyer, 2008), user click prediction (Shaparenko etal., 2009; Richardson et al, 2007; Ciaramita et al,2008), ad relevance (Hillard et al, 2010) and bouncerate prediction (Sculley et al, 2009).
As far weknow, our method is the only one that aims to solve amuch more complex task of segmentation and entityextraction from ad creatives.
Supervised methodsare a poor choice to solve this task as they requirelarge amounts of labeled ads, which is expensive,time-consuming and noisy.
Most semi-supervisedmethods also rely on some labeled data, and scalebadly with the size of unlabeled data, which is in-tractable for most ad databases.Considerable research has been undertaken to ex-ploit forms of domain knowledge other than la-beled data to efficiently train a model while utiliz-ing the unlabeled data.
These include methods thatexpress domain knowledge as constraints on fea-tures, which have shown to provide high accuracyon natural language datasets (Chang et al, 2007;Chang et al, 2008; Mann and McCallum, 2008;Bellare et al, 2009; Singh et al, 2010).
We usethe method of alternating projections for constraint-driven learning (Bellare et al, 2009) since it spec-ifies constraints on feature expectations instead ofless intuitive constraints on feature parameters (asin (Chang et al, 2008)).
Additionally, the alternat-ing projection method is computationally more effi-cient than Generalized Expectation (Mann and Mc-Callum, 2008) and can be applied in an online fash-ion using stochastic gradient.Our approach is most similar to (Li et al, 2009),which uses semi-supervised learning for CRFs to ex-tract structured information from user queries.
Theyalso use a constraint-driven method that utilizes anexternal data source.
Their method, however, relieson labeled data for part of the supervision while ourmethod uses only unlabeled data.
Also, evaluationwas only shown for a small domain of user queries,while our work does not restrict itself to any specificdomain of ads for evaluation.8 ConclusionsAlthough important for a number of tasks in spon-sored search, extraction of structured informationfrom text advertisements is not a well-studied prob-lem.
The difficulty of the problem lies in the expen-sive, time-consuming and error-prone labeling pro-cess.
In this work, the aim was to explore machinelearning methods that do not use labeled data, re-lying instead on light supervision specified as con-straints on feature expectations.
The results clearlyshow this minimally-supervised method performssignificantly better than a dictionary based baseline.Our method also approaches the performance of asupervised model trained to extract entities fromweb search queries.
These findings strongly suggestthat domain knowledge expressed in forms otherthan directly labeled data may be preferable in do-mains for which labeling data is unsuitable.The most important limitation lies in the factthat specifying the target expectations of constraintsis an ad-hoc process, and robustness of the semi-supervised learning method to noise in these targetvalues needs to be investigated.
Further researchwill also explore using the extracted entities fromadvertisements to improve downstream sponsoredsearch tasks.80ReferencesEugene Agichtein and Venkatesh Ganti.
2004.
Min-ing reference tables for automatic text segmentation.In KDD: ACM SIGKDD International Conference onKnowledge Discovery and Data mining, pages 20?29,New York, NY, USA.Kedar Bellare, Gregory Druck, and Andrew McCallum.2009.
Alternating projections for learning with expec-tation constraints.
In UAI: Conference on Uncertaintyin Artificial Intelligence.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal on MachineLearning Research, 3:993?1022.Mary Elaine Califf and Raymond J. Mooney.
1999.
Re-lational learning of pattern-match rules for informationextraction.
In AAAI / IAAI ?99: National conferenceon Artificial intelligence and the Innovative Applica-tions of Artificial Intelligence conference, pages 328?334.Sander Canisius and Caroline Sporleder.
2007.
Boot-strapping information extraction from field books.In EMNLP-CoNLL: Joint Conference on EmpiricalMethods in Natural Language Processing and Compu-tational Natural Language Learning, pages 827?836.Ming-Wei Chang, Lev Ratinov, and Dan Roth.2007.
Guiding semi-supervision with constraint-driven learning.
In ACL: Annual meeting of the Asso-ciation for Computational Linguistics, pages 280?287.Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, andDan Roth.
2008.
Learning and inference with con-straints.
In AAAI: National Conference on ArtificialIntelligence, pages 1513?1518.O.
Chapelle, B. Scho?lkopf, and A. Zien, editors.2006.
Semi-Supervised Learning (Adaptive Computa-tion and Machine Learning).
The MIT Press, Septem-ber.Massimiliano Ciaramita, Vanessa Murdock, and VassilisPlachouras.
2008.
Online learning from click data forsponsored search.
In WWW: International World WideWeb Conference.Dustin Hillard, Stefan Schroedl, Eren Manavoglu, HemaRaghavan, and Chris Leggetter.
2010.
Improvingad relevance in sponsored search.
In WSDM: Inter-national conference on Web search and data mining,pages 361?370.John Lafferty, Andrew Mccallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In ICML:International Conference on Machine Learning, pages282?289.Xiao Li, Ye-Yi Wang, and Alex Acero.
2009.
Extractingstructured information from user queries with semi-supervised conditional random fields.
In SIGIR: In-ternational Conference on research and developmentin information retrieval, pages 572?579.
ACM.Gideon S. Mann and Andrew McCallum.
2008.
General-ized expectation criteria for semi-supervised learningof conditional random fields.
In ACL: Annual meet-ing of the Association for Computational Linguistics,pages 870?878.Andrew McCallum.
2002.
Mallet: A machine learningfor language toolkit.
http://mallet.cs.umass.edu.Ion Muslea.
1999.
Extraction patterns for informationextraction tasks: A survey.
In AAAI: Workshop on Ma-chine Learning for Information Extraction, pages 1?6.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.
Web-scaledistributional similarity and entity set expansion.
InEMNLP: Conference on Empirical Methods in Natu-ral Language Processing, pages 938?947.Hema Raghavan and Rukmini Iyer.
2008.
Evaluatingvector-space and probabilistic models for query to admatching.
In SIGIR Workshop on Information Re-trieval in Advertising (IRA).Matthew Richardson, Ewa Dominowska, and RobertRagno.
2007.
Predicting clicks: estimating the click-through rate for new ads.
In WWW: InternationalWorld Wide Web Conference.Sunita Sarawagi and William W. Cohen.
2004.
Semi-markov conditional random fields for information ex-traction.
In NIPS: Neural Information Processing Sys-tems.Sunita Sarawagi.
2006.
Efficient inference on sequencesegmentation models.
In ICML: International Confer-ence on Machine Learning, pages 793?800.D.
Sculley, Robert G. Malkin, Sugato Basu, andRoberto J. Bayardo.
2009.
Predicting bouncerates in sponsored search advertisements.
In KDD:ACM SIGKDD International Conference on Knowl-edge Discovery and Data mining, pages 1325?1334.Benyah Shaparenko, Ozgur Cetin, and Rukmini Iyer.2009.
Data driven text features for sponsored searchclick prediction.
In AdKDD: Workshop on Data min-ing and audience intelligence for advertising.Sameer Singh, Limin Yao, Sebastian Riedel, and AndrewMcCallum.
2010.
Constraint-driven rank-based learn-ing for information extraction.
In North AmericanChapter of the Association for Computational Linguis-tics - Human Language Technologies (NAACL HLT).Mark Steyvers and Tom Griffiths.
2007.
ProbabilisticTopic Models.
Lawrence Erlbaum Associates.81
