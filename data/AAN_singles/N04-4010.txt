Using N-best Lists for Named Entity Recognition from Chinese SpeechLufeng ZHAI*, Pascale FUNG*, Richard SCHWARTZ?, Marine CARPUAT?, Dekai WU?
* HKUSTHuman Language Technology CenterElectrical & Electronic EngineeringUniversity of Science and TechnologyClear Water Bay, Hong Kong{lfzhai,pascale}@ee.ust.hk?
BBN Technologies9861 Broken Land ParkwayColumbia, MD 21046U.S.Aschwartz@bbn.com?
HKUSTHuman Language Technology CenterDepartment of Computer ScienceUniversity of Science and TechnologyClear Water Bay, Hong Kong{marine,dekai}@cs.ust.hkAbstractWe present the first known result for namedentity recognition (NER) in realistic large-vocabulary spoken Chinese.
We establish thisresult by applying a maximum entropy model,currently the single best known approach fortextual Chinese NER, to the recognitionoutput of the BBN LVCSR system on ChineseBroadcast News utterances.
Our resultssupport the claim that transferring NERapproaches from text to spoken language is asignificantly more difficult task for Chinesethan for English.
We propose re-segmentingthe ASR hypotheses as well as applying post-classification to improve the performance.Finally, we introduce a method of using n-besthypotheses that yields a small but neverthelessuseful improvement NER accuracy.
We useacoustic, phonetic, language model, NER andother scores as confidence measure.Experimental results show an average of 6.7%relative improvement in precision and 1.7%relative improvement in F-measure.1.
IntroductionNamed Entity Recognition (NER) is the first step formany tasks in the fields of natural language processingand information retrieval.
It is a designated task in anumber of conferences, including the MessageUnderstanding Conference (MUC), the InformationRetrieval and Extraction Conference (IREX), theConferences on Natural Language Learning (CoNLL)and the recent Automatic Content ExtractionConference (ACE).There has been a considerable amount of work onEnglish NER yielding good performance (Tjong KimSang et al 2002, 2003; Cucerzan & Yarowsky 1999;Wu et al 2003).
However, Chinese NER is moredifficult, especially on speech output, due to tworeasons.
First, Chinese has a large number of homonymsand the vocabulary used in Chinese person names is anopen set so more characters/words are unseen in thetraining data.
Second, there is no standard definition ofChinese words.
Word segmentation errors made byrecognizers may lead to NER errors.
Previous work onChinese textual NER includes Jing et al (2003) and Sunet al (2003) but there has been no published work onNER in spoken Chinese.Named Entity Recognition for speech is more difficultthan for text, since the most reliable features for textualNER (punctuation, capitalization, and syntacticpatterns) are often not available in speech output.
NERon automatically recognized broadcast news was firstconducted by MITRE in 1997, and was subsequentlyadded to Hub-4 evaluation as a task.
Palmer et al(1999) used error modeling, and Horlock & King (2003)proposed discriminative training to handle NER errors;both used a hidden Markov model (HMM).
Miller et al(1999) also reported results in English speech NERusing an HMM model.
In a NIST 1999 evaluation, itwas found that NER errors on speech arise from acombination of ASR errors and errors of the underlyingNER system.In this work, we investigate whether the NIST findingholds for Chinese speech NER as well.
We present thefirst known result for recognizing named entities inrealistic large-vocabulary spoken Chinese.
We proposeto use the best-known model for Chinese textual NER?a maximum entropy model?on Chinese speech NER.We also propose using re-segmentation and post-classification to improve this model.
Finally, wepropose to integrate the ASR and NER components tooptimize NER performance by making use of the n-bestASR output.2.
A Spoken Chinese NER Model2.1 LVCSR outputWe use the ASR output from BBN?s Byblos system onbroadcast news data from the Xinhua News Agency,which has 1046 sentences.
This system has a charactererror rate of 7%.
We had manually annotated them withnamed entities as an evaluation set according to the PFRcorpus annotation guideline (PFR 2001).2.2 A maximum-entropy NER model with post-classificationTo establish a baseline spoken Chinese NER model, weselected a maximum entropy (MaxEnt) approach sincethis is currently the single most accurate approachknown for recognizing named entities in text (TjongKim Sang et al, 2002, 2003, Jing et al, 2003)1.
In theCoNLL 2003 NER evaluation, 5 out of 16 systems useMaxEnt models and the top 3 results for English and top2 results for German were obtained by systems that useMaxEnt.Natural language can be viewed as a stochastic process.We can use p(y|x) to denote the probability distributionof what we try to predict y (.e.g.
part-of-speech tag,Named Entity tag) conditioned on what we observe x(e.g.
previous POS or the actual word).
The MaximumEntropy principle can be stated as follows: given someset of constrains from observations, find the mostuniform probability distribution (Maximum Entropy)p(y|x) that satisfies these constrains:00 0* arg max ( | )1( | ) exp( ( , ))( )( ) exp( ( , ))yi i imi i j j i ijil mi j j i kk jy P y xP y x f x yZ xZ x f x y?
?== === ?= ???
?In the above equations, fj(xi,yk) is a binary valued featurefunction, and ?j is a weight that indicates how importantfeature fj is for the model.
Z(xi) is a normalization factor.We estimate the weights using the improved iterativescaling (IIS) algorithm.For our task, we first compare a character-basedMaxEnt model to a word-based model.
Sincerecognition errors also lead to segmentation errorswhich in turn have an adverse effect on the NERperformance, we experiment with disregarding the wordboundaries in the ASR hypothesis and instead re-segment using a MaxEnt segmenter.
We also comparean approach of one-pass identification/classification to atwo-pass approach where the identified NE candidatesare classified later.
In addition, we propose a hybridapproach of using one-pass identification/classificationresults, discarding the extracted NE tags, and re-classifying the extracted NE in a second pass.1 We exclude from the present focus the slight improvementsthat are usually possible to obtain by combination of multiplemodels, usually through ad hoc methods such as voting.2.3 Experimental setupWe use two annotated corpora for training.
One is acorpus of People?s Daily newspaper from January 1998,annotated by the Institute of Computational Linguisticsof Beijing University (the ?PFR?
corpus).
This corpusconsists of about 20k sentences, annotated with wordsegmentation, part-of-speech tags and three named-entity tags including person (PER), location (LOC) andorganization (ORG) .
We use the first 6k sentences totrain our NER system.
Our system is then evaluated on2k sentences from People?s Daily and 1k sentences fromthe BBN ASR output.
The results are shown in Tables 1and 3.To compare our system to the IBM baseline describedin (Jing et al 2003), we need to evaluate our system onthe same corpus as they used.
Among the data they used,the only publicly available corpus is a human-generatedtranscription of broadcast news, provided by NIST forthe Information Extraction ?
Entity Recognitionevaluation (the ?IEER?
corpus).
This corpus consists of10 hours of training data and 1 hour of test data.
Tencategories of NEs were annotated, including personnames, location, organization, date, duration, andmeasure.
A comparison of results is shown in Table 2.2.4 Results and discussionFrom text to speechTable 1 compares the NER performances of the sameMaxEnt model on the Chinese textual PFR test data andthe one-best BBN ASR hypotheses.
We can see asignificant drop in performance in the latter.
Theseresults support the claim that transferring NERapproaches from text to spoken language is asignificantly more difficult task for Chinese than forEnglish.
We argue that this is due to the combination ofdifferent factors specific to spoken Chinese.
First,Chinese has a large number of homonyms that leads to adegradation in speech recognition accuracy which inturn leads to low NER accuracy.
Second, the vocabularyused in Chinese person names is an open set so manycharacters/words are unseen in the training data.Comparison to IBM baselineTable 2 compares results on IEER data from ourbaseline word-based MaxEnt model compared with thatof IBM?s HMM word-based model.
These two modelsachieved almost the same results, which show that ourNER system based on MaxEnt is state-of-the-art.Re-segmentation effectTable 3 shows that by discarding word boundaries fromthe ASR hypothesis, and then re-segmenting using ourMaxEnt segmenter, we obtained a better performance inmost cases.
We believe that some reduction insegmentation errors due to recognition errors is obtainedthis way; for example, in the ASR output, two words??
?
?
in ???
?
?
???
?
?
?
aremisrecognized as one word ???
?, which can becorrected by re-segmentation.Post-classification effectTable 3 also shows that the one-passidentification/classification method yields better resultsthan the two-pass method.
However, there are stillerrors in the one-pass output where the bracketing iscorrect, but the NE classification is wrong.
In particular,the type ORG is easily confusable with LOC in Chinese.Both types of NEs tend to be rather long.
We propose ahybrid approach by first using the one-pass method toextract NEs, and then removing all type information,combining words of one NE to a whole NE-word andpost-classifying all the NE-words again.
Our results inFigure 1 show that the post-classification combinedwith the one-pass approach performs much better on alltypes.Table 1.
NER results on Chinese speech data are worsethan on Chinese text data.Table 2.
Our NER baseline is comparable to the IBMbaseline.Table 3.
The character model is better than the wordmodel, and one-pass NER is better than two-pass.3.
Using N-Best Lists to Improve NERMiller et al (1999) performed NER on the one-besthypothesis of English Broadcast News data.
Palmer &Ostendorf (2001) and Horlock & King (2003) carriedout English NER on word lattices.
We are interested ininvestigating how to best utilize the n-best hypothesisfrom the ASR system to improve NER performances.From Figure 1, we can see that recall increases as thenumber of hypotheses increases.
Thus it would appearpossible to find a way to make use of the n-best ASRoutput, in order to improve the NER performance.However, we can expect it to be difficult to getsignificant improvement since the same figure (Figure 1)shows that precision drops much more quickly thanrecall.
This is because the nth hypothesis tends to havemore character errors than the (n-1)th hypothesis, whichmay lead to more NER errors.
Therefore the question is,given n NE-tagged hypotheses, what is the best way touse them to obtain a better NER overall performancethan by using the one-best hypothesis alone?One simple approach is to allow all the hypotheses tovote on a possible NE output.
In simple voting, arecognized named-entity is considered correct onlywhen it appears in more than 30 percent of the totalnumber of all the hypotheses for one utterance.
Theresult of this simple voting is shown in Table 4.
Next,we propose a mechanism of weighted voting usingconfidence measure for each hypothesis.
In oneexperiment, we use the MaxEnt NER score asconfidence measure.
In another experiment, we use allthe six scores (acoustic, language model, number ofwords, number of phones, number of silence, or NERscore) provided by the BBN ASR system as confidencemeasure.
During implementation, an optimizer based onPowell?s algorithm is used to find the 6 weights (?k) foreach score (Sk).
For any given hypothesis, confidencemeasure is given by:PER LOC ORG  P R F P R F P R FNewspapertext .86 .76 .81 .87 .75 .81 .83 .83 .831-best ASRhypothesis  .22 .18 .20 .75 .79 .77 .43 .35 .39Model Precision Recall F-measureIBM HMM 77.51% 65.22% 70.83%MaxEnt 77.3% 65.4% 70.9%61k kkW S ?== ?
?The above confidence measure is then normalized into afinal confidence measure for each hypothesis:1exp( )?exp( )ii NllWWW==?Finally, an NE output is considered valid if1?
( ) 0.Ni iiW NE?=?
>?
31,( )0,iNE occurs in the i th hypothesisNEOtherwise?
?
?= ?
?PER LOC ORGP R F P R F P R F2-pass,word  .23  .18  .20  .75  .79  .77  .43 .35 .391-pass,word .25  .20  .21  .76  .84  .80  .70 .25 .362-pass,character  .53  .43  .48  .67  .70  .68  .75 .59 .661-pass,character  .60  .45  .52  .56  .69  .62  .55 .35 .433.1 Experimental setupWe use the n-best hypothesis of 1,046 Broadcast NewsChinese utterances from the BBN LVCSR system.
nranges from one to 300, averaging at 68.
Each utterancehas a reference transcription with no recognition error.3.2 Results and discussionTable 4 presents the NER results for the referencesentence, one best hypothesis, and different n-bestvoting methods.
Results for the reference sentencesshow the upper bound performance (68% F-measure) ofapplying a MaxEnt NER system trained from theChinese text corpus (e.g., PFR) to Chinese speechoutput (e.g., Broadcast News).
From Table 4, we canconclude that it is possible to improve NER precision byusing n-best hypothesis by finding the optimizedcombination of different acoustic, language model,NER, and other scores.
In particular, since most errorsin Chinese ASR seem to be for person names, usingNER score on the n-best hypotheses can improverecognition results by a relative 6.7% in precision and1.7% in F-measure.PER LOC ORG ResultsF P F P F PReferencesentence0.71 0.75 0.78 0.77 0.56 0.72One best 0.46 0.50 0.75 0.74 0.54 0.69n-bestsimple vote0.45 0.59 0.76 0.75 0.56 0.71n-bestweighted vote(NE score)0.46 0.59 0.77 0.76 0.55 0.71n-bestweighted vote(all scores)0.48 0.53 0.75 0.73 0.55 0.69Table 4. n-best weighted voting with NE score gives thebest performance.RecallPrecisionF-measureFigure 1.
Post-classification improves NER performance.4.
ConclusionWe present the first known result for named entityrecognition (NER) in realistic large-vocabulary spokenChinese.
We apply a maximum entropy (MaxEnt)based system to the n-best output of the BBN LVCSRsystem on Chinese Broadcast News utterances.
Ourresults support the claim that transferring NERapproaches from text to spoken language is asignificantly more difficult task for Chinese than forEnglish.
We show that re-segmenting the ASRhypotheses improves the NER performance by 24%.
Wealso show that applying post-classification improves theNER performance by 13%.
Finally, we introduce amethod of using n-best hypotheses that yields a useful6.7% relative improvement in NER precision, and 1.7%relative improvement in F-measure, by weighted voting.Institute of Computational Linguistics, Beijing University.
2001.
The PFRcorpus.
http://icl.pku.edu.cn/research/corpus/shengming.htm.Hongyan JING, Radu FLORIAN, Xiaoqiang LUO, Tong ZHANG and AbrahamITTYCHERIAH.
2003.
HowtogetaChineseName(Entity): Segmentation andcombination issues.
Proceedings of EMNLP.
Sapporo, Japan: July 2003.David MILLER, Richard SCHWARTZ, Ralph WEISCHEDEL and Rebecca STONE.1999.
Named entity extraction from broadcast news.
Proceedings of theDARPA Broadcast News Workshop.
Herndon, Virginia: 1999.
37-40.David D. PALMER, Mari OSTENDORF and John D. BURGER.
1999.
Robustinformation extraction from spoken language data.
Proceedings ofEurospeech 1999.
Sep 1999.Acknowledgements.
We would like to thank the Hong KongResearch Grants Council (RGC) for supporting this researchin part via grants HKUST6206/03E, HKUST6256/00E,HKUST6083/99E, DAG03/04.EG30, and DAG03/04.EG09.Jian SUN, Ming ZHOU and Jianfeng GAO.
2003.
A class-based language modelapproach to Chinese named entity identification.
Computational Linguisticsand Chinese Language Processing.
2003.References Erik F. TJONG KIM SANG.
2002.
Introduction to the CoNLL-2002 Shared Task: Language-independent named entity recognition.
Proceedings of CoNLL-2002.
Taipei, Taiwan: 2002.
155-158.Silviu CUCERZAN and David YAROWSKY.
1999.
Language independent namedentity recognition combining morphological and contextual evidence.Proceedings of the 1999 Joint SIGDAT Conference on EMNLP and VLC.University of Maryland, MD.Erik F. TJONG KIM SANG and Fien DE MEULDER.
2003.
Introduction to theCoNLL-2003 Shared Task: Language-Independent Named EntityRecognition.
Proceedings of CoNLL-2003.
Edmonton, Canada.
142-147.James HORLOCK and Simon KING.
2003.
Discriminative Methods for ImprovingNamed Entity Extraction on Speech Data.
Proceedings of Eurospeech 2003.Geneva.Dekai WU, Grace NGAI and Marine CARPUAT.
2003.
A Stacked, Voted, StackedModel for Named Entity Recognition.
Proceedings of CoNLL-2003.Edmonton, Canada: 2003.
200-203.
