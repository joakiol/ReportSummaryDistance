Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218?226,Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics11,001 New Features for Statistical Machine Translation?David Chiang and Kevin KnightUSC Information Sciences Institute4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292 USAWei WangLanguage Weaver, Inc.4640 Admiralty Way, Suite 1210Marina del Rey, CA 90292 USAAbstractWe use the Margin Infused Relaxed Algo-rithm of Crammer et al to add a large num-ber of new features to two machine transla-tion systems: the Hiero hierarchical phrase-based translation system and our syntax-basedtranslation system.
On a large-scale Chinese-English translation task, we obtain statisticallysignificant improvements of +1.5 B???
and+1.1 B??
?, respectively.
We analyze the im-pact of the new features and the performanceof the learning algorithm.1 IntroductionWhat linguistic features can improve statistical ma-chine translation (MT)?
This is a fundamental ques-tion for the discipline, particularly as it pertains toimproving the best systems we have.
Further:?
Do syntax-based translation systems haveunique and effective levers to pull when design-ing new features??
Can large numbers of feature weights belearned efficiently and stably on modestamounts of data?In this paper, we address these questions by exper-imenting with a large number of new features.
Weadd more than 250 features to improve a syntax-based MT system?already the highest-scoring sin-gle system in the NIST 2008 Chinese-Englishcommon-data track?by +1.1 B???.
We also addmore than 10,000 features to Hiero (Chiang, 2005)and obtain a +1.5 B???
improvement.
?This research was supported in part by DARPA contractHR0011-06-C-0022 under subcontract to BBN Technologies.Many of the new features use syntactic informa-tion, and in particular depend on information thatis available only inside a syntax-based translationmodel.
Thus they widen the advantage that syntax-based models have over other types of models.The models are trained using the Margin InfusedRelaxed Algorithm or MIRA (Crammer et al, 2006)instead of the standard minimum-error-rate trainingor MERT algorithm (Och, 2003).
Our results addto a growing body of evidence (Watanabe et al,2007; Chiang et al, 2008) that MIRA is preferable toMERT across languages and systems, even for verylarge-scale tasks.2 Related WorkThe work of Och et al(2004) is perhaps the best-known study of new features and their impact ontranslation quality.
However, it had a few shortcom-ings.
First, it used the features for reranking n-bestlists of translations, rather than for decoding or for-est reranking (Huang, 2008).
Second, it attempted toincorporate syntax by applying off-the-shelf part-of-speech taggers and parsers to MT output, a task thesetools were never designed for.
By contrast, we incor-porate features directly into hierarchical and syntax-based decoders.A third difficulty with Och et al?s study was thatit used MERT, which is not an ideal vehicle for fea-ture exploration because it is observed not to per-form well with large feature sets.
Others have in-troduced alternative discriminative training meth-ods (Tillmann and Zhang, 2006; Liang et al, 2006;Turian et al, 2007; Blunsom et al, 2008; Machereyet al, 2008), in which a recurring challenge is scal-ability: to train many features, we need many train-218ing examples, and to train discriminatively, we needto search through all possible translations of eachtraining example.
Another line of research (Watan-abe et al, 2007; Chiang et al, 2008) tries to squeezeas many features as possible from a relatively smalldataset.
We follow this approach here.3 Systems Used3.1 HieroHiero (Chiang, 2005) is a hierarchical, string-to-string translation system.
Its rules, which are ex-tracted from unparsed, word-aligned parallel text,are synchronous CFG productions, for example:X?
X1 de X2,X2 of X1As the number of nonterminals is limited to two, thegrammar is equivalent to an inversion transductiongrammar (Wu, 1997).The baseline model includes 12 features whoseweights are optimized using MERT.
Two of the fea-tures are n-gram language models, which requireintersecting the synchronous CFG with finite-stateautomata representing the language models.
Thisgrammar can be parsed efficiently using cube prun-ing (Chiang, 2007).3.2 Syntax-based systemOur syntax-based system transforms source Chinesestrings into target English syntax trees.
Followingprevious work in statistical MT (Brown et al, 1993),we envision a noisy-channel model in which a lan-guage model generates English, and then a transla-tion model transforms English trees into Chinese.We represent the translation model as a tree trans-ducer (Knight and Graehl, 2005).
It is obtained frombilingual text that has been word-aligned and whoseEnglish side has been syntactically parsed.
From thisdata, we use the the GHKM minimal-rule extractionalgorithm of (Galley et al, 2004) to yield rules like:NP-C(x0:NPB PP(IN(of x1:NPB))?
x1 de x0Though this rule can be used in either direction,here we use it right-to-left (Chinese to English).
Wefollow Galley et al (2006) in allowing unalignedChinese words to participate in multiple translationrules, and in collecting larger rules composed ofminimal rules.
These larger rules have been shownto substantially improve translation accuracy (Gal-ley et al, 2006; DeNeefe et al, 2007).We apply Good-Turing discounting to the trans-ducer rule counts and obtain probability estimates:P(rule) = count(rule)count(LHS-root(rule))When we apply these probabilities to derive an En-glish sentence e and a corresponding Chinese sen-tence c, we wind up with the joint probability P(e, c).The baseline model includes log P(e, c), the twon-gram language models log P(e), and other featuresfor a total of 25.
For example, there is a pair offeatures to punish rules that drop Chinese contentwords or introduce spurious English content words.All features are linearly combined and their weightsare optimized using MERT.For efficient decoding with integrated n-gram lan-guage models, all transducer rules must be binarizedinto rules that contain at most two variables andcan be incrementally scored by the language model(Zhang et al, 2006).
Then we use a CKY-style parser(Yamada and Knight, 2002; Galley et al, 2006) withcube pruning to decode new sentences.We include two other techniques in our baseline.To get more general translation rules, we restruc-ture our English training trees using expectation-maximization (Wang et al, 2007), and to get morespecific translation rules, we relabel the trees with upto 4 specialized versions of each nonterminal sym-bol, again using expectation-maximization and thesplit/merge technique of Petrov et al (2006).3.3 MIRA trainingWe incorporate all our new features into a linearmodel (Och and Ney, 2002) and train them usingMIRA (Crammer et al, 2006), following previouswork (Watanabe et al, 2007; Chiang et al, 2008).Let e stand for output strings or their derivations,and let h(e) stand for the feature vector for e. Initial-ize the feature weights w. Then, repeatedly:?
Select a batch of input sentences f1, .
.
.
, fm anddecode each fi to obtain a forest of translations.?
For each i, select from the forest a set of hy-pothesis translations ei1, .
.
.
, ein, which are the21910-best translations according to each of:h(e) ?
wB???
(e) + h(e) ?
w?B???
(e) + h(e) ?
w(1)?
For each i, select an oracle translation:e?
= arg maxe(B???
(e) + h(e) ?
w) (2)Let ?hi j = h(e?i ) ?
h(ei j).?
For each ei j, compute the loss`i j = B???
(e?i ) ?
B???
(ei j) (3)?
Update w to the value of w?
that minimizes:12?w?
?
w?2 + Cm?i=1max1?
j?n(`i j ?
?hi j ?
w?)
(4)where C = 0.01.
This minimization is per-formed by a variant of sequential minimal opti-mization (Platt, 1998).Following Chiang et al (2008), we calculate the sen-tence B???
scores in (1), (2), and (3) in the contextof some previous 1-best translations.
We run 20 ofthese learners in parallel, and when training is fin-ished, the weight vectors from all iterations of alllearners are averaged together.Since the interface between the trainer and the de-coder is fairly simple?for each sentence, the de-coder sends the trainer a forest, and the trainer re-turns a weight update?it is easy to use this algo-rithm with a variety of CKY-based decoders: here,we are using it in conjunction with both the Hierodecoder and our syntax-based decoder.4 FeaturesIn this section, we describe the new features intro-duced on top of our baseline systems.Discount features Both of our systems calculateseveral features based on observed counts of rules inthe training data.
Though the syntax-based systemuses Good-Turing discounting when computing theP(e, c) feature, we find, as noted above, that it usesquite a few one-count rules, suggesting that theirprobabilities have been overestimated.
We can di-rectly attack this problem by adding features countithat reward or punish rules seen i times, or featurescount[i, j] for rules seen between i and j times.4.1 Target-side featuresString-to-tree MT offers some unique levers to pull,in terms of target-side features.
Because the systemoutputs English trees, we can analyze output trees onthe tuning set and design new features to encouragethe decoder to produce more grammatical trees.Rule overlap features While individual rules ob-served in decoder output are often quite reasonable,two adjacent rules can create problems.
For exam-ple, a rule that has a variable of type IN (preposi-tion) needs another rule rooted with IN to fill the po-sition.
If the second rule supplies the wrong prepo-sition, a bad translation results.
The IN node hereis an overlap point between rules.
Considering thatcertain nonterminal symbols may be more reliableoverlap points than others, we create a binary fea-ture for each nonterminal.
A rule like:IN(at)?
zaiwill have feature rule-root-IN set to 1 and allother rule-root features set to 0.
Our rule root fea-tures range over the original (non-split) nontermi-nal set; we have 105 in total.
Even though therule root features are locally attached to individualrules?and therefore cause no additional problemsfor the decoder search?they are aimed at problem-atic rule/rule interactions.Bad single-level rewrites Sometimes the decoderuses questionable rules, for example:PP(x0:VBN x1:NP-C)?
x0 x1This rule is learned from 62 cases in our trainingdata, where the VBN is almost always the wordgiven.
However, the decoder misuses this rule withother VBNs.
So we can add a feature that penalizesany rule in which a PP dominates a VBN and NP-C.The feature class bad-rewrite comprises penaltiesfor the following configurations based on our analy-sis of the tuning set:PP?
VBN NP-CPP-BAR?
NP-C INVP?
NP-C PPCONJP?
RB IN220Node count features It is possible that the de-coder creates English trees with too many or too fewnodes of a particular syntactic category.
For exam-ple, there may be an tendency to generate too manydeterminers or past-tense verbs.
We therefore add acount feature for each of the 109 (non-split) Englishnonterminal symbols.
For a rule likeNPB(NNP(us) NNP(president) x0:NNP)?
meiguo zongtong x0the feature node-count-NPB gets value 1, node-count-NNP gets value 2, and all others get 0.Insertion features Among the rules we extractfrom bilingual corpora are target-language insertionrules, which have a word on the English side, but nowords on the source Chinese side.
Sample syntax-based insertion rules are:NPB(DT(the) x0:NN)?
x0S(x0:NP-C VP(VBZ(is) x1:VP-C))?
x0 x1We notice that our decoder, however, frequently failsto insert words like is and are, which often have noequivalent in the Chinese source.
We also notice thatthe-insertion rules sometimes have a good effect, asin the translation ?in the bloom of youth,?
but othertimes have a bad effect, as in ?people seek areas ofthe conspiracy.
?Each time the decoder uses (or fails to use) an in-sertion rule, it incurs some risk.
There is no guaran-tee that the interaction of the rule probabilities andthe language model provides the best way to managethis risk.
We therefore provide MIRA with a featurefor each of the most common English words appear-ing in insertion rules, e.g., insert-the and insert-is.There are 35 such features.4.2 Source-side featuresWe now turn to features that make use of source-sidecontext.
Although these features capture dependen-cies that cross boundaries between rules, they arestill local in the sense that no new states need tobe added to the decoder.
This is because the entiresource sentence, being fixed, is always available toevery feature.Soft syntactic constraints Neither of our systemsuses source-side syntactic information; hence, bothcould potentially benefit from soft syntactic con-straints as described by Marton and Resnik (2008).In brief, these features use the output of an in-dependent syntactic parser on the source sentence,rewarding decoder constituents that match syntac-tic constituents and punishing decoder constituentsthat cross syntactic constituents.
We use separately-tunable features for each syntactic category.Structural distortion features Both of our sys-tems have rules with variables that generalize overpossible fillers, but neither system?s basic modelconditions a rule application on the size of a filler,making it difficult to distinguish long-distance re-orderings from short-distance reorderings.
To rem-edy this problem, Chiang et al (2008) introduce astructural distortion model, which we include in ourexperiment.
Our syntax-based baseline includes thegenerative version of this model already.Word context During rule extraction, we retainword alignments from the training data in the ex-tracted rules.
(If a rule is observed with more thanone set of word alignments, we keep only themost frequent one.)
We then define, for each triple( f , e, f+1), a feature that counts the number of timesthat f is aligned to e and f+1 occurs to the right off ; and similarly for triples ( f , e, f?1) with f?1 occur-ring to the left of f .
In order to limit the size of themodel, we restrict words to be among the 100 mostfrequently occurring words from the training data;all other words are replaced with a token <unk>.These features are somewhat similar to featuresused by Watanabe et al (2007), but more in the spiritof features used in the word sense disambiguationmodel introduced by Lee and Ng (2002) and incor-porated as a submodel of a translation system byChan et al (2007); here, we are incorporating someof its features directly into the translation model.5 ExperimentsFor our experiments, we used a 260 million wordChinese/English bitext.
We ran GIZA++ on the en-tire bitext to produce IBM Model 4 word align-ments, and then the link deletion algorithm (Fossumet al, 2008) to yield better-quality alignments.
For221System Training Features # Tune TestHiero MERT baseline 11 35.4 36.1MIRA syntax, distortion 56 35.9 36.9?syntax, distortion, discount 61 36.6 37.3?
?all source-side, discount 10990 38.4 37.6?
?Syntax MERT baseline 25 38.6 39.5MIRA baseline 25 38.5 39.8?overlap 132 38.7 39.9?node count 136 38.7 40.0?
?all target-side, discount 283 39.6 40.6?
?Table 1: Adding new features with MIRA significantly improves translation accuracy.
Scores are case-insensitive IBMB???
scores.
?
or ??
= significantly better than MERT baseline (p < 0.05 or 0.01, respectively).the syntax-based system, we ran a reimplementationof the Collins parser (Collins, 1997) on the Englishhalf of the bitext to produce parse trees, then restruc-tured and relabeled them as described in Section 3.2.Syntax-based rule extraction was performed on a 65million word subset of the training data.
For Hiero,rules with up to two nonterminals were extractedfrom a 38 million word subset and phrasal rules wereextracted from the remainder of the training data.We trained three 5-gram language models: one onthe English half of the bitext, used by both systems,one on one billion words of English, used by thesyntax-based system, and one on two billion wordsof English, used by Hiero.
Modified Kneser-Neysmoothing (Chen and Goodman, 1998) was appliedto all language models.
The language models arerepresented using randomized data structures simi-lar to those of Talbot et al (2007).Our tuning set (2010 sentences) and test set (1994sentences) were drawn from newswire data from theNIST 2004 and 2005 evaluations and the GALE pro-gram (with no overlap at either the segment or doc-ument level).
For the source-side syntax features,we used the Berkeley parser (Petrov et al, 2006) toparse the Chinese side of both sets.We implemented the source-side context featuresfor Hiero and the target-side syntax features for thesyntax-based system, and the discount features forboth.
We then ran MIRA on the tuning set with 20parallel learners for Hiero and 73 parallel learnersfor the syntax-based system.
We chose a stopping it-eration based on the B???
score on the tuning set,and used the averaged feature weights from all iter-Syntax-based Hierocount weight count weight1 +1.28 1 +2.232 +0.35 2 +0.773?5 ?0.73 3 +0.546?10 ?0.64 4 +0.295+ ?0.02Table 2: Weights learned for discount features.
Nega-tive weights indicate bonuses; positive weights indicatepenalties.ations of all learners to decode the test set.The results (Table 1) show significant improve-ments in both systems (p < 0.01) over already verystrong MERT baselines.
Adding the source-side anddiscount features to Hiero yields a +1.5 B???
im-provement, and adding the target-side syntax anddiscount features to the syntax-based system yields a+1.1 B???
improvement.
The results also show thatfor Hiero, the various classes of features contributedroughly equally; for the syntax-based system, we seethat two of the feature classes make small contribu-tions but time constraints unfortunately did not per-mit isolated testing of all feature classes.6 AnalysisHow did the various new features improve the trans-lation quality of our two systems?
We begin by ex-amining the discount features.
For these features,we used slightly different schemes for the two sys-tems, shown in Table 2 with their learned featureweights.
We see in both cases that one-count rulesare strongly penalized, as expected.222Reward?0.42 a?0.13 are?0.09 at?0.09 on?0.05 was?0.05 from?0.04 ?s?0.04 by?0.04 is?0.03 it?0.03 its...Penalty+0.67 of+0.56 the+0.47 comma+0.13 period+0.11 in+0.08 for+0.06 to+0.05 will+0.04 and+0.02 as+0.02 have...Table 3: Weights learned for inserting target Englishwords with rules that lack Chinese words.6.1 Syntax featuresTable 3 shows word-insertion feature weights.
Thesystem rewards insertion of forms of be; examples1?3 in Figure 1 show typical improved translationsthat result.
Among determiners, inserting a is re-warded, while inserting the is punished.
This seemsto be because the is often part of a fixed phrase, suchas the White House, and therefore comes naturallyas part of larger phrasal rules.
Inserting the outsidethese fixed phrases is a risk that the generative modelis too inclined to take.
We also note that the systemlearns to punish unmotivated insertions of commasand periods, which get into our grammar via quirksin the MT training data.Table 4 shows weights for rule-overlap features.MIRA punishes the case where rules overlap withan IN (preposition) node.
This makes sense: if arule has a variable that can be filled by any Englishpreposition, there is a risk that an incorrect preposi-tion will fill it.
On the other hand, splitting at a pe-riod is a safe bet, and frees the model to use rules thatdig deeper into NP and VP trees when constructinga top-level S. Table 5 shows weights for generatedEnglish nonterminals: SBAR-C nodes are rewardedand commas are punished.The combined effect of all weights is subtle.To interpret them further, it helps to look at grosschanges in the system?s behavior.
For example, amajor error in the baseline system is to move ?Xsaid?
or ?X asked?
from the beginning of the Chi-nese input to the middle or end of the English trans-Bonus?0.50 period?0.39 VP-C?0.36 VB?0.31 SG-C?0.30 MD?0.26 VBG?0.25 ADJP?0.22 -LRB-?0.21 VP-BAR?0.20 NPB-BAR?0.16 FRAG?0.16 PRN?0.15 NPB?0.13 RB?0.12 SBAR-C?0.12 VP-C-BAR?0.11 -RRB-...Penalty+0.93 IN+0.57 NNP+0.44 NN+0.41 DT+0.34 JJ+0.24 right double quote+0.20 VBZ+0.19 NP+0.16 TO+0.15 ADJP-BAR+0.14 PRN-BAR+0.14 NML+0.13 comma+0.12 VBD+0.12 NNPS+0.12 PRP+0.11 SG...Table 4: Weights learned for employing rules whose En-glish sides are rooted at particular syntactic categories.Bonus?0.73 SBAR-C?0.54 VBZ?0.54 IN?0.52 NN?0.51 PP-C?0.47 right double quote?0.39 ADJP?0.34 POS?0.31 ADVP?0.30 RP?0.29 PRT?0.27 SG-C?0.22 S-C?0.21 NNPS?0.21 VP-BAR?0.20 PRP?0.20 NPB-BAR...Penalty+1.30 comma+0.80 DT+0.58 PP+0.44 TO+0.33 NNP+0.30 NNS+0.30 NML+0.22 CD+0.18 PRN+0.16 SYM+0.15 ADJP-BAR+0.15 NP+0.15 MD+0.15 HYPH+0.14 PRN-BAR+0.14 NP-C+0.11 ADJP-C...Table 5: Weights learned for generating syntactic nodesof various types anywhere in the English translation.223lation.
The error occurs with many speaking verbs,and each time, we trace it to a different rule.
Theproblematic rules can even be non-lexical, e.g.
:S(x0:NP-C x1:VP x2:, x3:NP-C x4:VP x5:.)?
x3 x4 x2 x0 x1 x5It is therefore difficult to come up with a straightfor-ward feature to address the problem.
However, whenwe apply MIRA with the features already listed,these translation errors all disappear, as demon-strated by examples 4?5 in Figure 1.
Why does thishappen?
It turns out that in translation hypothesesthat move ?X said?
or ?X asked?
away from the be-ginning of the sentence, more commas appear, andfewer S-C and SBAR-C nodes appear.
Therefore, thenew features work to discourage these hypotheses.Example 6 shows additionally that commas next tospeaking verbs are now correctly deleted.Examples 7?8 in Figure 1 show other kinds ofunanticipated improvements.
We do not have spacefor a fuller analysis, but we note that the specific ef-fects we describe above account for only part of theoverall B???
improvement.6.2 Word context featuresIn Table 6 are shown feature weights learned for theword-context features.
A surprising number of thehighest-weighted features have to do with transla-tions of dates and bylines.
Many of the penaltiesseem to discourage spurious insertion or deletionof frequent words (for, ?s, said, parentheses, andquotes).
Finally, we note that several of the features(the third- and eighth-ranked reward and twelfth-ranked penalty) shape the translation of shuo ?said?,preferring translations with an overt complementizerthat and without a comma.
Thus these features worktogether to attack a frequent problem that our target-syntax features also addressed.Figure 2 shows the performance of Hiero with allof its features on the tuning and test sets over time.The scores on the tuning set rise rapidly, and thescores on the test set alo rise, but much more slowly,and there appears to be slight degradation after the18th pass through the tuning data.
This seems in linewith the finding of Watanabe et al (2007) that withon the order of 10,000 features, overfitting is possi-ble, but we can still improve accuracy on new data.3535.53636.53737.53838.50  5  10  15  20  25BLEUEpochTuneTestFigure 2: Using over 10,000 word-context features leadsto overfitting, but its detrimental effects are modest.Scores on the tuning set were obtained from the 1-bestoutput of the online learning algorithm, whereas scoreson the test set were obtained using averaged weights.Early stopping would have given +0.2 B???
over theresults reported in Table 1.17 ConclusionWe have described a variety of features for statisti-cal machine translation and applied them to syntax-based and hierarchical systems.
We saw that thesefeatures, discriminatively trained using MIRA, ledto significant improvements, and took a closer lookat the results to see how the new features qualita-tively improved translation quality.
We draw threeconclusions from this study.First, we have shown that these new features canimprove the performance even of top-scoring MTsystems.
Second, these results add to a growing bodyof evidence that MIRA is preferable to MERT fordiscriminative training.
When training over 10,000features on a modest amount of data, we, like Watan-abe et al (2007), did observe overfitting, yet sawimprovements on new data.
Third, we have shownthat syntax-based machine translation offers possi-bilities for features not available in other models,making syntax-based MT and MIRA an especiallystrong combination for future work.1It was this iteration, in fact, which was used to derive thecombined feature count used in the title of this paper.2241 MERT: the united states pending israeli clarification on golan settlement planMIRA: the united states is waiting for israeli clarification on golan settlement plan2 MERT: .
.
.
the average life expectancy of only 18 months , canada ?s minority goverment will .
.
.MIRA: .
.
.
the average life expectancy of canada?s previous minority government is only 18 months .
.
.3 MERT: .
.
.
since un inspectors expelled by north korea .
.
.MIRA: .
.
.
since un inspectors were expelled by north korea .
.
.4 MERT: another thing is .
.
.
, " he said , " obviously , the first thing we need to do .
.
.
.MIRA: he said : " obviously , the first thing we need to do .
.
.
, and another thing is .
.
.
.
"5 MERT: the actual timing .
.
.
reopened in january , yoon said .MIRA: yoon said the issue of the timing .
.
.6 MERT: .
.
.
us - led coalition forces , said today that the crash .
.
.MIRA: .
.
.
us - led coalition forces said today that a us military .
.
.7 MERT: .
.
.
and others will feel the danger .MIRA: .
.
.
and others will not feel the danger .8 MERT: in residential or public activities within 200 meters of the region , .
.
.MIRA: within 200 m of residential or public activities area , .
.
.Figure 1: Improved syntax-based translations due to MIRA-trained weights.Bonusf e context?1.19 <unk> <unk> f?1 = ri ?day?
?1.01 <unk> <unk> f?1 = (?0.84 , that f?1 = shuo ?say?
?0.82 yue ?month?
<unk> f+1 = <unk>?0.78 " " f?1 = <unk>?0.76 " " f+1 = <unk>?0.66 <unk> <unk> f+1 = nian ?year?
?0.65 , that f+1 = <unk>...Penaltyf e context+1.12 <unk> ) f+1 = <unk>+0.83 jiang ?shall?
be f+1 = <unk>+0.83 zhengfu ?government?
the f?1 = <unk>+0.73 <unk> ) f?1 = <unk>+0.73 <unk> ( f+1 = <unk>+0.72 <unk> ) f?1 = ri ?day?+0.70 <unk> ( f?1 = ri ?day?+0.69 <unk> ( f?1 = <unk>+0.66 <unk> for f?1 = <unk>+0.66 <unk> ?s f?1 = ,+0.65 <unk> said f?1 = <unk>+0.60 , , f?1 = shuo ?say?...Table 6: Weights learned for word-context features, which fire when English word e is generated aligned to Chineseword f , with Chinese word f?1 to the left or f+1 to the right.
Glosses for Chinese words are not part of features.225ReferencesPhil Blunsom, Trevor Cohn, and Miles Osborne.
2008.
Adiscriminative latent variable model for statistical ma-chine translation.
In Proc.
ACL-08: HLT.Peter F. Brown, Stephen A. Della Pietra, Vincent Della J.Pietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational Linguistics, 19(2):263?312.Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.Word sense disambiguation improves statistical ma-chine translation.
In Proc.
ACL 2007.Stanley F. Chen and Joshua T. Goodman.
1998.
Anempirical study of smoothing techniques for languagemodeling.
Technical Report TR-10-98, Computer Sci-ence Group, Harvard University.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proc.
EMNLP 2008.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
ACL 2005.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2).Michael Collins.
1997.
Three generative, lexicalizedmodels for statistical parsing.
In Proc.
ACL 1997.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
Journal of Machine LearningResearch, 7:551?585.Steve DeNeefe, Kevin Knight, Wei Wang, and DanielMarcu.
2007.
What can syntax-based MT learn fromphrase-based MT?
In Proc.
EMNLP-CoNLL-2007.Victoria Fossum, Kevin Knight, and Steven Abney.
2008.Using syntax to improve word alignment for syntax-based statistical machine translation.
In Proc.
ThirdWorkshop on Statistical Machine Translation.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
InProc.
HLT-NAACL 2004, Boston, Massachusetts.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic models.
In Proc.
ACL 2006.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proc.
ACL 2008.Kevin Knight and Jonathan Graehl.
2005.
An overviewof probabilistic tree transducers for natural languageprocessing.
In Proceedings of the Sixth InternationalConference on Intelligent Text Processing and Compu-tational Linguistics (CICLing).Yoong Keok Lee and Hwee Tou Ng.
2002.
An em-pirical evaluation of knowledge sources and learn-ing algorithms for word sense disambiguation.
InProc.
EMNLP 2002, pages 41?48.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminative ap-proach to machine translation.
In Proc.
COLING-ACL2006.Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,and Jakob Uskoreit.
2008.
Lattice-based minimumerror rate training for statistical machine translation.In Proc.
EMNLP 2008.Yuval Marton and Philip Resnik.
2008.
Soft syntacticconstraints for hierarchical phrased-based translation.In Proc.
ACL-08: HLT.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In Proc.
ACL 2002.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2004.
Asmorgasbord of features for statistical machine trans-lation.
In Proc.
HLT-NAACL 2004, pages 161?168.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
ACL 2003.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc.
ACL 2006.John C. Platt.
1998.
Fast training of support vectormachines using sequential minimal optimization.
InB.
Scho?lkopf, C. J. C. Burges, and A. J. Smola, editors,Advances in Kernel Methods: Support Vector Learn-ing, pages 195?208.
MIT Press.David Talbot and Miles Osborne.
2007.
Randomisedlanguage modelling for statistical machine translation.In Proc.
ACL 2007, pages 512?519.Christoph Tillmann and Tong Zhang.
2006.
A discrimi-native global training algorithm for statistical MT.
InProc.
COLING-ACL 2006.Joseph Turian, Benjamin Wellington, and I. DanMelamed.
2007.
Scalable discriminative learn-ing for natural language parsing and translation.
InProc.
NIPS 2006.Wei Wang, Kevin Knight, and Daniel Marcu.
2007.
Bi-narizing syntax trees to improve syntax-based machinetranslation accuracy.
In Proc.
EMNLP-CoNLL 2007.Taro Watanabe, Jun Suzuki, Hajime Tsukuda, and HidekiIsozaki.
2007.
Online large-margin training for statis-tical machine translation.
In Proc.
EMNLP 2007.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23:377?404.Kenji Yamada and Kevin Knight.
2002.
A decoder forsyntax-based statistical MT.
In Proc.
ACL 2002.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for machinetranslation.
In Proc.
HLT-NAACL 2006.226
