Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 621?626,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsCorrecting Keyboard Layout Errors and Homoglyphs in QueriesDerek Barnesdebarnes@ebay.comMahesh Joshimahesh.joshi@ebay.comeBay Inc., 2065 Hamilton Ave, San Jose, CA, 95125, USAHassan Sawafhsawaf@ebay.comAbstractKeyboard layout errors and homoglyphsin cross-language queries impact our abil-ity to correctly interpret user informa-tion needs and offer relevant results.We present a machine learning approachto correcting these errors, based largelyon character-level n-gram features.
Wedemonstrate superior performance overrule-based methods, as well as a signif-icant reduction in the number of queriesthat yield null search results.1 IntroductionThe success of an eCommerce site depends onhow well users are connected with products andservices of interest.
Users typically communi-cate their desires through search queries; however,queries are often incomplete and contain errors,which impact the quantity and quality of searchresults.New challenges arise for search engines incross-border eCommerce.
In this paper, we fo-cus on two cross-linguistic phenomena that makeinterpreting queries difficult: (i) Homoglyphs:(Miller, 2013): Tokens such as ?case?
(underlinedletters Cyrillic), in which users mix charactersfrom different character sets that are visually simi-lar or identical.
For instance, English and Russianalphabets share homoglyphs such as c, a, e, o, y,k, etc.
Although the letters are visually similar orin some cases identical, the underlying charactercodes are different.
(ii) Keyboard Layout Errors(KLEs): (Baytin et al., 2013): When switchingone?s keyboard between language modes, users attimes enter terms in the wrong character set.
Forinstance, ??????
?????
may appear to be a Rus-sian query.
While ???????
is the Russian wordfor ?case?, ??????
is actually the user?s attemptto enter the characters ?ipad?
while leaving theirkeyboard in Russian language mode.
Queries con-taining KLEs or homoglyphs are unlikely to pro-duce any search results, unless the intended ASCIIsequences can be recovered.
In a test set sam-pled from Russian/English queries with null (i.e.empty) search results (see Section 3.1), we foundapproximately 7.8% contained at least one KLE orhomoglyph.In this paper, we present a machine learningapproach to identifying and correcting query to-kens containing homoglyphs and KLEs.
We showthat the proposed method offers superior accuracyover rule-based methods, as well as significant im-provement in search recall.
Although we focus ourresults on Russian/English queries, the techniques(particularly for KLEs) can be applied to other lan-guage pairs that use different character sets, suchas Korean-English and Thai-English.2 MethodologyIn cross-border trade at eBay, multilingual queriesare translated into the inventory?s source languageprior to search.
A key application of this, andthe focus of this paper, is the translation of Rus-sian queries into English, in order to provide Rus-sian users a more convenient interface to English-based inventory in North America.
The presenceof KLEs and homoglyphs in multilingual queries,however, leads to poor query translations, which inturn increases the incidence of null search results.We have found that null search results correlatewith users exiting our site.In this work, we seek to correct for KLEs andhomoglyphs, thereby improving query translation,reducing the incidence of null search results, andincreasing user engagement.
Prior to translationand search, we preprocess multilingual queriesby identifying and transforming KLEs and homo-glyphs as follows (we use the query ??????
???
?2 new?
as a running example):(a) Tag Tokens: label each query token621with one of the following semantically moti-vated classes, which identify the user?s informa-tion need: (i) E: a token intended as an Englishsearch term; (ii) R: a Cyrillic token intended as aRussian search term; (iii) K: A KLE, e.g.
?????
?for the term ?ipad?.
A token intended as an En-glish search term, but at least partially entered inthe Russian keyboard layout; (iv) H: A Russianhomoglyph for an English term, e.g.
???w?
(un-derlined letters Cyrillic).
Employs visually sim-ilar letters from the Cyrillic character set whenspelling an intended English term; (v) A: Ambigu-ous tokens, consisting of numbers and punctuationcharacters with equivalent codes that can be en-tered in both Russian and English keyboard lay-outs.
Given the above classes, our example query??????
????
2 new?
should be tagged as ?R K AE?.
(b) Transform Queries: Apply a deterministicmapping to transform KLE and homoglyph tokensfrom Cyrillic to ASCII characters.
For KLEs thetransformation maps between characters that sharethe same location in Russian and English keyboardlayouts (e.g.
?
?
a, ?
?
s).
For homoglyphs thetransformation maps between a smaller set of vi-sually similar characters (e.g.
??
e, ??m).
Ourexample query would be transformed into ?????
?ipad 2 new?.
(c) Translate and Search: Translate the trans-formed query (into ?case ipad 2 new?
for our ex-ample), and dispatch it to the search engine.In this paper, we formulate the token-level tag-ging task as a standard multiclass classificationproblem (each token is labeled independently), aswell as a sequence labeling problem (a first orderconditional Markov model).
In order to provideend-to-end results, we preprocess queries by de-terministically transforming into ASCII the tokenstagged by our model as KLEs or homoglyphs.
Weconclude by presenting an evaluation of the impactof this transformation on search.2.1 FeaturesOur classification and sequence models share acommon set of features grouped into the follow-ing categories:2.1.1 Language Model FeaturesA series of 5-gram, character-level language mod-els (LMs) capture the structure of different typesof words.
Intuitively, valid Russian terms willhave high probability in Russian LMs.
In contrast,KLEs or homoglyph tokens, despite appearing onthe surface to be Russian terms, will generallyhave low probability in the LMs trained on validRussian words.
Once mapped into ASCII (seeSection 2 above), however, these tokens tend tohave higher probability in the English LMs.
LMsare trained on the following corpora:English and Russian Vocabulary: based ona collection of open source, parallel En-glish/Russian corpora (?50M words in all).English Brands: built from a curated list of 35KEnglish brand names, which often have distinctivelinguistic properties compared with common En-glish words (Lowrey et al., 2013).Russian Transliterations: built from a col-lection of Russian transliterations of propernames from Wikipedia (the Russian portion ofguessed-names.ru-en made available as apart of WMT 20131).For every input token, each of the above LMsfires a real-valued feature ?
the negated log-probability of the token in the given languagemodel.
Additionally, for tokens containing Cyril-lic characters, we consider the token?s KLE andhomoglyph ASCII mappings, where available.
Foreach mapping, a real-valued feature fires corre-sponding to the negated log-probability of themapped token in the English and Brands LMs.Lastly, an equivalent set of LM features fires forthe two preceding and following tokens around thecurrent token, if applicable.2.1.2 Token FeaturesWe include several features commonly used intoken-level tagging problems, such as case andshape features, token class (such as letters-only,digits-only), position of the token within the query,and token length.
In addition, we include fea-tures indicating the presence of characters fromthe ASCII and/or Cyrillic character sets.2.1.3 Dictionary FeaturesWe incorporate a set of features that indicatewhether a given lowercased query token is a mem-ber of one of the lexicons described below.UNIX: The English dictionary shipped with Cen-tOS, including ?480K entries, used as a lexiconof common English words.BRANDS: An expanded version of the curated listof brand names used for LM features.
Includes1www.statmt.org/wmt13/translation-task.html#download622?58K brands.PRODUCT TITLES: A lexicon of over 1.6M en-tries extracted from a collection of 10M producttitles from eBay?s North American inventory.QUERY LOGS: A larger, in-domain collection ofapproximately 5M entries extracted from ?100MEnglish search queries on eBay.Dictionary features fire for Cyrillic tokens whenthe KLE and/or homoglyph-mapped version of thetoken appears in the above lexicons.
Dictionaryfeatures are binary for the Unix and Brands dictio-naries, and weighted by relative frequency of theentry for the Product Titles and Query Logs dic-tionaries.3 Experiments3.1 DatasetsThe following datasets were used for training andevaluating the baseline (see Section 3.2 below) andour proposed systems:Training Set: A training set of 6472 human-labeled query examples (17,239 tokens).In-Domain Query Test Set: A set of 2500 Rus-sian/English queries (8,357 tokens) randomly se-lected from queries with null search results.
Byfocusing on queries with null results, we empha-size the presence of KLEs and homoglyphs, whichoccur in 7.8% of queries in our test set.Queries were labeled by a team of Russian lan-guage specialists.
The test set was also indepen-dently reviewed, which resulted in the correctionof labels for 8 out of the 8,357 query tokens.Although our test set is representative of thetypes of problematic queries targeted by ourmodel, our training data was not sampled using thesame methodology.
We expect that the differencesin distributions between training and test sets, ifanything, make the results reported in Section 3.3somewhat pessimistic2.3.2 Dictionary BaselineWe implemented a rule-based baseline system em-ploying the dictionaries described in Section 2.1.3.In this system, each token was assigned a classk ?
{E,R,K,H,A} using a set of rules: a tokenamong a list of 101 Russian stopwords3is tagged2As expected, cross-validation experiments on the train-ing data (for parameter tuning) yielded results slightly higherthan the results reported in Section 3.3, which use a held-outtest set3Taken from the Russian Analyzer packaged with Lucene?
see lucene.apache.org.as R. A token containing only ASCII characters islabeled as A if all characters are common to En-glish and Russian keyboards (i.e.
numbers andsome punctuation), otherwise E. For tokens con-taining Cyrillic characters, KLE and homoglyph-mapped versions are searched in our dictionaries.If found, K or H are assigned.
If both mapped ver-sions are found in the dictionaries, then either Kor H is assigned probabilistically4.
In cases whereneither mapped version is found in the dictionary,the token assigned is either R or A, depending onwhether it consists of purely Cyrillic characters, ora mix of Cyrillic and ASCII, respectively.Note that the above tagging rules allow tokenswith classes E and A to be identified with perfectaccuracy.
As a result, we omit these classes fromall results reported in this work.
We also notethat this simplification applies because we haverestricted our attention to the Russian ?
Englishdirection.
In the bidirectional case, ASCII tokenscould represent either English tokens or KLEs (i.e.a Russian term entered in the English keyboardlayout).
We leave the joint treatment of the bidi-rectional case to future work.Tag Prec Recall F1K .528 .924 .672H .347 .510 .413R .996 .967 .982Table 1: Baseline results on the test set, usingUNIX, BRANDS, and the PRODUCT TITLES dic-tionaries.We experimented with different combinationsof dictionaries, and found the best combination tobe UNIX, BRANDS, and PRODUCT TITLES dic-tionaries (see Table 1).
We observed a sharp de-crease in precision when incorporating the QUERYLOGS dictionary, likely due to noise in the user-generated content.Error analysis suggests that shorter words arethe most problematic for the baseline system5.Shorter Cyrillic tokens, when transformed fromCyrillic to ASCII using KLE or homoglyph map-pings, have a higher probability of spuriouslymapping to valid English acronyms, model IDs,or short words.
For instance, Russian car brand?????
maps across keyboard layouts to ?dfp?,4We experimented with selecting K or H based on a priorcomputed from training data; however, results were lowerthan those reported, which use random selection.5Stopwords are particularly problematic, and hence ex-cluded from consideration as KLEs or homoglyphs.623TagClassification SequenceP R F1 P R F1LRK .925 .944 .935 .915 .934 .925H .708 .667 .687 .686 .686 .686R .996 .997 .996 .997 .996 .997RFK .926 .949 .937 .935 .949 .942H .732 .588 .652 .750 .588 .659R .997 .997 .997 .996 .998 .997Table 2: Classification and sequence tagging re-sults on the test seta commonly used acronym in product titles for?Digital Flat Panel?.
Russian words ??????
and?????
similarly map by chance to English words?verb?
and ?her?.A related problem occurs with product modelIDs, and highlights the limits of treating query to-kens independently.
Consider Cyrillic query ????e46?.
The first token is a Russian transliterationfor the BMW brand.
The second token, ?e46?,has three possible interpretations: i) as a Russiantoken; ii) a homoglyph for ASCII ?e46?
; or iii)a KLE for ?t46?.
It is difficult to discriminatebetween these options without considering tokencontext, and in this case having some prior knowl-edge that e46 is a BMW model.3.3 Machine Learning ModelsWe trained linear classification models using lo-gistic regression (LR)6, and non-linear models us-ing random forests (RFs), using implementationsfrom the Scikit-learn package (Pedregosa et al.,2011).
Sequence models are implemented as firstorder conditional Markov models by applying abeam search (k = 3) on top of the LR and RFclassifiers.
The LR and RF models were tuned us-ing 5-fold cross-validation results, with models se-lected based on the mean F1 score across R, K, andH tags.Table 2 shows the token-level results on our in-domain test set.
As with the baseline, we focus themodel on disambiguating between classes R, K andH.
Each of the reported models performs signifi-cantly better than the baseline (on each tag), withstatistical significance evaluated usingMcNemar?stest.
The differences between LR and RF mod-els, as well as sequence and classification variants,however, are not statistically significant.
Each ofthe machine learning models achieves a query-level accuracy score of roughly 98% (the LR se-6Although CRFs are state-of-the-art for many taggingproblems, in our experiments they yielded results slightlylower than LR or RF models.quence model achieved the lowest with 97.78%,the RF sequence model the highest with 97.90%).Our feature ablation experiments show thatthe majority of predictive power comes from thecharacter-level LM features.
Dropping LM fea-tures results in a significant reduction in perfor-mance (F1 scores .878 and .638 for the RF Se-quence model on classes K and H).
These resultsare still significantly above the baseline, suggest-ing that token and dictionary features are by them-selves good predictors.
However, we do not seea similar performance reduction when droppingthese feature groups.We experimented with lexical features, whichare commonly used in token-level tagging prob-lems.
Results, however, were slightly lower thanthe results reported in this section.
We suspect theissue is one of overfitting, due to the limited size ofour training data, and general sparsity associatedwith lexical features.
Continuous word presenta-tions (Mikolov et al., 2013), noted as future work,may offer improved generalization.Error analysis for our machine learning mod-els suggests patterns similar to those reported inSection 3.2.
Although errors are significantly lessfrequent than in our dictionary baseline, shorterwords still present the most difficulty.
We noteas future work the use of word-level LM scoresto target errors with shorter words.3.4 Search ResultsRecall that we translate multilingual queries intoEnglish prior to search.
KLEs and homoglyphsin queries result in poor query translations, oftenleading to null search results.To evaluate the impact of KLE and homoglyphcorrection, we consider a set of 100k randomly se-lected Russian/English queries.
We consider thesubset of queries that the RF or baseline modelspredict as containing a KLE or homoglyph.
Next,we translate into English both the original query,as well as a transformed version of it, with KLEsand homoglyphs replaced with their ASCII map-pings.
Lastly, we execute independent searchesusing original and transformed query translations.Table 3 provides details on search results fororiginal and transformed queries.
The baselinemodel transforms over 12.6% of the 100k queries.Of those, 24.3% yield search results where the un-modified queries had null search results (i.e.
Null?
Non-null).
In 20.9% of the cases, however, the624transformations are destructive (i.e.
Non-null ?Null), and yield null results where the unmodifiedquery produced results.Compared with the baseline, the RF modeltransforms only 7.4% of the 100k queries; a frac-tion that is roughly in line with the 7.8% of queriesin our test set that contain KLEs or homoglyphs.In over 42% of the cases (versus 24.3% for thebaseline), the transformed query generates searchresults where the original query yields none.
Only4.81% of the transformations using the RF modelare destructive; a fraction significantly lower thanthe baseline.Note that we distinguish here only betweenqueries that produce null results, and those that donot.
We do not include queries for which originaland transformed queries both produce (potentiallydiffering) search results.
Evaluating these casesrequires deeper insight into the relevance of searchresults, which is left as future work.Baseline RF model#Transformed 12,661 7,364Null?
Non-Null 3,078 (24.3%) 3,142 (42.7%)Non-Null?
Null 2,651 (20.9%) 354 (4.81%)Table 3: Impact of KLE and homoglyph correctionon search results for 100k queries4 Related WorkBaytin et al.
(2013) first refer to keyboard lay-out errors in their work.
However, their focus ison predicting the performance of spell-correction,not on fixing KLEs observed in their data.
Toour knowledge, our work is the first to introducethis problem and to propose a machine learningsolution.
Since our task is a token-level taggingproblem, it is very similar to the part-of-speech(POS) tagging task (Ratnaparkhi, 1996), only witha very small set of candidate tags.
We chosea supervised machine learning approach in orderto achieve maximum precision.
However, thisproblem can also be approached in an unsuper-vised setting, similar to the methodWhitelaw et al.
(2009) use for spelling correction.
In that setup,the goal would be to directly choose the correcttransformation for an ill-formed KLE or homo-glyph, instead of a tagging step followed by a de-terministic mapping to ASCII.5 Conclusions and Future WorkWe investigate two kinds of errors in searchqueries: keyboard layout errors (KLEs) and ho-moglyphs.
Applying machine learning methods,we are able to accurately identify a user?s intendedquery, in spite of the presence of KLEs and ho-moglyphs.
The proposed models are based largelyon compact, character-level language models.
Theproposed techniques, when applied to multilingualqueries prior to translation and search, offer signif-icant gains in search results.In the future, we plan to focus on additional fea-tures to improve KLE and homoglyph discrimina-tion for shorter words and acronyms.
Althoughlexical features did not prove useful for this work,presumably due to data sparsity and overfittingissues, we intend to explore the application ofcontinuous word representations (Mikolov et al.,2013).
Compared with lexical features, we expectcontinuous representations to be less susceptibleto overfitting, and to generalize better to unknownwords.
For instance, using continuous word rep-resentations, Turian et al.
(2010) show significantgains for a named entity recognition task.We also intend on exploring the use of featuresfrom in-domain, word-level LMs.
Word-level fea-tures are expected to be particularly useful in thecase of spurious mappings (e.g.
?????
vs. ?dfp?from Section 3.2), where context from surround-ing tokens in a query can often help in resolvingambiguity.
Word-level features may also be usefulin re-ranking translated queries prior to search, inorder to reduce the incidence of erroneous querytransformations generated through our methods.Finally, our future work will explore KLE and ho-moglyph correction bidirectionally, as opposed tothe unidirectional approach explored in this work.AcknowledgmentsWe would like to thank Jean-David Ruvini, MikeDillinger, Sa?sa Hasan, Irina Borisova and theanonymous reviewers for their valuable feedback.We also thank our Russian language special-ists Tanya Badeka, Tatiana Kontsevich and OlgaPospelova for their support in labeling and review-ing datasets.ReferencesAlexey Baytin, Irina Galinskaya, Marina Panina, andPavel Serdyukov.
2013.
Speller performance pre-625diction for query autocorrection.
In Proceedingsof the 22nd ACM International Conference on Con-ference on Information & Knowledge Management,pages 1821?1824.Tina M. Lowrey, Larry J. Shrum, and Tony M. Du-bitsky.
2013.
The Relation Between Brand-nameLinguistic Characteristics and Brand-nameMemory.Journal of Advertising, 32(3):7?17.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
CoRR, abs/1301.3781.Tristan Miller.
2013.
Russian?English Homoglyphs,Homographs, and Homographic Translations.
WordWays: The Journal of Recreational Linguistics,46(3):165?168.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and Edouard Duchesnay.
2011.Scikit-learn: Machine Learning in Python.
Journalof Machine Learning Research, 12:2825?2830.Adwait Ratnaparkhi.
1996.
A Maximum EntropyModel for Part?of?Speech Tagging.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 133?142.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings ofACL, pages 384?394.Casey Whitelaw, Ben Hutchinson, Grace Y. Chung,and Ged Ellis.
2009.
Using the Web for Lan-guage Independent Spellchecking and Autocorrec-tion.
In Proceedings of the 2009 Conference on Em-pirical Methods in Natural Language Processing,pages 890?899.626
