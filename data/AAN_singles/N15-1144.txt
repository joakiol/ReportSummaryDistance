Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1311?1316,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsUnsupervised POS Induction with Word EmbeddingsChu-Cheng Lin Waleed Ammar Chris Dyer Lori LevinSchool of Computer ScienceCarnegie Mellon University{chuchenl,wammar,cdyer,lsl}@cs.cmu.eduAbstractUnsupervised word embeddings have beenshown to be valuable as features in supervisedlearning problems; however, their role in unsu-pervised problems has been less thoroughly ex-plored.
In this paper, we show that embeddingscan likewise add value to the problem of unsu-pervised POS induction.
In two representativemodels of POS induction, we replace multi-nomial distributions over the vocabulary withmultivariate Gaussian distributions over wordembeddings and observe consistent improve-ments in eight languages.
We also analyze theeffect of various choices while inducing wordembeddings on ?downstream?
POS inductionresults.1 IntroductionUnsupervised POS induction is the problem of as-signing word tokens to syntactic categories givenonly a corpus of untagged text.
In this paper we ex-plore the effect of replacing words with their vectorspace embeddings1 in two POS induction models:the classic first-order HMM (Kupiec, 1992) and thenewly introduced conditional random field autoen-coder (Ammar et al, 2014).
In each model, insteadof using a conditional multinomial distribution2 togenerate a word token wi ?
V given a POS tag ti ?
T ,we use a conditional Gaussian distribution and gen-erate a d-dimensional word embedding vwi ?
Rdgiven ti .1Unlike Yatbaz et al (2014), we leverage easily obtainableand widely used embeddings of word types.2Also known as a categorical distribution.Our findings suggest that, in both models, sub-stantial improvements are possible when word em-beddings are used rather than opaque word types.However, the independence assumptions made bythe model used to induce embeddings strongly deter-mines its effectiveness for POS induction: embeddingmodels that model short-range context are more ef-fective than those that model longer-range contexts.This result is unsurprising, but it illustrates the lackof an evaluation metric that measures the syntactic(rather than semantic) information in word embed-dings.
Our results also confirm the conclusions ofSirts et al (2014) who were likewise able to improvePOS induction results, albeit using a custom clus-tering model based on the the distance-dependentChinese restaurant process (Blei and Frazier, 2011).Our contributions are as follows: (i) reparameter-ization of token-level POS induction models to useword embeddings; and (ii) a systematic evaluationof word embeddings with respect to the syntacticinformation they contain.2 Vector Space Word EmbeddingsWord embeddings represent words in a language?svocabulary as points in a d-dimensional space suchthat nearby words (points) are similar in terms of theirdistributional properties.
A variety of techniques forlearning embeddings have been proposed, e.g., matrixfactorization (Deerwester et al, 1990; Dhillon et al,2011) and neural language modeling (Mikolov et al,2011; Collobert and Weston, 2008).For the POS induction task, we specifically needembeddings that capture syntactic similarities.
There-fore we experiment with two types of embeddings1311that are known for such properties:?
Skip-gram embeddings (Mikolov et al, 2013) arebased on a log bilinear model that predicts an un-ordered set of context words given a target word.Bansal et al (2014) found that smaller context win-dow sizes tend to result in embeddings with moresyntactic information.
We confirm this finding inour experiments.?
Structured skip-gram embeddings (Ling et al,2015) extend the standard skip-gram embeddings(Mikolov et al, 2013) by taking into account therelative positions of words in a given context.We use the tool word2vec3 and Ling et al (2015)?smodified version4 to generate both plain and struc-tured skip-gram embeddings in nine languages.3 Models for POS InductionIn this section, we briefly review two classes of mod-els used for POS induction (HMMs and CRF autoen-coders), and explain how to generate word embed-ding observations in each class.
We will represent asentence of length ` as w = ?w1,w2, .
.
.
,w`?
?
V `and a sequence of tags as t = ?t1, t2, .
.
.
, t`?
?
T`.The embeddings of word type w ?
V will be writtenas vw ?
Rd.3.1 Hidden Markov ModelsThe hidden Markov model with multinomial emis-sions is a classic model for POS induction.
Thismodel makes the assumption that a latent Markov pro-cess with discrete states representing POS categoriesemits individual words in the vocabulary accordingto state (i.e., tag) specific emission distributions.
AnHMM thus defines the following joint distributionover sequences of observations and tags:p(w, t) =?`i=1p(ti | ti?1) ?
p(wi | ti ) (1)where distributions p(ti | ti?1) represents the transi-tion probability and p(wi | ti ) is the emission prob-ability, the probability of a particular tag generatingthe word at position i.5We consider two variants of the HMM as baselines:3https://code.google.com/p/word2vec/4https://github.com/wlin12/wang2vec/5Terms for the starting and stopping transition probabilitiesare omitted for brevity.?
p(wi | ti ) is parameterized as a ?na?ve multino-mial?
distribution with one distinct parameter foreach word type.?
p(wi | ti ) is parameterized as a multinomial logis-tic regression model with hand-engineered featuresas detailed in (Berg-Kirkpatrick et al, 2010).Gaussian Emissions.
We now consider incorporat-ing word embeddings in the HMM.
Given a tag t ?
T ,instead of generating the observed word w ?
V , wegenerate the (pre-trained) embedding vw ?
Rdof thatword.
The conditional probability density assignedto vw | t follows a multivariate Gaussian distributionwith mean ?t and covariance matrix ?t :p(vw ; ?t ,?t ) =exp(?12(vw ?
?t )>?
?1t (vw ?
?t ))?
(2pi)d|?t |(2)This parameterization makes the assumption that em-beddings of words which are often tagged as t areconcentrated around some point ?t ?
Rd, and theconcentration decays according to the covariance ma-trix ?t .6Now, the joint distribution over a sequence ofobservations v = ?vw1,vw2.
.
.
,vw` ?
(which corre-sponds to word sequence w = ?w1,w2, .
.
.
,w`,?)
anda tag sequence t = ?t1, t2.
.
.
, t`?
becomes:p(v, t) =?`i=1p(ti | ti?1) ?
p(vwi ; ?ti ,?ti )We use the Baum?Welch algorithm to fit the ?tand ?ti parameters.
In every iteration, we update ?t?as follows:?newt?
=?v?T?i=1...` p(ti = t?| v) ?
vwi?v?T?i=1...` p(ti = t?| v)(3)where T is a data set of word embedding sequencesv each of length |v| = `, and p(ti = t?| v) is theposterior probability of label t?at position i in thesequence v. Likewise the update to ?t?
is:?newt?
=?v?T?i=1...` p(ti = t?| v) ?
?
?>?v?T?i=1...` p(ti = t?| v)(4)where ?
= vwi ?
?newt?
.6?Essentially, all models are wrong, but some are useful.?
?George E. P. Box13123.2 Conditional Random Field AutoencodersThe second class of models this work extends iscalled CRF autoencoders, which we recently pro-posed in (Ammar et al, 2014).
It is a scalable familyof models for feature-rich learning from unlabeledexamples.
The model conditions on one copy of thestructured input, and generates a reconstruction ofthe input via a set of interdependent latent variableswhich represent the linguistic structure of interest.
Asshown in Eq.
5, the model factorizes into two distinctparts: the encoding model p(t | w) and the recon-struction model p(w?
| t); where w is the structuredinput (e.g., a token sequence), t is the linguistic struc-ture of interest (e.g., a sequence of POS tags), andw?
is a generic reconstruction of the input.
For POSinduction, the encoding model is a linear-chain CRFwith feature vector ?
and local feature functions f.p(w?,t | w) = p(t | w) ?
p(w?
| t)?
p(w?
| t) ?
exp?
?|w |?i=1f(ti , ti?1,w) (5)In (Ammar et al, 2014), we explored two kinds ofreconstructions w?
: surface forms and Brown clusters(Brown et al, 1992), and used ?stupid multinomials?as the underlying distributions for re-generating w?.Gaussian Reconstruction.
In this paper, we use d-dimensional word embedding reconstructions w?i =vwi ?
Rd, and replace the multinomial distribution ofthe reconstruction model with the multivariate Gaus-sian distribution in Eq.
2.
We again use the Baum?Welch algorithm to estimate ?t?
and ?t?
similar toEq.
3.
The only difference is that posterior label prob-abilities are now conditional on both the input se-quence w and the embeddings sequence v, i.e., re-place p(ti = t?| v) in Eq.
2 with p(ti = t?| w,v).4 ExperimentsIn this section, we attempt to answer the followingquestions:?
?4.1: Do syntactically-informed word embeddingsimprove POS induction?
Which model performsbest??
?4.2: What kind of word embeddings are suitablefor POS induction?4.1 Choice of POS Induction ModelsHere, we compare the following models for POSinduction:?
Baseline: HMM with multinomial emissions (Ku-piec, 1992),?
Baseline: HMM with log-linear emissions (Berg-Kirkpatrick et al, 2010),?
Baseline: CRF autoencoder with multinomial re-constructions (Ammar et al, 2014),7?
Proposed: HMM with Gaussian emissions, and?
Proposed: CRF autoencoder with Gaussian recon-structions.Data.
To train the POS induction models, we usedthe plain text from the training sections of theCoNLL-X shared task (Buchholz and Marsi, 2006)(for Danish and Turkish), the CoNLL 2007 sharedtask (Nivre et al, 2007) (for Arabic, Basque, Greek,Hungarian and Italian), and the Ukwabelana corpus(Spiegler et al, 2010) (for Zulu).
For evaluation, weobtain the corresponding gold-standard POS tags bydeterministically mapping the language-specific POStags in the aforementioned corpora to the correspond-ing universal POS tag set (Petrov et al, 2012).
Thisis the same set up we used in (Ammar et al, 2014).Setup.
In this section, we used skip-gram (i.e.,word2vec) embeddings with a context window size= 1 and with dimensionality d = 100, trained withthe largest corpora for each language in (Quasthoffet al, 2006), in addition to the plain text used to trainthe POS induction models.8 In the proposed models,we only show results for estimating ?t , assuminga diagonal covariance matrix ?t (k, k) = 0.45?k ?
{1, .
.
.
,d}.9 While the CRF autoencoder with multi-nomial reconstructions were carefully initialized as7We use the configuration with best performance which re-constructs Brown clusters.8We used the corpus/tokenize-anything.sh script inthe cdec decoder (Dyer et al, 2010) to tokenize the corporafrom (Quasthoff et al, 2006).
The other corpora were alreadytokenized.
In Arabic and Italian, we found a lot of discrepanciesbetween the tokenization used for inducing word embeddingsand the tokenization used for evaluation.
We expect our resultsto improve with consistent tokenization.9Surprisingly, we found that estimating ?t significantly de-grades the performance.
This may be due to overfitting (Shi-nozaki and Kawahara, 2007).
Possible remedies include using aprior (Gauvain and Lee, 1994).1313Arabic Basque Danish Greek Hungarian Italian Turkish Zulu AverageV?measure0.00.20.40.60.8 Multinomial HMMMultinomial Featurized HMM Multinomial CRF AutoencoderGaussian HMM Gaussian CRF AutoencoderArabic Basque Danish Greek Hungarian Italian Turkish Zulu AverageV?measure0.00.20.40.60.8 HMM (standard skip?gram)CRF Autoencoder (standard skip?gram) HMM (structured skip?gram)CRF Autoencoder (structured skip?gram)Figure 1: POS induction results.
(V-measure, higher is better.)
Window size is 1 for all word embeddings.Left: Models which use standard skip-gram word embeddings (i.e., Gaussian HMM and Gaussian CRFAutoencoder) outperform all baselines on average across languages.
Right: comparison between standardand structured skip-grams on Gaussian HMM and CRF Autoencoder.discussed in (Ammar et al, 2014), CRF autoencoderwith Gaussian reconstructions were initialized uni-formly at random in [?1,1].
All HMM models werealso randomly initialized.
We tuned all hyperparame-ters on the English PTB corpus, then fixed them forall languages.Evaluation.
We use the V-measure evaluation met-ric (Rosenberg and Hirschberg, 2007) to evaluate thepredicted syntactic classes at the token level.10Results.
The results in Fig.
1 clearly suggest thatwe can use word embeddings to improve POS induc-tion.
Surprisingly, the feature-less Gaussian HMMmodel outperforms the strong feature-rich baselines:Multinomial Featurized HMM and Multinomial CRFAutoencoder.
One explanation is that our word em-beddings were induced using larger unlabeled cor-pora than those used to train the POS induction mod-els.
The best results are obtained using both word em-beddings and feature-rich models using the GaussianCRF autoencoder model.
This set of results suggestthat word embeddings and hand-engineered featuresplay complementary roles in POS induction.
It isworth noting that the CRF autoencoder model withGaussian reconstructions did not require careful ini-tialization.1110We found the V-measure results to be consistent with themany-to-one evaluation metric (Johnson, 2007).
We only showone set of results for brevity.11In (Ammar et al, 2014), we found that careful initializationfor the CRF autoencoder model with multinomial reconstructionsis necessary.4.2 Choice of EmbeddingsStandard skip-gram vs. structured skip-gram.On Gaussian HMMs, structured skip-gram embed-dings score moderately higher than standard skip-grams.
And as context window size gets larger, thegap widens (as shown in Fig.
2.)
The reason maybe that structured skip-gram embeddings give eachposition within the context window its own projectmatrix, so the smearing effect is not as pronouncedas the window grows when compared to the standardembeddings.
However the best performance is stillobtained when window size is small.12Dimensions = 20 vs. 200.
We also varied thenumber of dimensions in the word vectors (d ?{20,50,100,200}).
The best V-measure we obtainis 0.504 (d = 20) and the worst is 0.460 (d = 100).However, we did not observe a consistent pattern asshown in Fig.
3.Window size = 1 vs. 16.
Finally, we varied the win-dow size for the context surrounding target words(w ?
{1,2,4,8,16}).
w = 1 yields the best averageV-measure across the eight languages as shown inFig.
2.
This is true for both standard and structured12In preliminary experiments, we also compared standard skip-gram embeddings to SENNA embeddings (Collobert et al, 2011)(which are trained in a semi-supervised multi-task learning setup,with one task being POS tagging) on a subset of the EnglishPTB corpus.
As expected, the induced POS tags are much betterwhen using SENNA embeddings, yielding a V-measure score of0.57 compared to 0.51 for skip-gram embeddings.
Since SENNAembeddings are only available in English, we did not include itin the comparison in Fig.
1.13141 2 4 8 16Window sizeavg.
V?measure0.300.45standard skip?gram structured skip?gramFigure 2: Effect of window size and embeddings typeon POS induction over the languages in Fig.
1. d =100.
The model is HMM with Gaussian emissions.skip-gram models.
Notably, larger window sizes ap-pear to produce word embeddings with less syntacticinformation.
This result confirms the observations ofBansal et al (2014).4.3 DiscussionWe have shown that (re)generating word embeddingsdoes much better than generating opaque word typesin unsupervised POS induction.
At a high level, thisconfirms prior findings that unsupervised word em-beddings capture syntactic properties of words, andshows that different embeddings capture more syn-tactically salient information than others.
As such,we contend that unsupervised POS induction can beseen as a diagnostic metric for assessing the syntacticquality of embeddings.To get a better understanding of what the multi-variate Gaussian models have learned, we conduct ahill-climbing experiment on our English dataset.
Weseed each POS category with the average vector of10 randomly sampled words from that category andtrain the model.
Seeding unsurprisingly improves tag-ging performance.
We also find words that are thenearest to the centroids generally agree with the cor-rect category label, which validate our assumptionthat syntactically similar words tend to cluster in thehigh-dimensional embedding space.
It also showsthat careful initialization of model parameters canbring further improvements.However we also find that words that are closeto the centroid are not necessarily representative ofwhat linguists consider to be prototypical.
For exam-ple, Hopper and Thompson (1983) show that physical,telic, past tense verbs are more prototypical with re-spect to case marking, agreement, and other syntactic20 50 100 200Dimension sizeV?measure0.300.45Figure 3: Effect of dimension size on POS inductionon a subset of the English PTB corpus.
w = 1.
Themodel is HMM with Gaussian emissions.behavior.
However, the verbs nearest our centroid allseem rather abstract.
In English, the nearest 5 wordsin the verb category are entails, aspires, attaches,foresees, deems.
This may be because these wordsseldom serve functions other than verbs; and plac-ing the centroid around them incurs less penalty (incontrast to physical verbs, e.g.
bite, which often alsoact as nouns).
Therefore one should be cautious ininterpreting what is prototypical about them.5 ConclusionWe propose using a multivariate Gaussian model togenerate vector space representations of observedwords in generative or hybrid models for POS induc-tion, as a superior alternative to using multinomialdistributions to generate categorical word types.
Wefind the performance from a simple Gaussian HMMcompetitive with strong feature-rich baselines.
Wefurther show that substituting the emission part of theCRF autoencoder can bring further improvements.We also confirm previous findings which suggestthat smaller context windows in skip-gram modelsresult in word embeddings which encode more syn-tactic information.
It would be interesting to see if wecan apply this approach to other tasks which requiregenerative modeling of textual observations such aslanguage modeling and grammar induction.AcknowledgementsThis work was sponsored by the U.S. Army ResearchLaboratory and the U.S. Army Research Office un-der contract/grant numbers W911NF-11-2-0042 andW911NF-10-1-0533.
The statements made herein aresolely the responsibility of the authors.1315ReferencesWaleed Ammar, Chris Dyer, and Noah A. Smith.
2014.Conditional random field autoencoders for unsuper-vised structured prediction.
In NIPS.Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014.Tailoring continuous word representations for depen-dency parsing.
In Proc.
of ACL.Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?t?, JohnDeNero, and Dan Klein.
2010.
Painless unsupervisedlearning with features.
In Proc.
of NAACL.David M. Blei and Peter I. Frazier.
2011.
Distance depen-dent Chinese restaurant processes.
JMLR.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InCoNLL-X.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neuralnetworks with multitask learning.
In Proc.
of ICML.Ronan Collobert, Jason Weston, L?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.JMLR, 12:2493?2537, November.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican society for information science, 41(6):391?407.Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.
2011.Multi-view learning of word embeddings via CCA.
InNIPS, volume 24.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, and Philip Resnik.
2010. cdec: Adecoder, alignment, and learning framework for finite-state and context-free translation models.
In Proc.
ofACL.J.
Gauvain and Chin-Hui Lee.
1994.
Maximum a pos-teriori estimation for multivariate Gaussian mixtureobservations of Markov chains.
Speech and Audio Pro-cessing, IEEE Transactions on, 2(2):291?298, Apr.Paul Hopper and Sandra Thompson.
1983.
The iconicityof the universal categories ?noun?
and ?verb?.
In JohnHaiman, editor, Iconicity in Syntax: Proceedings of asymposium on iconicity in syntax.Mark Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers?
In Proc.
of EMNLP.Julian Kupiec.
1992.
Robust part-of-speech tagging us-ing a hidden Markov model.
Computer Speech andLanguage, 6:225?242.Wang Ling, Chris Dyer, Alan Black, and Isabel Trancoso.2015.
Two/too simple adaptations of word2vec forsyntax problems.
In Proc.
of NAACL.Tomas Mikolov, Stefan Kombrink, Anoop Deoras, LukarBurget, and J Cernocky.
2011.
RNNLM ?
recurrentneural network language modeling toolkit.
In Proc.
ofthe 2011 ASRU Workshop, pages 196?201.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
ArXiv e-prints, January.Joakim Nivre, Johan Hall, Sandra Kubler, Ryan McDonald,Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007.The CoNLL 2007 shared task on dependency parsing.In Proc.
of CoNLL.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In Proc.
of LREC,May.Uwe Quasthoff, Matthias Richter, and Christian Biemann.2006.
Corpus portal for search in monolingual corpora.In Proc.
of LREC, pages 1799?1802.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clusterevaluation measure.
In EMNLP-CoNLL.T.
Shinozaki and T. Kawahara.
2007.
HMM trainingbased on CV-EM and CV gaussian mixture optimiza-tion.
In Proc.
of the 2007 ASRU Workshop, pages 318?322, Dec.Kairit Sirts, Jacob Eisenstein, Micha Elsner, and SharonGoldwater.
2014.
POS induction with distribu-tional and morphological information using a distance-dependent Chinese restaurant process.
In Proc.
of ACL.Sebastian Spiegler, Andrew van der Spuy, and Peter A.Flach.
2010.
Ukwabelana: An open-source morpho-logical Zulu corpus.
In Proc.
of COLING, pages 1020?1028.Mehmet Ali Yatbaz, Enis R?fat Sert, and Deniz Yuret.2014.
Unsupervised instance-based part of speech in-duction using probable substitutes.
In Proc.
of COL-ING.1316
