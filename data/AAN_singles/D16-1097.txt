Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 961?967,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsNeural Morphological Analysis: Encoding-Decoding Canonical SegmentsKatharina KannCenter for Information and Language ProcessingLMU Munich, Germanykann@cis.lmu.deRyan CotterellDepartment of Computer ScienceJohns Hopkins University, USAryan.cotterell@jhu.eduHinrich Schu?tzeCenter for Information and Language ProcessingLMU Munich, Germanyinquiries@cislmu.orgAbstractCanonical morphological segmentation aimsto divide words into a sequence of stan-dardized segments.
In this work, wepropose a character-based neural encoder-decoder model for this task.
Additionally,we extend our model to include morpheme-level and lexical information through a neuralreranker.
We set the new state of the art forthe task improving previous results by up to21% accuracy.
Our experiments cover threelanguages: English, German and Indonesian.1 IntroductionMorphological segmentation aims to divide wordsinto morphemes, meaning-bearing sub-word units.Indeed, segmentations have found use in a diverseset of NLP applications, e.g., automatic speechrecognition (Afify et al, 2006), keyword spot-ting (Narasimhan et al, 2014), machine transla-tion (Clifton and Sarkar, 2011) and parsing (Seekerand C?etinog?lu, 2015).
In the literature, most re-search has traditionally focused on surface segmen-tation, whereby a word w is segmented into a se-quence of substrings whose concatenation is the en-tire word; see Ruokolainen et al (2016) for a sur-vey.
In contrast, we consider canonical segmenta-tion: w is divided into a sequence of standardizedsegments.
To make the difference concrete, con-sider the following example: the surface segmen-tation of the complex English word achievability isachiev+abil+ity, whereas its canonical segmenta-tion is achieve+able+ity, i.e., we restore the alter-ations made during word formation.Canonical versions of morphological segmenta-tion have been introduced multiple times in the lit-erature (Kay, 1977; Naradowsky and Goldwater,2009; Cotterell et al, 2016).
Canonical segmen-tation has several representational advantages oversurface segmentation, e.g., whether two words sharea morpheme is no longer obfuscated by orthogra-phy.
However, it also introduces a hard algorith-mic challenge: in addition to segmenting a word,we must reverse orthographic changes, e.g., map-ping achievability7?achieveableity.Computationally, canonical segmentation can beseen as a sequence-to-sequence problem: we mustmap a word form to a canonicalized version withsegmentation boundaries.
Inspired by the re-cent success of neural encoder-decoder models(Sutskever et al, 2014) for sequence-to-sequenceproblems in NLP, we design a neural architecturefor the task.
However, a na?
?ve application of theencoder-decoder model ignores much of the linguis-tic structure of canonical segmentation?it cannotdirectly model the individual canonical segments,e.g., it cannot easily produce segment-level embed-dings.
To solve this, we use a neural reranker ontop of the encoder-decoder, allowing us to embedboth characters and entire segments.
The combinedapproach outperforms the state of the art by a widemargin (up to 21% accuracy) in three languages: En-glish, German and Indonesian.2 Neural Canonical SegmentationWe begin by formally describing the canonicalsegmentation task.
Given a discrete alphabet?
(e.g., the 26 letters of the English alphabet),961Figure 1: Detailed view of the attention mechanism of the neu-ral encoder-decoder.our goal is to map a word w ?
??
(e.g.,w=achievability), to a canonical segmentation c ???
(e.g., c=achieve+able+ity).
We define ?
=??
{+}, where + is a distinguished separation sym-bol.
Additionally, we will write the segmented formas c=?1+?2+.
.
.+?n, where each segment ?i ?
?
?and n is the number of canonical segments.We take a probabilistic approach and, thus, at-tempt to learn a distribution p(c | w).
Our modelconsists of two parts.
First, we apply an encoder-decoder recurrent neural network (RNN) (Bahdanauet al, 2014) to the sequence of characters of theinput word to obtain candidate canonical segmen-tations.
Second, we define a neural reranker thatallows us to embed individual morphemes andchooses the final answer from within a set of can-didates generated by the encoder-decoder.2.1 Neural Encoder-DecoderOur encoder-decoder is based on Bahdanau et al(2014)?s neural machine translation model.1 The en-coder is a bidirectional gated RNN (GRU) (Cho etal., 2014b).
Given a word w ?
?
?, the input to1github.com/mila-udem/blocks-examples/tree/master/machine_translationthe encoder is the sequence of characters of w, rep-resented as one-hot vectors.
The decoder definesa conditional probability distribution over c ?
?
?given w:pED(c |w) =|c|?t=1p(ct|c1, .
.
.
, ct?1, w)=|c|?t=1g(ct?1, st, at)where g is a nonlinear activation function, st is thestate of the decoder at t and at is a weighted sum ofthe |w| states of the encoder.
The state of the encoderfor wi is the concatenation of forward and backwardhidden states ?
?hi and?
?hi for wi.
An overview of howthe attention weight and the weighted sum at areincluded in the architecture can be seen in Figure1.
The attention weights ?t,i at each timestep t arecomputed based on the respective encoder state andthe decoder state st. See Bahdanau et al (2014) forfurther details.2.2 Neural RerankerThe encoder-decoder, while effective, predicts eachoutput character in ?
sequentially.
It does not useexplicit representations for entire segments and is in-capable of incorporating simple lexical information,e.g., does this canonical segment occur as an inde-pendent word in the lexicon?
Therefore, we extendour model with a reranker.The reranker rescores canonical segmentationsfrom a candidate set, which in our setting is sampledfrom pED.
Let the sample set be Sw = {k(i)}Ni=1where k(i) ?
pED(c | w).
We define the neuralreranker asp?
(c |w)=exp(u> tanh(Wvc) + ?
log pED(c |w))Z?where vc=?ni=1 v?i (recall c = ?1+?2+.
.
.+?n)and v?i is a one-hot morpheme embedding of ?iwith an additional binary dimension marking if ?ioccurs independently as a word in the language.2The partition function is Z?
(w) and the parame-ters are ?
= {u,W, ?}.
The parameters W and u2To determine if a canonical segment is in the lexicon, wecheck its occurrence in ASPELL.
Alternatively, one could askwhether it occurs in a large corpus, e.g., Wikipedia.962are projection and hidden layers, respectively, of amulti-layered perceptron and ?
can be seen as a tem-perature parameter that anneals the encoder-decodermodel pED (Kirkpatrick, 1984).
We define the parti-tion function over the sample set Sw:Z?
=?k?Swexp(u>tanh(Wvk)+?
log pED(k |w)).The reranking model?s ability to embed mor-phemes is important for morphological segmenta-tion since we often have strong corpus-level signals.The reranker also takes into account the character-level information through the score of the encoder-decoder model.
Due to this combination we expectstronger performance.3 Related WorkVarious approaches to morphological segmentationhave been proposed in the literature.
In the un-supervised realm, most work has been based onthe principle of minimum description length (Coverand Thomas, 2012), e.g., LINGUISTICA (Goldsmith,2001; Lee and Goldsmith, 2016) or MORFESSOR(Creutz and Lagus, 2002; Creutz et al, 2007; Poonet al, 2009).
MORFESSOR was later extended to asemi-supervised version by Kohonen et al (2010).Supervised approaches have also been considered.Most notably, Ruokolainen et al (2013) developeda supervised approach for morphological segmen-tation based on conditional random fields (CRFs)which they later extended to work also in a semi-supervised way (Ruokolainen et al, 2014) usingletter successor variety features (Hafer and Weiss,1974).
Similarly, Cotterell et al (2015) improvedperformance with a semi-Markov CRF.More recently, Wang et al (2016) achieved state-of-the-art results on surface morphological segmen-tation using a window LSTM.
Even though Wang etal.
(2016) also employ a recurrent neural network,we distinguish our approach, in that we focus oncanonical morphological segmentation, rather thansurface morphological segmentation.Naturally, our approach is also relevant to otherapplications of recurrent neural network transduc-tion models (Sutskever et al, 2014; Cho et al,2014a).
In addition to machine translation (Bah-danau et al, 2014), these models have been success-fully applied to many areas of NLP, including pars-ing (Vinyals et al, 2015), morphological reinflec-tion (Kann and Schu?tze, 2016) and automatic speechrecognition (Graves and Schmidhuber, 2005; Graveset al, 2013).4 ExperimentsTo enable comparison to earlier work, we use adataset that was prepared by Cotterell et al (2016)for canonical segmentation.34.1 LanguagesThe dataset we work on covers 3 languages: En-glish, German and Indonesian.
English and Germanare West Germanic Languages, with the former be-ing an official languages in nearly 60 different statesand the latter being mainly spoken in Western Eu-rope.
Indonesian ?
or Bahasa Indonesia?
is theofficial language of Indonesia.Cotterell et al (2016) report the best experimentalresults for Indonesian, followed by English and fi-nally German.
The high error rate for German mightbe caused by it being rich in orthografic changes.
Incontrast, Indonesian morphology is comparativelysimple.4.2 CorporaThe data for the English language was extractedfrom segmentations derived from the CELEXdatabase (Baayen et al, 1993).
The German datawas extracted from DerivBase (Zeller et al, 2013),which provides a collection of derived forms to-gether with the transformation rules, which wereused to create the canonical segmentations.
Finally,the data for Bahasa Indonesia was collected by us-ing the output of the MORPHIND analyzer (Larasatiet al, 2011), together with an open-source corpus ofIndonesian.
For each language we used the 10,000forms that were selected at random by Cotterell etal.
(2016) from a uniform distribution over types toform the corpus.
Following them, we perform ourexperiments on 5 splits of the data into 8000 train-ing forms, 1000 development forms and 1000 testforms and report averages.3ryancotterell.github.io/canonical-segmentation9634.3 TrainingWe train an ensemble of five encoder-decoder mod-els.
The encoder and decoder RNNs each have100 hidden units.
Embedding size is 300.
We useADADELTA (Zeiler, 2012) with a minibatch size of20.
We initialize all weights (encoder, decoder, em-beddings) to the identity matrix and the biases tozero (Le et al, 2015).
All models are trained for 20epochs.
The hyperparameter values are taken fromKann and Schu?tze (2016) and kept unchanged forthe application to canonical segmentation describedhere.To train the reranking model, we first gather thesample set Sw on the training data.
We take 500individual samples, but (as we often sample thesame form multiple times) |Sw| ?
5.
We op-timize the log-likelihood of the training data usingADADELTA.
For generalization, we employ L2 reg-ularization and we perform grid search to determinethe coefficient ?
?
{0.0, 0.1, 0.2, 0.3, 0.4, 0.5}.
Todecode the model, we again take 500 samples topopulate Sw and select the best segmentation.Baselines.
Our first baseline is the joint transductionand segmentation model (JOINT) of Cotterell et al(2016).
It is the current state of the art on the datasetswe use and the task of canonical segmentation ingeneral.
This model uses a jointly trained, separatetransduction and segmentation component.
Impor-tantly, the joint model of Cotterell et al (2016) al-ready contains segment-level features.
Thus, rerank-ing this baseline would not provide a similar boost.Our second baseline is a weighted finite-statetransducer (WFST) (Mohri et al, 2002) with a log-linear parameterization (Dreyer et al, 2008), again,taken from Cotterell et al (2016).
The WFSTbaseline is particularly relevant because, like ourencoder-decoder, it formulates the problem directlyas a string-to-string transduction.Evaluation Metrics.
We follow Cotterell et al(2016) and use the following evaluation measures:error rate, edit distance and morpheme F1.
Errorrate is defined as 1 minus the proportion of guessesthat are completely correct.
Edit distance is the Lev-enshtein distance between guess and gold standard.For this, guess and gold are each represented as onestring with a distinguished character denoting thesegment boundaries.
Morpheme F1 compares theRR ED Joint WFST UBerror en .19 (.01) .25 (.01) 0.27 (.02) 0.63 (.01) .06 (.01)de .20 (.01) .26 (.02) 0.41 (.03) 0.74 (.01) .04 (.01)id .05 (.01) .09 (.01) 0.10 (.01) 0.71 (.01) .02 (.01)edit en .21 (.02) .47 (.02) 0.98 (.34) 1.35 (.01) .10 (.02)de .29 (.02) .51 (.03) 1.01 (.07) 4.24 (.20) .06 (.01)id .05 (.00) .12 (.01) 0.15 (.02) 2.13 (.01) .02 (.01)F 1en .82 (.01) .78 (.01) 0.76 (.02) 0.53 (.02) .96 (.01)de .87 (.01) .86 (.01) 0.76 (.02) 0.59 (.02) .98 (.00)id .96 (.01) .93 (.01) 0.80 (.01) 0.62 (.02) .99 (.00)Table 1: Error rate (top), edit distance (middle), F1 (bottom)for canonical segmentation.
Each double column gives the mea-sure and its standard deviation.
Best result on each line (exclud-ing UB) in bold.
RR: encoder-decoder+reranker.
ED: encoder-decoder.
JOINT, WFST: baselines (see text).
UB: upper bound,the maximum score our reranker could obtain, i.e., consideringthe best sample in the predictions of ED.morphemes in guess and gold.
Precision (resp.
re-call) is the proportion of morphemes in guess (resp.gold) that occur in gold (resp.
guess).5 ResultsThe results of the canonical segmentation experi-ment in Table 1 show that both of our models im-prove over all baselines.
The encoder-decoder alonehas a .02 (English), .15 (German) and .01 (Indone-sion) lower error rate than the best baseline.
Theencoder-decoder improves most for the language forwhich the baselines did worst.
This suggests that, formore complex languages, a neural network modelmight be a good choice.The reranker achieves an additional improvementof .04 to .06. for the error rate.
This is likely dueto the additional information the reranker has accessto: morpheme embeddings and existing words.Important is also the upper bound we report.
Itshows the maximum performance the reranker couldachieve, i.e., evaluates the best solution that appearsin the set of candidate answers for the reranker.
Theright answer is contained in?
94% of samples.
Notethat, even though the upper bound goes up with thenumber of samples we take, there is no guaranteefor any finite number of samples that they will con-tain the true answer.
Thus, we would need to takean infinite number of samples to get a perfect upperbound.
However, as the current upper bound is quitehigh, the encoder-decoder proves to be an appropri-964ate model for the task.
Due to the large gap betweenthe performance of the encoder-decoder and the up-per bound, a better reranker could further increaseperformance.
We will investigate ways to improvethe reranker in future work.Error analysis.
We give for representative samplesthe error (E for the segmentation produced by ourmethod) and the correct analysis (G for gold).We first analyze cases in which the right an-swer does not appear at all in the samplesdrawn from the encoder-decoder.
Those in-clude problems with umlauts in German (G:verflu?chtigen7?
ver+flu?chten+ig, E: verflucht+ig)and orthographic changes at morpheme boundaries(G:cutter7?cut+er, E: cutter or cutt+er, sampledwith similar frequency).
There are also errors thatare due to problems with the annotation, e.g., the fol-lowing two gold segmentations are arguably incor-rect: tec7?detective and syrerin7?syr+er+in (syr isneither a word nor an affix in German).In other cases, the encoder-decoder does find theright solution (G), but gives a higher probabilityto an incorrect analysis (E).
Examples are a wrongsplit into adjectives or nouns instead of verbs (G:fu?gsamkeit7?fu?gen+sam+keit, E: fu?gsam+keit),the other way around (G: za?hler7?zahl+er, E:za?hlen+er), cases where the wrong morphemesare chosen (G: precognition7?pre+cognition, E:precognit+ion), difficult cases where letters haveto be inserted (G: redolence7?redolent+ence, E:re+dolence) or words the model does not splitup, even though they should be (G: additive7?addition+ive, E: additive).Based on its access to lexical information andmorpheme embeddings, the reranker is able tocorrect some of the errors made by the encoder-decoder.
Samples are G: geschwisterpa?rchen7?geschwisterpaar+chen, E: geschwisterpar+chen(geschwisterpaar is a word in German but geschwis-terpar is not) or G: zickig 7?
zicken+ig, E: zick+ig(with zicken, but not zick, being a German word).Finally, we want to know if segments that appearin the test set without being present in the trainingset are a source of errors.
In order to investigatethat, we split the test samples into two groups: Thefirst group contains the samples for which our sys-tem finds the right answer.
The second one containsall other samples.
We compare the percentage ofwrong samples right samples27.33 (.02) 36.60 (.01)Table 2: Percentage of segments in the solutions for the testdata that do not appear in the training set - split by samples thatour system does or does not get right.
We use the German dataand average over the 5 splits.
Standard deviation in parenthesis.samples that do not appear in the training data forboth groups.
We exemplarily use the German dataand the results results are shown in Table 2.
First,it can be seen that very roughly about a third of allsegments does not appear in the training data.
Thisis mainly due to unseen lemmas as their stems arenaturally unknown to the system.
However, the cor-rectly solved samples contain nearly 10% more un-seen segments.
As the average number of segmentsper word for wrong and right solutions ?
2.44 and2.11, respectively ?
does not differ by much, itseems unlikely that many errors are caused by un-known segments.6 Conclusion and Future WorkWe developed a model consisting of an encoder-decoder and a neural reranker for the task of canoni-cal morphological segmentation.
Our model com-bines character-level information with features onthe morpheme level and external information aboutwords.
It defines a new state of the art, improv-ing over baseline models by up to .21 accuracy, 16points F1 and .77 Levenshtein distance.We found that ?
94% of correct segmentationsare in the sample set drawn from the encoder-decoder model, demonstrating the upper bound onthe performance of our reranker is quite high; in fu-ture work, we hope to develop models to exploit this.AcknowledgmentsWe gratefully acknowledge the financial support ofSiemens for this research.ReferencesMohamed Afify, Ruhi Sarikaya, Hong-Kwang Jeff Kuo,Laurent Besacier, and Yuqing Gao.
2006.
On the useof morphological analysis for dialectal Arabic speechrecognition.
In Proc.
of INTERSPEECH.R.
H. Baayen, R. Piepenbrock, and H. Van Rijn.
1993.The CELEX lexical data base on CD-ROM.965Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
arXiv preprintarXiv:1409.0473.Kyunghyun Cho, Bart van Merrie?nboer, Dzmitry Bah-danau, and Yoshua Bengio.
2014a.
On the proper-ties of neural machine translation: Encoder-decoderapproaches.
arXiv preprint arXiv:1409.1259.Kyunghyun Cho, Bart Van Merrie?nboer, C?alar Gu?lc?ehre,Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,and Yoshua Bengio.
2014b.
Learning phrase repre-sentations using RNN encoder?decoder for statisticalmachine translation.
In Proc.
of EMNLP.Ann Clifton and Anoop Sarkar.
2011.
Combin-ing morpheme-based machine translation with post-processing morpheme prediction.
In Proc.
of ACL.Ryan Cotterell, Thomas Mu?ller, Alexander Fraser, andHinrich Schu?tze.
2015.
Labeled morphological seg-mentation with semi-markov models.
In Proc.
ofCoNLL.Ryan Cotterell, Tim Vieira, and Hinrich Schu?tze.
2016.A joint model of orthography and morphological seg-mentation.
In Proc.
of NAACL.Thomas M Cover and Joy A Thomas.
2012.
Elements ofInformation Theory.
John Wiley & Sons.Mathias Creutz and Krista Lagus.
2002.
Unsuperviseddiscovery of morphemes.
In Proc.
of the ACL-02Workshop on Morphological and Phonological Learn-ing.Mathias Creutz, Teemu Hirsima?ki, Mikko Kurimo, AnttiPuurula, Janne Pylkko?nen, Vesa Siivola, Matti Var-jokallio, Ebru Arisoy, Murat Sarac?lar, and AndreasStolcke.
2007.
Morph-based speech recognitionand modeling of out-of-vocabulary words across lan-guages.
ACM Transactions on Speech and LanguageProcessing, 5(1):3:1?3:29.Markus Dreyer, Jason R. Smith, and Jason Eisner.
2008.Latent-variable modeling of string transductions withfinite-state methods.
In Proc.
of EMNLP.John Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
ComputationalLinguistics, 27(2):153?198.Alex Graves and Ju?rgen Schmidhuber.
2005.
Frame-wise phoneme classification with bidirectional lstmand other neural network architectures.
Neural Net-works, 18(5):602?610.Alan Graves, Abdel-rahman Mohamed, and GeoffreyHinton.
2013.
Speech recognition with deep recurrentneural networks.
In Proc of.
ICASSP.Margaret A. Hafer and Stephen F. Weiss.
1974.
Wordsegmentation by letter successor varieties.
Informa-tion storage and retrieval, 10(11):371?385.Katharina Kann and Hinrich Schu?tze.
2016.
Single-model encoder-decoder with explicit morphologicalrepresentation for reinflection.
In Proc.
of ACL.Martin Kay.
1977.
Morphological and syntactic analysis.Linguistic Structures Processing, 5:131?234.Scott Kirkpatrick.
1984.
Optimization by simulated an-nealing: Quantitative studies.
Journal of StatisticalPhysics, 34(5-6):975?986.Oskar Kohonen, Sami Virpioja, and Krista Lagus.
2010.Semi-supervised learning of concatenative morphol-ogy.
In Proc.
of the 11th Meeting of the ACL Spe-cial Interest Group on Computational Morphology andPhonology.Septina Dian Larasati, Vladislav Kubon?, and Daniel Ze-man.
2011.
Indonesian morphology tool (morphind):Towards an indonesian corpus.
In Proc.
of SFCM.Springer.Quoc V. Le, Navdeep Jaitly, and Geoffrey E. Hin-ton.
2015.
A simple way to initialize recurrentnetworks of rectified linear units.
arXiv preprintarXiv:1504.00941.Jackson L. Lee and John A. Goldsmith.
2016.
Linguis-tica 5: Unsupervised learning of linguistic structure.In Proc.
of NAACL.Mehryar Mohri, Fernando Pereira, and Michael Ri-ley.
2002.
Weighted finite-state transducers inspeech recognition.
Computer Speech & Language,16(1):69?88.Jason Naradowsky and Sharon Goldwater.
2009.
Im-proving morphology induction by learning spellingrules.
In Proc.
of IJCAI.Karthik Narasimhan, Damianos Karakos, RichardSchwartz, Stavros Tsakalidis, and Regina Barzilay.2014.
Morphological segmentation for keyword spot-ting.
In Proc.
of EMNLP.Hoifung Poon, Colin Cherry, and Kristina Toutanova.2009.
Unsupervised morphological segmentation withlog-linear models.
In Proc.
of NAACL.Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja, andMikko Kurimo.
2013.
Supervised morphological seg-mentation in a low-resource learning setting using con-ditional random fields.
In Proc.
of CoNLL.Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja,and mikko kurimo.
2014.
Painless semi-supervisedmorphological segmentation using conditional randomfields.
In Proc.
of EACL.Teemu Ruokolainen, Oskar Kohonen, Kairit Sirts, Stig-Arne Gro?nroos, Mikko Kurimo, and Sami Virpioja.2016.
Comparative study of minimally supervisedmorphological segmentation.
Computational Linguis-tics, 42(1):91?120.966Wolfgang Seeker and O?zlem C?etinog?lu.
2015.
A graph-based lattice dependency parser for joint morphologi-cal segmentation and syntactic analysis.
TACL, 3:359?373.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to sequence learning with neural networks.In Proc.
of NIPS.Oriol Vinyals, ?ukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey Hinton.
2015.
Grammaras a foreign language.
In Proc.
of NIPS.Linlin Wang, Zhu Cao, Yu Xia, and Gerard de Melo.2016.
Morphological segmentation with windowLSTM neural networks.
In Proc.
of AAAI.Matthew D Zeiler.
2012.
Adadelta: an adaptive learningrate method.
arXiv preprint arXiv:1212.5701.Britta Zeller, Jan S?najder, and Sebastian Pado?.
2013.
De-rivbase: Inducing and evaluating a derivational mor-phology resource for german.
In Proc.
of ACL.967
