Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 16?23,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsUsing WordNet and Semantic Similarity for Bilingual TerminologyMining from Comparable CorporaDhouha BouamorCEA, LIST, Vision andContent Engineering Laboratory,91191 Gif-sur-Yvette CEDEXFrancedhouha.bouamor@cea.frNasredine SemmarCEA, LIST, Vision and ContentEngineering Laboratory,91191 Gif-sur-YvetteCEDEX Francenasredine.semmar@cea.frPierre ZweigenbaumLIMSI-CNRS,F-91403 Orsay CEDEXFrancepz@limsi.frAbstractThis paper presents an extension of thestandard approach used for bilingual lex-icon extraction from comparable corpora.We study of the ambiguity problem re-vealed by the seed bilingual dictionaryused to translate context vectors.
Forthis purpose, we augment the standard ap-proach by a Word Sense Disambiguationprocess relying on a WordNet-based se-mantic similarity measure.
The aim ofthis process is to identify the translationsthat are more likely to give the best rep-resentation of words in the target lan-guage.
On two specialized French-Englishcomparable corpora, empirical experimen-tal results show that the proposed methodconsistently outperforms the standard ap-proach.1 IntroductionBilingual lexicons play a vital role in many Natu-ral Language Processing applications such as Ma-chine Translation (Och and Ney, 2003) or Cross-Language Information Retrieval (Shi, 2009).
Re-search on lexical extraction from multilingual cor-pora have largely focused on parallel corpora.
Thescarcity of such corpora in particular for special-ized domains and for language pairs not involv-ing English pushed researchers to investigate theuse of comparable corpora (Fung, 1998; Chiaoand Zweigenbaum, 2003).
These corpora are com-prised of texts which are not exact translation ofeach other but share common features such as do-main, genre, sampling period, etc.The main work in this research area could beseen as an extension of Harris?s distributional hy-pothesis (Harris, 1954).
It is based on the sim-ple observation that a word and its translation arelikely to appear in similar contexts across lan-guages (Rapp, 1995).
Based on this assumption,the alignment method, known as the standard ap-proach builds and compares context vectors foreach word of the source and target languages.A particularity of this approach is that, to enablethe comparison of context vectors, it requires theexistence of a seed bilingual dictionary to translatesource context vectors.
The use of the bilingualdictionary is problematic when a word has sev-eral translations, whether they are synonymous orpolysemous.
For instance, the French word actioncan be translated into English as share, stock, law-suit or deed.
In such cases, it is difficult to iden-tify in flat resources like bilingual dictionaries,wherein entries are usually unweighted and un-ordered, which translations are most relevant.
Thestandard approach considers all available trans-lations and gives them the same importance inthe resulting translated context vectors indepen-dently of the domain of interest and word ambigu-ity.
Thus, in the financial domain, translating ac-tion into deed or lawsuit would probably introducenoise in context vectors.In this paper, we present a novel approachwhich addresses the word ambiguity problem ne-glected in the standard approach.
We introduce ause of a WordNet-based semantic similarity mea-sure permitting the disambiguation of translatedcontext vectors.
The basic intuition behind thismethod is that instead of taking all translationsof each seed word to translate a context vector,we only use the translations that are more likelyto give the best representation of the context vec-tor in the target language.
We test the method ontwo specialized French-English comparable cor-16pora (financial and medical) and report improvedresults, especially when many of the words in thecorpus are ambiguous.The remainder of the paper is organized as fol-lows: Section 2 presents the standard approachand recalls in some details previous work address-ing the task of bilingual lexicon extraction fromcomparable corpora.
In section 3 we present ourcontext disambiguation process.
Before conclud-ing and presenting directions for future work, wedescribe in section 4 the experimental protocol wefollowed and discuss the obtained results.2 Bilingual lexicon extraction2.1 Standard ApproachMost previous works addressing the task of bilin-gual lexicon extraction from comparable corporaare based on the standard approach (Fung, 1998;Chiao and Zweigenbaum, 2002; Laroche andLanglais, 2010).
Formally, this approach is com-posed of the following three steps:1.
Building context vectors: Vectors are firstextracted by identifying the words that appeararound the term to be translated S in a win-dow of N words.
Generally, an associationmeasure like the mutual information (Morinand Daille, 2006), the log-likelihood (Morinand Prochasson, 2011) or the DiscountedOdds-Ratio (Laroche and Langlais, 2010) areemployed to shape the context vectors.2.
Translation of context vectors: To enablethe comparison of source and target vectors,source terms vectors are translated in the tar-get language by using a seed bilingual dic-tionary.
Whenever it provides several trans-lations for an element, all proposed transla-tions are considered.
Words not included inthe bilingual dictionary are simply ignored.3.
Comparison of source and target vectors:Translated vectors are compared to targetones using a similarity measure.
The mostwidely used is the cosine similarity, butmany authors have studied alternative metricssuch as the Weighted Jaccard index (Prochas-son et al 2009) or the City-Block dis-tance (Rapp, 1999).
According to similarityvalues, a ranked list of translations for S isobtained.2.2 Related WorkRecent improvements of the standard approach arebased on the assumption that the more the con-text vectors are representative, the better the bilin-gual lexicon extraction is.
Prochasson et al(2009)used transliterated words and scientific compoundwords as ?anchor points?.
Giving these wordshigher priority when comparing target vectors im-proved bilingual lexicon extraction.
In addition totransliteration, Rubino and Linare`s (2011) com-bined the contextual representation within a the-matic one.
The basic intuition of their work is thata term and its translation share thematic similari-ties.
Hazem and Morin (2012) recently proposed amethod that filters the entries of the bilingual dic-tionary based upon POS-tagging and domain rel-evance criteria, but no improvements was demon-strated.Gaussier et al(2004) attempted to solve theproblem of different word ambiguities in thesource and target languages.
They investigated anumber of techniques including canonical corre-lation analysis and multilingual probabilistic la-tent semantic analysis.
The best results, with avery small improvement were reported for a mixedmethod.
One important difference with Gaussieret al(2004) is that they focus on words ambigu-ities on source and target languages, whereas weconsider that it is sufficient to disambiguate onlytranslated source context vectors.A large number of Word Sense Disambigua-tion WSD techniques were previously proposedin the literature.
The most popular ones are thosethat compute semantic similarity with the helpof existing thesauri such as WordNet (Fellbaum,1998).
This resource groups English words intosets of synonyms called synsets, provides short,general definitions and records various semanticrelations (hypernymy, meronymy, etc.)
betweenthese synonym sets.
This thesaurus has been ap-plied to many tasks relying on word-based sim-ilarity, including document (Hwang et al 2011)and image (Cho et al 2007; Choi et al 2012)retrieval systems.
In this work, we use this re-source to derive a semantic similarity between lex-ical units within the same context vector.
To thebest of our knowledge, this is the first applicationof WordNet to the task of bilingual lexicon extrac-tion from comparable corpora.17Word?to?be?translated?(source?language)Building?Context?VectorContext?vector?
Translated?Context?vectorBilingual?Dictionary WordNetDisambiguated??Context?vectorContext?Vectors?
(Target?language)Figure 1: Overall architecture of the lexical extraction approach3 Context Vector DisambiguationThe approach we propose includes the three stepsof the standard approach.
As it was mentioned insection 1, when lexical extraction applies to a spe-cific domain, not all translations in the bilingualdictionary are relevant for the target context vec-tor representation.
For this reason, we introducea WordNet-based WSD process that aims at im-proving the adequacy of context vectors and there-fore improve the results of the standard approach.Figure 1 shows the overall architecture of the lexi-cal extraction process.
Once translated into the tar-get language, the context vectors disambiguationprocess intervenes.
This process operates locallyon each context vector and aims at finding themost prominent translations of polysemous words.For this purpose, we use monosemic words as aseed set of disambiguated words to infer the pol-ysemous word?s translations senses.
We hypoth-esize that a word is monosemic if it is associatedto only one entry in the bilingual dictionary.
Wechecked this assumption by probing monosemicentries of the bilingual dictionary against WordNetand found that 95% of the entries are monosemicin both resources.Formally, we derive a semantic similarity valuebetween all the translations provided for each pol-ysemous word by the bilingual dictionary andall monosemic words appearing whithin the samecontext vector.
There is a relatively large numberof word-to-word similarity metrics that were pre-viously proposed in the literature, ranging frompath-length measures computed on semantic net-works, to metrics based on models of distribu-tional similarity learned from large text collec-tions.
For simplicity, we use in this work, the Wuand Palmer (1994) (WUP) path-length-based se-mantic similarity measure.
It was demonstrated by(Lin, 1998) that this metric achieves good perfor-mances among other measures.
WUP computes ascore (equation 1) denoting how similar two wordsenses are, based on the depth of the two synsets(s1 and s2) in the WordNet taxonomy and that oftheir Least Common Subsumer (LCS), i.e., themost specific word that they share as an ancestor.WupSim(s1, s2) =2?
depth(LCS)depth(s1) + depth(s2)(1)In practice, since a word can belong to morethan one synset in WordNet, we determine thesemantic similarity between two words w1 andw2 as the maximum WupSim between the synsetor the synsets that include the synsets(w1) andsynsets(w2) according to the following equation:SemSim(w1, w2) = max{WupSim(s1, s2);(s1, s2) ?
synsets(w1)?
synsets(w2)} (2)18Context Vector Translations Comparison Ave Simliquidite?
liquidity ?
?actionact SemSim(act,liquidity), SemSim(act,dividend) 0.2139action SemSim(action,liquidity), SemSim(action,dividend) 0.4256stock SemSim(stock,liquidity), SemSim(stock,dividend) 0.5236deed SemSim(deed,liquidity), SemSim(deed,dividend) 0.1594lawsuit SemSim(lawsuit,liquidity), SemSim(lawsuit,dividend) 0.1212fact SemSim(fact,liquidity), SemSim(fact,dividend) 0.1934operation SemSim(operation,liquidity), SemSim(operation,dividend) 0.2045share SemSim(share,liquidity), SemSim(share,dividend) 0.5236plot SemSim(plot,liquidity), SemSim(plot,dividend) 0.2011dividende dividend ?
?Table 1: Disambiguation of the context vector of the French term be?ne?fice [income] in the corporatefinance domain.
liquidite?
and dividende are monosemic and are used to infer the most similar translationsof the term action.Then, to identify the most prominent translationsof each polysemous unit wp, an average similarityis computed for each translation wjp of wp:Ave Sim(wjp) =?Ni=1 SemSim(wi, wjp)N(3)where N is the total number of monosemic wordsand SemSim is the similarity value of wjp and theith monosemic word.
Hence, according to averagerelatedness values Ave Sim(wjp), we obtain foreach polysemous word wp an ordered list of trans-lations w1p .
.
.
wnp .
This allows us to select trans-lations of words which are more salient than theothers to represent the word to be translated.In Table 1, we present the results of the dis-ambiguation process for the context vector of theFrench term be?ne?fice in the corporate finance cor-pus.
This vector contains the words action, div-idende, liquidite?
and others.
The bilingual dic-tionary provides the following translations {act,stock, action, deed, lawsuit, fact, operation, plot,share} for the French polysemous word action.We use the monosemic words dividende and liq-uidite?
to disambiguate the word action.
From ob-serving average similariy values (Ave Sim), wenotice that the words share and stock are on thetop of the list and therefore are most likely to rep-resent the source word action in this context.Corpus French EnglishCorporate finance 402, 486 756, 840Breast cancer 396, 524 524, 805Table 2: Comparable corpora sizes in term ofwords.4 Experiments and Results4.1 Resources4.1.1 Comparable corporaWe conducted our experiments on two French-English comparable corpora specialized onthe corporate finance and the breast cancerdomains.
Both corpora were extracted fromWikipedia1.
We consider the topic in the sourcelanguage (for instance finance des entreprises[corporate finance]) as a query to Wikipediaand extract all its sub-topics (i.e., sub-categoriesin Wikipedia) to construct a domain-specificcategory tree.
A sample of the corporate fi-nance sub-domain?s category tree is shown inFigure 2.
Then, based on the constructed tree,we collect all Wikipedia pages belonging to oneof these categories and use inter-language linksto build the comparable corpus.
Both corporawere normalized through the following linguisticpreprocessing steps: tokenisation, part-of-speechtagging, lemmatisation, and function word re-moval.
The resulting corpora2 sizes are given inTable 2.1http://dumps.wikimedia.org/2Comparable corpora will be shared publicly19Finance?des?entreprise?[Corporate?Finance]Analyse?Financi?re?
[Financial?Analysis] Comptabilit?
?g?n?rale[Financial?accountancy] Indicateur?Financier[Financial?ratios]Risque[Risk] Cr?dit[Credit]Actifs[Asset] Bilan[Balance?sheet] Salaire[Salary]Solde[Balance]B?n?fice[profit] Revenu[Income]...?...
?Figure 2: Wikipedia categories tree of the corporate finance sub-domain.4.1.2 Bilingual dictionaryThe bilingual dictionary used to translate contextvectors consists of an in-house manually revisedbilingual dictionary which contains about 120,000entries belonging to the general domain.
It is im-portant to note that words on both corpora has onaverage, 7 translations in the bilingual dictionary.4.1.3 Evaluation listIn bilingual terminology extraction from compa-rable corpora, a reference list is required to eval-uate the performance of the alignment.
Suchlists are usually composed of about 100 sin-gle terms (Hazem and Morin, 2012; Chiao andZweigenbaum, 2002).
Here, we created two refer-ence lists3 for the corporate finance and the breastcancer domains.
The first list is composed of 125single terms extracted from the glossary of bilin-gual micro-finance terms4.
The second list con-tains 96 terms extracted from the French-EnglishMESH and the UMLS thesauri5.
Note that refer-ence terms pairs appear at least five times in eachpart of both comparable corpora.4.2 Experimental setupThree other parameters need to be set up: (1) thewindow size, (2) the association measure and the(3) similarity measure.
To define context vectors,we use a seven-word window as it approximatessyntactic dependencies.
Concerning the rest of the3Reference lists will be shared publicly4http://www.microfinance.lu/en/5http://www.nlm.nih.gov/parameters, we followed Laroche and Langlais(2010) for their definition.
The authors carried outa complete study of the influence of these param-eters on the bilingual alignment and showed thatthe most effective configuration is to combine theDiscounted Log-Odds ratio (equation 4) with thecosine similarity.
The Discounted Log-Odds ratiois defined as follows:Odds-Ratiodisc = log(O11 + 12)(O22 +12)(O12 + 12)(O21 +12)(4)where Oij are the cells of the 2 ?
2 contingencymatrix of a token s co-occurring with the term Swithin a given window size.4.3 Results and discussionIt is difficult to compare results between differentstudies published on bilingual lexicon extractionfrom comparable corpora, because of differencebetween (1) used corpora (in particular their con-struction constraints and volume), (2) target do-mains, and also (3) the coverage and relevance oflinguistic resources used for translation.
To thebest of our knowledge, there is no common bench-mark that can serve as a reference.
For this reason,we use the results of the standard approach (SA)described in section 2.1 as a reference.
We evalu-ate the performance of both the SA and ours withrespect to TopN precision (PN ), recall (RN ) andMean Reciprocal Rank (MRR) (Voorhees, 1999).Precision is the total number of correct translationsdivided by the number of terms for which the sys-tem gave at least one answer.
Recall is equal to20a)CorporateFinanceMethod P1 P10 P20 R1 R10 R20 MRRStandard Approach (SA) 0.046 0.140 0.186 0.040 0.120 0.160 0.064WN-T1 0.065 0.196 0.261 0.056 0.168 0.224 0.089WN-T2 0.102 0.252 0.308 0.080 0.216 0.264 0.122WN-T3 0.102 0.242 0.327 0.088 0.208 0.280 0.122WN-T4 0.112 0.224 0.299 0.090 0.190 0.250 0.124WN-T5 0.093 0.205 0.280 0.080 0.176 0.240 0.110WN-T6 0.084 0.205 0.233 0.072 0.176 0.200 0.094WN-T7 0.074 0.177 0.242 0.064 0.152 0.208 0.090b)BreastCancerMethod P1 P10 P20 R1 R10 R20 MRRStandard Approach (SA) 0.342 0.542 0.585 0.250 0.395 0.427 0.314WN-T1 0.257 0.500 0.571 0.187 0.364 0.416 0.257WN-T2 0.314 0.614 0.671 0.229 0.447 0.489 0.313WN-T3 0.342 0.628 0.671 0.250 0.458 0.489 0.342WN-T4 0.342 0.571 0.642 0.250 0.416 0.468 0.332WN-T5 0.357 0.571 0.657 0.260 0.416 0.479 0.348WN-T6 0.357 0.571 0.652 0.260 0.416 0.468 0.347WN-T7 0.357 0.585 0.657 0.260 0.427 0.479 0.339Table 3: Precision, Recall at TopN (N=1,10,20) and MRR at Top20 for the two domains.
In each column,bold show best results.
Underline show best results overall.the ratio of correct translation to the total numberof terms.
The MRR takes into account the rankof the first good translation found for each entry.Formally, it is defined as:MRR =1Qi=1?|Q|1ranki(5)where Q is the total number of terms to be trans-lated and ranki is the position of the first correcttranslation in the translations candidates.Our method provides a ranked list of transla-tions for each polysemous word.
A question thatarises here is whether we should introduce onlythe best ranked translation in the context vectoror consider a larger number of words, especiallywhen a translations list contain synonyms (shareand stock in Table 1).
For this reason, we takeinto account in our experiments different numberof translations, noted WN-Ti, ranging from thepivot translation (i = 1) to the seventh word in thetranslations list.
This choice is motivated by thefact that words in both corpora have on average 7translations in the bilingual dictionary.
The base-line (SA) uses all translations associated to eachentry in the bilingual dictionary.
Table 3a displaysthe results obtained for the corporate finance cor-pus.
The first substantial observation is that ourmethod which consists in disambiguating polyse-mous words within context vectors consistentlyoutperforms the standard approach (SA) for allconfigurations.
The best MRR is reported whenfor each polysemous word, we keep the most simi-lar four translations (WN-T4) in the context vectorof the term to be translated.
However, the highestTop20 precision and recall are obtained by WN-T3.
Using the top three word translations in thevector boosts the Top20 precision from 0.186 to0.327 and the Top20 recall from 0.160 to 0.280.Concerning the Breast Cancer corpus, slightly dif-ferent results were obtained.
As Table 3b show,when the context vectors are totally disambiguated(i.e.
each source unit is translated by at most oneword in context vectors), all TopN precision, re-call and MRR decrease.
However, we report im-provements against the SA in most other cases.For WN-T5, we obtain the maximum MRR scorewith an improvement of +0.034 over the SA.
But,as for the corporate finance corpus, the best Top20precision and recall are reached by the WN-T3method, with a gain of +0.082 in both Top10 andTop20 precision and of about +0.06 in Top10 andTop20 recall.From observing result tables of both corporatefinance and breast cancer domains, we notice thatour approach performs better than the SA but withdifferent degrees.
The improvements achieved in21Corpus Corpus PR Vectors PRCorporate finance 41% 91, 6%Breast cancer 47% 85, 1%Table 4: Comparable corpora?s and context vec-tor?s Polysemy Rates PR.the corporate finance domain are higher than thosereported in the breast cancer domain.
The reasonbeing that the vocabulary used in the breast cancercorpus is more specific and therefore less ambigu-ous than that used in corporate finance texts.
Theresults given in table 4 validate this assumption.
Inthis table, we give the polysemy rates of the com-parable corpora (Corpus PR) and that of contextvectors (Vectors PR).
PR indicates the percent-age of words that are associated to more than onetranslation in the bilingual dictionary.
The resultsshow that breast cancer corpus is more polysemicthan that of the corporate finance.
Nevertheless,even if in both corpora, the candidates?
contextvectors are highly polysemous, breast cancer?scontext vectors are less polysemous than those ofthe corporate finance texts.
In this corpus, 91, 6%of the words used as entries to define context vec-tors are polysemous.
This shows that the ambi-guity present in specialized comparable corporahampers bilingual lexicon extraction, and that dis-ambiguation positively affects the overall results.Even though the two corpora are fairly different(subject and polysemy rate), the optimal Top20precision and recall results are obtained when con-sidering up to three most similar translations incontext vectors.
This behavior shows that the dis-ambiguation method is relatively robust to domainchange.
We notice also that the addition of supple-mentary translations, which are probably noisy inthe given domain, degrades the overall results butremains greater than the SA.5 ConclusionWe presented in this paper a novel method thatextends the standard approach used for bilin-gual lexicon extraction from comparable corpora.The proposed method disambiguates polysemouswords in context vectors and selects only the trans-lations that are most relevant to the general con-text of the corpus.
Conducted experiments on twohighly polysemous specialized comparable cor-pora show that integrating such process leads toa better performance than the standard approach.Although our initial experiments are positive, webelieve that they could be improved in a numberof ways.
In addition to the metric defined by (Wuand Palmer, 1994), we plan to apply other seman-tic similarity and relatedness measures and com-pare their performance.
It would also be interest-ing to mine much more larger comparable corporaand focus on their quality as presented in (Li andGaussier, 2010).
We want also to test our methodon bilingual lexicon extraction for a larger panel ofspecialized corpora, where disambiguation meth-ods are needed to prune translations that are irrel-evant to the domain.ReferencesYun-Chuang Chiao and Pierre Zweigenbaum.
2002.Looking for candidate translational equivalents inspecialized, comparable corpora.
In Proceedings ofthe 19th international conference on Computationallinguistics - Volume 2, COLING ?02, pages 1?5.
As-sociation for Computational Linguistics.Yun-Chuang Chiao and Pierre Zweigenbaum.
2003.The effect of a general lexicon in corpus-based iden-tification of french-english medical word transla-tions.
In Proceedings Medical Informatics Europe,volume 95 of Studies in Health Technology and In-formatics, pages 397?402, Amsterdam.Miyoung Cho, Chang Choi, Hanil Kim, Jungpil Shin,and PanKoo Kim.
2007.
Efficient image retrievalusing conceptualization of annotated images.
Lec-ture Notes in Computer Science, pages 426?433.Springer.Dongjin Choi, Jungin Kim, Hayoung Kim, Myungg-won Hwang, and Pankoo Kim.
2012.
A method forenhancing image retrieval based on annotation usingmodified wup similarity in wordnet.
In Proceed-ings of the 11th WSEAS international conferenceon Artificial Intelligence, Knowledge Engineeringand Data Bases, AIKED?12, pages 83?87, StevensPoint, Wisconsin, USA.
World Scientific and Engi-neering Academy and Society (WSEAS).Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.Pascale Fung.
1998.
A statistical view on bilinguallexicon extraction: From parallel corpora to non-parallel corpora.
In Parallel Text Processing, pages1?17.
Springer.E?ric Gaussier, Jean-Michel Renders, Irina Matveeva,Cyril Goutte, and Herve?
De?jean.
2004.
A geometricview on bilingual lexicon extraction from compara-ble corpora.
In ACL, pages 526?533.Z.S.
Harris.
1954.
Distributional structure.
Word.22Amir Hazem and Emmanuel Morin.
2012.
Adap-tive dictionary for bilingual lexicon extraction fromcomparable corpora.
In Proceedings, 8th interna-tional conference on Language Resources and Eval-uation (LREC), Istanbul, Turkey, May.Myunggwon Hwang, Chang Choi, and Pankoo Kim.2011.
Automatic enrichment of semantic relationnetwork and its application to word sense disam-biguation.
IEEE Transactions on Knowledge andData Engineering, 23:845?858.Audrey Laroche and Philippe Langlais.
2010.
Re-visiting context-based projection methods for term-translation spotting in comparable corpora.
In 23rdInternational Conference on Computational Lin-guistics (Coling 2010), pages 617?625, Beijing,China, Aug.Bo Li and E?ric Gaussier.
2010.
Improving corpuscomparability for bilingual lexicon extraction fromcomparable corpora.
In 23rd International Confer-ence on Computational Linguistics (Coling 2010),Beijing, China, Aug.Dekang Lin.
1998.
An information-theoretic def-inition of similarity.
In Proceedings of the Fif-teenth International Conference on Machine Learn-ing, ICML ?98, pages 296?304, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.Emmanuel Morin and Be?atrice Daille.
2006.
Com-parabilite?
de corpus et fouille terminologique mul-tilingue.
In Traitement Automatique des Langues(TAL).Emmanuel Morin and Emmanuel Prochasson.
2011.Bilingual lexicon extraction from comparable cor-pora enhanced with parallel corpora.
In Proceed-ings, 4th Workshop on Building and Using Compa-rable Corpora (BUCC), page 27?34, Portland, Ore-gon, USA.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Comput.
Linguist., 29(1):19?51, March.Emmanuel Prochasson, Emmanuel Morin, and KyoKageura.
2009.
Anchor points for bilingual lexi-con extraction from small comparable corpora.
InProceedings, 12th Conference on Machine Transla-tion Summit (MT Summit XII), page 284?291, Ot-tawa, Ontario, Canada.Reinhard Rapp.
1995.
Identifying word translations innon-parallel texts.
In Proceedings of the 33rd an-nual meeting on Association for Computational Lin-guistics, ACL ?95, pages 320?322.
Association forComputational Linguistics.Reinhard Rapp.
1999.
Automatic identification ofword translations from unrelated english and germancorpora.
In Proceedings of the 37th annual meet-ing of the Association for Computational Linguisticson Computational Linguistics, ACL ?99, pages 519?526.
Association for Computational Linguistics.Raphae?l Rubino and Georges Linare`s.
2011.
A multi-view approach for term translation spotting.
InComputational Linguistics and Intelligent Text Pro-cessing, Lecture Notes in Computer Science, pages29?40.Lei Shi.
2009.
Adaptive web mining of bilinguallexicons for cross language information retrieval.In Proceedings of the 18th ACM conference on In-formation and knowledge management, CIKM ?09,pages 1561?1564, New York, NY, USA.
ACM.Ellen M. Voorhees.
1999.
The trec-8 question an-swering track report.
In In Proceedings of TREC-8,pages 77?82.Zhibiao Wu and Martha Palmer.
1994.
Verbs seman-tics and lexical selection.
In Proceedings of the 32ndannual meeting on Association for ComputationalLinguistics, ACL ?94, pages 133?138.
Associationfor Computational Linguistics.23
