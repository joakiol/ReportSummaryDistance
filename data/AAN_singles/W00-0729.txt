In: Proceedings of CoNLL-2000 and LLL-2000, pages 139-141, Lisbon, Portugal, 2000.Chunking with Maximum Entropy ModelsRob Koe l ingSRI Cambridgekoel?ng0cam, srJ.. com1 In t roduct ionIn this paper I discuss a first attempt to create atext chunker using a Maximum Entropy model.The first experiments, implementing classifiersthat tag every word in a sentence with a phrase-tag using very local lexical information, part-of-speech tags and phrase tags of surroundingwords, give encouraging results.2 Max imum Ent ropy  mode lsMaximum Entropy (MaxEnt) models (Jaynes,1957) are exponential models that implementthe intuition that if there is no evidence tofavour one alternative solution above another,both alternatives should be equally likely.
Inorder to accomplish this, as much informationas possible about the process you want to modelmust be collected.
This information consistsof frequencies of events relevant o the process.The frequencies of relevant events are consid-ered to be properties of the process.
Whenbuilding a model we have to constrain our at-tention to models with these properties.
Inmost cases the process is only partially de-scribed.
The MaxEnt framework now demandsthat from all the models that satisfy these con-straints, we choose the model with the flattestprobability distribution.
This is the model withthe highest entropy (given the fact that the con-straints are met).
When we are looking for aconditional model P(w\]h), the MaxEnt solutionhas the form:1 .
e~ i Aifi(h,w) P(wlh) = Z(h)where fi(h,w) refers to a (binary valued) fea-ture function that describes a certain event; Aiis a parameter that indicates how important fea-ture fi is for the model and Z(h) is a normali-sation factor.In the last few years there has been an in-creasing interest in applying MaxEnt models forNLP applications (Ratnaparkhi, 1998; Berger etal., 1996; Rosenfeld, 1994; Ristad, 1998).
Theattraction of the framework lies in the ease withwhich different information sources used in themodelling process are combined and the goodresults that are reported with the use of thesemodels.
Another strong point of this frameworkis the fact that general software can easily beapplied to a wide range of problems.
For theseexperiments we have used off-the-shelf software(Maccent) (Dehaspe, 1997).3 An  MaxEnt  chunker3.1 At t r ibutes  usedFirst need to be decided which informationsources might help to predict he chunk tag.
Weneed to work with the information that is in-cluded in the WSJ corpus, so the choice is firstlimited to:?
Current word?
POS tag of current word?
Surrounding words?
POS tags of surrounding wordsAll these sources will be used, but in case of theinformation sources using surrounding words wewill have to decide how much context is takeninto account.
I did not perform exhaustive t stson finding the best configuration, but following(Tjong Kim Sang and Veenstra, 1999; Ratna-parkhi, 1997) I only used very local context.
Inthese experiments I used a left context of threewords and a right context of two words.
Exper-iments described in (Mufioz et al, 1999) suc-cessfully used larger contexts, but the few teststhat I performed to confirm this did not giveevidence that we could benefit significantly byextending the context.
Apart from information139given by the WSJ corpus, information generatedby the model itself will also be used:?
Chunk tags of previous wordsIt would of course sometimes be desirable to usethe chunk tags of the following words also, butthese are not instantly available and thereforewe will need a cascaded approach.
I have exper-imented with a cascaded chunker, but I did notimprove the results significantly.In order to use previously predicted chunktags, the evaluation part of the Maccent soft-ware had to be modified.
The evaluation pro-gram needs a previously created file with allthe attributes and the actual class, but thechunk tag of the previous two words cannotbe provided beforehand as they are producedin the process of evaluation.
A ca~scaded ap-proach where after the first run the predictedtags are added to the file with test data is alsonot completely satisfactory as the provided tagsare then predicted on basis of all the other at-tributes, but not the previous chunk tags.
Ide-ally the information about the tags of the pre-vious words would be added during evaluation.This required some modification of the evalua-tion script.3.2 ResultsThe experiments are evaluated using the follow-ing standard evaluation measures:?
Tagging accuracy =Number of correct ta~6ed wordsTotal number of words?
Recall =Number of correct proposed baseNWsNumber of correct baseNWs?
Precision =Number of correct proposed baseNWsNumber of proposed baseNWs?
F~-score (f12 + 1)'Recall'Precisi?n~ fl2.Recall+Precision(fl=l in all experiments.
)In all the experiments a left context of 3 wordsand a right context of 2 words was used.
Thepart of speech tags of the surrounding words andthe word itself were all used as atomic features.The lexical information used consisted of theprevious word, the current word and the nextword.
The word W-2 was omitted because itdid not seem to improve the model.
Using onlythese atomic features, the model scored an tag-ging accuracy of about 95.5% and a F-score ofabout 90.5 %.
Well below the reported results inthe literature.
Adding i~atures combining POStags improved the results significantly to justbelow state of the art scores.
Finally 2 complexfeatures involving NP chunk tags predicted forprevious words were added.
The most successfulset of features used in our experiments is givenin figure 1.
It is not claimed that this is the bestTemplateTAG_3POS-3POS-3/POSoPOS-3/POS_2TAG_3/TAG_2/TAG_i/POSoTAG-2POS-2W-1 Previous wordTAG_IPOS-1Wo Current wordPOSoW+iPOS+iPOS_2/POSoPOS-2/POS_ITAG_2/TAG_i/POSoPOS_~/TAG_~/POSoPOS_i/POSo/POS+iPOS-2/POS-1/POSo/POS+iPOSo/POS+iPOS+2POSo/POS+2POSo/POS+i/POS+2MeaningBase-NP tag of W-3Part of Speech tag of W-3Figure 1: Feature set-up for best scoring exper-imentset of features possible for this task.
Trying newfeature combinations, by adding them manuallyand testing the new configuration is a time con-suming and not very interesting activity.
Es-pecially when the scores are close to the bestpublished scores, adding new features have littleimpact on the behaviour of the model.
An al-gorithm that discovers the interaction betweenfeatures and suggests which features could becombined to improve the model would be veryhelpful here.
I did not include any complex fea-tures involving lexical information.
It might be140useful to include more features with lexical in-formation if more training data is available (forexample the full R&M data set consisting of sec-tion 2-21 of WSJ).For feature selection a simple count cut-offwas used.
I experimented with several combi-nations of thresholds and the number of itera-tions used to train the model.
When the thresh-old was set to 2, unique contexts (can be prob-lematic during training of the model; see (Rat-naparkhi, 1998)) did not occur very frequentlyanymore and an upper bound on the number ofiterations did not seem to be necessary.
It wasfound that (using a threshold of 2 for every sin-gle feature) after about 100 iterations the modeldid not improve very much anymore.
Using thefeature setup given in figure 1 a threshold of 2for all the features and allowing the model totrain over 100 iterations, the scores given in ta-ble 1 were obtained.4 Conc lud ing  remarksThe first observation that I would like to makehere, is the fact that it was relatively easy toget results that are comparable with previouslypublished results.
Even though some improve-ment is to be expected when more detailed fea-tures, more context and/or more training datais used, it seems to be necessary to incorporateother sources of information to improve signifi-cantly on these results.Further, it is not satisfactory to find out whatattribute combinations to use by trying newcombinations and testing them.
It might beworth to examine ways to automatically de-tect which feature combinations are promis-ing (Mikheev, forthcoming; Della Pietra et al,1997).Re ferencesAdam L. Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A maximum entropyapproach to natural anguage processing.
Com-putational Linguistics, 22(1).Luc Dehaspe.
1997.
Maximum entropy modelingwith clausal constraints.
In Proceedings of the 7thInternational Workshop on Inductive Logic Pro-gramming.S.
Della Pietra, V. Della Pietra, and J. Laf-ferty.
1997.
Inducing features from random fields.IEEE Transactions on Patterns Analysis and Ma-chine Intelligence, 19(4).test dataADJPADVPCONJPINTJLSTNPPPPRTSBARVPprecision65.53 %78.98 %55.56 %50.00 %0.00 %93.18 %97.05 %58.49 %63.36 %93.22 %all 92.08 % 91.86 %recall Ff~=l75.33 % 70.0978.08 % 78.5345.45 % 50.00100.00 % 66.670.00 % 0.0092.84 % 93.0193.60 % 95.3073.81% 65.2682.68 % 71.7592.54 % 92.88i 91.97Table 1: ResultsE.T.
Jaynes.
1957.
Information theory and statisti-cal mechanics.
Physical Review, 108:171-190.Andrei Mikheev.
forthcoming.
Feature lattices andmaximum entropy models.
Journal of MachineLearning.Marcia Mufioz, Vasin Punyakanok, Dan Roth, andDav Zimak.
1999.
A learning approach to shallowparsing.
In Proceedings of/EMNLP- WVLC'99.Adwait Ratnaparkhi.
1997.
A linear observed timestatistical parser based on maximum entropymodels.
In Proceedings of the Second Conferenceon Empirical Methods in Natural Language Pro-cessing, Brown University, Providence, Rhode Is-land.Adwait Ratnaparkhi.
1998.
Maximum EntropyModels for Natural Language Ambiguity Resolu-tion.
Ph.D. thesis, UPenn.Sven Eric Ristad.
1998.
Maximum entropy mod-elling toolkit.
Technical report.Ronald Rosenfeld.
1994.
Adaptive Statistical Lan-guage Modelling: A Maximum Entropy Approach.Ph.D.
thesis, Carnegy Mellon University.Erik F. Tjong Kim Sang and Jorn Veenstra.
1999.Strategy and tactics: a model for language pro-duction.
In Ninth Conference of the EuropeanChapter of the Association for ComputationalLinguistics, University of Bergen, Bergen, Nor-way.141
