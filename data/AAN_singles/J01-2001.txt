Unsupervised Learning of theMorphology of a Natural LanguageJohn Goldsmith*University of ChicagoThis study reports the results of using minimum description length (MDL) analysis to modelunsupervised learning of the morphological segmentation fEuropean languages, using corporaranging in size from 5,000 words to 500,000 words.
We develop aset of heuristics that rapidlydevelop aprobabilistic morphological grammar, and use MDL as our primary tool to determinewhether the modifications proposed by the heuristics will be adopted or not.
The resulting rammarmatches well the analysis that would be developed by a human morphologist.In the final section, we discuss the relationship of this style of MDL grammatical nalysis tothe notion of evaluation metric in early generative grammar.1.
IntroductionThis is a report on the present results of a study on unsupervised acquisitionof morphology.
1 The central task of morphological nalysis is the segmentation fwords into the components that form the word by the operation of concatenation.While that view is not free of controversy, it remains the traditional conception ofmorphology, and the one that we shall employ here.
2Issues of interface with phonol-ogy, traditionally known as morphophonology, and with syntax are not directlyaddressed.
3 While some of the discussion is relevant o the unrestricted set oflanguages, ome of the assumptions made in the implementation restrict he use-ful application of the algorithms to languages in which the average number of affixesper word is less than what is found in such languages as Finnish, Hungarian, andSwahili, and we restrict our testing in the present report to more widely studied Eu-ropean languages.
Our general goal, however, is the treatment of unrestricted naturallanguages.
* Department of Linguistics, University of Chicago, 1010 E. 59th Street, Chicago, IL 60637.
E-mail:ja-goldsmith@uchicago.edu.1 Some of the work reported here was done while I was a visitor at Microsoft Research in the winter of1998, and I am grateful for the support I received there.
A first version was written in September, 1998,and a much-revised version was completed in December, 1999.
This work was also supported in partby a grant from the Argonne National Laboratory-University of Chicago consortium, which I thank forits support.
I am also grateful for helpful discussion of this material with a number of people,including Carl de Marcken, Jason Eisner, Zhiyi Chi, Derrick Higgins, Jorma Rissanen, Janos Simon,Svetlana Soglasnova, Hisami Suzuki, and Jessie Pinkham.
As noted below, I owe a great deal to theremarkable work reported in de Marcken's dissertation, without which I would not have undertakenthe work described here.
I am grateful as well to several anonymous reviewers for their considerableimprovements to the content of this paper.2 Sylvain Neuvel has recently produced an interesting computational implementation f a theory ofmorphology that does not have a place for morphemes, asdescribed at http://www.neuvel.net.
It iswell established that nonconcatenative morphology is found in some scattered language families,notably Semitic and Penutian.
African tone languages require simultaneous morphological nalyses ofthe tonal and the segmental material.3 But see the following note.
@ 2001 Association for Computational LinguisticsComputational Linguistics Volume 27, Number 2The program in question takes a text file as its input (typically in the range of 5,000to 1,000,000 words) and produces a partial morphological nalysis of most of the wordsof the corpus; the goal is to produce an output hat matches as closely as possible theanalysis that would be given by a human morphologist.
It performs unsupervisedlearning in the sense that the program's ole input is the corpus; we provide theprogram with the tools to analyze, but no dictionary and no morphological rulesparticular to any specific language.
At present, the goal of the program is restricted toproviding the correct analysis of words into component pieces (morphemes), thoughwith only a rudimentary categorical labeling.The underlying model that is utilized invokes the principles of the minimumdescription length (MDL) framework (Rissanen 1989), which provides a helpful per-spective for understanding the goals of traditional linguistic analysis.
MDL focuseson the analysis of a corpus of data that is optimal by virtue of providing both themost compact representation f the data and the most compact means of extractingthat compression from the original data.
It thus requires both a quantitative accountwhose parameters match the original corpus reasonably well (in order to providethe basis for a satisfactory compression) and a spare, elegant account of the overallstructure.The novelty of the present account lies in the use of simple statements of mor-phological patterns (called signatures below), which aid both in quantifying the MDLaccount and in constructively building a satisfactory morphological grammar (for MDLoffers no guidance in the task of seeking the optimal analysis).
In addition, the systemwhose development is described here sets reasonably high goals: the reformulation ialgorithmic terms of the strategies of analysis used by traditional morphologists.Developing an unsupervised learner using raw text data as its sole input offersseveral attractive aspects, both theoretical and practical.
At its most theoretical, un-supervised learning constitutes a (partial) linguistic theory, producing a completelyexplicit relationship between data and analysis of that data.
A tradition of consider-able age in linguistic theory sees the ultimate justification of an analysis A of any singlelanguage L as residing in the possibility of demonstrating that analysis A derives froma particular linguistic theory LT, and that that LT works properly across a range oflanguages (not just for language L).
There can be no better way to make the case thata particular analysis derives from a particular theory than to automate that process,so that all the linguist has to do is to develop the theory-as-computer-algorithm; theapplication of the theory to a particular language is carried out with no surreptitioushelp.From a practical point of view, the development of a fully automated morphologygenerator would be of considerable interest, since we still need good morphologiesof many European languages and to produce a morphology of a given language "byhand" can take weeks or months.
With the advent of considerable historical text avail-able on-line (such as the ARTFL database of historical French), it is of great interestto develop morphologies of particular stages of a language, and the process of auto-matic morphology writing can simplify this stage--where there are no native speakersavailable---considerably.A third motivation for this project is that it can serve as an excellent preparatoryphase (in other words, a bootstrapping phase) for an unsupervised grammar acqui-sition system.
As we will see, a significant proportion of the words in a large corpuscan be assigned to categories, though the labels that are assigned by the morpholog-ical analysis are corpus internal; nonetheless, the assignment of words into distinctmorphologically motivated categories can be of great service to a syntax acquisitiondevice.154Goldsmith Unsupervised Learning of the Morphology of a Natural LanguageTable 1Some signatures from Tom Sawyer.Signature Example Stem Count (type) Token CountNULL.ed.ing betray betrayed betraying 69 864NULL.ed.ing.s remain remained remaining remains 14 516NULL.s.
cow cows 253 3,414e.ed.es.ing notice noticed notices noticing 4 62The problem, then, involves both the determination of the correct morphologicalsplit for individual words, and the establishment of accurate categories of stems basedon the range of suffixes that they accept:..Splitting words: We wish to accurately analyze any word into successivemorphemes in a fashion that corresponds to the traditional linguisticanalysis.
Minimally, we wish to identify the stem, as opposed to anyinflectional suffixes.
Ideally we would also like to identify all theinflectional suffixes on a word which contains a stem that is followed bytwo or more inflectional suffixes, and we would like to identifyderivational prefixes and suffixes.
We want to be told that in this corpus,the most important suffixes are -s, -ing, -ed, and so forth, while in thenext corpus, the most important suffixes are -e, -en, -heit, -ig, and so on.Of course, the program is not a language identification program, so itwill not name the first as "English" and the second as "German" (that isa far easier task), but it will perform the task of deciding for each wordwhat is stem and what is affix.Range of suffixes: The most salient characteristic of a stem in the languagesthat we will consider here is the range of suffixes with which it canappear.
Adjectives in English, for example, will appear with some subsetof the suffixes -er, -est, -ity, -hess, etc.
We would like to determineautomatically what the range of the most regular suffix groups is for thelanguage in question, and rank suffix groupings by order of frequency inthe corpus.
4To give a sense of the results of the program, consider one aspect of its analysisof the novel The Adventures of Tom Sawyer--and this result is consistent, by and large,regardless of the corpus one chooses.
Consider the top-ranked signatures, illustratedin Table 1: a signature is an alphabetized list of affixes that appear with a particularstem in a corpus.
(A larger list of these patterns of suffixation in English are given inTable 2, in Section 5.
)The present morphology learning algorithm is contained in a C++ program calledLinguistica that runs on a desktop PC and takes a text file as its input.
5 Analyzing a4 In addition, one would like a statement of general rules of allomorphy as well; for example, astatement that the stems hit and hitt (as in hits and hitting, respectively) are forms of the same linguisticstem.
In an earlier version of this paper, we discussed a practical method for achieving this.
The workis currently under considerable r vision, and we will leave the reporting on this aspect of the problemto a later paper; there is a very brief discussion below.5 The executable is available at http://humanities.uchicago.edu/faculty/goldsmith/Linguistica2000,along with instructions for use.
The functions described in this paper can be incrementally applied to acorpus by the user of Linguistica.155Computational Linguistics Volume 27, Number 2corpus of 500,000 words in English requires about five minutes on a Pentium II 333.Perfectly respectable results can be obtained from corpora as small as 5,000 words.The system has been tested on corpora in English, French, German, Spanish, Italian,Dutch, Latin, and Russian; some quantitative results are reported below.
The corporathat serve as its input are largely materials that have been obtained over the Internet,and I have endeavored to make no editorial changes to the files that are the input.In this paper, I will discuss prior work in this area (Section 2), the nature of theMDL model we propose (Section 3), heuristics for the task of the initial splitting ofwords into stem and affix (Section 4), the resulting signatures (Section 5), use of MDLto search the space of morphologies (Section 6), results (Section 7), the identificationof entirely spurious generalizations ( ection 8), the grouping of signatures into largerunits (Section 9), and directions for further improvements (Section 10).
Finally, I willoffer some speculative observations about the larger perspective that this work sug-gests and work in progress (Section 11).2.
Previous Research in this AreaThe task of automatic word analysis has intrigued workers in a range of disciplines,and the practical and theoretical goals that have driven them have varied consider-ably.
Some, like Zellig Harris (and the present writer), view the task as an essentialone in defining the nature of the linguistic analysis.
But workers in the area of datacompression, dictionary construction, and information retrieval have all contributedto the literature on automatic morphological analysis.
(As noted earlier, our primaryconcern here is with morphology and not with regular allomorphy or morphophonol-ogy, which is the study of the changes in the realization of a given morpheme thatare dependent on the grammatical context in which it appears, an area occasionallyconfused for morphology.
Several researchers have explored the morphophonologiesof natural anguage in the context of two-level systems in the style of the model de-veloped by Kimmo Koskenniemi \[1983\], Lauri Karttunen \[1993\], and others.)
The onlygeneral review of work in this area that I am aware of is found in Langer (1991), whichis ten years old and unpublished.Work in automatic morphological nalysis can be usefully divided into four majorapproaches.
The first approach proposes to identify morpheme boundar ies  first, andthus indirectly to identify morphemes, on the basis of the degree of predictability of then + 1st letter given the first n letters (or the mirror-image measure).
This was first pro-posed by Zellig Harris (1955, 1967), and further developed by others, notably by Haferand Weiss (1974).
The second approach seeks to identify bigrams (and trigrams) thathave a high likelihood of being morpheme internal, a view pursued in work discussedbelow by Klenk, Langer, and others.
The third approach focuses on the discovery ofpatterns (we might say, of rules) of phonological relationships between pairs of relatedwords.
The fourth approach, which includes that used in this paper, is top-down, andseeks an analysis that is globally most concise.
In this section, we shall review someof the work that has pursued these approaches--briefly, necessarily.
6 While not allof the approaches discussed here use no prior language-particular knowledge (whichis the goal of the present system), I exclude from discussions those systems that arebased essentially on a prior human-designed analysis of the grammatical morphemesof a language, aiming at identifying the stem(s) and the correct parsing; such is the6 Another effort is that attributed toAndreev (1965) and discussed in Altmann and Lehfeldt (1980),especially p.195 and following, though their description does not facilitate stablishing a comparisonwith the present approach.156Goldsmith Unsupervised Learning of the Morphology of a Natural Languagecase, for example, in Pacak and Pratt (1976), Koch, K~stner, and Riidiger (1989), andWothke and Schrnidt (1992).
With the exception of Harris's algorithm, the complex-ity of the algorithms is such as to make implementation for purposes of comparisonprohibitively time-consuming.At the heart of the first approach, due to Harris, is the desire to place boundariesbetween letters (respectively, phonemes) in a word based on conditional entropy, inthe following sense.
We construct a device that generates a finite list of words, ourcorpus, letter by letter and with uniform probability, in such a way that at any pointin its generation (having generated the first n letters 111213 ?
?
?
In )  we can inquire of itwhat the entropy is of the set consisting of the next letter of all the continuations itmight make.
(In current parlance, we would most naturally think of this as a pathfrom the root of a trie to one of its terminals, inquiring of each node its associatedone-letter entropy, based on the continuations from that node.)
Let us refer to this asthe prefix conditional entropy; clearly we may be equally interested in constructinga trie from the right edge of words, which then provides us with a suffix conditionalentropy, in mirror-image fashion.Harris himself employed no probabilistic notions, and the inclusion of entropyin the formulation had to await Hafer and Weiss (1974); but allowing ourselves theanachronism, we may say that Harris proposed that local peaks of prefix (and suffix)conditional entropy should identify morpheme breaks.
The method proposed in Harris(1955) appealed to what today we would call an oracle for information about the lan-guage under scrutiny, but in his 1967 article, Harris implemented a similar procedureon a computer and a fixed corpus, restricting his problem to that of finding morphemeboundaries within words.
Harris's method is quite good as a heuristic for finding agood set of candidate morphemes, comparable in quality to the mutual information-based heuristic that I have used, and which I describe below.
It has the same problemthat good heuristics frequently have: it has many inaccuracies, and it does not lenditself to a next step, a qualitatively more reliable approximation of the correct solution.
7Hafer and Weiss (1974) explore in detail various ways of clarifying and improvingon Harris's algorithm while remaining faithful to the original intent.
A brief summarydoes not do justice to their fascinating discussion, but for our purposes, their resultsconfirm the character of the Harrisian test as heuristic: with Harris's proposal, a quan-titative measure is proposed (and Hafer and Weiss develop a range of 15 differentmeasures, all of them rooted in Harris's proposal), and best results for morphologicalanalysis are obtained in some cases by seeking a local maximum of prefix conditionalentropy, in others by seeking a value above a threshold, and in yet others, good resultsare obtained only when this measure is paired with a similar measure constructed inmirror-image fashion from the end of the word- -and then some arbitrary thresholdsare selected which yield the best results.
While no single method emerges as the  best,one of the best yields precision of 0.91 and recall of 0.61 on a corpus of approximately6,200 word types.
(Precision here indicates proportion of predicted morpheme breaksthat are correct, and recall denotes the proportion of correct breaks that are predicted.
)The second approach that can be found in the literature is based on the hypothesisthat local information in the string of letters (respectively, phonemes) is sufficient oidentify morpheme boundaries.
This hypothesis would be clearly correct if all mor-pheme boundaries were between pairs of letters 11-12 that never occur in that sequence7 But Harris's method oes lend itself to a generalization to more difficult cases of morphologicalanalysis going beyond the scope of the present paper.
In work in progress, we have used minimizationof mutual information between successive candidate morphemes a part of a heuristic for preferring amorphological analysis in languages with a large number of suffixes per word.157Computational Linguistics Volume 27, Number 2morpheme internally, and the hypothesis would be invalidated if conditional proba-bilities of a letter given the previous letter were independent of the presence of anintervening boundary.
The question is where real languages distribute themselvesalong the continuum that stretches between these two extremes.A series of publications has explored this question, including Janssen (1992), Klenk(1992), and Flenner (1994, 1995).
Any brief description that overlooks the differencesamong these publications i  certain to do less than full justice to all of them.
Theprocedure described in Janssen (1992) and Flenner (1994, 1995) begins with a trainingcorpus with morpheme boundaries inserted by a human, and hence the algorithm isnot in the domain of unsupervised learning.
Each bigram (and the algorithm has beenextended in the natural way to treating trigrams as well) is associated with a triple(whose sum must be less than or equal to 1.0) indicating the frequency in the trainingcorpus of a morpheme boundary occurring to the left of, between, or to the rightof that bigram.
In a test word, each space between letters (respectively, phonemes)is assigned a score that is the sum of the relevant values derived from the trainingsession: in the word string, for example, the score for the potential cut between strand ing is the sum of three values: the probability of a morpheme boundary after tr(given tr), the probability of a morpheme boundary between r and i (given ri), andthe probability of a morpheme boundary before in (given in).That these numbers hould give some indication of the presence of a morphemeboundary is certain, for they are the sums of numbers that were explicitly assignedon the basis of overtly marked morpheme boundaries.
But it remains unclear howone should proceed further with the sum.
As Hafer and Weiss discover with Harris'smeasure, it is unclear whether local peaks of this measure should predict morphemeboundaries, or whether a threshold should be set, above which a morpheme boundaryis predicted.
Flenner (1995, 64-65) and proponents of this approach ave felt somefreedom on making this choice in an ad hoc fashion.
Janssen (1992, 81-82) observesthat the French word linguistique displays three peaks, predicting the analysis lin-guist-ique, employing a trigram model.
The reason for the strong, but spurious, peakafter lin is that lin occurs with high frequency word finally, just as gui appears withhigh frequency word initially.
One could respond to this observation i several ways:word-final frequency should not contribute to word-internal, morpheme-final status;or perhaps frequencies ofthis sort should not be added.
Indeed, it is not clear at all whythese numbers hould be added; they do not, for example, represent probabilities thatcan be added.
Janssen otes that the other two trigrams that enter into the picture (ingand ngu) had a zero frequency of morpheme break in the desired spot, and proposesthat the presence of any zeros in the sum forces the sum to be 0, raising again thequestion of what kind of quantity is being modeled; there is no scholarly traditionaccording to which the presence of zero in a sum should lead to a total of 0.I do not have room to discuss the range of greedy affix-parsing algorithms theseauthors explore, but that aspect of their work has less bearing on the comparison withthe present paper, whose focus is on data-driven learning.
The major question to carryaway from this approach is this: can the information that is expressed in the divisionof a set of words into morphemes be compressed into local information (bigrams,trigrams)?
The answer, I believe, is in general negative.
Morphology operates at ahigher level, so to speak, and has only weak statistical links to local sequencing ofphonemes or letters,8 On this score, language will surely vary to some degree.
English, for example, tends to employ rules ofmorphophonology to modify the surface form of morphologically complex words o as to better matchthe phonological pattern of unanalyzed words.
This is discussed atlength in Goldsmith (1990, Chap.
5).158Goldsmith Unsupervised Learning of the Morphology of a Natural LanguageThe third approach focuses on the discovery of patterns explicating the overtshapes of related forms in a paradigm.
Dzeroski and Erjavec (1997) report on workthat they have done on Slovene, a South Slavic language with a complex morphology,in the context of a similar project.
Their goal essentially was to see if an inductivelogic program could infer the principles of Slovene morphology to the point whereit could correctly predict the nominative singular form of a word if it were given anoblique (nonnominative) form.
Their project apparently shares with the present onethe requirement that the automatic learning algorithm be responsible for the decisionas to which letters constitute the stem and which are part of the suffix(es), though thedetails offered by Dzeroski and Erjavec are sketchy as to how this is accomplished.In any event, they present heir learning algorithm with a labeled pair of words- -abase form and an inflected form.
It is not clear from their description whether thebase form that they supply is a surface form from a particular point in the inflectionalparadigm (the nominative singular), or a more articulated underlying representationin a generative linguistic sense; the former appears to be their policy.Dzeroski and Erjavec's goal is the development of rules couched in traditionallinguistic terms; the categories of analysis are decided upon ahead of time by theprogrammer (or, more specifically, by the tagger of the corpus), and each individualword is identified with regard to what morphosyntactic features it bears.
The formbolecina is marked, for example, as a feminine noun singular genitive.
In sum, theirproject thus gives the system a good deal more information than the present projectdoes.
9Two recent papers, Jacquemin (1997) and Gaussier (1999), deserve considerationhere.
1?
Gaussier (1999) approaches a very similar task to that which we consider, andtakes some similar steps.
His goal is to acquire derivational rules from an inflectionallexicon, thus insuring that his algorithm has access to the lexical category of the wordsit deals with (unlike the present study, which is allowed no such access).
Using theterminology of the present paper, Gaussier considers candidate suffixes if they appearwith at least two stems of length 5.
His first task is (in our terms) to infer paradigmsfrom signatures (see Section 9), which is to say, to find appropriate clusters of signa-tures.
One example cited is depart, departure, departer.
He used a hierarchical agglomera-tive clustering method, which begins with all signatures forming distinct clusters, andsuccessively collapses the two most similar clusters, where similarity between stems isdefined as the number of suffixes that two stems share, and similarity between clustersis defined as the similarity between the two least similar stems in the respective clus-ters.
He reports a success rate of 77%, but it is not clear how to evaluate this figure.
11The task that Gaussier addresses i defined from the start to be that of derivationalmorphology, and because of that, his analysis does not need to address the problem ofinflectional morphology, but it is there (front and center, so to speak) that the difficultclustering problem arises, which is how to ensure that the signatures NULL.s.
's (fornouns in English) and the signature NULL.ed.s (or NULL.ed.ing.s) are not assigned tosingle clusters.
12 That is, in English both nouns and verbs freely occur with the suffixes9 Baroni (2000) reported success using an MDL-based model in the task of discovering English prefixes.
Ihave not had access to further details of the operation of the system.10 I am grateful to a referee for drawing my attention to these papers.11 The analysis of a word w in cluster C counts as a success if most of the words that in fact are related tow also appear in the cluster C, and if the cluster "comprised inmajority words of the derivationalfamily of w." I am not certain how to interpret this latter condition; it means perhaps that more thanhalf of the words in C contain suffixes hared by forms related to w.12 In traditional terms, inflectional morphology isresponsible for marking different forms of the samelexical item (lemma), while derivational morphology isresponsible for the changes in form betweendistinct but morphologically related lexical items (lemmas).159Computational Linguistics Volume 27, Number 2NULL and -s, and while -ed and -~s disambiguate he two cases, it is very difficult tofind a statistical and morphological basis for this knowledge, lBJacquemin (1997) explores an additional source of evidence regarding clustering ofhypothesized segmentation f words into stems and suffixes; he notes that the hypoth-esis that there is a common stem gen in gene and genetic, and a common stem expressin expression and expressed, is supported by the existence of small windows in corporacontaining the word pair genetic...expression a d the word pair gene.., expressed (asindicated, the words need not be adjacent in order to provide evidence for the rela-tionship).
As this example suggests, Jacquemin's work is situated within the contextof a desire for superior information retrieval.In terms of the present study, Jacquemin's algorithm consists of (1) finding sig-natures with the longest possible stems and (2) establishing pairs of stems that occurtogether in two or more windows of length 5 or less.
He tests his results on 100 ran-dom pairs discovered in this fashion, placing upper bounds on the length of the suffixpermitted between one and five letters, and independently varying the length of thewindow in question.
He does not vary the minimum size of the stem, a considerationthat turns out to be quite important in Germanic languages, though less so in Ro-mance languages.
He finds that precision varies from 97% when suffixes are limitedto a length of one letter, to 64% when suffixes may be five letters long, with bothfigures assuming an adjacency window of two words; precision falls to 15% when awindow of four words is permitted.Jacquemin also employs the term signature in a sense not entirely dissimilar tothat employed in the present paper, referring to the structured set of four suffixesthat appear in the two windows (in the case above, the suffixes are -ion, -ed; NULL,-tic).
He notes that incorrect signatures arise in a large number of cases (e.g., good:optical control ~ optimal control; adoptive transfer ~ adoptively tranfer, paralleled by bad:ear disease ~ early disease), and suggests a quality function along the following lines:Stems are linked in pairs (adopt-transfer, ear-disease); compute then the average lengthof the shorter stem in each pair (that is, create a set of the shorter member of eachpair, and find the average length of that set).
The quality function is defined as thataverage divided by the length of the largest suffix in the signature; reject any signatureclass for which that ratio is less than 1.0.
This formula, and the threshold, is purelyempirical, in the sense that there is no larger perspective that bears on determiningthe appropriateness of the formula, or the values of the parameters.The strength of this approach, clearly, is its use of information that co-occurrencein a small window provides regarding semantic relatedness.
This allows a more ag-gressive stance toward suffix identification (e.g., alpha interferon ~ alpha2 interferon).There can be little question that the type of corpus studied (a large technical medicalcorpus, and a list of terms--partially multiword terms) lends itself particularly to thisstyle of inference, and that similar patterns would be far rarer in unrestricted text suchas Tom Sawyer or the Brown corpus.
1413 Gaussier also offers a discussion of inference of regular morphophonemics, which we do not treat inthe present paper, and a discussion in a final section of additional analysis, though without est results.Gaussier aptly calls our attention to the relevance of minimum edit distance relating two potentialallomorphs, and he proposes a probabilistic model based on patterns established between allomorphs.In work not discussed in this paper, I have explored the integration of minimum edit distance to anMDL account of allomorphy as well, and will discuss this material in future work.14 In a final section, Jacquemin considers how his notion of signatures can be extended to identify sets ofrelated suffixes (e.g., onic/atic/ic--his example).
He uses a greedy clustering algorithm to successivelyadd nonclustered signatures to clusters, in a fashion similar to that of Gaussier (who Jacquemin thanksfor discussion, and of course Jacquemin's paper preceded Gaussier's paper by two years), using a160Goldsmith Unsupervised Learning of the Morphology of a Natural LanguageI laughed laughing laughswalked walking walksjumped jumping jumps J(a) Word list with no internal structureTotal etter count: 57 letterswalk ~ ijump j ~g(b) Word list with morphological structureTotal etter count: 19 lettersFigure 1Naive description length.The fourth approach to morphology analysis is top-down, and seeks a globallyoptimal analysis of the corpus.
This general approach is based on the insight thatthe number of letters in a list of words is greater than the number of letters in alist of the stems and affixes that are present in the original list.
This is illustrated inFigure 1.
This simple observation lends hope to the notion that we might be able tospecify a relatively simple figure of merit independently of how we attempt o findanalyses of particular data.
This view, appropriately elaborated, is part of the minimumdescription length approach that we will discuss in detail in this paper.Kazakov (1997) presents an analysis in this fourth approach, using a straightfor-ward measurement of the success of a morphological nalysis that we have mentioned,counting the number of letters in the inventory of stems and suffixes that have beenhypothesized; the improvement in this count over the number of letters in the origi-nal word list is a measure of the fitness of the analysis.
15 He used a list of 120 Frenchwords in one experiment, and 39 forms of the same verb in another experiment, andemployed what he terms a genetic algorithm to find the best cut in each word.
Heassociated each of the 120 words (respectively, 39) with an integer (between 1 andthe length of the word minus 1) indicating where the morphological split was to be,and measured the fitness of that grammar in terms of its decrease in number of totalletters.
He does not describe the fitness function used, but seems to suggest hat themetric more complex than the familiar minimum edit distance, but no results are offered in support ofthe choice of the additional complexity.15 I am grateful to Scott Meredith for drawing my attention to this paper.161Computational Linguistics Volume 27, Number 2single top-performing grammar of each generation is preserved, all others are elim-inated, and the top-performing grammar is then subjected to mutation.
That is, in acase-by-case fashion, the split between stems and suffixes is modified (in some casesby a shift of a single letter, in others by an unconstrained shift to another locationwithin the word) to form a new grammar.
In one experiment described by Kazakov,the population was set to 800, and 2,000 generations were modeled.
On a Pentium 90and a vocabulary of 120 items, the computation took over eight hours.Work by Michael Brent (1993) and Carl de Marcken (1995) has explored analyses ofthe fourth type as well.
Researchers have been aware of the utility of the information-theoretic notion of compression from the earliest days of information theory, and therehave been efforts to discover useful, frequent chunks of letters in text, such as Rad-hakrishnan (1978), but to my knowledge, Brent's and de Marcken's works were thefirst to explicitly propose the guiding of linguistic hypotheses by such notions.
Brent'swork addresses the question of determining the correct morphological analysis of acorpus of English words, given their syntactic ategory, utilizing the notion of minimalencoding, while de Marcken's addresses the problem of determining the "breaking" ofan unbroken stream of letters or phonemes into chunks that correspond as well as pos-sible to our conception of words, implementing a well-articulated algorithm couchedin a min imum description length framework, and exploring its effects on several largecorpora.Brent (1993) aims at finding the appropriate set of suffixes from a corpus, ratherthan the more comprehensive goal of finding the correct analysis for each word, bothstem and suffix, and I think it would not be unfair to describe it as a test-of-concepttrial on a corpus ranging in size from 500 to 8,000 words; while this is not a smallnumber of words, our studies below focus on corpora with on the order of 30,000distinct words.
Brent indicates that he places other limitations as well on the hypothesisspace, such as permitting no suffix which ends in a sequence that is also a suffix (i.e.,if s is a suffix, then less and ness are not suffixes, and if y is a suffix, ity is not).Brent's observation is very much in line with the spirit of the present analysis: "Theinput lexicons contained thousands of non-morphemic endings and mere dozens ofmorphemic suffixes, but the output contained primarily morphemic suffixes in all casesbut one.
Thus, the effects of non-morphemic regularities are minimal" (p. 35).
Brent'scorpora were quite different from those used in the experiments reported below; hiswere based on choosing the n most common words in a Wall Street Journal corpus,while the present study has used large and heterogeneous sources for corpora, whichmakes for a considerably more difficult task.
In addition, Brent scored his algorithmsolely on how well it succeeded in identifying suffixes (or combinations of suffixes),rather than on how well it simultaneously analysed stem and suffix for each word,the goal of the present study.
~6 Brent makes clear the relevance and importance ofinformation-theoretic notions, but does not provide a synthetic and overall measureof the length of the morphological grammar.16 Brent's description of his algorithm isnot detailed enough to satisfy the curiosity of someone like thepresent writer, who has encountered problems that Brent's approach would seem certain to encounterequally.
As we shall see below, the central practical problem to grapple with is the fact that whenconsidering suffixes (or candidate suffixes) consisting of only a single letter (let us say, s, for example),it is extremely difficult o get a good estimate of how many of the potential occurrences (of word-finals) are suffixal sand how many are not.
As we shall suggest towards the end of this paper, the onlyaccurate way to make an estimate ison the basis of a multinomial estimate once larger suffixsignatures have been established.
Without his, it is difficult not to overestimate the frequency ofsingle-letter suffixes, aresult hat may often, in my experience, deflect the learning algorithm fromdiscovering a correct two-letter suffix (e.g., the suffix -al in French).162Goldsmith Unsupervised Learning of the Morphology of a Natural LanguageDe Marcken (1995) addresses a similar but distinct task, that of determining thecorrect breaking of a continuous tream of segments into distinct words.
This prob-lem has been addressed in the context of Asian (Chinese-Japanese-Korean) l guages,where standard orthography does not include white space between words, and it hasbeen discussed in the context of language acquisition as well.
De Marcken describesan unsupervised learning algorithm for the development of a lexicon using a mini-mum description length framework.
He applies the algorithm to a written corpus ofChinese, as well as to written and spoken corpora of English (the English text has hadthe spaces between words removed), and his effort inspired the work reported here.De Marcken's algorithm begins by taking all individual characters to be the baselinelexicon, and it successively adds items to the lexicon if the items will be useful increating a better compression of the corpus in question, or rather, when the improve-ment in compression yielded by the addition of a new item to the codebook is greaterthan the length (or "cost") associated with the new item in the codebook.
In general, alexical item of frequency F can be associated with a compressed length of - log F, andde Marcken's algorithm computes the compressed length of the Viterbi-best parse ofthe corpus, where the compressed length of the whole is the sum of the compressedlengths of the individual words (or hypothesized chunks, we might say) plus that ofthe lexicon.
In general, the addition of chunks to the lexicon (beginning with suchhigh-frequency items as th) will improve the compression of the corpus as a whole,and de Marcken shows that successive iterations add successively arger pieces to thelexicon.
De Marcken's procedure builds in a bottom-up fashion, looking for largerand larger chunks that are worth (in an MDL sense) assigning the status of dictionaryentries.
Thus, if we look at unbroken orthographic texts in English, the two-letter com-bination th will become the first candidate chosen for lexical status; later, is will achievethat status too, and soon this will as well.
The entry this will not, in effect, point toits four letters directly, but will rather point to the chunks th and is, which still retaintheir status in the lexicon (for their robust integrity is supported by their appearancethroughout the lexicon).
The creation of larger constituents will occasionally lead tothe elimination of smaller chunks, but only when the smaller chunk appears almostalways in a single larger unit.An example of an analysis provided by de Marcken's algorithm is given in (1),taken from de Marcken (1995), in which I have indicated the smallest-level constituentby placing letters immediately next to one another, and then higher structure withvarious pair brackets (parentheses, etc.)
for orthographic convenience; there is no the-oretical significance to the difference between "( )" and "0", etc.
De Marcken's analysissucceeds quite well at identifying words, but does not make any significant effort atidentifying morphemes as such.
( \ [ the \ ]{ ( \ [un i t \ ]ed) ( \ [ s ta t \ ]es )}) (o f{ame( \ [ r i c \ ] )a})  (1)Applying de Marcken's algorithm to a "broken" corpus of a language in whichword boundaries are indicated (for example, English) provides interesting results, butnone that provide anything even approaching a linguistic analysis, such as identifica-tion of stems and affixes.
The broken character of the corpus serves essentially as anupper bound for the chunks that are postulated, while the letters represent the lowerbound.De Marcken's MDL-based figure of merit for the analysis of a substring of thecorpus is the sum of the inverse log frequencies of the components of the string inquestion; the best analysis is that which minimizes that number (which is, again, theoptimal compressed length of that substring), plus the compressed length of each163Computational Linguistics Volume 27, Number 2of the lexical items that have been hypothesized to form the lexicon of the corpus.It would certainly be natural to try using this figure of merit on words in English,along with the constraint that all words should be divided into exactly two pieces.Applied straightforwardly, however, this gives uninteresting results: words will alwaysbe divided into two pieces, where one of the pieces is the first or the last letter ofthe word, since individual etters are so much more common than morphemes.
17 (Iwill refer to this effect as peripheral cutting below.)
In addition--and this is lessobvious--the hierarchical character of de Marcken's model of chunking leaves noplace for a qualitative difference between high-frequency "chunks," on the one hand,and true morphemes, on the other: str is a high-frequency hunk in English (as schlis in German), but it is not at all a morpheme.
The possessive marker ~s, on the otherhand, is of relatively low frequency in English, but is clearly a morpheme.MDL is nonetheless the key to understanding this problem.
In the next section,I will present a brief description of the algorithm used to bootstrap the problem,one which avoids the trap mentioned briefly in note 21.
This provides us with aset of candidate splittings, and the notion of the signature of the stem becomes theworking tool for determining which of these splits is linguistically significant.
MDLis a framework for evaluating proposed analyses, but it does not provide a set ofheuristics that are nonetheless essential for obtaining candidate analyses, which willbe the subject of the next two sections.3.
Minimum Description Length ModelThe central idea of minimum description length analysis (Rissanen 1989) is composedof four parts: first, a model of a set of data assigns a probability distribution to thesample space from which the data is assumed to be drawn; second, the model can thenbe used to assign a compressed length to the data, using familiar information-theoreticnotions; third, the model can itself be assigned a length; and fourth, the optimal anal-ysis of the data is the one for which the sum of the length of the compressed dataand the length of the model is the smallest.
That is, we seek a minimally compactspecification of both the model and the data, simultaneously.
Accordingly, we use theconceptual vocabulary of information theory as it becomes relevant to computing thelength, in bits, of various aspects of the morphology and the data representation.3.1 A First ModelLet us suppose that we know (part of) the correct analysis of a set of words, and wewish to create a model using that knowledge.
In particular, we know which wordshave no morphological nalysis, and for all the words that do have a morphologicalanalysis, we know the final suffix of the word.
(We return in the next section to how wemight arrive at that knowledge.)
An MDL model can most easily be conceptualized ifwe encode all such knowledge by means of lists; see Figure 2.
In the present case, wehave three lists: a list of stems, of suffixes, and of signatures.
We construct a list of thestems of the corpus defined as the set of the unanalyzed words, plus the material thatprecedes the final suffix of each morphologically analyzed word.
We also constructa list of suffixes that occur with at least one stem.
Finally, each stem is empiricallyassociated with a set of suffixes (those with which it appears in the corpus); we callthis set the stem's ignature, and we construct a third list, consisting of the signaturesthat appear in this corpus.
This third list, however, contains no letters (as the other17 See note 21 below.164Goldsmith Unsupervised Learning of the Morphology of a Natural LanguageA.
Affixes: 61.
NULL2.
ed3.
ing4.
s5.
ei6.
esB.
Stems: 9!1.
cat2.
dog3.
hat4.
John5.
jump6.
laugh7.
sav8.
the9.
walkC.
Signatures: 4Signature 1:/ treatSimpleStem : ptr(dog)SimpleStem : ptr(hat) L ptr(s) JComplexStem : ptr(Sig2): ptr(sav) + ptr(ing)Signature 2:f ptr(e) ~{SimpleStem:ptr(sav)} ~ptr(es) ~~, ptr(ing) )Signature 3:ptr(NULL) "f SimpleStem:ptr(jump) ~ ~ ptr(ed)~ SimpleStem :ptr(laugh) ~ | ptr(ing)I, SimpleStem : ptr(walk) ) I, ptr(s)Signature 4:SimpleStem : ptr(John)SimpleStem : ptr(the) JFigure 2A sample morphology.
This morphology covers the words: cat, cats, dog, dogs, hat, hats, save,saves, saving, savings, jump, jumped, jumping, jumps, laugh, laughed, laughing, laughs, walk, walked,walking, walks, the, John.lists do), but rather pointers to stems and suffixes.
We do this, in one sense, becauseour goal is to construct he smallest morphology,  and in general a pointer requires lessinformation than an explicit set of letters.
But in a deeper sense, it is the signatureswhose compactness provides the explicit measurement  of the conciseness of the entireanalysis.
Note that by  construction, each stem is associated with exactly one signature.165Computational Linguistics Volume 27, Number 2Since stem, suffix, and signature all begin with s, we opt for using t to representa stem, f to represent a suffix, and cr to represent a signature, while the uppercaseT, F, E represent the sets of stems, suffixes, and signatures, respectively.
The numberof members of such a set will be represented (T) , (F/, etc., while the number ofoccurrences of a stem, suffix, etc., will be represented as \[t\], \[f\], etc.
The set of allwords in the corpus will be represented as W; hence the length of the corpus is \[W\],and the size of the vocabulary is (W).Note the structure of the signatures in Figure 2.
Logically a signature consistsof two lists of pointers, one a list of pointers to stems, the other a list of pointers tosuffixes.
To specify a list of length N, we must specify at the beginning of the signaturethat N items will follow, and this requires just slightly more than log 2 N bits to do (seeRissanen \[1989, 33-34\] for detailed discussion); I will use the notation A(N) to indicatethis function.A pointer to a stem t, in turn, is of length - log  prob (t), a basic principle ofinformation theory (Li and Vit8nyi 1997).
Hence the length of a signature is the sumof the (inverse) log probabilities of its stems, plus that of its suffixes, plus the numberof bits it takes to specify the number of its stems and suffixes, using the A function.We will return in a moment o how we determine the probabilities of the stems andsuffixes; looking ahead, it will be the empirical frequency.Let us consider the length of stem list T. As we have already observed, its lengthis ),((T))--this is the length of the information specifying how long the list is--plusthe length of each stem specification.
In most of our work, we make the assumptionthat the length of a stem is the number of letters in it, weighted by the factor log 26converting to binary bits, in a language with 26 lettersJ 8 The same reasoning holdsfor the suffix list F: its length is X((F)) plus the length of each suffix, which we maytake to be the total number of letters in the suffix times log 26.We return to the question of how long the pointer (found inside a signature) to astem or suffix is.
The probability of a stem is its (empirical) frequency, i.e., the totalnumber of words in the corpus corresponding to the words whose analysis includesthe stem in question; the probability of a suffix is defined in parallel fashion.
UsingW to indicate all the words of the corpus, we may say that the length of a pointer toa stem t is of lengtha pointer to suffix f is of lengthlog \[w\]\[t\] 'log \[% K'18 This is a reasonable, and convenient, assumption, but it may not be precise nough for all work.
Amore refined measure would take the length of a letter to be -1 times the binary log of its frequency.A still more refined measure would base the probability of a letter on bigram context; his matters forEnglish, where stem final t is very common.
In addition, there is information i the linear order inwhich the letters are stored, roughly equal ton~-~ log 2 kk=lfor a string of length n (compare the information that distinguishes the lexical representation fanagrams).
This is an additional consideration in an MDL analysis of morphology pressing in favor ofbreaking words into morphemes when possible.166Goldsmith Unsupervised Learning of the Morphology of a Natural Languageand a pointer to a signature cr is of length\[w\] log -\[cr\] "We have now settled the question of how to determine the length of our initialmodel; we next must determine the probability that the model assigns to each wordin the corpus, and armed with that knowledge, we will be able to compute the com-pressed length of the corpus.The morphology assigns a probability to each word w as the product of the prob-ability of w's signature times w's stem, given its signature, and w's suffix, given itssignature: prob (w = t +f )  = prob (c 0 prob (t I or) prob (f \] or), where cr is the signa-ture associated with t: cr = sig(t).
Thus while stems and suffixes, which are definedrelative to a particular morphological model, are assigned their empirical frequencyas their probability, words are assigned a probability based on the model, one whichwill always depart from the empirical frequency.
The compression to the corpus isthus worse than would be a compression based on word frequency alone, 19 or to putit another way, the morphological analysis in which all words are unanalyzed is theanalysis in which each word is trivially assigned its own empirical frequency (sincethe word equals the stem).
But this decrease in compression that comes with morpho-logical analysis is the price willingly paid for not having to enter every distinct wordin the stem list of the morphology.Summarizing, the compressed length of the corpus isZ \[w\](log prob(cr(w)) + log prob(t) + log prob(f \] or(w))),w~tq-fwhere we have summed over the words in the corpus, and or(w) is the signature towhich word w is assigned.
The compressed length of the model is the length of thestem list, the suffix list, and the signature list.
The length in bits of the stem list is&((T)) + ~ Ltypo(t)tCStemsand the length of the suffix list isA((r)) + L, po(f),f ff Suff ixeswhere LtvpoO is the measurement of the length of a string of letters in bits, which wetake to be log 2 26 times the number of letters (but recall note 18).
The length of thesignature list isA((~,)) + Z L(?
),c~ ff Sign atureswhere L(~) is the length of signature or.
If the set of stems linked to signature a isT(~r) and the set of suffixes linked to signature a is F(a), then+ + S-" log \[w\] + fcr(?
)Z log \[words(f) N words(cr)\]"19 Due to the fact that the cross-entropy is always greater than or equal to the entropy.167Computational Linguistics Volume 27, Number 2(The denominator in the last term consists of the token count of words in a particularsignature with the given suffix f ,  and we will refer to this below more simply asin cr\].
)It is no doubt easy to get lost in the formalism, so it may be helpful to point outwhat the contribution of the additional structure accomplishes.
We observed above thatthe MDL analysis is an elaboration of the insight that the best morphological nalysisof a corpus is obtained by counting the total number of letters in the list of stems andsuffixes according to various analyses, and choosing the analysis for which this sum isthe least (cf.
Figure 2).
This simple insight fails rapidly when we observe in a languagesuch as English that there are a large number of verb stems that end in t. Verbs appearwith a null suffix (that is, in bare stem form), with the suffixes -s, -ed, and -ing.
Butonce we have 11 stems ending in t, the naive letter-counting approach will judge it agood idea to create a new set of suffixes: -t, -ted, -ts, and -ting, because those 10 letterswill allow us to remove 11 or more letters from the list of stems.
It is the creation of thelists, notably the signature list, and an information cost which increases as probabilitydecreases, that overcomes that problem.
Creating a new signature may save someinformation associated with the stem list in the morphology, but since the length ofpointers to a signature cr is - log freq (0), the length of the pointers to the signaturesfor all of the words in the corpus associated with the old signature (-O, -ed, -s, -ing) orthe new signature (-ts, -ted, -ting, -ts) will be longer than the length of the pointers to asignature whose token count is the sum of the token count of the two combined, i.e.,x l?g (~-~)+y l?g  (~)~ (x+y) l ?g  (x -~y)  ?3.2 Recursive Morphological StructureThe model presented above is too simple in that it underestimates the gain achievedby morphological analysis in case the word that is analyzed is also a stem of a largerword.
For example, if a corpus contains the words work and working, then morpholog-ical analysis will allow us to dispense with the form working; it is modeled by the stemwork and the suffixes -O and -ing.
If the corpus also includes workings, the analysisworking-s additionally lowers the cost of the stem working.
Clearly we would like stemsto be in turn analyzable as stems + suffixes.
Implementing this suggestion involvesthe following modifications: (i) Each pointer to a stem (and these are found both in thecompressed representation f each individual word in the corpus, and inside the indi-vidual signatures of the morphological model) must contain a flag indicating whetherwhat follows is a pointer to a simple member of the stem list (as in the original model),or a triple pointer to a signature, stem, and suffix.
In the latter case, which would bethe case for the word \[work-ing\]-s, the pointer to the stem consists of a triple identicalto the signature for the word work-ing.
(ii) The number of words in the corpus hasnow changed, in that the word \[work-ing\]-s now contains two words, not one.
We willneed to distinguish between counts of a word w where w is a freestanding word, andcounts where it is part of a larger word; we shall refer to the latter class as secondarycounts.
In order to simplify computation and exposition, we have adopted the con-vention that the total number of words remains fixed, even when nested structure isposited by the morphology, thus forcing the convention that counts are distributed ina nonintegral fashion over the two or more nested word structures found in complexwords.
We consider the more complex case in the appendix.
2?20 In addition, the number of words in a corpus will change if the analysis determines that alloccurrences of (let us say) -ings are to be reanalyzed as complex words, and the stem in question168Goldsmith Unsupervised Learning of the Morphology of a Natural LanguageWe may distinguish between those words, like work or working, whose immediateanalysis involves a stem appearing in the stem list (we may call these WSIMPLE ) andthose whose analysis, like workings, involves recursive structure (we may call theseWCOMPLEX ).
AS we have noted, every stern entry in a signature begins with a flagindicating which kind of stem it is, and this flag will be of length\[wllog \[WsIMPLE \]for simple stems, and of length\[w\]log \[WcoMPrZX\]for complex stems.
We also keep track separately of the total number of words in thecorpus (token count) that are morphologically analyzed, and refer to this set as WA;this consists of all words except hose that are analyzed as having no suffix (see item(ii) in (2), below).
(2) Compressed length of morphology(i)(ii)(ii)(iii)(iv)hiT) + a(r) +Suffixlist: fc s~E (/~*lf\[ + lOg \[WA\]'~Suffixlist: E (l?g26*lengthOC)+l?g~ -~)f ff SuffizesStem list: t~cT (lOg26*length(t) + log (~)  )Signature componentStated once for the whole component:(a) Signature list: E log \[w\]For each signature:(b) Size of the count of the number of stems plus size of thecount of the number of suffixes:;~((stems(a))) + ~((suffixes(a)))(c) A pointer to each stem, consisting of a simple/complex flag,and a pointer to either a simple or complex stem:(i) Case of simple stem: flag of length\[w\]log \[WsIMPLE\](perhaps work-ing) did not appear independently as a freestanding word in the corpus; we will refer tothese inferred words as being "virtual" words with virtual counts.169Computational Linguistics Volume 27, Number 2plus a pointer to a stem of lengthlog \[w\].\[t\] '(ii)orCase of complex stem: flag of length\[w\]log \ [WcoMPLEX\]"followed by a sequence of two pointers of totallength\[w\] \[~\]log \[stem(t)~-~ + log \[suffix(t) in cr\]"(d) a pointer to each suffix, of total lengthv'z_.
log ~ in ~\]f c suyfixe~ ( ~ )(3) Compressed length of corpus\[w\] \[~(w)\] \[~(w)\] \] \[w\] log ~ + log + log\[stem(w)\] \[suffix(w)in a(w)\]\] wEWMDL thus provides a figure of merit that we wish to minimize, and we will seekheuristics that modify the morphological nalysis in such a fashion as to decrease thisfigure of merit in a large proportion of cases.
In any given case, we will accept amodification to our analysis just in case the description length decreases, and we willsuggest hat this strategy coincides with traditional linguistic judgment in all clearcases.4.
Heuristics for Word SegmentationThe MDL model designed in the preceding section will be of use only if we can providea practical means of creating one or more plausible morphologies for a given corpus.That is, we need bootstrapping heuristics that enable us to go from a corpus to sucha morphology.
As we shall see, it is not in fact difficult to come up with a plausibleinitial morphology, but I would like to consider first an approach which, though itmight seem like the most natural one to try, fails, and for an interesting reason.The problem we wish to solve can be thought of as one suited to an expectation-maximization (EM) approach (Dempster, Laird, and Rubin 1977).
Along such a line,each word w of length N would be initially conceived of as being analyzed in Ndifferent ways, cutting the word into stem + suffix after i letters, 1 K i < N, with eachof these N analyses being assigned probability mass of\[w\]N\[W\]"170Goldsmith Unsupervised Learning of the Morphology of a Natural LanguageThat probability mass is then summed over the resulting set of stems and suffixes,and on successive iterations, each of the N cuts into stem + suffix is weighted by itsprobability; that is, if the ith cut of word w, of length I, cuts it into a stem t of length iand suffix of length 1 - i, then the probability of that cut is defined aspv(stem t = wl,i)pr(suffix f = Wi+l,l)Npr(stem t = Wl,k)pf(suffl"x f = Wk+l,l)k=lwhere ZOj,k refers to the substring of w from the jth to the kth letter.
Probability massfor the stem and the suffix in each such cut is then augmented by an amount equalto the frequency of word w times the probability of the cut.
After several iterations(approximately four), estimated probabilities tabilize, and each word is analyzed onthe basis of the cut with the largest probability.This initially plausible approach fails because it always prefers an analysis in whicheither the stem or (more often) the suffix consists of a single letter.
More importantly,the probability that a sequence of one or more word-final etters is a suffix is verypoorly modeled by the sequence's frequency.
21To put the point another way, even theinitial heuristic analyzing one particular word must take into account all of the otheranalyses in a more articulated way than this particular approach does.I will turn now to two alternative heuristics that succeed in producing an initialmorphological nalysis (and refer to a third in a note).
It seems likely that one couldconstruct a number of additional heuristics of this sort.
The point to emphasize isthat the primary responsibility of the overall morphology is not that of the initialheuristic, but rather of the MDL model described in the previous ection.
The heuristicsdescribed in this section create an initial morphology that can serve as a starting pointin a search for the shortest overall description of the morphology.
We deal with thatprocess in Section 5.4.1 First HeuristicA heuristic that I will call the take-all-splits heuristic, and which considers all cuts of aword of length 1 into stem+suffix Wl,i -t- Wi+l,l, where 1 G i < 1, much like the EM ap-proach mentioned immediately above, works much more effectively if the probabilityis assigned on the basis of a Boltzmann distribution; see (4) below.
The function H(.
)in (4) assigns a value to a split of word w of length h w U + wi+l,l.
H does not assign aproper distribution; we use it to assign a probability to the cut of w into w~,i + wi+u asin (5).
Clearly the effect of this model is to encourage splits containing relatively longsuffixes and stems.H(Wl,i q- Wi+l,1) = - ( / log  freq (stem = Wl , i )  q -  ( l  - i)log freq (suffix = wi+u)) (4)prob (w = Wl, i q- Wi+l,l) = le-H(w"i+Wi+l,l) (5)z,21 It is instructive to think about why this should be so.
Consider a word such as diplomacy.
If we cut theword into the pieces diplomac + y, its probability is freq (diplomac)* freq (y), and constrast that valuewith the corresponding values of two other analyses: freq (diploma)* freq (cy), andfreq (diplom)* freq (acy).
Now, the ratio of the frequency of words that begin with diploma and thosethat begin with diplomac is less than 3, while the ratio of the frequency of words that end in y andthose that end in cy is much greater.
In graphical terms, we might note that tries (the data structure)based on forward spelling have by far the greatest branching structure arly in the word, while triesbased on backward spelling have the greatest branching structure close to the root node, which is tosay at the end of the word.171Computational Linguistics Volume 27, Number 2wheren--1Z = ~ H(Wl,i q- Wi+l,1)i=1For each word, we note what the best parse is, that is, which parse has the highestrating by virtue of the H-function.
We iterate until no word changes its optimal parse,which empirically is typically less than five iterations on the entire lexicon.
22 We nowhave an initial split of all words into stem plus suffix.
Even for words like this andstomach we have such an initial split.4.2 Second HeuristicThe second approach that we have employed provides a much more rapid conver-gence on the suffixes of a language.
Since our goal presently is to identify word-finalsuffixes, we assume by convention that all words end with an end-of-word symbol(traditionally "#') ,  and we then tally the counts of all n-grams of length between twoand six letters that appear word finally.
Thus, for example, the word elephant# containsone occurrence of the word-final bigram t#, one occurrence of the word-final trigramnt#, and so forth; we stop at 6-grams, on the grounds that no grammatical morphemesrequire more than five letters in the languages we are dealing with.
We also requirethat the n-gram in question be a proper substring of its word.We employ as a rough indicator of likelihood that such an n-gram nln2.., nk is agrammatical morpheme the measure:\[nln2...nk\] log \[nln2...nk\]Total count of k-grams \[n1-~2\] -(~-k\]'which we may refer to as the weighted mutual information.
We choose the top 100n-grams on the basis of this measure as our set of candidate suffixes.We should bear in mind that this ranking will be guaranteed to give incorrectresults as well as correct ones; for example, while ing is very highly ranked in anEnglish corpus, ting and ng will also be highly ranked, the former because so manystems end in t, the latter because all ings end in ng, but of the three, only ing is amorpheme in English.We then parse all words into stem plus suffix if such a parse is possible using asuffix from this candidate set.
A considerable number of words will have more thanone such parse under those conditions, and we utilize the figure of merit described inthe preceding section to choose among those potential parses.4.3 Evaluating the Results of Initial Word SplittingRegardless of which of the two approaches we have taken, our task now is to decidewhich splits are worth keeping, which ones need to be dropped, and which ones needto be modified.
23 In addition, if we follow the take-all-splits approach, we have many22 Experimenting with other functions uggests empirically that the details of our choices for a figure ofmerit, and the distribution reported in the text, are relatively unimportant.
As long as the measurementis capable of ensuring that the cuts are not strongly pushed towards the periphery, the results we getare robust.23 Various versions of Harris's method of morpheme identification can be used as well.
Harris's approachhas the interesting characteristic (unlike the heuristics discussed in the text) that it is possible to imposerestrictions that improve its precision while at the same time worsening its recall to unacceptably lowlevels.
In work in progress, we are exploring the consequences of using such an initial heuristic withsignificantly higher precision, while depending on MDL considerations to extend the recall of theentire morphology.172Goldsmith Unsupervised Learning of the Morphology of a Natural Languagesplits which (from our external vantage point) are splits between prefix and stem:words begim~ng with de (defense, demand, elete, etc.)
will at this point all be split afterthe initial de.
So there is work to be done, and for this we return to the central notionof the signature.5.
SignaturesEach word now has been assigned an optimal split into stem and suffix by the initialheuristic hosen, and we consider henceforth only the best parse for that word, and weretain only those stems and suffixes that were optimal for at least one word.
For eachstem, we make a list of those suffixes that appear with it, and we call an alphabetizedlist of such suffixes (separated by an arbitrary symbol, such as period) the stem'ssignature; we may think of it as a miniparadigm.
For example, in one English corpus,the stems despair, pity, appeal, and insult appear with the suffixes ing and ingly.
However,they also appear as freestanding words, and so we use the word NULL, to indicatea zero suffix.
Thus their signature is NULL.ing.ingly.
Similarly, the stems assist andignor are assigned the signature ance.ant.ed.ing  a certain corpus.
Because each stemis associated with exactly one signature, we will also use the term signature to refer tothe set of affixes along with the associated set of stems when no ambiguity arises.We establish a data structure of all signatures, keeping track for each signature ofwhich stems are associated with that signature.
As an initial heuristic, subject o cor-rection below, we discard all signatures that are associated with only one stem (theselatter form the overwhelming majority, well over 90%) and all signatures with onlyone suffix.
The remaining signatures we shall call regular signatures, and we will callall of the suffixes that we find in them the regular suffixes.
As we shall see, the regularsuffixes are not quite the suffixes we would like to establish for the language, but theyare a very good approximation, and constitute a good initial analysis.
The nonregu-lar signatures produced by the take-all-splits approach are typically of no interest, asexamples uch as ch.e.erial.erials.rimony.rons.uring and el.ezed.nce.reupon.ther illustrate.The reader may identify the single English pseudostem that occurs with each of thesesignatures.The regular signatures are thus those that specify exactly the entire set of suffixesused by at least two stems in the corpus.
The presence of a signature rests upon theexistence of a structure as in (6), where there are at least two members present in eachcolumn, and all combinations indicated in this structure are present in the corpus,and, in addition, each stem is found with no other suffix.
(This last condition doesnot hold for the suffixes; a suffix may well appear in other signatures, and this is thedifference between stems and af f ixes. )
24stem1} f suffi.Xl ~ stem2stem3 ~ suffix2 J(6)If we have a morphological pattern of five suffixes, let us say, and there is a largeset of stems that appear with all five suffixes, then that set will give rise to a reg-ular signature with five suffixal members.
This simple pattern would be perturbedby the (for our purpose) extraneous fact that a stem appearing with these suffixes24 Langer 1991 discusses some of the historical origins of this criterion, known in the literature as aGreenburg square (Greenberg 1957).
As Langer points out, important antecedents in the literatureinclude Bloomfield's brief discussion (1933, 161) as well as Nida (1948, 1949).173Computational Linguistics Volume 27, Number 2should also appear with some other suffix; and if all stems that associate with thesefive suffixes appear with idiosyncratic suffixes (i.e., each different from the others),then the signature of those five suffixes would never emerge.
In general, however, ina given corpus, a good proportion of stems appears with a complete set of what agrammarian would take to be the paradigmatic set of suffixes for its class: this willbe neither the stems with the highest nor the stems with the lowest frequency, butthose in between.
In addition, there will be a large range of words with no accept-able morphological nalysis, which is just as it should be: John, stomach, the, and soforth.To get a sense of what are identified as regular signatures in a language such asEnglish, let us look at the results of a preliminary analysis in Table 2 of the 86,976 wordsof The Adventures ofTom Sawyer, by Mark Twain.
The signatures in Table 2 are orderedby the breadth of a signature, defined as follows.
A signature ?r has both a stem count(the number of stems associated with it) and an affix count (the number of affixesit contains), and we use log (stem count) ~ log (affix count) as a rough guide to thecentrality of a signature in the corpus.
The suffixes identified are given in Table 3 forthe final analysis of this text.In this corpus of some 87,000 words, there are 202 regular signatures identifiedthrough the procedure we have outlined so far (that is, preceding the refining opera-tions described in the next section), and 803 signatures composed entirely of regularsuffixes (the 601 additional signatures either have only one suffix, or pertain to onlya single stem).The top five signatures are: NULL.ed.ing, e.ed.ing, NULL.s, NULL.ed.s, andNULL.ed.ing.s; the third is primarily composed of noun stems (though it includesa few words from other categories--hundred, bleed, new), while the others are verbstems.
Number 7, NULL.ly, identifies 105 words, of which all are adjectives (appre-hensive, sumptuous, gay .
.
.
.  )
except for Sal, name, love, shape, and perhaps earth.
Theresults in English are typical of the results in the other European languages that Ihave studied.These results, then, are derived by the application of the heuristics described above.The overall sketch of the morphology of the language is quite reasonable already inits outlines.
Nevertheless, the results, when studied up close, show that there remaina good number of errors that must be uncovered using additional heuristics andevaluated using the MDL measure.
These errors may be organized in the followingways:.2..The collapsing of two suffixes into one: for example, we find the suffixings here; in most corpora, the equally spurious uffix ments is found.The systematic nclusion of stem-final material into a set of (spurious)suffixes.
In English, for example, the high frequency of stem-final ts canlead the system to analyze a set of suffixes as in the spurious ignatureted.ting.ts, or ted.tion.The inclusion of spurious ignatures, largely derived from short stemsand short suffixes, and the related question of the extent of the inclusionof signatures based on real suffixes but overapplied.
For example, s is areal suffix of English, but not every word ending in s should be analyzedas containing that suffix.
On the other hand, every word ending in nessshould be analyzed as containing that suffix (in this corpus, this revealsthe stems: selfish, uneasi, wretched, loveli, unkind, cheeri, wakeful, drowsi,cleanli, outrageous, and loneli).
In the initial analysis of Tom Sawyer, thestem ca is posited with the signature n.n't.p.red.st.t.174Goldsmith Unsupervised Learning of the Morphology of a Natural LanguageTab le  2Top 81 signatures from Tom Sawyer.Number NumberRank Signature Stems Rank Signature Stems1 NULL.ed.ing 69 42 's.NULL,lys 32 e.ed.ing 35 43 NULL.ed.s.y 33 NULL.s 253 44 t.tion 84 NULL.ed.s 30 45 NULL.less 85 NULL.ed.ing.s 14 46 e.er 86 's.NULL.s 23 47 NULL.ment 87 NULL.ly 105 48 le.ly 88 NULL.ing.s 18 49 NULL.ted 79 NULL.ed 89 50 NULL.tion 710 NULL.ing 77 51 1.t 711 ed.ing 74 52 ence.ent 612 's.NULL 65 53 NULL.ity 613 e.ed 44 54 NULL.est.ly 314 e.es 42 55 ed.er.ing 315 NULL.er.est.ly 5 56 NULL.ed.ive 316 e.es.ing 7 57 NULL.led.s 317 NULL.ly.ness 7 58 NULL.er.ly 318 NULL.ness 20 59 NULL.ily.y 319 e.ing 18 60 NULL.n.s 320 NULL.ly.s 6 61 NULL.ed.ings 321 NULL.y 17 62 NULL.ed.es 322 NULL.er 16 63 e.en.ing 323 e.ed.es.ing 4 64 NULL.ly.st 324 NULL.ed.er.ing 4 65 NULL.s.ter 325 NULL.es 16 66 NULL.ed.ing.ings.s 226 NULL.ful 13 67 NULL.i.ii.v.x 227 NULL.e 13 68 NULL.ed.ful.ing.s 228 ed.s 13 69 ious.y 529 e.ed.es 5 70 NULL.en 530 ed.es.ing 5 71 ation.ed 531 NULL.ed.ly 5 72 NULL.able 532 NULL.n't 10 73 ed.er 533 NULL.t 10 74 nce.nt 534 'll.
's.NULL 4 75 NULL.an 435 ed.ing.ings 4 76 NUL.ed.ing.y 236 NULL.s.y 4 77 NULL.en.ing.s 237 NULL.ed.er 4 78 NULL.ed.ful.ing 238 NULL.ed.ment 4 79 NULL.st 439 NULL.ful.s 4 80 e.ion 440 NULL.ed.ing.ings 3 81 NULL.al.ed.s 241 ted.tion 9..The failure to break all words actually containing the same stem in aconsistent fashion: for example, the stem abbreviate with the signatureNULL.d.s is not related to abbreviat with the signature ing.Stems may be related in a language without being identical.
The stemwin may be identified as appearing with the signature NULL.s and thestem winn may be identified with the signature r.ing, but these stemsshould be related in the morphology.In the next section, we discuss some of the approaches we have taken to resolvingthese problems.Computational Linguistics Volume 27, Number 2Table 3Suffixes from Tom Sawyer.Suffix Remarks Suffix Remarkss ted chat-ted, fit-ted, submit-ted, etc.ed esting ityer ouse ard drunk-ardly able's iousd lessy mentn id id.or for stems horr-, splend-, liqu-on Spurious (bent-on, rivers-on): uretriage issuees ivet tyst Signature NULL.ly.st, for stems encesuch as safe-en behold, dea l  weak, sunk, etc.
ilyle Error: analyzed le.ly for e.y (stems wardsuch as feeb-, audib-, simp-).al ationn't lednce Signature nce.nt, for stems fragr-, 'ddista-, indiffere-ent Spurious: triage problem (pot-ent) ryrious tionr rster triage problem nedk triage problem ningful ageion h'11 tean triage problem antness r 'snt see above ancenovel, uncertain, six, propertriage problemerror: stems such as glo- with sig-nature rious.ryerror: stems such as glo- with sig-nature rious.ryerror: r should be in stemawake-ned, white-ned, thin-nedbegin-ning, run-ningtriage problemshould be -ate (e.g., punctua-te)triumph-ant, expect-anterror6.
Optimizing Description Length Using Heuristics and MDLWe can use the descr ip t ion  length  of the grammar  fo rmulated  in (2) and  (3) to eva luateany proposed  revis ion,  as we have  a l ready  observed:  note the descr ip t ion  length of thegrammar  and  the compressed  corpus,  per fo rm a modi f i cat ion  of the grammar ,  recom-pute  the two lengths,  and  see if the modi f i cat ion  improved the resu l t ing descr ip t ionlength.
2525 This computation is rather lengthy, and in actual practice it may be preferable to replace it with farfaster approaches totesting a change.
One way to speed up the task is to compute the differential ofthe MDL function, so that we can directly compute the change in description length given some priorchanges in the variables that define the morphology that are modified in the hypothetical change beingevaluated (see the Appendix).
The second way to speed up the task is, again, to use heuristics toidentify clear cases for which full description length computation is not necessary, and to identify asmaller number of cases where fine description length is appropriate.
For example, in the case176Goldsmith Unsupervised Learning of the Morphology of a Natural LanguageFollowing the morphological nalysis of words described in the previous ection,suffixes are checked to determine if they are spurious amalgams of independently mo-tivated suffixes: ments is typically, but wrongly, analyzed as a suffix.
Upon identifica-tion of such suffixes as spurious, the vocabulary containing these words is reanalyzed.For example, in Tom Sawyer, the suffix ings is split into ing and s, and thus the wordbeings is split into being plus s; the word being is, of course, already in the lexicon.The word breathings i  similarly reanalyzed as breathing plus s, but the word breathingis not found in the lexicon; it is entered, with the morphological nalysis breath+ing.Words that already existed include chafing, dripping, evening, feeling, and flogging, whilenew "virtual" words include belonging, bustling, chafing, and fastening.
The only newword that arises that is worthy of notice is jing, derived from the word jings foundin Twain's expression by jings!
In a larger corpus of 500,000 words, 64 suffixes aretested for splitting, and 31 are split, including tions, ists, ians, ened, lines, ents, and ively.Note that what it means to say that "suffixes are checked to see if they are spuriousamalgams" is that each suffix is checked to see if it is the concatenation f two inde-pendently existing suffixes, and then if that is the case, the entire description lengthof the corpus is recomputed under the alternative analysis; the reanalysis i adoptedif and only if the description length decreases.
The same holds for the other heuristicsdiscussed immediately below.
26Following this stage, the signatures are studied to determine if there is a consistentpattern in which all suffixes from the signature begin with the same letter or sequenceof letters, as in te.ting.ts.
27 Such signatures are evaluated to determine if the descriptionlength improves when such a signature ismodified to become .ing.s, etc.
It is necessaryto precede this analysis by one in which all signatures are removed which consist of asingle suffix composed of a single letter.
This set of signatures includes, for example,the singleton signature , which is a perfectly valid suffix in English; however, if wepermit all words ending in e, but having no other related forms, to be analyzed ascontaining the suffix e, then the e will be inappropriately highly valued in the analysis.
(We return to this question in Section 11, where we address the question of how manyoccurrences of a stem with a single suffix we would expect o find in a corpus.
)In the next stage of analysis, triage, signatures containing a small number of stemsor a single suffix are explored in greater detail.
The challenge of triage is to determinewhen the data is rich and strong enough to support he existence of a linguisticallyreal signature.
A special case of this is the question of how many stems must ex-ist to motivate the existence of a signature (and hence, a morphological nalysis forthe words in question) when the stems only appear with a single suffix.
For exam-ple, if a set of words appear in English ending with hood, should the morphologicalanalysis split the words in that fashion, even if the stems thereby created appearwith no other suffixes?
And, at the other extreme, what about a corpus which con-tains the words look, book, loot, and boot?
Does that data motivate the signature l.k,for the stems boo and loo?
The matter is rendered more complex by a number of fac-tors.
The length of the stems and suffixes in question clearly plays a role: suffixesof one letter are, all other things being equal, suspicious; the pair of stems Ioo andboo, appearing with the signature k.t, does not provide an example of a convincingmentioned in the text, that of determining whether a suffix such as ments should always be split intotwo independently motivated suffixes ment and s, we can compute the fraction of words ending inments that correspond to freestanding words ending in ment.
Empirical observation suggests that ratiosover 0.5 should always be split into two suffixes, ratios under 0.3 should not be split, and those inbetween must be studied with more care.26 This is accomplished by the command am4 in Linguistica.27 This is accomplished by the command am5 in Linguistica.177Computational Linguistics Volume 27, Number 2linguistic pattern.
On the other hand, if the suffix is long enough, even one stemmay be enough to motivate a signature, especially if the suffix in question is oth-erwise quite frequent in the language.
A single stem occurring with a single pairof suffixes may be a very convincing signature for other reasons as well.
In Ital-ian, for example, even in a relatively small corpus we are likely to find a signa-ture such as a.ando.ano.are.ata.ate.ati.ato.azione.~ with several stems in it; once we aresure that the 10-suffix signature is correct, then the discovery of a subsignature alongwith a stem is perfectly natural, and we would not expect o find multiple stemsassociated with each of the occurring combinations.
(A similar example in Englishfrom Tom Sawyer is NULL.ed.ful.ing.ive.less for the single stem rest.)
And a signaturemay be "contaminated," so to speak, by a spurious intruder.
A corpus containingrag, rage, raged, raging, and rags gave rise to a signature: NULL.e.ed.ing.s for the stemrag.
It seems clear that we need to use information that we have obtained regard-ing the larger, robust patterns of suffix combinations in the language to influenceour decisions regarding smaller combinations.
We return to the matter of triage be-low.We are currently experimenting with methods to improve the identification of re-lated stems.
Current efforts yield interesting but inconclusive results.
We compare allpairs of stems to determine whether they can be related by a simple substitution pro-cess (one letter for none, one letter for one letter, one letter for two letters), ignoringthose pairs that are related by virtue of one being the stem of the other already withinthe analysis.
We collect all such rules, and compare by frequency.
In a 500,000-wordEnglish corpus, the top two such pairs of 1:1 relationships are (1) 46 stems related bya final d/s alternation, including intrud/intrus, apprendend/apprenhens, provid/provis, us-pend/suspens, and elud/elus, and (2) 43 stems related by a final i/y alternation, includ-ing reli/rely, ordinari/ordinary, decri/decry, suppli/supply, and accompani/accompany.
Thisapproach can quickly locate patterns of allomorphy that are well known in the Eu-ropean languages (e.g., alternation between a and/~ in German, between o and ue inSpanish, between c and q in French).
However, we do not currently have a satisfactorymeans of segregating meaningful cases, such as these, from the (typically less frequentand) spurious cases of stems whose forms are parallel but ultimately not related.7.
ResultsOn the whole, the inclusion of the strategies described in the preceding sections leadsto very good, but by no means perfect, results.
In this section we shall review someof these results qualitatively, some quantitatively, and discuss briefly the origin of theincorrect parses.We obtain the most striking result by looking at the top list of signatures in alanguage, if we have some familiarity with the language: it is almost as if the textbookpatterns have been ripped out and placed in a chart.
As these examples uggest,the large morphological patterns identified tend to be quite accurately depicted.
Toillustrate the results on European languages, we include signatures found from a500,000-word corpus of English (Table 4), a 350,000-word corpus of French (Table 5),Don Quijote, which contains 124,716 words of Spanish (Table 6), a 125,000-word corpusof Latin (Table 7), and 100,000 words and 1,000,000 words of Italian (Tables 8 and 9).The 500,000-word (token-count) corpus of English (the first part of the Brown Corpus)contains lightly more than 30,000 distinct words.To illustrate the difference of scale that is observed epending on the size ofthe corpus, compare the signatures obtained in Italian on a corpus of 100,000 words(Table 8) and a corpus of 1,000,000 words (Table 9).
When one sees the rich inflectional178Goldsmith Unsupervised Learning of the Morphology of a Natural LanguageTable 4Top 10 signatures, 500,000-word English corpus.1.
NULL.ed.ing.s 4.
NULL.saccentaddadministeraffordalertamountappealassaultattempt2.
's.NULL.s7.
NULL.ed.ingabberation applaudabolitionist arrestabortion astoundabsence blastabstractionist blessabutment bloomaccolade boastaccommodation bolsteraccomodation broadencater5.
e.ed.es.ingadolescent achiev 8.
NULL.er.ing.safternoon assum blowairline brac bombambassador chang broadcastamendment charg dealannouncer compris drawarchitect conced drinkassessor conclud dwellassociation decid farmdescrib feed3.
NULL.ed.er.ing.s feelattack 6. e.ed.er.es.ingback advertis 9.
NULL.d.sbath announc abbreviateboil bak accommodateborrow challeng aggravatecharm consum apprenticecondition enforc arcadedemand gaz balancedown glaz barbecueflow invad bruiseliv cataloguepac costume10.
NULL.ed.sacclaimbeckonbenefitblendblisterbogeybotherbreakfastbuffetburden179Computational Linguistics Volume 27, Number 2Table 5Top 10 signatures, 350,000-word French corpus.1.
NULL.e.es.s 4.
NULL.e.es 7.
NULL.eabondant acquis accueillantabstrait a6ropostal acharn6adjacent afghan admisappropri6 albanais adsorbantatteint allong6 albigeoisbantou anglais alicantbleu appel6 ali6nantbrillant arrondi all6chantbyzantin bavarois amarantcarthaginois ambiant2.
NULL.sabandonn6e 5.
NULL.e.s 8.
NULL.es.sabbaye adh6rent antioxydantabdication adolescent bassinabdominale affili6 civilab61ienne aLn6 craintaberration assign6 cristallinabolitionniste assistant cutan6abord6e bovin descendantabrasif cinglant dot6abr6viation colorant emulsifiantennemi3.
NULL.ment.s 6.
NULL.ne.sadministrative ab61ienagressive acheul6enanatomique alsacienancienne am6rindienannuelle ancienautomatique anglo-saxonbiologique aram6enchimique aristot61icienclassique ath6nien9.
a.aient.ait.ant.e.ent.er.es.6rent.4.4e.6scontr61joulaissrest10.
NULL.esadopt6ag6alli6annul6apparent6apprdci6arm6assi6g6associ6attach6pat tern  emerg ing,  as w i th  the example  of the 10 suffixes on f i rst -conjugat ion stems(a.ando.ano.are.ata.ate.ati.ato.azione.~), one cannot  but  be st ruck by  the grammat ica l  deta i lthat  is emerg ing  f rom the s tudy  of a larger corpus,  as28 Signature 1 is formed from adjectival stems in the fem.sg., fem.pl., masc.pl, and masc.sg, forms;Signature 2 is entirely parallel, based on stems ending with the morpheme -ic/-ich, where ich is usedbefore i and e. Signature 4 is an extension of Signature 2, including nominalized (sg.
and pl.)
forms.Signature 5 is the large regular verb inflection pattern (seven such verb stems are identified).
Signature3 is a subset of Signature 1, composed of stems accidentally not found in the feminine plural form.Signatures 6 and 8 are primarily masculine nouns, sg., and pl., Signature 10 is feminine nouns, sg., andpl., and the remaining Signatures 7 and 9 are again subsets of the regular adjective pattern ofSignature 1.180Goldsmith Unsupervised Learning of the Morphology of a Natural LanguageTable 6Top 10 signatures, 130,000-word Spanish corpus.1.
a.as.o.os 4.
NULL.n 7.
NULL.a.as.o.osabiert abrfa algunaficionad abriria buenajen acabase esamig acabe mfantigu acaece primercompuest acertaba uncortesan acometfacubiert acompafiaba 8.
NULL.escuy acordaba - ~ngeldelicad aguardaba animal~rbol2.
NULLs 5.
NULL.n.s azulaborrecido caballero bachillerabrasado cante belianisabundante debfa bienacaecimiento dice bueyaccidente dijere calidadachaque duerme cardenalacompafiado entiendeacontecimiento fuerza 9. da.do.racosado hubiera - amancebaacostumbrado miente ata3.
a.o.os 6. a.as.o averiguaafligid agradezc colga~inim anch empleaasalt at6nit fericaballeriz confus fingidesagradecid conozc heridescubiert decill pedidespiert dificultos perseguidorad estrechenemig extrafi 10.
NULL.leflac fresc abraz6acomodaraconsej6afligi6seagradeci6aguardaralegr6arroj6atraerbes6Turning to French, we may briefly inspect he top 10 signatures that we find in a350,000-word corpus in Table 5.
It is instructive to consider the signature a.aient.ait.ant.e.ent.er.es.~rent.d.de.ds, which is ranked ninth among signatures.
It contains a large partof the suffixal pattern from the most common regular conjugation, the first conjuga-tion.Within the scope of the effort covered by this project, the large-scale generaliza-tions extracted about these languages appear to be quite accurate (leaving for furtherdiscussion below the questions of how to link related signatures and related stems).
Itis equally important o take a finer-grained look at the results and quantify them.
To181Computational Linguistics Volume 27, Number 2Table 7Top 10 signatures, 125,000-word Latin corpus.1.
NULL.que 4.
NULL.m 7.
NULL.e.mabierunt abdia angustiaacceperunt abia baptistaaccepit abira barachiaaccinctus abra bethaniaaccipient adonira blasphemiaaddidit adsistente causaadiuvit adulescente conscientiaadoravit adulescentia coronaadplicabis adustione ignorantiaadprehendens aetate lorica2.
NULL.m.s 5. i .
is.o.orum.os.um.us 8. a.ae.am.as.i.is.o.orum.os.um.usacieaquaeductubyssinacivitatecoetudieezechiafacultatefidefimbria3.
a.ae.am.as.isancillaqulucernparabolplagpuellstellsynagogtabultunicangelcubitdiscipuliustoculpopul6.
e.em.es.i.
ibus.is.umfratrgreghominregvicvocannmagnmultunivers9.
NULL.e.m.sazariabanaiaesaiaiosiaiudalucustamassamatthathiapluviasagitta10.
i .o.umbrachicarmelcenaculdanmevangelihysoplectullibanofficioledo this, we have selected from the English and the French analyses a set of 1,000 con-secutive words in the alphabetical list of words from the corpus and divided them intodistinct sets regarding the analysis provided by the present algorithm.
See Tables 10and 11.The first category of analyses, labeled Good, is self-explanatory in the case of mostwords (e.g., proceed, proceeded, proceeding, proceeds), and many of the errors are equallyeasy to identify by eye (abide with no analysis, next to abid-e and abid-ing, or Abn-er).Quite honestly, I was surprised how many words there were in which it was difficultto say what the correct analysis was.
For example, consider the pair aboli-tion and abol-ish.
The words are clearly related, and abolition clearly has a suffix; but does it have thesuffix -ion, -tion, or -ition, and does abolish have the suffix -ish, or -sh?
It is hard to say.182Goldsmith Unsupervised Learning of the Morphology of a Natural LanguageTable 8Top 10 signatures, 100,000-word Italian corpus.Rank Signature Number of Stems Participating in this Signature1 a.e.i.o 552 ica.iche.ici.ico 173 a.i.o 334 e.i 2215 i.o 1646 e.i.o 247 a.e.o 238 a.e.i 239 a.e 13110 NULL.o 7111 e.i.it~ 14Table 9Top 10 signatures, 1,000,000-word Italian corpus.Rank Signature Number of Stems Participatingin this Signature1 .a.e.i.o.
1362 .ica.iche.ici.ico.
433 .a.i.o.
1144 .ia.ica.iche.ici.ico.ie.
135 .a.ando.ano.are.ata.ate 7.ati.ato.azione.6.6 .e.i.
5837 .a.e.i.
478 .i.o.
3839 .a.e.o.
3210 .a.e.
236Table 10Results (English).Category Count PercentGood 829 82.9%Wrong analysis 52 5.2%Failed to analyze 36 3.6%Spurious analysis 83 8.3%Table 11Results (French).Category Count PercentGood 833 83.3%Wrong analysis 61 6.1%Failed to analyze 42 4.2%Spurious analysis 64 6.4%In a case of this sort, my pol icy for assigning success or failure has been inf luenced bytwo criteria.
The first is that analyses are better insofar as they explicitly relate wordsthat are appropr iate ly parallel in semantics, as in the abolish~abolition case; thus I wou ld183Computational Linguistics Volume 27, Number 2give credit to either the analysis aboli-tion/aboli-sh or the analysis abol-ition/abol-ish.
Thesecond criterion is a bit more subtle.
Consider the pair of words alumnus and alumni.Should these be morphologically analyzed in a corpus of English, or rather, shouldfailure to analyze them be penalized for this morphology algorithm?
(Compare in likemanner alibi or allegretti; do these English words contain suffixes?).
My principle hasbeen that if I would have given the system additional credit by virtue of discoveringthat relationship, I have penalized it if it did not discover it; that is a relatively harshcriterion to apply, to be sure.
Should proper names be morphologically analyzed?The answer is often unclear.
In the 500,000 word English corpus, we encounter Alexand Alexis, and the latter is analyzed as alex-is.
I have scored this as correct, muchas I have scored as correct the analyses of Alexand-er and Alexand-re.
On the otherhand, the failure to analyze Alexeyeva despite the presence of Alex and Alexei doesnot seem to me to be an error, while the analysis Anab-el has been scored as anerror, but John-son (and a bit less obviously Wat-son) have not been treated as errors.
29Difficult to classify, too, is the treatment of words such as abet~abetted~abetting.
Thepresent algorithm selects the uniform stem abet in that case, assigning the signatureNULL.ted.ting.
Ultimately what we would like to have is a means of indicating thatthe doubled t is predictable, and that the correct signature is NULL.ed.ing.
At presentthis is not implemented, and I have chosen to mark this as correct, on the groundsthat it is more important o identify words with the same stem than to identify the(in some sense) correct signature.
Still, unclear cases remain: for example, consider thewords accompani-ed/accompani-ment/accompani-st.
The word accompany does not appearas such, but the stem accompany is identified in the word accompany-ing.
The analysisaccompani-st fails to identify the suffix -ist, but it will successfully identify the stem asbeing the same as the one found in accompanied and accompaniment, which it wouldnot have done if it had associated the i with the suffix.
I have, in any event, markedthis analysis as wrong, but without much conviction behind the decision.
Similarly,the analysis of French putative stem embelli with suffixes e/rent/t passes the low testof treating related words with the same stem, but I have counted it as in error, on thegrounds that the analysis is unquestionably one letter off from the correct, traditionalanalysis of second-conjugation verbs.
This points to a more general issue regardingFrench morphology, which is more complex than that of English.
The infinitive ~crire'to write' would ideally be analyzed as a stem &r plus a derivational suffix i followedby an infinitival suffix re.
Since the derivational suffix i occurs in all its inflected forms,it is not unreasonable to find an analysis in which the i is integrated into the stemitself.
This is what the algorithm does, employing the stem dcri for the words dcri-re and~cri-t. Ecrit in turn is the stem for dcrite, ~crite, ~crites, &rits, and ~criture.
An alternatestem form dcriv is used for past tense forms (and the nominalization dcrivain) withthe suffixes aient, ait, ant, irent, it.
The algorithm does not make explicit the connectionbetween these two stems, as it ideally would.Thus in the tables, Good indicates the categories of words where the analysis wasclearly right, while the incorrect analyses have been broken into several categories.Wrong Analysis is for bimorphemic words that are analyzed, but incorrectly analyzed,by the algorithm.
Failed to Analyze are the cases of words that are bimorphemic but29 My inability to determine the correct morphological nalysis in a wide range of words that I knowperfectly well seems to me to be essentially the same response as has often been observed in the caseof speakers of Japanese, Chinese, and Korean when forced to place word boundaries in e-mailromanizations of their language.
Ultimately the quality of a morphological nalysis must be measuredby how well the algorithm handles the clear cases, how well it displays the relationships betweenwords perceived to be related, and how well it serves as the language model for a stochasticmorphology of the language in question.184Goldsmith Unsupervised Learning of the Morphology of a Natural Languagefor which no analysis was provided by the algorithm, and Spurious Analysis are thecases of words that are not morphologically complex but were analyzed as containinga suffix.For both English and French, correct performance is found in 83% of the words;details are presented in Tables 10 and 11.
For English, these figures correspond toprecision of 829/(829 + 52 + 83) = 85.9%, and recall of 829/(829 + 52 + 36) = 90.4%.8.
TriageAs noted above, the goal of triage is to determine how many stems must occur inorder for the data to be strong enough to support he existence of a linguistically realsignature.
MDL provides a simple but not altogether satisfactory method of achievingthis end.Using MDL for this task amounts to determining whether the total descriptionlength decreases when a signature is eliminated by taking all of its words and elim-inating their morphological structure, and reanalyzing the words as morphologicallysimple (i.e., as having no morphological structure).
This is how we have implementedit, in any event; one could well imagine a variant under which some or all subpartsof the signature that comprised other signatures were made part of those other sig-natures.
For example, the signature NULL.ine.ly is motivated just for the stem just.Under the former triage criterion, justine and justly would be treated as unanalyzedwords, whereas under the latter, just and justly would be made members of the (large)NULL.ly signature, and just and justine might additionally be treated as comprisingparts of the signature NULL.ine along with bernard, gerald, eng, capitol, elephant, def, andsup (although that would involve permitting a single stem to participate in two distinctsignatures).Our MDL-based measure tests the goodness of a signature by testing each sig-nature cr to see if the analysis is better when that signature is deleted.
This deletionentails treating the signature's words as members of the signature of unanalyzed words(which is the largest signature, and hence such signature pointers are relatively short).Each word member of the signature, however, now becomes a separate stem, with allof the increase in pointer length that that entails, as well as increase in letter contentfor the stem component.One may draw the following conclusions, I believe, from the straightforward ap-plication of such a measure.
On the whole, the effects are quite good, but by no meansas close as one would like to a human's decisions in a certain number of cases.
Inaddition, the effects are significantly influenced by two decisions that we have al-ready discussed: (i) the information associated with each letter, and (ii) the decisionas to whether to model suffix frequency based solely on signature-internal frequences,or based on frequency across the entire morphology.
The greater the information as-sociated with each letter, the more worthwhile morphology is (because maintainingmultiple copies of nearly similar stems becomes increasingly costly and burdensome).When suffix frequencies (which are used to compute the compressed length of anyanalyzed word) are based on the frequency of the suffixes in the entire lexicon, ratherthan conditionally within the signature in question, the loss of a signature ntails a hiton the compression of all other words in the lexicon that employed that suffix; hencetriage is less dramatic under that modeling assumption.Consider the effect of this computation on the signatures produced from a 500,000-word corpus of English.
After the modifications discussed to this point, but beforetriage, there were 603 signatures with two or more stems and two or more suffixes,and there were 1,490 signatures altogether.
Application of triage leads to the loss185Computational Linguistics Volume 27, Number 2of only 240 signatures.
The single-suffix signatures that were eliminated were: ide,it, rs, he, ton, o, and ie, all of which are spurious.
However, a number of signaturesthat should not have been lost were eliminated, most strikingly: NULL.ness, with 51good analyses, NULL.ful, with 18 good analyses, and NULL.ish with only 8 analyses.Most of the cases eliminated, however, were indeed spurious.
Counting only thosesignatures that involves suffixes (rather than compounds) and that were in fact correct,the percentage of the words whose analysis was incorrectly eliminated by triage was21.9% (236 out of 1,077 changes).
Interestingly, in light of the discussion on resultsabove, one of the signatures that was lost was i.us for the Latin plural (based in thisparticular case on genii~genius).
Also eliminated (and this is most regrettable) wasNULL.n't (could~had~does~were~would/did).Because maximizing correct results is as important as testing the MDL modelproposed here, I have also utilized a triage algorithm that departs from the MDL-based optimization in certain cases, which I shall identify in a moment.
I believe thatwhen the improvements identified in Section 10 below are made, the purely MDL-based algorithm will be more accurate; that prediction remains to be tested, to besure.
On this account, we discard any signature for which the total number of stemletters is less than five, and any signature consisting of a single, one-letter suffix; wekeep, then, only signatures for which the savings in letter counts is greater than 15(where savings in letter counts is simply the difference between the sum of the lengthof words spelled out as a monomorphemic word and the sum of the lengths of thestems and the suffixes); 15 is chosen empirically.9.
ParadigmsAs we noted briefly above, the existence of a regular pattern of suffixation with ndistinct suffixes will generally give rise to a large set of stems displaying all n suffixes,but it will also give rise in general to stems displaying most possible combinationsof subsets of these suffixes.
Thus, if there is a regular paradigm in English consistingof the suffixes NULL, -s, -ing, and -ed, we expect o find stems appearing with mostpossible combinations of these suffixes as well.
As this case clearly shows, not all suchpredicted subpatterns are merely partially filled paradigms.
Of stems appearing withthe signature NULL.s, some are verbs (such as occur~occurs), but the overwhelmingmajority, of course, are nouns.In the present version of the algorithm, no effort is made to directly relate signa-tures to one another, and this has a significant and negative impact on performance,because analyses in which stems are affiliated with high-frequency signatures are morehighly valued than those in which they are affiliated with low-frequency signatures; itis thus of capital importance not to underestimate he total frequency of a signature.
B?When two signatures as we have defined them here are collapsed, there are two majoreffects on the description length: pointers to the merged signature are shorter-- leadingto a shorter total description length--but,  in general, predicted frequencies of the com-30 As long as we keep the total number of words fixed, the global task of minimizing description lengthcan generally be obtained by the local strategy of finding the largest cohort for a group of forms toassociate with: if the same data can be analyzed in two ways, with the data forming groups of sizes{a}} in one case, and {a2}in the other, maximal compression is obtained by choosing the case (k -- 1, 2)for which~ log(a~)iis the greatest.186Goldsmith Unsupervised Learning of the Morphology of a Natural Languageposite words are worse than they were, leading to a poorer description (via increasedcross-entropy, we might say).
In practice, the collapsing of signatures is rejected bythe MDL measure that we have implemented here.In work in progress, we treat groups of signatures (as defined here) as parts oflarger groups, called paradigms.
A paradigm consisting of tile suffixes NULL.ed.ing.s,for example, includes all 15 possible combinations of these suffixes.
We can in generalestimate the number of stems we would expect o appear with zero counts for one ormore of the suffixes, given a frequency distribution, such as a multinomial distribution,for the su f f ixes .
31 In this way, we can establish some reasonable frequencies for the caseof stems appearing in a corpus with only a single suffix.
It appears at this time that theunavailability of this information is the single most significant cause of inaccuraciesin the present algorithm.
It is thus of considerable importance to get a handle on suchestimates.
3210.
Remaining IssuesA number of practical questions remain at this point.
The most important are thefollowing:.
Identifying related stems (allomorphs).
Languages typically haveprinciples at work relating pairs of stems, as in English many stems (likewin) are related to another stem with a doubled consonant (winn, as inwinn-ing).
We have been reasonably successful in identifying suchsemiregular morphology, and will report this in a future publication.There is a soft line between the discovery of related stems, on the onehand, and the parsing of a word into several suffixes.
For example, inthe case mentioned briefly above for French, it is not unreasonable topropose two stems for 'to write' ~cri and dcriv, each used in distinctforms.
It would also be reasonable, in this case, to analyze the latter stemdcriv as composed of ~cri plus a suffix -v, although in this case, there areno additional benefits to be gained from the more fine-grained analysis.31 In particular, consider a paradigm with a set {j~i} of suffixes.
We may represent a subsignature of thatsignature as a string of 0s and ls (a Boolean string b, of the form {0,1}*, abbreviated bk) indicatingwhether (or not) the ith suffix is contained in the subsignature.
If a stem t occurs \[t\] times, then theprobability that it occurs without a particular suffix~ is (1 -prob(fi))\[tJ; the probability that it occurswithout all of the suffixes missing from the particular subsignature b = {bk} isI - I (1  -- bk)(1 -- prob(fi))\[t\];kand the probability that the particular subsignature b will arise at all is the sum of those values overall of the stems in the signature:l - I (1  - bk)(1 -- probOCi)) \[tn\] .tn C stems(G) kThus all that is necessary is to estimate the hidden parameters of the frequencies of the individualsuffixes in the entire paradigm.
See the following note as well.32 There may appear to be a contradiction between this observation about paradigms and the statementin the preceding paragraph that MDL rejects ignature mergers- -but  there is no contradiction.
Therejection of signature mergers is performed (so to speak) by the model which posits that frequencies ofsuffixes inside a signature are based only on suffix frequencies of the stems that appear with exactlythe same set of suffixes in the corpus.
It is that modeling assumption that needs to be dropped, andreplaced by a multinomial-based frequency prediction based on counts over the 2 n - 1 signaturesbelonging to each paradigm of length n.187Computational Linguistics Volume 27, Number 2....Identi fy ing parad igms f rom signatures.
We wou ld  like to automatical lyidentify NULL.ed.
ing as a subcase of the more general NULL.ed.ing.s.
Thisis a difficult task to accompl ish well, as Engl ish illustrates, for we wou ldlike to be able to determine that NULL.s  is pr imari ly  a subcase of's.NULL.s, and not of (e.g.)
NULL.ed.s.
33Determining the relationship between prefixation and suffixation.
Thesystem currently assumes that prefixes are to be str ipped off the stemthat has already been identif ied by  suffix stripping.
In future work,  wewou ld  like to see alternative hypotheses regarding the relationship ofprefixation and suffixation tested by  the MDL criterion.Ident i fy ing compounds .
In work  reported in Goldsmith  and Reutter(1998), we have explored the usefulness of the present system fordetermin ing the l inking elements used in German compounds ,  but  morework  remains to be done to identify compounds  in general.
Here we runstraight into the prob lem of assigning very short strings a lowerl ikel ihood of being words  than longer strings.
That is, it is difficult toavoid posit ing a certain number  of very short stems, as in Engl ish m andan, the first because of pairs such as me and my, the second because ofpairs such as an and any, but  these facts should  not  be taken as strongevidence that man is a compound.As noted at the outset, the present a lgor i thm is l imited in its ability todiscover the morpho logy  of a language in wh ich  there are not  asufficient number  of words  with only one suffix in the corpus.
In workin progress, we are developing a related a lgor i thm that deals with the33 We noted in the preceding section that we can estimate the likelihood of a subsignature assuming amultinomial distribution.
We can in fact do better than was indicated there, in the sense that for agiven observed signature a*, whose suffixes constitute asubset of a larger signature ~r, we cancompute the likelihood that a is responsible for the generation of ?
*, where {?i} are the frequencies(summing to 1.0) associating with each of the suffixes in a, and {ci} are the counts of thecorresponding suffixes in the observed signature a*:it1 ) it\[,~,\[Cl\], \[c2\] .
.
.
.
.
\[c,\] ~(i)c' - -  \[C11!\[C21\[ ... Cn\]!i= I  i=1The log likelihood is thenor approximately/tlog\[t\]!
+ ~ ci log ~i - log\[ci\] !,i=1,,ogtfrom Stirling's approximation.
If we normalize the cis to form a distribution (by dividing by \[t\]) anddenote these by di, then this can be simply expressed in terms of the Kullback-Leibler distanceD(a* II a):It\] log\[t\] - ~ ci log = It\] log\[t\] - It\]= \[t\] log\[t\]- \[t\]D(?
* \[I~) -  \[t\] ~_dilog(\[t\])= \[t\]log\[t\] - [t\]D(a* II a) - \[t\] log\[t\]= -\[t\]D(?
* I\]c~).188Goldsmith Unsupervised Learning of the Morphology of a Natural Language.more general case.
In the more general case, it is even more important todevelop a model that deals with the layered relationship among suffixesin a language.
The present system does not explicitly deal with theserelationships: for example, while it does break up ments into ment and s,it does not explicitly determine which suffixes may attach to, etc.
Thismust be done in a more adequate version.In work in progress, we have added to the capability of the algorithmthe ability to posit suffixes that are in part subtractive morphemes.
Thatis, in English, we would like to establish a single signature that combinesNULL.ed.ing.s and e.ed.es.ing (for jump and love, respectively).
We posit anoperator Ix/which deletes a preceding character x, and with themechanism, we can establish a single signature NULL.leled.leling.s,composed of familiar suffixes NULL and s, plus two suffixes leled andleling, which delete a preceding (stem-final) e if one is present.11.
ConclusionLinguists face at the present ime the question of whether, and to what extent,information-theoretic notions will play a significant role in our understanding of lin-guistic theory over the years to come, and the present system perhaps casts a smallray of light in this area.
As we have already noted, MDL analysis makes clear what thetwo areas are in which an analysis can be judged: it can be judged in its ability to dealwith the data, as measured by its ability to compress the data, and it can be judged onits complexity as a theory.
While the former view is undoubtedly controversial whenviewed from the light of mainstream linguistics, it is the prospect of being able to saysomething about he complexity of a theory that is potentially the most exciting.
Evenmore importantly, to the extent hat we can make these notions explicit, we stand achance of being able to develop an explicit model of language acquisition employingthese ideas.A natural question to ask is whether the algorithm presented here is intendedto be understood as a hypothesis regarding the way in which human beings acquiremorphology.
I have not employed, in the design of this algorithm, a great deal of innateknowledge regarding morphology, but that is for the simple reason that knowledge ofhow words divide into subpieces i  an area of knowledge which no one would taketo be innate in any direct fashion: if sanity is parsed as san + ity in one language, itmay perfectly well be parsed as sa + nity in another language.That is, while passion may flame disagreements between partisans of UniversalGrammar and partisans of statistically grounded empiricism regarding the task ofsyntax acquisition, the task which we have studied here is a considerably more humbleone, which must in some fashion or other be figured out by grunt work by the languagelearner.
It thus allows us a much sharper image of how powerful the tools are likelyto be that the language acquirer brings to the task.
And does the human child performcomputations atall like the ones proposed here?From most practical points of view, nothing hinges on our answer to this question,but it is a question that ultimately we cannot avoid facing.
Reformulated a bit, onemight pose the question, does the young language learner--who has access not onlyto the spoken language, but perhaps also to the rudiments of the syntax and to theintended meaning of the words and sentences--does the young learner have accessto additional information that simplifies the task of morpheme identification?
It isthe belief that the answer to this question is yes that drives the intuition (if one has189Computational Linguistics Volume 27, Number 2this intuition) that an MDL-based analysis of the present sort is an unlikely model ofhuman language acquisition.But I think that such a belief is very likely mistaken.
Knowledge of semantics andeven grammar is unlikely to make the problem of morphology discovery significantlyeasier.
In surveying the various approaches to the problem that I have explored (onlythe best of which have been described here), I do not know of any problem (of thosewhich the present algorithm deals with successfully) that would have been solvedby having direct access to either syntax or semantics.
To the contrary: I have tried tofind the simplest algorithm capable of dealing with the facts as we know them.
Theproblem of determining whether two distinct signatures derive from a single largerparadigm would be simplified with such knowledge, but that is the exception and notthe rule.So in the end, I think that the hypothesis that the child uses an MDL-like analysishas a good deal going for it.
In any event, it is far from clear to me how one coulduse information, either grammatical or contextual, to elucidate the problem of thediscovery of morphemes without recourse to notions along the lines of those used inthe present algorithm.Of course, in all likelihood, the task of the present algorithm is not the sameas the language learner's task; it seems unlikely that the child first determines whatthe words are in the language (at least, the words as they are defined in traditionalorthographic terms) and then infers the morphemes.
The more general problem oflanguage acquisition is one that includes the problems of identifying morphemes,of identifying words both morphologically analyzed and nonanalyzed, of identifyingsyntactic ategories of the words in question, and of inferring the rules guiding thedistribution of such syntactic categories.
It seems to me that the only manageablekind of approach to dealing with such a complex task is to view it as an optimizationproblem, of which MDL is one particular style.Chomsky's early conception of generative grammar (Chomsky 1975 \[1955\]; hence-forth LSLT) was developed along these lines as well; his notion of an evaluation metricfor grammars was equivalent in its essential purpose to the description length of themorphology utilized in the present paper.
The primary difference between the LSLTapproach and the MDL approach is this: the LSLT approach conjectured that the gram-mar of a language could be factored into two parts, one universal and one language-particular; and when we look for the simplest grammatical description of a givencorpus (the child's input) it is only the language-particular part of the description thatcontributes to complexity--that is what the theory stipulates.
By contrast, the MDLapproach makes minimal universal assumptions, and so the complexity of everythingcomprising the description of the corpus must be counted in determining the complex-ity of the description.
The difference between these hypotheses vanishes asymptotically(as Janos Simon has pointed out to me) as the size of the language increases, or to put itanother way, strong Chomskian rationalism is indistinguishable from pure empiricismas the information content of the (empiricist) MDL-induced grammar increases in sizerelative to the information content of UG.
Rephrasing that slightly, the significanceof Chomskian-style rationalism is greater, the simpler language-particular grammarsare, and it is less significant as language-particular grammars grow larger, and in thelimit, as the size of grammars grows asymptotically, traditional generative grammaris indistinguishable from MDL-style rationalism.
We return to this point below.There is a striking point that has so far remained tacit regarding the treatmentof this problem in contemporary linguistic theory.
That point is this: the problem ad-dressed in this paper is not mentioned, not defined, and not addressed.
The problemof dividing up words into morphemes i generally taken as one that is so trivial and190Goldsmith Unsupervised Learning of the Morphology of a Natural Languagedevoid of interest hat morphologists, or linguists more generally, simply do not feelobliged to think about the problem.
34 In a very uninteresting sense, the challenge pre-sented by the present paper to current morphological theory is no challenge at all,because morphological theory makes no claims to knowing how to discover morpho-logical analysis; it claims only to know what to do once the morphemes have beenidentified.The early generative grammar view, as explored in LSLT, posits a grammar ofpossible grammars, that is, a format in which the rules of the morphology and syntaxmust be written, and it establishes the semantics of these rules, which is to say, howthey function.
This grammar of grammars is called variously Universal Grammar, orLinguistic Theory, and it is generally assumed to be accessible to humans on the basisof an innate endowment, hough one need not buy into that assumption to acceptthe rest of the theory.
In Syntactic Structures (Chomsky 1957, 51ff.
), Chomsky famouslyargued that the goal of a linguistic theory that produces a grammar automatically,given a corpus as input, is far too demanding a goal.
His own theory cannot do that,and he suggests that no one else has any idea how to accomplish the task.
He suggestsfurthermore that the next weaker position--that of developing a linguistic theory thatcould determine, given the data and the account (grammar), whether this was the bestgrammar--was still significantly past our theoretical reach, and he suggests finally thatthe next weaker position is a not unreasonable one to expect of linguistic theory: thatit be able to pass judgment on which of two grammars is superior with respect o agiven corpus.That position is, of course, exactly the position taken by the MDL framework,which offers no help in coming up with analyses, but which is excellent at judging therelative merits of two analyses of a single corpus of data.
In this paper, we have seenthis point throughout, for we have carefully distinguished between heuristics, whichpropose possible analyses and modifications of analyses, on the one hand, and theMDL measurement, which makes the final judgment call, deciding whether to accepta modification proposed by the heuristics, on the other.On so much, the early generative grammar of LSLT and MDL agree.
But theydisagree with regard to two points, and on these points, MDL makes clearer, moreexplicit claims, and both claims appear to be strongly supported by the present study.The two points are these: the generative view is that there is inevitably an idiosyn-cratic character to Universal Grammar that amounts to a substantive innate capacity,on the grounds (in part) that the task of discovering the correct grammar of a humanlanguage, given only the corpus available to the child, is insurmountable, because thiscorpus is not sufficient o home in on the correct grammar.
The research strategy asso-ciated with this position is to hypothesize certain compression techniques (generallycalled "rule formalisms" in generative grammar) that lead to significant reduction inthe size of the grammars of a number of natural anguages, compared to what wouldhave been possible without them.
Sequential rule ordering is one such suggestiondiscussed at length in LSLT.To reformulate this in a fashion that allows us to make a clearer comparison withMDL, we may formulate early generative grammar in the following way: To selectthe correct Universal Grammar out of a set of proposed Universal Grammars {UGi},given corpora for a range of human languages, elect hat UG for which the sum of thesizes of the grammars for all of the corpora is the smallest.
It does not follow--it need notbe the case--that the grammar of English (or German, etc.)
selected by the winning34 Though see Dobrin (1999) for a sophisticated look at this problem.191Computational Linguistics Volume 27, Number 2UG is the shortest one of all the candidate English grammars, but the winning UG isall-round the supplier of the shortest grammars around the worldJ  sMDL could be formulated in those terms, undoubtedly, but it also can be formu-lated in a language-particular f shion, which is how it has been used in this paper.Generative grammar is inherently universalist; it has no language-particular format,other than to say that the best grammar for a given language is the shortest grammar.But we know that such a position is untenable, and it is precisely out of thatknowledge that MDL was born.
The position is untenable because we can alwaysmake an arbitrarily small compression of a given set of data, if we are allowed tomake the grammar arbitrarily complex, to match and, potentially, to overfit the data,and it is untenable because generative grammar offers no explicit notion of how wella grammar must match the training data.
MDUs insight is that it is possible to makeexplicit the trade-off between complexity of the analysis and snugness of fit to thedata-corpus in question.The first tool in that computational trade-off is the use of a probabilistic modelto compress the data, using stock tools of classical information theory.
These notionswere rejected as irrelevant by early workers in early generative grammar (Goldsmith2001).
Notions of probabilistic grammar due to Solomonoff (1995) were not integratedinto that framework, and the possibility of using them to quantify the goodness of fitof a grammar to a corpus was not exploited.It seems to me that it is in this context that we can best understand the wayin which traditional generative grammar and contemporary probabilistic grammarformalism can be understood as complementing each other.
I, at least, take it in thatway, and this paper is offered in that spirit.AppendixSince what we are really interested in computing is not the min imum descriptionlength as such, but rather the difference between the description length of one modeland that of a variant, it is convenient to consider the general form of the differencebetween two MDL computations.
In general, let us say we will compare two analyses$1 and $2 for the same corpus, where $2 typically contains some item(s) that $1 doesnot (or they may differ by where they break a string into factors).
Let us write out thedifference in length between these two analyses, as in (7)-(11), calculating the lengthof $1 minus the length of $2.
The general formulas derived in (7)-(11) are not of directcomputational interest; they serve rather as a template that can be filled in to computethe change in description length occasioned by a particular structural change in themorphology proposed by a particular heuristic.
This template is rather complex inits most general form, but it simplifies considerably in any specific application.
Theheuristic determines which of the terms in these formulas take on nonzero values,and what their values are; the overall formula determines whether the change inquestion improves the description length.
In addition, we may regard the formulas in35 As the discussion i  the text may suggest, I arn skeptical of the generative position, and I would like toidentify what empirical result would confirm the generative position and dissolve my skepticism.
Theresult would be the discovery of two grammars of English, G1 and G2, with the following properties:G1 is inherently simpler than G2, using some appropriate notion of Turing machine programcomplexity, and yet G2 is the correct grammar of English, based on some of the complexity of G2 beingthe responsibility oflinguistic theory, hence "free" in the complexity competition between G1 and G2.That is, the proponent of the generative iew must be willing to acknowledge that overall complexityof the grammar of a language may be greater than logically necessary due to evolution's investment inone particular style of programming language.192Goldsmith Unsupervised Learning of the Morphology of a Natural Language(7)-(11) as offering us an exact and explicit statement of how a morphology can beimproved.The notation can be considerably simplified if we take some care in advance.Note first that in (7) and below, several items are subscripted to indicate whether theyshould be counted as in $1 or $2.
Much of the simplification comes from observing,first, thatlog MI~ _ log 9 = log M22 - log ;second, that this difference is generally computed inside a summation over a set ofmorphemes, and hence the first term simplifies to a constant times the type count ofthe morphemes in the set in question.
Indeed, so prevalent in these calculations i theformulalog X~t~t~lXstate2that the introduction of a new abbreviation considerably simplifies the notation.
Weuse A(x) to denotelog ~,where the numerator is a count in $1, and the denominator a count of the same variablein $2; if no confusion would result, we write Ax.
36Let us review the terms listed in (7)-(11).
AN is a measure of the change in thenumber of total words due to tile proposed modification (the difference between the $1and $2 analyses); an increase in the total number of words results in a slightly negativevalue.
In the text above, I indicated that we could, by judicious choice of word countdistribution, keep Wl = W2; I have included the more general case in (7)-(11) wherethe two may be different.
AWs and AWc are similar measures in the change of wordsthat have morphologically simple, and morphologically complex, stems, respectively.They measure the global effects of the typically small changes brought about by ahypothetical change in morphological model.
In the derivation of each formula, weconsider first the case of those morphemes that are found in both $1 and $2 (indicated($1, $2)), followed by those found only in S1 ($l, ~ $2), and then those only found in$2 ('-~ $1, $2).
Recall that angle brackets are used to indicate the type count of a set, thenumber of typographically distinct members of a set.In (8), we derive a formula for the change in length of the suffix component ofthe morphology.
Observe the final formulation, in which the first two terms involvesuffixes present in both $1 and $2, while the third term involves uffixes present onlyin $1 and the fourth term involves uffixes present only in $2.
This format will appearin all of the components of this computation.
Recall that the function Ltypo specifiesthe length of a string in bits, which we may take here to be simply log(26) times thenumber of characters in the string.In (9), we derive the corresponding formula for the stem component.The general form of the computation of the change to the signature component(10) is more complicated, and this complexity motivates a little bit more notation tosimplify it.
First, we can compute the change in the pointers to the signatures, and theinformation that each signature contains regarding the count of its stems and suffixes36 We beg the reader's indulgence in recognizing that we prepend the operator A immediately to the leftof the name of a set to indicate the change in the size of the counts of the set, which is to say, "AW" isshorthand for "A(\[W\])", and "A(W}" for "A((W))".193Computational Linguistics Volume 27, Number 2as in (10a).
But the heart of the matter is the treatment of the stems and suffixes withinthe signatures, given in (10b)-(10d).Bear in mind, first of all, that each signature consists of a list of pointers to stems,and a list of pointers to suffixes.
The treatment of suffixes is given in (10d), and isrelatively straightforward, but the treatment of stems (10c) is a bit more complex.Recall that all items on the stem list will be pointed to by exactly one stem pointer,located in some particular signature.
All stem pointers in a signature that point tostems on the suffix list are directly described a "simple" word, a notion we havealready encountered: a word whose stem is not further analyzable.
But other wordsmay be complex, that is, may contain a stem whose pointer is to an analyzable word,and hence the stem's representation consists of a pointer triple: a pointer to a signature,a stem within the signature, and a suffix within the signature.
And each stem pointeris preceded by a flag indicating which type of stem it is.We thus have three things whose difference in the two states, $1 and $2, we wishto compute.
The difference of the lengths of the flag is given in (10c.i).
In (10c.ii), weneed change in the total length of the pointers to the stems, and this has actuallyalready been computed, during the computation of (9).
37 Finally in (10c.iii), the set ofpointers from certain stem positions to words consists of pointers to all of the wordsthat we have already labeled as being in Wo and we can compute the length of thesepointers by adding counts to these words; the length of the pointers to these wordsneeds to be computed anyway in determining the compressed length of the corpus.This completes the computations needed to compare two states of the morphology.In addition, we must compute the difference in the compressed length of thecorpus in the two states, and this is given in (11).
(7) Differences in description length due to organizational information:A (Suffixes) + A (Stems) + A (Signatures)(8) Difference in description length for suffix component of the morphology:AW(Suffixes)(1,2) - ~ Af q- ~_~f~  S'tLYff:~2:e'~(1,2 ) f~  Sud~xe$(1,~2)fE Suffixes(~l,2)(9) Difference in description length for stem component of the morphology:r \[W\]l  \] AW (Stems)(1,2) - ~_, At + ~ \[log ~ + Ltypo(t)t6 Steads(i,2 ) t6 Ste11"~,8(1,~2)- }2 \[,og \[w\]2 -~  +Ltyvo(t)\]tC St e~7,s (~1,2)37 The equivalence between the number computed in (9) and the number needed here is not exactlyfortuitous, but it is not an error either.
The figure computed in (9) describes an aspect of the complexityof the morphology as a whole, whereas the computation described here in the text is what it is becausewe have made the assumption that each stem occurs in exactly one signature.
That assumption is not,strictly speaking, correct in natural anguage; we could well imagine an analysis that permitted thesame stem to appear in several distinction signatures, and in that case, the computation here would notreduce to (9).
But the assumption made in the text is entirely reasonable, and simplifies theconstruction for us.194Goldsmith Unsupervised Learning of the Morphology of a Natural Language(10) Difference in description length for the signature component of the morphology:(a) + (b) + (c) + (d)(a) Change in size of list of pointers to the signatures,& W I Signatures (1,2 ) ) -\[wll+ E logcrC Sigrtatures(1N2)crC Signatures(l,2 )E log \[WIzcrC Signatures(~l,2)(b) Change in counts of stems and suffixes within each signature, summedover all signatures:Z \[A (stems(a)} + A (suffixes(a)}\]crff Signatures(i,2 )- ~ \[log (stems(a)) + log (suffi'xes(er))\]ere Signature~(1,~2)+ Z \[log (stems(a)) + log (suffixes(c@\]~C Sigctature8(~l,2)(c)(c.i)(c.ii)Change in the lengths of the stem pointers within the signatures = (c.i)+ (c.ii) + (c.iii), as follows:Change in total length of flags for each stem indicating whethersimple or complex:(WfiIMPLE)I,2 (&W - &WsIMPLE)q- ( WCOMPLEX ) 1,2 * (A  W - & WCOMPLEX )\[whq- (WsIMPLE) I ,~ 2 log \[WsIMPLE\]I-- (WsIMPLE)~I ,  2 log \[W\]2 \[Ws~veL.\]2\[wh + (WcoMPLEX)1 ~2 log - -' \ [WCOMULEXh\[w\]2- (WcoMPLEX)~I,2 log \[WcoMPLUX\]2Set of simple stems, change of pointers to stems:\[w\]2 AW (Stems)o,2) - Z At + Z log \[W\]I E log -~-Ste~q,8(1 2) tE Stems(l ~2) It\] tff , , tff Stems (~1,2)(c.iii) Change in length of pointers to complex stems from withinsignatures:&W (WcoMPLEX}(1,2) q- Z &stem(w)wffWcoMPLIdJX(L2)195Computational Linguistics Volume 27, Number 2\[w\]l \[w\]2+ E log \[stem(w)\]1 E log \[stem(w)\]2wE WCOMPLE X (1,~2) wE WCOMPLE X (~1,2)+ E&a(w)  - &\[suff(w)in ?
(w)\]wE WCOMPLE X+ \[~(w)\]K-"Z.., log \[suff(w)in a(w)\]wE WCOMPLE X O,~2 )\[?
(w)\]K-"/_._, log \[suff(w)in a(w)\]wE WCOMPLE X (~1,2)(d) Change in size of suffix information in signatures:cr E Signatures (1,2)+:EoZ as\]E E l ?g  ~In\]a\]ere Signatures(i,~2) fEo"E E l?g ~fi\[n\]a\]aE Signatures(~l,2) fE(11) Change in compressed length of corpus\ [W\ ] ra~AW --WEWA(1,2)+ ~ \[Wlra~aWwEWuN(1,2)+ ~ \[Wlra~lOgWEWA(1,~2)- ~ \[Wlra~ logwEWa(~l,2)E \[w\],.a~\[&stem(w) + &\[suffix(w) Na(w)\] - &?
(w)\]\[stem(w)\]l \[suffix(w)1 N o-(W)l\]\[o-(w)\]l\[W\]2 ,\[stem(w) \]2\[suffix(w)2 V~ o-(w)2 \]\[o-(w)\]2 \[w\] 1ReferencesAltmann, Gabriel and Werner Lehfeldt.1980.
Einfiihrung in die QuantitativePhonologie.
Quantitative Linguistics,vol.
7.
Studienverlag Dr. N. Brockmeyer,Bochum.Andreev, Nikolai Dmitrievich, editor.
1965.Statistiko-kombinatornoe m delirovanieiazykov.
Nauka, Moscow, Russia.Baroni, Marco.
2000.
Paper presented attheannual meeting of the Linguistics Societyof America.
Chicago, IL.Bloomfield, Leonard.
1933.
Language.
H.Holt and Company, New York.Brent, Michael R. 1993.
Minimal generativemodels: A middle ground betweenneurons and triggers.
In Proceedings ofthe15th Annual Conference of the CognitiveScience Society.
pages 28-36, LawrenceErlbaum Associates, Hillsdale, NJ.Chomsky, Noam.
1957.
Syntactic Structures.Mouton, The Hague.Chomsky, Noam.
1975 \[1955\].
The LogicalStructure of Linguistic Theory.
PlenumPress, New York.de Marcken, Carl.
1995.
UnsupervisedLanguage Acquisition.
Ph.D. dissertation,MIT, Cambridge, MA.196Goldsmith Unsupervised Learning of the Morphology of a Natural LanguageDempster, Arthur Pentland, Nan M. Laird,and Donald B. Rubin.
1977.
Maximumlikelihood from incomplete data via theEM algorithm.
Journal of the RoyalStatistical Society, B 39(1):1-38.Dobrin, Lise.
1999.
Phonological Form,Morphological Class, and Syntactic Gender:The Noun Class Systems of Papua NewGuinea Arapeshan.
Ph.D. dissertation,Department of Linguistics, University ofChicago, Chicago, IL.Dzeroski, Saso and Tomaz Erjavec.
1997.Induction of Slovene nominal paradigms.In Nada Lavrac and Saso Dzeroski,editors, Inductive Logic Programming, 7thInternational Workshop, ILP-97,pages 17-20, Prague, Czech Republic,September.
Lecture Notes in ComputerScience, Vol.
1297.
Springer, Berlin.Flenner, Gudrun.
1994.
Ein quantitativesMorphsegmentierungssystem f~irspanische Wortformen.
In Ursula Klenk,editor, Computatio Linguae II.
SteinerVerlag, Stuttgart, pages 31-62.Flenner, Gudrun.
1995.
QuantitativeMorphsegmentierung im Spanischen aufphonologischer Basis.
Sprache undDatenverarbeitung, 19(2):63-79.Gaussier, Eric.
1999.
Unsupervised learningof derivational morphology frominflectional lexicons.
In Proceedings oftheWorkshop on Unsupervised Learning inNatural Language Processing, pages 24-30.Association for ComputationalLinguistics.Goldsmith, John.
1990.
Autosegmental andMetrical Phonology.
Basil Blackwell,Oxford, England.Goldsmith, John.
2001.
On informationtheory, entropy, and phonology in the20th century.
Folia Linguistica XXXIV(1-2):85-100.Goldsmith, John and Tom Reutter.
1998.Automatic ollection and analysis ofGerman compounds.
In Frederica Busa,Inderjeet Mani, and Patrick Saint-Dizier,editors, The Computational Treatment ofNominals: Proceedings ofthe Workshop,pages 61-69, COLING-ACL '98, Montreal.Greenberg, Joseph Harold.
1957.
Essays inLinguistics.
University of Chicago Press,Chicago, IL.Hafer, Margaret A. and Stephen F. Weiss.1974.
Word segmentation by lettersuccessor varieties.
Information Storage andRetrieval, 10:371-385.Harris, Zellig.
1955.
From phoneme tomorpheme.
Language, 31:190-222.Reprinted in Harris 1970.Harris, Zellig.
1967.
Morpheme boundarieswithin words: Report on a computer test.Transformations and Discourse AnalysisPapers 73, Department of Linguistics,University of Pennsylvania.
Reprinted inHarris 1970.Harris, Zellig.
1970.
Papers in Structural andTransformational Linguistics.
D. Reidel,Dordrecht.Jacquemin, Christian.
1997.
Guessingmorphology from terms and corpora.Proceedings ofSIGIR 97, pages 156--165,ACM, Philadelphia.Janssen, Axel.
1992.
SegmentierungfranzOsischer Wortformen i  Morpheohne Verwendung eines Lexikons.
InUrsula Klenk, editor, Computatio Linguae.Steiner Verlag, Stuttgart, pages 74-95.Karttunen, Lauri.
1993.
Finite stateconstraints.
In John Goldsmith, editor, TheLast Phonological Rule.
University ofChicago Press, pages 173-194.Kazakov, Dimitar.
1997.
Unsupervisedlearning of naive morphology withgenetic algorithms.
In W. Daelemans, A.van den Bosch, and A. Weijtera, editors,Workshop Notes of the ECML/MlnetWorkshop on Empirical Learning of NaturalLanguage Processing Tasks, April 26, 1997.Klenk, Ursula.
1992.
Verfahrenmorphologischer Segmentierung und dieWortstruktur im Spanischen.
In UrsulaKlenk, editor, Computatio Linguae.
SteinerVerlag, Stuttgart.Koch, Sabine, Andreas K(istner, and BarbaraR~idiger.
1989.
DeutscheWortformensegmentierung oh e Lexicon.Sprache und Datenverarbeitung, 13:35-44.Koskenniemi, Kimmo.
1983.
Two-levelmorphology: A general computationalmodel for word-form recognition andproduction.
Publication o.
11,Department of General Linguistics,University of Helsinki, Helsinki.Langer, Hagen.
1991.
Ein automatischesMorphosegmentierungsver fahren f6rdeutsche Wortformen.
Manuscript.Li, Ming and Paul Vit~nyi.
1997.
AnIntroduction to Kolmogorov Complexity andits Applications (2nd ed.).
Springer, NewYork.Nida, Eugene.
1948.
The identification ofmorphemes.
In Martin Joos, editor,Readings in Linguistics I.
University ofChicago Press, Chicago, IL,pages 255-271.Nida, Eugene.
1949.
Morphology: TheDescriptive Analysis of Words.
University ofMichigan, Ann Arbor.Pacak, Milos and Arnold W. Pratt.
1976.Automated morphosyntactic analysis ofmedical language.
Information Processingand Management, 12:71-76.197Computational Linguistics Volume 27, Number 2Radhakrishnan, T. 1978.
Selection of prefixand postfix word fragments for datacompression.
Information Processing andManagement, 14(2):97-106.Rissanen, Jorma.
1989.
Stochastic Complexityin Statistical Inquiry.
World ScientificPublishing Co, Singapore.Solomonoff, Ray.
1995.
The discovery ofalgorithmic probability: A guide for theprogramming of true creativity.
In P.Vitdnyi, editor, Computational LearningTheory.
Spring Verlag, Berlin.Wothke, Klaus and Rudolf Schmidt.
1992.
Amorphological segmentation procedurefor German.
Sprache und Datenverarbeitung,16(1):15-28.198
