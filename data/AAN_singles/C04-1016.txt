Extending MT evaluation tools with translation complexity metricsBogdan BABYCHCentre for TranslationStudies, University of LeedsLeeds, UK, LS2 9JTbogdan@comp.leeds.ac.ukDebbie ELLIOTTSchool of ComputingUniversity of LeedsLeeds, UK, LS2 9JTdebe@comp.leeds.ac.ukAnthony HARTLEYCentre for TranslationStudies, University of LeedsLeeds, UK, LS2 9JTa.hartley@leeds.ac.ukAbstractIn this paper we report on the results of anexperiment in designing resource-light metrics thatpredict the potential translation complexity of atext or a corpus of homogenous texts for state-of-the-art MT systems.
We show that the bestprediction of translation complexity is given by theaverage number of syllables per word (ASW).
Thetranslation complexity metrics based on thisparameter are used to normalise automated MTevaluation scores such as BLEU, which otherwiseare variable across texts of different types.
Thesuggested approach makes a fairer comparisonbetween the MT systems evaluated on differentcorpora.
The translation complexity metric wasintegrated into two automated MT evaluationpackages ?
BLEU and the Weighted N-grammodel.
The extended MT evaluation tools areavailable from the first author?s web site:http://www.comp.leeds.ac.uk/bogdan/evalMT.html1 IntroductionAutomated evaluation tools for MT systems aimat producing scores that are consistent with theresults of human assessment of translation qualityparameters, such as adequacy and fluency.Automated metrics such as BLEU (Papineni et al,2002), RED (Akiba et al 2001), Weighted N-grammodel (WNM) (Babych, 2004), syntactic relation /semantic vector model (Rajman and Hartley, 2001)have been shown to correlate closely with scoringor ranking by different human evaluationparameters.
Automated evaluation is much quickerand cheaper than human evaluation.Another advantage of the scores produced byautomated MT evaluation tools is that intuitivehuman scores depend on the exact formulation ofan evaluation task, on the granularity of themeasuring scale and on the relative quality of thepresented translation variants: human judges mayadjust their evaluation scale in order todiscriminate between slightly better and slightlyworse variants ?
but only those variants which arepresent in the evaluation set.
For example, absolutefigures for a human evaluation of a set whichincludes MT output only are not directlycomparable with the figures for another evaluationwhich might include MT plus a non-native humantranslation, or several human translations ofdifferent quality.
Because of the instability of thisintuitive scale, human evaluation figures should betreated as relative rather than absolute.
Theycapture only a local picture within an evaluated set,but not the quality of the presented texts in a largercontext.
Although automated evaluation scores arealways calibrated with respect to human evaluationresults, only the relative performance of MTsystems within one particular evaluation exerciseprovide meaningful information for suchcalibration.In this respect, automated MT evaluation scoreshave some added value: they rely on objectiveparameters in the evaluated texts, so their resultsare comparable across different evaluations.Furthermore, they are also comparable fordifferent types of texts translated by the same MTsystem, which is not the case for human scores.For example, automated scores are capable ofdistinguishing improved MT performance oneasier texts or degraded performance on hardertexts, so the automated scores also giveinformation on whether one collection of texts iseasier or harder than the other for an MT system:the complexity of the evaluation task is directlyreflected in the evaluation scores.However, there may be a need to avoid suchsensitivity.
MT developers and users are oftenmore interested in scores that would be stableacross different types of texts for the same MTsystem, i.e., would reliably characterise a system?sperformance irrespective of the material used forevaluation.
Such characterisation is especiallyimportant for state-of-the-art commercial MTsystems, which typically target a wide range ofgeneral-purpose text types and are not specificallytuned to any particular genre, like weather reportsor aircraft maintenance manuals.The typical problem of having ?task-dependent?evaluation scores (which change according to thecomplexity of the evaluated texts) is that thereported scores for different MT systems are notdirectly comparable.
Since there is no standardcollection of texts used for benchmarking all MTsystems, it is not clear how a system that achieves,e.g., BLEUr4n4 1  score 0.556 tested on ?490utterances selected from the WSJ?
(Cmejrek et al2003:89) may be compared to another systemwhich achieves, e.g., the BLEUr1n4 score 0.240tested on 10,150 sentences from the ?Basic TravelExpression Corpus?
(Imamura et al, 2003:161).Moreover, even if there is no comparisoninvolved, there is a great degree of uncertainty inhow to interpret the reported automated scores.
Forexample, BLEUr2n4 0.3668 is the highest scorefor a top MT system if MT performance ismeasured on news reports, but it is a relativelypoor score for a corpus of e-mails, and a score thatis still beyond the state-of-the-art for a corpus oflegal documents.
These levels of perfection have tobe established experimentally for each type of text,and there is no way of knowing whether somereported automated score is better or worse if anew type of text is involved in the evaluation.The need to use stable evaluation scores,normalised by the complexity of the evaluatedtask, has been recognised in other NLP areas, suchas anaphora resolution, where the results may berelative with regard to a specific evaluation set.
So?more absolute?
figures are obtained if we usesome measure which quantifies the complexity ofanaphors to be resolved (Mitkov, 2002).MT evaluation is harder than evaluation of otherNLP tasks, which makes it partially dependent onintuitive human judgements about text quality.However, automated tools are capable of capturingand representing the ?absolute?
level ofperformance for MT systems, and this level couldthen be projected into task-dependent figures forharder or easier texts.
In this respect, there isanother ?added value?
in using automated scoresfor MT evaluation.Stable evaluation scores could be achieved if aformal measure of a text?s complexity fortranslation could be cheaply computed for a sourcetext.
Firstly, the score for translation complexityallows the user to predict ?absolute?
performancefigures of an MT system on harder or easier texts,by computing the ?absolute?
evaluation figures andthe complexity scores for just one type of text.Secondly, it lets the user compute ?standardised?performance figures for an MT system that do notdepend on the complexity of a text (they arereliably within some relatively small range for anytype of evaluated texts).Designing such standardised evaluation scoresrequires choosing a point of reference for thecomplexity measure: e.g., one may choose an1BLEUrXnY means the BLEU score with producedwith X reference translations and the maximum size ofcompared N-grams = Y.average complexity of texts usually translated byMT as the reference point.
Then the absolutescores for harder or easier texts will be corrected tofit the region of absolute scores for texts of averagecomplexity.In this paper we report on the results of anexperiment in measuring the complexity oftranslation tasks using resource-light parameterssuch as the average number of syllables per word(ASW), which is also used for computing thereadability of a text.
On the basis of theseparameters we compute normalised BLEU andWNM scores which are relatively stable acrosstranslations produced by the same general-purposeMT systems for texts of varying difficulty.
Wesuggest that further testing and fine-tuning of theproposed approach on larger corpora of differenttext types and using additional source textparameters and normalisation techniques can givebetter prediction of translation complexity andincrease the stability of the normalised MTevaluation scores.2 Set-up of the experimentWe compared the results of the human andautomated evaluation of translations from Frenchinto English of three different types of texts whichvary in size and style: an EU whitepaper on childand youth policy (120 sentences), a collection of36 business and private e-mails and 100 news textsfrom the DARPA 94 MT evaluation corpus (Whiteet al, 1994).
The translations were produced bytwo leading commercial MT systems.
Humanevaluation results are available for all of the texts,with the exception of the news reports translatedby System-2, which was not part of the DARPA 94evaluation.
However, the human evaluation scoreswere collected at different times under differentexperimental conditions using differentformulations of the evaluation tasks, which leads tosubstantial differences between human scoresacross different evaluations, even if the evaluationswere done at the same time.Further, we produced two sets of automatedscores: BLEUr1n4, which have a high correlationwith human scores for fluency, and WNM Recall,which strongly correlate with human scores foradequacy.
These scores were produced under thesame experimental conditions, but they uniformlydiffer for both evaluated systems: BLEU andWNM scores were relatively higher for e-mailsand relatively low for the whitepaper, with thenews texts coming in between.
We interpretedthese differences as reflecting the relativecomplexity of texts for translation.For the French originals of all three sets of textswe computed resource-light parameters used instandard readability measures (Flesch ReadingEase score or Flesch-Kincaid Grade Level score),i.e.
average sentence length (ASL ?
the number ofwords divided by the number of sentences) andaverage number of syllables per word (ASW ?
thenumber of syllables divided by the number ofwords).We computed Pearson?s correlation coefficient rbetween the automated MT evaluation scores andeach of the two readability parameters.
Differencesin the ASL parameter were not strongly linked tothe differences in automated scores, but for theASW parameter a strong negative correlation wasfound.Finally, we computed normalised (?absolute?
)BLEU and WNM scores using the automatedevaluation results for the DARPA news texts (themedium complexity texts) as a reference point.
Wecompared the stability of these scores with thestability of the standard automated scores bycomputing standard deviations for the differenttypes of text.
The absolute automated scores can becomputed on any type of text and they will indicatewhat score is achievable if the same MT systemruns on DARPA news reports.
The normalisedscores allow the user to make comparisonsbetween different MT systems evaluated ondifferent texts at different times.
In most cases theaccuracy of the comparison is currently limited tothe first rounded decimal point of the automatedscore.3 Results of human evaluationsThe human evaluation results were producedunder different experimental conditions.
Theoutput of the compared systems was evaluatedeach time within a different evaluation set, in somecases together with different MT systems, or nativeor non-native human translations.
As a resulthuman evaluation scores are not comparable acrossdifferent evaluations.Human scores available from the DARPA 94MT corpus of news reports were the result of acomparison of five MT systems (one of which wasa statistical MT system) and a professional(?expert?)
human translation.
For our experimentwe used DARPA scores for adequacy and fluencyfor one of the participating systems.We obtained human scores for translations of thewhitepaper and the e-mails from one of our MTevaluation projects at the University of Leeds.
Thishad involved the evaluation of French-to-Englishversions of two leading commercial MT systems ?System 1 and System 2 ?
in order to assess thequality of their output and to determine whetherupdating the system dictionaries brought about animprovement in performance.
(An earlier versionof System 1 also participated in the DARPAevaluation.)
Although the human evaluations ofboth texts were carried out at the same time, theexperimental set-up was different in each case.The evaluation of the whitepaper for adequacywas performed by 20 postgraduate students whoknew very little or no French.
A professionalhuman translation of each segment was availableto the judges as a gold standard reference.
Using afive-point scale in each case, judgments weresolicited on adequacy by means of the followingquestion:?For each segment, read carefully the referencetext on the left.
Then judge how much of thesame content you can find in the candidate text.
?Five independent judgments were collected foreach segment.The whitepaper fluency evaluation wasperformed by 8 postgraduate students and 16business users under similar experimentalconditions with the exception that the goldstandard reference text was not available to thejudges.
The following question was asked:?Look carefully at each segment of text and giveeach one a score according to how much youthink the text reads like fluent English written bya native speaker.
?For e-mails a different quality evaluationparameter was used: 26 human judges (businessusers) evaluated the usability (or utility) of thetranslations.
We also included translationsproduced by a non-professional, French-speakingtranslator in the evaluation set for e-mails.
(Thiswas intended to simulate a situation where, in theabsence of MT, the author of the e-mail wouldhave to write in a foreign language (here English);we anticipated that the quality would be judgedlower than the professional, native speakertranslations.)
The non-native translations weredispersed anonymously in the data set and so werealso judged.
The following question was asked:?Using each reference e-mail on the left, rate thethree alternative versions on the right accordingto how usable you consider them to be forgetting business done.
?Figure 1 and Table 1 summarise the humanevaluation scores for the two compared MTsystems.
The judges had scored versions of the e-mails (?em?)
and whitepaper (?wp?)
produced bothbefore and after dictionary update (?DA?
),although no judge saw the before and after variantsof the same text.
(The scores for the DARPA newstexts are converted from [0, 1] to [0, 5] scale).00.511.522.533.544.55em-USL wp-FLU wp-ADE news-FLU news-ADESystem-1 Before DASystem-1 After DASystem-2 Before DASystem-2 After DANon-native transl.Figure 1.
Human evaluation resultsS1 S1da S2 S2da NNem [usl] 2.511 3.139 2.35 2.733 4.314wp [flu] 3.15 3.47 2.838 3.157wp [ade] 3.94 4.077 3.858 3.977news [flu] 2.54news [ade] 3.945Table 1.
Human evaluation scoresIt can be inferred from the data that humanevaluation scores do not allow us to make anymeaningful comparison of the scores outside aparticular evaluation experiment, whichnecessarily must be interpreted as relative ratherthan absolute.We can see that dictionary update consistentlyimproves the performance of both systems, thatSystem 1 is slightly better than System 2 in allcases, although after dictionary update System 2 iscapable of reaching the baseline quality of System1.
However, the usability scores for supposedlyeasier texts (e-mails) are considerably lower thanthe adequacy scores for harder texts (thewhitepaper), although the experimental set-up foradequacy and usability is very similar: both used agold-standard human reference translation.
Wesuggest that the presence of a higher qualitytranslation done by a human non-native speaker ofthe target language ?over-shadowed?
lower qualityMT output, which dragged down evaluation scoresfor e-mail usability.
No such higher qualitytranslation was present in the evaluation set for thewhitepaper adequacy, so the scores went up.Therefore, no meaning can be given to anyabsolute value of the evaluation scores acrossdifferent experiments involving intuitive humanjudgements.
Only a relative comparison of theseevaluation scores produced within the sameexperiment is possible.4 Results of automated evaluationsAutomated evaluation scores use objectiveparameters, such the number of N-gram matches inthe evaluated text and in a gold standard referencetranslation.
Therefore, these scores are moreconsistent and comparable across differentevaluation experiments.
The comparison of thescores indicates the relative complexity of the textsfor translation.
For the output of both MT systemsunder consideration we generated two sets ofautomated evaluation scores: BLEUr1n4 andWNM Recall.BLEU computes the modified precision of N-gram matches between the evaluated text and aprofessional human reference translation.
It wasfound to produce automated scores, which stronglycorrelate with human judgements about translationfluency (Papineni et al, 2002).WNM is an extension of BLEU with weights ofa term?s salience within a given text.
As comparedto BLEU, the WNM recall-based evaluation scorewas found to produce a higher correlation withhuman judgements about adequacy (Babych,2004).
The salience weights are similar to standardtf.idf scores and are computed as follows: ( ))()()(),( /)(log),(icorpiidoccorpjidocPNdfNPPjiS ??
?= ?
,where:?
Pdoc(i,j) is the relative frequency of the word wi inthe text j; (?Relative frequency?
is the numberof tokens of this word-type divided by the totalnumber of tokens).?
Pcorp-doc(i) is the relative frequency of the sameword wi in the rest of the corpus, without thistext;?
dfi is the number of documents in the corpuswhere the word wi occurs;?
N is the total number of documents in the corpus.?
Pcorp(i) is the relative frequency of the word wi inthe whole corpus, including this particulartext.Figures 2 and 3 and Table 2 summarise theautomated evaluation scores for the two MTsystems.00.050.10.150.20.250.30.350.4bleu-wp bleu-news bleu-emS1S1daS2S2daFigure 2.
Automated BLEUr1n4 scores00.050.10.150.20.250.30.350.40.45wnmR-wp wnmR-news wnmR-emS1S1daS2S2daFigure 3.
Automated WMN Recall scoresscores S1 S1da S2 S2dableu-wp 0.1874 0.2351 0.1315 0.1701bleu-news 0.2831  0.1896bleu-em 0.3257 0.3573 0.2006 0.326wnmR-wp 0.3247 0.3851 0.2758 0.3172wnmR-news 0.3644  0.3439wnmR-em 0.3915 0.4256 0.3792 0.4129r correlation [flu] [ade/usl]bleu-wp 0.9827 0.9453bleu-em  0.7872wnmR-wp 0.9896 0.9705wnmR-em  0.9673Table 2.
Automated evaluation scoresIt can be seen from the charts that automatedscores consistently change according to the type ofthe evaluated text: for both evaluated systemsBLEU and WNM are the lowest for the whitepapertexts, which emerge as most complex to translate,the news reports are in the middle and the highestscores are given to the e-mails, which appear to berelatively easy.
A similar tendency also holds forthe system after dictionary update.
However,technically speaking the compared systems are nolonger the same, because the dictionary update wasdone individually for each system, so the quality ofthe update is an additional factor in the system?sperformance ?
in addition to the complexity of thetranslated texts.The complexity of the translation task isintegrated into the automated MT evaluationscores, but for the same type of texts the scores areperfectly comparable.
For example, for theDARPA news texts, newly generated BLEU andWNM scores confirm the observation made, on thebasis of comparison of the whitepaper and the e-mail texts, that S1 produces higher translationquality than S2, although there is no humanevaluation experiment where such translations aredirectly compared.Thus the automated MT evaluation scores derivefrom both the ?absolute?
output quality of anevaluated general-purpose MT system and thecomplexity of the translated text.5 Readability parametersIn order to isolate the ?absolute?
MT quality andto filter out the contribution of the complexity ofthe evaluated text from automated scores, we needto find a formal parameter of translationcomplexity which should preferably be resource-light, so as to be easily computed for any sourcetext in any language submitted to an MT system.Since automated scores already integrate thetranslation complexity of the evaluated text, wecan validate such a parameter by its correlationwith automated MT evaluation scores computed onthe same set that includes different text types.In our experiment, we examined the followingresource-light parameters for their correlation withboth automated scores:?
Flesch Reading Ease score, which rates text ona 100-point scale according to how easy it is tounderstand; the score is computed as follows:FR = 206.835 ?
(1.015 * ASL) ?
(84.6 *ASW), where:ASL is the average sentence length (thenumber of words divided by the number ofsentences);ASW is the average number of syllables perword (the number of syllables divided by thenumber of words)?
Flesch-Kincaid Grade Level score which ratestexts on US grade-school level and iscomputed as:FKGL = (0.39 * ASL) + (11.8 * ASW) ?15.59?
each of the ASL and ASW parametersindividually.Table 3 presents the averaged readabilityparameters for all French original texts used in ourevaluation experiment and the r correlationbetween these parameters and the correspondingautomated MT evaluation scores.FR FKGL ASL ASWwp 17.3 15.7 19.65 2news 27.8 14.7 21.4 1.86em 61.44   6.98   9.22 1.608r/bleu-S1 0.872 -0.804 -0.641 -0.928r/bleu-S2 0.785 -0.701 -0.513 -0.859r/wnm-S1 0.92 -0.864 -0.721 -0.963r/wnm-S2 0.889 -0.825 -0.669 -0.941r Average 0.866 -0.799 -0.636 -0.923Table 3.
Readability of French originalsTable 3 shows that the strongest negativecorrelation exists between ASW (average numberof syllables per word) and the automatedevaluation scores.
Therefore the ASW parametercan be used to normalise MT evaluation scores.Therefore translation complexity is highlydependent on the complexity of the lexicon, whichis approximated by the ASW parameter.The other parameter used to compute readability?
ASL (average sentence length in words) ?
has amuch weaker influence on the quality of MT,which may be due to the fact that local context isin many cases sufficient to produce accuratetranslation and the use of the global sentencestructure in MT analysis is limited.6 Normalised evaluation scoresWe used the ASW parameter to normalise theautomated evaluation scores in order to obtainabsolute figures for MT performance, where theinfluence of translation complexity is neutralised.Normalisation requires choosing some referencepoint ?
some average level of translationcomplexity ?
to which all other scores for the sameMT system will be scaled.
We suggest using thedifficulty of the news texts in the DARPA 94 MTevaluation corpus as one such ?absolute?
referencepoint.
Normalised figures obtained on other typesof texts will mean that if the same general-purposeMT system is run on the DARPA news texts, itwill produce raw BLEU or WNM scoresapproximately equal to the normalised scores.
Thisallows users to make a fairer comparison betweenMT systems evaluated on different types of texts.We found that for the WNM scores the bestnormalisation can be achieved by multiplying thescore by the complexity normalisation coefficientC, which is the ratio:C = ASWevalText/ ASWDARPAnews.For BLEU the best normalisation is achieved bymultiplying the score by C2 (the squared value ofASWevalText/ ASWDARPAnews).Normalisation makes the evaluation relativelystable ?
in general, the scores for the same systemare the same up to the first rounded decimal point.Table 4 summarises the normalised automatedscores for the evaluated systems.C S1 S1da S2 S2dableu-wp 1.156 0.217 0.272 0.152 0.197bleu-news 1 0.283  0.19bleu-em 0.747 0.243 0.267 0.15 0.244wnmR-wp 1.075 0.349 0.414 0.297 0.341wnmR-news 1 0.364  0.344wnmR-em 0.865 0.338 0.368 0.328 0.357Table 4.
Normalised BLEU and WNM scoresThe accuracy of the normalisation can bemeasured by standard deviations of the normalisedscores across texts of different types.
We alsomeasured the improvement in stability of thenormalised scores as compared to the stability ofthe raw scores generated on different text types.Standard deviation was computed using theformula:)1()( 22?
?=   nnxxnSTDEVTable 5 summarises standard deviations of theraw and normalised automated scores for the e-mails, whitepaper and news texts.S1 S1da S2 S2da Ave-ragebleu-stdev 0.071 0.086 0.037 0.11 0.076N-bleu-stdev 0.033 0.003 0.022 0.033 0.023improved *X     3.299wnm-stdev 0.034 0.029 0.053 0.068 0.046N-wnm-stdev 0.013 0.033 0.024 0.011 0.02improved *X     2.253Table 5.
Standard deviation of BLEU and WNMIt can be seen from the table that the standarddeviation of the normalised BLEU scores acrossdifferent text types is 3.3 times smaller; and thedeviation of the normalised WNM scores is 2.25times smaller than for the corresponding rawscores.
So the normalised scores are much morestable than the raw scores across differentevaluated text types.7 Conclusion and future workIn this paper, we presented empirical evidencefor the observation that the complexity of an MTtask influences automated evaluation scores.
Weproposed a method for normalising the automatedscores by using a resource-light parameter of theaverage number of syllables per word (ASW),which relatively accurately approximates thecomplexity of the particular text type fortranslation.The fact that the potential complexity of aparticular text type for translation can beaccurately approximated by the ASW parametercan have an interesting linguistic interpretation.The relation between the length of the word andthe number of its meanings in a dictionary isgoverned by the Menzerath?s law (Koehler, 1993:49), which in its most general formulation statesthat there is a negative correlation between thelength of a language construct and the size of its?components?
(Menzerath, 1954; Hubey, 1999:239).
In this particular case the size of a word?scomponents can be interpreted as the number of itspossible word senses.
We suggest that the linkbetween ASW and translation difficulty can beexplained by the fact that the presence of longerwords with a smaller number of senses requires amore precise word sense disambiguation forshorter polysemantic words, so the task of wordsense disambiguation becomes more demanding:the choice of very specific senses and the use ofmore precise (often terminological translationequivalents) is required.Future work will involve empirical testing of thissuggestion as well as further experiments onimproving the stability of the normalised scores bydeveloping better normalisation methods.
We willevaluate the proposed approach on larger corporacontaining different genres, and will investigateother possible resource-light parameters, such astype/token ratio of the source text or unigramentropy, which can predict the complexity of thetranslated text more accurately.
Another directionof future research is comparison of stability ofevaluation scores on subsets of the evaluated datawithin one particular text type and across differenttext types.AcknowledgmentsWe are very grateful for the insightful commentsof the three anonymous reviewers.ReferencesY.
Akiba, K. Imamura and E. Sumita.
2001.
Usingmultiple edit distances to automatically rankmachine translation output.
In "Proc.
MTSummit VIII".
pages 15?20.B.
Babych.
2004.
Weighted N-gram model forevaluating Machine Translation output.
In"Proceedings of the 7th Annual Colloquium forthe UK Special Interest Group for ComputationalLinguistics".
M. Lee, ed., University ofBirmingham, 6-7 January, 2004. pages 15-22.M.
Cmejrek, J. Curin and J. Havelka.
2003.
Czech-English Dependency-based MachineTranslation.
In ?Proceedings of the 10thConference of the European Chapter ofAssociation for Computational Linguistics(EACL 2003)?.
April 12th-17th 2003, Budapest,Hungary.K.
Imamura, E. Sumita and Y. Matsumoto.
2003.Automatic Construction of Machine TranslationKnowledge Using Translation Literalness.
In?Proceedings of the 10th Conference of theEuropean Chapter of Association forComputational Linguistics (EACL 2003)?.
April12th-17th 2003, Budapest, Hungary.M.
Hubey.
1999.
Mathematical Foundations ofLinguistics.
Lincom Europa, Muenchen.R.
Koehler.
1993.
Synergetic Linguistics.
In"Contributions to Quantitative Linguistics",R. Koehler and B.B.
Rieger (eds.
), pages 41-51.P.
Menzerath.
1954.
Die Architektonik desdeutchen Wortschatzes.
Dummler, Bonn.R.
Mitkov.
2002.
Anaphora Resolution.
Longman,Harlow, UK.K.
Papineni, S. Roukos, T. Ward, W-J Zhu.
2002BLEU: a method for automatic evaluation of ma-chine translation.
In "Proceedings of the 40thAnnual Meeting of the Association for theComputational Linguistics (ACL)", Philadelphia,July 2002, pages 311-318.M.
Rajman and T. Hartley.
2001.
Automaticallypredicting MT systems ranking compatible withFluency, Adequacy and Informativeness scores.In "Proceedings of the 4th ISLE Workshop onMT Evaluation, MT Summit VIII".
Santiago deCompostela, September 2001. pages.
29-34.J.
White, T. O?Connell and F. O?Mara.
1994.
TheARPA MT evaluation methodologies: evolution,lessons and future approaches.
Procs.
1stConference of the Association for MachineTranslation in the Americas.
Columbia, MD,October 1994.
193-205.
