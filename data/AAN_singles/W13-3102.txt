Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 13?19,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsMulti-document multilingual summarization corpus preparation, Part 2:Czech, Hebrew and SpanishMichael ElhadadBen-Gurion Univ.in the Negev, Israelelhadad@cs.bgu.ac.ilSabino Miranda-Jim?nezInstituto Polit?cnicoNacional, Mexicosabino_m@hotmail.comJosef SteinbergerUniv.
ofWest Bohemia,Czech Republicjstein@kiv.zcu.czGeorge GiannakopoulosNCSR Demokritos, GreeceSciFY NPC, Greeceggianna@iit.demokritos.grAbstractThis document overviews the strategy, ef-fort and aftermath of the MultiLing 2013multilingual summarization data collec-tion.
We describe how the Data Contrib-utors of MultiLing collected and gener-ated a multilingual multi-document sum-marization corpus on 10 different lan-guages: Arabic, Chinese, Czech, English,French, Greek, Hebrew, Hindi, Romanianand Spanish.
We discuss the rationale be-hind the main decisions of the collection,the methodology used to generate the mul-tilingual corpus, as well as challenges andproblems faced per language.
This paperoverviews the work on Czech, Hebrew andSpanish languages.1 IntroductionIn this document we present the language-specific problems and challenges faced by Con-tributors during the corpus creation process.
Tofacilitate the reader we repeat some informationfound in the first part of the overview (Li et al2013): the MultiLing tasks and the main steps ofthe corpus creation process.2 The MultiLing tasksThere are two main tasks (and a single-document multilingual summarization pilot de-scribed in a separate paper) in MultiLing 2013:Summarization Task This MultiLing task aimsto evaluate the application of (partially orfully) language-independent summarizationalgorithms on a variety of languages.
Eachsystem participating in the task was calledto provide summaries for a range of differ-ent languages, based on corresponding cor-pora.
In the MultiLing Pilot of 2011 the lan-guages used were 7, while this year systemswere called to summarize texts in 10 differ-ent languages: Arabic, Chinese, Czech, En-glish, French, Greek, Hebrew, Hindi, Roma-nian, Spanish.
Participating systems were re-quired to apply their methods to a minimumof two languages.The task was aiming at the real problem ofsummarizing news topics, parts of which maybe described or may happen in different mo-ments in time.
We consider, similarly to Mul-tiLing 2011(Giannakopoulos et al 2011) thatnews topics can be seen as event sequences:Definition 1 An event sequence is a set ofatomic (self-sufficient) event descriptions, se-quenced in time, that share main actors, lo-cation of occurence or some other importantfactor.
Event sequences may refer to topicssuch as a natural disaster, a crime investiga-tion, a set of negotiations focused on a singlepolitical issue, a sports event.The summarization task requires to generatea single, fluent, representative summary froma set of documents describing an event se-quence.
The language of the document setwill be within the given range of 10 languagesand all documents in a set share the same lan-guage.
The output summary should be of thesame language as its source documents.
Theoutput summary should be between 240 and250 words.Evaluation Task This task aims to examine howwell automated systems can evaluate sum-maries from different languages.
This tasktakes as input the summaries generated fromautomatic systems and humans in the Sum-marization Task.
The output should be a grad-ing of the summaries.
Ideally, we would wantthe automatic evaluation to maximally corre-late to human judgement.13The first task was aiming at the real problem ofsummarizing news topics, parts of which may bedescribed or happen in different moments in time.The implications of including multiple aspects ofthe same event, as well as time relations at a vary-ing level (from consequtive days to years), are stilldifficult to tackle in a summarization context.
Fur-thermore, the requirement for multilingual appli-cability of the methods, further accentuates the dif-ficulty of the task.The second task, summarization evaluation hascome to be a prominent research problem, based onthe difficulty of the summary evaluation process.While commonly used methods build upon a fewhuman summaries to be able to judge automaticsummaries (e.g., (Lin, 2004; Hovy et al 2005)),there also exist works on fully automatic evalua-tion of summaries, without human?model?
sum-maries (Louis and Nenkova, 2012; Saggion et al2010).
The Text Analysis Conference has a sepa-rate track, named AESOP (Dang and Owczarzak,2009) aiming to test and evaluate different auto-matic evaluation methods of summarization sys-tems.Given the tasks, a corpus needed to be gener-ated, that would be able to:?
provide input texts in different languages tosummarization systems.?
provide model summaries in different lan-guages as gold standard summaries, to alsoallow for automatic evaluation using model-dependent methods.?
provide human grades to automatic and hu-man summaries in different languages, tosupport the testing of summary evaluationsystems.In the following section we show how these re-quirements were met in MultiLing 2013.3 Corpus collection and generationThe overall process of creating the corpus ofMultiLing 2013 was, similarly to MultiLing 2011,based on a community effort.
The main processesconsisting the generation of the corpus are as fol-lows:?
Selection of a source corpus in a single lan-guage.?
Translation of the source corpus to differentlanguages.?
Human summarization of corpus topics perlanguage.?
Evaluation of human summaries, as well as ofsubmitted system runs.4 Language specific notesIn the following paragraphs we providelanguage-specific overviews related to the corpuscontribution effort.
The aim of these overviews isto provide a reusable pool of knowledge for futuresimilar efforts.In this document we elaborate on Czech, He-brew, and Spanish languages.
A second document(Elhadad et al 2013) elaborates on the rest of thelanguages.4.1 Czech languageThe first part of the Czech subcorpus (10 top-ics) was created for the multilingual pilot task atTAC 2011.
Five new topics were added for Mul-tiling 2013.
In total, 14 annotators participated inthe Czech corpus creation.The most time consuming part of the annota-tion work was the translation of the articles.
Theannotators were not professional translators andmany topics required domain knowledge for cor-rect translation.
To be able to translate a per-son name, the translator needs to know its correctspelling in Czech, which is usually different fromEnglish.
The gender also plays an important rolein the translation, because a suffix ?ov??
must beadded to female surnames.Translation of organisation names or person?sfunctions within an organisation needs some do-main knowledge as well.
Complicated morphol-ogy and word order in Czech (more free but some-times very different fromEnglish) makes the trans-lation even more difficult.For the creation of model summaries the anno-tator needed to analyse the topic well in order todecide what is important and what is redundant.Sometimes, it was very difficult, mainly in thecase of topics which covered a long period (even5 years) and which contained articles sharing verylittle information.The main question of the evaluation part washow to evaluate a summary which contains a read-able, continuous text ?
mainly the case of the14Group SysID Avg Perfa B 4.75a A 4.63ab C 4.61b D 4.21b E 4.10Table 1: Czech: Tukey?s HSD test groups for hu-man summarizersbaseline system with ID6) ?
however not impor-tant information from the article cluster point ofview.An overview of the Overall Responsiveness andthe corresponding average grades of the humansummarizers can be seen in Table 1.
We notethat on average the human summaries are consid-ered excellent (graded above 4 out of 5), but thatthere exist statistically significant differences be-tween summarizers, essentially forming two dis-tinct groups.4.2 Hebrew languageThis section describes the process of preparingthe dataset for MultiLing 2013 in Hebrew: transla-tion of source texts from English, and the summa-rization for the translated texts, by the Ben GurionUniversity Natural Language Processing team.4.2.1 Translation ProcessFour people participated in the translation andthe summarization of the dataset of the 50 newsarticles: three graduate students, one a native En-glish speaker with fluent Hebrew and the other twowith Hebrew as a mother tongue and very goodEnglish skills.
The process was supervised by aprofessional translator with a doctoral degree withexperience in translation and scientific editing.The average times to read an article was 2.5min-utes (std.
dev 1.2min), the average translation timewas 30 minutes (std.
dev 15min), and the averageproofing time was 18.5min (std.
dev 10.5min).4.2.2 Translation MethodologyWe tested two translation methodologies by dif-ferent translators.
In some of the cases, translationwas aided with Google Translate1, while in othercases, translation was performed from scratch.In the cases where texts were first translatedusing Google Translate, the translator reviewed1See http://translate.google.com/.the text and edited changes according to her judg-ment.
Relying on the time that was reported for theproofreading of each translation, we could tell thattexts that were translated using this method, re-quired longer periods of proofreading (and some-times more time was required to proofread than totranslate).
This is most likely because once the au-tomatic translation was available, the human trans-lator was biased by the automatic outcome, re-maining anchored?
to the given text with reducedcriticism and creativity.Translating the text manually, aided with onlineor offline dictionaries, Wikipedia and news site onthe subject that was translated, showed better qual-ity as analysis of time shows, where the ratio be-tween the time needed to proofread was less thanhalf.In addition, we found, that inmost cases the timethat the translation took for the first texts of a givensubject (for each article cluster), tends to be signif-icantly longer than the subsequent articles in thesame cluster.
This reflects the ?learning phase?
ex-perienced by the translators who approached eachcluster, getting to know the vocabulary of eachsubject.4.2.3 Topic ClustersThe text collection includes five clusters of tenarticles each.
Some of the topics were very famil-iar to the Hebrew-speaking readers, and some sub-jects were less familiar or relevant.
The IranianNuclear issue is very common in the local newsand terminology is well known.
Moreover, it waspossible to track the articles from the news as theywere published in Hebrew news websites at thattime; this was important for the usage of actualand correct news-wise terminology.
The hardestbatch to translate was on the Paralympics champi-onship, which had no publicity in Hebrew, and theterminology of winter sports is culturally foreignto native Hebrew speakers.4.2.4 Special Issues in HebrewA couple of issues have surfaced during thetranslation and should be noted.
Many words inHebrew have a foreign transliterated usage and anoriginal Hebrew word as well.
For instance, theLatin word Atomic is very common in Hebrewand, therefore, it will be equally acceptable to useit in the Hebrew form, ?????
/ ?atomi?but alsothe Hebrew word ??????
(?gar?
ini?
/ nuclear).Traditional HebrewNews Agencies have for many15Summarizer Reading time SummarizationA 43 min 49 minB 22 min 84 minC 35 min 62 minTable 2: Summarization process times (averaged)years adopted an editorial line which strongly en-courages using original Hebrew words wheneverpossible.
In recent years, however, this approachis relaxed, and both registers are equally accepted.We have tried to use a ?common notion?
in all textsusing the way terms are written inWikipedia as thevoice of majority.
In most cases, this meant usingmany transliterations.Another issue in Hebrew concerns the orthog-raphy variations of plene vs. deficient spelling.Since Hebrew can be written with or without vo-calization, words may be written with variations.For instance, the vocalized version of the word?air?
is ??????
(?avir? )
while the non-vocalizedversion is ?????
(?avvir?).
The rules of spellingrelated to these variations are complicated and arenot common knowledge.
Even educated peoplewrite words with high variability, and in manycases, usage is skewed by the rules embedded inthe Microsoft Word editor.
We did not make anyspecific effort to enforce standard spelling in thedataset.4.2.5 Summarization ProcessEach cluster of articles was summarized by threepersons, and each summary was proof-read by theother summarizers.
Most of the summarizers readthe texts before summarization, while translatingor proofreading them, and, therefore, the time thatwas required to read all texts was reduced.The time spent reading and summarizing wasextremely different for each of the three summa-rizers, reflecting widely different summarizationstrategies, as indicated in the Table 2 (averagetimes over the 5 new clusters of MultiLing 2013):The trend indicates that investing more time upfront reading the clusters pays off later in summa-rization time.The instructions did not explicitly recommendabstractive vs. extractive summarization.
Twosummarizers applied abstractive methods, onetended to use mostly extractive (C).
The extractivemethod did not take markedly less time than theabstractive one.
In the evaluation, the extractiveGroup SysID Avg Perfa A 4.80ab B 4.40b C 4.13Table 3: Hebrew: Tukey?s HSD test groups for hu-man summarizerssummary was found markedly less fluent.As the best technique to summarize efficiently,all summarizers found that ordering the texts bydate of publication was the best way to conduct thesummaries in the most fluent manner.However, it was not completely a linear process,since it was often found that general information,which should be located at the beginning of thesummary as background information, appeared ina later text.
In such cases, summarizers changedtheir usual strategy and consciously moved infor-mation from a later text to the beginning of thesummary.
This was felt as a distinct deviation ?as the dominant strategy was to keep track of thestory told across the chronology of the cluster, andto only add new and important information to thesummary that was collected so far.The most difficult subject to summarize wasthe set on Paralympic winter sports championshipwhich was a collection of anecdotal descriptionswhich were not necessarily a developing or a se-quential story and had no natural coherence as acluster.4.2.6 Human evaluationThe results of human evaluation over the humansummarizers are provided in Table 3.
It is inter-esting to note that even between humans there ex-ist two groups with statistically significant differ-ences in their grades.
On the other hand, the hu-man grades are high enough to show high qualitysummaries (over 4 on a 5 point scale).4.3 Spanish languageThirty undergraduate students, from NationalInstitute Polytechnic and Autonomous Universityof the State of Mexico, were involved in creatingof Spanish corpus for MultiLing 2013.The Spanish corpus built upon the Text Analy-sis Conference (TAC) MultiLing Corpus of 2011.The source documents were news fromWikiNewswebsite, in English language.
The source corpusfor translating consisted of 15 topics and 10 docu-ments per topic.
In the following paragraphs, we16show the measured times for each stage and prob-lems that people had to face during the generationof corpus that includes translation of documents,multi-document summarization, and evaluation ofhuman (manual) summaries.At the translation step, people had to translatesentence by sentence or paraphrase a sentence upto completing the whole document.
When a docu-ment was translated, it was sent to another personto verify the quality of the translated document.The effort was measured by three different timemeasurements: reading time, translation time, andverification time.The reading average at document level was 7.6minutes (with a standard deviation of 3.4 minutes),the average translation of each document was 19.2minutes (with a standard deviation of 7.8 min-utes), and the average verification was 14.9 min-utes (with a standard deviation of 7.7 minutes).The translation stage took 104.5 man-hours.At summarization step, people had to read thewhole set of translated documents (topic) and cre-ate a summary per each set of documents.
Thelength of a summary is between 240 and 250words.
Three summaries were created for eachtopic.
Also, reading time of the topic and time ofwriting the summary were measured.The average reading of a set of documents was31.6 minutes (with a standard deviation of 10.2minutes), and the average time to generate a sum-mary was 27.7 minutes (with a standard deviationof 6.5 minutes).
This stage took 44.5 man-hours.At evaluation step, people had to read the wholeset of translated documents and assess its corre-sponding summary.
The summary quality wasevaluated.
Three evaluations were done for eachsummary.
The human judges assessed the overallresponsiveness of the summary based on coveringall important aspects of the document set, fluentand readable language.
The human summary qual-ity average was 3.8 (on a scale 1 to 5) (with a stan-dard deviation of 0.81).
The results are detailed inTable 4.
It is interesting to note that all humanshave no statistically significant differences in theirgrades.
On the other hand, the human grades arenot excellent on average (i.e.
exceeding 4 out of 5)which shows that the evaluators considered humansummaries non-optimal.Group SysID Avg Perfa C 3.867a B 3.778a A 3.667Table 4: Spanish: Tukey?s HSD test groups for hu-man summarizers4.3.1 Problems during Generation of SpanishCorpusDuring the translation step, translators had toface problems related to proper names, acronyms,abbreviations, and specific themes.
For instance,the proper name?United States?can be depictedwith different Spanish words such as ?EE.
UU.
?2,?Estados Unidos?, and?EUA?
?all of themare valid words.
Even though translators knowall the correct translations, they decided to use thefrequent terms in a context of news (the first twoterms are frequently used).In relation to acronyms, well-known acronymswere translated into equivalent well-known (or fre-quent) Spanish translations such as UN (UnitedNations) became into ONU (Organizaci?n de lasNaciones Unidas), or they were kept in the sourcelanguage, because they are frequently used inSpanish, for example, UNICEF, BBC, AP (thenews agency, Associated Press), etc.On the contrary, for not well-known acronymsof agencies, monitoring centers, etc., translatorslooked for the common translation of the propername on Spanish news websites in order to cre-ate the acronym based on the name.
Other trans-lators chose to translate the proper name, but theykept the acronym from the source document besidethe translated name.
In cases where acronyms ap-peared alone, they kept the acronym from sourcelanguage.
It is a serious problem because a set oftranslated documents has a mix of acronyms.Abbreviations were mainly faced with rankssuch as lieutenant (Lt.), Colonel (Col.), etc.
Trans-lators used an equivalent rank in Spanish.
For in-stance, lieutenant (Lt.) is translated into?teniente(Tte.)?
; however, translators preferred to use thecomplete word rather than the abbreviation.In case of specific topics, translators used Span-ish websites related to the topic in order to knowthe particular vocabulary and to decide what (tech-2The double E and double U indicate that the letter rep-resents a plural: e.g.
EE.
may stand for Asuntos Exteriores(Foreign Affairs).17nical) words should be translated and how theyshould be expressed.As regards at text summarization step, sum-marizers dealt with how to organize the sum-mary because there were ten documents per topic,and all documents involved dates.
Two strategieswere employed to solve the problem: generatingthe summary according to representative dates, orstarting the summary based on a particular date.In the first case, summarizers took the chainof events and wrote the summary considering thedates of events.
They gathered important eventsand put together under one date, typically, the lat-est date according to a part of the chain of events.They grouped all events in several dates; thus, thesummary is a sequence of dates that gather events.However, the dates are chosen arbitrary accordingto the summarizers.In the second case, summarizers started the sum-mary based on a specific date, and continued writ-ing the sequence of important events.
The se-quence of events represents the temporality start-ing from a specific point of time (usually, thefirst date in the set of documents).
Finally, inmost cases, evaluators think that human sum-maries meet the requirements of covering all im-portant aspects of the document set, fluent andreadable language.5 Conclusions and lessons learntThe findings from the languages presented inthis paper appear to second the claims found in therest of the languages (Li et al 2013):?
Translation is a non-trivial process, often re-quiring expert know-how to be performed.?
The distribution of time in summarization cansignificantly vary among human summariz-ers: it essentially sketches different strate-gies of summarization.
It would be interest-ing to follow different strategies and recordtheir effectiveness in the multilingual setting,similarly to previous works on human-stylesummarization (Endres-Niggemeyer, 2000;Endres-Niggemeyer and Wansorra, 2004).Our find may be related to the (implied) ef-fort of taking notes while reading, which canbe a difficult cognitive process (Piolat et al2005).?
The time aspect is important when generat-ing a summary.
The exact use of time (a sim-ple timeline?
a grouping of events based ontime?)
is apparently arbitrary.We remind the reader that extended technical re-ports recapitulating discussions and findings fromthe MultiLingWorkshop will be available after theworkshop at the MultiLing Community website3,as an addenum to the proceedings.What can definitely be derived from all the ef-fort and discussion related to the gathering of sum-marization corpora is that it is a research challengein itself.
If the future we plan to broaden the scopeof the MultiLing effort, integrating all the findingsin tools that will support the whole process and al-low quantifying the apparent problems in the dif-ferent stages of corpus creation.
We have also beenconsidering to generate comparable corpora (e.g.,see (Saggion and Szasz, 2012)) for future Multi-Ling efforts.
We examine this course of actionto avoid the significant overhead by the transla-tion process required for parallel corpus genera-tion.
We should note here that so far we have beenusing parallel corpora to:?
allow for secondary studies, related to thehuman summarization effort in different lan-guages.
Having a parallel corpus is such casescan prove critical, in that it provides a com-mon working base.?
be able to study topic-related or domain-related summarization difficulty across lan-guages.?
highlight language-specific problems (suchas ambiguity in word meaning, named entityrepresentation across languages).?
fixes the setting in which methods can showtheir cross-language applicability.
Exam-ining significantly varying results in differ-ent languages over a parallel corpus offerssome background on how to improve exist-ing methods and may highlight the need forlanguage-specific resources.On the other hand, the significant organizationaland implementaion effort required for the transla-tion may turn the balance towards comparable cor-pora for future MultiLing endeavours.3See http://multiling.iit.demokritos.gr/pages/view/1256/proceedings-addenum)18AcknowledgmentsMultiLing is a community effort and this com-munity is what keeps it alive and interesting.
Wewould like to thank contributors for their organi-zational effort, which made MultiLing possible inso many languages and all volunteers, helpers andresearchers that helped realize individual steps ofthe process.
A more detailed reference of the con-tributor teams can be found in the Appendix.The MultiLing 2013 organization has been par-tially supported by the NOMAD FP7 EU Project(cf.
http://www.nomad-project.eu).References[Dang and Owczarzak2009] Hoa Trang Dang andK.
Owczarzak.
2009.
Overview of the tac 2009summarization track, Nov.[Elhadad et al013] Michael Elhadad, SabinoMiranda-Jim?nez, Josef Steinberger, and GeorgeGiannakopoulos.
2013.
Multi-document multi-lingual summarization corpus preparation, part 2:Czech, hebrew and spanish.
In MultiLing 2013Workshop in ACL 2013, Sofia, Bulgaria, August.
[Endres-Niggemeyer and Wansorra2004] BrigitteEndres-Niggemeyer and Elisabeth Wansorra.
2004.Making cognitive summarization agents work ina real-world domain.
In Proceedings of NLUCSWorkshop, pages 86?96.
Citeseer.
[Endres-Niggemeyer2000] Brigitte Endres-Niggemeyer.
2000.
Human-style WWW sum-marization.
Technical report.
[Giannakopoulos et al011] G. Giannakopoulos,M.
El-Haj, B. Favre, M. Litvak, J. Steinberger,and V. Varma.
2011.
TAC 2011 MultiLing pilotoverview.
In TAC 2011 Workshop, Maryland MD,USA, November.
[Hovy et al005] E. Hovy, C. Y. Lin, L. Zhou, andJ.
Fukumoto.
2005.
Basic elements.
[Li et al013] Lei Li, Corina Forascu, Mahmoud El-Haj, and George Giannakopoulos.
2013.
Multi-document multilingual summarization corpus prepa-ration, part 1: Arabic, english, greek, chinese, ro-manian.
In MultiLing 2013 Workshop in ACL 2013,Sofia, Bulgaria, August.
[Lin2004] C. Y. Lin.
2004.
Rouge: A package forautomatic evaluation of summaries.
Proceedings ofthe Workshop on Text Summarization Branches Out(WAS 2004), pages 25?26.
[Louis and Nenkova2012] Annie Louis and AniNenkova.
2012.
Automatically assessing ma-chine summary content without a gold standard.Computational Linguistics, 39(2):267?300, Aug.[Piolat et al005] Annie Piolat, Thierry Olive, andRonald T Kellogg.
2005.
Cognitive effort dur-ing note taking.
Applied Cognitive Psychology,19(3):291?312.
[Saggion and Szasz2012] Horacio Saggion and SandraSzasz.
2012.
The concisus corpus of event sum-maries.
In LREC, pages 2031?2037.
[Saggion et al010] H. Saggion, J. M. Torres-Moreno,I.
Cunha, and E. SanJuan.
2010.
Multilingual sum-marization evaluation without human models.
InProceedings of the 23rd International Conferenceon Computational Linguistics: Posters, page 1059?1067.Appendix: Contributor teamsCzech language teamTeam members Brychc?n Tom?
?, Campr Michal,Fiala Dalibor, Habernal Ivan, Habernalov?Anna, Je?ek Karel, Konkol Michal, Konop?kMiloslav, Kr?m??
Lubom?r, Nejezchlebov?Pavla, Pelechov?
Blanka, Pt?
?ek Tom?
?,Steinberger Josef, Z?ma Martin.Team affiliation University of West Bohemia,Czech RepublicContact e-mail jstein@kiv.zcu.czHebrew language teamTeam members Tal Baumel, Raphael Cohen,Michael Elhadad, Sagit Fried, Avi Hayoun,Yael NetzerTeam affiliation Computer Science Dept.
Ben-Gurion University in the Negev, IsraelContact e-mail elhadad@cs.bgu.ac.ilSpanish language teamTeam members Sabino Miranda-Jim?nez, Grig-ori Sidorov, Alexander Gelbukh (NaturalLanguage and Text Processing Laboratory,Center for Computing Research, National In-stitute Polytechnic, Mexico City, Mexico)Obdulia Pichardo-Lagunas (InterdisciplinaryProfessional Unit on Engineering and Ad-vanced Technologies (UPIITA), National In-stitute Polytechnic, Mexico City, Mexico)Contact e-mail sabino_m@hotmail.com19
