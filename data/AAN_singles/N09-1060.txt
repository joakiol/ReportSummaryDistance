Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 530?538,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSemantic-based Estimation of Term InformativenessKirill KireyevUniversity of Colorado ?
Boulderkireyev@colorado.eduAbstractThe idea that some words carry more semanticcontent  than  others,  has  led  to  the  notion  ofterm specificity,  or informativeness.
Computa-tional  estimation of  this  quantity  is  importantfor various applications such as information re-trieval.
We propose a new method of comput-ing term specificity, based on modeling the rateof learning of word meaning in Latent SemanticAnalysis  (LSA).
We analyze  the performanceof this method both qualitatively and quantitat-ively and  demonstrate  that  it  shows excellentperformance compared to existing methods ona  broad  range  of  tests.
We  also  demonstratehow it can be used to improve existing applica-tions  in  information  retrieval  and  summariza-tion.1 IntroductionThe idea that some words carry more semanticcontent than others has been occurring in variousliterature in linguistics, psychology and computerscience for some time.
The intuitive notion of spe-cificity has long existed before it was formalized;consider, for example, the distinction between themore general word ?beverage?
and more specificterms   ?tea?, ?coffee?
and ?cocoa?
made by  Sp?r-ck-Jones (1973).
Another informal mention of spe-cificity is mentioned by Gorman (1961):A word may be ?abstract?
and either generalor specific, or ?concrete?
and either general orspecific.where it  is contrasted with another psycholinguisticproperty of concreteness, which is generally definedas ?the  extent to which the word's referent can betouched or felt?
(Reilly et al, 2007).The field of information retrieval has attractedgreater  attention  to  the  computational  estimationand  applications  of  term specificity.
It  has  beennoted that words with higher specificity, or inform-ation content, deserve to be weighted more heavilywhen  matching  documents  with  queries,  sincethese words play a greater importance in character-izing what a query or a document is about.
By con-trast,  stopwords,  words  that  contribute  the  leastamount  of  semantic  content,  are  often  down-weighted  or  removed  altogether  (see  (Lo  et  al.,2005), for example).In addition to IR, term specificity, or informat-iveness,  has been shown useful  in other applica-tions,  such as  Named  Entity Tagging (Rennie  etal.,  2005),  creating  back-of-the-book  glossaries(Csomai  et al,  2007),  and extractive summariza-tion (Kireyev, 2008).A related notion of  communication density  hasbeen introduced by Gorman et al (2003) in teamcommunication analysis, to measure the extent towhich  a  team conveys  information  in  a  concisemanner, or, in other words, the rate of meaningfuldiscourse, defined by the ratio of  meaningfulnessto number  of  words spoken.
The  meaningfulnessdescribed here should not  be confused with psy-cholinguistic  quality  of  meaningfulness  as  de-scribed by Toglia and Battig (1978), which is thedegree to  which a  word is  associated with otherwords.In this paper we consider the terms  specificity,informativeness and  information content of wordsto mean the same thing.
A precise formulation oranalysis of important qualitative characteristics ofthese concepts has not been performed in previousliterature; we hope to make some progress in thatdirection in this paper.Our main goal is to introduce a new method ofcomputing word specificity based on the rate andstrength of semantic associations between words,as modeled by Latent Semantic Analysis (LSA).2 Previous ApproachesTo date, most of the known approaches to estim-ating  term  informativeness  have  relied  on  fre-quency-based methods.530A very basic, yet surprisingly effective approachto measuring term informativeness is its frequencyof occurrence in a large representative corpus oflanguage.
Sp?rck Jones (1973) defines  IDF or  in-verse document frequency, which is determined bythe  probability  of  occurrence  of  documents  con-taining a particular word:IDF ?w ?=?log2?df w /D?where  D is the total number of documents in thecorpus.
The assumption behind it is that low fre-quency words tend to be rich in content, and viceversa.Church and Gale (1995) correctly note that thismeasure is fundamentally different from collectionfrequency fw,  (the total number of times the wordtype occurs in the corpus) or  its transformations,despite  the  fact  that  the  two  measures  appearhighly correlated.
In fact, what is particularly of in-terest are the words for which these two quantitiesdeviate the most.
This happens most dramaticallyfor  most  informative,  or  content  words,  such  as?boycott?
(Church, 1995a).
These words happen toexhibit ?bursty?
behavior, where they tend to ap-pear multiple times but in fewer documents,  thushaving  fw  > dfw.
In  contrast,  less  content-loadedwords like ?somewhat?
tend to occur on averageonce in documents,  and thus have similar  valuesfor collection and document frequencies ( fw ?
dfw ).As a result, more informative words can be less ac-curately  estimated  by  the  Poisson  distribution,which is  based on the simplistic assumption thatthe expected number of occurrences of word in adocument can be estimated by its total number ofoccurrences in the corpus.Most prominent statistical measures of term in-formativeness  rely  on  quantifying  this  notion  ofdeviation  from  the  Poisson  distribution.
If  themean expected word rate is:?tw= f wDthen the variance metric can be defined as:variance ?w?= 1D?1?d=1D?
tdw?
?t w?2where  tdw is  the actual  number  of  occurrences ofterm  w in document  d.  The idea is that  a highervariance would indicate greater deviation from ex-pected  frequency  of  occurrence  in  a  document,which  is  assumed  to  be  higher  for  informativewords.Another  measure,  suggested by  Church  andGale (1995a) is burstiness which attempts to com-pare collection frequency and document frequencydirectly:burstiness ?w?= ?twdf w /D=f wdf wChurch and Gale also noted that nearly all wordshave IDF scores that are larger than what onewould expect according to an independence-basedmodel such as the Poisson.
They note that interest-ing or informative words tend to have the largestdeviations from what would be expected.
Theythus introduce the notion of residual IDF whichmeasures exactly this deviation:residualIDF ?w?= IDF ?w ?
?log2 ?1?e?
?t ?Papineni (2001) introduces the notion of gain:gain ?w?= df wD ?
df wD ?1?log ?
df wD ?
?This  measure  tends  to  give low weights  to  veryhigh- and very low- frequency words.Most closely related to our work is the notion ofmeaningfulness in (Gorman et al2003), computedas the LSA vector length.
We will discuss it furtherin the subsequent sections, and show that a smallbut crucial modification to this quantity gives thebest results.3 Using Latent Semantic Analysis for Ap-proximating Term Informativeness3.1 Latent Semantic AnalysisLatent Semantic Analysis  (LSA) is a languagemodel  that  represents  semantic  word meaning asvectors  in  high-dimensional  space.
Word  vectorsare positioned in such a way that semantically-re-lated words vectors point in similar directions orhave a smaller angle / higher cosine between them.The representation is  derived in  an unsupervisedmanner, by observing occurrence patterns of wordsin a large corpus of natural language documents.Singular  Value  Decomposition  on  the  matrix  ofword/document occurrence counts is used to derivethe optimal set of dimensions of the space in which531all of the words can be represented as vectors.
Thenumber of dimensions is then artificially reducedto a smaller number (typically around 300) of mostimportant  dimensions,  which  has  the  effect  ofsmoothing  out  incidental  relationships  and  pre-serving significant ones between words.The  resulting  geometric  space  allows  forstraightforward  representation  of  meaning  ofwords  and/or  documents;  the  latter  are  simply  aweighted  geometric  composition  of  constituentword vectors.
Similarity in meaning between a pairof words or documents can be obtained by comput-ing the cosine between their corresponding vectors.For  details  of  LSA,  please  see  (Landauer  et  al.,2007), and others3.2 LSA Term Vector LengthMost of the LSA applications focus on compar-ing semantic similarity between words and/or text,using the cosine measure of the angle between thecorresponding vectors.
There is, however, anothersignificant characteristic of LSA word vectors be-sides  their  direction  in  space;  it  is  their  vectorlength.
The vector length for words differs signi-ficantly, as is shown in Table 1.Word dfw Vector Lengthdog 1365 1.3144green 2067 0.7125run 2721 0.4788puppy 127 0.2648electron 264 0.9009the 44474 0.0098Table 1: LSA vector length for some of the words inTASA corpus.The vector length plays a very important role inmany LSA calculations,  in particular  ?
in givingrelative weights to the word vectors that constitutea particular text passage.What causes differences in vector lengths?
Theyare based roughly on how much information LSAlearns about a word based on its patterns of occur-rence in the corpus.
Kintsch (2001) writes:Intuitively, the vector length tells us how much in-formation LSA has about this vector.
[...] Words thatLSA  knows  a  lot  about  (because  they  appear  fre-quently in the training corpus[...]) have greater vectorlengths than words LSA does not know well.
Func-tion words that are used frequently in many differentcontexts have low vector lengths -- LSA knows noth-ing about them and cannot tell them apart since theyappear in all contexts.Essentially, there are two factors that affect vec-tor length: (1) number of occurrences and (2) theconsistency of contexts in which the word occurs.3.3 Deriving Specificity from Vector LengthBased on the observations above we propose anew metric of term informativeness, or specificity,which we call  LSAspec, which is simply the ratioof LSA vector length to the number of documentsin the LSA training corpus that contain a particularword:LSAspec ?w?=?
?w?/df wThe value can be interpreted as the rate of vectorlength growth.
We argue that more specific, or in-formative, words have the greatest rate of  vectorlength  growth;  LSA  learns  about  their  meaningfaster,  with  relatively  fewer  exposures.
To  illus-trate this concept, let's look at a few examples, thatwere obtained by controlling the number of occur-rences of a particular word in the LSA training cor-pus.
The  base  corpus  was  obtained  using  the44000-passage  TASA  corpus  with  all  passagescontaining  the  three  words  below  initially  re-moved.
Each data point on the graph reflects thevector  length  of  a  particular  word,  after  trainingLSA on the base corpus plus the specified numberof  passages  containing  a  particular  word  addedback.
Highly specific words like ?cellulose?
gainvector length quite quickly compared to a low-spe-cificity word like ?dismay?.4 Comparison of Specificity MetricsPast attempts to examine the merits of variousexisting term informativeness estimation methodsin the literature thus far has largely involved em-Illustration 1: Vector lengths for some words vs thenumber of documents containing those words.10 20 30 40 500.000.010.020.030.040.05Vector Lengthcelluloseclassmatedismay# Occurences532pirical summative evaluations as part of informa-tion  retrieval  or  named  entity  tagging  systems(Rennie  et  al.,  2005).
Here,  we  provide  somemeasures which hopefully provide more illuminat-ing insights into the various methods.In all of the tests below we derived the metrics(including the LSA space for  LSAspec) from thesame  corpus  ?
MetaMetrics  2002  corpus,  com-posed of ~188k passages mostly used in education-al texts.
No stemming or stopword removal of anykind was performed.
All word types were conver-ted  to  lowercase.
We  computed  the  specificityscore for each of the 174,374 most frequent wordsin the corpus using each of the metrics describedabove:  LSAspec,  IDF,  residualIDF,  burstiness,gain and variance.4.1 Correlation with Number of SensesIntuitively,  one  would  expect  more  specificwords  to have more  precise  meaning,  and there-fore, generally fewer senses.
For example, ?elec-tron?
is a specific physics term that has only onesense, whereas ?run?
has a very general meaning,and thus has over 50 senses in the WordNet data-base (Miller et al, 1990).
There are many excep-tions to this, of course, but overall, one would ex-pect a negative correlation between specificity andnumber of senses.In this test, we measure the correlation betweenthe specificity score of a word by various methodsand its number of senses in WordNet version 3.0.A total of 75,978 words were considered.
We useSpearman  correlation  coefficient,  since  the  rela-tionships are likely to be non-linear.Metric Corr Metric CorrLSAspec -0.46 burstiness -0.02IDF -0.44 variance 0.40residualIDF -0.03 gain 0.44Table 2: Correlation of specificity metrics with numberof senses in WordNetLSAspec gives the highest  negative correlationwith number of WordNet senses.4.2 Correlation with HypernymyWordNet organizes concepts into a hypernymytree, where each parent node is a hypernym of thechild node below it.
For example:substanceelementmetalnickel copperIn general one would expect that for each pair ofchild-parent pairs in the hypernym tree, the childwill  have greater specificity than the parent1.
Weexamined of  a total  of  14451 of  such hypernymword pairs and computed how often the child's in-formativeness  score,  according  to  each  of  themeasures,  is  greater  than  its  parent's  (its  hyper-nym's) score.Metric Percent Metric PercentIDF 88.8% burstiness 47.2%LSAspec 87.7% variance 13.4%residualIDF 48.8% gain 11.1%Table 3: Percentage of the time specificity of child ex-ceeds that of its hypernym in WordNet4.3 Writing Styles and LevelsOne may expect that the specificity of words onaverage would change with texts that are known tobe of different writing styles and difficulty level.To test this hypotheses we extracted texts from theTASA  collection  of  educational  materials.
Thetexts are annotated with genre (?Science?, ?SocialStudies?
or ?Language Arts?
), and difficulty levelon the DRP readability scale (Koslin et al, 1987).Intuitively,  one would expect to see two patternsamong these texts:(1) The specificity of words would generally in-crease with increasing level of difficulty of texts.
(2)  Informative  (Science)  texts  should  havemore specific terms than narrative (Language Arts)texts;  with Social  Studies somewhere in between(McCarthy et al, 2006).We extracted 100 text  passages for  each com-bination  of  style  (?Science?,  ?Social  Studies?,?Language Arts?)
and DRP difficulty level (50, 55,60,  65,  70)2,  thus resulting in 15 batches of  100passages.
For each passage we computed the medi-an specificity measure of each unique word type in1 In practice this is more difficult to determine, since some Word-Net entries are actually phrases, rather than words (e.g.
?tulip?
?
?liliaceous plant?
?
...  ?
?plant?).
In such cases we search upthe tree until we stumble upon a node where the entry (or one ofthe entries) is a single word.2 DRP level of 50 roughly corresponds to the beginning of 6th gradein US schools, 70 corresponds to end of 10th grade.533the passage,  and averaged these values over 100passages of each batch.
Table 4 shows the results.LSAspecLSAVectorLengthIDFresidualIDFburstinessvariancegainTable 4: Average median specificity scores for texts ofdifferent genres and DRP levels.Note  that  the  absolute  values  for  a  particularbatch of texts are not important in this case; it's  therelative  differences  between  batches  of  differentstyles and difficulty levels that are of interest.
Ofall the measures, only  LSAspec appears to exhibitthe two characteristics described above (increasingwith text difficulty, and separating the three genresin the expected way).
The metrics residualIDF andburstiness also appear to separate the genres as ex-pected,  but  they do  not  increase  with  text  diffi-culty.It is also evident that  LSA Vector Length alonedoes not serve as a good measure of informative-ness, contrary to its use as such in (Gorman et al,2003).
In fact, it shows the most dramatic and reli-able  inverse relationship with text difficulty.
Thisis likely due to the fact that texts of lower diffi-culty use common (easier) words more often; thesewords tend to have longer LSA vector lengths.4.4 Back-of-the-Book GlossaryEducational textbooks typically have a glossary(index) at the end which lists important terms orconcepts mentioned in the book.
One would expectthese terms to have greater informativeness com-pared to other words in the textbook.
This was acrucial assumption used by Csomai and Mihalcea(2007), who used informativeness (as measured byIDF and other metrics) as one of the main featuresused  to  automatically  generate   glossaries  fromtextbooks.We can use existing textbooks and their glossar-ies to validate this assumptions, by observing theextent  to  which  the  words  in  the  glossary  areranked higher by different specificity metrics com-pared to other words.
Note that the goal here is notto actually achieve optimal  performance  in  auto-matically finding glossary words;  for  this  reasonwe do not use recall/precision- based evaluation orrely on  additional features such as term frequency(or the popular tfw?idfw measure).
Rather the goal isto simply see how much the glossary words exhibitthe property (informativeness) that we are trying tocompute with various methods.We obtained a  collection of textbook chapters(middle-school  level  material  from Prentice  HallPublishing) and their corresponding glossaries, intwo  different  genres:  8  on  World  Studies  (e.g.
?Africa?,  ?Medieval  Times?)
and  13  on  Science(e.g.
?Structure of Animals?, ?Electricity?).
Each50 55 60 65 70SocialStudiesLanguageArtsScience50 55 60 65 70SocialStudiesLanguageArtsScience50 55 60 65 70SocialStudiesLanguageArtsScience50 55 60 65 70SocialStudiesLanguageArtsScience50 55 60 65 70SocialStudiesLanguageArtsScience50 55 60 65 70SocialStudiesLanguageArtsScience50 55 60 65 70SocialStudiesLanguageArtsScience534chapter was converted into text and a list of uniquewords was extracted.For each of the specificity metrics, we computehow well it predicts glossary words:1.
Compute the specificity of each word in achapter, according to the metric.2.
Order all the words in decreasing order ofspecificity.3.
Compute the median percentile rank (posi-tion)  in  the  list  above  of  all  single-wordentries in the glossary (top word has  therank of 0; bottom has a rank of 100).If  a  specificity  metric  predicts  the  glossarywords well, we would expect the average rank tobe low; i.e.
glossary words would be near the topof the specificity-ordered list.Metric Word Studies(~9000 total wds / ch~260 gloss wds / ch)Science(~1000 total wds / ch~20 gloss wds / ch)LSAspec 0.21 0.10residualIDF 0.21 0.11burstiness 0.21 0.12IDF 0.29 0.16variance 0.49 0.64gain 0.51 0.68Table 5: Average median rank of glossary words amongall other words in textbook by specificity.LSAspec shows the lowest median percentile forboth genres of books.4.5 Qualitative AnalysisIt is useful to inspect the significant differencesbetween the word rankings by different methods,to see if some notable patterns emerge.
We canfind words  on  which the  methods  disagree  mostdramatically by observing which of them have themost significant differences of position (0-100) inthe word lists ranked by different specificity met-rics.
To avoid dealing with overly-rare words, werestrict  our attention to the 23,000 most  frequentwords in the corpus.Let's  first  compare  LSAspec and  residualIDF.From the list of 100 words with the most extremedisagreements, we select some examples that havehigh rank for  LSAspec (and low for  residuaIDF)and vice-versa.
From Table 6 we can see that  re-sidualIDF misses some term words (such as ?chro-matin?)
which  LSAspec correctly rates as highly-specific  words.
Conversely,  residualIDF,  incor-rectly ranks common words like ?her?
and ?water?as highly-specific.
The reason for this behavior isthat words like ?chromatin?
happen to occur onlyonce per document in the texts they are mentioned(e.g.
dfcromatin =  tfchromatin =  7),  whereas  ?her?
and?school?
tend to occur frequently per document.
Inreal applications ?her?
will probably be discardedusing  stopword  lists,  but  ?school?
will  probablynot.Word LSAspec residualIDFoviducts 0.5 98.8cuspids 0.6 98.8chromatin 0.7 98.7disassembly 0.7 98.7her 99.9 1.5my 99.9 3.5water 97.5 5.1school 97.8 10.3Table 6: Words ranked most differently by LSAspec andresidualIDFComparing  LSAspec  and  burstiness we see al-most  the  same  pattern,  which  is  not  surprising,since  burstiness and  residualIDF work  from thesame assumptions that content words tend to occurmultiple times but in fewer documents.The table below lists examples of most notabledifferences between LSAspec and IDF.Word LSAspec IDFbilly 10.3 93.5jack 15.0 95.9melody 4.1 83.8cells 10.8 86.3inducing 34.0 9.8vagueness 32.5 9.6initiating 31.5 8.7apathetic 32.3 9.8Table 7: Words ranked most differently by LSAspec andIDF and their percentilesThere is a large disagreement between rankingsof  common proper  names  (e.g.
?jack?).
It  is  notclear what the correct answer for these should be,although Rennie & Jaakkola (2005) use informat-iveness for named entity detection, assuming thatproper names should have high specificity.
Com-mon  but  important  words  like  ?melody?
and?cells?
are considered low-specificity by  IDF.
By535contrast, rare but vague words like ?inducing?
or?vagueness?
are  improperly   given  a  high   spe-cificity ranking.5 Applications in LSAHaving demonstrated that  our word specificitymetric performs well with regards to some naturallinguistic phenomena, we can now show that it canbe used successfully as part of existing NLP tech-nologies.
Here we will  focus particularly on  ap-plications within  Latent Semantic Analysis (LSA),although it is highly likely that this specificity met-ric can be used successfully in other places as well.We will demonstrate that  LSAspec shows betterresults  that  the  conventional  term  weightingscheme in LSA.
It is also important to note that al-though LSAspec is derived using LSA, it is in factlogically  independent  from  the  term  weightingmechanism used by LSA; other metrics  (such asthe ones described above) could also be potentiallyused for LSA term weighting.In order to represent the meaning of text in LSA,one typically computes the document vector of thetext by geometric addition of word vectors for eachof the constituent words:?V d=?w?daw?log?1?tdw??
?vwwhere aw is the log-entropy weight of the wordw, typically set to tfw?idfw (or some variation there-of) , tdw is the number of occurrences of the word win the document, and vw is the vector of the word.Implicit in  vw  is its geometric length, which tendsto be much greater for frequently-used words (un-less  they  are  extremely  vague).
It  is  temperedsomewhat by  aw which is higher for content words,but  perhaps  not  effectively  enough,  as  the  sub-sequent  tests  will  show.
McNamara et  al.
(2007)experimented  with  changing  the  weightingscheme,  mainly  focusing  on  prioritizing  rare  vs.frequent words, and has shown significant differ-ences in short-sentence comparison results.In the sections below we compare the originalLSA  weighting  scheme  with  our  new  schemebased on LSAspec:?V d=?w?dLSAspec ?w??log?1?tdw??
?vw?
?vw?In other words, we replace the weight parameter awand the implicit weight contained in the length ofeach word vector (by normalizing it) with the spe-cificity value of LSAspec.We  show  that  the  resulting  term  weightingscheme improves  performance  in three importantapplications:  information  retrieval,  gisting  andshort-sentence comparison.5.1 Information RetrievalLSA was first introduced as Latent Semantic In-dexing (Deerwester et al 1990), designed for thegoal of more effective information retrieval by rep-resenting both documents and queries as vectors ina common latent semantic space.In this  IR context,  the type  of term weightingused to compose document and query vectors playsan  important  role.
We  show  that  using  ourLSAspec-based term weighting gives superior per-formance to the traditional weighting scheme de-scribed in the previous section.We used the SMART Time3 dataset, a collectionof 425 documents and 83 queries related to Timemagazine news articles.
For this task only, we useda LSA space that was built using the AQUAINT-2corpus4, a large collection (~440,000) of news art-icles from prominent newspapers such as the NewYork Times.
The variable parameter in the LSAIR models was the cosine threshold between thedocument and the query, which was variedbetween 0 and 1Figure 1 shows the performance of the originalLSA and LSA with LSAspec5 term weightingmethod, in terms of the F-measure, which is theharmonic mean of precision and recall; a highervalue means better performance.
The abscissa in3   ftp://ftp.cs.cornell.edu/pub/smart/time/4 TREC conference: http://trec.nist.gov/5 LSAspec measure was the same as before, derived from LSA builton MetaMetrics corpus.Figure 2: The performance of default LSA andLSA+LSASpec on SMART IR dataset.0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.00.000.100.200.300.40F-measureDefaultLSAspec536the graph is the value of the threshold cosine para-meter.
The LSAspec term weighting outperformsthe original term weighting.5.2 Sentence SimilarityHere we analyze performance of the two LSAterm  weighting  methods  on  automated  sentencesimilarity comparisons.
Although LSA works beston units of text of paragraph-size or larger, it canwork reasonably well on sentence-length units.We  use  the  dataset  reported  by  McNamara(2007),  where the authors collected a set  of sen-tence pairs from several books.
A total of 96 sen-tence pairs was provided, consisting of a combina-tion of subsequent sentences in the book (16), non-adjacent  sentences  in  the  same  book  (16),  sen-tences  from  two  different  books  (48),  and  sen-tences where one is a manually-created paraphraseof one another (16).
The standard of reference forthis task is human similarity ratings of these sen-tences within each pair, reported on a Likert scalebetween 6 (most  similar)  and 1 (completely dis-similar).
Here we report correlations between hu-man rating and LSA similarity with the two termweighting metrics.Original LSA: 0.66 LSA + LSAspec: 0.85Using LSAspec term weighting gives better per-formance  compared  to  the  original  LSA  termweighting scheme.5.3 Gisting (Very Short Summarization)The ability to represent documents and words ina common geometric space allows LSA to easilycompute  the  gist  of  a  document  by  finding  theword (or sentence) whose vector is most similar bycosine metric to the document  vector.
This wordcan be interpreted as the most representative of thecumulative meaning of the document; it can alsobe thought  as a one-word summary of the docu-ment.
Gisting  is  discussed  from a  psychologicalperspective by Kintsch (2002).Once again, the choice of term weighting mech-anism can make a significant difference in how theoverall  document vector is constructed.
Here, wecompare  the  original  weighting  scheme  andLSAspec in the performance on gisting.
To performthis evaluation, we selected 46 well-written Wiki-pedia6 articles in various categories: Sports, Anim-als, Countries, Sciences, Religions, Diseases.
The6 http://en.wikipedia.org  , circa May 2008.original single-word Wikipedia title of each of thearticles  can  be  thought  as  the  optimal  one-wordgist of the article, thus serving as a reference an-swer in evaluation.
A perfect gisting performanceby the model would always select the original titleas the closest  word to the  meaning of the docu-ment.
We also measure the position of the originaltitle in the list of all words in the article ranked bytheir  similarity to  the  document  vector,  and ran-ging from 0 (original title picked as top word) and1.
Table 10 shows a few examples of both the topword and rank of the title, as well  as the overallmean rank of all 46 articles.Title Orig LSA LSA + LSAspectop word rank top word rankSkiing skiing 0.0000 skiing 0.0000Thailand buddhism 0.0189 thailand 0.0000Sociology sociologists 0.0012 sociology 0.0000Pneumonia infections 0.0064 infections 0.0092Mean rank (all 46 articles) 0.0191 0.0061St.
dev.
of rank 0.0847 0.0133Table 8: Examples of gisting (picking most representat-ive word for text) in with and without LSASpec in LSAUsing  LSAspec noticeably  improves  gisting  per-formance,  compared  to  the  original  LSA  termweighting method, as is evidenced by much lowermean rank of the original title.6 ConclusionWe have introduced a new method of measuringword informativeness.
The method gives good res-ults modeling some real linguistic phenomena, andimproves LSA applications.We attempted to look more deeply at the relev-ant characteristics of word specificity (such as cor-relation  with  number  of  senses).
Our  methodseems to correspond with intuition on emulating awide range of these characteristics.
It also avoids alot  of  pitfalls  of  existing methods  that  are basedpurely on frequency statistics, such as unduly pri-oritizing rare but vague words.Further research should examine the stability ofthis method (compared to others) with regards tovariation/size of the training corpus.
It should alsoanalyze application of the method in other naturallanguage tasks.
Lastly, it should be correlated withhuman judgments, similar to other psycholinguisticproperties.537ReferencesKenneth W. Church and  William A. Gale.
1995.
Pois-son mixtures.
Journal  of  Natural  Language Engin-eering,  1995Kenneth W. Church and  William A. Gale.
1995a.
In-verse document frequency (IDF): A measure of devi-ation  from  Poisson.
In  Proceedings  of  the  ThirdWorkshop  on  Very  Large  Corpora,  pp  121?130,1995.Andr?s  Csomai  and  Rada  Mihalcea.
2007.
Investiga-tions in Unsupervised Back-of-the-Book Indexing.
InProceedings of the Florida Artificial Intelligence Re-search Society, Key West.Scott Deerwester, Susan T. Dumais, George W. Furnasand Thomas K. Landauer.
1990.
Indexing by LatentSemantic Analysis.
Journal of the American Societyfor Information Science, 41.Aloysia  M.  Gorman.
1961.
Recognition  Memory  forNouns as a Function of Abstractness and Frequency.Journal of Experimental Psychology.
Vol.
61, No.
1.Jamie  C.  Gorman,  Peter  W.  Foltz,  Preston  A.Kiekel and Melanie J. Martin.
2003.
Evaluation ofLatent Semantic Analysis-based  Measures of TeamCommunication Content.
Proceedings of the HumanFactors and Ergonomics Society, 47th Annual Meet-ing, pp 424-428.Walter Kintsch.
2002.
On the notions of theme and top-ic in psychological  process models of text compre-hension.
In  M.  Louwerse  &  W.  van  Peer  (Eds.
)Thematics  :  Interdisciplinary  Studies,  Amsterdam,Benjamins, pp 157-170.Walter Kintsch.
2001.
Predication.
Journal of CognitiveScience, 25.Kirill  Kireyev.
2008.
Using Latent  Semantic Analysisfor  Extractive  Summarization.
Proceedings  of  TextAnalysis Conference, 2008.B.
L. Koslin, S. Zeno, and S. Koslin.
1987.
The DRP:An Effective Measure in Reading.
New York CollegeEntrance Examination Board.Thomas K Landauer and Susan Dumais.
1997.
A solu-tion to Plato's problem: The Latent Semantic nalysistheory of the   acquisition, induction, and representa-tion of knowledge.
Psychological  Review, 104, pp211-240.Thomas  K  Landauer,  Danielle  S.  McNamara,  SimonDennis, and Walter Kintsch.
2007.
Handbook of Lat-ent Semantic Analysis Lawrence Erlbaum.Rachel  TszWai  Lo,  Ben  He,  and  Iadh  Ounis.
2005.Automatically Building a Stopword List  for  an In-formation Retrieval  System.
5th Dutch-Belgium In-formation Retrieval Workshop (DIR).
2005.Philip  M.  McCarthy,  Arthur  C.  Graesser,  Danielle  S.McNamara.
2006.
Distinguishing Genre Using Coh-Metrix Indices of Cohesion.
16th Annual Meeting ofthe  Society  for  Text  and  Discourse, Minneapolis,MN, 2006.Danielle  S.  McNamara,  Zhiqiang  Cai,  and  MaxM.Louwerse.
2007.
Optimizing LSA Measures of Cohe-sion.
Handbook of Latent Semantic Analysis .
Mah-wah, NJ: Erlbaum.
ch 19, pp 379-399.George  A.  Miller,  Richard  Beckwith,  Christiane  Fell-baum,  Derek  Gross  and  Katherine  Miller.
1990.WordNet: An on-line lexical database.
InternationalJournal of Lexicography, 3 (4), 1990.Kishore  Papineni.
2001.
Why  inverse  document  fre-quency.
In Proceedings of the NAACL, 2001Jamie Reilly and Jacob Kean.
2007.
Formal Distinctive-ness Of High- and Low- Imageability Nouns: Ana-lyses  and  Theoretical  Implications.
Cognitive  Sci-ence, 31.Jason D. M. Rennie and Tommi Jaakkola.
2005.
UsingTerm Informativeness  for  Named Entity  Detection.Proceedings of ACM SIGIR 2005.Karen Sp?rck-Jones.
1973.
"A Statistical Interpretationof Term Specificity and its Application in Retrieval,"Journal of Documentation, 28:1.Michael P. Toglia and William R. Battig.
1978.
Hand-book  of  semantic  word  norms.
Hillsdale,  NJ:Lawrence Erlbaum Associates.538
