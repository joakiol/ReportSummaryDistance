Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 263?270,Sydney, July 2006. c?2006 Association for Computational LinguisticsDiscriminative Reranking for Semantic ParsingRuifang Ge Raymond J. MooneyDepartment of Computer SciencesUniversity of Texas at AustinAustin, TX 78712{grf,mooney}@cs.utexas.eduAbstractSemantic parsing is the task of mappingnatural language sentences to completeformal meaning representations.
The per-formance of semantic parsing can be po-tentially improved by using discrimina-tive reranking, which explores arbitraryglobal features.
In this paper, we investi-gate discriminative reranking upon a base-line semantic parser, SCISSOR, where thecomposition of meaning representations isguided by syntax.
We examine if featuresused for syntactic parsing can be adaptedfor semantic parsing by creating similarsemantic features based on the mappingbetween syntax and semantics.
We re-port experimental results on two real ap-plications, an interpreter for coaching in-structions in robotic soccer and a natural-language database interface.
The resultsshow that reranking can improve the per-formance on the coaching interpreter, butnot on the database interface.1 IntroductionA long-standing challenge within natural languageprocessing has been to understand the meaning ofnatural language sentences.
In comparison withshallow semantic analysis tasks, such as word-sense disambiguation (Ide and Jeane?ronis, 1998)and semantic role labeling (Gildea and Jurafsky,2002; Carreras and Ma`rquez, 2005), which onlypartially tackle this problem by identifying themeanings of target words or finding semantic rolesof predicates, semantic parsing (Kate et al, 2005;Ge and Mooney, 2005; Zettlemoyer and Collins,2005) pursues a more ambitious goal ?
mappingnatural language sentences to complete formalmeaning representations (MRs), where the mean-ing of each part of a sentence is analyzed, includ-ing noun phrases, verb phrases, negation, quanti-fiers and so on.
Semantic parsing enables logicreasoning and is critical in many practical tasks,such as speech understanding (Zue and Glass,2000), question answering (Lev et al, 2004) andadvice taking (Kuhlmann et al, 2004).Ge and Mooney (2005) introduced an approach,SCISSOR, where the composition of meaning rep-resentations is guided by syntax.
First, a statis-tical parser is used to generate a semantically-augmented parse tree (SAPT), where each internalnode includes both a syntactic and semantic label.Once a SAPT is generated, an additional meaning-composition process guided by the tree structure isused to translate it into a final formal meaning rep-resentation.The performance of semantic parsing can be po-tentially improved by using discriminative rerank-ing, which explores arbitrary global features.While reranking has benefited many tagging andparsing tasks (Collins, 2000; Collins, 2002c;Charniak and Johnson, 2005) including semanticrole labeling (Toutanova et al, 2005), it has notyet been applied to semantic parsing.
In this paper,we investigate the effect of discriminative rerank-ing to semantic parsing.We examine if the features used in rerankingsyntactic parses can be adapted for semantic pars-ing, more concretely, for reranking the top SAPTsfrom the baseline model SCISSOR.
The syntac-tic features introduced by Collins (2000) for syn-tactic parsing are extended with similar semanticfeatures, based on the coupling of syntax and se-mantics.
We present experimental results on twocorpora: an interpreter for coaching instructions263in robotic soccer (CLANG) and a natural-languagedatabase interface (GeoQuery).
The best rerank-ing model significantly improves F-measure onCLANG from 82.3% to 85.1% (15.8% relative er-ror reduction), however, it fails to show improve-ments on GEOQUERY.2 Background2.1 Application Domains2.1.1 CLANG: the RoboCup Coach LanguageRoboCup (www.robocup.org) is an inter-national AI research initiative using robotic socceras its primary domain.
In the Coach Competition,teams of agents compete on a simulated soccerfield and receive advice from a team coach ina formal language called CLANG.
In CLANG,tactics and behaviors are expressed in terms ofif-then rules.
As described in Chen et al (2003),its grammar consists of 37 non-terminal symbolsand 133 productions.
Negation and quantifierslike all are included in the language.
Below is asample rule with its English gloss:((bpos (penalty-area our))(do (player-except our {4})(pos (half our))))?If the ball is in our penalty area, all our playersexcept player 4 should stay in our half.
?2.1.2 GEOQUERY: a DB Query LanguageGEOQUERY is a logical query language fora small database of U.S. geography containingabout 800 facts.
The GEOQUERY languageconsists of Prolog queries augmented with severalmeta-predicates (Zelle and Mooney, 1996).
Nega-tion and quantifiers like all and each are includedin the language.
Below is a sample query with itsEnglish gloss:answer(A,count(B,(city(B),loc(B,C),const(C,countryid(usa))),A))?How many cities are there in the US?
?2.2 SCISSOR: the Baseline ModelSCISSOR is based on a fairly standard approachto compositional semantics (Jurafsky and Martin,2000).
First, a statistical parser is used to con-struct a semantically-augmented parse tree thatcaptures the semantic interpretation of individualNP-PLAYERPRP$-TEAMourNN-PLAYERplayerCD-UNUM2Figure 1: A SAPT for describing a simple CLANGconcept PLAYER .words and the basic predicate-argument structureof a sentence.
Next, a recursive deterministic pro-cedure is used to compose the MR of a parentnode from the MR of its children following thetree structure.Figure 1 shows the SAPT for a simple naturallanguage phrase describing the concept PLAYERin CLANG.
We can see that each internal nodein the parse tree is annotated with a semantic la-bel (shown after dashes) representing concepts inan application domain; when a node is semanti-cally vacuous in the application domain, it is as-signed with the semantic label NULL.
The seman-tic labels on words and non-terminal nodes repre-sent the meanings of these words and constituentsrespectively.
For example, the word our repre-sents a TEAM concept in CLANG with the valueour, whereas the constituent OUR PLAYER 2 rep-resents a PLAYER concept.
Some type conceptsdo not take arguments, like team and unum (uni-form number), while some concepts, which werefer to as predicates, take an ordered list of ar-guments, like player which requires both a TEAMand a UNUM as its arguments.SAPTs are given to a meaning compositionprocess to compose meaning, guided by bothtree structures and domain predicate-argument re-quirements.
In figure 1, the MR of our and 2would fill the arguments of PLAYER to generatethe MR of the whole constituent PLAYER(OUR,2)using this process.SCISSOR is implemented by augmentingCollins?
(1997) head-driven parsing model II toincorporate the generation of semantic labels oninternal nodes.
In a head-driven parsing model,a tree can be seen as generated by expandingnon-terminals with grammar rules recursively.To deal with the sparse data problem, the expan-sion of a non-terminal (parent) is decomposedinto primitive steps: a child is chosen as thehead and is generated first, and then the otherchildren (modifiers) are generated independently264BACK-OFFLEVEL PL1(Li|...)1 P,H,w,t,?,LC2 P,H,t,?,LC3 P,H,?,LC4 P,H5 PTable 1: Extended back-off levels for the semanticparameter PL1(Li|...), using the same notation asin Ge and Mooney (2005).
The symbols P , H andLi are the semantic label of the parent , head, andthe ith left child, w is the head word of the parent,t is the semantic label of the head word, ?
is thedistance between the head and the modifier, andLC is the left semantic subcat.constrained by the head.
Here, we only describechanges made to SCISSOR for reranking, for afull description of SCISSOR see Ge and Mooney(2005).In SCISSOR, the generation of semantic labelson modifiers are constrained by semantic subcat-egorization frames, for which data can be verysparse.
An example of a semantic subcat in Fig-ure 1 is that the head PLAYER associated with NNrequires a TEAM as its modifier.
Although thisconstraint improves SCISSOR?s precision, whichis important for semantic parsing, it also limitsits recall.
To generate plenty of candidate SAPTsfor reranking, we extended the back-off levels forthe parameters generating semantic labels of mod-ifiers.
The new set is shown in Table 1 using theparameters for the generation of the left-side mod-ifiers as an example.
The back-off levels 4 and 5are newly added by removing the constraints fromthe semantic subcat.
Although the best SAPTsfound by the model may not be as precise as be-fore, we expect that reranking can improve the re-sults and rank correct SAPTs higher.2.3 The Averaged Perceptron RerankingModelAveraged perceptron (Collins, 2002a) has beensuccessfully applied to several tagging and parsingreranking tasks (Collins, 2002c; Collins, 2002a),and in this paper, we employed it in rerankingsemantic parses generated by the base semanticparser SCISSOR.
The model is composed of threeparts (Collins, 2002a): a set of candidate SAPTsGEN , which is the top n SAPTs of a sentencefrom SCISSOR; a function ?
that maps a sentenceInputs: A set of training examples (xi, y?i ), i = 1...n, where xiis a sentence, and y?i is a candidate SAPT that has the highestsimilarity score with the gold-standard SAPTInitialization: Set W?
= 0Algorithm:For t = 1...T, i = 1...nCalculate yi = arg maxy?GEN(xi) ?
(xi, y) ?
W?If (yi 6= y?i ) then W?
= W?
+ ?
(xi, y?i ) ?
?
(xi, yi)Output: The parameter vector W?Figure 2: The perceptron training algorithm.x and its SAPT y into a feature vector ?
(x, y) ?Rd; and a weight vector W?
associated with the setof features.
Each feature in a feature vector is afunction on a SAPT that maps the SAPT to a realvalue.
The SAPT with the highest score under aparameter vector W?
is outputted, where the scoreis calculated as:score(x, y) = ?
(x, y) ?
W?
(1)The perceptron training algorithm for estimat-ing the parameter vector W?
is shown in Fig-ure 2.
For a full description of the algorithm,see (Collins, 2002a).
The averaged perceptron, avariant of the perceptron algorithm is often used intesting to decrease generalization errors on unseentest examples, where the parameter vectors usedin testing is the average of each parameter vectorgenerated during the training process.3 Features for Reranking SAPTsIn our setting, reranking models discriminate be-tween SAPTs that can lead to correct MRs andthose that can not.
Intuitively, both syntactic andsemantic features describing the syntactic and se-mantic substructures of a SAPT would be good in-dicators of the SAPT?s correctness.The syntactic features introduced by Collins(2000) for reranking syntactic parse trees havebeen proven successfully in both English andSpanish (Cowan and Collins, 2005).
We exam-ine if these syntactic features can be adapted forsemantic parsing by creating similar semantic fea-tures.
In the following section, we first briefly de-scribe the syntactic features introduced by Collins(2000), and then introduce two adapted semanticfeature sets.
A SAPT in CLANG is shown in Fig-ure 3 for illustrating the features throughout thissection.265VP-ACTION.PASSVBbeVP-ACTION.PASSVBN-ACTION.PASSpassedPP-POINTTOtoNP-POINTPRN-POINT-LRB?POINT(NP-NUM1CD-NUM36COMMA,NP-NUM2CD-NUM10-RRB-)Figure 3: A SAPT for illustrating the reranking features, where the syntactic label ?,?
is replaced byCOMMA for a clearer description of features, and the NULL semantic labels are not shown.
The headof the rule ?PRN-POINT?
-LRB?POINT NP-NUM1 COMMA NP-NUM2 -RRB-?
is -LRB?POINT.
Thesemantic labels NUM1 and NUM2 are meta concepts in CLANG specifying the semantic role filled sinceNUM can fill multiple semantic roles in the predicate POINT.3.1 Syntactic FeaturesAll syntactic features introduced by Collins (2000)are included for reranking SAPTs.
While the fulldescription of all the features is beyond the scopeof this paper, we still introduce several featuretypes here for the convenience of introducing se-mantic features later.1.
Rules.
These are the counts of unique syntac-tic context-free rules in a SAPT.
The examplein Figure 3 has the feature f (PRN?
-LRB- NPCOMMA NP -RRB-)=1.2.
Bigrams.
These are the counts of uniquebigrams of syntactic labels in a constituent.They are also featured with the syntactic la-bel of the constituent, and the bigram?s rel-ative direction (left, right) to the head of theconstituent.
The example in Figure 3 has thefeature f (NP COMMA, right, PRN)=1.3.
Grandparent Rules.
These are the same asRules, but also include the syntactic labelabove a rule.
The example in Figure 3 hasthe feature f ([PRN?
-LRB- NP COMMA NP-RRB-], NP)=1, where NP is the syntactic la-bel above the rule ?PRN?
-LRB- NP COMMANP -RRB-?.4.
Grandparent Bigrams.
These are the sameas Bigrams, but also include the syntacticlabel above the constituent containing a bi-gram.
The example in Figure 3 has thefeature f ([NP COMMA, right, PRN], NP)=1,where NP is the syntactic label above the con-stituent PRN.3.2 Semantic Features3.2.1 Semantic Feature Set IA similar semantic feature type is introduced foreach syntactic feature type used by Collins (2000)by replacing syntactic labels with semantic ones(with the semantic label NULL not included).
Thecorresponding semantic feature types for the fea-tures in Section 3.1 are:1.
Rules.
The example in Figure 3 has the fea-ture f (POINT?
POINT NUM1 NUM2)=1.2.
Bigrams.
The example in Figure 3 has thefeature f (NUM1 NUM2, right, POINT)=1,where the bigram ?NUM1 NUM2?appears tothe right of the head POINT.3.
Grandparent Rules.
The example in Figure 3has the feature f ([POINT?
POINT NUM1NUM2], POINT)=1, where the last POINT is266ACTION.PASSACTION.PASSpassedPOINTPOINT(NUM1NUM36NUM2NUM10Figure 4: The tree generated by removing purely-syntactic nodes from the SAPT in Figure 3 (withsyntactic labels omitted.
)the semantic label above the semantic rule?POINT?
POINT NUM1 NUM2?.4.
Grandparent Bigrams.
The example in Fig-ure 3 has the feature f ([NUM1 NUM2, right,POINT], POINT)=1, where the last POINT isthe semantic label above the POINT associ-ated with PRN.3.2.2 Semantic Feature Set IIPurely-syntactic structures in SAPTs exist withno meaning composition involved, such as the ex-pansions from NP to PRN, and from PP to ?TO NP?in Figure 3.
One possible drawback of the seman-tic features derived directly from SAPTs as in Sec-tion 3.2.1 is that they could include features withno meaning composition involved, which are in-tuitively not very useful.
For example, the nodeswith purely-syntactic expansions mentioned abovewould trigger a semantic rule feature with mean-ing unchanged (from POINT to POINT).
Anotherpossible drawback of these features is that the fea-tures covering broader context could potentiallyfail to capture the real high-level meaning compo-sition information.
For example, the GrandparentRule example in Section 3.2.1 has POINT as thesemantic grandparent of a POINT composition, butnot the real one ACTION.PASS.To address these problems, another semanticfeature set is introduced by deriving semantic fea-tures from trees where purely-syntactic nodes ofSAPTs are removed (the resulting tree for theSAPT in Figure 3 is shown in Figure 4).
In thistree representation, the example in Figure 4 wouldhave the Grandparent Rule feature f ([POINT?POINT NUM1 NUM2], ACTION.PASS)=1, with thecorrect semantic grandparent ACTION.PASS in-cluded.4 Experimental Evaluation4.1 Experimental MethodologyTwo corpora of natural language sentences pairedwith MRs were used in the reranking experiments.For CLANG, 300 pieces of coaching advice wererandomly selected from the log files of the 2003RoboCup Coach Competition.
Each formal in-struction was translated into English by one offour annotators (Kate et al, 2005).
The averagelength of an natural language sentence in this cor-pus is 22.52 words.
For GEOQUERY, 250 ques-tions were collected by asking undergraduate stu-dents to generate English queries for the givendatabase.
Queries were then manually translatedinto logical form (Zelle and Mooney, 1996).
Theaverage length of a natural language sentence inthis corpus is 6.87 words.We adopted standard 10-fold cross validationfor evaluation: 9/10 of the whole dataset was usedfor training (training set), and 1/10 for testing (testset).
To train a reranking model on a training set,a separate ?internal?
10-fold cross validation overthe training set was employed to generate n-bestSAPTs for each training example using a base-line learner, where each training set was againseparated into 10 folds with 9/10 for training thebaseline learner, and 1/10 for producing the n-best SAPTs for training the reranker.
Rerankingmodels trained in this way ensure that the n-bestSAPTs for each training example are not gener-ated by a baseline model that has already seen thatexample.
To test a reranking model on a test set, abaseline model trained on a whole training set wasused to generate n-best SAPTs for each test ex-ample, and then the reranking model trained withthe above method was used to choose a best SAPTfrom the candidate SAPTs.The performance of semantic parsing was mea-sured in terms of precision (the percentage of com-pleted MRs that were correct), recall (the percent-age of all sentences whose MRs were correctlygenerated) and F-measure (the harmonic mean ofprecision and recall).
Since even a single mistakein an MR could totally change the meaning of anexample (e.g.
having OUR in an MR instead of OP-PONENT in CLANG), no partial credit was givenfor examples with partially-correct SAPTs.Averaged perceptron (Collins, 2002a), whichhas been successfully applied to several tag-ging and parsing reranking tasks (Collins, 2002c;Collins, 2002a), was employed for training rerank-267CLANG GEOQUERYP R F P R FSCISSOR 89.5 73.7 80.8 98.5 74.4 84.8SCISSOR+ 87.0 78.0 82.3 95.5 77.2 85.4Table 2: The performance of the baseline model SCISSOR+ compared with SCISSOR (with the best result inbold), where P = precision, R = recall, and F = F-measure.n 1 2 5 10 20 50CLANG 78.0 81.3 83.0 84.0 85.0 85.3GEOQUERY 77.2 77.6 80.0 81.2 81.6 81.6Table 3: Oracle recalls on CLANG and GEOQUERY as a function of number n of n-best SAPTs.ing models.
To choose the correct SAPT of atraining example required for training the aver-aged perceptron, we selected a SAPT that resultsin the correct MR; if multiple such SAPTs exist,the one with the highest baseline score was cho-sen.
Since no partial credit was awarded in evalua-tion, a training example was discarded if it had nocorrect SAPT.
Rerankers were trained on the 50-best SAPTs provided by SCISSOR, and the num-ber of perceptron iterations over the training exam-ples was limited to 10.
Typically, in order to avoidover-fitting, reranking features are filtered by re-moving those occurring in less than some mini-mal number of training examples.
We only re-moved features that never occurred in the trainingdata since experiments with higher cut-offs failedto show any improvements.4.2 Results4.2.1 Baseline ResultsTable 2 shows the results comparing the base-line learner SCISSOR using both the back-off pa-rameters in Ge and Mooney (2005) (SCISSOR) andthe revised parameters in Section 2.2 (SCISSOR+).As we expected, SCISSOR+ has better recall andworse precision than SCISSOR on both corporadue to the additional levels of back-off.
SCISSOR+is used as the baseline model for all reranking ex-periments in the next section.Table 3 gives oracle recalls for CLANG andGEOQUERY where an oracle picks the correctparse from the n-best SAPTs if any of them arecorrect.
Results are shown for increasing valuesof n. The trends for CLANG and GEOQUERY aredifferent: small values of n show significant im-provements for CLANG, while a larger n is neededto improve results for GEOQUERY.4.2.2 Reranking ResultsIn this section, we describe the experimentswith reranking models utilizing different featuresets.
All models include the score assigned to aSAPT by the baseline model as a special feature.Table 4 shows results using different feature setsderived directly from SAPTs.
In general, rerank-ing improves the performance of semantic parsingon CLANG, but not on GEOQUERY.
This couldbe explained by the different oracle recall trends ofCLANG and GEOQUERY.
We can see that in Ta-ble 3, even a small n can increase the oracle scoreon CLANG significantly, but not on GEOQUERY.With the baseline score included as a feature, cor-rect SAPTs closer to the top are more likely tobe reranked to the top than the ones in the back,thus CLANG is more likely to have more sentencesreranked correct than GEOQUERY.
On CLANG,using the semantic feature set alne achieves thebest improvements over the baseline with 2.8%absolute improvement in F-measure (15.8% rel-ative error reduction), which is significant at the95% confidence level using a paired Student?s t-test.
Nevertheless, the difference between SEM1and SYN+SEM1 is very small (only one example).Using syntactic features alone only slightly im-proves the results because the syntactic featuresdo not directly discriminate between correct andincorrect meaning representations.
To put thisin perspective, Charniak and Johnson (2005) re-ported that reranking improves the F-measure ofsyntactic parsing from 89.7% to 91.0% with a 50-best oracle F-measure score of 96.8%.Table 5 compares results using semantic fea-tures directly derived from SAPTs (SEM1), andfrom trees with purely-syntactic nodes removed(SEM2).
It compares reranking models using these268CLANG GEOQUERYP R F P R FSCISSOR+ 87.0 78.0 82.3 95.5 77.2 85.4SYN 87.7 78.7 83.0 95.5 77.2 85.4SEM1 90.0(23.1) 80.7(12.3) 85.1(15.8) 95.5 76.8 85.1SYN+SEM1 89.6 80.3 84.7 95.5 76.4 84.9Table 4: Reranking results on CLANG and GEOQUERY using different feature sets derived directly fromSAPTs (with the best results in bold and relative error reduction in parentheses).
The reranking modelSYN uses the syntactic feature set in Section 3.1, SEM1 uses the semantic feature set in Section 3.2.1, andSYN+SEM1 uses both.CLANG GEOQUERYP R F P R FSEM1 90.0 80.7 85.1 95.5 76.8 85.1SEM2 88.1 79.0 83.3 96.0 77.2 85.6SEM1+SEM2 88.5 79.3 83.7 95.5 76.4 84.9SYN+SEM1 89.6 80.3 84.7 95.5 76.4 84.9SYN+SEM2 88.1 79.0 83.3 95.5 76.8 85.1SYN+SEM1+SEM2 88.9 79.7 84.0 95.5 76.4 84.9Table 5: Reranking results on CLANG and GEOQUERY comparing semantic features derived directly fromSAPTs, and semantic features from trees with purely-syntactic nodes removed.
The symbol SEM1 and SEM2refer to the semantic feature sets in Section 3.2.1 and 3.2.1 respectively, and SYN refers to the syntacticfeature set in Section 3.1.feature sets alone and together, and using themalong with the syntactic feature set (SYN) aloneand together.
Overall, SEM1 provides better resultsthan SEM2 on CLANG and slightly worse resultson GEOQUERY (only in one sentence), regard-less of whether or not syntactic features are in-cluded.
Using both semantic feature sets does notimprove the results over just using SEM1.
On onehand, the better performance of SEM1 on CLANGcontradicts our expectation because of the reasonsdiscussed in Section 3.2.2; the reason behind thisneeds to be investigated.
On the other hand, how-ever, it also suggests that the semantic features de-rived directly from SAPTs can provide good evi-dence for semantic correctness, even with redun-dant purely syntactically motivated features.We have also informally experimented withsmoothed semantic features utilizing domain on-tology given by CLANG, which did not show im-provements over reranking models not using thesefeatures.5 ConclusionWe have applied discriminative reranking to se-mantic parsing, where reranking features are de-veloped from features for reranking syntacticparses based on the coupling of syntax and se-mantics.
The best reranking model significantlyimproves F-measure on a Robocup coaching task(CLANG) from 82.3% to 85.1%, while it fails toimprove the performance on a geography databasequery task (GEOQUERY).Future work includes further investigation ofthe reasons behind the different utility of rerank-ing for the CLANG and GEOQUERY tasks.
Wealso plan to explore other types of rerankingfeatures, such as the features used in semanticrole labeling (SRL) (Gildea and Jurafsky, 2002;Carreras and Ma`rquez, 2005), like the path be-tween a target predicate and its argument, andkernel methods (Collins, 2002b).
Experimentingwith other effective reranking algorithms, such asSVMs (Joachims, 2002) and MaxEnt (Charniakand Johnson, 2005), is also a direction of our fu-ture research.6 AcknowledgementsWe would like to thank Rohit J. Kate and anony-mous reviewers for their insightful comments.This research was supported by Defense Ad-269vanced Research Projects Agency under grantHR0011-04-1-0007.ReferencesXavier Carreras and Lu?
?s Ma`rquez.
2005.
Introduc-tion to the CoNLL-2005 shared task: Semantic rolelabeling.
In Proc.
of 9th Conf.
on ComputationalNatural Language Learning (CoNLL-2005), pages152?164, Ann Arbor, MI, June.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminativereranking.
In Proc.
of the 43nd Annual Meetingof the Association for Computational Linguistics(ACL-05), pages 173?180, Ann Arbor, MI, June.Mao Chen, Ehsan Foroughi, Fredrik Heintz, SpirosKapetanakis, Kostas Kostiadis, Johan Kummeneje,Itsuki Noda, Oliver Obst, Patrick Riley, Timo Stef-fens, Yi Wang, and Xiang Yin.
2003.
Usersmanual: RoboCup soccer server manual for soccerserver version 7.07 and later.
Available at http://sourceforge.net/projects/sserver/.Michael J. Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proc.
of the 35thAnnual Meeting of the Association for Computa-tional Linguistics (ACL-97), pages 16?23.Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
In Proc.
of 17th Intl.
Conf.on Machine Learning (ICML-2000), pages 175?182,Stanford, CA, June.Michael Collins.
2002a.
Discriminative training meth-ods for hidden Markov models: Theory and exper-iments with perceptron algorithms.
In Proc.
of the2002 Conf.
on Empirical Methods in Natural Lan-guage Processing (EMNLP-02), Philadelphia, PA,July.Michael Collins.
2002b.
New ranking algorithms forparsing and tagging: Kernels over discrete struc-tures, and the voted perceptron.
In Proc.
of the40th Annual Meeting of the Association for Com-putational Linguistics (ACL-2002), pages 263?270,Philadelphia, PA, July.Michael Collins.
2002c.
Ranking algorithms fornamed-entity extraction: Boosting and the votedperceptron.
In Proc.
of the 40th Annual Meet-ing of the Association for Computational Linguistics(ACL-2002), pages 489?496, Philadelphia, PA.Brooke Cowan and Michael Collins.
2005.
Mor-phology and reranking for the statistical parsing ofSpanish.
In Proc.
of the Human Language Technol-ogy Conf.
and Conf.
on Empirical Methods in Nat-ural Language Processing (HLT/EMNLP-05), Van-couver, B.C., Canada, October.Ruifang Ge and Raymond J. Mooney.
2005.
A statis-tical semantic parser that integrates syntax and se-mantics.
In Proc.
of 9th Conf.
on ComputationalNatural Language Learning (CoNLL-2005), pages9?16, Ann Arbor, MI, July.Daniel Gildea and Daniel Jurafsky.
2002.
Automatedlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Nancy A. Ide and Jeane?ronis.
1998.
Introduction tothe special issue on word sense disambiguation: Thestate of the art.
Computational Linguistics, 24(1):1?40.Thorsten Joachims.
2002.
Optimizing search en-gines using clickthrough data.
In Proc.
of 8th ACMSIGKDD Intl.
Conf.
on Knowledge Discovery andData Mining (KDD-2002), Edmonton, Canada.Daniel Jurafsky and James H. Martin.
2000.
Speechand Language Processing: An Introduction to Nat-ural Language Processing, Computational Linguis-tics, and Speech Recognition.
Prentice Hall, UpperSaddle River, NJ.R.
J. Kate, Y. W. Wong, and R. J. Mooney.
2005.Learning to transform natural to formal languages.In Proc.
of 20th Natl.
Conf.
on Artificial Intelli-gence (AAAI-2005), pages 1062?1068, Pittsburgh,PA, July.Gregory Kuhlmann, Peter Stone, Raymond J. Mooney,and Jude W. Shavlik.
2004.
Guiding a reinforce-ment learner with natural language advice: Initialresults in RoboCup soccer.
In Proc.
of the AAAI-04Workshop on Supervisory Control of Learning andAdaptive Systems, San Jose, CA, July.Iddo Lev, Bill MacCartney, Christopher D. Manning,and Roger Levy.
2004.
Solving logic puzzles: Fromrobust processing to precise semantics.
In Proc.
of2nd Workshop on Text Meaning and Interpretation,ACL-04, Barcelona, Spain.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2005.
Joint learning improves semanticrole labeling.
In Proc.
of the 43nd Annual Meet-ing of the Association for Computational Linguistics(ACL-05), Ann Arbor, MI, June.John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to parse database queries using inductive logicprogramming.
In Proc.
of 13th Natl.
Conf.
on Artifi-cial Intelligence (AAAI-96), pages 1050?1055, Port-land, OR, August.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In Proc.
of 21th Conf.
on Uncertainty inArtificial Intelligence (UAI-2005), Edinburgh, Scot-land, July.Victor W. Zue and James R. Glass.
2000.
Conversa-tional interfaces: Advances and challenges.
In Proc.of the IEEE, volume 88(8), pages 1166?1180.270
