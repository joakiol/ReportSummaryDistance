Automated Essay Scoring for Nonnative English SpeakersJill BursteinEducational Testing ServicePrinceton, New Jersey 08540jburstein@ets.orgMartin ChodorowHunter College, CUNYNew York, New Yorkmartin.chodorow@hunter.cuny.eduAbstractThe e-rater system TM ~ is an operationalautomated essay scoring system, developedat Educational Testing Service (ETS).
Theaverage agreement between human readers,and between independent human readersand e-rater is approximately 92%.
There ismuch interest in the larger writingcommunity in examining the system'sperformance on nonnative speaker essays.This paper focuses on results of a study thatshow e-rater's performance on Test ofWritten English (TWE) essay responseswritten by nonnative English speakerswhose native language is Chinese, Arabic,or Spanish.
In addition, one small sample ofthe data is from US-born English speakers,and another is from non-US-born candidateswho report that their native language isEnglish.
As expected, significantdifferences were found among the scores ofthe English groups and the nonnativespeakers.
While there were also differencesbetween e-rater and the human readers forthe various language groups, the averageagreement rate was as high as operationalagreement.
At least four of the five featuresthat are included in e-rater's currentoperational models (including discourse,topical, and syntactic features) also appearin the TWE models.
This suggests that thefeatures generalize well over a wide rangeof linguistic variation, as e-rater was not1 The e-rater system TM is a trademark ofEducational Testing Service.
In the paper, wewill refer to the e-rater system TM as e-rater.confounded by non-standard Englishsyntactic structures or stylistic discoursestructures which one might expect to be aproblem for a system designed to evaluatenative speaker writing.IntroductionResearch and development in automatedessay scoring has begun to flourish in thepast five years or so, bringing about a wholenew field of interest to the NLP community(Burstein, et al(1998a, 1998b and 1998c),Foltz, et al(1998), Larkey (1998), Page andPeterson (1995)).
Research at EducationalTesting Service (ETS) has led to the recentdevelopment of e-rater, an operationalautomated essay scoring system.
E-rater isbased on features in holistic scoring guidesfor human reader scoring.
Scoring guideshave a 6-point score scale.
Six's areassigned to the "best" essays, and "l 's" tothe least well-written.
Scoring guide criteriaare based on structural (syntax anddiscourse) and vocabulary usage in essayresponses ( ee http://www.gmat.org).E-rater builds new models for each topic(prompt-specific models) by evaluatingapproximately 52 syntactic, discourse andtopical analysis variables for 270 humanreader scored training essays.
Relevantfeatures for each model are based on thepredictive feature set identified by astepwise linear regression.
In operationalscoring, when compared to a human reader,68e-rater assigns an exactly matching oradjacent score (on the 6-point scale) about92% of the time.
This is the same as theagreement rate typically found between twohuman readers.
Correlations between e-rater scores and those of a single humanreader are about .73; correlations betweentwo human readers are .75.The scoring guide criteria assume standardwritten English.
Non-standard English mayshow up in the writing of native Englishspeakers of non-standard ialects.
Forgeneral NLP research purposes, it is usefulto have computer-based corpora thatrepresent language variation (Biber (1993)).Such corpora llow us to explore issues withregard to how the system will handleresponses that might be written in non-standard English.
Current research at ETSfor the Graduate Record Examination(GRE) (Burstein, et al 1999) is making useof essay corpora that represent subgroupswhere variations in standard written Englishmight be found, such as in the writing ofAfrican Americans, Latinos and Asians(Breland, et al(1995) and Bridgeman andMcHale (1996)).
In addition, ETS isaccumulating essay corpora of nonnativespeakers that can be used for research.This paper focuses on preliminary data thatshow e-rater's performance on Test ofWritten English (TWE) essay responseswritten by nonnative English speakerswhose native language is Chinese, Arabic,or Spanish.
A small sample of the data isfrom US-born English speakers and asecond small sample is from non-US-borncandidates who report that their nativelanguage is English.
The data wereoriginally collected for a study by Frase, etal (1997) in which analyses of the essays arealso discussed.
The current work is only thebeginning of a program of research at ETSthat will examine automated scoring fornonnative English speakers.
Overall goalsinclude determining how features used inautomated scoring may also be used to (a)examine the difficulty of an essay questionfor speakers of particular language groups,and (b) automatically formulate diagnosticsand instruction for nonnative Englishspeakers, with customization for differentlanguage groups.1.
E-rater Feature Identification,Model Building and ScoringThe driving concept that underlies e-rater isthat it needs to evaluate the same kinds offeatures that human readers do.
This is whyfrom the beginning of its development, wemade it a priority to use features from thescoring guide and to eliminate any directmeasures of essay length.
Even thoughlength measures can be shown to be highlycorrelated with human reader essay scores,length variables are not scoring guidecriteria (Page and Peterson, 1995).
Thefeatures currently used by the system aresyntactic features, discourse cue words,terms and structures, and topical analysis,specifically, vocabulary usage at the level ofthe essay (big bag of words) and at the levelof the argument.
Argument, in this case,refers generally to the different discussionpoints made by the writer.1.1 Syntactic Structure and SyntacticVarietyThe holistic rubric criteria specify that thesyntactic variety used by a candidate shouldbe considered with regard to essay score.The e-rater system uses an ETS-enhancedversion of the CASS syntactic chunker(Abney (1996)), referred to here as theparser.
The parser identifies severalsyntactic structures in the essay responses,such as subjunctive auxiliary verbs (e.g.,would, should, might), and complex clausalstructures, such as complement, infinitive,and subordinate clauses.
Recognition ofsuch features in an essay yields informationabout its syntactic variety.691.2 Discourse Cues and Organizationof IdeasOrganization of ideas is another criterionthat the scoring guide asks human readers toconsider in assigning essay score.
E-ratercontains a lexicon based on the conceptualframework of conjunctive relations fromQuirk, et al(1985) in which cue terms, suchas "In summary" and "In conclusion," areclassified as conjuncts used forsummarizing.
The conjunct classifierscontain information about whether or notthe item is a kind of discourse developmentterm (e.g., "for example" and "because"), orwhether it is more likely to be used to begina discourse statement (e.g., First, Second, orThird).
E-rater also contains heuristics thatdefine the syntactic or essay-basedstructures in which these terms must appearto be considered as discourse markers.
Forexample, for the word "first" to beconsidered a discourse marker, it must notbe a nominal modifier, as in the sentence,"The first time I went to Europe was in1982," in which "first" modifies the noun"time."
Instead, "first" must occur as anadverbial conjunct to be considered adiscourse marker, as in the sentence, "First,it has often been noted that length is highlycorrelated with essay score."
The cue termlexicon and the associated heuristics areused by e-rater to automatically annotate ahigh-level discourse structure of each essay.These annotations are also used by thesystem to partition each essay into separatearguments which are input to the system'stopical analysis component, describedbelow, for analyzing topical content.1.3 Topical Analysis and VocabularyUsageVocabulary usage is listed as anothercriterion on human reader scoring guides.Good essays are relevant to the assignedtopic.
They also tend to use a morespecialized and precise vocabulary indiscussing the topic than poorer essays do.We should therefore xpect a good essay toresemble other good essays in its choice ofwords and, conversely, a poor essay toresemble other poor ones.
To capture use ofvocabulary or identification of topic, e-rateruses content vector analyses that are basedon the vector-space model commonly foundin information retrieval applications.Training essays are converted into vectorsof word frequencies, and the frequencies arethen transformed into word weights) Theseweight vectors populate the training space.To score a test essay, it is converted into aweight vector, and a search is conducted tofind the training vectors most similar to it,as measured by the cosine between the testand training vectors.
The closest matchesamong the training set are used to assign ascore to the test essay.E-rater uses two different forms of thegeneral procedure sketched above.
In oneform, for looking at topical analysis at theessay level, each of the 270 training essaysis represented by a separate vector in thetraining space.
The score assigned to the testessay is a weighted mean of the scores forthe 6 training essays whose vectors areclosest o the vector of the test essay.
This2 Word (or term) weight reflects not only aword's frequency in the essay but also itsdistribution across essays.
E-rater's formula forthe weight of word w in essay j is:weightwj=(freqw/maxfreqj) * log(nessays/essaysw)where freqwj is the frequency of word w in essayj, maxfreqi is the frequency of the most frequentword in essay j, nessays is the total number oftraining essays, and essaysw is the number oftraining essays that contain w. The first part ofthe formula measures the relative importance ofthe word in the essay.
The second part gauges itsspecificity across essays, so that a word thatappears in many essays will have a lower weightthan one which appears in only a few.
In theextreme case, a word that appears in all essays(e.g., "the") has a weight of 0.70score is computed using the followingformula, rounded to the nearest integer:Score for test essay t =E(cosinetj * scorej)/~ cosinetjwhere j ranges over the 6 closest trainingessays, scorej is the human rater score fortraining essay j, and cosineg is the cosinebetween test essay t and training essayj.The other form of content vector analysisthat the system uses combines all of thetraining essays for each score category andpopulates the training space with just 6"supervectors", one each for scores 1-6.This method is used to evaluate thevocabulary usage at the argument level.
Thetest essay is evaluated one argument at atime.
Each argument is converted into avector of word weights and compared to the6 vectors in the training space.
The closestvector is found and its score is assigned tothe argument.
This process continues untilall the arguments have been assigned ascore.
The overall score for the test essay isan adjusted mean of the argument scoresusing the following formula, rounded to thenearest integer:Score for test essay t =(~argscorej + nargst)l(nargst + 1)where j ranges over the arguments in testessay t, argscorej is the score of argument j,and nargst is the number of arguments in t.Using this adjusted mean has the overalleffect of reducing, slightly, the score foressays with few arguments, and ofincreasing somewhat he score of essayswith many arguments.2.
Model Building and Essay ScoringE-rater builds a new model for each testquestion (prompt).
In pre-operational trials,a set of 270 essays scored by at least twohuman readers has been shown to beoptimal for training.
The distribution at eachscore point in the 270 training essays is asfollows: five O's, fifteen l's, and fifty 2'sthrough 6' s. 3The syntactic, discourse, and topicalanalysis features are identified for each ofthe 270 essays.
Vectors of raw counts ofoccurrences of syntactic and discoursestructure information, and scores generatedfor the two topical analysis components aresubmitted to a stepwise linear regression.For each prompt, the regression selects thesubset of predictive features.
Typically, 8 to12 features are selected.
Although everymodel has a different combination offeatures, in the 75 models that we arecurrently running, the five most frequentlyoccurring features are: 1) the topicalanalysis score by argument, 2) the topicalanalysis score by essay, 3) the number ofsubjunctive auxiliary words, 4) the ratio ofsubjunctive auxiliary words to total wordsin the essay, and 5) the total number ofargument development terms.The coefficient weightings for each of thepredictive features generated from theregression for each prompt are then used toscore new essays for that prompt.3.
E-rater Agreement Performance onNonnative Speaker DataSome questions that will now be addressedin looking at e-rater system performance onnonnative speaker essay data are: (1) Howdoes performance for nonnative speakers onTWE compare with performance inoperational sconng?
(2) How does thesystem's agreement with human readersdiffer for each of the language groups in this3 To date, this training sample composition hasgiven us the best cross-validation results.
Someprevious studies experimenting with smallertraining samples with this fairly flat distribution,or samples which reflect more directly the naturaldistribution of the data at each score point haveshown lower performance in scoring cross-validation sets of 500 - 900 essays.71study?
(3) How does e-rater 's  agreementwith human readers differ for the nonnativespeaker language groups as compared to theEnglish speaking language groups?
(4) Isthere a significant difference between thefeatures used most often in models foroperational prompts as compared to theTWE prompts?3.1 Data sampleFor this study, two prompts from the Test ofWritten English were used.
These prompts(TWE1 and TWE2) ask candidates to readand think about a statement, and then toagree or disagree with the statement, and togive reasons to support he opinion given bythe candidate.
The scoring guides for theseessays have a 6-point scale, where a "6" isthe highest score and a "1" is the lowestscore.
They are holistic guides, though thecriteria are more generally stated than in thescoring guides used to build e-rater.For each of the prompts a total of 255essays were used for training.
Fifty trainingessays were randomly selected from each ofthe score categories 2-6.
Because of thesmall number of essays with a score of 1,only five l 's were included in each trainingset.
The remainder of the essays were usedfor cross-validation purposes.4.
ResultsTables 1-3 show overall and languagespecific scoring results for TWE1 andTWE2 cross-validation data.
The data arepresented in terms of mean score and also aspercent agreement between e-rater  andhuman readers, where agreement is definedas exactly matching or adjacent scores onthe 6-point scale.
In previous studies ofholistically scored essays (Burstein, et al(1998a, 1998b and 1998c)), we haveexamined e-rater 's  agreement with twoindividual human readers.
For these TWEdata, only a final human reader score(labeled GDF in the Tables) was available.The final score reflects the average of twoor three human reader scores.
A thirdhuman reader is typically used if the firsttwo humans disagree by more that a singlepoint.
For the operational essay data, themean agreement between e-rater  and thefinal human reader score is 90%, about thesame as the mean agreement between twoindividual human readers at about 92%.
4For the same data, Pearson correlationsbetween e-rater  and final human readerscores, and between two human readers areabout the same at .75.
Table 1 shows that,for TWE essays, overall e-rater  agreementwith the human reader final score is high.The values are comparable to those for theoperational essays although the correlationsare somewhat lower.4 Baseline agreement for the TWE data isapproximately 84%.
This is determined bycalculating agreement if he most common score,"4", is assigned to all essays.
Using the sametechnique for GMAT essays howed baselineagreement to be about 83%.72Table 1: Comparison of Human Readers Final Score (GDF) & e-rater Score (E) Over AllLanguage Groups in TWE1 and TWE2Prompt n=TWE1 562TWE2 576Mean%Agreement Pearson r GDF E(Exact+AdJacent) Score ScoreMean S.D.
Mean S.D.91.1 .667 4.16 .974 4.08 1.04193.4 .718 4.16 .936 4.07 .98992.3 .693 4.16 .955 4.08 1.015An analysis of variance was performed onthe essay scores, using Reader (GDF, E) asa within factor and Prompt (TWE1, TWE2)and Language Group as between factors.Although small, the difference in meanscore between GDFand e-rater was statistically significant(F(1,u28) = 5.469, p < .05).
There was nosignificant main effect for Prompt, and nointeractions between Prompt and the otherfactors.
Tables 2 and 3 show the results forTWE1 and TWE2 by Language Group andReader.Table 2: Comparison of Human Readers Final Score (GDF) & e.rater Score (E) ByLanguage Groups in TWE1LanguageGroupArabicChineseSpanishUS-EnglishNon-USEnglishr l=1461531319735%Agreement(Exact+Adjacent)Pearson r GDFScoreEScoreMean S.D.
Mean S.D.89.0 .645 3.83 .973 3.67 .94788.2 .543 4.09 .884 4.12 1.0092.4 .644 3.96 .986 3.70 .91596.9 .632 4.96 .624 4.93 .81491.4 .544 4.31 .900 4.51 .981Table 3: Comparison of Human Readers Final Score (GDF) & e.rater Score (E) ByLanguage Groups in TWE2LanguageGroupArabicChineseSpanishUS-EnglishNon-USEnglishn = %Agreement(Exact+Adjacent)151 96.4139 91..0138 93.5103 92.045 93.3Pearson r.783.707.616.519.465GDF EScore ScoreMean S.D.
Mean S.D.3.85 .959 3.70 .9093.92 .957 4.04 1.034.07 .845 3.69 .7334.83 .613 4.95 .7594.68 .732 4.60 .780The main effect for Language Group wassignificant (F(4,1128) = 76.561, p < .001).
Asexpected, the two English groups scoredsubstantially higher than the nonnativespeakers.
Finally, the interaction ofLanguage Group by Reader was also73significant (F(4,H28) = 12.397, p < .001),reflecting higher scores for GDF than for e-rater in some groups (e.g., Spanish) andlower scores for GDF than for e-rater inothers (e.g., Chinese).Despite the score differences, Z2 analysesshowed no significant differences on theAgreement measure for Language Group ineither TWE1 Or TWE2.
There-was howeveran effect of Prompt in the analysis ofAgreement for Arabic speakers, whereAgreement levels in TWE1 and TWE2 weresignificantly different (Z2(1) = 6.607, p <.01); no other group differences inAgreement were found between the twoprompts.5.
Discussion and ConclusionsIn this study we have evaluated theperformance and effects of e-rater on twosets of nonnative speaker essay responses,approximately 1100 essays.
The resultsshow that overall system performance isquite good and highly comparable to resultsfor scoring the primarily native speaker datafound in operational essays.
The models thate-rater built to score TWE1 and TWE2contain 7 or 8 features, and these includesyntactic, discourse and topical analysisfeatures.
Importantly, at least 4 of the top 5features that are included in the currentoperational models also appear in themodels for TWE1 and TWE2.
It is useful toknow that even when 75% of essays usedfor model building were written bynonnative English speakers (as in thisstudy), the features selected by theregression procedure were largely the sameas those in models based on operationalwriting samples in which the majority of thesample were native English speakers.
Thissuggests that the features that the systemconsiders are generalizable from nativespeaker writing to nonnative speakerwriting.
Further, e-rater was notconfounded by non-standard Englishsyntactic structures or stylistic discoursestructures, which one might expect o be aproblem for a system designed to evaluatenative speaker writing.Although there were significant differencesbetween final human reader score and e-rater score across language groups, inabsolute terms the differences were small(only a fraction of a score point) and did notproduce significant differences inagreement.
For one group, prompt made adifference.
It would be useful to analyze theessays in more detail to see what featuresare responsible for the score variations andhow essay topic might explain anydifferences due to prompt.
We are currentlyinvestigating the use of tree-basedregression models to supplement linearregression (Sheehan, 1997).
Preliminaryanalyses of tree-based regressions, however,do not show an improvement in e-raterperformance.
This may be explained by thefact that the most predictive features in e-rater are linearly related to score.In future studies, we will have sufficientdata to build individual models for differentlanguage groups to examine how this affectse-rater's performance.
In addition, we hopeto learn about how building language-specific models can be used for automatedgeneration of diagnostic and instructionalfeedback -- perhaps customized for differentlanguage groups.ReferencesAbney, Steven.
(1996) Part-of-speechtagging and partial parsing.
In Church,Young and Bloothooft (eds), Corpus-basedMethods in Language and Speech.Dordrecht: Kluwer.Biber, D. (1993) Using register-diversifiedcorpora for general language studies.Computational Linguistics, 19, 219-241.Breland, Bonner and Kubota (1995), Factorsin Performance on Brief, Impromptu Essay74Examinations, College Board Report No.95 -4.Bridgeman and McHale (1996).
Gender andEthnic Group Differences on the GMATAnalytical Writing Assessment.
ETS RR-96-2.Burstein, Jill, Karen Kukich, SusanneWolff, Chi Lu, Martin Chodorow, LisaBraden-Harder, and Mary Dee Harris(1998).
Automated Scoring Using A HybridFeature Identification Technique.
In theProceedings of the Annual Meeting of theAsSociation of Computational Linguistics,August, 1998.
Montreal, Canada.Burstein, Jill, Karen Kukich, SusanneWolff, Chi Lu, & Martin Chodorow (1998).Enriching Automated Scoring UsingDiscourse Marking.
In the Proceedings ofthe Workshop on Discourse Relations &Discourse Marking, Annual Meeting of theAssociation of Computational Linguistics,August, 1998.
Montreal, Canada.Burstein, Jill C., Lisa Braden-Harder,Martin Chodorow, Shuyi Hua, BruceKaplan, Karen Kukich, Chi Lu, JamesNolan, Don Rock and Susanne Wolff(1998).
Computer Analysis of EssayContent for Automated Score Prediction.ETS RR 98-15.Foltz, P. W., W. Kintsch, and T. K.Landauer.
(1998).
The Measurement ofTextual Coherence with Latent SemanticAnalysis.
Discourse Processes, 25(2&3),285-307.Frase, L. T., Faletti, J., Ginther, A., &Grant, L. (1997).
Computer Analysis of theTOEFL Test of Written English (TWE).Educational Testing Service, Princeton, NJ.Larkey, L. (1998).
Automatic Essay GradingUsing Text Categorization Techniques.Proceedings of the 21 st A CM-SIGIRConference on Research and Developmentin Information Retrieval, Melbourne,Australia, 90-95.Page, E. B. and N. Petersen.
(1995).
Thecomputer moves into essay grading:updating the ancient test.
Phi Delta Kappan.March, 561-565.Sheehan, K (1997).
A Tree-Based Approachto Proficiency Scaling and DiagnosticAssessment.
Journal of EducationalMeasurement.
34(4), 333-352.AcknowledgementsWe owe considerable thanks to KarenKukich, who designed the architecture forthe operational version of e-rater, and wholed the team at ETS that refined andimplemented the production version of thesystem.
The production version makes theprocess of continued research in automatedscoring much smoother.
Thanks to SusanneWolff for her invaluable collaboration onthe e-rater system from its inception.
Weare also grateful to Steve Abney forallowing us to use CASS in the operationalversion of e-rater; to Claudia Leacock forleading the design and development effort tocreate the ETS-enhanced version of CASS;to Thomas Morton and Hoa Trang Dang forthe implementation of the ETS-enhancedversion of CASS; and to Daniel Zuckerman,who made it possible to run CASS on bothUnix and NT platforms.
We thank Chi Lufor implementations and re-implementationsof statistical programming embedded in e-rater, and Magdalena Wolska for the workshe did implementing tools that allowed usto conduct his study with ease.
Finally, weare grateful to ETS test developers for all ofthe dialogue that has helped us understandthe educational direction of automated essayscoring, and all the people at ETS andGMAC who supported this research from itsinception.75
