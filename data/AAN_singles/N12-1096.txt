2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 783?792,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsShared Components Topic ModelsMatthew R. Gormley Mark Dredze Benjamin Van Durme Jason EisnerCenter for Language and Speech ProcessingHuman Language Technology Center of ExcellenceDepartment of Computer ScienceJohns Hopkins University, Baltimore, MD{mrg,mdredze,vandurme,jason}@cs.jhu.eduAbstractWith a few exceptions, extensions to latentDirichlet alocation (LDA) have focused onthe distribution over topics for each document.Much less attention has been given to the un-derlying structure of the topics themselves.
Asa result, most topic models generate topics in-dependently from a single underlying distri-bution and require millions of parameters, inthe form of multinomial distributions over thevocabulary.
In this paper, we introduce theShared Components Topic Model (SCTM), inwhich each topic is a normalized product of asmaller number of underlying component dis-tributions.
Our model learns these componentdistributions and the structure of how to com-bine subsets of them into topics.
The SCTMcan represent topics in a much more compactrepresentation than LDA and achieves betterperplexity with fewer parameters.1 IntroductionTopic models are probabilistic graphical modelsmeant to capture the semantic associations underly-ing corpora.
Since the introduction of latent Dirich-let alocation (LDA) (Blei et al, 2003), these mod-els have been extended to account for more complexdistributions over topics, such as adding supervision(Blei and McAuliffe, 2007), non-parametric priors(Blei et al, 2004; Teh et al, 2006), topic correla-tions (Li and McCallum, 2006; Mimno et al, 2007;Blei and Lafferty, 2006) and sparsity (Williamson etal., 2010; Eisenstein et al, 2011).While much research has focused on modelingdistributions over topics, less focus has been given tothe makeup of the topics themselves.
This emphasisleads us to find two problems with LDA and its vari-ants mentioned above: (1) independently generatedtopics and (2) overparameterized models.Independent Topics In the models above, the top-ics are modeled as independent draws from a singleunderlying distribution, typically a Dirichlet.
Thisviolates the topic modeling community?s intuitionthat these distributions over words are often related.As an example, consider a corpus that supports tworelated topics, baseball and hockey.
These topicslikely overlap in their allocation of mass to highprobability words (e.g.
team, season, game, play-ers), even though the two topics are unlikely to ap-pear in the same documents.
When topics are gen-erated independently, the model does not provide away to capture this sharing between related topics.Many extensions to LDA have addressed a relatedissue, LDA?s inability to model topic correlation,1by changing the distributions over topics (Blei andLafferty, 2006; Li and McCallum, 2006; Mimno etal., 2007; Paisley et al, 2011).
Yet, none of thesechange the underlying structure of the topic?s distri-butions over words.Overparameterization Topics are most oftenparameterized as multinomial distributions overwords: increasing the topics means learning newmultinomials over large vocabularies, resulting inmodels consisting of millions of parameters.
Thisissue was partially addressed in SAGE (Eisensteinet al, 2011) by encouraging sparsity in the topicswhich are parameterized by their difference in log-frequencies from a fixed background distribution.Yet the problem of overparameterization is also tied1Two correlated topics, e.g.
nutrition and exercise, are likelyto co-occur, but their word distributions might not overlap.783to the number of topics, and though SAGE reducesthe number of non-zero parameters, it still requiresa vocabulary-sized parameter vector for each topic.We present the Shared Components Topic Model(SCTM), which addresses both of these issues bygenerating each topic as a normalized product of asmaller number of underlying components.
Ratherthan learning each new topic from scratch, we modela set of underlying component distributions thatconstrain topic formation.
Each topic can then beviewed as a combination of these underlying com-ponents, where in a model such as LDA, we wouldsay that components and topics stand in a one to onerelationship.
The key advantages of the SCTM arethat it can learn and share structure between overlap-ping topics (e.g.
baseball and hockey) and that it canrepresent the same number of topics in a much morecompact representation, with far fewer parameters.Because the topics are products of components,we present a new training algorithm for the sig-nificantly more complex product case which re-lies on a Contrastive Divergence (CD) objective.Since SCTM topics, which are products of distri-butions, could be represented directly by distribu-tions as in LDA, our goal is not necessarily to learnbetter topics, but to learn models that are substan-tially smaller in size and generalize better to unseendata.
Experiments on two corpora show that ourmodel uses fewer underlying multinomials and stillachieves lower perplexity than LDA, which suggeststhat these constraints could lead to better topics.2 Shared Components Topic ModelsThe Shared Components Topic Model (SCTM) fol-lows previous topic models in inducing admixturedistributions of topics that are used to generate eachdocument.
However, here each topic multinomialdistribution over words itself results from a normal-ized product of shared components, each a multino-mial over words.
Each topic selects a subset of com-ponents.
We begin with a review and then introducethe SCTM.Latent Dirichlet alocation (LDA) (Blei et al,2003) is a probabilistic topic model which definesa generative process whereby sets of observationsare generated from latent topic distributions.
In theSCTM, we use the same generative process of topicassignments as LDA, but replace the K indepen-dently generated topics (multinomials over words)with products of C components.Latent Dirichlet alocation generative processFor each topic k ?
{1, .
.
.
,K}:?k ?
Dir(?)
[draw distribution over words]For each document m ?
{1, .
.
.
,M}:?m ?
Dir(?)
[draw distribution over topics]For each word n ?
{1, .
.
.
, Nm}:zmn ?
Mult(1,?m) [draw topic]xmn ?
?zmi [draw word]LDA draws each topic ?k independently from aDirichlet.
The model generates each document mof length M , by first sampling a distribution overtopics ?m.
Then, for each word n, a topic zmn ischosen and a word type xmn is generated from thattopic?s distribution over words ?zmi .A Product of Experts (PoE) model (Hinton,1999) is the normalized product of the expert dis-tributions.
In the SCTM, each component (an ex-pert) models an underlying multinomial word dis-tribution.
We let ?c be the parameters of the cthcomponent, where ?cv is the probability of the cthcomponent generating word v. If the structure of aPoE included only components c ?
C in the prod-uct, it would have the form: p(x|?1, .
.
.
,?C) =Qc?C ?cxPVv=1Qc?C ?cv, where there are C components, andthe summation in the denominator is over the vocab-ulary.
In a PoE, each component can overrule theothers by giving low probability to some word.
APoE can be viewed as a soft intersection of its com-ponents, whereas a mixture is a soft union.The Beta-Bernoulli model (Griffiths andGhahramani, 2006) is a distribution over binarymatrices with a fixed number of rows and columns.It is the finite counterpart to the Indian BuffetProcess.
In this work, we use the Beta-Bernoulli asour prior for an unobserved binary matrix B with Ccolumns and K rows.
In the SCTM, each row bk ofthe matrix, a binary feature vector, defines a topicdistribution.
The binary vector acts as a selectorfor the structure of the PoE for that topic.
The rowdetermines which components to include in theproduct by which entries bkc are ?on?
(equal to 1)in that row.
Under Beta-Bernoulli prior, for eachcolumn, a coin with weight pic is chosen.
For eachentry in the column, the coin is flipped to determineif the entry is ?on?
or ?off?.
This corresponds to784the notion that some components are a priori morelikely to be included in topics.The Beta-Bernoulli model generative processFor each component c ?
{1, .
.
.
, C}: [columns]pic ?
Beta(?C , 1) [draw probability of component c]For each topic k ?
{1, .
.
.
,K}: [rows]bkc ?
Bernoulli(pic) [draw whether topic includes cthcomponent in its PoE]2.1 Shared Components Topic ModelsThe Shared Components Topic Model generateseach document just like LDA, the only differenceis the topics are not drawn independently from aDirichlet prior.
Instead, topics are soft intersectionsof underlying components, each of which is a multi-nomial distribution over words.
These componentsare combined via a PoE model, and each topic isconstructed according to a length C binary vectorbk; where bkc = 1 includes and bkc = 0 excludescomponent c. Stacking theK vectors forms aK?Cmatrix; rows correspond to topics and columns tocomponents.
Overlapping topics share componentsin common.Generative process SCTM?s generative processgenerates topics and words, but must also generatethe binary matrix.
For each of the C shared com-ponents, we generate a distribution ?c over the Vwords from a Dirichlet parametrized by ?.
Next,we generate a K ?
C binary matrix using the Beta-Bernoulli prior.
These components and the binarymatrix implicitly define the complete set of K topicdistributions, each of which is a PoE.p(x|bk,?)
=?Cc=1 ?bkccx?Vv=1?Cc=1 ?bkccv(1)The distribution p(?|bk,?)
defines the kth topic.Conditioned on these K topics, the remainder of thegenerative process, which generates the documents,is just like LDA.The Shared Components Topic Model generative processFor each component c ?
{1, .
.
.
, C}:?c ?
Dir(?)
[draw distribution over words]pic ?
Beta(?C , 1) [draw probability of component c]For each topic k ?
{1, .
.
.
,K}:bkc ?
Bernoulli(pic) [draw whether topic includes cthcomponent in its PoE]For each document m ?
{1, .
.
.
,M}?m ?
Dir(?)
[draw distribution over topics]For each word n ?
{1, .
.
.
, Nm}zmn ?
Mult(1,?m) [draw topic]xmn ?
p(?
|bzmn ,?)
given by Eq.
(1) [draw word]See Figure 1 for the graphical model.Discussion An advantage of this formulation is theability to model many topics using few components.While LDA must maintain V ?K parameters for thetopic distributions, the SCTM maintains just V ?Cparameters, plus an additional K?C binary matrix.Since C < K  V this results in many fewer pa-rameters for the SCTM.2 Extending the number oftopics (rows) requires storing additional binary vec-tors, a lightweight requirement.
In theory, we couldenable all 2C possible component combinations, al-though we expect to use far less.
On the other hand,constraining the SCTM?s topics by the componentsgives less flexible topics as compared to LDA.
How-ever, we find empirically that a large number of top-ics can be effectively modeled with a smaller num-ber of components.Observe that we can reparameterize the SCTM asLDA by assuming an identity square matrix; eachcomponent corresponds to a topic in LDA, makingLDA a special case of the SCTM with an identitymatrix IC .
Intuitively, SCTM learning could pro-duce an LDA model where appropriate.
Finally, wecan also think of the SCTM as learning the struc-ture of many PoE models.
In applications where ex-perts abstain, the SCTM could learn in which setting(row) each expert casts a vote.3 Parameter EstimationParameter estimation infers values for model pa-rameters ?, pi, and ?
from data using an unsuper-vised training procedure.
Because exact inferenceis intractable in the SCTM, we turn to approximatemethods.
As is common in these models, we willintegrate out pi and ?, sample latent variables Z andB, and optimize the components ?.
Our algorithmfollows the outline of the Monte Carlo EM (MCEM)algorithm (Wei and Tanner, 1990).
In the MonteCarlo E-step, we will re-sample the latent variablesZ and B based on current model parameters ?
andobserved data X .
In the M-step, we will find newmodel parameters ?.
Since these parameters corre-spond to experts in the PoE, we rely on a contrastivedivergence (CD) objective (Hinton, 2002), popularfor PoE training, rather than maximizing the data2The vocabulary size V could be much larger if n-grams orrelational triples are used, as opposed to unigrams.785log-likelihood.
Normally, CD only estimates the pa-rameters of the expert distributions.
However, in ourmodel, the structure of the PoEs themselves changebased on the E-step.
Since we generate multiplesamples in the E-step, we modify the CD objectiveto compute the gradient for each E-step sample andtake the average to approximate the expectation un-der B and Z.33.1 E-StepThe E-step approximates an expectation underp(B,Z|X,?,?, ?)
for latent topic assignments Zand matrix B using Gibbs sampling.
The Gibbssampler uses the full conditionals for both zi (7) andbkc (12), which we derive in Appendix A.
Using thissampler, we obtain J samples of Z and B by iterat-ing through each value of zi and bkc J times (in ourexperiments, we use J=1, which appears to work aswell on this task as multiple samples).
These J sam-ples are then used in the M-step as an approximationof the expectation of the latent variables.3.2 M-StepGiven many samples of B and Z, the M-step opti-mizes the component parameters ?
which cannot becollapsed out.
We utilize the standard PoE trainingprocedure for experts: contrastive divergence (CD).We approximate the CD gradient as the difference ofthe data distribution and the one-step reconstructionof the data according to the current parameters.
Asin Generalized EM (Dempster et al, 1977), a singlegradient step in the direction of the contrastive di-vergence objective is sufficient for each M-step.
Akey difference in our model is that we must incor-porate the expectation of the PoE model structure,which in our case is a random variable instead of afixed observed structure.
We achieve this by simply3CD training within MCEM is not the only possible ap-proach.
One alternative would be to compute the CD gradientsumming over all values of B and Z, effectively training theentire model using CD.
This approach prevents the normal CDobjective derivation from being simplified into a more tractableform.
Another approach would be a pure MCMC algorithm,which sampled ?
directly.
While using the natural parametersallows the sampler to mix, it is too computationally intensive tobe practical.
Finally, we could train with Generalized MCEM,where the exact gradient of the log-likelihood (or log-posterior)is used, but this easily gets stuck in local minima.
After exper-imenting with these and other options, we present our currentmost effective estimation method.computing the CD gradient for each PoE given eachof the J samples {Z,B}(j) from the E-Step, thenaverage the result.Another difficulty arises from computing the gra-dient directly for the multinomial?c due to the V ?1degrees of freedom imposed by sum-to-one con-straints.
Therefore, we switch to the natural pa-rameters, which obviates the need for consideringthe sum-to-one constraint in the optimization, bydefining ?c in terms of V real valued parameters{?c1, .
.
.
, ?cV }:?cv =exp(?cv)?Vt=1 exp(?cv)(2)The V parameters ?cv are then used to compute ?cvfor use in the E-step.As explained above, the M-step does not maxi-mize the data log-likelihood, but instead minimizescontrastive divergence.
Hinton (2002) explains thatmaximizing data log-likelihood is equivalent to min-imizing Q0||Q??
, the KL divergence between theobserved data distribution, Q0, and the model?sequilibrium distribution,Q??
.4 MinimizingQ0||Q?
?would require the computation of an intractable ex-pectation under the equilibrium distribution.
Weavoid this by instead minimizing the contrastive di-vergence objective,CD(?|{Z,B}(j)) = Q0||Q??
?Q1?
||Q??
, (3)where Q1?
is the distribution over one-step recon-structions of the data, X given Z,B, ?, that are gen-erated by a single step of Gibbs sampling.Unlike standard applications of CD training, thehidden variables (Z,B) are not contained within theexperts.
Instead they define the structure of the PoEmodel, where B indicates which experts to use ineach product (topic) andZ indicates which PoE gen-erates each word.
Unfortunately, CD training cannotinfer this structure since the CD derivation makesuse of a fixed structure in the one-step reconstruc-tion.
Therefore, we have taken a MCEM approach,first sampling the PoE structure in the E-step, then4Hinton (2002) used this notation because the data distribu-tion,Q0, can be described as the state of a Markov chain at time0 that was started at the data distribution.
Similarly, the equilib-rium distribution, Q??
could be obtained by running the sameMarkov chain to time?.786MNmCKxmnzmn?m??cbkcpic?
?Figure 1: The graphical model for the SCTM.fixing these samples for Z and B when computingthe one-step reconstruction of the data, X .Contrastive Divergence Gradient We providethe approximate derivative of the contrastive di-vergence objective, where Z and B are treated asfixed.5dCD(?|{Z,B}(j))d??
?
?d log f(x|bz, ?)d?
?Q0+?d log f(x|bz, ?)d?
?Q1?where f(x|bz, ?)
=?Cc=1 ?bzccx is the numerator ofp(x|bz, ?)
and the derivative of its log is efficient tocompute:d log f(x|bz, ?)d?cv={bzc(1?
?cv) for x = v?bzc?cv for x 6= vTo approximate the expectation under Q1?
, we holdZ,B, ?
fixed and resample the data, X , using onestep of Gibbs sampling.3.3 SummaryOur learning algorithm can be viewedin terms of a Q function: Q(?|?
(t)) ?1J?Jj=1 CD(?|{Z,B}(j))where we average overJ samples.
The E-step computes Q(?|?(t)).
TheM-step minimizes Q with respect to ?
to obtain theupdated ?
(t+1) by performing gradient descent onthe Q function as ?
(t+1)cv = ?
(t)cv ?
?
?dQ(?|?
(t))d?cvforall values of c, v.5The derivative is approximate because we drop the term:?dQ1?d?
?dQ1?||Q?
?dQ1?, which is ?problematic to compute?
(Hinton,2002).
This is the standard use of CD.Algorithm 1 SCTM TrainingInitialize parameters: ?c, bkc, zi.while not converged do{E-step:}for j = 1 to J do{Draw jth sample {Z,B}(j)}for i = 1 to N doSample zi using Eq.
(7)for k = 1 to K dofor c = 1 to C doSample bkc using ratio in Eq.
(12){M-step:}for c = 1 to C dofor v = 1 to V doSingle gradient step over ??
(t+1)cv = ?
(t)cv ?
?
?dQ(?|?
(t))d?cv4 Related ModelsThe SCTM is closely related to the the InfiniteOverlapping Mixture Model (IOMM) (Heller andGhahramani, 2007), yet our model differs from and,in some ways, extends theirs.
The IOMM mod-els the geometric overlap of Gaussian clusters us-ing PoEs, and models the structure of the PoEs withthe rows of a binary matrix.
The SCTM models afinite number of columns, where the IOMM mod-els an infinite number.
The IOMM generates a rowfor each data point, whereas the SCTM generates arow for each topic.
Thus, the SCTM goes beyondthe IOMM by allowing the rows to be shared amongdocuments and models document-specific mixturesover the rows of the matrix.6SAGE for topic modeling (Eisenstein et al, 2011)can be viewed as a restricted form of the SCTM.Consider an SCTM in which the binary matrix is re-stricted such that the first column, b?,1, consists ofall ones and the remainder forms a diagonal matrix.If we then set the first component, ?1, to the cor-pus background distribution, and add a Laplace prioron the natural parameters, ?cv, we have the SAGEmodel.
Note that by removing the restriction thatthe matrix contain a diagonal, we could allow mul-tiple components to combine in the SCTM fashion,while incorporating SAGE?s sparsity benefits.6The IOMM uses Metropolis-Hastings (MH) to sample theparameters of the experts.
This approach is computationallyfeasible because their experts are Gaussian, unlike the SCTMin which the experts are multinomials and the MH step too ex-pensive.787The relation of TagLDA (Zhu et al, 2006) tothe SCTM is similar to that of SAGE and SCTM.TagLDA has a PoE of exactly two experts: one ex-pert for the topic, and one for the supervised word-level tag.
Examples of tags are abstract or body,indicating which part of a research paper the wordappears in.Unlike the SCTM and SAGE, most prior exten-sions to LDA have enhanced the distribution overtopics for each document.
One of the closest is hier-archical LDA (hLDA) (Blei et al, 2004) and its ap-plication to PAM (Mimno et al, 2007).
Though top-ics are still generated independently from a Dirich-let prior, hLDA learns a tree structure underlyingthe topics.
Each document samples a single paththrough the tree and samples words from topicsalong that path.
The SCTM models an orthogonalissue to topic hierarchy: how the topics themselvesare represented as the intersection of components.Finally, while prior work has primarily used mix-tures for the sake of conjugacy, we take a fundamen-tally different approach to modeling the structure byusing normalized product distributions.5 EvaluationWe compare the SCTM with LDA in terms of over-all model performance (held-out perplexity) as wellas parameter usage (varying numbers of componentsand topics).
We select LDA as our baseline since ourmodel differs only in how it forms topics, which fo-cuses evaluation on the benefit of this model change.We consider two popular data sets for compar-ison: NIPS: A collection of 1,617 NIPS abstractsfrom 1987 to 19997, with 77,952 tokens and 1,632types.
20NEWS: 1,000 randomly selected articlesfrom the 20 Newsgroups dataset,8 with 70,011 to-kens and 1,722 types.
Both data sets excluded stopwords and words occurring in fewer than 10 docu-ments.
For 20NEWS, we used the standard by-datetrain/test split.
For NIPS, we randomly partitionedthe data by document into 75% train and 25% test.We compare the SCTM to LDA by evaluatingthe average perplexity-per-word of the held-out test7We follow prior work (Blei et al, 2004; Li and Mc-Callum, 2006; Li et al, 2007) in using only the abstracts:http://www.cs.nyu.edu/?roweis/data.html8Williamson et al (2010) created a similar subset:http://people.csail.mit.edu/jrennie/20Newsgroups/data, perplexity = 2?
log2(data|model)/N .
Exact com-putation is intractable, so we use the left-to-right al-gorithm (Wallach et al, 2009) as an accurate alter-native.
With the topics fixed, the SCTM is equiva-lent to LDA and requires no adaptation of the left-to-right algorithm.We used a collapsed Gibbs sampler for trainingLDA and the algorithm described above for trainingthe SCTM.
Both were trained for 4000 iterations,sampling topics every 10 iterations after a burn-in of3000.
The hyperparameter ?
was optimized as anasymmetric Dirichlet, ?
as a symmetric Dirichlet,and ?
= 3.0 was fixed.9 Following the observation ofHinton (2002) that CD training benefits from initial-izing the experts to nearly uniform distributions, weinitialize the component distributions from a sym-metric Dirichlet with parameter ??
= 1?106.
We useJ = 1 samples per iteration and a decaying learningrate centered at ?
= 100.10 We ranged LDA from 10to 200 topics, and the SCTM from 10 to 100 com-ponents (C).
We then selected the number of SCTMtopics (K) as K ?
{C, 2C, 3C, 4C, 5C}.
For eachmodel, we used five random restarts, selecting themodel with the highest training data likelihood.5.1 ResultsOur goal is to demonstrate that (1) modeling topicsas products of components is an expressive alterna-tive to generating topics independently and (2) theSCTM can both achieve lower perplexity than LDAand use fewer model parameters in doing so.Topics as Products of Components Figures 3band 3c show the perplexity for the held-out portionsof 20NEWS and NIPS for different numbers of com-ponents C. The shaded region shows the full SCTMperplexity range we observed for different K andat each value of C, we label the number of topicsK (rows in the binary matrix).
For each number ofcomponents, LDA falls within the upper portion ofthe shaded region.
While for some (small) values ofK for the SCTM, LDA does better, the SCTM caneasily include more K (requiring few new param-eters) to achieve better results.
This supports ourhypothesis that topics can be comprised of the over-lap between shared underlying components.
More-9On development data the model was rather insensitive to ?.10We experimented with larger J but it had no effect.788Figure 2: SCTM binary matrix and topics from 3599 training documents of 20NEWS for C = 10, K = 20.
Bluesquares are ?on?
(equal to 1).xy51015202 4 6 8 10k ?k Top words for topic Top words for topic after ablating component c=1?
1 0.306 subject organization israel return define law org organization subject israel law peace define israeli?
2 0.031 encryption chip clipper keys des escrow security law administration president year market money senior?
3 0.025 turkish armenian armenians war turkey turks armenia years food center year air russian war army?
4 0.102 drive card disk scsi hard controller mac drives opinions drive hard power support cost research price?
5 0.071 image jpeg window display code gif color mit pitt file program year center programs image division?
6 0.018 jews israeli jewish arab peace land war arabs?
7 0.074 org money back question years thing things point?
8 0.106 christian bible church question christ christians life?
9 0.011 administration president year market money senior?
10 0.055 health medical center research information april?
11 0.063 gun law state guns control bill rights states?
12 0.160 world organization system israel state usa cwru reply?
13 0.042 space nasa gov launch power wire ground air?
14 0.038 space nasa gov launch power wire ground air?
15 0.079 team game year play games season players hockey?
16 0.158 car lines dod bike good uiuc sun cars?
17 0.136 windows file government key jesus system program?
18 0.122 article writes center page harvard virginia research?
19 0.017 max output access digex int entry col line?
20 0.380 lines people don university posting host nntp time # of Model Parameters (thousands)Perplexity800100012001400lllllllll1010011120 1402014060 8010,2010,3010,4010,50100,200100,300100,400100,50020,10020,4020,6020,8040,12040,16040,20040,8060,12060,18060,24060,30080,16080,24080,32080,4000 100 200 300 400 500 600l LDASCTM(a)# of ComponentsPerplexity80010001200140016001800llllllll102030405010020030040050010020406080120160200408012018024030060160240320400800 20 40 60 80 100l LDASCTM(b)# of ComponentsPerplexity300400500600700llllllll1020304050100200340050010020406080120160200408012018024030060160240320400800 20 40 60 80 100l LDASCTM(c)# of Model Parameters (thousands)Perplexity300350400450500550600llll l l lllllll1010011120 140 160 180202002140608010,2010,3010,4010,50100,200100,300100,400100,50020,10020,4020,6020,8040,12040,16040,20040,8060,12060,18060,24060,3 080,16080,24080,32080,4000 100 200 300 400l LDASCTM(d)Figure 3: Perplexity results on held-out data for 20NEWS (b) and NIPS (c) showing the results of LDA and the SCTMfor the same number of components and varying K (SCTM).
For the same number of components (multinomials), theSCTM achieves lower perplexity by combining them into more topics.
Results for 20NEWS (a) and NIPS (d) showingnon-square SCTM achieves lower perplexity than LDA with a more compact model.over, this suggests that our products (PoEs) provideadditional and complementary expressivity over justmixtures of topics.Model Compactness Including an additionaltopic in the SCTM only adds C binary parameters,for an extra row in the matrix.
Whereas in LDA, anadditional topic requires V (the size of the vocab-ulary) additional parameters to represent the multi-nomial.
In both cases, the number of document-specific parameters must increase as well.
Figures3a and 3d present held-out perplexity vs. numberof model parameters on 20NEWS and NIPS, exclud-ing the case of square (C = K) binary matrices forthe SCTM.
The regions show a confidence inter-val (p = 0.05) around the smoothed fit to the data,LDA labels show C, and SCTM labels show C,K.The SCTM achieves lower perplexity with fewermodel parameters, even when the increase in non-component parameters is taken into account.
We ex-pect that because of its smaller size the SCTM ex-hibits lower sample complexity, allowing for bettergeneralization to unseen data.5.2 AnalysisFigure 2 gives the binary matrix and topics learnedon a larger section of 20NEWS training documents.These topics evidence that the SCTM is able toachieve a diversity of topics by combining varioussubsets of components, and we expect that the lowperplexity achieved by the SCTM can be attributed789k=12 ?k=0.13problem statecontrolreinforcementproblems modelstime baseddecision markovsystems functionk=11 ?k=0.08learningnetworks systemrecognition timenetworkdescribes handcontext viewsclassificationk=14 ?k=0.07models imagesimage problemstructureanalysis mixtureclusteringapproach showcomputationalk=13 ?k=0.05networksnetwork learningdistributedsystem weightvectors propertybinary pointoptimal realk=16 ?k=0.11training unitspaper hiddennumber outputproblem rule setorder unit showpresent methodweights taskk=15 ?k=0.12cells neuronsvisual cortexmotion responseprocessingspatial cellpropertiespatterns spikek=18 ?k=0.07informationanalysiscomponent rulessignalindependentrepresentationsnoise basisk=17 ?k=0.10numberfunctionsweights functionlayergeneralizationerror resultsloss linear sizek=20 ?k=0.02time networkweightsactivation delaycurrent chaoticconnecteddiscreteconnectionsk=19 ?k=0.03system networksset neuronsvisual phasefeatureprocessingfeatures outputassociativec=1modelinformationparameterskalman robustmatriceslikelihoodexperimentallyc=2networknetworks datalearning optimallinear vectorindependentbinary naturalalgorithms pcac=4paper unitsoutput layernetworkspatterns unitpattern set rulenetwork rulesweights trainingc=9visual imageimages cellscortex scenesupport spatialfeature visioncues stimulusstatisticsk=10 ?k=0.09neural neuronsanalog synapticneuron networksmemory timecapacity modelassociativenoise dynamicsk=9 ?k=0.02vector featureclassificationsupport vectorskernelregressionweight inputsdimensionalityk=2 ?k=0.13network inputinformation timerecurrent backpropagationunitsarchitectureforward layerk=1 ?k=0.11model learningsysteminformationparametersnetworks robustkalman rulesestimationk=4 ?k=0.12bayesianresults showestimationmethod basedparameterslikelihoodmethods modelsk=3 ?k=0.06objectrecognitionsystem objectsinformationvisual matchingproblem basedclassificationk=6 ?k=0.23neural networkpaperrecognitionspeech systemsbased resultsperformanceartificialk=5 ?k=0.04objectrecognitionsystem objectsinformationvisual matchingproblem basedclassificationk=8 ?k=0.23algorithmtraining errorfunction methodperformanceinputclassificationclassifierk=7 ?k=0.08data papernetworks networkoutput featurefeaturespatterns settrain introducedunit functionsFigure 4: Hasse diagram on NIPS for C = 10, K = 20 showing the top words for topics and unrepresented com-ponents (in shaded box).
Notice that some topics only consist of a single component.
The shaded box contains thecomponents that didn?t appear as a topic.
For the sake of clarity, we only show arrows for the subsumption rela-tionships between the topics, and we omit the implicit arrows between the components in the shaded box and thetopics.to the high-level of component re-use across topics.Topics are typically interpreted by looking at thetop-N words, whereas the top-N words of a compo-nent often do not even appear in the topics to whichit contributes.
Instead, we find that the componentscontribution to a topic is typically through vetoingwords.
For example, the top words of componentc=1, corresponding to the first column of the binarymatrix in figure 2, are [subject organization posting apple mitscreen write window video port], yet only a few of these ap-pear in topics k=1,2,3,4,5, which use it.On the right of figure 2, we show what the top-ics become when we ablate component c=1 fromthe matrix by setting the column to all zeros.
Topick=2 changes from being about information securityto general politics and is identical to k=9.
Topic k=3changes from the Turkish-Armenian War to a moregeneral war topic.
Topic k=4 changes to a less fo-cused version of itself.
In this way, we can gain fur-ther insight into the contribution of this component,and the way in which components tend to increasethe specificity of a topic to which they are added.The SCTM learns each topic as a soft intersec-tion of its components, as represented by the binarymatrix.
We can describe the overlap between topicsbased on the components that they have in common.One topic subsumes another topic when the parentconsists of a subset of the child?s components.
Inthis way, the binary matrix defines a Hasse diagram,a directed acyclic graph describing all the subsump-tion relationships between topics.
Figure 4 showssuch a Hasse diagram on the NIPS data.
Several top-ics consist of only a single component, such as k=12on reinforcement learning and k=8 on optimization.These two topics combine with the component c=1so that their overlap forms the topic k=4 on Bayesianmethods.
These subsumption relationships are dif-ferent from and complementary to hLDA (see ?4),which models topic co-occurrence, not componentintersection.
For example, topic k=10 on connec-tionism and k=2 on neural networks intersect toform k=20 which contains words that would onlyappear in both of its subsuming topics, thereby ex-plicitly modeling topic overlap.790The SCTM sometimes learns identical topics (tworows with the same binary entries ?on?)
such ask=13 and k=14 in figure 2 and k=3 and k=5 in fig-ure 4, which is likely due to the Gibbs sampler forthe binary matrix getting stuck in a local optimum.6 DiscussionWe have presented the Shared Components TopicModel (SCTM), in which topics are products ofunderlying component distributions.
This modelchange learns shared topic structures?as expressedthrough components?as opposed to generatingeach topic independently.
Reducing the number ofcomponents yields more compact models with lowerperplexity than LDA.
The two main limitations ofthe current SCTM are, when restricted to a squarebinary matrix (C = K), the inference procedure isunable to recover a model with perplexity as low asa collapsed Gibbs sampler for LDA, and the compo-nents are not consistently interpretable.The use of components opens up interesting di-rections of research.
For example, task specific sideinformation can be expressed as priors or constraintsover the components, or by adding conditioningvariables tied to the components.
Additionally, tasksbeyond document modeling may benefit from repre-senting topics as products of distributions.
For ex-ample, in vision, where topics are classes of objects,the components could be features of those objects.For selectional preference, components could cor-respond to semantic features that intersect to definesemantic classes (Gormley et al, 2011).
We hopenew opportunities will arise as this work explores anew research area for topic models.Appendix A: Derivation of Full ConditionalsThe model?s complete data likelihood over allvariables?observed words X , latent topic assign-ments Z, matrix B, and component/expert distribu-tions ?
:p(X,Z,B,?|?,?, ?)
=p(X|Z,B,?)p(Z|?)p(B|?)p(?|?)
(4)This follows from the conditional independence as-sumptions.
It is tractable to integrate out all parame-ters except Z,B,?
and hyperparameters ?,?, ?.
1111For simplicity, we switch from indexing examples as xmnto xi.
In this presentation, xi is the ith example in the corpus,Full conditional of zi Recall that p(Z|?)
isthe Dirichlet-Multinomial distribution over topicassignments, where ?
has been integrated out.The form of this distribution is identical to thecorresponding distribution over topics in LDA.The derivation of the full conditional of zi ?
{1, .
.
.
,K}, follows from the factorization in Eq.
4:p(zi|X,Z?
(i),B,?,?,?, ?)
(5)?
p(X|Z,B,?)p(Z|?)
(6)?
p(xi|bzi ,?)(n??
(i)mzi + ?zi) (7)Z?
(i) is the set of all topic assignments except zi.We use the independence of each document, recall-ing that example i belongs to document m. In prac-tice, we cache p(x|bz,?)
for all x, z (V ?K values)and these are shared by all zi in a sampling iteration.Above, just as in LDA, p(Z|?)
is simplified byproportionality to (n??
(i)mzi + ?zi), where n??
(i)mk is thecount of examples for document m that are assignedtopic k excluding zi?s contribution (Heinrich, 2008).Full conditional of bkc Recall that p(B|?)
is theprior for a Beta-Bernoulli matrix.
The full condi-tional distribution of a position in the binary vectoris (Griffiths and Ghahramani, 2006):p(bkc = 1|B?
(kc), ?)
=n??
(k)c +?CK + ?C(8)where n??
(k)c is the count of topics with componentc excluding topic k, and B?
(kc) is the entire matrixexcept for the entry bkc.To find the full conditional for bkc ?
{0, 1}, weagain start with the factorization from Eq.
4.p(bkc|X,Z,B?
(kc),?,?,?, ?)
(9)?
p(X|Z,B,?)p(B|?)
(10)?
[?i:zi=kp(xi|bzi ,?)]p(bkc|B?
(kc), ?)
(11)where p(bkc|B?
(kc), ?)
is given by Eq.
8,=???
(?Vv=1 ?n?kvcv)bkc(?Vv=1?Cj=1 ?bkjjv)?||n?k||1???
p(bkc|B?
(kc), ?
)(12)and where n?kv is the count of words assigned topick that are type v, and ||n?k||1 (the L1-norm of countvector n?k) is the count of all words with topic k.which corresponds to some m,n pair.791ReferencesDavid Blei and John Lafferty.
2006.
Correlated topicmodels.
In Advances in Neural Information Process-ing Systems (NIPS), volume 18.David Blei and Jon McAuliffe.
2007.
Supervised topicmodels.
In Advances in Neural Information Process-ing Systems (NIPS).David Blei, Andrew Ng, and Michael Jordan.
2003.
La-tent dirichlet alocation.
Journal of Machine LearningResearch, 3.David Blei, Thomas Griffiths, Michael Jordan, andJoshua Tenenbaum.
2004.
Hierarchical topic modelsand the nested chinese restaurant process.
In Advancesin Neural Information Processing Systems (NIPS), vol-ume 16.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society.
Se-ries B (Methodological), 39(1):1?38.Jacob Eisenstein, Amr Ahmed, and Eric P. Xing.
2011.Sparse additive generative models of text.
In Interna-tional Conference on Machine Learning (ICML).Matthew R. Gormley, Mark Dredze, Benjamin VanDurme, and Jason Eisner.
2011.
Shared componentstopic models with application to selectional prefer-ence.
In Learning Semantics Workshop at NIPS 2011,December.Thomas Griffiths and Zoubin Ghahramani.
2006.
Infinitelatent feature models and the indian buffet process.
InAdvances in Neural Information Processing Systems(NIPS), volume 18.Gregor Heinrich.
2008.
Parameter estimation for textanalysis.
Technical report, Fraunhofer IGD.Katherine A. Heller and Zoubin Ghahramani.
2007.
Anonparametric bayesian approach to modeling over-lapping clusters.
In Artificial Intelligence and Statis-tics (AISTATS), pages 187?194.Geoffrey Hinton.
1999.
Products of experts.
In In-ternational Conference on Artificial Neural Networks(ICANN).Geoffrey Hinton.
2002.
Training products of experts byminimizing contrastive divergence.
Neural Computa-tion, 14(8):1771?1800.Wei Li and Andrew McCallum.
2006.
Pachinko alloca-tion: DAG-structured mixture models of topic correla-tions.
In International Conference on Machine Learn-ing (ICML), pages 577?584.Wei Li, David Blei, and Andrew McCallum.
2007.
Non-parametric bayes pachinko allocation.
In Uncertaintyin Artificial Intelligence (UAI).David Mimno, Wei Li, and Andrew McCallum.
2007.Mixtures of hierarchical topics with pachinko alloca-tion.
In International Conference on Machine Learn-ing (ICML), pages 633?640.John Paisley, Chong Wang, and David Blei.
2011.
Thediscrete infinite logistic normal distribution for Mixed-Membership modeling.
In International Conferenceon Artificial Intelligence and Statistics (AISTATS).Yee Whye Teh, Michael Jordan, Matthew Beal, andDavid Blei.
2006.
Hierarchical dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, 101(476):1566?1581.Hanna Wallach, Ian Murray, Ruslan Salakhutdinov, andDavid Mimno.
2009.
Evaluation methods for topicmodels.
In International Conference on MachineLearning (ICML), pages 1105?1112.Greg Wei and Martin Tanner.
1990.
A monte carlo im-plementation of the EM algorithm and the poor man?sdata augmentation algorithms.
Journal of the Ameri-can Statistical Association, 85(411):699?704.Sinead Williamson, Chong Wang, Katherine Heller, andDavid Blei.
2010.
The IBP compound dirichletprocess and its application to focused topic model-ing.
In International Conference on Machine Learn-ing (ICML).Xiaojin Zhu, David Blei, and John Lafferty.
2006.TagLDA: bringing document structure knowledge intotopic models.
Technical Report TR-1553, Universityof Wisconsin.792
