Exceptionality and Natural Language LearningMihai Rotaru Diane J. LitmanComputer Science DepartmentUniversity of PittsburghPittsburgh, PA 15260mrotaru, litman @cs.pitt.eduAbstractPrevious work has argued that memory-basedlearning is better than abstraction-based learn-ing for a set of language learning tasks.
In thispaper, we first attempt to generalize these re-sults to a new set of language learning tasksfrom the area of spoken dialog systems and toa different abstraction-based learner.
We thenexamine the utility of various exceptionalitymeasures for predicting where one learner isbetter than the other.
Our results show thatgeneralization of previous results to our tasksis not so obvious and some of the exceptional-ity measures may be used to characterize theperformance of our learners.1 Introduction2Our paper is a follow-up of the study done by Daele-mans et al (1999) in which the authors show that keep-ing exceptional training instances is useful forincreasing generalization accuracy when natural lan-guage learning tasks are involved.
The tasks used intheir experiments are: grapheme-phoneme conversion,part of speech tagging, prepositional phrase attachmentand base noun phrase chunking.
Their study providesempirical evidence that editing exceptional instancesleads to a decrease in memory-based learner perform-ance.
Next, the memory-based learner is compared onthe same tasks with a decision-tree learner and theirresults favor the memory-based learner.
Moreover, theauthors provide evidence that the performance of theirmemory-based learner is linked to its property of hold-ing all instances (including exceptional ones) and gen-eral properties of language learning tasks (difficultnessin discriminating between noise and valid exceptionsand sub-regularities for those tasks).We continue on the same track by investigating iftheir results hold on a different set of tasks.
Our taskscome from the area of spoken dialog systems and havesmaller datasets and more features (with many of thefeatures being numeric, in contrast with the previousstudy that had none).
We observe in our experimentswith these tasks a much smaller exceptionality measurerange compared with the previous study.
Our resultsindicate that the previous results do not generalize to allour tasks.An additional goal of our research is to investigate anew topic by looking into whether exceptionality meas-ures can be used to characterize the performance of ourlearners: a memory-based learner (IB1-IG) and a rule-based learner (Ripper).
Our results indicate that forsome of the exceptionality measures we will examine,IB1-IG is better for predicting typical instances whileRipper is better for predicting exceptional instances.We will use the following conventions throughoutthe paper.
The term ?exceptional?
will be used to labelinstances that do not follow the rules that characterizethe class they are part of (in language learning terms,they are ?bad?
examples of their class rules).
We willuse ?typical?
as the antonym of this term; it will labelinstances that are good examples of their class rules.The fact that an instance is typical should not be con-fused with an exceptionality measure we will use thathas the same name (typicality measure).Learning methodsWe will use in our study the same memory-basedlearner that was used in the previous study: IB1-IG.
Theabstraction-based learner used in the previous study wasC5.0 (a commercial implementation of the C4.5 deci-sion tree learner).
In our study we will use a rule-basedlearner, Ripper.
Although the two abstraction-basedlearners are different, they share many features (manytechniques used in rule-based learning have beenadapted from decision tree learning (Cohen, 1995))1.1 We used Ripper because its implementation was availableand previous studies on our language learning tasks were per-formed using Ripper2.1 IB1-IGOur memory-based learner is called IB1-IG and is partof TiMBL, a software package developed by the ILKResearch Group, Tilburg University and the CNTS Re-search Group, University of Antwerp.
TiMBL is a col-lection of memory-based learners that sit on top of theclassic k-NN classification kernel with added metrics,algorithms, and extra functions.Memory-based reasoning is based on the hypothesisthat humans, in order to react to a new situation, firstcompare the new situation with previously encounteredsituations (which reside in their memory), pick one ormore similar situations, and react to the new one basedon how they reacted to those similar situations.
Thistype of learning is also called lazy learning because thelearner does not build a model from the training data.Instead, typically, the whole training set is stored.
Topredict the class for a new instance, the lazy learnercompares it with stored instances using a similarity met-ric and the new instance class is determined based onthe classes of the most similar training instances.
At thealgorithm level, lazy learning algorithms are versions ofk-nearest neighbor (k-NN) classifiers.IB1-IG is a k-NN classifier that uses a weightedoverlap metric, where a feature weight is automaticallycomputed as the Information Gain (IG) of that feature.The weighted overlap metric for two instances X and Yis defined as:?==?niiii yxwYX1),(),( ?
(1)where:iiiiiiiiiiyxyxminmaxyxabsyx?=????????
?=ififelse numeric, if10)(),(?Information gain is computed for every feature inisolation by computing the difference in uncertaintybetween situations with or without knowledge of thefeature value (for more information, see Daelemans etal., 2001).
These values describe the importance of thatfeature in predicting the class of an instance and areused as feature weights.2.233.1RipperRipper is a fast and effective rule-based learner devel-oped by William Cohen (Cohen, 1995).
The algorithmhas an overfit-and-simplify learning strategy: first aninitial rule set is devised by overfitting a part of thetraining set (called the growing set) and then this ruleset is repeatedly simplified by applying pruning opera-tors and testing the error reduction on another part of thetraining set (called the pruning set).
Ripper produces amodel consisting of an ordered set of if-then rules.There are several advantages to using rule-basedlearners.
The most important one is the fact that peoplecan understand relatively easy the model learned by arule-based learner compared with the one learned by adecision-tree learner, neural network or memory-basedlearner.
Also, domain knowledge can be incorporated ina rule-based learner by altering the type of rules it canlearn.
Finally, rule-based learners are relatively good atfiltering the potential noise from the training set.
But inthe context of natural language learning tasks wheredistinguishing between noise and exceptions and sub-regularities is very hard, this filtering may result in adecrease in accuracy.
In contrast, memory-based learn-ers, by keeping all instances around (including excep-tional ones), may have higher classification accuracy forsuch tasks.Exceptionality measuresOne of the main disadvantages of memory-based learn-ing is the fact that the entire training set is kept.
Thisleads to serious time and memory performance draw-backs if the training set is big enough.
Moreover, toimprove accuracy, one may want to have noisy in-stances present in the training set pruned.
To addressthese problems there has been a lot of work on trying toedit part of the training set without hampering the accu-racy of the predictor.
Two types of editing can be done.One can edit redundant regular instances (because thetraining set contains a lot of similar instances for thatclass) and/or unproductive instances (the ones that pre-sent irregularities with respect to the training set space).There are many measures that capture both types ofinstances.
We will use the ones from the previous study(typicality and class prediction strength) and a new onecalled local typicality.
Even though these measures weredevised with the purpose of editing part of the trainingset, they are used in our study and the previous study topoint out instances that should not be removed, at leastfor language learning tasks.TypicalityWe will use the typicality definition from Daelemans etal.
(1999) which is similar to the definition from Zhang(1992).
In both cases, a typicality function is definedwhose extremes correspond to exceptional and typicalinstances.
The function requires a similarity measurewhich is defined in both cases as the inverse of the dis-tance between two instances.
The difference betweenthe two implementations of typicality is that Zhang(1992) defines the distance as the Euclidian distancewhile Daelemans et al (1999) use the normalizedweighted Manhattan distance from (1).
Thus, our simi-larity measure will be defined as:?=?=niiii yxwYXsim1)),(1(),( ?For every instance X, a subset of the dataset calledfamily of X, Fam(X), is defined as being all instancesfrom the dataset that have the same class as X.
All re-maining instances form the unrelated instances subset,Unr(X).
Then, intra-concept similarity is defined as theaverage similarity between X and instances fromFam(X) and inter-concept similarity as the averagesimilarity between X and instances from Unr(X).
?==|)(|1))(,(|)(|1)(XFamiiXFamXsimXFamXIntra?==|)(|1))(,(|)(|1)(XUnriiXUnrXsimXUnrXInterFinally, typicality of an instance X is defined as theratio of its intra-concept and inter-concept similarity.
)()()(XInterXIntraXTypicality =The typicality values are interpreted as follows: ifthe value is higher than 1, then that instance has an in-tra-concept similarity higher than inter-concept similar-ity, thus one can say that the instance is a good exampleof its class (it is a typical instance).
A value less than 1implies the opposite: the instance is not a good exampleof its class (it is an exceptional instance).
Values around1 are called by Zhang boundary instances since theyseem to reside at the border between concepts.3.23.3Class prediction strengthAnother measure used in the previous study is the classprediction strength (CPS).
This measure tries to capturethe ability of an instance to predict correctly the class ofa new instance.
We will employ the same CPS defini-tion used in the previous study (the one proposed bySalzberg (1990)).
In the context of k-NN, predicting theclass means, typically, that the instance is the closestneighbor for a new instance.
Thus the CPS function isdefined as the ratio of the number of times our instanceis the closest neighbor for an instance of the same classand the number of times our instance is the closestneighbor for another instance regardless of its class.
ACPS value of 1 means that if our instance is to influenceanother instance class (by being its closest neighbor) itsinfluence is good (in the sense that predicting the classusing our instance class will result in an accurate predic-tion).
Thus our instance is a good predictor for our class,i.e.
it is a typical instance.
In contrast, a value of 0 indi-cates a bad predictor for the class and thus labels anexception instance.
A value of 0.5 will correspond toinstances at the border between concepts.Unlike typicality, when computing CPS, we can en-counter situations when its value is undefined (zero di-vided by zero).
This means that the instance is not theclosest neighbor for any other instance.
Since there is noclear interpretation of instance properties in this case,we will set its CPS value to a constant higher than 1 (noparticular meaning of the value, just to recognize it inour graphs).Local typicalityWhile CPS captures information very close to an in-stance, typicality as defined by Zhang captures informa-tion from the entire dataset.
But this may not be themost desirable measure in cases such as those when aconcept is made of at least two disjunctive clusters.Consider the example from Figure 1.
For an instance inthe center of cluster A1, its similarity with instancesfrom the same cluster is very high but very low withinstances from cluster A2.
At the same time, its similar-ity with instances from class B is somewhere betweenabo in-stan avecomarouclusof Fstanweinstsetsity,instvalustantheiof mvicineigto thsetsnishandputiFve two values.
When everything is averaged,ce intra-concept and inter-concept similarity hparable values thus leading to a typicality valuend 1 even if the instance is highly typical for theter A1.A1To address this problem, we changed the definitionam(X) and Unr(X).
Instead of considering all in-ces from the dataset when building the two subsets,will be using only instances from a vicinity of ourance.
The typicality computed using these new sub-will be called local typicality.
To define the vicin-we used again the similarity metric.
When twoances are identical, their similarity has the maximume which is the sum of all feature weights.
An in-ce is in the vicinity of another instance if and only ifr similarity has a value higher than a given percentaximum similarity value (using this definition ofnity instead of a specified number of nearesthbors, makes our exceptionality measure adaptivee density of the local neighborhood).
For our data-, a percent value of 90% yields the best results fur-ing a measure that is different from both typicalityCPS.Like CPS, division by zero can appear when com-ng local typicality.
This means that inter-conceptBA2igure 1.
Class distribution that causes flattening in typicalitydistributionsimilarity is zero and this can only happen if there is noinstance with a different class in the vicinity of our in-stance.
In this case, if the intra-concept similarity ishigher than 0 (there is at least one instance from thesame class in the vicinity) we set the local typicality to amaximum value, while if the intra-concept similarity is0, then we set the typicality to a minimum value (no onein the vicinity of this instance is a good indication of anexceptional instance).
When inter-concept similarity ishigher than 0, we will set the local typicality to a mini-mum value if its intra-concept similarity is 0 (so that wewill not have a big gap between local typicality values).Minimum and maximum values are computed as valuesto the left and right of the local typicality interval fornon-exceptional cases.We can rank our exceptionality measures by thelevel of information they capture (from most general tomost local): typicality, local typicality and CPS.4 Language learning tasksThe tasks we will be using in our study come from thearea of spoken dialog systems (SDS).
They were alldesigned as methods for potentially improving the dia-log manager of a SDS system called TOOT (Litman andPan, 2002).
This system provides access to train infor-mation from the web via telephone and it was developedfor the purpose of comparing differences in dialog strat-egy.Our tasks are: (1) Identifying user corrections(ISCORR), (2) Identifying correction-aware sites(STATUS), (3) Identifying concept-level speech recog-nition errors (CABIN) and (4) Identifying word-levelspeech recognition errors (WERBIN).
The first task is abinary classification task that labels each user turn as towhether or not it is an attempt from the user to correct aprior system recognition failure.
The second task is a 4-way classification task that extends the previous onewith whether or not the user is aware the system made arecognition error.
The four classes are: normal user turn,user only tries to correct the system, user is only awareof a system recognition error, and user is both aware ofand tries to correct the system error.
The third and thefourth tasks are binary classification tasks that try topredict the system speech recognition accuracy whenrecognizing a user turn.
CABIN measures a binary ver-sion of the Concept Accuracy (percent of semantic con-cepts recognized correctly) while WERBIN measures abinary version of the Word Error Rate (percent of wordsrecognized incorrectly).Data for our tasks was gathered from a corpus of2,328 user turns from 152 dialogues between humansubjects and TOOT.
The features used to represent eachuser turn include prosodic information, informationfrom the automatic speech recognizer, system condi-tions and dialog history.
Then, each user turn was la-beled with respect to every classification task.
Eventhough our classification tasks share the same data,there are clear differences between them.
ISCORR andSTATUS both deal with user corrections which is quitedifferent from predicting speech recognition errors(handled in WERBIN and CABIN).
Moreover, one willexpect very little noise or no noise at all when manuallyannotating WERBIN and CABIN.
For more informationon our tasks and features, see (Litman et al, 2000;Hirschberg et al, 2001; Litman et al, 2001).There are a number of dimensions where our tasksdiffer from the tasks from the previous study.
First of allour datasets are smaller (2,328 instances compared withat least 23,898).
Second, the number of features used ismuch bigger than the previous study (141 comparedwith 4-11).
Moreover, many features from our datasetsare numeric while the previous study had none.
Thesedifferences will also reflect on our exceptionality meas-ures values.
For example, the smallest range for typical-ity in the previous study was between 0.43 and 10.57while for our tasks it is between 0.9 and 1.1.
To explorethese differences we varied the feature set used.
Insteadof using all the available features (this feature set iscalled All), we restricted the feature set by using onlynon-numeric features (Nonnum ?
22 features).
The typi-cality range increased when using this feature set (0.77-1.45), but the number of features used was still largerthan the previous study.
For this reason, we next de-vised two set of features with only 9 (First9) and 15features (First15).
The features were selected based ontheir information gain (see section 2.1).Before proceeding with our results, there is onemore thing we want to mention.
At least half of our in-stances have one or more missing values and while theRipper implementation offered a way to handle them,there was no default handling of missing values in theIB1-IG implementation.
Thus, we decided to replacemissing values ourselves before presenting the datasetsto our learners.
In particular there are two types of miss-ing values: genuine missing values (no value was pro-vided; we will refer to them as missing values) andundefined values.
Undefined values come from featuresthat are not defined in that user turn (for example, in thefirst user turn, most of the dialog history features wereundefined because there was no previous user turn).For symbolic features, we replaced missing and un-defined values with a given string for missing valuesand another one for undefined values.
For numeric fea-tures, the problem was more complicated since the dis-tance metric uses the difference between two numericvalues and thus, the values used to fix the problem caninfluence the distance between instances.
We experi-mented with different replacement values: to the left andright of the interval boundaries for that features, bothreplacement values on one side of the interval or veryfar from the interval boundaries.
All experiments withthe values provided comparable results.
For our experi-ments, missing values were replaced with a value to theright of the interval for that feature and undefined val-ues were replaced with a value to the left of that inter-val.5 Results5.1In 5.1 we reproduce the editing and comparison experi-ments from the previous study to see if their results gen-eralize to our tasks.
In 5.2, we move to our next goal:characterizing learners?
performance using exceptional-ity measures.
Both learners were run using default pa-rameters2.Natural language learning and memory-based learningFirst, we performed the editing experiments from theprevious study.
The purpose of those experiments wasto see the impact of editing exceptional and typical in-stances on the accuracy of the memory-based learner.Since our datasets were small, unlike the previous studywhich performed editing only on the first train-test par-tition of a 10-fold cross validation, we performed theediting experiment on all partitions of a 10-fold crossvalidation.
For every fold, we edited 0, 1, 2, 5, 10, 20,30, 40 and 50% of the training set based on extremevalues of all our exceptionality criteria.
Accuracy afterediting a given percent was averaged among all folds(there is a significant difference in accuracies amongfolds but all folds exhibit a similar trend with the aver-age).
Figure 2 shows our results for the ISCORR dataset79.0%79.5%80.0%80.5%81.0%81.5%82.0%82.5%83.0%0 1 2 5 10 20 30 40 50Percentage of instances removedAverageaccuracyHigh CPSLow CPSHigh Local Typ.Low Local Typ.High TypicalityLow TypicalityFigure 2.
IB1-IG average accuracy after editing a given percent of thetraining set based on high and low extremes of all exceptionalitymeasures (ISCORR dataset with all features)2 We performed parameter tuning experiments for both predic-tors: for every fold of a 10-fold cross validation, part of thetraining set was used as a validation set (for tuning parame-ters).
Our results indicate that the tuned parameters depend onthe fold used and there was no clear gain to accuracy fromtuning (in some cases there was even loss in accuracy).
Inte-grating tuned parameters with our leave-one-out experimentspresents additional problems.using six types of editing (editing based on low and highvalue for all three criteria).
In contrast with the previousstudy, where for all tasks even the smallest editing ledto significant accuracy decreases, for our task there wasno clear decrease in performance.
Moreover, for somecriteria (like low local-typicality) we can even see aninitial increase in performance.
Only after editing halfof the training set is there a clear decrease in perform-ance for all editing criteria on this task.Editing experiments for the other dataset-feature setcombinations yield similar results.Next, we compared the memory-based learner withour abstraction-based learner on all tasks.
Since thedatasets were relatively small, we performed leave-one-out cross validations.
Table 1 summarizes our results.The baseline used is the majority class baseline.
First,we run the predictors on all tasks using all features.
Incontrast with the previous study which favored thememory-based learner for almost all their tasks, ourresults favor IB1-IG for only two of the four tasks(ISCORR and STATUS).
In Section 4, we mentionedthat the typicality range for our tasks was very smallcompared with the previous study.
Contrary to what weexpected, the tasks where IB1-IG performed better werethe ones with smaller typicality range.
To investigatethe typicality range impact on our predictors, we tried tomake our datasets similar to the datasets from the previ-ous study by tackling the feature set.
We eliminated allnumeric features (since the tasks from the previousstudy had none) and performed experiments on the tasksthat had the less typicality range (again, ISCORR andSTATUS).
Again, when typicality range was increased,even though there were no numeric features, IB1-IGperformed worse than Ripper.
IB1-IG error rate in-creased when using only non-numeric features for bothtasks compared with the error rate when using all fea-tures.
This observation led us to assume that, at least forIB1-IG, some of the relevant features for classificationwere numeric and they were not present in our featureset.
Thus, we selected two sets of features (First9 andFirst15) based on the features?
relevance and performedthe experiments again on the ISCORR dataset.
We canError rateData-Feat.
set IB1-IG Ripper BaselineTypicalityrangeIscorr-All 14.99% 16.15% 28.99% 0.94 - 1.06Status-All 22.25% 23.71% 43.04% 0.96 - 1.10Cabin-All 13.10% 12.11% 30.50% 0.90 - 1.12Werbin-All 17.65% 11.90% 39.22% 0.90 - 1.10Iscorr-Nonnum 17.01% 16.24% 28.99% 0.81 - 1.49Status-Nonnum 23.93% 21.99% 43.04% 0.88 - 1.62Iscorr-First9 17.78% 16.07% 28.99% 0.86 - 1.17Iscorr-First15 14.69% 14.95% 28.99% 0.88 - 1.14Table 1.
IB1-IG, Ripper and majority class baseline errorrate on some of our dataset-feature set combinationsobserve that as the number of relevant features is in-creased, the error rate for both predictors and the typi-cality range are decreasing and IB1-IG takes the leadwhen the First15 feature set is used.
Our results indicatethat the predictor that performs better depends on thetask, the number of features and the type of features weuse.To explore why the previous study?s results do notgeneralize in our case, we are planning to replicate theseexperiments on the dialog-act tagging task on theSwitchboard corpus (a task more similar in size andfeature types with the previous study than our tasks butstill in the area of spoken dialog systems ?
see Shriberget al (1998)).5.2 Characterizing learners?
performanceusing exceptionality measuresThe next goal of our study was to see if we can charac-terize the performance of our predictors on variousclasses of instances defined by our exceptionality crite-ria.
In other words, we wanted to try to answer ques-tions like: is IB1-IG better at predicting exceptionalinstances than Ripper?
How about typical instances?Can we combine the two learners and select betweenthem based on the instance exceptionality?To answer these questions, we performed the leave-one-out experiments described above and recorded forevery instance whether our predictors predicted it cor-rectly or incorrectly.
Next, we computed the exception-ality of every instance using all three measures.
Figure 3shows the exceptionality distribution using the typicalitymeasure for the ISCORR dataset with all features3.
The0501001502002503003504004500.94 0.95 0.96 0.97 0.98 0.99 1.00 1.01 1.02 1.03 1.04 1.05 1.06TypicalityFrequencyIB1-IGRipperFull datasetFigure 3.
Typicality distribution for all instances, instances correctlypredicted by IB1-IG and instances correctly predicted by Ripper(ISCORR dataset with all features)typicality distributions of all instances from theISCORR dataset, of instances correctly predicted byIB1-IG, and of instances correctly predicted by Ripperare plotted in the figure.
The graph shows that for thisdataset there are a lot of boundary instances, very fewexceptional instances and few typical instances.
Thetypicality range for all our datasets (usually between0.85 and 1.15) is far less than the one from the previousstudy (0.43 up to 10 or even 3500).
According to Zhang(1992) hard concepts are often characterized by smalltypicality spread.
Moreover, small typicality spread isassociated with low accuracy in predicting.3 For other dataset-feature set combination graphs see:http://www.cs.pitt.edu/~mrotaru/exceptionalityFigure 4 shows the same information as Figure 3,but instead of plotting the count, we plot the percentageof the instances with typicality between a given intervalthat have been correctly classified by one of the predic-tors.
We can observe that accuracy of both predictorsincreases with typicality.
That is, the more typical the0%10%20%30%40%50%60%70%80%90%100%0.94 0.95 0.96 0.97 0.98 0.99 1.00 1.01 1.02 1.03 1.04 1.05 1.06TypicalityCorrectlypredicted-percentageIB1-IGRipperFigure 4.
Percent of instances predicted correctly by IB1-IG and Rip-per based on instance typicality (ISCORR dataset with all features)instance, the more reliable the prediction; the more ex-ceptional the instance, the more unreliable the predic-tion.
This observation holds for all our dataset-featureset combinations.
It is not clear for the ISCORR datasetwhether one predictor is better than the other based onthe typicality.
But for datasets CABIN and WERBINwhere, overall, IB1-IG did worse than Ripper, the samegraph (see Figure 5) shows that IB1-IG?s accuracy isworse than Ripper?s accuracy when predicting low typi-cality instances4.
Given the problems with typicality ifthe concepts we want to learn are clustered, we decided0%10%20%30%40%50%60%70%80%90%100%0.89 0.91 0.93 0.94 0.96 0.98 1.00 1.02 1.04 1.06 1.07 1.09 1.11T ypicalityCorrectlypredicted-percentageIB1-IGRipperFigure 5.
Percent of instances predicted correctly by IB1-IG and Rip-per based on instance typicality (CABIN dataset with all features)4 It was not our point to investigate statistical significance ofthis trend.
As we will see later, this trend is powerful enoughto yield interesting results when combining the predictorsbased on exceptionality measures.to investigate if this observation holds for other excep-tionality measures.We continued the experiments on the other excep-tionality measures hoping to get more insight into thetrend observed for typicality.
Indeed, Figure 6 (same asFigure 4  but using the CPS instead of typicality) showsthe same trend: IB1-IG is worse than Ripper when pre-dicting exceptional instances and it is better when pre-dicting typical instances.
The accuracy curves of thetwo predictors seem to cross at a CPS value of 0.5,which corresponds to boundary instances.
UndefinedCPS values (0/0) are assigned a value above 1 (therightmost point on the graph).
Ripper was the one thatoffered higher accuracy in predicting instances withundefined CPS value for almost all datasets (althoughnot in Figure 6).
The result holds for all our dataset-feature set combinations.0%10%20%30%40%50%60%70%80%90%100%0.00 0.08 0.17 0.25 0.34 0.42 0.51 0.59 0.68 0.76 0.85 0.93 1.02CPSPredictedcorrectly-percentageIB1-IGRipperFigure 6.
Percent of instances predicted correctly by IB1-IG and Rip-per based on instance CPS (ISCORR dataset with all features)5The experiments with local typicality yield the sameresults: Ripper constantly outperforms IB1-IG for ex-ceptional instances and they switch places for typicalinstances (see Figure 7).
Again, the accuracy curvescross at boundary instances (local typicality value of 1)and the same observation holds for all dataset-featureset combinations.0%10%20%30%40%50%60%70%80%90%100%0.91 0.92 0.93 0.95 0.96 0.97 0.99 1.00 1.01 1.03 1.04 1.05 1.07Local TypicalityCorrectlypredicted-percentageIB1-IGRipperFigure 7.
Percent of instances predicted correctly by IB1-IG andRipper based on instance local typicality(ISCORR dataset with all features)5 Abrupt movements in curves are caused by small number ofinstances in that class.
We expect that a larger dataset willsmooth our graphs.We computed what could be the reduction in errorrate if we were to employ both predictors and decidebetween them based on the instance exceptionalitymeasure.
In other words, Ripper prediction was used forexceptional instances and for the left-hand side bound-ary instances (CPS less than 0.5; typicality less than 1;local typicality less than 1); otherwise IB1-IG predictionwas used.
The lower bound of this reduction is when weperfectly know which of the predictors offer the correctprediction (in other words the error rate is the number oftimes both learners furnished wrong predictions).
Figure8 plots the reduction in error rate achieved when decid-ing between predictors based on typicality, CPS, localtypicality and perfect discrimination.
The reduction isrelative to the best performer on that task.
While dis-criminating based on typicality offered no improvementrelative to the best performer, CPS was able to con-stantly achieve improvement and local typicality im-proved in six out of eight cases.
CPS improved the errorrate of the best performer by decreasing it by 1.33% to3.18% (absolute percentage).
In contrast with CPS, localtypicality offered, for the cases when it improved theaccuracy, more improvement decreasing the error rateby up to 4.94% (absolute percentage).
A possible expla-nation of this difference can be the fact that local typi-cality captures much more information than CPS(vicinity-level information compared with informationvery close to the instance).-20%-10%0%10%20%30%40%50%60%Iscorr-AllStatus-AllCabin-AllWerbin-AllIscorr-NonnumStatus-NonnumIscorr-First9Iscorr-First15TypicalityCPSLocal TypicalityPerfect Discr.Figure 8.
Reduction in error rate relative to the best performer fortypicality, CPS, local typicality and prefect discriminationIn summary, all our exceptionality measures showthe same trend in predicting ability: Ripper performsbetter than IB1-IG on exceptional instances while IB1-IG performs better than Ripper on typical instances.While the fact that IB1-IG does better on typical in-stances may be linked to its ability to handle sub-regularities, we have no interpretation for the fact thatRipper does better on exceptional instances.
We plan toaddress this by future work that will look at the distancebetween exceptional instances and the instances thatgenerated the rule that made the correct prediction forthose exceptional instances.5.3 Current directionsThe previous section showed that we can improve theoverall accuracy on our datasets if we combine the pre-diction generated by our learners based on the excep-tionality measure of the new instance.
Unfortunately, allour exceptionality measures require the class of the in-stance.
Moreover, for binary classification tasks, sinceall exceptionality criteria are a ratio, changing the in-stance class will turn an exceptional instance into atypical instance.To move our results from offline to online, we con-sidered interpolating the exceptionality value for aninstance based on its neighbors?
exceptionality values(the neighbors from the training set).
We performed avery simple interpolation by using the exceptionalityvalue of the closest neighbor (relative to equation (1)).While previous observations are not obvious anymore inonline graphs (there is no clear crossing at boundaryinstances), there is a small improvement over the bestpredictor.
Figure 9 shows that even for this simple in-terpolation there is a small reduction in almost all casesin error rate relative to the best performer when usingonline CPS (interpolated CPS).-15%-10%-5%0%5%10%15%20%Iscorr-AllStatus-AllCabin-AllWerbin-AllIscorr-NonnumStatus-NonnumIscorr-First9Iscorr-First15Offline CPSOnline CPSFigure 9.
Reduction in error rate relative to the best performer foroffline CPS and online CPSWe are currently investigating more complicated in-terpolation strategies like learning of a model from thetraining set that will predict the exceptionality value ofan instance based on its closest neighbors.6 ConclusionsIn this paper we attempted to generalize the results of aprevious study to a new set of language learning tasksfrom the area of spoken dialog systems.
Our experi-ments indicate that previous results do not generalize soobviously to the new tasks.
Next, we showed that someexceptionality measures can be used as means to im-prove the prediction accuracy on our tasks by combin-ing the prediction of our learners based on measures ofinstance exceptionality.
We observed that our memory-based learner performs better than the rule-based learneron typical instances and they exchange places for excep-tional instances.
We also showed that there is potentialfor moving these results from offline to online by per-forming a simple interpolation.
Future work needs toaddress more complicated methods of interpolation,comparison between our method and other attempts tocombine rule-based learning and memory-based learn-ing (Domingos, 1996; Golding and Rosenbloom, 1991),comparison with ensemble methods, and whether theresults from this paper generalize to other spoken dialogcorpora.AcknowledgementsWe would like to thank Walter Daelemans and Antalvan den Bosch for starting us on this work.ReferencesWilliam Cohen.
1995.
Fast effective rule induction.
ICML.Walter Daelemans, Antal van den Bosch, and Jakub Zavrel.1999.
Forgetting exceptions is harmful in languagelearning.
Machine Learning 1999, 34 :11-43.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antalvan den Bosch.
2001.
TiMBL: Tilburg Memory BasedLearner, version 4.1, Reference Guide.
ILK TechnicalReport ?
ILK 01-04.Pedro Domingos.
1996.
Unifying Instance-Based and Rule-Based Induction.
Machine Learning 1996, 24:141-168Andrew R. Golding and Paul S. Rosenbloom.
1991.
ImprovingRule-Based Systems Through Case-Based Reasoning.
Proc.AAAI.Julia Hirschberg, Diane J. Litman, and Marc Swerts.
2001.Identifying User Corrections Automatically in SpokenDialogue Systems.
Proc.
NAACL.Diane J. Litman, Julia Hirschberg, and Marc Swerts.
2000.Predicting Automatic Speech Recognition PerformanceUsing Prosodic Cues.
Proc.
NAACL.Diane J. Litman, Julia Hirschberg, and Marc Swerts.
2001.Predicting User Reactions to System Error.
Proc.
ACL.Diane J. Litman, Shimei Pan.
2002.
Designing and Evaluatingan Adaptive Spoken Dialogue System.
User Modeling andUser-Adapted Interaction, 12(2/3):111-137.Salzberg, S. 1990.
Learning with nested generalisedexemplars.
Kluwer Academic Publishers.Elizabeth Shriberg, Rebecca Bates, Paul Taylor, AndreasStolcke, Klaus Ries, Daniel Jurafsky, Noah Coccaro,Rachel Martin, Marie Meteer, and Carol Van Ess-Dykema.1998.
Can prosody aid the automatic classification ofdialog acts in conversational speech?.
Language andSpeech 41:439?487.Jianping Zhang.
1992.
Selecting typical instances ininstance-based learning.
Proc.
ICML, 470-479.
