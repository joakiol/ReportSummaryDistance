Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 1?11,Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational LinguisticsA New Sentence Compression Dataset and Its Use in an AbstractiveGenerate-and-Rank Sentence CompressorDimitrios Galanis?
and Ion Androutsopoulos?+?Department of Informatics, Athens University of Economics and Business, Greece+Digital Curation Unit ?
IMIS, Research Center ?Athena?, GreeceAbstractSentence compression has attracted much in-terest in recent years, but most sentence com-pressors are extractive, i.e., they only deletewords.
There is a lack of appropriate datasetsto train and evaluate abstractive sentence com-pressors, i.e., methods that apart from delet-ing words can also rephrase expressions.
Wepresent a new dataset that contains candi-date extractive and abstractive compressionsof source sentences.
The candidate compres-sions are annotated with human judgementsfor grammaticality and meaning preservation.We discuss how the dataset was created, andhow it can be used in generate-and-rank ab-stractive sentence compressors.
We also re-port experimental results with a novel abstrac-tive sentence compressor that uses the dataset.1 IntroductionSentence compression is the task of producing ashorter form of a grammatical source (input) sen-tence, so that the new form will still be grammati-cal and it will retain the most important informationof the source (Jing, 2000).
Sentence compression isuseful in many applications, such as text summariza-tion (Madnani et al, 2007) and subtitle generation(Corston-Oliver, 2001).
Methods for sentence com-pression can be divided in two categories: extrac-tive methods produce compressions by only remov-ing words, whereas abstractive methods may addi-tionally rephrase expressions of the source sentence.Extractive methods are generally simpler and havedominated the sentence compression literature (Jing,2000; Knight and Marcu, 2002; McDonald, 2006;Cohn and Lapata, 2007; Clarke and Lapata, 2008;Cohn and Lapata, 2009; Nomoto, 2009; Galanisand Androutsopoulos, 2010; Yamangil and Shieber,2010).
Abstractive methods, however, can in prin-ciple produce shorter compressions that convey thesame information as longer extractive ones.
Further-more, humans produce mostly abstractive compres-sions (Cohn and Lapata, 2008); hence, abstractivecompressors may generate more natural outputs.When evaluating extractive methods, it sufficesto have a single human gold extractive compres-sion per source sentence, because it has been shownthat measuring the similarity (as F1-measure of de-pendencies) between the dependency tree of thegold compression and that of a machine-generatedcompression correlates well with human judgements(Riezler et al, 2003; Clarke and Lapata, 2006a).With abstractive methods, however, there is a muchwider range of acceptable abstractive compressionsof each source sentence, to the extent that a singlegold compression per source is insufficient.
Indeed,to the best of our knowledge no measure to com-pare a machine-generated abstractive compressionto a single human gold compression has been shownto correlate well with human judgements.One might attempt to provide multiple humangold abstractive compressions per source sentenceand employ measures from machine translation, forexample BLEU (Papineni et al, 2002), to compareeach machine-generated compression to all the cor-responding gold ones.
However, a large number ofgold compressions would be necessary to capture all(or at least most) of the acceptable shorter rephras-1ings of the source sentences, and it is questionableif human judges could provide (or even think of) allthe acceptable rephrasings.
In machine translation,n-gram-based evaluation measures like BLEU havebeen criticized exactly because they cannot copesufficiently well with paraphrases (Callison-Burchet al, 2006), which play a central role in abstractivesentence compression (Zhao et al, 2009a).1Although it is difficult to construct datasets forend-to-end automatic evaluation of abstractive sen-tence compression methods, it is possible to con-struct datasets to evaluate the ranking componentsof generate-and-rank abstractive sentence compres-sors, i.e., compressors that first generate a large setof candidate abstractive (and possibly also extrac-tive) compressions of the source and then rank themto select the best one.
In previous work (Galanis andAndroutsopoulos, 2010), we presented a generate-and-rank extractive sentence compressor, hereaftercalled GA-EXTR, which achieved state-of-the art re-sults.
We aim to construct a similar abstractivegenerate-and-rank sentence compressor.
As part ofthis endeavour, we needed a dataset to automaticallytest (and train) several alternative ranking compo-nents.
In this paper, we introduce a dataset of thiskind, which we also make publicly available.2The dataset consists of pairs of source sentencesand candidate extractive or abstractive compres-sions.
The candidate compressions were generatedby first using GA-EXTR and then applying exist-ing paraphrasing rules (Zhao et al, 2009b) to thebest extractive compressions of GA-EXTR.
Each pair(source and candidate compression) was then scoredby a human judge for grammaticality and meaningpreservation.
We discuss how the dataset was con-structed and how we established upper and lowerperformance boundaries for ranking components ofcompressors that may use it.
We also present the1Ways to extend n-gram measures to account for para-phrases have been proposed (Zhou et al, 2006; Kauchak andBarzilay, 2006; Pado?
et al, 2009), but they require accu-rate paraphrase recognizers (Androutsopoulos and Malakasio-tis, 2010), which are not yet available; or they assume thatthe same paraphrase generation resources (Madnani and Dorr,2010), for example paraphrasing rules, that some abstractivesentence compressors (including ours) use always produce ac-ceptable paraphrases, which is not the case as discussed below.2The new dataset and GA-EXTR are freely available fromhttp://nlp.cs.aueb.gr/software.html.current version of our abstractive sentence compres-sor, and we discuss how its ranking component wasimproved by performing experiments on the dataset.Section 2 below summarizes prior work on ab-stractive sentence compression.
Section 3 discussesthe dataset we constructed.
Section 4 describes ourabstractive sentence compressor.
Section 5 presentsour experimental results, and Section 6 concludes.2 Prior work on abstractive compressionThe first abstractive compression method was pro-posed by Cohn and Lapata (2008).
It learns a set ofparse tree transduction rules from a training datasetof pairs, each pair consisting of a source sentenceand a single human-authored gold abstractive com-pression.
The set of transduction rules is then aug-mented by applying a pivoting approach to a par-allel bilingual corpus; we discuss similar pivotingmechanisms below.
To compress a new sentence, achart-based decoder and a Structured Support Vec-tor Machine (Tsochantaridis et al, 2005) are used toselect the best abstractive compression among thoselicensed by the rules learnt.The dataset that Cohn and Lapata (2008) usedto learn transduction rules consists of 570 pairs ofsource sentences and abstractive compressions.
Thecompressions were produced by humans who wereallowed to use any transformation they wished.
Weused a sample of 50 pairs from that dataset to con-firm that humans produce mostly abstractive com-pressions.
Indeed, 42 (84%) of the compressionswere abstractive, and only 7 (14%) were simply ex-tractive.3 We could not use that dataset, however,for automatic evaluation purposes, since it only pro-vides a single human gold abstract compression persource, which is insufficient as already discussed.More recently, Zhao et al (2009a) presented asentence paraphrasing method that can be config-ured for different tasks, including a form of sentencecompression.
For each source sentence, Zhao et al?smethod uses a decoder to produce the best possibleparaphrase, much as in phrase-based statistical ma-chine translation (Koehn, 2009), but with phrase ta-bles corresponding to paraphrasing rules (e.g., ?X3Cohn and Lapata?s dataset is available from http://staffwww.dcs.shef.ac.uk/people/T.Cohn/t3/#Corpus.
One pair (2%) of our sample had a ?compression?that was identical to the input.2is the author of Y ?
?
?X wrote Y ?)
obtained fromparallel and comparable corpora (Zhao et al, 2008).The decoder uses a log-linear objective function, theweights of which are estimated with a minimum er-ror rate training approach (Och, 2003).
The objec-tive function combines a language model, a para-phrase model (combining the quality scores of theparaphrasing rules that turn the source into the can-didate paraphrase), and a task-specific model; in thecase of sentence compression, the latter model re-wards shorter candidate paraphrases.We note that Zhao et al?s method (2009a) is in-tended to produce paraphrases, even when config-ured to prefer shorter paraphrases, i.e., the compres-sions are still intended to convey the same informa-tion as the source sentences.
By contrast, most sen-tence compression methods (both extractive and ab-stractive, including ours) are expected to retain onlythe most important information of the source sen-tence, in order to achieve better compression rates.Hence, Zhao et al?s sentence compression task is notthe same as the task we are concerned with, and thecompressions we aim for are significantly shorter.3 The new datasetTo construct the new dataset, we used source sen-tences from the 570 pairs of Cohn and Lapata (Sec-tion 2).
This way a human gold abstractive com-pression is also available for each source sentence,though we do not currently use the gold compres-sions in our experiments.
We actually used only 346of the 570 source sentences of Cohn and Lapata, re-serving the remaining 224 for further experiments.4To obtain candidate compressions, we first ap-plied GA-EXTR to the 346 source sentences, and wethen applied the paraphrasing rules of Zhao et al(2009b) to the resulting extractive compressions; weprovide more information about GA-EXTR and theparaphrasing rules below.
We decided to apply para-phrasing rules to extractive compressions, becausewe noticed that most of the 42 human abstractivecompressions of the 50 sample pairs from Cohn andLapata?s dataset that we initially considered (Sec-tion 2) could be produced from the correspondingsource sentences by first deleting words and then us-4The 346 sources are from 19 randomly selected articlesamong the 30 that Cohn and Lapata drew source sentences from.ing shorter paraphrases, as in the following example.source: Constraints on recruiting are constraints onsafety and have to be removed.extractive: Constraints on recruiting have to be re-moved.abstractive: Recruiting constraints must be removed.3.1 Extractive candidate compressionsGA-EXTR, which we first applied to the dataset?ssource sentences, generates extractive candidatecompressions by pruning branches of each source?sdependency tree; a Maximum Entropy classifier isused to guide the pruning.
Subsequently, GA-EXTRranks the extractive candidates using a Support Vec-tor Regression (SVR) model, which assigns a scoreF (eij |si) to each candidate extractive compressioneij of a source sentence si by examining featuresof si and eij ; consult our previous work (Galanisand Androutsopoulos, 2010) for details.5 For eachsource si, we kept the (at most) kmax = 10 extrac-tive candidates eij with the highest F (eij |si) scores.3.2 Abstractive candidate compressionsWe then applied Zhao et al?s (2009b) paraphrasingrules to each one of the extractive compressions eij .The rules are of the form left ?
right, with left andright being sequences of words and slots; the slotsare part-of-speech tagged and they can be filled inwith words of the corresponding categories.
Exam-ples of rules are shown below.?
get rid of NNS1?
remove NNS1?
get into NNP1?
enter NNP1?
NNP1 was written by NNP2?
NNP2 wrote NNP1Roughly speaking, the rules were extracted froma parallel English-Chinese corpus, based on the as-sumption that two English phrases ?1 and ?2 thatare often aligned to the same Chinese phrase ?
are5We trained GA-EXTR on approximately 1,050 pairs ofsource sentences and gold human extractive compressions,obtained from Edinburgh?s ?written?
extractive dataset; seehttp://jamesclarke.net/research/resources.The source sentences of that dataset are from 82 documents.The 1,050 pairs that we used had source sentences from 52 outof the 82 documents.
We did not use source sentences fromthe other 30 documents, because they were used by Cohn andLapata (2008) to build their abstractive dataset (Section 2),from which we drew source sentences for our dataset.3si '' ,,ei1 '' ,,ei2,, -- ..?
?
?
eik-- ..ai1.1 && ,,ai1.2,,?
?
?
ai1.mi1 ai2.1 ai2.2 ?
?
?
ai2.mi2 ?
?
?
aik.1 ?
?
?
aik.mikai1.1.1ai1.1.2 ?
?
?
ai1.1.mi1.1 ai1.2.1 ?
?
??
?
?Figure 1: Generating candidate extractive (eij) and abstractive (aij...) compressions from a source sentence (si).likely to be paraphrases and, hence, can be treatedas a paraphrasing rule ?1 ?
?2.
This pivoting wasused, for example, by Bannard and Callison-Burch(2005), and it underlies several other paraphraseextraction methods (Riezler et al, 2007; Callison-Burch, 2008; Kok and Brockett, 2010).
Zhao etal.
(2009b) provide approximately one million rules,but we use only approximately half of them, becausewe use only rules that can shorten a sentence, andonly in the direction that shortens the sentence.From each extractive candidate eij , we pro-duced abstractive candidates aij.1, aij.2, .
.
.
, aij.mij(Figure 1) by applying a single (each timedifferent) applicable paraphrasing rule to eij .From each of the resulting abstractive candidatesaij.l, we produced further abstractive candidatesaij.l.1, aij.l.2, .
.
.
, aij.l.mij.l by applying again a sin-gle (each time different) rule.
We repeated this pro-cess in a breadth-first manner, allowing up to at mostrulemax = 5 rule applications to an extractive candi-date eij , i.e., up to depth six in Figure 1, and up toa total of abstrmax = 50 abstractive candidates pereij .
Zhao et al (2009b) associate each paraphrasingrule with a score, intended to indicate its quality.6Whenever multiple paraphrasing rules could be ap-plied, we applied the rule with the highest score first.3.3 Human judgement annotationsFor each one of the 346 sources si, we placed itsextractive (at most kmax = 10) and abstractive (atmost abstrmax = 50) candidate compressions intoa single pool (extractive and abstractive together),and we selected from the pool the (at most) 10 can-didate compressions cij with the highest language6Each rule is actually associated with three scores.
We usethe ?Model 1?
score; see Zhao et al (2009b) for details.model scores, computed using a 3-gram languagemodel.7 For each cij , we formed a pair ?si, cij?,where si is a source sentence and cij a candidate(extractive or abstractive) compression.
This led to3,072 ?si, cij?
pairs.
Each pair was given to a humanjudge, who scored it for grammaticality (how gram-matical cij was) and meaning preservation (to whatextent cij preserved the most important informationof si).
Both scores were provided on a 1?5 scale (1for rubbish, 5 for perfect).
The dataset that we usein the following sections and that we make publiclyavailable comprises the 3,072 pairs and their gram-maticality and meaning preservation scores.We define the GM score of an ?si, cij?
pair to bethe sum of its grammaticality and meaning preser-vation scores.
Table 1 shows the distribution ofGM scores in the 3,072 pairs.
Low GM scores (2?5) are less frequent than higher scores (6?10), butthis is not surprising given that we selected pairswhose cij had high language model scores, thatwe used the kmax extractive compressions of eachsi that GA-EXTR considered best, and that we as-signed higher preference to applying paraphrasingrules with higher scores.
We note, however, that ap-plying a paraphrasing rule does not necessarily pre-serve neither grammaticality nor meaning, even ifthe rule has a high score.
Szpektor et al (2008) pointout that, for example, a rule like ?X acquire Y ??
?X buy Y ?
may work well in many contexts, butnot in ?Children acquire language quickly?.
Sim-ilarly, ?X charged Y with?
?
?X accused Y of?should not be applied to sentences about batteries.Many (but not all) inappropriate rule applications7We used SRILM with modified Kneser-Ney smoothing(Stolcke, 2002).
We trained the language model on approxi-mately 4.5 million sentences from the TIPSTER corpus.4Training part Test partGM extractive abstractive total extractive abstractive totalscore candidates candidates candidates candidates candidates candidates2 13 (1.3%) 10 (1.3%) 23 (1.3%) 19 (1.9%) 2 (0.4%) 21 (1.5%)3 26 (2.7%) 28 (3.6%) 54 (3.1%) 10 (1.0%) 0 (0%) 10 (0.7%)4 55 (5.8%) 29 (5.1%) 94 (5.5%) 51 (5.3%) 26 (6.2%) 77 (5.5%)5 52 (5.5%) 65 (8.5%) 117 (6.9%) 77 (8.0%) 42 (10.0%) 119 (8.6%)6 102 (10.9%) 74 (9.7%) 176 (10.3%) 125 (13.0%) 83 (19.8%) 208 (15.1%)7 129 (13.8%) 128 (16.8%) 257 (15.1%) 151 (15.7%) 53 (12.6%) 204 (14.8%)8 157 (16.8%) 175 (23.0%) 332 (19.5%) 138 (14.3%) 85 (20.3%) 223 (16.1%)9 177 (18.9%) 132 (17.3%) 309 (18.2%) 183 (19.0%) 84 (20.1%) 267 (19.3%)10 223 (23.8%) 110 (14.4%) 333 (19.6%) 205 (21.3%) 43 (10.2%) 248 (18.0%)total 934 (55.1%) 761 (44.9%) 1,695 (100%) 959 (69.6%) 418 (30.4%) 1,377 (100%)Table 1: Distribution of GM scores (grammaticality plus meaning preservation) in our dataset.lead to low language model scores, which is partlywhy there are more extractive than abstractive can-didate compressions in the dataset; another reason isthat few or no paraphrasing rules apply to some ofthe extractive candidates.We use 1,695 (from 188 source sentences) of the3,072 pairs to train different versions of our abstrac-tive compressor?s ranking component, discussed be-low, and 1,377 pairs (from 158 sources) as a test set.3.4 Inter-annotator agreementAlthough we used a total of 16 judges (computer sci-ence graduate students), each one of the 3,072 pairswas scored by a single judge, because a prelimi-nary study indicated reasonably high inter-annotatoragreement.8 More specifically, before the datasetwas constructed, we created 161 ?si, cij?
pairs (from22 source sentences) in the same way, and we gavethem to 3 of the 16 judges.
Each pair was scored byall three judges.
The average (over pairs of judges)Pearson correlation of the grammaticality, meaningpreservation, and GM scores, was 0.63, 0.60, and0.69, respectively.9 We conjecture that the highercorrelation of GM scores, compared to grammati-cality and meaning preservation, is due to the factthat when a candidate compression looks bad thejudges sometimes do not agree if they should re-duce the grammaticality or the meaning preservation8The judges were fluent, but not native, English speakers.9The Pearson correlation ranges in [?1,+1] and measuresthe linear relationship of two variables.
A correlation of +1 in-dicates perfect positive relationship, while ?1 indicates perfectnegative relationship; a correlation of 0 signals no relationship.candidate average Pearsoncompressions correlationExtractive 112 0.71Abstractive 49 0.64All 161 0.69Table 2: Inter-annotator agreement on GM scores.score, but the difference does not show up in the GMscore (the sum).
Table 2 shows the average corre-lation of the GM scores of the three judges on the161 pairs, and separately for pairs that involved ex-tractive or abstractive candidate compressions.
Thejudges agreed more on extractive candidates, sincethe paraphrasing stage that is involved in the abstrac-tive candidates makes the task more subjective.103.5 Performance boundariesWhen presented with two pairs ?si, cij?
and?si, cij?
?with the same si and equally long cij andcij?
, an ideal ranking component should prefer thepair with the highest GM score.
More generally, toconsider the possibly different lengths of cij and cij?
,we first define the compression rate CR(cij |si) of acandidate compression cij as follows, where |?| islength in characters; lower values of CR are better.CR(cij |si) =|cij ||si|The GMC?
score of a candidate compression, whichalso considers the compression rate by assigning it a10The correlation that we measured on extractive candidates(0.71) is very close to the corresponding figure (0.746) that hasbeen reported by Clarke and Lapata (2006b).5Figure 2: Results of three SVR-based ranking components on our dataset, along with performance boundaries obtainedusing an oracle and a random baseline.
The right diagram shows how the performance of our best SVR-based rankingcomponent is affected when using only 33% and 63% of the training examples.weight ?, is then defined as follows.GMC?
(cij |si) = GM(cij |si)?
?
?
CR(cij |si)For a given ?, when presented with ?si, cij?
and?si, cij?
?, an ideal ranking component should preferthe pair with the highest GMC?
score.The upper curve of the left diagram of Figure 2shows the performance of an ideal ranking com-ponent, an oracle, on the test part of the dataset.For every source si, the oracle selects the ?si, cij?pair (among the at most 10 pairs of si) for whichGMC?
(cij |si) is maximum; if two pairs have iden-tical GMC?
scores, it prefers the one with the low-est CR(cij |si).
The vertical axis shows the averageGM(cij |si) score of the selected pairs, for all the sisources, and the horizontal axis shows the averageCR(cij |si).
Different points of the curve are obtainedby using different ?
values.
As the selected candi-dates get shorter (lower compression rate), the aver-age GM score decreases, as one would expect.1111The discontinuity in the oracle?s curve for average com-The other curves of Figure 2 correspond to al-ternative ranking components that we tested, dis-cussed below, which do not consult the judges?
GMscores.
For each si, these ranking components at-tempt to guess the GM scores of the ?si, cij?
pairsthat are available for si, and they then rank the pairsby GMC?
using the guessed GM scores.
The lowerpoints of the left diagram were obtained with a base-line ranking component that assigns a random GMscore to each pair.
The oracle and the baseline canbe seen as establishing upper and lower performanceboundaries of ranking components on our dataset.4 Our abstractive compressorOur abstractive sentence compressor operates in twostages.
Given a source sentence si, extractive andpression rates above 0.7, i.e., when long compressions are onlymildly penalized, is caused by the fact that many long candi-date compressions have high and almost equal GM scores, butstill very different compression rates; hence, a slight modifica-tion of ?
leads the oracle to select candidates with the same GMscores, but very different compression rates.6abstractive candidate compressions are first gener-ated as in Sections 3.1 and 3.2.
In a second stage, aranking component is used to select the best candi-date.
Below we discuss the three SVR-based rankingcomponents that we experimented with.4.1 Ranking candidates with an SVRAn SVR is very similar to a Support Vector Machine(Vapnik, 1998; Cristianini and Shawe-Taylor, 2000;Joachims, 2002), but it is trained on examples of theform ?xl, y(xl)?, where each xl ?
Rn is a vector of nfeatures, and y(xl) ?
R. The SVR learns a functionf : Rn ?
R intended to return f(x) values as closeas possible to the correct y(x) values.12 In our case,each vector xij contains features providing informa-tion about an ?si, cij?
pair of a source sentence siand a candidate compression cij .
For pairs that havebeen scored by human judges, the f(xij) returned bythe SVR should ideally be y(xij) = GMC?
(cij |si);once trained, however, the SVR may be presentedwith xij vectors of unseen ?si, cij?
pairs.For an unseen source si, our abstractive compres-sor first generates extractive and abstractive candi-dates cij , it then forms the vectors xij of all thepairs ?si, cij?, and it returns the cij for which theSVR?s f(xij) is maximum.
On a test set (like thetest part of our dataset), if the f(xij) values theSVR returns are very close to the correspondingy(xij) = GMC?
(cij |si) scores, the ranking compo-nent will tend to select the same cij for each si as theoracle, i.e., it will achieve optimum performance.4.2 Base form of our SVR ranking componentThe simplest form of our SVR-based ranking compo-nent, called SVR-BASE, uses vectors xij that includethe following features of ?si, cij?.
Hereafter, if cij isan extractive candidate, then e(cij) = cij ; otherwisee(cij) is the extractive candidate that cij was derivedfrom by applying paraphrasing rules.13?
The language model score of si and cij (2 fea-12We use LIBSVM (http://www.csie.ntu.edu.tw/?cjlin/libsvm) with an RBF kernel, which permits theSVR to learn non-linear functions.
We also experimented with aranking SVM, but the results were slightly inferior.13All the feature values are normalized in [0, 1]; this also ap-plies to the GMC?
scores when they are used by the SVR.
Thee(cij) of each cij and the paraphrasing rules that were appliedto e(cij) to produce cij are also included in the dataset.tures), computed as in Section 3.3.?
The F (e(cij)|si) score that GA-EXTR returned.?
The compression rate CR(e(cij)|si).?
The number (possibly zero) of paraphrasingrules that were applied to e(cij) to produce cij .4.3 Additional PMI-based featuresFor two words w1, w2, their PMI score is:PMI(w1, w2) = logP (w1, w2)P (w1) ?
P (w2)where P (w1, w2) is the probability of w1, w2 co-occurring; we require them to co-occur in the samesentence at a maximum distance of 10 tokens.14If w1, w2 are completely independent, then theirPMI score is zero.
If they always co-occur, theirPMI score is maximum, equal to ?
logP (w1) =?
logP (w2).15 We use PMI to assess if the wordsof a candidate compression co-occur as frequentlyas those of the source sentence; if not, this may indi-cate an inappropriate application of a paraphrasingrule (e.g., having replaced ?charged Y with?
by ?Xaccused Y of?
in a sentence about batteries).More specifically, we define the PMI(?)
score ofa sentence ?
to be the average PMI(wi, wj) of ev-ery two content words wi, wj that co-occur in ?
ata maximum distance of 10 tokens; below N is thenumber of such pairs.PMI(?)
=1N?
?i,jPMI(wi, wj)In our second SVR-based ranking component, SVR-PMI, we compute PMI(si), PMI(e), and PMI(cij),and we include them as three additional features;otherwise SVR-PMI is identical to SVR-BASE.14We used texts from TIPSTER and AQUAINT, a total of 953million tokens, to estimate PMI(w1, w2).15A problem with PMI is that two frequent and completely de-pendent words receive lower scores than two other, less frequentcompletely dependent words (Manning and Schutze, 2000).Pecina (2005), however, found PMI to be the best collocationextraction measure; and Newman et al (2010) found it to be thebest measure of ?topical coherence?
for sets of words.74.4 Additional LDA-based featuresOur third SVR-based ranking component includesfeatures from a Latent Dirichlet Allocation (LDA)model (Blei et al, 2003).
Roughly speaking, LDAmodels assume that each document d of |d| wordsw1, .
.
.
, w|d| is generated by iteratively (for r =1, .
.
.
, |d|) selecting a topic tr from a document-specific multinomial distribution P (t|d) overK top-ics, and then (for each r) selecting a word wr from atopic-specific multinomial distribution P (w|t) overthe vocabulary.16 The probability, then, of encoun-tering a word w in a document d is the following.P (w|d) =?tP (w|t) ?
P (t|d) (1)An LDA model can be trained on a corpus to estimatethe parameters of the distributions it involves; andgiven a trained model, there are methods to infer thetopic distribution P (t|d?)
of a new document d?.17In our case, we treat each source sentence asa new document d?, and we use an LDA modeltrained on a generic corpus to infer the topic distri-bution P (t|d?)
of the source sentence.18 We assumethat a good candidate compression should containwords with high P (w|d?
), computed as in Equation1 with P (t|d) = P (t|d?)
and using the P (w|t) thatwas learnt during training, because words with highP (w|d?)
are more likely to express (high P (w|t))prominent topics (high P (t|d?))
of the source.Consequently, we can assess how good a can-didate compression is by computing the averageP (w|d?)
of its words; we actually compute theaverage logP (w|d?).
More specifically, for agiven source si and another sentence ?, we defineLDA(?|si) as follows (d?
= si), where w1, .
.
.
, w|?|are now the words of ?, ignoring stop-words.LDA(?|si) =1|?|?|?|?r=1logP (wr|si)16The document-specific parameters of the first multinomialdistribution are drawn from a Dirichlet distribution.17We use MALLET (http://mallet.cs.umass.edu),with Gibbs sampling (Griffiths and Steyvers, 2004).
Weset K = 800, having first experimented with K =200, 400, 600, 800, 1000.18We trained the LDA model on approximately 106,000 arti-cles from the TIPSTER and AQUAINT corpora.In our third SVR-based ranking component, SVR-PMI-LDA, the feature vector xij of each ?si, cij?pair includes LDA(cij |si), LDA(e(cij)|si), andLDA(si|si) as additional features; otherwise, SVR-PMI-LDA is identical to SVR-PMI.
The third featureallows the SVR to check how far LDA(cij |si) andLDA(e(cij)|si) are from LDA(si|si).5 ExperimentsTo assess the performance of SVR-BASE, SVR-PMI,and SVR-PMI-LDA, we trained the three SVR-basedranking components on the training part of ourdataset, and we evaluated them on the test part.
Werepeated the experiments for 81 different ?
values toobtain average GM scores at different average com-pression rates (Section 3.5).
The resulting curvesof the three SVR-based ranking components are in-cluded in Figure 2 (left diagram).
Overall, SVR-PMI-LDA performed better than SVR-PMI and SVR-BASE, since it achieved the best average GM scoresthroughout the range of average compression rates.In general, SVR-PMI also performed better thanSVR-BASE, though the average GM score of SVR-BASE was sometimes higher.
All three SVR-basedranking components performed better than the ran-dom baseline, but worse than the oracle; hence, thereis scope for further improvements in the rankingcomponents, which is also why we believe other re-searchers may wish to experiment with our dataset.The oracle selected abstractive (as opposed tosimply extractive) candidates for 20 (13%) to 30(19%, depending on ?)
of the 158 source sentencesof the test part; the same applies to the SVR-basedranking components.
Hence, good abstractive can-didates (or at least better than the corresponding ex-tractive ones) are present in the dataset.
Humans,however, produce mostly abstractive compressions,as already discussed; the fact that the oracle (whichuses human judgements) does not select abstrac-tive candidates more frequently may be an indica-tion that more or better abstractive candidates areneeded.
We plan to investigate alternative methodsto produce more abstractive candidates.
For exam-ple, one could translate each source to multiple pivotlanguages and back to the original language by usingmultiple commercial machine translation engines in-stead of, or in addition to applying paraphrasing8source generatedGillette was considered a leading financial analyst on the beverage in-dustry - one who also had an expert palate for wine tasting.Gillette was seen as a leading financial analyst on the beverage industry- one who also had an expert palate.Nearly 200,000 lawsuits were brought by women who said they suf-fered injuries ranging from minor inflammation to infertility and insome cases, death.Lawsuits were made by women who said they suffered injuries rangingfrom inflammation to infertility in some cases, death.Marcello Mastroianni, the witty, affable and darkly handsome Italianactor who sprang on international consciousness in Federico Fellini?s1960 classic ?La Dolce Vita,?
died Wednesday at his Paris home.Marcello Mastroianni died Wednesday at his home.A pioneer in laparoscopy, he held over 30 patents for medical instru-ments used in abdominal surgery such as tubal ligations.He held over 30 patents for the medical tools used in abdominal surgery.LOS ANGELES - James Arnold Doolittle, a Los Angeles dance im-presario who brought names such as Joffrey and Baryshnikov to localdance stages and ensured that a high-profile ?Nutcracker Suite?
waspresented here every Christmas, has died.James Arnold Doolittle, a Los Angeles dance impresario is dead.After working as a cashier for a British filmmaker in Rome, he joinedan amateur theatrical group at the University of Rome, where he wastaking some classes.After working as a cashier for a British filmmaker in Rome, he joinedan amateur group at the University of Rome, where he was using someclasses.He was a 1953 graduate of the Johns Hopkins Medical School and aftercompleting his residency in gynecology and surgery, traveled to Den-mark where he joined the staff of the National Cancer Center there.He was a graduate of the Johns Hopkins Medical School and traveledto Denmark where he joined a member of the National Cancer Centerthere.Mastroianni, a comic but also suave and romantic leading man in some120 motion pictures, had suffered from pancreatic cancer.Mastroianni, a leading man in some 120 motion pictures, had subjectedto cancer.Table 3: Examples of good (upper five) and bad (lower three) compressions generated by our abstractive compressor.rules.
An approach of this kind has been proposedfor sentence paraphrasing (Zhao et al, 2010).The right diagram of Figure 2 shows how the per-formance of SVR-PMI-LDA is affected when using33% or 63% of the training ?si, ci?
pairs.
As moreexamples are used, the performance improves, sug-gesting that better results could be obtained by usingmore training data.
Finally, Table 3 shows examplesof good and bad compressions the abstractive com-pressor produced with SVR-PMI-LDA.6 Conclusions and future workWe presented a new dataset that can be used to trainand evaluate the ranking components of generate-and-rank abstractive sentence compressors.
Thedataset contains pairs of source sentences and can-didate extractive or abstractive compressions.
Thecandidate compressions were obtained by first ap-plying a state-of-the-art extractive compressor to thesource sentences, and then applying existing para-phrasing rules, obtained from parallel corpora.
Thedataset?s pairs have been scored by human judgesfor grammaticality and meaning preservation.
Wediscussed how performance boundaries for rankingcomponents that use the dataset can be establishedby using an oracle and a random baseline, and byconsidering different compression rates.
We alsodiscussed the current version of an abstractive sen-tence compressor that we are developing, and howthe dataset was used to train and evaluate three dif-ferent SVR-based ranking components of the com-pressor with gradually more elaborate features sets.The feature set of the best ranking component thatwe tested includes language model scores, the con-fidence and compression rate of the underlying ex-tractive compressor, the number of paraphrasingrules that have been applied, word co-occurrencefeatures, as well as features based on an LDA model.In future work, we plan to improve our abstractivesentence compressor, possibly by including morefeatures in the ranking component.
We also planto investigate alternative ways to produce candidatecompressions, such as sentence paraphrasing meth-ods that exploit multiple commercial machine trans-lation engines to translate the source sentences tomultiple pivot languages and back to the originallanguage (Zhao et al, 2010).
Using methods of thiskind, it may be possible to produce a second, alterna-tive dataset with more and possibly better abstractivecandidates.
We also plan to make the final version ofour abstractive compressor publicly available.AcknowledgmentsThis work was partly carried out during INDIGO, anFP6 IST project funded by the European Union, withadditional funding from the Greek General Secre-9tariat of Research and Technology.19ReferencesI.
Androutsopoulos and P. Malakasiotis.
2010.
A surveyof paraphrasing and textual entailment methods.
Jour-nal of Artificial Intelligence Research, 38:135?187.C.
Bannard and C. Callison-Burch.
2005.
Paraphras-ing with bilingual parallel corpora.
In Proceedings ofACL, pages 597?604, Ann Arbor, MI.D.
Blei, A. Ng, and M. Jordan.
2003.
Latent Dirichletallocation.
In Journal of Machine Learning Research.C.
Callison-Burch, M. Osborne, and P. Koehn.
2006.
Re-evaluating the role of BLEU in machine translationresearch.
In Proceedings of EACL, pages 249?256,Trento, Italy.C.
Callison-Burch.
2008.
Syntactic constraints on para-phrases extracted from parallel corpora.
In Proceed-ings of EMNLP, pages 196?205, Honolulu, HI.J.
Clarke and M. Lapata.
2006a.
Constraint-basedsentence compression: An integer programming ap-proach.
In Proceedings of ACL-COLING.J.
Clarke and M. Lapata.
2006b.
Models for sentencecompression: A comparison across domains, trainingrequirements and evaluation measures.
In Proceedingsof ACL-COLING.J.
Clarke and M. Lapata.
2008.
Global inference forsentence compression: An integer linear programmingapproach.
Journal of Artificial Intelligence Research,1(31):399?429.T.
Cohn and M. Lapata.
2007.
Large margin syn-chronous generation and its application to sentencecompression.
In Proceedings of EMNLP-CONLL.T.
Cohn and M. Lapata.
2008.
Sentence compressionbeyond word deletion.
In Proceedings of COLING.T.
Cohn and M. Lapata.
2009.
Sentence compression astree to tree tranduction.
Journal of Artificial Intelli-gence Research, 34:637?674.S.
Corston-Oliver.
2001.
Text compaction for displayon very small screens.
In Proceedings of the NAACLWorkshop on Automatic Summarization.N.
Cristianini and J. Shawe-Taylor.
2000.
An In-troduction to Support Vector Machines and OtherKernel-based Learning Methods.
Cambridge Univer-sity Press.D.
Galanis and I. Androutsopoulos.
2010.
An extrac-tive supervised two-stage method for sentence com-pression.
In Proceedings of HLT-NAACL.T.
Griffiths and M. Steyvers.
2004.
Finding scientifictopics.
In Proceedings of the National Academy of Sci-ences.19Consult http://www.ics.forth.gr/indigo/.H.
Jing.
2000.
Sentence reduction for automatic textsummarization.
In Proceedings of ANLP.T.
Joachims.
2002.
Learning to Classify Text Using Sup-port Vector Machines: Methods, Theory, Algorithms.Kluwer.D.
Kauchak and R. Barzilay.
2006.
Paraphrasing forautomatic evaluation.
In Proceedings of the HLT-NAACL, pages 455?462, New York, NY.K.
Knight and D. Marcu.
2002.
Summarization be-yond sentence extraction: A probalistic approach tosentence compression.
Artificial Intelligence, 139(1).P.
Koehn.
2009.
Statistical Machine Translation.
Cam-bridge University Press.S.
Kok and C. Brockett.
2010.
Hitting the right para-phrases in good time.
In Proceedings of HLT-NAACL,pages 145?153, Los Angeles, CA.N.
Madnani and B.J.
Dorr.
2010.
Generating phrasal andsentential paraphrases: A survey of data-driven meth-ods.
Computational Linguistics, 36(3):341?387.N.
Madnani, D. Zajic, B. Dorr, N. F. Ayan, and J. Lin.2007.
Multiple alternative sentence compressionsfor automatic text summarization.
In Proceedings ofDUC.C.D.
Manning and H. Schutze.
2000.
Foundations ofStatistical Natural Language Processing.
MIT Press.R.
McDonald.
2006.
Discriminative sentence compres-sion with soft syntactic constraints.
In Proceedings ofEACL.D.
Newman, J.H.
Lau, K. Grieser, and T. Baldwin.
2010.Automatic evaluation of topic coherence.
In Proceed-ings of HLT-NAACL.T.
Nomoto.
2009.
A comparison of model free versusmodel intensive approaches to sentence compression.In Proceedings of EMNLP.J.
F. Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proceedings of ACL.S.
Pado?, M. Galley, D. Jurafsky, and C. D. Manning.2009.
Robust machine translation evaluation with en-tailment features.
In Proceedings of ACL-IJCNLP,pages 297?305, Singapore.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proceedings of ACL, pages 311?318,Philadelphia, PA.P.
Pecina.
2005.
An extensive empirical study of colloca-tion extraction methods.
In Proceedings of the StudentResearch Workshop of ACL.S.
Riezler, T.H.
King, R. Crouch, and A. Zaenen.2003.
Statistical sentence condensation using ambigu-ity packing and stochastic disambiguation methods forlexical-functional grammar.
In Proceedings of HLT-NAACL.10S.
Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, andY.
Liu.
2007.
Statistical machine translation for queryexpansion in answer retrieval.
In Proceedings of ACL,pages 464?471, Prague, Czech Republic.A.
Stolcke.
2002.
SRILM - an extensible language mod-eling toolkit.
In Proceedings of the International Con-ference on Spoken Language Processing, pages 901?904.I.
Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.2008.
Contextual preferences.
In Proceedings ofACL-HLT, pages 683?691, Columbus, OH.I.
Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.2005.
Support vector machine learning for indepen-dent and structured output spaces.
Machine LearningResearch, 6:1453?1484.V.
Vapnik.
1998.
Statistical Learning Theory.
John Wi-ley.E.
Yamangil and S. M. Shieber.
2010.
Bayesian syn-chronous tree-substitution grammar induction and itsapplication to sentence compression.
In Proceedingsof ACL.S.
Zhao, C. Niu, M. Zhou, T. Liu, and S. Li.
2008.
Com-bining multiple resources to improve SMT-based para-phrasing model.
In Proceedings of ACL-HLT, pages1021?1029, Columbus, OH.S.
Zhao, X. Lan, T. Liu, and S. Li.
2009a.
Application-driven statistical paraphrase generation.
In Proceed-ings of ACL.S.
Zhao, H. Wang, T. Liu, and S. Li.
2009b.
Extract-ing paraphrase patterns from bilingual parallel cor-pora.
Natural Language Engineering, 15(4):503?526.S.
Zhao, H. Wang, X. Lan, and T. Liu.
2010.
Leverag-ing multiple MT engines for paraphrase generation.
InProceedings of COLING.L.
Zhou, C.-Y.
Lin, and Eduard Hovy.
2006.
Re-evaluating machine translation results with paraphrasesupport.
In Proceedings of EMNLP, pages 77?84.11
