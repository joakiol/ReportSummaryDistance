Proceedings of NAACL-HLT 2013, pages 138?147,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsLearning a Part-of-Speech Tagger from Two Hours of AnnotationDan GarretteDepartment of Computer ScienceThe University of Texas at Austindhg@cs.utexas.eduJason BaldridgeDepartment of LinguisticsThe University of Texas at Austinjbaldrid@utexas.eduAbstractMost work on weakly-supervised learning forpart-of-speech taggers has been based on un-realistic assumptions about the amount andquality of training data.
For this paper, weattempt to create true low-resource scenariosby allowing a linguist just two hours to anno-tate data and evaluating on the languages Kin-yarwanda and Malagasy.
Given these severelylimited amounts of either type supervision(tag dictionaries) or token supervision (labeledsentences), we are able to dramatically im-prove the learning of a hidden Markov modelthrough our method of automatically general-izing the annotations, reducing noise, and in-ducing word-tag frequency information.1 IntroductionThe high performance achieved by part-of-speech(POS) taggers trained on plentiful amounts of la-beled word tokens is a success story of computa-tional linguistics (Manning, 2011).
However, re-search on learning taggers using type supervision(e.g.
tag dictionaries or morphological transducers)has had a more checkered history.
The setting isa seductive one: by labeling the possible parts-of-speech for high frequency words, one might learnaccurate taggers by incorporating the type informa-tion as constraints to a semi-supervised generativelearning model like a hidden Markov model (HMM).Early work showed much promise for this strategy(Kupiec, 1992; Merialdo, 1994), but successive ef-forts in recent years have continued to peel away andaddress layers of unrealistic assumptions about thesize, coverage, and quality of the tag dictionariesthat had been used (Toutanova and Johnson, 2008;Ravi and Knight, 2009; Hasan and Ng, 2009; Gar-rette and Baldridge, 2012).
This paper attempts tostrip away further layers so we can build better intu-itions about the effectiveness of type-supervised andtoken-supervised strategies in a realistic setting ofPOS-tagging for low-resource languages.In most previous work, tag dictionaries are ex-tracted from a corpus of annotated tokens.
To ex-plore the type-supervised scenario, these have beenused as a proxy for dictionaries produced by lin-guists.
However, this overstates their effectiveness.Researchers have often manually pruned tag dictio-naries by removing low-frequency word/tag pairs;this violates the assumption that frequency informa-tion is not available.
Others have also created tagdictionaries by extracting every word/tag pair in alarge, labeled corpus, including the test data?eventhough actual applications would never have suchcomplete lexical knowledge.
Dictionaries extractedfrom corpora are also biased towards including onlythe most likely tag for each word type, resulting ina cleaner dictionary than one would find in real sce-nario.
Finally, tag dictionaries extracted from anno-tated tokens benefit from the annotation process oflabeling and review and refinement over an extendedcollaboration period.
Such high quality annotationsare simply not available for most low-resource lan-guages.This paper describes an approach to learninga POS-tagger that can be applied in a truly low-resource scenario.
Specifically, we discuss tech-niques that allow us to learn a tagger given only138the amount of labeled data that a human annotatorcould provide in two hours.
Here, we evaluate onthe languages Malagasy and Kinyarwanda, as wellas English as a control language.
Furthermore, weare interested in whether type-supervision or token-supervision is more effective, given the strict timeconstraint; accordingly, we had annotators produceboth a tag dictionary and a set of labeled sentences.The data produced under our conditions differs inseveral ways from the labeled data used in previouswork.
Most obviously, there is less of it.
Insteadof using hundreds of thousands of labeled tokensto construct a tag dictionary (and hundreds of thou-sands more as unlabeled (raw) data for training), weonly use the 1k-2k labeled tokens or types providedby our annotators within the timeframe.
Our train-ing data is also much noisier than the data from atypical corpus: the annotations were produced bya single non-native-speaker working alone for twohours.
Therefore, dealing with the size and qualityof training data were core challenges to our task.To learn a POS-tagger from so little labeled data,we developed an approach that starts by generalizingthe initial annotations to the entire raw corpus.
Ourapproach uses label propagation (LP) (Talukdar andCrammer, 2009) to infer tag distributions on unla-beled tokens.
We then apply a novel weighted vari-ant of the model minimization procedure originallydeveloped by Ravi and Knight (2009) to estimate se-quence and word-tag frequency information from anunlabeled corpus by approximating the minimal setof tag bigrams needed to explain the data.
This com-bination of techniques turns a tiny, unweighted, ini-tial tag dictionary into a weighted tag dictionary thatcovers the entire corpus?s vocabulary.
This weightedinformation limits the potential damage of tag dic-tionary noise and bootstraps frequency informationto approximate a good starting point for the learningof an HMM using expectation-maximization (EM),and far outperforms just using EM on the raw an-notations themselves.2 DataOur experiments use Kinyarwanda (KIN), Malagasy(MLG), and English (ENG).
KIN is a Niger-Congolanguage spoken in Rwanda.
MLG is an Austrone-sian language spoken in Madagascar.
Both KIN andMLG are low-resource and KIN is morphologically-rich.
For each language, the word tokens are dividedinto four sets: training data to be labeled by anno-tators, raw training data, development data, and testdata.
For consistency, we use 100k raw tokens foreach language.Data sources For ENG, we used the Penn Tree-bank (PTB) (Marcus et al 1993).
Sections 00-04were used as raw data, 05-14 as a dev set, and 15-24(473K tokens) as a test set.
The PTB uses 45 dis-tinct POS tags.
The KIN texts are transcripts of testi-monies by survivors of the Rwandan genocide pro-vided by the Kigali Genocide Memorial Center.
TheMLG texts are articles from the websites1 Lakroa andLa Gazette and Malagasy Global Voices,2 a citizenjournalism site.3 Texts in both KIN and MLG weretokenized and labeled with POS tags by two linguis-tics graduate students, each of which was studyingone of the languages.
The KIN and MLG data have14 and 24 distinct POS tags, respectively, and weredeveloped by the annotators.Time-bounded annotation One of our main goalsis to evaluate POS-tagging for low-resource lan-guages in experiments that correspond better to areal-world scenario than previous work.
As such, wecollected two forms of annotation, each constrainedby a two-hour time limit.
The annotations were doneby the same linguists who had annotated the KINand MLG data mentioned above.
Our experimentsare thus relevant to the reasonable context in whichone has access to a linguist who is familiar with thetarget language and a given set of POS tags.The first annotation task was to directly produce adictionary of words to their possible POS tags?i.e.,collecting an actual tag dictionary of the form that istypically simulated in POS-tagging experiments.
Foreach language, we compiled a list of word types, or-dered starting with most frequent, and presented itto the annotator with a list of admissible POS tags.The annotator had two hours to specify POS tags foras many words as possible.
The word types and fre-quencies used for this task were taken from the rawtraining data and did not include the test sets.
This1www.lakroa.mg and www.lagazette-dgi.com2mg.globalvoicesonline.org/3The public-domain data is available at github.com/dhgarrette/low-resource-pos-tagging-2013139data is used for what will call type-supervised train-ing.
The second task was annotating full sentenceswith POS tags, again for two hours.
We refer to thisas token-supervised training.Having both sets of annotations allows us to in-vestigate the relative value of each with respect totraining taggers.
Token-supervision provides valu-able frequency and tag context information, buttype-supervision produces larger dictionaries.
Thiscan be seen in Table 1, where the dictionary sizecolumn in the table gives the number of uniqueword/tag pairs derived from the data.We also wanted to directly compare the two an-notators to see how the differences in their relativeannotation speeds and quality would affect the over-all ability to learn an accurate tagger.
We thus hadthem complete the same two tasks for English.
Ascan be seen in Table 1, there are clear differencesbetween the two annotators.
Most notably, annota-tor B was faster at annotating full sentences whileannotator A was faster at annotating word types.3 ApproachOur approach to learning POS-taggers is based onGarrette and Baldridge (2012), which properly sep-arated test data from learning data, unlike much pre-vious work.
The input to our system is a raw cor-pus and either a human-generated tag dictionary orhuman-tagged sentences.
The majority of the sys-tem is the same for both kinds of labeled trainingdata, but the following description will point out dif-ferences.
The system has four main parts, in order:1.
Tag dictionary expansion2.
Weighted model minimization3.
Expectation maximization (EM) HMM training4.
MaxEnt Markov Model (MEMM) training3.1 Tag dictionary expansionIn a low-resource setting, most word types will notbe found in the initial tag dictionary.
EM-HMM train-ing uses the tag dictionary to limit ambiguity, so asparse tag dictionary is problematic because it doesnot sufficiently confine the parameter space.4 Small4This is of course not the case for purely unsupervised tag-gers, though we also note that it is not at all clear they are actu-ally learning taggers for part-of-speech.sent.
tok.
dict.KIN human sentences A 90 1537 750KIN human TD A 1798MLG human sentences B 92 1805 666MLG human TD B 1067ENG human sentences A 86 1897 903ENG human TD A 1644ENG human sentences B 107 2650 959ENG human TD B 1090Table 1: Statistics for Kinyarwanda, Malagasy, andEnglish data annotated by annotators A and B.dictionaries also interact poorly with the model min-imization of Ravi et al(2010): if there are too manyunknown words, and every tag must be consideredfor them, then the minimal model will simply be theone that assumes that they all have the same tag.For these reasons, we automatically expand aninitial small dictionary into one that has coverage formost of the vocabulary.
We use label propagation(LP)?specifically, the Modified Adsorption (MAD)algorithm (Talukdar and Crammer, 2009)5?whichis a graph-based technique for spreading labels be-tween related items.
Our graphs connect tokennodes to each other via feature nodes and are seededwith POS-tag labels from the human-annotated data.Defining the LP graph Our LP graph has severaltypes of nodes, as shown in Figure 1.
The graphcontains a TOKEN node for each token of the la-beled corpus (when available) and raw corpus.
Eachword type has one TYPE node that is connected toits TOKEN nodes.
Both kinds of nodes are con-nected with feature nodes.
The PREVWORD x andNEXTWORD x nodes represent the features of a to-ken being preceded by or followed by word type x inthe corpus.
These bigram features capture extremelysimple syntactic information.
To capture shallowmorphological relatedness, we use prefix and suffixnodes that connect word types that share prefix orsuffix character sequences up to length 5.
For eachnode-feature pair, the connecting edge is weightedas 1/N where N is the number of nodes connectedto the particular feature.5The open-source MAD implementation is provided throughJunto: github.com/parthatalukdar/junto140TOKEN A 1 1 TOKEN walks 2 3SUFFIX1 eTOKEN barks 1 3SUFFIX1 sPREVWORD dogSUFFIX2 heTYPE ATOKEN The 2 1 TOKEN walks 3 3TOKEN The 3 1PREVWORD manNEXTWORD .TYPE barksTYPE TheSUFFIX2 ksDICTPOS DNEXTWORD dogDICTPOS N DICTPOS VTYPE walksNEXTWORD manPREVWORD ?b?Figure 1: Subsets of the LP graph showing regions of connected nodes.
Graph represents the sentences ?Adog barks .
?, ?The dog walks .
?, and ?The man walks .
?We also explored the effectiveness of using an ex-ternal dictionary in the graph since this is one of thefew available sources of information for many low-resource languages.
Though a standard dictionaryprobably will not use the same POS tag set that weare targeting, it nevertheless provides informationabout the relatedness of various word types.
Thus,we use nodes DICTPOS p that indicate that a particu-lar word type is listed as having POS p in the dictio-nary.
Crucially, these tags bear no particular con-nection to the tags we are predicting: we still tar-get the tags defined by the linguist who annotatedthe types or tokens used, which may be more orless granular than those provided in the dictionary.As external dictionaries, we use English Wiktionary(614k entries), malagasyworld.org (78k entries),and kinyarwanda.net (3.7k entries).6Seeding the graph is straightforward.
With token-supervision, labels for tokens are injected into thecorresponding TOKEN nodes with a weight of 1.0.In the type-supervised case, any TYPE node that ap-pears in the tag dictionary is injected with a uniformdistribution over the tags in its tag dictionary entry.Toutanova and Johnson (2008) (also, Ravi andKnight (2009)) use a simple method for predict-ing possible tags for unknown words: a set of 100most common suffixes are extracted and then mod-els of P(tag|suffix) are built and applied to unknownwords.
However, these models suffer with an ex-tremely small set of labeled data.
Our method usescharacter affix feature nodes along with sequencefeature nodes in the LP graph to get distributionsover unknown words.
Our technique thus subsumes6Wiktionary (wiktionary.org) has only 3,365 en-tries for Malagasy and 9 for Kinyarwanda.theirs as it can infer tag dictionary entries for wordswhose suffixes do not show up in the labeled data (orwith enough frequency to be reliable predictors).Extracting a result from LP LP assigns a labeldistribution to every node.
Importantly, each indi-vidual TOKEN gets its own distribution instead ofsharing an aggregation over the entire word type.From this graph, we extract a new version of theraw corpus that contains tags for each token.
Thisprovides the input for model minimization.We seek a small set of likely tags for each token,but LP gives each token a distribution over the entireset of tags.
Most of the tags are simply noise, someof which we remove by normalizing the weights andexcluding tags with probability less than 0.1.
Af-ter applying this cutoff, the weights of the remain-ing tags are re-normalized.
We stress that this tagdictionary cutoff is not like those used in past re-search, which were done with respect to frequen-cies obtained from labeled tokens: we use either noword-tag frequency information (type-supervision)or very small amounts of word-tag frequency infor-mation indirectly through LP (token-supervision).7Some tokens might not have any associated taglabels after LP.
This occurs when there is nopath from a TOKEN node to any seeded nodes orwhen all tags for the TOKEN node have weights lessthan the threshold.
Since we require a distributionfor every token, we use a default distribution forsuch cases.
Specifically, we use the unsupervisedemission probability initialization of Garrette andBaldridge (2012), which captures both the estimatedfrequency of a tag and its openness using only a7See Banko and Moore (2004) for further discussion of theseissues.141?b?
The man saw the saw ?b?
?b?DNV1.01.01.0 0.20.81.00.70.31.0Figure 2: Weighted, greedy model minimizationgraph showing a potential state between the stagesof the tag bigram choosing algorithm.
Solid edges:selected bigrams.
Dotted edges: holes in the path.small tag dictionary and unlabeled text.Finally, we ensure that tokens of words in theoriginal tag dictionary are only assigned tags fromits entry.
With this filter, LP of course does not addnew tags to known words (without it, we found per-formance drops).
If the intersection of the small tagdictionary entry and the token?s resulting distribu-tion from LP (after thresholding) is empty, we fallback to the filtered and renormalized default distri-bution for that token?s type.The result of this process is a sequence of (ini-tially raw) tokens, each associated with a distribu-tion over a subset of tags.
From this we can extractan expanded tag dictionary for use in subsequentstages that, crucially, provides tag information forwords not covered by the human-supplied tag dic-tionary.
This expansion is simple: an unknown wordtype?s set of tags is the union of all tags assigned toits tokens.
Additionally, we add the full entries ofword types given in the original tag dictionary.3.2 Weighted model minimizationEM-HMM training depends crucially on having aclean tag dictionary and a good starting point for theemission distributions.
Given only raw text and atag dictionary, these distributions are difficult to es-timate, especially in the presence of a very sparseor noisy tag dictionary.
Ravi and Knight (2009) usemodel minimization to remove tag dictionary noiseand induce tag frequency information from raw text.Their method works by finding a minimal set of tagbigrams needed to explain a raw corpus.Model minimization is a natural fit for our systemsince we start with little or no frequency informa-tion and automatic dictionary expansion introducesnoise.
We extend the greedy model minimizationprocedure of Ravi et al(2010), and its enhance-ments by Garrette and Baldridge (2012), to developa novel weighted minimization procedure that usesthe tag weights from LP to find a minimal modelthat is biased toward keeping tag bigrams that haveconsistently high weights across the entire corpus.The new weighted minimization procedure fits wellin our pipeline by allowing us to carry the tag dis-tributions forward from LP instead of simply throw-ing that information away and using a traditional tagdictionary.In brief, the procedure works by creating a graphsuch that each possible tag of each raw-corpus tokenis a vertex (see Figure 2).
Any edge that would con-nect two tags of adjacent tokens is a potential tag bi-gram choice.
The algorithm first selects tag bigramsuntil every token is covered by at least one bigram,then selects tag bigrams that fill gaps between exist-ing edges until there is a complete bigram path forevery sentence in the raw corpus.8Ravi et al(2010) select tag bigrams that coverthe most new words (stage 1) or fill the most holesin the tag paths (stage 2).
Garrette and Baldridge(2012) introduced the tie-breaking criterion that bi-gram choices should seek to introduce the small-est number of new word/tag pairs possible into thepaths.
Our criteria adds to this by using the tagweights on each token: a tag bigram b is chosen bysumming up the node weights of any not-yet cov-ered words touched by the tag bigram b, dividingthis sum by one plus the number of new word/tagpairs that would be added by b, and choosing the bthat maximizes this value.9Summing node weights captures the intuition ofRavi et al(2010) that good bigrams are those whichhave high coverage of new words: each newly cov-ered node contributes additional (partial) counts.However, by using the weights instead of full counts,we also account for the confidence assigned by LP.Dividing by the number of new word/tag pairs addedfocuses on bigrams that reuse existing tags for words8Ravi et al(2010) include a third phase of iterative modelfitting; however, we found this stage to be not only expensive,but also unhelpful because it frequently yields negative results.9In the case of token-supervision, we pre-select all tag bi-grams appearing in the labeled corpus since these are assumedto be known high-quality tag bigrams and word/tag pairs.142and thereby limits the addition of new tags for eachword type.At the start of model minimization, there are noselected tag bigrams, and thus no valid path throughany sentence in the corpus.
As bigrams are selected,we can begin to cover subsequences and eventuallyfull sentences.
There may be multiple valid taggingsfor a sentence, so after each new bigram is selected,we run the Viterbi algorithm over the raw corpus us-ing the set of selected tag bigrams as a hard con-straint on the allowable transitions.
This efficientlyidentifies the highest-weight path through each sen-tence, if one exists.
If such a path is found, we re-move the sentence from the corpus and store the tagsfrom the Viterbi tagging.
The algorithm terminateswhen a path is found for every raw corpus sentence.The result of weighted model minimization is thisset of tag paths.
Since each path represents a validtagging of the sentence, we use this output as a nois-ily labeled corpus for initializing EM in stage three.3.3 Tagger trainingStage one provides an expansion of the initial la-beled data and stage two turns that into a corpus ofnoisily labeled sentences.
Stage three uses the EMalgorithm initialized by the noisy labeling and con-strained by the expanded tag dictionary to producean HMM.10 The initial distributions are smoothedwith one-count smoothing (Chen and Goodman,1996).
If human-tagged sentences are available astraining data, then we use their counts to supplementthe noisy labeled text for initialization and we addtheir counts into every iteration?s result.The HMM produced by stage three is not useddirectly for tagging since it will contain zero-probabilities for test-corpus words that were unseenduring training.
Instead, we use it to provide aViterbi labeling of the raw corpus, following the?auto-supervision?
step of Garrette and Baldridge(2012).
This material is then concatenated with thetoken-supervised corpus (when available), and usedto train a Maximum Entropy Markov Model tag-ger.11 The MEMM exploits subword features and10An added benefit of this strategy is that the EM algorithmwith the expanded dictionary runs much more quickly thanwithout it since it does not have to consider every possible tagfor unknown words, averaging 20x faster on PTB experiments.11We use OpenNLP: opennlp.apache.org.generally produces 1-2% better results than an HMMtrained on the same material.4 Experiments12Experimental results are shown in Table 2.
Each ex-periment starts with an initial data set provided byannotator A or B.
Experiment (1) simply uses EMwith the initial small tag dictionary to learn a tag-ger from the raw corpus.
(2) uses LP to infer an ex-panded tag dictionary and tag distributions over rawcorpus tokens, but then takes the highest-weightedtag from each token for use as noisily-labeled train-ing data to initialize EM.
(3) performs greedy model-minimization on the LP output to derive that noisily-labeled corpus.
Finally, (4) is the same as (3), butadditionally uses external dictionary nodes in the LPgraph.
In the case of token-supervision, we also in-clude (0), in which we simply used the tagged sen-tences as supervised data for an HMM without EM(followed by MEMM training).The results show that performance improves withour LP and minimization techniques compared tobasic EM-HMM training.
LP gives large across-the-board improvements over EM training with only theoriginal tag dictionary (compare columns 1 & 2).Weighted model minimization further improves re-sults for type-supervision settings, but not for tokensupervision (compare 2 & 3).Using an external dictionary in the LP graph haslittle effect for KIN, probably due to the availabledictionary?s very small size.
However, MLG withits larger dictionary obtains an improvement in bothscenarios.
Results on ENG are mixed; this may bebecause the PTB tagset has 45 tags (far more thanthe dictionary) so the external dictionary nodes inthe LP graph may consequently serve to collapse dis-tinctions (e.g.
singular and plural) in the larger set.Our results show differences between token- andtype-supervised annotations.
Tag dictionary expan-sion is helpful no matter what the annotations looklike: in both cases, the initial dictionary is toosmall for effective EM learning, so expansion is nec-essary.
However, model minimization only ben-efits the type-supervised scenarios, leaving token-supervised performance unchanged.
This suggests12Our code is available at github.com/dhgarrette/low-resource-pos-tagging-2013143Human Annotations 0.
No EM 1.
EM only 2.
With LP 3.
LP+min 4.
LP(ed)+minInitial data T K U T K U T K U T K U T K UKIN tokens A 72 90 58 55 82 32 71 86 58 71 86 58 71 86 58KIN types A 63 77 32 78 83 69 79 83 70 79 83 70MLG tokens B 74 89 49 68 87 39 74 89 49 74 89 49 76 90 53MLG types B 71 87 46 72 81 57 74 86 56 76 86 60ENG tokens A 63 83 38 62 83 37 72 85 55 72 85 55 72 85 56ENG types A 66 76 37 75 81 56 76 83 56 74 81 55ENG tokens B 70 87 44 70 87 43 78 90 60 78 90 60 78 89 61ENG types B 69 83 38 75 82 61 78 85 61 78 86 61Table 2: Experimental results.
Three languages are shown: Kinyarwanda (KIN), Malagasy (MLG), andEnglish (ENG).
The letters A and B refer to the annotator.
LP(ed) refers to label propagation including nodesfrom an external dictionary.
Each result given as percentages for Total (T), Known (K), and Unknown (U).that minimization is working as intended: it inducesfrequency information when none is provided.
Withtoken-supervision, the annotator provides some in-formation about which tag transitions are best andwhich emissions are most likely.
This is miss-ing with type-supervision, so model minimization isneeded to bootstrap word/tag frequency guesses.This leads to perhaps our most interesting result:in a time-critical annotation scenario, it seems betterto collect a simple tag dictionary than tagged sen-tences.
While the tagged sentences certainly containuseful information regarding tag frequencies, ourtechniques can learn this missing information auto-matically.
Thus, having wider coverage of word typeinformation, and having that information be focusedon the most frequent words, is more important.
Thiscan be seen as a validation of the last two decadesof work on (simulated) type-supervision learning forPOS-tagging?with the caveat that the additional ef-fort we do is needed to realize the benefit.Our experiments also allow us to compare how thedata from different annotators affects the quality oftaggers learned.
Looking at the direct comparisonon English data, annotator B was able to tag moresentences than A, but A produced more tag dictio-nary entries in the type-supervision scenario.
How-ever, it appears, based on the EM-only training, thatthe annotations provided by B were of higher qualityand produced more accurate taggers in both scenar-ios.
Regardless, our full training procedure is ableto substantially improve results in all scenarios.Table 3 gives the recall and precision of the tagTag Dictionary Source R P(1) human-annotated TD 18.42 29.33(2) LP output 35.55 2.62(3) model min output 30.49 4.63Table 3: Recall (R) and precision (P) for tag dictio-naries versus the test data in a ?MLG types B?
run.dictionaries for MLG for settings 1, 2 and 3.
The ini-tial, human-provided tag dictionary unsurprisinglyhas the highest precision and lowest recall.
LP ex-pands that data to greatly improve recall with a largedrop in precision.
Minimization culls many entriesand improves precision with a small relative loss inrecall.
Of course, this is only a rough indicator ofthe quality of the tag dictionaries since the word/tagpairs of the test set only partially overlap with theraw training data and annotations.Because gold-standard annotations are availablefor the English sentences, we also ran oracle ex-periments using labels from the PTB corpus (es-sentially, the kind of data used in previous work).We selected the same amount of labeled tokens orword/tag pairs as were obtained by the annotators.We found similar patterns of improved performanceby using LP expansion and model minimization,and all accuracies are improved compared to theirhuman-annotator equivalents (about 2-6%).
Overallaccuracy for both type and token supervision comesto 78-80%.144#Errors 11k 6k 5k 4k 3kGold TO NNP NN JJ NNPModel IN NN JJ NN JJTable 4: Top errors from an ?ENG types B?
run.Error Analysis One potential source of errorscomes directly from the annotators themselves.Though our approach is designed to be robust to an-notation errors, it cannot correct all mistakes.
Forexample, for the ?ENG types B?
experiment, the an-notator listed IN (preposition) as the only tag forword type ?to?.
However, the test set only ever as-signs tag TO for this type.
This single error accountsfor a 2.3% loss in overall tagging accuracy (Table 4).In many situations, however, we are able to auto-matically remove improbable tag dictionary entries,as shown in Table 5.
Consider the word type ?for?.The annotator has listed RP (particle) as a potentialtag, but only five out of 4k tokens have this tag.
WithRP included, EM becomes confused and labels a ma-jority of the tokens as RP when nearly all should belabeled IN.
We are able to eliminate RP as a possi-bility, giving excellent overall accuracy for the type.Likewise for the comma type, the annotator has in-correctly given ?:?
as a valid tag, and LP, whichuses the tag dictionary, pushes this label to many to-kens with high confidence.
However, minimizationis able to correct the problem.Finally, the word type ?opposition?
provides anexample of the expected behavior for unknownwords.
The type is not in the tag dictionary, soEM assumes all tags are valid and uses many labels.LP expands the starting dictionary to cover the type,limiting it to only two tags.
Minimization then de-termines that NN is the best tag for each token.5 Related workGoldberg et al(2008) trained a tagger for Hebrewusing a manually-created lexicon which was not de-rived from an annotated corpus.
However, their lexi-con was constructed by trained lexicographers over along period of time and achieves very high coverageof the language with very good quality.
In contrast,our annotated data was created by untrained linguis-tics students working alone for just two hours.Cucerzan and Yarowsky (2002) learn a POS-for *IN *RP JJ NN CD(1) EM 1,221 2764 9 5(2) LP 4,003(3) min 4,004 1gold 3,999 5, (comma) *, *: JJS PTD VBP(1) EM 24,708 4 3 3(2) LP 15,505 9226 1(3) min 24,730gold 24,732opposition NN JJ DT NNS VBP(1) EM 24 4 1 4 4(2) LP 41 4(3) min 45gold 45Table 5: Tag assignments in different scenarios.
Astar indicates an entry in the human-provided TD.tagger from existing linguistic resources, namely adictionary and a reference grammar, but these re-sources are not available, much less digitized, formost under-studied languages.Subramanya et al(2010) apply LP to the prob-lem of tagging for domain adaptation.
They con-struct an LP graph that connects tokens in low- andhigh-resource domains, and propagate labels fromhigh to low.
This approach addresses the prob-lem of learning appropriate tags for unknown wordswithin a language, but it requires that the languagehave at least one high-resource domain as a sourceof high quality information.
For low-resource lan-guages that have no significant annotated resourcesavailable in any domain, this technique cannot beapplied.Das and Petrov (2011) and Ta?ckstro?m et al(2013) learn taggers for languages in which thereare no POS-annotated resources, but for which par-allel texts are available between that language and ahigh-resource language.
They project tag informa-tion from the high-resource language to the lower-resource language via alignments in the parallel text.However, large parallel corpora are not available formost low-resource languages.
These are also ex-pensive resources to create and would take consid-erably more effort to produce than the monolingualresources that our annotators were able to generate145in a two-hour timeframe.
Of course, if they are avail-able, such parallel text links could be incorporatedinto our approach.Furthermore, their approaches require the use ofa universal tag set shared between both languages.As such, their approach is only able to induce POStags for the low-resource language if the same tagset is used to tag the high-resource language.
Ourapproach does not rely on any such universal tagset; we learn whichever tags the human annotatorchooses to use when they provide their annotations.In fact, in our experiments we learn much more de-tailed tag sets than the fairly coarse universal tag setused by Das and Petrov (2011) or Ta?ckstro?m et al(2013): we learn a tagger for the full Penn Treebanktag set of 45 tags versus the 12 tags in the universalset.Ding (2011) constructed an LP graph for learningPOS tags on Chinese text by propagating labels froman initial tag dictionary to a larger set of data.
ThisLP graph contained Wiktionary word/POS relation-ships as features as well as Chinese-English wordalignment information and used it to directly esti-mate emission probabilities to initialize an EM train-ing of an HMM.Li et al(2012) train an HMM using EM and aninitial tag dictionary derived from Wiktionary.
LikeDas and Petrov (2011), they use a universal POS tagset, so Wiktionary can be directly applied as a wide-coverage tag dictionary in their case.
Additionally,they evaluate their approach on languages for whichWiktionary has high coverage?which would cer-tainly not get far with Kinyarwanda (9 entries).
Ourapproach does not rely on a high-coverage tag dic-tionary nor is it restricted to use with a small tag set.6 Conclusions and future workWith just two hours of annotation, we obtain 71-78%accuracy for POS-tagging across three languages us-ing both type and token supervision.
Without tagdictionary expansion and model minimization, per-formance is much worse, from 63-74%.
We dramat-ically improve performance on unknown words: therange of 37-58% improves to 53-70%.We also have a provisional answer to whether an-notation should be on types or tokens: use type-supervision if you also expand and minimize.
Thesemethods can identify missing word/tag entries andestimate frequency information, and they produce asgood or better results compared to starting with to-ken supervision.
The case of Kinyarwanda was mostdramatic: 71% accuracy for token-supervision com-pared to 79% for type-supervision.
Studies usingmore annotators and across more languages wouldbe necessary to make any stronger claim about therelative efficacy of the two strategies.AcknowledgementsWe thank Kyle Jerro, Vijay John, Katrin Erk,Yoav Goldberg, Ray Mooney, Slav Petrov, OscarTa?ckstro?m, and the reviewers for their assistanceand feedback.
This work was supported by the U.S.Department of Defense through the U.S. Army Re-search Office (grant number W911NF-10-1-0533)and via a National Defense Science and Engineer-ing Graduate Fellowship for the first author.
Experi-ments were run on the UTCS Mastodon Cluster, pro-vided by NSF grant EIA-0303609.ReferencesMichele Banko and Robert C. Moore.
2004.
Part-of-speech tagging in context.
In Proceedings of COLING,Geneva, Switzerland.Stanley F. Chen and Joshua Goodman.
1996.
An empir-ical study of smoothing techniques for language mod-eling.
In Proceedings of ACL, Santa Cruz, California,USA.Silviu Cucerzan and David Yarowsky.
2002.
Boot-strapping a multilingual part-of-speech tagger in oneperson-day.
In Proceedings of CoNLL, Taipei, Taiwan.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proceedings of ACL-HLT, Portland, Oregon,USA.Weiwei Ding.
2011.
Weakly supervised part-of-speechtagging for Chinese using label propagation.
Master?sthesis, University of Texas at Austin.Dan Garrette and Jason Baldridge.
2012.
Type-supervised hidden Markov models for part-of-speechtagging with incomplete tag dictionaries.
In Proceed-ings of EMNLP, Jeju, Korea.Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008.EM can find pretty good HMM POS-taggers (whengiven a good start).
In Proceedings ACL.Kazi Saidul Hasan and Vincent Ng.
2009.
Weakly super-vised part-of-speech tagging for morphologically-rich,146resource-scarce languages.
In Proceedings of EACL,Athens, Greece.Julian Kupiec.
1992.
Robust part-of-speech tagging us-ing a hidden Markov model.
Computer Speech & Lan-guage, 6(3).Shen Li, Joa?o Grac?a, and Ben Taskar.
2012.
Wiki-lysupervised part-of-speech tagging.
In Proceedings ofEMNLP, Jeju Island, Korea.Christopher D. Manning.
2011.
Part-of-speech taggingfrom 97% to 100%: Is it time for some linguistics?
InProceedings of CICLing.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2).Bernard Merialdo.
1994.
Tagging English text witha probabilistic model.
Computational Linguistics,20(2).Sujith Ravi and Kevin Knight.
2009.
Minimized modelsfor unsupervised part-of-speech tagging.
In Proceed-ings of ACL-AFNLP.Sujith Ravi, Ashish Vaswani, Kevin Knight, and DavidChiang.
2010.
Fast, greedy model minimization forunsupervised tagging.
In Proceedings of COLING.Amarnag Subramanya, Slav Petrov, and FernandoPereira.
2010.
Efficient graph-based semi-supervisedlearning of structured tagging models.
In ProceedingsEMNLP, Cambridge, MA.Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan Mc-Donald, and Joakim Nivre.
2013.
Token and type con-straints for cross-lingual part-of-speech tagging.
InTransactions of the ACL.
Association for Computa-tional Linguistics.Partha Pratim Talukdar and Koby Crammer.
2009.
Newregularized algorithms for transductive learning.
InProceedings of ECML-PKDD, Bled, Slovenia.Kristina Toutanova and Mark Johnson.
2008.
ABayesian LDA-based model for semi-supervised part-of-speech tagging.
In Proceedings of NIPS.147
