Arc-Eager Parsing with the Tree ConstraintJoakim Nivre?Uppsala UniversityDaniel Ferna?ndez-Gonza?lez?
?University of VigoThe arc-eager system for transition-based dependency parsing is widely used in natural languageprocessing despite the fact that it does not guarantee that the output is a well-formed dependencytree.
We propose a simple modification to the original system that enforces the tree constraintwithout requiring any modification to the parser training procedure.
Experiments on multiplelanguages show that the method on average achieves 72% of the error reduction possible andconsistently outperforms the standard heuristic in current use.1.
IntroductionOne of the most widely used transition systems for dependency parsing is the arc-eager system first described in Nivre (2003), which has been used as the backbonefor greedy deterministic dependency parsers (Nivre, Hall, and Nilsson 2004; Goldbergand Nivre 2012), beam search parsers with structured prediction (Zhang and Clark2008; Zhang and Nivre 2011), neural network parsers with latent variables (Titov andHenderson 2007), and delexicalized transfer parsers (McDonald, Petrov, and Hall 2011).However, in contrast to most similar transition systems, the arc-eager system doesnot guarantee that the output is a well-formed dependency tree, which sometimesleads to fragmented parses and lower parsing accuracy.
Although various heuristicshave been proposed to deal with this problem, there has so far been no clean the-oretical solution that also gives good parsing accuracy.
In this article, we present amodified version of the original arc-eager system, which is provably correct for theclass of projective dependency trees, which maintains the linear time complexity ofgreedy (or beam search) parsers, and which does not require any modifications to theparser training procedure.
Experimental evaluation on the CoNLL-X data sets showthat the new system consistently outperforms the standard heuristic in current use,on average achieving 72% of the error reduction possible (compared with 41% for theold heuristic).?
Uppsala University, Department of Linguistics and Philology, Box 635, SE-75126, Uppsala, Sweden.E-mail: joakim.nivre@lingfil.uu.se.??
Universidad de Vigo, Departamento de Informa?tica, Campus As Lagoas, 32004, Ourense, Spain.E-mail: danifg@uvigo.es.Submission received: 25 June 2013; accepted for publication: 4 November 2013.doi:10.1162/COLI a 00185?
2014 Association for Computational LinguisticsComputational Linguistics Volume 40, Number 22.
The ProblemThe dependency parsing problem is usually defined as the task of mapping a sen-tence x = w1, .
.
.
,wn to a dependency tree T, which is a directed tree with one nodefor each input token wi, plus optionally an artificial root node corresponding to adummy word w0, and with arcs representing dependency relations, optionally labeledwith dependency types (Ku?bler, McDonald, and Nivre 2009).
In this article, we willfurthermore restrict our attention to dependency trees that are projective, meaning thatevery subtree has a contiguous yield.
Figure 1 shows a labeled projective dependencytree.Transition-based dependency parsing views parsing as heuristic search through anon-deterministic transition system for deriving dependency trees, guided by a statisti-cal model for scoring transitions from one configuration to the next.
Figure 2 shows thearc-eager transition system for dependency parsing (Nivre 2003, 2008).
A parser con-figuration consists of a stack ?, a buffer ?, and a set of arcs A.
The initial configurationfor parsing a sentence x = w1, .
.
.
,wn has an empty stack, a buffer containing the wordsw1, .
.
.
,wn, and an empty arc set.
A terminal configuration is any configuration withan empty buffer.
Whatever arcs have then been accumulated in the arc set A definesthe output dependency tree.
There are four possible transitions from a configurationHe1 ?SBJwrote2 her3 ?IOBJa4 ?DETletter5 ?DOBJ.6? PFigure 1Projective labeled dependency tree for an English sentence.Initial: ([ ], [w1, .
.
.
,wn], { })Terminal: (?, [ ],A)Shift: (?,wi|?,A) ?
(?|wi,?,A)Reduce: (?|wi,?,A) ?
(?,?,A) HEAD(wi)Right-Arc: (?|wi,wj|?,A) ?
(?|wi|wj,?,A ?
{wi ?
wj})Left-Arc: (?|wi,wj|?,A) ?
(?,wj|?,A ?
{wi ?
wj}) ?HEAD(wi)Figure 2Arc-eager transition system for dependency parsing.
We use | as list constructor, meaningthat ?|wi is a stack with top wi and remainder ?
and wj|?
is a buffer with head wj and tail ?.The condition HEAD(wi) is true in a configuration (?,?,A) if A contains an arc wk ?
wi(for some k).260Nivre and Ferna?ndez-Gonza?lez Arc-Eager Parsing with the Tree Constraintwhere top is the word on top of the stack (if any) and next is the first word of thebuffer:11.
Shift moves next to the stack.2.
Reduce pops the stack; allowed only if top has a head.3.
Right-Arc adds a dependency arc from top to next and moves next to thestack.4.
Left-Arc adds a dependency arc from next to top and pops the stack;allowed only if top has no head.The arc-eager system defines an incremental left-to-right parsing order, where leftdependents are added bottom?up and right dependents top?down, which is advanta-geous for postponing certain attachment decisions.
However, a fundamental problemwith this system is that it does not guarantee that the output parse is a projectivedependency tree, only a projective dependency forest, that is, a sequence of adjacent,non-overlapping projective trees (Nivre 2008).
This is different from the closely relatedarc-standard system (Nivre 2004), which constructs all dependencies bottom?up andcan easily be constrained to only output trees.
The failure to implement the tree con-straint may lead to fragmented parses and lower parsing accuracy, especially withrespect to the global structure of the sentence.
Moreover, even if the loss in accuracyis not substantial, this may be problematic when using the parser in applications wheredownstream components may not function correctly if the parser output is not a well-formed tree.The standard solution to this problem in practical implementations, such as Malt-Parser (Nivre, Hall, and Nilsson 2006), is to use an artificial root node and to attachall remaining words on the stack to the root node at the end of parsing.
This fixes theformal problem, but normally does not improve accuracy because it is usually unlikelythat more than one word should attach to the artificial root node.
Thus, in the erroranalysis presented by McDonald and Nivre (2007), MaltParser tends to have very lowprecision on attachments to the root node.
Other heuristic solutions have been tried,usually by post-processing the nodes remaining on the stack in some way, but thesetechniques often require modifications to the training procedure and/or undermine thelinear time complexity of the parsing system.
In any case, a clean theoretical solution tothis problem has so far been lacking.3.
The SolutionWe propose a modified version of the arc-eager system, which guarantees that the arcset A in a terminal configuration forms a projective dependency tree.
The new system,shown in Figure 3, differs in four ways from the old system:1.
Configurations are extended with a boolean variable e, keeping track ofwhether we have seen the end of the input, that is, whether we havepassed through a configuration with an empty buffer.1 For simplicity, we only consider unlabeled parsing here.
In labeled parsing, which is used in allexperiments, Right-Arc and Left-Arc also have to select a label for the new arc.261Computational Linguistics Volume 40, Number 2Initial: ([ ], [w1, .
.
.
,wn], { }, false)Terminal: ([wi], [ ],A, true)Shift: (?,wi|?,A, false) ?
(?|wi,?,A, [[?
= [ ]]])Unshift: (?|wi, [ ],A, true) ?
(?, [wi],A, true) ?HEAD(wi)Reduce: (?|wi,?,A, e) ?
(?,?,A, e) HEAD(wi)Right-Arc: (?|wi,wj|?,A, e) ?
(?|wi|wj,?,A ?
{wi ?
wj}, [[e ?
?
= [ ]]])Left-Arc: (?|wi,wj|?,A, e) ?
(?,wj|?,A ?
{wi ?
wj}, e) ?HEAD(wi)Figure 3Arc-eager transition system enforcing the tree constraint.
The expression [[?]]
evaluates to true if?
is true and false otherwise.2.
Terminal configurations have the form ([wi], [ ],A, true), that is, they havean empty buffer, exactly one word on the stack, and e = true.3.
The Shift transition is allowed only if e = false.4.
There is a new transition Unshift, which moves top back to the buffer andwhich is allowed only if top has no head and the buffer is empty.The new system behaves exactly like the old system until we reach a configuration withan empty buffer, after which there are two alternatives.
If the stack contains exactly oneword, we terminate and output a tree, which was true also in the old system.
However,if the stack contains more than one word, we now go on parsing but are forbidden tomake any Shift transitions.
After this point, there are two cases.
If the buffer is empty,wemake a deterministic choice between Reduce and Unshift depending onwhether tophas a head or not.
If the buffer is not empty, we non-deterministically choose betweenRight-Arc and either Left-Arc or Reduce (the latter again depending on whether tophas a head).
Because the new Unshift transition is only used in completely deterministiccases, we can use the same statistical model to score transitions both before and after wehave reached the end of the input, as long as we make sure to block any Shift transitionfavored by the model.We first show that the new system is still guaranteed to terminate and that themaximum number of transitions is O(n), where n is the length of the input sentence,which guarantees linear parsing complexity for greedy (and beam search) parsers withconstant-time model predictions and transitions.
From previous results, we know thatthe system is guaranteed to reach a configuration of the form (?, [ ],A) in 2n?
k transi-tions, where k = |?| (Nivre 2008).2 In any non-terminal configuration arising from thispoint on, we can always perform Reduce or Unshift (in case the buffer is empty) orRight-Arc (otherwise), which means that termination is guaranteed if we can show thatthe number of additional transitions is bounded.2 This holds because we must move n words from the buffer to the stack (in either a Shift or a Right-Arctransition) and pop n?
k words from the stack (in either a Reduce or Left-Arc transition).262Nivre and Ferna?ndez-Gonza?lez Arc-Eager Parsing with the Tree ConstraintNote first that we can perform atmost k?
1 Unshift transitionsmoving aword backto the buffer (because a word can only be moved back to the buffer if it has no head,which can only happen once since Shift is now forbidden).3 Therefore, we can performat most k?
1 Right-Arc transitions, moving a word back to the stack and attachingit to its head.
Finally, we can perform at most k?
1 Reduce and Left-Arc transitions,removing a word from the stack (regardless of whether it has first been moved backto the buffer).
In total, we can thus perform at most 2n?
k+ 3(k?
1) < 4n transitions,which means that the number of transitions is O(n).Having shown that the new system terminates after a linear number of transitions,we now show that it also guarantees that the output is a well-formed dependency tree.In order to reach a terminal configuration, we must pop n?
1 words from the stack,each of which has exactly one incoming arc and is therefore connected to at least oneother node in the graph.
Because the word remaining in the stack has no incoming arcbut must be connected to (at least) the last word that was popped, it follows that theresulting graph is connected with exactly n?
1 arcs, which entails that it is a tree.It is worth noting that, although the new system can always construct a tree overthe unattached words left on the stack in the first configuration of the form (?, [ ],A),it may not be able to construct every possible tree over these nodes.
More precisely,a sequence of words wj, .
.
.
,wk can only attach to a word on the left in the form ofa chain (not as siblings) and can only attach to a word on the right as siblings (notas a chain).
Nevertheless, the new system is both sound and complete for the classof projective dependency trees, because every terminating transition sequence derivesa projective tree (soundness) and every projective tree is derived by some transitionsequence (completeness).
By contrast, the original arc-eager system is complete but notsound for the class of projective trees.4.
ExperimentsIn our empirical evaluation we make use of the open-source system MaltParser (Nivre,Hall, and Nilsson 2006), which is a data-driven parser-generator for transition-baseddependency parsing supporting the use of different transition systems.
Besides theoriginal arc-eager system, which is already implemented in MaltParser, we have addedan implementation of the new modified system.
The training procedure used in Malt-Parser derives an oracle transition sequence for each sentence and gold tree in thetraining corpus and uses every configuration?transition pair in these sequences as atraining instance for a multi-class classifier.
Because the oracle sequences in the arc-eager system always produce a well-formed tree, there will be no training instancescorresponding to the extended transition sequences in the new system (i.e., sequencescontaining one or more non-terminal configurations of the form (?, [ ],A)).
However,because the Unshift transition is only used in completely deterministic cases, where theclassifier is not called upon to rank alternative transitions, we can make use of exactlythe same classifier for both the old and the new system.4We compare the original andmodified arc-eager systems on all 13 data sets from theCoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi 2006),3 The number is k?
1, rather than k, because Unshift requires an empty buffer, which together with onlyone word on the stack would imply a terminal configuration.4 Although this greatly simplifies the integration of the new system into existing parsing frameworks, it isconceivable that accuracy could be improved further through specialized training methods, for example,using a dynamic oracle along the lines of Goldberg and Nivre (2012).
We leave this for future research.263Computational Linguistics Volume 40, Number 2which all assume the existence of a dummy root word prefixed to the sentence.
We tunethe feature representations separately for each language and projectivize the trainingdata for languages with non-projective dependencies but otherwise use default settingsin MaltParser (including the standard heuristic of attaching any unattached tokens tothe artificial root node at the end of parsing for the original system).
Because we wantto perform a detailed error analysis for fragmented parses, we initially avoid using thededicated test set for each language and instead report results on a development setcreated by splitting off 10% of the training data.Table 1 (columns 2?3) shows the unlabeled attachment score (including punctua-tion) achieved with the two systems.
We see that the new system improves over theold one by 0.19 percentage points on average, with individual improvements rangingfrom 0.00 (Japanese) to 0.50 (Slovene).
These differences may seem quantitatively small,but it must be remembered that the unattached tokens left on the stack in fragmentedparses constitute a very small fraction of the total number of tokens on which thesescores are calculated.
In order to get a more fine-grained picture of the behavior of thetwo systems, we therefore zoom in specifically on these tokens in the rest of Table 1.Column 4 shows the number of unattached tokens left on the stack when reachingthe end of the input (excluding the artificial root node).
Column 5 shows for how manyof these tokens the correct head is also on the stack (including the artificial root node).Both statistics are summed over all sentences in the development set.
We see from thesefigures that the amount of fragmentation varies greatly between languages, from onlyfour unattached tokens for Japanese to 230 tokens for Slovene.
These tendencies seemto reflect properties of the data sets, with Japanese having the lowest average sentencelength of all languages and Slovene having a high percentage of non-projective depen-dencies and a very small training set.
They also partly explain why these languagesshow the smallest and largest improvement, respectively, in overall attachment score.Table 1Experimental results for the old and new arc-eager transition systems (development sets).UAS = unlabeled attachment score; Stack-Token = number of unattached tokens left in the stackwhen reaching the end of the input (excluding the artificial root node); Stack-Head = number ofunattached tokens for which the head is also left in the stack (including the artificial root node);Correct = number of tokens left in the stack that are correctly attached in the final parser output;Recall = Correct/Stack-Head (%).UAS Stack Correct RecallLanguage Old New Token Head Old New Old NewArabic 77.38 77.74 38 24 3 22 12.50 91.67Bulgarian 90.32 90.42 20 15 7 13 46.67 86.67Chinese 89.28 89.48 33 31 8 18 25.81 58.06Czech 83.26 83.46 41 36 8 21 22.22 58.33Danish 88.10 88.17 29 23 18 22 78.26 95.65Dutch 86.23 86.49 63 57 13 28 22.80 49.12German 87.44 87.75 66 39 6 27 15.38 69.23Japanese 93.53 93.53 4 4 4 4 100.00 100.00Portuguese 87.68 87.84 44 36 15 26 41.67 72.22Slovene 76.50 77.00 230 173 50 97 28.90 56.07Spanish 81.43 81.59 57 42 13 22 30.95 52.38Swedish 88.18 88.33 43 28 13 24 46.43 85.71Turkish 81.44 81.45 24 16 9 10 56.25 62.50Average 85.44 85.63 53.23 40.31 12.85 25.69 40.60 72.12264Nivre and Ferna?ndez-Gonza?lez Arc-Eager Parsing with the Tree ConstraintTable 2Results on final test sets.
LAS = labeled attachment score.
UAS = unlabeled attachment score.Ara Bul Cze Chi Dan Dut Ger Jap Por Slo Spa Swe Tur AveLAS Old 65.90 87.39 86.11 77.82 84.11 77.28 85.42 90.88 83.52 70.30 79.72 83.19 73.92 80.43New 66.13 87.45 86.27 77.93 84.16 77.37 85.51 90.84 83.76 70.86 79.73 83.19 73.98 80.55UAS Old 76.33 90.51 89.79 83.09 88.74 79.95 87.92 92.44 86.28 77.09 82.47 88.70 81.09 84.95New 76.49 90.58 89.94 83.09 88.82 80.18 88.00 92.40 86.33 77.34 82.49 88.76 81.20 85.05Columns 6 and 7 show, for the old and the new system, howmany of the unattachedtokens on the stack are attached to their correct head in the final parser output, as aresult of heuristic root attachment for the old system and extended transition sequencesfor the new system.
Columns 8 and 9 show the same results expressed in terms of recallor error reduction (dividing column 6/7 by column 5).
These results clearly demonstratethe superiority of the new system over the old system with heuristic root attachment.Whereas the old system correctly attaches 40.60% of the tokens for which a head can befound on the stack, the new system finds correct attachments in 72.12% of the cases.
Forsome languages, the effect is dramatic, with Arabic improving from just above 10% toover 90% and German from about 15% to almost 70%, but all languages clearly benefitfrom the new technique for enforcing the tree constraint.Variation across languages can to a large extent be explained by the proportionof unattached tokens that should be attached to the artificial root node.
Because theold root attachment heuristic attaches all tokens to the root, it will have 100% recallon tokens for which this is the correct attachment and 0% recall on all other tokens.This explains why the old system gets 100% recall on Japanese, where all four tokensleft on the stack should indeed be attached to the root.
It also means that, on average,root attachment is only correct for about 40% of the cases (which is the overall recallachieved by this method).
By contrast, the new system only achieves a recall of 82.81%on root attachments, but this is easily compensated by a recall of 63.50% on non-rootattachments.For completeness, we report also the labeled and unlabeled attachment scores (in-cluding punctuation) on the dedicated test sets from the CoNLL-X shared task, shownin Table 2.
The results are perfectly consistent with those analyzed in depth for thedevelopment sets.
The average improvement is 0.12 for LAS and 0.10 for UAS.
Thelargest improvement is again found for Slovene (0.58 LAS, 0.25 UAS) and the smallestfor Japanese, where there is in fact a marginal drop in accuracy (0.04 LAS/UAS).5 Forall other languages, however, the new system is at least as good as the old systemand in addition guarantees a well-formed output without heuristic post-processing.Moreover, although the overall improvement is small, there is a statistically significantimprovement in either LAS or UAS for all languages except Bulgarian, Czech, Japanese,Spanish, and Swedish, and in both LAS and UAS on average over all languages accord-ing to a randomized permutation test (?
= .05) (Yeh 2000).
Finally, it is worth notingthat there is no significant difference in running time between the old and the newsystem.5 As we saw in the previous analysis, fragmentation happens very rarely for Japanese and all unattachedtokens should normally be attached to the root node, which gives 100% recall for the baseline parser.265Computational Linguistics Volume 40, Number 25.
ConclusionIn conclusion, we have presented a modified version of the arc-eager transition systemfor dependency parsing, which, unlike the old system, guarantees that the output isa well-formed dependency tree.
The system is provably sound and complete for theclass of projective dependency trees, and the number of transitions is still linear inthe length of the sentence, which is important for efficient parsing.
The system can beused without modifying the standard training procedure for greedy transition-basedparsers, because the statistical model used to score transitions is the same as for theold system.
An empirical evaluation on all 13 languages from the CoNLL-X shared taskshows that the new system consistently outperforms the old system with the standardheuristic of attaching all unattached tokens to the artificial root node.
Whereas theold method only recovers about 41% of the attachments that are still feasible, the newsystem achieves an average recall of 72%.
Although this gives only a marginal effect onoverall attachment score (at most 0.5%), being able to guarantee that output parses arealways well formed may be critical for downstream modules that take these as input.Moreover, the proposed method achieves this guarantee as a theoretical property of thetransition system without having to rely on ad hoc post-processing and works equallywell regardless of whether a dummy root word is used or not.AcknowledgmentsThis research has been partially funded bythe Spanish Ministry of Economy andCompetitiveness and FEDER (projectTIN2010-18552-C03-01), Ministry ofEducation (FPU Grant Program), and Xuntade Galicia (projects CN 2012/319 and CN2012/317).ReferencesBuchholz, Sabine and Erwin Marsi.
2006.CoNLL-X shared task on multilingualdependency parsing.
In Proceedings of the10th Conference on Computational NaturalLanguage Learning (CoNLL), pages 149?164,New York, NY.Goldberg, Yoav and Joakim Nivre.
2012.A dynamic oracle for arc-eagerdependency parsing.
In Proceedingsof the 24th International Conference onComputational Linguistics (COLING),pages 959?976, Jeju Island.Ku?bler, Sandra, Ryan McDonald, andJoakim Nivre.
2009.
Dependency Parsing.Morgan and Claypool.McDonald, Ryan and Joakim Nivre.
2007.Characterizing the errors of data-drivendependency parsing models.
In Proceedingsof the 2007 Joint Conference on EmpiricalMethods in Natural Language Processingand Computational Natural LanguageLearning (EMNLP-CoNLL), pages 122?131,Prague.McDonald, Ryan, Slav Petrov, and KeithHall.
2011.
Multi-source transfer ofdelexicalized dependency parsers.
InProceedings of the Conference on EmpiricalMethods in Natural Language Processing(EMNLP), pages 62?72, Edinburgh.Nivre, Joakim.
2003.
An efficient algorithmfor projective dependency parsing.In Proceedings of the 8th InternationalWorkshop on Parsing Technologies (IWPT),pages 149?160, Nancy.Nivre, Joakim.
2004.
Incrementality indeterministic dependency parsing.In Proceedings of the Workshop onIncremental Parsing: Bringing Engineeringand Cognition Together (ACL), pages 50?57,Stroudsburg, PA.Nivre, Joakim.
2008.
Algorithms fordeterministic incremental dependencyparsing.
Computational Linguistics,34:513?553.Nivre, Joakim, Johan Hall, and Jens Nilsson.2004.
Memory-based dependency parsing.In Proceedings of the 8th Conference onComputational Natural Language Learning(CoNLL), pages 49?56, Boston, MA.Nivre, Joakim, Johan Hall, and Jens Nilsson.2006.
Maltparser: A data-drivenparser-generator for dependency parsing.In Proceedings of the 5th InternationalConference on Language Resources andEvaluation (LREC), pages 2,216?2,219,Genoa.Titov, Ivan and James Henderson.
2007.A latent variable model for generative266Nivre and Ferna?ndez-Gonza?lez Arc-Eager Parsing with the Tree Constraintdependency parsing.
In Proceedings of the10th International Conference on ParsingTechnologies (IWPT), pages 144?155,Prague.Yeh, Alexander.
2000.
More accurate testsfor the statistical significance of resultdifferences.
In Proceedings of the 18thInternational Conference on ComputationalLinguistics (COLING), pages 947?953,Saarbru?ken.Zhang, Yue and Stephen Clark.
2008.A tale of two parsers: Investigatingand combining graph-based andtransition-based dependency parsing.In Proceedings of the Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP), pages 562?571,Honolulu, HI.Zhang, Yue and Joakim Nivre.
2011.Transition-based parsing with richnon-local features.
In Proceedings of the49th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 188?193, Portland, OR.267
