On compositional semanticsWIodek ZadroznyIBM ResearchT.
J. Watson Research CenterYorktown Heights, NY 10598WLODZ @ WATSON.1BM.COMAbstract.
We prove a theorem stating that anysemantics can be encoded as a compositionalsemantics, which means that, essentially, thestandard definition of compositionality is for-mally vacuous.
We then show that when one re-quires compositional semantics to be"systematic" (that is the meaning function can-not be arbitrary, but must belong to some class),one can easily distinguish between compositionaland non-composit ional semantics.
We also pre-sent an example of  a simple grammar for whichthere is no "systematic" compositional seman-tics.
This implies that it is possible to distinguish"good" and "bad" grammars oll the basis ofwhether they can have compositional semantics.As a result, we believe that the paper clarifies theconcept of  compositionality and opens a possi-bility of making systematic omparisons of  dif-ferent systems of grammars and NLU programs.l.lntroduction.Compositionality is defined as the property thatthe meaning of  a whole is a function of the me-aning of  its parts (cf.
e.g.
Keenan and Faltz(1985),pp.24-25)?
This definition, although intu-itively clear, does not work formally.
For in-stance, Ilirst (1987) pp.27-43 claims that thesemantics of  Woods (1967) and (Woods, 1969),is not compositional, because "the interpretationof the word depart varies as different preposi-tional phrases are attached to it":AA-57 departs from Boston= > depart(aa-57, boston).AA-57 departs from Boston to Chicago= > connect(aa-57, boston, chicago).AA-37 departs from Boston on Monday= > dday(aa-57, boston, monday).AA-57 departs from Boston at 8:00 a,m.= > equal(dtime(aa-57, boston), 8:00am).AA-57 departs from Boston after 8:00 a.m.= > greater(dtime(aa-57,boston),8:l10am).AA-57 departs from Boston before 8:00 a.m.= > greater(8:0Oam,dtime(aa-57,boston)).Although this semantics does look like non-compositional, it is easy to create a function thatproduces the meanings of  all these sentencesfrom the meanings of  its parts -- we can simplydefine such a function by cases: the meaning ofdeparts~from/ is connect, the meaning ofdeparts/from/on is dday, and so on.
Hirst there-fore changes the definition of  compositionalityto "the meaning of a whole is a Lystematic me-aning of the parts" (op.
cit.
p.27.
; tile emphasisis ours), but without defining the meaning of  theword "systematic.
"in this paper we show that, indeed, tlirst wasright in assuming that the standard efinition ofcompositionality has to be amended.
Namely,we prove a theorem stating that any semanticscan be encoded as a compositional semantics,which means that, essentially, the standard efi-nition of  compositionality is formally vacuous.We then show that when one requires composi-An equivalent definition, e.g.
Partee t al.
(1990), postulates the existence of a homomorphism from syntax to semantics.ACTES DE COLING-92, NANrEs, 23-28 ^Ot~T 1992 2 6 0 PROC.
OF COLING-92, NANTES, AUO.
23-28, 1992tional semantics to be "systematic" (i.e.
the me-aning function must belong to some class), onecan easily distinguish between compositionaland non-compositional semantics.
We also givean example of a simple grammar lor which thereis no "systematic" compositional semantics".This result implies that it is possible to distin-guish "good" and "bad" grammars on the basisof whether they can have a compositional se-mantics with a meaning function belonging to acertain class.
As a result, we believe that thepaper finally clarifies the concept of composi-tionality and opens a possibility of  making sys-tematic comparisons of  different systems ofgrammars and NLU programs.2.Some compositional meaningfunction can always be foundCompositional semantics, or CS, is usually de-fined as a functional dependence of  the mean-ing of  an expression on the meanings of itsparts.
One of  the first natural questionswe might want to ask is whether a set of NLexpressions, i.e.
a language, can have some CS.This question has been answered positively byvan Benthem (1982).
ttowever his result saysnothing about what kinds of things shoukl beassigned e.g.
to nouns, where, obviously, wewould like nouns to be mapped into sets of en-tities, or something like that.
That is, we wantsemantics to encode some basic intuitions, e.g.that nouns denote sets of entities, and verbs de-note relations between entities, and so on.So what about having a compositional semanticsthat agrees with intuitions?
That is, the ques-tions is whether after deciding what sentencesand their parts mean, we can find a function thatwould compose the meaning of a whole fromthe meanings of its parts.The answer to this question is somewhat dis-turbing.
It turns out that whatever we decidethat some language xpressions should mean, itis always possible to produce a ffmction thatwould give CS to it (see below tor a more preciseformulation of this fact).
The upshot is thatcompositionality, as commonly defined, is not astrong constraint on a semantic theory.The intuitions behind this result can be illus-trated quite simply: Consider tile languageof finite strings of digits from 0 to 7.
Let's fix arandom function (i.e.
an intuitively bizarre func-tion) from this language into {0,1).
Let the me-aning function be defined as the value of thestring as the corresponding number in base 8 iftile value of the function is 0, and in base 10,otherwise.
Clearly, the meaning of any stringis a composition of the meanings of digits (noticethat the values of the digits are the same in bothbases).
But, intuitively, this situation is differentfi'om standard cases when we consider only onebase and the meaning of a string is given by asimple lbrmula relizrring only to digits and theirpositions in the string, The theorem we provebelow shows that however complex is the lan-guage, aqd whatever strange meanings we wantto assign to its expressions, we can always do itcompositionally.One of the more bizarre consequences of thisfact is that we do not have to start buildingcompositional semantics fbr NL beginning withassigning meanings to words.
We can do equallywell by assigning meanings to phonems or evenLETTFRS, assuring that, for any sentence, theintuitive meaning we associate with it would bea lhnction of the meaning of the letters fromwhich this sentence is composed.PROVING EXISTENCE OF C( )MPOSI -T IONAL SEMANTICSLet S be any collection of  expressions (intu-itively, sentences and their parts).
Let M be a sets.t.
for any seS ,  there is m=m(s)  which is amember of M s.t.
n, is the meaning of s. Wewant to show that there is a cmnpositional se-mantics for S which agrees with the function as-sociating m with re(s) , which will be denoted byre(x).Since elements of M can be of  any type, we donot automatically have (for all elements of S)ACqES DE COL1NG-92, NANTES, 23-28 AO~r 1992 2 6 1 Pgoc.
OF COLING-92.
NANrES.
AUG. 23-28, 1992m(s.t) = m(s)#m(t) (where # is some operationon the meanings).
To get this kind of homo-morphism we have to perform a type raisingoperation that would map elements of  S intofunctions and then the functions into the re-quired meanings.
We begin by trivially extendingthe language S by adding to it an ~end of ex-pression H character $, which may appear onlyas the last element of any expression.
The pur-pose of  it is to encode the function re(x) in thefollowing way: The meaning function tz thatprovides compositional semantics for S maps itinto a set of  functions in such a way thatl~(s.t) = t~(s)(#(t)).
We want that the originalsemantics be easily decoded from p(s), andtherefore we require that, for all .
s,I~(s.$) = re(s).
Note that such a type raising op-eration is quite common both in mathematics(e.g.
1 being a function equal to 1 for all values)and in mathematical linguistics.
Secondly, weassume here that there is only one way of com-posing elements of  S -- by concatenation 2 butall our arguments work for languages with manyoperators as well.THEOREM.
Under the above assumptions.There is a function ~t s.t, for all s,#(s.t) = #(s)(tt(t)) , and l~(s.$) = re(s).Proof.
See Section 5.1.3.What do we really want fromcompositional semantics?In view of the above theorem, any semantics iequivalent o a compositional semantics, andhence it would be meaningless to keep the defi-nition of  eompositionality as the existence of  ahomomorphism from syntax to semantics with-out imposing some conditions on this homo-morphism.
Notice that requiring thecomputability of the meaning function won't do.
sPropos i t ion .
I f  the original function re(x) iscomputable, so is the meaning function/~(x).Proof.
See the proof of  the solution lemma inAczel (1987).3.1 What  do we really want?We have some intuitions and a bunch of  exam-ples associated with the concept of  composi-tionality; e.g.
for NP -> Adj N , we can mapnouns and adjectives into sets and the concat-enation into set intersection, and get an intu-itively correct semantics for expressions like"grey carpet", "blue dog", etc.There seem to be two issues here: (1) Such pro-cedures work for limited domains like "everydaysolids ~ and colors; (2) The function that com-poses the meanings hould be "easily" definable,e.g.
in terms of boolean operations on sets.
Thiscan be made precise for instance along the linesof  Manaster-Ramer and Zadrozny (1990), wherewe argue that one can compare expressivepower of  various grammatical formalisms interms of relations that they allow us to define;the same approach can be applied to semantics,as we show it below.3.1 ,4  s imple grammar  without a system-atic semanticsIf meanings have to be expressed using certainnatural, but restricted, set of  operations, it mayturn out that even simple grammars do not havea compositional semantics.Consider two grammars of  numerals in base 10:Grammar  ND?
N <- -ND?
N<- -D?
D <-0111213141516171819\ [And the second grammar2 We do not assume that concatenation is associative, that is (a.
(b.c)) = ((a.h).c).
Intuitively, this means that we assign se-mantics to parse trees, not to strings of words.
But the method of proof can be modified to handle the case when concat-enation is associative.Also, note that in mathematics (where semantics obviously is compositional) we can talk about noncomputable functions,and it is usually clear what we postulate about hem.AL-TRS DE COLING-92, NANTES.
23-28 AOUT 1992 2 6 2 PRoc.
OF COLING-92, NANTES, AUG. 23-28, 1992Grammar DN?
N <- -DN?
N<- -D* D <- -01112131415161718191For the grammar ND, the meaning of any nu-meral can be expressed in the model(Nat, +,  x ,  10) as#(N D) = 10 x #(N) + u(D)that is a polynomial in two variables with coef-ficients in natural numbers.For the grammar DN, we can prove that nosuch a polynomial exists, that isTheorem.
There is no polynomial p in the vari-ables #(D), ~(N) such that#(D iV) = p(p.(D), #(N))and such that the value of #(D N) is the numberexpressed by the string D N in base I0.Proof.
See Section 5.2.But notice that there is a compositional seman-tics for the grammar DN that does not agreewith intuitions: #(D N) = 10 ?
#(N) +/~(D),which corresponds to reading the number back-wards.
And there are many other semantics cor-responding to all possible polynomials in #(D)and/I(N).Also observe that (a) if we specify enough valuesof the meaning function we can exclude anyparticular polynomial; (b) if we do not restrictthe degree of the polynomial, we can write onethat would give any values we want on a finitenumber of words in the grammar.The moral is that not only it is natural to restrictmeaning functions to, say, polynomials, but tofurther restrict hem, e.g.
to polynomials of de-gree 1.
Then by specifying only three values ofthe meaning function we can (a) have a uniquecompositional semantics lbr the first granmaar;(b) show that there is no compositional seman-tics tbr tile second grammar (directly from theproof of the above theorem).4.
Conclusions4.1.
Relevance for theories of grammar4.
I.
1.
On reduction of syntax to lexieal meaningsT.
Wasow on pp.204-205 of Sells (1985) writes:It is interesting that contemporary syn-tactic theories eem to be converging onthe idea that sentence structure is gen-erally predictable from word meanings\[...\].
\[...\] The surprising thing (to lin-guist) has been how little needs to bestipulated beyond lexical meaning.
\[_.\]The reader should notice that the meaning func-tion m in our main theorem is arbitrary.
In par-ticular we can take re(s) to be the preferredsyntactic analysis of the string s. The theoremthen confirms the above observation: indeed,the syntax can be reduced to lexical meanings.At the same time, it both trivializes it and callsout for a deeper explanation.
It trivializes thereduction to lexical meanings, since it also saysthat with no restriction on the types of meaningspermitted, syntax can be reduced to the meaningof phonems or letters.
The benefits of the re-duction to lexical meanings would have to beexplained, especially if such meanings refer toabstract properties such as binding features,BAR, or different kinds of subcategorization.It is the view of this author (cf.
Zadrozny andManaster-Ramer (1997)) and, implicitly, of  Fill-more et al (1988), that such a reductionist ap-proach is inappropriate.
But we have no roomto elaborate it here.4.1.2.
On good and bad grammarsBy introducing restrictions on semantic func-tions, i.e.
demanding the systematicity of se-mantics, we can for the first time formalize theintuitions that linguists have had for a long timeabout "good" and "bad" grammars (cf.
Manast-er-Ramcr and Zadrozny (1992)).
This allows usACrF~ DE COLING-92.
NANTES, 23-28 AOI3T 1992 2 63 PROC.
of COLING-92.
NANTES, AUO.
23-28, 1992to begin dealing in a rigorous way with theproblem (posed by Marsh and Partee) of con-straining the power of the semantic as well as thesyntactic omponents of a grammar.We can show for instance (ibid.)
that some re-strictions have the effect of making it in principleimpossible to assign correct meanings to arbi-trary sets of matched singulars and plurals if theunderlying rammar does not have a unitary ruleof reduplication.
Thus, grammars such aswrapping grammars (Bach (1979)), TAGs (e.g.,Joshi (1987)), head grammars, LFG (e.g., Sells(1985)) and queue grammars (Manaster-Ramer(1986)), all of  which can generate such a lan-guage, all fail to have systematic semantics forit.
On the other hand, we can exhibit othergrammars (including old-fashioned trans-formational grammars) which do have systcm-atic semantics for such a language.4.2.
What kind of semantics for NL?In view of the above results we should perhapsdiscuss some of the options we have in semanticsof NL, especially in context of NLU by com-puters.
To focus our attention, let's consider theoptions we have to deal with the semantics ofdepart as described in Section 1.?
Do nothing.
That is, to assume that the se-mantics is given by sets of procedures asso-ciated with particular patterns; e.g.
"Xdeparts from Y" gets translated into"depart(X,Y)".?
Give it semantics a la Montague, for in-stance, along the lines of Dowry (1979) (seeesp.
Chapter 7).
Such a semantics is quitecomplicated, not very readable, and it is notclear what would be accomplished by doingthis.
However note that this doesn't meanthat it would not be computational -- ttobbsand Rosenschein (1977) show how to trans-late Montagovian semantics into Lisp func-tions.Restrict he meaning of compositionality re-quiring for example that the meaning of averb is a relation with the number of argu-ments equal to the number of arguments ofthe verb.
If the PP following the verb istreated as one argument, there is no com-positional semantics that would agree withthe intended meanings of the example sen-tences.
This would formally justify tile argu-ments of t lirst.?
Recognize that depending on the PPs themeaning of "X departs PP" varies, and de-scribe this dependence via a set of meaningpostulates (Bernth and Lappin (1991) showhow to do it in a computational context).
Insuch a case the semantics is not given di-rectly as a homomorphism from syntax intosome algebra of  meanings, but indirectly, byrestricting the class of such algebras by themeaning postulates.
* Admit that the separation of syntax and se-mantics does not work in practice, and workwith representations in which form and me-aning are not separated, that is, there cannotbe a separate syntax, except in restricteddomains or for small fragments of language.This view of language has been advocatedby Fillmore et al (1988), and shown in Za-drozny and Manaster-Ramer (1997), Za-drozny and Manaster-Ramer (1997) to becomputationally feasible.?
t iope that the meaning will emerge fromnon-symbolic representations, as advocatedby the "connectionists.
"5.The proofs5.1.
The Existence of compositional me-aning functionsLet S be any collection of expressions (intu-itively, sentences and their parts).
Let M be aset s.t.
for any swhich is a member of S, thereism=m(s) which i sa  member of Ms .
t .m isthe meaning of s. We want to show that thereis a compositional semantics for S which agreeswith the function associating m with m(s) ,which will be denoted by rn(x).
To get the ho-Afzr~:s DE COLING-92, NANTES.
23-28 AO13T 1992 2 6 4 PROC.
OI~ COLING-92.
NANTES, AUO.
23-28, 1992momorphism from syntax to semantics we haveto perfbrm a type raising operation that Wouldmap elements of  S into fhnctions and then thefunctions into the required meanings.As we have described it in Section 2, we extendS by adding to it the "end of expression" char-acter $, which may appear only as the last ele-ment of any expression.
Under theseassumptions we prove:THEOREM.
There is a function ~t s.t, for all s,#(s.t) = #(s)(tt(t)) , and~(s.$) : m(s).Proof.
Let /(0),1(1) .... , t(a) enumerate S. Wecan create a big table specifying meaning valuesfor aU strings and their combinations.
Then theconditions above can be written as a set ofequations hown in the figure belowt,(t(0)) ~ { < $, m(t(0)) > ,  < it(t(0)),/t(t(0).t(0)) > .
.
.
.
.
< #(t(u)), it(t(O).t(a)) > .... }#(t(I)) ~ { < $, m(t(1)) > ,  < ~t(t(0)),/~(t(1).t(0)) > .
.
.
.
.
</~(t(~)), tt(t(I).t(a)) > .... }tt(t(a)) = { < $, m(t(a)) >,  < #(t(0)), ~(/(a).t(0)) > .
.
.
.
.
< #(t(a)), #(t(~).t(a)) > .... }Continuing the proof: By the solution lemma(Aczel (1987) and Barwise and Etchemendy(1987)) this set of equations has a solution(unique), which is a function.To finish the proof we have to make sure thatthe equation #($)= $ holds.
Formally, this re-quires adding the pair < $, $ > into the graph oftt that was obtained from the solution letmna.\[\]We have directly specified the function as a setof pairs with appropriate values.
Note that thatthere is place for recursion in syntactic atego-ries.
Also, if a certain string dues not belong tothe language, we assume that the correspondingvalue in this table is undefined; thus # is not ne-cessarily defined for all possible concatenationsof strings of S.Note: The above theorem has been proved in settheory with the anti-loundation axiom, ZFA.This set theory is equiconsistent with the stand-ard system of ZFC, thus the theorem does notassume anything more than what is needed for"standard mathematical practice".
Furthermore,ZFA is better suited as foundations for seman-tics of NL than ZFC (Barwise and Etchemendy(1987)).5.2.
A grammar  w i thout  compos i t iona lsemant icsVor the grammar DN, we can prove that nosuch a polynomial exists, that is1qaeorem.
There is no polynomial p in the vari-ables #(D), #(N) such that#( D IV) = p(la( D ), I~( N) )and such that the value of#(D P0 is the numberexpressed by the string D N in base 10.Proof.
We are looking for~(~ ~0 = p(u(~,  ~(O))= #(D) x (1() tength(~) + p(N)where the function p must be a polynmnial inthese two variables.
But such a polynomial doesnot exist, since it would have to be equal to#(N) for p(N) in tile interval 0..9, and to/~(D) ?
10+/~(N) for /~(N) in 10..99, and to#(D) ?
100 + ~t(N) for ~t(N) in 100..999, and soon.
And if the degree of this polynomial was lessthan l~ ,  lbr some n, it would have to be equalidentically to /~(D) ?
10" +/~(N) , since it wouldshare with it all the values in l@..10 ~ - 1, andtherefore could not give correct values on theother intervals.AcrEs DE COLING-92, NANTES, 23-28 AOl\]'r 1992 2 6 5 PROC.
OF COL1NG-92, NANTES, Au(L 23-28, 1992Acknowledgements.
Alexis Manaster Ramer broughtto my attention the need to clarify the meaning ofcompositionality, and commented on various aspectsof this work.
I also benefited greatly from a discussionwith John Nerborme and exchanges of e-mail withFeruando Pereira and Nelson Correa.
(Needless tosay, all the remaining faults of the paper are mine).Parts of this paper were written at the University ofKaiserslautern; I'd like to thank Alexander yon Hum-boldt Stiftung for supporting my visit there.6.
ReferencesP.
Aczel (1987).
Lectures on Non-Well-Founded Sets.Stanford, CA: CSLI Lecture Notes.E.
Bach (1979).
Control in Montague Grammar.Linguistic Inquiry, 515-531.J.
Barwise & J. Etchemendy (1987).
The Liar.
NewYork, NY: Oxford University Press.A.
Beruth & S. Lappin (1991).
A meaning postulatebased inference system for natural anguage.RC 16947, Yorktown lteights, NY: IBM T.J.Watson Research Center.D.
R. Dowry (1979).
Word Meaning and MontagueGrammar.
Dordrecht, Holland: D. Reidel.C.
J. Fillmore, P. Kay, & M. C. O'Connor (1988).Regularity and idiomatieity in grammaticalconstructions.
Language, 3, 501-538.G.
tlirst (1987).
Semantic interpretation and the re-solution of ambiguity.
Cambridge, GreatBritain: Cambridge University Press.J.
llobbs & S. Rosenschein (1977).
Making compu-tational sense of Montague's intentional logic.Artificial Intelligence, 3, 287-306.A.
K. Joshi (1987).
An Introduction to Tree Adjoin-ing Grammars: In A. Manaster-Ramer (Ed.
),Mathematics of Language (pp.
87-114).Amsterdam/Philadelphia: John Benjamins.E.
L. Keenan & L. M. Faltz (1985).
Boolean Seman-tics for Natural Language.
Dordrecht,Holland: D Reidel.A.
Manaster-Ramer (1986).
Copying in Natural Lan-guages, Context-Freeness, and Queue G-tam-mars.
Proc.
ACL'86, 85-89.A.
Manaster-Ramer & W. Zadrozny (1990).Expressive Power of Grammatical Formalisms(Proceedings of Coling-90).
tlelsinki,Finnland: Universitas ltelsingiensis.A.
Manaster-Ramer & W. Zadrozny (1992).
System-atic Semantics.
in preparation.B.
H. Partee, A. t. Meulen, & R. E. Wall (1990).Mathematical Methods in Lingusitics.
Dor-dreeht, The Netherlands: Kluwer.P.
Sells (1985).
Lectures on Contemporary SyntacticTheories.
Stanford, CA: CSLI lecture Notes(3).J.
van Benthem (1982).
The Logic of Semantics: InFred Lanman & Frank Veltman (Eds.
),Varieties of Formal Semantics.
Dordrecht,Holland: Foils.W.
A.
Woods (1967).
Semantics for a question an-swering system.
Harvard University.PhDThesisW.
A.
Woods (1969).
Procedural semantics for aquestion answering machine.
AFIPS confer-ence proceedings, 33, 457-471.W.
Zadrozny & A. Manaster-Ramer (1997a).
TheSignificance of Constructions.
ubmitted.W.
Zadrozny & A. Manaster-Ramer (199~).
Assem-bling Constructions.
ubmitted.ALT~ DE COLING-92, NANT~, 23-28 AOt3"r 1992 2 6 6 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992
