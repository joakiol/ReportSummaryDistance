Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 1?9,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsEmotion Analysis Using Latent Affective Folding and EmbeddingJerome R. BellegardaSpeech & Language TechnologiesApple Inc.Cupertino, California 95014, USAjerome @ apple.comAbstractThough data-driven in nature, emotion analy-sis based on latent semantic analysis still relieson some measure of expert knowledge in or-der to isolate the emotional keywords or key-sets necessary to the construction of affectivecategories.
This makes it vulnerable to anydiscrepancy between the ensuing taxonomy ofaffective states and the underlying domain ofdiscourse.
This paper proposes a more gen-eral strategy which leverages two distincts se-mantic levels, one that encapsulates the foun-dations of the domain considered, and one thatspecifically accounts for the overall affectivefabric of the language.
Exposing the emergentrelationship between these two levels advan-tageously informs the emotion classificationprocess.
Empirical evidence suggests that thisis a promising solution for automatic emotiondetection in text.1 IntroductionThe automatic detection of emotions in text isa necessary pre-processing step in many differ-ent fields touching on affective computing (Picard,1997), such as natural language interfaces (Cosattoet al, 2003), e-learning environments (Ryan et al,2000), educational or entertainment games (Pivecand Kearney, 2007), opinion mining and sentimentanalysis (Pang and Lee, 2008), humor recognition(Mihalcea and Strapparava, 2006), and security in-formatics (Abbasi, 2007).
In the latter case, for ex-ample, it can be used for monitoring levels of hate-ful or violent rhetoric (perhaps in multilingual set-tings).
More generally, emotion detection is of greatinterest in human-computer interaction: if a systemdetermines that a user is upset or annoyed, for in-stance, it could switch to a different mode of inter-action (Liscombe et al, 2005).
And of course, itplays a critical role in the generation of expressivesynthetic speech (Schro?der, 2006).Emphasis has traditionally been placed on the setof six ?universal?
emotions (Ekman, 1993): ANGER,DISGUST, FEAR, JOY, SADNESS, and SURPRISE(Alm et al, 2005; Liu et al, 2003; Subasic and Huet-tner, 2001).
Emotion analysis is typically carried outusing a simplified description of emotional states ina low-dimensional space, which normally comprisesdimensions such as valence (positive/negative eva-lution), activation (stimulation of activity), and/orcontrol (dominant/submissive power) (Mehrabian,1995; Russell, 1980; Strapparava and Mihalcea,2008).
Classification proceeds based on an underly-ing emotional knowledge base, which strives to pro-vide adequate distinctions between different emo-tions.
This affective information can either be builtentirely upon manually selected vocabulary as in(Whissell, 1989), or derived automatically from databased on expert knowledge of the most relevant fea-tures that can be extracted from the input text (Almet al, 2005).
In both cases, the resulting systemtends to rely, for the most part, on a few thousandannotated ?emotional keywords,?
the presence ofwhich triggers the associated emotional label(s).The drawback of such confined lexical affinity isthat the analysis tends to be hampered by the biasinherent in the underlying taxonomy of emotionalstates.
Because this taxonomy only supports simpli-fied relationships between affective words and emo-1tional categories, it often fails to meaningfully gen-eralize beyond the relatively few core terms explic-itly considered in its construction.
This has sparkedinterest in data-driven approaches based on latentsemantic analysis (LSA), a paradigm originally de-veloped for information retrieval (Deerwester et al,1990).
Upon suitable training using a large corpusof texts, LSA allows a similarity score to be com-puted between generic terms and affective categories(Strapparava et al, 2006).
This way, every word canautomatically be assigned some fractional affectiveinfluence.
Still, the affective categories themselvesare usually specified with the help of a reference lex-ical database like WordNet (Fellbaum, 1998).The purpose of this paper is to more broadly lever-age the principle of latent semantics in emotion anal-ysis.
We cast the problem as a general applicationof latent semantic mapping (LSM), an extrapolationof LSA for modeling global relationships implicitin large volumes of data (Bellegarda, 2005; Belle-garda, 2008).
More specifically, we use the LSMframework to describe two distinct semantic levels:one that encapsulates the foundations of the domainconsidered (e.g., broadcast news, email messages,SMS conversations, etc.
), and one that specificallyaccounts for the overall affective fabric of the lan-guage.
Then, we leverage these two descriptionsto appropriately relate domain and affective levels,and thereby inform the emotion classification pro-cess.
This de facto bypasses the need for any explicitexternal knowledge.The paper is organized as follows.
The next sec-tion provides some motivation for, and gives anoverview of, the proposed latent affective frame-work.
In Sections 3 and 4, we describe the two mainalternatives considered, latent folding and latent em-bedding.
In Section 5, we discuss the mechanicsof emotion detection based on such latent affectiveprocessing.
Finally, Section 6 reports the outcomeof experimental evaluations conducted on the ?Af-fective Text?
portion of the SemEval-2007 corpus(Strapparava and Mihalcea, 2007).2 Motivation and OverviewAs alluded to above, lexical affinity alone failsto provide sufficient distinction between differentemotions, in large part because only relatively fewAll SynsetsLSAProcessingHomogeneousRepresentationPseudo?documentSimilarityWordNet SynsetInput TextSpecific WordLargeCorpus EmotionDetectedFigure 1: Typical LSA-Based Emotion Analysis.words have inherently clear, unambiguous emo-tional meaning.
For example, happy and sad encap-sulate JOY and SADNESS, respectively, in all con-ceivable scenarios.
But is thrilling a marker of JOYor SURPRISE?
Does awful capture SADNESS or DIS-GUST?
It largely depends on contextual informa-tion: thrilling as a synonym for uplifting conveysJOY (as in a thrilling speech), while thrilling as asynonym for amazing may well mark SURPRISE (asin a thrilling waterfall ride); similarly, awful as asynonym for grave reflects SADNESS (as in an aw-ful car accident), while awful as a synonym for foulis closer to DISGUST (as in an awful smell).
The vastmajority of words likewise carry multiple potentialemotional connotations, with the degree of affectivepolysemy tightly linked to the granularity selectedfor the underlying taxonomy of emotions.Data-driven approaches based on LSA purportto ?individuate?
such indirect affective words viainference mechanisms automatically derived in anunsupervised way from a large corpus of texts,such as the British National Corpus (Strapparavaet al, 2006).
By looking at document-level co-occurrences, contextual information is exploited toencapsulate semantic information into a relativelylow dimensional vector space.
Suitable affective cat-egories are then constructed in that space by ?foldingin?
either the specific word denoting the emotion, orits associated synset (say, from WordNet), or eventhe entire set of words in all synsets that can be la-belled with that emotion (Strapparava and Mihalcea,2008).
This is typically done by placing the rele-vant word(s) into a ?pseudo-document,?
and map itinto the space as if it were a real one (Deerwester etal., 1990).
Finally, the global emotional affinity of agiven input text is determined by computing similar-ities between all pseudo-documents.
The resultingframework is depicted in Fig.
1.2This solution is attractive, if for no other reasonthan it allows every word to automatically be as-signed some fractional affective influence.
However,it suffers from two limitations which may well provedeleterious in practical situations.
First, the inherentlack of supervision routinely leads to a latent seman-tic space which is not particularly representative ofthe underlying domain of discourse.
And second,the construction of the affective categories still reliesheavily on pre-defined lexical affinity, potentially re-sulting in an unwarranted bias in the taxonomy ofaffective states.The first limitation impinges on the effectivenessof any LSA-based approach, which is known to varysubstantially based on the size and quality of thetraining data (Bellegarda, 2008; Mohler and Mihal-cea, 2009).
In the present case, any discrepancybetween latent semantic space and domain of dis-course may distort the position of certain words inthe space, which could in turn lead to subsequentsub-optimal affective weight assignment.
For in-stance, in the examples above, the word smell is con-siderably more critical to the resolution of awful asa marker of DISGUST than the word car.
But thatfact may never be uncovered if the only pertinentdocuments in the training corpus happen to be aboutexpensive fragrances and automobiles.
Thus, it ishighly desirable to derive the latent semantic spaceusing data representative of the application consid-ered.
This points to a modicum of supervision.The second limitation is tied to the difficulty ofcoming up with an a priori affective description thatwill work universally.
Stipulating the affective cat-egories using only the specific word denoting theemotion is likely to be less robust than using the setof words in all synsets labelled with that emotion.On the other hand, the latter may well expose someinherent ambiguities resulting from affective poly-semy.
This is compounded by the relatively smallnumber of words for which an affective distributionis even available.
For example, the well-known Gen-eral Inquirer content analysis system (Stone, 1997)lists only about 2000 words with positive outlookand 2000 words with negative outlook.
There are ex-actly 1281 words inventoried in the affective exten-sion of WordNet (Strapparava and Mihalcea, 2008),and the affective word list from (Johnson?Laird andOatley, 1989) comprises less than 1000 words.
ThisAffective CorpusProcessingLSMDomainCorpusDetectedEmotionDomainSpaceInput TextPseudo?documentSimilarityLatent AffectiveEmbeddingLatent AffectiveFoldingAnchorsAffectiveProcessingLSM AffectiveSpaceFigure 2: Proposed Latent Affective Framework.considerably complicates the construction of reli-able affective categories in the latent space.To address the two limitations above, we pro-pose to more broadly leverage the LSM paradigm(Bellegarda, 2005; Bellegarda, 2008), following theoverall framework depicted in Fig.
2.
Compared toFig.
1, we inject some supervision at two separatelevels: not only regarding the particular domain con-sidered, but also how the affective categories them-selves are defined.
The first task is to exploit a suit-able training collection to encapsulate into a (do-main) latent semantic space the general foundationsof the domain at hand.
Next, we leverage a sepa-rate affective corpus, such as mood-annotated blogentries from LiveJournal.com (Strapparava and Mi-halcea, 2008), to serve as a descriptive blueprint forthe construction of affective categories.This blueprint is then folded into the domainspace in one of two ways.
The easiest approach,called latent affective folding, is simply to super-impose affective anchors inferred in the space forevery affective category.
This is largely analogousto what happens in Fig.
1, with a crucial differenceregarding the representation of affective categories:in latent affective folding, it is derived from a cor-pus of texts as opposed to a pre-specified keywordor keyset.
This is likely to help making the cat-egories more robust, but may not satisfactorily re-solve subtle distinctions between emotional conno-tations.
This technique is described in detail in thenext section.The second approach, called latent affective em-bedding, is to extract a distinct LSM representation3DomainCorpusInputTextTRAININGANALYSISDomainSpaceInputVectorLatentFoldingAffectiveSimilarityComputationCloseness MeasureAffective CorpusAnchorsEmotionDetectedMappingLSMLSMMap CreationFigure 3: Emotion Analysis Using Latent Folding.from the affective corpus, to encapsulate all prioraffective information into a separate (affective) la-tent semantic space.
In this space, affective anchorscan be computed directly, instead of inferred afterfolding, presumably leading to a more accurate posi-tioning.
Domain and affective LSM spaces can thenbe related to each other via a mapping derived fromwords that are common to both.
This way, the af-fective anchors can be precisely embedded into thedomain space.
This technique is described in detailin Section 4.In both cases, the input text is mapped into thedomain space as before.
Emotion classification thenfollows from assessing how closely it aligns witheach affective anchor.3 Latent Affective FoldingExpanding the basic framework of Fig.
2 to take intoaccount the two separate phases of training and anal-ysis, latent affective folding proceeds as illustratedin Fig.
3.Let T1, |T1| = N1, be a collection of training texts(be they sentences, paragraphs, or documents) re-flecting the domain of interest, and V1, |V1| = M1,the associated set of all words (possibly augmentedwith some strategic word pairs, triplets, etc., as ap-propriate) observed in this collection.
Generally, M1is on the order of several tens of thousands, while N1may be as high as a million.We first construct a (M1?N1) matrix W1, whoseelements wij suitably reflect the extent to whicheach word wi ?
V1 appeared in each text tj ?
T1.From (Bellegarda, 2008), a reasonable expressionfor wij is:wi,j = (1 ?
?i)ci,jnj, (1)where ci,j is the number of times wi occurs in texttj , nj is the total number of words present in thistext, and ?i is the normalized entropy of wi in V1.The global weighting implied by 1 ?
?i reflects thefact that two words appearing with the same count ina particular text do not necessarily convey the sameamount of information; this is subordinated to thedistribution of words in the entire set V1.We then perform a singular value decomposition(SVD) of W1as (Bellegarda, 2008):W1= U1S1VT1, (2)where U1is the (M1?R1) left singular matrix withrow vectors u1,i (1 ?
i ?
M1), S1 is the (R1 ?
R1)diagonal matrix of singular values s1,1 ?
s1,2 ?.
.
.
?
s1,R1> 0, V1is the (N1?
R1) right sin-gular matrix with row vectors v1,j (1 ?
j ?
N1),R1M1, N1is the order of the decomposition,and T denotes matrix transposition.As is well known, both left and right singularmatrices U1and V1are column-orthonormal, i.e.,UT1U1= VT1V1= IR1(the identity matrix of orderR1).
Thus, the column vectors of U1and V1eachdefine an orthornormal basis for the space of dimen-sion R1spanned by the u1,i?s and v1,j?s.
We referto this space as the latent semantic space L1.
The(rank-R1) decomposition (2) encapsulates a map-ping between the set of words wi and texts tj and(after apropriate scaling by the singular values) theset of R1-dimensional vectors y1,i = u1,iS1 andz1,j = v1,jS1.The basic idea behind (2) is that the rank-R1de-composition captures the major structural associa-tions in W1and ignores higher order effects.
Hence,the relative positions of the input words in the spaceL1reflect a parsimonious encoding of the semanticconcepts used in the domain considered.
This meansthat any new text mapped onto a vector ?close?
(insome suitable metric) to a particular set of words canbe expected to be closely related to the concept en-capsulated by this set.
If each of these words is thenscored in terms of their affective affinity, this offersa way to automatically predict the overall emotionalaffinity of the text.4In order to do so, we need to isolate regions inthat space which are representative of the underly-ing taxonomy of emotions considered.
The centroidof each such region is the affective anchor associ-ated with that basic emotion.
Affective anchors aresuperimposed onto the space L1on the basis of theaffective corpus available.Let T2, |T2| = N2, represent a separate collectionof mood-annotated texts (again they could be sen-tences, paragraphs, or documents), representative ofthe desired categories of emotions (such as JOY andSADNESS), and V2, |V2| = M2, the associated set ofwords or expressions observed in this collection.
Assuch affective data may be more difficult to gatherthan regular texts (especially in annotated form), inpractice N2< N1.Further let V12, |V12| = M12, represent the in-tersection between V1and V2.
We will denote therepresentations of these words in L1by ?1,k (1 ?k ?
M12).Clearly, it is possible to form, for each 1 ?
` ?
L,where L is the number of distinct emotions consid-ered, each subset V(`)12of all entries from V12whichis aligned with a particular emotion.1 We can thencompute:z?1,` =1|V(`)12|?V(`)12?1,k , (3)as the affective anchor of emotion ` (1 ?
` ?
L)in the domain space.
The notation z?1,` is chosen tounderscore the connection with z1,j : in essence, z?1,`represents the (fictitious) text in the domain spacethat would be perfectly aligned with emotion `, hadit been seen the training collection T1.
Comparingthe representation of an input text to each of theseanchors therefore leads to a quantitative assessmentfor the overall emotional affinity of the text.A potential drawback of this approach is that (3) ispatently sensitive to the distribution of words withinT2, which may be quite different from the distribu-tion of words within T1.
In such a case, ?folding in?the affective anchors as described above may wellintroduce a bias in the position of the anchors in thedomain space.
This could in turn lead to an inabilityto satisfactorily resolve subtle distinctions betweenemotional connotations.1Note that one entry could conceivably contribute to severalsuch subsets.LSMInputTextLSMANALYSISDomainCorpusTRAININGMappingMap CreationDomainSpaceAffectiveSimilarityComputationCloseness MeasureAnchorsAffective SpaceAffectiveCorpusEmbeddingLatentLSMEmotionDetectedInputVectorMap CreationFigure 4: Emotion Analysis Using Latent Embedding.4 Latent Affective EmbeddingTo remedy this situation, a natural solution is tobuild a separate LSM space from the affective train-ing data.
Referring back to the basic frameworkof Fig.
2 and taking into account the two separatephases of training and analysis as in Fig.
3, latent af-fective embedding proceeds as illustrated in Fig.
4.The first task is to group all N2documents presentin T2into L bins, one for each of the emotions con-sidered.
Then we can construct a (M2?
L) matrixW2, whose elements w?k,` suitably reflect the extentto which each word or expression w?k ?
V2 appearedin each affective category c`, 1 ?
` ?
L. This leadsto:w?k,` = (1 ?
?
?k)c?k,`n?`, (4)with c?k,`, n?`, and ?
?k following definitions analogousto (1), albeit with domain texts replaced by affectivecategories.We then perform the SVD of W2in a similar veinas (2):W2= U2S2VT2, (5)where all definitions are analogous.
As before,both left and right singular matrices U2and V2arecolumn-orthonormal, and their column vectors eachdefine an orthornormal basis for the space of dimen-sion R2spanned by the u2,k?s and v2,`?s.
We referto this space as the latent affective space L2.
The5(rank-R2) decomposition (5) encapsulates a map-ping between the set of words w?k and categories c`and (after apropriate scaling by the singular values)the set of R2-dimensional vectors y2,k = u2,kS2 andz2,` = v2,`S2.Thus, each vector z2,` can be viewed as the cen-troid of an emotion in L2, or, said another way, anaffective anchor in the affective space.
Since theirrelative positions reflect a parsimonious encoding ofthe affective annotations observed in the emotioncorpus, these affective anchors now properly takeinto account any accidental skew in the distributionof words which contribute to them.
All that remainsto do is map them back to the domain space.This is done on the basis of words that are com-mon to both the affective space and the domainspace, i.e., the words in V12.
Since these words weredenoted by ?1,k in L1, we similarly denote them by?2,k (1 ?
k ?
M12) in L2.Now let ?1, ?2and ?1, ?2denote the mean vec-tor and covariance matrix for all observations ?1,kand ?2,k in the two spaces, respectively.
We firsttransform each feature vector as:?
?1,k = ?
?1/21(?1,k ?
?1) , (6)?
?2,k = ?
?1/22(?2,k ?
?2) , (7)so that the resulting sets {?
?1,k} and {?
?2,k} each havezero mean and identity covariance matrix.For this purpose, the inverse square root of eachcovariance matrix can be obtained as:?
?1/2= Q?
?1/2QT, (8)where Q is the eigenvector matrix of the covariancematrix ?, and ?
is the diagonal matrix of corre-sponding eigenvalues.
This applies to both domainand affective data.We next relate each vector ?
?2,k in the affectivespace to the corresponding vector ?
?1,k in the do-main space.
For a relative measure of how the twospaces are correlated with each other, as accumu-lated on a common word basis, we first project ?
?1,kinto the unit sphere of same dimension as ?
?2,k, i.e.,R2= min(R1, R2).
We then compute the (normal-ized) cross-covariance matrix between the two unitsphere representations, specified as:K12=M12?k=1P??1,kPT?
?T2,k , (9)where P is the R1to R2projection matrix.
Notethat K12is typically full rank as long as M12> R22.Performing the SVD of K12yields the expression:K12= ??
?T, (10)where as before ?
is the diagonal matrix of singu-lar values, and ?
and ?
are both unitary in the unitsphere of dimension R2.
This in turn leads to thedefinition:?
= ?
?T, (11)which can be shown (cf.
(Bellegarda et al, 1994))to represent the least squares rotation that must beapplied (in that unit sphere) to ?
?2,k to obtain an esti-mate of P ?
?1,kPT .Now what is needed is to apply this transforma-tion to the centroids z2,` (1 ?
` ?
L) of the affectivecategories in the affective space, so as to map themto the domain space.
We first project each vectorinto the unit sphere, resulting in:z?2,` = ?
?1/22(z2,` ?
?2) , (12)as prescribed in (7).
We then synthesize from z?2,`a unit sphere vector corresponding to the estimatein the projected domain space.
From the foregoing,this estimate is given by:?z?1,` = ?
z?2,` .
(13)Finally, we restore the resulting contribution at theappropriate place in the domain space, by reversingthe transformation (6):z?1,` = ?1/21?z?1,` + ?1 .
(14)Combining the three steps (12)?
(14) together, theoverall mapping can be written as:z?1,` = (?1/21??
?1/22) z2,` + (?1??1/21??
?1/22?2) .
(15)This expression stipulates how to leverage the ob-served affective anchors z2,` in the affective spaceto obtain an estimate of the unobserved affective an-chors z?1,` in the domain space, for 1 ?
` ?
L. Theoverall procedure is illustrated in Fig.
5 (in the sim-ple case of two dimensions).Once the affective anchors are suitably embeddedinto the domain space, we proceed as before to com-pare the representation of a given input text to eachof these anchors, which leads to the desired quan-titative assessment for the overall emotional affinityof the text.6+z2,l1,k?z?
2,l^1,lz   =z2,l??1/22??1/22?
?1/212,k?AffectiveSphereUnitSpaceDomainxxxxx+++xx+xxxx xxx+ ++ ++ ++ +?xxSpacez2,l ?2(      ?
)2,k ?2(?
?
)1,k ?1(?
?
)+^?1/21 z    +1,l ?1Figure 5: Affective Anchor Embedding (2-D Case).5 Emotion ClassificationTo summarize, using either latent affective foldingor latent affective embedding, we end up with an es-timate z?1,` of the affective anchor for each emotion` in the domain space L1.
What remains to be de-scribed is how to perform emotion classification inthat space.To proceed, we first need to specify how to repre-sent in that space an input text not seen in the train-ing corpus, say tp (where p > N1).
For each entry inT1, we compute for the new text the weighted counts(1) with j = p. The resulting feature vector, a col-umn vector of dimension N1, can be thought of asan additional column of the matrix W1.
Assumingthe matrices U1and S1do not change appreciably,the SVD expansion (2) therefore implies:tp = U1 S1 vT1,p , (16)where the R1-dimensional vector v T1,p acts as an ad-ditional column of the matrix V T1.
Thus, the repre-sention of the new text in the domain space can beobtained from z1,p = v1,pS1.All is needed now is a suitable closeness measureto compare this representation to each affective an-chor z?1,` (1 ?
` ?
L).
From (Bellegarda, 2008), anatural metric to consider is the cosine of the anglebetween them.
This yields:C(z1,p, z?1,`) =z1,p z?
T1,`?z1,p?
?z?1,`?, (17)for any 1 ?
` ?
L. Using (17), it is a simple matterto directly compute the relevance of the input text toeach emotional category.
It is important to note thatword weighting is now implicitly taken into accountby the LSM formalism.6 Experimental EvaluationIn order to evaluate the latent affective frameworkdescribed above, we used the data set that was devel-oped for the SemEval 2007 task on ?Affective Text?
(Strapparava and Mihalcea, 2007).
This task was fo-cused on the emotion classification of news head-lines.
Headlines typically consist of a few wordsand are often written by creative people with theintention to ?provoke?
emotions, and consequentlyattract the readers?
attention.
These characteris-tics make this kind of data particularly suitable foruse in an automatic emotion recognition setting,as the affective/emotional features (if present) areguaranteed to appear in these short sentences.
Thetest data accordingly consisted of 1,250 short newsheadlines2 extracted from news web sites (such asGoogle news, CNN) and/or newspapers, and anno-tated along L = 6 emotions (ANGER, DISGUST,FEAR, JOY, SADNESS, and SURPRISE) by differentevaluators.For baseline purposes, we considered the follow-ing approaches: (i) a simple word accumulation sys-tem, which annotates the emotions in a text based onthe presence of words from the WordNet-Affect lex-icon; and (ii) three LSA-based systems implementedas in Fig.
1, which only differ in the way each emo-tion is represented in the LSA space: either basedon a specific word only (e.g., JOY), or the wordplus its WordNet synset, or the word plus all Word-Net synsets labelled with that emotion in WordNet-Affect (cf.
(Strapparava and Mihalcea, 2007)).
In allthree cases, the large corpus used for LSA process-ing was the Wall Street Journal text collection (Graffet al, 1995), comprising about 86,000 articles.For the latent affective framework, we needed toselect two separate training corpora.
For the ?do-main?
corpus, we selected a collection of aboutN1= 8, 500 relatively short English sentences (witha vocabulary of roughly M1= 12, 000 words)originally compiled for the purpose of a buildinga concatenative text-to-speech voice.
Though not2Development data was merged into the original SemEval2007 test set to produce a larger test set.7Table I: Results on SemEval-2007 Test Corpus.Approach Considered Precision Recall F-MeasureBaseline Word Accumulation 44.7 2.4 4.6LSA (Specific Word Only) 11.5 65.8 19.6LSA (With WordNet Synset) 12.2 77.5 21.1LSA (With All WordNet Synsets) 11.4 89.6 20.3Latent Affective Folding 18.8 90.1 31.1Latent Affective Embedding 20.9 91.7 34.0completely congruent with news headlines, we feltthat the type and range of topics covered was closeenough to serve as a good proxy for the domain.For the ?affective?
corpus, we relied on about N2=5, 000 mood-annotated blog entries from LiveJour-nal.com, with a filtered3 vocabulary of about M2=20, 000 words.
The indication of mood being ex-plicitly specified when posting on LiveJournal, with-out particular coercion from the interface, mood-annotated posts are likely to reflect the true mood ofthe blog authors (Strapparava and Mihalcea, 2008).The moods were then mapped to the L = 6 emotionsconsidered in the classification.Next, we formed the domain and affective matri-ces W1and W2and processed them as in (2) and (5).We used R1= 100 for the dimension of the domainspace L1and R2= L = 6 for the dimension ofthe affective space L2.
We then compared latent af-fective folding and embedding to the above systems.The results are summarized in Table I.Consistent with the observations in (Strapparavaand Mihalcea, 2008), word accumulation secures thehighest precision at the cost of the lowest recall,while LSA-based systems achieve high recall butsignificantly lower precision.
Encouragingly, the F-measure obtained with both latent affective mappingtechniques is substantially higher than with all fourbaseline approaches.
Of the two techniques, latentembedding performs better, presumably because theembedded affective anchors are less sensitive thanthe folded affective anchors to the distribution ofwords within the affective corpus.
Both techniquesseem to exhibit an improved ability to resolve dis-tinctions between emotional connotations.3Extensive text pre-processing is usually required on blogentries, to address typos and assorted creative license.7 ConclusionWe have proposed a data-driven strategy for emotionanalysis which focuses on two coupled phases: (i)separately encapsulate both the foundations of thedomain considered and the overall affective fabricof the language, and (ii) exploit the emergent rela-tionship between these two semantic levels of de-scription in order to inform the emotion classifica-tion process.
We address (i) by leveraging the la-tent topicality of two distinct corpora, as uncoveredby a global LSM analysis of domain-oriented andemotion-oriented training documents.
The two de-scriptions are then superimposed to produce the de-sired connection between all terms and emotionalcategories.
Because this connection automaticallytakes into account the influence of the entire train-ing corpora, it is more encompassing than that basedon the relatively few affective terms typically con-sidered in conventional processing.Empirical evidence gathered on the ?AffectiveText?
portion of the SemEval-2007 corpus (Strap-parava and Mihalcea, 2007) shows the effective-ness of the proposed strategy.
Classification per-formance with latent affective embedding is slightlybetter than with latent affective folding, presumablybecause of its ability to more richly describe theaffective space.
Both techniques outperform stan-dard LSA-based approaches, as well as affectivelyweighted word accumulation.
This bodes well forthe general deployability of latent affective process-ing across a wide range of applications.Future efforts will concentrate on characterizingthe influence of the parameters R1and R2on thevector spaces L1and L2, and the correspondingtrade-off between modeling power and generaliza-tion properties.
It is also of interest to investigate8how incorporating higher level units (such as com-mon lexical compounds) into the LSM proceduremight further increase performance.ReferencesA.
Abbasi (2007), ?Affect Intensity Analysis of DarkWeb Forums,?
in Proc.
IEEE Int.
Conf.
Intelligenceand Security Informatics (ISI), New Brunswick, NJ,282?288.C.
Ovesdotter Alm, D. Roth, and R. Sproat (2005),?Emotions from Text: Machine Learning for Text?Based Emotion Prediction,?
in Proc.
Conf.
HumanLanguage Technology and Empirical Methods in NLP,Vancouver, BC, 579?586.J.R.
Bellegarda (2005), ?Latent Semantic Mapping: AData?Driven Framework for Modeling Global Rela-tionships Implicit in Large Volumes of Data,?
IEEESignal Processing Magazine, 22(5):70?80.J.R.
Bellegarda (2008), Latent Semantic Mapping: Prin-ciples & Applications, Synthesis Lectures on Speechand Audio Processing Series, Fort Collins, CO: Mor-gan & Claypool.J.R.
Bellegarda, P.V.
de Souza, A. Nadas, D. Nahamoo,M.A.
Picheny and L.R.
Bahl (1994), ?The Metamor-phic Algorithm: A Speaker Mapping Approach toData Augmentation,?
IEEE Trans.
Speech and AudioProcessing, 2(3):413?420.E.
Cosatto, J. Ostermann, H.P.
Graf, and J. Schroeter(2003), ?Lifelike talking faces for interactive ser-vices,?
in Proc.
IEEE, 91(9), 1406?1429.S.
Deerwester, S.T.
Dumais, G.W.
Furnas, T.K.
Landauer,and R. Harshman (1990), ?Indexing by Latent Se-mantic Analysis,?
J. Amer.
Soc.
Information Science,41:391?407.P.
Ekman (1993), ?Facial Expression and Emotion?,American Psychologist, 48(4), 384?392.C.
Fellbaum, Ed., (1998), WordNet: An Electronic Lexi-cal Database, Cambridge, MA: MIT Press.D.
Graff, R. Rosenfeld, and D. Paul (1995), ?CSR-IIIText,?
Linguistic Data Consortium, #LDC95T6.P.
Johnson?Laird and K. Oatley (1989), ?The Languageof Emotions: An Analysis of a Semantic Field,?
Cog-nition and Emotion, 3:81?123.J.
Liscombe, G. Riccardi, and D. Hakkani-Tu?r (2005),?Using Context to Improve Emotion Detection in Spo-ken Dialog Systems,?
Proc.
Interspeech, Lisbon, Por-tugal, 1845?1848.H.
Liu, H. Lieberman, and T. Selker (2003), ?A Modelof Textual Affect Sensing Using Real-World Knowl-edge,?
in Proc.
Intelligent User Interfaces (IUI), Mi-ami, FL, 125?132.A.
Mehrabian (1995), ?Framework for a ComprehensiveDescription and Measurement of Emotional States,?Genetic, Social, and General Psychology Mono-graphs, 121(3):339?361.R.
Mihalcea and C. Strapparava (2006), ?Learning toLaugh (Automatically): Computational Models forHumor Recognition,?
J. Computational Intelligence,22(2):126?142.M.
Mohler and R. Mihalcea (2009), ?Text-to-text Seman-tic Similarity for Automatic Short Answer Grading,?in Proc.
12th Conf.
European Chap.
ACL, Athens,Greece, 567?575.B.
Pang and L. Lee (2008), ?Opinion Mining and Sen-timent Analysis,?
in Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135.R.W.
Picard (1997), Affective Computing, Cambridge,MA: MIT Press.M.
Pivec and P. Kearney (2007), ?Games for Learningand Learning from Games,?
Informatica, 31:419?423.J.A.
Russell (1980), ?A Circumplex Model of Affect,?
J.Personality and Social Psychology, 39:1161?1178.S.
Ryan, B. Scott, H. Freeman, and D. Patel (2000), TheVirtual University: The Internet and Resource-basedLearning, London, UK: Kogan Page.M.
Schro?der (2006), ?Expressing Degree of Activationin Synthetic Speech,?
IEEE Trans.
Audio, Speech, andLanguage Processing, 14(4):1128?1136.P.J.
Stone (1997), ?Thematic Text Analysis: New agen-das for Analyzing Text Content,?
in Text Analysis forthe Social Sciences: Methods for Drawing StatisticalInferences from Texts and Transcripts, C.W.
Roberts,Ed., Mahwah, NJ: Lawrence Erlbaum Assoc.
Publish-ers, 35?54.C.
Strapparava and R. Mihalcea (2007), ?SemEval-2007Task 14: Affective Text,?
in Proc.
4th Int.
Workshop onSemantic Evaluations (SemEval 2007), Prague, CzechRepublic.C.
Strapparava and R. Mihalcea (2008), ?Learning toIdentify Emotions in Text,?
in Proc.
2008 ACM Sym-posium on Applied Computing, New York, NY, 1556?1560.C.
Strapparava, A. Valitutti, and O.
Stock (2006), ?TheAffective Weight of Lexicon,?
in Proc.
5th Int.
Conf.Language Resources and Evaluation (LREC), Lisbon,Portugal.P.
Subasic and A. Huettner (2001), ?Affect Analysisof Text Using Fuzzy Semantic Typing,?
IEEE Trans.Fuzzy Systems, 9(4):483?496.C.M.
Whissell (1989), ?The Dictionary of Affect in Lan-guage,?
in Emotion: Theory, Research, and Experi-ence, R. Plutchik and H. Kellerman, Eds., New York,NY: Academic Press, 13?131.9
