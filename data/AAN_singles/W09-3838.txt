Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 234?237,Paris, October 2009. c?2009 Association for Computational LinguisticsDependency Parsing with Energy-based Reinforcement LearningLidan ZhangDepartment of Computer ScienceThe University of Hong KongPokfulam Road, Hong Konglzhang@cs.hku.hkKwok Ping ChanDepartment of Computer ScienceThe University of Hong KongPokfulam Road, Hong Kongkpchan@cs.hku.hkAbstractWe present a model which integratesdependency parsing with reinforcementlearning based on Markov decision pro-cess.
At each time step, a transition ispicked up to construct the dependency treein terms of the long-run reward.
The op-timal policy for choosing transitions canbe found with the SARSA algorithm.
InSARSA, an approximation of the state-action function can be obtained by calcu-lating the negative free energies for theRestricted Boltzmann Machine.
The ex-perimental results on CoNLL-X multilin-gual data show that the proposed modelachieves comparable results with the cur-rent state-of-the-art methods.1 IntroductionDependency parsing, an important task, can beused to facilitate some natural language applica-tions.
Given a sentence, dependency parsing isto find an acyclic labeled directed tree, projectiveor non-projective.The label of each edge gives thesyntactic relationship between two words.Data-driven dependency parsers can be catego-rized into graph-based and transition-based mod-els.
Both of these two models have their advan-tages as well as drawbacks.
As discussed in (Mc-Donald and Satta, 2007), transition-based mod-els use local training and greedy inference algo-rithms, with a rich feature set, whereas they mightlead to error propagation.
In contrast, graph-basedmodels are globally trained coupled with exact in-ference algorithms, whereas their features are re-stricted to a limited number of graph arcs.
Nivreand McDonald (2008) presented a successful at-tempt to integrate these two models by exploitingtheir complementary strengths.There are other researches on improving theindividual model with a novel framework.
Forexample, Daume?
et al (2006) applied a greedysearch to transition-based model, which was ad-justed by the resulting errors.
Motivated by hiswork, our transition-based model is expected toovercome local dependencies by using a long-termdesirability introduced by reinforcement learning(RL).
We rely on a ?global?
policy to guide eachaction selection for a particular state during pars-ing.
This policy considers not only the currentconfiguration but also a few of look-ahead steps.Thus it yields an optimal action from the long-term goal.
For example, an action might returna high value even if it produces a low immediatereward, because its following state-actions mightyield high rewards.
The reverse also holds true.Finally we formulate the parsing problem with theMarkov Decision Process (MDP) for the dynamicsettings.The reminder of this paper is organized as fol-lows: Section 2 describes the transition-based de-pendency parsing.
Section 3 presents the proposedreinforcement learning model.
Section 4 gives theexperimental results.
Finally, Section 5 concludesthe paper.2 Transition-based Dependency ParsingIn this paper, we focus on the transition-baseddependency parsing in a shift-reduce frame-work (Ku?bler et al, 2009).
Given a sentencex = w0, w1, ..., wn, its dependency tree is con-structed by a sequence of transitions.
The datastructures include a stack S to store partially pro-cessed words and a queue I to record the remain-ing input words and the partial labeled dependencystructure constructed by the previous transitions.Four permissible transitions are considered: Re-duce: pops word wi from the stack; Shift: pushesthe next input wj onto the stack; Left-Arcr: addsa labeled dependency arc r from the next input wjto the top of the stack wi, then pops word wi fromthe stack; Right-Arcr: adds a dependency arc r234S1A1r1S2A2r2StAtrtSt+1At+1rt+1 AtStHttis tjatkhtikw tjkvtFigure 1: The MDP with factored states and actions.
Left: The general network.
Right: Detailed networkwith one hidden layer at time t. Visible variables (states and actions) are shaded.
Clear circles representhidden variables.from the top of the stack wi to the next input wj ,and pushes word wj onto the stack.Starting from the empty stack and initializingthe queue I as the input words, the parser termi-nates when the queue I is empty.
The optimaltransition (or say, action/decision A) in each stepis conditioned on the current configuration c ofthe parser.
For non-projective cases, preprocess-ing and postprocessing are applied.3 Reinforcement Learning3.1 General FrameworkWe begin with looking at the general framework tointegrate RL into the transition-based dependencymodel.
In this paper, we reformulate the depen-dency parsing as Markov Decision Process (MDP,(S,A, T , r)) where:?
S is the set of states.?
A is the set of possible actions.?
T is the transition function, T : S ?A ?
S.we denote the transition probability Pij(a) =P (st+1 = j|st = i, At = a).?
r is the reward function by executing actiona in a certain state, which is denoted as ri(a).As aforesaid, the key task of dependency pars-ing is to select the optimal action to be performedbased on the current state.
Given the expected im-mediate reward r, the optimal policy (pi : S 7?
A)is to maximize the long-term expected reward asfollows:Rt =?
?k=0?krt+k (1)Given a policy pi, state-action function Qpi(i, a)can be defined as the expected accumulative re-ward received by taking action a in state s. It takesthe following form:Qpi(i, a) = Epi[?
?k=0?krt+k|st = i, at = a]= ?jPij(a)[ri(a) + ?
?bpi(j, b)Qpi(j, b)](2)Here pi(j, b) is the probability of picking up actionb in state j, ?
?
[0, 1] is a discount factor to con-trol the involvement of further actions.
Accordingto the Bellman equation, the state-action functioncan be updated iteratively with equation( 2).Given the state-action function, a greedy policycan be found by maximizing over possible actions:pi?
= argmaxaQpi(i, a) (3)In the following, we will discuss how to com-pute the state-action function Q by investigatingthe free energy in RBM.3.2 Restricted Boltzmann Machine3.2.1 Free EnergyFigure 1 shows the general framework of ourmodel.
At each time step t, there is no connectionsbetween nodes within the same layer.
In the net-work, ?visible?
variables include both states andactions (V = S?A).
The visible layer is fully con-nected to a ?hidden?
layer, which can be regardedas a Restricted Boltzmann Machine (RBM).In our model, both states and actions are fac-tored.
They are consisted of a sets of discrete vari-ables (Sallans and Hinton, 2004).
The stochas-tic energy of the network can be computed bythe conductivities between visible and hidden vari-ables.E(s, a, h) = ?
?i,kwiksihk ?
?j,k?jkajhk (4)235The above energy determine their equilibriumprobabilities via the Boltzmann distribution:P (s, a, h) = exp(?E(s, a, h))?s?,a?,h?
exp(?E(s?, a?, h?
))(5)By marginalizing out the hidden variables, wecan obtain the ?equilibrium free energy?
of s anda, which can be expressed as an expected energyminus an entropy:F (s,a)=??k(?i(wiksi?hk?)+?j(?jkaj?hk?))+?k?hk?
log?hk?+(1??hk?)
log(1??hk?
)(6)where ?hk?
is the expected value of variable hk:?hk?
= ?
(?i,kwiksi +?j,k?jkaj) (7)where ?
= 1/(1 + e?x) is a sigmoid function.As is proved in (Sallans and Hinton, 2004), thevalue of a state-action function can be approxi-mated by the negative free energy of the network:Q(s, a) ?
?F (s, a) (8)3.2.2 Parameter LearningThe parameters of the network can be updated bythe SARSA (State-Action-Reward-State-Action)algorithm.
The inputs of the SARSA algorithmare the state-action pairs of the two neighboringslices.
Then the error can be computed as:E(st, at) = [rt+?Q(st+1, tt+1)]?Q(st, at) (9)Suppose the state-action function is parameter-ized by ?.
The update equation for the parameteris:4?
?
E(st, at)?
?Q(st, at) (10)Back to our model, the parameters ?
= (w, u)are given by:?wik ?(rt+?Q(st+1,at+1)?Q(st,at))sti?hk?
?ujk ?(rt+?Q(st+1,at+1)?Q(st,at))atj?hk?
(11)Leemon (1993) showed that the above updaterules can work well in practice even though thereis no proof of convergence in theory.
In addition,in dependency parsing task, the possible actionnumber is small (=4).
Our experimental resultsalso showed that the learning rule can converge inpractice.3.3 Action SelectionAfter training, we use the softmax rules to selectthe optimal action for a given state.
The probabil-ity of an action is given by Boltzmann distribution:P (a|s) ?
eQ(s,a)/?Z (12)Here Z is an normalization factor.
?
is a pos-itive number called the temperature.
High tem-perature means the actions are evenly distributed.Low temperature case a great variety in selectionprobability.
In the limit as ?
?
0, softmax actionselection becomes greedy action selection.4 Experiments4.1 SettingsWe use the CoNLL-X (Buchholz and Marsi,2006) distribution data from seven different lan-guages (Arabic, Bulgarian, Dutch, Portuguese,Slovene, Spanish and Swedish).
These treebanksvaried in sizes from 29,000 to 207,000 tokens.
Thecut-off frequency for training data is 20, whichmeans we ignores any attribute (FORM, LEMMA,POS or FEATS) occurred less than 20.
Further-more we randomly selected 10 percent of train-ing data to construct the validation set.
Test setsare about equal for all languages.
Since our algo-rithm only deals with projective cases, we use pro-jectivization/deprojectivization method for train-ing and testing data.For fair comparison, we use the exactly samefeature set as Nivre et al (2006), which is com-prised of a variety of features extracted from thestack, the queue and the partially built dependencygraph.In our experiment, the immediate reward valueis defined as the Hamming Loss between partialtree and expected tree, which counts the numberof places that the partial output y?
differs from thetrue output y: ?Ti=1 1[yi 6= y?i].As shown in Figure 1, we compute the state-action function using a feed-forward neural net-work with one hidden layer.
The number of hid-den variables is set to match the variable number inthe visible layer (i.e.
total number of state and ac-tion variables).
The parameters of the network aremodified by SARSA algorithm according to equa-tion 2.
Finally, 10-width beam search is employedfor all languages, during testing.There are other parameters in our experiments,which can be tuned using search.
For simplicity,236Ar Bu Du Po Sl Sp SwLAS Our 63.24 88.89 79.06 87.54 72.44 82.79 87.20Nivre 66.71 87.41 78.59 87.60 70.30 81.29 84.58UAS Our 75.30 92.88 83.14 91.34 80.06 86.18 91.84Nivre 77.51 91.72 81.35 91.22 78.72 84.67 89.50Table 1: Comparison of dependency accuracy with Nivrethe learning rate was exponentially decreased form0.1 to 0.01 in the course of each epoch.
In idealcases, the discount factor should be set to 1.
In ourexperiments, discount factor is fixed to 0.6 consid-ering the computational burden in long sentence.The study of the this parameter is still left for fu-ture work.
Finally, the inverse temperature linearlyincreased from 0 to 2.4.2 ResultsThe performance of our model is evaluated bythe official attachment score, including labeled(LAS=the percentage of tokens with the correcthead and label) and unlabeled (UAS=the percent-age of tokens with the correct head).
Punctuationtokens were excluded from scoring.The result comparison between our system andNivre?s transition-based system is shown in Ta-ble 11.
From the table, we can see that the pro-posed model outperformed the Nivre?s score in alllanguages except Arabic.
In Arabic, our results areworse than Nivre, with about 3.5% performancereduction in LAS measure and 2.2% in UAS.
Mostof our errors occur in POSTAGS with N (16%head errors and 31% dep errors) and P (47% headerrors and 8% dep errors), which is probably dueto the flexible usage of those two tags in Ara-bic.
The best performance of our model happensin Swedish.
The LAS improves from 84.58% to87.20%, whereas UAS improves from 89.5% to91.84%.
The reason might be that the long depen-dent relationship is not popular in Swedish.
Fi-nally, we believe the performance will be furtherimproved by carefully tuning parameters or broad-ening the beam search width.5 ConclusionsIn this paper we proposed a dependency parsingbased on reinforcement learning.
The parser usesa policy to select the optimal transition in eachparsing stage.
The policy is learned from RL in1The performance of other systems can be accessed fromhttp://nextens.uvt.nl/?conllterms of the long-term reward.
Tentative experi-mental evaluations show that the introduction ofRL is feasible for some NLP applications.
Finally,there are a lot of future work, including the hierar-chical model and parameter selections.ReferencesSabine Buchholz and Erwin Marsi.
2006.
Conll-x shared task on multilingual dependency parsing.In Proceedings of the Tenth Conference on Com-putational Natural Language Learning (CoNLL-X),pages 149?164, New York City, June.
Associationfor Computational Linguistics.Hal Daume?
III, John Langford, and Daniel Marcu.2006.
Searn in practice.Leemon C. Baird III and A. Harry.
Klopf.
1993.
Rein-forcement learning with high-dimensional, contin-uous actions.
Technical Report WL?TR-93-1147,Wright-Patterson Air Force Base Ohio: Wright Lab-oratory.Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.2009.
Dependency parsing.
Calif, Morgan & Clay-pool publishers, US.Ryan McDonald and Giorgio Satta.
2007.
On the com-plexity of non-projective data-driven dependencyparsing.
In Proceedings of the Tenth InternationalConference on Parsing Technologies, pages 121?132, Prague, Czech Republic, June.
Association forComputational Linguistics.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing graph-based and transition-based dependencyparsers.
In Proceedings of ACL-08: HLT, pages950?958, Columbus, Ohio, June.
Association forComputational Linguistics.Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?enEryig?it, and Svetoslav Marinov.
2006.
Labeledpseudo-projective dependency parsing with supportvector machines.
In Proceedings of the Tenth Con-ference on Computational Natural Language Learn-ing (CoNLL-X), pages 221?225, New York City,June.
Association for Computational Linguistics.Brian Sallans and Geoffrey E. Hinton.
2004.
Rein-forcement learning with factored states and actions.Journal of Machine Learning Research, 5:1063?1088.237
