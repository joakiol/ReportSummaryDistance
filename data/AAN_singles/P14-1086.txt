Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 913?922,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsQuery-Chain Focused SummarizationTal BaumelDept.
of Computer ScienceBen-Gurion UniversityBeer-Sheva, Israeltalbau@cs.bgu.ac.ilRaphael CohenDept.
of Computer ScienceBen-Gurion UniversityBeer-Sheva, Israelcohenrap@cs.bgu.ac.ilMichael ElhadadDept.
of Computer ScienceBen-Gurion UniversityBeer-Sheva, Israelelhadad@cs.bgu.ac.ilAbstractUpdate summarization is a form of multi-document summarization where a documentset must be summarized in the context of otherdocuments assumed to be known.
Efficientupdate summarization must focus on identify-ing new information and avoiding repetition ofknown information.
In Query-focused summa-rization, the task is to produce a summary asan answer to a given query.
We introduce anew task, Query-Chain Summarization, whichcombines aspects of the two previous tasks:starting from a given document set, increas-ingly specific queries are considered, and anew summary is produced at each step.
Thisprocess models exploratory search: a user ex-plores a new topic by submitting a sequence ofqueries, inspecting a summary of the result setand phrasing a new query at each step.
Wepresent a novel dataset comprising 22 query-chains sessions of length up to 3 with 3 match-ing human summaries each in the consumer-health domain.
Our analysis demonstrates thatsummaries produced in the context of suchexploratory process are different from in-formative summaries.
We present an algorithmfor Query-Chain Summarization based on anew LDA topic model variant.
Evaluation in-dicates the algorithm improves on strong base-lines.1 IntroductionIn the past 10 years, the general objective oftext summarization has been refined into morespecific tasks.
Such summarization tasks include:(i) Generic Multi Document Summarization:aims at summarizing a cluster of topically relateddocuments, such as the top results of a searchengine query; (ii) in Update Summarization, a setof documents is summarized while assuming theuser has already read a summary of earlier doc-uments on the same topic; (iii) in Query-FocusedSummarization, the summary of a documents setis produced to convey an informative answer inthe context of a specific query.
The importanceof these specialized tasks is that they help us dis-tinguish criteria that lead to the selection of con-tent in a summary: centrality, novelty, relevance,and techniques to avoid redundancy.We present in this paper a variant summariza-tion task which combines the two aspects of up-date and query-focused summarization.
The taskis related to exploratory search (Marchionini,2006).
In contrast to classical information seek-ing, in exploratory search, the user is uncertainabout the information available, and aims atlearning and understanding a new topic (Whiteand Roth, 2009).
In typical exploratory searchbehavior, a user posts a series of queries, andbased on information gathered at each step, de-cides how to further explore a set of documents.The metaphor of berrypicking introduced in(Bates, 1989) captures this interactive process.At each step, the user may zoom in to a morespecific information need, zoom out to a moregeneral query, or pan sideways, in order to inves-tigate a new aspect of the topic.We define Query-Chain Focused Summariza-tion as follows: for each query in an exploratorysearch session, we aim to extract a summary thatanswers the information need of the user, in amanner similar to Query-Focused Summariza-tion, while not repeating information alreadyprovided in previous steps, in a manner similar toUpdate Summarization.
In contrast to query-focused summarization, the context of a sum-913mary is not a single query, but the set of queriesthat led to the current step, their result sets andthe corresponding summaries.We have constructed a novel dataset of Query-Sets with matching manual summarizations inthe consumer health domain (Cline and Haynes,2001).
Queries are extracted from PubMedsearch logs (Dogan et al, 2009).
We have ana-lyzed this manual dataset and confirm that sum-maries written in the context of berry-picking aremarkedly different from those written for similarqueries on the same document set, but withoutthe query-chain context.We have adapted well-known multi-documentalgorithms to the task, and present baseline algo-rithms based on LexRank (Erkan and Radev,2004), KLSum and TopicSum (Haghighi andVanderwende, 2009).
We introduce a new algo-rithm to address the task of Query-Chain Fo-cused Summarization, based on a new LDA topicmodel variant, and present an evaluation whichdemonstrates it improves on these baselines.The paper is structured as follows.
Section 2formulates the task of Query-Chain FocusedSummarization.
Section 3 reviews related work.In Section 4, we describe the data collection pro-cess and the resulting dataset.
We then presentour algorithm, as well as the baseline algorithmsused for evaluation.
We conclude with evalua-tion and discussion.2 Query- Chain SummarizationIn this work, we focus on the zoom in aspectof the exploratory search process describedabove.
We formulate the Query-Chain FocusedSummarization (QCFS) task as follows:Given an ordered chain of queries Q and a setof documents D , for each query Qqi?
a sum-mary Si is generated from D answeringiq  underthe assumption that the user has already read thesummaries Si-1 for queries10... ?iqq .A typical example of query chain in the con-sumer health domain we investigate includes thefollowing 3 successive queries: (Causes of asth-ma, Asthma and Allergy, Asthma and Mold Al-lergy).
We consider a single set of documentsrelevant to the domain of Asthma as the refer-ence set D.  The QCFS task consists of generat-ing one summary of D as an answer to each que-ry, so that the successive answers do not repeatinformation already provided in a previous an-swer.3 Previous WorkWe first review the closely related tasks ofUpdate Summarization and Query-FocusedSummarization.
We also review key summariza-tion algorithms that we have selected as baselineand adapted to the QCFS task.Update Summarization focuses on identifyingnew information relative to a previous body ofinformation, modeled as a set of documents.
Ithas been introduced in shared tasks in DUC 2007and TAC 2008.
This task consists of producing amulti-document summary for a document set ona specific topic, and then a multi-documentsummary for a different set of articles on thesame topic published at later dates.
This taskhelps us understand how update summaries iden-tified and focused on new information while re-ducing redundancy compared to the originalsummaries.The TAC 2008 dataset includes 48 sets of 20documents, each cluster split in two subsets of 10documents (called A and B).
Subset B docu-ments were more recent.
Original summarieswere generated for the A subsets and updatesummaries were then produced for the B subsets.Human summaries and candidate systems areevaluated using the Pyramid method (Nenkovaand Passonneau, 2004).
For automatic evaluation,ROUGE (Lin, 2004) variants have been pro-posed (Conroy et al, 2011).
In contrast to thissetup, QCFS distinguishes the subsets of docu-ments considered at each step of the process byfacets of the underlying topic, and not by chro-nology.
In addition, the document subsets are notidentified as part of the task in QCFS (as op-posed to the explicit split in A and B subsets inUpdate Summarization).Most systems working on Update Summariza-tion have focused on removing redundancy.
Du-alSum (Delort and Alfonseca, 2012) is notable inattempting to directly model novelty using a spe-cialized topic-model to distinguish words ex-pressing background information and those in-troducing new information in each document.In Query-Focused Summarization (QFS), thetask consists of identifying information in a doc-ument set that is most relevant to a given query.914This differs from generic summarization, whereone attempts to identify central information.QFS helps us distinguish models of relevanceand centrality.
Unfortunately, detailed analysisof the datasets produced for QFS indicates thatthese two notions are not strongly distinguishedin practice: (Gupta et al, 2007) observed that inQFS datasets, up to 57% of the words in the doc-ument sets were closely related to the query(through simple query expansion).
They notethat as a consequence, a generic summarizerforms a strong baseline for such biased QFStasks.We address this limitation of existing QFS da-tasets in our definition of QCFS: we identify achain of at least 3 related queries which focus ondifferent facets of the same central topic and re-quire the generation of distinct summaries foreach query, with little repetition across the steps.A specific evaluation aspect of QFS measuresresponsiveness (how well the summary answersthe specific query).
QFS must rely on Infor-mation Retrieval techniques to overcome thescarceness of the query to establish relevance.As evidenced since (Daume and Marcu, 2006),Bayesian techniques have proven effective at thistask: we construct a latent topic model on thebasis of the document set and the query.
Thistopic model effectively serves as a query expan-sion mechanism, which helps assess the rele-vance of individual sentences to the original que-ry.In recent years, three major techniques haveemerged to perform multi-document summariza-tion: graph-based methods such as LexRank (Er-kan and Radev, 2004) for multi document sum-marization and Biased-LexRank (Otterbacher etal., 2008) for query focused summarization, lan-guage model methods such as KLSum (Haghighiand Vanderwende, 2009) and variants of KLSumbased on topic models such as BayesSum (Dau-me and Marcu, 2006) and TopicSum (Haghighiand Vanderwende, 2009).LexRank is a stochastic graph-based methodfor computing the relative importance of textualunits in a natural text.
The LexRank algorithmbuilds a weighted graph ?
= (?, ?)
where eachvertex in ?
is a linguistic unit (in our case sen-tences) and each weighted edge in ?
is a measureof similarity between the nodes.
In our imple-mentation, we model similarity by computing thecosine distance between the ??
?
???
vectorsrepresenting each node.
After the graph is gener-ated, the PageRank algorithm (Page et al, 1999)is used to determine the most central linguisticunits in the graph.
To generate a summary weuse the ?
most central lexical units, until thelength of the target summary is reached.
Thismethod has no explicit control to avoid redun-dancy among the selected sentences, and theoriginal algorithm does not address update orquery-focused variants.
Biased-LexRank (Otter-bacher et al, 2008) makes LexRank sensitive tothe query by introducing a prior belief about theranking of the nodes in the graph, which reflectsthe similarity of sentences to the query.
Pag-eRank spreads the query similarity of a vertex toits close neighbors, so that we rank higher sen-tences that are similar to other sentences whichare similar to the query.
As a result, Biased-LexRank overcomes the lexical sparseness of thequery and obtained state of the art results on theDUC 2005 dataset.KLSum adopts a language model approach tocompute relevance: the documents in the inputset are modeled as a distribution over words (theoriginal algorithm uses a unigram distributionover the bag of words in documents D).
KLSumis a sentence extraction algorithm: it searches fora subset of the sentences in D with a unigramdistribution as similar as possible to that of theoverall collection D, but with a limited length.The algorithm uses Kullback-Lieber (KL) diver-gence ??(?||?)
= ?
log?
(?(?)?(?))?(?)
to com-pute the similarity of the distributions.
It searchesfor ??
= argmin|?|<???(??||??).
This search isperformed in a greedy manner, adding sentencesone by one to S until the length L is reached, andchoosing the best sentence as measured by KL-divergence at each step.
The original method hasno update or query focusing capability, but as ageneral modeling framework it is easy to adapt toa wide range of specific tasks.TopicSum uses an LDA-like topic model (Bleiet al 2003) to classify words from a number ofdocument sets (each set discussing a differenttopic) as either general non-content words, topicspecific words and document specific word (thiscategory refers to words that are specific to thewriter and not shared across the document set).After the words are classified, the algorithm usesa KLSum variant to find the summary that bestmatches the unigram distribution of topic specif-ic words.
This method improves the results of915KLSum but it also has no update summary orquery answering capabilities.4 Dataset CollectionWe now describe how we have constructed adataset to evaluate QCFS algorithms, which weare publishing freely.
We selected to build ourdataset in the Consumer Health domain, a popu-lar domain in the web (Cline and Haynes 2001)providing medical information at various levelsof complexity, ranging from layman and up toexpert information, because consumer health il-lustrates the need for exploratory search.The PubMed repository, while primarily servingthe academic community, is also used by laymento ask health related questions.
The PubMed que-ry logs (Dogan et al, 2009) provide user querieswith timestamps and anonymized user identifica-tion.
They are publically available and includeover 600K queries per day.
In this dataset, Doganand Murray found that query reformulation (typ-ical of exploratory search) is quite frequent: "Inour dataset, 47% of all queries are followed by anew subsequent query.
These users did not selectany abstract or full text views from the result set.We make an operational assumption that theseusers?
intent was to modify their search by re-formulating their query."
We used these logs toextract laymen queries relating to four topics:Asthma, Lung Cancer 2EHVLW\ DQG $O]KHLPHU?Vdisease.
We extracted a single day query log.From these, we extracted sessions which con-WDLQHG WKH WHUPV ?Asthma? ?Lung Cancer ,??Obesity?
RU ?Alzheimer .?
Sessions containingVHDUFK WDJV VXFK DV ?>$XWKRU@?
ZHUH removedto reduce the number of academic searches.
Thesessions were then manually examined and usedto create zoom-in query chains of length 3 atmost.
The queries appear below:Asthma:Asthma causes?
asthma allergy?
asthma mold allergy;Asthma treatment?asthma medication?corticosteroids;Exercise induced asthma?
exercise for asthmatic;Atopic dermatitis?
atopic dermatitis medications?
atopicdermatitis side effects;Atopic dermatitis?
atopic dermatitis children?
atopic der-matitis treatment;Atopic dermatitis?
atopic dermatitis exercise activity?atopic dermatitis treatment;Cancer:Lung cancer?
lung cancer causes?
lung cancer symptoms;Lung cancer diagnosis?
lung cancer treatment?lung cancertreatment side effects;Stage of lung cancer?
lung cancer staging tests?
lung can-cer TNM staging system;Types of lung cancer?non-small cell lung cancer treat-ment?non-small cell lung cancer surgery;Lung cancer in women?
risk factors for lung cancer inwomen?
treatment of lung cancer in women;Lung cancer chemotherapy?
goals of lung cancer chemo-therapy?
palliative care for lung cancer;Obesity:Salt obesity?retaining fluid;Obesity screening?body mass index?BMI Validity;Childhood obesity?childhood obesity low income?chil-dren diet and exercise;Causes of childhood obesity?obesity and nutrition?schoollunch;Obesity and lifestyle change?obesity metabolism?super-foods antioxidant;Obesity and diabetes?emergence of type 2 diabetes?type 2diabetes and obesity in children;Alzheimer?s disease:Alzheimer memory?helping retrieve memory alzheimer?alzheimer memory impairment nursing;Cognitive impairment?Vascular Dementia?Vascular De-mentia difference alzheimer;$O]KHLPHU?V symptoms?alzheimer diagnosis?alzheimermedications;Semantic dementia?first symptoms dementia?first symp-toms alzheimer;Figure 1: Queries Used to Construct DatasetWe asked medical experts to construct fourdocument collections from well-known and reli-able consumer health websites relating to thefour subjects (Wikipedia, WebMD, and theNHS), so that they would provide general infor-mation relevant to the queries.We then asked medical students to manuallyproduce summaries of these four document col-lections for each query-chain.
The medical stu-dents were instructed construct a text of up to250 words that provides a good answer to eachquery in the chain.
For each query in a chain thesummarizers should assume that the person read-ing the summaries is familiar with the previous916summaries in the chain so they should avoid re-dundancy.Three distinct human summaries were pro-duced for each chain.
For each chain, one sum-mary was produced for each of the three queries,where the person producing the summary wasnot shown the next steps in the chain when an-swering the first query.To simulate the exploratory search of the userwe provided the annotators with a Solr1  queryinterface for each document collection.
The in-terface allowed querying the document set, read-ing the documents and choosing sentences whichanswer the query.
After choosing the sentences,annotators can copy and edit the resulting sum-mary in order to create an answer of up to 250words.
After processing the first two query chainsummaries, the annotators held a post-hoc dis-cussion about the different summaries in order toadjust their conception of the task.The statistics on the collected dataset appear inthe Tables below:Document sets # Docs # Sentences #Tokens /UniqueAsthma  125 1,924 19,662 / 2,284Lung-Cancer 135 1,450 17,842 / 2,228Obesity 289 1,615 21,561 / 2,907$O]KHLPHU?V 'LVHDVH 191 1,163 14,813 / 2,508Queries # Sessions # Sentences #Tokens /UniqueAsthma  5 15 36 / 14Lung-Cancer 6 18 71 / 25Obesity 6 17 45 / 29$O]KHLPHU?V 'LVHDVH 4 12 33 / 16Manual Summaries # Docs # Sentences #Tokens /UniqueAsthma  45 543 6,349  / 1,011Lung-Cancer 54 669 8,287  / 1,130Obesity 51 538 7,079  / 1,270$O]KHLPHU?V 'LVHDVH 36 385 5,031  /    966Table 1: Collected Dataset Size StatisticsA key aspect of the dataset is that the samedocuments are summarized for each step of thechains, and we expect the summaries for eachstep to be different (that is, each answer is indeedresponsive to the specific query it addresses).
Inaddition, each answer is produced in the contextof the previous steps, and only provides updated1 http://lucene.apache.org/solr/information with respect to previous answers.
Toensure that the dataset indeed reflects these twoaspects (responsiveness and freshness), we em-pirically verified that summaries created for ad-vanced queries are different from the summariescreated for the same queries by summarizers whodid not see the previous summaries in the chain.We asked from additional annotators to createmanual summaries of advanced queries from thequery chain without ever seeing the queries fromthe beginning of the chain.
For example, giventhe chain (asthma causes?
asthma allergy?asthma mold allergy), we asked summarizers toproduce an answer for the second query (asthmaallergy) without seeing the first step, on the sameinput documents.We used ROUGE to perform this validation:ROUGE compares a summary with a set of ref-erence summaries and source documents.
Wefirst computed the mean ROUGE score of thesecond query summaries.
The mean ROUGEscore is the mean score of each manual summaryvs.
all other summaries about the same query.We got (?1 = 0.52, ?2 = 0.22, ?
?4 = 0.13).
Themean ROUGE scores of the same second querysummaries by people who did not see the previ-ous query were markedly lower: ( ?1 =0.40, ?2 = 0.22, ?
?4 = 0.01).
We only verifiedthe asthma dataset in this manner.
The results,except for the R2 test, had statistically significantdifference with 95%  confidence interval.All the data, code and an annotated example canbe found in our site2.We show below slightly shortened manualsummaries created in answer to the followingquery chain: "obesity screening -> body massindex -> BMI Validity".
We manually annotatedthe summaries to highlight how the explorationprocess develops and indicate some of the infor-mation retrieval challenges faced when address-ing the task: underlined terms are directly rele-vant to the current query; bold italic terms havelikely triggered the curiosity of the reader and ledto asking the next query.
In this example, a userasks about obesity screening, and reformulateshis query using the technical term of BMI, whichis prevalent in the produced answer.Q1:  obesity screeningBody mass index (BMI) is a measure used to determine2 http://www.cs.bgu.ac.il/~nlpproj/QCFS/dataset.html917childhood overweight and obesity.
A child's weight statusis determined using an age- and sex-specific percentilefor BMI rather than the BMI categories used for adultsbecause children's body composition varies as they ageand varies between boys and girls.
For children and ado-lescents (aged 2-19 years): Overweight is defined asa BMI at or above the 85th percentile and lower than the95th percentile for children of the same age andsex.
Obesity is defined as a BMI at or above the 95thpercentile for children of the same age and sex.
BMI is nota perfect measure, but rather a helpful indicator.
It doesn'tconsider things like being muscular or having a larger thanaverage body frame and because growth patterns varygreatly among children.
As a result, doctors also factor achild's growth and development into considerationwhen screening for overweight and obesity.
Obesityscreening of adults using BMI may be less accurate foradults, due to the coarseness of the measurement: Heavilymuscled athletes may have a high BMI, as many womenwho enter menopause, who tend to gain weight due toincreased water retention.Q2: body mass indexThe body mass index (BMI), or Quetelet index, is a meas-ure for human body shape based on an individu-al's mass and height.
Devised in the mid-1800s by AdolpheQuetelet during the course of developing "social physics",it is defined as an individual's body mass divided by thesquare of their height, the units being kg/m^2.
BMI wasexplicitly cited as being appropriate for population studies,and inappropriate for individual diagnosis.
BMI provides asimple measure of a person's thickness, allowing healthprofessionals to discuss over-weight and underweightproblems more objectively with their patients.
Howev-er, BMI has become controversial because many people,including physicians, have come to rely on its appar-ent authority for medical diagnosis.
However, it was origi-nally meant to be used as a simple means of classifyingsedentary individuals, or rather, populations, with an aver-age body composition.
For these individuals, the currentvalue settings are as follows: (...).
Nick Korevaar (a mathe-matics lecturer from the University of Utah) suggests thatinstead of squaring the body height or cubingthe body height, it would be more appropriate to use anexponent of between 2.3 and 2.7 (as originally noted byQuetelet).Q3: BMI ValidityBMI has become controversial because many people, in-cluding physicians, have come to rely on its apparent nu-merical authority for medical diagnosis, but that was neverthe BMI's purpose; it is meant to be used as a simplemeans of classifying sedentary populations with an averagebody composition.
In an article published in the July edi-tion of 1972 of the Journal of Chronic Diseases, Ancel Keysexplicitly cited BMI as being appropriate for populationstudies, but inappropriate for individual diagnosis.
Theseranges of BMI values are valid only as statistical categoriesWhile BMI is a simple, inexpensive method of screening forweight categories, it is not a good diagnostic tool: It doesnot take into account age, gender, or muscle mass.
(...).Figure 2: Query Chain Summary Annotated Example5 AlgorithmsIn this section, we first explain how weadapted the previously mentioned methods to theQCFS task, thus producing 3 strong baselines.We then describe our new algorithm for QCFS.5.1 Focused KLSumWe adapted KLSum to QCFS by introducinga simple document selection step in the algo-rithm.
The method is: given a query step ?, wefirst select a focused subset of documents from?,?(?).
We then apply the usual KLSum algo-rithm over ?(?).
This approach does not makeany effort to reduce redundancy from step to stepin the query chain.
In our implementation, wecompute ?(?)
by selecting the top-10 documentsin ?
ranked by ??
?
???
scores to the query, asimplemented in SolR.5.2 KL-Chain-UpdateKL-Chain-Update is a slightly more sophisti-cated variation of KLSum that answers a querychain (instead a single query).
When construct-ing a summary, we update the unigram distribu-tion of the constructed summary so that it in-cludes a smoothed distribution of the previoussummaries in order to eliminate redundancy be-tween the successive steps in the chain.
For ex-ample, when we summarize the documents thatwere retrieved as a result to the first query, wecalculate the unigram distribution in the samemanner as we did in Focused KLSum; but for thesecond query, we calculate the unigram distribu-tion as if all the sentences we selected for theprevious summary were selected for the currentquery too, with a damping factor.
In this variant,the Unigram Distribution estimate of word X iscomputed as:918(Count(?,??????????)
+Count(?, ???????????)???????????????
)Length(??????????)
+Length(PreviousSum ?
??????????)??????????????
?5.3 ChainSumChainSum is our adaptation of TopicSum tothe QCFS task.
We developed a novel TopicModel to identify words that are associated to thecurrent query and not shared with the previousqueries.
We achieved this with the followingmodel.
For each query in a chain, we considerthe documents ?
?which are "good answers" tothe query; and ??
which are the documents usedto answer the previous steps of the chain.
Weassume in this model that these document subsetsare observable (in our implementation, we selectthese subsets by ranking the documents for thequery based on TFxIDF similarity).1. ?
is the general words topic, it is intendedto capture stop words and non-topic spe-cific vocabulary.
Its distribution ??
isdrawn for all the documents from?????????
(?, ??).2.
??
is the document specific topic; it repre-sents words which are local for a specificdocument.
???
is drawn for each docu-ment from ?????????
(?, ???).3.
?
is the new content topic, which shouldcapture words that are characteristic for??.
??
is drawn for all the documents in??
from ?????????
(?, ??).4.
?
captures old content from ??
, ??
isdrawn for all the documents in ??
from?????????
(?, ??).5.
?
captures redundant information between??
and ?
?, ??
is drawn for all the docu-ments in ??
?
??
from ?????????
(?, ??).6.
For documents from ??
we draw from thedistribution ?
?1  over topics (?, ?, ?, ??
)from a Dirichlet prior with pseudo-counts (10.0,15.0,15.0,1.0)3 .
For eachword in the document, we draw a topic ?from ?
?, and a word ?
from the topic in-dicated by ?.3 All pseudo-counts were selected empirically7.
For documents from ?
?, we draw from thedistribution ?
?2  over topics (?, ?, ?, ??
)from a Dirichlet prior with pseudo-counts  (10.0,15.0,15.0,1.0) .
The wordsare drawn in the same manner as in ?1.8.
For documents in ?
?
(??
?
??)
we drawfrom the distribution ?
?3  over topics(?, ??)
from a Dirichlet prior with pseudo-counts (10.0,1.0) .
The words are alsodrawn in the same manner as in ?1.The plate diagram of this generative model isshown in Fig.3.Figure 3 Plate Model for Our Topic ModelWe implemented inference over this topicmodel using Gibbs Sampling (we distribute thecode of the sampler together with our dataset).After the topic model is applied to the currentquery, we apply KLSum only on words that areassigned to the new content topic.
Fig.4 summa-rizes the algorithm data flow.When running this topic model on our dataset,we observe: ??
mean size was 978 words and375 unique words.
??
mean size was 1374words and 436 unique words.
??
and ??
meanon average 159 words.
These figures show thereis high lexical overlap between the summariesanswering query qi and qi+1 and highlight theneed to distinguish new and previously exposedcontent.In the ChainSum model, the topic R aims atmodeling redundant information between theprevious summaries and the new summary.
Weintend in the future to exploit this information toconstruct a contrastive model of content selec-tion.
In the current version, R does not play anactive role in content selection.
We, therefore,tested a variant of ChainSum that did not in-clude ??
and obtained results extremely similarto the full model, which we report below.919Figure 4 ChainSum Architecture5.4 Adapted LexRankIn LexRank, the algorithm creates a graphwhere nodes represent the sentences from thetext and weighted edges represent the cosine-distance of each sentence's TFxIDF vec-tors.
After creating the graph, PageRank is run torank sentences.
We adapted LexRank to QCFS intwo main ways: we extend the sentence represen-tation scheme to capture semantic informationand refine the model of sentences similarity sothat it captures query answering instead of cen-trality.
We tagged each sentence with Wikipediaterms using the Illinois Wikifier (Ratinov et al,2011) and with UMLS (Bodenreider, 2004)terms using HealthTermFinder (Lipsky-Gormanand Elhadad, 2011).
UMLS is a rich medical on-tology, which is appropriate to the consumerhealth domain.We changed the edges scoring formula to usethe sum of Lexical Semantic Similarity (LSS)functions (Li et al, 2007) on lexical terms, Wik-ipedia terms and UMLS terms:?????
(?, ?)
= ??????????
(?, ?)
+ ??
???????
(?, ?)
+ ??
???????
(?, ?)Where:???
(?1, ?2) =?
(????(???(??1,??2)???(??1,??1))???(??1))??
???(?
?1)?Instead of using the cosine distance, in order toincorporate advanced word/term similarity func-tions.
For lexical terms, we used the identityfunction, for Wikipedia term we used Wikiminer(Milne, 2007), and for UMLS we used TedPedersen UMLS similarity function (McInnes etal., 2009).
Finally, instead of PageRank, weused SimRank (Haveliwala, 2002) to identify thenodes most similar to the query node and notonly the central sentences in the graph.6 Evaluation6.1 Evaluation DatasetWe worked on the dataset we created forQCFS and added semantic tags: 10% of the to-kens had Wikipedia annotations and 33% had aUMLS annotation.6.2 ResultsFigure 5: ROUGE Recall Scores (with stemming andstop-words)For Focused KLSum we received ROUGEscores of (r1 = 0.281, r2 = 0.061, su4 = 0.100),KL-Chain-Update (r1 = 0.424, r2 = 0.149, su4 =0.193), ChainSum (r1 = 0.44988, r2 = 0.1587,su4 = 0.20594), ChainSum with t SimplifiedTopic model (r1 = 0.44992, r2 = 0.15814, su4 =0.20507) and for Modified-LexRank (r1 = 0.444,r2 = 0.151, su4 = 0.201).
All of the modified ver-sions of our algorithm performed better than Fo-cused KLSum with more than 95% confidence.7 ConclusionsWe presented a new summarization task tai-lored for the needs of exploratory search system.This task combines elements of question answer-ing by sentence extraction with those of updatesummarization.The main contribution of this paper is the def-inition of a new summarization task that corre-sponds to exploratory search behavior and thecontribution of a novel dataset containing humansummaries.
This dataset is annotated with Wik-ipedia and UMLS terms for over 30% of the to-kens.
We controlled that the summaries coveronly part of the input document sets (and are,therefore, properly focused) and sensitive to theposition of the queries in the chain.Four methods were evaluated for the task.
Thebaseline methods based on KL-Sum show a sig-00.5R1 R2 R3 R4 SU4Focused-KLSum KLSum-Update LexRank-UQC-LDA QC-simplified920nificant improvement when penalizing redun-dancy with the previous summarization.7KLV SDSHU FRQFHQWUDWHG RQ ?
]RRP LQ?
TXHU\FKDLQV RWKHU XVHU DFWLRQV VXFK DV ?
]RRP RXW?
RU?VZLWFK WRSLF?
ZHUH OHIW WR IXWXUH ZRUN This pa-SHU FRQFHQWUDWHG RQ ?
]RRP LQ?
TXHU\ FKDLQV RWKHU XVHU DFWLRQV VXFK DV ?
]RRP RXW?
RU ?VZLWFKWRSLF?
ZHUH OHIW WR IXWXUH ZRUN  The task remainsextremely challenging, and we hope the datasetavailability will allow further research to refineour understanding of topic-sensitive summariza-tion and redundancy control.In future work, we will attempt to derive atask-specific evaluation metric that exploits thestructure of the chains to better assess relevance,redundancy and contrast.AcknowledgmentsThis work was supported by the Israeli Minis-ter of Science (Grant #3-8705) and by the Lynnand William Frankel Center for Computer Sci-ences, Ben-Gurion University.
We thank thereviewers for extremely helpful advice.ReferencesMarcia J. Bates.
1989.
The design of browsing andberrypicking techniques for the online searchinterface, Online Information Review, 13(5), 407-424.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation, the Journal ofmachine Learning research, 3, 993-1022.Olivier Bodenreider.
2004.
The unified medicallanguage system (UMLS): integrating biomedicalterminology, Nucleic acids research, 32(suppl 1),D267-D270.John M. Conroy, Judith D. Schlesinger, and Dianne P.O'Leary.
2011.
Nouveau-rouge: A novelty metricfor update summarization, ComputationalLinguistics, 37(1), 1-8.Rebecca JW Cline, and Katie M. Haynes.
2001.Consumer health information seeking on theInternet: the state of the art, Health educationresearch, 16(6), 671-692.Daume Hal and Daniel Marcu.
2006.
Bayesian query-focused summarization, In Proceedings of the 21stInternational Conference on ComputationalLinguistics and the 44th annual meeting of theAssociation for Computational Linguistics (pp.305-312).
Association for ComputationalLinguistics.Jean-Yves Delort, and Enrique Alfonseca.
2012.DualSum: a Topic-Model based approach forupdate summarization, In Proceedings of the 13thConference of the European Chapter of theAssociation for Computational Linguistics (pp.214-223).
Association for ComputationalLinguistics.Rezarta Islamaj Dogan, G. Craig Murray, Aur?lieN?v?ol, and Zhiyong Lu.
2009.
UnderstandingPubMed?
user search behavior through loganalysis, Database: The Journal of BiologicalDatabases & Curation, 2009.G?nes Erkan, and Dragomir R. Radev.
2004.LexRank: Graph-based lexical centrality assalience in text summarization, J. Artif.
Intell.Res.
(JAIR), 22(1), 457-479.Surabhi Gupta, Ani Nenkova, and Dan Jurafsky.2007.
Measuring importance and query relevancein topic-focused multi-document summarization, InProceedings of the 45th Annual Meeting of theACL on Interactive Poster and DemonstrationSessions (pp.
193-196).
Association forComputational Linguistics.Aria Haghighi, and Lucy Vanderwende.
2009.Exploring content models for multi-documentsummarization, In Proceedings of HumanLanguage Technologies: The 2009 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics (pp.362-370).
Association for ComputationalLinguistics.Glen Jeh, and Jennifer Widom.
2002.
SimRank: ameasure of structural-context similarity, InProceedings of the eighth ACM SIGKDDinternational conference on Knowledge discoveryand data mining (pp.
538-543).
ACM.Baoli Li, Joseph Irwin, Ernest V. Garcia, and AshwinRam.
2007.
Machine learning based semanticinference: Experiments and Observations at RTE-3, In Proceedings of the ACL-PASCAL Workshopon Textual Entailment and Paraphrasing (pp.
159-164).
Association for Computational Linguistics.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries, In Text SummarizationBranches Out: Proceedings of the ACL-04Workshop (pp.
74-81).Sharon Lipsky-Gorman, and No?mie Elhadad 2011.ClinNote and HealthTermFinder: a pipeline for921processing clinical notes, Columbia UniversityTechnical Report, Columbia University.Gary Marchionini.
2006.
Exploratory search: fromfinding to understanding, Communications of theACM, 49(4), 41-46.Bridget T. McInnes, Ted Pedersen, and Serguei VSPakhomov.
(2009).
UMLS-Interface and UMLS-Similarity: open source software for measuringpaths and semantic similarity, AMIA AnnualSymposium Proceedings, American MedicalInformatics Association.David Milne.
2007.
Computing semantic relatednessusing wikipedia link structure, In Proceedings ofthe new zealand computer science research studentconference.Ani Nenkova, and Rebecca J. Passonneau.
2004.Evaluating Content Selection in Summarization:The Pyramid Method, In HLT-NAACL (pp.
145-152).Jahna Otterbacher, Gunes Erkan, and Dragomir R.Radev.
2009.
Biased LexRank: Passage retrievalusing random walks with question-based priors,Information Processing & Management, 45(1), 42-54.Lawrence Page, Sergey Brin, Rajeev Motwani, andTerry Winograd.
1999.
The PageRank citationranking: bringing order to the web,Lev Ratinov, Dan Roth, Doug Downey, and MikeAnderson.
2011.
Local and Global Algorithms forDisambiguation to Wikipedia, In ACL (Vol.
11,pp.
1375-1384).Ryen W. White, and Resa A. Roth.
2009.
Exploratorysearch: Beyond the query-response paradigm.Synthesis Lectures on Information Concepts,Retrieval, and Services, 1(1), 1-98.922
