Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 570?579,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPImproving Dependency Parsing with Subtrees from Auto-Parsed DataWenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto, and Kentaro TorisawaLanguage Infrastructure Group, MASTAR ProjectNational Institute of Information and Communications Technology3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289{chenwl, kazama, uchimoto, torisawa}@nict.go.jpAbstractThis paper presents a simple and effectiveapproach to improve dependency parsingby using subtrees from auto-parsed data.First, we use a baseline parser to parselarge-scale unannotated data.
Then we ex-tract subtrees from dependency parse treesin the auto-parsed data.
Finally, we con-struct new subtree-based features for pars-ing algorithms.
To demonstrate the ef-fectiveness of our proposed approach, wepresent the experimental results on the En-glish Penn Treebank and the Chinese PennTreebank.
These results show that our ap-proach significantly outperforms baselinesystems.
And, it achieves the best accu-racy for the Chinese data and an accuracywhich is competitive with the best knownsystems for the English data.1 IntroductionDependency parsing, which attempts to build de-pendency links between words in a sentence, hasexperienced a surge of interest in recent times,owing to its usefulness in such applications asmachine translation (Nakazawa et al, 2006) andquestion answering (Cui et al, 2005).
To ob-tain dependency parsers with high accuracy, super-vised techniques require a large amount of hand-annotated data.
While hand-annotated data arevery expensive, large-scale unannotated data canbe obtained easily.
Therefore, the use of large-scale unannotated data in training is an attractiveidea to improve dependency parsing performance.In this paper, we present an approach that ex-tracts subtrees from dependency trees in auto-parsed data to improve dependency parsing.
Theauto-parsed data are generated from large-scaleunannotated data by using a baseline parser.
Then,from dependency trees in the data, we extract dif-ferent types of subtrees.
Finally, we representsubtree-based features on training data to train de-pendency parsers.The use of auto-parsed data is not new.
How-ever, unlike most of the previous studies (Sagaeand Tsujii, 2007; Steedman et al, 2003) that im-proved the performance by using entire trees fromauto-parsed data, we exploit partial information(i.e., subtrees) in auto-parsed data.
In their ap-proaches, they used entire auto-parsed trees asnewly labeled data to train the parsing models,while we use subtree-based features and employthe original gold-standard data to train the mod-els.
The use of subtrees instead of complete treescan be justified by the fact that the accuracy of par-tial dependencies is much higher than that of en-tire dependency trees.
Previous studies (McDon-ald and Pereira, 2006; Yamada and Matsumoto,2003; Zhang and Clark, 2008) show that the accu-racies of complete trees are about 40% for Englishand about 35% for Chinese, while the accuraciesof relations between two words are much higher:about 90% for English and about 85% for Chinese.From these observations, we may conjecture thatit is possible to conduct a more effective selectionby using subtrees as the unit of information.The use of word pairs in auto-parsed data wastried in van Noord (2007) and Chen et al (2008).However, the information on word pairs is limited.To provide richer information, we consider morewords besides word pairs.
Specifically, we usesubtrees containing two or three words extractedfrom dependency trees in the auto-parsed data.
Todemonstrate the effectiveness of our proposed ap-proach, we present experimental results on En-570glish and Chinese data.
We show that this sim-ple approach greatly improves the accuracy andthat the use of richer structures (i.e, word triples)indeed gives additional improvement.
We alsodemonstrate that our approach and other improve-ment techniques (Koo et al, 2008; Nivre and Mc-Donald, 2008) are complementary and that we canachieve very high accuracies when we combineour method with other improvement techniques.Specifically, we achieve the best accuracy for theChinese data.The rest of this paper is as follows: Section 2introduces the background of dependency parsing.Section 3 proposes an approach for extracting sub-trees and represents the subtree-based features fordependency parsers.
Section 4 explains the ex-perimental results and Section 5 discusses relatedwork.
Finally, in section 6 we draw conclusions.2 Dependency parsingDependency parsing assigns head-dependent rela-tions between the words in a sentence.
A sim-ple example is shown in Figure 1, where an arcbetween two words indicates a dependency rela-tion between them.
For example, the arc between?ate?
and ?fish?
indicates that ?ate?
is the head of?fish?
and ?fish?
is the dependent.
The arc be-tween ?ROOT?
and ?ate?
indicates that ?ate?
is theROOT of the sentence.ROOT    I    ate    the    fish    with    a    fork    .Figure 1: Example for dependency structure2.1 Parsing approachFor dependency parsing, there ar two maintypes of parsing models (Nivre and McDonald,2008): graph-based model and transition-basedmodel, which achieved state-of-the-art accuracyfor a wide range of languages as shown in recentCoNLL shared tasks (Buchholz et al, 2006; Nivreet al, 2007).
Our subtree-based features can beapplied in both of the two parsing models.In this paper, as the base parsing system, weemploy the graph-based MST parsing model pro-posed by McDonald et al (2005) and McDonaldand Pereira (2006), which uses the idea of Max-imum Spanning Trees of a graph and large mar-gin structured learning algorithms.
The detailsof parsing model were presented in McDonald etal.
(2005) and McDonald and Pereira (2006).2.2 Baseline ParserIn the MST parsing model, there are two well-usedmodes: the first-order and the second-order.
Thefirst-order model uses first-order features that aredefined over single graph edges and the second-order model adds second-order features that aredefined on adjacent edges.For the parsing of unannotated data, we use thefirst-order MST parsing model, because we needto parse a large number of sentences and the parsermust be fast.
We call this parser the BaselineParser.3 Our approachIn this section, we describe our approach of ex-tracting subtrees from unannotated data.
First,we preprocess unannotated data using the BaselineParser and obtain auto-parsed data.
Subsequently,we extract the subtrees from dependency trees inthe auto-parsed data.
Finally, we generate subtree-based features for the parsing models.3.1 Subtrees extractionTo ease explanation, we transform the dependencystructure into a more tree-like structure as shownin Figure 2, the sentence is the same as the one inFigure 1.ateI                             fish      with                    .the                                       forkROOTaI       ate      the      fish      with      a      fork .Figure 2: Example for dependency structure intree-formatOur task is to extract subtrees from dependencytrees.
If a subtree contains two nodes, we call it abigram-subtree.
If a subtree contains three nodes,we call it a trigram-subtree.3.2 List of subtreesWe extract subtrees from dependency trees andstore them in list Lst.
First, we extract bigram-subtrees that contain two words.
If two words have571a dependency relation in a tree, we add these twowords as a subtree into list Lst.
Similarly, we canextract trigram-subtrees.
Note that the dependencydirection and the order of the words in the originalsentence are important in the extraction.
To enablethis, the subtrees are encoded in the string formatthat is expressed as st = w : wid : hid(?w :wid : hid)+1, where w refers to a word in thesubtree, wid refers to the ID (starting from 1) ofa word in the subtree (words are ordered accord-ing to the positions of the original sentence)2, andhid refers to an ID of the head of the word (hid=0means that this word is the root of a subtree).
Forexample, ?ate?
and ?fish?
have a right dependencyarc in the sentence shown in Figure 2.
So thesubtree is encoded as ?ate:1:0-fish:2:1?.
Figure 3shows all the subtrees extracted from the sentencein Figure 2, where the subtrees in (a) are bigram-subtrees and the ones in (b) are trigram-subtrees.ateI I:1:1-ate:2:0atefish ate:1:0-fish:2:1atefish  with ate:1:0-fish:2:1-with:3:1atewith ate:1:0-with:2:1ate.ate:1:0-.:2:1fishthe the:1:1-fish:2:0with fork with:1:0-fork:2:1forka a:1:1-fork:2:0atewith   .
ate:1:0-with:2:1-.:3:1(b)(a)Figure 3: Examples of subtreesNote that we only used the trigram-subtreescontaining a head, its dependent d1, and d1?sleftmost right sibling3.
We could not considerthe case where two children are on differentsides4of the head (for instance, ?I?
and ?fish?for ?ate?
in Figure 2).
We also do not use thechild-parent-grandparent type (grandparent-typein short) trigram-subtrees.
These are due to thelimitations of the parsing algorithm of (McDonaldand Pereira, 2006), which does not allow the fea-tures defined on those types of trigram-subtrees.We extract the subtrees from the auto-parseddata, then merge the same subtrees into one en-try, and count their frequency.
We eliminate allsubtrees that occur only once in the data.1+ refers to matching the preceding element one or moretimes and is the same as a regular expression in Perl.2So, wid is in fact redundant but we include it for ease ofunderstanding.3Note that the order of the siblings is based on the orderof the words in the original sentence.4Here, ?side?
means the position of a word relative to thehead in the original sentence.3.3 Subtree-based featuresWe represent new features based on the extractedsubtrees and call them subtree-based features.
Thefeatures based on bigram-subtrees correspond tothe first-order features in the MST parsing modeland those based on trigram-subtrees features cor-respond to the second-order features.We first group the extracted subtrees into dif-ferent sets based on their frequencies.
After ex-periments with many different threshold settingson development data sets, we chose the follow-ing way.
We group the subtrees into three setscorresponding to three levels of frequency: ?high-frequency (HF)?, ?middle-frequency (MF)?, and?low-frequency (LF)?.
HF, MF, and LF are usedas set IDs for the three sets.
The following are thesettings: if a subtree is one of the TOP-10% mostfrequent subtrees, it is in set HF; else if a subtree isone of the TOP-20% subtrees, it is in set MF; elseit is in set LF.
Note that we compute these levelswithin a set of subtrees with the same number ofnodes.
We store the set ID for every subtree inLst.
For example, if subtree ?ate:1:0-with:2:1?
isamong the TOP-10%, its set ID is HF.3.3.1 First-order subtree-based featuresThe first-order features are based on bigram-subtrees that are related to word pairs.
We gener-ate new features for a head h and a dependent d inthe parsing process.
Figure 4-(a)5shows the wordsand their surrounding words, where h?1refers tothe word to the left of the head in the sentence,h+1refers to the word to the right of the head, d?1refers to the word to the left of the dependent, andd+1refers to the word to the right of the depen-dent.
Temporary bigram-subtrees are formed byword pairs that are linked by dashed-lines in thefigure.
Then we retrieve these subtrees in Lsttoget their set IDs (if a subtree is not included inLst, its set ID is ZERO.
That is, we have four sets:HF, MF, LF, and ZERO.
).Then we generate first-order subtree-based fea-tures, consisting of indicator functions for set IDsof the retrieved bigram-subtrees.
When generatingsubtree-based features, each dashed line in Figure4-(a) triggers a different feature.To demonstrate how to generate first-ordersubtree-based features, we use an example that isas follows.
Suppose that we are going to parse thesentence ?He ate the cake with a fork.?
as shown5Please note that d could be before h.572?
h-1 h      h+1 ?
d-1     d      d+1  ?(a)(b)?
h      ?
d1 ?
d2 ?Figure 4: Word pairs and triple for feature repre-sentationin Figure 5, where h is ?ate?
and d is ?with?.We can generate the features for the pairs linkedby dashed-lines, such as h ?
d, h ?
d+1and soon.
Then we have the temporary bigram-subtrees?ate:1:0-with:2:1?
for h ?
d and ?ate:1:0-a:2:1?for h ?
d+1, and so on.
If we can find subtree?ate:1:0-with:2:1?
for h ?
d from Lstwith set IDHF, we generate the feature ?H-D:HF?, and if wefind subtree ?ate:1:0-a:2:1?
for h?d+1with set IDZERO, we generate the feature ?H-D+1:ZERO?.The other three features are also generated simi-larly.He    ate    the    cake    with    a    fork    .h-1 h       h+1 d-1 d      d+1Figure 5: First-order subtree-based features3.3.2 Second-order subtree-based featuresThe second-order features are based on trigram-subtrees that are related to triples of words.
Wegenerate features for a triple of a head h, its de-pendent d1, and d1?s right-leftmost sibling d2.The triple is shown in Figure 4-(b).
A temporarytrigram-subtree is formed by the word forms of h,d1, and d2.
Then we retrieve the subtree in Lsttoget its set ID.
In addition, we consider the triplesof ?h-NULL?6, d1, and d2, which means that weonly check the words of sibling nodes withoutchecking the head word.Then, we generate second-order subtree-basedfeatures, consisting of indicator functions for setIDs of the retrieved trigram-subtrees.6h-NULL is a dummy tokenWe also generate combined features involvingthe set IDs and part-of-speech tags of heads, andthe set IDs and word forms of heads.
Specifically,for any feature related to word form, we removethis feature if the word is not one of the Top-Nmost frequent words in the training data.
We usedN=1000 for the experiments in this paper.
Thismethod can reduce the size of the feature sets.In this paper, we only used bigram-subtrees andthe limited form of trigram-subtrees, though intheory we can use k-gram-subtrees, which are lim-ited in the same way as our trigram subtrees, in(k-1)th-order MST parsing models mentioned inMcDonald and Pereira (2006) or use grandparent-type trigram-subtrees in parsing models of Car-reras (2007).
Although the higher-order MSTparsing models will be slow with exact inference,requiring O(nk) time (McDonald and Pereira,2006), it might be possible to use higher-order k-gram subtrees with approximated parsing modelin the future.
Of course, our method can also beeasily extended to the labeled dependency case.4 ExperimentsIn order to evaluate the effectiveness of thesubtree-based features, we conducted experimentson English data and Chinese Data.For English, we used the Penn Treebank (Mar-cus et al, 1993) in our experiments and the tool?Penn2Malt?7to convert the data into dependencystructures using a standard set of head rules (Ya-mada and Matsumoto, 2003).
To match previ-ous work (McDonald et al, 2005; McDonald andPereira, 2006; Koo et al, 2008), we split the datainto a training set (sections 2-21), a developmentset (Section 22), and a test set (section 23).
Fol-lowing the work of Koo et al (2008), we usedthe MXPOST (Ratnaparkhi, 1996) tagger trainedon training data to provide part-of-speech tags forthe development and the test set, and we used 10-way jackknifing to generate tags for the trainingset.
For the unannotated data, we used the BLLIPcorpus (Charniak et al, 2000) that contains about43 million words of WSJ text.8We used the MX-POST tagger trained on training data to assignpart-of-speech tags and used the Basic Parser toprocess the sentences of the BLLIP corpus.For Chinese, we used the Chinese Treebank7http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html8We ensured that the text used for extracting subtrees didnot include the sentences of the Penn Treebank.573(CTB) version 4.09in the experiments.
We alsoused the ?Penn2Malt?
tool to convert the data andcreated a data split: files 1-270 and files 400-931for training, files 271-300 for testing, and files301-325 for development.
We used gold standardsegmentation and part-of-speech tags in the CTB.The data partition and part-of-speech settings werechosen to match previous work (Chen et al, 2008;Yu et al, 2008).
For the unannotated data, weused the PFR corpus10, which has approximately15 million words whose segmentation and POStags are given.
We used its original segmentationthough there are differences in segmentation pol-icy between CTB and this corpus.
As for POStags, we discarded the original POS tags and as-signed CTB style POS tags using a TNT-basedtagger (Brants, 2000) trained on the training data.We used the Basic Parser to process all the sen-tences of the PFR corpus.We measured the parser quality by the unla-beled attachment score (UAS), i.e., the percentageof tokens (excluding all punctuation tokens) withthe correct HEAD.
And we also evaluated on com-plete dependency analysis.4.1 Experimental ResultsIn our experiments, we used MSTParser, afreely available implementation11of the first- andsecond-order MST parsing models.
For baselinesystems, we used the first- and second-order basicfeatures, which were the same as the features usedby McDonald and Pereira (2006), and we usedthe default settings of MSTParser throughout thepaper: iters=10; training-k=1; decode-type=proj.We implemented our systems based on the MST-Parser by incorporating the subtree-based features.4.1.1 Main results of English dataEnglishUAS CompleteOrd1 90.95 37.45Ord1s 91.76(+0.81) 40.68Ord2 91.71 42.88Ord2s 92.51(+0.80) 46.19Ord2b 92.28(+0.57) 45.44Ord2t 92.06(+0.35) 42.96Table 1: Dependency parsing results for English9http://www.cis.upenn.edu/?chinese/.10http://www.icl.pku.edu.11http://mstparser.sourceforge.netThe results are shown in Table 1, whereOrd1/Ord2 refers to a first-/second-orderMSTParser with basic features, Ord1s/Ord2srefers to a first-/second-order MSTParser withbasic+subtree-based features, and the improve-ments by the subtree-based features over the basicfeatures are shown in parentheses.
Note thatwe use both the bigram- and trigram- subtreesin Ord2s.
The parsers using the subtree-basedfeatures consistently outperformed those usingthe basic features.
For the first-order parser,we found that there is an absolute improvementof 0.81 points (UAS) by adding subtree-basedfeatures.
For the second-order parser, we got anabsolute improvement of 0.8 points (UAS) byincluding subtree-based features.
The improve-ments of parsing with subtree-based features weresignificant in McNemar?s Test (p < 10?6).We also checked the sole effect of bigram- andtrigram-subtrees.
The results are also shown inTable 1, where Ord2b/Ord2t refers to a second-order MSTParser with bigram-/trigram-subtreesonly.
The results showed that trigram-subtrees canprovide further improvement, although the effectof the bigram-subtrees seemed larger.4.1.2 Comparative results of English dataTable 2 shows the performance of the systemsthat were compared, where Y&M2003 refers tothe parser of Yamada and Matsumoto (2003),CO2006 refers to the parser of Corston-Oliver etal.
(2006), Hall2006 refers to the parser of Hallet al (2006), Wang2007 refers to the parser ofWang et al (2007), Z&C 2008 refers to the combi-nation graph-based and transition-based system ofZhang and Clark (2008), KOO08-dep1c/KOO08-dep2c refers to a graph-based system with first-/second-order cluster-based features by Koo et al(2008), and Carreras2008 refers to the paper ofCarreras et al (2008).
The results showed thatOrd2s performed better than the first five systems.The second-order system of Koo et al (2008) per-formed better than our systems.
The reason maybe that the MSTParser only uses sibling interac-tions for second-order, while Koo et al (2008)uses both sibling and grandparent interactions, anduses cluster-based features.
Carreras et al (2008)reported a very high accuracy using information ofconstituent structure of the TAG grammar formal-ism.
In our systems, we did not use such knowl-edge.Our subtree-based features could be combined574with the techniques presented in other work,such as the cluster-based features in Koo et al(2008), the integrating methods of Zhang andClark (2008), and Nivre and McDonald (2008),and the parsing methods of Carreras et al (2008).EnglishUAS CompleteY&M2003 90.3 38.4CO2006 90.8 37.6Hall2006 89.4 36.4Wang2007 89.2 34.4Z&C2008 92.1 45.4KOO08-dep1c 92.23 ?KOO08-dep2c 93.16 ?Carreras2008 93.5 ?Ord1 90.95 37.45Ord1s 91.76 40.68Ord1c 91.88 40.71Ord1i 91.68 41.43Ord1sc 92.20 42.98Ord1sci 92.60 44.28Ord2 91.71 42.88Ord2s 92.51 46.19Ord2c 92.40 44.08Ord2i 92.12 44.37Ord2sc 92.70 46.56Ord2sci 93.16 47.15Table 2: Dependency parsing results for English,for our parsers and previous workTo demonstrate that our approach and otherwork are complementary, we thus implementeda system using all the techniques we had at handthat used subtree- and cluster-based featuresand applied the integrating method of Nivre andMcDonald (2008).
We used the word clusteringtool12, which was used by Koo et al (2008), toproduce word clusters on the BLLIP corpus.
Thecluster-based features were the same as the fea-tures used by Koo et al (2008).
For the integratingmethod, we used the transition MaxEnt-basedparser of Zhao and Kit (2008) because it wasfaster than the MaltParser.
The results are shownin the bottom part of Table 2, where Ord1c/Ord2crefers to a first-/second-order MSTParser withcluster-based features, Ord1i/Ordli refers to a first-/second-order MSTParser with integrating-basedfeatures, Ord1sc/Ord2sc refers to a first-/second-order MSTParser with subtree-based+cluster-based features, and Ord1sci/Ord2sci refers toa first-/second-order MSTParser with subtree-based+cluster-based+integrating-based features.Ord1c/Ord2c was worse than KOO08-dep1c/-dep2c, but Ord1sci outperformed KOO08-dep1c12http://www.cs.berkeley.edu/?pliang/software/brown-cluster-1.2.zipand Ord2sci performed similarly to KOO08-dep2cby using all of the techniques we had.
Theseresults indicated that subtree-based features canprovide different information and work well withother techniques.4.1.3 Main results of Chinese dataThe results are shown in Table 3 where abbrevia-tions are the same as in Table 1.
As in the Englishexperiments, parsers with the subtree-based fea-tures outperformed parsers with the basic features,and second-order parsers outperformed first-orderparsers.
For the first-order parser, the subtree-based features provided 1.3 absolute points im-provement.
For the second-order parser, thesubtree-based features achieved an absolute im-provement of 1.25 points.
The improvements ofparsing with subtree-based features were signifi-cant in McNemar?s Test (p < 10?5).ChineseUAS CompleteOrd1 86.38 40.80Ord1s 87.68(+1.30) 42.24Ord2 88.18 47.12Ord2s 89.43(+1.25) 47.53Ord2b 89.16(+0.98) 47.12Ord2t 88.55(+0.37) 47.12Table 3: Dependency parsing results for Chinese.4.1.4 Comparative results of Chinese dataTable 4 shows the comparative results, whereWang2007 refers to the parser of Wang etal.
(2007), Chen2008 refers to the parser of Chenet al (2008), and Yu2008 refers to the parser ofYu et al (2008) that is the best reported resultsfor this data set.
And ?all words?
refers to all thesentences in test set and ??
40 words?13refers tothe sentences with the length up to 40.
The tableshows that our parsers outperformed previous sys-tems.We also implemented integrating systems forChinese data as well.
When we applied thecluster-based features, the performance dropped alittle.
The reason may be that we are using gold-POS tags for Chinese data14.
Thus we did not13Wang et al (2007) and Chen et al (2008) reported thescores on these sentences.14We tried to use the cluster-based features for Chinesewith the same setting of POS tags as English data, then thecluster-based features did provide improvement.575use cluster-based features for the integrating sys-tems.
The results are shown in Table 4, whereOrd1si/Ord2si refers to the first-order/second-order system with subtree-based+intergrating-based features.
We found that the integrating sys-tems provided better results.
Overall, we haveachieved a high accuracy, which is the best knownresult for this dataset.Zhang and Clark (2008) and Duan et al (2007)reported results on a different data split of PennChinese Treebank.
We also ran our systems(Ord2s) on their data and provided UAS 86.70(for non-root words)/77.39 (for root words), betterthan their results: 86.21/76.26 in Zhang and Clark(2008) and 84.36/73.70 in Duan et al (2007).Chineseall words ?
40 wordsUAS Complete UAS CompleteWang2007 ?
?
86.6 28.4Chen2008 86.52 ?
88.4 ?Yu2008 87.26 ?
?
?Ord1s 87.68 42.24 91.11 54.40Ord1si 88.24 43.96 91.32 55.93Ord2s 89.43 47.53 91.67 59.77Ord2si 89.91 48.56 92.34 62.83Table 4: Dependency parsing results for Chinese,for our parsers and for previous work4.1.5 Effect of different sizes of unannotateddataHere, we consider the improvement relative to thesizes of the unannotated data.
Figure 6 shows theresults of first-order parsers with different num-bers of words in the unannotated data.
Please notethat the size of full English unannotated data is43M and the size of full Chinese unannotated datais 15M.
From the figure, we found that the parserobtained more benefits as we added more unanno-tated data.868788899091924332168420UASSize of unannotated data(M)EnglishChineseFigure 6: Results with different sizes of large-scale unannotated data.0 0.10.2 0.30.4 0.50.6 0.70.8 0.90  1  2  3  4  5  6Percentage(smoothed)Number of unknown wordsBetterNoChangeWorseFigure 7: Improvement relative to unknown wordsfor English00.10.20.30.40.50.60.70.80.90  1  2  3  4  5  6Percentage(smoothed)Number of unknown wordsBetterNoChangeWorseFigure 8: Improvement relative to unknown wordsfor Chinese4.2 Additional AnalysisIn this section, we investigated the results onsentence level from different views.
For Fig-ures 7-12, we classified each sentence into one ofthree classes: ?Better?
for those where the pro-posed parsers provided better results relative tothe parsers with basic features, ?Worse?
for thosewhere the proposed parsers provided worse resultsrelative to the basic parsers, and ?NoChange?
forthose where the accuracies remained the same.4.2.1 Unknown wordsHere, we consider the unknown word15problem,which is an important issue for parsing.
We cal-culated the number of unknown words in one sen-tence, and listed the changes of the sentences withunknown words.
Here, we compared the Ord1system and the Ord1s system.Figures 7 and 8 show the results, where the xaxis refers to the number of unknown words in onesentence and the y axis shows the percentages ofthe three classes.
For example, for the sentenceshaving three unknown words in the Chinese data,31.58% improved, 23.68% worsened, and 44.74%were unchanged.
We did not show the results of15An unknown word is a word that is not included in thetraining data.5760 0.10.2 0.30.4 0.50.6 0.70.8 0.943210Percentage(smoothed)Number of CCsBetterNoChangeWorseFigure 9: Improvement relative to number ofconjunctions for English0 0.10.2 0.30.4 0.50.6 0.70.8 0.93210Percentage(smoothed)Number of CCsBetterNoChangeWorseFigure 10: Improvement relative to number ofconjunctions for Chinesethe sentences with more than six unknown wordsbecause their numbers were very small.
The Bet-ter and Worse curves showed that our approach al-ways provided better results.
The results indicatedthat the improvements apparently became largerwhen the sentences had more unknown words forthe Chinese data.
And for the English data, thegraph also showed the similar trend, although theimprovements for the sentences have three andfour unknown words were slightly less than theothers.4.2.2 Coordinating conjunctionsWe analyzed our new parsers?
behavior for coordi-nating conjunction structures, which is a very dif-ficult problem for parsing (Kawahara and Kuro-hashi, 2008).
Here, we compared the Ord2 systemwith the Ord2s system.Figures 9 and 10 show how the subtree-basedfeatures affect accuracy as a function of the num-ber of conjunctions, where the x axis refers to thenumber of conjunctions in one sentence and they axis shows the percentages of the three classes.The figures indicated that the subtree-based fea-tures improved the coordinating conjunction prob-lem.
In the trigram-subtree list, many subtreesare related to coordinating conjunctions, such as?utilities:1:3 and:2:3 businesses:3:0?
and ?pull:1:0and:2:1 protect:3:1?.
These subtrees can provideadditional information for parsing models.4.2.3 PP attachmentWe analyzed our new parsers?
behavior forpreposition-phrase attachment, which is also a dif-ficult task for parsing (Ratnaparkhi et al, 1994).We compared the Ord2 system with the Ord2s sys-tem.
Figures 11 and 12 show how the subtree-based features affect accuracy as a function of thenumber of prepositions, where the x axis refers tothe number of prepositions in one sentence and they axis shows the percentages of the three classes.The figures indicated that the subtree-based fea-tures improved preposition-phrase attachment.5 Related workOur approach is to incorporate unannotated datainto parsing models for dependency parsing.
Sev-eral previous studies relevant to our approach havebeen conducted.Chen et al (2008) previously proposed an ap-proach that used the information on short de-pendency relations for Chinese dependency pars-ing.
They only used the word pairs within twoword distances for a transition-based parsing al-gorithm.
The approach in this paper differs inthat we use richer information on trigram-subtreesbesides bigram-subtrees that contain word pairs.And our work is focused on graph-based parsingmodels as opposed to transition-based models.
Yuet al (2008) constructed case structures from auto-parsed data and utilized them in parsing.
Com-pared with their method, our method is much sim-pler but has great effects.Koo et al (2008) used the Brown algorithm toproduce word clusters on large-scale unannotateddata and represented new features based on theclusters for parsing models.
The cluster-based fea-tures provided very impressive results.
In addition,they used the parsing model by Carreras (2007)that applied second-order features on both siblingand grandparent interactions.
Note that our ap-proach and their approach are complementary inthat we can use both subtree- and cluster-basedfeatures for parsing models.
The experimental re-sults showed that we achieved better accuracy forfirst-order models when we used both of these twotypes of features.Sagae and Tsujii (2007) presented an co-training approach for dependency parsing adap-5770 0.10.2 0.30.4 0.50.6 0.70.8 0.90  1  2  3  4  5  6  7Percentage(smoothed)Number of prepositionsBetterNoChangeWorseFigure 11: Improvement relative to number ofprepositions for English0 0.10.2 0.30.4 0.50.6 0.70.8 0.93210Percentage(smoothed)Number of prepositionsBetterNoChangeWorseFigure 12: Improvement relative to number ofprepositions for Chinesetation.
They used two parsers to parse the sen-tences in unannotated data and selected only iden-tical results produced by the two parsers.
Then,they retrained a parser on newly parsed sentencesand the original labeled data.
Our approach repre-sents subtree-based features on the original gold-standard data to retrain parsers.
McClosky etal.
(2006) presented a self-training approach forphrase structure parsing and the approach wasshown to be effective in practice.
However,their approach depends on a high-quality reranker,while we simply augment the features of an ex-isting parser.
Moreover, we could use the outputof our systems for co-training/self-training tech-niques.6 ConclusionsWe present a simple and effective approach toimprove dependency parsing using subtrees fromauto-parsed data.
In our method, first we use abaseline parser to parse large-scale unannotateddata, and then we extract subtrees from depen-dency parsing trees in the auto-parsed data.
Fi-nally, we construct new subtree-based features forparsing models.
The results show that our ap-proach significantly outperforms baseline systems.We also show that our approach and other tech-niques are complementary, and then achieve thebest reported accuracy for the Chinese data and anaccuracy that is competitive with the best knownsystems for the English data.ReferencesT.
Brants.
2000.
TnT?a statistical part-of-speech tag-ger.
Proceedings of ANLP, pages 224?231.S.
Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.2006.
CoNLL-X shared task on multilingual depen-dency parsing.
Proceedings of CoNLL-X.Xavier Carreras, Michael Collins, and Terry Koo.2008.
Tag, dynamic programming, and the percep-tron for efficient, feature-rich parsing.
In Proceed-ings of CoNLL 2008, pages 9?16, Manchester, Eng-land, August.
Coling 2008 Organizing Committee.X.
Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In Proceedings ofthe CoNLL Shared Task Session of EMNLP-CoNLL2007, pages 957?961.E.
Charniak, D. Blaheta, N. Ge, K. Hall, J. Hale, andM.
Johnson.
2000.
BLLIP 1987-89 WSJ CorpusRelease 1, LDC2000T43.
Linguistic Data Consor-tium.WL.
Chen, D. Kawahara, K. Uchimoto, YJ.
Zhang, andH.
Isahara.
2008.
Dependency parsing with shortdependency relations in unlabeled data.
In Proceed-ings of IJCNLP 2008.S.
Corston-Oliver, A. Aue, Kevin.
Duh, and Eric Ring-ger.
2006.
Multilingual dependency parsing usingbayes point machines.
In HLT-NAACL2006.H.
Cui, RX.
Sun, KY. Li, MY.
Kan, and TS.
Chua.2005.
Question answering passage retrieval us-ing dependency relations.
In Proceedings of SIGIR2005, pages 400?407, New York, NY, USA.
ACM.Xiangyu Duan, Jun Zhao, and Bo Xu.
2007.
Proba-bilistic models for action-based chinese dependencyparsing.
In Proceedings of ECML/ECPPKDD, War-saw, Poland.Johan Hall, Joakim Nivre, and Jens Nilsson.
2006.Discriminative classifiers for deterministic depen-dency parsing.
In In Proceedings of CoLING-ACL.D.
Kawahara and S. Kurohashi.
2008.
Coordinationdisambiguation without any similarities.
In Pro-ceedings of Coling 2008, pages 425?432, Manch-ester, UK, August.T.
Koo, X. Carreras, and M. Collins.
2008.
Simplesemi-supervised dependency parsing.
In Proceed-ings of ACL-08: HLT, Columbus, Ohio, June.M.
Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: the Penn Treebank.
Computational Linguis-ticss, 19(2):313?330.578D.
McClosky, E. Charniak, and M. Johnson.
2006.Reranking and self-training for parser adaptation.
InProceedings of Coling-ACL, pages 337?344.R.
McDonald and F. Pereira.
2006.
Online learningof approximate dependency parsing algorithms.
InProc.
of EACL2006.R.
McDonald, K. Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
InProc.
of ACL 2005.T.
Nakazawa, K. Yu, D. Kawahara, and S. Kurohashi.2006.
Example-based machine translation based ondeeper nlp.
In Proceedings of IWSLT 2006, pages64?70, Kyoto, Japan.J.
Nivre and R. McDonald.
2008.
Integrating graph-based and transition-based dependency parsers.
InProceedings of ACL-08: HLT, Columbus, Ohio,June.J.
Nivre, J.
Hall, S. K?ubler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The CoNLL 2007shared task on dependency parsing.
In Proceed-ings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915?932.A.
Ratnaparkhi, J. Reynar, and S. Roukos.
1994.
Amaximum entropy model for prepositional phrase at-tachment.
In Proceedings of HLT, pages 250?255.A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proceedings of EMNLP,pages 133?142.K.
Sagae and J. Tsujii.
2007.
Dependency parsing anddomain adaptation with LR models and parser en-sembles.
In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL 2007, pages 1044?1050.M.
Steedman, M. Osborne, A. Sarkar, S. Clark,R.
Hwa, J. Hockenmaier, P. Ruhlen, S. Baker, andJ.
Crim.
2003.
Bootstrapping statistical parsersfrom small datasets.
In Proceedings of EACL 2003,pages 331?338.Gertjan van Noord.
2007.
Using self-trained bilexicalpreferences to improve disambiguation accuracy.
InProceedings of IWPT-07, June.Qin Iris Wang, Dekang Lin, and Dale Schuurmans.2007.
Simple training of dependency parsers viastructured boosting.
In Proceedings of IJCAI2007.H.
Yamada and Y. Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.
InProceedings of IWPT2003, pages 195?206.K.
Yu, D. Kawahara, and S. Kurohashi.
2008.
Chi-nese dependency parsing with large scale automat-ically constructed case structures.
In Proceedingsof Coling 2008, pages 1049?1056, Manchester, UK,August.Y.
Zhang and S. Clark.
2008.
A tale of twoparsers: Investigating and combining graph-basedand transition-based dependency parsing.
In Pro-ceedings of EMNLP 2008, pages 562?571, Hon-olulu, Hawaii, October.H.
Zhao and CY.
Kit.
2008.
Parsing syntactic andsemantic dependencies with two single-stage max-imum entropy models.
In Proceedings of CoNLL2008, pages 203?207, Manchester, England, Au-gust.579
