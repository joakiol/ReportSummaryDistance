Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 69?75,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsZoom: a corpus of natural language descriptions of map locationsRomina AltamiranoFaMAFNat.Univ.
of C?ordobaC?ordoba, Argentinaialtamir@famaf.unc.edu.arThiago C. FerreiraEACH-USPUniv.
of S?ao PauloS?ao Paulo, Brazilthiago.castro.ferreira@usp.brIvandr?e ParaboniEACH-USPUniv.
of S?ao PauloS?ao Paulo, Brazilivandre@usp.brLuciana BenottiFaMAFNat.Univ.
of C?ordobaC?ordoba, Argentinabenotti@famaf.unc.edu.arAbstractThis paper describes an experiment toelicit referring expressions from humansubjects for research in natural languagegeneration and related fields, and prelim-inary results of a computational model forthe generation of these expressions.
Un-like existing resources of this kind, the re-sulting data set - the Zoom corpus of natu-ral language descriptions of map locations- takes into account a domain that is sig-nificantly closer to real-world applicationsthan what has been considered in previouswork, and addresses more complex situa-tions of reference, including contexts withdifferent levels of detail, and instances ofsingular and plural reference produced byspeakers of Spanish and Portuguese.1 IntroductionReferring Expression Generation (REG) is thecomputational task of producing adequate naturallanguage descriptions (e.g., pronouns, definite de-scriptions, proper names, etc.)
of domain entities.In particular, the issue of how to determine the se-mantic contents of definite descriptions (e.g., ?theIndian restaurant on 5th street?, ?the restaurant wewent to last night?, etc.)
has received significantattention in the field, and it is also the focus of thepresent work.The input to a REG algorithm is a context setC containing an intended referent r and a numberof distractor objects.
All objects are representedas attribute-value pairs representing either atomic(type-restaurant) or relational (on-5thstreet) prop-erties (Krahmer and Theune, 2002; Krahmer et al,2003; Kelleher and Kruijff, 2006; Viethen et al,2013).
The expected output is a uniquely identi-fying list L of properties known to be true of rso that L distinguishes r from all distractors in C(Dale and Reiter, 1995).Properties are selected for inclusion in L ac-cording to multiple - and often conflicting - cri-teria, including discriminatory power (i.e., theability to rule out distractors) as in (Dale, 2002;Gardent, 2002), domain preferences (Pechmann,1989; Gatt et al, 2013) and many others.
A de-scription that conveys more information than whatis strictly required for disambiguation is said tobe overspecified (Arts et al, 2011; Koolen et al,2011; van Gompel et al, 2012; Engelhardt andFerreira, 2006; Engelhardt et al, 2011).
For areview of the research challenges in REG, see(Krahmer and van Deemter, 2012).Existing approaches to REG largely consist ofalgorithmic solutions, many of which have beeninfluenced by, or adapted from, the Dale & ReiterIncremental algorithm in (Dale and Reiter, 1995).The use of machine learning (ML) techniques, bycontrast, seems to be less frequent than in otherNLG tasks, although a number of exceptions doexist (e.g., (Jordan and Walker, 2005; Viethen andDale, 2010; Viethen, 2011; Garoufi and Koller,2013; Ferreira and Paraboni, 2014)).A possible explanation for the small interest inML for REG may be the relatively low availabil-ity of data.
While research in many fields maybenefit from the wide availability of text corpora(e.g., obtainable from the web), research in REGusually requires highly specialised data - herebycalled REG corpora - conveying not only refer-ring expressions produced by human speakers, butalso a fully-annotated representation of the con-text (i.e., all objects and their semantic properties)within which the expressions have been produced.REG corpora such as TUNA (Gatt et al, 2007)and GRE3D3 (Dale and Viethen, 2009) are usefulboth to gain general insights on human languageproduction, and to benefit from data-intensivecomputational techniques such as ML.
However,being usually the final product of controlled ex-periments involving human subjects, REG cor-69pora tend to address highly specific research ques-tions.
For instance, GRE3D3 is largely devotedto the investigation of relational referring expres-sions (Kelleher and Kruijff, 2006) in simple visualscenes involving geometric shapes, as in ?the largeball next to the red cube?.
As a result, and despitethe usefulness of these resources to a large body ofwork in REG, further research questions will usu-ally require the collection of new data.In this paper we introduce the Zoom corpus ofreferring expressions.
Zoom addresses a domainthat is considerably closer to real-world applica-tions (namely, city maps in different degrees ofdetail represented by zoom levels) than what hasbeen considered in previous work, involving bothsingular and plural reference, and making exten-sive use of relational properties.
Moreover, Zoomdescriptions were produced by both Spanish andPortuguese speakers, which will allow (to the bestof our knowledge, for the first time) a comprehen-sive study of the REG surface realisation subtaskin these languages, and enable research on the is-sues of human variation in REG (Fabbrizio et al,2008; Altamirano et al, 2012; Gatt et al, 2011).2 Related workTUNA (Gatt et al, 2007) was the first promi-nent REG corpus to be made publicly availablefor research purposes.
The corpus was developedin a series of controlled experiments, containing2280 atomic descriptions produced by 60 speakersof English in two domains (1200 descriptions offurniture items and 1080 descriptions of people?sphotographs).
TUNA has been used in a series ofREG shared tasks (Gatt et al, 2009).GRE3D3 and its extension GRE3D7 (Dale andViethen, 2009; Viethen and Dale, 2011) were de-veloped in a series of web-based experiments pri-marily focussed on the study of relational descrip-tions.
GRE3D3 contains 630 descriptions pro-duced by 63 speakers, and GRE3D7 contains 4480descriptions produced by 287 speakers.
In bothcases, the language of the experiment was English.The domain consists of simple visual scenes con-veying boxes and spheres.Stars (Teixeira et al, 2014) and its extensionStars2 were collected for the study of referentialoverspecification.
Stars contains 704 descriptionsproduced by 64 speakers in a web-based exper-iment.
Stars2 was produced in dialogue situa-tions involving subject pairs, and it contains 884descriptions produced by 56 speakers.
Both do-mains make use of simple visual scenes containingup to four object types (e.g., stars, boxes, conesand spheres) and include atomic and relational de-scriptions alike.
The language of both experimentswas Brazilian Portuguese.3 ExperimentWe designed a web-based experiment to collectnatural language descriptions of map locations inboth Spanish and Portuguese.
The collected dataset comprises a corpus of referring expressions forresearch in REG and related fields.
The situationsof reference under consideration make use of mapscenes in two degrees of detail (represented by lowand high zoom levels), and address instances ofsingular and plural reference.
A fragment of theexperiment interface is shown in Fig.
1.Figure 1: Experiment interface3.1 SubjectsVolunteers were recruited upon invitation sent byemail.
The Portuguese data had 93 participants,being 66 (71.0%) male and 27 (29.0%) female.The Spanish data had 80 participants, being 59male (69.4%) and 26 female (30.6%).3.2 ProcedureSubjects received a web link to the on-line experi-ment interface (cf.
Fig.
1) with self-contained in-structions.
Age and gender details were collectedfor statistical purposes.
The experiment consistedof a series of map images presented in random or-der, one by one.
Each map scene showed a partic-ular location (e.g., a restaurant, pub, theatre etc.
)pointed by an arrow.
For each scene, subjects wererequired to imagine that they were giving traveladvice to a friend, and to complete the sentence ?It70would be interesting to visit...?
with a descriptionof the location pointed by the arrow.
After press-ing a ?Next?
button, another stimulus was selected,until the end of the experiment.
The first two im-ages were fillers solely intended to make subjectsfamiliar with the experiment setting, and the cor-responding responses were not recorded.
Incom-plete trials, and ill-formed descriptions, were alsodiscarded.3.3 MaterialsThe experiment made use of the purpose-built in-terface illustrated in Fig.
1, and a set of map im-ages obtained from OpenStreetMap1, which con-sisted of selected portions of maps of Madrid andLisbon to be presented to Spanish and Portuguesespeakers, respectively.
For each city, 10 map lo-cations were used.
Each location was shown inlow and high zoom levels, making 20 images intotal.
In both cases, the intended target was keptthe same, but the more detailed version would dis-play a larger number of distractors and additionaldetails in general.
In addition to that, certain streetand landmark names might not be depicted at dif-ferent zoom levels.
Half images showed a singlearrow pointing to one map location (i.e., requir-ing a single description as ?the restaurant on Bakerstreet?
), whereas the other half showed two arrowspointed to two different locations (and hence re-quiring a reference to a set, as in ?the two restau-rants near the museum?
).3.4 Data collectionUpon manual verification, 602 ill-formed Por-tuguese descriptions and 366 Spanish descriptionswere discarded.
Thus, the Portuguese subcor-pus consists of 1358 descriptions, and the Span-ish subcorpus consists of 1234 descriptions.
Inthe Portuguese subcorpus, 78.6% of the descrip-tions include relational properties.
In addition tothat, 36.4% were minimally distinguishing, 44.3%were overspecified, and 19.3% were underspeci-fied.
In the Spanish subcorpus, 70% of the de-scriptions include relational properties, 35% wereminimally distinguishing, 40% were overspeci-fied, and 25% were underspecified.
Underspeci-fied descriptions are not common in existing REGcorpora (i.e., certainly not in this proportion),which may reflect the complexity of the domainand/or limitations of the web-based setting.1openstreetmap.org3.5 AnnotationEach referring expression was modelled as con-veying a description of the main target object and,optionally, up to four descriptions of related land-marks.
The annotation scheme consisted of threetarget attributes, four landmark attributes for eachof the four possible landmark objects, and sevenrelational properties.
This makes 26 possible at-tributes for each referring expression.
In the caseof plural descriptions (i.e., those involving two tar-get objects), this attribute set is doubled.Every object was annotated with the atomic at-tributes type, name and others and, in the case oflandmark objects, also with their id.
In additionto that, seven relational properties were consid-ered: in/on/at2, next-to, right-of, left-of, in-front-of, behind-of, and the multivalue relation betweenintended to represent ?corner?
relations.Possible values for the type and name attributesare predefined by each referential context.
Theothers attribute may be assigned any string value,and it is intended to represent any non-standardpiece of information conveyed by the expression.For the spatial relations, possible values are theobject identifiers available from each scene.The collected descriptions were fully annotatedby two independent annotators.
After completion,a third annotator assumed the role of judge andprovided the final annotation.
Since the annotationscheme was fairly straightforward (i.e., largely be-cause all non-standard responses were simply as-signed to the others attribute), agreement betweenjudges as measured by Kappa (Cohen, 1960) was84% at the attribute level.
Both referential con-texts and referring expressions were representedin XML format using a relational version of theformat adopted in TUNA (Gatt et al, 2007).3.6 Comparison with previous workTable 1 presents a comparison between the col-lected data and existing REG corpora3: the num-ber of referring expressions (REs), the number ofsubjects in each experiment, the number of possi-ble atomic attributes (Attrib.)
and possible land-marks (LMs) in a description, the average descrip-tion size (in number of annotated properties), andthe proportion of property usage, which is taken to2The three prepositions were aggregated as a single at-tribute because they have approximately the same meaningin the languages under consideration3The information on TUNA and Zoom descriptions isbased on the singular portion of each corpus only71be the proportion of properties that appear in thedescription over the total number of possible at-tributes and landmarks.
From a REG perspective,larger description sizes and lower usage rates maysuggest more complex situations of reference.Table 1: Comparison with existing REG corporaCorpus REs Subj.
Attrib.
LMs Avg.size UsageTUNA-F 1200 60 4 0 3.1 0.8TUNA-P 1080 60 10 0 3.1 0.3GRE3D3 630 63 9 1 3.4 0.3GRE3D7 4480 287 6 1 3.0 0.4Stars 704 64 8 2 4.4 0.4Stars2 884 56 9 2 3.3 0.3Zoom-Pt 1358 93 19 4 6.7 0.3Zoom-Sp 1234 80 19 4 7.2 0.34 REG evaluationIn what follows we illustrate the use of the Zoomcorpus as training and test data for a simple ma-chine learning approach to REG adapted from(Ferreira and Paraboni, 2014).
The goal of thisevaluation is to provide reference results for futurecomparison with purpose-built REG algorithms,and not to present a complete REG solution forthe Zoom domain or others.The present model consists of 12 binary clas-sifiers representing whether individual referentialattributes should be selected for inclusion in anoutput description.
The classifiers correspond toatomic attributes of the target and first landmarkobject (type, name and others), and relations.
Ref-erential attributes of other landmark objects werenot modelled due to data sparsity and also to re-duce computational costs.
For similar reasons, themultivalue between relation is also presently disre-garded, and ?corner?
relations involving two land-marks (e.g., two streets) will be modelled as twoindependent classification tasks.Only two learning features are considered byeach classifier: landmarkCount, which representsthe number of landmark objects near the maintarget, and distractorCount, which represents thenumber of objects of the same type as the targetwithin the relevant context in the map.
For otherpossible features applicable to this task, see, forinstance, (dos Santos Silva and Paraboni, 2015).From the outcome of the 12 binary classifiers,a description is built by considering atomic targetattributes in the first place.
All attributes that cor-respond to a positive prediction are selected for in-clusion in the output description.
Next, relationsare considered.
If no relation is predicted, thealgorithm terminates by returning an atomic de-scription of the main target object.
If the descrip-tion includes a relation, the corresponding land-mark object is selected, and the algorithm is calledrecursively to describe it as well.
Since every at-tribute that corresponds to a positive predictionis always selected, the algorithm does not regarduniqueness as a stop condition.
As a result, theoutput description may convey a certain amountof overspecification.For evaluation purposes, we used the subset ofsingular descriptions from the Portuguese portionof the corpus, comprising 821 descriptions.
Evalu-ation was carried out by comparing the corpus de-scription with the system output to measure over-all accuracy (i.e., the number of exact matches be-tween the two descriptions), Dice (Dice, 1945) andMASI (Passonneau, 2006) coefficients.Following (Ferreira and Paraboni, 2014), webuilt a REG model using support vector machineswith radial basis function kernel.
The classifierswere trained and tested using 6-fold cross valida-tion.
Optimal parameters were selected using gridsearch as follows: for each step in the main k-foldvalidation, one fold was reserved for testing, andthe remaining k ?
1 folds were subject to a sec-ondary cross-validation procedure in which differ-ent parameter combinations were attempted.
TheC parameter was assigned the values 1, 10, 100and 1000, and ?
was assigned 1, 0.1, 0.001 and0.0001.
The best-performing parameter set wasselected to build a classifier trained from the k?
1fold, and tested on the test data.
This was repeatedfor every iteration of the main cross-validationprocedure.Table 2 summarises the results obtained by theREG algorithm built from SVM classifiers, thoseobtained by a baseline system representing a rela-tional extension of the Dale & Reiter IncrementalAlgorithm, and by a Random selection strategy.Table 2: REG resultsAlgorithm Acc.
Dice MASISVM 0.15 0.51 0.28Incremental 0.04 0.53 0.21Random selection 0.03 0.45 0.15We compare accuracy scores obtained by ev-ery algorithm pair using the chi-square test, andwe compare Dice scores using Wilcoxon?s signed-rank test.
In terms of overall accuracy, the SVM72approach outperforms both alternatives.
The dif-ference from the second best-performing algo-rithm (i.e., the Incremental approach) is significant(?2= 79.87, df=1, p<0.0001).
Only in terms ofDice scores a small effect in the opposite directionis observed (T=137570.5, p= 0.01413).We also assessed the performance of the indi-vidual classifiers.
Table 3 shows these results asmeasured by precision (P), recall (R), F1-measure(F1) and area under the ROC curve (AUC).Table 3: Classifier resultsClassifier P R F1AUCtg type 0.95 1.00 0.98 0.25tg name 0.09 0.05 0.07 0.41tg other 0.00 0.00 0.00 0.05lm type 0.93 1.00 0.96 0.44lm name 0.97 1.00 0.98 0.35lm other 0.00 0.00 0.00 0.43next-to 0.50 0.24 0.32 0.63right-of 0.00 0.00 0.00 0.28left-of 0.00 0.00 0.00 0.27in-front-of 0.00 0.00 0.00 0.42behind-of 0.00 0.00 0.00 0.17in/on/at 0.60 0.60 0.60 0.61From these results we notice that highly fre-quent attributes (e.g., target type and landmarkname) were classified with high accuracy, whereasothers (e.g., multivalue attributes and relations)were not.5 DiscussionThis paper has introduced the Zoom corpus of nat-ural language descriptions of map locations, a re-source intended to support future research in REGand related fields.
Preliminary results of a SVM-based approach to REG - which were solely pre-sented for the future assessment of REG algo-rithms based on Zoom data - hint at the actualcomplexity of the REG task in this domain in anumber of ways.
First, we notice that a simi-lar approach in (Ferreira and Paraboni, 2014) onGRE3D3 and GRE3D7 data has obtained consid-erably higher mean accuracy.
This is partially ex-plained by the increased complexity of the Zoomdomain, but also by the currently simple annota-tion scheme.Second, we notice that Zoom descriptions areprone to convey relations between a single targetand multiple landmark objects, as in ?the restau-rant between the 5th and 6th streets?.
Althoughcommon in language use, the use of multiple rela-tional properties in this way has been little investi-gated in the REG field.Finally, we notice that the Zoom domain con-tains two descriptions for every target object,which are based on different - but related - mod-els corresponding to the same map location seenat different zoom levels.
Interestingly, the refer-ring expression in a 1X situation may or may notbe the same as in a 2X situation.
Consider a mapwith higher zoom level (2X) as illustrated in theprevious Fig.
2, and the same map location as seenwith lower zoom level in the previous Fig.
1.Figure 2: Map with a more detailed zoom levelThe underlying models for these two maps arecertainly different, but not unrelated.
The mapwith 2X zoom contains fewer objects but may in-clude more properties due to the added level of de-tail.
The referring expression for the target in the1X map may or may not be the same as in the 2Xmap.
For instance, the referring expression ?thepub at Cowgate?
is underspecified on the 1X map,but it is minimally distinguishing on the 2X map.Differences of this kind are common in inter-active applications (e.g., in which the context ofreference may change in structure or in the num-ber of objects and referable properties), and thechallenge for REG algorithms would be to pro-duced an appropriate description for the modifiedcontext without starting from scratch.
REG algo-rithms based on local context partitioning (Areceset al, 2008) may have an advantage in this respect,but further investigation is still required.AcknowledgementsThis work has been supported by FAPESP andCAPES.
The authors are also grateful to the an-notators Adriano S. R. da Silva, Jefferson S. F. daSilva and Alan K. Yamasaki.73ReferencesRomina Altamirano, Carlos Areces, and LucianaBenotti.
2012.
Probabilistic refinement algorithmsfor the generation of referring expressions.
In COL-ING (Posters), pages 53?62.Carlos Areces, Alexander Koller, and Kristina Strieg-nitz.
2008.
Referring expressions as formulas ofdescription logic.
In Proceedings of the Fifth Inter-national Natural Language Generation Conference,INLG ?08, pages 42?49, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.A.
Arts, A. Maes, L. G. M. Noordman, and C. Jansen.2011.
Overspecification facilitates object identifica-tion.
Journal of Pragmatics, 43(1):361?374.J.
Cohen.
1960.
A coeficient of agreement for nom-inal scales.
Educational and Psychological Mea-surement, 20(1):37?46.Robert Dale and Ehud Reiter.
1995.
Computationalinterpretations of the Gricean maxims in the gener-ation of referring expressions.
Cognitive Science,19(2):233?263.Robert Dale and Jette Viethen.
2009.
Referring ex-pression generation through attribute-based heuris-tics.
In Proceedings of ENLG-2009, pages 58?65.Robert Dale.
2002.
Cooking up referring expressions.In Proceedings of the 27th Annual Meeting of the As-sociation for Computational Linguistics, pages 68?75.L.
R. Dice.
1945.
Measures of the amount of ecologicassociation between species.
Ecology, 26(3):297?302.Diego dos Santos Silva and Ivandr?e Paraboni.
2015.Generating spatial referring expressions in interac-tive 3D worlds.
Spatial Cognition and Computation.P.
E. Engelhardt and K. Baileyand F. Ferreira.
2006.Do speakers and listeners observe the Griceanmaxim of quantity?
Journal of Memory and Lan-guage, 54(4):554?573.P.
E. Engelhardt, S. B. Demiral, and Fernanda Ferreira.2011.
Over-specified referring expressions impaircomprehension: An ERP study.
Brain and Cogni-tion, 77(2):304?314.Giuseppe Di Fabbrizio, Amanda J. Stent, and Srini-vas Bangalore.
2008.
Trainable speaker-based re-ferring expression generation.
In Proceedings ofthe Twelfth Conference on Computational NaturalLanguage Learning, CoNLL ?08, pages 151?158,Stroudsburg, PA, USA.Thiago Castro Ferreira and Ivandr?e Paraboni.
2014.Classification-based referring expression genera-tion.
Lecture Notes in Computer Science, 8403:481?491.C.
Gardent.
2002.
Generating minimal definite de-scriptions.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguis-tics, pages 96?103.Konstantina Garoufi and Alexander Koller.
2013.Generation of effective referring expressions in sit-uated context.
Language and Cognitive Processes,29(8):986?1001.Albert Gatt, Ilka van der Sluis, and Kees van Deemter.2007.
Evaluating algorithms for the generation ofreferring expressions using a balanced corpus.
InProceedings of ENLG-07.Albert Gatt, Anja Belz, and Eric Kow.
2009.
TheTUNA challenge 2009: Overview and evaluation re-sults.
In Proceedings of the 12nd European Work-shop on Natural Language Generation, pages 174?182.Albert Gatt, R. van Gompel, E. Krahmer, and K. vanDeemter.
2011.
Non-deterministic attribute selec-tion in reference production.
In Workshop on theProduction of Referring Expressions (PRE-CogSci2011), pages 1?7.Albert Gatt, E. Krahmer, R. van Gompel, and K. vanDeemter.
2013.
Production of referring expres-sions: Preference trumps discrimination.
In 35thMeeting of the Cognitive Science Society, pages483?488.Pamela W. Jordan and Marilyn A. Walker.
2005.Learning content selection rules for generating ob-ject descriptions in dialogue.
J. Artif.
Int.
Res.,24(1):157?194.J.
D. Kelleher and G. Kruijff.
2006.
Incremental gen-eration of spatial referring expressions in situated di-alog.
In Proceedings of the 21st International Con-ference on Computational Linguistics and 44th An-nual Meeting of the ACL, pages 1041?1048.Ruud Koolen, Albert Gatt, Martijn Goudbeek, andEmiel Krahmer.
2011.
Factors causing overspec-ification in definite descriptions.
Journal of Prag-matics, 43(13):3231?3250.Emiel Krahmer and Mariet Theune.
2002.
Effi-cient context-sensitive generation of referring ex-pressions.
In Kees van Deemter and Rodger Kibble,editors, Information Sharing: Reference and Pre-supposition in Language Generation and Interpre-tation, pages 223?264.
CSLI Publications, Stanford,CA.Emiel Krahmer and Kees van Deemter.
2012.
Compu-tational generation of referring expressions: A sur-vey.
Computational Linguistics, 38(1):173?218.Emiel Krahmer, Sebastiaan van Erk, and Andre Verleg.2003.
Graph-based generation of referring expres-sions.
Computational Linguistics, 29(1):53?72.74Rebecca Passonneau.
2006.
Measuring agreementon set-valued items (MASI) for semantic and prag-matic annotation.
In Proceedings of the Interna-tional Conference on Language Resources and Eval-uation (LREC).T.
Pechmann.
1989.
Incremental speech produc-tion and referential overspecification.
Linguistics,27(1):98?110.Caio V. M. Teixeira, Ivandr?e Paraboni, Adriano S. R.da Silva, and Alan K. Yamasaki.
2014.
Gener-ating relational descriptions involving mutual dis-ambiguation.
Lecture Notes in Computer Science,8403:492?502.R.
van Gompel, Albert Gatt, E. Krahmer, and K. vanDeemter.
2012.
PRO: A computational modelof referential overspecification.
In Proceedings ofAMLAP-2012.Jette Viethen and Robert Dale.
2010.
Speaker-dependent variation in content selection for referringexpression generation.
In Proceedings of the Aus-tralasian Language Technology Association Work-shop 2010, pages 81?89, Melbourne, Australia.Jette Viethen and Robert Dale.
2011.
GRE3D7: A cor-pus of distinguishing descriptions for objects in vi-sual scenes.
In Proceedings of UCNLG+Eval-2011,pages 12?22.Jette Viethen, Margaret Mitchell, and Emiel Krahmer.2013.
Graphs and spatial relations in the generationof referring expressions.
In Proceedings of the 14thEuropean Workshop on Natural Language Genera-tion, pages 72?81, Sofia, Bulgaria, August.
Associ-ation for Computational Linguistics.Jette Viethen.
2011.
The Generation of Natural De-scriptions: Corpus-Based Investigations of Refer-ring Expressions in Visual Domains.
Ph.D. thesis,Macquarie University, Sydney, Australia.75
