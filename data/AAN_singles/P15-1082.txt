Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 846?856,Beijing, China, July 26-31, 2015. c?2015 Association for Computational LinguisticsNon-projective Dependency-based Pre-Reordering with RecurrentNeural Network for Machine TranslationAntonio Valerio Miceli-BaroneUniversit`a di PisaLargo B. Pontecorvo, 356127 Pisa, Italymiceli@di.unipi.itGiuseppe AttardiUniversit`a di PisaLargo B. Pontecorvo, 356127 Pisa, Italyattardi@di.unipi.itAbstractThe quality of statistical machinetranslation performed with phrasebased approaches can be increased bypermuting the words in the sourcesentences in an order which resem-bles that of the target language.
Wepropose a class of recurrent neu-ral models which exploit source-sidedependency syntax features to re-order the words into a target-like or-der.
We evaluate these models onthe German-to-English and Italian-to-English language pairs, showing sig-nificant improvements over a phrase-based Moses baseline.
We also com-pare with state of the art German-to-English pre-reordering rules, showingthat our method obtains similar or bet-ter results.1 IntroductionStatistical machine translation is typicallyperformed using phrase-based systems(Koehn et al, 2007).
These systems canusually produce accurate local reorderingbut they have difficulties dealing with thelong-distance reordering that tends to occurbetween certain language pairs (Birch et al,2008).The quality of phrase-based machine trans-lation can be improved by reordering thewords in each sentence of source-side of theparallel training corpus in a ?target-like?
or-der and then applying the same transforma-tion as a pre-processing step to input stringsduring execution.When the source-side sentences can be ac-curately parsed, pre-reordering can be per-formed using hand-coded rules.
This ap-proach has been successfully applied toGerman-to-English (Collins et al, 2005) andother languages.
The main issue with it is thatthese rules must be designed for each spe-cific language pair, which requires consider-able linguistic expertise.Fully statistical approaches, on the otherhand, learn the reordering relation from wordalignments.
Some of them learn reorderingrules on the constituency (Dyer and Resnik,2010) (Khalilov and Fonollosa, 2011) or pro-jective dependency (Genzel, 2010), (Lernerand Petrov, 2013) parse trees of source sen-tences.
The permutations that these meth-ods can learn can be generally non-local(i.e.
high distance) on the sentences but lo-cal (parent-child or sibling-sibling swaps) onthe parse trees.
Moreover, constituency orprojective dependency trees may not be theideal way of representing the syntax of non-analytic languages such as German or Ital-ian, which could be better described usingnon-projective dependency trees (Bosco andLombardo, 2004).
Other methods, based onrecasting reordering as a combinatorial opti-mization problem (Tromble and Eisner, 2009),(Visweswariah et al, 2011), can learn to gen-erate in principle arbitrary permutations, butthey can only make use of minimal syntacticinformation (part-of-speech tags) and there-fore can?t exploit the potentially valuablestructural syntactic information provided bya parser.In this work we propose a class of reorder-ing models which attempt to close this gap by846exploiting rich dependency syntax featuresand at the same time being able to processnon-projective dependency parse trees andgenerate permutations which may be non-local both on the sentences and on the parsetrees.We represent these problems as sequence pre-diction machine learning tasks, which we ad-dress using recurrent neural networks.We applied our model to reorder Germansentences into an English-like word order asa pre-processing step for phrase-based ma-chine translation, obtaining significant im-provements over the unreordered baselinesystem and quality comparable to the hand-coded rules introduced by Collins et al(2005).
We also applied our model to Italian-to-English pre-reordering, obtaining a con-siderable improvement over the unreorderedbaseline.2 Reordering as a walk on adependency treeIn order to describe the non-local reorderingphenomena that can occur between languagepairs such as German-to-English and Italian-to-English, we introduce a reordering frame-work similar to (Miceli Barone and Attardi,2013), based on a graph walk of the depen-dency parse tree of the source sentence.
Thisframework doesn?t restrict the parse tree to beprojective, and allows the generation of arbi-trary permutations.Let f ?
( f1, f2, .
.
.
, fLf) be a source sen-tence, annotated by a rooted dependencyparse tree: ?j ?
1, .
.
.
, Lf, hj?
PARENT(j)We define a walker process that walks fromword to word across the edges of the parsetree, and at each steps optionally emits thecurrent word, with the constraint that eachword must be eventually emitted exactlyonce.Therefore, the final string of emitted words f?is a permutation of the original sentence f ,and any permutation can be generated by asuitable walk on the parse tree.2.1 Reordering automatonWe formalize the walker process as a non-deterministic finite-state automaton.The state v of the automaton is the tuple v ?
(j, E, a) where j ?
1, .
.
.
, Lfis the current ver-tex (word index), E is the set of emitted ver-tices, a is the last action taken by the automa-ton.The initial state is: v(0) ?
(rootf, {}, null)where rootfis the root vertex of the parse tree.At each step t, the automaton chooses oneof the following actions:?
EMIT: emit the word fjat the currentvertex j.
This action is enabled only if thecurrent vertex has not been already emit-ted:j /?
E(j, E, a)EMIT?
(j, E ?
{j}, EMIT)(1)?
UP: move to the parent of the currentvertex.
Enabled if there is a parent andwe did not just come down from it:hj6= null, a 6= DOWNj(j, E, a)UP?
(hj, E, UPj)(2)?
DOWNj?
: move to the child j?of the cur-rent vertex.
Enabled if the subtree s(j?
)rooted at j?contains vertices that havenot been already emitted and if we didnot just come up from it:hj?= j, a 6= UPj?, ?k ?
s(j?)
: k /?
E(j, E, a)DOWNj??
(j?, E, DOWNj?
)(3)The execution continues until all the verticeshave been emitted.We define the sequence of states of thewalker automaton during one run as an execu-tion?v ?
GEN( f ).
An execution also uniquelyspecifies the sequence of actions performedby the automation.The preconditions make sure that all execu-tion of the automaton always end generatinga permutation of the source sentence.
Fur-thermore, no cycles are possible: progress ismade at every step, and it is not possible toenter in an execution that later turns out to beinvalid.Every permutation of the source sentence canbe generated by some execution.
In fact, eachpermutation f?can be generated by exactlyone execution, which we denote as?v( f?
).847We can split the execution?v( f?)
into a se-quence of Lfemission fragments?vj( f?
), eachending with an EMIT action.The first fragment has zero or more DOWN?actions followed by one EMIT action, whileeach other fragment has a non-empty se-quence of UP and DOWN?actions (alwayszero or more UPs followed by zero or moreDOWNs) followed by one EMIT action.Finally, we define an action in an executionas forced if it was the only action enabled atthe step where it occurred.2.2 ApplicationSuppose we perform reordering using atypical syntax-based system which pro-cesses source-side projective dependencyparse trees and is limited to swaps betweenpair of vertices which are either in a parent-child relation or in a sibling relation.
In suchexecution the UP actions are always forced,since the ?walker?
process never leaves a sub-tree before all its vertices have been emitted.Suppose instead that we could perform re-ordering according to an ?oracle?.
The ex-ecutions of our automaton corresponding tothese permutations will in general containunforced UP actions.
We define these ac-tions, and the execution fragments that ex-hibit them, as non-tree-local.In practice we don?t have access to areordering ?oracle?, but for sentences pairsin a parallel corpus we can compute heuristic?pseudo-oracle?
reference permutations ofthe source sentences from word-alignments.Following (Al-Onaizan and Pap-ineni, 2006), (Tromble and Eisner, 2009),(Visweswariah et al, 2011), (Navratil et al,2012), we generate word alignments in boththe source-to-target and the target-to-sourcedirections using IBM model 4 as imple-mented in GIZA++ (Och et al, 1999) and thenwe combine them into a symmetrical wordalignment using the ?grow-diag-final-and?heuristic implemented in Moses (Koehn etal., 2007).Given the symmetric word-aligned corpus,we assign to each source-side word an in-teger index corresponding to the position ofthe leftmost target-side word it is aligned to(attaching unaligned words to the followingaligned word) and finally we perform a sta-ble sort of source-side words according to thisindex.2.3 Reordering exampleConsider the segment of a German sentenceshown in fig.
1.
The English-reorderedsegment ?die Wa?hrungsreserven anfangslediglich dienen sollten zur Verteidigung?corresponds to the English: ?the reserve as-sets were originally intended to provideprotection?.In order to compose this segment from theoriginal German, the reordering automatondescribed in our framework must perform acomplex sequence of moves on the parse tree:?
Starting from ?sollten?, de-scend to ?dienen?, descent to?Wa?hrungsreserven?
and finallyto ?die?.
Emit it, then go up to?Wa?hrungsreserven?, emit it andgo up to ?dienen?
and up again to?sollten?.
Note that the last UP isunforced since ?dienen?
has not beenemitted at that point and has also un-emitted children.
This unforced actionindicates non-tree-local reordering.?
Go down to ?anfangs?.
Note that thein the parse tree edge crosses anotheredge, indicating non-projectivity.
Emit?anfangs?
and go up (forced) back to?sollten?.?
Go down to ?dienen?, down to ?zur?,down to ?lediglich?
and emit it.
Goup (forced) to ?zur?, up (unforced) to?dienen?, emit it, go up (unforced) to?sollten?, emit it.
Go down to ?dienen?,down to ?zur?
emit it, go down to?Verteidigung?
and emit it.Correct reordering of this segment wouldbe difficult both for a phrase-based system(since the words are further apart than boththe typical maximum distortion distance andmaximum phrase length) and for a syntax-based system (due to the presence of non-projectivity and non-tree-locality).848Figure 1: Section of the dependency parse tree of a German sentence.3 Recurrent Neural Networkreordering modelsGiven the reordering framework describedabove, we could try to directly predict the ex-ecutions as Miceli Barone and Attardi (2013)attempted with their version of the frame-work.
However, the executions of a givensentence can have widely different lengths,which could make incremental inexact decod-ing such as beam search difficult due to theneed to prune over partial hypotheses thathave different numbers of emitted words.Therefore, we decided to investigate a dif-ferent class of models which have the prop-erty that state transition happen only in corre-spondence with word emission.
This enablesus to leverage the technology of incrementallanguage models.Using language models for reordering isnot something new (Feng et al, 2010), (Dur-rani et al, 2011), (Bisazza and Federico, 2013),but instead of using a more or less standardn-gram language model, we are going to baseour model on recurrent neural network languagemodels (Mikolov et al, 2010).Neural networks allow easy incorpora-tion of multiple types of features and canbe trained more specifically on the typesof sequences that will occur during decod-ing, hence they can avoid wasting modelspace to represent the probabilities of non-permutations.3.1 Base RNN-RMLet f ?
( f1, f2, .
.
.
, fLf) be a source sentence.We model the reordering system as a deter-ministic single hidden layer recurrent neuralnetwork:v(t) = ?(?(1)?
x(t) +?REC?
v(t?
1)) (4)where x(t) ?
Rnis a feature vector associatedto the t-th word in a permutation f?, v(0) ?vinit,?
(1)and ?RECare parameters1and ?(?
)is the hyperbolic tangent function.If we know the first t?
1 words of the per-mutation f?in order to compute the proba-bility distribution of the t-th word we do thefollowing:?
Iteratively compute the state v(t ?
1)from the feature vectors x(1), .
.
.
, x(t ?1).?
For the all the indices of the words thathaven?t occurred in the permutation sofar j ?
J(t) ?
([1, Lf] ?
?it?1:), computea score hj,t?
ho(v(t ?
1), xo(j)), wherexo(?)
is the feature vector of the candidatetarget word.?
Normalize the scores using the logisticsoftmax function: P(?It= j| f ,?it?1:, t) =exp(hj,t)?j?
?J(t)exp(hj?,t).The scoring function ho(v(t?
1), xo(j)) ap-plies a feed-forward hidden layer to the fea-ture inputs xo(j), and then takes a weighedinner product between the activation of thislayer and the state v(t ?
1).
The result isthen linearly combined to an additional fea-ture equal to the logarithm of the remainingwords in the permutation (Lf?
t),2and to abias feature:hj,t?< ?(?(o)?
xo(j)), ?(2)v(t?
1) >+ ?(?)?
log(Lf?
t) + ?
(bias)(5)where hj,t?
ho(v(t?
1), xo(j)).1we don?t use a bias feature since it is redundantwhen the layer has input features encoded with the?one-hot?
encoding2since we are then passing this score to a softmax ofvariable size (Lf?
t), this feature helps the model tokeep the score already approximately scaled.849We can compute the probability of an entirepermutation f?just by multiplying the proba-bilities for each word: P( f?| f ) = P(?I =?i| f ) =?Lft=1P(?It=?it| f , t)3.1.1 TrainingGiven a training set of pairs of sentences andreference permutations, the training problemis defined as finding the set of parameters?
?
(vinit,?
(1), ?(2),?REC,?
(o), ?(?
), ?
(bias))which minimize the per-word empiricalcross-entropy of the model w.r.t.
the referencepermutations in the training set.
Gradientscan be efficiently computed using backpropa-gation through time (BPTT).In practice we used the following trainingarchitecture:Stochastic gradient descent, with each train-ing pair ( f , f?)
considered as a single mini-batch for updating purposes.
Gradients com-puted using the automatic differentiation fa-cilities of Theano (Bergstra et al, 2010) (whichimplements a generalized BPTT).
No trun-cation is used.
L2-regularization3.
Learn-ing rates dynamically adjusted per scalar pa-rameter using the AdaDelta heuristic (Zeiler,2012).
Gradient clipping heuristic to preventthe ?exploding gradient?
problem (Graves,2013).
Early stopping w.r.t.
a validation set toprevent overfitting.
Uniform random initial-ization for parameters other than the recur-rent parameter matrix ?REC.
Random initial-ization with echo state property for ?REC, withcontraction coefficient ?
= 0.99 (Jaeger, 2001),(Gallicchio and Micheli, 2011).Training time complexity is O(L2f) per sen-tence, which could be reduced to O(Lf) usingtruncated BTTP at the expense of update ac-curacy and hence convergence speed.
Spacecomplexity is O(Lf) per sentence.3.1.2 DecodingIn order to use the RNN-RM model for pre-reordering we need to compute the mostlikely permutation?f?of the source sentencef :?f??
argmaxf?
?GEN( f )P( f?| f ) (6)3?
= 10?4on the recurrent matrix, ?
= 10?6on thefinal layer, per minibatch.Solving this problem to the global optimum iscomputationally hard4, hence we solve it to alocal optimum using a beam search strategy.We generate the permutation incrementallyfrom left to right.
Starting from an initialstate consisting of an empty string and the ini-tial state vector vinit, at each step we generateall possible successor states and retain the B-most probable of them (histogram pruning),according to the probability of the entire pre-fix of permutation they represent.Since RNN state vectors do not decomposein a meaningful way, we don?t use any hy-pothesis recombination.At step t there are Lf?
t possible succes-sor states, and the process always takes ex-actly Lfsteps5, therefore time complexity isO(B ?
L2f) and space complexity is O(B).3.1.3 FeaturesWe use two different feature configurations:unlexicalized and lexicalized.In the unlexicalized configuration, the statetransition input feature function x(j) is com-posed by the following features, all encodedusing the ?one-hot?
encoding scheme:?
Unigram: POS(j), DEPREL(j), POS(j) ?DEPREL(j).
Left, right and parent un-igram: POS(k), DEPREL(k), POS(k) ?DEPREL(k), where k is the index of re-spectively the word at the left (in theoriginal sentence), at the right and thedependency parent of word j. Uniquetags are used for padding.?
Pair features: POS(j) ?
POS(k), POS(j) ?DEPREL(k), DEPREL(j) ?
POS(k),DEPREL(j) ?
DEPREL(k), for k definedas above.?
Triple features POS(j) ?
POS(le f tj) ?POS(rightj), POS(j) ?
POS(le f tj) ?POS(parentj), POS(j) ?
POS(rightj) ?POS(parentj).?
Bigram: POS(j) ?
POS(k), POS(j) ?DEPREL(k), DEPREL(j) ?
POS(k)where k is the previous emitted word inthe permutation.4NP-hard for at least certain choices of features andparameters5actually, Lf?
1, since the last choice is forced850?
Topological features: three binary fea-tures which indicate whether word jand the previously emitted word are ina parent-child, child-parent or sibling-sibling relation, respectively.The target word feature function xo(j) isthe same as x(j) except that each feature isalso conjoined with a quantized signed dis-tance6between word j and the previous emit-ted word.
Feature value combinations thatappear less than 100 times in the training setare replaced by a distinguished ?rare?
tag.The lexicalized configuration is equivalentto the unlexicalized one except that x(j) andxo(j) also have the surface form of word j (notconjoined with the signed distance).3.2 Fragment RNN-RMThe Base RNN-RM described in the previ-ous section includes dependency informa-tion, but not the full information of reorder-ing fragments as defined by our automa-ton model (sec.
2).
In order to determinewhether this rich information is relevant tomachine translation pre-reordering, we pro-pose an extension, denoted as Fragment RNN-RM, which includes reordering fragment fea-tures, at expense of a significant increase oftime complexity.We consider a hierarchical recurrent neuralnetwork.
At top level, this is defined as theprevious RNN.
However, the x(j) and xo(j)vectors, in addition to the feature vectors de-scribed as above now contain also the finalstates of another recurrent neural network.This internal RNN has a separate clock anda separate state vector.
For each step t ofthe top-level RNN which transitions betweenword f?(t?
1) and f?
(t), the internal RNN isreinitialized to its own initial state and per-forms multiple internal steps, one for each ac-tion in the fragment of the execution that thewalker automaton must perform to walk be-tween words f?(t?
1) and f?
(t) in the depen-dency parse (with a special shortcut of lengthone if they are adjacent in f with monotonicrelative order).6values greater than 5 and smaller than 10 are quan-tized as 5, values greater or equal to 10 are quantized as10.
Negative values are treated similarly.The state transition of the inner RNN is de-fined as:vr(t) = ?(?(r1)?
xr(tr)+?rREC?
vr(tr?
1))(7)where xr(tr) is the feature function for theword traversed at inner time trin the execu-tion fragment.
vr(0) = vinitr, ?
(r1)and ?rRECare parameters.Evaluation and decoding are performed es-sentially in the same was as in Base RNN-RM, except that the time complexity is nowO(L3f) since the length of execution fragmentsis O(Lf).Training is also essentially performed in thesame way, though gradient computation ismuch more involved since gradients prop-agate from the top-level RNN to the innerRNN.
In our implementation we just used theautomatic differentiation facilities of Theano.3.2.1 FeaturesThe unlexicalized features for the inner RNNinput vector xr(tr) depend on the currentword in the execution fragment (at index tr),the previous one and the action label: UP,DOWN or RIGHT (shortcut).
EMIT actionsare not included as they always implicitly oc-cur at the end of each fragment.Specifically the features, encoded with the?one-hot?
encoding are: A ?
POS(tr) ?POS(tr?
1), A ?
POS(tr) ?
DEPREL(tr?1), A ?
DEPREL(tr) ?
POS(tr?
1), A ?DEPREL(tr) ?
DEPREL(tr?
1).These features are also conjoined with thequantized signed distance (in the originalsentence) between each pair of words.The lexicalized features just include the surfaceform of each visited word at tr.3.3 Base GRU-RMWe also propose a variant of the Base RNN-RM where the standard recurrent hiddenlayer is replaced by a Gated Recurrent Unitlayer, recently proposed by Cho et al (2014)for machine translation applications.The Base GRU-RM is defined as the BaseRNN-RM of sec.
3.1, except that the recur-rence relation 4 is replaced by fig.
2Features are the same of unlexicalized BaseRNN-RM (we experienced difficulties train-ing the Base GRU-RM with lexicalized fea-tures).851vrst(t) = pi(?(1)rst?
x(t) +?RECrst?
v(t?
1))vupd(t) = pi(?(1)upd?
x(t) +?RECupd?
v(t?
1))vraw(t) = ?(?(1)?
x(t) +?REC?
v(t?
1) vupd(t))v(t) = vrst(t) v(t?
1) + (1?
vrst(t)) vraw(t)(8)Figure 2: GRU recurrence equations.
vrst(t) and vupd(t) are the activation vectors of the ?reset?and ?update?
gates, respectively, and pi(?)
is the logistic sigmoid function..Training is also performed in the sameway except that we found more benefi-cial to convergence speed to optimize usingAdam (Kingma and Ba, 2014)7rather thanAdaDelta.In principle we could also extend the Frag-ment RNN-RM into a Fragment GRU-RM,but we did not investigate that model in thiswork.4 ExperimentsWe performed German-to-English pre-reordering experiments with Base RNN-RM(both unlexicalized and lexicalized), Frag-ment RNN-RM and Base GRU-RM.In order to validate the experimental re-sults on a different language pair, we addi-tionally performed an Italian-to-English pre-reordering experiment with the Base GRU-RM, after assessing that this was the modelthat obtained the largest improvement onGerman-to-English.4.1 SetupThe German-to-English baseline phrase-based system was trained on the Europarl v7corpus (Koehn, 2005).
We randomly split itin a 1,881,531 sentence pairs training set, a2,000 sentence pairs development set (usedfor tuning) and a 2,000 sentence pairs testset.
The English language model was trainedon the English side of the parallel corpusaugmented with a corpus of sentences fromAP News, for a total of 22,891,001 sentences.The baseline system is phrase-based Mosesin a default configuration with maximumdistortion distance equal to 6 and lexicalizedreordering enabled.
Maximum phrase size is7with learning rate 2 ?
10?5and all the other hyper-parameters equal to the default values in the article.equal to 7.The language model is a 5-gramIRSTLM/KenLM.The pseudo-oracle system was trained onthe training and tuning corpus obtained bypermuting the German source side usingthe heuristic described in section 2.2 and isotherwise equal to the baseline system.In addition to the test set extracted fromEuroparl, we also used a 2,525 sentencepairs test set (?news2009?)
a 3,000 sentencepairs ?challenge?
set used for the WMT 2013translation task (?news2013?
).The Italian-to-English baseline system wastrained on a parallel corpus assembled fromEuroparl v7, JRC-ACQUIS v2.2 (Steinbergeret al, 2006) and additional bilingual articlescrawled from online newspaper websites8, to-taling 3,081,700 sentence pairs, which weresplit into a 3,075,777 sentence pairs phrase-table training corpus, a 3,923 sentence pairstuning corpus, and a 2,000 sentence pairs testcorpus.Non-projective dependency parsing for ourmodels, both for German and Italian wasperformed with the DeSR transition-basedparser (Attardi, 2006).We also trained a German-to-EnglishMoses system with pre-reordering performedby Collins et al (2005) rules, implemented byHowlett and Dras (2011).Constituency parsing for Collins et al (2005)rules was performed with the Berkeley parser(Petrov et al, 2006).
For Italian-to-Englishwe did not compare with a hand-codedreordering system as we are not aware ofany strong pre-reordering baseline for thislanguage pair.For our experiments, we extract approxi-8Corriere.it and Asianews.it852mately 300,000 sentence pairs from the Mosestraining set based on a heuristic confidencemeasure of word-alignment quality (Huang,2009), (Navratil et al, 2012).
We randomlyremoved 2,000 sentences from this filtereddataset to form a validation set for early stop-ping, the rest were used for training the pre-reordering models.4.2 ResultsThe hidden state size s of the RNNs was set to100 while it was set to 30 for the GRU model,validation was performed every 2,000 train-ing examples.
After 50 consecutive validationrounds without improvement, training wasstopped and the set of training parametersthat resulted in the lowest validation cross-entropy were saved.Training took approximately 1.5 days for theunlexicalized Base RNN-RM, 2.5 days for thelexicalized Base RNN-RM and for the unlexi-calized Base GRU-RM and 5 days for the un-lexicalized Fragment RNN-RM on a 24-coremachine without GPU (CPU load never roseto more than 400%).Decoding was performed with a beam sizeof 4.
Decoding the whole German corpustook about 1.0-1.2 days for all the models ex-cept Fragment RNN-RM for which it tookabout 3 days.
Decoding for the Italian corpusfor the Base GRU-RM took approximately 1.5days.Effects on monolingual reordering scoreare shown in fig.
3 (German) and fig.
4(Italian), effects on translation quality areshown in fig.
5 (German-to-English) and fig.6 (Italian-to-English)9.4.3 Discussion and analysisAll our German-to-English models signifi-cantly improved over the phrase-based base-line, performing as well as or almost as wellas (Collins et al, 2005), which is an interestingresult since our models doesn?t require anyspecific linguistic expertise.Surprisingly, the lexicalized version of BaseRNN-RM performed worse than the unlexi-9Although the baseline systems were trained on thesame datasets used in Miceli Barone and Attardi (2013),the results are different since we used a different ver-sion of Mosescalized one.
This goes contrary to expectationas neural language models are usually lexical-ized and in fact often use nothing but lexicalfeatures.The unlexicalized Fragment RNN-RM wasquite accurate but very expensive both dur-ing training and decoding, thus it may not bepractical.The unlexicalized Base GRU-RM per-formed very well, especially on the Europarldataset (where all the scores are much higherthan the other datasets) and it never per-formed significantly worse than the unlexi-calized Fragment RNN-RM which is muchslower.We also performed exploratory experi-ments with different feature sets (such aslexical-only features) but we couldn?t obtaina good training error.
Larger network sizesshould increase model capacity and may pos-sibly enable training on simpler feature sets.The Italian-to-English experiment withBase GRU-RM confirmed that this model per-forms very well on a language pair with dif-ferent reordering phenomena than German-to-English.5 ConclusionsWe presented a class of statistical syntax-based, non-projective, non-tree-local pre-reordering systems for machine translation.Our systems processes source sentencesparsed with non-projective dependencyparsers and permutes them into a target-like word order, suitable for translationby an appropriately trained downstreamphrase-based system.The models we proposed are completelytrained with machine learning approachesand is, in principle, capable of generating ar-bitrary permutations, without the hard con-straints that are commonly present in otherstatistical syntax-based pre-reordering meth-ods.Practical constraints depend on the choiceof features and are therefore quite flexible,allowing a trade-off between accuracy andspeed.In our experiments with the RNN-RM andGRU-RM models we managed to achievetranslation quality improvements compara-853Reordering BLEU improvementnone 62.10unlex.
Base RNN-RM 64.03 +1.93lex.
Base RNN-RM 63.99 +1.89unlex.
Fragment RNN-RM 64.43 +2.33unlex.
Base GRU-RM 64.78 +2.68Figure 3: German ?Monolingual?
reordering scores (upstream system output vs. ?oracle?-permuted German) on the Europarl test set.
All improvements are significant at 1% level.Reordering BLEU improvementnone 73.11unlex.
Base GRU-RM 81.09 +7.98Figure 4: Italian ?Monolingual?
reordering scores on the Europarl test set.
All improvementsare significant at 1% level.Test set system BLEU improvementEuroparl baseline 33.00Europarl ?oracle?
41.80 +8.80Europarl Collins 33.52 +0.52Europarl unlex.
Base RNN-RM 33.41 +0.41Europarl lex.
Base RNN-RM 33.38 +0.38Europarl unlex.
Fragment RNN-RM 33.54 +0.54Europarl unlex.
Base GRU-RM 34.15 +1.15news2013 baseline 18.80news2013 Collins NA NAnews2013 unlex.
Base RNN-RM 19.19 +0.39news2013 lex.
Base RNN-RM 19.01 +0.21news2013 unlex.
Fragment RNN-RM 19.27 +0.47news2013 unlex.
Base GRU-RM 19.28 +0.48news2009 baseline 18.09news2009 Collins 18.74 +0.65news2009 unlex.
Base RNN-RM 18.50 +0.41news2009 lex.
Base RNN-RM 18.44 +0.35news2009 unlex.
Fragment RNN-RM 18.60 +0.51news2009 unlex.
Base GRU-RM 18.58 +0.49Figure 5: German-to-English RNN-RM translation scores.
All improvements are significant at1% level.Test set system BLEU improvementEuroparl baseline 29.58Europarl unlex.
Base GRU-RM 30.84 +1.26Figure 6: Italian-to-English RNN-RM translation scores.
Improvement is significant at 1% level.854ble to those of the best hand-coded pre-reordering rules.ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.Distortion models for statistical machine trans-lation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the44th Annual Meeting of the Association for Com-putational Linguistics, ACL-44, pages 529?536,Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Giuseppe Attardi.
2006.
Experiments with a mul-tilanguage non-projective dependency parser.In Proceedings of the Tenth Conference on Computa-tional Natural Language Learning, CoNLL-X ?06,pages 166?170, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.James Bergstra, Olivier Breuleux, Fr?ed?ericBastien, Pascal Lamblin, Razvan Pascanu,Guillaume Desjardins, Joseph Turian, DavidWarde-Farley, and Yoshua Bengio.
2010.Theano: a CPU and GPU math expressioncompiler.
In Proceedings of the Python for Scien-tific Computing Conference (SciPy), June.
OralPresentation.Alexandra Birch, Miles Osborne, and PhilippKoehn.
2008.
Predicting success in machinetranslation.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?08, pages 745?754, Stroudsburg,PA, USA.
Association for Computational Lin-guistics.Arianna Bisazza and Marcello Federico.
2013.Efficient solutions for word reordering inGerman-English phrase-based statistical ma-chine translation.
In Proceedings of the EighthWorkshop on Statistical Machine Translation,pages 440?451, Sofia, Bulgaria, August.
Asso-ciation for Computational Linguistics.Cristina Bosco and Vincenzo Lombardo.
2004.Dependency and relational structure in tree-bank annotation.
In COLING 2004 Recent Ad-vances in Dependency Grammar, pages 1?8.Kyunghyun Cho, Bart van Merri?enboer, DzmitryBahdanau, and Yoshua Bengio.
2014.
Onthe properties of neural machine translation:Encoder-decoder approaches.
arXiv preprintarXiv:1409.1259.Michael Collins, Philipp Koehn, and IvonaKu?cerov?a.
2005.
Clause restructuring for sta-tistical machine translation.
In Proceedings ofthe 43rd annual meeting on association for computa-tional linguistics, pages 531?540.
Association forComputational Linguistics.Nadir Durrani, Helmut Schmid, and AlexanderFraser.
2011.
A joint sequence translationmodel with integrated reordering.
In Proceed-ings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies-Volume 1, pages 1045?1054.
Associ-ation for Computational Linguistics.Chris Dyer and Philip Resnik.
2010.
Context-freereordering, finite-state translation.
In HumanLanguage Technologies: The 2010 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, HLT ?10, pages858?866, Stroudsburg, PA, USA.
Association forComputational Linguistics.Minwei Feng, Arne Mauser, and Hermann Ney.2010.
A source-side decoding sequence modelfor statistical machine translation.
In Conferenceof the Association for Machine Translation in theAmericas (AMTA).C.
Gallicchio and A. Micheli.
2011.
Architecturaland markovian factors of echo state networks.Neural Networks, 24(5):440 ?
456.Dmitriy Genzel.
2010.
Automatically learningsource-side reordering rules for large scale ma-chine translation.
In Proceedings of the 23rd In-ternational Conference on Computational Linguis-tics, COLING ?10, pages 376?384, Stroudsburg,PA, USA.
Association for Computational Lin-guistics.Alex Graves.
2013.
Generating sequenceswith recurrent neural networks.
arXiv preprintarXiv:1308.0850.Susan Howlett and Mark Dras.
2011.
Clauserestructuring for SMT not absolutely helpful.In Proceedings of the 49th Annual Meeting of theAssocation for Computational Linguistics: HumanLanguage Technologies, pages 384?388.Fei Huang.
2009.
Confidence measure for wordalignment.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natu-ral Language Processing of the AFNLP: Volume 2-Volume 2, pages 932?940.
Association for Com-putational Linguistics.Herbert Jaeger.
2001.
The echo state ap-proach to analysing and training recurrent neu-ral networks-with an erratum note.
Bonn, Ger-many: German National Research Center for Infor-mation Technology GMD Technical Report, 148:34.Maxim Khalilov and Jos?e AR Fonollosa.
2011.Syntax-based reordering for statistical ma-chine translation.
Computer speech & language,25(4):761?788.Diederik Kingma and Jimmy Ba.
2014.
Adam:A method for stochastic optimization.
arXivpreprint arXiv:1412.6980.855Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen,Christine Moran, Richard Zens, Chris Dyer,Ond?rej Bojar, Alexandra Constantin, and EvanHerbst.
2007.
Moses: open source toolkit forstatistical machine translation.
In Proceedingsof the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, ACL?07, pages 177?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Philipp Koehn.
2005.
Europarl: A parallel cor-pus for statistical machine translation.
In Con-ference Proceedings: the tenth Machine TranslationSummit, pages 79?86, Phuket, Thailand.
AAMT,AAMT.Uri Lerner and Slav Petrov.
2013.
Source-sideclassifier preordering for machine translation.In Proceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing (EMNLP?13).Antonio Valerio Miceli Barone and Giuseppe At-tardi.
2013.
Pre-reordering for machinetranslation using transition-based walks on de-pendency parse trees.
In Proceedings of theEighth Workshop on Statistical Machine Transla-tion, pages 164?169, Sofia, Bulgaria, August.Association for Computational Linguistics.Tomas Mikolov, Martin Karafi?at, Lukas Bur-get, Jan Cernock`y, and Sanjeev Khudanpur.2010.
Recurrent neural network based lan-guage model.
In INTERSPEECH, pages 1045?1048.Jiri Navratil, Karthik Visweswariah, and Anan-thakrishnan Ramanathan.
2012.
A com-parison of syntactic reordering methods forenglish-german machine translation.
In COL-ING, pages 2043?2058.Franz Josef Och, Christoph Tillmann, HermannNey, et al 1999.
Improved alignment modelsfor statistical machine translation.
In Proc.
of theJoint SIGDAT Conf.
on Empirical Methods in Nat-ural Language Processing and Very Large Corpora,pages 20?28.Slav Petrov, Leon Barrett, Romain Thibaux, andDan Klein.
2006.
Learning accurate, compact,and interpretable tree annotation.
In Proceed-ings of the 21st International Conference on Com-putational Linguistics and the 44th annual meetingof the Association for Computational Linguistics,pages 433?440.
Association for ComputationalLinguistics.Ralf Steinberger, Bruno Pouliquen, Anna Widiger,Camelia Ignat, Tomaz Erjavec, Dan Tufis, andDniel Varga.
2006.
The jrc-acquis: A multi-lingual aligned parallel corpus with 20+ lan-guages.
In Proceedings of the 5th InternationalConference on Language Resources and Evaluation(LREC?2006), Genoa, Italy.Roy Tromble and Jason Eisner.
2009.
Learninglinear ordering problems for better translation.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing: Vol-ume 2 - Volume 2, EMNLP ?09, pages 1007?1016,Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Karthik Visweswariah, Rajakrishnan Rajku-mar, Ankur Gandhe, Ananthakrishnan Ra-manathan, and Jiri Navratil.
2011.
A wordreordering model for improved machinetranslation.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?11, pages 486?496, Stroudsburg,PA, USA.
Association for ComputationalLinguistics.Matthew D Zeiler.
2012.
Adadelta: An adap-tive learning rate method.
arXiv preprintarXiv:1212.5701.856
