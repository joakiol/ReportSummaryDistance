Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 399?409,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsA Semi-Supervised Approach to Improve Classification ofInfrequent Discourse Relations using Feature Vector ExtensionHugo Hernaulthugo@mi.ci.i.u-tokyo.ac.jpDanushka Bollegaladanushka@iba.t.u-tokyo.ac.jpGraduate School of Information Science & TechnologyThe University of Tokyo7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, JapanMitsuru Ishizukaishizuka@i.u-tokyo.ac.jpAbstractSeveral recent discourse parsers have em-ployed fully-supervised machine learning ap-proaches.
These methods require human an-notators to beforehand create an extensivetraining corpus, which is a time-consumingand costly process.
On the other hand, un-labeled data is abundant and cheap to col-lect.
In this paper, we propose a novelsemi-supervised method for discourse rela-tion classification based on the analysis of co-occurring features in unlabeled data, which isthen taken into account for extending the fea-ture vectors given to a classifier.
Our exper-imental results on the RST Discourse Tree-bank corpus and Penn Discourse Treebank in-dicate that the proposed method brings a sig-nificant improvement in classification accu-racy and macro-average F-score when smalltraining datasets are used.
For instance, withtraining sets of c.a.
1000 labeled instances, theproposed method brings improvements in ac-curacy and macro-average F-score up to 50%compared to a baseline classifier.
We believethat the proposed method is a first step towardsdetecting low-occurrence relations, which isuseful for domains with a lack of annotateddata.1 IntroductionAutomatic detection of discourse relations in natu-ral language text is important for numerous tasks inNLP, such as sentiment analysis (Somasundaran etal., 2009), text summarization (Marcu, 2000) and di-alogue generation (Piwek et al, 2007).
However,most of the recent work employing discourse re-lation classifiers are based on fully-supervised ma-chine learning approaches (duVerle and Prendinger,2009; Pitler et al, 2009; Lin et al, 2009).
Twoof the main corpora with discourse annotations arethe RST Discourse Treebank (RSTDT) (Carlson etal., 2001) and the Penn Discourse Treebank (PDTB)(Prasad et al, 2008a), which are both based on theWall Street Journal (WSJ) corpus.In the RSTDT, annotation is done using 78fine-grained discourse relations, which are usuallygrouped into 18 coarser-grained relations.
Each ofthese relations has furthermore several possible con-figurations for its arguments?its ?nuclearity?
(Mannand Thompson, 1988).
In practice, a classifiertrained on these coarse-grained relations must solvea 41-class classification problem.
Some of the re-lations corresponding to these classes are relativelymore frequent in the corpus, such as the ELAB-ORATION[N][S] relation (4441 instances), or theATTRIBUTION[S][N] relation (1612 instances).1However, other relation types occur very rarely,such as TOPIC-COMMENT[S][N] (2 instances), orEVALUATION[N][N] (3 instances).
A similar phe-nomenon can be observed in PDTB, in which 15level-two relations are employed: Some, such asEXPANSION.CONJUNCTION, occur as often as 8759times throughout the corpus, whereas the remainderof the relations, such as EXPANSION.EXCEPTIONand COMPARISON.PRAGMATIC CONCESSION, canappear as rarely as 17 and 12 times respectively.
Al-though supervised approaches to discourse relationlearning achieve good results on frequent relations,performance is poor on rare relation types (duVerleand Prendinger, 2009).Nonetheless, certain infrequent relation typesmight be important for specific tasks.
For instance,1We use the notation [N] and [S] respectively to denote thenucleus and satellite in a RST discourse relation.399capturing the RST TOPIC-COMMENT[S][N] andEVALUATION[N][N] relations can be useful forsentiment analysis (Pang and Lee, 2008).Another situation where detection of low-occurring relations is desirable is the case where wehave only a small training set at our disposal, for in-stance when there is not enough annotated data forall the relation types described in a discourse the-ory.
In this case, all the dataset?s relations can beconsidered rare, and being able to build an efficientclassifier depends on the capacity to deal with thislack of annotated data.Our contributions in this paper are summarized asfollows.?
We propose a semi-supervised method thatexploits the abundant, freely-available unla-beled data, which is harvested for feature co-occurrence information, and used as a basis toextend feature vectors to help classification forcases where unknown features are found in testvectors.?
The proposed method is evaluated on theRSTDT and PDTB corpus, where it signifi-cantly improves accuracy and macro-averageF-score when small training sets are used.
Forinstance, when trained on moderately smalldatasets with ca.
1000 instances, the proposedmethod increases the macro-average F-scoreand accuracy up to 50%, compared to a base-line classifier.2 Related WorkSince the release in 2001 of the RSTDT corpus,several fully-supervised discourse parsers have beenbuilt in the RST framework.
In the recent work ofduVerle and Prendinger (2009), a discourse parserbased on Support Vector Machines (SVM) (Vapnik,1995) is proposed.
SVMs are employed to train twoclassifiers: One, binary, for determining the pres-ence of a relation, and another, multi-class, for deter-mining the relation label between related text spans.For the discourse relation classifier, shallow lexical,syntactic and structural features, including ?domi-nance sets?
(Soricut and Marcu, 2003) are used.
Forrelation classification, they report an accuracy of0.668, and an F-score of 0.509 for the creation ofthe full discourse tree.The unsupervised method of Marcu and Echihabi(2002) was the first that tried to detect implicit rela-tions (i.e.
relations not accompanied by a cue phrase,such as ?however?, ?but?
), using word pairs extractedfrom two spans of text.
Their method attempts tocapture the difference of polarity in words.
For ex-ample, the word pair (sell, hold) indicates a CON-TRAST relation.Discourse relation classifiers have also beentrained using PDTB.
Pitler et al (2008) performed acorpus study of the PDTB, and found that ?explicit?relations can be most of the times distinguished bytheir discourse connectives.
Their discourse relationclassifier reported an accuracy of 0.93 for explicitrelations and in overall an accuracy of 0.744 for allrelations in PDTB.Lin et al (2009) studied the problem of detectingimplicit relations in PDTB.
Their relational classi-fier is trained using features extracted from depen-dency paths, contextual information, word pairs andproduction rules in parse trees.
They reported fortheir classifier an accuracy of 0.402, which is an im-provement of 14.1% over the previous state-of-the-art for implicit relation classification in PDTB.
Forthe same task, Pitler et al (2009) also used wordpairs, as well as several other types of features suchas verb classes, modality, context, and lexical fea-tures.In text classification, similarity measures havebeen employed in kernel methods, where they havebeen shown to improve accuracy over ?bag-of-words?
approaches.
In Siolas and d?Alche?-Buc(2000), a semantic proximity measure based onWordNet (Fellbaum, 1998) is defined, as a basis tocreate a proximity matrix for all terms of the prob-lem.
This matrix is then used to smooth the vectorialdata, and the resulting ?semantic?
metric is incorpo-rated into a SVM kernel, resulting in a significantincrease of accuracy and F-score over a baseline.Cristianini et al (2002) have used a lexical sim-ilarity measure derived from Latent Semantic In-dexing (Deerwester et al, 1990), where the seman-tic similarity between two terms is inferred fromthe analysis of their co-occurrence patterns: Termsthat co-occur often in the same documents are con-sidered as related.
In this work, the statistical co-occurrence information is extracted by the means ofsingular value decomposition.
The authors observe400substantial improvements in performance for somedatasets, while little effect is obtained for others.Semantic kernels have also been shown to be effi-cient for text classification tasks, in the case in of un-balanced and sparse datasets.
In Basili et al (2006),a ?conceptual density?
metric based on WordNet isintroduced, and employed in a SVM kernel.
Usingthis metric results in improved accuracy of 10% fortext classification in poor training conditions.
How-ever, the authors observe that when the number oftraining documents is increased, the improvementproduced by the semantic kernel is lower.Bloehdorn et al (2006) compare the performanceof different semantic kernels, based on several mea-sures of semantic relatedness in WordNet.
For eachmeasure, the authors note a performance increasewhen little training data is available, or when thefeature representations are very sparse.
However,for our task, classification of discourse relations, weemploy not only words but also other types of fea-tures such as parse tree production rules, and thuscannot compute semantic kernels using WordNet.In this paper, we are not aiming at definingnovel features for improving performance in RST orPDTB relation classification.
Instead we incorporatenumerous features that have been shown to be usefulfor discourse relation learning and explore the pos-sibilities of using unlabeled data for this task.
Oneof our goals is to improve classification accuracy forrare discourse relations.3 MethodGiven a set of unlabeled instances U and labeled in-stances L, our objective is to learn an n-class rela-tion classifier H such that for a given test instancex return its correct relation type H(x).
In the caseof discourse relation learning we are interested inthe situation where |U | >> |L|.
Here, we use thenotation |A| to denote the number of elements in aset A.
A fundamental problem that one encounterswhen trying to learn a classifier for a large numberof relations with small training dataset is that mostof the features that appear in the test instances ei-ther never occur in training instances or appear asmall number of times.
Therefore, the classifica-tion algorithm does not have sufficient informationto correctly predict the relation type of the given testinstance.
We propose a method that first computesthe co-occurrence between features using unlabeleddata and use that information to extend the featurevectors during training and testing, thereby reducingthe sparseness in test feature vectors.
In Section 3.1,we introduce the concept of feature co-occurrencematrix and describe how it is computed using unla-beled data.
A method to extend feature vectors dur-ing training and testing is presented in Section 3.2.We defer the details on exact features used in themethod to Section 3.3.
It is noteworthy that theproposed method does not depend or assume a par-ticular multi-class classification algorithm.
Conse-quently, it can be used with any multi-class classifi-cation algorithm to learn a discourse relation classi-fier.3.1 Feature Co-occurrence MatrixWe represent an instance using a d dimensional fea-ture vector f = [f1, .
.
.
, fd]T, where fi ?
R. Wedefine a feature co-occurrence matrix, C such thatthe (i, j)-th element of C, C(i,j) ?
[0, 1] denotesthe degree of co-occurrence between the two fea-tures fi and fj .
If both fi and fj appear in a fea-ture vector then we define them to be co-occurring.The number of different feature vectors in which fiand fj co-occur is denoted by the function h(fi, fj).From our definition of co-occurrence it follows thath(fi, fj) = h(fj , fi).
Importantly, feature co-occurrences can be calculated only using unlabeleddata.Feature co-occurrence matrices can be computedusing any co-occurrence measure.
For the currenttask we use the ?2-measure (Plackett, 1983) as thepreferred co-occurrence measure because of its sim-plicity.
?2-measure between two features fi and fjis defined as follows,?2i,j =2?k=12?l=1(Oi,jk,l ?
Ei,jk,l)2Ei,jk,l.
(1)Therein,Oi,j andEi,j are the 2?2 matrices contain-ing respectively observed frequencies and expectedfrequencies, which are respectively computed usingC as,Oi,j =(h(fi, fj) Zi ?
h(fi, fj)Zj ?
h(fi, fj) Zs ?
Zi ?
Zj), (2)401andEi,j =(Zi?ZjZsZi?
(Zs?Zj)ZsZj ?(Zs?Zi)Zs(Zs?Zi)?(Zs?Zj)Zs).
(3)Here, Zi =?k 6=i h(fi, fk), and Zs =?ni=1 Zi.Finally, we create the feature co-occurrence ma-trix C, such that, for all pairs of features (fi, fj),C(i,j) ={?
?2i,j if ?2i,j > c0 otherwise.
(4)Here ?
?2i,j =?2i,j??2min?2max??2min?
[0, 1], and c is the criticalvalue, which, for a confidence level of 0.05 and onedegree of freedom, can be set to 3.84.
KeepingC(i,j)in the range [0, 1] makes it convenient to filter outlow-relevance co-occurrences at the feature vectorextension step of Section 3.2.In discourse relation learning, the feature spacecan be extremely large.
For example, with wordpair features (discussed later in Section 3.3), anytwo words that appear in two adjoining discourseunits can form a feature.
Because the number ofelements in the feature co-occurrence matrix is pro-portional to the square of the feature space?s dimen-sion, computing co-occurrences for all pairs of fea-tures can be computationally costly.
Moreover, stor-ing a large matrix in memory for further computa-tions can be problematic.
To reduce the dimension-ality and improve the sparseness in the feature co-occurrence matrix, we use entropy-based feature se-lection (Manning and Schu?tze, 1999).
The negativeentropy, E(fi), of a feature fi is defined as follows,E(fi) = ?
?j 6=ip(i, j) ?
log (p (i, j)) .
(5)Here, p(i, j) is the probability that feature fi co-occurs with feature fj , and is given by p(i, j) =h(fi, fj)/Zi.If a particular feature fi co-occurs with manyother features, then its negative entropy E(fi) de-creases.
Because we are interested in identifyingsalient co-occurrences between features, we can ig-nore the features that tend to co-occur with manyother features.
Consequently, we sort the features inthe descending order of their entropy, and select thetop rankedN number of features to build the featureco-occurrence matrix.
This feature selection proce-dure can efficiently reduce the dimensions of the fea-ture co-occurrence matrix to N ?
N .
Because thefeature co-occurrence matrix is symmetric, we mustonly store the elements for the upper (or lower) tri-angular portion of it.3.2 Feature Vector ExtensionOnce the feature co-occurrence matrix is computedusing unlabeled data as described in Section 3.1, wecan use it to extend a feature vector during train-ing and testing.
The proposed feature vector exten-sion method is inspired by query expansion in thefield of Information Retrieval (Salton and Buckley,1983; Fang, 2008).
One of the reasons that a clas-sifier might perform poorly on a test instance is thatthere are features in the test instance that were notobserved during training.
We call FU = {fi} theset of features that were not observed by the clas-sifier during training (i.e.
occurring in test data butnot in training data).
For each of those features, weuse the feature co-occurrence matrix to find the setof co-occurring features, Fc(fi).Let us denote the feature vector corresponding toa training or test instance x by fx.
We use the su-perscript notation, f ix to denote the i-th feature in fx.Moreover, the total number of features of fx is indi-cated by d(x).
For a feature f ix in fx, we define n(i)number of expansion features, f (i,1)x , .
.
.
, f(i,n(i))x asfollows.
First, we require that each expansion fea-ture f (i,j)x belongs to Fc(fi).
Second, the value off (i,j)x is set to f ix ?
C(i,j).
The expansion featuresfor each feature f ix are then appended to the orig-inal feature vector fx to create an extended featurevector, f ?x, where,f ?x = (f1x , .
.
.
, fd(x)x , (6)f (i,1)x , .
.
.
, f(i,n(i))x , .
.
.
,f (d(x),1)x , .
.
.
, f(d(x),n(d(x))x ).In total, doing so augments the original vector?s sizeby?fi?U|Fc(fi)|.
All training and test instancesare extended in this fashion.Note that because this process can potentially in-crease the dimension too much, it is possible to re-tain only candidate co-occurring features of Fc(fi)possessing a co-occurrence value C(i,j) above a cer-tain threshold.
In the experiments of Section 4 how-402ever, we experienced dimension increase of 10000 atmost, which did not require us to use thresholding.3.3 FeaturesWe use three types of features: Word pairs, produc-tion rules from the parse tree, as well as features en-coding the lexico-syntactic context at the border be-tween two units of text (Soricut and Marcu, 2003).Our word pairs are lemmatized using the Wordnet-based lemmatizer of NLTK (Loper and Bird, 2002).Figure 1 shows the parse tree for a sentence com-posed of two discourse units, which serve as argu-ments of a discourse relation we want to generate afeature vector from.
Lexical heads have been calcu-lated using the projection rules of Magerman (1995),and annotated between brackets.
Surrounded bydots is, for each argument, the minimal set of sub-parse trees containing strictly all the words of theargument.We first extract all possible lemmatized word-pairs from the two arguments, such as (Mr., when),(decline, ask) or (comment, sale).
Next, we extractfrom left and right argument separately, all produc-tion rules from the sub-parse trees, such as NP 7?NNP NNP, NNP 7?
?Sherry?
or TO 7?
?to?.Finally, we encode in our features three nodes ofthe parse tree, which capture the local context at theconnection point between the two arguments: Thefirst node, which we call Nw, is the highest ances-tor of the first argument?s last word w, and is suchthat Nw?s right-sibling is the ancestor of the secondargument?s first word.
Nw?s right-sibling node iscalled Nr.
Finally, we call Np the parent of Nw andNr.
For each node, we encode in the feature vec-tor its part-of-speech (POS) and lexical head.
Forinstance, in Figure 1, we have Nw = S(comment),Nr = SBAR(when), and Np = VP(declined).
In thePDTB, certain discourse relations have disjoint ar-guments.
In this case, as well as in the case wherethe two arguments belong to different sentences, thenodes Nw, Nr, Np cannot be defined, and their cor-responding features are given the value zero.4 ExperimentsThe proposed method is independent of any partic-ular classification algorithm.
Because our goal isstrictly to evaluate the relative benefit of employingthe proposed method, and not the absolute perfor-mance when used with a specific classification algo-rithm, we select a logistic regression classifier, for itssimplicity.
We use the multi-class logistic regression(maximum entropy model) implemented in the Clas-sias toolkit (Okazaki, 2009).
Regularization param-eters are set to their default value of one and are fixedthroughout the experiments described in the paper.To create our unlabeled dataset, we use sentencesextracted from the English Wikipedia2, as they arefreely available and relatively easy to collect.
Forfurther extraction of syntactic features, these sen-tences are automatically parsed using the Stanfordparser (Klein and Manning, 2003).
Then, they aresegmented into elementary discourse units (EDUs)using our sequential discourse segmenter (Hernaultet al, 2010).
The relatively high performance ofthis RST segmenter, which has an F-score of 0.95compared to that of 0.98 between human annota-tors (Soricut and Marcu, 2003), is acceptable for thistask.
We collect and parse 100000 sentences fromrandom Wikipedia articles.
As there is no segmen-tation tool for the PDTB framework, we assume thatco-occurrence information taken from EDUs createdusing a RST segmenter is also useful for extendingfeature vectors of PDTB relations.
Unless other-wise noted, the experiments presented in the rest ofthis paper are done using those 100000 unlabeled in-stances.In the unlabeled data, any two consecutive dis-course units might not always be connected by a dis-course relation.
Therefore, we introduce an artificialNONE relation in the training set, in order to facil-itate this.
Instances of the NONE relation are gen-erated randomly by pairing consecutive discourseunits which are not connected by a discourse relationin the training data.
NONE is also learnt as a separatediscourse relation class by the multi-class classifica-tion algorithm.
This enables us to detect discourseunits between which there exist no discourse rela-tion, thereby improving the classification accuracyfor other relation types.We follow the common practice in discourse re-search for partitioning the discourse corpora intotraining and test set.
For the RST classifier, thededicated training and test sets of the RSTDT are2http://en.wikipedia.org403NP (Sherry)S (declined)VP (declined)NNP NNPdeclinedVBD (declined)Mr. Sherry toVP (comment)comment when asked about the salesTO VPSBAR (when)WHADVP (when)WRBS (asked)VP (asked)VBNPP (about)IN NP (sales)DT NNS..
(.
)Argument 1 Argument 2VBS (comment)Figure 1: Two arguments of a discourse relation, and the minimum set of subtrees that contain them?lexical headsare indicated between brackets.employed.
For the PDTB classifier, we conform tothe guidelines of Prasad et al (2008b, 5): The por-tion of the corpus corresponding to sections 2?21of the WSJ is used for training the classifier, whilethe portion corresponding to WSJ section 23 is usedfor testing.
In order to extract syntactic features, alltraining and test data are furthermore aligned withtheir corresponding parse trees in the Penn Treebank(Marcus et al, 1993).Because in the PDTB an instance can beannotated with several discourse relationssimultaneously?called ?senses?
in Prasad etal.
(2008b)?for each instance with n senses inthe corpus, we create n identical feature vectors,each being labeled by one of the instance?s senses.However, in the RST framework, only one relationis allowed to hold between two EDUs.
Conse-quently, each instance from the RSTDT is labeledwith a single discourse relation, from which asingle feature vector is created.
For RSTDT, weextract 25078 training vectors and 1633 test vectors.For PDTB we extract 49748 training vectors and1688 test vectors.
There are 41 classes (relationtypes) in the RSTDT relation classification task,and 29 classes in the PDTB task.
For the PDTB,we selected level-two relations, because they havebetter expressivity and are not too fine-grained.We experimentally set the entropy-based featureselection parameter to N = 5000.
With large Nvalues, we must store and process large featureco-occurrence matrices.
For example, doublingthe number of selected features, N to 10000 didnot improve the classification accuracy, althoughit required 4GB of memory to store the featureco-occurrence matrix.Figure 2 shows the number of features that occurin test data but not in labeled training data, againstthe number of training instances.
It can be seen fromFigure 2 that, with less training data available to theclassifier, we can potentially obtain more informa-tion regarding features by looking at unlabeled data.However, when the training dataset?s size increases,the number of features that only appear in test datadecreases rapidly.
This inverse relation between thetraining dataset size and the number of features thatonly appear in test data can be observed in bothRSTDT and PDTB datasets.
For a training set of100 instances, there are 23580 unseen features inthe case of RSTDT, and 27757 in the case of PDTB.The number of unseen features is halved for a train-ing set of 1800 instances in the case of RSTDT, andfor a training set of 1300 instances in the case ofPDTB.
Finally, when selecting all available trainingdata, we count only 1365 unseen test features in thecase of RSTDT, and 87 in the case of PDTB.In the following experiments, we use macro-averaged F-scores to evaluate the performance of theproposed discourse relation classifier on test data.Macro-averaged F-score is not influenced by thenumber of instances that exist in each relation type.It equally weights the performance on both frequentrelation types and infrequent relation types.
Becausewe are interested in measuring the overall perfor-mance of a discourse relation classifier across all re-4040 5000 10000 15000 20000 25000Number of training instances050001000015000200002500030000Numberof unseen testfeatures RSTDTPDTBFigure 2: Number of features seen only in the test set, asa function of the number of training instances used.lation types we use macro-averaged F-score as thepreferred evaluation metric for this task.We train a multi-class logistic regression modelwithout extending the feature vectors as a baselinemethod.
This baseline is expected to show the ef-fect of using the proposed feature vector extensionapproach for the task of discourse relation learn-ing.
Experimental results on RSTDT and PDTBdatasets are depicted in Figures 3 and 4.
Fromthese figures, we see that the proposed feature ex-tension method outperforms the baseline for bothRSTDT and PDTB datasets for the full range oftraining dataset sizes.
However, whereas the differ-ence of scores between the two methods is obviousfor small amounts of training data, this differenceprogressively decreases as we increase the amountof training data.
Specifically, with 100 training in-stances, the difference between baseline and pro-posed method is the largest: For RSTDT, the base-line has a macro-averaged F-score of 0.084, whereasthe the proposed method has a macro-averaged F-score of 0.189 (ca.
119% increase in F-score).
ForPDTB, the baseline has an F-score of 0.016, whilethe proposed method has an F-score of 0.089 (459%increase).
The difference of scores between the twomethods then progressively diminishes as the num-ber of training instances is increased, and fades be-yond 10000 training instances.
The reason for thisbehavior is given by Figure 2: For a small numberof training instances, the number of unseen featuresin training data is large.
In this case, the feature vec-tor extension process is comprehensive, and scorecan be increased by the use of unlabeled data.
Whenmore training data is progressively used, the num-ber of unseen test features sharply diminishes, whichmeans feature vector extension becomes more lim-ited, and the performance of the proposed methodgets progressively closer to the baseline.
Note thatwe plotted PDTB performance up to 25000 train-ing instances, as the number of unseen test featuresbecomes so small past this point that the perfor-mances of the proposed method and baseline areidentical.
Using all PDTB training data (49748 in-stances), both baseline and proposed method reach amacro-average F-score of 0.308.0 5000 10000 15000 20000 25000Number of training instances0.050.100.150.200.250.30Macro-average F-scoreBaseline RSTDTProposed methodFigure 3: Macro-average F-score (RSTDT) as a functionof the number of training instances used.0 5000 10000 15000 20000 25000Number of training instances0.000.050.100.150.200.250.30Macro-average F-scoreBaseline PDTBProposed methodFigure 4: Macro-average F-score (PDTB) as a functionof the number of training instances used.405#Tr = 1 #Tr = 2 #Tr = 3 #Tr = 5 #Tr = 7Relation name B.
P.M. B.
P.M. B.
P.M. B.
P.M. B. P.M.Attribution[N][S] ?
0.127 ?
0.237 ?
0.458 0.038 0.290 0.724 0.773Attribution[S][N] ?
0.597 ?
0.449 0.009 0.639 0.250 0.721 0.579 0.623Background[N][S] ?
0.113 ?
?
?
0.036 ?
0.095 ?
0.089Cause[N][S] ?
?
?
0.128 ?
?
?
0.034 0.057 0.187Comparison[N][S] ?
0.118 ?
0.037 ?
?
0.133 0.130 0.143 0.031Condition[N][S] ?
0.041 ?
0.136 ?
0.113 ?
0.154 0.242 0.152Condition[S][N] ?
?
?
0.122 0.133 0.148 0.214 0.233 0.390 0.308Contrast[N][N] ?
?
?
0.086 ?
0.073 0.050 0.111 ?
0.109Contrast[N][S] ?
0.071 ?
?
?
0.188 ?
0.087 ?
0.136Elaboration[N][S] ?
0.134 ?
0.126 0.004 0.067 0.004 0.340 ?
0.165Enablement[N][S] ?
?
?
0.462 ?
0.579 0.115 0.423 0.419 0.438Joint[N][N] ?
0.030 ?
0.015 ?
?
0.016 0.059 0.015 0.155Manner-Means[N][S] ?
?
?
0.056 ?
0.103 0.345 0.372 0.412 0.383Summary[N][S] ?
0.429 ?
0.453 0.080 0.358 ?
0.349 0.154 0.471Temporal[N][S] ?
0.158 ?
?
?
0.091 ?
0.052 0.204 0.101Accuracy 0.000 0.110 0.000 0.105 0.004 0.146 0.034 0.222 0.122 0.213Macro-average F-score 0.000 0.060 0.000 0.069 0.008 0.101 0.038 0.118 0.107 0.134Table 1: F-scores for RSTDT relations, using a training set containing #Tr instances of each relation.
B. indicatesF-score for baseline, P.M. for the proposed method.
A boldface indicates the best classifier for each relation.Although the distribution of discourse relationsin RSTDT and PDTB is not uniform, it is possi-ble to study the performance of the proposed methodwhen all relations are made equally rare.
We evalu-ate performance on artificially-created training setscontaining an equal amount of each discourse rela-tion.
Table 1 contains the F-score for each RSTDTrelation, using training sets containing respectivelyone, two, three, five and seven instances of eachrelation.
For space considerations, only relationswith significant results are shown.
We observe that,when using respectively one and two instances ofeach relation, the baseline classifier is unable to de-tect any relation, and has a macro-average F-scoreof zero.
Contrastingly, the classifier built with fea-ture vector extension reaches in those cases an F-score of 0.06.
Furthermore, when employing theproposed method, certain relations have relativelyhigh F-scores even with very little labeled data: Withone training instance, ATTRIBUTION[S][N] has anF-score of 0.597, while SUMMARY[N][S] has an F-score of 0.429.
With three training instances, EN-ABLEMENT[N][S] has an F-score of 0.579.
Whenthe amount of each relation is increased, the baselineclassifier starts detecting more relations.
In all cases,the proposed method performs better in terms of ac-curacy and macro-average F-score.
With a train-ing set containing seven instances of each relation,the baseline?s macro-average F-score is starting toget closer to the extended classifier?s, with superiorperformances for several relations, such as COM-PARISON[N][S], CONDITION[N][S], and TEMPO-RAL[N][S].
Still, in this case, the extended classi-fier?s accuracy is higher than the baseline (0.213 ver-sus 0.122).
Table 2 summarizes the outcome of thesame experiments performed on the PDTB dataset.The results exhibit a similar trend, despite the base-line classifier having a relatively high accuracy foreach case.Using the data from Figures 2, 3 and 4, it is pos-sible to calculate the relative score change occur-ring when using the proposed method, as a func-tion of the number of unseen features found in testdata.
This graph is plotted in Figure 5.
Besidesmacro-average F-score, we additionally plot accu-racy change.
In the top subfigure, representing thecase of RSTDT, we see that, for the lowest amountof unseen test features, the proposed method does406#Tr = 1 #Tr = 2 #Tr = 3 #Tr = 5 #Tr = 7Relation name B.
P.M. B.
P.M. B.
P.M. B.
P.M. B. P.M.Comparison.Concession[2][1] ?
0.056 ?
?
?
0.133 ?
?
?
0.154Comparison.Contrast[2][1] ?
?
?
0.333 ?
?
?
0.190 0.105 0.368Contingency.Cause[1][2] ?
0.013 ?
0.007 ?
?
?
0.026 ?
0.013Contingency.Condition[1][2] ?
0.082 ?
0.160 ?
0.127 0.250 0.253 0.214 0.171Contingency.Condition[2][1] ?
?
?
?
?
0.074 ?
0.143 0.250 0.296Contingency.Prag.
cond.
[1][2] ?
?
?
0.133 ?
0.034 ?
?
0.133 0.043Contingency.Prag.
cond.
[2][1] ?
?
?
?
?
?
0.133 0.087 0.154 0.087Expansion.Conjunction[1][2] 0.326 0.352 0.326 0.351 0.326 0.368 0.332 0.371 0.335 0.384Expansion.Instantiation[1][2] ?
?
?
?
?
0.042 ?
0.057 ?
0.131Temporal.Asynchronous[1][2] ?
0.204 ?
?
?
0.142 0.039 0.148 ?
0.035Temporal.Asynchronous[2][1] ?
?
?
?
?
0.316 ?
0.483 0.143 ?Temporal.Synchrony[1][2] ?
?
?
0.032 ?
0.162 0.032 0.103 0.032 0.157Temporal.Synchrony[2][1] ?
?
?
0.083 ?
0.143 0.200 0.308 0.211 0.174Accuracy 0.195 0.201 0.195 0.202 0.195 0.212 0.202 0.214 0.204 0.213Macro-average F-score 0.015 0.033 0.015 0.054 0.015 0.084 0.045 0.108 0.072 0.100Table 2: F-scores for PDTB relations.not bring any change in F-score or accuracy.
In-deed, as the number of unknown features is low,feature vector extension is very limited, and doesnot improve the performance compared to the base-line.
Then, a progressive increase of both accuracyand macro-average F-score is observed, as the num-ber of unseen test features is incremented.
For in-stance, for 8500 unseen test features, the macro-average F-score increase (resp.
accuracy increase)is 25% (resp.
2.5%), while it is 20% (resp.
1%) for11000 unseen test instances.
These values reach amaximum of 119% macro-average F-score increase,and 66% accuracy increase, when 23500 featuresunseen during training are present in test data.
Thissituation corresponds in Figures 3 and 4 to the caseof very small training sets.
The bottom subfigureof Figure 2, for the case of PDTB, reveals a sim-ilar tendency.
The macro-average F-score increase(resp.
accuracy increase) is negligible for 1000 un-seen test features, while this increase is 21% for bothmacro-average F-score and accuracy in the case of9700 unseen test features, and 459% (resp.
630% foraccuracy) when 28000 unseen features are found intest data.
This shows that the proposed method isuseful when large numbers of features are missingfrom the training set, which corresponds in practiceto small training sets, with few training instances foreach relation type.
For large training sets, most fea-tures are encountered by the classifier during train-ing, and feature vector extension does not bring use-ful information.We empirically evaluate the effect of using differ-ent amounts of unlabeled data on the performance ofthe proposed method.
We use respectively 100 and10000 labeled training instances, create feature co-occurrence matrices with different amounts of unla-beled data, and evaluate the performance in relationclassification.
Experimental results for RSTDT areillustrated in Figure 6 (top).
From Figure 6 it appearsclearly that macro-average F-scores improve withincreased number of unlabeled instances.
However,the benefit of using larger amounts of unlabeled datais more pronounced when only a small number of la-beled training instances are employed (ca.
100).
Infact, with 100 labeled training instances, the maxi-mum improvement in F-score is 119% (correspondsto using all our 100000 unlabeled instances).
How-ever, the maximum improvement in F-score with10000 labeled training instances is small, only 2.5%(corresponds to 10000 unlabeled instances).The effect of using unlabeled data on PDTB rela-tion classification is illustrated in Figure 6 (bottom).Similarly, we consecutively set the labeled trainingdataset size to 100 and 10000 instances, and plot themacro-average F-score against the unlabeled datasetsize.
As in the RSTDT experiment, the benefit of us-4070 5000 10000 15000 20000 25000Number of unseen features in test data020406080100120Score change overbaseline (%) Accuracy change RSTDTMacro-av.
F-score change RSTDT0 5000 10000 15000 20000 25000 30000Number of unseen features in test data0100200300400500600700Score change overbaseline (%) Accuracy change PDTBMacro-av.
F-score change PDTBFigure 5: Score change as a function of unseen test fea-tures for RSTDT (top) and PDTB (bottom).ing unlabeled data is more obvious when the num-ber of labeled training instances is small.
In par-ticular, with 100 training instances, the maximumimprovement in F-score is 459% (corresponds to100000 unlabeled instances).
However, with 10000labeled training instances the maximum improve-ment in F-score is 15% (corresponds to 100 unla-beled instances).
These results confirm that, on theone hand performance improvement is more promi-nent for smaller training sets, and that on the otherhand, performance is increased when using largeramounts of unlabeled data.5 ConclusionWe presented a semi-supervised method which ex-ploits the co-occurrence of features in unlabeleddata, to extend feature vectors during training andtesting in a discourse relation classifier.
Despite the0.000.050.100.150.200.25Macro-average F-score101 102 103 104 105Number of unlabeled instancesRSTDT (100)Baseline RSTDT (100)RSTDT (10000)Baseline RSTDT (10000)0.100.050.000.050.100.150.200.25Macro-average F-score101 102 103 104 105Number of unlabeled instancesPDTB (100)Baseline PDTB (100)PDTB (10000)Baseline PDTB (10000)Figure 6: Macro-average F-score for RSTDT (top) andPDTB (bottom), for 100 and 10000 training instances,against the number of unlabeled instances.simplicity of the proposed method, it significantlyimproved the macro-average F-score in discourse re-lation classification for small training datasets, con-taining low-occurrence relations.
We performed anevaluation on two popular datasets, the RSTDT andPDTB.
We empirically evaluated the benefit of usinga variable amount of unlabeled data for the proposedmethod.
Although the macro-average F-scores ofthe classifiers described are too low to be used di-rectly as discourse analyzers, the gain in F-score andaccuracy for small labeled datasets are a promisingperspective for improving classification accuracy forinfrequent relation types.
In particular, the proposedmethod can be employed in existing discourse clas-sifiers that work well on popular relations, and beexpected to improve the overall accuracy.408ReferencesR.
Basili, M. Cammisa, and A. Moschitti.
2006.
A se-mantic kernel to classify texts with very few trainingexamples.
Informatica (Slovenia), 30(2):163?172.S.
Bloehdorn, R. Basili, M. Cammisa, and A. Moschitti.2006.
Semantic kernels for text classification based ontopological measures of feature similarity.
In Proc.
ofICDM?06, pages 808?812.L.
Carlson, D. Marcu, and M. E. Okurowski.
2001.Building a discourse-tagged corpus in the frameworkof Rhetorical Structure Theory.
Proc.
of Second SIG-dial Workshop on Discourse and Dialogue-Volume 16,pages 1?10.N.
Cristianini, J. Shawe-Taylor, and H. Lodhi.
2002.
La-tent semantic kernels.
Journal of Intelligent Informa-tion Systems, 18:127?152.S.
C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.Furnas, and R. A. Harshman.
1990.
Indexing by latentsemantic analysis.
Journal of the American Society ofInformation Science, 41(6):391?407.D.
A. duVerle and H. Prendinger.
2009.
A novel dis-course parser based on Support Vector Machine clas-sification.
In Proc.
of ACL?09, pages 665?673.H.
Fang.
2008.
A re-examination of query expansion us-ing lexical resources.
In Proc.
of ACL?08, pages 139?147.C.
Fellbaum, editor.
1998.
WordNet: An electronic lexi-cal database.
MIT Press.H.
Hernault, D. Bollegala, and M. Ishizuka.
2010.
Asequential model for discourse segmentation.
In Proc.of CICLing?10, pages 315?326.D.
Klein and C. D. Manning.
2003.
Fast exact inferencewith a factored model for natural language parsing.
InAdvances in Neural Information Processing Systems,volume 15.
MIT Press.Z.
Lin, M-Y.
Kan, and H. T. Ng.
2009.
Recognizing im-plicit discourse relations in the Penn Discourse Tree-bank.
In Proc.
of EMNLP?09, pages 343?351.E.
Loper and S. Bird.
2002.
NLTK: The natural lan-guage toolkit.
In Proc.
of ACL?02 Workshop on Effec-tive tools and methodologies for teaching natural lan-guage processing and computational linguistics, pages63?70.D.
M. Magerman.
1995.
Statistical decision-tree modelsfor parsing.
Proc.
of ACL?95, pages 276?283.W.
C. Mann and S. A. Thompson.
1988.
RhetoricalStructure Theory: Toward a functional theory of textorganization.
Text, 8(3):243?281.C.
D. Manning and H. Schu?tze.
1999.
Foundations ofStatistical Natural Language processing.
MIT Press.D.
Marcu and A. Echihabi.
2002.
An unsupervised ap-proach to recognizing discourse relations.
In Proc.
ofACL?02, pages 368?375.D.
Marcu.
2000.
The Theory and Practice of DiscourseParsing and Summarization.
MIT Press.M.
P. Marcus, M. A. Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):313?330.N.
Okazaki.
2009.
Classias: A collection of machine-learning algorithms for classification.
http://www.chokkan.org/software/classias/.B.
Pang and L. Lee.
2008.
Opinion mining and senti-ment analysis.
Foundations and Trends in InformationRetrieval, 2(1-2):1?135.E.
Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A. Lee,and A. Joshi.
2008.
Easily identifiable discourse rela-tions.
In Proc.
of COLING?08 (Posters), pages 87?90.E.
Pitler, A. Louis, and A. Nenkova.
2009.
Automaticsense prediction for implicit discourse relations in text.In Proc.
of ACL?09, pages 683?691.P.
Piwek, H. Hernault, H. Prendinger, and M. Ishizuka.2007.
Generating dialogues between virtual agents au-tomatically from text.
In Proc.
of IVA?07, pages 161?174.R.
L. Plackett.
1983.
Karl Pearson and the chi-squaredtest.
International Statistical Review / Revue Interna-tionale de Statistique, 51(1):59?72.R.
Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,A.
Joshi, and B. Webber.
2008a.
The Penn DiscourseTreeBank 2.0.
In Proc.
of LREC?08.R.
Prasad, E. Miltsakaki, N. Dinesh, A. Lee, A. Joshi,L.
Robaldo, and B. Webber.
2008b.
The Penn Dis-course Treebank 2.0 annotation manual.
Technical re-port, University of Pennsylvania Institute for Researchin Cognitive Science.G.
Salton and C. Buckley.
1983.
Introduction to ModernInformation Retrieval.
McGraw-Hill Book Company.G.
Siolas and F. d?Alche?-Buc.
2000.
Support Vector Ma-chines based on a semantic kernel for text categoriza-tion.
In Proc.
of IJCNN?00, volume 5, page 5205.S.
Somasundaran, G. Namata, J. Wiebe, and L. Getoor.2009.
Supervised and unsupervised methods in em-ploying discourse relations for improving opinion po-larity classification.
In Proc.
of EMNLP?09, pages170?179.R.
Soricut and D. Marcu.
2003.
Sentence level discourseparsing using syntactic and lexical information.
Proc.of NA-ACL?03, 1:149?156.V.
N. Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer-Verlag New York, Inc.409
