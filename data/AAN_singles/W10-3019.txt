Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 132?137,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsUncertainty Learning Using SVMs and CRFsVinodkumar PrabhakaranComputer Science DepartmentColumbia University, New Yorkvp2198@columbia.eduAbstractIn this work, we explore the use of SVMsand CRFs in the problem of predicting cer-tainty in sentences.
We consider this as atask of tagging uncertainty cues in context,for which we used lexical, wordlist-basedand deep-syntactic features.
Results showthat the syntactic context of the tokens inconjunction with the wordlist-based fea-tures turned out to be useful in predictinguncertainty cues.1 IntroductionExtracting factual information from text is a crit-ical NLP task which has important applicationsin Information Extraction, Textual Entailment etc.It is found that linguistic devices such as hedgephrases help to distinguish facts from uncertaininformation.
Hedge phrases usually indicate thatauthors do not or cannot back up their opin-ions/statements with facts.
As part of the CoNLLshared task 2010 (Farkas et al, 2010), we exploredthe applicability of different machine learning ap-proaches and feature sets to learn to detect sen-tences containing uncertainty.In Section 2, we present the task formally anddescribe the data used.
Section 3 presents thesystem description and explains the features usedin the task in detail.
We investigated two differ-ent machine learning frameworks in this task anddid experiments on various feature configurations.Section 4 presents those experiments and analyzesthe results.
Section 5 describes the system usedfor the shared task final submission and presentsthe results obtained in the evaluation.
Section 6concludes the paper and discusses a few future di-rections to extend this work.2 Task Description and DataWe attempt only the Task 1 of the CoNLL sharedtask which was to identify sentences in texts whichcontain unreliable or uncertain information.
Inparticular, the task is a binary classification prob-lem, i.e.
to distinguish factual versus uncertainsentences.As training data, we use only the corpus ofWikipedia paragraphs with weasel cues manuallyannotated (Ganter and Strube, 2009).
The annota-tion of weasel/hedge cues was carried out on thephrase level, and sentences containing at least onecue are considered as uncertain, while sentenceswith no cues are considered as factual.
The corpuscontained 11, 110 sentences out of which 2, 484were tagged as uncertain.
A sentence could havemore than one cue phrases.
There were 3143 cuephrases altogether.3 System Description3.1 ApproachWe considered this task as a cue tagging taskwhere in phrases suggesting uncertainty will betagged in context.
This is a 3-way classificationproblem at token level - B-cue, I-cue and O denot-ing beginning, inside and outside of a cue phrase.We applied a supervised learning framework forthis task, for which We experimented with bothSVMs and CRFs.
For SVM, we used the Yam-cha1 system which is built on top of the tinySVM2package.
Yamcha has been shown useful in simi-lar tasks before.
It was the best performing systemin the CoNLL-2000 Shared task on chunking.
Inthis task, Yamcha obtained the best performancefor a quadratic kernel with a c value of 0.5.
Allresults presented here use this setting.
For CRF,we used the Mallet3 software package.
Experi-ments are done only with order-0 CRFs.
CRFsproved to marginally improve the prediction accu-racy while substantially improving the speed.
Fore.g, for a configuration of 10 features with contextwidth of 2, Yamcha took around 5-6 hrs for 9-fold1http://chasen.org/ taku/software/YamCha/2http://chasen.org/ taku/software/TinySVM/3http://mallet.cs.umass.edu/132cross validation on the whole training set, whereas Mallet took only around 30-40 minutes only.3.2 FeaturesOur approach was to explore the use of deep syn-tactic features in this tagging task.
Deep syntac-tic features had been proven useful in many simi-lar tagging tasks before.
We used the dependencyparser MICA (Bangalore et al, 2009) based onTree Adjoining Grammar (Joshi et al, 1975) to ex-tract these deep syntactic features.We classified the features into three classes -Lexical (L), Syntactic (S) and Wordlist-based (W).Lexical features are those which could be found atthe token level without using any wordlists or dic-tionaries and can be extracted without any parsingwith relatively high accuracy.
For example, isNu-meric, which denotes whether the word is a num-ber or alphabetic, is a lexical feature.
Under thisdefinition, POS tag will be considered as a lexicalfeature.Syntactic features of a token access its syntacticcontext in the dependency tree.
For example, par-entPOS, the POS tag of the parent word in thedependency parse tree, is a syntactic feature.
Thetree below shows the dependency parse tree outputby MICA for the sentence Republican leader BillFrist said the Senate was hijacked.saidFristRepublican leader BillhijackedSenatethewasIn this case, the feature haveReportingAnces-tor of the word hijacked is ?Y?
because it is a verbwith a parent verb said.
Similarly, the featurehaveDaughterAux would also be ?Y?
because ofdaughter was, whereas whichAuxIsMyDaughterwould get the value was.Wordlist-based features utilized a list of wordswhich occurred frequently as a cue word in thetraining corpus.
We used two such lists ?
onewhich included adjectives like many, most, someetc.
The other list contained adverbs like proba-bly, possibly etc.
The complete list of words inthese wordlists are given in Table 1.For finding the best performing feature set -context width configuration, we did an exhaustivesearch on the feature space, pruning away featureswhich were proven not useful by results at stages.The list of features we used in our experimentsare summarized in Table 1 and Table 2.
Ta-ble 1 contains features which were useful andare present in the results presented in section 4.Out of the syntactic features, parentPOS and is-MyNNSparentGeneric turned out to be the mostuseful.
It was noticed that in most cases in whicha generic adjective (i.e., a quantifier such as many,several, ...) has a parent which is a plural noun,and this noun has only adjectival daughters, thenit is part of a cue phrase.
This distinction can bemade clear by the below example.?
?ccue?
Many people ?/ccue?
enjoy havingprofessionally made ?family portraits??
Many departments, especially those in whichstudents have research or teaching responsi-bilities ...In the first case, the noun people comes with theadjective Many, but is not qualified further.
Thismakes it insufficiently defined and hence is taggedas a cue phrase.
However in the second case, theclause which starts with especially is qualifyingthe noun departments further and hence the phraseis not tagged as a cue word despite the presenceof Many.
This scenario occurred often with otheradjectives like most, some etc.
This distinctionwas caught to a good extent by the combinationof isMyNNSparentGeneric and isGenericAdj.Hence, the best performing configuration used fea-tures from both W and S categories.The features which were found to be not usefulis listed in Table 2.
We used only two wordlistfeatures, both of which were useful.4 ExperimentsTo find the best configuration, we used 10% of thetraining data as the development set to tune param-eters.
Since even the development set was fairlylarge, we used 9-fold cross validation to evaluateeach models.
The development set was dividedinto 9 folds of which 8 folds were used to train amodel which was tested on the 9th fold.
All thereported results in this section are averaged overthe 9 folds.
We report F?=1 (F)-measure as theharmonic mean between (P)recision and (R)ecall.We categorized the experiments into three dis-tinct classes as shown in Table 3.
For each class,we did experiments with different feature sets and133No Feature DescriptionLexical Features1 verbType Modal/Aux/Reg ( = ?nil?
if the word is not a verb)2 lemma Lemma of the token3 POS Word?s POS tag4 whichModalAmI If I am a modal, what am I?
( = ?nil?
if I am not a modal)Word List Features1 isGenericAdj Am I one of some, many, certain, several?2 isUncertainAdv Am I one of generally, probably, usually, likely, typically, possibly, commonly, nearly,perhaps, often?3 levinClass If I am a verb, which levin class do I belong to?Syntactic Features1 parentPOS What is my parent?s POS tag?2 leftSisPOS What is my left sister?s POS tag?3 rightSisPOS What is my right sister?s POS tag?4 whichModalIsMyDaughter If I have a daughter which is a modal, what is it?
( = ?nil?
if I do not have a modaldaughter)5 Voice Active/Passive (refer MICA documentation for details)6 Mpos MICA?s mapping of POS tags (refer MICA documentation for details)7 isMyNNSparentGeneric If I am an adjective and if my parent is NNS and does not have a child other thanadjectives8 haveDaughterAux Do I have a daughter which is an auxiliary.9 whichAuxIsMyDaughter If I have a daughter which is an auxiliary, what is it?
( = ?nil?
if I do not have anauxiliary daughter)Table 1: Features used in the configurations listed in Table 4 and Table 6Class DescriptionL Lexical featuresLW Lexical and Wordlist featuresLS Lexical and Syntactic featuresLSW Lexical, Syntactic and Wordlist fea-turesTable 3: Experiment Sets(linear) context widths.
Here, context width de-notes the window of tokens whose features areconsidered.
For example, a context width of 2means that the feature vector of any given tokenincludes, in addition to its own features, those of2 tokens before and after it as well as the predic-tion for 2 tokens before it.
We varied the contextwidths from 1 to 5, and found that the best resultswere obtained for context width of 1 and 2.4.1 Experimental ResultsIn this section, we present the results of experi-ments conducted on the development set as partof this task.
The results for the system using Yam-cha and Mallet are given in Table 4.
CW stands forContext Width and P, R and F stands for Precision,Recall and F-measure, respectively.
These resultsinclude the top performing 5 feature set - contextwidth configurations using all three classes of fea-tures in both cases.
It includes cue level predic-tion performance as well as sentence level predic-tion performance, where in a sentence is taggedas uncertain if it contains at least one cue phrase.In case of Mallet, it is observed that the best per-forming top 5 feature sets were all from the LSWcategory whereas in Yamcha, even configurationsof LS category worked well.We also present cue level results across featurecategories for the Mallet experiments.
Table 5shows the best feature set - context width configu-ration for each class of experiments.Class Feature Set CWL POS, verbType 2LW lemma, POS, modalMe, isGenericAdj,isUncertainAdj2LS POS, parentPOS, modalDaughter, left-SisPOS, rightSisPOS, voice2LSW POS, parentPOS, modalMe, isDaughter-Aux, leftSisPOS, mpos, isUncertainAdj,isGenericAdj, myNNSparentIsGeneric1Table 5: Best Feature sets - Across feature classesTable 6 shows the cue level results of the bestmodel for each class of experiments.134No Feature DescriptionLexical Features1 Stem Word stem (Using Porter Stemmer)2 isNumeric Word is Alphabet or Numeric?Syntactic Features1 parentStem Parent word stem (Using Porter Stemmer)2 parentLemma Parent word?s Lemma3 wordSupertag Word?s Super Tag (from Penn Treebank)4 parentSupertag Parent word?s super tag (from Penn Treebank)5 isRoot Is the word the root of the MICA Parse tree?6 pred Is the word a predicate?
(pred in MICA features)7 drole Deep role (drole in MICA features)8 haveDaughterTo Do I have a daughter ?to?
?9 haveDaughterPerfect Do I have a daughter which is one of has, have, had?10 haveDaughterShould Do I have a daughter should?11 haveDaughterWh Do I have a daughter who is one of where, when, while, who, why?Table 2: Features which turned out to be not usefulClass Cue P Cue R Cue FL 54.89 21.99 30.07LW 51.14 20.70 28.81LS 52.08 25.71 33.23LSW 51.13 29.38 36.71Table 6: Cue level Results - Across feature classes4.2 AnalysisIt is observed that the best results were observedon LSW category.
The main constituent of thiscategory was the combination of isMyNNSpar-entGeneric and isGenericAdj.
Also, it wasfound that W features used without S features de-creased the prediction performance.
Out of thesyntactic features, parentPOS, leftSisPOS andrightSisPOS proved to be the most useful in ad-dition to isMyNNSparentGeneric.Also, the highest cue level precision of 54.89%was obtained for L class, whereas it was loweredto 51.13% by the addition of S and W features.However, the performance improvement is due tothe improved recall, which is as per the expec-tation that syntactic features would help identifynew patterns, which lexical features alone cannot.It is also worth noting that addition of W featuresdecreased the precision by 3.75 percentage pointswhereas addition of S features decreased the pre-cision by 2.81 percentage points.
Addition of Sfeatures improved the recall by 3.72 percentagepoints where as addition of both S and W featuresimproved it by 7.39 percentage points.
However,addition of W features alone decreased the recallby 1.29 percentage points.
This suggests that thewords in the wordlists were useful only when pre-sented with the syntactic context in which they oc-curred.Mallet proved to consistently over performYamcha in this task in terms of prediction perfor-mance as well as speed.
For e.g, for a configura-tion of 10 features with context width of 2, Yam-cha took around 5-6 hrs to perform the 9-fold crossvalidation on the entire training dataset, whereasMallet took only around 30-40 minutes.5 System used for EvaluationIn this section, we explain in detail the systemwhich was used for the results submitted in theshared task evaluation.For predicting the cue phrases on evaluationdataset for the shared task, we trained a model us-ing the best performing configuration (feature setand machinery) from the experiments described inSection 4.
The best configuration used the featureset <POS, parentPOS, modalMe, isDaugh-terAux, leftSisPOS, mpos, isUncertainAdj, is-GenericAdj, myNNSparentIsGeneric> with acontext width of 1 and it was trained using Mal-let?s CRF.
The cross validation results of this con-figuration is reported in Table 4 (First feature set inthe Mallet section).
This model was trained on theentire Wikipedia training set provided for Task 1.We used this model to tag the evaluation datasetwith uncertainty cues and any sentence where acue phrase was tagged was classified as an uncer-tain sentence.135Feature Set CW Cue SentP R F P R FYamcha - Top 5 ConfigurationsPOS, parentPOS, modalDaughter, leftSisPOS, rightSisPOS,levinClass, myNNSparentIsGeneric2 51.59 26.96 34.10 65.27 38.33 48.30POS, parentPOS, amIuncertain 1 43.13 29.41 33.79 55.37 41.77 47.62POS, parentPOS, modalDaughter, leftSisPOS, rightSisPOS,voice2 52.08 25.71 33.23 66.52 37.10 47.63POS, parentPOS, modalDaughter, leftSisPOS 2 54.25 25.16 33.20 69.38 35.63 47.08POS, parentPOS, modalDaughter, leftSisPOS, rightSisPOS,mpos2 51.82 25.56 33.01 65.62 36.12 46.59Mallet - Top 5 ConfigurationsPOS, parentPOS, modalMe, isDaughterAux, leftSisPOS,mpos, isUncertainAdj, isGenericAdj, myNNSparentIsGeneric1 51.13 29.38 36.71 66.29 42.71 51.95POS, parentPOS, modalMe, isDaughterAux, leftSisPOS,mpos, voice, isUncertainAdj, isGenericAdj, myNNSparentIs-Generic1 49.81 29.07 36.04 65.64 42.24 51.40POS, parentPOS, modalMe, isUncertainAdj, isGenericAdj,myNNSparentIsGeneric2 52.57 28.96 35.55 65.18 39.56 49.24POS, parentPOS, modalMe, auxDaughter, leftSisPOS, mpos,voice, isUncertainAdj, isGenericAdj, myNNSparentIsGeneric1 48.22 28.67 35.40 65.25 42.80 51.69POS, parentPOS, modalMe, leftSisPOS, mpos, voice,isUncertainAdj, isGenericAdj, myNNSparentIsGeneric1 52.26 28.12 35.34 65.99 40.05 49.85Table 4: Overall Results5.1 Evaluation ResultsThis section presents the results obtained on theshared task evaluation in detail.
The sentence levelresults are given in Table 7.
Our system obtaineda high precision of 87.95% with a low recall of28.42% and F-measure of 42.96% on the task.This was the 3rd best precision reported for theWikipedia task 1.System Precision Recall F-MeasureBest System 72.04 51.66 60.17... ... ...
...This System 87.95 28.42 42.96Last System 94.23 6.58 12.30Table 7: Evaluation - Cue Level ResultsTable 8 presents the cue level results for thetask.
Our system had a cue level prediction pre-cision of 67.14% with a low recall of 16.70% andF-measure of 26.75%, which is the 3rd best F-measure result among the published cue level re-sults4.We ran the best model trained onWikipedia cor-pus on the biomedical evaluation dataset.
As ex-pected, the results were much lower.
It obtained aprecision of 67.54% with a low recall of 19.49%and F-measure of 30.26%.4In the submitted result, cues were tagged in IOB format.Hence, cue level statistics were not computed and publishedin the CoNLL website.System Precision Recall F-MeasureX 63.01 25.94 36.55X 76.06 21.64 33.69This System 67.14 16.70 26.75X 28.95 14.70 19.50X 24.57 7.35 11.32Table 8: Evaluation - Cue Level Results6 Conclusion and Future WorkA simple bag of words approach at the sentencelevel could have given similar or even better per-formance for the sentence level prediction task.However, identifying cues in context is importantto extend this task to application where we need tomake semantic inferences or even identifying thescope of uncertainty (which was the task 2 of theshared task).
Hence, we infer that this or a simi-lar cue tagging approach with a more sophisticatedfeature set and machinery should be explored fur-ther.Our experiments show that the addition of syn-tactic features helps in improving recall.
However,the advantage given by syntactic features were sur-prisingly marginal.
In detailed error analysis, itwas found that the syntactic patterns that provedhelpful for this task were fairly local.
So, proba-bly exploring shallow syntactic features instead ofdeep syntactic features might be helpful for thistask.
Also, we assume that using more sophis-136ticated lexical features or custom made lexiconscould also improve performance.AcknowledgementsThis work was supported by grants from the Hu-man Language Technology Center of Excellence.Any opinions, findings, and conclusions or recom-mendations expressed in this material are those ofthe author and do not necessarily reflect the viewsof the sponsor.I would also like to extend my heartfelt grat-itude to Prof. Kathy McKeown and Yves Petinotfor their vital encouragement and support through-out this project.
I would also like to thank my ad-visors Dr. Mona Diab and Dr. Owen Rambow fortheir valuable suggestions and support.ReferencesSrinivas Bangalore, Pierre Boullier, Alexis Nasr, OwenRambow, and Beno?
?t Sagot.
2009.
MICA: A prob-abilistic dependency parser based on tree insertiongrammars.
In NAACL HLT 2009 (Short Papers).Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010 Shared Task: Learning to Detect Hedges andtheir Scope in Natural Language Text.
In Proceed-ings of the Fourteenth Conference on ComputationalNatural Language Learning (CoNLL-2010): SharedTask, pages 1?12, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.Viola Ganter and Michael Strube.
2009.
FindingHedges by Chasing Weasels: Hedge Detection Us-ingWikipedia Tags and Shallow Linguistic Features.In Proceedings of the ACL-IJCNLP 2009 Confer-ence Short Papers, pages 173?176, Suntec, Singa-pore, August.
Association for Computational Lin-guistics.Aravind K. Joshi, Leon Levy, and M Takahashi.
1975.Tree adjunct grammars.
Journal of the Computerand System Sciences, 10:136?163.137
