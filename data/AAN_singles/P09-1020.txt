Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 172?180,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPForest-based Tree Sequence to String Translation ModelHui Zhang1, 2   Min Zhang1   Haizhou Li1   Aiti Aw1   Chew Lim Tan21Institute for Infocomm Research                    2National University of Singaporezhangh1982@gmail.com   {mzhang, hli, aaiti}@i2r.a-star.edu.sg   tancl@comp.nus.edu.sgAbstractThis paper proposes a forest-based tree se-quence to string translation model for syntax-based statistical machine translation, whichautomatically learns tree sequence to stringtranslation rules from word-aligned source-side-parsed bilingual texts.
The proposedmodel leverages on the strengths of both treesequence-based and forest-based translationmodels.
Therefore, it can not only utilize foreststructure that compactly encodes exponentialnumber of parse trees but also capture non-syntactic translation equivalences with linguis-tically structured information through tree se-quence.
This makes our model potentiallymore robust to parse errors and structure di-vergence.
Experimental results on the NISTMT-2003 Chinese-English translation taskshow that our method statistically significantlyoutperforms the four baseline systems.1 IntroductionRecently syntax-based statistical machine trans-lation (SMT) methods have achieved very prom-ising results and attracted more and more inter-ests in the SMT research community.
Fundamen-tally, syntax-based SMT views translation as astructural transformation process.
Therefore,structure divergence and parse errors are two ofthe major issues that may largely compromisethe performance of syntax-based SMT (Zhang etal., 2008a; Mi et al, 2008).Many solutions have been proposed to addressthe above two issues.
Among these advances,forest-based modeling (Mi et al, 2008; Mi andHuang, 2008) and tree sequence-based modeling(Liu et al, 2007; Zhang et al, 2008a) are twointeresting modeling methods with promisingresults reported.
Forest-based modeling aims toimprove translation accuracy through digging thepotential better parses from n-bests (i.e.
forest)while tree sequence-based modeling aims tomodel non-syntactic translations with structuredsyntactic knowledge.
In nature, the two methodswould be complementary to each other sincethey manage to solve the negative impacts ofmonolingual parse errors and cross-lingual struc-ture divergence on translation results from dif-ferent viewpoints.
Therefore, one natural way isto combine the strengths of the two modelingmethods for better performance of syntax-basedSMT.
However, there are many challenges incombining the two methods into a single modelfrom both theoretical and implementation engi-neering viewpoints.
In theory, one may worryabout whether the advantage of tree sequence hasalready been covered by forest because forestencodes implicitly a huge number of parse treesand these parse trees may generate many differ-ent phrases and structure segmentations given asource sentence.
In system implementation, theexponential combinations of tree sequences withforest structures make the rule extraction anddecoding tasks much more complicated than thatof the two individual methods.In this paper, we propose a forest-based treesequence to string model, which is designed tointegrate the strengths of the forest-based and thetree sequence-based modeling methods.
We pre-sent our solutions that are able to extract transla-tion rules and decode translation results for ourmodel very efficiently.
A general, configurableplatform was designed for our model.
With thisplatform, we can easily implement our methodand many previous syntax-based methods bysimple parameter setting.
We evaluate ourmethod on the NIST MT-2003 Chinese-Englishtranslation tasks.
Experimental results show thatour method significantly outperforms the twoindividual methods and other baseline methods.Our study shows that the proposed method isable to effectively combine the strengths of theforest-based and tree sequence-based methods,and thus having great potential to address theissues of parse errors and non-syntactic transla-172tions resulting from structure divergence.
It alsoindicates that tree sequence and forest play dif-ferent roles and make contributions to our modelin different ways.The remainder of the paper is organized as fol-lows.
Section 2 describes related work while sec-tion 3 defines our translation model.
In section 4and section 5, the key rule extraction and decod-ing algorithms are elaborated.
Experimental re-sults are reported in section 6 and the paper isconcluded in section 7.2 Related workAs discussed in section 1, two of the major chal-lenges to syntax-based SMT are structure diver-gence and parse errors.
Many techniques havebeen proposed to address the structure diver-gence issue while only fewer studies are reportedin addressing the parse errors in the SMT re-search community.To address structure divergence issue, manyresearchers (Eisner, 2003; Zhang et al, 2007)propose using the Synchronous Tree SubstitutionGrammar (STSG) grammar in syntax-basedSMT since the STSG uses larger tree fragment astranslation unit.
Although promising results havebeen reported, STSG only uses one single sub-tree as translation unit which is still committed tothe syntax strictly.
Motivated by the fact thatnon-syntactic phrases make non-trivial contribu-tion to phrase-based SMT, the tree sequence-based translation model is proposed (Liu et al,2007; Zhang et al, 2008a) that uses tree se-quence as the basic translation unit, rather thanusing single sub-tree as in the STSG.
Here, a treesequence refers to a sequence of consecutivesub-trees that are embedded in a full parse tree.For any given phrase in a sentence, there is atleast one tree sequence covering it.
Thus the treesequence-based model has great potential to ad-dress the structure divergence issue by using treesequence-based non-syntactic translation rules.Liu et al (2007) propose the tree sequence con-cept and design a tree sequence to string transla-tion model.
Zhang et al (2008a) propose a treesequence-based tree to tree translation model andZhang et al (2008b) demonstrate that the treesequence-based modelling method can well ad-dress the structure divergence issue for syntax-based SMT.To overcome the parse errors for SMT, Mi etal.
(2008) propose a forest-based translationmethod that uses forest instead of one best tree astranslation input, where a forest is a compact rep-resentation of exponentially number of n-bestparse trees.
Mi and Huang (2008) propose a for-est-based rule extraction algorithm, which learntree to string rules from source forest and targetstring.
By using forest in rule extraction and de-coding, their methods are able to well address theparse error issue.From the above discussion, we can see thattraditional tree sequence-based method uses sin-gle tree as translation input while the forest-based model uses single sub-tree as the basictranslation unit that can only learn tree-to-string(Galley et al 2004; Liu et al, 2006) rules.
There-fore, the two methods display different strengths,and which would be complementary to eachother.
To integrate their strengths, in this paper,we propose a forest-based tree sequence to stringtranslation model.3 Forest-based tree sequence to stringmodelIn this section, we first explain what a packedforest is and then define the concept of the treesequence in the context of forest followed by thediscussion on our proposed model.3.1 Packed ForestA packed forest (forest in short) is a special kindof hyper-graph (Klein and Manning, 2001;Huang and Chiang, 2005), which is used to rep-resent all derivations (i.e.
parse trees) for a givensentence under a context free grammar (CFG).
Aforest F is defined as a triple ?
?, ?, ?
?, where?
is non-terminal node set, ?
is hyper-edge setand ?
is leaf node set (i.e.
all sentence words).
Aforest F satisfies the following two conditions:1) Each node ?
in ?
should cover a phrase,which is a continuous word sub-sequence in ?.2) Each hyper-edge ?
in ?
is defined as??
?
??
???
?
?
?, ???
?
??
?
?
?, ??
?
??
,where ??
?
??
???
covers a sequence of conti-nuous and non-overlap phrases, ??
is the fathernode of the children sequence ??
???
???.
Thephrase covered by ??
is just the sum of all thephrases covered by each child node ?
?.We here introduce another concept that is usedin our subsequent discussions.
A complete forestCF is a general forest with one additional condi-tion that there is only one root node N in CF, i.e.,all nodes except the root N in a CF must have atleast one father node.Fig.
1 is a complete forest while Fig.
7 is anon-complete forest due to the virtual node?VV+VV?
introduced in Fig.
7.
Fig.
2 is a hyper-edge (IP => NP VP) of Fig.
1, where NP covers173the phrase ?Xinhuashe?, VP covers the phrase?shengming youguan guiding?
and IP covers theentire sentence.
In Fig.1, only root IP has no fa-ther node, so it is a complete forest.
The twoparse trees T1 and T2 encoded in Fig.
1 areshown separately in Fig.
3 and Fig.
41.Different parse tree represents different deri-vations and explanations for a given sentence.For example, for the same input sentence in Fig.1, T1 interprets it as ?XNA (Xinhua NewsAgency) declares some regulations.?
while T2interprets it as ?XNA declaration is related tosome regulations.
?.Figure 1.
A packed forest for sentence ???
?/Xinhuashe ??
/shengming ??
/youguan ?
?/guiding?Figure 2.
A hyper-edge used in Fig.
1Figure 3.
Tree 1 (T1)            Figure 4.
Tree 2 (T2)3.2 Tree sequence in packed forestSimilar to the definition of tree sequence used ina single parse tree defined in Liu et al (2007)and Zhang et al (2008a), a tree sequence in aforest also refers to an ordered sub-tree sequencethat covers a continuous phrase without overlap-ping.
However, the major difference between1 Please note that a single tree (as T1 and T2 shown in Fig.3 and Fig.
4) is represented by edges instead of hyper-edges.A hyper-edge is a group of edges satisfying the 2nd condi-tion as shown in the forest definition.them lies in that the sub-trees of a tree sequencein forest may belongs to different single parsetrees while, in a single parse tree-based model,all the sub-trees in a tree sequence are committedto the same parse tree.The forest-based tree sequence enables ourmodel to have the potential of exploring addi-tional parse trees that may be wrongly pruned outby the parser and thus are not encoded in the for-est.
This is because that a tree sequence in a for-est allows its sub-trees coming from differentparse trees, where these sub-trees may not bemerged finally to form a complete parse tree inthe forest.
Take the forest in Fig.
1 as an exam-ple, where ((VV shengming) (JJ youguan)) is atree sequence that all sub-trees appear in T1while ((VV shengming) (VV youguan)) is a treesequence whose sub-trees do not belong to anysingle tree in the forest.
But, indeed the two sub-trees (VV shengming) and (VV youguan) can bemerged together and further lead to a completesingle parse tree which may offer a correct inter-pretation to the input sentence (as shown in Fig.5).
In addition, please note that, on the otherhand, more parse trees may introduce more noisystructures.
In this paper, we leave this problem toour model and let the model decide which sub-structures are noisy features.Figure 5.
A parse tree that was wronglypruned outFigure 6.
A tree sequence to string rule174A tree-sequence to string translation rule in aforest is a triple <L, R, A>, where L is the treesequence in source language, R is the string con-taining words and variables in target language,and A is the alignment between the leaf nodes ofL and R. This definition is similar to that of (Liuet al 2007, Zhang et al 2008a) except our tree-sequence is defined in forest.
The shaded area ofFig.
6 exemplifies a tree sequence to string trans-lation rule in the forest.3.3 Forest-based tree-sequence to stringtranslation modelGiven a source forest F and target translation TSas well as word alignment A, our translationmodel is formulated as:Pr?
?, ?
?, ??
?
?
?
?????????????
?,??????
?, ??,?
?By the above Eq., translation becomes a treesequence structure to string mapping issue.
Giv-en the F, TS and A, there are multiple derivationsthat could map F to TS under the constraint A.The mapping probability Pr?
?, ?
?, ??
in ourstudy is obtained by summing over the probabili-ties of all derivations ?.
The probability of eachderivation ??
is given as the product of the prob-abilities of all the rules ( )ip r  used in the deriva-tion (here we assume that each rule is appliedindependently in a derivation).Our model is implemented under log-linearframework (Och and Ney, 2002).
We use sevenbasic features that are analogous to the common-ly used features in phrase-based systems (Koehn,2003): 1) bidirectional rule mapping probabilities,2) bidirectional lexical rule translation probabili-ties, 3) target language model, 4) number of rulesused and 5) number of target words.
In addition,we define two new features: 1) number of leafnodes in auxiliary rules (the auxiliary rule will beexplained later in this paper) and 2) product ofthe probabilities of all hyper-edges of the treesequences in forest.4 TrainingThis section discusses how to extract our transla-tion rules given a triple ?
?, ?
?, ?
?
.
As weknow, the traditional tree-to-string rules can beeasily extracted from ?
?, ?
?, ?
?
using the algo-rithm of Mi and Huang (2008)2.
We would like2 Mi and Huang (2008) extend the tree-based rule extractionalgorithm (Galley et al, 2004) to forest-based by introduc-ing non-deterministic mechanism.
Their algorithm consistsof two steps, minimal rule extraction and composed rulegeneration.to leverage on their algorithm in our study.
Un-fortunately, their algorithm is not directly appli-cable to our problem because tree rules have onlyone root while tree sequence rules have multipleroots.
This makes the tree sequence rule extrac-tion very complex due to its interaction with for-est structure.
To address this issue, we introducethe concepts of virtual node and virtual hyper-edge to convert a complete parse forest ?
to anon-complete forest ?
which is designed to en-code all the tree sequences that we want.
There-fore, by doing so, the tree sequence rules can beextracted from a forest in the following twosteps:1) Convert the complete parse forest ?
into anon-complete forest ?
in order to cover thosetree sequences that cannot be covered by a singletree node.2) Employ the forest-based tree rule extractionalgorithm (Mi and Huang, 2008) to extract ourrules from the non-complete forest.To facilitate our discussion, here we introducetwo notations:?
Alignable: A consecutive source phrase isan alignable phrase if and only if it can bealigned with at least one consecutive targetphrase under the word-alignment con-straint.
The covered source span is calledalignable span.?
Node sequence: a sequence of nodes (ei-ther leaf or internal nodes) in a forest cov-ering a consecutive span.Algorithm 1 illustrates the first step of our ruleextraction algorithm, which is a CKY-style Dy-namic Programming (DP) algorithm to add vir-tual nodes into forest.
It includes the followingsteps:1) We traverse the forest to visit each span inbottom-up fashion (line 1-2),1.1) for each span [u,v] that is covered bysingle tree nodes3, we put these treenodes into the set NSS(u,v) and goback to step 1 (line 4-6).1.2) otherwise we concatenate the tree se-quences of sub-spans to generate theset of tree sequences covering the cur-rent larger span (line 8-13).
Then, weprune the set of node sequences (line14).
If this span is alignable, wecreate virtual father nodes and corres-ponding virtual hyper-edges to linkthe node sequences with the virtualfather nodes (line 15-20).3 Note that in a forest, there would be multiple single treenodes covering the same span as shown Fig.1.1752) Finally we obtain a forest with each align-able span covered by either original treenodes or the newly-created tree sequencevirtual nodes.Theoretically, there is exponential number ofnode sequences in a forest.
Take Fig.
7 as an ex-ample.
The NSS of span [1,2] only contains ?NP?since it is alignable and covered by the singletree node NP.
However, span [2,3] cannot becovered by any single tree node, so we have tocreate the NSS of span[2,3] by concatenating theNSSs of span [2,2] and span [3,3].
Since NSS ofspan [2,2] contains 4 element {?NN?, ?NP?,?VV?, ?VP?}
and NSS of span [3, 3] also con-tains 4 element {?VV?, ?VP?, ?JJ?, ?ADJP?
},NSS of span [2,3] contains 16=4*4 elements.
Tomake the NSS manageable, we prune it with thefollowing thresholds:?
each node sequence should contain lessthan n nodes?
each node sequence set should contain lessthan m node sequences?
sort node sequences according to theirlengths and only keep the k shortest onesEach virtual node is simply labeled by theconcatenation of all its children?s labels asshown in Fig.
7.Algorithm 1. add virtual nodes into forestInput: packed forest F, alignment ANotation:L: length of source sentenceNSS(u,v): the set of node sequences covering span [u,v]VN(ns): virtual father node for node sequence ns.Output: modified forest F with virtual nodes1.
for length := 0 to L - 1 do2.
for start := 1 to L - length do3.
stop := start + length4.
if span[start, stop] covered by tree nodes then5.
for each node n of span [start, stop] do6.
add n into NSS(start, stop)7.          else8.
for pivot := start to stop - 19.                     for each ns1 in NSS(start, pivot) do10.
for each ns2 in NSS(pivot+1, stop) do11.
create ??
??
?1?
?
?2?12.
if ns is not in NSS(start, stop) then13.
add ns into NSS(start, stop)14.                do pruning on NSS(start, stop)15.                if the span[start, stop] is alignable then16.
for each ns of NSS(start, stop) do17.
if node VN(ns) is not in F then18.
add node VN(ns) into F19.
add a hyper-edge h into F,20.
let lhs(h) := VN(ns), rhs(h) := nsAlgorithm 1 outputs a non-complete forest CFwith each alignable span covered by either treenodes or virtual nodes.
Then we can easily ex-tract our rules from the CF using the tree ruleextraction algorithm (Mi and Huang, 2008).Finally, to calculate rule feature probabilitiesfor our model, we need to calculate the fractionalcounts (it is a kind of probability defined in Miand Huang, 2008) of each translation rule in aparse forest.
In the tree case, we can use the in-side-outside-based methods (Mi and Huang2008) to do it.
In the tree sequence case, sincethe previous method cannot be used directly, weprovide another solution by making an indepen-dent assumption that each tree in a tree sequenceis independent to each other.
With this assump-tion, the fractional counts of both tree and treesequence can be calculated as follows:????
?
?????????????????????????
?
?
?????????????????
?
???????????
?
?????????????????
?where ????
is the fractional counts to be calcu-lated for rule r, a frag is either lhs(r) (excludingvirtual nodes and virtual hyper-edges) or any treenode in a forest, TOP is the root of the forest,??.
?
and ??.)
are the outside and inside probabil-ities of nodes, ?????.
?
returns the root nodes of atree sequence fragment, ???????.
?
returns theleaf nodes of a tree sequence fragment, ????
isthe hyper-edge probability.Figure 7.
A virtual node in forest5 DecodingWe benefit from the same strategy as used in ourrule extraction algorithm in designing our decod-ing algorithm, recasting the forest-based tree se-quence-to-string decoding problem as a forest-based tree-to-string decoding problem.
Our de-coding algorithm consists of four steps:1) Convert the complete parse forest to a non-complete one by introducing virtual nodes.1762) Convert the non-complete parse forest intoa translation forest4 ??
by using the translationrules and the pattern-matching algorithm pre-sented in Mi et al (2008).3) Prune out redundant nodes and add auxil-iary hyper-edge into the translation forest forthose nodes that have either no child or no father.By this step, the translation forest ??
becomes acomplete forest.4) Decode the translation forest using ourtranslation model and a dynamic search algo-rithm.The process of step 1 is similar to Algorithm 1except no alignment constraint used here.
Thismay generate a large number of additional virtualnodes; however, all redundant nodes will be fil-tered out in step 3.
In step 2, we employ the tree-to-string pattern match algorithm (Mi et al,2008) to convert a parse forest to a translationforest.
In step 3, all those nodes not covered byany translation rules are removed.
In addition,please note that the translation forest is alreadynot a complete forest due to the virtual nodes andthe pruning of rule-unmatchable nodes.
We,therefore, propose Algorithm 2 to add auxiliaryhyper-edges to make the translation forest com-plete.In Algorithm 2, we travel the forest in bottom-up fashion (line 4-5).
For each span, we do:1) generate all the NSS for this span (line 7-12)2) filter the NSS to a manageable size (line 13)3) add auxiliary hyper-edges for the currentspan (line 15-19) if it can be covered by at leastone single tree node, otherwise go to step 1 .
Thisis the key step in our Algorithm 2.
For each treenode and each node sequences covering the samespan (stored in the current NSS), if the tree nodehas no children or at least one node in the nodesequence has no father, we add an auxiliary hy-per-edge to connect the tree node as father nodewith the node sequence as children.
Since Algo-rithm 2 is DP-based and traverses the forest in abottom-up way, all the nodes in a node sequenceshould already have children node after the lowerlevel process in a small span.
Finally, we re-buildthe NSS of current span for upper level NSScombination use (line 20-22).In Fig.
8, the hyper-edge ?IP=>NP VV+VVNP?
is an auxiliary hyper-edge introduced byAlgorithm 2.
By Algorithm 2, we convert thetranslation forest into a complete translation for-est.
We then use a bottom-up node-based search4 The concept of translation forest is proposed in Mi etal.
(2008).
It is a forest that consists of only the hyper-edges induced from translation rules.algorithm to do decoding on the complete trans-lation forest.
We also use Cube Pruning algo-rithm (Huang and Chiang 2007) to speed up thetranslation process.Figure 8.
Auxiliary hyper-edge in a translationforestAlgorithm 2. add auxiliary hyper-edges into mt forest FInput:  mt forest FOutput: complete forest F with auxiliary hyper-edges1.
for i := 1 to L do2.
for each node n of span [i, i] do3.
add n into NSS(i, i)4. for length := 1 to L - 1 do5.
for start := 1 to L - length do6.
stop := start + length7.
for pivot := start to stop-1 do8.
for each ns1 in NSS (start, pivot) do9.
for each ns2 in NSS (pivot+1,stop) do10.
create ??
??
?1?
?
?2?11.
if ns is not in NSS(start, stop) then12.
add ns into NSS (start, stop)13.           do pruning on NSS(start, stop)14.           if there is tree node cover span [start, stop] then15.
for each tree node n of span [start,stop] do16.
for each ns of NSS(start, stop) do17.
if node n have no children orthere is node in ns with no fatherthen18.
add auxiliary hyper-edge h into F19.
let lhs(h) := n, rhs(h) := ns20.
empty NSS(start, stop)21.          for each node n of span [start, stop] do22.
add n into NSS(start, stop)6 Experiment6.1 Experimental SettingsWe evaluate our method on Chinese-Englishtranslation task.
We use the FBIS corpus as train-ing set, the NIST MT-2002 test set as develop-ment (dev) set and the NIST MT-2003 test set astest set.
We train Charniak?s parser (Charniak2000) on CTB5 to do Chinese parsing, and modi-fy it to output packed forest.
We tune the parseron section 301-325 and test it on section 271-300.
The F-measure on all sentences is 80.85%.A 3-gram language model is trained on the Xin-177hua portion of the English Gigaword3 corpus andthe target side of the FBIS corpus using theSRILM Toolkits (Stolcke, 2002) with modifiedKneser-Ney smoothing (Kenser and Ney, 1995).GIZA++ (Och and Ney, 2003) and the heuristics?grow-diag-final-and?
are used to generate m-to-n word alignments.
For the MER training (Och,2003), Koehn?s MER trainer (Koehn, 2007) ismodified for our system.
For significance test,we use Zhang et al?s implementation (Zhang etal, 2004).
Our evaluation metrics is case-sensitive BLEU-4 (Papineni et al, 2002).For parse forest pruning (Mi et al, 2008), weutilize the Margin-based pruning algorithm pre-sented in (Huang, 2008).
Different from Mi et al(2008) that use a static pruning threshold, ourthreshold is sentence-depended.
For each sen-tence, we compute the Margin between the n-thbest and the top 1 parse tree, then use the Mar-gin-based pruning algorithm presented in(Huang, 2008) to do pruning.
By doing so, wecan guarantee to use at least all the top n bestparse trees in the forest.
However, please notethat even after pruning there is still exponentialnumber of additional trees embedded in the for-est because of the sharing structure of forest.Other parameters are set as follows: maximumnumber of roots in a tree sequence is 3, maxi-mum height of a translation rule is 3, maximumnumber of leaf nodes is 7, maximum number ofnode sequences on each span is 10, and maxi-mum number of rules extracted from one node is10000.6.2 Experimental ResultsWe implement our proposed methods as a gen-eral, configurable platform for syntax-basedSMT study.
Based on this platform, we are ableto easily implement most of the state-of-the-artsyntax-based x-to-string SMT methods via sim-ple parameter setting.
For training, we set forestpruning threshold to 1 best for tree-based me-thods and 100 best for forest-based methods.
Fordecoding, we set:1) TT2S: tree-based tree-to-string model bysetting the forest pruning threshold to 1 best andthe number of sub-trees in a tree sequence to 1.2) TTS2S: tree-based tree-sequence to stringsystem by setting the forest pruning threshold to1 best and the maximum number of sub-trees in atree sequence to 3.3) FT2S: forest-based tree-to-string system bysetting the forest pruning threshold to 500 best,the number of sub-trees in a tree sequence to 1.4) FTS2S: forest-based tree-sequence to stringsystem by setting the forest pruning threshold to500 best and the maximum number of sub-treesin a tree sequence to 3.Model BLEU(%)Moses 25.68TT2S 26.08TTS2S 26.95FT2S 27.66FTS2S 28.83Table 1.
Performance ComparisonWe use the first three syntax-based systems(TT2S, TTS2S, FT2S) and Moses (Koehn et al,2007), the state-of-the-art phrase-based system,as our baseline systems.
Table 1 compares theperformance of the five methods, all of which arefine-tuned.
It shows that:1) FTS2S significantly outperforms (p<0.05)FT2S.
This shows that tree sequence is very use-ful to forest-based model.
Although a forest cancover much more phrases than a single tree does,there are still many non-syntactic phrases thatcannot be captured by a forest due to structuredivergence issue.
On the other hand, tree se-quence is a good solution to non-syntactic trans-lation equivalence modeling.
This is mainly be-cause tree sequence rules are only sensitive toword alignment while tree rules, even extractedfrom a forest (like in FT2S), are also limited bysyntax according to grammar parsing rules.2) FTS2S shows significant performance im-provement (p<0.05) over TTS2S due to the con-tribution of forest.
This is mainly due to the factthat forest can offer very large number of parsetrees for rule extraction and decoder.3) Our model statistically significantly outper-forms all the baselines system.
This clearly de-monstrates the effectiveness of our proposedmodel for syntax-based SMT.
It also shows thatthe forest-based method and tree sequence-basedmethod are complementary to each other and ourproposed method is able to effectively integratetheir strengths.4) All the four syntax-based systems show bet-ter performance than Moses and three of themsignificantly outperforms (p<0.05) Moses.
Thissuggests that syntax is very useful to SMT andtranslation can be viewed as a structure mappingissue as done in the four syntax-based systems.Table 2 and Table 3 report the distribution ofdifferent kinds of translation rules in our model(training forest pruning threshold is set to 100best) and in our decoding (decoding forest prun-ing threshold is set to 500 best) for one besttranslation generation.
From the two tables, wecan find that:178Rule Type Treeto StringTree Sequenceto StringL 4,854,406 20,526,674P 37,360,684 58,826,261U 3,297,302 3,775,734All 45,512,392 83,128,669Table 2.
# of rules extracted from training cor-pus.
L means fully lexicalized, P means partiallylexicalized, U means unlexicalized.Rule Type Treeto StringTree Sequenceto StringL 10,592 1,161P 7,132 742U 4,874 278All 22,598 2,181Table 3.
# of rules used to generate one-besttranslation result in testing1) In Table 2, the number of tree sequencerules is much larger than that of tree rules al-though our rule extraction algorithm only ex-tracts those tree sequence rules over the spansthat tree rules cannot cover.
This suggests thatthe non-syntactic structure mapping is still a bigchallenge to syntax-based SMT.2) Table 3 shows that the tree sequence rulesis around 9% of the tree rules when generatingthe one-best translation.
This suggests thataround 9% of translation equivalences in the testset can be better modeled by tree sequence tostring rules than by tree to string rules.
The 9%tree sequence rules contribute 1.17 BLEU scoreimprovement (28.83-27.66 in Table 1) to FTS2Sover FT2S.3) In Table 3, the fully-lexicalized rules arethe major part (around 60%), followed by thepartially-lexicalized (around 35%) and un-lexicalized (around 15%).
However, in Table 2,partially-lexicalized rules extracted from trainingcorpus are the major part (more than 70%).
Thissuggests that most partially-lexicalized rules areless effective in our model.
This clearly directsour future work in model optimization.BLEU (%)N-best \ model FT2S FTS2S100 Best 27.40 28.61500 Best  27.66 28.832500 Best  27.66 28.965000 Best  27.79 28.89Table 4.
Impact of the forest pruningForest pruning is a key step for forest-basedmethod.
Table 4 reports the performance of thetwo forest-based models using different values ofthe forest pruning threshold for decoding.
Itshows that:1) FTS2S significantly outperforms (p<0.05)FT2S consistently in all test cases.
This againdemonstrates the effectiveness of our proposedmodel.
Even if in the 5000 Best case, tree se-quence is still able to contribute 1.1 BLEU scoreimprovement (28.89-27.79).
It indicates the ad-vantage of tree sequence cannot be covered byforest even if we utilize a very large forest.2) The BLEU scores are very similar to eachother when we increase the forest pruning thre-shold.
Moreover, in one case the performanceeven drops.
This suggests that although moreparse trees in a forest can offer more structureinformation, they may also introduce more noisethat may confuse the decoder.7 ConclusionIn this paper, we propose a forest-based tree-sequence to string translation model to combinethe strengths of forest-based methods and tree-sequence based methods.
This enables our modelto have the great potential to address the issuesof structure divergence and parse errors for syn-tax-based SMT.
We convert our forest-based treesequence rule extraction and decoding issues totree-based by introducing virtual nodes, virtualhyper-edges and auxiliary rules (hyper-edges).
Inour system implementation, we design a generaland configurable platform for our method, basedon which we can easily realize many previoussyntax-based methods.
Finally, we examine ourmethods on the FBIS corpus and the NIST MT-2003 Chinese-English translation task.
Experi-mental results show that our model greatly out-performs the four baseline systems.
Our studydemonstrates that forest-based method and treesequence-based method are complementary toeach other and our proposed method is able toeffectively combine the strengths of the two in-dividual methods for syntax-based SMT.AcknowledgementWe would like to thank Huang Yun for preparingthe pictures in this paper; Run Yan for providingthe java version modified MERT program anddiscussion on the details of MOSES; Mi Haitaofor his help and discussion on re-implementingthe FT2S model; Sun Jun and Xiong Deyi fortheir valuable suggestions.179ReferencesEugene Charniak.
2000.
A maximum-entropy inspiredparser.
NAACL-00.Jason Eisner.
2003.
Learning non-isomorphic treemappings for MT.
ACL-03 (companion volume).Michel Galley, Mark Hopkins, Kevin Knight and Da-niel Marcu.
2004.
What?s in a translation rule?HLT-NAACL-04.
273-280.Liang Huang.
2008.
Forest Reranking: DiscriminativeParsing with Non-Local Features.
ACL-HLT-08.586-594Liang Huang and David Chiang.
2005.
Better k-bestParsing.
IWPT-05.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated languagemodels.
ACL-07.
144?151Liang Huang, Kevin Knight and Aravind Joshi.
2006.Statistical Syntax-Directed Translation with Ex-tended Domain of Locality.
AMTA-06.
(poster)Reinhard Kenser and Hermann Ney.
1995.
Improvedbacking-off for M-gram language modeling.ICASSP-95.
181-184Dan Klein and Christopher D. Manning.
2001.
Pars-ing and Hypergraphs.
IWPT-2001.Philipp Koehn, F. J. Och and D. Marcu.
2003.
Statis-tical phrase-based translation.
HLT-NAACL-03.127-133.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertol-di, Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.ACL-07.
177-180.
(poster)Yang Liu, Qun Liu and Shouxun Lin.
2006.
Tree-to-String Alignment Template for Statistical MachineTranslation.
COLING-ACL-06.
609-616.Yang Liu, Yun Huang, Qun Liu and Shouxun Lin.2007.
Forest-to-String Statistical TranslationRules.
ACL-07.
704-711.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
ACL-HLT-08.
192-199.Haitao Mi and Liang Huang.
2008.
Forest-basedTranslation Rule Extraction.
EMNLP-08.
206-214.Franz J. Och and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statis-tical machine translation.
ACL-02.
295-302.Franz J. Och.
2003.
Minimum error rate training instatistical machine translation.
ACL-03.
160-167.Franz Josef Och and Hermann Ney.
2003.
A Syste-matic Comparison of Various Statistical AlignmentModels.
Computational Linguistics.
29(1) 19-51.Kishore Papineni, Salim Roukos, ToddWard andWei-Jing Zhu.
2002.
BLEU: a method for automat-ic evaluation of machine translation.
ACL-02.
311-318.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
ICSLP-02.
901-904.Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, ShengLi and Chew Lim Tan.
2007.
A Tree-to-TreeAlignment-based Model for Statistical MachineTranslation.
MT-Summit-07.
535-542.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, Sheng Li.
2008a.
A Tree SequenceAlignment-based Tree-to-Tree Translation Model.ACL-HLT-08.
559-567.Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw,Sheng Li.
2008b.
Grammar Comparison Study forTranslational Equivalence Modeling and Statistic-al Machine Translation.
COLING-08.
1097-1104.Ying Zhang, Stephan Vogel, Alex Waibel.
2004.
In-terpreting BLEU/NIST scores: How much im-provement do we need to have a better system?LREC-04.
2051-2054.180
