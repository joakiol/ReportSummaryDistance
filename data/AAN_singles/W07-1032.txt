BioNLP 2007: Biological, translational, and clinical language processing, pages 201?208,Prague, June 2007. c?2007 Association for Computational LinguisticsUnsupervised Learning of the Morpho-Semantic Relationship inMEDLINE?W.
John WilburNational Center for BiotechnologyInformation / National Library ofMedicine, National Institutes ofHealth, Bethesda, MD, U.S.A.wilbur@ncbi.nlm.nih.govAbstractMorphological analysis as applied to Eng-lish has generally involved the study ofrules for inflections and derivations.
Recentwork has attempted to derive such rulesfrom automatic analysis of corpora.
Herewe study similar issues, but in the contextof the biological literature.
We introduce anew approach which allows us to assignprobabilities of the semantic relatedness ofpairs of tokens that occur in text in conse-quence of their relatedness as characterstrings.
Our analysis is based on over 84million sentences that compose the MED-LINE database and over 2.3 million tokentypes that occur in MEDLINE and enablesus to identify over 36 million token typepairs which have assigned probabilities ofsemantic relatedness of at least 0.7 basedon their similarity as strings.1 IntroductionMorphological analysis is an important elementin natural language processing.
Jurafsky andMartin (2000) define morphology as the study ofthe way words are built up from smaller meaningbearing units, called morphemes.
Robust toolsfor morphological analysis enable one to predictthe root of a word and its syntactic class or partof speech in a sentence.
A good deal of work hasbeen done toward the automatic acquisition ofrules, morphemes, and analyses of words fromlarge corpora (Freitag, 2005; Jacquemin, 1997;Monson, 2004; Schone and Jurafsky, 2000;Wicentowski, 2004; Xu and Croft, 1998;Yarowsky and Wicentowski, 2000).
While thiswork is important it is mostly concerned withinflectional and derivational rules that can bederived from the study of texts in a language.While our interest is related to this work, we areconcerned with the multitude of tokens that ap-pear in English texts on the subject of biology.We believe it is clear to anyone who has exam-ined the literature on biology that there are manytokens that appear in textual material that arerelated to each other, but not in any standard wayor by any simple rules that have general applica-bility even in biology.
It is our goal here toachieve some understanding of when two tokenscan be said to be semantically related based ontheir similarity as strings of characters.Thus for us morphological relationship will be abit more general in that we wish to infer the re-latedness of two strings based on the fact thatthey have a certain substring of characters onwhich they match.
But we do not require to sayexactly on what part of the matching substringtheir semantic relationship depends.
In otherwords we do not insist on the identification ofthe smaller meaning bearing units or mor-phemes.
Key to our approach is the ability tomeasure the contextual similarity between twotoken types as well as their similarity as strings.Neither kind of measurement is unique to ourapplication.
Contextual similarity has been stud-ied and applied in morphology (Jacquemin,1997; Schone and Jurafsky, 2000; Xu and Croft,1998; Yarowsky and Wicentowski, 2000) andmore generally (Means and others, 2004).
String201similarity has also received much attention(Adamson and Boreham, 1974; Alberga, 1967;Damashek, 1995; Findler and Leeuwen, 1979;Hall and Dowling, 1980; Wilbur and Kim, 2001;Willett, 1979; Zobel and Dart, 1995).
However,the way we use these two measurements is, toour knowledge, new.
Our approach is based on asimple postulate: If two token types are similaras strings, but they are not semantically relatedbecause of their similarity, then their contextualsimilarity is no greater than would be expectedfor two randomly chosen token types.
Based onthis observation we carry out an analysis whichallows us to assign a probability of relatedness topairs of token types.
This proves sufficient togenerate a large repository of related token typepairs among which are the expected inflectionaland derivationally related pairs and much morebesides.2 MethodologyWe work with a set of 2,341,917 token typeswhich are the unique token types that occurredthroughout MEDLINE in the title and abstract re-cord fields in November of 2006.
These tokentypes do not include a set of 313 token types thatrepresent stop words and are removed from con-sideration.
Our analysis consists of several steps.2.1 Measuring Contextual SimilarityIn considering the context of a token in a MED-LINE record we do not consider all the text ofthe record.
In those cases when there are multi-ple sentences in the record the text that does notoccur in the same sentence as the token may betoo distant to have any direct bearing on the in-terpretation of the token and will in such casesadd noise to our considerations.
Thus we breakthe whole of MEDLINE into sentences and con-sider the context of a token to be the additionaltokens of the sentence in which it occurs.
Like-wise the context of a token type consists of allthe additional token types that occur in all thesentences in which it occurs.
We used our ownsoftware to identify sentence boundaries (unpub-lished), but suspect that published and freelyavailable methods could equally be used for thispurpose.
This produced 84,475,092 sentencesover all of MEDLINE.
While there is an advan-tage in the specificity that comes from consider-ing context at the sentence level, this approachalso gives rise to a problem.
It is not uncommonfor two terms to be related semantically, but tonever occur in the same sentence.
This will hap-pen, for example, if one term is a misspelling ofthe other or if the two terms are alternate namesfor the same object.
Because of this we must es-timate the context of each term without regard tothe occurrence of the other term.
Then the twoestimates can be compared to compute a similar-ity of context.
This we accomplish using formu-las of probability theory applied to our setting.Let T  denote the set of 2,341,917 token typeswe consider and let 1t  and 2t  be two token typeswe wish to compare.
Then we define1 12 2( ) ( | ) ( ) and( ) ( | ) ( )c i Tc i Tp t p t i p ip t p t i p i??==??
.
(1)Here we refer to 1( )cp t  and 2( )cp t  as contextualprobabilities for 1t  and 2t , respectively.
The ex-pressions on the right sides in (1) are given thestandard interpretations.
Thus ( )p i  is the frac-tion of tokens in MEDLINE that are equal to iand 1( | )p t i  is the fraction of sentences inMEDLINE that contain i  that also contain 1t .We make a similar computation for the pair oftoken types1 2 1 21 2( ) ( | ) ( )( | ) ( | ) ( )c i Ti Tp t t p t t i p ip t i p t i p i???
= ?=??
.
(2)Here we have made use of an additional assump-tion, that given i , 1t  and 2t  are independent intheir probability  of occurrence.
While inde-pendence is not true, this seems to be just theright assumption for our purposes.
It allows ourestimate of 1 2( )cp t t?
to be nonzero eventhough 1t  and 2t  may never occur together in asentence.
In other words it allows our estimate toreflect what context would imply if there wereno rule that says the same intended word willalmost never occur twice in a single sentence,202etc.
Our contextual similarity is then the mutualinformation based on contextual probabilities1 21 21 2( )( , ) log( ) ( )cc cp t tconSim t tp t p t?
?
?= ?
??
?
(3)There is one minor practical difficulty with thisdefinition.
There are many cases where 1 2( )cp t t?is zero.
In any such case we define 1 2( , )conSim t tto be -1000.2.2 Measuring Lexical SimilarityHere we treat the two token types, 1t  and 2t  ofthe previous section, as two ASCII strings andask how similar they are as strings.
String simi-larity has been studied from a number of view-points (Adamson and Boreham, 1974; Alberga,1967; Damashek, 1995; Findler and Leeuwen,1979; Hall and Dowling, 1980; Wilbur and Kim,2001; Willett, 1979; Zobel and Dart, 1995).
Weavoided approaches based on edit distance orother measures designed for spell checking  be-cause our problem requires the recognition ofrelationships more distant than simple misspell-ings.
Our method is based on letter ngrams asfeatures to represent any string (Adamson andBoreham, 1974; Damashek, 1995; Wilbur andKim, 2001; Willett, 1979).
If " "t abcdefgh=represents a token type, then we define ( )F t  tobe the feature set associated with t  and we take( )F t  to be composed of i) all the contiguousthree character substrings  ?abc?, ?bcd?, ?cde?,?def?, ?efg?, ?fgh?
; ii) the specially marked firsttrigram " !
"abc ; and iii) the specially markedfirst letter " #"a .
This is the form of ( )F t  forany t  at least three characters long.
If t  consistsof only two characters, say " "ab , we take i)" "ab ; ii) " !
"ab ; and iii) is unchanged.
If t  con-sists of only a single character " "a , we likewisetake i) ?a?
; ii) ?a!?
; and iii) is again unchanged.Here ii) and iii) are included to allow the empha-sis of the beginning of strings as more importantfor their recognition than the remainder.
We em-phasize that ( )F t  is a set of features, not a ?bag-of-words?, and any duplication of features is ig-nored.
While this is a simplification, it does havethe minor drawback that different strings, e.g.," "aaab  and " "aaaaab , can be represented bythe same set of features.Given that each string is represented by a set offeatures, it remains to define how we computethe similarity between two such representations.Our basic assumption here is that the probability2 1( | )p t t , that the semantic implications of 1t  arealso represented at some level in 2t , should berepresented by the fraction of the features repre-senting 1t  that also appear in 2t .
Of course thereis no reason that all features should be consid-ered of equal value.
Let F  denote the set of allfeatures coming from all 2.34 million strings weare considering.
We will make the assumptionthat there exists a set of weights ( )w f  definedover all of f F?
and representing their seman-tic importance.
Then we have( ) ( )1 2 12 1 ( )( | ) ( ) / ( )f F t F t f F tp t t w f w f?
?
?=?
?
.
(4)Based on (4) we define the lexical similarity oftwo token types as1 2 2 1 1 2( , ) ( ( | ) ( | )) / 2lexSim t t p t t p t t= +  (5)In our initial application of lexSim we take asweights the so-called inverse document fre-quency weights that are commonly used in in-formation retrieval (Sparck Jones, 1972).
If2,341,917, N =  the number of token types, andfor any feature f , fn  represents the number oftoken types with the feature f , the inversedocument frequency weight is( ) logfNw fn?
?= ?
??
??
?.
(6)This weight is based on the observation that veryfrequent features tend not to be very important, butimportance increases on the average as frequencydecreases.2.3 Estimating Semantic RelatednessThe first step is to compute the distribution of1 2( , )conSim t t  over a large random sample ofpairs of token types 1t  and 2t .
For this purposewe computed 1 2( , )conSim t t  over a random203sample of 302,515 pairs.
This resulted in thevalue -1000, 180,845 times (60% of values).The remainder of the values, based on nonzero1 2( )cp t t?
are distributed as shown in Figure 1.Let ?
denote the probability density for1 2( , )conSim t t  over random pairs 1t  and 2t .
Let1 2( , )Sem t t  denote the predicate that asserts that 1tand 2t  are semantically related.
Then our mainassumption which underlies the method isPostulate.
For any nonnegative real number r{ }1 2 1 2 1 2( , ) | ( , ) ( , )Q conSim t t lexSim t t r Sem t t= > ?
?
(7)-3 -1 1 3 5 7conSim0200040006000800010000FrequencyDistribution of conSim ValuesFigure 1.
Distribution of  conSim values for the40% of randomly selected token type pairswhich gave values above -1000, i.e., for which1 2( ) 0cp t t?
> .has probability density function equal to ?
.This postulate says that if you have two tokentypes that have some level of similarity as strings( 1 2( , )lexSim t t r> ) but which are not semanticallyrelated, then 1 2( , )lexSim t t r>   is just an accidentand it provides no information about1 2( , )conSim t t .The next step is to consider a pair of real numbers1 20 r r?
<  and the set{ }1 2 1 2 1 1 2 2( , ) ( , ) | ( , )S r r t t r lexSim t t r= ?
<  (8)they define.
We will refer to such a set as a lexSimslice.
According to our postulate the subset of1 2( , )S r r  which are pairs of tokens without a se-mantic relationship will produce conSim valuesobeying the ?
density.
We compute the conSimvalues and assume that all of those pairs that pro-duce a conSim value of -1000 represent pairs thatare unrelated semantically.
As an example, in oneof our computations we computed a slice(0.7,0.725)S  and found the lexSim value -1000produced 931,042 times.
In comparing this withthe random sample which produced 180,845 valuesof -1000, we see that931,042 180,845 5.148=  (9)So we need to multiply the frequency distributionfor the random sample (shown in Figure 1) by5.148 to represent the part of the slice(0.7,0.725)S  that represents pairs not semanticallyrelated.
This situation is illustrated in Figure 2.Two observations are important here.
First, the twocurves match almost perfectly along their leftedges for conSim values below zero.
This suggeststhat sematically related pairs do not produce con-Sim scores below about -1 and adds some credibil-ity to our assumption that semantically relatedpairs do not produce conSim values of -1000.
Thesecond observation is that while the higher graphin Figure 2 represents all pairs in the lexSim sliceand the lower graph all pairs that are not semanti-cally related, we do not know which pairs are notsemantically related.
We can only estimate theprobability of any pair at a particular conSim scorelevel being semantically related.
If we let ?
rep-resent the upper curve coming from the lexSimslice and ?
the lower curve coming from the ran-dom sample, then (10) represents the probability( ) ( )( )( )x xp xx?
?
?= ?
(10)that a token type pair with a conSim score of x  is asemantically related pair.
Curve fitting or regres-sion methods can be used to estimate p .
Since it isreasonable to expect p  to be a nondecreasingfunction of its argument, we use isotonic regres-sion to make our estimates.
For a full analysis weset0.5 0.025ir i= + ?
(11)204and consider the set of lexSim slices { }201 0( , )i i iS r r + =and determine the corresponding set of probabilityfunctions { }200i ip = .2.4 Learned WeightsOur initial step was to use the IDF weights definedin equation (6) and compute a database of all non-identical token type pairs among the 2,341,917token types occurring in MEDLINE for which1 2( , ) 0.5lexSim t t ?
.
We focus on the value 0.5 be-cause the similarity measure lexSim has the-4 -2 0 2 4 6 8 10conSim020000400006000080000FrequencyRandom Sample x 5.148lexSim Slice S(0.7,0.725)Comparison of HistogramsFigure 2.
The distribution based on the random sampleof pairs represents those pairs in the slice that are notsemantically related, while the portion between the twocurves represents the number of semantically relatedpairs.property that if one of 1t  or 2t  is an initial seg-ment of the other (e.g., ?glucuron?
is an initialsegment of ?glucuronidase?)
then1 2( , ) 0.5lexSim t t ?
will be satisfied regardless ofthe set of weights used.
The resulting data in-cluded the lexSim and the conSim scores andconsisted of 141,164,755 pairs.
We performed acomplete slice analysis of this data and based onthe resulting probability estimates 20,681,478pairs among the 141,164,755 total had a prob-ability of being semantically related which wasgreater than or equal to 0.7.
While this seemslike a very useful result, there is reason to be-lieve the IDF weights used to compute lexSimare far from optimal.
In an attempt to improvethe weighting we divided the 141,164,755 pairsinto 1C?
consisting of 68,912,915 pairs with aconSim score of -1000 and 1C  consisting of theremaining 72,251,839 pairs.
Letting wG  denotethe vector of weights we defined a cost function( )( )1 2 11 2 11 2( , )1 2( , )( ) log ( , )log 1 ( , )t t Ct t Cw lexSim t tlexSim t t????
= ?+ ?
??
?G(12)and carried out a minimization of ?
to obtain aset of learned weights which we will denote by0wG .
The minimization was done using the L-BFGS algorithm (Nash and Nocedal, 1991).Since it is important to avoid negative weightswe associate a potential ( )v f  with each ngramfeature f  and set( ) exp( ( ))w f v f= .
(13)The optimization is carried out using the poten-tials.The optimization can be understood as an at-tempt to make lexSim as close to zero as possibleon the large set 1C?
where 1000conSim = ?
andwe have assumed there are no semantically re-lated pairs, while at the same time making lex-Sim large on the remainder.
While this seemsreasonable as a first step it is not conservative asmany pairs in 1C  will not be semantically re-lated.
Because of this we would expect thatthere are ngrams for which we have learnedweights that are not really appropriate outside ofthe set of 141,164,755 pairs on which wetrained.
If there are such, presumably the mostimportant cases would be those where we wouldscore pairs with inappropriately high lexSimscores.
Our approach to correct for this possibil-ity is to add to the initial database of141,164,755 pairs all additional pairs which pro-duced a 1 2( , ) 0.5lexSim t t ?
based on the newweight set 0wG .
This augmented the data to a newset of 223,051,360 pairs with conSim scores.
Wethen applied our learning scheme based onminimization of the function ?
to learn a newset of weights 1wG .
There was one difference.Here and in all subsequent rounds we chose todefine 1C?
as all those pairs with2051 2( , ) 0conSim t t ?
and 1C  those pairs with1 2( , ) 0conSim t t > .
We take this to be a conserva-tive approach as one would expect semanticallyrelated pairs to have a similar context and satisfy1 2( , ) 0conSim t t > and  graphs such as Figure 2support this.
In any case we view this as a con-servative move and calculated to produce fewerfalse positives based on lexSim score recommen-dations of semantic relatedness.
We actually gothrough repeated rounds of training and addingnew pairs to the set of pairs.
This process is con-vergent as we reach a point where the weightslearned on the set of pairs does not result in theaddition of a significant amount of new material.This happened with weight set 4wG  and a totalaccumulation of 440.4 million token type pairs.Table 1.
Number of token pairs and the level oftheir predicted probability of semantic related-ness found with three different weight sets.WeightSetProb.
Se-manticallyRelated0.7?Prob.
Se-manticallyRelated0.8?Prob.
Se-manticallyRelated0.9?4wG36,173,520 22,381,318 10,805,085Constant 34,667,988 20,282,976 8,607,863IDF 31,617,441 18,769,424 8,516,3293 Probability PredictionsBased on the learned weight set 4wG  we per-formed a slice analysis of the 440 million tokenpairs on which the weights were learned and ob-tained a set of 36,173,520 token pairs with pre-dicted probabilities of being semantically relatedof 0.7 or greater.
We performed the same sliceanalysis on this 440 million token pair set withthe IDF weights and the set of constant weightsall equal to 1.
The results are given in Table 1.Here it is interesting to note that the constantweights perform substantially better than the IDFweights and come close to the performance ofthe 4wG  weights.
While the 4wG  predicted about1.5 million more relationships at the 0.7 prob-ability level, it is also interesting to note that thedifference between the 4wG  and constant weightsactually increases as one goes to higher probabil-ity levels so that the learned weights allow us toTable 2.
A table showing 30 out of a total of 379tokens predicted to be semantically related to?lacz?
and the estimated probabilities.
Ten en-tries are from the beginning of the list, ten fromthe middle, and ten from the end.
Breaks wheredata was omitted are marked with asterisks.ProbabilitySemanticRelation Token 1  Token 20.973028 lacz 'lacz0.975617 lacz 010cblacz0.963364 lacz 010cmvlacz0.935771 lacz 07lacz0.847727 lacz 110cmvlacz0.851617 lacz 1716lacz0.90737 lacz 1acz0.9774 lacz 1hsplacz0.762373 lacz 27lacz0.974001 lacz 2hsplacz*** *** ***0.95951 lacz laczalone0.95951 lacz laczalpha0.989079 lacz laczam0.920344 lacz laczam150.903068 lacz laczamber0.911691 lacz laczatttn70.975162 lacz laczbg0.953791 lacz laczbgi0.995333 lacz laczbla0.991714 lacz laczc141*** *** ***0.979416 lacz ul42lacz0.846753 lacz veroicp6lacz0.985656 lacz vglacz10.987626 lacz vm5lacz0.856636 lacz vm5neolacz0.985475 lacz vtkgpedeltab8rlacz0.963028 lacz vttdeltab8rlacz0.993296 lacz wlacz0.990673 lacz xlacz0.946067 lacz zflaczpredict over 2 million more relationships at the0.9 level of reliability.
This is more than a 25%increase at this high reliability level and justifiesthe extra effort in learning the weights.206Table 3.
A table showing 30 out of a total of 96tokens predicted to be semantically related to?nociception?
and the estimated probabilities.Ten entries are from the beginning of the list,ten from the middle, and ten from the end.Breaks where data was omitted are markedwith asterisks.ProbabilitySemanticRelation Token 1  Token 20.727885 nociception actinociception0.90132 nociception actinociceptive0.848615 nociception anticociception0.89437 nociception anticociceptive0.880249 nociception antincociceptive0.82569 nociception antinoceiception0.923254 nociception antinociceptic0.953812 nociception antinociceptin0.920291 nociception antinociceptio0.824706 nociception antinociceptions*** *** ***0.802133 nociception nociceptice0.985352 nociception nociceptin0.940022 nociception nociceptin's0.930218 nociception nociceptine0.944004 nociception nociceptinerg0.882768 nociception nociceptinergic0.975783 nociception nociceptinnh20.921745 nociception nociceptins0.927747 nociception nociceptiometric0.976135 nociception nociceptions*** *** ***0.88983 nociception subnociceptive0.814733 nociception thermoantinociception0.939505 nociception thermonociception0.862587 nociception thermonociceptive0.810878 nociception thermonociceptor0.947374 nociception thermonociceptors0.81756 nociception tyr14nociceptin0.981115 nociception visceronociception0.957359 nociception visceronociceptive0.862587 nociception withnociceptinA sample of the learned relationships based onthe 4wG  weights is contained inTable 2 and Table 3.
The symbol ?lacz?
standsfor a well known and much studied gene in theE.
coli bacterium.
Due to its many uses it hasgiven rise to myriad strings representing differ-ent aspects of molecules, systems, or method-ologies derived from or related to it.
The resultsare not typical of the inflectional or derivationalmethods generally found useful in studying themorphology of English.
Some might representmisspellings, but this is not readily apparent byexamining them.
On the other hand ?nocicep-tion?
is an English word found in a dictionaryand meaning ?a measurable physiological eventof a type usually associated with pain and agonyand suffering?
(Wikepedia).
The data in Table 3shows that ?nociception?
is related to theexpected inflectional and derivational forms,forms with affixes unique to biology, readilyapparent misspellings, and foreign analogs.4 Discussion & ConclusionsThere are several possible uses for the type ofdata produced by our analysis.
Words semanti-cally related to a query term or terms typed by asearch engine user can provide a useful queryexpansion in either an automatic mode or withthe user selecting from a displayed list of optionsfor query expansion.
Many misspellings occur inthe literature and are disambiguated in the tokenpairs produced by the analysis.
They can be rec-ognized as closely related low frequency-highfrequency pairs.
They may allow better curationof the literature on the one hand or improvedspelling correction of user queries on the other.In the area of more typical language analysis, alarge repository of semantically related pairs cancontribute to semantic tagging of text and ulti-mately to better performance on the semanticaspects of parsing.
Also the material we haveproduced can serve as a rich source of morpho-logical information.
For example, inflectionaland derivational transformations applicable tothe technical language of biology are well repre-sented in the data.There is the possibility of improving on themethods we have used, while still applying thegeneral approach.
Either a more sensitive con-Sim or lexSim measure or both could lead to su-perior results.
While it is unclear to us how con-Sim might be improved, it seems there is morepotential with lexSim.
lexSim treats features asbasically independent contributors to the similar-ity of token types and this is not ideal.
For ex-ample the feature ?hiv?
usually refers to the hu-207man immunodeficiency virus.
However, if ?ive?is also a feature of the token we may well bedealing with the word ?hive?
which has no rela-tion to a human immunodeficiency virus.
Thus amore complicated model of the lexical similarityof strings could result in improved recognition ofsemantically related strings.In future work we hope to investigate the applica-tion of the approach we have developed to multi-token terms.
We also hope to investigate the possi-bility of more sensitive lexSim measures for im-proved performance.Acknowledgment This research was supported bythe Intramural Research Program of the National Centerfor Biotechnology Information, National Library ofMedicine, NIH, Bethesda, MD, USA.ReferencesAdamson, G. W., and Boreham, J.
1974.
The use of anassociation measure based on character structure toidentify semantically related pairs of words anddocument titles.
Information Storage and Retrieval,10: 253-260.Alberga, C. N. 1967.
String similarity and misspellings.Communications of the ACM, 10: 302-313.Damashek, M. 1995.
Gauging similarity with n-grams:Language-independent categorization of text.
Sci-ence, 267: 843-848.Findler, N. V., and Leeuwen, J. v. 1979.
A family ofsimilarity measures between two strings.
IEEETransactions on Pattern Analysis and Machine Intel-ligence, PAMI-1: 116-119.Freitag, D. 2005.
Morphology Induction From TermClusters, 9th Conference on Computational NaturalLanguage Learning (CoNLL): Ann Arbor, Michigan,Association for Computational Linguistics.Hall, P. A., and Dowling, G. R. 1980.
Approximatestring matching.
Computing Surveys, 12: 381-402.Jacquemin, C. 1997.
Guessing morphology from termsand corpora, in Belkin, N. J., Narasimhalu, A. D.,and Willett, P., editors, 20th Annual InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval: Philadelphia, PA,ACM Press, p. 156-165.Jurafsky, D., and Martin, J. H. 2000.
Speech and Lan-guage Processing: Upper Saddle River, New Jersey,Prentice Hall.Means, R. W., Nemat-Nasser, S. C., Fan, A. T., andHecht-Nielsen, R. 2004.
A Powerful and GeneralApproach to Context Exploitation in Natural Lan-guage Processing, HLT-NAACL 2004: Workshop onComputational Lexical Semantics Boston, Massachu-setts, USA, Association for Computational Linguis-tics.Monson, C. 2004.
A framework for unsupervised natu-ral language morphology induction, Proceedings ofthe ACL 2004 on Student research workshop: Barce-lona, Spain, Association for Computational Linguis-tics.Nash, S. G., and Nocedal, J.
1991.
A numerical study ofhte limited memory BFGS method and hte truncated-Newton method for large scale optimization.
SIAMJournal of Optimization, 1: 358-372.Schone, P., and Jurafsky, D. 2000.
Knowledge-free in-duction of morphology using latent semantic analy-sis, Proceedings of the 2nd workshop on Learninglanguage in logic and the 4th conference on Compu-tational natural language learning - Volume 7: Lis-bon, Portugal, Association for Computational Lin-guistics.Sparck Jones, K. 1972.
A statistical interpretation ofterm specificity and its application in retrieval.
TheJournal of Documentation, 28: 11-21.Wicentowski, R. 2004.
Multilingual Noise-Robust Su-pervised Morphological Analysis using the Word-Frame Model, SIGPHON: Barcelona, Spain, Asso-ciation for Computational Linguistics.Wilbur, W. J., and Kim, W. 2001.
Flexible phrase basedquery handling algorithms, in Aversa, E., and Man-ley, C., editors, Proceedings of the ASIST 2001 An-nual Meeting: Washington, D.C., Information Today,Inc., p. 438-449.Willett, P. 1979.
Document retrieval experiments usingindexing vocabularies of varying size.
II.
Hashing,truncation, digram and trigram encoding of indexterms.
Journal of Documentation, 35: 296-305.Xu, J., and Croft, W. B.
1998.
Corpus-based stemmingusing cooccurrence of word variants.
ACM TOIS,16: 61-81.Yarowsky, D., and Wicentowski, R. 2000.
Minimallysupervised morphological analysis by multimodalalignment, Proceedings of the 38th Annual Meetingon Association for Computational Linguistics: HongKong, Association for Computational Linguistics.Zobel, J., and Dart, P. 1995.
Finding approximatematches in large lexicons.
Software-Practice and Ex-perience, 25: 331-345.208
