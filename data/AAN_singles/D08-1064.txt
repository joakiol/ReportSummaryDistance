Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 610?619,Honolulu, October 2008. c?2008 Association for Computational LinguisticsDecomposability of Translation Metricsfor Improved Evaluation and Efficient AlgorithmsDavid Chiang and Steve DeNeefeInformation Sciences InstituteUniversity of Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292 USA{chiang,sdeneefe}@isi.eduYee Seng Chan and Hwee Tou NgDepartment of Computer ScienceNational University of SingaporeLaw LinkSingapore 117590{chanys,nght}@comp.nus.edu.sgAbstractB???
is the de facto standard for evaluationand development of statistical machine trans-lation systems.
We describe three real-worldsituations involving comparisons between dif-ferent versions of the same systems where onecan obtain improvements in B???
scores thatare questionable or even absurd.
These situ-ations arise because B???
lacks the propertyof decomposability, a property which is alsocomputationally convenient for various appli-cations.
We propose a very conservative modi-fication to B???
and a cross between B???
andword error rate that address these issues whileimproving correlation with human judgments.1 IntroductionB???
(Papineni et al, 2002) was one of the first au-tomatic evaluation metrics for machine translation(MT), and despite being challenged by a numberof alternative metrics (Melamed et al, 2003; Baner-jee and Lavie, 2005; Snover et al, 2006; Chan andNg, 2008), it remains the standard in the statisticalMT literature.
Callison-Burch et al (2006) have sub-jected B???
to a searching criticism, with two real-world case studies of significant failures of corre-lation between B???
and human adequacy/fluencyjudgments.
Both cases involve comparisons betweenstatistical MT systems and other translation meth-ods (human post-editing and a rule-based MT sys-tem), and they recommend that the use of B???
berestricted to comparisons between related systems ordifferent versions of the same systems.
In B???
?s de-fense, comparisons between different versions of thesame system were exactly what B???
was designedfor.However, we show that even in such situations,difficulties with B???
can arise.
We illustrate threeways that properties of B???
can be exploited toyield improvements that are questionable or evenabsurd.
All of these scenarios arose in actual prac-tice and involve comparisons between different ver-sions of the same statistical MT systems.
They canbe traced to the fact that B???
is not decomposableat the sentence level: that is, it lacks the propertythat improving a sentence in a test set leads to anincrease in overall score, and degrading a sentenceleads to a decrease in the overall score.
This prop-erty is not only intuitive, but also computationallyconvenient for various applications such as transla-tion reranking and discriminative training.
We pro-pose a minimal modification to B???
that reducesits nondecomposability, as well as a cross betweenB???
and word error rate (WER) that is decompos-able down to the subsentential level (in a sense to bemade more precise below).
Both metrics correct theobserved problems and correlate with human judg-ments better than B??
?.2 The B???
metricLet gk(w) be the multiset of all k-grams of a sentencew.
We are given a sequence of candidate translationsc to be scored against a set of sequences of referencetranslations, {r j} = r1, .
.
.
, rR:c = c1, c2, c3, .
.
.
, cNr1 = r11, r12, r13, .
.
.
, r1N...rR = rR1 , rR2 , rR3 , .
.
.
, rRN610Then the B???
score of c is defined to beB???
(c, {r j}) =4?k=1prk(c, {rj})14 ?
bp(c, {r j}) (1)where1prk(c, {rj}) =?i???
?gk(ci) ?
?j gk(rji )????
?i |gk(ci)|(2)is the k-gram precision of c with respect to {r j}, andbp(c, r), known as the brevity penalty, is defined asfollows.
Let ?
(x) = exp(1 ?
1/x).
In the case of asingle reference r,bp(c, r) = ?
(min{1,?i |ci|?i |ri|})(3)In the multiple-reference case, the length |ri| is re-placed with an effective reference length, which canbe calculated in several ways.?
In the original definition (Papineni et al, 2002),it is the length of the reference sentence whoselength is closest to the test sentence.?
In the NIST definition, it is the length of theshortest reference sentence.?
A third possibility would be to take the averagelength of the reference sentences.The purpose of the brevity penalty is to preventa system from generating very short but precisetranslations, and the definition of effective referencelength impacts how strong the penalty is.
The NISTdefinition is the most tolerant of short translationsand becomes more tolerant with more reference sen-tences.
The original definition is less tolerant buthas the counterintuitive property that decreasing thelength of a test sentence can eliminate the brevitypenalty.
Using the average reference length seemsattractive but has the counterintuitive property that1We use the following definitions about multisets: if X is amultiset, let #X(a) be the number of times a occurs in X.
Then:|X| ?
?a#X(a)#X?Y (a) ?
min{#X(a), #Y (a)}#X?Y (a) ?
max{#X(a), #Y (a)}an exact match with one of the references may notget a 100% score.
Throughout this paper we use theNIST definition, as it is currently the definition mostused in the literature and in evaluations.The brevity penalty can also be seen as a stand-in for recall.
The fraction?i |ci |?i |ri |in the definition ofthe brevity penalty (3) indeed resembles a weak re-call score in which every guessed item counts as amatch.
However, with recall, the per-sentence score|ci ||ri |would never exceed unity, but with the brevitypenalty, it can.
This means that if a system generatesa long translation for one sentence, it can generatea short translation for another sentence without fac-ing a penalty.
This is a serious weakness in the B??
?metric, as we demonstrate below using three scenar-ios, encountered in actual practice.3 Exploiting the B???
metric3.1 The sign testWe are aware of two methods that have been pro-posed for significance testing with B???
: bootstrapresampling (Koehn, 2004b; Zhang et al, 2004) andthe sign test (Collins et al, 2005).
In bootstrap re-sampling, we sample with replacement from the testset to synthesize a large number of test sets, andthen we compare the performance of two systems onthose synthetic test sets to see whether one is better95% (or 99%) of the time.
But Collins et al (2005)note that it is not clear whether the conditions re-quired by bootstrap resampling are met in the case ofB??
?, and recommend the sign test instead.
Supposewe want to determine whether a set of outputs c froma test system is better or worse than a set of baselineoutputs b.
The sign test requires a function f (bi, ci)that indicates whether ci is a better, worse, or same-quality translation relative to bi.
However, becauseB???
is not defined on single sentences, Collins etal.
use an approximation: for each i, form a compos-ite set of outputs b?
= {b1, .
.
.
, bi?1, ci, bi+1, .
.
.
, bN},and compare the B???
scores of b and b?.The goodness of this approximation depends onto what extent the comparison between b and b?
isdependent only on bi and ci, and independent of theother sentences.
However, B???
scores are highlycontext-dependent: for example, if the sentences inb are on average  words longer than the referencesentences, then ci can be as short as (N ?
1) words611shorter than ri without incurring the brevity penalty.Moreover, since the ci are substituted in one at atime, we can do this for all of the ci.
Hence, c couldhave a disastrously low B???
score (because of thebrevity penalty) yet be found by the sign test to besignificantly better than the baseline.We have encountered this situation in practice:two versions of the same system with B???
scores of29.6 (length ratio 1.02) and 29.3 (length ratio 0.97),where the sign test finds the second system to be sig-nificantly better than the first (and the first systemsignificantly better than the second).
Clearly, in or-der for a significance test to be sensible, it should notcontradict the observed scores, and should certainlynot contradict itself.
In the rest of this paper, exceptwhere indicated, all significance tests are performedusing bootstrap resampling.3.2 Genre-specific trainingFor several years, much statistical MT research hasfocused on translating newswire documents.
Onelikely reason is that the DARPA TIDES programused newswire documents for evaluation for severalyears.
But more recent evaluations have includedother genres such as weblogs and conversation.
Theconventional wisdom has been that if one uses asingle statistical translation system to translate textfrom several different genres, it may perform poorly,and it is better to use several systems optimized sep-arately for each genre.However, if our task is to translate documentsfrom multiple known genres, but they are evaluatedtogether, the B???
metric allows us to use that factto our advantage.
To understand how, notice thatour system has an optimal number of words that itshould generate for the entire corpus: too few and itwill be penalized by B???
?s brevity penalty, and toomany increases the risk of additional non-matchingk-grams.
But these words can be distributed amongthe sentences (and genres) in any way we like.
In-stead of translating sentences from each genre withthe best genre-specific systems possible, we cangenerate longer outputs for the genre we have moreconfidence in, while generating shorter outputs forthe harder genre.
This strategy will have mediocreperformance on each individual genre (according toboth intuition and B???
), yet will receive a higherB???
score on the combined test set than the com-bined systems optimized for each genre.In fact, knowing which sentence is in which genreis not even always necessary.
In one recent task,we translated documents from two different genres,without knowing the genre of any given sentence.The easier genre, newswire, also tended to haveshorter reference sentences (relative to the sourcesentences) than the harder genre, weblogs.
For ex-ample, in one dataset, the newswire reference setshad between 1.3 and 1.37 English words per Ara-bic word, but the weblog reference set had 1.52 En-glish words per Arabic word.
Thus, a system thatis uniformly verbose across both genres will appor-tion more of its output to newswire than to weblogs,serendipitously leading to a higher score.
This phe-nomenon has subsequently been observed by Och(2008) as well.We trained three Arabic-English syntax-basedstatistical MT systems (Galley et al, 2004; Galleyet al, 2006) using max-B???
training (Och, 2003):one on a newswire development set, one on a we-blog development set, and one on a combined devel-opment set containing documents from both genres.We then translated a new mixed-genre test set in twoways: (1) each document with its appropriate genre-specific system, and (2) all documents with the sys-tem trained on the combined (mixed-genre) devel-opment set.
In Table 3, we report the results of bothapproaches on the entire test dataset as well as theportion of the test dataset in each genre, for both thegenre-specific and mixed-genre trainings.The genre-specific systems each outperform themixed system on their own genre as expected, butwhen the same results are combined, the mixed sys-tem?s output is a full B???
point higher than the com-bination of the genre-specific systems.
This is be-cause the mixed system produces outputs that haveabout 1.35 English words per Arabic word on av-erage: longer than the shortest newswire references,but shorter than the weblog references.
The mixedsystem does worse on each genre but better on thecombined test set, whereas, according to intuition,a system that does worse on the two subsets shouldalso do worse on the combined test set.3.3 Word deletionA third way to take advantage of the B???
metricis to permit an MT system to delete arbitrary words612in the input sentence.
We can do this by introduc-ing new phrases or rules into the system that matchwords in the input sentence but generate no output;to these rules we attach a feature whose weight istuned during max-B???
training.
Such rules havebeen in use for some time but were only recentlydiscussed by Li et al (2008).When we add word-deletion rules to our MT sys-tem, we find that the B???
increases significantly(Table 6, line 2).
Figure 1 shows some examplesof deletion in Chinese-English translation.
The firstsentence has a proper name,?<[[/maigesaisai?Magsaysay?, which has been mistokenized into fourtokens.
The baseline system attempts to translate thefirst two phonetic characters as ?wheat Georgia,?whereas the other system simply deletes them.
Onthe other hand, the second sentence shows how worddeletion can sacrifice adequacy for the sake of flu-ency, and the third sentence shows that sometimesword deletion removes words that could have beentranslated well (as seen in the baseline translation).Does B???
reward word deletion fairly?
We notetwo reasons why word deletion might be desirable.First, some function words should truly be deleted:for example, the Chinese particle?/de and Chinesemeasure words often have no counterpart in English(Li et al, 2008).
Second, even content word deletionmight be helpful if it allows a more fluent translationto be assembled from the remnants.
We observe thatin the above experiment, word deletion caused theabsolute number of k-gram matches, and not just k-gram precision, to increase for all 1 ?
k ?
4.Human evaluation is needed to conclusively de-termine whether B???
rewards deletion fairly.
But tocontrol for these potentially positive effects of dele-tion, we tested a sentence-deletion system, whichis the same as the word-deletion system but con-strained to delete all of the words in a sentence ornone of them.
This system (Table 6, line 3) deleted8?10% of its input and yielded a B???
score withno significant decrease (p ?
0.05) from the base-line system?s.
Given that our model treats sentencesindependently, so that it cannot move informationfrom one sentence to another, we claim that dele-tion of nearly 10% of the input is a grave translationdeficiency, yet B???
is insensitive to it.What does this tell us about word deletion?
Whileacknowledging that some word deletions can im-prove translation quality, we suggest in addition thatbecause word deletion provides a way for the systemto translate the test set selectively, a behavior whichwe have shown that B???
is insensitive to, part ofthe score increase due to word deletion is likely anartifact of B??
?.4 Other metricsAre other metrics susceptible to the same problemsas the B???
metric?
In this section we examine sev-eral other popular metrics for these problems, pro-pose two of our own, and discuss some desirablecharacteristics for any new MT evaluation metric.4.1 Previous metricsWe ran a suite of other metrics on the above problemcases to see whether they were affected.
In none ofthese cases did we repeat minimum-error-rate train-ing; all these systems were trained using max-B??
?.The metrics we tested were:?
METEOR (Banerjee and Lavie, 2005), version0.6, using the exact, Porter-stemmer, andWord-Net synonmy stages, and the optimized param-eters ?
= 0.81, ?
= 0.83, ?
= 0.28 as reportedin (Lavie and Agarwal, 2007).?
GTM (Melamed et al, 2003), version 1.4, withdefault settings, except e = 1.2, following theWMT 2007 shared task (Callison-Burch et al,2007).?
M??S??
(Chan and Ng, 2008), more specifi-cally M??S?
?n, which skips the dependency re-lations.On the sign test (Table 2), all metrics found sig-nificant differences consistent with the difference inscore between the two systems.
The problem relatedto genre-specific training does not seem to affect theother metrics (see Table 4), but they still manifestthe unintuitive result that genre-specific training issometimes worse than mixed-genre training.
Finally,all metrics but GTM disfavored both word deletionand sentence deletion (Table 7).4.2 Strict brevity penaltyA very conservative way of modifying the B???
met-ric to combat the effects described above is to im-613(a) source 9]???
?<[[Vreference fei xiaotong awarded magsaysay prizebaseline fei xiaotong was awarded the wheat georgia xaixai prizedelete fei xiaotong was awarded xaixai award(b) source ???c-/ EAp?-N?q??H??
?areference the center of the yuhua stone bears an image which very much resembles the territoryof the people ?s republic of china .baseline rain huashi center is a big clear images of chinese territory .delete rain is a clear picture of the people ?s republic of china .
(c) source ?
?:F?DRw ??
?reference urban construction becomes new hotspot for foreign investment in qinghaibaseline urban construction become new hotspot for foreign investment qinghaidelete become new foreign investment hotspotFigure 1: Examples of word deletion.
Underlined Chinese words were deleted in the word-deletion system; underlinedEnglish words correspond to deleted Chinese words.pose a stricter brevity penalty.
In Section 2, we pre-sented the brevity penalty as a stand-in for recall,but noted that unlike recall, the per-sentence score|ci ||ri |can exceed unity.
This suggests the simple fix ofclipping the per-sentence recall scores in a similarfashion to the clipping of precision scores:bp(c, r) = ?
(?i min {|ci|, |ri|}?i |ri|)(4)Then if a translation system produces overlongtranslations for some sentences, it cannot use thosetranslations to license short translations for othersentences.
Call this revised metric B???-???
(forB???
with strict brevity penalty).We can test this revised definition on the prob-lem cases described above.
Table 2 shows that B???-???
resolves the inconsistency observed betweenB???
and the sign test, using the example test setsfrom Section 3.1 (no max-B???-???
training was per-formed).
Table 5 shows the new scores of the mixed-genre example from Section 3.2 after max-B???-???training.
These results fall in line with intuition?tuning separately for each genre leads to slightlybetter scores in all cases.
Finally, Table 8 shows theB???-???
scores for the word-deletion example fromSection 3.3, using both max-B???
training and max-B???-???
training.
We see that B???-???
reduces thebenefit of word deletion to an insignificant level onthe test set, and severely punishes sentence deletion.When we retrain using max-B???-??
?, the rate ofword deletion is reduced and sentence deletion is allbut eliminated, and there are no significant differ-ences on the test set.4.3 4-gram recognition rateAll of the problems we have examined?except forword deletion?are traceable to the fact that B??
?is not a sentence-level metric.
Any metric whichis defined as a weighted average of sentence-levelscores, where the weights are system-independent,will be immune to these problems.
Note that anymetric involving micro-averaged precision (in whichthe sentence-level counts of matches and guessesare summed separately before forming their ratio)cannot have this property.
Of the metrics surveyedin the WMT 2007 evaluation-evaluation (Callison-Burch et al, 2007), at least the following metricshave this property: WER (Nie?en et al, 2000), TER(Snover et al, 2006), and ParaEval-Recall (Zhou etal., 2006).Moreover, this evaluation concern dovetails witha frequent engineering concern, that sentence-levelscores are useful at various points in the MTpipeline: for example, minimum Bayes risk de-coding (Kumar and Byrne, 2004), selecting ora-cle translations for discriminative reranking (Liang614et al, 2006; Watanabe et al, 2007), and sentence-by-sentence comparisons of outputs during erroranalysis.
A variation on B???
is often used forthese purposes, in which the k-gram precisions are?smoothed?
by adding one to the numerator and de-nominator (Lin and Och, 2004); this addresses theproblem of a zero k-gram match canceling out theentire score, but it does not address the problems il-lustrated above.The remaining issue, word deletion, is more dif-ficult to assess.
It could be argued that part of thegain due to word deletion is caused by B???
allow-ing a system to selectively translate those parts ofa sentence on which higher precision can be ob-tained.
It would be difficult indeed to argue that anevaluation metric, in order to be fair, must be de-composable into subsentential scores, and we makeno such claim.
However, there is again a dovetail-ing engineering concern which is quite legitimate.
Ifone wants to select the minimum-Bayes-risk trans-lation from a lattice (or shared forest) instead of ann-best list (Tromble et al, 2008), or to select an or-acle translation from a lattice (Tillmann and Zhang,2006; Dreyer et al, 2007; Leusch et al, 2008), or toperform discriminative training on all the examplescontained in a lattice (Taskar et al, 2004), one wouldneed a metric that can be calculated on the edges ofthe lattice.Of the metrics surveyed in the WMT 2007evaluation-evaluation, only one metric, to ourknowledge, has this property: word error rate(Nie?en et al, 2000).
Here, we deal with the relatedword recognition rate (McCowan et al, 2005),WRR = 1 ?
WER= 1 ?
minI + D + S|r|= maxM ?
I|r|(5)where I is the number of insertions, D of deletions,S of substitutions, and M = |r| ?
D ?
S the numberof matches.
The dynamic program for WRR can beformulated as a Viterbi search through a finite-stateautomaton: given a candidate sentence c and a refer-ence sentence r, find the highest-scoring path match-ing c through the automaton with states 0, .
.
.
, |r|,initial state 0, final state |r|, and the following transi-tions (a ?
matches any symbol):For 0 ?
i < |r|:iri+1:1?????
i + 1 matchi:0???
i + 1 deletioni?:0???
i + 1 substitutionFor 0 ?
i ?
|r|:i?:?1????
i insertionThis automaton can be intersected with a typicalstack-based phrase-based decoder lattice (Koehn,2004a) or CKY-style shared forest (Chiang, 2007)in much the same way that a language model can,yielding a polynomial-time algorithm for extractingthe best-scoring translation from a lattice or forest(Wagner, 1974).
Intuitively, the reason for this isthat WRR, like most metrics, implicitly constructsa word alignment between c and r and only countsmatches between aligned words; but unlike othermetrics, this alignment is constrained to be mono-tone.We can combine WRR with the idea of k-grammatching in B???
to yield a new metric, the 4-gramrecognition rate:4-GRR = max?4k=1 Mk ?
?I ?
?D?4k=1 |gk(r)|(6)where Mk is the number of k-gram matches, ?
and ?control the penalty for insertions and deletions, andgk is as defined in Section 2.
We presently set ?
=1, ?
= 0 by analogy with WRR, but explore othersettings below.
To calculate 4-GRR on a whole testset, we sum the numerators and denominators as inmicro-averaged recall.The 4-GRR can also be formulated as a finite-state automaton, with states {(i,m) | 0 ?
i ?
|r|, 0 ?m ?
3}, initial state (0, 0), final states (|r|,m), and thefollowing transitions:For 0 ?
i < |r|, 0 ?
m ?
3:(i,m)ri+1:m+1???????
(i + 1,min{m + 1, 3}) match(i,m):??????
(i + 1, 0) deletion(i,m)?:0???
(i + 1, 0) substitution615Metric Adq Flu Rank Con AvgSem.
role overlap 77.4 83.9 80.3 74.1 78.9ParaEval recall 71.2 74.2 76.8 79.8 75.5METEOR 70.1 71.9 74.5 66.9 70.9B???
68.9 72.1 67.2 60.2 67.1WER 51.0 54.2 34.5 52.4 48.0B???-???
73.9 76.7 73.5 63.4 71.94-GRR 72.3 75.5 74.3 64.2 71.6Table 1: Our new metrics correlate with human judg-ments better than B???
(case-sensitive).
Adq =Adequacy,Flu = Fluency, Con = Constituent, Avg = Average.For 0 ?
i ?
|r|, 0 ?
m ?
3:(i,m)?:??????
(i, 0) insertionTherefore 4-GRR can also be calculated efficientlyon lattices or shared forests.We did not attempt max-4-GRR training, but weevaluated the word-deletion test sets obtained bymax-B???
and max-B???-???
training using 4-GRR.The results are shown in Table 7.
In general, the re-sults are very similar to B???-???
except that 4-GRRsometimes scores word deletion slightly lower thanbaseline.5 Correlation with human judgmentsThe shared task of the 2007 Workshop on StatisticalMachine Translation (Callison-Burch et al, 2007)was conducted with several aims, one of whichwas to measure the correlation of several automaticMT evaluation metrics (including B???)
against hu-man judgments.
The task included two datasets (onedrawn from the Europarl corpus and the other fromthe News Commentary corpus) and across three lan-guage pairs (from German, Spanish, and French toEnglish, and back).
In our experiments, we focus onthe tasks where the target language is English.For human evaluations of the MT submissions,four different criteria were used:?
Adequacy: how much of the meaning ex-pressed in the reference translation is also ex-pressed in the hypothesis translation.?
Fluency: how well the translation reads in thetarget language.?
Rank: each translation is ranked from best toworst, relative to the other translations of thesame sentence.?
Constituent: constituents are selected fromsource-side parse-trees, and human judges areasked to rank their translations.We scored the workshop shared task submissionswith B???-???
and 4-GRR, then converted the rawscores to rankings and calculated the Spearman cor-relations with the human judgments.
Table 1 showsthe results along with B???
and the three metrics thatachieved higher correlations than B???
: semanticrole overlap (Gime?nez and Ma?rquez, 2007), ParaE-val recall (Zhou et al, 2006), and METEOR (Baner-jee and Lavie, 2005).
We find that both our proposedmetrics correlate with human judgments better thanB???
does.However, recall the parameters ?
and ?
in the def-inition of 4-GRR that control the penalty for insertedand deleted words.
Experimenting with this param-eter reveals that ?
= ?0.9, ?
= 1 yields a corre-lation of 78.9%.
In other words, a metric that un-boundedly rewards spuriously inserted words corre-lates better with human judgments than a metric thatpunishes them.
We assume this is because there arenot enough data points (systems) in the sample andask that all these figures be taken with a grain of salt.As a general remark, it may be beneficial for human-correlation datasets to include a few straw-man sys-tems that have very short or very long translations.6 ConclusionWe have described three real-world scenarios in-volving comparisons between different versionsof the same statistical MT systems where B??
?gives counterintuitive results.
All these issues centeraround the issue of decomposability: the sign testfails because substituting translations one sentenceat a time can improve the overall score yet substitut-ing them all at once can decrease it; genre-specifictraining fails because improving the score of twohalves of a test set can decrease the overall score;and sentence deletion is not harmful because gener-ating empty translations for selected sentences doesnot necessarily decrease the overall score.We proposed a minimal modification to B??
?,called B???-??
?, and showed that it ameliorates these616problems.
We also proposed a metric, 4-GRR, that isdecomposable at the sentence level and is thereforeguaranteed to solve the sign test, genre-specific tun-ing, and sentence deletion problems; moreoever, it isdecomposable at the subsentential level, which haspotential implications for evaluating word deletionand promising applications to translation rerankingand discriminative training.AcknowledgmentsOur thanks go to Daniel Marcu for suggesting modi-fying the B???
brevity penalty, and to Jonathan Mayand Kevin Knight for their insightful comments.This research was supported in part by DARPA grantHR0011-06-C-0022 under BBN Technologies sub-contract 9500008412.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with improvedcorrelation with human judgments.
In Proc.
Workshopon Intrinsic and Extrinsic Evaluation Measures for MTand/or Summarization, pages 65?72.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the role of B???
in ma-chine translation research.
In Proc.
EACL 2006, pages249?256.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2007.
(Meta-)evaluation of machine translation.
In Proc.
SecondWorkshop on Statistical Machine Translation, pages136?158.Yee Seng Chan and Hwee Tou Ng.
2008.
M??S??
:A maximum similarity metric for machine translationevaluation.
In Proc.
ACL-08: HLT, pages 55?62.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Proc.
ACL 2005, pages 531?540.Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.2007.
Comparing reordering constraints for SMT us-ing efficient B???
oracle computation.
In Proc.
2007Workshop on Syntax and Structure in Statistical Trans-lation, pages 103?110.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Proc.HLT-NAACL 2004, pages 273?280.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.COLING-ACL 2006, pages 961?968.Jesu?s Gime?nez and Llu?
?s Ma?rquez.
2007.
Linguistic fea-tures for automatic evaluation of heterogeneous MTsystems.
In Proc.
Second Workshop on Statistical Ma-chine Translation, pages 256?264.Philipp Koehn.
2004a.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
In Proc.
AMTA 2004, pages 115?124.Philipp Koehn.
2004b.
Statistical significance testsfor machine translation evaluation.
In Proc.
EMNLP2004, pages 388?395.Shankar Kumar and William Byrne.
2004.
MinimumBayes-risk decoding for statistical machine transla-tion.
In Proc.
HLT-NAACL 2004, pages 169?176.Alon Lavie and Abhaya Agarwal.
2007.
METEOR: Anautomatic metric for MT evaluation with high levelsof correlation with human judgments.
In Proc.
SecondWorkshop on Statistical Machine Translation, pages228?231.Gregor Leusch, Evgeny Matusov, and Hermann Ney.2008.
Complexity of finding the BLEU-optimal hy-pothesis in a confusion network.
In Proc.
EMNLP2008.
This volume.Chi-Ho Li, Dongdong Zhang, Ming Zhou, and HaileiZhang.
2008.
An empirical study in source worddeletion for phrase-based statistical machine transla-tion.
In Proc.
Third Workshop on Statistical MachineTranslation, pages 1?8.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminative ap-proach to machine translation.
In Proc.
COLING-ACL2006, pages 761?768.Chin-Yew Lin and Franz Josef Och.
2004.
ORANGE: amethod for evaluating automatic evaluation metrics formachine translation.
In Proc.
COLING 2004, pages501?507.Iaian McCowan, Darren Moore, John Dines, DanielGatica-Perez, Mike Flynn, Pierre Wellner, and Herve?Bourlard.
2005.
On the use of information retrievalmeasures for speech recognition evaluation.
ResearchReport 04-73, IDIAP Research Institute.I.
Dan Melamed, Ryan Green, and Joseph P. Turian.2003.
Precision and recall of machine translation.
InProc.
HLT-NAACL 2003, pages 61?63.
Companionvolume.Sonia Nie?en, Franz Josef Och, Gregor Leusch, and Her-mann Ney.
2000.
An evaluation tool for machinetranslation: Fast evaluation for MT research.
In Proc.LREC 2000, pages 39?45.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proc.
ACL 2003,pages 160?167.617sys B???
B???-???
METEOR GTM M??S?
?1 29.6++ 28.0 53.1++ 45.5++ 40.7++2 29.3++ 27.8 52.2??
44.8??
39.6?
?Table 2: The sign test yields inconsistent results withB???
but not with other metrics.
Significances are rela-tive to other system.mixed-genre genre-specifictest set B???
length B???
length ?B??
?nw 47.9 1.14 51.1 0.98 +3.2web 16.3 0.87 16.8 0.95 +0.5nw+web 31.5 0.97 30.4 0.96 ?1.1Table 3: When performing two genre-specific max-B??
?trainings instead of a single mixed-genre training, we ex-pect that improvements in the newswire (nw) and websubsets should result in a similar improvement in thecombined test set (nw+web), but this is not the case.
Key:length = length ratio relative to effective reference length.test set ?METEOR ?GTM ?M??S?
?nw ?2.2 ?1.3 ?2.8web +0.8 +0.7 +1.3nw+web ?0.7 ?0.6 ?0.2Table 4: Contradictory effects of genre-specific trainingwere not observed with other metrics.mixed-genre genre-specifictest set B???-???
B???-???
?B???-??
?nw 49.6 49.9 +0.3web 15.3 15.7 +0.4nw+web 29.3 29.5 +0.2Table 5: When performing two genre-specific max-B???-???
trainings instead of a single mixed-genre training, wefind as expected that improvements in the newswire (nw)and web subsets correlate with a similar improvement inthe combined test set (nw+web).dev testdeletion del% B???
del% B??
?none 0 37.7 0 39.3word 8.4 38.6++ 7.7 40.1++sentence 10.2 37.7 8.6 39.1Table 6: Use of word-deletion rules can improve the B??
?score, and use of sentence-deletion rules shows no signif-icant degradation, even though they are used heavily.
Sig-nificances are relative to baseline (no deletion); all otherdifferences are not statistically significant.testdeletion METEOR GTM M??S??
4-GRRnone 59.2 41.0 45.6 18.7word 57.9 41.9 45.0 18.6sentence 57.2 41.3 44.0 17.1Table 7: Word and sentence deletion are punished bymost of the other metrics.
All systems used max-B???training.
Significance testing was not performed.max-B???
trainingdeletion dev B???-???
test B???-??
?none 35.3 36.9word 35.8+ 37.1sentence 33.0??
34.5??max-B???-???
trainingdev testdeletion del% B???-???
del% B???-??
?none 0 35.8 0 37.1word 5.3 36.3+ 5.0 37.3sentence 0.02 35.9 0 37.5Table 8: B???-???
severely punishes the max-B??
?-trained sentence-deletion system; when we perform max-B???-???
training, word deletion occurs less frequentlyand sentence deletion is nearly unused.
Significances arerelative to baseline (no deletion); other differences are notstatistically significant.Key: +, ++ significant improvement (p < 0.05 or p < 0.01, respectively)?, ??
significant degradation (p < 0.05 or p < 0.01, respectively)?metric change in metric due to genre-specific trainingdel% percentage of words deleted618Franz Josef Och.
2008.
The Google statistical machinetranslation system for the 2008 NIST MT Evaluation.Presentation at the NIST Open Machine Translation2008 Evaluation Workshop.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
B???
: a method for automatic evalua-tion of machine translation.
In Proc.
ACL 2002, pages311?318.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proc.
AMTA 2006, pages 223?231.Ben Taskar, Carlos Guestrin, and Daphne Koller.
2004.Max-margin markov networks.
In Proc.
NIPS 2003.Christoph Tillmann and Tong Zhang.
2006.
A discrimi-native global training algorithm for statistical MT.
InProc.
COLING-ACL 2006, pages 721?728.Roy W. Tromble, Shankar Kumar, Franz Och, and Wolf-gang Macherey.
2008.
Lattice minimum Bayes-riskdecoding for statistical machine translation.
In Proc.EMNLP 2008.
This volume.Robert A. Wagner.
1974.
Order-n correction for regularlanguages.
Communications of the ACM, 17(5):265.Taro Watanabe, Jun Suzuki, Hajime Tsukuda, and HidekiIsozaki.
2007.
Online large-margin training for sta-tistical machine translation.
In Proc.
EMNLP 2007,pages 764?773.Ying Zhang, Stephan Vogel, and Alex Waibel.
2004.
In-terpreting BLEU/NIST scores: How much improve-ment do we need to have a better system?
In Proc.LREC 2004, pages 2051?2054.Liang Zhou, Chin-Yew Lin, and Eduard Hovy.
2006.
Re-evaluating machine translation results with paraphasesupport.
In Proc.
EMNLP 2006, pages 77?84.619
