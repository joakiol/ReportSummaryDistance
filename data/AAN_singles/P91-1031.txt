STRATEGIES FOR ADDING CONTROL INFORMATIONTO DECLARATIVE GRAMMARSHans UszkoreitUniversity of Saarbrtickenand German Research Centerfor Arlfficial Intelligence (DFKI)W-6600 Saarbriicken 11, FRGuszkoreit@coli.uni-sb.deAbstractStrategies are proposed for combining different kinds ofconstraints in declarative grammars with a detachablelayer of control information.
The added controlinformation is the basis for parametrized dynamicallycontrolled linguistic deduction, a form of linguisticprocessing that permits the implementation f plausiblelinguistic performance models without giving up thedeclarative formulation of linguistic competence.
Theinformation can be used by the linguistic processor forordering the sequence in which conjuncts and disjunctsare processed, for mixing depth-first and breadth-firstsearch, for cutting off undesired erivations, and forconstraint-relaxation.1 IntroductionFeature term formalisms (FTF) have proven extremelyuseful for the declarative representation f linguisticknowledge.
The family of grammar models that arebased on such formalisms include Generalized PhraseStructure Grammar (GPSG) \[Gazdar et al 1985\],Lexical Functional Grammar (LFG) \[Bresnan 1982\],Functional Unification Grammar (bUG) \[Kay 1984\],Head-Driven Phrase Structure Grammar (I-IPSG) \[Pollardand Sag 1988\], and Categorial Unification Grammar(CUG) \[Karttunen 1986, Uszkoreit 1986, Zeevat et al1987\].Research for this paper was carried out in parts at DFKI inthe project DIsco which is funded by the German Ministryfor Research and Technology under Grant-No.
: 1TW 9002.Partial funding was also provided by the German ResearchAssociation (DFG) through the Project BiLD in the SFB314: Artificial Intelligence and Knowledge-Based Systems.For fruitful discussions we would like to thank ourcolleagues inthe projects DISCO, BiLD and LIIX)G as well asmembers of audiences at Austin, Texas, and Kyoto, Japan,where preliminary versions were presented.
Special thanksfor valuable comment and suggestions go to Gregor Erbach,Stanley Peters, Jim Talley, and Gertjan van Noord.The expressive means of feature term formalisms haveenabled linguists to design schemes for a very uniformencoding of universal and language-particular linguisticprinciples.
The most radical approach of organizinglinguistic knowledge in a uniform way that was inspiredby proposals of Kay can be found in HPSG.Unification grammar formalisms, or constraint-basedgrammar formalisms as they are sometimes calledcurrently constitute the preferred paradigm forgrammatical processing in computational linguistics.One important reason for the success of unificationgrammars I in computational linguistics is their purelydeclarative nature.
Since these grammars are notcommitted to any particular processing model, they canbe used in combination with a number of processingstrategies and algorithms.
The modularity has a numberof advantages:?
freedom for experimentation with different processingschemes,?
compatibility of the grammar with improved systemversions,?
use of the same grammar for analysis and generation,?
reusability of a grammar in different systems.Unification grammars have been used by theoreticallinguists for describing linguistic competence.
Thereexist no processing models for unification grammars yetthat incorporate at least a few of the most widelyaccepted observations about human linguisticperformance.?
Robustness: Human listeners can easily parseillformed input and adapt to patterns ofungrammaticality.1The notion of grammar assumed here is equivalent to thestructured collection of linguistic knowledge basesincluding the lexicon, different ypes of rule sets, linguisticprinciples, etc.237?
Syntactic disambiguation i parsing: Unlikelyderivations should be cut off or only tried after morelikely ones failed.
(attachment ambiguities, gardenpaths)?
Lexical disarnbiguation i  parsing: Highly unlikelyreadings hould be suppressed or tried only if noresult can be obtained otherwise.?
Syntactic hoice in generation: In generation onederivation eeds to be picked out of a potentiallyinfinite number of paraphrases.?
Lexical choice in generation: One item needs to bepicked out of a large number of alternatives.?
Relationship between active and passive command ofa language: The set of actively used constructionsand lexical items is a proper subset of the onesmastered passively.The theoretical grammarian has the option to neglectquestions of linguistic performance and fully concentrateon the grammar as a correct and complete declarativerecursive definition of a language fragment.
Thepsycholinguist, on the other hand, will not acceptgrammar theory and formalism if no plausibleprocessing models can be shown.Computational linguists-independent of their theoreticalinterests-have no choice but to worry about theefficiency of processing.
Unfortunately, asof this date,no implementations exist hat allow efficient processingwith the type of powerful unification grammars that arecurrently preferred by theoretical grammarians orgrammar engineers.
As soon as the grammar formalismemploys disjunction and negation, processing becomesextremely slow.
Yet the conclusion should not be toabandon unification grammar but to search for betterprocessing models.Certain effective control strategies for linguisticdeduction with unification grammars have beensuggested in the recent literature.
\[Shieber et al 1990,Gerdemarm and Hinrichs 1990\] The strategies do notallow the grammar writer to attach control informationto the constraints in the grammar.
Neither can they beused for dynamic preference assignments.
The model ofcontrol proposed in this paper can be used to implementthese strategies in combination with others.
However,the strategies are not encoded in the program but controlinformation and parametrization of deduction.The claim is that unification grammar is much bettersuited for the experimental and inductive development ofplausible processing models than previous grammarmodels.
The uniformily encoded constraints of thegrammar need to be enriched by control information.This information serves the purpose to reduce localindeterminism through reordering and pruning of thesearch graph during linguistic deduction.This paper discusses several strategies for adding controlinformation to the grammar without sacrificing itsdeclarative nature.
One of the central hypotheses ofthepaper is that-in contrast to the declarative meaning ofthe grammar-the order in which subterms inconjunctions and disjunctions are processed is ofimportance for a realistic processing model.
Indisjunctions, the disjuncts that have the highestprobability of success hould be processed first, whereasin conjunctions the situation is reversed.2 Control information in conjunctions2.1 Ordering conjunctsIn this context conjuncts are all feature subterms that arecombined explicitly or implicitly by the operation offeature unification.
The most basic kind of conjunctiveterm that can be found in all FFFs is the conjunction offeature-value pairs.t"2" V2Other types of conjunctive terms in the knowledge basemay occur in formalisms that allow template, type orsort names in feature term specifications.Verb\[Transitive\] |3raSing /|lex : hits /t_sem : hit'-\]If these calls are processed (expanded) at compile time,the conjunction will also be processed at compile timeand not much can be gained by adding controlinformation.
If, however, the type or template calls areprocessed on demand at run time, as it needs to be thecase in FTFs with recursive types, these names can betreated as regular conjuncts.If a conjunction is unified with some other feature term,every conjunct has to be unified.
Controlling the orderin which operands are processed in conjunctions maysave time if conjuncts can be processed first that aremost likely to fail.
This observation is the basis for areordering method proposed by Kogure \[1990\].
If, e.g.,in syntactic rule applications, the value of the attributeagreement in the representation f nominal elements238leads to clashes more often than the value of theattribute definiteneness, it would in general be moreefficient to unify agreement before definiteness.Every unification failure in processing cuts off someunsuccessful branch in the search tree.
For every pieceof information in a linguistic knowledge base we willcall the probability at which it is directly involved insearch tree pruning its failure potential.
More exactly,the failure potential of a piece of information is theaverage number of times, copies of this (sub)term turnto _1.
during the processing of some input.The failure path from the value that turns to _1_ fh'st upto the root is determined by the logical equivalences_1_ = a : _1_ (for any attribute c02_ = \[_1.
x\] (for any term x)x = {.J_ x} (for any term x)?
= {.L}plus the appropriate associative laws.Our experience ingrammar development has shown thatit is very difficult for the linguist o make good guessesabout he relative failure potential of subterms of rules,principles, lexical entries and other feature terms in thegrammar.
However, relative rankings bases on failurepotential can be calculated by counting failures during atraining phase.However, the failure potential, as it is defined here, maydepend on the processing scheme and on the order ofsubterms in the grammar.
If, e.g., the value of theagreement feature person in the definition of the typeVerb leads to failure more often than the value of thefeature number, this may simply be due to the order inwhich the two subterms are processed.
Assume theunlikely situation that the value of number would haveled to failure-if the order had been reversed-in all thecases in which the value of person did in the oM order.Thus for any automatic counting scheme some constantshuffling and reshuffling of the conjunct order needs tobe applied until the order stabilizes (see also \[Kogure1990\]).There is a second criterion to consider.
Someunifications with conjuncts build a lot of structurewhereas others do not.
Even if two conjuncts lead tofailure the same number of times, it may still make adifference inwhich order they are processed.Finally there might good reasons to process someconjuncts before others imply because processing themwill bring in additional constraints hat can reduce thesize of the search tree.
Good examples of such strategiesare the so-called head-driven orfunctor-driven processingschemes.The model of controlled linguistic deduction allows themarking of conjuncts derived by failure counting,processing effort comparisons, or psyeholinguisticobservations.
However, the markings do not bythemselves cause a different processing order.
Only ifdeduction is parametrized appropriately, the markingswill be considered by the type inference ngine.2.2 Relaxation markingsMany attempts have been made to achieve morerobustness in parsing through more or less intricateschemes of rule relaxation.
In FTFs all linguisticknowledge is encoded in feature terms that denotedifferent kinds of constraints on linguistic objects.
Forthe processing of grammatically illformed input,constraint relaxation techniques are needed.Depending on the task, communication type, and manyother factors certain constraints will be singled out forpossible relaxation.A relaxation marking is added to the control informationof any subterm c encoding a constraint that may berelaxed.
A relaxation marking consists of a function r cfrom relaxation levels to relaxed constraints, i.e., a setof ordered pairs <i, ci> where i is an integer greater than0 denoting a relaxation level and ci is a relaxedconstraint, i.e., a term subsuming c.2The relaxation level is set as a global parameter forprocessing.
The default level is 0 for working with anunrelaxed constraint base.
Level 1 is the first level atwhich constraints are weakened.
More than tworelaxation levels are only needed if relaxation issupposed to take place in several steps.If the unification of a subterm bearing some relaxationmarking with some other term yields &, unification isstopped without putting .L into the partial result.
Thebranch in the derivation is discontinued just as if a realfailure had occurred but a continuation point forbacktracking is kept on a backtracking stack.
Thepartial result of the unification that was interrupted isalso kept.
If no result can be derived using the grammarwithout relaxation, the relaxation level is increased andbacktracking tothe continuation points is activated.
The2Implicitely the ordered pair <0, c> is part of the controlinformation for every subterm.
Therefore it can be omitted.239subterm that is marked for relaxation is replaced by therelaxed equivalent.
Unification continues.
Whenever a(sub)term c from the grammar is encountered for whichre(i) is defined, the relaxed constraint isused.This method also allows processing with an initialrelaxation level greater than 0 in applications ordiscourse situations with a high probability of ungram-matical inpuLFor a grammar G let Gi be the grammar G except hatevery constraint is replaced by rc(i).
Let L i stand forthe language generated or recognized by a grammar G i.If constraints are always properly relaxed, i.e., ifrelaxation does not take place inside the scope ofnegation in FITs that provide negation, L i will alwaysbe a subset ofLi+ 1.Note that correctness and completeness of the declarativegrammar GO is preserved under the proposed relaxationscheme.
All that is provided is an efficient way ofjumping from processing with one grammar toprocessing with another closely related grammar.
Themethod is based on the assumption that the relaxedgrammars axe properly relaxed and very close to theunrelaxed grammar.
Therefore all intermediate r sultsfrom a derivation on a lower elaxation level can be kepton a higher one.3 Control information in disjunctions3.1 Ordering of disjunctsIn this section, it will be shown how the processing offeature terms may be controlled through the associationof preference weights to disjuncts in disjunctions ofconstraints.
The preference weights determine the orderin which the disjuncts are processed.
This method is themost relevant part of controlled linguistic deduction.
Inone model control information is given statically, in asecond model it is calculated dynamically.Control information cannot be specified independentfrom linguistic knowledge.
For parsing some readingsin lexical entries might be preferred over others.
Forgeneration lexical choice might be guided by preferenceassignments.
For both parsing and generation certainsyntactic onstructions might be preferred over others atchoice points.
Certain translations might receive higherpreference during the transfer phase in machinetranslation.Computational linguists have experimented withassignments ofpreferences tosyntax and transfer rules,lexical entries and lexical readings.
Preferences areusually assigned through numerical preference markersthat guide lexical ookup and lexical choice as well asthe choice of rules in parsing, generation, and transferprocesses.
Intricate schemes have been designed forarithmetically calculating the preference marker of acomplex unit from the preference markers of its parts.In a pure context-free grammar only one type ofdisjunction is used which corrresponds to the choiceamong rules.
In some unification grammars such aslexical functional grammars, there exist disjunctionbetween rules, disjunction between lexical items anddisjunction between feature-values in f-structures.
Insuch grammars a uniform preference strategy cannot beachieved.
In other unification grammar formalisms suchas FUG or HPSG, the phrase structure has beenincorporated into the feature terms.
The onlydisjunction is feature term disjunction.
Our preferencescheme is based on the assumption that the formalismpermits one type of disjunction only.For readers not familiar with such grammars, a briefoutline is presented.
In HPSG grammatical knowledgeis fully encoded in feature terms.
The formalismemploys conjunction (unification), disjunction,implication, and negation as well as special data typesfor lists and sets.
Subterms can also be connectedthrough relational constraints.
Linguistically relevantfeature terms are order-sorted, i.e., there is a partiallyordered set of sorts such that every feature term thatdescribes a linguistic object is assigned to a sort.The grammar can be viewed as a huge disjunctiveconstraint on the wellformedness of linguistic signs.Every wellformed sign must unifiy with the grammar.The grammar consists of a set of universal principles, aset of language-particular principles, a set of lexicalentries (the lexicon), and a set of phrase-structure rules.The grammar of English contains all principles ofuniversal grammar, all principles of English, theEnglish lexicon, and the phrase-structure rules ofEnglish.
A sign has to conform with all universal andlanguage-particular principles, therefore these principlesare combined in conjunctions.
It is either a lexical signin which case it has to unify with at least one lexicalentry or it is a phrasal sign in which case it needs tounify with at least one phrase-structure rule.
Thelexicon and the set of rules are therefore combined indisjunctions.240\[Pi\] UniversalGrammar= P2 \['P':~\]Principles_of_English = ~P.."+LpoRules_of_English = R2P\[U ve  G  mar lGrammar o f  English = \[Principles__ofEnglish|l/Rules--?f--English I\]L/Lexicon_of_English JJFigure 1.
Organization of the Grammar ofEnglish in HPSGSuch a grammar enables the computational linguist toimplement processing in either direction as mere typeinference.
However, we claim that any attempts tofollow this elegant approach will lead to terriblyinefficient systems unless controlled linguistic deductionor an equally powerful paramelrizable control scheme isemployed.Controlled linguistic deduction takes advantage of thefact that a grammar of the sort shown in Figure 1allows a uniform characterization f possible choicepoints in grammatical derivation.
Every choice point inthe derivation involves the processing of a disjunction.Thus feature disjunction is the only source ofdisjunction or nondeterminism in processing.
This iseasy to see in the case of lexical lookup.
We assumethat a lexicon is indexed for the type of informationneeded for access.
By means of distributive andassociative laws, the relevant index is factored out.
Alexicon for parsing written input is indexed by a featurewith the attribute graph that encodes the graphemicform.
A lexicon with the same content might be usedfor generation except hat the index will be the semanticcontent.An ambiguous entry contains a disjunction of itsreadings.
In the following schematized entry for theEnglish homograph bow the disjunction containseverything but the graphemic form.
3graph: (bow)-(bowl~ I?+ l~OWkl3.2 Static preferencesThere exist two basic strategies for dealing withdisjunctions.
One is based on the concept ofbacktracking.
One disjunct is picked (either at randomor from the top of a stack), a continuation point is set,and processing continues as if the picked disjtmct werethe only one, i.e., as if it were the whole term.
Ifprocessing leads to failure, the computation is set backcompletely to the fixed continuation point and adifferent (or next) disjunct is picked for continuation.
Ifthe computation with the first disjunct yields success,one has the choice of either to be satisfied with the(first) solution or to set the computation back to thecontinuation point and try the next disjunct.
Withrespect o the disjunction, this strategy amounts todepth-first search for a solution.The second strategy isbased on breadth-f'wst search.
Alldisjuncts are used in the operation.
If, e.g., a disjunction3Additional information such as syntactic ategory mightalso be factored out within the entry:-  ph:-synllocallcat: n\]/Jsynllocallcat: vJ~Ibow,+,,a1I \]However, all we are interested in in this context is theobservation that in any case the preferences amongreadings have to be associated with disjuncts.241is unified with a nondisjunctive t rm, the term is unifiedwith every disjunct.
The result is again a disjunction.The strategy proposed here is to allow for combinationsof depth-first and breadth-first processing.
Depth-firstsearch is useful if there are good reasons to believe thatthe use of one disjunct will lead to the only result or tothe best result.
A mix of the two basic strategies iuseful if there are several disjuncts that offer betterchances than the others.Preference markers (or preference values) are attached tothe disjuncts of a disjunction.
Assume that a preferencevalue is a continuous value p in 0 < p _< 10.
Now aglobal width factor w in 0 < w < 10 can be set thatseparates the disjuncts to be tried out fast from the onesthat can only be reached through backtracking.All disjuncts are tried out f'n-st in parallel whose valuesPi are in Praax-W <- Pi <- Pmax.
If the width is set to 2,all disjuncts would be picked that have values Pi inPmax -2 <- Pi < Pmax.
Purely depth-first and purelybreadth-fast earch are forced by setting the threshold to0 or 10 respectively.3.3 Dynamic preferencesOne of the major problems in working with preferencesis their contextual dependence.
Although staticpreference values can be very helpful in guiding thederivation, especially for generation, transfer, orlimiting lexical ambiguity, often different preferencesapply to different contexts.Take as an example again the reduction of lexicalambiguity.
It is clearly the context that influences thehearers preferences in selecting a reading.
4The astronomer marr/ed astar.
vs.The movie director married a star.The tennis player opened the ball.
vs.The mayor opened the ball.Preferences among syntactic onstructions, that ispreferences among rules, depend on the sort of text to beA trivial but unsatisfactory solution is to substitute thepreference values by a vector of values.
Depending onthe subject matter, the context, or the approriate style or4 The fnst example isdue to Reder \[1983\].register, different fields of the vector values might beconsidered for controlling the processing.However, there are several reasons that speak againstsuch a simple extension of the preference mechanism.First of all, the number of fields that would be needed ismuch too large.
For lexical disambiguation, a mereclassification of readings according to a small set ofsubject domains as it can be found in many dictionariesis much too coarse.Take, e.g., the English word line.
The word is highlyambiguous.
We can easily imagine appropriate preferredreadings in the subject domains of telecommunication,geometry, genealogy, and drug culture.
However, evenin a single computer manual the word may, dependingon the context, refer to a terminal ine, to a line ofcharacters on the screen, to a horizontal separation linebetween editing windows, or to many other things.
(Ineach case there is a different translation i to German.
)A second reason comes from the fact that preferences arehighly dynamic, i.e., they can change at any time duringprocessing.
Psycholinguistic experiments tronglysuggest that he mere perception of a word totally out ofcontext already primes the subject, i.e., influences hispreferences in lexical choice.
\[Swinney 1979\]The third reason to be mentioned here is themultifactorial dependency of preferences.
Preferencescan be the result of a combination of factors uch as thetopic of the text or discourse, previous occurrence ofpriming words, register, style, and many more.In order to model the dynamics of preferences, aprocessing model is proposed that combines techniquesfrom connectionist research with the declarativegrammar formalisms through dynamic preference values.Instead of assigning permanent preference values orvalue vectors to disjuncts, the values are dynamicallycalculated by a spreading-activation net.
So far thepotentials of neural nets for learning (e.g.backpropagation schemes) have not been exploited.Every other metaphor for setting up weightedconnections between constraints in disjunctions wouldserve our purpose qually well.
55For an introduction to connectionist nets see Rumelhart,Hinton, and McCleUand \[1986\].
For an overview ofdifferent connectionist models see Feldman and Ballard\[1982\] and Kemke \[1988\].242The type of net employed for our purposes i  extremelysimple.
6 Every term in the linguistic knowledge baseswhose activation may influence a preference and everyterm whose preference value may be influenced isassociated with a unit.
These sets are not disjoint sincethe selection of one disjunct may influence otherpreferences.
In addition there can be units forextralinguistic nfluences on preferences.
Units areconnected by unidirectional weighted finks.
They havean input value i, an activation value a, a resting value r,and a preservation function f. The input value is thesum of incoming activation.
The resting value is theminimal activation value, i.e., the degree of activationthat is independent from current or previous input.
Theactivation value is either equal to the sum of input andsome fraction of the previous activation, which isdetermined by the preservation function or it is equal tothe resting value, whichever is greater.ai+ 1 = max{r, ii +f(a/)}.In this simple model the output is equal to theactivation.
The weights of the links l are factors suchthat 0 < l < 1.
If a link goes from unit Ul to unit u2,it contributes an activation of l*aul to the input of u2.4 Conclusion and future researchStrategies are proposed for combining declarativelinguistic knowledge bases with an additional layer ofcontrol information.
The unification grammar itselfremains declarative.
The grammar also retainscompleteness.
It is the processing model that uses thecontrol information for ordering and pruning the searchgraph.
However, if the control information is neglectedor if all solutions are demanded and sought bybacktracking, the same processing model can be used toobtain exactly those results derived without controlinformation.Yet, if control is used to prune the search tree in such away that the number of solutions is reduced, manyobservations about human linguistic performance someof which are mentioned in Section 1 can be simulated.6The selected simple model is sufficient for illustrating thebasic idea.
Certainly more sophisticated eormectionistmodels will have to be employed for eognitively plausiblesimulation.
One reason for the simple design of the net isthe lack of a learning.
Kt this time, no learning model hasbeen worked out yet for the proposed type of spreading-activation ets.
For the time being it is assumed that theweights are set by hand using linguistic knowledge,corpora, and association dictionaries.Criteria for selection among alternatives can be encoded.The smaller set of actively used constructions andlexemes is simply explained by the fact that for all theitems in the knowledge base that are not actively usedthere are alternatives that have a higher preference.The controlled linguistic deduction approach offers anew view of the competence-performance distinction,which plays an important r61e in theoretical linguistics.Uncontrolled eduction cannot serve as a plausibleperformance model.
On the other hand, the performancemodel extends beyond the processing model, it alsoincludes the structuring of the knowledge base andcontrol information that influence processing.Linguistic Processing Linguistic Knowledge?
?l?
5 ~ arametrizatio control ?
?t J .~_ ,= of deduction information -'#.?
1 ~ J linguistic declarative '5~a.
L deduction j grammar5Eo ?
0Figure 2.
A new view of the competence-performance distinctionSince this paper reports about the first results from anew line of research, many questions remain open anddemand further research.Other types of control need to be investigated in relationwith the strategies proposed in this paper.
Uszkoreit\[1990\], e.g., argues that functional uncertainty needs tobe controlled in order to reduce the search space and atthe same time simulate syntactic preferences in humanprocessing.Unification grammar formalisms may be viewed asconstraint languages in the spirit of constraint logicprogramming (CLP).
Efficiency can be gained throughappropriate strategies for delaying the evaluation ofdifferent constraint types.
Such schemes for delayedevaluation of constraints have been implemented forLFG.
They play an even greater role in the processingof Constraint Logic Grammars (CLG) \[Balari et al1990\].
The delaying scheme is a more sophisticated243method for the ordering of conjuncts.
More research isneeded in this area before the techniques of CLP/CLGcan be integrated in a general model of controlled(linguistic) deduction.So far the weight of the links for preference assignmentcan only be assigned on the basis of associationdictionaries as they have been compiled by psy-chologists.
For nonlexieal links the grammar writer hasto rely on a trial and error method.A training method for inducing the best conjunct orderon the basis of failure potential was described inSection2.1.
The training problem, .ie., the problem ofautomatic induction of the best control information ismuch harder for disjunctions.
Parallel to the method forconjunctions, during the training phase the successpotential of a disjunct needs to be determined, i.e., theaverage number of contributions to successfulderivations for a given number of inputs.
The problemis much harder for assigning weights to links in thespreading-activation net employed for dynamicpreference assignment.Hirst \[1988\] uses the structure of a semantic net fordynamic lexical disambiguation.
Corresponding totheirmarker passing method a strategy should be developedthat activates all supertypes of an activated type indecreasing quantity.
Wherever activations meet, amutual reinforcement of the paths, that is of thehypotheses occurs.Another topic for future research is the relationshipbetwccn control information and feature logic.
Whathappens if, for instance, a disjunction is transformedinto a conjunction using De Morgans law?The immediate reply is that control structures are onlyvalid on a certain formulation of the grammar and noton its logically eqtfivalent syntactic variants.
However,assume that a fraction of a statically or dynamicallycalculated fraction involving success potential sp andfailure potentialfp is attached to every subterm.
Fordisjuncts, p is ?fivided by fp, for conjuncts fp is dividedbysp.De Morgans law yields an intuitive result if we assumethat negation of a term causes the attached fraction to beinverted.
More research needs to be carried out beforeone can even start to argue for or against apreservationof control information under logical equivalences.Head-driven or functor-driven deduction has proven veryuseful.
In this approach the order of processingconjuncts has been fixed in order to avoid the logicallyperfect but much less effcient orderings in which thecomplement conjuncts in the phrase structure (e.g., inthe value of the daughter feature) are processed before thehead conjunct.
This strategy could not be induced orlearned using the simple ordering criteria that are merelybased on failure and success.
In order to induce thestrategy from experience, the relative computationaleffort needs to be measured and compared for thelogically equivalent orderings.
Ongoing work isdedicated to the task of formulating well-knownprocessing algorithms uch as the Earley algorithm forparsing or the functor-driven approach for generationpurely in terms of preferences among conjuncts anddisjuncts.244ReferencesBalari, S. and L. Damas (1990) CLG(n): ConstraintLogic Grammars.
In COLING 90.Bresnan, J.
(Ed.)
(1982) The Mental Representation fGrammatical Relations.
MIT Press, Cambridge,Mass.Feldman, LA.
and D.H. Ballard (1982) Connectionistmodels and their Properties.
In Cognitive Science,6:205-254.Gazdar, G., E. Klein, G. K. Pullum, and I.
A.
Sag(1985) Generalized Phrase Structure Grammar.Harvard University Press, Cambridge, Mass.Gerdemann, D. and E. W. Hinrichs (1990) Functor-Driven Natural Language Generation with Categorial-Unification Grammars.
In COLING 90.Hirst, G. (1988) Resolving Lexical Ambiguity Compu-tationaUy with Spreading Activation and PolaroidWords.
In S.I.
Small, G.W.
Cottrell and M.K.
Tanen-haus (eds.
), Lexical Ambiguity Resolution, pp.73-107.
San Mateo: Morgan Kaufmann Publishers.Karttunen, L. (1986) Radical Lexicalism.TechnicalReport CSLI-86-66, CSLI - Stanford University.Kasper, R. and W. Rounds (1986) A Logical Semanticsfor Feature Structures.
In Proceedings of the 24thACL.Kay, M. (1984) Functional Unification Grammar: AFormalism for Machine Translation.
In COLING 84.Kemke, C. (1988) Der Neuere Konnektionismus - EinUberblick.
lmformatik-Spektrum, 11:143-162.Kogure, K. (1990) Strategic Lazy Incremental CopyGraph Unification.
In COLING 90.Pollard, C. and I.
A.
Sag (1988) An Information-BasedSyntax and Semantics, Volumed Fundamentals,CSLI Lecture Notes 13, CSLI, Stanford, CA.Reder, L.M., (1983) What kind of pitcher can a catcherf'dl?
Effects of priming in sentence comprehension.
IJournal of Verbal Learning and Verbal Behavior 22(2):189-202.Rumelhart, D.E., G.E.
Hinton and J.L.
McClelland(1986) A general framework for parallel distributedprocessing.
In Rumelhart, D.E., McClelland, J.L.,and the PDP Research Group, editors, ParallelDistributed Processing, Explorations in theMicrostructure of Cognition: Foundations, volume 1pages 45-76.
Cambridge, MA: MIT Press.Shieber, S. (1984) The Design of a Computer Languagefor Linguistic Information.
In S. Shieber, L.Karttunen, and F. Pereira (Eds.)
Notes from theUnification Underground.
SRI Technical Note 327,SRI International, Menlo Park, CA.Shieber, S. M. (1986) An Introduction to Unification-Based Approaches to Grammar.
CSLI Lecture Notes4.
CSLI, Stanford, CA.Shieber, S., H. Uszkoreit, J. Robinson, and M. Tyson(1983) The Formalism and Implementation f PATR-II.
In Research on Interactive Acquisition and Use ofKnowledge.
SRI International, Menlo Park, CA.Shieber, S., G. van Noord, R.C.
Moore, and F.C.N.Pereira (1990) Semantic-Head-Driven G eration InComputational Linguistics 16(1).Smolka, G. (1988) A Feature Logic with Subsorts.LILOG Report 33, IBM Germany, Stuttgart.Smolka, G., and H. AYt-Kaci (1987) InheritanceHierarchies: Semantics and Unification.
MCC ReportAI-057-87, MCC Austin, TX.Swinney, D.A.
(1979) Lexical Access during sentencecomprehension: (Re)Consideration fcontext effects.In Journal of Verbal Learning and Verbal Behavior18(6):645-659.Uszkoreit, H. (1986) Categorial Unification Grammars.In COLING 86, Bonn.Uszkoreit, H. (1988) From Feature Bundles to AbstractData Types: New Directions in the Representation a dProcessing of Linguistic Knowledge.
In A.
Blaser(Ed.)
Natural Language at the Computer.
Springer,Berlin, Heidelberg, New York.Uszkoreit, H. (1990) ,Extraposition and AdjunctAttachment in Categorial Unification Grammar" InW.
Bahner (Hrsg.)
Proceedings of the XIVthInternational Congress of Linguists, August 1987,Akademie Verlag Berlin, DDR, 1990.7_~evat, H., E. Klein, and J. Calder (1987) UnificationCategorial Grammar.
In Haddock, Klein, and Morrill(Eds.)
Categorial Grammar, Unification Grammar, andParsing.
Edinburgh Working Papers in CognitiveScience, Vol.1.
Centre for Cognitive Science, Univer-sity of Edinburgh, Edinburgh.245
