Coling 2010: Poster Volume, pages 885?893,Beijing, August 2010A Learnable Constraint-based Grammar FormalismSmaranda MuresanSchool of Communication and InformationRutgers Universitysmuresan@rci.rutgers.eduAbstractLexicalized Well-Founded Grammar(LWFG) is a recently developed syntactic-semantic grammar formalism for deeplanguage understanding, which balancesexpressiveness with provable learnabilityresults.
The learnability result for LWFGsassumes that the semantic compositionconstraints are learnable.
In this paper,we show what are the properties andprinciples the semantic representation andgrammar formalism require, in order tobe able to learn these constraints fromexamples, and give a learning algorithm.We also introduce a LWFG parser as adeductive system, used as an inferenceengine during LWFG induction.
Anexample for learning a grammar for nouncompounds is given.1 IntroductionRecently, several machine learning approacheshave been proposed for mapping sentences to theirformal meaning representations (Ge and Mooney,2005; Zettlemoyer and Collins, 2005; Muresan,2008; Wong and Mooney, 2007; Zettlemoyer andCollins, 2009).
However, only few of them in-tegrate the semantic representation with a gram-mar formalism: ?-expressions and CombinatoryCategorial Grammars (CCGs) (Steedman, 1996)are used by Zettlemoyer and Collins (2005;2009),and ontology-based representations and Lexical-ized Well-Founded Grammars (LWFGs) (Mure-san and Rambow, 2007) are used by Muresan(2008).An advantage of the LWFG formalism, com-pared to most constraint-based grammar for-malisms developed for deep language understand-ing, is that it is accompanied by a learnabilityguarantee, the search space for LWFG induc-tion being a complete grammar lattice (Muresanand Rambow, 2007).
Like other constraint-basedgrammar formalisms, the semantic structures inLWFG are composed by constraint solving, se-mantic composition being realized through con-straints at the grammar rule level.
Moreover, se-mantic interpretation is also realized through con-straints at the grammar rule level, providing ac-cess to meaning during parsing.However, the learnability result given by Mure-san and Rambow (2007) assumed that the gram-mar constraints were learnable.
In this paper wepresent the properties and principles of the seman-tic representation and grammar formalism that al-low us to learn the semantic composition con-straints.
These constraints are a simplified versionof ?path equations?
(Shieber et al, 1983), and wepresent an algorithm for learning these constraintsfrom examples (Section 5).
We also present aLWFG parser as a deductive system (Shieber etal., 1995) (Section 3).
The LWFG parser is usedas an innate inference engine during LWFG learn-ing, and we present an algorithm for learningLWFGs from examples (Section 4).
A discussionand an example of learning a grammar for nouncompounds are given is Section 6.2 Lexicalized Well-Founded GrammarsLexicalized Well-Founded Grammar (LWFG) isa recently developed formalism that balancesexpressiveness with provable learnability results(Muresan and Rambow, 2007).
LWFGs are a typeof Definite Clause Grammars (Pereira and War-ren, 1980) in which (1) the context-free back-bone is extended by introducing a partial orderingrelation among nonterminals, 2) grammar non-terminals are augmented with strings and theirsyntactic-semantic representations, called seman-tic molecules, and (3) grammar rules can have8851.
Syntagmas containing elementary semantic moleculesa.
(w1, `h1b1?
)= (laser,0BBBBB@264cat nounhead X1mod X2375DX1.isa = laser, X2.P1=X1E1CCCCCA) b.
(w2, `h2b2?
)=(printer,0BBBBB@264cat nounnr sghead X3375DX3.isa = printerE1CCCCCA)2.
Syntagmas containing a derived semantic molecule(w, `hb?
)=(laser printer,0BBBBB@264cat ncnr sghead X375DX1.isa = laser, X .P1=X1, X .isa=printerE1CCCCCA)3.
Constraint Grammar RuleNC(w,`hb?)
?
Noun(w1,`h1b1?
), Noun(w2,`h2b2?)
: ?c(h, h1, h2),?onto(b)?c(h, h1, h2) = {h.cat = nc, h1.cat = noun, h2.cat = noun, h.head = h1.mod, h.head = h2.head, h.nr = h2.nr}?onto(b) returns ?X1.isa = laser,X.instr = X1,X.isa = printer?Figure 1: Syntagmas containing elementary semantic molecules (1) and a derived semantic molecule(2); A constraint grammar rule together with the semantic composition and ontology-based interpreta-tion constraints, ?c and ?onto (3)two types of constraints, one for semantic com-position and one for semantic interpretation.
Thefirst property allows LWFG learning from a smallset of examples.
The last two properties makeLWFGs a type of syntactic-semantic grammars.Definition 1.
A semantic molecule associatedwith a natural language string w, is a syntactic-semantic representation, w?
= (hb), where h(head) encodes compositional information, whileb (body) is the actual semantic representation ofthe string w.Grammar nonterminals are augmented withpairs of strings and their semantic molecules.These pairs are called syntagmas, and are denotedby ?
= (w,w?)
= (w, (hb)).Examples of semantic molecules for the nounslaser and printer and the noun-noun compoundlaser printer are given in Figure 1.
When as-sociated with lexical items, semantic moleculesare called elementary semantic molecules.
Whensemantic molecules are built by the combina-tion of others, they are called derived semanticmolecules.
Formally, the semantic molecule head,h, is a one-level feature structure (i.e., values areatomic), while the semantic molecule body, b, is alogical form built as a conjunction of atomic pred-icates ?concept?.?attr?
= ?concept?, where vari-ables are either concept or slot identifiers in an on-tology.11The body of a semantic molecule is called OntoSeR andMuresan and Rambow (2007) formally definedLWFGs, and we present here a slight modificationof their definition.Definition 2.
A Lexicalized Well-Founded Gram-mar (LWFG) is a 7-tuple, G = ??,?
?, NG,, PG, P?, S?, where:1. ?
is a finite set of terminal symbols.2.
??
is a finite set of elementary seman-tic molecules corresponding to the terminalsymbols.3.
NG is a finite set of nonterminal symbols.NG??
= ?.
We denote pre(NG) ?
NG, theset of pre-terminals (a.k.a, parts of speech)4.
 is a partial ordering relation among non-terminals.5.
PG is the set of constraint grammar rules.
Aconstraint grammar rule is written A(?)
?B1(?1), .
.
.
, Bn(?n) : ?(??
), where A,Bi ?NG, ??
= (?, ?1, ..., ?n) such that ?
=(w,w?
), ?i = (wi, wi?
), 1 ?
i ?
n,w =w1 ?
?
?wn, w?
= w?1 ?
?
?
?
?
w?n, and ?
is thecomposition operator for semantic molecules(more details about the composition oper-ator are given in Section 5).
For brevity,we denote a rule by A ?
?
: ?, whereA ?
NG, ?
?
N+G .
P?
is the set of con-straint grammar rules whose left-hand sideare pre-terminals, A(?)
?, A ?
pre(NG).is a flat ontology-based semantic representation.886We use the notation A ?
?
for this gram-mar rules.
In LWFG due to partial orderingamong nonterminals we can have orderedconstraint grammar rules and non-orderedconstraint grammar rules (both types can berecursive or non-recursive).
A grammar ruleA(?)
?
B1(?1), .
.
.
, Bn(?n) : ?(??
), is anordered rule, if for all Bi, we have A  Bi.In LWFGs, each nonterminal symbol is aleft-hand side in at least one ordered non-recursive rule and the empty string cannot bederived from any nonterminal symbol.6.
S ?
NG is the start nonterminal symbol, and?A ?
NG, S  A (we use the same notationfor the reflexive, transitive closure of ).The partial ordering relationmakes the set ofnonterminals well-founded2 , which allows the or-dering of the grammar rules, as well as the order-ing of the syntagmas generated by LWFGs.
Thisordering allow LWFG learning from a small set ofrepresentative examples (Muresan and Rambow,2007) (P?
is not learned).An example of a LWFG rule is given in Fig-ure 1(3).
Nonterminals are augmented with syn-tagmas.
Moreover, in LWFG the semantic com-position and interpretation are realized via con-straints at the grammar rule level (?(??)
in Defi-nition 2).
More precisely, syntagma compositionmeans string concatenation (w = w1w2) and se-mantic molecule composition ((hb)=(h1b1)?
(h2b2))?- where the bodies of semantic molecules areconcatenated through logical conjunction (b =(b1, b2)?, where ?
is a variable substitution ?
={X2/X,X3/X}), while the semantic moleculesheads are composed through compositional con-straints ?c(h, h1, h2), which are a simplified ver-sion of ?path equations?
(Shieber et al, 1983) (seeFigure 1(3)).
During LWFG learning, composi-tional constraints ?c are learned together with thegrammar rules.
Semantic interpretation, whichis ontology-based in LWFG, is also encoded asconstraints at the grammar rule level ?
?onto?
providing access to meaning during parsing.
?onto(b) constraints are applied to the body ofthe semantic molecule corresponding to the syn-2 should not be confused with information ordering de-rived from flat feature structurestagma associated with the left-hand side nonter-minal.
The ontology-based constraints are notlearned; rather, ?onto is a general predicate thatsucceed or fail as a result of querying an ontology?
when it succeeds, it instantiates the variablesof the semantic representation with concepts/slotsin the ontology (see the example in Figure 1(3)).2.1 Derivation in LWFGThe derivation in LWFG is called ground syn-tagma derivation, and it can be seen as thebottom up counterpart of the usual derivation.Given a LWFG, G, the ground syntagma deriva-tion relation, ?G?, is defined as: A??A?G??
(if ?
=(w,w?
), w ?
?, w?
?
?
?, i.e., A ?
pre(NG, ),and Bi?G?
?i, i=1,...,n, A(?
)?B1(?1),...,Bn(?n) : ?(??)A?G?
?.The set of all syntagmas generated by a gram-mar G is L?
(G) = {?|?
= (w,w?
), w ?
?+, ?A ?
NG, A ?G?
?}.
Given a LWFG G,E?
?
L?
(G) is called a sublanguage of G. Ex-tending the notation, given a LWFG G, the set ofsyntagmas generated by a rule (A?
?
: ?)
?
PGis L?
(A ?
?
: ?)
= {?|?
= (w,w?
), w ?
?+, (A ?
?
: ?)
?G?
?
}, where (A ?
?
: ?)
?G??
denotes the ground derivation A ?G?
?
obtainedusing the rule A ?
?
: ?
in the last derivationstep.3 LWFG Parsing as DeductionFollowing Shieber (1995), we present the Lexical-ized Well-Founded Grammar parser as a deduc-tive proof system in Table 1.
The items of thelogic are of the form [i, j, ?ij , A ?
?
?
?
?A],where A ?
??
: ?A is a grammar rule, ?A ?the constraints corresponding to the grammar rulewhose left-hand side nonterminal is A?
can betrue, ?
shows how much of the right-hand sideof the rule has been recognized so far, i points tothe parent node where the rule was invoked, and jpoints to the position in the input that the recogni-tion has reached.
We use the following notations:?Rij = (wRij ,(hRijbRij)) are syntagmas correspondingto the partially parsed right-hand side of a rule;?Lij = (wLij ,(hLijbLij)) are ground-derived syntagmas(i.e., they are augmenting the left-hand side non-887Item form [i, j, ?ij , A?
?
?
?
?A] 1 ?
i, j ?
n+ 1, A ?
NG, ??
?
N?Gthe ?A constraint can be trueAxioms [i, i+ 1, ?Lii+1, Bi ?
?]
1 ?
i ?
n,Bi ?
pre(NG), Bi ?
?Lii+1 ?
P?Goals [i, j, ?Lij , A?
??A?]
1 ?
i, j ?
n+ 1, A ?
NG, ?
?
N+GInference RulesPrediction [i,j,?Lij ,B???B?][i,i,?Rii,A??B?
?A] ?A?
B?
: ?A?
(A?
B?
: ?A) ?
PG?Rii = ??
(i.e., wRii = , bRii = true and hRii = ?
)Completion [i,j,?Rij ,A??
?
B ?
?A] [j,k,?Ljk,B??
?B ?][i,k,?Rik,A??
B ?
?
?A] ?Rik = ?Rij ?
?Ljk, wherewRik = wRijwLjk, bRik = bRijbLjk, hRik = hRij ?
hLjkConstraint [i,j,?Rij ,A???
?A][i,j,?Lij ,A???A?]
?
?A is satisfiable ?
?Lij = ?
(?Rij)Table 1: LWFG parsing as deductive systemterminal of a LWFG rule).
The goal items areof the form [i, j, ?Lij , A ?
??A?
], where ?Lij isground-derived from the rule A?
?
: ?A.Compared to the deductive system in (Shieberet al, 1995), the LWFG parser has the follow-ing characteristics: each item is augmented witha syntagma; the Constraint rule is a new infer-ence rule, and the goal items are associated toevery nonterminal in the grammar, not only tothe start symbol (i.e., LWFG parser is a robustparser).
The Constraint inference rule is the onlyone that obtains an inactive edge3, from an activeedge by executing the grammar constraint ?A (the?
is shifted across the constraint).
By applying theConstraint rule as the last inference rule we obtainthe ground-derived syntagmas ?Lij .
Thus, the goalitems are obtained only after the Constraint rule isapplied.
During this inference rule we have that?Lij = ?
(?Rij), where ?
is defined by: wLij = wRij ,bLij = bRij?ij , and hLij = ?(hRij).
The substitution?ij and the function ?
are implicitly contained inthe grammar constraint ?Ac (hLij , hRij) (see Section5 for details)Definition 3 (Robust parsing provability).
Robustparsing provability corresponds to reaching thegoal item: `rp A(?Lij) iff [i, j, ?Lij , A?
??A?
].Thus, we can notice that the ground syntagmaderivation is equivalent to robust parsing provabil-ity, i.e., A ?G?
?
iff G `rp A(?
).3We use Kay?s terminology: items are edges, where theaxioms and goals are inactive edges having ?
at the end,while the rest are active edges (Kay, 1986).4 Learning LWFGsThe theoretical learning model for LWFG induc-tion, Grammar Approximation by RepresentativeSublanguage (GARS), together with a learnabilitytheorem was introduced in (Muresan and Ram-bow, 2007).
LWFG?s learning framework char-acterizes the ?importance?
of substructures in themodel not simply by frequency, but rather lin-guistically, by defining a notion of ?representa-tive examples?
that drives the acquisition process.Informally, representative examples are ?buildingblocks?
from which larger structures can be in-ferred via reference to a larger generalization cor-pus referred to as representative sublanguage in(Muresan and Rambow, 2007).
The GARS modeluses a polynomial algorithm for LWFG learningthat take advantage of the building blocks natureof representative examples.The LWFG induction algorithm belongs to theclass of Inductive Logic Programming methods(ILP), based on entailment (Muggleton, 1995;Dzeroski, 2007).
At each step a new constraintgrammar rule is learned from the current repre-sentative example, ?.
Then this rule is added tothe grammar rule set.
The process continues untilall the representative examples are covered.
Wedescribe below the process of learning a grammarrule from the current representative example:1.
Most Specific Grammar Rule Generation.In the first step, the most specific grammarrule is generated from the current represen-tative example ?.
The category annotated888STEP 1 (Most Specific Grammar Rule Generation)STEP 2 (Grammar Rule Generalization)(laser printer,CANDIDATE GRAMMAR RULESlaser printerPerformance CriteriaBEST RULE((laser printer) manual)(desktop (laser printer))K ?
Background KnowledgeLexicon(laser, )Previously learned grammar rulescat   nc)(printer, )?
= (w, (hb)) - Current representative examplea) chunks={[NA(laser), Noun(laser)], [NC(printer),Noun(printer)]}rg1 NC ?
Noun Noun:?c4 (score=1)rg2 NC ?
NA Noun:?c5 (score=2)b) r: NC(w, (hb)) ?
Noun(w1, (h1b1)) Noun(w2, (h2b2)):?c4(h, h1, h2)?c4(h, h1, h2) = {h.cat = nc, h1.cat = noun, h2.cat = noun,E?
- Representative SublanguageNC ?
NA NC:?c7rg4 NC ?
NA NC:?c7 (score=3)rg3 NC ?
Noun NC:?c6 (score=2)Noun ?cat nounhead X1mod X2?X1.isa = laser,X2.Y = X1?cat noun?X3.isa = printer?NA ?
Noun:?c1?B.isa = laser, A.P1 = B,A.isa = printer?NA ?
NA NA:?c2NC ?
Noun:?c3nr sghead Ahead X3nr sgNoun ?h.head = h1.mod, h.head = h2.head, h.nr = h2.nr}Figure 2: Example of Grammar Rule Learningin the representative example gives theleft-hand-side nonterminal, while a robustparser returns the minimum number ofchunks covering the representative example.The categories of the chunks give the non-terminals of the right-hand side of the mostspecific rule.
For example, in Figure 2, giventhe representative example laser printerannotated with its semantic molecule, andthe background knowledge containing thealready learned rules NA ?
Noun : ?c1 ,NA ?
NA NA : ?c2 , NC ?
Noun : ?c3the robust parser generates the chunkscorresponding to the noun laser and thenoun printer: [NA(laser),Noun(laser)]and [NC(printer),Noun(printer)], re-spectively.
The most specific rule isNC ?
Noun Noun : ?c4 , where theleft-hand side nonterminal is given by thecategory of the representative example, inthis case nc.
Compositional constraints ?c4are learned as well.
In section 5 we givethe algorithm for learning these constraints,and several properties and principles that areneeded in order for these constraints to belearnable.2.
Grammar Rule Generalization.
In the sec-ond step, this most specific rule is gener-alized, obtaining a set of candidate gram-mar rules (the generalization step is the in-verse of the derivation step used to definethe complete grammar lattice search space in(Muresan and Rambow, 2007)).
The perfor-mance criterion in choosing the best gram-mar rule among these candidate hypothesesis the number of examples in the representa-tive sublanguage E?
(generalization corpus)that can be parsed using the candidate gram-mar rule, rgi in the last ground derivationstep, together with the previous learned rules,i.e., |E??L?(rgi)|.
In Figure 2 given the rep-resentative sublanguage E?={ laser printer,laser printer manual, desktop laser printer}the learner will generalize to the recursiverule NC ?
NA NC : ?7, since only thisrule can parse all the examples in E?.5 Learnable Composition ConstraintsIn LWFG, the semantic structures are composedby constraint solving, rather than functional ap-plication (with lambda expressions and lambda re-duction).
This section presents the properties andprinciples that guarantee the learnability of thecompositional constraints,?c, and presents an al-gorithm to generate these constraints from exam-ples, which is a key result for LWFG learnability.The information for semantic composition isencoded in the head of semantic molecules.
Thereare three types of attributes that belong to the se-mantic molecule head h: category attributes Ach,variable attributes Avh, and feature attributes Afh.Thus, Ah = Ach ?
Avh ?
Afh and Ach,Avh,Afh arepairwise disjoint.
For example, in Figure 1 for thenoun-noun compound laser printer, we have thatAch = {cat}, Afh = {nr}, and Avh = {head},while for the noun laser we have that Ach1 ={cat}, Afh1 = ?, andAvh1 = {head,mod} (nounscan be modifiers of other nouns, so their represen-tation is similar to that of an adjective).We describe in turn each of these types of at-tributes and their corresponding principles.
Allprinciples, except the first and the last mirrorprinciples in other constraint-based linguistic for-malisms, such as HPSG (Pollard and Sag, 1994).The category attributes Ach are state at-tributes, and their value set gives the category ofthe semantic molecule.
There is one attribute, cat?
Ach, which is mandatory and whose value is thename of the category (e.g., h.cat = nc in Figure8891).
The category of a semantic molecule can begiven by: 1) the cat attribute alone, or 2) the catattribute together with other state attributes in Achwhich are syntactic-semantic markers.Principle 1 (Category Name Principle).
The cat-egory name h.cat of a syntagma ?
= (w, (hb)) isthe same as the grammar nonterminal augmentedwith syntagma ?.When learning a LWFG rule from an example?, the above principle allows us to determine thenonterminal in the left-hand side of the grammarrule.
For example, when learning the LWFG rulefrom the syntagma corresponding to laser printerin Figure 2, the nonterminal in the left-hand sideof the LWFG rule is NC since h.cat = nc.The variable attributes Avh are attributeswhose values are logical variables and representthe semantic valence of the molecule, which al-lows the binding of the semantic representations.These logical variables appear in the semanticmolecule body as well.
For example, in Figure1(2) for the noun-noun compound laser printer,the value of the variable attribute head ?
Avh isa variable X , which appears also in the body ofthe semantic molecule ?X1.isa = laser,X.P1 =X1, X.isa = printer?.
It can be noticed that thesemantic molecule body contains other variablesas well (X1, P1).
However, only the variablespresent in the semantic molecule head as well (X)will participate in further composition.Principle 2 (Semantic Representation BindingPrinciple).
All the logical variables that the bodyb of a semantic molecule corresponding to a syn-tagma ?
= (w, (hb)), share with other syntagmas,are at the same time values of the variable at-tributes (Avh) of the semantic molecule head.There is one variable attribute, head ?
Avh thatrepresents the head of a syntagma, giving the fol-lowing principle:Principle 3 (Semantic Head Principle).
Given asyntagma ?
= (w, (hb)) ground derived from agrammar rule, r, there exists one and only onesyntagma ?i = (wi,(hibi)) corresponding to a non-terminal Bi in rule r?s right-hand side, whichhas the same value of the attribute head, i.e.,h.head = hi.head.The feature attributes Afh are the attributeswhose values express the specific properties of thesemantic molecules (e.g., number, person).Principle 4 (Feature Inheritance Principle).
If?i = (wi,(hibi)) is the semantic head of a ground-derived syntagma ?
= (w, (hb)), then all fea-ture attributes of ?
inherit the values of the cor-responding attributes that belong to the seman-tic head ?i.
That is, if h.head = hi.head , thenh.f = hi.f , ?f ?
Afh ?Afhi .Besides this principle, the feature attributes areused for category agreement.
The categories thatenter in agreement are maximum projection cat-egories.
This linguistic knowledge about agree-ment is used in the form of the following princi-ple:Principle 5 (Feature Agreement Principle).
Theagreeing categories and the agreement featuresare a-priori given based on linguistic knowledge,and are applied only at the semantic head level.Given all the above principles, we can now for-mulate the general Composition Principle:Principle 6 (Composition Principle).
A syntagma?
= (w,w?)
corresponding to the left-hand sidenonterminal of a grammar rule is obtained bystring concatenation (w = w1 .
.
.
wn) and thecomposition of semantic molecules correspondingto the nonterminals from the rule right-hand side:w?
=(hb)= (w1 ?
?
?wn)?
= w?1 ?
?
?
?
?
w?n=(h1b1)?
?
?
?
?
(hnbn)=(h1 ?
?
?
?
?
hn?b1, .
.
.
, bn??
)The composition of the semantic molecule bod-ies is realized through conjunction after the ap-plication of a variable substitution ?.
The bodyvariable specialization substitution ?
is the mostgeneral unifier (mgu) of b and b1, .
.
.
, bn, s.tb = (b1, .
.
.
, bn)?.
It is a particular form of thecommonly used substitution (Lloyd, 2003), i.e.,a finite set of the form {X1/Y1, .
.
.
, Xm/Ym},whereX1, .
.
.
, Xm, Y1, .
.
.
, Ym are variables, andX1, .
.
.
, Xm are distinct.The composition of the semantic moleculeheads is realized by a set of constraints?c(h, h1..., hn), which is a system of equations890similar to ?path equations?
(Shieber et al, 1983;van Noord, 1993), but applied to flat feature struc-tures:??
?hi.c = cthi.vi = hj .vjhi.f = ct orhi.f = hj .f???
where0 ?
i, j ?
n, i 6= jc ?
Achivi ?
Avhi , vj ?
Avhjf ?
Afhi , f ?
AfhjWhen learning a LWFG rule from a repre-sentative example ?
as in Figure 2, the robustparser returns the minimum number of chunks,n, covering ?.
The body variable substitution ?is fully determined by the representative exam-ple as mgu of b and b1, .
.
.
, bn, and the compo-sitional constraints ?c(h, h1, .
.
.
, hn) are learnedusing Alg 1.
For example, in Figure 2, whenlearning from the representative example corre-sponding to the string laser printer, we have that?
= {X1/B,X2/A,X3/A, Y/P1}.In Alg 1 we use the notation ?0 = (w0,(h0b0)) todenote the representative example ?.Alg 1: Learn Constraints(?0, ?1, .
.
.
, ?n)?i = (wi,`hibi?
), 0 ?
i ?
n?c ?
??
?
mgu(b0, (b1, .
.
.
, bn))foreach 0 ?
i ?
n ?
c ?
Achi do1 if hi.c = c1 then?c ?
?c ?
{hi.c = c1}foreach 0 ?
i, j ?
n ?
i 6= j ?X/Y ?
?
?2vi ?
Avhi ?
vj ?
Avhj doif hi.vi = X ?
hj .vj = Y then?c ?
?c ?
{hi.vi = hj .vj}if hs.head = h0.head, 1 ?
s ?
n then3foreach f ?
Afh0 ?
Afhs doif h0.f = c1 ?
hs.f = c1 then?c ?
?c ?
{h0.f = hs.f}if hs.cat = cs ?
hi.cat = ci ?
agr(cs, ci),1 ?
i ?
n thenforeach f ?
agrFeatures(cs, ci) doif hs.f = c1 ?
hi.f = c1 then?c ?
?c ?
{hs.f = hi.f}for all other f ?
Afhi , 0 ?
i ?
n do4 /*i.e., if we are not in case 3 */if hi.f = c1 then?c ?
?c ?
{hi.f = c1}return ?c /*i.e., ?c(h0, h1, .
.
.
, hn) */In the first step, the constraints correspondingto category attributes are fully determined by thevalues of these attributes that appear in the se-mantic molecule heads of ?0, .
.
.
?n.
In Figure2, when learning the most specific rule r fromthe representative example laser printer, the setof constraints {h.cat = nc, h1.cat = noun, h2 =noun} ?
?c4 are the constraints correspondingto category attributes.
In the second step, the con-straints corresponding to variable attributes arefully determined by the variables in the substitu-tion ?
that also appear as values of variable at-tributes hi.vi, hj .vj , where 0 ?
i, j ?
n andi 6= j.
In Figure 2, only {X2/A,X3/A} ?
?will be used, generating the set of constraints{h.head = h1.mod, h.head = h2.head} ?
?c4 .In the third step, the values of the feature at-tributes which obey Principles 4 and 5 are gen-eralized ?
agr(cs, ci) is the predicate which givesus the agreement between the categories cs andci (e.g., the subject agrees with the verb), andagrFeatures(cs, ci) gives us the set of feature at-tributes that participate in agreement (e.g., nr,pers, case).
In Figure 2, the set of constraints{h.nr = h2.nr} ?
?c4 represents the general-ization of the feature attribute values for nr, usingPrinciple 4 .
For all features attributes besides theones that obey the above two principles, the gener-ated constraints keep the particular values of theseattributes (step 4 of Alg 1).6 ExamplesThe LWFG formalism allows us to learn gram-mars for deep language understanding from ex-amples.
Instead of writing syntactic-semanticgrammar by hand (both rules and constraints),we need to provide only a small set of repre-sentative examples ?
strings and their semanticmolecules.
Qualitative experiments on learningLWFGs showed that complex linguistic construc-tions can be learned and covered, such as com-plex noun phrases, relative clauses and reducedrelative clauses, finite and non-finite verbal con-structions (including, tense, aspect, negation, andsubject-verb agreement), and raising and controlconstructions (Muresan and Rambow, 2007).
InFigure 3 we show an example of learning a LWFGgrammar for noun-noun compounds.
The firstfour examples (1-4) are representative examples,while the last four examples are used for gener-891A.
Learning Examples:1.
(laser,0BBBBB@264cat nahead Amod B375DA.isa = laser, B.P1=AE1CCCCCA) 5.
(laser printer manual,0BBBBB@264cat nahead Amod B375DC.isa = laser, D.P1=C, D.isa=printer,A.P2=D, A.isa=manual, B.P3=AE1CCCCCA)2.
(laser printer,0BBBBB@264cat nahead Amod B375DC.isa = laser, A.P1=C, A.isa=printer, B.P2=AE1CCCCCA) 6.
(desktop laser printer,0BBBBB@264cat nahead Amod B375DC.isa = desktop, A.P1=C, D.isa=laser,A.P2=D, A.isa=printer, B.P3=AE1CCCCCA)3.
(printer,0BBBBB@264cat ncnr sghead A375DA.isa = printerE1CCCCCA) 7.
(laser printer manual,0BBBBB@264cat ncnr sghead A375DB.isa = laser, C.P1=B, C.isa=printer, A.P2=C, A.isa=manualE1CCCCCA)4.
(laser printer,0BBBBB@264cat ncnr sghead A375DB.isa = laser, A.P1=B, A.isa=printerE1CCCCCA) 8.
(desktop laser printer,0BBBBB@264cat ncnr sghead A375DB.isa = desktop, A.P1=B, C.isa=laser, A.P2=C, A.isa=printerE1CCCCCA)B.
Learned LWFG Rules:NA(w,?hb?)?
Noun(w1 ,?h1b1?)
: ?c1 (h, h1) , where ?c1 (h, h1) =8><>:h.cat = nah1.cat = nounh.head = h1.headh.mod = h1.mod9>=>;NA(w,?hb?)?
NA(w1,?h1b1?
), NA(w2,?h2b2?)
: ?c2 (h, h1, h2) where ?c2 (h, h1, h2) =8>>><>>>:h.cat = nah1.cat = nah2.cat = nah.head = h1.modh.head = h2.headh.mod = h2.mod9>>>=>>>;NC(w,?hb?)?
Noun(w1,?h1b1?)
: ?c3 (h, h1) , where ?c3 (h, h1) =8><>:h.cat = nch1.cat = nounh.head = h1.headh.nr = h1.nr9>=>;NC(w,?hb?)?
NA(w1,?h1b1?
), NC(w2,?h2b2?)
: ?c4 (h, h1, h2) where ?c4 (h, h1, h2) =8>>><>>>:h.cat = nch1.cat = nah2.cat = nch.head = h1.modh.head = h2.headh.nr = h2.nr9>>>=>>>;Figure 3: Learning LWFG Rules for Noun-Noun Compoundsalization (5-8).
The learned grammar rules, in-cluding the learned composition constraints arealso shown.
The first two LWFG rules ground de-rive syntagmas for noun adjuncts, while the lasttwo rules ground derive syntagmas for noun com-pounds.
For example, ?desktop laser printer?
canbe either a fully-formed noun compound (cate-gory nc), or it can be further combined with thenoun ?invoice?
to obtain ?desktop laser printer in-voice?, case in which it is a noun adjunct (cate-gory na).
The learned rule for noun adjuncts isboth left and right recursive, accounting for bothleft and right-branching noun compounds.
Eventhough we can obtain overgeneralization in syn-tax, the ontology-based interpretation constraintat the rule level will prune some erroneous parses.Preliminary results in the medical domain showthat ?onto can help remove erroneous parses evenwhen using just a weak ontological model (se-mantic roles of verbs, prepositions, attributes ofadjectives and adverbs, but no synonymy, or hi-erarchy of concepts or roles).
However, more ex-periments need to be run for reporting quantitativeresults.7 ConclusionsWe have presented the properties and princi-ples that the semantic representation integratedin LWFG requires so that the semantic compo-sitional constraints are learnable from examples.These properties together with Alg 1 give a the-oretical result that in conjunction with the learn-ability result of Muresan and Rambow (2007)show that LWFG is a learnable constraint-basedgrammar formalism that can be used for deep lan-guage understanding.
Instead of writing grammarrules and constraints by hand, one needs to pro-vide only a small set of annotated examples.44The author acknowledges the support of the NSF (SGERgrant IIS-0838801).
Any opinions, findings, or conclusionsare those of the author, and do not necessarily reflect theviews of the funding organization.892ReferencesDzeroski, Saso.
2007.
Inductive logic programming ina nutshell.
In Getoor, Lise and Ben Taskar, editors,Introduction to Statistical Relational Learning.
TheMIT Press.Ge, Ruifang and Raymond J. Mooney.
2005.
A statis-tical semantic parser that integrates syntax and se-mantics.
In Proceedings of CoNLL-2005.Kay, M. 1986.
Algorithm schemata and data struc-tures in syntactic processing.
In Readings in naturallanguage processing, pages 35?70.
Morgan Kauf-mann Publishers Inc., San Francisco, CA, USA.Lloyd, John W. 2003.
Logic for Learning: Learn-ing Comprehensible Theories from Structured Data.Springer, Cognitive Technologies Series.Muggleton, Stephen.
1995.
Inverse Entailment andProgol.
New Generation Computing, Special Issueon Inductive Logic Programming, 13(3-4):245?286.Muresan, Smaranda and Owen Rambow.
2007.
Gram-mar approximation by representative sublanguage:A new model for language learning.
In Proceedingsof the 45th Annual Meeting of the Association forComputational Linguistics (ACL).Muresan, Smaranda.
2008.
Learning to map textto graph-based meaning representations via gram-mar induction.
In Coling 2008: Proceedings ofthe 3rd Textgraphs workshop on Graph-based Al-gorithms for Natural Language Processing, pages9?16, Manchester, UK, August.
Coling 2008 Orga-nizing Committee.Neumann, Gu?nter and Gertjan van Noord.
1994.
Re-versibility and self-monitoring in natural languagegeneration.
In Strzalkowski, Tomek, editor, Re-versible Grammar in Natural Language Processing,pages 59?96.
Kluwer Academic Publishers, Boston.Pollard, Carl and Ivan Sag.
1994.
Head-DrivenPhrase Structure Grammar.
University of ChicagoPress, Chicago, Illinois.Shieber, Stuart, Hans Uszkoreit, Fernando Pereira,Jane Robinson, and Mabry Tyson.
1983.
Theformalism and implementation of PATR-II.
InGrosz, Barbara J. and Mark Stickel, editors, Re-search on Interactive Acquisition and Use of Knowl-edge, pages 39?79.
SRI International, Menlo Park,CA, November.Shieber, Stuart, Yves Schabes, and Fernando Pereira.1995.
Principles and implementation of deductiveparsing.
Journal of Logic Programming, 24(1-2):3?36.Steedman, Mark.
1996.
Surface Structure and Inter-pretation.
The MIT Press.van Noord, Gertjan.
1993.
Reversibility in NaturalLanguage Processing.
Ph.D. thesis, University ofUtrecht.Wong, Yuk Wah and Raymond Mooney.
2007.
Learn-ing synchronous grammars for semantic parsingwith lambda calculus.
In Proceedings of the 45thAnnual Meeting of the Association for Computa-tional Linguistics (ACL-2007).Zettlemoyer, Luke S. and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In Proceedings of UAI-05.Zettlemoyer, Luke and Michael Collins.
2009.
Learn-ing context-dependent mappings from sentences tological form.
In Proceedings of the Association forComputational Linguistics (ACL?09).893
