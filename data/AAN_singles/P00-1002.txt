Generic NLP Technologies: Language, Knowledge andInformation ExtractionJunichi TsujiiDepartment of Information Science, Faculty of ScienceUniversity of Tokyo, JAPANAndCentre for Computational Linguistics, UMIST, UK1 IntroductionWe have witnessed signicant progress inNLP applications such as information ex-traction (IE), summarization, machine trans-lation, cross-lingual information retrieval(CLIR), etc.
The progress will be acceleratedby advances in speech technology, which notonly enables us to interact with systems viaspeech but also to store and retrieve texts in-put via speech.The progress of NLP applications in thisdecade has been mainly accomplished by therapid development of corpus-based and sta-tistical techniques, while rather simple tech-niques have been used as far as the structuralaspects of language are concerned.In this paper, we will discuss how wecan combine more sophisticated, linguisticallyelaborate techniques with the current statis-tical techniques and what kinds of improve-ment we can expect from such an integrationof dierent knowledge types and methods.2 Argument against linguisticallyelaborate techniquesThroughout the 80s, research based on lin-guistics hadourished even in application ori-ented NLP research such as machine transla-tion.
Eurotra, a European MT project, hadattracted a large number of theoretical lin-guists into MT and the linguists developedclean and linguistically elaborate frameworkssuch as CTA-2, Simple Transfer, Eurotra-6,etc.ATR, a Japanese research institute fortelephone dialogue translation supported bya consortium of private companies and theMinistry of Post and Communication, alsoadopted a linguistics-based framework, al-though they changed their direction in thelater stage of the project.
They also adoptedsophisticated plan-based dialogue models aswell at the initial stage of the project.However, the trend changed rather dras-tically in the early 90s and most researchgroups with practical applications in mindgave up such strategies and switched to morecorpus-oriented and statistical methods.
In-stead of sentential parsing based on linguis-tically well founded grammar, for example,they started to use simpler but more ro-bust techniques based on nite-state models.Neither did knowledge-based techniques likeplan-recognition, etc.
survive, which presumeexplicit representation of domain knowledge.One of the major reasons for the failureof these techniques is that, while these tech-niques alone cannot solve the whole range ofproblems that NLP application encounters,both linguists and AI researchers made strongclaims that their techniques would be able tosolve most, if not all, of the problems.
Al-though formalisms based on linguistic theo-ries can certainly contribute to the develop-ment of clean and modular frameworks forNLP, it is rather obvious that linguistics the-ories alone cannot solve most of NLP's prob-lems.
Most of MT's problems, for example,are related with semantics or interpretationof language which linguistic theories of syntaxcan hardly oer solutions for (Tsujii 1995).However, this does not imply, either, thatframeworks based on linguistic theories are ofno use for MT or NLP application in general.This only implies that we need techniquescomplementary to those based on linguistictheories and that frameworks based on lin-guistic theories should be augmented or com-bined with other techniques.
Since techniquesfrom complementary elds such as statisticalor corpus-based ones have made signicantprogresses, it is our contention in this paperthat we should start to think seriously aboutcombining the fruits of the research results ofthe 80s with those of the 90s.The other claims against linguistics-basedand knowledge-based techniques which haveoften been made by practical-minded peopleare :(1) Eciency: The techniques such as sen-tential parsing and knowledge-based in-ference, etc.
are slow and require a largeamount of memory(2) Ambiguity of Parsing: Sententialparsing tends to generate thousands ofparse results from which systems cannotchoose the correct one.
(3) Incompleteness of Knowledge andRobustness: In practice one cannotprovide systems with complete knowl-edge.
Defects in knowledge often causefailures in processing, which result in thefragile behavior of systems.While these claims may have been the caseduring the 80s, the steady progress of suchtechnologies have largely removed these dif-culties.
Instead, the disadvantages of cur-rent technologies based on nite state tech-nologies, etc.
have increasingly becomeclearer; the disadvantages such ad-hocnessand opaqueness of systems which preventthem from being transferred from an appli-cation in one domain to another domain.3 The current state of the JSPSprojectIn a ve-year project funded by JSPS (JapanSociety of Promotion of Science) whichstarted in September 1996, we have focussedour research on generic techniques that willbe used for dierent kinds of NLP applicationand domains.The project comprises three universitygroups from the University of Tokyo, TokyoInstitute of Technology (Prof. Tokunaga)and Kyoto University (Dr. Kurohashi), andcoordinated by myself (at the University ofTokyo).
The University of Tokyo has beenengaged in development of software infras-tructure for ecient NLP, parsing technologyand ontology building from texts, while thegroups of Tokyo Institute of Technology andKyoto University have been responsible forNLP application to IR and Knowledge-basedNLP techniques, respectively.Since we have delivered promising results inresearch on generic NLP methods, we are nowengaged in developing several application sys-tems that integrate various research results toshow their feasibility in actual application en-vironments.
One such application is a systemthat helps biochemists working in the eld ofgenome research.The system integrates various research re-sults of our project such as new techniquesfor query expansion and intelligent indexingin IR, etc.
The two results to be integratedinto the system that we focus on in this paperare IE using a full-parser (sentential parserbased on grammar) and ontology buildingfrom texts.IE is very much in demand in genome re-search, since quite a large portion of researchis now being targeted to construct systemsthat model complete sequences of interac-tion of various materials in biological organ-isms.
These systems require extraction of rel-evant information from texts and its integra-tion in xed formats.
This entails that theresearchers there should have a model of in-teraction among materials, into which actualpieces of information extracted from texts aretted.
Such a model should have a set ofclasses of interaction (event classes) and a setof classes of entities that participate in events.That is, the ontology of the domain should ex-ist.
However, since the building of an ultimateontology is, in a sense, the goal of science,the explicit ontology exists only in a very re-stricted and partial form.
In other words, IEand Ontology building are inevitably inter-twined here.In short, we found that IE and Ontologybuilding from texts in genome research pro-vide an ideal test bed for our generic NLPtechniques, namely software infrastructure forecient NLP, parsing technology, and ontol-ogy building from texts with initial partialknowledge of the domain.4 Software Infrastructure andParsing TechnologyWhile tree structures are a versatile schemefor linguistic representation, invention of fea-ture structures that allow complex featuresand reentrancy (structure sharing) makeslinguistic representation concise and allowsdeclarative specications of mutual relation-ships among representation of dierent lin-guistic levels (e.g.
: morphology, syntax, se-mantics, discourse, etc.).
More importantly,using bundles of features instead of simplenon-terminal symbols to characterize linguis-tic objects allow us to use much richer statis-tical means such as ME (maximum entropymodel), etc.
instead of simple probabilisticCFG.
However, the potential has hardly beenpursued yet mostly due to the ineciency andfragility of parsing based on feature-based for-malisms.In order to remove the eciency obstacle,we have in the rst two years devoted our-selves to the development of :(A) Software infrastructure that makes pro-cessing of feature-based formalisms e-cient enough both for practical applica-tion and for combining it with statisticalmeans.
(B) Grammar (Japanese and English) withwide coverage for processing real worldtexts (not examples in textbooks of lin-guistics).
At the same time, processingtechniques that make a system robustenough for application.
(C) Ecient parsing algorithm forlinguistics-based frameworks, in particu-lar HPSG.We describe the current states of these threein the following.
(A) Software Infrastructure (Miyao2000):We designed and develop a programming sys-tem, LiLFeS, which is an extension of Pro-log for expressing typed feature structures in-stead of rst order terms.
The system's coreengine is an abstract machine that can pro-cess features and execute denite clause pro-gram.
While similar attempts treat featurestructure processing separately from that ofdenite clause programs, the LiLFeS abstractmachine increases processing speed by seam-lessly processing feature structures and de-nite clause programs.Diverse systems, such as large scale Englishand Japanese grammar, a statistical disam-biguation module for the Japanese parser, arobust parser for English, etc., have alreadybeen developed in the LiLFeS system.We compared the performance of the sys-tem with other systems, in particular withLKB developed by CSLI, Stanford Univer-sity, by using the same grammar (LinGo alsoprovided by Stanford University).
A parsingsystem in the LiLFeS system, which adoptsa naive CKY algorithm without any sophis-tication, shows similar performance as thatof LKB which uses a more rened algorithmto lter out unnecessary unication.
The de-tailed examination reveals that feature uni-cation of the LiLFeS system is about fourtimes faster than LKB.Furthermore, since LiLFeS has quite a fewbuilt-in functions that facilitate fast sub-sumption checking, ecient memory manage-ment, etc., the performance comparison re-veals that more advanced parsing algorithmslike the one we developed in (C) can benetfrom the LiLFeS system.
We have almost n-ished the second version of the LiLFeS systemthat uses a more ne-grained instruction set,directly translatable to naive machine code ofa Pentium CPU.
The new version shows morethan twice improvement in execution speed,which means the naive CKY algorithm with-out any sophistication in the LiLFeS systemwill outperform LKB.
(B) Grammar with wide coverage(Tateisi 1998; Mitsuishi 1998):While LinGo that we used for comparison isan interesting grammar from the view pointof linguistics, the coverage of the grammar israther restricted.
We have cooperated withthe University of Pennsylvania to develop agrammar with wide coverage.
In this co-operation, we translated an existing wide-coverage grammar of XTAG to the frameworkof HPSG, since our parsing algorithms in (C)all assume that the grammar are HPSG.
Aswe discuss in the following section, we willuse this translated grammar as the core gram-mar for information extraction from texts ingenome science.As for wide-coverage Japanese Gram-mar, we have developed our own grammar(SLUNG) .
SLUNG exploits the propertyof HPSG that allows under-specied con-straints.
That is, in order to obtain wide-coverage from the very beginning of grammardevelopment, we only give loose constraintsto individual words that may over-generatewrong interpretations but nonetheless guar-antee correct ones to be always generated.Instead of rather rigid and strict con-straints, we prepare 76 templates for lexicalentries that specify behaviors of words be-longing to these 76 classes.
The approachis against the spirit of HPSG or lexicalizedgrammar that emphasizes constraints specicto individual lexical items.
However, ourgoal is rst to develop wide-coverage gram-mar that can be improved by adding lexical-item specic constraints in the later stageof grammar development.
The strategy hasproved to be eective and the current gram-mar can produce successful parse results for98.3 % of sentences in the EDR corpus withhigh eciency (0.38 sec per sentence for theEDR corpus).
Since the grammar overgen-erates, we have to choose single parse resultsamong a combinatorially large number of pos-sible parses.
However, an experiment showsthat a statistic method using ME (we use theprogram for ME developed by NYU) can se-lect around 88.6 % of correct analysis in termsof dependency relationships among !
!
bun-setsu's - the phrases in Japanese).
(C) Ecient parsing algorithm(Torisawa 2000):While feature structure representation pro-vides an eective means of representing lin-guistic objects and constraints on them,checking satisability of constraints by lin-guistic objects, i.e.
unication, is computa-tionally expensive in terms of time and space.One way of improving the eciency is to avoidunication operations as much as possible,while the other way is to provide ecient soft-ware infrastructure such as in (A).
Once wechoose a specic task like parsing, genera-tion, etc., we can devise ecient algorithmsfor avoiding unication.LKB accomplishes such reduction by in-specting dependencies among features, whilethe algorithm we chose is to reduce necessaryunication by compiling given HPSG gram-mar into CFG.
The CFG skeleton of givenHPSG, which is semi-automatically extractedfrom the original HPSG, is applied to pro-duce possible candidates of parse trees in therst phase.
The skeletal parsing based on ex-tracted CFG lters out the local constituentstructures which do not contribute to anyparse covering the whole sentence.
Since alarge proportion of local constituent struc-tures do not actually contribute to the wholeparse, this rst CFG phase helps the secondphase to avoid most of the globally mean-ingless unication.
The eciency gain bythis compilation technique depends on the na-ture of the original grammar to be compiled.While the eciency gain for SLUNG is justtwo times, the gain for XHPSG (HPSG gram-mar obtained by translating the XTAG gram-mar into HPSG) is around 47 times for theATIS corpus (Tateisi 1998).5 Information extraction bysentential parsingThe basic arguments against use of sententialparsing in practical application such as IE arethe ineciency in terms of time and space,the fragility of systems based on linguisticallyrigid frameworks and highly ambiguous parseresults that we often have as results of pars-ing.On the other hand, there are argumentsfor sentential parsing or the deep analysisapproach.
One argument is that an ap-proach based on linguistically sound frame-works makes systems transparent and easy tore-use.
The other is the limit on the qual-ity that is achievable by the pattern match-ing approach.
While a higher recall rate ofIE requires a large amount of patterns tocover diverse surface realization of the sameinformation, we have to widen linguistic con-texts to improve the precision by preventingextraction of false information.
A pattern-based system may end up with a set of pat-terns whose complex mutual nullify the initialappeal of simplicity of the pattern-based ap-proach.As we see in the previous section, the e-ciency problem becomes less problematic byutilizing the current parsing technology.
Itis still a problem when we apply the deepanalysis to texts in the eld of genome sci-ence, which tend to have much longer sen-tences than in the ATIS corpus.
However, asin the pattern-based approach, we can reducethe complexity of problems by combining dif-ferent techniques.In a preliminary experiment, we rst use ashallow parser (ENGCG) to reduce part-of-speech ambiguities before sentential parsing.Unlike statistic POS taggers, the constraintgrammar adopted by ENGCG preserves allpossible POS interpretations just by droppinginterpretations that are impossible in given lo-cal contexts.
Therefore, the use of ENGCGdoes not aect the soundness and complete-ness of the whole system, while it reduces sig-nicantly the local ambiguities that do notcontribute to the whole parse.The experiment shows that ENGCG pre-vents 60 % of edges produced by a parserBased on naive CKY algorithm, when it is ap-plied to 180 sentences randomly chosen fromMEDLINE abstracts (Yakushiji 2000).
As aresult, the parsing by XHPSG becomes fourtimes faster from 20.0 seconds to 4.8 secondper sentence, which is further improved by us-ing chunking based on the output of a NamedEntity recognition tool to 2.57 second per sen-tence.
Since the experiment was conductedwith a naive parser based on CYK and theold version of LiLFeS, the performance canbe improved further.The problems of fragility and ambiguitystill remain.
XHPSG fails to produce parsesfor about half of the sentences that coverthe whole.
However, in application such asIE, a system needs not have parses coveringthe whole sentence.
If the part in which therelevant pieces of information appear can beparsed, the system can extract them.
This isone of the major reasons why pattern-basedsystems can work in a robust manner.
Thesame idea can be used in IE based on sen-tential parser.
That is, techniques that canextract information from partial parse resultswill make the system robust.The problem of ambiguity can be treated ina similar manner.
In a pattern-based system,the system extracts information when parts ofthe text match with a pattern, independentlyof whether other interpretations that competewith the interpretation intended by the pat-tern exist or not.
In this way, a pattern-basedsystem treats ambiguity implicitly.
In caseof the approach based on sentential parsing,we treat the ambiguity problem by preference.That is, an interpretation that indicates rel-evant pieces of information exist is preferredto other interpretations.Although the methods illustrated in theabove make IE based on sentential pars-ing similar to the pattern-based approach,the approach retains the advantages over thepattern-based one.
For example, it can pre-vent false extraction if the pattern that dic-tates extraction contradicts with wider lin-guistic structures or with the more preferredinterpretations.
It keeps separate the generallinguistic knowledge embodied in the form ofXHPSG grammar that can be used in any do-main.
The mapping between syntactic struc-tures to predicate structures can also be sys-tematic.6 Information extraction of namedentities using a hidden MarkovmodelThe named entity tool mentioned above,called NEHMM (Collier 2000), has been de-veloped as a generalizable supervised learningmethod for identifying and classifying termsgiven a training corpus of SGML marked-uptexts.
HMMs themselves belong to a class oflearning algorithms that can be considered tobe stochastic nite state machines.
They haveenjoyed success in a wide number of elds in-cluding speech recognition and part of speechtagging.
We therefore consider their exten-sion to the named entity task, which is es-sentially a kind of semantic tagging of wordsbased on their class, to be quite natural.NEHMM itself strives to be highly gen-eralizable to terms in dierent domains andthe initial version uses bigrams based on lex-ical and character features with one stateper name class.
Data-sparseness is over-come using the character features and linear-interpolation.Nobata et al (Nobata 1999) comment onthe particular diculties with identifying andclassifying terms in the biochemistry domainincluding an open vocabulary and irregularnaming conventions as well as extensive cross-over in vocabulary between classes.
The irreg-ular naming arises in part because of the num-ber of researchers from dierent elds whoare working on the same knowledge discov-ery area as well as the large number of pro-teins, DNA etc.
that need to be named.
De-spite the best eorts of major journals to stan-dardize the terminology, there is also a sig-nicant problem with synonymy so that of-ten an entity has more than one name that iswidely used such as the protein names AKTand PKB.
Class cross-over of terms is anotherproblem that arises because many DNA andRNA are named after the protein with whichthey transcribe.Despite the apparent simplicity of theknowledge in NEHMM, the model has provento be quite powerful in application.
In thegenome domain with only 80 training MED-LINE abstracts it could achieve over 74% F-score (a common metric for evaluation used inIE that combines recall and precision).
Simi-lar performance has been found when trainingusing the dry-run and test set for MUC-6 (60articles) in the news domain.The next stage in the development of ourmodel is to train using larger test sets andto incorporate wider contextual knowledge,perhaps by marking-up for dependencies ofnamed-entities in the training corpus.
Thisextra level of structural knowledge shouldhelp to constrain class assignment and alsoto aid in higher levels of IE such as event ex-traction.7 Knowledge Building and TextAnnotationAnnotated corpora constitute not only an in-tegral part of a linguistic investigation butalso an essential part of the design methodol-ogy for an NLP systems.
In particular, the de-sign of IE systems requires clear understand-ing of information formats of the domain, i.e.what kinds of entities and events are consid-ered as essential ingredients of information.However, such information formats are oftenimplicit in the minds of domain specialistsand the process of annotating texts helps toreveal them.It is also the case that the mapping be-tween information formats and surface lin-guistic realization is not trivial and that cap-turing the mapping requires empirical exam-ination of actual corpora.
While generic pro-grams with learning ability may learn sucha mapping, learning algorithms need trainingdata, i.e.
annotated corpora.In order to design a NE recognition pro-gram, for example, we have to have a reason-able amount of annotated texts which showin what linguistic contexts named entities ap-pear and what internal structures typical lin-guistic expressions of named entities of a giveneld have.
Such human inspection of anno-tated texts suggests feasible tools for NE (e.g.HMM, ME, decision trees, dictionary look-up,etc.)
and a set of feasible features, if one usesprograms with learning ability.
Human in-spection of annotated corpora is still an in-evitable step of feature selection, even if oneuses programs with learning ability.More importantly, to determine classes ofnamed entities and events which should re-ect the views of domain specialists requiresempirical investigation, since these often existimplicitly only in the mind of specialists.
Thisis particularly the case in the eld of med-ical and biological sciences, since they havea much larger collection of terms (i.e.
classnames) than, for example, mathematical sci-ence, physics, etc.In order to see the magnitude of the workand diculties involved, we chose a well-circumscribed eld and collected texts (MED-LINE abstracts) in the eld to be annotated.The eld is the reaction of transcription fac-tors in human blood cells.
The kinds of infor-mation that we try to extract are the infor-mation on protein-protein interactions.The eld was chosen because a researchgroup of National Health Research Instituteof the Ministry of Health in Japan is buildinga database called CSNDB (Cell Signal Net-work DB), which gathers this type of infor-mation.
They read papers every week to ex-tract relevant information and store them inthe database.
IE of this eld can reduce thework that is done manually at present.We selected abstracts from MEDLINE bythe key words of "human", "transcription fac-tors" and "blood cells", which yield 3300 ab-stracts.
The abstracts are from 100 to 200words in length.
500 abstracts were chosenrandomly and annotated.
Currently, seman-tic annotation of 300 abstracts has been n-ished and we expect 500 abstracts to be doneby April (Ohta 2000).The task of annotation can be regarded asidentifying and classifying the terms that ap-pear in texts according to a pre-dened clas-sication scheme.
The classication scheme,in turn, reects the view of the elds that bio-chemists have.
That is, semantic tags we useare the class names in an ontology of the eld.Ontologies of biological terminology havebeen created in projects such as the EUfunded GALEN project to provide a modelof biological concepts that can be used tointegrate heterogeneous information sourceswhile some ontologies such as MeSH are builtfor the purpose of information retrieval Ac-cording to their purposes, ontologies dierfrom ne-grained to coarse ones and from as-sociative to logical ones.
Since there is noappropriate ontology that covers the domainthat we are interested in, we decided to buildone for this specic domain.The design of our ontology is in progress,in which we distinguish classication basedon roles that proteins play in events fromthat based on internal structures of proteins.The former classication is closely linked withclassication of events.
Since classication isbased on feature lattices, we plan to use theLiLFeS system to dene these classicationschemes and their relationships among them.8 Future DirectionsWhile the researches of the 80s and 90s inNLP focussed on dierent aspects of lan-guage, they have been so far considered sepa-rate development and no serious attempt hasbeen made to integrate them.In the JSPS project, we have prepared nec-essary background for such integration.
Tech-nological background such as ecient parsing,a programming system based on types, etc.will contribute to resolving eciency prob-lems.
The techniques such as NE recogni-tion, staged architecture in conventional IE,etc.
will give hints on how to incorporate sev-eral dierent techniques in the whole system.A reasonable size of semantically annotatedtexts, together with relevant ontology, havebeen prepared.We are engaged now in integrating thesecomponents in the whole system, in order toshow how theoretical work, together with col-lection of empirical data, can facilitate sys-tematic development of NLP application sys-tems.ReferencesCollier, N., Nobata, C., and Tsujii, J.: "Extract-ing the Names of Genes and Gene products witha Hidden Markov Model", COLING'2000 (Au-gust), 2000Mitsuishi, Y.
et.al.
: HPSG-style UnderspeciedJapanese Grammar with Wide Coverage, inProc.
of Coling-ACL 98, Montreal, 1998Miyao, Y., Makino, T., et.al.
: The LiLFeS Ab-stract Machine and its Evaluation with LinGo,A Special Issue on Ecient Processing ofHPSG, Journal of Natural Language Engineer-ing, Cambridge University Press, 2000 (to ap-pear)Nobata, C., Collier, N., and Tsujii, J.: "Au-tomatic Term Identication and Classicationin Biology Texts", in proceedings of the Nat-ural Language Pacic Rim Symposium (NL-PRS'99), Beijing, China, 1999.Ohta, T., et.al.
: A Semantically Tagged Corpusbased on an Ontology for Molecular Biology, inProc.
of JSPS Symposium 2000, Tokyo, 2000Tateisi, Y.
et.al.
: Translating the XTAG EnglishGrammar to HPSG, in Proc.
of TAG+4 work-shop, University of Pennsylvania, 1998Torisawa, K.
et.al.
: An HPSG Parser with CFGFiltering, A Special Issue on Ecient Process-ing of HPSG, Journal of Natural language Pro-cessing, Cambridge University Press, 2000 (toappear)Tsujii, J.: MT Research : Productivity and Con-ventionality of Language, RANLP-95,TzigovChark,Bulgaria,14-16 September,1995Yakushiji, A.: Domain-Independent System forEvent Frame Extraction using an HPSG Parser,Bsc Dissertation, Department of InformationScience, University of Tokyo, 2000
