Proceedings of the 10th Conference on Parsing Technologies, pages 133?143,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsDependency Parsing with Second-Order Feature Maps and AnnotatedSemantic InformationMassimiliano CiaramitaYahoo!
ResearchOcata 1, S-08003Barcelona, Spainmassi@yahoo-inc.comGiuseppe AttardiDipartimento di InformaticaUniversita` di PisaL.
B. Pontecorvo 3, I-56127Pisa, Italyattardi@di.unipi.itAbstractThis paper investigates new design optionsfor the feature space of a dependency parser.We focus on one of the simplest and mostefficient architectures, based on a determin-istic shift-reduce algorithm, trained with theperceptron.
By adopting second-order fea-ture maps, the primal form of the perceptronproduces models with comparable accuracyto more complex architectures, with no needfor approximations.
Further gains in accu-racy are obtained by designing features forparsing extracted from semantic annotationsgenerated by a tagger.
We provide experi-mental evaluations on the Penn Treebank.1 IntroductionA dependency tree represents a sentence as a labeleddirected graph encoding syntactic and semantic in-formation.
The labels on the arcs can represent ba-sic grammatical relations such as ?subject?
and ?ob-ject?.
Dependency trees capture grammatical struc-tures that can be useful in several language process-ing tasks such as information extraction (Culotta &Sorensen, 2004) and machine translation (Ding &Palmer, 2005).
Dependency treebanks are becomingavailable in many languages, and several approachesto dependency parsing on multiple languages havebeen evaluated in the CoNLL 2006 and 2007 sharedtasks (Buchholz & Marsi, 2006; Nivre et al, 2007).Dependency parsing is simpler than constituencyparsing, since dependency trees do not have extranon-terminal nodes and there is no need for a gram-mar to generate them.
Approaches to dependencyparsing either generate such trees by considering allpossible spanning trees (McDonald et al, 2005), orbuild a single tree by means of shift-reduce parsingactions (Yamada & Matsumoto, 2003).
Determinis-tic dependency parsers which run in linear time havealso been developed (Nivre & Scholz, 2004; Attardi,2006).
These parsers process the sentence sequen-tially, hence their efficiency makes them suitable forprocessing large amounts of text, as required, for ex-ample, in information retrieval applications.Recent work on dependency parsing has high-lighted the benefits of using rich feature setsand high-order modeling.
Yamada and Mat-sumoto (2003) showed that learning an SVM modelin the dual space with higher-degree polynomial ker-nel functions improves significantly the parser?s ac-curacy.
McDonald and Pereira (2006) have shownthat incorporating second order features relating toadjacent edge pairs improves the accuracy of max-imum spanning tree parsers (MST).
In the SVM-based approach, if the training data is large, it is notfeasible to train a single model.
Rather, Yamada andMatsumoto (see also (Hall et al, 2006)) partition thetraining data in different sets, on the basis of Part-of-Speech, then train one dual SVM model per set.While this approach simplifies the learning task itmakes the parser more sensitive to the error rate ofthe POS tagger.
The second-order MST algorithmhas cubic time complexity.
For non-projective lan-guages the algorithm is NP-hard and McDonald andPereira (2006) introduce an approximate algorithmto handle such cases.In this paper we extend shift reduce parsing withsecond-order feature maps which explicitly repre-133sent all feature pairs.
Also the augmented fea-ture sets impose additional computational costs.However, excellent efficiency/accuracy trade-off isachieved by using the perceptron algorithm, with-out the need to resort to approximations, producinghigh-accuracy classifiers based on a single model.We also evaluate a novel set of features for pars-ing.
Recently various forms of shallow semanticprocessing have been investigated such as named-entity recognition (NER), semantic role labeling(SRL) and relation extraction.
Syntactic parsing canprovide useful features for these tasks; e.g., Pun-yakanok et al (2005) show that full parsing is effec-tive for semantic role labeling (see also related ap-proaches evaluated within the CoNNL 2005 sharedtask (Carreras et al, 2005)).
However, no evidencehas been provided so far that annotated semanticinformation can be leveraged for improving parserperformance.
We report experiments showing thatadding features extracted by an entity tagger im-proves the accuracy of a dependency parser.2 Dependency parsingA dependency parser takes as input a sentence s andreturns a dependency graph d. Figure 1 shows a de-pendency tree for the sentence ?Last week CBS Inc.canceled ?The People Next Door?.?1.
Dependenciesare represented as labeled arrows from the head ofthe relation to the modifier word; thus, in the exam-ple, ?Inc.?
is the modifier of a dependency labeled?SUB?
(subject) to the main verb, the head, ?can-celed?.In statistical syntactic parsing a generator (e.g.,a PCFG) is used to produce a number of candi-date trees (Collins, 2000) with associated proba-bility scores.
This approach has been used alsofor dependency parsing, generating spanning treesas candidates and computing the maximum span-ning tree (MST) using discriminative learning algo-rithms (McDonald et al, 2005).
Second-order MSTdependency parsers currently represent the state ofthe art in terms of accuracy.
Yamada and Mat-sumoto (2003) proposed a deterministic classifier-based parser.
Instead of learning directly whichtree to assign to a sentence, the parser learns which1The figure also contains entity annotations which will beexplained below in Section 4.1.Shift/Reduce actions to use in building the tree.
Pars-ing is cast as a classification problem: at each stepthe parser applies a classifier to the features rep-resenting its current state to predict which actionto perform on the tree.
Similar deterministic ap-proaches to parsing have been investigated also inthe context of constituent parsing (Wong & Wu,1999; Kalt, 2004).Nivre and Scholz (2004) proposed a variant of themodel of Yamada and Matsumoto that reduces thecomplexity, from the worst case quadratic to linear.Attardi (2006) proposed a variant of the rules thathandle non-projective relations while parsing deter-ministically in a single pass.
Shift-reduce algorithmsare simple and efficient, yet competitive in termsof accuracy: in the CoNLL-X shared task, for sev-eral languages, there was no statistically significantdifference between second-order MST parsers andshift-reduce parsers.3 A shift-reduce parserWe build upon DeSR, the shift-reduce parser de-scribed in (Attardi, 2006).
This and Nivre andScholz?s (2004) provide among the simplest andmost efficient methods.
This parser constructs de-pendency trees by scanning input sentences in asingle left-to-right pass and performing shift/reduceparsing actions.
The parsing algorithm is fully de-terministic and has linear complexity.
The parser?sbehavior can be described as repeatedly selectingand applying a parsing rule to transform its state,while advancing through the sentence.
Each to-ken is analyzed once and a decision is made lo-cally concerning the action to take, that is, withoutconsidering global properties of the tree being built.Nivre (2004) investigated the issue of (strict) incre-mentality for this type of parsers; i.e., if at any pointof the analysis the processed input forms one con-nected structure.
Nivre found that strict incremen-tality is not guaranteed within this parsing frame-work, although for correctly parsed trees the prop-erty holds in almost 90% of the cases.3.1 Parsing algorithmThe state of the parser is represented by a triple?S, I,A?, where S is the stack, I is the list of inputtokens that remain to be processed and A is the arc134Figure 1.
A dependency tree from the Penn Treebank, with additional entity annotation from the BBN corpus.relation for the dependency graph, which consists ofa set of labeled arcs (wi, r, wj), where wi, wj ?
W(the set of tokens), d ?
D (the set of dependencies).Given an input sentence s, the parser is initializedto ?
?, s, ?
?, and terminates at configuration ?s, ?, A?.There are three parsing schemata:Shift ?S,n|I,A??n|S,I,A?(1)Rightr?s|S,n|I,A??S,n|I,A?{(s,r,n)}?(2)Leftr?s|S,n|I,A??S,s|I,A?{(n,r,s)}?
(3)The Shift rule advances on the input; each Leftr andRightr rule creates a link r between the next inputtoken n and the top token on the stack s. For produc-ing labeled dependencies the rules Leftr and Rightrare instantiated several times once for each depen-dency label.Additional parsing actions (cf.
(Attardi, 2006))have been introduced for handling non-projectivedependency trees: i.e., trees that cannot be drawnin the plane without crossing edges.
However, theyare not needed in the experiments reported here,because in the Penn Treebank used in our experi-ments dependencies are extracted without consider-ing empty nodes and the resulting trees are all pro-jective2.The pseudo code in Algorithm 1 reproducesschematically the parsing process.The function getContext() extracts a vector offeatures x relative to the structure built up to thatpoint from the context of the current token, i.e., froma subset of I , S and A.
The step estimateAction()predicts a parsing action y, given a trained model ?2Instead, the version of the Penn Treebank used for theCoNLL 2007 shared task includes also non-projective represen-tations.Algorithm 1: DeSR: Dependency Shift Reduceparser.input: s = w1, w2, ..., wnbeginS ?
?
?I ?
?w1, w2, ..., wn?A?
?
?while I 6= ??
dox?
getContext(S, I,A)y ?
estimateAction(x, ?
)performAction(y, S, I, A)endand x.
The final step performAction() updates thestate according to the predicted parsing rule.3.2 FeaturesThe set of features used in this paper were chosenwith a few simple experiments on the developmentdata as a variant of a generic model.
The only fea-tures of the tokens used are ?Lemma?, ?Pos?
and?Dep?
: ?Lemma?
refers to the morphologically sim-plified form of the token, ?Pos?
is the Part-of-Speechand ?Dep?
is the label on a dependency.
?Child?refers to the child of a node (right or left): up totwo furthest children of a node are considered.
Ta-ble 1 lists which feature is extracted for which to-ken: negative numbers refer to tokens on the stack,positive numbers refer to input tokens.
As an exam-ple, POS(-1) is the Part-of-Speech of the token onthe top of the stack, while Lemma(0) is the lemmaof the next token in the input, PosLeftChild(-1) ex-tracts the Part-of-Speech of the leftmost child of thetoken on the top of the stack, etc.135TOKENFEATURES Stack InputLemma -2 -1 0 1 2 3Pos -2 -1 0 1 2 3LemmaLeftChild -1 0PosLeftChild -1 0DepLeftChild -1 0LemmaRightChild -1 0PosRightChild -1 0DepRightChild -1LemmaPrev 0PosSucc -1Table 1.
Configuration of the feature parameters used inthe experiments.3.3 Learning a parsing model with theperceptronThe problem of learning a parsing model can beframed as a classification task where each classyi ?
Y represents one of k possible parsing actions.Each of such actions is associated with a weight vec-tor ?k ?
IRd.
Given a datapoint x ?
X , a d-dimensional vector of binary features in the inputspace X , a parsing action is chosen with a winner-take-all discriminant function:estimateAction(x, ?)
= argmaxkf(x, ?k) (4)when using a linear classifier, such as the perceptronor SVM, f(u,v) = ?u,v?
is the inner product be-tween vectors u and v.We learn the parameters ?
from the training datawith the perceptron (Rosemblatt, 1958), in the on-line multiclass formulation of the algorithm (Cram-mer & Singer, 2003) with uniform negative updates.The perceptron has been used in previous work ondependency parsing by Carreras et al (2006), witha parser based on Eisner?s algorithm (Eisner, 2000),and also on incremental constituent parsing (Collins& Roark, 2006).
Also the MST parser of McDonalduses a variant of the perceptron algorithm (McDon-ald, 2006).
The choice is motivated by the simplicityand performance of perceptrons, which have provedcompetitive on a number of tasks; e.g., in shallowparsing, where perceptron?s performance is com-parable to that of Conditional Random Field mod-els (Sha & Pereira, 2003).The only adjustable parameter of the model is thenumber of instances T to use for training.
We fixedT using the development portion of the data.
Inour experiments, the best value is between 20 and30 times the size of the training data.
To regularizethe model we take as the final model the average ofall weight vectors posited during training (Collins,2002).
Algorithm 2 illustrates the perceptron learn-ing procedure.
The final average model can be com-puted efficiently during training without storing theindividual ?
vectors (e.g., see (Ciaramita & Johnson,2003)).Algorithm 2: Average multiclass perceptroninput : S = (xi, yi)N ;?0k = ~0, ?k ?
Yfor t = 1 to T dochoose jEt = {r ?
Y : ?xj , ?tr?
?
?xj , ?tyj ?
}if |Et| > 0 then?t+1r = ?tr ?xj|Et| , ?r ?
Et?t+1yj = ?tyj + xjoutput: ?k = 1T?t ?tk, ?k ?
Y3.4 Higher-order feature spacesYamada and Matsumoto (2003) and McDonald andPereira (2006) have shown that higher-order fea-ture representations and modeling can improve pars-ing accuracy, although at significant computationalcosts.
To make SVM training feasible in the dualmodel with polynomial kernels, Yamada and Mat-sumoto split the training data into several sets, basedon POS tags, and train a parsing model for eachset.
McDonald and Pereira?s second-order MSTparser has O(n3) complexity, while for handlingnon-projective trees, otherwise an NP-hard problem,the parser resorts to an approximate algorithm.
Herewe discuss how the feature representation can beenriched to improve parsing while maintaining thesimplicity of the shift-reduce architecture, and per-forming discriminative learning without partitioningthe training data.The linear classifier (see Equation 4) learned withthe perceptron is inherently limited in the types ofsolutions it can learn.
As originally pointed out byMinsky and Papert (1969), there are problems whichrequire non-linear solutions that cannot be learnedby such models.
A simple workaround this limi-tation relies on feature maps ?
: IRd ?
IRh that136map the input vectors x ?
X into some higher h-dimensional representation ?
(X ) ?
IRh, the fea-ture space.
The feature space can represent, for ex-ample, all combinations of individual features in theinput space.
We define a feature map which ex-tracts all second order features of the form xixj ;i.e., ?
(x) = (xi, xj |i = 1, ..., d, j = i, ..., d).
Thelinear perceptron working in ?
(X ) effectively im-plements a non-linear classifier in the original in-put space X .
One shortcoming of this approach isthat it inflates considerably the feature representa-tion and might not scale.
In general, the number offeatures of degree g over an input space of dimen-sion d is(d+g?1g).
In practice, a second-order fea-ture map can be handled with reasonable efficiencyby the perceptron.
We call this the 2nd-order model,which uses a modified scoring function:g(x, ?k) = f(?
(x), ?k) (5)where also ?k is h-dimensional.
The proposed fea-ture map is equivalent to a polynomial kernel func-tion of degree two.
Yamada and Matsumoto (2003)have shown that the degree two polynomial ker-nel has superior accuracy than the linear model andpolynomial kernels of higher degrees.
However, us-ing the dual model is not always practical for depen-dency parsing.
The discriminant function of the dualmodel is defined as:f ?
(x, ?)
= argmaxkN?i=1?k,i?x,xi?g (6)where the weights ?
are associated with class-instance pairs rather than class-feature pairs.
Withrespect to the discriminant function of equation (4)there is an additional summation.
In principle, theinner products can be cached in a Kernel matrix tospeed up training.There are two shortcomings to using such a modelin dependency parsing.
First, if the amount of train-ing data is large it might not be feasible to store theKernel matrix; which for a dataset of sizeN requiresO(N3) computations and O(N2) space.
As an ex-ample, the number of training instances N in thePenn Treebank is over 1.8 million, caching the Ker-nel matrix would require several Terabytes of space.The second shortcoming is independent of training.In predicting a tree for unseen sentences the modelwill have to recompute the inner products betweenthe observation and all the support vectors; i.e., allclass-instance pairs with ?k,i > 0.
The second-orderfeature map with the perceptron is more efficient andallows faster training and prediction.
Training a sin-gle parsing model avoids a potential loss of accuracythat occurs when using the technique of partitioningthe training data according to the POS.
Inaccuratepredictions of the POS can affect significantly theaccuracy of the actions predicted, while the singlemodel is more robust, since the POS is just one ofthe many features used in prediction.4 Semantic featuresSemantic information is used implicitly in parsing.For example, conditioning on lexical heads pro-vides a source of semantic information.
There havebeen a few attempts at using semantic informationmore explicitly.
Charniak?s 1997 parser (1997), de-fined probability estimates backed off to word clus-ters.
Collins and Koo (Collins & Koo, 2005) in-troduced an improved reranking model for parsingwhich includes a hidden layer of semantic features.Yi and Palmer (2005) retrained a constituent parserin which phrases were annotated with argument in-formation to improve SRL, however this didn?t im-prove over the output of the basic parser.In recent years there has been a significantamount of work on semantic annotation tasks suchas named-entity recognition, semantic role labelingand relation extraction.
There is evidence that de-pendency and constituent parsing can be helpful inthese and other tasks; e.g., by means of tree ker-nels in question classification and semantic role la-beling (Zhang & Lee, 2003; Moschitti, 2006).It is natural to ask if also the opposite holds:whether semantic annotations can be used to im-prove parsing.
In particular, it would be interestingto know if entity-like tags can be used for this pur-pose.
One reason for this is that entity tagging is ef-ficient and does not seem to need parsing for achiev-ing top performance.
Beyond improving traditionalparsing, independently learned semantic tags mightbe helpful in adapting a parser to a new domain.
Tothe best of our knowledge, no evidence has been pro-duced yet that annotated semantic information canimprove parsing.
In the following we investigate137adding entity tags as features of our parser.4.1 BBN Entity corpusThe BBN corpus (BBN, 2005) supplements the WallStreet Journal Penn Treebank with annotation of alarge set of entity types.
The corpus includes an-notation of 12 named entity types (Person, Facility,Organization, GPE, Location, Nationality, Product,Event, Work of Art, Law, Language, and Contact-Info), nine nominal entity types (Person, Facility,Organization, GPE, Product, Plant, Animal, Sub-stance, Disease and Game), and seven numeric types(Date, Time, Percent, Money, Quantity, Ordinal andCardinal).
Several of these types are further dividedinto subtypes3.
This corpus provides adequate sup-port for experimenting semantic features for parsing.Figure 1 illustrates the annotation layer providedby the BBN corpus4.
It is interesting to notice oneapparent property of the combination of semantictags and dependencies.
When we consider segmentscomposed of several words there is exactly one de-pendency connecting a token outside the segmentwith a token inside the segment; e.g., ?CBS Inc.?
isconnected outside only through the token ?Inc.
?, thesubject of the main verb.
With respect to the rest ofthe tree, segments tend to form units, with their owninternal structure.
Intuitively, this information seemsrelevant for parsing.
This locally-structured patternscould help particularly simple algorithms like ours,which have limited knowledge of the global struc-ture being built.Table 2 lists the 40 most frequent categories insections 2 to 21 of the BBN corpus, and the per-centage of all entities they represent ?
together morethan 97%.
Sections 2-21 are comprised of 949,853tokens, 23.5% of the tokens have a non-null BBNentity tag, on average there is one tagged token everyfour.
The total number of entities is 139,029, 70.5%of which are named entities and nominal concepts,17% are numerical types and the remaining 12.5%describe time entities.We designed three new features which extractsimple properties of entities from the semantic an-notation information:3BBN Corpus documentation.4The full label for ?ORG?
is ?ORG:Corporation?, and?WOA?
stands for ?WorkOfArt:Other?.TOKENFEATURES Stack InputAS-0 = EOS+BIO+TAG 0AS-1 = EOS+BIO+TAG -1 0 1AS-2 = EOS+BIO+TAG -2 -1 0 1 2EOS -2 -1 0 1 2BIO -2 -1 0 1 2TAG -2 -1 0 1 2Table 3.
Additional configurations for the models withBBN entity features.?
EOS: Distance to the end of the segment; e.g.,EOS(?Last?)
= 1, EOS(?canceled?)
= 0;?
BIO: The first character of the BBN labelfor a token; e.g., BIO(?CBS?)
= ?B?, andBIO(?canceled?)
= 0;?
TAG: Full BBN tag for the token; e.g.,TAG(?CBS?)
= ?B-ORG:Corporation?,TAG(?week?)
= ?I-DATE?.The feature EOS provides information about the rel-ative position of the token within a segment with re-spect to the end of the segment.
The feature BIO dis-criminates tokens with no semantic annotation as-sociated, from tokens within a segment and tokenwhich start a segment.
Finally the feature TAG iden-tifies the full semantic tag associated with the token.With respect to the former two features this bearsthe most fine-grained semantics.
Table 3 summa-rizes six additional models we implemented.
Thefirst three use all additional features together, ap-plied to different sets of tokens, while the last threeapply only one feature, on top of the base model,relative to the next token in the input, the followingtwo tokens in the input, and the previous two tokenson the stack.4.2 Corpus pre-processingThe original BBN corpus has its own tokeniza-tion which often does not reflect the Penn Tree-bank tokenization; e.g., when an entity intersectsan hyphenated compound, thus ?third-highest?
be-comes ?thirdORDINAL - highest?.
This is problem-atic for combining entity annotation and dependencytrees.
Since our main focus is parsing we re-alignedthe BBN Corpus with the Treebank tokenization.Thus, for example, when an entity splits a Tree-bank token we extend the entity boundary to contain138WSJ-BBN Corpus CategoriesTag % Tag % Tag % Tag %PER DESC 15.5 ORG:CORP 13.7 DATE:DATE 9.2 ORG DESC:CORP 8.9PERSON 8.13 MONEY 6.5 CARDINAL 6.0 PERCENT 3.5GPE:CITY 3.12 GPE:COUNTRY 2.9 ORG:GOV 2.6 NORP:NATION-TY 1.9DATE:DURATION 1.8 GPE:PROVINCE 1.5 ORG DESC:GOV 1.4 FAC DESC:BLDG 1.1ORG:OTHER 0.7 PROD DESC:VEHICLE 0.7 ORG DESC:OTHER 0.6 ORDINAL 0.6TIME 0.5 GPE DESC:COUNTRY 0.5 SUBST:OTHER 0.5 SUBST:FOOD 0.5DATE:OTHER 0.4 NORP:POLITICAL 0.4 DATE:AGE 0.4 LOC:REGION 0.3SUBST:CHEM 0.3 WOA:OTHER 0.3 FAC DESC:OTHER 0.3 SUBST:DRUG 0.3ANIMAL 0.3 GPE DESC:PROVINCE 0.2 PROD:VEHICLE 0.2 GPE DESC:CITY 0.2PRODUCT:OTHER 0.2 LAW 0.2 ORG:POLITICAL 0.2 ORG:EDU 0.2Table 2.
The 40 most frequent labels in sections 2 to 21 of the Wall Street Journal BBN Corpus and the percentage oftags occurrences.the whole original Treebank token, thus obtaining?third-highestORDINAL?
in the example above.4.3 Semantic taggerWe treated semantic tags as POS tags.
A taggerwas trained on the BBN gold standard annotationand used it to annotate development and evaluationdata.
We briefly describe the tagger (see (Ciaramita& Altun, 2006) for more details), a Hidden MarkovModel trained with the perceptron algorithm intro-duced in (Collins, 2002).
The tagger uses Viterbidecoding.
Label to label dependencies are limited tothe previous tag (first order HMM).
A generic fea-ture set for NER based on words, lemmas, POS tags,and word shape features was used.The tagger is trained on sections 2-21 of the BBNcorpus.
As before, section 22 of the BBN corpusis used for choosing the perceptron?s parameter T .The tagger?s model is regularized as described forAlgorithm 2.
The full BBN tagset is comprisedof 105 classes organized hierarchically, we ignoredthe hierarchical organization and treated each tag asan independent class in the standard BIO encoding.The tagger evaluated on section 23 achieves an F-score of 86.8%.
The part of speech for the evalua-tion/development sections was produced with Tree-Tagger.
As a final remark we notice that the tagger?scomplexity, linear in the length of the sentence, pre-serves the parser?s complexity.5 Parsing experiments5.1 Data and setupWe used the standard partitions of the Wall StreetJournal Penn Treebank (Marcus et al, 1993); i.e.,sections 2-21 for training, section 22 for develop-ment and section 23 for evaluation.
The constituenttrees were transformed into dependency trees bymeans of a program created by Joakim Nivre thatimplements the rules proposed by Yamada and Mat-sumoto, which in turn are based on the head rulesof Collins?
parser (Collins, 1999)5.
The lemma foreach token was produced using the ?morph?
func-tion of the WordNet (Fellbaum, 1998) library6.
Thedata in the WSJ sections 22 and 23, both for theparser and for the semantic tagger, was POS-taggedusing TreeTagger7, which has an accuracy of 97.0%on section 23.Training a parsing model on the Wall Street Jour-nal requires a set of 22 classes: 10 of the 11 labelsin the dependency corpus generated from the PennTreebank (e.g., subj, obj, sbar, vmod, nmod, root,etc.)
are paired with both a Left and Right actions.In addition, there is in one rule for the ?root?
labeland one for the Shift action.
The total number offeatures found in training ranges from two hundredthousand for the 1st-order model to approximately20 million of the 2nd-order models.We evaluated several models, each trained with1st-order and 2nd-order features.
The base model(BASE) only uses the traditional set of features (cf.Table 1).
Models EOS, BIO and TAG each use onlyone type of semantic feature with the configurationdescribed in Table 3.
Models AS-0, AS-1, and AS-2use all three semantic features for the token on thestack in AS-0, plus the previous token on the stackand the new token in the input in AS-1, plus an addi-5The script is available fromhttp://w3.msi.vxu.se/%7enivre/research/Penn2Malt.html6http://wordnet.princeton.edu7TreeTagger is available from http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/1391st-order scores 2nd-order scoresDeSR MODEL LAS UAS Imp LAC LAS UAS Imp LACBASE 84.01 85.56 - 88.24 89.20 90.55 - 92.22EOS 84.89 86.37 +5.6 88.94 89.36 90.64 +1.0 92.37BIO 84.95 86.37 +6.6 89.06 89.63 90.89 +3.6 92.55TAG 84.76 86.26 +4.8 88.80 89.54 90.81 +2.8 92.55AS-0 84.40 85.95 +2.7 88.38 89.41 90.72 +1.8 92.38AS-1 85.13 86.52 +6.6 89.11 89.57 90.77 +2.3 92.49AS-2 85.32 86.71 +8.0 89.25 89.87 91.10 +5.8 92.68Table 4.
Results of the different models on WSJ section 23 using the CoNLL scores Labeled attachment score (LAS),Unlabeled attachment score (UAS), and Label accuracy score (LAC).
The column labeled ?Imp?
reports the improve-ment in terms of relative error reduction with respect to the BASE model for the UAS score.
In bold the best results.tional token from the stack and an additional tokenfrom the input for AS-2 (cf.
Table 3).5.2 Results of 2nd-order modelsTable 4 summarizes the results of all experiments.We report the following scores, obtained with theCoNLL-X scoring script: labeled attachment score(LAS), unlabeled attachment score (UAS) and labelaccuracy score (LAC).
For the UAS score, the mostfrequently reported, we include the improvement inrelative error reduction.The 2nd-order base model improves on all mea-sures over the 1st-order model by approximately5%.
The UAS score is 90.55%, with an improve-ment of 4.9%.
The magnitude of the improve-ment is remarkable and reflects the 4.6% improve-ment that Yamada and Matsumoto (Yamada & Mat-sumoto, 2003) report going from the linear SVM tothe polynomial of degree two.
Our base model?s ac-curacy (90.55% UAS) compares well with the ac-curacy of the parsers based on the polynomial ker-nel trained with SVM of Yamada and Matsumoto(UAS 90.3%), and Hall et al (2006) (UAS 89.4%).We notice in particular that, given the lack of non-projective cases/rules, the parser of Hall et al (2006)is almost identical to our parser, hence the differ-ence in accuracy (+1.1%) might effectively be dueto a better classifier.
Yamada & Matsumoto?s parseris slightly more complex than our parser, and hasquadratic worst-case complexity.
Overall, the accu-racy of the 2nd-order parser is comparable to that ofthe 1st-order MST parser (90.7%).There is no direct evidence that our perceptronproduces better classifiers than SVM.
Rather, thepattern of results produced by the perceptron seemscomparable to that of SVM (Yamada & Matsumoto,2003).
This is a useful finding in itself, given thatthe former is more efficient: perceptron?s update islinear while SVM solves a quadratic problem at eachupdate.
However, one major difference between thetwo approaches lies in the fact that learning with theprimal model does not require splitting the modelby Part-of-Speech, or other means.
As a conse-quence, beyond the greater simplicity, our methodmight benefit from not depending so strongly on thequality of POS tagging.
POS information is encodedas a feature and contributes its weight to the selec-tion of the parsing action, together with all addi-tionally available information.
In the SVM-trainedmethods the model that makes the prediction for theparsing rule is essentially chosen by an oracle, theprediction of the POS tagger.
Furthermore, it mightbe argued that learning a single model makes a bet-ter use of the training data by exploiting the cor-relations between all datapoints, while in the dualsplit-training case the interaction is limited to dat-apoints in the same partition.
In any case, second-order feature maps could be used also with SVM orother classifiers.
The advantage of using the per-ceptron lies in the unchallenged accuracy/efficiencytrade-off.
Finally, we recall that training in the pri-mal model can be performed fully on-line withoutaffecting the resulting model nor the complexity ofthe algorithm.5.3 Results of models with semantic featuresAll models based on semantic features improve overthe base model on all measures.
The best configura-140Parser UASHall et al ?06 89.4Yamada & Matsumoto ?03 90.3DeSR 90.55McDonald & Pereira 1st-order MST 90.7DeSR AS-2 91.1McDonald & Pereira 2nd-order MST 91.5Sagae & Lavie ?06 92.7Table 5.
Comparison of main results on the Penn Tree-bank dataset.tion is that of model AS-2 which extracts all seman-tic features from the widest context.
In the 1st-orderAS-2 model the improvement, 86.71% UAS (+8%relative error reduction) is more marked than in the2nd-order AS-2 model, 91.1% UAS (+5.8% errorreduction).
A possible simple exaplanation is thatsome information captured by the semantic featuresis correlated with other higher-order features whichdo not occur in the 1st-order encoding.
Overall theaccuracy of the DeSR parser with semantic informa-tion is slightly inferior to that of the second-orderMST parser (McDonald & Pereira, 2006) (91.5%UAS).
The best result on this dataset to date (92.7%UAS) is that of Sagae and Lavie (Sagae & Lavie,2006) who use a parser which combines the predic-tions of several pre-existing parsers, including Mc-Donald?s and Nivre?s parsers.
Table 5 lists the mainresults to date on the version of the Penn Treebankfor dependency parsing task used in this paper.In Table 4 we also evaluate the gain obtained byadding one semantic feature type at a time (cf.
rowsEOS/BIO/TAG).
These results show that all seman-tic features provide some improvement (with the du-bious case of EOS in the 2nd-order model).
TheBIO encoding seems to produce the most accuratefeatures.
This could be promising because it sug-gests that the benefit does not depend only on thespecific tags, but that the segmentation in itself isimportant.
Hence tagging could improve the adapta-tion of parsers to new domains even if only generictagging methods are available.5.4 Remarks on efficiencyAll experiments were performed on a 2.4GHz AMDOpteron CPU machine with 32GB RAM.
The 2nd-order parser uses almost 3GB of memory.
WhileParsing time/secParser English ChineseMST 2n-order 97.52 59.05MST 1st-order 76.62 49.13DeSR 36.90 21.22Table 6.
Parsing times for the CoNNL 2007 English andChinese datasets for MST and DeSR.it is several times slower and larger than the 1st-order model8 the 2nd-order model performance isstill competitive.
It takes 3 minutes (user time) toparse section 23, POS tagging included.
In train-ing, the model takes about 1 hour to process the fulldataset once.
As a comparison, Hall et al (2006)reports 1.5 hours for training the partitioned SVMmodel and 10 minutes for parsing the evaluation seton the same Penn Treebank data.
We also compareddirectly the parsing time of our parser with that ofthe MST parser using the version 0.4.3 of MST-Parser9.
For these experiments we used two datasetsfrom the CoNLL 2007 shared task for English andChinese.
Table 6 reports the times, in seconds, toparse the test sets for these languages on a 3.3GHzXeon machine with 4 GB Ram, of the MST 1st and2nd-order parser and DeSR parser (without semanticfeatures).The architecture of the model presented here of-fers several options for optimization.
For exam-ple, implementing the ?
models with full vectorsrather than hash tables speeds up parsing by a factorof three, at the expense of memory.
Alternatively,memory load in training can be reduced, at the ex-pense of time, by using on-line training.
However,the most valuable option for space need reductionmight be to filter out low-frequency second-orderfeatures.
Since the frequency of such features seemsto follow a power law distribution, this reduces sig-nificantly the feature space size even for low thresh-olds at small accuracy expense.
In this paper how-ever we focused on the full model, no approxima-tions were required to run the experiments.8The 1st-order parser takes 7 seconds (user time) to processSection 23.9Available from sourceforge.net.1416 ConclusionWe explored the design space of a dependencyparser by modeling and extending the feature repre-sentation, while adopting one of the simplest parsingarchitecture: a single-pass deterministic shift-reducealgorithm trained with a regularized multiclass per-ceptron.
We showed that with the perceptron it ispossible to adopt higher-order feature maps equiva-lent to polynomial kernels without need of approx-imating the model (although this remains an optionfor optimization).
The resulting models achieve ac-curacies comparable (or better) to more complex ar-chitectures based on dual SVM training, and fasterparsing on unseen data.
With respect to learning, it ispossible that more sophisticated formulations of theperceptron (e.g.
MIRA (Crammer & Singer, 2003))could provide further gains in accuracy, as shownwith the MST parser (McDonald et al, 2005).We also experimented with novel types of se-mantic features, extracted from the annotations pro-duced by an entity tagger trained on the BBN cor-pus.
This model further improves over the standardmodel yielding an additional 5.8% relative error re-duction.
Although the magnitude of the improve-ment is not striking, to the best of our knowledgethis is the first encouraging evidence that annotatedsemantic information can improve parsing and sug-gests several options for further research.
For exam-ple, this finding might indicate that this type of ap-proach, which combines semantic tagging and pars-ing, is viable for the adaptation of parsing to newdomains for which semantic taggers exist.
Seman-tic features could be also easily included in othertypes of dependency parsing algorithms, e.g., MST,and in current methods for constituent parse rerank-ing (Collins, 2000; Charniak & Johnson, 2005).For future research several issues concerning thesemantic features could be tackled.
We notice thatmore complex semantic features can be designedand evaluated.
For example, it might be useful toguess the ?head?
of segments with simple heuris-tics, i.e., the guess the node which is more likely toconnect the segment with the rest of the tree, whichall internal components of the entity depend upon.It would be also interesting to extract semantic fea-tures from taggers trained on different datasets andbased on different tagsets.AcknowledgmentsThe first author would like to thank Thomas Hof-mann for useful inputs concerning the presentationof the issue of higher-order feature representationsof Section 3.4.
We would also like to thank BrianRoark and the anonymous reviewers for useful com-ments and pointers to related work.ReferencesG.
Attardi.
2006.
Experiments with a MultilanguageNon-Projective Dependency Parser.
In Proceedings ofCoNNL-X 2006.BBN.
2005.
BBN Pronoun Coreference and Entity TypeCorpus.
Linguistic Data Consortium (LDC) catalognumber LDC2005T33.S.
Buchholz and E. Marsi.
2006.
Introduction toCoNNL-X Shared Task on Multilingual DependencyParsing.
In Proceedings of CoNNL-X 2006.X.
Carreras and L. Ma`rquez.
2005.
Introduction to theCoNLL-2005 Shared Task: Semantic Role Labeling.In Proceedings of CoNLL 2005.X.
Carreras, M. Surdeanu, and L. Ma`rquez.
2006 Pro-jective Dependency Parsing with Perceptron.
In Pro-ceedings of CoNLL-X.E.
Charniak.
1997.
Statistical Parsing with a Context-Free Grammar and Word Statistics.
In Proceedings ofthe Fourteenth National Conference on Artificial Intel-ligence AAAI.E.
Charniak and M. Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking.In Proceedings of ACL 2005.M.
Ciaramita and Y. Altun.
2006.
Broad-Coverage SenseDisambiguation and Information Extraction with a Su-persense Sequence Tagger.
In Proceedings of EMNLP2006.M.
Ciaramita and M. Johnson.
2003.
Supersense Tag-ging of Unknown Nouns in WordNet.
In Proceedingsof EMNLP 2003.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. Thesis, Universityof Pennsylvania.M.
Collins.
2000.
Discriminative Reranking for NaturalLanguage Parsing.
In Proceedings of ICML 2000.M.
Collins.
2002.
Discriminative Training Meth-ods for Hidden Markov Models: Theory and Experi-ments with Perceptron Algorithms.
In Proceedings ofEMNLP 2002.142M.
Collins and T. Koo.
2005.
Hidden-Variable Mod-els for Discriminative Reranking.
In Proceedings ofEMNLP 2005.M.
Collins and B. Roark.
2004.
Incremental Parsingwith the Perceptron Algorithm.
In Proceedings of ACL2004.K.
Crammer and Y.
Singer.
2003.
Ultraconservative On-line Algorithms for Multiclass Problems.
Journal ofMachine Learning Research 3: pp.951-991.A.
Culotta and J. Sorensen.
2004.
Dependency Tree Ker-nels for Relation Extraction.
In Proceedings of ACL2004.Y.
Ding and M. Palmer.
2005.
Machine Translation us-ing Probabilistic Synchronous Dependency InsertionGrammars.
In Proceedings of ACL 2005.J.
Eisner.
2000.
Bilexical Grammars and their Cubic-Time Parsing Algorithms.
In H.C. Bunt and A. Ni-jholt, eds.
New Developments in Natural LanguageParsing, pp.
29-62.
Kluwer Academic Publishers.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase MIT Press, Cambridge, MA.
1969.J.
Hall, J. Nivre and J. Nilsson.
2006.
DiscriminativeClassifiers for Deterministic Dependency Parsing.
InProceedings of the COLING/ACL 2006.T.
Kalt.
2004.
Induction of Greedy Controllers for Deter-ministic Treebank Parsers.
In Proceedings of EMNLP2004.M.
Marcus, B. Santorini and M. Marcinkiewicz.
1993.Building a Large Annotated Corpus of English: ThePenn Treebank.
Computational Linguistics, 19(2): pp.313-330.R.
McDonald.
2006.
Discriminative Training and Span-ning Tree Algorithms for Dependency Parsing.
Ph.D.Thesis, University of Pennsylvania.R.
McDonald, F. Pereira, K. Ribarov and J. Hajic?.
2005.Non-projective Dependency Parsing using SpanningTree Algorithms.
In Proceedings of HLT-EMNLP2005.R.
McDonald and F. Pereira.
2006.
Online Learningof Approximate Dependency Parsing Algorithms.
InProceedings of EACL 2006.M.L.
Minsky and S.A. Papert.
1969.
Perceptrons: AnIntroduction to Computational Geometry.
MIT Press,Cambridge, MA.
1969.A.
Moschitti.
2006.
Efficient Convolution Kernels forDependency and Constituent Syntactic Trees.
In Pro-ceedings of ECML 2006.J.
Nivre.
2004.
Incrementality in Deterministic Depen-dency Parsing.
In Incremental Parsing: Bringing En-gineering and Cognition Together.
Workshop at ACL-2004.Spain, 50-57.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson,S.
Riedel and D. Yuret.
2007.
The CoNLL 2007Shared Task on Dependency Parsing, In Proceedingsof EMNLP-CoNLL 2007.J.
Nivre and M. Scholz.
2004.
Deterministic Depen-dency Parsing of English Text.
In Proceedings ofCOLING 2004.V.
Punyakanok, D. Roth, and W. Yih.
2005.
The Neces-sity of Syntactic Parsing for Semantic Role Labeling.In Proceedings of IJCAI 2005.F.
Rosemblatt.
1958.
The Perceptron: A ProbabilisticModel for Information Storage and Organization in theBrain.
Psych.
Rev., 68: pp.
386-407.K.
Sagae and A. Lavie.
2005.
Parser Combination byReparsing.
In Proceedings of HLT-NAACL 2006.F.
Sha and F. Pereira.
2003.
Shallow Parsing with Condi-tional Random Fields.
In Proceedings of HLT-NAACL2003.H.
Yamada and Y. Matsumoto.
2003.
Statistical De-pendency Analysis with Support Vector Machines.
InProceedings of the Eighth International Workshop onParsing Technologies.
Nancy, France.S.
Yi and M. Palmer.
2005.
The Integration of SyntacticParsing and Semantic Role Labeling.
In Proceedingsof CoNLL 2005.A.
Wong and D. Wu.
1999.
Learning a Lightweight De-terministic Parser.
In Proceedings of EUROSPEECH1999.D.
Zhang and W.S.
Less.
2003.
Question Classificationusing Support Vector Machines.
In Proceedings of SI-GIR 2003.143
