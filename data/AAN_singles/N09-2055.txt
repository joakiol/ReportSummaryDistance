Proceedings of NAACL HLT 2009: Short Papers, pages 217?220,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsStatistical Post-Editing of aRule-Based Machine Translation System?A.-L. Lagarda, V. Alabau, F. CasacubertaInstituto Tecnolo?gico de Informa?ticaUniversidad Polite?cnica de Valencia, Spainalagarda@iti.upv.esR.
Silva, and E.
D?
?az-de-Lian?oCeler Soluciones, S.L.Madrid, SpainAbstractAutomatic post-editing (APE) systems aim atcorrecting the output of machine translationsystems to produce better quality translations,i.e.
produce translations can be manually post-edited with an increase in productivity.
In thiswork, we present an APE system that uses sta-tistical models to enhance a commercial rule-based machine translation (RBMT) system.
Inaddition, a procedure for effortless human eva-luation has been established.
We have testedthe APE system with two corpora of differ-ent complexity.
For the Parliament corpus, weshow that the APE system significantly com-plements and improves the RBMT system.
Re-sults for the Protocols corpus, although lessconclusive, are promising as well.
Finally,several possible sources of errors have beenidentified which will help develop future sys-tem enhancements.1 IntroductionCurrent machine translation systems are far fromperfect.
To achieve high-quality output, the rawtranslations they generate often need to be corrected,or post-edited by human translators.
One way of in-creasing the productivity of the whole process is thedevelopment of automatic post-editing (APE) sys-tems (Dugast et al, 2007; Simard et al, 2007).?
Work supported by the EC (FEDER) and the SpanishMEC under grant TIN2006-15694-CO2-01, by the Spanishresearch programme Consolider Ingenio 2010:MIPRCV(CSD2007-00018), and by the i3media Cenit project (CDTI2007-1012).Many of these works propose a combination ofrule-based machine translation (RBMT) and statisti-cal machine translation (SMT) systems, in order totake advantage of the particular capabilities of eachsystem (Chen and Chen, 1997).A possible combination is to automatically post-edit the output of a RBMT system employing a SMTsystem.
In this work, we will apply this techniqueinto two different corpora: Parliament and Proto-cols.
In addition, we will propose a new human eva-luation measure that will deal with the impact of theautomatic post-editing.This paper is structured as follows: after a briefintroduction of the RBMT, SMT, and APE systemsin Section 2, Section 3 details the carried out experi-mentation, discussing its results.
Finally, some con-clusions and future work are presented in Section 4.2 Systems descriptionThree different systems are compared in this work,namely the RBMT, SMT, and APE systems.Rule-based machine translation.
RBMT was thefirst approach to machine translation, and thus, arelatively mature area in this field.
RBMT sys-tems are basically constituted by two components:the rules, that account for the syntactic knowledge,and the lexicon, which deals with the morphologi-cal, syntactic, and semantic information.
Both rulesand lexicons are grounded on linguistic knowledgeand generated by expert linguists.
As a result, thebuild process is expensive and the system is difficultto maintain (Bennett and Slocum, 1985).
Further-more, RBMT systems fail to adapt to new domains.217Although they usually provide a mechanism to cre-ate new rules and extend and adapt the lexicon,changes are usually very costly and the results, fre-quently, do not pay off (Isabelle et al, 2007).Statistical machine translation.
In SMT, transla-tions are generated on the basis of statistical models,which are derived from the analysis of bilingual textcorpora.
The translation problem can be statisticallyformulated as in (Brown et al, 1993).
In practice,several models are often combined into a log-linearfashion.
Each model can represent an important fea-ture for the translation, such as phrase-based, lan-guage, or lexical models (Koehn et al, 2003).Automatic post-editing.
An APE system can beviewed as a translation process between the outputfrom a previous MT system, and the target language.In our case, an APE system based on statistical mod-els will be trained to correct the translation errorsmade by a RBMT system.
As a result, both RBMTand SMT technologies will be combined in order toincrease the overall translation quality.3 ExperimentsWe present some experiments carried out using theintroduced APE system, and comparing its perfor-mance with that of the RBMT and SMT systems.In the experimentation, two different English-to-Spanish corpora have been chosen, Parliament andProtocols, both of them provided by a professionaltranslation agency.Corpora.
The Parliament corpus consists of a se-ries of documents from proceedings of parliamen-tary sessions, provided by a client of the transla-tion agency involved in this work.
Most of the sen-tences are transcriptions of parliamentary speeches,and thus, with the peculiarities of the oral language.Despite of the multi-topic nature of the speeches,differences in training and test perplexities indicatethat the topics in test are well represented in thetraining set (corpus statistics in Table 1).On the other hand, the Protocols corpus is acollection of medical protocols.
This is a moredifficult task, as its statistics reflect in Table 1.
Thereare many factors that explain this complexity, suchas the different companies involved in training andtest sets, out-of-domain test data (see perplexity andTable 1: Corpus statistics for Parliament and Protocols.OOV stands for out-of-vocabulary words.Parliament ProtocolsEn Sp En SpTraining Sentences 90K 90K 154K 154KRun.
words 2.3M 2.5M 3.2M 3.6MVocabulary 29K 45K 41K 47KPerplexity 42 37 21 19TestSentences 1K 1K 3K 3KRun.
words 33K 33K 54K 71KOOVs 157 219 2K 1.7KPerplexity 44 43 131 173out-of-vocabulary words), non-native authors, etc.Evaluation.
In order to assess the proposed sys-tems, a series of measures have been considered.
Infirst place, some state-of-the-art automatic metricshave been chosen to give a first idea of the quality ofthe translations.
These translations have been alsoevaluated by professional translators to assess the in-crease of productivity when using each system.Automatic evaluation.
The automatic assessmentof the translation quality has been carried out us-ing the BiLingual Evaluation Understudy (BLEU)(Papineni et al, 2002), and the Translation ErrorRate (TER) (Snover et al, 2006).
The latter takesinto account the number of edits required to con-vert the system output into the reference.
Hence, thismeasure roughly estimates the post-edition process.Human evaluation.
A new human evaluationmeasure has been proposed to roughly estimatethe productivity increase when using each of thesystems in a real scenario, grounded on previousworks for human evaluation of qualitative fac-tors (Callison-Burch et al, 2007).
One of the de-sired qualities for this measure was that it shouldpose little effort to the human evaluator.
Thus, abinary measure was chosen, the suitability, wherethe translations are identified as suitable or not sui-table.
A given translation is considered to be suitableif it can be manually post-edited with effort savings,i.e., the evaluator thinks that a manual post-editingwill increase his productivity.
On the contrary, if theevaluator prefers to ignore the proposed translationand start it over, the sentence is deemed not suitable.218Significance tests.
Significance of the results hasbeen assessed by the paired bootstrap resamplingmethod, described in (Koehn, 2004).
It estimateshow confidently the conclusion that a system outper-forms another one can be drawn from a test result.Experimental setup.
Rule-based translation wasperformed by means of a commercial RBMT system.On the other hand, statistical training and translationin both SMT and APE systems were carried out usingthe Moses toolkit (Koehn et al, 2007).
It should benoted that APE system was trained taking the RBMToutput as source, instead of the original text.
In thisway, it is able to post-edit the RBMT translations.Finally, the texts employed for the human eva-luation were composed by 350 sentences randomlydrawn from each one of the two test corpora des-cribed in this paper.
Two professional translatorscarried out the human evaluation.3.1 Results and discussionExperimentation results in terms of automatic andhuman evaluation are shown in this section.Automatic evaluation.
Table 2 presents Parlia-ment and Protocols corpora translation results interms of automatic metrics.
Note that, as there isa single reference, this results are somehow pes-simistic.In the case of the Parliament corpus, SMT systemoutperforms the rest of the systems.
APE results areslightly worse than SMT, but far better than RBMT.However, when moving to the Protocols corpus, amore difficult task (as seen in perplexity in Table 1),the results show quite the contrary.
SMT and APEsystems show how they are more sensitive to out-of-domain documents.
Nevertheless, the RBMT sys-tem seems to be more robust under such conditions.Despite of the degradation of the statistical models,APE manages to achieve much better results than theother two systems.
It is able to conserve the robust-ness of RBMT, while its statistical counterpart dealswith the particularities of the corpus.Human evaluation.
Table 3 shows the percentageof translations deemed suitable by the human eva-luators.
Two professional evaluators analysed thesuitability of the output of each systemIn the Parliament case, APE performance is foundmuch more suitable than the rest of the systems.
InTable 2: Automatic evaluation for Parliament and Proto-cols tests.Parliament ProtocolsBLEU TER BLEU TERRBMT 29.1 46.7 29.5 48.0SMT 49.9 34.9 22.4 59.6APE 48.4 35.9 33.6 46.2fact, this difference between APE and the rest is sta-tistically significant at a 99% level of confidence.In addition, significance tests show that, on average,APE improves RBMT on 59.5% of translations.Regarding to the Protocols corpus, it must benoted that a first review of the translations pointedout that the SMT system performed quite poorly.Hence, SMT was not considered for the human eva-luation on this corpus.Figures show that APE complements and im-proves RBMT, although differences between themare tighter than in the Parliament corpus.
However,significance tests still prove that these improvementsare statistically significant (68% of confidence), andthat the average improvement is 6.5%.Table 3: Human evaluation for Parliament and Protocolscorpora.
Percentage of suitable translated sentences foreach system.Parliament ProtocolsRBMT 58 60SMT 60 ?APE 94 67It is interesting to note how automatic measuresand human evaluation seem not to be quite corre-lated.
In terms of automatic measures, the best sys-tem to translate the Parliament test is the SMT.
Thisimprovement has been checked by carrying out sig-nificance tests, resulting statistically significant witha 99% of confidence.
However, in the human eva-luation, SMT is worse than APE (this difference isalso significant at 99%).
On the other hand, whenworking with the Protocols corpus, automatic me-trics indicate that APE improves the rest (significantimprovement at 99%).
Nevertheless, human evalua-tors seem to think that the difference between APEand RBMT is not so significant, only with a confi-dence of 68%.
Previous works confirm this apparent219discrepancy between automatic and human evalua-tions (Callison-Burch et al, 2007).Translator?s commentaries.
As a subproduct ofthe human evaluation, the evaluators gave somepersonal impressions regarding each system perfor-mance.
They concluded that, when working with theParliament corpus, there was a net improvement inthe overall performance when using APE.
Changesbetween RBMT and APE were minor but useful.Thus, APE did not pose a system degradation withrespect to the RBMT.
Furthermore, a rough estima-tion indicated that over 10% of the sentences wereperfectly translated, i.e.
the translation was human-like.
In addition, some frequent collocations werefound to be correctly post-edited by the APE system,which was felt very effort saving.With respect to the Protocols corpus, as expected,results were found not so satisfactory.
However,human translators find themselves these documentscomplex.Finally, in both cases, APE is able to make thetranslation more similar to the reference by fix-ing some words without altering the grammaticalstructure of the sentence.
Finally, translators wouldfind very useful a system that automatically decidedwhen to automatically post-edit the RBMT outputs.4 ConclusionsWe have presented an automatic post-editing sys-tem that can be added at the core of the professionaltranslation workflow.
Furthermore, we have tested itwith two corpora of different complexity.For the Parliament corpus, we have shown thatthe APE system complements and improves theRBMT system in terms of suitability in a real transla-tion scenario (average improvement 59.5%).
Resultsfor the Protocols corpus, although less conclusive,are promising as well (average improvement 6.5%).Moreover, 67% of Protocols translations, and 94%of Parliament translations were considered to be sui-table.Finally, a procedure for effortless human eva-luation has been established.
A future improve-ment for this would be to integrate the process in thecore of the translator?s workflow, so that on-the-flyevaluation can be made.
In addition, several pos-sible sources of errors have been identified whichwill help develop future system enhancements.
Forexample, as stated in the translator?s commentaries,the automatic selection of the most suitable transla-tion among the systems is a desirable feature.ReferencesW.
S. Bennett and J. Slocum.
1985.
The lrc machinetranslation system.
Comp.
Linguist., 11(2-3):111?121.P.
F. Brown, S. Della Pietra, V. J. Della Pietra, and R. L.Mercer.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
Comp.
Linguist.,19(2):263?312.C.
Callison-Burch, C. Fordyce, P. Koehn, C. Monz, andJ.
Schroeder.
2007.
(meta-) evaluation of machinetranslation.
In Proc.
of the 2nd Workshop on SMT,pages 136?158, Prague, Czech Republic.
ACL.K.
Chen and H. Chen.
1997.
A hybrid approach to ma-chine translation system design.
In Comp.
Linguist.and Chinese Language Processing 23, pages 241?265.L.
Dugast, J. Senellart, and P. Koehn.
2007.
Statisti-cal post-editing on SYSTRAN?s rule-based translationsystem.
In Proc.
of the 2nd Workshop on SMT, pages220?223, Prague, Czech Republic.
ACL.P.
Isabelle, C. Goutte, and M. Simard.
2007.
Do-main adaptation of mt systems through automatic post-editing.
In Proc.
of MTSummit XI, pages 255?261,Copenhagen, Denmark.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
of NAACL-HLT,pages 48?54, Edmonton, Canada.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Proc.
of ACL, pages177?180, Prague, Czech Republic.P.
Koehn.
2004.
Statistical significance tests for ma-chine translation evaluation.
In Proc.
of EMNLP 2004,Barcelona, Spain.K.
Papineni, S. Roukos, T. Ward, and W.-Jing Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL, pages 311?318, Philadel-phia, PA, USA.M.
Simard, C. Goutte, and P. Isabelle.
2007.
Statis-tical phrase-based post-editing.
In Proc.
of NAACL-HLT2007, pages 508?515, Rochester, NY.
ACL.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit rate withtargeted human annotation.
In Proc.
of AMTA, pages223?231.220
