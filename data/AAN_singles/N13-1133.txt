Proceedings of NAACL-HLT 2013, pages 1131?1141,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsSupervised All-Words Lexical Substitution using Delexicalized FeaturesGyo?rgy Szarvas1 Chris Biemann2 Iryna Gurevych3,4(1) Nuance Communications Deutschland GmbHKackertstrasse 10, D-52072 Aachen, Germany(2) FG Language TechnologyDepartment of Computer Science, Technische Universita?t Darmstadt(3) Ubiquitous Knowledge Processing Lab (UKP-TUDA)Department of Computer Science, Technische Universita?t Darmstadt(4) Ubiquitous Knowledge Processing Lab (UKP-DIPF)German Institute for Educational Research and Educational Informationhttp://www.nuance.com , http://www.ukp.tu-darmstadt.deAbstractWe propose a supervised lexical substitu-tion system that does not use separate clas-sifiers per word and is therefore applicableto any word in the vocabulary.
Instead oflearning word-specific substitution patterns, aglobal model for lexical substitution is trainedon delexicalized (i.e., non lexical) features,which allows to exploit the power of super-vised methods while being able to general-ize beyond target words in the training set.This way, our approach remains technicallystraightforward, provides better performanceand similar coverage in comparison to unsu-pervised approaches.
Using features from lex-ical resources, as well as a variety of featurescomputed from large corpora (n-gram counts,distributional similarity) and a ranking methodbased on the posterior probabilities obtainedfrom a Maximum Entropy classifier, we im-prove over the state of the art in the LexSubBest-Precision metric and the Generalized Av-erage Precision measure.
Robustness of ourapproach is demonstrated by evaluating it suc-cessfully on two different datasets.1 IntroductionIn recent years, the task of automatically providinglexical substitutions in context (McCarthy and Nav-igli, 2007) received much attention.
The premiseto be able to replace words in a sentence with-out changing its meaning gave rise to applicationslike linguistic steganography (Topkara et al 2006;Chang and Clark, 2010), semantic text similarity(Agirre et al 2012), and plagiarism detection (Gippet al 2011).Lexical substitution, a special form of contex-tual paraphrasing where only a single word is re-placed, is closely related to word sense disambigua-tion (WSD): polysemous words have possible sub-stitutions reflecting several senses, and the correctsense has to be picked to avoid spurious system be-havior.
However, no explicit word sense inventory isrequired for lexical substitution (Dagan et al 2006).The prominent tasks in a lexical substitution sys-tem are generation and ranking, i.e.
to generate a setof possible substitutions for the target word and thento rank this set of possible substitutions according totheir contextual fitness.
The task to generate a highquality set of possible substitutions is challenging initself, for two reasons.
First, the available lexicalresources are seldom complete in listing synonyms.Second, manually annotated substitutions show thatnot all synonyms of a word are appropriate in a givencontext, and many good substitutions have other lex-ical relation than synonymy to the original word.In this work, we present a supervised lexical sub-stitution system that, unlike the usual lexical sam-ple supervised approaches, can produce substitu-tions for targets that are not contained in the train-ing material.
We reach this by using non-lexicalfeatures from heterogeneous evidence, includinglexical-semantic resources and distributional simi-larity, n-gram and shallow syntactic features basedon large, unannotated background corpora.
In lightof the existence of lexical resources such as Word-Net (Fellbaum, 1998) or machine readable dictio-naries that can serve as the source for lexical infor-mation, and with the ever-increasing availability oflarge unannotated corpora for many languages and1131domains, our proposal enables us to leverage thequality gain of supervised machine learning whilegeneralizing over a large vocabulary through theavoidance of lexicalized features.
Using a singleclassifier for all substitution targets in this way re-sults in an all-words substitution system.
As our re-sults demonstrate, our model improves over the stateof the art in lexical substitution with practically noopen parameters that have to be optimized and se-lected carefully according to the dataset at hand.2 Related WorkPrevious works in lexical substitution either ad-dress both the generation and the ranking tasks, andare therefore applicable to any word without pre-labeled data (c.f.
the Semeval 2007 task (McCarthyand Navigli, 2007) and related work) or focus onthe more challenging ranking step only (c.f.
Erkand Pado?
(2008) and related work).
The latter ap-proaches take the list of possible substitutions di-rectly from the testing data as a workaround to gen-erating the possible substitutions, and merely evalu-ate the ranking capabilities of these methods.The most accurate lexical substitution systemsuse supervised machine learning to train (and test)a separate classifier per target word, using lexicaland shallow syntactic features.
These systems relyon the existence of a large number of annotatedexamples (i.e.
sentences together with the con-textually valid substitutions) for each word.
Bie-mann (2012) describes a supervised lexical sub-stitution system for frequent nouns.
Exploiting alarge amount of sense tagged examples and (sense-specific) data annotated with substitutions, an ac-curate coarse-grained WSD model is trained andthen the most frequent substitutions of the predictedsense are assigned to the new occurrences of the tar-get words.
The results demonstrate that lexical sub-stitution of noun targets can be attained with veryhigh precision (over 90%) if sufficient training ma-terial is available.
However, due to high annotationcosts, methods that do not require labeled trainingdata per target scale better to a large vocabulary.Knowledge-based systems like e.g.
by Hassan etal.
(2007), who use a number of knowledge-basedand unsupervised methods and combine these cluesusing a voting scheme, do not need training data pertarget.
The combination of different signals, how-ever, has to be done manually.
Unsupervised sys-tems that rely on distributional similarity (Thater etal., 2011) or topic models (Li et al 2010) are singlesignals in this sense, and their development is guidedby the performance and observations on standarddatasets.
Such signals, however, can also be keptsimple avoiding any task-specific optimization andcan be integrated in a single model for all words us-ing a limited amount of training data and delexical-ized features, as in Senselearner (Mihalcea and Cso-mai, 2005) for weakly supervised all-words disam-biguation.
This way, task specific development canbe replaced by a machine learning component andthe resulting model applies also to unseen words,similar to the knowledge-based approaches.2.1 Full Lexical Substitution SystemsRelated works that address the lexical substitutionproblem according to the settings established by theEnglish Lexical Substitution Task (McCarthy andNavigli, 2007) at Semeval 2007 (LexSub) typicallyemploy a simple ranking strategy based on localn-gram frequencies and focus on finding an opti-mal source of possible substitutions, as the selec-tion of lexical resources has largest impact on theoverall system performance: Sinha and Mihalcea(2009) systematically explored the benefits of mul-tiple lexical resources and found that a supervisedcombination of several resources lead to statisti-cally significant improvements in accuracy (about3.5% points over the best single resource, WordNet).They tested LSA (Deerwester et al 1990), ESA(Gabrilovich and Markovitch, 2007) and n-gram fre-quencies for contextualization and found n-gram fre-quencies to be more effective than dimensionalityreduction techniques by a large margin.
Their im-provements were obtained by supervised learning onthe combination of several lexical resources.
Ourwork, on the other hand, is concerned with usingmore advanced features and we obtain significantimprovements based on a diverse set of features anda different learning setup: we train a model for con-textualization, rather than to combine substitutionsfrom several different resources.A recent work by Sinha and Mihalcea (2011) usedan approach based on graph centrality to rank thecandidates and achieved comparable performance1132to n-gram-frequency-based ranking.
To summarize,the use of n-gram frequencies for ranking and Word-Net as the (most appropriate single) source of syn-onyms is competitive to more complex solutions andprovides a simple and strong lexical substitution sys-tem.
This motivated the follow-up work by Changand Clark (2010) to use WordNet and n-grams in alinguistic steganography application and this moti-vates us to use this method as our baseline.2.2 Ranking Word Meaning in ContextAnother prominent line of related work focusedsolely on the accurate ranking of a pre-given set ofpossible synonyms, according to their plausibility asa substitution in a given context.
Typically, lexi-cal substitution data is used for evaluation purposes,taking the candidate substitutions directly from thetest data.
This choice is motivated by the assump-tion that better semantic models should rank near-synonyms more accurately according to how they fitin the original word?s context.Erk and Pado?
(2008) proposed the use of multiplevector representations of words, where the basic rep-resentation corresponds to a standard co-occurrencevector, while further vectors are used to characterizewords according to their inverse selectional prefer-ence statistics for typical dependency relations.
Therepresentation of a word in its context is computedvia combining the basic representation of a wordwith the inverse selectional preference vectors of itsrelated words from the context.
Ranking is done bycomparing vectors of possible substitutions with thesubstitution target.
Thater et al(2010) took a sim-ilar approach but used second order co-occurrencevectors and report improved performance.An exemplar-based approach is presented by Erkand Pado?
(2010) and Reisinger and Mooney (2010b)to model word meaning with respect to its context:instead of representing the word and the context asseparate vectors and combining them, a set of wordoccurrences in similar contexts is picked first, andthen only these exemplars are used to represent theword in context.
While this approach provides goodresults with relatively simple and transparent mod-els, each occurrence of a word has a unique repre-sentation (that can only be computed at testing time),and it is computationally expensive to scale thesemodels to a large number of examples.Dinu and Lapata (2010) used a bag of words la-tent variable model to characterize the meaning of aword as a distribution over a set of latent variables(that is, probabilistic senses).
Contextualized repre-sentation of word meaning is then attained by con-ditioning the model on the context words in whichthe target word occurs.
A similar approach hasbeen evaluated for word similarity (Reisinger andMooney, 2010a) and word sense disambiguation (Liet al 2010).Although our main goal here is to develop a full-fledged lexical substitution system, we mainly fo-cus on the construction of better ranking modelsbased on supervised machine learning and delexi-calized features that scale well for unseen words.This approach has similar properties (applicabilityto all words without word-specific training data) tothe knowledge-based and unsupervised models de-scribed above, so we will also refer to these systemsfor comparison.3 DatasetsIn our work, we use two major freely availabledatasets that contain human-annotated substitutionsfor single words in their full-sentence context.3.1 LexSub datasetThis dataset was introduced in the Lexical Substi-tution task at Semeval 20071.
It consists of 2002sentences for a total of 201 words (10 sentencesper word, but 8 sentences does not have gold stan-dard labels).
Each sentence was assigned to 5 na-tive speaker annotators, who entered as many para-phrases or substitutions as they found appropriatefor the word in context.
Paraphrases are assigned aweight (or frequency) that denotes how many anno-tators suggested that particular word as a substitute.3.2 TWSIA similar, but larger dataset is the Turk BootstrapWord Sense Inventory (TWSI2, (Biemann, 2012)).The data was collected through a three-step crowd-sourcing process and comprises 24,647 sentences1download at http://nlp.cs.swarthmore.edu/semeval/tasks/task10/data.shtml2http://www.ukp.tu-darmstadt.de/data/lexical-resources/twsi-lexical-substitutions/1133for a total of 1,012 target nouns, where crowdwork-ers have provided substitutions for a target word incontext.
We did not use the roughly 150,000 sense-labeled contexts and the sense inventory of this re-source, i.e.
this dataset ?
as used in this study ?
istransparent to the LexSub data.
For the majority ofthe data, responses from 3 annotators were collectedper context, and there are on average 24 sentencesper target word in the dataset.
Due to this, the aver-age weight of good substitutions is somewhat lowerthan in the LexSub dataset (1.27 vs. 1.58 in Lex-Sub), but the average number of unique substitutionsper target word is slightly higher in TWSI (averageof 22 words / target vs. 17 in LexSub).3.3 Source of Possible SubstitutionsIn our lexical substitution system, we used WordNetas the source for candidate synonyms.
For each sub-stitution target, we took all synonyms from all of theword?s WordNet synsets as candidates, together withthe words from synsets in similar to, entailment andalso see relation to these synsets3.
In order to evalu-ate and compare our ranking methodology in a trans-parent way with those studies that focused just onthe candidate ranking task, we also performed exper-iments where we pooled the set of candidates fromthe gold standard dataset.
This setting ensures thateach set contains a positive candidate, and that allhuman-suggested paraphrases are available as posi-tive examples for a given sentence.The main characteristics of the datasets (with bothWordNet or the gold standard as the source of candi-date substitutions) are summarized in Table 1.
Therows in the table indicate the source of possible sub-stitutions, number of target words, instances with atleast one non-multiword possible substitution, aver-age size of candidate sets, and number of instanceswith no good candidate and frequency of differentlabels.
The labels denote how many annotators pro-posed a particular word as substitution in the givencontext and can be interpreted as a measure of good-ness: the higher the value, the better the candidatefits in the context.
Similarly, the label 0 denotes thetotal number of negative examples in our datasets,i.e.
bad substitutions ?
words that belong to the can-3This candidate set was found best for WordNet by Martinezet al(2007).LexSub TWSIsource WN Gold St. WN Gold St.# words 201 201 908 1007#inst 2002 2002 22543 24643avg.
set 21 17 7.5 22# empty 508 17 11165 620#0 39465 27300 151538 443993#1 1302 4698 10678 77417#2 582 1251 4171 17585#3 308 571 2069 5629#4 212 319 74 325#5+ 129 179 121 411Table 1: Details of the datasets: WN=WordNetdidate set for a particular target word, but are notlisted as good substitutions in the given context inthe dataset.4 Methodology4.1 Experimental Setup and EvaluationWe follow previous works in lexical substitution andevaluate our models using the Generalized AveragePrecision (GAP) (Kishida, 2005) measure which as-sesses the quality of the entire ranked list.
In addi-tion, we also provide the precision of our system atthe first rank (P@1), i.e.
the percentage of correctparaphrases at rank 1.
This is a realistic evaluationcriterion for many applications, such as paraphras-ing for linguistic steganography: it is the highest-ranked candidate that can be used to replace the orig-inal word (the manipulated text should preserve theoriginal meaning) and there is no straightforwardway to exploit multiple correct answers.
In addition,we also provide the Semeval 2007 best precision4metric (McCarthy and Navigli, 2007) for the Lex-Sub dataset for comparison to Semeval 2007 partic-ipants.
This metric also evaluates the first guess ofa system (per context), but gives less credit to easiercontexts, where several good options exist.
This factmotivates us to use P@1 rather than the best preci-sion metric in all other experiments.4Since our system always provides an answer, the Semeval2007 best recall equals best precision.11344.2 Machine Learning on DelexicalizedFeaturesAfter the list of potential substitutions is obtained,lexical substitution is cast as a ranking task wherethe goal is to prefer contextually plausible substitu-tions over implausible ones.
The goal of this studyis to learn a ranking model that is applicable to anyword, for which a list of synonyms is available.
Asupervised model can generalize over the exampletarget words in the datasets, if aggregate featurescan be defined that have the same semantics regard-less of the actual context, target word or candidatesubstitution they are computed from.
Having such arepresentation, one can expect to learn patterns thatgeneralize over the words/contexts seen in the train-ing dataset, and thus the setup constitutes a super-vised all-word system.To simulate an all-word scenario, we perform a10-fold cross validation in our experiments, splittingthe dataset into equal-sized folds randomly on thetarget word level.
That is, all sentences for a particu-lar target word fall into the same fold and thus eitherthe training or the test set (but never both).
This waywe always train and test the model on disjoint sets ofwords and as such, the learnt models cannot exploitword-specific properties.
This makes our results re-alistic estimates of an open vocabulary paraphrasingsystem, where we would apply the models (mostly)to words that were not in the training material.4.2.1 Machine Learning ModelIn our experiments, we used a Maximum Entropy(MaxEnt) classifier model implemented in the Mal-let (McCallum, 2002) package and trained a binaryclassifier to predict if a given substitution is valid ina particular context or not.We chose to use Maximum Entropy models fortwo main reasons: MaxEnt is not sensitive to param-eter settings and handles correlated features well,which is crucial in our situation where many featuresare highly correlated.Due to the low number of positive examples in thedatasets (see Table 1, labels 1-5+) and to emphasizebetter paraphrases suggested by several annotators,we assigned a weight to positive instances during thetraining process equal to their score (the number ofannotators suggesting that paraphrase; the weight ofnegative instances was set to 1).The output of the MaxEnt classifier is a posteriorprobability distribution for each target/substitutionpair, denoting the probabilities of the instance tobe a good or a bad substitution, given the featurevalues that describe both the words and their con-text.
The ranking over a set of candidates can benaturally induced based on their posterior scores forthe positive class, i.e.
a number that denotes ?howgood the candidate is, given the context?.
That is,the best substitution candidate s (characterized by aset of features F) from a set of candidates S is ob-tained as argmaxs?S[P (good|F)], the next best asthe argmax of the remaining elements, and so on.This pointwise approach to subset ranking (Cos-sock and Zhang, 2008) is arguably simplistic, butseveral studies (c.f.
Li et al(2007; Busa-Feketeet al(2011)) found this approach to perform rea-sonably well given that the model provides accurateprobability estimates, which is the case for MaxEnt.4.3 Delexicalized FeaturesWe use heterogeneous sources of information to de-scribe each target word/candidate substitution pairin its context.
The most important features describethe syntagmatic coherence of the substitute in con-text, measured as local n-gram frequencies obtainedfrom web data, in a sliding window around the tar-get word.
In addition we use features to describe the(non-positional, i.e.
non-local) distributional simi-larity of the target and its candidate substitution interms of sentence level co-occurrence statistics col-lected from newspaper texts.
A further set of fea-tures captures the properties of the target and can-didate word in a lexical resource (WordNet), suchas their number of senses, how frequent senses aresynonymous, etc.
Lastly, we use part of speech pat-terns to describe the target word in context.
Thisway, unlike many other methods suggested in previ-ous works (Thater et al 2011; Erk and Pado?, 2008),our model does not require deep syntactic analysisof the test sentences in order to rank the candidates.Even though we make intensive use of WordNet tocompute some of our feature functions, this is nota severe restriction for a practical paraphrasing sys-tem: one has to have a decent lexical resource in or-der to mine a reasonable set of candidate synonymsand such a resource can also serve as a source forfeatures in the classifier.
The rest of the feature func-1135tions exploit only large unannotated corpora and aPOS tagger at application time.For a target word t, and candidate substitution sifrom a set of candidates S, we used the features be-low.
Each numeric feature is used both in the formgiven below, and set-wise scaled to [0, 1] (we leaveit to the classifier to pick the more useful form ofinformation).
For the LexSub dataset, each featureis defined once for all instances, and once specificto the four POS categories in the dataset.
That iseach instance would have the described features de-fined twice, once the general form (defined for everyinstance) and once the form according to the pre-dicted POS category of the target word.
This allowsthe model to learn general and also POS-specificpatterns based on the information described below(i.e.
frequency thresholds, distributional propertiesetc.
for nouns or verbs etc.
in particular).
We denotethe left and right contexts around t and all words inthe sentence except t with cl, cr and c, respectively)4.3.1 Lexical Resource FeaturesWe used Wordnet 3.0 as the source for substi-tution candidates and as a source for delexicalizedfeatures.
We found the measure of ambiguity andthe sense number to provide useful information ina more general context: it is informative how manysenses a word has, and it is informative from whichsense number of the substitution target the substitu-tion candidate came from, since they are ordered bycorpus frequency.
In addition, we used the synsetsIDs of the words?
hypernyms as features, which cancapture more general semantics (the word to replaceis ?animate?, ?abstract?, etc.).
The following featureswere extracted from WordNet:?
number of senses of t and si in WordNet?
the sense numbers of t and si which are syn-onymous (in case they are direct synonyms, c.f.WN sense numbers encode sense frequencies)?
binary features for synset IDs of the hypernymsof the synset containing t and si (this featuretype did not significantly improve results)4.3.2 Corpus-based FeaturesIn order to create a Distributional Thesaurus (DT)similar to Lin (1998), we parsed a source corpusof 120M sentence English newspaper texts fromthe LCC5 (Richter et al 2006) with the Stanfordparser (de Marneffe et al 2006) and used depen-dencies to extract features for words: each depen-dency triple (w1, r, w2) denoting a dependency oftype r between words w1 and w2 results in a fea-ture (r, w2) characterizing w1, and a feature (w1, r)characterizing w26.
After counting the frequencyof each feature for each word, we apply a signifi-cance measure (log-likelihood test (LL), (Dunning,1993)), rank features per word according to theirsignificance, and prune the data, keeping only the1000 most salient features (Fw) per word7.
The sim-ilarity of two words is then given by the numberof their common features.
Our distributional the-saurus provides a list of the 1000 most salient fea-tures and a ranked list of up to 200 similar words(simw, based on the number of shared features) forall words above a certain frequency in the sourcecorpus.
We compute the following features to char-acterize a target word / substitution pair:?
To what extent the context c characterizes si:?c?FsiLL(Fsi (c))?sj?S?c?FsjLL(Fsj (c))?
percentage of shared words amongthe top k similar words to t andto si:|simt|k?|simsi |kmax(|simt|k,|simsi |k), for k =1, 5, 10, 20, 50, 100, 2008?
percentage of shared salient features among thetop k features of t and si, globally and re-stricted to the words from the target sentence:|Ft|k?|Fsi |kmax(|Ft|k,|Fsi |k)and|Ft|k?|Fsi |k?|c||c| , for k =1, 5, 10, 20, 50, 100, 1000?
boolean feature indicating whether si ?
simtor not (in top 100 similar words)5http://corpora.informatik.uni-leipzig.de/6open source implementation and data available athttp://sourceforge.net/p/jobimtext7The pruning operation greatly reduces runtime at the-saurus collection, rendering memory reduction techniques like(Charikar et al 2004) as unnecessary.8The various values for k trade off the salience of this fea-ture for coverage: only very few substitutions have overlap inthe top 1-5 similar words set, but if this happens, it is a verystrong indicator of contextual fitness, whereas overlap withinthe top 100-200 similar words is present for much more tar-get/substitution pairs, but it is a weaker indicator of fitness.11364.3.3 Local n-gram Features (from Web 1T)Syntagmatic coherence, measured as the n-gramfrequency of the context with the candidate substi-tution serves as the basis of ranking in the best Se-meval 2007 system (Giuliano et al 2007), which isalso our baseline method here.
We use the same n-grams as features in our supervised model:?
1-5-gram frequencies in a sliding windowaround t: freq(clsicr)/freq(cltcr), normal-ized w.r.t t?
1-5-gram frequencies in a sliding windowaround t: freq(clsicr)/?freq(clScr), nor-malized w.r.t.
S?
for each of x in {?and?, ?or?, ?,?
}, 3-5-gram frequencies in a sliding window aroundt: freq(cltxsicr)/freq(cltcr) (how frequentlythe target and candidate are part of a list or con-junctive phrase)4.3.4 Shallow Syntactic FeaturesWe also use part of speech information (fromTreeTagger (Schmid, 1994)) as features, in orderto enable the model to learn POS-specific patterns.This is especially important for the LexSub dataset,which contains examples from all major parts ofspeech (the TWSI dataset contains only noun tar-gets).
Specifically, we use:?
1-3-grams of main POS categories in a windowaround t, e.g.
NVV for a noun, verb, verb con-text?
Penn Treebank POS code of t4.3.5 ExampleFor clarity, we exemplify our delexicalized fea-tures briefly.
Using WordNet as a source for theword bright, we considered the 11 words brilliant,vivid, smart, burnished, lustrous, shining, shiny,undimmed, brilliant, hopeful, promising from thesynsets of bright, and 64 further words from its re-lated synsets (e.g.
intelligent, glimmery, polished,happy, ...) as potential paraphrases.
That is, forthe sentence ?He was bright and independent andproud.
?, where the human annotators listed intelli-gent, clever as suitable paraphrases, our system had1 correct (intelligent) and 74 incorrect substituionsin the candidate set (that is, clever is not found inWordNet in the above described way).
The substitu-tion intelligent in this context is characterized by atotal of 178 active features.
Of those, 112 featuresare based on local n-gram features (Sect.
4.3.3),where the large number stems from different n inn-gram, as well as the different variants of normal-ization and copies for the particular POS (here: JJ)and for all POS.
For instance, ?bright?
and ?intelli-gent?
are frequently occurring in comma-separatedenumerations, and ?intelligent?
fits well in the targetcontext based on n-gram probabilities.
The secondlargest block of features is constituted by 48 activedistributional similarity features (Sect.
4.3.2), whichare also available per POS and for different normal-izations.
Here is e.g.
captured that the candidatehas a high distributional similarity to the target withrespect to our background corpus.
The 12 shallowsyntatic features (Sect 4.3.4) capture various presentPOS patterns around the target, and the 6 resource-based features (Sect.
4.3.1) e.g.
inform about thenumber of senses of the target (10) and the candi-date (4).4.4 ResultsNow, we describe our results in detail.
First we com-pare our system on two datasets with a competitivebaseline, which uses the same candidate set as ourML-based model, and the simple and effective rank-ing function based on Google n-grams described byGiuliano et al(2007).
Later on we analyze how thefour major feature groups contribute to the results ina feature ablation experiment, and then we providea detailed and thorough comparison to earlier worksthat are similar to the model presented here and usedthe same dataset (LexSub) for evaluation.4.4.1 Semeval 2007 Lexical SubstitutionIn Table 2 we report results on the LexSub dataset.As can be seen, our model outperforms the baselineby a significant margin (p < 0.01 for all measures,using a paired t-test for significance).
Both the over-all rankings and the P@1 scores are of higher qualitythan the rankings based only on n-grams.4.4.2 Turk Bootstrap Word Sense InventoryThe results on the TWSI dataset are provided inTable 3.
Our model outperforms the baseline in all1137cand.
from WN from Gold St.GAP P@1 GAP P@1Baseline 36.8 31.1 46.9 49.5Our model 43.8 40.2 52.4 57.7Table 2: Comparison to the baseline on LexSub 2007.cand.
from WN from Gold St.GAP P@1 GAP P@1Baseline 33.8 28.2 44.4 44.5Our model 36.6 32.4 47.2 49.5Table 3: Comparison to the baseline on the TWSI dataset.the comparisons similar to the LexSub dataset.
Thedifferences are not so pronounced but still highlysignificant (p < 0.01).
This is consistent with theobservation by several Semeval 2007 participantsand with a per-POS analysis of our results on Lex-Sub: the ranking task seems to be more challengingfor nouns than for other parts of speech.
When us-ing WordNet, for about half (11165/22543) of theinstances, individual scores are 0 (cf.
Table 1).
Forthe other half, avg.
P@1 score is around 0.7, whichresults in 0.324 overall.
Note that the task of rankingin avg.
7.5 items is considerably easier than rank-ing in avg.
22 items, which explains the high P@1scores for cases where good candidates exist ?
also,a random ranker would score higher in this case.These results demonstrate that the proposeddelexicalized approach is superior to a competitivebaseline across two datasets.4.5 Feature ExplorationWe explored the contribution of our various fea-ture types on the LexSub dataset with candidate setfrom the gold standard.
Our MaxEnt model rely-ing only on local n-gram frequency features, i.e.
theGAP P@1w/o n-gram features 47.3 48.9w/o distr.
thesaurus 49.8 55.0w/o POS features 51.6 56.3w/o WN features 51.7 57.0Our model (all) 52.4 57.7Table 4: Feature ablation experiment (on LexSub dataset,with candidates from Gold Standard).same information as the baseline model, achieved aGAP score of 48.3 and P@1 of 52.1, respectively.This result is significantly better than the baseline(p < 0.01), i.e.
the machine learnt ranking modelis better than a state-of-the-art handcrafted rankerbased on the same data.
All single feature groups,when combined with n-grams, lead to significant im-provements (p < 0.01), which proves the usefulnessof each feature group.
In order to assess the contri-bution of each group to the overall system perfor-mance, we performed a feature ablation experiment.That is, we trained the MaxEnt model with usingall feature groups (this equals the model in Table 2)and then with leaving each of the feature groups outonce.
As can be seen, all feature groups improve theoverall results in a noticeable way, i.e.
their contri-bution is complementary.4.5.1 Comparison to Previous WorksIn Table 5 we compare our method with previousworks in the field, using the LexSub dataset.candidates from WN from Gold StandardBest-P GAPPado?Erk10 38.6Giuliano 12.93 DinuLapata 42.9Martinez 12.68 Thater10 46.0Sinha 13.60 Thater11 51.7Baseline 11.75 Baseline 46.9Our model 15.94 Our model 52.4Table 5: Comparison to previous works (LexSub dataset).In the left column of Table 5, we compare the per-formance of our system to representative Semeval2007 participants, namely Martinez et al(2007) andGiuliano et al(2007).
In order to make a fair com-parison, we report scores for the official test dataof Semeval 2007, using a 10-fold cross-validationscheme.
Martinez et al(2007) developed their sys-tem based on WordNet and we use the same can-didate set here that they proposed in their systemdescription.
Our reimplementation of (Giuliano etal., 2007) performs below the original scores, dueto the more restricted source of substitution can-didates (they use more lexical resources), yet usesthe same ranking methodology based on Google n-grams that we adopted here as our baseline.
We alsoreport the best previous result for this task, which1138was achieved via the (supervised) combination oflexical resources to improve the performance (Sinhaand Mihalcea, 2009).
Our model outperforms thisresult by a large margin for the best-precision eval-uation (mode-P, precision measured on those exam-ples where there is a clear best substitution providedby humans was 26.3%, compared to 21.3% reportedby Sinha and Mihalcea (2009).
This is especiallypromising in light of the fact that we use only a sin-gle source (WordNet) for synonyms and achieve ourimprovements through more advanced delexicalizedfeatures in an improved ranking model.
Sinha andMihalcea (2009), on the other hand, used compara-bly simple features for contextualization, of whichn-gram features were deemed most successful.
AsSinha and Mihalcea (2009) showed improvementsthrough utilizing several synonym sources, a combi-nation of their approach with ours should allow forfurther improvements in the future.In the right column of Table 5, we compare ourmodel to previous works that addressed only theranking task, and report performance on the wholedataset (i.e.
trial and test).
As can be seen, themethodology proposed here outperforms previousranking models, without the need to develop a high-quality ranking model by hand, and without the needto parse the test sentences.
Our delexicalized super-vised model only requires the development of fea-tures, and achieves excellent results without majortask-specific tuning or customization: we omittedthe optimization of the feature set and the parame-ters of the learning model.
This fact makes us as-sume that the proposed model can be applied morequickly and easily than previous models that haveseveral important design aspects to choose from.5 Conclusion and Future WorkIn this study, we presented a supervised approach toall-words lexical substitution based on delexicalizedfeatures, which enables us to fully exploit the powerof supervised models while ensuring applicability toa large, open vocabulary.Results demonstrate the feasibility of this method:our MaxEnt-based ranking approach improved overthe baseline in all settings and yielded ?
to ourknowledge ?
the best scores for lexical substitu-tion with automatically gathered synonyms on theSemeval 2007 LexSub dataset.
Also, it performedslightly better than the state of the art for candidatespooled from the gold standard without any parame-ter tuning or empirical design choices.In this study, we established transparency be-tween Semeval-style and ranking-only studies inlexical substitution ?
two lines of work that were dif-ficult to compare in the past.
Further, we observesimilar improvements on two different datasets,showing the robustness of the approach.While previous works showed the potential ofmore/improved lexical resources for lexical substi-tution, we improved over the best Semeval-style per-formance just by exploiting an improved rankingmodel over a standard WordNet-based candidate set.These results indicate that improvements from lexi-cal resources and better ranking models are additive,thus we want to add more lexical resources in oursystem in the future.Of course there are several other ways to improvefurther the work described here.
First of all, simi-lar to the best ranking approaches (e.g.
Thater et al(2011)), one could use contextualized feature func-tions to make global information from the distri-butional thesaurus more accurate.
Instead of usingglobally calculated similarities, information fromthe distributional thesaurus could be contextualizedvia constraining the statistics with words from thecontext.Other natural ways to improve the model de-scribed here are to make heavier use of parser infor-mation or to employ pair-wise or list-wise machinelearning models (Cao et al 2007), which are specif-ically designed for subset ranking.
Lastly, while in-trinsic evaluation of lexical substitution is important,we would like to show its practicability in tasks suchas steganography or information retrieval.AcknowledgementsThis work has been supported by the Hes-sian research excellence program Landes-Offensivezur Entwicklung Wissenschaftlich-o?konomischerExzellenz (LOEWE) as part of the research centerDigital Humanities, and by the German Ministry ofEducation and Research under grant SiDiM (grantno.
01IS10054G).1139ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: Apilot on semantic textual similarity.
In *SEM 2012:The First Joint Conference on Lexical and Computa-tional Semantics ?
Volume 1: Proceedings of the mainconference and the shared task, and Volume 2: Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation (SemEval 2012), pages 385?393,Montre?al, Canada.Chris Biemann.
2012.
Creating a System for Lexi-cal Substitutions from Scratch using Crowdsourcing.Language Resources and Evaluation: Special Issueon Collaboratively Constructed Language Resources,46(2).Ro?bert Busa-Fekete, Bala?zs Ke?gl, E?lteto?
Yama?s, andGyo?rgi Szarvas.
2011.
A robust ranking method-ology based on diverse calibration of adaboost.
InEuropean Conference on Machine Learning, volumeLNCS, 6911, pages 263?279.Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, andHang Li.
2007.
Learning to rank: from pairwiseapproach to listwise approach.
In Proceedings of the24rd International Conference on Machine Learning,pages 129?136.Ching-Yun Chang and Stephen Clark.
2010.
Practi-cal linguistic steganography using contextual synonymsubstitution and vertex colour coding.
In Proceedingsof the 2010 Conference on Empirical Methods in Nat-ural Language Processing, pages 1194?1203, Cam-bridge, MA.Moses Charikar, Kevin Chen, and Martin Farach-Colton.2004.
Finding frequent items in data streams.
Theor.Comput.
Sci., 312(1):3?15.D.
Cossock and T. Zhang.
2008.
Statistical analysis ofBayes optimal subset ranking.
IEEE Transactions onInformation Theory, 54(11):5140?5154.Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-morshtein, and Carlo Strapparava.
2006.
Direct wordsense matching for lexical substitution.
In Proceed-ings of the 21st International Conference on Compu-tational Linguistics and the 44th annual meeting of theAssociation for Computational Linguistics, ACL-44,pages 449?456, Sydney, Australia.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InLREC 2006, Genova, Italy.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican Society for Information Science, 41(6):391?407.Georgiana Dinu and Mirella Lapata.
2010.
Measuringdistributional similarity in context.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1162?1172, Cambridge,MA.Ted Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Linguis-tics, 19(1):61?74.Katrin Erk and Sebastian Pado?.
2008.
A structured vec-tor space model for word meaning in context.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 897?906,Honolulu, Hawaii.Katrin Erk and Sebastian Pado?.
2010.
Exemplar-basedmodels for word meaning in context.
In Proceedingsof the ACL 2010 Conference Short Papers, pages 92?97, Uppsala, Sweden.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting Semantic Relatedness using Wikipedia-basedExplicit Semantic Analysis.
In Proceedings of the20th International Joint Conference on Artificial In-telligence, pages 1606?1611.Bela Gipp, Norman Meuschke, and Joeran Beel.
2011.Comparative Evaluation of Text- and Citation-basedPlagiarism Detection Approaches using GuttenPlag.In Proceedings of 11th ACM/IEEE-CS Joint Confer-ence on Digital Libraries (JCDL?11), pages 255?258,Ottawa, Canada.
ACM New York, NY, USA.
Avail-able at http://sciplore.org/pub/.Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.2007.
FBK-irst: Lexical substitution task exploit-ing domain and syntagmatic coherence.
In Proceed-ings of the Fourth International Workshop on SemanticEvaluations (SemEval-2007), pages 145?148, Prague,Czech Republic.Samer Hassan, Andras Csomai, Carmen Banea, RaviSinha, and Rada Mihalcea.
2007.
UNT: SubFinder:Combining knowledge sources for automatic lexicalsubstitution.
In Proceedings of the Fourth Interna-tional Workshop on Semantic Evaluations (SemEval-2007), pages 410?413, Prague, Czech Republic.Kazuaki Kishida.
2005.
Property of Average Precisionand Its Generalization: An Examination of EvaluationIndicator for Information Retrieval Experiments.
NIItechnical report.
National Institute of Informatics.Ping Li, Christopher J.C. Burges, and Qiang Wu.
2007.McRank: Learning to rank using multiple classifica-tion and gradient boosting.
In Advances in Neural In-formation Processing Systems, volume 19, pages 897?904.
The MIT Press.Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010.Topic models for word sense disambiguation and1140token-based idiom detection.
In Proceedings of the48th Annual Meeting of the Association for Computa-tional Linguistics, ACL ?10, pages 1138?1147.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Linguis-tics and the 17th International Conference on Compu-tational Linguistics, volume 2 of ACL ?98, pages 768?774, Montreal, Quebec, Canada.David Martinez, Su Nam Kim, and Timothy Bald-win.
2007.
MELB-MKB: Lexical substitution systembased on relatives in context.
In Proceedings of theFourth International Workshop on Semantic Evalua-tions (SemEval-2007), pages 237?240, Prague, CzechRepublic.Andrew Kachites McCallum.
2002.
MALLET:A Machine Learning for Language Toolkit.http://mallet.cs.umass.edu.Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.
InProceedings of the Fourth International Workshop onSemantic Evaluations (SemEval-2007), pages 48?53,Prague, Czech Republic.Rada Mihalcea and Andras Csomai.
2005.
Senselearner:word sense disambiguation for all words in unre-stricted text.
In Proceedings of the ACL 2005 on Inter-active poster and demonstration sessions, ACLdemo?05, pages 53?56.Joseph Reisinger and Raymond Mooney.
2010a.
A mix-ture model with sharing for lexical semantics.
In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, pages 1173?1182, Cambridge, MA.Joseph Reisinger and Raymond J. Mooney.
2010b.Multi-prototype vector-space models of word mean-ing.
In Human Language Technologies: The 2010Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages109?117, Los Angeles, California.M.
Richter, U. Quasthoff, E. Hallsteinsdo?ttir, and C. Bie-mann.
2006.
Exploiting the leipzig corpora collection.In Proceesings of the IS-LTC 2006.
Ljubljana, Slove-nia.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of the In-ternational Conference on New Methods in LanguageProcessing, Manchester, UK.Ravi Sinha and Rada Mihalcea.
2009.
Combining lex-ical resources for contextual synonym expansion.
InProceedings of the International Conference RANLP-2009, pages 404?410, Borovets, Bulgaria.Ravi Som Sinha and Rada Flavia Mihalcea.
2011.
Usingcentrality algorithms on directed graphs for synonymexpansion.
In R. Charles Murray and Philip M. Mc-Carthy, editors, FLAIRS Conference.
AAAI Press.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations us-ing syntactically enriched vector models.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 948?957, Uppsala,Sweden.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2011.
Word meaning in context: A simple and effec-tive vector model.
In Proceedings of the Fifth Interna-tional Joint Conference on Natural Language Process-ing : IJCNLP 2011, pages 1134?1143, Chiang Mai,Thailand.
MP, ISSN 978-974-466-564-5.Umut Topkara, Mercan Topkara, and Mikhail J. Atal-lah.
2006.
The hiding virtues of ambiguity: quan-tifiably resilient watermarking of natural language textthrough synonym substitutions.
In Proceedings of the8th workshop on Multimedia and security, pages 164?174, New York, NY, USA.
ACM.1141
