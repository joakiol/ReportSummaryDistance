Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 155?162,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsEvaluating Joint Modeling of Yeast Biology Literature and Protein-ProteinInteraction NetworksRamnath Balasubramanyan and Kathryn Rivard and William W. CohenSchool of Computer ScienceCarnegie Mellon Universityrbalasub,krivard,wcohen@cs.cmu.eduJelena Jakovljevic and John WoolfordDeparment of Biological SciencesCarnegie Mellon Universityjelena,jw17@andrew.cmu.eduAbstractBlock-LDA is a topic modeling approach toperform data fusion between entity-annotatedtext documents and graphs with entity-entitylinks.
We evaluate Block-LDA in the yeast bi-ology domain by jointly modeling PubMed R?articles and yeast protein-protein interactionnetworks.
The topic coherence of the emer-gent topics and the ability of the model to re-trieve relevant scientific articles and proteinsrelated to the topic are compared to that of atext-only approach that does not make use ofthe protein-protein interaction matrix.
Eval-uation of the results by biologists show thatthe joint modeling results in better topic co-herence and improves retrieval performance inthe task of identifying top related papers andproteins.1 IntroductionThe prodigious rate at which scientific literatureis produced makes it virtually impossible for re-searchers to manually read every article to identifyinteresting and relevant papers.
It is therefore crit-ical to have automatic methods to analyze the liter-ature to identify topical structure in it.
The latentstructure that is identified can be used for differentapplications such as enabling browsing, retrieval ofpapers related to a particular sub-topic etc.
Such ap-plications assist in common scenarios such as help-ing a researcher identify a set of articles to read (per-haps a set of well-regarded surveys) to familiarizeherself with a new sub-field; helping a researcher tostay abreast with the latest advances in his field byidentifying relevant articles etc.In this paper, we focus on the task of organiz-ing a large collection of literature about yeast biol-ogy to enable topic oriented browsing and retrievalfrom the literature.
The analysis is performed usingtopic modeling(Blei et al, 2003) which has, in thelast decade, emerged as a versatile tool to uncoverlatent structure in document corpora by identifyingbroad topics that are discussed in it.
This approachcomplements traditional information retrieval taskswhere the objective is to fulfill very specific infor-mation needs.In addition to literature, there often exist othersources of domain information related to it.
In thecase of yeast biology, an example of such a resourceis a database of known protein-protein interactions(PPI) which have been identified using wetlab exper-iments.
We perform data fusion by combining textinformation from articles and the database of yeastprotein-protein interactions, by using a latent vari-able model ?
Block-LDA (Balasubramanyan andCohen, 2011) that jointly models the literature andPPI networks.We evaluate the ability of the topic models to re-turn meaningful topics by inspecting the top papersand proteins that pertain to them.
We compare theperformance of the joint model i.e.
Block-LDA witha model that only considers the text corpora by ask-ing a yeast biologist to evaluate the coherence oftopics and the relevance of the retrieved articles andproteins.
This evaluation serves to test the utility ofBlock-LDA on a real task as opposed to an internalevaluation (such as by using perplexity metrics forexample).
Our evaluaton shows that the joint modeloutperforms the text-only approach both in topic co-155herence and in top paper and protein retrieval asmeasured by precision@10 values.The rest of the paper is organized as follows.
Sec-tion 2 describes the topic modeling approach usedin the paper.
Section 3 describes the datasets usedfollowed by Section 4 which details the setup of theexperiments.
The results of the evaluation are pre-sented in Section 5 which is followed by the conclu-sion.2 Block-LDAThe Block-LDA model (plate diagram in Figure 1)enables sharing of information between the compo-nent on the left that models links between pairs ofentities represented as edges in a graph with latentblock structure, and the component on the right thatmodels text documents, through shared latent topics.More specifically, the distribution over the entities ofthe type that are linked is shared between the blockmodel and the text model.The component on the right, which is an extensionof the LDA models documents as sets of ?bags of en-tities?, each bag corresponding to a particular typeof entity.
Every entity type has a topic wise multi-nomial distribution over the set of entities that canoccur as an instance of the entity type.
This modelis termed as Link-LDA(Nallapati et al, 2008) in theliterature.The component on the left in the figure is a gen-erative model for graphs representing entity-entitylinks with an underlying block structure, derivedfrom the sparse block model introduced by Parkki-nen et al (2009).
Linked entities are generated fromtopic specific entity distributions conditioned on thetopic pairs sampled for the edges.
Topic pairs foredges (links) are drawn from a multinomial definedover the Cartesian product of the topic set with it-self.
Vertices in the graph representing entities there-fore have mixed memberships in topics.
In con-trast to Mixed-membership Stochastic Blockmodel(MMSB) introduced by Airoldi et al (2008), onlyobserved links are sampled, making this model suit-able for sparse graphs.LetK be the number of latent topics (clusters) wewish to recover.
Assuming documents consist of Tdifferent types of entities (i.e.
each document con-tains T bags of entities), and that links in the graphare between entities of type tl, the generative processis as follows.1.
Generate topics: For each type t ?
1, .
.
.
, T , andtopic z ?
1, .
.
.
,K, sample ?t,z ?
Dirichlet(?
), thetopic specific entity distribution.2.
Generate documents.
For every document d ?
{1 .
.
.
D}:?
Sample ?d ?
Dirichlet(?D) where ?d is thetopic mixing distribution for the document.?
For each type t and its associated set of entitymentions et,i, i ?
{1, ?
?
?
, Nd,t}:?
Sample a topic zt,i ?Multinomial(?d)?
Sample an entity et,i ?Multinomial(?t,zt,i)3.
Generate the link matrix of entities of type tl:?
Sample piL ?
Dirichlet(?L) where piL de-scribes a distribution over the Cartesian prod-uct of the set of topics with itself, for links inthe dataset.?
For every link ei1 ?
ei2, i ?
{1 ?
?
?NL}:?
Sample a topic pair ?zi1, zi2?
?Multinomial(piL)?
Sample ei1 ?Multinomial(?tl,zi1)?
Sample ei2 ?Multinomial(?tl,zi2)Note that unlike the MMSB model, this modelgenerates only realized links between entities.Given the hyperparameters ?D, ?L and ?, thejoint distribution over the documents, links, theirtopic distributions and topic assignments is given byp(piL,?,?, z, e, ?z1, z2?, ?e1, e2?|?D, ?L, ?)
?
(1)K?z=1T?t=1Dir(?t,z|?t)?D?d=1Dir(?d|?D)T?t=1Nd,t?i=1?z(d)t,id ?et,it,z(d)t,i?Dir(piL|?L)NL?i=1pi?zi1,zi2?L ?ei1tl,z1?ei2tl,z2156...?d?L?Dpi LN L?Dim: K x KDim: Kz i 1 z i 2e i 2e i 1LinksDocs?t,zTKDz 1,ie 1,iz T ,ie T ,iNd,TNd, 1?L - Dirichlet prior for the topic pair distribution for links?D - Dirichlet prior for document specific topic distributions?
- Dirichlet prior for topic multinomialspiL - multinomial distribution over topic pairs for links?d - multinomial distribution over topics for document d?t,z - multinomial over entities of type t for topic zzt,i - topic chosen for the i-th entity of type t in a documentet,i - the i-th entity of type t occurring in a documentzi1 and zi2 - topics chosen for the two nodes participating in the i-th linkei1 and ei2 - the two nodes participating in the i-th linkFigure 1: Block-LDAA commonly required operation when using mod-els like Block-LDA is to perform inference on themodel to query the topic distributions and the topicassignments of documents and links.
Due to theintractability of exact inference in the Block-LDAmodel, a collapsed Gibbs sampler is used to performapproximate inference.
It samples a latent topic foran entity mention of type t in the text corpus con-ditioned on the assignments to all other entity men-tions using the following expression (after collaps-ing ?D):p(zt,i = z|et,i, z?i, e?i, ?D, ?)
(2)?
(n?idz + ?D)n?iztet,i + ??e?
n?izte?+ |Et|?Similarly, we sample a topic pair for every link con-ditional on topic pair assignments to all other linksafter collapsing piL using the expression:p(zi = ?z1, z2?|?ei1, ei2?, z?i, ?e1, e2?
?i, ?L, ?)(3)?(nL?i?z1,z2?
+ ?L)?(n?iz1tlei1+?)(n?iz2tlei2+?
)(?e n?iz1tle+|Etl |?
)(?e n?iz2tle+|Etl |?
)Et refers to the set of all entities of type t. The n?sare counts of observations in the training set.?
nzte - the number of times an entity e of type tis observed under topic z?
nzd - the number of entities (of any type) withtopic z in document d?
nL?z1,z2?
- count of links assigned to topic pair?z1, z2?The topic multinomial parameters and the topicdistributions of links and documents are easily re-covered using their MAP estimates after inference157using the counts of observations.?
(e)t,z =nzte + ??e?
nzte?
+ |Et|?, (4)?
(z)d =ndz + ?D?z?
ndz?
+K?Dand (5)pi?z1,z2?L =n?z1,z2?
+ ?L?z?1,z?2n?z?1,z?2?
+K2?L(6)A de-noised form of the entity-entity link matrixcan also be recovered from the estimated parame-ters of the model.
Let B be a matrix of dimensionsK ?
|Etl | where row k = ?tl,k, k ?
{1, ?
?
?
,K}.Let Z be a matrix of dimensions K ?K s.t Zp,q =?NLi=1 I(zi1 = p, zi2 = q).
The de-noised matrix Mof the strength of association between the entities inEtl is given by M = BTZB.In the context of this paper, de-noising theprotein-protein interaction networks studied is animportant application.
The joint model permits in-formation from the large text corpus of yeast publi-cations to be used to de-noise the PPI network andto identify potential interactions that are missing inthe observed network.
While this task is importantand interesting, it is outside the scope of this paperand is a direction for future work.3 DataWe use a collection of publications about yeast bi-ology that is derived from the repository of sci-entific publications at PubMed R?.
PubMed R?
is afree, open-access on-line archive of over 18 mil-lion biological abstracts and bibliographies, includ-ing citation lists, for papers published since 1948.The subset we work with consists of approximately40,000 publications about the yeast organism thathave been curated in the Saccharomyces GenomeDatabase (SGD) (Dwight et al, 2004) with anno-tations of proteins that are discussed in the publi-cation.
We further restrict the dataset to only thosedocuments that are annotated with at least one pro-tein from the protein-protein interactions databasesdescribed below.
This results in a protein annotateddocument collection of 15,776 publications.
Thepublications in this set were written by a total of47,215 authors.
We tokenize the titles and abstractsbased on white space, lowercase all tokens and elim-inate stopwords.
Low frequency (< 5 occurrences)terms are also eliminated.
The vocabulary that is ob-tained consists of 45,648 words.The Munich Institute for Protein Sequencing(MIPS) database (Mewes et al, 2004) includes ahand-crafted collection of protein interactions cover-ing 8000 protein complex associations in yeast.
Weuse a subset of this collection containing 844 pro-teins, for which all interactions were hand-curated.Finally, we use another dataset of protein-proteininteractions in yeast that were observed as a result ofwetlab experiments by collaborators of the authorsof the paper.
This dataset consists of 635 interac-tions that deal primarily with ribosomal proteins andassembly factors in yeast.4 SetupWe conduct three different evaluations of the emer-gent topics.
Firstly, we obtain topics from onlythe text corpus using a model that comprises of theright half of Figure 1 which is equivalent to usingthe Link-LDA model.
For the second evaluation,we use the Block-LDA model that is trained on thetext corpus and the MIPS protein-protein interac-tion database.
Finally, for the third evaluation, wereplace the MIPS database with the interaction ob-tained from the wetlab experiments.
In all the cases,we set K, the number of topics to be 15.
In eachvariant, we represent documents as 3 sets of entitiesi.e.
the words in the abstracts of the article, the setof proteins associated with the article as indicated inthe SGD database and finally the authors who wrotethe article.
Each topic therefore consists of 3 differ-ent multinomial distributions over the sets of the 3kinds of entities described.Topics that emerge from the different variants canpossibly be assigned different indices even whenthey discuss the same semantic concept.
To com-pare topics across variants, we need a method todetermine which topic indices from the differentvariants correspond to the same semantic concept.To obtain the mapping between topics from eachvariant, we utilize the Hungarian algorithm (Kuhn,1955) to solve the assignment problem where thecost of aligning topics together is determined usingthe Jensen-Shannon divergence measure.Once the topics are obtained, we firstly obtain theproteins associated with the topic by retrieving the158Figure 2: Screenshot of the Article Relevance Annotation ToolVariant Num.
Coherent TopicsOnly Text 12 / 15Text + MIPS 13 / 15Text + Wetlab 15 / 15Table 1: Topic Coherence Evaluationtop proteins from the multinomial distribution cor-responding to proteins.
Then, the top articles cor-responding to each topic is obtained using a rankedlist of documents with the highest mass of their topicproportion distributions (?)
residing in the topic be-ing considered.4.1 Manual EvaluationTo evaluate the topics, a yeast biologist who is anexpert in the field was asked to mark each topic witha binary flag indicating if the top words of the dis-tribution represented a coherent sub-topic in yeastbiology.
This process was repeated for the 3 differ-ent variants of the model.
The variant used to obtainresults is concealed from the evaluator to remove thepossibility of bias.
In the next step of the evaluation,the top articles and proteins assigned to each topicwere presented in a ranked list and a similar judge-ment was requested to indicate if the article/proteinwas relevant to the topic in question.
Similar tothe topic coherence judgements, the process was re-peated for each variant of the model.
Screenshotsof the tool used for obtaining the judgments can beseen in Figure 2.
It should be noted that since thenature of the topics in the literature considered washighly technical and specialized, it was impracticalto get judgements from multiple annotators.159TopicPrecision@100.20.40.60.81.0 l l l lll l l l l l lVariantl With MIPS interactionsOnly TextWith Wetlab interactions(a) Article RetrievalTopicPrecision@100.20.40.60.81.0 llll l l l l lVariantl With MIPS interactionsOnly TextWith Wetlab interactions(b) Protein RetrievalFigure 3: Retrieval Performance Evaluation (Horizontal lines indicate mean across all topics)To evaluate the retrieval of the top articles andproteins, we measure the quality of the results bycomputing its precision@10 score.5 ResultsFirst we evaluate the coherence of the topics ob-tained from the 3 variants described above.
Table1 shows that out of the 15 topics that were obtained,12 topics were deemed coherent from the text-onlymodel and 13 and 15 topics were deemed coherentfrom the Block-LDA models using the MIPS andwetlab PPI datasets respectively.Next, we study the precision@10 values for eachtopic and variant for the article retrieval and proteinretrieval tasks, which is shown in Figure 3.
The plotsalso show horizontal lines representing the mean ofthe precision@10 across all topics.
It can be seenfrom the plots that for both the article and proteinretrieval tasks, the joint models work better than thetext-only model on average.
For the article retrievaltask, the model trained with the text + MIPS resultedin the higher mean precision@10 whereas for theprotein retrieval task, the text + Wetlab PPI datasetreturned a higher mean precision@10 value.
Forboth the protein retrieval and paper retrieval tasks,the improvements shown by the joint models usingeither of the PPI datasets over the text-only model(i.e.
the Link LDA model) were statistically sig-nificant at the 0.05 level using the paired Wilcoxonsign test.
The difference in performance between the160Topic: Protein Structure & InteractionsTop articles using Publications Only Top articles using Block-LDA with Wetlab PPI* X-ray fiber diffraction of amyloid fibrils.
* X-ray fiber diffraction of amyloid fibrils.
* Molecular surface area and hydrophobic effect.
* Scalar couplings across hydrogen bonds.
* Counterdiffusion methods for macromolecularcrystallization.
* Dipolar couplings in macromolecular structuredetermination.
* Navigating the ClpB channel to solution.
* Structure of alpha-keratin.
* Two Rippled-Sheet Configurations of Polypep-tide Chains, and a Note about the Pleated Sheets.
* Stable configurations of polypeptide chains.
* Molecular chaperones.
Unfolding protein fold-ing.
* The glucamylase and debrancher of S.
diastati-cus.
* The molten globule state as a clue for under-standing the folding and cooperativity of globular-protein structure.
* A study of 150 cases of pneumonia.
* Unfolding and hydrogen exchange of proteins:the three-dimensional ising lattice as a model.
* Glycobiology.
* Packing of alpha-helices: geometrical con-straints and contact areas.
* The conformation of thermolysin.Topic: DNA RepairTop articles using Publications Only Top articles using Block-LDA with Wetlab PPI* Passing the baton in base excision repair.
* Telomeres and telomerase.
* The bypass of DNA lesions by DNA and RNApolymerases.
* Enzymatic photoreactivation: overview.
* The glucamylase and debrancher of S.
diastati-cus.
* High-efficiency transformation of plasmid DNAinto yeast.
* DNA replication fidelity.
* The effect of ultraviolet light on recombinationin yeast.
* Base excision repair.
* T-loops and the origin of telomeres.
* Nucleotide excision repair.
* Directed mutation: between unicorns and goats.
* The replication of DNA in Escherichia Coli.
* Functions of DNA polymerases.
* DNA topoisomerases: why so many?
* Immortal strands?
Give me a break.Table 2: Sample of Improvements in Article Retrievaltwo joint models that used the two different PPI net-works were however insignificant which indicatesthat there is no observable advantage in using onePPI dataset over the other in conjunction with thetext corpus.Table 2 shows examples of poor results of articleretrieval obtained using the publications-only modeland the improved set of results obtained using thejoint model.5.1 TopicsTable 3 shows 3 sample topics that were retrievedfrom each variant described earlier.
The table showsthe top words and proteins associated with the top-ics.
The topic label on the left column was assignedmanually during the evaluation by the expert anno-tator.ConclusionWe evaluated topics obtained from the joint mod-eling of yeast biology literature and protein-proteininteractions in yeast and compared them to top-ics that were obtained from using only the litera-ture.
The topics were evaluated for coherence andby measuring the mean precision@10 score of thetop articles and proteins that were retrieved for eachtopic.
Evaluation by a domain expert showed that161Topic Top Words & ProteinsProtein Structure & Inter-actionsWords: protein structure binding residues domain structural beta complexatp proteins alpha interactions folding structures form terminal peptide helixmodel interaction bound domains molecular changes conformational(Publications Only) Proteins: CYC1 SSA1 HSP82 SUP35 HSP104 HSC82 SSA2 YDJ1 URE2KAR2 SSB1 SSA4 GCN4 SSA3 SSB2 PGK1 PDI1 SSC1 HSP60 STI1SIS1 RNQ1 SEC61 SSE1 CCP1DNA Repair Words:dna recombination repair replication strand single double cells mu-tations stranded induced base uv mutants mutation homologous virus telom-ere human type yeast activity telomerase mutant dna polymerase(Using MIPS PPI) Proteins: RAD52 RAD51 RAD50 MRE11 RAD1 RAD54 SGS1 MSH2RAD6 YKU70 REV3 POL30 RAD3 XRS2 RAD18 RAD2 POL3 RAD27YKU80 RAD9 RFA1 TLC1 TEL1 EST2 HOVesicular Transport Words:membrane protein transport proteins atp golgi er atpase membranesplasma membrane vesicles cells endoplasmic reticulum complex fusionca2 dependent translocation vacuolar intracellular yeast lipid channel hsp90vesicle(Using Wetlab PPI) Proteins: SSA1 HSP82 KAR2 PMA1 HSC82 SEC18 SSA2 YDJ1 SEC61PEP4 HSP104 SEC23 VAM3 IRE1 SEC4 SSA4 SEC1 PMR1 PEP12VMA3 VPH1 SSB1 VMA1 SAR1 HAC1Table 3: Sample Topicsthe joint modeling produced more coherent topicsand showed better precision@10 scores in the articleand protein retrieval tasks indicating that the modelenabled information sharing between the literatureand the PPI networks.ReferencesEdoardo M. Airoldi, David Blei, Stephen E. Fienberg,and Eric P. Xing.
2008.
Mixed membership stochasticblockmodels.
Journal of Machine Learning Research,9:1981?2014, September.Ramnath Balasubramanyan and William W. Cohen.2011.
Block-LDA: Jointly modeling entity-annotatedtext and entity-entity links.
In SDM, pages 450?461.SIAM / Omnipress.David.
M Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
The Journal of Ma-chine Learning Research, 3:993?1022.Selina S. Dwight, Rama Balakrishnan, Karen R.Christie, Maria C. Costanzo, Kara Dolinski, Sta-cia R. Engel, Becket Feierbach, Dianna G. Fisk,Jodi Hirschman, Eurie L. Hong, Laurie Issel-Tarver,Robert S. Nash, Anand Sethuraman, Barry Starr,Chandra L. Theesfeld, Rey Andrada, Gail Binkley,Qing Dong, Christopher Lane, Mark Schroeder, ShuaiWeng, David Botstein, and Michael Cherry J.
2004.Saccharomyces genome database: Underlying prin-ciples and organisation.
Briefings in bioinformatics,5(1):9.Harold W. Kuhn.
1955.
The Hungarian method for theassignment problem.
Naval Research Logistics Quar-terly, 2(1-2):83?97.Hans-Werner Mewes, C. Amid, Roland Arnold, DmitrijFrishman, Ulrich Gldener, Gertrud Mannhaupt, MartinMnsterktter, Philipp Pagel, Normann Strack, VolkerStmpflen, Jens Warfsmann, and Andreas Ruepp.
2004.MIPS: Analysis and annotation of proteins from wholegenomes.
Nucleic Acids Res, 32:41?44.Ramesh M. Nallapati, Amr Ahmed, Eric P. Xing, andWilliam W. Cohen.
2008.
Joint latent topic modelsfor text and citations.
In Proceeding of the 14th ACMSIGKDD international conference on Knowledge dis-covery and data mining, pages 542?550, Las Vegas,Nevada, USA.
ACM.Juuso Parkkinen, Janne Sinkkonen, Adam Gyenge, andSamuel Kaski.
2009.
A block model suitable forsparse graphs.
In Proceedings of the 7th InternationalWorkshop on Mining and Learning with Graphs (MLG2009), Leuven.
Poster.162
