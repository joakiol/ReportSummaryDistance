Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 32?39,Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational LinguisticsFrom Language to Family and Back: Native Language and LanguageFamily Identification from English TextAriel Stolerman Aylin Caliskan IslamDept.
of Computer ScienceDrexel UniversityPhiladelphia, PA{ams573,ac993,greenie}@cs.drexel.eduRachel GreenstadtAbstractRevealing an anonymous author?s traits fromtext is a well-researched area.
In this paper weaim to identify the native language and lan-guage family of a non-native English author,given his/her English writings.
We extract fea-tures from the text based on prior work, andextend or modify it to construct different fea-ture sets, and use support vector machines forclassification.
We show that native languageidentification accuracy can be improved by upto 6.43% for a 9-class task, depending on thefeature set, by introducing a novel method toincorporate language family information.
Inaddition we show that introducing grammar-based features improves accuracy of both na-tive language and language family identifica-tion.1 IntroductionMining text for features to infer characteristics onits author is an important research field.
One au-thor property that has been researched is native lan-guage, extracted from the author?s writing in a non-native language.
Learning the native language of ananonymous author can assist in profiling criminalsor terrorists, and may also undermine the privacy oflegitimate anonymous authors by helping to unveiltheir identity.Influences of native language (L1) on second lan-guage (L2), referred as the L1-L2 transfer effect, isseen in writing and can be utilized to identify na-tive language.
In this paper we examine aspects ofa broader class ?
the language family to which thenative language of an author belongs.
In the rest ofthe paper native language and native language fam-ily will be referred as L1 and LF, respectively.First, we examine the correct classification ratesof LF compared to L1.
As L1 is a subset of LF,the number of L1 classes is greater than or equal tothe number of corresponding LF classes.
Therefore,higher LF classification accuracy can be achievedtrivially by taking the family of the attributed L1in a L1 classification task.
This can be helpful incases where high accuracy is preferred over reso-lution.
We introduce a novel, improved methodthat achieves higher correct classification rate for LFidentification, compared to the trivial method.Our main contribution is showing that L1 identi-fication accuracy can be increased by incorporatingfamily information via LF identification.We use stylometric analysis and machine learn-ing techniques to identify L1 and LF.
We conduct aseries of experiments by mining English text writtenby non-native English authors for linguistic features.We use 4 different feature sets detailed in section 3.We evaluate the accuracy of our results by examin-ing the true-positive rate.The novelty of our work is in exploring the LF-L2 transfer effect using stylometric methods, andexpanding L1 identification methods accordingly.Increasing the state-of-the-art correct classificationrate for L1 detection is not our main goal.
Instead,we introduce concepts to increase achieved accuracyby incorporating LF knowledge into the classifica-tion process.The next section (2) provides background andprior work.
Section 3 describes the experimentalsetup.
In section 4 we describe the different experi-ments that were performed, followed by results andevaluation.
We finalize with discussion on the givenresults (section 5), followed by conclusions and di-rections for future research (section 6).2 Related WorkLiterature includes work on extracting demographicand psychological traits from different data formats,such as speech and text samples.
Native languageand accent identification from speech can be found32in (Choueiter et al 2008; Tomokiyo and Jones,2001).
Identifying an author?s native language fromL2 text, which is English in most cases, is the closestproblem to our work.Introductory studies in the area identified the writ-ten or spoken language itself, focusing on telephonedialogue corpora (Ahmed et al 2004; Zissman,1993).
Further studies focused on extracting specificinformation from text or speech after identifying thelanguage being used.
Wanneroy et al(1999) inves-tigated how non-native speech deteriorated languageidentification and used acoustic adaptation to im-prove it.
Choueiter et al(2008) classified differentforeign accented English speech samples by using acombination of heteroscedastic LDA and maximummutual information training.
Tomokiyo and Jones(2001) characterized part-of-speech sequences andshowed that Na?
?ve Bayes classification can be usedto identify non-native utterances of English.The first work that utilized stylometric methodsfor native language attribution is introduced by Kop-pel et al(2005a; 2005b).
They explored frequenciesof sets of features, and used them with multi-linearsupport vector machines to classify text by author?snative language.
They used a set of features con-sisted of function words, letter n-grams, errors andidiosyncrasies, and experimented on a dataset of au-thors of five different native languages taken fromICLEv1 (Granger et al 2002), reaching to 80.2%accuracy.
Tsur and Rappoport (2007) revisited Kop-pel?s work using only the 200 most frequent charac-ter bigrams, and achieved 65.6% accuracy, with onlya small degradation when removing dominant wordsor function words.Brooke and Hirst (2012) presented a method ofutilizing native language corpora for identifying na-tive language in non-native texts.
They used word-by-word translation of large native language corporato create sets of second language forms that are pos-sible results of language transfer, later used in unsu-pervised classification.
They achieved results aboverandom chance for L1 identification, however insuf-ficiently accurate.More related work can be found in (Estival etal., 2007; van Halteren, 2008; Carrio-Pastor, 2009;Golcher and Reznicek, 2009; Wong and Dras, 2009;Wong et al 2011; Brooke and Hirst, 2011; Ahn,2011).
The work mentioned above and our approachboth utilize the L1-L2 transfer effect to gain infor-mation about an author?s native language.
Gibbons(2009) proved the impact of native language fam-ily?s typological properties on L2.
As far as weknow, our work is the first to combine stylometryand native language family?s effect on L2, utilizedfor L1 identification.3 Experimental Setting3.1 CorpusWe use the ICLEv2 (Granger et al 2009) corpusthat contains English documents written by interme-diate to advanced international learners of English,with language backgrounds of 16 mother-tongues.The first version of the corpus was used in signif-icant previous work (Koppel et al 2005a; Koppelet al 2005b; Tsur and Rappoport, 2007).
They re-ported that they were able to use 258 documents ofsizes 500-1000 words for each language they used.We use version 2 of the corpus and restrict all doc-uments in our experiments to those with 500-1000words as well.
However, we found that constrainingour documents to these lengths allows us to use only133-146 documents per language.
We conduct a se-ries of experiments with different sub-corpora con-structed of documents representing 11 native lan-guages out of the 16 available in the corpus.
Thenative languages we used are: Bulgarian, Czech,Dutch, French, German, Italian, Norwegian, Polish,Russian, Spanish and Swedish, all Indo-Europeanlanguages.
These languages represent 3 language-families in a coarse partition: Germanic, Slavic andRomance, which are used as the LF class in the ex-periments to follow.
All sub-corpora configurationsare detailed in section 4.Since we are looking at a set of languages fromboth L1 and LF aspects, we maintained only thesub-corpora that allowed a sufficient amount of lan-guages in each represented family, i.e.
3 languagesin each of the Germanic, Slavic and Romance fam-ilies.
Therefore we removed 5 of the 16 availablelanguages in the corpus.3.2 Feature SelectionKoppel et alrepresented each document in their ex-periment as a 1,035-dimensional feature vector: 400function words, 200 most frequent letter n-grams,185 misspellings and syntactic errors and 250 rarePOS bigrams.
The 250 rare POS bigrams are theleast common bigrams extracted from the BrownCorpus (Francis and Kucera, 1983), and their ap-pearances are considered to be erroneous or non-standard.In our experiments we used 4 different featuresets, partially based on that used by Koppel et alWeused the authorship attribution tool JStylo (McDon-ald et al 2012) for feature extraction.
The feature33sets are the following:Basic: includes the 400 most frequent functionwords, 200 most frequent letter bigrams, 250 rarePOS bigrams and 300 most frequent spelling errors.The 400 most frequent function words were takenfrom a list of 512 function words used in the orig-inal experiments by Koppel et alFor the 200 let-ter n-grams, we chose bigrams, as they are shownto be effective for the task in previous research.The 250 rare POS bigrams were extracted from theBrown Corpus using the POS tagger in (Toutanovaet al 2003).
Finally, we simplified the error typesby considering only misspelled words, based ona list of 5,753 common misspellings, constructedfrom Wikipedia common misspellings and thoseused in (Abbasi and Chen, 2008).
We ignored anymisspellings with 0-1 appearances across the entiresub-corpus.
Since many of the rare POS bigramsand misspellings had no appearances, the effectivevector lengths vary between 653-870 features.Extended: identical to the former, with the addi-tion of the 200 most frequent POS bigrams acrossthe entire sub-corpus used for each experiment.These syntactic features were selected as an addi-tional representation of grammatical structures inthe text.There are several methods for natural languageclassification, including genetic, typological andareal (Campbell and Poser, 2008).
We considerthe typological classification that uses structural fea-tures to compare similarities between languages andclassify them into families.
Therefore we chosegrammatical evidence in L2 as features that may rep-resent similar transfer effects among languages inthe same family.Grammatical: constructed only from the 200most frequent POS bigrams, representing the gram-matical level of the text.InfoGain: We used the 200 features with the high-est information gain extracted from the extendedfeature set using Weka (Hall et al 2009), calculatedfor any given feature by measuring the expected re-duction in entropy caused by partitioning the test in-stances according to that feature.3.3 ClassificationWe trained a SMO (Platt, 1998) SVM classifier withpolynomial kernel, chosen as SVMs are used exten-sively in prior work and ours outperformed othermethods tested, including decision trees, nearest-neighbors, Bayesian and logistic regression classi-fiers.4 Experimental Variations and EvaluationWe conducted 3 different experiments using vari-ous sub-corpora and the 4 feature sets described inthe previous sections, with L1 and LF classificationtasks.
We evaluated the results by using the true-positive rate to capture accuracy.
Following is a de-tailed description of the different variations and re-sults.4.1 9-Class Languages, 3-Class FamiliesSetup: We compared 9-L1 identification with thecorresponding 3-LF identification, using datasetsconstructed of the sub-corpus containing all 11 lan-guages mentioned before.
For the 9-L1 task werandomly sampled documents of 9 languages, 3 foreach of the Germanic, Slavic and Romance languagefamilies, in order to maintain the same number oflanguages per family in every experiment.
We con-structed 16 different 9-L1 sets, choosing 3 out of 4Germanic languages, 3 out of 4 Slavic languages andthe only 3 Romance languages available.
In each ofthe 16 experiments we used the same number of doc-uments per language, varying between 133-146.In order to compare results with LF identification,we conducted 3 sets of experiments, each containing16 3-LF experiments, corresponding to the 16 thatwere performed for L1 identification.First, we ran the trivial experiment of attributingthe family of the predicted language resulted fromthe L1 identification experiments.
This method isdenoted as the trivial method.Next, we ran the same experiments conducted forL1, with the only difference of using LF as the classrather than L1.
As a result of that configuration,each experiment also contained the same numberof documents per language family, varying between399-438.
This method is denoted as the standalonemethod (as it is a standalone experiment, indepen-dent of L1 classification results).Lastly, we ran experiments combining the stan-dalone and trivial approaches.
We hypothesize thatif L1 is attributed with high confidence, so is the LFof that attributed L1, however if the confidence leveldecreases, a standalone LF experiment achieves bet-ter results.
We ran the L1 identification experimentsand set a threshold as the averaged probability of thepredicted class across the entire test set, based on theclass probability distribution outputted by the SVMclassifier.
To obtain proper probability estimates,we fit logistic regression models to the outputs ofthe SVM.
Every instance classified with probabilityabove the threshold was attributed the family using34the trivial method, and every instance below ?
usingthe standalone method.
This method is denoted asthe combined method.Results: We averaged the results of all 16 L1 iden-tification experiments, and those of the 3 sets of 16LF identification experiments.
See figure 1.0102030405060708090100Basic Extended Grammar InfoGainL1 (9)LF trivial (3)LF standalone (3)LF combined (3)L1 RandomLF RandomFigure 1: Accuracy for 9-class L1 and 3-class LF iden-tification.
The combined method for LF outperforms theother two.The accuracy for L1 identification was 67.78%,65.64%, 59.34% and 44.02% for the extended, ba-sic, InfoGain and grammatical feature sets, respec-tively.Out of the 3 LF identification experiment sets,the combined method achieved the best accuracy:90.57%, 86.24%, 86.2% and 85.29% for the Info-Gain, grammatical, extended and basic feature sets,respectively.
These results support our hypothesis.The trivial method achieved better results than thestandalone method for the basic and extended fea-ture sets: 78.33% and 79.87% for the first, 74.53%and 77.24% for the latter.
For the grammatical andInfoGain feature sets, the standalone performed bet-ter than the trivial: 63.61% and 76.02% for the first,63.1% and 73.94% for the latter.Since the L1 identification experiments have moreclasses than the LF experiments, the random chancevaries between them: 11.11% for L1 and 33.33%for LF.
Although the absolute accuracy for LF isconsistently higher than for L1, if we subtract thecorresponding random chance values to obtain ?ef-fective?
accuracy, in most cases L1 is more accu-rate than LF.
The LF combined method is the onlyone out of the 3 LF methods that exceeds the effec-tive accuracy of L1, for the grammatical and Info-Gain feature sets.
Combined with the standard (non-effective) results, it appears that the InfoGain featureset with the LF combined method achieves the high-est accuracy with the most added knowledge overrandom classification, across all tasks and featuresets.
It is also notable that the smallest differencebetween L1 and LF identification accuracy is seenfor the grammatical feature set.
See figure 2.0102030405060708090100Basic Extended Grammar InfoGainL1 (9)LF trivial (3)LF standalone (3)LF combined (3)Figure 2: Effective accuracy for 9-L1 and 3-LF identifi-cation.
Accuracy for L1 exceeds most accuracy resultsfor LF, except for the combined method on the grammat-ical and InfoGain feature sets.4.2 3-Class Languages, 3-Class FamiliesSetup: In order to have the same random-chancebaseline for both L1 and LF tasks, we compared3-L1 with 3-LF identification, using the same sub-corpus as before.For L1 we constructed 9 experiments, in each ran-domly sampling 3 languages from 1, 2 and 3 differ-ent language families (3 experiments each).
The rea-son for this choice is that as more families are used,the farther the chosen languages are from one an-other.
Therefore the choice above is intended to bal-ance the effect of LF in those experiments.
We used133 documents per language for all experiments.For LF we constructed 2 sets of 9 experiments,in order to examine the notion that languages in thesame family have more family-distinguishable com-monalities as opposed to random sets of languages.In the first, for each of the experiments we randomlycreated 3 sets of languages to be considered as fam-ilies.
We randomly sampled documents from all11 languages to construct sets for the 3 randomly-generated families used as classes.
Here we alsomaintained 133 documents per language family.
Inthe second we ran a similar configuration, only usingthe actual language families.Results: The averaged accuracy for L1 was 84.23%,82.29%, 81.67% and 66.97% for the extended, In-foGain, basic and grammatical feature sets, respec-tively.
These results consistently outperformed theresults of both sets of LF experiments.
See figure 3.The accuracy attained for actual language families350102030405060708090100Basic Extended Grammar InfoGainL1 (3)LF (3)Random LF (3)RandomFigure 3: Accuracy for 3-L1, 3-LF and 3-randomly-generated families identification.
Using the original fam-ilies achieves the highest accuracy for LF identification.was 72.43%, 70.09%, 68.72% and 56.55% for theextended, basic, InfoGain and grammatical featuresets, respectively, which consistently outperformedthat of the randomly-generated families: 61.46%,60.01%, 58.81% and 48.67%.
This shows that par-titioning the languages into sets by their actual fam-ily achieves the highest accuracy for LF identifica-tion.
As in the previous experiment, the differencein accuracy between L1 and LF identification wasthe smallest with the grammatical feature set.4.3 9-Class Languages, Reclassify by FamilySetup: We wanted to examine whether LF classifi-cation can improve L1 classification.
In this exper-iment we conducted the same 16 9-L1 experimentsfrom section 4.1.
We then set a threshold as in thecombined method in section 4.1, such that each clas-sified instance with predicted probability less thanthat threshold is treated as misclassified.
For allallegedly-misclassified instances we attributed thefamily they belong to, using various methods de-tailed later.
As last step we reclassified those in-stances using a training set constructed only of the3 languages in the family they were classified as,and considered these results as L1 classification-correction for those instances.
We measured theoverall change in accuracy.The entire 16 10-fold cross-validation experi-ments were conducted 3 times, each with a differentmethod for LF attribution for the instances below thethreshold: 1) The standalone method ?
running LFidentification task over all those instances, using thesame training set (with families as classes rather thanlanguages), 2) The trivial method ?
using the familyof the predicted language of those instances, and 3)Random ?
randomly selecting the family.Results: We averaged the results of all 16 L1 exper-iments for each of the 3 LF attribution methods andeach of the 4 feature sets used.We measured the net fix in accuracy (added num-ber of correctly classified instances, taking into ac-count corrected classifications and new misclassifi-cations).
For all feature sets, LF attribution usingthe standalone method yielded the highest fix rate,followed by LF attribution using the trivial method.The randomly attributed family method consistentlyyielded negative fix rate (i.e.
reduced overall accu-racy).
See figure 4.0102030405060708090100Basic Extended Grammar InfoGainL1 w/o fixLF standaloneLF trivialRandom LFFigure 4: Accuracy for L1 identification without fixand with fixing using LF attribution by the standalonemethod, trivial method and random selection of family.The standalone method yields the highest net fix in L1classification accuracy.The extended feature set yielded the best results.Starting at a baseline of 67.17% for L1 identifica-tion without any fix, the true-positive rates obtainedfor this feature set were 70.9% and 68.05% for at-tributing LF by the standalone and the trivial meth-ods, respectively.
The increase in accuracy is statis-tically significant (p < 0.01).
The random familyattribution method yielded a decrease in accuracy to66.35%.It is notable that although yielding best resultsfor the extended feature set, the standalone methodachieved higher increase in accuracy in some of theother feature sets.
The increase rates for this methodwere: 6.43%, 4.48%, 3.73% and 3.67% for the Info-Gain, grammatical, extended and basic feature sets,respectively.5 DiscussionThe first notable result is seen in experiment 4.1,where using the combined method for LF identifi-cation derives higher accuracy than both the trivialand the standalone methods.
This may suggest thatwhen L1 is predicted with high confidence, LF is36predicted well, but when the confidence level is low,it is better to run standalone LF classification.
Sincethe combined method uses the best of the two others,it outperforms both.The most important result is seen in experiment4.3, where L1 identification is improved by up to6.43% in accuracy for 9-L1 classification by in-troducing information about the language family,thus providing a smaller set of language classes inwhich the actual language is more likely to be found.Attributing LF by standalone experiments yieldedhigher L1 classification accuracy than attributing itby the family of the predicted language.
This out-come seemingly contradicts the results seen in sec-tion 4.1, where the latter LF attribution method out-performed the first.
However, this only supports theidea suggested above regarding the threshold, thatthe family of the attributed L1 is the actual familywith higher probability than LF attributed by a stan-dalone experiment, only when L1 is attributed withhigh confidence (i.e.
above the selected threshold).The results in sections 4.1 and 4.2 suggest thatall 4 feature sets achieve better accuracy for L1 thanfor LF (standalone) classification.
We believe this issince for L1 we try to distinguish between individuallanguages as they transfer to English.
However, LFidentification necessitates finding features that inter-sect between languages in a particular family, anddistinguish well between different families as theyare transferred to English.
This makes LF identifi-cation a more difficult task.The results obtained for randomly generated fam-ilies in sections 4.2 and 4.3, which are consistentlylower than using the actual families, suggest that thecontribution of using the latter yields the best perfor-mance.
That is, languages in the same family havemore commonalities distinguishing them from otherfamilies, than random sets of languages have.Looking at the results using the different featuresets, in most cases the extended feature set out-performed the rest.
This shows that adding gram-matical features increases accuracy for both L1 andLF.
Furthermore, in all experiments using only thegrammatical features achieved a rather good accu-racy (significantly higher than random chance), con-sidering that we used only 200 of these features.This supports the notion that grammatical featuresare useful for both L1 and LF identification.Another interesting notion regarding the gram-matical feature set is seen in the portion these fea-tures consist of the InfoGain feature set for the ex-periments of section 4.2: 33.05% for L1 and 57.16%for LF.
This suggests that the grammatical level ofthe text has greater significance for identifying LFcompared to L1.
When analyzing the portion lexicalfeatures consist of the InfoGain feature set, an oppo-site trend is seen: function words and letter bigramsconsist 29.94% and 33.94% of the features for L1, asopposed to 17.44% and 23.55% for LF, respectively.This suggests that the lexical level of the text is bet-ter for L1 detection than for LF detection.
Althoughless significant, the same trend is seen with spellingerrors: 3% for L1 and 1.83% for LF.6 ConclusionThe main conclusion is that when trying to gain in-formation about the native language of an Englishtext author, integrating family identification can in-crease the total accuracy, using the method intro-duced in section 4.3, where all low-confidence clas-sifications are reapplied within a smaller set of can-didates ?
languages within the family attributed tothose instances using a standalone experiment.Furthermore, when dealing with a large numberof L1 classes, higher accuracy can be attained byreducing the level of specification to language fami-lies, which can be obtained with high accuracy usingthe combined method presented in this paper that in-tegrates both the trivial LF by predicted L1 and LFby standalone experiment methods using the averageconfidence level as threshold.In addition, using the most frequent POS bigrams,which represent the grammatical level of the text, isshown to increase accuracy in both L1 and LF identi-fication tasks, especially for the latter.
Using lexicalfeatures as function words and character bigrams ishelpful especially for L1 identification.We suggest several directions for future work.First, trying new feature sets that may capture othersimilarities between languages in the same family.For instance, since languages in the same familytend to share basic vocabulary, it may have somelevel of transfer to L2 that could be captured by asynonym-based classifier.
For instance, ?verde?
inSpanish and ?vert?
in French may be translated to?verdant?, whereas ?gru?n?
in German and ?groen?in Dutch may be translated to ?green?.In addition, we can further explore the notionof increasing accuracy by applying knowledge of abroader class on the task applied in other stylometry-based information extraction tasks.
For instance, us-ing wide age ranges as the broader class for classi-fying age of anonymous authors, or personality pro-totypes for personality type identification.37ReferencesAhmed Abbasi and Hsinchun Chen.
2008.
Writeprints:A stylometric approach to identity-level identificationand similarity detection in cyberspace.
ACM Trans.Inf.
Syst., 26(2):7:1?7:29, April.Bashir Ahmed, Sung-Hyuk Cha, and Charles Tappert.2004.
Language identification from text using n-grambased cumulative frequency addition.
Proc.
CSIS Re-search Day, May.Charles S. Ahn.
2011.
Automatically detecting authors?native language.
Thesis, Naval Postgraduate School,March.Julian Brooke and Graeme Hirst.
2011.
Native languagedetection with ?cheap?
learner corpora.
In The 2011Conference of Learner Corpus Research (LCR2011).Julian Brooke and Graeme Hirst.
2012.
Measur-ing interlanguage: Native language identification withl1-influence metrics.
In Nicoletta Calzolari (Con-ference Chair), Khalid Choukri, Thierry Declerck,Mehmet Ugur Dogan, Bente Maegaard, Joseph Mari-ani, Jan Odijk, and Stelios Piperidis, editors, Proceed-ings of the Eight International Conference on Lan-guage Resources and Evaluation (LREC?12), Istanbul,Turkey, may.
European Language Resources Associa-tion (ELRA).Lyle Campbell and William J. Poser.
2008.
LanguageClassification: History and Method.
Cambridge Uni-versity Press.Maria Luisa Carrio-Pastor.
2009.
Contrasting specificenglish corpora: Language variation.
InternationalJournal of English Studies, Special Issue, pages 221?233.Ghinwa F. Choueiter, Geoffrey Zweig, and PatrickNguyen.
2008.
An empirical study of automatic ac-cent classification.
In ICASSP, pages 4265?4268.Dominique Estival, Tanja Gaustad, Son B. Pham, WillRadford, and Ben Hutchinson.
2007.
Author profilingfor english emails.
In 10th Conference of the PacificAssociation for Computational Linguistics (PACLING2007), pages 262?272.Winthrop Nelson Francis and Henry Kucera.
1983.
Fre-quency Analysis of English Usage: Lexicon and Gram-mar.
Houghton Mifflin.Erin Elizabeth Gibbons.
2009.
The effects of second lan-guage experience on typologically similar and dissimi-lar third language.
Thesis, Brigham Young University,Center for Language Studies.Felix Golcher and Marc Reznicek.
2009.
Stylome-try and the interplay of topic and l1 in the differentannotation layers in the falko corpus.
In Humboldt-Universitat zu Berlin, QITL-4.
[Online: Stand 2012-03-22T16:09:09Z].Sylvaine Granger, Estelle Dagneaux, and Fanny Meunier.2002.
International Corpus of Learner English : Ver-sion 1 ; Handbook and CD-ROM.
Pr.
Univ.
de Lou-vain, Louvain-la-Neuve.Sylvaine Granger, Estelle Dagneaux, Magali Paquot, andFanny Meunier.
2009.
The International Corpus ofLearner English, Version 2: Handbook and CD-Rom.Pr.
Univ.
de Louvain, Louvain-la-Neuve.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18, November.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005a.Automatically determining an anonymous author?s na-tive language.
In Proceedings of the 2005 IEEE inter-national conference on Intelligence and Security In-formatics, ISI?05, pages 209?217, Berlin, Heidelberg.Springer-Verlag.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005b.Determining an author?s native language by mining atext for errors.
In Proceedings of the eleventh ACMSIGKDD international conference on Knowledge dis-covery in data mining, KDD ?05, pages 624?628, NewYork, NY, USA.
ACM.Andrew McDonald, Sadia Afroz, Aylin Caliskan, ArielStolerman, and Rachel Greenstadt.
2012.
Usefewer instances of the letter ?i?
: Toward writing styleanonymization.
July.J.
Platt.
1998.
Fast training of support vector ma-chines using sequential minimal optimization.
InB.
Schoelkopf, C. Burges, and A. Smola, editors, Ad-vances in Kernel Methods - Support Vector Learning.MIT Press.Laura Mayfield Tomokiyo and Rosie Jones.
2001.You?re not from ?round here, are you?
: naive bayesdetection of non-native utterance text.
In Proceedingsof the second meeting of the North American Chap-ter of the Association for Computational Linguisticson Language technologies, NAACL ?01, pages 1?8,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Hu-man Language Technology Conference (HLT-NAACL2003).Oren Tsur and Ari Rappoport.
2007.
Using classifier fea-tures for studying the effect of native language on thechoice of written second language words.
In Proceed-ings of the Workshop on Cognitive Aspects of Com-putational Language Acquisition, CACLA ?07, pages9?16, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.38Hans van Halteren.
2008.
Source language markers ineuroparl translations.
In Proceedings of the 22nd In-ternational Conference on Computational Linguistics- Volume 1, COLING ?08, pages 937?944, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.R.
Wanneroy, E. Bilinski, C. Barras, M. Adda-Decker,and E. Geoffrois.
1999.
Acoustic-phonetic modelingof non-native speech for language identification.
InProceedings of the ESCA-NATO Workshop on Multi-Lingual Interoperability in Speech Technology (MIST),The Netherlands.Sze-Meng Jojo Wong and Mark Dras.
2009.
Contrastiveanalysis and native language identification.
In Pro-ceedings of the Australasian Language Technology As-sociation Workshop 2009, pages 53?61, Sydney, Aus-tralia, December.Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.2011.
Topic modeling for native language identifi-cation.
In Proceedings of the Australasian LanguageTechnology Association Workshop 2011, pages 115?124, Canberra, Australia, December.Marc A. Zissman.
1993.
Automatic language identifica-tion using gaussian mixture and hidden markov mod-els.
In Proceedings of the 1993 IEEE internationalconference on Acoustics, speech, and signal process-ing: speech processing - Volume II, ICASSP?93, pages399?402, Washington, DC, USA.
IEEE Computer So-ciety.39
