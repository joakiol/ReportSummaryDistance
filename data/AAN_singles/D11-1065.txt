Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 700?711,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsThe Imagination of Crowds: Conversational AAC Language Modeling usingCrowdsourcing and Large Data SourcesKeith VertanenDepartment of Computer SciencePrinceton Universityvertanen@princeton.eduPer Ola KristenssonSchool of Computer ScienceUniversity of St Andrewspok@st-andrews.ac.ukAbstractAugmented and alternative communication(AAC) devices enable users with certain com-munication disabilities to participate in every-day conversations.
Such devices often relyon statistical language models to improve textentry by offering word predictions.
Thesepredictions can be improved if the languagemodel is trained on data that closely reflectsthe style of the users?
intended communica-tions.
Unfortunately, there is no large datasetconsisting of genuine AAC messages.
In thispaper we demonstrate how we can crowd-source the creation of a large set of fictionalAAC messages.
We show that these messagesmodel conversational AAC better than the cur-rently used datasets based on telephone con-versations or newswire text.
We leverage ourcrowdsourced messages to intelligently selectsentences from much larger sets of Twitter,blog and Usenet data.
Compared to a modeltrained only on telephone transcripts, our bestperforming model reduced perplexity on threetest sets of AAC-like communications by 60?82% relative.
This translated to a potentialkeystroke savings in a predictive keyboard in-terface of 5?11%.1 IntroductionUsers with certain communication disabilitiesrely on augmented and alternative communication(AAC) devices to take part in everyday conversa-tions.
Often these devices consist of a predictivetext input method coupled with text-to-speech out-put.
Unfortunately, the text entry rates provided byAAC devices are typically low, between 0.5 and 16words-per-minute (Trnka et al, 2009).As a consequence, researchers have made nu-merous efforts to increase AAC text entry rates byemploying a variety of improved language model-ing techniques.
Examples of approaches includeadapting the language model to recently used words(Wandmacher et al, 2008; Trnka, 2008), using syn-tactic information (Hunnicutt, 1989; Garay-Vitoriaand Gonza?lez-Abascal, 1997), using semantic in-formation (Wandmacher and Antoine, 2007; Liand Hirst, 2005), and modeling topics (Lesher andRinkus, 2002; Trnka et al, 2006).
For a recent sur-vey, see Garay-Vitoria and Abascal (2006).While such language model improvement tech-niques are undoubtedly helpful, certainly they canall benefit from starting with a long-span languagemodel trained on large amounts of closely matcheddata.
For AAC devices this means closely modelingeveryday face-to-face communications.
However,a long-standing problem in the field is the lack ofgood data sources that adequately model such AACcommunications.
Due to privacy-reasons and otherethical concerns, there is no large dataset consist-ing of genuine AAC messages.
Therefore, previousresearch has used transcripts of telephone conversa-tions or newswire text.
However, these data sourcesare unlikely to be an ideal basis for AAC languagemodels.In this paper we show that it is possible to signif-icantly improve conversational AAC language mod-eling by first crowdsourcing the creation of a fic-tional collection of AAC messages on the AmazonMechanical Turk microtask market.
Using a care-700fully designed microtask we collected 5890 mes-sages from 298 unique workers.
As we will see,word-for-word these fictional AAC messages arebetter at predicting AAC test sets than a wide-rangeof other text sources.
Further, we demonstrate thatTwitter, blog and Usenet data outperform telephonetranscripts or newswire text.While our crowdsourced AAC data is better thanother text sources, it is too small to train high-qualitylong-span language models.
We therefore investi-gate how to use our crowdsourced collection to in-telligently select AAC-like sentences from Twitter,blog and Usenet data.
We compare a variety ofdifferent techniques for doing this intelligent selec-tion.
We find that the best selection technique is therecently proposed cross-entropy difference method(Moore and Lewis, 2010).
Using this method, webuild a compact and well-performing mixture modelfrom the Twitter, blog and Usenet sentences mostsimilar to our crowdsourced data.We evaluate our mixture model on four differenttest sets.
On the three most AAC-like test sets, wefound substantial reductions in not only perplexitybut also in potential keystroke savings when usedin a predictive keyboard interface.
Finally, to aidother AAC researchers, we have publicly releasedour crowdsourced AAC collection, word lists andbest-performing language models1.2 Crowdsourcing AAC-like MessagesAs we mentioned in the introduction, there are un-fortunately no publicly available sources of gen-uine conversational AAC messages.
We conjecturedwe could create surrogate data by asking workerson Amazon Mechanical Turk to imagine they werea user of an AAC device and having them inventthings they might want to say.
While crowdsourcingis commonly used for simple human computationtasks, such as labeling images and transcribing au-dio, it is an open research question whether we canleverage workers?
creativity to invent plausible anduseful AAC-like messages.
In this section, we de-scribe our carefully constructed microtask and com-pare how well our collected messages correspond tocommunications from actual AAC users.1http://www.aactext.org/imagine/Figure 1: The interface for HITs of type 1 in ourcrowdsourced data collection.Figure 2: The interface for HITs of type 2 in ourcrowdsourced data collection.2.1 Collection TasksTo collect our data, we used two different typesof human intelligence tasks (HITs).
In type 1, theworkers were told to imagine that due to an accidentor medical condition they had to use a communica-tion device to speak for them.
Workers were askedto invent a plausible communication.
Workers wereprevented from pasting text.
After several pilot ex-periments, we arrived at the instructions shown infigure 1.In type 2, a worker first judged the plausibilityof a communication written by a previous worker(figure 2).
After judging, the worker was askedto ?invent a completely new communication?
as ifthe worker was the AAC user.
Workers were pre-vented from pasting text or typing the identical textas the one just judged.
The same communication701was judged by three separate workers.
In this workwe did not make use of these judgments.2.2 Data CleaningWhile most workers produced plausible and oftencreative communications, some workers entered ob-vious garbage.
These workers were identified by aquick visual scan of the submitted communications.We rejected the work of 9% of the workers in type1 and 4% of the workers in type 2.
After removingthese workers, we had 2481 communications fromtype 1 and 4440 communications from type 2.After combining the data from all accepted HITs,we conducted further semi-automatic data clean-ing.
We first manually reviewed communicationssorted by worker.
We removed workers whose textwas non-fluent English or not plausible (e.g.
someworkers entered news headlines or proverbs).
Iden-tical communications from the same worker wereremoved.
We removed communications with anout-of-vocabulary (OOV) rate of over 20% with re-spect to a large word list of 330K words obtainedfrom human-edited dictionaries2.
We also removedcommunications that were all in upper case, con-tained common texting abbreviations (e.g.
?plz?,?ru?, ?2day?
), communications over 80 characters,and communications with excessive letter repeti-tions (e.g.
?yippeeee?).
After cleaning, we had 5890messages from 298 unique workers.2.3 ResultsTables 1 and 2 show some example communicationsobtained in each HIT type.
Sometimes, but not al-ways, type 2 resulted in the worker writing a similarcommunication as the one judged.
This is a mixedblessing.
While it may reduce the diversity of com-munications, we found that workers were more ea-ger to accept HITs of type 2.
The average HIT com-pletion time was also shorter, 24 seconds in type 2versus 36 seconds in type 1.
While we initially paid$0.04/HIT for both types, we found in subsequentrounds that we could pay $0.02/HIT for type 2.
Wealso had to reject less work in type 2 and qualita-tively found the communications to be more AAC-like.
Since workers had to imagine themselves in a2We combined Wiktionary, Webster?s dictionary providedby Project Gutenberg, the CMU pronouncing dictionary andGNU aspell.Is the dog friendly?Can I have some water please?I need to start making a shopping list soon.What I would really like right now is a plate of fruit.Who will drive me to the doctor?s office tomorrow?Table 1: Example communications from type 1.Can you bring my slippers?I am cold, is there another blanket.How did Pam take the news?Bring the fuzzy slippers here.Did you have breakfast?why are you so late?I am pretty hungry, can we go eat?I had bacon eggs and hashbrowns for breakfast.Table 2: Example communications from type 2.
Thetext in bold is the message workers judged.
It is fol-lowed in plain text by the workers?
new messages.very unfamiliar situation, it appears that providing aconcrete example was helpful to workers.3 Comparison of Training SourcesIn this section, we compare the predictive perfor-mance of language models trained on our Turk AACdata with models trained on other text sources.
Weuse the following training sets:?
NEWS ?
Newspaper articles from the CSR-III(Graff et al, 1995) and Gigaword corpora (Graff,2003).
60M sentences, 1323M words.?
WIKIPEDIA ?
Current articles and discussionthreads from a snapshot of Wikipedia (January 3,2008).
24M sentences, 452M words.?
USENET ?
Messages from a Usenet corpus(Shaoul and Westbury, 2009).
123M sentences,1847M words.?
SWITCHBOARD ?
Transcripts of 2217 telephoneconversations from the Switchboard corpus (God-frey et al, 1992).
Due to its conversational style,this corpus has been popular for AAC languagemodeling (Lesher and Rinkus, 2002; Trnka et al,2009).
0.2M sentences, 2.6M words.?
BLOG ?
Blog posts from the ICWSM corpus(Burton et al, 2009).
25M sentences, 387Mwords.702?
TWITTER ?
We collected Twitter messages viathe streaming API between December 2010 andMarch 2011.
We used the free Twitter streamwhich provides access to 5% of all tweets.
Twit-ter may be particularly well suited for modelingAAC communications as tweets are short typedmessages that are often informal person-to-personcommunications.
Twitter has previously beenproposed as a candidate for modeling conversa-tions, see for example Ritter et al (2010).
7Msentences, 55M words.?
TURKTRAIN ?
Communications from 80% of theworkers in our crowdsourced collection.
4981sentences, 24860 words.WIKIPEDIA, USENET, BLOG and TWITTER allconsisted of raw text that required significant filter-ing to eliminate garbage, spam, repeated messages,XML tags, non-English text, etc.
Given the largeamount of data available, our approach was to throwaway any text that did not appear to be a sensibleEnglish sentence.
For example, we eliminated anysentence having a large number of words not in our330K word list.3.1 Test SetsWe evaluated our models on the following test sets:?
COMM ?
Sentences written in response to hy-pothetical communication situations collected byVenkatagiri (1999).
We removed nine sentencescontaining numbers.
This set is used throughoutthe paper.
251 sentences, 1789 words.?
SPECIALISTS ?
Context specific phrases sug-gested by AAC specialists3.
This set is usedthroughout the paper.
952 sentences, 3842 words.?
TURKDEV ?
Communications from 10% of theworkers in our crowdsourced collection (disjointfrom TURKTRAIN and TURKTEST).
This set willbe used for initial evaluations and also to tune ourmodels.
551 sentences, 2916 words.?
TURKTEST ?
Communications from 10% of theworkers in our crowdsourced collection (disjointfrom TURKTRAIN and TURKDEV).
This set isused only in the final evaluation section.
563 sen-tences, 2721 words.3http://aac.unl.edu/vocabulary.htmlTest set SentenceCOMM I love your new haircut.COMM How many children do you have?SPECIALISTS Are you sure you don?t mind?SPECIALISTS I?ll keep an eye on that for youSWITCHTEST yeah he?s a good actor thoughSWITCHTEST what did she have likeTable 3: Examples from three of our test sets.?
SWITCHTEST ?
Transcripts of three Switchboardconversations (disjoint from the SWITCHBOARDtraining set).
This is the same set used in Trnka etal.
(2009).
We dropped one sentence containing adash.
This set is only used in the final evaluationsection.
59 sentences, 508 words.TURKDEV and TURKTEST contain text similarto table 1 and 2.
Table 3 shows some examples fromthe other three test sets.
Sentences in COMM tendedto be richer in vocabulary and subject matter thanthose in SPECIALISTS.
The SPECIALISTS sentencestended to be general phrases that avoided mention-ing specific situations, proper names, etc.
Sentencesin SWITCHTEST exhibited phenomena typical ofhuman-to-human voice conversations (filler words,backchannels, interruptions, etc).3.2 Language Model TrainingAll language models were trained using the SRILMtoolkit (Stolcke, 2002).
All models used interpo-lated modified Kneser-Ney smoothing (Kneser andNey, 1995; Chen and Goodman, 1998).
In this sec-tion, we trained 3-gram language models with nocount-cutoffs.
All text was converted to lowercaseand we removed punctuation except for apostrophes.We believe punctuation would likely slow down auser?s conversation for only a small potential advan-tage (e.g.
improving text-to-speech prosody).All models used a vocabulary of 63K words in-cluding an unknown word.
We obtained our vocab-ulary by taking all words occurring in TURKTRAINand all words occurring four or more times in theTWITTER training set.
We restricted our vocabu-lary to words from our large list of 330K words.This restriction prevented the inclusion of com-mon misspellings prevalent in many of our train-ing sets.
Our 63K vocabulary resulted in low OOV703l l l l l l l l l l l4 6 8 10 12 14 16 18 20 22 24050010001500Words of training data (K)Average perplexitylNewsWikipediaUsenetSwitchboardBlogTwitterTurkTrain(a) TURKDEV test setll l l l l l l l l l4 6 8 10 12 14 16 18 20 22 24050010001500Words of training data (K)Average perplexitylNewsWikipediaUsenetSwitchboardBlogTwitterTurkTrain(b) COMM test setl l l l l l l l l l l4 6 8 10 12 14 16 18 20 22 24050010001500Words of training data (K)Average perplexitylNewsWikipediaUsenetSwitchboardBlogTwitterTurkTrain(c) SPECIALISTS test setFigure 3: Perplexity of language models trained on the same amount of data from different sources.
Theperplexity is the average of 20 models trained on random subsets of the training data (one standard deviationerror bars).rates for all test sets: COMM 0%, SPECIALISTS0.05%, TURKDEV 0.1%, TURKTEST 0.07%, andSWITCHTEST 0.8%.3.3 Small Training Size ExperimentWe trained language models on each dataset, vary-ing the number of training words from 4K to 24K(the limit of the TURKTRAIN set).
For each datasetand training amount, we built 20 different models bychoosing sentences from the full training set at ran-dom.
We computed the mean and standard deviationof the per-word perplexity of the set of 20 models.As shown in figure 3, word-for-word the TURK-TRAIN data was superior for our three most AAC-like test sets.
Thus it appears our crowdsourcing pro-cedure was successful at generating AAC-like data.TWITTER was consistently the second best.
BLOG,USENET and SWITCHBOARD also performed well.3.4 Large Training Size ExperimentThe previous experiment used a small amount oftraining data.
We selected the best three datasetshaving tens of millions of words of training data:USENET, BLOG, and TWITTER.
As in the previ-ous experiment, we computed the mean and stan-dard deviation of the per-word perplexity of a setof 20 models.
Increasing the amount of trainingdata substantially reduced perplexity compared toour small TURKTRAIN collection (figure 4).
Tweetswere clearly well suited for modeling AAC-like textas 3M words of TWITTER data was better than 40Mwords of BLOG data.3.5 Comparison with Real AAC DataBeukelman et al (1984) analyzed the communica-tions made by five nonspeaking adults over 14 days.All users were experienced using a tape-typewriterAAC device.
Beukelman gives a ranked list of thetop 500 words, the frequency of the top 20 words,and statistics calculated on the communications.For the top 10 words in Beukelman?s AAC userdata, we computed the probability of each word inour various datasets (figure 5).
As shown, somewords such as ?to?
and ?a?
occur with similar fre-quency across all datasets.
Some words such as?the?
are overrepresented in data such as news text.Other words such as ?I?
and ?you?
are much morevariable.
Our Turk data has the closest matchingfrequency for the most popular word ?I?.
Interest-ingly, our Turk data shows a much higher probabil-ity for ?you?
than the AAC data.
We believe this re-sulted from the situation we asked workers to imag-ine (i.e.
communicating via a letter-at-a-time scan-ning interface).
Workers presumed in such a situa-tion they would need to ask others to do many tasks.We observed many requests in the data such as ?Can704l l l l l l l l l l l l l l l l l l0 10 20 30 40050100150200Words of training data (M)Average perplexity lUsenetBlogTwitter(a) TURKDEV test setl l l l l l l l l l l l l l l l l l0 10 20 30 40050100150200Words of training data (M)Average perplexity lUsenetBlogTwitter(b) COMM test setl l l l l l l l l l l l l l l l l l0 10 20 30 40050100150200Words of training data (M)Average perplexity lUsenetBlogTwitter(c) SPECIALISTS test setFigure 4: Perplexity of language models trained on increasing amounts of data from three different trainingsources.
Results on the TURKDEV, COMM and SPECIALISTS test sets.00.010.020.030.040.050.060.07i to you the a it my and in isUnigramprobabilityAAC users, BeukelmanTurk workersSwitchboardTwitterBlogUsenetWikipediaNewsFigure 5: The unigram probabilities of the top 10 words reported by Beukelman et al (1984).you change my sheets??
and ?Can you walk the dogfor me?
?Beukelman reports 33% of all communicationscould be made using only the top 500 words.
Thesame 500 words allowed writing of 34% of our Turkcommunications.
Other datasets exhibited muchlower percentages.
Note that this is at least partiallydue to the longer sentences present in some datasets.Unfortunately, Beukelman does not report the aver-age communication length.
Our Turk communica-tions were 5.0 words on average.
The next shortestdataset was TWITTER with 7.5 words per communi-cation.
Despite their short average length, only 10%of Tweets could be written using the top 500 words.Beukelman reports that 80% of words in the AACusers?
communications were in the top 500 words.81% of the words in our crowdsourced data were inthis word list.
For comparison, only 65% of wordsin our TWITTER data were in the 500 word vocabu-lary.
While our TURKTRAIN set contains only 2141unique words, this may in fact be good since it hasbeen argued that rare words have received too muchattention in AAC (Baker et al, 2000).4 Using Large Datasets EffectivelyIn the previous section, we found our crowdsourceddata was good at predicting AAC-like test sets.However, in order to build a good long-span lan-guage model, we would require millions of suchcommunications.
Crowdsourcing such a large col-lection would be prohibitively expensive.
There-fore, we instead investigated how to use our crowd-sourced data to intelligently select AAC-like datafrom other large datasets.
For large datasets, weused TWITTER, BLOG and USENET as they wereboth large and well-matched to AAC data.4.1 Selecting AAC-like DataFor each training sentence, we calculated three val-ues:?
WER ?
The minimum word error rate betweenthe training sentence and one of the crowdsourced705lllllllllllllllllll l lll l l l l l ll0 5 10 15 20 25406080100120140Language model parameters (M)PerplexitylEntropy pruningCount cutoff pruningCross?entropy selectionWER selectionCross?entropy difference(a) TWITTERl l l l l l l l l l l l l l l l0 5 10 15 20 25406080100120140Language model parameters (M)PerplexitylEntropy pruningCount cutoff pruningCross?entropy selectionWER selectionCross?entropy difference(b) BLOGl l l l l l l l lllll l l l l l l0 5 10 15 20 25406080100120140Language model parameters (M)PerplexitylEntropy pruningCount cutoff pruningCross?entropy selectionWER selectionCross?entropy difference(c) USENETFigure 6: Perplexity on TURKDEV using different data selection and pruning techniques.communications.
This is the minimum number ofwords that must be inserted, substituted or deletedto transform the training sentence into a TURK-TRAIN sentence divided by the number of wordsin the TURKTRAIN sentence.
For example, thetraining sentence ?I didn?t sleep well Mondaynight either?
was given a WER of 0.33 becausetwo word-changes transformed it into a messagewritten by a worker: ?I didn?t sleep well lastnight?.?
Cross-entropy, in-domain ?
The average per-wordcross-entropy of the training sentence under a 3-gram model trained on TURKTRAIN.?
Cross-entropy, background ?
The average per-word cross-entropy of the training sentence un-der a 3-gram model trained on a random portionof the training set.
The random portion was thesame size as TURKTRAIN.We used these values to limit training to onlyAAC-like sentences.
We tried three different selec-tion methods.
In WER selection, only sentences be-low a threshold on the word error rate were kept inthe training data.
This tends to find variants of exist-ing communications in our Turk collection.In cross-entropy selection, we used only sen-tences below a threshold on the per-word cross-entropy with respect to a TURKTRAIN languagemodel.
This is equivalent to placing a threshold onthe perplexity.
Previously this technique has beenused to improve language models based on web data(Bulyko et al, 2007; Gao et al, 2002) and to con-struct domain-specific models (Lin et al, 1997).In cross-entropy difference selection, a sentence?sscore is the in-domain cross-entropy minus the back-ground cross-entropy (Moore and Lewis, 2010).This technique has been used to supplement Euro-pean parliamentary text (48M words) with newswiredata (3.4B words) (Moore and Lewis, 2010).
Wewere curious how this technique would work givenour much smaller in-domain set of 24K words.4.2 Data Selection and PruningWe built models selecting sentences below differentthresholds on the WER, in-domain cross-entropy, orcross-entropy difference.
For comparison, we alsopruned our models using conventional count-cutoffand entropy pruning (Stolcke, 1998).
During en-tropy pruning, we used a Good-Turing estimatedmodel for computing the history marginals as thelower-order Kneser-Ney distributions are unsuitablefor this purpose (Chelba et al, 2010).We calculated the perplexity of each model onthree test sets.
We also tallied the number of modelparameters (all n-gram probabilities plus all backoffweights).
On TURKDEV, cross-entropy differenceselection performed the best for all models sizes andfor all training sets (figure 6).
We also found cross-706l l l l l l l l l l l l l l ll l l ll l405060708090100ThresholdPerplexity?1 0 1l2?gram3?gram4?gram(a) TWITTERl l l l l l l llllllllllll l ll ll llThreshold?1 0 1l2?gram3?gram4?gram(b) BLOGl l l l l l l l l llllllllllllllllllllThreshold?1.4 ?0.4 0.4l2?gram3?gram4?gram(c) USENETFigure 7: Perplexity on TURKDEV varying thecross-entropy difference threshold.entropy difference was the best on COMM, reducingperplexity by 10?20% relative compared to cross-entropy selection.
Results on SPECIALISTS showedthat WER and both forms of cross-entropy selectionperformed similarly.
All three data selection meth-ods were superior to count-cutoff or entropy prun-ing.
We use cross-entropy difference selection forthe remainder of this paper.4.3 Model Order and Optimal ThresholdsWe created 2-gram, 3-gram and 4-gram models onTWITTER, BLOG, and USENET using a range ofcross-entropy difference thresholds.
4-gram modelsslightly outperformed 3-gram models (figure 7).
Theoptimal threshold for 4-gram models were as fol-lows: TWITTER 0.0, BLOG -0.4, and USENET -0.7.These thresholds resulted in using 20% of TWIT-TER, 5% of BLOG, and 1% of USENET.4.4 Mixture ModelWe created a mixture model using linear interpo-lation from the TWITTER, USENET and BLOG 4-gram models created with each set?s optimal thresh-old.
The mixture weights were optimized with re-spect to TURKDEV using SRILM.
The final mix-ture weights were: TWITTER 0.42, BLOG 0.29, andUSENET 0.29.
Our final 4-gram mixture model had43M total parameters and a compressed disk size of316 MB.5 EvaluationIn this section, we compare our mixture modelagainst baseline models.
We show performance withrespect to usage in a typical AAC text entry interfacebased on word prediction.5.1 Predictive Text EntryMany AAC communication devices use word pre-dictions.
In a word prediction interface users typeletters and the interface offers word completionsbased on the prefix of the current word and often theprior text.
By selecting one of the predictions, theuser can potentially save keystrokes as compared totyping out every letter of each word.We assume a hypothetical predictive keyboard in-terface that displays five word predictions.
Our key-board makes predictions based on up to three wordsof prior context.
Our keyboard predicts words evenbefore the first letter of a new word is typed.
Asa user types letters, predictions are limited to wordsconsistent with the typed letters.
If the system makesa correct prediction, we assume it takes only onekeystroke to enter the word and any following space.We only predict words in our 63K word vocab-ulary (empty prediction slots are possible).
We dis-play a word even if it was already a proposed predic-tion for a shorter prefix of the current word.
The firstword in a sentence is conditioned on the sentence-start pseudo-word.
If an out-of-vocabulary word istyped, the word is replaced in the language model?scontext with the unknown pseudo-word.We evaluate our predictive keyboard using thecommon metric of keystroke savings (KS):KS =(1?(kpka))?
100%,where kp is the number of keystrokes required withword predictions and ka is the number of keystrokesrequired without word prediction.5.2 Predictive Performance ExperimentWe compared our mixture model using cross-entropy difference selection with three baselinemodels trained on all of TWITTER, SWITCHBOARDand TURKTRAIN.
The baseline models were un-pruned 4-gram models trained using interpolatedmodified Kneser-Ney smoothing.
They had 72M,5M, and 129K parameters respectively.As shown in table 4, our mixture model per-formed the best on the three most AAC-like testsets (COMM, SPECIALISTS, and TURKTEST).
The707LM Test set PPL KSMixture COMM 47.9 62.5%Twitter COMM 55.9 60.9%Switchboard COMM 151.1 54.4%Turk COMM 165.9 52.7%Mixture SPECIALISTS 25.7 63.1%Twitter SPECIALISTS 27.3 61.9%Switchboard SPECIALISTS 64.5 57.7%Turk SPECIALISTS 85.9 52.8%Mixture TURKTEST 31.2 62.0%Twitter TURKTEST 42.3 59.3%Switchboard TURKTEST 172.5 50.6%Turk TURKTEST 51.0 57.6%Mixture SWITCHTEST 174.3 52.8%Twitter SWITCHTEST 142.6 54.9%Switchboard SWITCHTEST 79.2 58.8%Turk SWITCHTEST 642.5 42.9%Table 4: Perplexity (PPL) and keystroke savings(KS) of different language models on four test sets.The bold line shows the best performing languagemodel on each test set.mixture model provided substantial increases inkeystroke savings compared to a model trainedsolely on Switchboard.
The mixture model also per-formed better than simply training a model on alarge amount of Twitter data.
The model trained ononly 24K words of Turk data did surprisingly wellgiven its extremely limited training data.Our Switchboard model performed the best onSWITCHTEST with a keystroke savings of 58.8%.For comparison, past work reported a keystroke sav-ings of 55.7% on SWITCHTEST using a 3-grammodel trained on Switchboard (Trnka et al, 2009).While our mixture model performed less well onSWITCHTEST (52.8%), it is likely the other threetest sets better represent AAC communications.5.3 Larger Mixture Model ExperimentOur mixture language model used the best thresh-olds with respect to TURKDEV.
This resulted inthrowing away most of the training data.
This mightbe suboptimal in practice if an AAC user?s com-munications are somewhat different or more diversethan the language generated by the Turk workers.We trained a series of mixture models in whichwe varied the cross-entropy difference thresholdsl l l l l l l l l l l l l l l l50556065Change from optimal thresholdsKeystrokesavings(%)?0.5 0.0 0.5 1.0lSpecialistCommTurkTestSwitchTestFigure 8: Keystroke savings on mixture modelsvarying a constant added to the optimal thresholdswith respect to TURKDEV.by adding a constant to all three thresholds.
Themixture weights for each new model were opti-mized with respect to TURKDEV.
Using somewhatlarger models did improve keystroke savings for alltest sets except for TURKTEST (figure 8).
How-ever, using too large thresholds eventually hurt per-formance except on SWITCHTEST.
Performanceon SWITCHTEST steadily increased from 52.8% to56.6%.
These gains however came at the cost of big-ger models.
The model using +1.0 of the optimalthresholds had 384M parameters and a compressedsize of 3.0 GB.6 DiscussionGiven the ethical implications of collecting mes-sages from actual AAC users, it is unlikely that alarge corpus of genuine AAC messages will ever beavailable to researchers.
An important finding inthis paper is that crowdsourcing can be an effectiveway to obtain surrogate data for improving AAC lan-guage models.
Another finding is that Twitter pro-vides a continuous stream of large amounts of veryAAC-like data.
Twitter also has the advantage of al-lowing models to be continually updated to reflectcurrent events, new vocabulary, etc.6.1 Limitations and ImplicationsWe collected data from a large number of workers,some of whom may have written only a single com-708munication.
This may have resulted in more mes-sages about simple situations and perceived needswhich could differ from true AAC usage.Our data does not contain long-term two-sidedconversations.
Thus it may not be as useful for eval-uating techniques that adapt to past messages or thatuse the conversation partner?s communications.We asked workers to imagine they were usinga scanning-style AAC device.
We believe this ledworkers to presume they would require assistancein many routine physical tasks.
Our workers were(presumably) without cognitive or language impair-ments.
Thus our collection is more representativeof one subgroup of AAC communicators (scanningusers with normal cognitive function and languageskills).
By modifying the situation given to workers,it is likely we can expand our collection to better rep-resent other groups of AAC users, such as those us-ing predictive keyboards or eye-trackers.
However,obtaining data representative of users with cognitiveor language impairments via crowdsourcing wouldprobably be difficult.While we were unable to obtain real AAC mes-sages for testing, we believe the COMM and SPE-CIALIST test sets provide a good indication of thereal-world potential for our methods.
Our collectedTurk data was compared with reported data from ac-tual AAC users (though this comparison was neces-sarily coarse-grained).
We hope that by releasingour data and models it may be possible for thoseprivy to real AAC communications to validate andreport about the techniques described in this paper.We evaluated our models in terms of perplexityand keystrokes savings within the auspices of a pre-dictive keyboard.
Further work is needed to inves-tigate how our numeric gains translate to real-worldbenefits to users.
However, past work indicates moreaccurate predictions do in fact yield improvementsin human performance (Trnka et al, 2009).Finally, while the predictive keyboard is a com-monly studied interface, it is not appropriate for allAAC users.
Eye-tracker users may prefer an in-terface such as Dasher (Ward and MacKay, 2002).Single-switch users may prefer an interface such asNomon (Broderick and MacKay, 2009).
Any AACinterface based on word- or letter-based predictionsstands to benefit from the methods described in thispaper.7 ConclusionsIn this paper we have shown how workers?
creativityon a microtask crowdsourcing market can be usedto create fictional but plausible AAC communica-tions.
We have demonstrated that these messagesmodel conversational AAC better than the currentlyused datasets based on telephone conversations ornewswire text.
We used our new crowdsourceddataset to intelligently select sentences from Twit-ter, blog and Usenet data.We compared a variety of different techniques forintelligent training data selection.
We found thateven for our small amount of in-domain data, therecently proposed cross-entropy difference methodwas consistently the best (Moore and Lewis, 2010).Finally, compared to a model trained only onSwitchboard, our best performing model reducedperplexity by 60-82% relative on three AAC-like testsets.
This translated to a potential keystroke savingsin a predictive keyboard interface of 5?11%.In conclusion, we have shown how to create long-span AAC language models using openly avail-able resources.
Our models significantly outperformmodels trained on the commonly used data sourcesof telephone transcripts and newswire text.
To aidother researchers, we have publicly released ourcrowdsourced AAC collection, word lists and best-performing models.
We hope complementary tech-niques such as topic modeling and language modeladaptation will provide additive gains to those ob-tained by training models on large amounts of AAC-like data.
We plan to use our models to design andtest new interfaces that enable faster communicationfor AAC users.AcknowledgmentsWe thank Keith Trnka and Horabail Venkatagiri fortheir assistance.
This work was supported by the En-gineering and Physical Sciences Research Council(grant number EP/H027408/1).ReferencesBruce Baker, Katya Hill, and Richard Devylder.
2000.Core vocabulary is the same across environments.
InCalifornia State University at Northridge Conference.David R. Beukelman, Kathryn M. Yorkston, MiguelPoblete, and Carlos Naranjo.
1984.
Frequency of709word occurrence in communication samples producedby adult communication aid users.
Journal of Speechand Hearing Disorders, 49:360?367.Tamara Broderick and David J. C. MacKay.
2009.
Fastand flexible selection with a single switch.
PLoS ONE,4(10):e7481.Ivan Bulyko, Mari Ostendorf, Manhung Siu, Tim Ng,Andreas Stolcke, and O?zgu?r C?etin.
2007.
Webresources for language modeling in conversationalspeech recognition.
ACM Transactions on Speech andLanguage Processing, 5(1):1?25.Kevin Burton, Akshay Java, and Ian Soboroff.
2009.
TheICWSM 2009 Spinn3r dataset.
In Proceedings of the3rd Annual Conference on Weblogs and Social Media.Ciprian Chelba, Thorsten Brants, Will Neveitt, and PengXu.
2010.
Study on interaction between entropy prun-ing and Kneser-Ney smoothing.
In Proceedings of theInternational Conference on Spoken Language Pro-cessing, pages 2422?2425.Stanley F. Chen and Joshua T. Goodman.
1998.
Anempirical study of smoothing techniques for languagemodeling.
Technical report, Computer Science Group,Harvard University.Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-Fu Lee.
2002.
Toward a unified approach to statisticallanguage modeling for chinese.
ACM Transactions onAsian Language Information Processing, 1:3?33.Nestor Garay-Vitoria and Julio Abascal.
2006.
Text pre-diction systems: A survey.
Universal Access in theInformation Society, 4:188?203.Nestor Garay-Vitoria and Julio Gonza?lez-Abascal.
1997.Intelligent word-prediction to enhance text input rate.In Proceedings of the 2nd ACM International Confer-ence on Intelligent User Interfaces, pages 241?244.J.J.
Godfrey, E.C.
Holliman, and J. McDaniel.
1992.SWITCHBOARD: Telephone speech corpus for re-search and development.
Proceedings of the IEEEConference on Acoustics, Speech, and Signal Process-ing, pages 517?520.David Graff, Roni Rosenfeld, and Doug Pau.
1995.CSR-III text.
Linguistic Data Consortium, Philadel-phia, PA, USA.David Graff.
2003.
English gigaword corpus.
LinguisticData Consortium, Philadelphia, PA, USA.Sheri Hunnicutt.
1989.
Using syntactic and semantic in-formation in a word prediction aid.
In Proceedings ofthe 1st European Conference on Speech Communica-tion and Technology, pages 1191?1193.Reinhard Kneser and Hermann Ney.
1995.
Im-proved backing-off for m-gram language modeling.In Proceedings of the IEEE Conference on Acoustics,Speech, and Signal Processing, pages 181?184.Gregory W. Lesher and Gerard J. Rinkus.
2002.Domain-specific word prediction for augmentativecommunication.
In Proceedings of the RESNA 2002Annual Conference.Jianhua Li and Graeme Hirst.
2005.
Semantic knowl-edge in word completion.
In Proceedings of the 7thInternational ACM SIGACCESS Conference on Com-puters and Accessibility, pages 121?128.Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, Ker-Jiann Chen, and Lin-Shan Lee.
1997.
Chinese lan-guage model adaptation based on document classifi-cation and multiple domain-specific language models.In Proceedings of the 5th European Conference onSpeech Communication and Technology, pages 1463?1466.Robert C. Moore and William Lewis.
2010.
Intelligentselection of language model training data.
In Proceed-ings of the 48th Annual Meeting of the Association ofComputational Linguistics, pages 220?224.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Un-supervised modeling of twitter conversations.
In Pro-ceedings of HLT-NAACL 2010, pages 172?180.Cyrus Shaoul and Chris Westbury.
2009.
A USENETcorpus (2005-2009).
University of Alberta, Canada.Andreas Stolcke.
1998.
Entropy-based pruning ofbackoff language models.
In Proceedings of DARPABroadcast News Transcription and UnderstandingWorkshop, pages 270?274.Andreas Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proceedings of the 7th Annual In-ternational Conference on Spoken Language Process-ing, pages 901?904.Keith Trnka, Debra Yarrington, and Christopher Penning-ton.
2006.
Topic modeling in fringe word predictionfor AAC.
In Proceedings of the 11th ACM Interna-tional Conference on Intelligent User Interfaces, pages276?278.Keith Trnka, John McCaw, Debra Yarrington, Kathleen F.McCoy, and Christopher Pennington.
2009.
User in-teraction with word prediction: The effects of predic-tion quality.
ACM Transactions on Accessible Com-puting, 1:17:1?17:34.Keith Trnka.
2008.
Adaptive language modeling forword prediction.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Linguis-tics on Human Language Technologies: Student Re-search Workshop, pages 61?66.Horabail Venkatagiri.
1999.
Efficient keyboard layoutsfor sequential access in augmentative and alternativecommunication.
Augmentative and Alternative Com-munication, 15(2):126?134.Tonio Wandmacher and Jean-Yves Antoine.
2007.Methods to integrate a language model with semantic710information for a word prediction component.
Pro-ceedings of the Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning, pages 506?513.Tonio Wandmacher, Jean-Yves Antoine, Franck Poirier,and Jean-Paul De?parte.
2008.
SIBYLLE, an assis-tive communication system adapting to the context andits user.
ACM Transactions on Accessible Computing,1:6:1?6:30.D.
J.
Ward and D. J. C. MacKay.
2002.
Fast hands-freewriting by gaze direction.
Nature, 418(6900):838.711
