Text Representations for Patent ClassificationEva D?hondt?Radboud University NijmegenSuzan Verberne?
?Radboud University NijmegenCornelis Koster?Radboud University NijmegenLou Boves?Radboud University NijmegenWith the increasing rate of patent application filings, automated patent classification is of risingeconomic importance.
This article investigates how patent classification can be improved byusing different representations of the patent documents.
Using the Linguistic ClassificationSystem (LCS), we compare the impact of adding statistical phrases (in the form of bigrams)and linguistic phrases (in two different dependency formats) to the standard bag-of-words textrepresentation on a subset of 532,264 English abstracts from the CLEF-IP 2010 corpus.
Incontrast to previous findings on classification with phrases in the Reuters-21578 data set, forpatent classification the addition of phrases results in significant improvements over the unigrambaseline.
The best results were achieved by combining all four representations, and the secondbest by combining unigrams and lemmatized bigrams.
This article includes extensive analyses ofthe class models (a.k.a.
class profiles) created by the classifiers in the LCS framework, to examinewhich types of phrases are most informative for patent classification.
It appears that bigramscontribute most to improvements in classification accuracy.
Similar experiments were performedon subsets of French and German abstracts to investigate the generalizability of these findings.1.
IntroductionAround the world, the patent filing rates in the national patent offices have been in-creasing year after year, creating an enormous volume of texts, which patent examiners?
Center for Language Studies, PO Box 9103, 6500 HD Nijmegen, the Netherlands.E-mail: e.dhondt@let.ru.nl.??
Center for Language Studies / Institute for Computing and Information Sciences, PO Box 9103, 6500 HDNijmegen, the Netherlands.
E-mail: s.verberne@let.ru.nl.?
Institute for Computing and Information Sciences, PO Box 9010, 6500 HD Nijmegen, the Netherlands.E-mail: kees@cs.ru.nl.?
Center for Language Studies, PO Box 9103, 6500 HD Nijmegen, the Netherlands.E-mail: l.boves@let.ru.nl.Submission received: 19 March 2012; revised submission received: 8 August 2012; accepted for publication:19 September 2012.doi:10.1162/COLI a 00149?
2013 Association for Computational LinguisticsComputational Linguistics Volume 39, Number 3are struggling to manage (Benzineb and Guyot 2011).
To speed up the examinationprocess, a patent application needs to be directed to patent examiners specializedin the subfield(s) of that particular patent as quickly as possible (Smith 2002).
Thispreclassification is done automatically in most patent offices, but substantial addi-tional manual labor is still necessary.
Furthermore, since 2010, the International PatentClassification1 (IPC) is revised every year to keep track of recent developments in thevarious subdomains.
Such a revision is followed by a reclassification of portions ofthe existing patent corpus, which is currently done mainly by hand by the nationalpatent offices (Held, Schellner, and Ota 2011).
Both preclassification and reclassificationcould be improved, and a higher consistency of the classifications of the documentsin the patent corpus could be obtained, if more reliable and precise automatic textclassification algorithms were available (Benzineb and Guyot 2011).Most approaches to text classification use the bag-of-words (BOW) text representa-tion, which represents each document by the words that occur in it, irrespective of theirordering in the original document.
In the last decades much research has gone intoexpanding this representation with additional information, such as statistical phrases2(n-grams) or some forms of syntactic or semantic knowledge.
Even though (statistical)phrases are more representative units for classes than single words (Caropreso, Matwin,and Sebastiani 2001), they are so sparsely distributed that they have limited impactduring the classification process.
Therefore, it is not surprising that the best scoringmulti-class, multi-label3 classification results for the well-known Reuters-21578 data sethave been obtained using a BOW representation (Bekkerman and Allan 2003).
But thelimited contribution of phrases in addition to the BOW-representation does not seemto hold for all classification tasks: O?zgu?r and Gu?ngo?r (2010) found significant differ-ences in the impact of linguistic phrases between short newswire texts (Reuters-21578),scientific abstracts (NSF), and informal posts in usenet groups (MiniNg): Especially theclassification of scientific abstracts could be improved by using phrases as index terms.In a follow-up study, O?zgu?r and Gu?ngo?r (2012) found that for the three different datasets, different types of linguistic phrases have most impact.
The authors conclude thatmore formal text types benefit from more complex syntactic dependencies.In this article, we investigate if similar improvements can be found for patentclassification and, more specifically, which types of phrases are most effective for thisparticular task.
In this article we investigate the value of phrases for classification bycomparing the improvements that can be gained from extending the BOW representa-tion with (1) statistical phrases (in the form of bigrams); (2) linguistic phrases originatingfrom the Stanford parser (see Section 3.2.2); (3) aboutness-based4 linguistic phrases fromthe AEGIR parser (Section 3.2.3); and (4) a combination of all of these.
Furthermore, wewill investigate the importance of different syntactic relations for the classification task,1 The IPC is a complex hierarchical classification system comprising sections, classes, subclasses, andgroups.
For example, the ?A42B 1/12?
class label, which groups designs for bathing caps, falls undersection A ?Human necessities,?
class 42 ?Headwear,?
subclass B ?Head coverings,?
group 1 ?Hats; caps;hoods.?
The latest edition of the IPC contains eight sections, 129 classes, 639 subclasses, 7,352 groups,and 61,847 subgroups.
The IPC covers inventions in all technological fields in which inventions canbe patented.2 By a phrase we mean an index unit consisting of two or more words, generated through either syntacticor statistical methods.3 Multi-class classification is the problem of classifying instances into more than two classes.
Multi-labelsignifies that documents in this test set are associated with more than one class, and must be assigned aset of labels during classification.4 The notion of aboutness refers to the conceptual content expressed by a dependency triple.
For a moredetailed description, see Section 3.2.3.756D?hondt et alText Representations for Patent Classificationand the extent to which the words in the phrases overlap with the unigrams.
We alsoinvestigate which syntactic relations capture most information in the opinion of humanannotators.
Finally, we perform experiments to investigate if our findings are language-dependent.
We will then draw some conclusions on what information is most valuablefor improving automatic patent classification.2.
Background2.1 Text Representations in ClassificationLewis (1992) was the first to investigate the use of phrases as index terms for textclassification.
He found that phrases generally suffer from data sparseness and mayactually cause classification performance to deteriorate.
These findings were confirmedby Apte?, Damerau, and Weiss (1994).
With the advent of increasing computationalpower and bigger data sets, however, the topic has been revisited in the last two decades(Bekkerman and Allan 2003).In this section we will give an overview of the major findings in previous re-search on the use of statistical and syntactic phrases for text classification.
Except whenmentioned explicitly, all the classification experiments reported here were conductedusing the Reuters-21578 data set, a well-known benchmark of 21,578 short newswiretexts for multi-class classification into 118 categories (a document has an average of1.24 class labels).2.1.1 Combining Unigrams with Statistical Phrases.
For an excellent overview of the workon using phrases done up to 2002, see Bekkerman and Allan (2003), and Tan, Wang, andLee (2002).Because they contain more specific information, one might think that phrases aremore powerful features for text classification.
There are two ways of using phrases asindex terms: either index terms only or in combination with unigrams.
All experimentalresults, however, show that using only phrases as index terms leads to a decreasein classification accuracy compared with the BOW baseline (Bekkerman and Allan2003).
Both Mladenic and Grobelnik (1998) and Fu?rnkranz (1998) showed that classifierstrained on combinations of unigrams and n-grams composed of at most three wordsperformed better than classifiers that only use unigrams; no improvement was obtainedwhen using larger n-grams.
Because trigrams are sparser than bigrams, most of thesubsequent research has focused on optimizing the combination of unigrams andbigrams using different feature selection techniques.2.1.2 Feature Selection.
Obviously, unigrams and bigrams overlap: Bigrams are pairs ofunigrams.
Caropreso, Matwin, and Sebastiani (2001) evaluated the relative importanceof unigrams and bigrams in a classifier-independent study: Instead of determining theimpact of features on the classification scores, they scored all unigrams and bigramsusing conventional feature evaluation functions to find the features that are mostrepresentative for the document classes.
For the Reuters-21578 data set, they foundthat many bigram features scored higher than unigram features.
These (theoretical)findings were not confirmed in subsequent classification experiments, however.
Whenthe bigram/unigram ratio for a fixed number of features is changed to favor bigrams,classification performance tends to go down.
It appears that the information in thebigrams does not turn the unigrams redundant.757Computational Linguistics Volume 39, Number 3Braga, Monard, and Matsubara (2009) used a Multinomial Naive Bayes classifierto investigate classification performance with unigrams and bigrams by comparingmultiview classification (the results of two independent classifiers trained with unigramand bigram features are merged) with monoview classification (unigrams and bigramsare combined in a single feature set).5 They found that there is little difference betweenthe output of the mono- and multiview classifiers.
In the multiview classifiers, theunigram and bigram classifiers make similar decisions in assigning labels, althoughthe latter generally yielded lower confidence values.
Consequently, in the merge theunigram and bigram classifiers affirm each other?s decisions, which does not resultin an overall improvement in classification accuracy.
The authors suggest combiningunigrams only with those bigrams for which it holds that the whole provides moreinformation than the sum of the parts.Tan, Wang, and Lee (2002) proposed selecting highly representative and meaningfulbigrams based on the Mutual Information scores of the words in a bigram comparedwith the unigram class model.
They selected only the top 2% of the bigrams as indexterms, and found a significant improvement over their unigram baseline, which was lowcompared to state-of-the-art results.
Bekkerman and Allan (2003) failed to improve overtheir unigram baseline when using similar selection criteria based on the distributionalclustering of unigram models.
Crawford, Koprinska, and Patrick (2004) were not able toimprove e-mail classification when using the selection criteria proposed by Tan, Wang,and Lee.2.1.3 Combining Unigrams with Syntactic Phrases.
Lewis (1992) and Apte?, Damerau, andWeiss (1994) were the first to investigate the impact of syntactic phrases6 as features fortext classification.
Dumais et al(1998) and Scott and Matwin (1999) did not observea significant improvement in classification on the Reuters-21578 collection when nounphrases obtained with a shallow parser were used instead of unigrams.
Moschitti andBasili (2004) found that neither words augmented with word sense information, norsyntactic phrases (acquired through shallow parsing) in combination with unigramsimproved over the BOW baseline.
Syntactic phrases appear to be even sparser thanbigrams.
Therefore, it is not surprising that most papers concluded that classifiers usingonly syntactic phrases perform worse than the baseline, except when the BOW baselineis low for that particular classification task (Mitra et al1997; Fu?rnkranz 1999).Deep syntactic parsing is a computationally expensive process, but thanks to theincrease in computational power it is now possible to use phrases acquired throughdeep syntactic parsing in classification tasks.
Nastase, Sayyad, and Caropreso (2007)used Minipar to generate dependency triples that are combined with lemmatized andunlemmatized unigrams to classify the 10 most frequent classes in the Reuters-21578data set.
Their criterion for selecting triples as index terms is document frequency ?
2.The small improvement over the lemmatized unigram baseline was not statisticallysignificant.
O?zgu?r and Gu?ngo?r (2010, 2012) achieve small but significant improvementswhen combining unigrams with a subset of the dependency types from the Stanfordparser on three different data sets, including the Reuters-21578 set.
They find thatseparate pruning levels (based on the term frequency?inverse document frequency[TF-IDF] score of the index units) for the unigrams and syntactic phrases influence5 The difference between multiview and monoview classification corresponds to what is called late andearly fusion in the pattern recognition literature.6 The concept ?syntactic phrase?
can be given several different interpretations, such as noun phrases,verb phrases, predicate structures, dependency triples, and so forth.758D?hondt et alText Representations for Patent Classificationclassification accuracy.
Which dependency relations prove most relevant for a classifi-cation task depends greatly on the language use in the different data sets: The informalMiniNG data set (usenet posts) benefits a little from ?simple?
dependencies such as part,denoting a phrasal verb, for example write down, while classification in the more formalReuters-21578 (newswire) and NSF (scientific abstracts) data sets is more improved byusing dependencies on phrase and clause level (adjectival modifier, compound noun,prepositional attachment; and subject and object, respectively).
The highest-rankingfeatures for the NSF data set are compound noun (nn), adjectival modifier (amod),subject (subj), and object (obj), respectively.
Furthermore, they observe that splitting upmore generic relator types (such as prep) into different, more specific, subtypes increasesthe classification accuracy.2.2 Patent ClassificationIt is not possible to draw far-reaching conclusions from previous research on patentclassification, because there is no tradition of using a ?standard?
data set, and a standardsplit of patent corpora in a training and test set.
Furthermore, there are differencesbetween the various experiments in task definitions (mono-label versus multi-labelclassification); the granularity of the classification (depth in the IPC hierarchy); and thechoices of (sub)sets of data.
Fall and Benzineb (2002) give an overview of the work donein patent classification research up to 2002 and of the commercial patent classificationsystems available; see Benzineb and Guyot (2011) for a general introduction to patentclassification.Larkey (1999) was the first to present a fully automated patent classification system,but she did not report her overall accuracy results.
Larkey (1998) used a combinationof weighted words and noun phrases as index terms to classify a subset of the USPTOdatabase, but found no improvement over a BOW baseline.
The weights were calculatedas follows: Frequency of a word or phrase in a particular section times the manuallyassigned weight (importance) given to that section.
The weights for each word or phrasewere then summed across sections.
Term selection was based on a threshold for theseweights.Krier and Zacca` (2002) organized a comparative study of various academic andcommercial systems for patent classification for a common data set.
In this informalbenchmark Koster, Seutter, and Beney (2001) achieved the best results, using the Bal-anced Winnow algorithm with a word-only text representation.
Classification is per-formed for 44 or 549 categories (which correspond to different levels of depth in thethen used version of the IPC), with around 78% and 68% precision at 100% recall,respectively.Fall et al(2003) introduced the EPO-alpha data set, attempting to create a commonbenchmark for patent classification.
Using only words as index terms, they testeddifferent classification algorithms and found that SVM outperform Naive Bayes, k-NN,SNoW, and decision-based classifiers.
They achieved P@3-scores7 of 73% and 59% on114 classes and 451 subclasses, respectively.
They also found that when using onlythe first 300 words from the abstract, claims, and description sections, classificationaccuracy is increased compared with using the complete sections.
The same data setwas later used by Koster and Seutter (2003), who experimented with a combined7 Precision at rank 3 (P@3) signifies the percentage correct labels in the first three labels by the classifier to agiven document.759Computational Linguistics Volume 39, Number 3representation of words and phrases consisting of head-modifier pairs.8 They foundthat head-modifier pairs could not improve on the BOW-baseline: The phrases were toosparse to have much impact on the classification process.Starting in 2009, the IRF9 has organized CLEF-IP patent classification tracks in anattempt to bridge the gap between academic research and the patent industry.
For thispurpose the IRF has put a lot of effort into providing very large patent data sets,10 whichhave enabled academic researchers to train their algorithms on real-life data.
In theCLEF-IP 2010 classification track the best results were achieved by Guyot, Benzineb,and Falquet (2010).
Using the Balanced Winnow algorithm, they achieved a P@1-scoreof 83%, while classifying on subclass level.
They used a combination of words andstatistical phrases (collocations of variable length extracted from the corpus) as indexterms and used all available documents (in English, French, and German) in the corpusas training data.
In the same competition, Derieux et al(2010) came second (in terms ofP@1).
They also used a mixed document representation of both single words and longerphrases, which had been extracted from the corpus by counting word co-occurrences.Verberne, Vogel, and D?hondt (2010) and Beney (2010) experimented with a combinedrepresentation of words and syntactic phrases derived from an English and Frenchsyntactic parser, respectively.
They both found that adding syntactic phrases to wordsimproves classification accuracy slightly.
Beney (2010) remarks that this improvementmay be language-dependent.
As a follow-up, Koster et al(2011) investigated the addedvalue of syntactic phrases.
They found that attributive phrases, that is, combinationsof adjective or nouns with nouns, were by far the most important syntactic phrases forpatent classification.
On a subset of the CLEF-IP 2010 corpus11 they also found a small,but significant, improvement when adding dependency triples to words.3.
Experimental Set-upIn this article, we investigate the relative contributions of different types of terms tothe performance of patent classification.
We use four different types of terms, namely,lemmatized unigrams, lemmatized bigrams (see Section 3.2.1), lemmatized dependencytriples obtained with the Stanford parser (see Section 3.2.2), and lemmatized depen-dency triples obtained with the AEGIR parser (see Section 3.2.3).
We will leave term(feature) selection to the preprocessing module of the Linguistic Classification System(LCS) which we used for all experiments (see Section 3.3).
We will analyze the rela-tion between unigrams and phrases in the class profiles in some detail, however (seeSections 4.2 and 4.3).3.1 Data SelectionWe conducted classification experiments on a collection of patent documents obtainedfrom the CLEF-IP 2010 corpus,12 which is a subset of the larger MAREC patent col-lection.
The corpus contains 2.6 million patent documents, which roughly correspond8 Head-modifier pairs were derived from the syntactic analysis output of the EP4IR syntactic parser.9 Information Retrieval Facility, see www.irf.com.10 The CLEF-IP 2009, CLEF-IP 2010, and CLEF-IP 2011 data sets can be obtained through the IRF.
The morerecent data sets subsume the older sets.11 The same data set as will be used in this article.
For a more detailed description, see Section 3.1.12 This test collection is available through the IRF (http://www.ir-facility.org/collection).760D?hondt et alText Representations for Patent Classificationto 1.3 million individual patents, published between 1985 and 2001.13 The documentsin the collection are encoded in a customized XML format and may include text inEnglish, French, and German.
In addition to the standard sections of a patent document(title, abstract, claims, and description section), the documents also include meta-information on inventor, date of application, assignee, and so forth.
Because our focuslies on text representation, we did not include any of the meta-data in our documentrepresentations.The most informative sections of a patent document are generally considered tobe the title, the abstract, and the beginning of the description (Benzineb and Guyot2011).
Verberne and D?hondt (2011) showed that using both the description and theabstract gives a small, but significant, improvement in classification results on theCLEF-IP 2011 corpus, compared with classification on abstracts only.
The effort in-volved in parsing the descriptions is considerable, however: Because of the longsentences and the dense word use, a parser will have much more difficulty in pro-cessing text from the description section than from the abstracts.
The titles of thepatent documents also pose a parsing problem: These are generally short noun phrasesthat contain ambiguous PP-attachments that are impossible to disambiguate withoutany domain knowledge.
This leads to incorrect syntactic analyses and, consequently,noisy dependency triple features.
Because we are interested in comparing classificationresults for different text representations, and not in comparing results for differentsections, we opted to use only the abstract sections of the patent document in thecurrent article.From the corpus, we extracted all files that contain both an abstract in English and atleast one IPC class14 in the <classification-ipcr> field.
We extracted the IPC classes on thedocument level; this means that we did not include the documents where the IPC classis in a separate file than the English abstract.
In total, there were 121 different classes inour data set.
Most documents have been assigned one to three different IPC classes (onclass level).
On average, a patent abstract in our data set has 2.12 class labels.
Previouscross-validation experiments on the same document set showed very little variation(standard deviation < 0.3%) between the classification accuracies in different training-test splits (Verberne, Vogel, and D?hondt 2010).
We therefore decided to use only onetraining and test set split.15The final data set contained 532,264 abstracts, divided into two sets: (1) a trainingset (425,811 documents) and (2) a test set (106,453 documents).
The distribution of thedata over the classes is in accordance with the Pareto Principle: 20% of the classes cover80% of the data, and 80% of the classes comprise only 20% of the data.3.2 Data PreprocessingPreprocessing included cleaning up character conversion errors like Expression (1)and removing claims and images references (Expression (2)) and list references13 Note the difference between a patent and a patent document: A patent is not a physical document itself,but a name for a group of patent documents that have the same patent ID number.14 For our classification experiments we use the codes on the class level in the IPC8 classification.15 The data split was performed using a perl script that randomly shuffles the documents and puts theminto a train set and test set, while ensuring that the class distribution of the examples in the train setapproximates that of the whole corpus.
It can be downloaded as part of the LCS distribution.761Computational Linguistics Volume 39, Number 3(Expression (3)) from the original texts.
This was done automatically, using the follow-ing regular expressions (based on Parapatics and Dittenbach 2009):s/;gt&/>/g (1)s/(\([ ]*[0-9][0-9a-z,.
; ]*\))//g (2)s/(\([ ]*[A-Za-z]\))//g (3)We then used a perl script to divide the running text into sentences, by splitting onend-of-sentence punctuation such as question marks and full stops.
In order to mini-mize incorrect splitting, the perl script was supplied with a list of common Englishabbreviations and a list containing abbreviations and acronyms that occur frequently intechnical texts, derived from the Specialist lexicon.163.2.1 Unigrams and Bigrams.
The sentences in the abstract documents were convertedto single words by splitting on whitespaces and removing punctuation.
The wordswere then lemmatized using the AEGIR lexicon.
Bigrams were created through asimilar procedure.
We did not create bigrams that spanned sentence boundaries.
Thisresulted in approximately 60 million unigram and bigram tokens for the presentcorpus.3.2.2 Stanford.
The Stanford parser is a broad-coverage natural language parser that istrained on newswire text, for which it achieves state-of-the-art performance.
The parserhas not been optimized/retrained for the patent domain.17 In spite of the technicaldifficulties (Parapatics and Dittenbach 2009) and loss of linguistic accuracy for patenttexts reported in Mille and Wanner (2008), most patent processing systems that uselinguistic phrases use the Stanford parser because its dependency scheme has a numberof properties that are valuable for Text Mining purposes (de Marneffe and Manning2008).
The Stanford parser collapsed typed dependency model has a set of 55 differ-ent syntactic relators to capture semantically contentful relations between words.
Forexample, the sentence The system will consist of four separate modules is analyzed into thefollowing set of dependency triples in the Stanford representation:det(system-2, The-1)nsubj(consist-4, system-2)aux(consist-4, will-3)root(ROOT-0, consist-4)num(modules-8, four-6)amod(modules-8, separate-7)prep_of(consist-4, modules-8)The Stanford parser was compiled with a maximum memory heap of 1.2 GB.Sentences longer than 100 words were automatically skipped.
Combined with failedparses this led to a 1.2% loss of parser output on the complete data set.
Parsing the16 The lexicon can be downloaded at http://lexsrv3.nlm.nih.gov/Specialist/Summary/lexicon.html.17 For retraining a parser, a substantial amount of annotated data (in the form of syntactically annotateddependency trees) is needed.
Creating such annotations is a very expensive task and beyond the scope ofthis article.762D?hondt et alText Representations for Patent ClassificationTable 1Impact of lemmatization on the different text types in the training set (80% of the corpus).# tokens # types (terms) token/type (lem.
)raw lemmatized gainunigram 48,898,738 160,424 142,396 1.12 343.39bigram 48,473,756 3,836,212 3,119,422 1.23 15.54Stanford 35,772,003 8,750,839 7,430,397 1.18 4.81AEGIR 31,004,525 ?
5,096,918 ?
6.08entire set of abstracts took 1.5 weeks on a computer cluster consisting of 60 2.4GHzcores with 4 GB RAM per core.
The resulting dependency triples were stripped of theword indexes and then lemmatized using the AEGIR lexicon.3.2.3 AEGIR.
AEGIR18 is a dependency parser that was designed specifically for ro-bust parsing of technical texts.
It combines a hand-crafted grammar with an exten-sive word-form lexicon.
The parser lexicon was compiled from different technicalterminologies, such as the SPECIALIST lexicon and the UMLS.19 The AEGIR parseraims to capture the aboutness of sentences.
Rather than outputting extensive linguis-tic detail on the syntactic structure of the input sentence as in the Stanford parser,AEGIR returns only the bare syntactic?semantic structure of the sentence.
During theparsing process, it effectively performs normalization at various levels, such as ty-pography (for example, upper and lower case, spacing), spelling (for example, Britishand American English, hyphenation), morphology (lemmatization of word forms), andsyntax (standardization of the word order and transforming passive structures intoactive ones).The AEGIR parser uses only eight syntactic relators and returns fewer uniquetriples than the Stanford parser.
The parser is currently still under development; for thisarticle we used the version AEGIR v.1.7.5.
The parser was constrained to a time limit ofmaximum three seconds per sentence.
This caused a loss of 0.7% of parser output on thecomplete data set.
Parsing the entire set of abstracts took slightly less than a week onthe computer cluster described above.
The AEGIR parser has several output formats,among which its own dependency format.
The example sentence used to illustrate theoutput of the Stanford parser is analyzed as follows:[system,SUBJ,consist][consist,PREPof,module][module,ATTR,separate][module,QUANT,four]3.2.4 Lemmatization.
Table 1 shows the impact of lemmatization (using the AEGIR lex-icon) on the distribution of terms for the different text representations.
Lemmatization18 AEGIR stands for Accurate English Grammar for Information Retrieval.
Using the AGFL compiler (found athttp://www.agfl.cs.ru.nl/) this grammar can be compiled into an operational parser.
The grammar isnot freely distributed.19 The Unified Medical Language System contains a widely-used terminology of the biomedical domainand can be downloaded at http://www.nlm.nih.gov/research/umls/.763Computational Linguistics Volume 39, Number 3and stemming are standard approaches to decreasing the sparsity of features; stemmingis more aggressive than lemmatization.
Ozgu?r and Gu?ngo?r (2009) showed that?whenusing only words as index terms?stemming (with the Porter Stemmer) appears toimprove performance; stemming dependency triples did not improve performance,however.We opted to use a less aggressive form of generalization: Lemmatizing the wordforms.
We found that the bigrams gain20 most by lemmatizing the word forms, resultingin a higher token/type ratio.
From Table 1 it can be seen that there are fewer tripletokens than bigram tokens: Whereas all the (high-frequency) function words are kept inthe bigram representations, both dependency formats discard some function words intheir parser output.
For example, the AEGIR parser does not create triples for auxiliaryverbs, and in both dependency formats, the prepositions become part of the relator.Consequently, the parsers will output fewer but more variable tokens, which results inlower token/type ratios and a lower impact of lemmatization.3.3 Classification ExperimentsThe classification experiments were carried out within the framework of the LCS(Koster, Seutter, and Beney 2003).
The LCS has been developed for the purpose ofcomparing different text representations.
Currently, three classifier algorithms are avail-able: Naive Bayes, Balanced Winnow (Dagan, Karov, and Roth 1997), and SVM-light(Joachims 1999).
Verberne, Vogel, and D?hondt (2010) found that Balanced Winnow andSVM-light yield comparable classification accuracy scores for patent texts on a similardata set, but that Balanced Winnow is much faster than SVM-light for classificationproblems with a large number of classes.
The Naive Bayes classifier yielded a loweraccuracy.
We therefore only used the Balanced Winnow algorithm for our classificationexperiments, which were run with the following LCS configuration, based on tuningexperiments on the same data by Koster et al(2011): Global term selection (GTS): Document frequency minimum is 2, term frequencyminimum is 3.
Although initial term selection is necessary when dealing with sucha large corpus, we deliberately aimed at keeping as many of the sparse phrasalterms as possible. Local term selection (LTS): Simple Chi Square (Galavotti, Sebastiani, and Simi2000).
We used the LCS option to automatically select the most representativeterms for every class, with a hard maximum of 10,000 terms per class.21 After LTS the selected terms of all classes are aggregated into one combined termvocabulary, which is used as the starting point for training the individual classes(see Table 3).20 By ?gain?
we mean the decrease in number of types for the lemmatized forms compared to thenon-lemmatized forms, which will result in higher corresponding token/type ratios.21 Increasing the cut-off to 100,000 terms resulted in a small increase in accuracy (F1 values) for thecombined representations, mostly for the larger classes.
Because the patent domain has a large lexicalvariety, a large amount of low-frequency terms in the tail of the term distribution can have a largeimpact on the accuracy scores.
Because we are more interested in the relative gains between differenttext representations and the corresponding top terms in the class profiles than in achieving maximumclassification scores, we opted to use only 10,000 terms for efficiency reasons.764D?hondt et alText Representations for Patent ClassificationTable 2Impact of global term selection (GTS) criteria on the different text types in the training set (80%of the corpus).total # of terms # of terms selected in GTS % of terms removed in GTSunigram 142,396 58,42322 58.97bigram 3,119,422 1,115,170 64.25stanford 7,430,397 1,618,478 78.22AEGIR 5,096,918 1,312,715 74.24 Term strength calculation: LTC algorithm (Salton and Buckley 1988) which is anextension of the TF?IDF measure. Training method: Ensemble learning based on one-versus-rest binary classifiers. Winnow configuration: We performed tuning experiments for the Winnow param-eters on a development set of around 100,000 documents.
We arrived at using thesame setting as Koster et al(2011), namely, ?
= 1.02, ?
= 0.98, ?+ = 2.0, ??
= 0.5,with a maximum of 10 training iterations. For each document the LCS returns a ranked list of all possible labels and theattendant confidence scores.
If the score assigned is higher than a predeterminedthreshold, the document is assigned that category.
The Winnow algorithm has adefault (natural) threshold equal to one.
We configured the LCS to return a min-imum of one label (with the highest score, even if it is lower than the threshold)and a maximum of four labels for each document. The classification quality was determined by calculating the Precision, Recall, andF1 measures per document/class combination (see, e.g., Koster, Seutter, and Beney2003), on the document level (micro-averaged scores).Table 2 shows the impact of our global term selection criteria for the different textrepresentations.
This first feature reduction step is category-independent: The featuresare discarded on the basis of the term and document frequencies over the corpus, dis-regarding their distributions for the specific categories.
We can see that the token/typeratio of Table 1 is mirrored in this table: The sparsest syntactic phrases lose most terms.Although the Stanford parser output is the sparsest text representation, it has the largestpool of terms to select from at the end of the GTS process.The impact of the second feature reduction phase is shown in Table 3.
During localterm selection, the LCS finds the most representative terms for each class by selectingthe terms whose distributions in the sets of positive and negative training examplesfor that class are maximally different from the general term distribution.
We can seethat in the combined runs only around 50% of the selectable unigrams (after GTS) are22 For the BOW baseline, the GTS criteria resulted in a too small term set that could then be used as astarting point for the local term selection process for the individual classes.
In such cases, the LCS hasa back-off mechanism that automatically (re)selects terms that were initially discarded during GTS.In other words, the baseline classifier used terms that do not comply with the criteria in the GTS asdescribed in the text.
In the combination runs, enough terms remained after GTS and no unigrams orphrases that did not match the GTS criteria were selected.765Computational Linguistics Volume 39, Number 3Table 3Impact of local term selection (LTS) criteria in the training set (80% of the corpus).# of terms after GTS # of terms after LTSbaseline uni 58,423 69,476unigrams + bigrams uni 58,423 23,753bi 1,115,170 300,826unigrams + stanford triples uni 58,423 26,630stanford 1,618,478 424,204unigrams + AEGIR triples uni 58,423 29,348AEGIR 1,312,715 409,851Table 4Classification results on CLEF-IP 2010 English abstracts, with ranges for a 95% confidenceinterval.
Bold figures indicate the best results obtained with the five classifiers.
(P: Precision;R: Recall, F1: F1-score).P R F1weighted random guessing 6.09% ?
0.14 6.04% ?
0.14 6.06% ?
0.14unigrams 76.27% ?
0.26 66.13% ?
0.28 70.84% ?
0.27unigrams + bigrams 79.00% ?
0.24 70.19% ?
0.27 74.34% ?
0.26unigrams + Stanford triples 78.35% ?
0.25 69.57% ?
0.28 73.70% ?
0.26unigrams + AEGIR triples 78.51% ?
0.25 69.18% ?
0.28 73.55% ?
0.26all representations 79.51% ?
0.24 71.11% ?
0.27 75.08% ?
0.26selected as features during LTS.
This means that the phrases replace at least a part of theinformation contained in the possible unigrams.4.
Results and Discussion4.1 Classification AccuracyTable 4 shows the micro-averages of Precision, Recall, and F1 for five classification ex-periments with different document representations.
To give an idea of the complexity ofthe task we have included a random guessing baseline in the first row.23 We found thatextending a unigram representation with statistical and/or linguistic phrases gives asignificant improvement in classification accuracy over the unigram baseline.
The best-performing classifier is the one that combines all four text representations.
When addingonly type of phrase to unigrams, the unigrams + bigrams combination is significantlybetter than the combinations with syntactic phrases.
Combining all four representationsboosts recall, but has less impact on precision.23 The script used to calculate the baseline can be downloaded at http://lands.let.ru.nl/~dhondt/.We used a weighted randomization that takes the category label distributions and label frequencydistributions into account.766D?hondt et alText Representations for Patent ClassificationTable 5Penetration of the bigrams and triples in the B60 class profiles (in % of terms at given rank).rnk10 rnk20 rnk50 rnk100 rnk1000bigrams 3.0 4.0 48.0 45.0 70.5stanford 0.0 1.0 24.0 26.0 48.0AEGIR 0.0 0.5 20.0 25.0 44.9all representationsbigrams 2.0 2.0 34.0 36.0 43.2stanford 0.0 0.0 4.0 6.0 13.0AEGIR 0.0 0.0 2.0 4.0 18.0The results are similar to O?zgu?r and Gu?ngo?r?s (2012) findings for scientificabstracts: Adding phrases to unigrams can significantly improve classification.
The textin the patent corpus is vastly different from the newswire text in the Reuters corpus.Like scientific abstracts, patents are full of jargon and terminology, often expressed inmulti-word units, which might favor phrasal representations.
Moreover, the innovativeconcepts in a patent are sometimes described in generalized terms combined with somespecifier (to ensure larger legal scope).
For example, a hose might be referred to as awatering device.
The term hose can be captured with a unigram representation, but themulti-word expression cannot.
The difference with the results on the Reuters-21578data set (discussed in Section 2.1.1), however, may not completely be due to genredifferences: Bekkerman and Allan (2003) remark that the unigram baseline for theReuters-21578 task is difficult to improve upon, because in that data set a few keywordsare enough to distinguish between the categories.4.2 Unigram versus PhrasesIn this section we investigate whether adding phrases suppresses, complements, orchanges unigram selection.
To examine the impact of phrases in the classificationprocess, we analyzed the class profiles24 of two large classes (H04 ?
Electric Communica-tion Technique; and H01 ?
Basic electric elements) that show significant improvementsin both Precision and Recall25 for the bigram classifier compared with the unigrambaseline.
We look at (1) the overlap of the single words in the class profiles of theunigram and combined representations; and (2) the overlap of the single words and thewords that make up the phrases (hereafter referred to as parts) within the class profileof one text representation.4.2.1 Overlap of Unigrams.
The class profiles in the baseline unigram classifier containedfar fewer terms ( < 20%) than the profiles in the classifiers that combine unigrams andphrases.
This could be expected from the data in tables 2 and 3.Unigrams are the highest ranked26 features in the combined representation classprofiles (see Table 5).
Furthermore, words that are important terms for unigram clas-sification also rank high in the combined class profiles: On average, there is an 80%24 A class profile is the model built by the LCS classifier for a class during training.
It consists of a rankedlist of terms that contribute most to distinguishing members from a class from all other classes.25 H04: P: + 3.09%; R: + 1.83%; H01: P: + 3.61%; R: + 5.14%.26 The rank of a term is based on the decreasing order of mass assigned to that term in the class profile.
(See Section 4.2.2.
)767Computational Linguistics Volume 39, Number 3overlap of the top 1,000 most important words in unigram and combined representationclass profiles.
This decreases to 75% when looking at the 5,000 most important words.This shows that the classifier tends to select mostly the same words as important termsfor the different text representation combinations.
The relative ranking of the words isvery similar in the class profiles of all the text representations.
Thus, adding phrasesto unigrams does not result in replacing the most important unigrams for a particularclass and the improvements in classification accuracy must derive from the additionalinformation in the selected phrases.4.2.2 Overlap of Single Words and Parts of Bigrams.
Like Caropreso, Matwin, and Sebastiani(2001), we investigated to what extent the parts of the high-ranked phrases overlap withwords in the unigrams + bigrams class profile.
We first looked at the lexical overlap ofthe words and the parts of the bigrams in the H01 unigrams + bigrams class profile.Interestingly, we found a relatively low overlap between the words and the parts ofthe phrases: For the 20 most important bigrams, only 11 of the 32 unique parts of thebigrams overlap with the 100 most important single word terms; in the complete classprofile only 56% of the 10,387 parts of the bigrams overlap with the 9,064 words inthe class profile.
This means that a large part of the bigrams contains complementaryinformation not present in the unigrams in the class profile.To gain a deeper insight into the relationship between the bigrams and their parts,we also looked at the mass of the different terms in the class profiles.
The mass of aterm for a certain class is the product of its TF?IDF score and its Winnow weight forthat class; ?mass?
provides an estimate of how much a term contributes to the score ofdocuments for a particular class.
We can divide the terms into three main categories:(a) mass(partA) ?
mass(partB) ?
mass(bigram);(b) mass(partA) ?
mass(bigram) > mass(partB);(c) mass(bigram) > mass(partA) ?
mass(partB).We note that 50% of the top 1,000 highest ranked bigrams fall within category (b) andtypically consist of one part with high mass accompanied by a part with a low mass,which can be a function word (for example a transmitter), or a general term (for example,device in optical device).
The highest ranked bigrams can be found in category (a) wheretwo highly informative words are combined to form very specific concepts, for example,fuel cell.
These are specifications of a more general concept that is typical for that class inthe corpus.
The bigrams in this category are similar to those investigated by Caropreso,Matwin, and Sebastiani (2001) and Tan, Wang, and Lee (2002).
Though highly ranked,they only make up a small subset (22%) of the important bigram features.The bigrams in category (c) (27%) are typically made up from low-ranked singlewords, such as mobile station.
Interestingly, most bigram parts in this subset do not occuras word terms in the unigram and bigram class profiles, but occur in the negative classprofiles (a selection of terms that are considered to describe anything but that particularclass).
The complementary information of bigram phrases (compared to unigrams) iscontained in this set of bigrams.4.3 Statistical versus Linguistic PhrasesResults in Section 4.1 indicate that bigrams are most important additional features, butthe experiment combining all four representations showed that dependency triples do768D?hondt et alText Representations for Patent Classificationcomplement bigrams.
In this section we examine what information is captured by thedifferent phrases and how this accounts for the differences in classification accuracy.4.3.1 Class Profile Analysis.
We first examined the differences between the statisticalphrases and the two types of linguistic phrases to discover what information containedin the bigrams leads to better classification results.
We performed our analysis on thedifferent class profiles of B60 (?Vehicles in general?
), a medium-sized class, which mostclearly shows the advantage of the bigram classifier compared to the classifiers withlinguistic phrases.27All four class profiles with phrases contain roughly the same set of unigrams(between 78% to 91% overlap) that occur quite high in the corresponding unigram classprofile.
The AEGIR class profile contains 10% more unigrams than the other combinedrepresentation class profiles; these are mainly words that appear in the negative classprofile of the corresponding unigram classifier.
As in class H01, the relative position ofthe words remains the same.
The absolute position of the words in the list, however,does change: Caropreso, Matwin, and Sebastiani (2001) introduced a measure for theeffectiveness of phrases as terms, called the penetration, that is, the percentage ofphrases in the top k terms when classifying with both words and phrases.Comparing the penetration levels at the various ranks for the different classifiers,we can see that the classification results correspond with the tendency of a classifier toselect phrases in the top k terms.
Interestingly, we see a large disparity in the phrasalfeatures that are selected by the combination classifier.
The preference for bigramsis mirrored by the penetration levels of the unigrams + bigrams classifier which hasselected more bigrams at higher ranks in the class profile than the classifiers with thelinguistic phrases.
This is in line with the findings of Caropreso, Matwin, and Sebastiani(2001) that penetration levels are a reasonable way to compute the contribution ofn-grams to the quality of a feature set.
On average, the linguistic phrases have muchsmaller weight in the class profiles than the bigrams and, consequently, are likelyto have a smaller impact during the classification process.
For the combination run,however, it seems that a long tail of small-impact features does improve classificationaccuracy.Linguistic analysis of the top 100 phrases in the profiles of class B60 shows thatall classifiers select similar types of phrases.
We manually annotated the bigrams withthe correct syntactic dependencies (in the Stanford collapsed typed dependency format)and compared these with the syntactic relations expressed in the linguistic phrases.
Theresults are summarized in Table 6.It appears that noun phrases and compounds such as circuit board and electricdevice are by far the most important terms in the class profiles.
Interestingly, phrasesthat contain a determiner relation (e.g., the device) are deemed equally important inall four different class profiles.
It is unlikely that this is a semantic effect, that is, thatthe determiner relation provides additional semantic information to the nouns in thephrases, but rather it seems an artefact of the abundance of noun phrases which occurin patent texts.
We also looked into the lexical overlap between the parts of the differenttypes of phrases.
We found that the selected phrases encode almost exactly the sameinformation in all three representations: There is an 80% overlap between the parts of27 Precision is 77.34% for unigrams+bigrams, 75.67% for unigrams+Stanford, 73.47% for unigrams+AEGIR,and 77.38% for unigrams+bigrams+Stanford+AEGIR.
The Recall scores are essentially equal for all three,that is, 68.81%, 68.38%, 69.7%, and 70.18%, respectively.769Computational Linguistics Volume 39, Number 3Table 6Distribution of the top 100 statistical and syntactic phrases in the B60 class profiles.grammatical relation bigrams stanford AEGIR combinationnoun?noun compounds 41 486228 4428adjectival modifier 11 8determiner 34 28 27 41subject 6 4 6 9prepositions 2 4 1 2<other> 7 8 4 4the top 100 most important phrases.
This decreases only to 75% when looking at the10,000 most important phrases.Given that the class profiles select the same set of words and contain phrases witha high lexical overlap, therefore, how do we explain the marked differences in classifi-cation accuracy between the three different representations?
These must stem from thedifferent combinations of the words in the phrasal features.
To examine in detail how thefeatures created through the different text represenations differ, we conducted a featurequality assessment experiment against a manually created reference set.4.3.2 Human Quality Assessment Experiment.
To gain more insight in the syntactic andsemantic relations that are considered most informative by humans, we conductedan experiment in which we asked human annotators to select the five to ten mostinformative phrases29 for 15 sentences taken at random from documents in the threelargest classes in the corpus.
We then compiled a reference set consisting of 70 phrases(4.6 phrases per sentence) which were considered as ?informative?
by at least threeout of four annotators.
Of these, 57 phrases were noun?noun compounds and 11 werecombinations of an adjectival modifier with a noun.
None of the annotators selectedphrases containing determiners.We created bigrams from the input and extracted head?modifier pairs30 from theparser output for the sentences in the test set.
We then compared the overlap of thegenerated phrases with the reference phrases.
We found that bigrams overlap with53 of the 70 reference phrases; Stanford triples overlap with 62 phrases and AEGIRtriples overlap with 57 phrases.
Although three data points are not enough to computea formal measure, it is interesting to note the correspondence with the number of termskept for the three text representations after Local Term Selection (see Table 3).
The factthat the text representation with the smallest number of terms after LTC and with thesmallest overlap with ?contentful?
phrases in a text as indicated by human annotatorsstill yields the best classification performance suggests that not all ?contentful?
phrasesare important or useful for the task of classifying that text.
This finding is reminiscent ofthe fact that the ?optimal?
summary of a text is dependent on the goal with which thesummary was produced (Nenkova and McKeown 2011).Only 15% of the phrases extracted by the human annotators contain word combi-nations that have long-distance dependencies in the original sentences.
This suggests28 As mentioned in Section 3.2.3 the AEGIR parser uses a more condensed dependency output format.The Stanford?s nn and amod are collapsed into the attributive (ATTR) relation.29 ?Phrase?
was defined as a combination of two words that both occur in the sentence, irrespective ofthe order in which they occur in the sentence.30 Head?modifier pairs are syntactic triples that are stripped of their grammatical relations.770D?hondt et alText Representations for Patent Classificationthat the most meaningful phrases are expressed in local dependencies, that is, adjacentwords.
Consequently, syntactic analysis aimed at discovering meaning expressed bylong-distance dependencies can only make a small contribution.
A further analysis ofthe phrases showed that the smaller coverage of the bigrams is due to the fact that someof the relevant noun?noun combinations are missed because function words, typicallydeterminers or prepositions, occur between the nouns.
For example, the annotatorsconstructed the reference phrase rotation axis for the noun phrase the rotation of thesecond axis.
This reference phrase cannot be captured by the bigram representation.When intervening function words are removed from the sentences, the coverage ofthe resulting bigrams on the reference set rises31 to 59 phrases (more than AEGIR, andalmost as many as Stanford).
Despite the fact that generating more phrases does notnecessarily lead to better classification performance, we intend to use bigrams strippedof function words as additional terms for patent classification in future experiments.The analysis also revealed an indication why syntactic phrases may lead to inferiorclassification results: Both syntactic parsers consistently fail to find the correct structuralanalysis of the long and complex noun phrases such as an implantable, inflatable dualchamber shape retention tissue expander, which are frequent in patent texts.
Phrases likethis contain many compounds in an otherwise complex syntactic structure, namely[an [implantable, inflatable [[dual chamber] [shape retention] [tissue expander]]]].For a parser it is impossible to parse this correctly without knowing which word se-quences are actually compounds.
That knowledge might be gleaned from the frequencywith which sequences of nouns and adjectives occur in a given domain.
For the timebeing, the Stanford parser (and the AEGIR parser, to a lesser extent) will parse anynoun phrase by attaching the individual words to the right-most head noun, resultingin the following analysis:[an [implantable, [inflatable [dual [chamber [shape [retention [tissue expander]]]]]]]].This effectively destroys many of the noun?noun compounds, which are the mostimportant features for patent classification (see Table 6).
Bigrams are less prone to thistype of ?error.
?These findings are confirmed when looking at the overlap of the word combi-nations: Although there is high lexical overlap between the phrases of the differentrepresentations (80% overlap of the parts of phrases in Section 4.3.1), the overlap ofthe word combinations that make up the phrases is much lower: Only 33% of the top1,000 phrases are common between all three representations.4.4 Stanford versus AEGIR TriplesThe performance with the unigrams + Stanford triples is not significantly different fromthe combination with AEGIR triples.
Because the AEGIR triples are slightly less sparse(see Table 1), we expected that these would have an advantage over Stanford triples.Most of the normalization processes that make the AEGIR triples less sparse concernsyntactic variation on the clause level, however.
But as was shown in Section 4.3,31 This result is language-dependent: English has a fairly rigid phrase-internal word order but for a moresynthetic language with a more variable word order, like Russian, bigram coverage might suffer fromthe variation in the surface form.771Computational Linguistics Volume 39, Number 3Table 7Classification results on CLEF-IP 2010 French and German abstracts, with ranges for95% confidence intervals.P R F1French unigrams 70.65% ?
0.68 61.40% ?
0.73 65.70% ?
0.70unigrams + bigrams 72.31% ?
0.67 62.58% ?
0.72 67.09% ?
0.69German unigrams 76.44% ?
0.34 65.82% ?
0.38 70.73% ?
0.37unigrams + bigrams 76.39% ?
0.34 65.41% ?
0.38 70.47% ?
0.37the most important terms for classification in the patent domain are found in thenoun phrase, where Stanford and AEGIR perform similar syntactic analyses.
AlthoughStanford?s dependency scheme is more detailed (see Table 6), the noun-phrase internaldependencies in the Stanford parser map practically one-to-one onto AEGIR?s set ofrelators, resulting in very similar dependency triple features for classification.
Con-sequently, there is no normalization gain in using the AEGIR dependency format todescribe the internal structure of the noun phrases.4.5 Comparison with French and German Patent ClassificationWe found that phrases contribute to improving classification on English patent ab-stracts.
The improvement might be language-dependent, however, because compoundsare treated differently in different languages.
A compounding language like Germanmight benefit less from using phrases than English.
To estimate the generalizability ofour findings, we conducted additional experiments in which we compared the impactof adding bigrams to unigrams for both French and German.Using the same methods described in sections 3.1 and 3.2, we extracted and pro-cessed all French and German abstracts from the CLEF-IP 2010 corpus, resulting in twonew data sets that contained 86,464 and 294,482 documents, respectively (Table 7).
Bothdata sets contained the same set of 121 labels and had label distributions similar to theEnglish data set.
The sentencing script was updated with the most common French andGerman abbreviations to minimize incorrect sentence splitting.
The resulting sentenceswere then tagged using the French and German versions of the TreeTagger.32 From thetagged output, we extracted the lemmas and used these to construct unigrams andbigrams for both languages.
We ran the experiments with the LCS using the settingsreported in Section 3.3.The results show a much smaller but still significant improvement for using bigramswhen classifying French patent abstracts and even a deterioration for German.
Due tothe difference in size between the English and French data set it is difficult to draw hardconclusions on which language benefits most from adding bigrams.
It is clear, however,that our findings are not generalizable to German (and probably other compoundinglanguages).5.
ConclusionIn this article we have examined the usefulness of statistical and linguistic phrasesfor patent classification.
Similar to O?zgu?r and Gu?ngo?r?s (2010) results for scientific32 http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/.772D?hondt et alText Representations for Patent Classificationabstracts, we found that adding phrases to unigrams significantly improves classifi-cation results for English.
Of the three types of phrases examined in this article, bigramshave the most impact, both in the experiment that combined all four text representa-tions, and in combination with unigrams only.The abundance of compounds in the terminology-rich language of the patent do-main results in a relatively high importance for the phrases.
The top phrases acrossthe different representations were mostly noun?noun compounds (for example wateringdevice), followed by phrases containing a determiner relation (for example the module)and adjective modifier phrases (for example separate module).The information in the phrases and unigrams overlaps to a large extent: Most of thephrases consist of words that are important unigram features in the combined profileand that also appear in the corresponding unigram class profile.
When examining theH01 class profiles, however, we found that 27% of the selected phrases contain wordsthat were not selected in the unigram profile (see Section 4.2.2).When comparing the impact of features created from the output of the aboutness-based AEGIR parser with those from the Stanford parser, we found the latter resulted inslightly (but not significantly) better classification results.
AEGIR?s normalization fea-tures are not advantageous (compared with Stanford) in creating noun-phrase internaltriples, which are the most informative features for patent classification.The parsers were not specifically trained for the patent domain and both experi-enced problems with long, complex noun phrases consisting of sequences of wordsthat can function as adjective/adverb or noun and that are not interrupted by functionwords that clarify the syntactic structure.
The right-headed bias of both syntactic parserscaused problems in analyzing those constructions, yielding erroneous and variabledata.
As a consequence, parsers may miss potentially relevant noun?noun compoundsand noun phrases with adjectival modifiers.
Because of the highly idiosyncratic natureof the terminology used in the patent domain, it is not evident whether this prob-lem can be solved by giving a parser access to information about the frequency withwhich specific noun?noun, adjective?noun, and adjective/adverb?adjective pairs occurin technical texts.
Bigrams, on the other hand, are less variable (as seen in Table 1) andtherefore yield better classification results.
This is the more important point because thedependency relations marked as important for understanding a sentence by the humanannotators consist mainly of pairs of adjacent words.We also performed additional experiments to examine the generalizability of ourfindings for French and German: As could be expected, compounding languages likeGerman which express complex concepts in ?one word?
do not gain from usingbigrams.In line with Bekkerman and Allan (2003) we can conclude that with the large quan-tities of text available today, the role of phrases as features in text classification mustbe reconsidered.
For the automated classification of English patents at least, addingphrases and more specifically bigrams significantly improves classification accuracy.ReferencesApte?, Chidanand, Fred Damerau, andSholom Weiss.
1994.
Automated learningof decision rules for text categorization.ACM Transactions on Information Systems,12(3):233?251.Bekkerman, Ron and John Allan.
2003.
Usingbigrams in text categorization.
TechnicalReport IR-408, Center of IntelligentInformation Retrieval, University ofMassachusetts, Amherst.Beney, Jean.
2010.
LCI-INSA linguisticexperiment for CLEF-IP classificationtrack.
In Proceedings of the Conference onMultilingual and Multimodal InformationAccess Evaluation (CLEF 2010), Padua.773Computational Linguistics Volume 39, Number 3Benzineb, Karim and Jacques Guyot.
2011.Automated patent classification.
InMihai Lupu, Katja Mayer, John Tait, andAnthony J. Trippe, editors, CurrentChallenges in Patent Information Retrieval,volume 29.
Springer, New York,pages 239?261.Braga, Igor, Maria Monard, and EdsonMatsubara.
2009.
Combining unigramsand bigrams in semi-supervised textclassification.
In Proceedings of Progressin Artificial Intelligence, 14th PortugueseConference on Artificial Intelligence(EPIA 2009), pages 489?500, Aveiro.Caropreso, Maria Fernanda, Stan Matwin,and Fabrizio Sebastiani.
2001.
Alearner-independent evaluation of theusefulness of statistical phrases forautomated text categorization.
InA.
G. Chin, editor, Text Databases &Document Management.
IGI Publishing,Hershey, PA, pages 78?102.Crawford, Elisabeth, Irena Koprinska,and Jon Patrick.
2004.
Phrases andfeature selection in e-mail classification.In Proceedings of the 9th AustralasianDocument Computing Symposium (ADCS),pages 59?62, Melbourne.Dagan, Ido, Yael Karov, and Dan Roth.1997.
Mistake-driven learning in textcategorization.
In Proceedings of 2ndConference on Empirical Methods in NLP,pages 55?63, Providence, RI.de Marneffe, Marie-Catherine andChristopher Manning.
2008.
The StanfordTyped Dependencies representation.
InColing 2008: Proceedings of the Workshop onCross-Framework and Cross-Domain ParserEvaluation, pages 1?8, Manchester.Derieux, Franck, Mihaela Bobeica, DelphinePois, and Jean-Pierre Raysz.
2010.Combining semantics and statistics forpatent classification.
In Proceedings of theConference on Multilingual and MultimodalInformation Access Evaluation (CLEF 2010),Padua.Dumais, Susan, John Platt, DavidHeckerman, and Mehran Sahami.
1998.Inductive learning algorithms andrepresentations for text categorization.In Proceedings of the Seventh InternationalConference on Information and KnowledgeManagement (CIKM ?98), pages 148?155,Bethesda.Fall, Caspar J. and Karim Benzineb.
2002.Literature survey: Issues to be consideredin the automatic classification of patents.Technical report, World IntellectualProperty Organization, Geneva.Fall, Caspar J., Atilla To?rcsva?ri, KarimBenzineb, and Gabor Karetka.
2003.Automated categorization in theinternational patent classification.ACM SIGIR Forum, 37(1):10?25.Fu?rnkranz, Johannes.
1998.
A study usingn-gram features for text categorization.Technical Report OEFAI-TR-98-30,Austrian Research Institute forArtificial Intelligence, Vienna.Fu?rnkranz, Johannes.
1999.
Exploitingstructural information for textclassification on the WWW.
In Proceedingsof Advances in Intelligent Data Analysis(IDA-99), pages 487?497, Amsterdam.Galavotti, Luigi, Fabrizio Sebastiani, andMaria Simi.
2000.
Experiments on theuse of feature selection and negativeevidence in automated text categorization.In Proceedings of Research and AdvancedTechnology for Digital Libraries,4th European Conference, pages 59?68,Lisbon.Guyot, Jacques, Karim Benzineb, and GillesFalquet.
2010.
Myclass: A mature tool forpatent classification.
In Proceedings of theConference on Multilingual and MultimodalInformation Access Evaluation (CLEF 2010),Padua.Held, Pierre, Irene Schellner, and RyuichiOta.
2011.
Understanding the world?smajor patent classification schemes.
Paperpresented at the PIUG 2011 AnnualConference Workshop, Vienna, 13 April.Joachims, Thorsten.
1999.
Making large-scalesupport vector machine learning practical.In Bernhard Scho?lkopf, Christopher J. C.Burges, and Alexander J. Smola, editors,Advances in Kernel Methods.
MIT Press,Cambridge, MA, pages 169?184.Koster, Cornelis, Jean Beney, Suzan Verberne,and Merijn Vogel.
2011.
Phrase-baseddocument categorization.
In Mihai Lupu,Katja Mayer, John Tait, and Anthony J.Trippe, editors, Current Challenges in PatentInformation Retrieval, volume 29.
Springer,New York, pages 263?286.Koster, Cornelis, Marc Seutter, andJean Beney.
2001.
Classifying patentapplications with winnow.
In ProceedingsBenelearn 2001. pages 19?26, Antwerpen.Koster, Cornelis, Marc Seutter, andJean Beney.
2003.
Multi-classificationof patent applications with winnow.In Manfred Broy and Alexandre V.Zamulin, editors, Perspectives ofSystems Informatics: 5th InternationalAndrei Ershov Memorial Conference,volume 2890 of Lecture Notes in774D?hondt et alText Representations for Patent ClassificationComputer Science.
Springer, New York,pages 546?555.Koster, Cornelis and Mark Seutter.
2003.Taming wild phrases.
In Proceedingsof the 25th European conference onIR research (ECIR?03), pages 161?176, Pisa.Krier, Marc and Francesco Zacca`.
2002.Automatic categorization applications atthe European patent office.
World PatentInformation, 24(3):187?196.Larkey, Leah.
1998.
Some issues in theautomatic classification of U.S. patents.In Working Notes of the Workshop onLearning for Text Categorization, 15thNational Conference on AI, pages 87?90,Madison, WI.Larkey, Leah S. 1999.
A patent search andclassification system.
In Proceedings of theFourth ACM Conference on Digital Libraries(DL?99), pages 179?187, Berkeley.Lewis, David D. 1992.
An evaluation ofphrasal and clustered representations on atext categorization task.
In Proceedings ofthe 15th Annual International ACM SIGIRConference on Research and Developmentin Information Retrieval (SIGIR ?92),pages 37?50, Copenhagen.Mille, Simon and Leo Wanner.
2008.
Makingtext resources accessible to the reader: Thecase of patent claims.
In Proceedings of the6th International Conference on LanguageResources and Evaluation (LREC?08),Marrakech.Mitra, Mandar, Chris Buckley, Amit Singhal,and Claire Cardie.
1997.
An analysisof statistical and syntactic phrases.In Proceedings of RIAO?97 Computer-AssistedInformation Searching on Internet,pages 200?214, Montreal.Mladenic, Dunja and Marko Grobelnik.1998.
Word Sequences as Features inText-Learning.
In Proceedings of the 17thElectrotechnical and Computer ScienceConference (ERK98), pages 145?148,Ljubljana.Moschitti, Alessandro and Roberto Basili.2004.
Complex linguistic features fortext classification: A comprehensivestudy.
In Sharon McDonald andJohn Tait, editors, Advances in InformationRetrieval, volume 2997 of Lecture Notes inComputer Science.
Springer, New York,pages 181?196.Nastase, Vivi, Jelber Sayyad, andMaria Fernanda Caropreso.
2007.Using dependency relations for textclassification.
Technical Report TR-2007-12,University of Ottawa.Nenkova, Ani and Kathleen McKeown.
2011.Automatic summarization.
Foundationsand Trends in Information Retrieval,5(2?3):103?233.Ozgu?r, Levent and Tunga Gu?ngo?r.2009.
Analysis of stemming alternativesand dependency pattern support in textclassification.
In Proceedings of TenthInternational Conference on IntelligentText Processing and ComputationalLinguistics (CICLing 2009),pages 195?206, Mexico City.O?zgu?r, Levent and Tunga Gu?ngo?r.2010.
Text classification with the support ofpruned dependency patterns.
PatternRecognition Letters, 31(12):1598?1607.O?zgu?r, Levent and Tunga Gu?ngo?r.2012.
Optimization of dependency andpruning usage in text classification.Pattern Analysis and Applications,15(1):45?58.Parapatics, Peter and Michael Dittenbach.2009.
Patent claim decomposition forimproved information extraction.
InProceedings of the 2nd InternationalWorkshop on Patent Information Retrieval(PAIR?09), pages 33?36, Hong Kong.Salton, Gerard and Christopher Buckley.1988.
Term-weighting approaches inautomatic text retrieval.
InformationProcessing Management, 24(5):513?523.Scott, Sam and Stan Matwin.
1999.
Featureengineering for text classification.In Proceedings of the Sixteenth InternationalConference on Machine Learning (ICML ?99),pages 379?388, Bled.Smith, Harold.
2002.
Automation of patentclassification.
World Patent Information,24(4):269?271.Tan, Chade-Meng, Yuan-Fang Wang,and Chan-Do Lee.
2002.
The use ofbigrams to enhance text categorization.Information Processing and Management,38(4):529?546.Verberne, Suzan and Eva D?hondt.
2011.Patent classification experiments withthe Linguistic Classification SystemLCS in CLEF-IP 2011.
In Proceedings of theConference on Multilingual and MultimodalInformation Access Evaluation (CLEF 2011),Amsterdam.Verberne, Suzan, Merijn Vogel, andEva D?hondt.
2010.
Patent classificationexperiments with the LinguisticClassification System LCS.
In Proceedingsof the Conference on Multilingual andMultimodal Information Access Evaluation(CLEF 2010), Padua.775
