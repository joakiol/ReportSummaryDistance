Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 487?496,Dublin, Ireland, August 23-24, 2014.RTM-DCU: Referential Translation Machines for Semantic SimilarityErgun Bic?iciCentre for Global Intelligent ContentSchool of ComputingDublin City University, Dublin, Ireland.ebicici@computing.dcu.ieAndy WayCentre for Global Intelligent ContentSchool of Computing,Dublin City University, Dublin, Ireland.away@computing.dcu.ieAbstractWe use referential translation machines(RTMs) for predicting the semantic simi-larity of text.
RTMs are a computationalmodel for identifying the translation actsbetween any two data sets with respectto interpretants selected in the same do-main, which are effective when makingmonolingual and bilingual similarity judg-ments.
RTMs judge the quality or the se-mantic similarity of text by using retrievedrelevant training data as interpretants forreaching shared semantics.
We derive fea-tures measuring the closeness of the testsentences to the training data via inter-pretants, the difficulty of translating them,and the presence of the acts of transla-tion, which may ubiquitously be observedin communication.
RTMs provide a lan-guage independent approach to all simi-larity tasks and achieve top performancewhen predicting monolingual cross-levelsemantic similarity (Task 3) and good re-sults in semantic relatedness and entail-ment (Task 1) and multilingual semantictextual similarity (STS) (Task 10).
RTMsremove the need to access any task or do-main specific information or resource.1 Semantic Similarity JudgmentsWe introduce a fully automated judge for seman-tic similarity that performs well in three seman-tic similarity tasks at SemEval-2014, SemanticEvaluation Exercises - International Workshop onSemantic Evaluation (Nakov and Zesch, 2014).RTMs provide a language independent solution forthe semantic textual similarity (STS) task (Task10) (Agirre et al., 2014), achieve top perfor-mance when predicting monolingual cross-levelsemantic similarity (Task 3) (Jurgens et al., 2014),and achieve good results in the semantic related-ness and entailment task (Task 1) (Marelli et al.,2014a).Referential translation machine (Section 2) isa computational model for identifying the acts oftranslation for translating between any given twodata sets with respect to a reference corpus se-lected in the same domain.
An RTM model isbased on the selection of interpretants, trainingdata close to both the training set and the test set,which allow shared semantics by providing con-text for similarity judgments.
In semiotics, an in-terpretant I interprets the signs used to refer to thereal objects (Bic?ici, 2008).
Each RTM model isa data translation and translation prediction modelbetween the instances in the training set and thetest set and translation acts are indicators of thedata transformation and translation.
RTMs presentan accurate and language independent solution formaking semantic similarity judgments.We describe the tasks we participated below.Section 2 describes the RTM model and the fea-tures used.
Section 3 presents the training and testresults we obtain on the three tasks we competedand the last section concludes.Task 1 Evaluation of Compositional Distribu-tional Semantic Models on Full Sentencesthrough Semantic Relatedness and Entail-ment (SRE) (Marelli et al., 2014a):Given two sentences, produce a related-ness score indicating the extent to whichthe sentences express a related meaning: anumber in the range [1, 5].We model the problem as a translation perfor-mance prediction task where one possible inter-pretation is obtained by translating S1(the sourceto translate, S) to S2(the target translation, T).Since linguistic processing can reveal deeper sim-ilarity relationships, we also look at the translationtask at different granularities of information: plain487text (R for regular) and after lemmatization (L).We lowercase all text.Task 3 Cross-Level Semantic Similarity(CLSS) (Jurgens et al., 2014):Given two text from different levels, pro-duce a semantic similarity rating: a num-ber in the range [0, 4].CLSS task targets semantic similarity compar-isons between text having different levels of gran-ularity and we address the following level cross-ings: paragraph to sentence, sentence to phrase,and phrase to word.
We model the problem asa translation performance prediction task amongtext from different levels.Task 10 Multilingual Semantic Textual Similarity(MSTS) (Agirre et al., 2014)Given two sentences S1and S2in the samelanguage, quantify the degree of similar-ity: a number in the range [0, 5].MSTS task addresses the problem in Englishand Spanish (score range is [0, 4]).
We model theproblem as a translation performance predictiontask between S1and S2.2 Referential Translation Machine(RTM)Referential translation machines provide a compu-tational model for quality and semantic similarityjudgments in monolingual and bilingual settingsusing retrieval of relevant training data (Bic?ici,2011; Bic?ici and Yuret, 2014) as interpretants forreaching shared semantics (Bic?ici, 2008).
RTMsare a language independent approach and achievetop performance when predicting the quality oftranslations (Bic?ici, 2013; Bic?ici and Way, 2014)and when predicting monolingual cross-level se-mantic similarity (Jurgens et al., 2014), and goodperformance when evaluating the semantic relat-edness of sentences and their entailment (Marelliet al., 2014a), as an automated student answergrader (Bic?ici and van Genabith, 2013b), andwhen judging the semantic similarity of sen-tences (Bic?ici and van Genabith, 2013a; Agirre etal., 2014).
We improve the RTM models by:?
using a parameterized, fast implementationof FDA, FDA5, and our Parallel FDA5 in-stance selection model (Bic?ici et al., 2014),?
better modeling of the language in whichAlgorithm 1: Referential Translation MachineInput: Training set train, test set test,corpus C, and learning model M .Data: Features of train and test, Ftrainand Ftest.Output: Predictions of similarity scores onthe test q?.1 FDA5(train,test, C)?
I2 MTPP(I,train)?
Ftrain3 MTPP(I,test)?
Ftest4 learn(M,Ftrain)?M5 predict(M,Ftest)?
q?similarity judgments are made with improvedoptimization and selection of the LM data,?
using a general domain corpus to select inter-pretants from,?
increased feature set for also modeling thestructural properties of sentences,?
extended learning models.We use the Parallel FDA5 (Feature Decay Algo-rithms) instance selection model for selecting theinterpretants (Bic?ici et al., 2014; Bic?ici and Yuret,2014) this year, which allows efficient parameteri-zation, optimization, and implementation of FDA,and build an MTPP model (Section 2.1).
We viewthat acts of translation are ubiquitously used dur-ing communication:Every act of communication is an act oftranslation (Bliss, 2012).Translation need not be between different lan-guages and paraphrasing or communication alsocontain acts of translation.
When creating sen-tences, we use our background knowledge andtranslate information content according to the cur-rent context.The inputs to the RTM algorithm Algorithm 1are a training set train, a test set test, somecorpus C, preferably in the same domain as thetraining and test sets, and a learning model.
Step 1selects the interpretants, I, relevant to both thetraining and test data.
Steps 2 and 3 use I to maptrain and test to a new space where similari-ties between translation acts can be derived moreeasily.
Step 4 trains a learning model M over thetraining features, Ftrain, and Step 5 obtains thepredictions.
Figure 1 depicts the RTM.488Figure 1: RTM depiction.Our encouraging results in the semantic simi-larity tasks increase our understanding of the actsof translation we ubiquitously use when commu-nicating and how they can be used to predict thesemantic similarity of text.
RTM and MTPP mod-els are not data or language specific and their mod-eling power and good performance are applicablein different domains and tasks.
RTM expands theapplicability of MTPP by making it feasible whenmaking monolingual quality and similarity judg-ments and it enhances the computational scalabil-ity by building models over smaller and more rel-evant set of interpretants.2.1 The Machine Translation PerformancePredictor (MTPP)MTPP (Bic?ici et al., 2013) is a state-of-the-artand top performing machine translation perfor-mance predictor, which uses machine learningmodels over features measuring how well the testset matches the training set to predict the qualityof a translation without using a reference trans-lation.
MTPP measures the coverage of individ-ual test sentence features found in the training setand derives indicators of the closeness of test sen-tences to the available training data, the difficultyof translating the sentence, and the presence ofacts of translation for data transformation.2.2 MTPP Features for Translation ActsMTPP feature functions use statistics involvingthe training set and the test sentences to deter-mine their closeness.
Since they are languageindependent, MTPP allows quality estimation tobe performed extrinsically.
MTPP uses n-gramfeatures defined over text or common cover link(CCL) (Seginer, 2007) structures as the basic unitsof information over which similarity calculationsare made.
Unsupervised parsing with CCL ex-tracts links from base words to head words, rep-resenting the grammatical information instantiatedin the training and test data.We extend the MTPP model we used lastyear (Bic?ici, 2013) in its learning module and thefeatures included.
Categories for the features (Sfor source, T for target) used are listed belowwhere the number of features are given in bracketsfor S and T, {#S, #T}, and the detailed descriptionsfor some of the features are presented in (Bic?ici etal., 2013).
The number of features for each taskdiffers since we perform an initial feature selectionstep on the tree structural features (Section 2.3).The number of features are in the range 337?437.?
Coverage {56, 54}: Measures the degree towhich the test features are found in the train-ing set for both S ({56}) and T ({54}).?
Perplexity {45, 45}: Measures the fluency ofthe sentences according to language models(LM).
We use both forward ({30}) and back-ward ({15}) LM features for S and T.?
TreeF {0, 10-110}: 10 base features and upto 100 selected features of T among parse treestructures (Section 2.3).?
Retrieval Closeness {16, 12}: Measures thedegree to which sentences close to the test setare found in the selected training set, I, usingFDA (Bic?ici and Yuret, 2011a) and BLEU,F1(Bic?ici, 2011), dice, and tf-idf cosine sim-ilarity metrics.?
IBM2 Alignment Features {0, 22}: Calcu-lates the sum of the entropy of the dis-tribution of alignment probabilities for S(?s?S?p log p for p = p(t|s) where s andt are tokens) and T, their average for S andT, the number of entries with p ?
0.2 andp ?
0.01, the entropy of the word align-ment between S and T and its average, andword alignment log probability and its valuein terms of bits per word.
We also com-pute word alignment percentage as in (Ca-margo de Souza et al., 2013) and potentialBLEU, F1, WER, PER scores for S and T.?
IBM1 Translation Probability {4, 12}: Cal-culates the translation probability of testsentences using the selected training set,I (Brown et al., 1993).?
Feature Vector Similarity {8, 8}: Calculatessimilarities between vector representations.489CCLnumB depthB avg depthB R/L avg R/L24.0 9.0 0.375 2.1429 3.40121 111 1311 211 812 1013 113 415 117 15Table 1: Tree features for a parsing output by CCL (immediate non-terminals replaced with NP).?
Entropy {2, 8}: Calculates the distributionalsimilarity of test sentences to the training setover top N retrieved sentences (Bic?ici et al.,2013).?
Length {6, 3}: Calculates the number ofwords and characters for S and T and theiraverage token lengths and their ratios.?
Diversity {3, 3}: Measures the diver-sity of co-occurring features in the trainingset (Bic?ici et al., 2013).?
Synthetic Translation Performance {3, 3}:Calculates translation scores achievable ac-cording to the n-gram coverage.?
Character n-grams {5}: Calculates cosinebetween character n-grams (for n=2,3,4,5,6)obtained for S and T (B?ar et al., 2012).?
Minimum Bayes Retrieval Risk {0, 4}: Cal-culates the translation probability for thetranslation having the minimum Bayes riskamong the retrieved training instances.?
Sentence Translation Performance {0, 3}:Calculates translation scores obtained ac-cording to q(T,R) using BLEU (Papineniet al., 2002), NIST (Doddington, 2002), orF1(Bic?ici and Yuret, 2011b) for q.?
LIX {1, 1}: Calculates the LIX readabilityscore (Wikipedia, 2013; Bj?ornsson, 1968) forS and T.12.3 Bracketing Tree Structural FeaturesWe use the parse tree outputs obtained by CCLto derive features based on the bracketing struc-ture.
We derive 5 statistics based on the geometricproperties of the parse trees: number of bracketsused (numB), depth (depthB), average depth (avg1LIX=AB+ C100A, where A is the number of words, C iswords longer than 6 characters, B is words that start or endwith any of ?.
?, ?
:?, ?!
?, ???
similar to (Hagstr?om, 2012).depthB), number of brackets on the right branchesover the number of brackets on the left (R/L)2, av-erage right to left branching over all internal treenodes (avg R/L).
The ratio of the number of rightto left branches shows the degree to which the sen-tence is right branching or not.
Additionally, wecapture the different types of branching presentin a given parse tree identified by the number ofnodes in each of its children.Table 1 depicts the parsing output obtained byCCL for the following sentence from WSJ233:Many fund managers argue that now ?s the timeto buy .We use Tregex (Levy and Andrew, 2006) for vi-sualizing the output parse trees presented on theleft.
The bracketing structure statistics and fea-tures are given on the right hand side.
The rootnode of each tree structural feature represents thenumber of times that feature is present in the pars-ing output of a document.3 SemEval-14 ResultsWe develop individual RTM models for each taskand subtask that we participate at SemEval-2014with the RTM-DCU team name.
The interpre-tants are selected from the LM corpora distributedby the translation task of WMT14 (Bojar et al.,2014) and the LM corpora provided by LDC forEnglish (Parker et al., 2011) and Spanish (?AngeloMendonc?a, 2011)4.
We use the Stanford POS tag-ger (Toutanova et al., 2003) to obtain the lemma-tized corpora for the SRE task.
For each RTM2For nodes with uneven number of children, the nodes inthe odd child contribute to the right branches.3Wall Street Journal (WSJ) corpus section 23, distributedwith Penn Treebank version 3 (Marcus et al., 1993).4English Gigaword 5th, Spanish Gigaword 3rd edition.490model, we extract the features both on the train-ing set and the test set.
The number of instanceswe select for the interpretants in each task is givenin Table 2.Task Setting Train LMTask 1, SRE English 770 10770Task 3, CLSS Par2S 302 2802Task 3, CLSS S2Phrase 202 2702Task 3, CLSS Phrase2W 102 2602Task 10, MSTS English 504 8002Task 10, MSTS English OnWN 504 8004Task 10, MSTS Spanish 502 8002Table 2: Number of sentences in I (in thousands)selected for each task.We use ridge regression (RR), support vectorregression (SVR) with RBF (radial basis func-tions) kernel (Smola and Sch?olkopf, 2004), andextremely randomized trees (TREE) (Geurts et al.,2006) as the learning models.
TREE is an en-semble learning method over randomized decisiontrees.
These models learn a regression functionusing the features to estimate a numerical targetvalue.
We also use these learning models aftera feature subset selection with recursive featureelimination (RFE) (Guyon et al., 2002) or a di-mensionality reduction and mapping step usingpartial least squares (PLS) (Specia et al., 2009),both of which are described in (Bic?ici et al., 2013).We optimize the learning parameters, the num-ber of features to select, the number of dimen-sions used for PLS, and the parameters for paral-lel FDA5.
More detailed descriptions of the opti-mization processes are given in (Bic?ici et al., 2013;Bic?ici et al., 2014).
We optimize the learning pa-rameters by selecting ?
close to the standard devi-ation of the noise in the training set (Bic?ici, 2013)since the optimal value for ?
is shown to havelinear dependence to the noise level for differentnoise models (Smola et al., 1998).
At testing time,the predictions are bounded to obtain scores in thecorresponding ranges.
We obtain the confidencescores using support vector classification (SVC).3.1 Task 1: Semantic Relatedness andEntailmentMSTS contains sentence pairs from the SICK(Sentences Involving Compositional Knowledge)data set (Marelli et al., 2014b), which contain sen-tence pairs that contain rich lexical, syntactic andsemantic phenomena.
Official evaluation metricin SRE is the Pearson?s correlation score, whichis used to select the top systems on the trainingset.
SRE task allows the submission of 5 entries.We present the performance of the top 5 individ-ual RTM models on the training set in Table 3.ACC is entailment accuracy, rPis Pearson?s corre-lation, rSis Spearman?s correlation, MSE is meansquared error, MAE is mean absolute error, andRAE is relative absolute error.
L uses the lem-matized corpora and R uses the true-cased corporacorresponding to regular.
R+L correspond to theperspective using the features from both R and L,which doubles the number of features.
We com-pute the entailment by SVC.Data Model ACC rPrSMSE MAE RAEL SVR 67.52 .7372 .6918 .6946 .5511 .6856L PLS-SVR 67.04 .7539 .6927 .6763 .5369 .668R+L PLS-SVR 66.76 .75 .6879 .6815 .539 .6705R+L SVR 66.66 .7295 .6814 .7027 .5591 .6956L PLS-RR 66.56 .7247 .6765 .7054 .5687 .7075Table 3: SRE training results of the top 5 RTMsystems selected.SRE challenge results on the test set are givenin Table 4.
The setting R using PLS-SVR learningbecomes the 8th out of 17 submissions when pre-dicting the semantic relatedness and 17th out of 18submissions when predicting the entailment.Data Model ACC rPrSRMSE MAE RAER PLS-SVR 67.20 .7639 .6877 .655 .5246 .6645R+L PLS-SVR 67.65 .7688 .6918 .6492 .5194 .658L SVR 67.65 .7559 .6887 .664 .531 .6726R+L SVR 67.44 .7625 .6899 .6555 .5251 .6651R PLS-SVR 66.61 .7570 .6683 .6637 .5324 .6744Table 4: RTM-DCU test results on the SRE task.Model rPRMSE MAE RAEPar2S TREE 0.8013 0.8345 0.6277 0.5083Par2S PLS-TREE 0.7737 0.8824 0.673 0.5449Par2S SVR 0.7718 0.8863 0.6791 0.5499S2Phrase TREE 0.6756 0.9887 0.7746 0.6665S2Phrase PLS-TREE 0.6119 1.0616 0.8582 0.7384S2Phrase SVR 0.6059 1.0662 0.8668 0.7458Phrase2W TREE 0.201 1.3275 1.1353 0.9706Phrase2W RR 0.1255 1.3463 1.1594 0.9912Phrase2W SVR 0.0847 1.3548 1.1663 0.9972Table 5: CLSS training results of the top 3 RTMsystems for each subtask.
Levels correspond toparagraph to sentence (Par2S), sentence to phrase(S2Phrase), and phrase to word (Phrase2W).4913.2 Task 3: Cross-Level Semantic SimilarityCLSS contains sentence pairs from different gen-res including text from newswire, travel, reviews,metaphoric text, community question answeringsites, idiomatic text, descriptions, lexicographictext, and search.
Official evaluation metric inCLSS is the sum of the Pearson?s correlationscores for different levels5.
CLSS task allows thesubmission of 3 entries per subtask.
We presentthe performance of the top 3 individual RTM mod-els on the training set in Table 5.
RMSE is the rootmean squared error.
As the compared text size de-crease, the performance decrease since it can be-come harder and more ambiguous to find the simi-larity using less context.
RTM-DCU results on theCLSS challenge test set are provided in Table 6.Model rPRMSE MAE RAEPar2S TREE .8445 .7417 .5622 .4579Par2S PLS-TREE .7847 .853 .6456 .5258Par2S SVR .7858 .8428 .6539 .5325S2Phrase TREE .75 .8827 .7053 .6255S2Phrase PLS-TREE .6979 .9491 .7781 .69S2Phrase SVR .6631 .9835 .7992 .7088Phrase2W TREE .3053 1.3351 1.14 .9488Phrase2W RR .2207 1.3644 1.1574 .9633Phrase2W SVR .1712 1.3792 1.1792 .9815Table 6: RTM-DCU test results on CLSS for thetop 3 RTM systems for each subtask.Table 7 lists the results along with their ranksfor rPand rS, Spearman?s correlation, out ofCHECK submissions.
The baseline in Table 7is normalized longest common substring (LCS)scaled in the range [0, 4].
Top individual rank rowlists the ranks in each subtask.
We present the re-sults for both our official and late (about 1 day)submissions including word to sense (W2S) re-sults6.
RTM-DCU is able to obtain the top resultin Par2S in the CLSS task.3.3 Task 10: Multilingual Semantic TextualSimilarityMSTS contains sentence pairs from different do-mains: sense definitions from semantic lexical re-sources such as OnWN (from OntoNotes (Prad-han et al., 2007) and WordNet (Miller, 1995)) andFNWN (from FrameNet (Baker et al., 1998) andWordNet), news headlines, image descriptions,news title tweet comments, deft forum and news,5Giving advantage to participants submitting to all levels.6W2S results for the late submission is obtained from theLCS baseline to calculate the ranks.rPPar2S S2Phrase Phrase2W W2S RankLCS 0.527 0.562 0.165 0.109 25Official0.780 0.677 0.208 140.747 0.588 0.164 190.786 0.666 0.171 18Late0.845 0.750 0.305 0.109 60.785 0.698 0.221 0.109 130.786 0.663 0.171 0.109 17Top Rank 1 5 3rSPar2S S2Phrase Phrase2W W2S RankLCS 0.527 0.562 0.165 0.13 23Official0.780 0.677 0.208 170.747 0.588 0.164 220.786 0.666 0.171 18Late0.829 0.734 0.295 0.13 80.778 0.687 0.219 0.13 150.778 0.667 0.166 0.13 16Top Rank 1 5 5Table 7: RTM-DCU test results on CLSS.paraphrases.
Official evaluation metric in MSTSis the Pearson?s correlation score.MSTS task provides 7622 training instancesand 3750 test instances.
For the OnWN domain,1316 training instances are available and therefore,we build a separate RTM model for this domain.Separate modeling of the OnWN dataset resultswith higher confidence scores on the test instancesthan we would obtain using the overall model topredict.
MSTS task allows the submission of 3 en-tries per subtask.
We present the performance ofthe top 3 individual RTM models on the trainingset in Table 8.Lang Model rPRMSE MAE RAEEnglishTREE 0.6931 1.0627 0.8058 0.6649PLS-TREE 0.6875 1.0753 0.8038 0.6632PLS-SVR 0.6884 1.0698 0.8157 0.6730OnWN TREE 0.8094 0.9295 0.694 0.5245PLS-TREE 0.7953 0.9604 0.7203 0.5444PLS-SVR 0.7888 0.9779 0.7234 0.5468SpanishTREE 0.6513 0.7341 0.5904 0.7508PLS-TREE 0.4157 0.9007 0.7108 0.9039PLS-SVR 0.4239 1.1427 0.8293 1.0545Table 8: MSTS training results on the English, En-glish OnWN, and Spanish tasks.RTM results on the MSTS challenge test set areprovided in Table 9 along with the RTM results inSTS 2013 (Bic?ici and van Genabith, 2013a).
Ta-ble 10 and Table 11 lists the official results on En-glish and Spanish tasks with rankings calculatedaccording to weighted rP, which weights accord-ing to the number of instances in each domain.RTM-DCU is able to become 10th in the OnWNdomain and 19th overall out of 38 submissions inMSTS English and 18th out of 22 submissions in492Model rPRMSE MAE RAEEnglishdeft-forumTREE .4341 1.4306 1.1609 1.0908PLS-TREE .3965 1.4115 1.1472 1.078PLS-SVR .3078 1.6277 1.3482 1.2669deft-newsTREE .6974 1.1469 .9032 .8716PLS-TREE .6811 1.1229 .8769 .8462PLS-SVR .5562 1.2803 .9835 .9491headlinesTREE .6199 1.1495 .9254 .7845PLS-TREE .6125 1.1552 .9314 .7896PLS-SVR .6301 1.1041 .8807 .7467imagesTREE .6995 1.2034 .9499 .7395PLS-TREE .6656 1.2298 .9692 .7545PLS-SVR .6474 1.4406 1.1057 .8607OnWNTREE .8058 1.3122 1.0028 .5585PLS-TREE .7992 1.2997 .9815 .5467PLS-SVR .8004 1.2913 .9449 .5263tweet-newsTREE .6882 .9869 .831 .8093PLS-TREE .6691 1.0101 .8433 .8213PLS-SVR .5531 1.0633 .8653 .8427SpanishNewsTREE .7 1.5185 1.351 1.4141PLS-TREE .6253 1.6523 1.4464 1.514PLS-SVR .6411 1.554 1.3196 1.3813WikipediaTREE .4216 1.5433 1.298 1.3579PLS-TREE .3689 1.6655 1.4015 1.4662PLS-SVR .4242 1.5998 1.3141 1.3748STS2013EnglishheadlinesL+S SVR .6552 1.5649 1.2763 1.0231L+P+S SVR .651 1.4845 1.1984 .9607L+P+S SVR TL .6385 1.4878 1.2008 .9626OnWNL+S SVR .6943 1.7065 1.3545 .8255L+P+S SVR .6971 1.6737 1.333 .8124L+P+S SVR TL .6755 1.7124 1.3598 .8287SMTL+S SVR .3005 .8833 .6886 1.6132L+P+S SVR .2861 .8810 .6821 1.598L+P+S SVR TL .3098 .8635 .6547 1.5339FNWNL+S SVR .2016 1.2957 1.0604 1.2633L+P+S SVR .118 1.4369 1.1866 1.4136L+P+S SVR TL .1823 1.3245 1.0962 1.3059Table 9: RTM-DCU test results on MSTS for thetop 3 RTM systems for each subtask as well asRTM results in STS 2013 (Bic?ici and van Gen-abith, 2013a).MSTS Spanish.
The performance difference be-tween MSTS English and MSTS Spanish may bedue to the fewer training data available for theMSTS Spanish task, which may be decreasing theperformance of our supervised learning approach.3.4 RTMs Across Tasks and YearsWe compare the difficulty of tasks according to theRAE levels achieved.
RAE measures the error rel-ative to the error when predicting the actual mean.A high RAE is an indicator that the task is hard.In Table 12, we list the RAE obtained for differ-ent tasks and subtasks, also listing RTM results inSTS 2013 (Bic?ici and van Genabith, 2013a) andRTM results (Bic?ici and Way, 2014) on the qualityestimation task (QET) (Bojar et al., 2014) wherepost-editing effort (PEE), human-targeted transla-Model Wikipedia News Weighted rPRankTREE 0.4216 0.7000 0.5878 18PLS-TREE 0.3689 0.6253 0.5219 20PLS-SVR 0.4242 0.6411 0.5537 19Table 11: RTM-DCU test results on MSTS Span-ish task.
Rankings are calculated according to theweighted Pearson?s correlation.tion edit rate (HTER), or post-editing time (PET)of translations are predicted.The best results are obtained for the CLSSPar2S subtask, which may be due to the largercontextual information that paragraphs can pro-vide for the RTM models.
For the SRE task, wecan only reduce the error with respect to knowingand predicting the mean by about 35%.
Predictionof bilingual similarity as in quality estimation oftranslation can be expected to be harder and RTMsachieve state-of-the-art performance in this task aswell (Bic?ici and Way, 2014).4 ConclusionReferential translation machines provide a cleanand intuitive computational model for automati-cally measuring semantic similarity by measur-ing the acts of translation involved and achieve tobe the top on some semantic similarity tasks atSemEval-2014.
RTMs make quality and seman-tic similarity judgments possible based on the re-trieval of relevant training data as interpretants forreaching shared semantics.AcknowledgmentsThis work is supported in part by SFI(07/CE/I1142) as part of the CNGL Centrefor Global Intelligent Content (www.cngl.org)at Dublin City University, in part by SFI(13/TIDA/I2740) for the project ?Monolin-gual and Bilingual Text Quality Judgmentswith Translation Performance Prediction?
(www.computing.dcu.ie/?ebicici/Projects/TIDA RTM.html), and in partby the European Commission through the QT-LaunchPad FP7 project (No: 296347).
Wealso thank the SFI/HEA Irish Centre for High-End Computing (ICHEC) for the provision ofcomputational facilities and support.493Model deft-forum deft-news headlines images OnWN tweet-news Weighted rPRankTREE .4341 .6974 .6199 .6995 .8058 .6882 .6706 20PLS-TREE .3965 .6811 .6125 .6656 .7992 .6691 .6513 23PLS-SVR .3078 .5562 .6301 .6475 .8004 .5531 .6076 27Top Rank 17 16 25 26 16 13WithConf.TREE .4181 .6846 .6216 .6981 .8331 .6870 .6729 19PLS-TREE .3831 .6739 .6094 .6629 .8260 .6691 .6534 23PLS-SVR .2731 .5526 .6330 .6441 .8246 .5683 .6110 26Top Rank 18 18 23 27 10 14Table 10: RTM-DCU test results with ranks on MSTS English task.Task Subtask Domain Model RAESRE English SICKR PLS-SVR .6645R+L PLS-SVR .6580L SVR .6726R+L SVR .6651R PLS-SVR .6744CLSSPar2SMixedTREE .4579S2Phrase TREE .6255Phrase2W TREE .9488MSTSEnglishdeft-forum PLS-TREE 1.078deft-news PLS-TREE .8462headlines PLS-SVR .7467images TREE .7395OnWN PLS-SVR .5263tweet-news TREE .8093SpanishNews PLS-SVR 1.3813Wikipedia TREE 1.3579STS 2013 Englishheadlines L+P+S SVR .9607OnWN L+P+S SVR .8124SMT L+P+S SVR TL 1.5339FNWN L+S SVR 1.2633QET PEESpanish-English Europarl FS-RR .9000Spanish-English Europarl PLS-RR .9409English-German Europarl PLS-TREE .8883English-German Europarl TREE .8602English-Spanish Europarl TREE 1.0983English-Spanish Europarl PLS-TREE 1.0794German-English Europarl RR .8204German-English Euruparl PLS-RR .8437QET HTEREnglish-Spanish Europarl SVR .8532English-Spanish Europarl TREE .8931QET PETEnglish-Spanish Europarl SVR .7223English-Spanish Europarl RR .7536Table 12: Best RTM-DCU RAE test results for different tasks and subtasks as well as STS 2013 re-sults (Bic?ici and van Genabith, 2013a) and results from quality estimation task of translation (Bojar etal., 2014; Bic?ici and Way, 2014).ReferencesEneko Agirre, Carmen Banea, Claire Cardie, DanielCer, Mona Diab, Aitor Gonzalez-Agirre, WeiweiGuo, Rada Mihalcea, German Rigau, and JanyceWiebe.
2014.
SemEval-2014 Task 10: Multilin-gual semantic textual similarity.
In Proc.
of the8th International Workshop on Semantic Evaluation(SemEval-2014), Dublin, Ireland, August.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proc.
of the36th Annual Meeting of the Association for Compu-tational Linguistics and 17th International Confer-ence on Computational Linguistics - Volume 1, ACL?98, pages 86?90, Stroudsburg, PA, USA.Daniel B?ar, Chris Biemann, Iryna Gurevych, andTorsten Zesch.
2012.
Ukp: Computing seman-tic textual similarity by combining multiple contentsimilarity measures.
In *SEM 2012: The First JointConference on Lexical and Computational Seman-tics ?
Volume 1: Proc.
of the main conference andthe shared task, and Volume 2: Proc.
of the Sixth In-ternational Workshop on Semantic Evaluation (Se-mEval 2012), pages 435?440, Montr?eal, Canada, 7-8 June.
Association for Computational Linguistics.Ergun Bic?ici and Josef van Genabith.
2013a.
CNGL-494CORE: Referential translation machines for measur-ing semantic similarity.
In *SEM 2013: The SecondJoint Conference on Lexical and Computational Se-mantics, Atlanta, Georgia, USA, 13-14 June.
Asso-ciation for Computational Linguistics.Ergun Bic?ici and Josef van Genabith.
2013b.
CNGL:Grading student answers by acts of translation.
In*SEM 2013: The Second Joint Conference on Lex-ical and Computational Semantics and Proc.
of theSeventh International Workshop on Semantic Eval-uation (SemEval 2013), Atlanta, Georgia, USA, 14-15 June.
Association for Computational Linguistics.Ergun Bic?ici and Andy Way.
2014.
Referential trans-lation machines for predicting translation quality.
InProc.
of the Ninth Workshop on Statistical MachineTranslation, Baltimore, USA, June.
Association forComputational Linguistics.Ergun Bic?ici and Deniz Yuret.
2011a.
Instance se-lection for machine translation using feature decayalgorithms.
In Proc.
of the Sixth Workshop on Sta-tistical Machine Translation, pages 272?283, Ed-inburgh, Scotland, July.
Association for Computa-tional Linguistics.Ergun Bic?ici and Deniz Yuret.
2011b.
RegMT sys-tem for machine translation, system combination,and evaluation.
In Proc.
of the Sixth Workshop onStatistical Machine Translation, pages 323?329, Ed-inburgh, Scotland, July.
Association for Computa-tional Linguistics.Ergun Bic?ici and Deniz Yuret.
2014.
Optimizing in-stance selection for statistical machine translationwith feature decay algorithms.
IEEE/ACM Transac-tions On Audio, Speech, and Language Processing(TASLP).Ergun Bic?ici, Declan Groves, and Josef van Genabith.2013.
Predicting sentence translation quality usingextrinsic and language independent features.
Ma-chine Translation, 27:171?192, December.Ergun Bic?ici, Qun Liu, and Andy Way.
2014.
Paral-lel FDA5 for fast deployment of accurate statisticalmachine translation systems.
In Proc.
of the NinthWorkshop on Statistical Machine Translation, Bal-timore, USA, June.
Association for ComputationalLinguistics.Ergun Bic?ici.
2011.
The Regression Model of MachineTranslation.
Ph.D. thesis, Koc?
University.
Supervi-sor: Deniz Yuret.Ergun Bic?ici.
2013.
Referential translation machinesfor quality estimation.
In Proc.
of the Eighth Work-shop on Statistical Machine Translation, Sofia, Bul-garia, August.
Association for Computational Lin-guistics.Ergun Bic?ici.
2008.
Consensus ontologies in sociallyinteracting multiagent systems.
Journal of Multia-gent and Grid Systems.Carl Hugo Bj?ornsson.
1968.
L?asbarhet.
Liber.Chris Bliss.
2012.
Comedy is transla-tion, February.
http://www.ted.com/talks/chris bliss comedy is translation.html.Ond?rej Bojar, Christian Buck, Christian Federmann,Barry Haddow, Philipp Koehn, Matou?s Mach?a?cek,Christof Monz, Pavel Pecina, Matt Post, HerveSaint-Amand, Radu Soricut, and Lucia Specia.2014.
Findings of the 2014 workshop on statisti-cal machine translation.
In Proc.
of the Ninth Work-shop on Statistical Machine Translation, Balrimore,USA, June.
Association for Computational Linguis-tics.Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:Parameter estimation.
Computational Linguistics,19(2):263?311, June.Jos?e Guilherme Camargo de Souza, Christian Buck,Marco Turchi, and Matteo Negri.
2013.
FBK-UEdin participation to the WMT13 quality estima-tion shared task.
In Proc.
of the Eighth Workshopon Statistical Machine Translation, pages 352?358,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proc.
of the second interna-tional conference on Human Language TechnologyResearch, pages 138?145, San Francisco, CA, USA.Morgan Kaufmann Publishers Inc.Pierre Geurts, Damien Ernst, and Louis Wehenkel.2006.
Extremely randomized trees.
Machine Learn-ing, 63(1):3?42.Isabelle Guyon, Jason Weston, Stephen Barnhill, andVladimir Vapnik.
2002.
Gene selection for cancerclassification using support vector machines.
Ma-chine Learning, 46(1-3):389?422.Kenth Hagstr?om.
2012.
Swedish readabil-ity calculator.
https://github.com/keha76/Swedish-Readability-Calculator.David Jurgens, Mohammad Taher Pilehvar, andRoberto Navigli.
2014.
SemEval-2014 Task 3:Cross-level semantic similarity.
In Proc.
of the8th International Workshop on Semantic Evaluation(SemEval-2014), Dublin, Ireland, August.Roger Levy and Galen Andrew.
2006.
Tregex andTsurgeon: tools for querying and manipulating treedata structures.
In Proc.
of the fifth internationalconference on Language Resources and Evaluation.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: the penn treebank.
Comput.Linguist., 19(2):313?330, June.495Marco Marelli, Stefano Menini, Marco Baroni, LuisaBentivogli, Raffaella Bernardi, and Roberto Zam-parelli.
2014a.
SemEval-2014 Task 1: Evaluationof compositional distributional semantic models onfull sentences through semantic relatedness and tex-tual entailment.
In Proc.
of the 8th InternationalWorkshop on Semantic Evaluation (SemEval-2014),Dublin, Ireland, August.Marco Marelli, Stefano Menini, Marco Baroni, LuisaBentivogli, Raffaella Bernardi, and Roberto Zam-parelli.
2014b.
A sick cure for the evaluation ofcompositional distributional semantic models.
InProc.
of LREC 2014, Reykjavik, Iceland.George A. Miller.
1995.
WordNet: A lexicaldatabase for English.
Communications of the ACM,38(11):39?41, November.Preslav Nakov and Torsten Zesch, editors.
2014.Proc.
of SemEval-2014 Semantic Evaluation Exer-cises - International Workshop on Semantic Evalua-tion.
Dublin, Ireland, 23-24 August.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a method for auto-matic evaluation of machine translation.
In Proc.of 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA, July.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2011.
English Gigaword fifth edi-tion, Linguistic Data Consortium.Sameer S. Pradhan, Eduard H. Hovy, Mitchell P.Marcus, Martha Palmer, Lance A. Ramshaw, andRalph M. Weischedel.
2007.
Ontonotes: a unifiedrelational semantic representation.
Int.
J. SemanticComputing, 1(4):405?419.Yoav Seginer.
2007.
Learning Syntactic Structure.Ph.D.
thesis, Universiteit van Amsterdam.Alex J. Smola and Bernhard Sch?olkopf.
2004.
A tu-torial on support vector regression.
Statistics andComputing, 14(3):199?222, August.A.
J. Smola, N. Murata, B. Sch?olkopf, and K.-R.M?uller.
1998.
Asymptotically optimal choice of ?-loss for support vector machines.
In L. Niklasson,M.
Boden, and T. Ziemke, editors, Proc.
of the Inter-national Conference on Artificial Neural Networks,Perspectives in Neural Computing, pages 105?110,Berlin.
Springer.Lucia Specia, Nicola Cancedda, Marc Dymetman,Marco Turchi, and Nello Cristianini.
2009.
Estimat-ing the sentence-level quality of machine translationsystems.
In Proc.
of the 13th Annual Conference ofthe European Association for Machine Translation(EAMT), pages 28?35, Barcelona, May.
EAMT.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proc.
of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology - Vol-ume 1, NAACL ?03, pages 173?180, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Wikipedia.
2013.
LIX.
http://en.wikipedia.org/wiki/LIX.David Graff Denise DiPersio?Angelo Mendonc?a,Daniel Jaquette.
2011.
Spanish Gigaword third edi-tion, Linguistic Data Consortium.496
