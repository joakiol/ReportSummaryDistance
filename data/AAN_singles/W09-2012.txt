Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 87?93,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsHow Creative is Your Writing?
A Linguistic Creativity Measure fromComputer Science and Cognitive Psychology PerspectivesXiaojin Zhu, Zhiting Xu and Tushar KhotDepartment of Computer SciencesUniversity of Wisconsin-MadisonMadison, WI, USA 53706{jerryzhu, zhiting, tushar}@cs.wisc.eduAbstractWe demonstrate that subjective creativity insentence-writing can in part be predicted us-ing computable quantities studied in Com-puter Science and Cognitive Psychology.
Weintroduce a task in which a writer is asked tocompose a sentence given a keyword.
Thesentence is then assigned a subjective creativ-ity score by human judges.
We build a linearregression model which, given the keywordand the sentence, predicts the creativity score.The model employs features on statistical lan-guage models from a large corpus, psycholog-ical word norms, and WordNet.1 IntroductionOne definition of creativity is ?the ability to tran-scend traditional ideas, rules, patterns, relationships,or the like, and to create meaningful new ideas,forms, methods, interpretations, etc.?
Therefore,any computational measure of creativity needs to ad-dress two aspects simultaneously:1.
The item to be measured has to be differentfrom other existing items.
If one can model ex-isting items with a statistical model, the newitem should be an ?outlier?.2.
The item has to be meaningful.
An item con-sists of random noise might well be an outlier,but it is not of interest.In this paper, we consider the task of measuring hu-man creativity in composing a single sentence, whenthe sentence is constrained by a given keyword.
Thissimple task is a first step towards automatically mea-suring creativity in more complex natural languagetext.
To further simplify the task, we will focus onthe first aspect of creativity, i.e., quantifying hownovel the sentence is.
The second aspect, how mean-ingful the sentence is, requires the full power of Nat-ural Language Processing, and is beyond the scopeof this initial work.
This, of course, raises the con-cern that we may regard a nonsense sentence ashighly creative.
This is a valid concern.
However,in many applications where a creativity measure isneeded, the input sentences are indeed well-formed.In such applications, our approach will be useful.We will leave this issue to future work.
The presentpaper uses a data set (see the next section) in whichall sentences are well-formed.A major difficulty in studying creativity is thelack of an objective definition of creativity.
Becausecreative writing is highly subjective (?I don?t knowwhat is creativity, but I recognize it when I see one?
),we circumvent this problem by using human judg-ment as the ground truth.
Our experiment procedureis the following.
First, we give a keyword z to ahuman writer, and ask her to compose a sentencex about z.
Then, the sentence x is evaluated by agroup of human judges who assign it a subjective?creativity score?
y.
Finally, given a dataset con-sisting of many such keyword-sentence-score triples(z,x, y), we develop a statistical predictor f(x, z)that predicts the score y from the sentence x andkeyword z.There has been some prior attempts on charac-terizing creativity from a computational perspec-tive, for examples see (Ritchie, 2001; Ritchie, 2007;87Pease et al, 2001).
The present work distinguishesitself in the use of a statistical machine learningframework, the design of candidate features, and itsempirical study.2 The Creativity Data SetWe select 105 keywords from the English version ofthe Leuven norms dataset (De Deyne and Storms,2008b; De Deyne and Storms, 2008a).
This ensuresthat each keyword has their norms feature defined,see Section 3.2.
These are common English words.The keywords are randomly distributed to 21 writ-ers, each writer receives 5 keywords.
Each writercomposes one sentence per keyword.
These 5 key-words are further randomly split into two groups:1.
The first group consists of 1 keyword.
Thewriters are instructed to ?write a not-so-creativesentence?
about the keyword.
Two examplesare given: ?Iguana has legs?
for ?Iguana?, and?Anvil can get rusty?
for ?Anvil.?
The purposeof this group is to establish a non-creative base-line for the writers, so that they have a sensewhat does not count as creative.2.
The second group consists of 4 keywords.
Thewriters are instructed to ?try to write a creativesentence?
about each keyword.
They are alsotold to write a sentence no matter what, even ifthey cannot come up with a creative one.
Noexample is given to avoid biasing their creativethinking.In the next stage, all sentences are given to fourhuman judges, who are native English speakers.
Thejudges are not the writers nor the authors of thispaper.
The order of the sentences are randomized.The judges see the sentences and their correspond-ing keywords, but not the identity of the writers,nor which group the keywords are in.
The judgeswork independently.
For each keyword-sentencepair, each judge assigns a subjective creativity scorebetween 0 and 10, with 0 being not creative at all(the judges are given the Iguana and Anvil exam-ples for this), and 10 the most creative.
The judgesare encouraged to use the full scale when scoring.There is statistically significant (p < 10?8) linearcorrelation among the four judges?
scores, showingtheir general agreement on subjective creativity.
Ta-ble 1 lists the pairwise linear correlation coefficientbetween all four judges.Table 1: The pairwise linear correlation coefficient be-tween four judges?
creativity scores given to the 105 sen-tences.
All correlations are statistically significant withp < 10?8.judge 2 judge 3 judge 4judge 1 0.68 0.61 0.74judge 2 0.55 0.74judge 3 0.61The scores from four judges on each sentence arethen averaged to produce a consensus score y. Ta-ble 2 shows the top and bottom three sentences assorted by y.As yet another sanity check, note that the judgeshave no information which sentences are from group1 (where the writers are instructed to be non-creative), and which are from group 2.
We wouldexpect that if both the writers and the judges sharesome common notion of creativity, the mean scoreof group 1 should be smaller than the mean score ofgroup 2.
Figure 1 shows that this is indeed the case,with the mean score of group 1 being 1.5?
0.6, andthat of group 2 being 5.1 ?
0.4 (95% confidence in-terval).
A t-test shows that this difference is signifi-cant (p < 10?11).1 20246groupscoreFigure 1: The mean creativity score for group 1 is signif-icantly smaller than that for group 2.
That is, the judgesfeel that sentences in group 2 are more creative.To summarize, in the end our dataset con-sists of 105 keyword, sentence, creativityscore tuples {(zi,xi, yi)} for i = 1, .
.
.
, 105.The sentence group information is not in-cluded.
This ?Wisconsin Creative Writ-ing?
dataset is publicly available at http:88Table 2: Example sentences with the largest and smallest consensus creativity scores.consensus score y keyword z sentence x9.25 hamster She asked if I had any pets, so I told her I once did until I discoveredthat I liked taste of hamster.9.0 wasp The wasp is a dinosaur in the ant world.8.5 dove Dove can still bring war by the information it carries....0.25 guitar A Guitar has strings.0.25 leech Leech lives in the water.0.25 elephant Elephant is a mammal.//pages.cs.wisc.edu/?jerryzhu/pub/WisconsinCreativeWriting.txt.3 Candidate Features for PredictingCreativityIn this section, we discuss two families of candi-date features we use in a statistical model to pre-dict the creativity of a sentence.
One family comesfrom a Computer Science perspective, using large-corpus statistics (how people write).
The other fam-ily comes from a Cognitive Psychology perspective,specifically the word norms data and WordNet (howpeople think).3.1 The Computer Science Perspective:Language ModelingWe start from the following hypothesis: if the wordsin the sentence x frequently co-occur with the key-word z, then x is probably not creative.
This is ofcourse an over-simplification, as many creative sen-tences are about novel usage of common words1.Nonetheless, this hypothesis inspires some candi-date features that can be computed from a large cor-pus.In this study, we use the Google Web 1T 5-gram Corpus (Brants et al, 2007).
This corpuswas generated from about 1012 word tokens fromWeb pages.
It consists of counts of N-gram forN = 1, .
.
.
, 5.
We denote the words in a sentenceby x = x1, .
.
.
, xn, where x1 = ?s?
and xn = ?/s?are special start- and end-of-sentence symbols.
We1For example, one might argue that Lincoln?s famous sen-tence on government: ?of the people, by the people, for thepeople?
is creative, even though the keyword ?government?
fre-quently co-occurs with all the words in that sentence.design the following candidate features:[f1: Zero N-gram Fraction] Let c(xi+N?1i ) bethe count of the N-gram xi .
.
.
xi+N?1 in the corpus.Let ?
(A) be the indicator function with value 1 ifthe predicate A is true, and 0 otherwise.
A ?ZeroN-gram Fraction?
feature is the fraction of zero N-gram counts in the sentence:f1,N (x) =?n?N+1i=1 ?
(c(xi+N?1i ) = 0)n ?
N + 1 .
(1)This provided us with 5 features, namely N-gramzero count fractions for each value of N. These fea-tures are a crude measure of how surprising the sen-tence x is.
A feature value of 1 indicates that none ofthe N-grams in the sentence appeared in the Googlecorpus, a rather surprising situation.
[f2: Per-Word Sentence Probability] This fea-ture is the per-word log likelihood of the sentence,to normalize for sentence length:f2(x) = 1n log p(x).
(2)We use a 5-gram language model to estimatep(x), with ?naive Jelinek-Mercer?
smoothing.
Asin Jelinek-Mercer smoothing (Jelinek and Mercer,1980), it is a linear interpolation of N-gram languagemodels for N = 1 .
.
.
5.
Let the Maximum Likeli-hood (ML) estimate of a N-gram language model bepNML(xi|xi?1i?N+1) =c(xii?N+1)c(xi?1i?N+1), (3)which is the familiar frequency estimate of proba-bility.
The denominator is the count of the historyof length N ?
1, and the numerator is the count ofthe history plus the word to be predicted.
A 5-gram89Jelinek-Mercer smoothing language model on sen-tence x isp(x) =n?i=1p(xi|xi?1i?5+1) (4)p(xi|xi?1i?5+1) =5?N=1?NPNML(xi|xi?1i?N+1),(5)where the linear interpolation weights ?1 + .
.
.
+?5 = 1.
The optimal values of ?
?s are a function ofhistory counts (binned into ?buckets?)
c(xi?1i?N+1),and should be optimized with convex optimiza-tion from corpus.
However, because our corpus islarge, and because we do not require precise lan-guage modeling, we instead set the ?
?s in a heuris-tic manner.
Starting from N=5 to 1, ?N is setto zero until the N where we have enough historycount for reliable estimate.
Specifically, we requirec(xi?1i?N+1) > 1000.
The first N that this happensreceives ?N = 0.9.
The next lower order modelreceives 0.9 fraction of the remaining weight, i.e.,?N?1 = 0.9 ?
(1 ?
0.9), and so on.
Finally, ?1 re-ceives all remaining weight to ensure ?1+.
.
.+?5 =1.
This heuristic captures the essence of Jelinek-Mercer smoothing and is highly efficient, at the priceof suboptimal interpolation weights.
[f3: Per-Word Context Probability] The previ-ous feature f2 ignores the fact that our sentence xis composed around a given keyword z.
Given thatthe writer was prompted with the keyword z, we areinterested in the novelty of the sentence surround-ing the keyword.
Let xk be the first occurrence ofz in the sentence, and let x?k be the context of thekeyword, i.e., the sentence with the k-th word (thekeyword) removed.
This notion of context noveltycan be captured byp(x?k|xk = z) = p(x?k, xk = z)p(xk = z) =p(x)p(z) , (6)where p(x) is estimated from the naive Jelinek-Mercer 5-gram language model above, and p(z) isestimated from a unigram language model.
Our thirdfeature is the length-normalized log likelihood of thecontext:f3(x, z) = 1n ?
1 (log p(x) ?
log p(z)) .
(7)3.2 The Cognitive Psychology Perspective:Word Norms and WordNetA text corpus like the one above captures how peo-ple write sentences related to a keyword.
However,this can be different from how people think about re-lated concepts in their head for the same keyword.In fact, common sense knowledge is often under-represented in a corpus ?
for example, why botherrepeating ?A duck has a long neck?
over and overagain?
However, this lack of co-occurrence does notnecessarily make the duck sentence creative.The way people think about concepts can in partbe captured by word norms experiments in psychol-ogy.
In such experiments, a human subject is pro-vided with a keyword z, and is asked to write downthe first (or a few) word x that comes to mind.When aggregated over multiple subjects on the samekeyword, the experiment provides an estimate ofthe concept transition probability p(x|z).
Givenenough keywords, one can construct a concept net-work where the nodes are the keywords, and theedges describe the transitions (Steyvers and Tenen-baum, 2005).
For our purpose, we posit that a sen-tence x may not be creative with respect to a key-word z, if many words in x can be readily retrievedas the norms of keyword z.
In a sense, the writerwas thinking the obvious.
[f4: Word Norms Fraction] We use the Leu-ven dataset, which consists of norms for 1,424 key-words (De Deyne and Storms, 2008b; De Deyne andStorms, 2008a).
The original Leuven dataset is inDutch, we use a version that is translated into En-glish.
For each sentence x, we first exclude the key-word z from the sentence.
We also remove punctu-ations, and map all words to lower case.
We furtherremove all stopwords using the Snowball stopwordlist (Porter, 2001), and stem all words in the sentenceand the norm word list using NLTK (Loper and Bird,2002).
We then count the number of words xi thatappear in the norm list of the keyword z in the Leu-ven data.
Let this count be cnorm(x, z).
The featureis the fraction of such norm words in the originalsentence:f4(x, z) = cnorm(x, z)n .
(8)It is worth noting that the Leuven dataset is relativelysmall, with less than two thousand keywords.
This90is a common issue with psychology norms datasets,as massive number of human subjects are difficultto obtain.
To scale our method up to handle largevocabulary in the future, one possible method is toautomatically infer the norms of novel keywords us-ing corpus statistics (e.g., distributional similarity).
[f5 ?
f13: WordNet Similarity] WordNet is an-other linguistic resource motivated by cognitive psy-chology.
For each sentence x, we compute Word-Net 3.0 similarity between the keyword z and eachword xi in the sentence.
Specifically, we use the?path similarity?
provided by NLTK (Loper andBird, 2002).
Path similarity returns a score denot-ing how similar two word senses are, based on theshortest path that connects the senses in the hyper-nym/hyponym taxonomy.
The score is in the range0 to 1, except in those cases where a path cannot befound, in which case -1 is returned.
A score of 1represents identity, i.e., comparing a sense with it-self.
Let the similarities be s1 .
.
.
sn.
We experimentwith the following features: The mean, median, andvariance of similarities:f5(x, z) = mean(s1 .
.
.
sn) (9)f6(x, z) = median(s1 .
.
.
sn) (10)f7(x, z) = var(s1 .
.
.
sn).
(11)Features f8, .
.
.
, f12 are the top five similarities.When the length of the sentence is shorter than five,we fill the remaining features with -1.
Finally, fea-ture f13 is the fraction of positive similarity:f13(x, z) =?ni=1 ?
(si > 0)n .
(12)4 Regression Analysis on CreativityWith the candidate features introduced in Section 3,we construct a linear regression model to predict thecreativity scores given a sentence and its keyword.The first question one asks in regression analy-sis is whether the features have a (linear) correlationwith the creativity score y.
We compute the correla-tion coefficient?i = Cov(fi, y)?fi?y (13)for each candidate feature fi separately on the firstrow in Table 3.
Some observations:?
The feature f4 (Word Norms Fraction) has thelargest correlation coefficient -0.48 in terms ofmagnitude.
That is, the more words in the sen-tence that are also in the norms of the keyword,the less creative the sentence is.?
The feature f12 (the 5-th WordNet similarity inthe sentence to the keyword) has a large posi-tive coefficient 0.47.
This is rather unexpected.A closer inspection reveals that f12 equals -1for about half of the sentences, and is around0.05 for the other half.
Furthermore, the secondhalf has on average higher creativity scores.
Al-though we hypothesized earlier that more simi-lar words means lower creativity, this (togetherwith the positive ?
for f10, f11) suggests theother way around: more similar words are cor-related with higher creativity.?
The feature f5 (mean WordNet similarity) hasa negative correlation with creativity.
This fea-ture is related to f12, but in a different direc-tion.
We speculate that this feature measuresthe strength of similar words, while f12 indi-rectly measures the number of similar words.?
The feature f3 (Per-Word Context Probability)has a negative correlation with creativity.
Themore predictable the sentence around the key-word using a language model, the lower thecreativity.Next, we build a linear regression model to pre-dict creativity.
We use stepwise regression, whichis a technique for feature selection by iterativelyincluding / excluding candidate features from theregression model based on statistical significancetests (Draper and Smith, 1998).
The result is a lin-ear regression model with a small number of salientfeatures.
For the creativity dataset, the features (andtheir regression coefficients) included by stepwiseregression are shown on the second row in Table 3.The corresponding linear regression model isy?
(x, z) = ?5.06 ?
f4 + 1.80 ?
f12 ?
0.76 ?
f3?3.39 ?
f5 + 0.92.
(14)A plot comparing y?
and y is given in Figure 2.
Theroot mean squared error (RMSE) of this model is91Table 3: ?
: The linear correlation coefficients between a candidate feature and the creativity score y.
?
: The selectedfeatures and their regression coefficients in stepwise linear regression.f1,1 f1,2 f1,3 f1,4 f1,5 f2 f3 f4 f5?
0.09 0.09 0.17 0.06 -0.04 -0.07 -0.32 -0.48 -0.41?
-0.76 -5.06 -3.39f6 f7 f8 f9 f10 f11 f12 f13?
-0.19 -0.25 -0.02 0.06 0.23 0.30 0.47 -0.01?
1.800 5 100246810predicted scoretrue scoreFigure 2: The creativity score y?
as predicted by the linearregression model in equation 14, compared to the truescore y.
Each dot is a sentence.1.51.
In contrast, the constant predictor would haveRMSE 2.37 (i.e., the standard deviation of y).We make two comments:1.
It is interesting to note that our intuitive fea-tures are able to partially predict subjective cre-ativity scores.
On the other hand, we certainlydo not claim that our features or model solvedthis difficult problem.2.
All three kinds of knowledge: corpus statistics(f3), word norms (f4), and WordNet (f5, f12)are included in the regression model.
Coin-cidentally, these features have the largest cor-relation coefficients with the creativity score.The fact that they are all included suggests thatthese are not redundant features, and each cap-tures some aspect of creativity.5 Conclusions and Future WorkWe presented a simplified creativity prediction task,and showed that features derived from statisticallanguage modeling, word norms, and WordNet canpartially predict human judges?
subjective creativityscores.Our problem setting is artificial, in that the cre-ativity of the sentences are judged with respect totheir respective keywords, which are assumed to beknown beforehand.
This allows us to design featurescentered around the keywords.
We hope our analysiscan be extended to the setting where the only input isthe sentence, without the keyword.
This can poten-tially be achieved by performing keyword extractionon the sentence first, and apply our analysis on theextracted keyword.As discussed in the introduction, our analysisis susceptible to nonsense input sentences, whichcould be predicted as highly creative.
Combiningour analysis with a ?sensibility analysis?
is an im-portant future direction.Finally, our model might be adapted to explainwhy a sentence is deemed creative, by analyzing thecontribution of individual features in the model.6 AcknowledgmentsWe thank the anonymous reviewers for suggestionson related work and other helpful comments, andChuck Dyer, Andrew Goldberg, Jake Rosin, andSteve Yazicioglu for assisting the project.
This workis supported in part by the Wisconsin Alumni Re-search Foundation.ReferencesThorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In EMNLP.S.
De Deyne and G Storms.
2008a.
Word associations:Network and semantic properties.
Behavior ResearchMethods, 40:213?231.S.
De Deyne and G Storms.
2008b.
Word associations:Norms for 1,424 Dutch words in a continuous task.Behavior Research Methods, 40:198?205.92Norman R. Draper and Harry Smith.
1998.
AppliedRegression Analysis (Wiley Series in Probability andStatistics).
John Wiley & Sons Inc, third edition.Frederick Jelinek and Robert L. Mercer.
1980.
Inter-polated estimation of Markov source parameters fromsparse data.
In Workshop on Pattern Recognition inPractice.Edward Loper and Steven Bird.
2002.
NLTK: The nat-ural language toolkit.
In The ACL Workshop on Ef-fective Tools and Methodologies for Teaching NaturalLanguage Processing and Computational Linguistics,pages 62?69.Alison Pease, Daniel Winterstein, and Simon Colton.2001.
Evaluating machine creativity.
In Workshopon Creative Systems, 4th International Conference onCase Based Reasoning, pages 129?137.Martin F. Porter.
2001.
Snowball: A language for stem-ming algorithms.
Published online.Graeme Ritchie.
2001.
Assessing creativity.
In Pro-ceedings of the AISB01 Symposium on Artificial Intel-ligence and Creativity in Arts and Science, pages 3?11.Graeme Ritchie.
2007.
Some empirical criteria for at-tributing creativity to a computer program.
Minds andMachines, 17(1):67?99.Mark Steyvers and Joshua Tenenbaum.
2005.
The largescale structure of semantic networks: Statistical anal-yses and a model of semantic growth.
Cognitive Sci-ence, 29(1):41?78.93
