Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1345?1350,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsContext-Dependent Automatic Response GenerationUsing Statistical Machine Translation TechniquesAndrew Shin Ryohei Sasano Hiroya Takamura Manabu OkumuraTokyo Institute of Technologyshin@lr.pi.titech.ac.jp {sasano,takamura,oku}@pi.titech.ac.jpAbstractDeveloping a system that can automaticallyrespond to a user?s utterance has recently be-come a topic of research in natural languageprocessing.
However, most works on the topictake into account only a single preceding ut-terance to generate a response.
Recent worksdemonstrate that the application of statisti-cal machine translation (SMT) techniques to-wards monolingual dialogue setting, in whicha response is treated as a translation of a stim-ulus, has a great potential, and we exploit theapproach to tackle the context-dependent re-sponse generation task.
We attempt to extractrelevant and significant information from thewider contextual scope of the conversation,and incorporate it into the SMT techniques.We also discuss the advantages and limitationsof this approach through our experimental re-sults.1 IntroductionVarious approaches have been applied to the re-sponse generation task, each with its own meritsand drawbacks.
While one of the main concerns onthe topic has been the semantic relevance of the re-sponse, it has mostly been discussed in terms of alimited conversational scope, mostly a single utter-ance.
This provides us with a room for research on awider scope of conversation, which reflects not onlya single preceding utterance, but the overall contextof the current conversation.SMT-based data-driven approach to the responsegeneration task was recently introduced by Ritteret al (2011).
They demonstrated that it wasbetter-suited for response generation than some ofthe previous approaches, including information re-trieval approach.
We exploit this model to addressthe above-mentioned problem of reflecting a widerscope of conversation.We present a context-dependent model where weattempt to generate more semantically relevant anddiverse responses by adding the semantically impor-tant words from previous utterances to the most re-cent one.
By doing so, we hope not only to diversifythe responses, but also to be able to take semanticsfrom broader scope of the conversation into account.2 Response Generation using SMT2.1 OverviewRitter et al (2011) remarked that stimulus-responsepairs in the same language often have a strong struc-tural resemblance, as shown in the example conver-sation below, that may be exploited in SMT plat-forms.
In the usual SMT setting, a string f in asource language is translated into a string e in atarget language according to probability distributionp(e|f ) (Brown et al, 1993).
Ritter et al applied theSMT techniques to monolingual conversation set-ting, and treated the response as the translation ofthe stimulus.Stimulus: What is your hobby?Response: My hobby is hiking.2.2 ChallengesAlthough the application of SMT to the responsegeneration task demonstrates potentials, it has a fewdrawbacks due to its nature.1345First, the lengths of the source and target utter-ances are not correlated in the conversational setting,and there is hardly any general tendency towards therelative length of the utterances, as shown in the ex-ample conversation below.
SMT usually works onthe data in which the ratio between the lengths ofthe source and target utterances stays relatively con-stant (Och and Ney, 2000).
However, conversationalsetting, in which such constant ratio is absent, jeop-ardizes the functionality of the usual SMT modelsto make alignments.
Although it is highly proba-ble that some of the semantic elements in the sourceutterance are reflected in the target utterance, it israrely on a one-to-one basis.A: Is something going on today?
(S1)B: Of course, it?s dad?s birthday.
(S2)A (most recent stimulus) : What?!
(S3)B (target) : Oh, you didn?t know?
(S4)Second, it cannot take into account what was pre-viously discussed in the conversation.
Unless themost recent utterance brings a completely new topic,or it has sufficient information in itself, such prob-lem is evident.Both problems regarding the context and thealignment become more pronounced especially incases where the source utterance is short as shown inthe above example.
Clearly, no meaningful responsecan be derived from the most recent stimulus alone,and it is highly unclear how the alignments shouldbe made.
Indeed, the response generated by apply-ing SMT to the most recent stimulus ?What??
in thisexample is ?that,?
which only mimics the syntacticstructure but fails to deliver any meaningful content.3 Context-Dependent Model3.1 OverviewIn order to deal with the issues of context and thelengths of the utterances without correlation, wework on building a context-dependent model, inwhich we balance the utterance lengths by selectingcontextually important words from the previous partof the conversation, and adding them to the sourceutterance.
For example, applying one of our mod-els to the most recent stimulus (S3) of the previousexample conversation results in the following utter-ance, where the words in the parenthesis are newlyadded:A: (today birthday) What?
!The rationale behind this approach is that thetopic of a conversation can be characterized by anumber of contextually important words, which pro-vide semantic information to be reflected in the re-sponse generation process.This approach seemingly reduces the grammat-ical integrity of the source utterance, and it mayseem as if we risk confusing the translation modeland losing grammaticality of the output.
However,grammaticality of the output is handled by the lan-guage model, and the language model is constructedupon the target language only, which in our casecorresponds to the target utterances that remain un-touched.
Also, the newly added words are of highrelevance to the topic, so the new source utterancefrequently demonstrates high semantic coherenceboth within itself, and in parallel with the target ut-terance.The question now is how to determine whichwords are contextually important throughout theconversation.
Since finding such contextually im-portant words is our main concern, we find sim-ple statistical significance test models more suitablethan conventional methods from discourse model-ing or dialogue systems (Oh et al, 2002).
We ex-amine two approaches, namely the pair-based ap-proach, and the token-based approach.
The pair-based approach uses Fisher?s Exact Test (Moore,2004), which is reported to give more accurate p-values than ?2or G2when the counts are small (Rit-ter et al, 2011).
This approach takes advantage ofthe proximity of utterances, and assumes that a ut-terance whose distance to the source utterance isshorter is likely to be more contextually related tothe source utterance, i.e.
Sn?1is more likely thanSn?2to be semantically relevant to Sn.
The token-based approach considers at the entire conversation,and selects the words most characteristic of the con-versation, using the most widely used term weight-ing algorithm, tf-idf.3.2 Pair-Based ModelGiven a conversation consisting of utterancesS1,...,Sn+1, where Snis the source utterance andSn+1is the target utterance, we start by computingthe p-value from Fisher?s Exact Test for every pos-sible word pair between Snand Sn?1.
If the p-value1346is less than the threshold, implying a significant rel-evance between the words constituting the pair, westore the words.
We then add the stored words to thesource utterance, avoiding duplicates with words al-ready in the source utterance, until its length is thesame as that of the target utterance.
Words are addedin a reversed order of their appearance, i.e., we givepriority to words that appeared in the later part ofthe discourse, in light of the previously mentionedassumption.
If, after adding all stored words to thesource utterance, the length of the source utteranceis still less than the length of the target utterance,we repeat the process with word pairs between Sn?1and Sn?2, and so forth.
This ?crawling-up?
is nec-essary because Snis often short or semantically triv-ial that further comparison of Snwith other previousutterances fails to capture the contextually importantwords that are continuously discussed in the previ-ous part of the conversation.
The procedure endswhen there are no more pairs whose p-value is lessthan the threshold, or the source utterance has thesame length as the target utterance.Note that, for training, we limit the applicationof our model only to the cases where source utter-ances are shorter than target utterances, since addingwords in the opposite case will exacerbate the diffi-culty of alignment.
In the test setting, however, wedo not know the length of the actual target utterance,and thus selectively apply our model based on theabsolute length of the source utterance, where thethreshold is set to the average length of the sourceutterance throughout the training data.
Also, since itis evident that we are not dealing with grammaticallywell-formed utterances whose ordering should mat-ter, we opt not to use the reordering table (Bisazzaet al, 2011).3.3 Token-Based ModelThe assumption behind the pair-based approach isthat a topic of a conversation is something that con-tinues to be discussed throughout the conversation,i.e.
something that gets reflected/matched in thelater part of the conversation.
Finding collocatedwords using significant test does just that.
How-ever, there may be a trade-off here in terms of rep-resenting the diversity of context; for example, theremay be a characteristic word that is not directly re-flected/matched in the later part of the conversation.That provides the motivation for our token-based ap-proach, using tf-idf.This approach follows a similar manner of addingwords to the source utterance until its length is equalto the target utterance, but differs in that it picks con-textually important words by examining individualtokens, rather than pairs of words, using tf-idf.
Foridf, the total number of documents was set to thenumber of conversations in our training data.Also, instead of crawling up the conversationfrom the source utterance, it scans through the entireconversation and selects characteristic words withinthe given scope of the conversation.
This is intendedto reflect that there could be words that are highlyrelevant to the overall topic of the conversation, yetnot very close to the current source utterance.
Forexample, in the following conversation, both (S3)and (S4) lack any element characteristic of the con-versation that leads to the final response (S5), while?NBA?
or ?fans?
in (S1) and (S2) is indicative ofthe topic of the conversation, and will be relevant towords like ?LeBron?
or ?dominating?
in the targetutterance.A: Well, the NBA season is near again.
(S1)B: Yeah!
So excited for all the NBA fans!
(S2)A: I?m not.
(S3)B (source) : How come?
(S4)A (target) : It?s just gonna be LeBron dominatingagain.
(S5)Although we examine the entire conversation,words that are too far from the source utterance (forexample, 50 utterances apart) will rarely have muchsemantic impact to the current topic.
Thus, it is nec-essary to keep the size of the conversation reason-ably small, and we restrict it to be at most 8 utter-ances.4 Experiment4.1 SettingWe first built our baseline model following the pro-cedure proposed by Ritter et al (2011).
In accor-dance with the paper, we also filtered out the phrasetable by Fisher?s Exact Test.
We then implementedour model using Moses (Koehn et al, 2007) toolkitwith KenLM (Heafield, 2011) as the language modelin 5-gram setting.
In accordance with the baseline,we built our training, tuning, and test data set from1347Model A Model B A>B A=B A<B p-value AgreementPairBaseline 287 32 81 4.0e-28 .488Actual 58 28 314 1.2e-43 .543Token 175 52 173 0.96 .373TokenBaseline 280 35 85 2.0e-25 .462Actual 62 35 303 3.2e-39 .529Table 1: Performances against Each ModelTwitter, except we collected conversations, consist-ing of a tweet and successive replies, rather thanpairs of tweets.
We also restricted each conversa-tion to have 3 to 8 utterances with only two speakerstaking turns, to make it more likely that the topic ofthe conversation is preserved.
Although there weresome cases in which the topic deviated, our valida-tion of the dataset showed that the amount of suchcases was negligible.
We ended up having approxi-mately 1.4M pairs of utterances in the training data,which constitute 425,547 conversations.
The thresh-old p-value for Fisher?s Exact Test was set to 0.0001,to well-balance the number of selected words withthe lengths of the utterances.4.2 EvaluationOne of the challenging aspects of the researches onconversation is its distinct nature in which there isan extremely wide range of acceptable candidate re-sponses to a stimulus, unlike usual bilingual trans-lation tasks where there are typically pre-set candi-dates to be referenced with high reliability.
Usingthe automatic evaluation metrics, we obtained slightimprovements; for example, BLEU score (Papineniet al, 2002), with the actual responses from Twitteras the gold standard, increased from 0.82 for base-line to 0.89 for the pair-based approach.
For theabove-mentioned reason, however, we found it du-bious whether a higher score in these metrics cor-responds to better responses, and we thus resort tohuman manual evaluation as our primary source ofevaluation.We performed a human evaluation on AmazonMechanical Turk (Buhrmester et al, 2011).
Theevaluation task consisted of four different sets of100 questions, each set of which was handled by10 workers.
Each question was a ranking task, andthe workers were shown a part of conversation andwere instructed to rank the responses that followedModel 1st 2nd 3rd 4th Avg.
RankActual .664 .129 .087 .120 1.66Baseline .092 .232 .271 .406 2.99Pair .112 .323 .346 .220 2.67Token .134 .315 .297 .255 2.67Table 2: Rankings from Human Evaluationthe conversation in consideration of their relevanceto the topic of the conversation.
For all questions,workers were given four responses; the actual re-sponse from Twitter, one generated by the baselinemodel, and two by each of our context-dependentmodels.The order of responses was randomized for eachquestion.
In addition, in order to filter out the work-ers who do not take the tasks seriously, generatingnoise answers, we selected 10 questions that hadobvious answers, and rejected the answers by theworkers who failed to achieve 70% or higher accu-racy on those questions.
As stated in Section 3.1, thethreshold for length of source sentence to determinewhether to add words or not was set to the averagelength of source sentences throughout the trainingdata, which in our case was 10.
In roughly half of400 questions, no words were added to the sourcesentence, and 1 to 6 words were added for roughly25 to 30 questions respectively.
Beyond 6 words, thenumber of questions begins to decline.4.3 ResultsTable 1 shows how our models performed againstthe actual responses, the baseline model, and eachother, in regards to the number of questions forwhich our models were ranked higher.
Overall, thetable shows that our models were preferred over thebaseline model, but performed poorly against theactual responses as expected.
Yet, it was able toperform better than the actual responses in roughly1348Conversation Response?Avg.
RankA: if youre following today maybe follow meB: lol ur off to an early start1: im at the bus stop how was your night 2.12: i dont fancy hame pass for 3.63: lmao i got my second home 3.24: updating haha I get u off too 1.9A: like you?re talking about the stupidest things ever.its annoyingB: who is this about1: the ppl behind you 1.92: I want tie you 3.63: like I said Im talking seriously are you 2.34: one of them is that you 2.9A: Aww man happy birthday bro!!
Lol you know yougotta die right?B: What?
Lol1: you gotta damn near die from drinking today 1.22: lol yea 3.83: thank you man you know me 2.44: lol I know I know 2.8* 1 is the actual response on Twitter, while 2,3 and 4 are responses generated by the baseline, pair-based, token-basedmodels respectively.Table 3: Examples of Responses15% of the questions, especially when the actual re-sponses were grammatically poor, or irrelevant tothe topic of the conversation.
There was no sig-nificant difference between the performances of ourmodels.
It also shows the p-value and mutual agree-ment between two models.
Using S coefficient(Bennett et al, 1954) as a measurement of agree-ment yields the following result.
Most of them fallinto ?moderate agreement?
range of 0.4 to 0.6, ex-cept Token-based model against Pair-based model isslightly lower and falls into ?fair agreement?
range(Landis and Koch, 1977).Table 2 shows the distribution of each model overeach ranking and their average rankings.
Our mod-els outperform the baseline model in higher rank-ings.
Table 3 features examples of responses gen-erated by each model and the actual responses onTwitter, along with their average ranking in the finalevaluation.
In the first conversation, one of our mod-els was ranked higher than both the baseline modeland the actual response.
In other conversations, ourmodels were ranked higher than the baseline model,but lower than the actual response.
Generally, ourmodels have a wider range of topic-relevant vocab-ularies, and sound comparatively coherent than thebaseline model, without too much grammatical vio-lations.5 Conclusion and Future WorkAs we observed in the experimental results, ourcontext-dependent model outperformed the baselinemodel when examined in a wider scope of conver-sations.
Although its performance against the actualresponses was not as satisfactory, it could outper-form them when the actual responses diverted fromthe topic, or had poor coherence and grammaticality.Possible applications include chatterbots or con-versational agents.
Most such applications are basedon one-turn conversation, where user says some-thing, system gives some response, and that is tech-nically the end of the conversation of current topic,which will not be referred to in later conversations.Our work can, for example, provide the system withpossible topics to talk about, especially when the in-put from the user is short or trivial.
Diversity of theresponses is obtained because, even when the systemis given the same input, it will return completely dif-ferent responses depending on what was previouslytalked about, as opposed to the applications wherecertain responses can be expected given an input.An improvement is likely to come from attempt-ing different methods to extract the core tokens fromthe past utterances.
We relied on the Fisher?s Ex-act Test and tf-idf throughout the research, but otherapproaches may perform better.
Alternatively, wemay try different weighting systems depending onwhether a token is from the same speaker as the cur-rent utterance or a different speaker, since it wouldgenerally make more sense for a particular speakernot to repeat him/herself.1349ReferencesArianna Bisazza, Nick Ruiz, and Marcello Federico.2011.
Fill-up versus Interpolation Methods forPhrase-based SMT Adaptation.
In InternationalWorkshop on Spoken Language Translation, pages136?143.E.
M. Bennett, R. Alpert, and A.C. Goldstein 1954.Communications through limited-response question-ing.
In Public Opinion Quarterly, pages 303?308.Peter Brown, Stephen A. Della Pietra, Vincent J. DellaPiera, and Robert J. Mercer.
1993.
The Mathematicsof Statistical Machine Translation: Parameter Estima-tion.
In Computational Linguistics, pages 263?311.Michael Burhmester, Tracy Kwang, and Samuel D.Gosling 2011.
Amazon?s Mechanical Turk: A NewSource of Inexpensive, Yet High-Quality Data?
In Per-spectives on Psychological Science, 6, pages 3?5.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proceedeings of theSixth Workshop on Statistical Machine Trnaslation,pages 187?197.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Pro-ceedings of the Association for Computational Lin-guistics, pages 177?180.J.
R. Landis and G. G. Koch.
1977.
The Measurementof Observer Agreement for Categorical Data.
Biomet-rics, Vol.
33, No.
1, pages 159?174.Robert C. Moore.
2004.
On Log-Likelihood Ratios andthe Significance of Rare Events.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing, pages 333?340.Franz J. Och, and Hermann Ney.
2000.
A Comparisonof Alignment Models for Statistical Machine Transla-tion.
In the 18th International Conference on Compu-tational Linguistics, pages 1086?1090.Alice H. Oh, and Alexander I. Rudnicky.
2002.
Stochas-tic Natural Language Generation for Spoken DialogueSystems.
In Computer Speech & Language, pages387?407.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedings ofthe Association for Computational Linguistics, pages311?318.Alan Ritter, Colin Cherry, and William B. Dolan.
2011.Data-Driven Response Generation in Social Media.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing, pages 583?593.1350
