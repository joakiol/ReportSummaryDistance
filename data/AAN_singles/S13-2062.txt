Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 380?383, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsuOttawa: System description for SemEval 2013 Task 2 SentimentAnalysis in TwitterHamid Poursepanj, Josh Weissbock, and Diana InkpenSchool of Electrical Engineering and Computer ScienceUniversity of OttawaOttawa, K1N6N5, Canada{hpour099, jweis035, Diana.Inkpen}@uottawa.caAbstractWe present two systems developed at the Uni-versity of Ottawa for the SemEval 2013 Task 2.The first system (for Task A) classifies the po-larity / sentiment orientation of one target wordin a Twitter message.
The second system (forTask B) classifies the polarity of whole Twittermessages.
Our two systems are very simple,based on supervised classifiers with bag-of-words feature representation, enriched with in-formation from several sources.
We present afew additional results, besides results of thesubmitted runs.1 IntroductionThe Semeval 2013 Task 2 focused on classifyingTwitter messages (?tweets?)
as expressing a posi-tive opinion, a negative opinion, a neutral opinion,or no opinion (objective).
In fact, the neutral andobjective were joined in one class for the require-ments of the shared task.
Task A contained targetwords whose sense had to be classified in the con-text, while Task B was to classify each text intoone of the three classes: positive, negative, andneutral/objective.
The training data that was madeavailable for each task consisted in annotatedTwitter message.
There were two test sets for eachtask, one composed of Twitter messages and oneof SMS message (even if there was no specifictraining data for SMS messages).
See more detailsabout the datasets in (Wilson et al 2013).2 System DescriptionWe used supervised learning classifiers from We-ka (Witten and Frank, 2005).
Initially we extractedsimple bag-of-word features (BOW).
For the sub-mitted systems, we also used features calculatedbased on SentiWordNet information (Baccianellaet al 2010).
SentiWordNet contains positivity,negativity, and objectivity scores for each sense ofa word.
We explain below how this informationwas used for each task.As classifiers, we used Support Vector Ma-chines (SVM) (SMO and libSVM from Weka withdefault values for parameters), because SVM isknown to perform well on many tasks, and Multi-nomial Naive Bayes (MNB), because MNB isknown to perform well on text data and it is fasterthan SVM.2.1 Task AOur system for Task A involved two parts: theexpansion of our training data and the classifica-tion.
The expansion was done with informationfrom SentiWordNet.
Stop words and words thatappeared only once in the training data were fil-tered out.
Then the classification was completedwith algorithms from Weka.As mentioned, the first task was to expand all ofthe tweets that were provided as training data.
Thiswas doing using Python and the Python NLTKlibrary, as well as SentiWordNet.
SentiWordNetprovides a score of the sentient state for each word(for each sense, in case the word has more than380one sense).
As an example, the word ?want?
canmean ?a state of extreme poverty?
with the Senti-WordNet score of (Positive: 0 Objective:0.75 Negative: 0.25).
The same word could alsomean ?a specific feeling of desire?
with a score of(Positive: 0.5 Objective: 0.5 Negative: 0).
We alsoused for expansion the definitions and synonymsof each word sense, from WordNet.The tweets in the training data are labeled withtheir sentiment type (Positive, Negative, Objectiveand Neutral).
Neutral and Objective are treated thesame.
The provided training data has the targetword marked, and also the sentiment orientation ofthe word in the context of the tweeter message.These target words were the ones expanded by ourmethod.
When the target was a multi-word expres-sion, if the expression was found in WordNet, thenthe expansion was done directly; if not, each wordwas expanded in a similar fashion and concatenat-ed to the original tweet.
These target words werelooked up in SentiWordNet and matched with thedefinition that had the highest score that alsomatched their sentiment label in the training data.Original Tweet The great Noel Gallagher is about tohit the stage in St. Paul.
Plenty ofroom here so we're 4th row center.Plenty of room.
Pretty fired upKey Words GreatSentiment PositiveDefinition very good; "he did a bully job"; "aneat sports car"; "had a great time atthe party"; "you look simply smash-ing"Synonyms Swell, smashing, slap-up, peachy,not_bad, nifty, neat, keen, groovy,dandy, cracking, corking, bully,bang-upExpandedTweetThe great Noel Gallagher is about tohit the stage in St. Paul.
Plenty ofroom here so were 4th row center.Plenty of room.
Pretty fired up  swellsmashing slap-up peachy not_badnifty neat keen groovy dandy crack-ing corking bully bang-up very goodhe did a bully job a neat sports carhad a great time at the party you looksimply smashingTable 1: Example of tweet expansion for Task AThe target word?s definition and synonyms were thenconcatenated to the original tweet.
No additionalchanges were made to either the original tweet or thefeatures that were added from SentiWordNet.
An ex-ample follows in Table 1.
The test data (Twitter andSMS) was not expanded, because there are no labels inthe test data to be able to choose the sense with corre-sponding sentiment.2.2 Task BFor this task, we used the following resources:SentiwordNet (Baccianella et al2010), the Polari-ty Lexicon (Wilson et al 2005), the General In-quirer (Stone et al 1966), and the Stanford NLPtools (Toutanova et al 2003) for preprocessingand feature selection.
The preprocessing of Twittermessages is implemented in three steps namely,stop-word removal, stemming, and removal ofwords with occurrence frequency of one.
Severalextra features will be used: the number of positivewords and negative words identified by three lexi-cal resources mentioned above, the number ofemoticons, the number of elongated words, and thenumber of punctuation tokens (single or repeatedexclamation marks, etc.).
As for SentiWordNet,for each word a score is calculated that shows thepositive or negative weight of that word.
No sensedisambiguation is done (the first sense is used), butthe scores are used for the right part-of-speech (incase a word has more than one possible part-of-speech).
Part-of-Speech tagging was done with theStanford NLP Tools.
As for General Inquirer andPolarity Lexicon, we simply used the list positiveand negative words from these resources in orderto count how many positive and how many nega-tive terms appear in a message.3 Results3.1 Task AFor classification, we first trained on our expandedtraining data using 10-fold cross-validation andusing the SVM (libSVM) and Multinomial Na-iveBayes classifiers from Weka, using their defaultsettings.
The training data was represented as abag of words (BOW).
These classifiers were cho-sen as they have given us good results in the pastfor text classification.
The classifiers were runwith 10-fold cross-validation.
See Table 2 for the381results.
Without expanding the tweets, the accura-cy of the SVM classifier was equal to the baselineof classifying everything into the most frequentclass, which was ?positive?
in the training data.For MNB, the results were lower than the baseline.After expanding the tweets, the accuracy increasedto 73% for SVM and to 80.36% for MNB.
Weconcluded that MNB works better for Task A. Thisis why the submitted runs used the MNB modelthat was created from the expanded training data.Then we used this to classify the Twitter and SMStest data.
The average F-score for the positive andthe negative class for our submitted runs can beseen in Table 3, compared to the other systemsthat participated in the task.
We report this meas-ure because it was the official evaluation measureused in the task.System SVM MNBBaseline 66.32% 66.32%BOW features 66.32% 33.23%BOW+ text expansion 73.00% 80.36%Table 2: Accuracy results for task A by 10-fold cross-validation on the training dataSystem Tweets SMSuOttawa system 0.6020 0.5589Median system 0.7489 0.7283Best system 0.8893 0.8837Table 3:  Results for Task A for the submitted runs(Average F-score for positive/negative class)The precision, recall and F-score on the Twitterand SMS test data for our submitted runs can beseen in Tables 4 and 5, respectively.
All our sub-mitted runs were for the ?constrained?
task; noadditional training data was used.Class Precision Recall F-ScorePositive 0.6934 0.7659 0.7278Negative 0.5371 0.4276 0.4762Neutral 0.0585 0.0688 0.0632Table 4: Results for Tweet test data for Task A, foreach class.Class Precision Recall F-ScorePositive 0.5606 0.5705 0.5655Negative 0.5998 0.5118 0.5523Neutral 0.1159 0.2201 0.1518Table 5: Results for SMS test data for Task A, for eachclass.3.2 Task BFirst we present results on the training data (10-fold cross-validation), then we present the resultsfor the submitted runs (also without any additionaltraining data).Table 6 shows the overall accuracy for BOWfeatures for two classifiers, evaluated based on 10-fold cross validation on the training data, for twoclassifiers: SVM (SMO in Weka) and Multidimen-sional Na?ve Bays (MNB in Weka).
The BOWplus SentiWordNet features also include the num-ber of positive and negative words identified fromSentiWordNet.
The BOW plus extra features rep-resentation includes the number of positive andnegative words identified from SentiWordNet,General Inquirer, and Polarity Lexicon (six extrafeatures).
The last row of the table shows the over-all accuracy for BOW features plus all the extrafeatures mentioned in Section 2.2, including in-formation extracted from SentiWordNet, PolarityLexicon, and General Inquirer.
We can see that theSentiWordNet features help, and that when includ-ing all the extra features, the results improve evenmore.
We noticed that the features from the Polari-ty Lexicon contributed the most.
When we re-moved GI, the accuracy did not change much; webelieve this is because GI has too small coverage.System SVM MNBBaseline 48.50% 48.50%BOW features 58.75% 59.56%BOW+ SentiWordNet 69.43% 63.30%BOW+ extra features 82.42% 73.09%Table 6: Accuracy results for task B by 10-fold cross-validation on the training dataThe baseline in Table 6 is the accuracy of a triv-ial classifier that puts everything in the most fre-quent class, which is neutral/objective for thetraining data (ZeroR classifier in Weka).382The results of the submitted runs are in Table 7for the two data sets.
The features representationwas BOW plus SentiWordNet information.
Theofficial evaluation measure is reported (average F-score for the positive and negative class).
The de-tailed results for each class are presented in Tables8 and 9.In Table 7, we added an extra row for a newuOttawa system (SVM with BOW plus extra fea-tures) that uses the best classifier that we designed(as chosen based on the experiments on the train-ing data, see Table 6).
This classifier uses SVMwith BOW and all the extra features.System Tweets SMSuOttawa submittedsystem0.4251 0.4051uOttawa new system 0.8684 0.9140Median system 0.5150 0.4523Best system 0.6902 0.6846Table 7:  Results for Task B for the submitted runs(Average F-score for positive/negative).Class Precision Recall F-scorePositive 0.6206 0.5089 0.5592Negative 0.4845 0.2080 0.2910Neutral 0.5357 0.7402 0.6216Table 8: Results for each class for task B, for the sub-mitted system (SVM with BOW plus SentiWordNetfeatures) for the Twitter test data.Class Precision Recall F-scorePositive 0.4822 0.5508 0.5142Negative 0.5643 0.2005 0.2959Neutral 0.6932 0.7988 0.7423Table 9: Results for each class for task B, for the sub-mitted system (SVM with BOW plus SentiWordNetfeatures) for the SMS test data.4 Conclusions and Future WorkIn Task A, we expanded upon the Twitter messag-es from the training data using their keyword?sdefinition and synonyms from SentiWordNet.
Weshowed that the expansion helped improve theclassification performance.
In future work, wewould like to try an SVM using asymmetric soft-boundaries to try and penalize the classifier formissing items in the neutral class, the class withthe least items in the Task A training data.The overall accuracy of the classifiers for TaskB increased a lot when we introduced the extrafeatures discussed in section 2.2.
The overall accu-racy of SVM increased from 58.75% to 82.42%(as measures by cross-validation on the trainingdata).
When applying this classifier on the two testdata sets, the results were very surprisingly good(even higher that the best system submitted by theSemEval participants for Task B1).ReferencesStefano Baccianella, Andrea Esuli and Fabrizio Sebas-tiani.
SentiWordNet 3.0: An Enhanced Lexical Re-source for Sentiment Analysis and Opinion Mining.In Proceedings of the Seventh International Confer-ence on Language Resources and Evaluation(LREC'10), Valletta, Malta, May 2010.Philip J.
Stone, Dexter C. Dunphy, Marshall S. Smith,and Daniel M. Ogilvie.
The General Inquirer: Acomputer approach to content analysis.
MIT Press,1966.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
Feature-Rich Part-of-SpeechTagging with a Cyclic Dependency Network.
In Pro-ceedings of HLT-NAACL 2003, pp.
252-259, 2003.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,Sara Rosenthal, Veselin Stoyanov and Alan Ritter.SemEval-2013 Task 2: Sentiment Analysis in Twit-ter.
In Proceedings of the International Workshop onSemantic Evaluation SemEval '13, Atlanta, Georgia,June 2013.Theresa Wilson, Janyce Wiebe and Paul Hoffmann.Recognizing contextual polarity in phrase- level sen-timent analysis.
In Proceedings of HLT/ EMNLP2005.Ian H. Witten and Eibe Frank.
Data Mining: PracticalMachine Learning Tools and Techniques, 2nd edi-tion, Morgan Kaufmann, San Francisco, 2005.1 Computed with the provided scoring script.383
