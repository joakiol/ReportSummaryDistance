Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 30?38,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsEfficient Linearization of Tree Kernel FunctionsDaniele PighinFBK-Irst, HLTVia di Sommarive, 18 I-38100 Povo (TN) Italypighin@fbk.euAlessandro MoschittiUniversity of Trento, DISIVia di Sommarive, 14 I-38100 Povo (TN) Italymoschitti@disi.unitn.itAbstractThe combination of Support Vector Machineswith very high dimensional kernels, such asstring or tree kernels, suffers from two ma-jor drawbacks: first, the implicit representa-tion of feature spaces does not allow us to un-derstand which features actually triggered thegeneralization; second, the resulting compu-tational burden may in some cases render un-feasible to use large data sets for training.
Wepropose an approach based on feature spacereverse engineering to tackle both problems.Our experiments with Tree Kernels on a Se-mantic Role Labeling data set show that theproposed approach can drastically reduce thecomputational footprint while yielding almostunaffected accuracy.1 IntroductionThe use of Support Vector Machines (SVMs)in supervised learning frameworks is spreadingacross different communities, including Computa-tional Linguistics and Natural Language Processing,thanks to their solid mathematical foundations, ef-ficiency and accuracy.
Another important reasonfor their success is the possibility of using kernelfunctions to implicitly represent examples in somehigh dimensional kernel space, where their similar-ity is evaluated.
Kernel functions can generate a verylarge number of features, which are then weightedby the SVM optimization algorithm obtaining a fea-ture selection side-effect.
Indeed, the weights en-coded by the gradient of the separating hyperplanelearnt by the SVM implicitly establish a ranking be-tween features in the kernel space.
This property hasbeen exploited in feature selection models based onapproximations or transformations of the gradient,e.g.
(Rakotomamonjy, 2003), (Weston et al, 2003)or (Kudo and Matsumoto, 2003).However, kernel based systems have two majordrawbacks: first, new features may be discoveredin the implicit space but they cannot be directly ob-served.
Second, since learning is carried out in thedual space, it is not possible to use the faster SVM orperceptron algorithms optimized for linear spaces.Consequently, the processing of large data sets canbe computationally very expensive, limiting the useof large amounts of data for our research or applica-tions.We propose an approach that tries to fill in thegap between explicit and implicit feature represen-tations by 1) selecting the most relevant features inaccordance with the weights estimated by the SVMand 2) using these features to build an explicit rep-resentation of the kernel space.
The most innovativeaspect of our work is the attempt to model and im-plement a solution in the context of structural ker-nels.
In particular we focus on Tree Kernel (TK)functions, which are especially interesting for theComputational Linguistics community as they caneffectively encode rich syntactic data into a kernel-based learning algorithm.
The high dimensionalityof a TK feature space poses interesting challenges interms of computational complexity that we need toaddress in order to come up with a viable solution.We will present a number of experiments carriedout in the context of Semantic Role Labeling, show-ing that our approach can noticeably reduce trainingtime while yielding almost unaffected classificationaccuracy, thus allowing us to handle larger data setsat a reasonable computational cost.The rest of the paper is structured as follows: Sec-30Fragment spaceAB AAB AB AAB ACAB AB ACACDB ADB AC1 2 3 4 5 6 7T1AB AB ACT2DB AC?
(T1) = [2, 1, 1, 1, 1, 0, 0]?
(T2) = [0, 0, 0, 0, 1, 1, 1]K(T1, T2) = ??
(T1), ?(T2)?
= 1Figure 1: Esemplification of a fragment space and thekernel product between two trees.tion 2 will briefly review SVMs and Tree Kernelfunctions; Section 3 will detail our proposal for thelinearization of a TK feature space; Section 4 willreview previous work on related subjects; Section 5will describe our experiments and comment on theirresults; finally, in Section 6 we will draw our con-clusions.2 Tree Kernel FunctionsThe decision function of an SVM is:f(~x) = ~w ?
~x+ b =n?i=1?iyi ~xi ?
~x+ b (1)where ~x is a classifying example and ~w and b arethe separating hyperplane?s gradient and its bias,respectively.
The gradient is a linear combinationof the training points ~xi, their labels yi and theirweights ?i.
These and the bias are optimized attraining time by the learning algorithm.
Applyingthe so-called kernel trick it is possible to replace thescalar product with a kernel function defined overpairs of objects:f(o) =n?i=1?iyik(oi, o) + bwith the advantage that we do not need to providean explicit mapping ?(?)
of our examples in a vectorspace.A Tree Kernel function is a convolution ker-nel (Haussler, 1999) defined over pairs of trees.Practically speaking, the kernel between two treesevaluates the number of substructures (or fragments)they have in common, i.e.
it is a measure of theiroverlap.
The function can be computed recursivelyin closed form, and quite efficient implementationsare available (Moschitti, 2006).
Different TK func-tions are characterized by alternative fragment defi-nitions, e.g.
(Collins and Duffy, 2002) and (Kashimaand Koyanagi, 2002).
In the context of this paperwe will be focusing on the SubSet Tree (SST) ker-nel described in (Collins and Duffy, 2002), whichrelies on a fragment definition that does not allow tobreak production rules (i.e.
if any child of a node isincluded in a fragment, then also all the other chil-dren have to).
As such, it is especially indicated fortasks involving constituency parsed texts.Implicitly, a TK function establishes a correspon-dence between distinct fragments and dimensions insome fragment space, i.e.
the space of all the pos-sible fragments.
To simplify, a tree t can be repre-sented as a vector whose attributes count the occur-rences of each fragment within the tree.
The ker-nel between two trees is then equivalent to the scalarproduct between pairs of such vectors, as exempli-fied in Figure 1.3 Mining the Fragment SpaceIf we were able to efficiently mine and store in adictionary all the fragments encoded in a model,we would be able to represent our objects explicitlyand use these representations to train larger modelsand very quick and accurate classifiers.
What weneed to devise are strategies to make this approachconvenient in terms of computational requirements,while yielding an accuracy comparable with directtree kernel usage.Our framework defines five distinct activities,which are detailed in the following paragraphs.Fragment Space Learning (FSL) First of all, wecan partition our training data into S smaller sets,and use the SVM and the SST kernel to learn S mod-els.
We will use the estimated weights to drive ourfeature selection process.
Since the time complexityof SVM training is approximately quadratic in thenumber of examples, this way we can considerablyaccelerate the process of estimating support vectorweights.According to statistical learning theory, beingtrained on smaller subsets of the available datathese models will be less robust with respect to the31minimization of the empirical risk (Vapnik, 1998).Nonetheless, since we do not need to employ themfor classification (but just to direct our feature se-lection process, as we will describe shortly), we canaccept to rely on sub-optimal weights.
Furthermore,research results in the field of SVM parallelizationusing cascades of SVMs (Graf et al, 2004) suggestthat support vectors collected from locally learntmodels can encode many of the relevant features re-tained by models learnt globally.
Henceforth, letMsbe the model associated with the s-th split, and Fsthe fragment space that can describe all the trees inMs.Fragment Mining and Indexing (FMI) In Equa-tion 1 it is possible to isolate the gradient ~w =?ni=1 ?iyi ~xi, with ~xi = [x(1)i , .
.
.
, x(N)i ], N beingthe dimensionality of the feature space.
For a treekernel function, we can rewrite x(j)i as:x(j)i = ti,j?`(fj)?ti?= ti,j?`(fj)?
?Nk=1(ti,k?`(fk))2(2)where: ti,j is the number of occurrences of the frag-ment fj , associated with the j-th dimension of thefeature space, in the tree ti; ?
is the kernel decayfactor; and `(fj) is the depth of the fragment.The relevance |w(j)| of the fragment fj can bemeasured as:|w(j)| =?????n?i=1?iyix(j)i?????
.
(3)We fix a threshold L and from each model Ms(learnt during FSL) we select the L most relevantfragments, i.e.
we build the set Fs,L = ?k{fk} sothat:|Fs,L| = L and |w(k)| ?
|w(i)|?fi ?
F \ Fs,L .In order to do so, we need to harvest all the frag-ments with a fast extraction function, store them ina compact data structure and finally select the frag-ments with the highest relevance.
Our strategy is ex-emplified in Figure 2.
First, we represent each frag-ment as a sequence as described in (Zaki, 2002).
Asequence contains the labels of the fragment nodesin depth-first order.
By default, each node is thechild of the previous node in the sequence.
A spe-cial symbol (?)
indicates that the next node in theR1A BZ WR2X Y BZ WBZ Wweight: w1 weight: w2 weight: w3R1, A, ?, B, Z, ?, WR2, X, ?, Y, ?, B, Z, ?, WB, Z, ?, WR1w1A?
WX R2w2ZYBw31,11,22,11,11,1 1,-1,11,33,11,212Figure 2: Fragment indexing.
Each fragment is repre-sented as a sequence 1 and then encoded as a path in theindex 2 which keeps track of its cumulative relevance.sequence should be attached after climbing one levelin the tree.
For example, the tree (B (Z W)) in figureis represented as the sequence [B, Z, ?, W].
Then, weadd the elements of the sequence to a graph (whichwe call an index of fragments) where each sequencebecomes a path.
The nodes of the index are the la-bels of the fragment nodes, and each arc is associ-ated with a pair of values ?d, n?
: d is a node identi-fier, which is unique with respect to the source node;n is the identifier of the arc that must be selected atthe destination node in order to follow the path as-sociated with the sequence.
Index nodes associatedwith a fragment root also have a field where the cu-mulative relevance of the fragment is stored.As an example, the index node labeled B in fig-ure has an associated weight of w3, thus identify-ing the root of a fragment.
Each outgoing edgeunivocally identifies an indexed fragment.
In thiscase, the only outgoing edge is labeled with the pair?d = 1, n = 1?, meaning that we should follow itto the next node, i.e.
Z, and there select the edge la-beled 1, as indicated by n. The edge with d = 1 in Zis ?d = 1, n = 1?, so we browse to ?
where we se-lect the edge ?d = 1, n = ??.
The missing value forn tells us that the next node, W, is the last elementof the sequence.
The complete sequence is then [B,Z, ?, W], which encodes the fragment (B (Z W)).The index implementation has been optimized forfast insertions and has the following features: 1)each node label is represented exactly once; 2) eachdistinct sequence tail is represented exactly once.The union of all the fragments harvested from eachmodel is then saved into a dictionary DL which willbe used by the next stage.To mine the fragments, we apply to each tree ineach model the algorithm shown in Algorithm 3.1.In this context, we call fragment expansion the pro-32Algorithm 3.1: MINE TREE(tree)global maxdepth,maxexpmainmined?
?
; indexed?
?
; MINE(FRAG(tree), 0)procedure MINE(frag, depth)if frag ?
indexedthen returnindexed?
indexed ?
{frag}INDEX(frag)for each node ?
TO EXPAND(frag)do??
?if node 6?
minedthen{mined?
mined ?
{node}MINE(FRAG(node), 0)if depth < maxdepththen{for each fragment ?
EXPAND(frag,maxexp)do MINE(fragment, depth+ 1)cess by which tree nodes are included in a frag-ment.
Fragment expansion is achieved via node ex-pansions, where expanding a node means includ-ing its direct children in the fragment.
The func-tion FRAG(n) builds the basic fragment rooted in agiven node n, i.e.
the fragment consisting only of nand its direct children.
The function TO EXPAND(f)returns the set of nodes in a fragment f that canbe expanded (i.e.
internal nodes in the origin tree),while the function EXPAND(f,maxexp) returns allthe possible expansions of a fragment f .
The pa-rameter maxexp is a limit to the number of nodesthat can be expanded at the same time when a newfragment is generated, while maxdepth sets a limiton the number of times that a base fragment can beexpanded.
The function INDEX(f) adds the frag-ment f to the index.
To keep the notation simple,here we assume that a fragment f contains all thenecessary information to calculate its relevance (i.e.the weight, label and norm of the support vector ?i,yi, and ?ti?, the depth of the fragment `(f) and thedecay factor ?, see equations 2 and 3).Performing in a different order the same node ex-pansions on the same fragment f results in the samefragment f ?.
To prevent the algorithm from enteringcircular loops, we use the set indexed so that thevery same fragment in each tree cannot be exploredmore than once.
Similarly, the mined set is usedso that the base fragment rooted in a given node isconsidered only once.Tree Fragment Extraction (TFX) During thisphase, a data file encoding label-tree pairs ?yi, ti?
isSNPNNPMaryVPVBboughtNPDaNNcat(A1)(A0)?VPVB-PboughtNPD-BaVPVB-PboughtNP-BDaNNcat-1: BC +1: BC,A1-1: A0,A2,A3,A4,A5Figure 3: Examples of ASTm structured features.transformed to encode label-vector pairs ?yi, ~vi?.
Todo so, we generate the fragment space of ti, usinga variant of the mining algorithm described in Fig-ure 3.1, and encode in ~vi all and only the fragmentsti,j so that ti,j ?
DL, i.e.
we perform feature extrac-tion based on the indexed fragments.
The process isapplied to the whole training and test sets.
The al-gorithm exploits labels and production rules foundin the fragments listed in the dictionary to generateonly the fragments that may be in the dictionary.
Forexample, if the dictionary does not contain a frag-ment whose root is labeled N , then if a node N isencountered during TFX neither its base fragmentnor its expansions are generated.Explicit Space Learning (ESL) After linearizingthe training data, we can learn a very fast model byusing all the available data and a linear kernel.
Thefragment space is now explicit, as there is a mappingbetween the input vectors and the fragments they en-code.Explicit Space Classification (ESC) After learn-ing the linear model, we can classify our linearizedtest data and evaluate the accuracy of the resultingclassifier.4 Previous workA rather comprehensive overview of feature selec-tion techniques is carried out in (Guyon and Elis-seeff, 2003).
Non-filter approaches for SVMs andkernel machines are often concerned with polyno-mial and Gaussian kernels, e.g.
(Weston et al, 2001)and (Neumann et al, 2005).
Weston et al (2003) usethe `0 norm in the SVM optimizer to stress the fea-ture selection capabilities of the learning algorithm.In (Kudo and Matsumoto, 2003), an extension of thePrefixSpan algorithm (Pei et al, 2001) is used to ef-ficiently mine the features in a low degree polyno-mial kernel space.
The authors discuss an approx-imation of their method that allows them to handlehigh degree polynomial kernels.33Data set Non-linearized classifiers Linearized classifiers (Thr=10k)Task Pos Neg Train Test P R F1 Train Test P R F1A0 60,900 118,191 521 7 90.26 92.95 91.59 209 3 88.95 91.91 90.40A1 90,636 88,455 1,206 11 89.45 88.62 89.03 376 3 89.39 88.13 88.76A2 21,291 157,800 692 7 84.56 64.42 73.13 248 3 81.23 68.29 74.20A3 3,481 175,610 127 2 97.67 40.00 56.76 114 3 97.56 38.10 54.79A4 2,713 176,378 47 1 92.68 55.07 69.10 92 2 95.00 55.07 69.72A5 69 179,022 3 0 100.00 50.00 66.67 63 2 100.00 50.00 66.67BC 61,062 938,938 3,059 247 82.57 80.96 81.76 916 39 83.36 78.95 81.10RM - - 2,596 27 89.37 86.00 87.65 1,090 16 88.50 85.81 87.13Table 1: Accuracy (P, R, F1), training (Train) and test (Test) time of non-linearized (center) and linearized (right)classifiers.
Times are in minutes.
For each task, columns Pos and Neg list the number of positive and negative trainingexamples, respectively.
The accuracy of the role multiclassifiers is the micro-average of the individual classifierstrained to recognize core PropBank roles.Suzuki and Isozaki (2005) present an embeddedapproach to feature selection for convolution ker-nels based on ?2-driven relevance assessment.
Toour knowledge, this is the only published workclearly focusing on feature selection for tree ker-nel functions.
In (Graf et al, 2004), an approachto SVM parallelization is presented which is basedon a divide-et-impera strategy to reduce optimiza-tion time.
The idea of using a compact graph rep-resentation to represent the support vectors of a TKfunction is explored in (Aiolli et al, 2006), where aDirect Acyclic Graph (DAG) is employed.Concerning the use of kernels for NLP, inter-esting models and results are described, for exam-ple, in (Collins and Duffy, 2002), (Moschitti et al,2008), (Kudo and Matsumoto, 2003), (Cumby andRoth, 2003), (Shen et al, 2003), (Cancedda et al,2003), (Culotta and Sorensen, 2004), (Daume?
IIIand Marcu, 2004), (Kazama and Torisawa, 2005),(Kudo et al, 2005), (Titov and Henderson, 2006),(Moschitti et al, 2006), (Moschitti and Bejan, 2004)or (Toutanova et al, 2004).5 ExperimentsWe tested our model on a Semantic Role La-beling (SRL) benchmark, using PropBank annota-tions (Palmer et al, 2005) and automatic Charniakparse trees (Charniak, 2000) as provided for theCoNLL 2005 evaluation campaign (Carreras andMa`rquez, 2005).
SRL can be decomposed intotwo tasks: boundary detection, where the word se-quences that are arguments of a predicate word ware identified, and role classification, where each ar-gument is assigned the proper role.
The former taskrequires a binary Boundary Classifier (BC), whereasthe second involves a Role Multi-class Classifier(RM).Setup.
If the constituency parse tree t of a sen-tence s is available, we can look at all the pairs?p, ni?, where ni is any node in the tree and p isthe node dominating w, and decide whether ni is anargument node or not, i.e.
whether it exactly dom-inates all and only the words encoding any of w?sarguments.
The objects that we classify are sub-sets of the input parse tree that encompass both pand ni.
Namely, we use the ASTm structure definedin (Moschitti et al, 2008), which is the minimal treethat covers all and only the words of p and ni.
Inthe ASTm, p and ni are marked so that they can bedistinguished from the other nodes.
An ASTm isregarded as a positive example for BC if ni is an ar-gument node, otherwise it is considered a negativeexample.
Positive BC examples can be used to trainan efficient RM: for each role r we can train a clas-sifier whose positive examples are argument nodeswhose label is exactly r, whereas negative examplesare argument nodes labeled r?
6= r. Two ASTmsextracted from an example parse tree are shown inFigure 3: the first structure is a negative example forBC and is not part of the data set of RM, whereasthe second is a positive instance for BC and A1.To train BC we used PropBank sections 1 through6, extracting ASTm structures out of the first 1 mil-lion ?p, ni?
pairs from the corresponding parse trees.As a test set we used the 149,140 instance collectedfrom the annotations in Section 24.
There are 61,062positive examples in the training set (i.e.
6.1%) and8,515 in the test set (i.e.
5.7%).For RM we considered all the argument nodes ofany of the six PropBank core roles (i.e.
A0, .
.
.
,341k 2k 5k 10k 20k30k 50k 100k02004006008001,0001,200929 916 1,0371,104Threshold (log10)Learningtime(minutes)Overall TFX ESLFMI FSLFigure 4: Training time decomposition for the linearizedBC with respect to its main components when varying thethreshold value.A5) from all the available training sections, i.e.
2through 21, for a total of 179,091 training instances.Similarly, we collected 5,928 test instances from theannotations of Section 24.In the remainder, we will mark with an ` the lin-earized classifiers, i.e.
BC` and RM` will refer tothe linearized boundary and role classifiers, respec-tively.
Their traditional, vanilla SST counterpartswill be simply referred to as BC and RM.We used 10 splits for the FMI stage and we setmaxdepth = 4 and maxexp = 5 during FMI andTFX.
We didn?t carry out an extensive validation ofthese parameters.
These values were selected dur-ing the development of the software because, on avery small development set, they resulted in a veryresponsive system.Since the main topic of this paper is the assess-ment of the efficiency and accuracy of our lineariza-tion technique, we did not carry out an evaluationon the whole SRL task using the official CoNLL?05evaluator.
Indeed, producing complete annotationsrequires several steps (e.g.
overlap resolution, OvAor Pairwise combination of individual role classi-fiers) that would shade off the actual impact of themethodology on classification.Platform.
All the experiments were run on a ma-chine equipped with 4 Intel R?
Xeon R?
CPUs clockedat 1.6 GHz and 4 GB of RAM running on a Linux2.6.9 kernel.
As a supervised learning frameworkwe used SVM-Light-TK 1, which extends the SVM-Light optimizer (Joachims, 2000) with tree kernel1http://disi.unitn.it/?moschitt/Tree-Kernel.htm1k 2k 5k 10k 20k30k 50k 100k72747678808284Threshold (log10)AccuracyBC` Prec BC PrecBC` Rec BC RecBC` F1 BC F1Figure 5: BC` accuracy for different thresholds.support.
During FSL, we learn the models using anormalized SST kernel and the default decay factor?
= 0.4.
The same parameters are used to trainthe models of the non linearized classifiers.
DuringESL, the classifier is trained using a linear kernel.We did not carry out further parametrization of thelearning algorithm.Results.
The left side of Table 1 shows the distri-bution of positive (Column Pos) and negative (Neg)data points in each classifier?s training set.
The cen-tral group of columns lists training and test effi-ciency and accuracy of BC and RM, i.e.
the non-linearized classifiers, along with figures for the indi-vidual role classifiers that make up RM.Training BC took more than two days of CPUtime and testing about 4 hours.
The classifierachieves an F1 measure of 81.76, with a good bal-ance between precision and recall.
Concerning RM,sequential training of the 6 models took 2,596 min-utes, while classification took 27 minutes.
The slow-est of the individual role classifiers happens to beA1, which has an almost 1:1 ratio between posi-tive and negative examples, i.e.
they are 90,636 and88,455 respectively.We varied the threshold value (i.e.
the number offragments that we mine from each model, see Sec-tion 3) to measure its effect on the resulting classi-fier accuracy and efficiency.
In this context, we calltraining time all the time necessary to obtain a lin-earized model, i.e.
the sum of FSL, FMI and TFXtime for every split, plus the time for ESL.
Similarly,we call test time the time necessary to classify a lin-earized test set, i.e.
the sum of TFX and ESC on testdata.In Figure 4 we plot the efficiency of BC` learn-35ing with respect to different threshold values.
TheOverall training time is shown alongside with par-tial times coming from FSL (which is the same forevery threshold value and amounts to 433 minutes),FMI, training data TFX and ESL.
The plot showsthat TFX has a logarithmic behaviour, and that quitesoon becomes the main player in total training timeafter FSL.
For threshold values lower than 10k, ESLtime decreases as the threshold increases: too fewfragments are available and adding new ones in-creases the probability of including relevant frag-ments in the dictionary.
After 10k, all the relevantfragments are already there and adding more onlymakes computation harder.
We can see that for athreshold value of 100k total training time amountsto 1,104 minutes, i.e.
36% of BC.
For a thresholdvalue of 10k, learning time further decreases to 916minutes, i.e.
less than 30%.
This threshold valuewas used to train the individual linearized role clas-sifiers that make up RM`.These considerations are backed by the trend ofclassification accuracy shown in Figure 5, where thePrecision, Recall and F1 measure of BC`, evaluatedon the test set, are shown in comparison with BC.We can see that BC` precision is almost constant,while its recall increases as we increase the thresh-old, reaches a maximum of 78.95% for a thresholdof 10k and then settles around 78.8%.
The F1 scoreis maximized for a threshold of 10k, where it mea-sures 81.10, i.e.
just 0.66 points less than BC.
Wecan also see that BC` is constantly more conserva-tive than BC, i.e.
it always has higher precision andlower recall.Table 1 compares side to side the accuracy(columns P, R and F1), training (Train) and test(Test) times of the different classifiers (central blockof columns) and their linearized counterparts (blockon the right).
Times are measured in minutes.
Forthe linearized classifiers, test time is the sum ofTFX and ESC time, but the only relevant contribu-tion comes from TFX, as the low dimensional linearspace and fast linear kernel allow us to classify testinstances very efficiently 2.
Overall, BC` test time is39 minutes, which is more than 6 times faster thanBC (i.e.
247 minutes).
It should be stressed that we2Although ESC is not shown in table, the classification of all149k test instances with BC` took 5 seconds with a threshold of1k and 17 seconds with a threshold of 100k.Learning parallelizationTask Non Lin.
Linearized (Thr=10k)1 cpu 5 cpus 10 cpusBC 3,059 916 293 215RM 2,596 1,090 297 198Table 2: Learning time when exploiting the framework?sparallelization capabilities.
Column Non Lin.
lists non-linearized training time.are comparing against a fast TK implementation thatis almost linear in time with respect to the number oftree nodes (Moschitti, 2006).Concerning RM`, we can see that the accuracyloss is even less than with BC`, i.e.
it reaches an F1measure of 87.13 which is just 0.52 less than RM.It is also interesting to note how the individual lin-earized role classifiers manage to perform accuratelyregardless of the distribution of examples in the dataset: for all the six classifiers the final accuracy isin line with that of the corresponding non-linearizedclassifier.
In two cases, i.e.
A2 and A4, the accuracyof the linearized classifier is even higher, i.e.
74.20vs.
73.13 and 69.72 vs. 69.10, respectively.
As forthe efficiency, total training time for RM` is 37% ofRM, i.e.
1,190 vs. 2,596 minutes, while test timeis reduced to 60%, i.e.
16 vs 27 minutes.
Theseimprovements are less evident than those measuredfor boundary detection.
The main reason is thatthe training set for boundary classification is muchlarger, i.e.
1 million vs. 179k instances: therefore,splitting training data during FSL has a reduced im-pact on the overall efficiency of RM`.Parallelization.
All the efficiency improvementsthat have been discussed so far considered a com-pletely sequential process.
But one of the advan-tages of our approach is that it allows us to paral-lelize some aspect of SVM training.
Indeed, everyactivity (but ESL) can exploit some degree of par-allelism: during FSL, all the models can be learntat the same time (for this activity, the maximum de-gree of parallelization is conditioned by the numberof training data splits); during FMI, models can bemined concurrently; during TFX, the data-set to belinearized can be split arbitrarily and individual seg-ments can be processed in parallel.
Exploiting thispossibility we can drastically improve learning ef-ficiency.
As an example, in Table 2 we show howthe total learning of the BC` can be cut to as low as215 seconds when exploiting ten CPUs and using a361 2 3 4 5 6 7 8 9 1020406080100ModelsCumulativecontribution(%)1k 5k 10k50k 100kFigure 6: Growth of dictionary size when including frag-ments from more splits at different threshold values.When a low threshold is used, the contribution of indi-vidual dictionaries tends to be more marginal.threshold of 10k.
Even running on just 5 CPUs, theoverall computational cost of BC` is less than 10%of BC (Column Non Lin.).
Similar considerationscan be drawn concerning the role multi-classifier.Fragment space.
In this section we take a look atthe fragments included in the dictionary of the BC`classifier.
During FMI, we incrementally merge thefragments mined from each of the models learnt dur-ing FSL.
Figure 6 plots, for different threshold val-ues, the percentage of new fragments (on the y axis)that the i-th model (on the x axis) contributes withrespect to the number of fragments mined from eachmodel (i.e.
the threshold value).If we consider the curve for a threshold equal to100k, we can see that each model after the first ap-proximately contributes with the same number offragments.
On the other hand, if the threshold is setto 1k than the contribution of subsequent models isincreasingly more marginal.
Eventually, less than10% of the fragments mined from the last model arenew ones.
This behaviour suggests that there is acore set of very relevant fragments which is com-mon across models learnt on different data, i.e.
theyare relevant for the task and do not strictly dependon the training data that we use.
When we increasethe threshold value, the new fragments that we indexare more and more data specific.The dictionary compiled with a threshold of 10klists 62,760 distinct fragments.
15% of the frag-ments contain the predicate node (which generallyis the node encoding the predicate word?s POS tag),more than one third contain the candidate argumentnode and, of these, about one third are rooted in it.This last figure strongly suggests that the internalstructure of an argument is indeed a very powerfulfeature not only for role classification, as we wouldexpect, but also for boundary detection.
About 10%of the fragments contain both the predicate and theargument node, while about 1% encode the Path fea-ture traditionally used in explicit semantic role label-ing models (Gildea and Jurafsky, 2002).
About 5%encode a sort of extended Path feature, where the ar-gument node is represented together with its descen-dants.
Overall, about 2/3 of the fragments contain atleast some terminal symbol (i.e.
words), generally apreposition or an adverb.6 ConclusionsWe presented a supervised learning framework forSupport Vector Machines that tries to combine thepower and modeling simplicity of convolution ker-nels with the advantages of linear kernels and ex-plicit feature representations.
We tested our modelon a Semantic Role Labeling benchmark and ob-tained very promising results in terms of accuracyand efficiency.
Indeed, our linearized classifiersmanage to be almost as accurate as non linearizedones, while drastically reducing the time required totrain and test a model on the same amounts of data.To our best knowledge, the main points of nov-elty of this work are the following: 1) it addressesthe problem of feature selection for tree kernels, ex-ploiting SVM decisions to guide the process; 2) itprovides an effective way to make the kernel spaceobservable; 3) it can efficiently linearize structureddata without the need for an explicit mapping; 4) itcombines feature selection and SVM parallelization.We began investigating the fragments generatedby a TK function for SRL, and believe that study-ing them in more depth will be useful to identifynew, relevant features for the characterization ofpredicate-argument relations.In the months to come, we plan to run a set of ex-periments on a wider list of tasks so as to consolidatethe results we obtained so far.
We will also test thegenerality of the approach by testing with differenthigh-dimensional kernel families, such as sequenceand polynomial kernels.37ReferencesFabio Aiolli, Giovanni Da San Martino, Alessandro Sper-duti, and Alessandro Moschitti.
2006.
Fast on-linekernel learning for trees.
In Proceedings of ICDM?06.Nicola Cancedda, Eric Gaussier, Cyril Goutte, andJean Michel Renders.
2003.
Word sequence kernels.Journal of Machine Learning Research, 3:1059?1082.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introductionto the CoNLL-2005 Shared Task: Semantic Role La-beling.
In Proceedings of CoNLL?05.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of NAACL?00.Michael Collins and Nigel Duffy.
2002.
New Rank-ing Algorithms for Parsing and Tagging: Kernels overDiscrete Structures, and the Voted Perceptron.
In Pro-ceedings of ACL?02.Aron Culotta and Jeffrey Sorensen.
2004.
DependencyTree Kernels for Relation Extraction.
In Proceedingsof ACL?04.Chad Cumby and Dan Roth.
2003.
Kernel Methods forRelational Learning.
In Proceedings of ICML 2003.Hal Daume?
III and Daniel Marcu.
2004.
Np bracketingby maximum entropy tagging and SVM reranking.
InProceedings of EMNLP?04.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28:245?288.Hans P. Graf, Eric Cosatto, Leon Bottou, Igor Dur-danovic, and Vladimir Vapnik.
2004.
Parallel supportvector machines: The cascade svm.
In Neural Infor-mation Processing Systems.Isabelle Guyon and Andre?
Elisseeff.
2003.
An intro-duction to variable and feature selection.
Journal ofMachine Learning Research, 3:1157?1182.David Haussler.
1999.
Convolution kernels on discretestructures.
Technical report, Dept.
of Computer Sci-ence, University of California at Santa Cruz.T.
Joachims.
2000.
Estimating the generalization per-formance of a SVM efficiently.
In Proceedings ofICML?00.Hisashi Kashima and Teruo Koyanagi.
2002.
Kernels forsemi-structured data.
In Proceedings of ICML?02.Jun?ichi Kazama and Kentaro Torisawa.
2005.
Speedingup training with tree kernels for node relation labeling.In Proceedings of HLT-EMNLP?05.Taku Kudo and Yuji Matsumoto.
2003.
Fast methods forkernel-based text analysis.
In Proceedings of ACL?03.Taku Kudo, Jun Suzuki, and Hideki Isozaki.
2005.Boosting-based parse reranking with subtree features.In Proceedings of ACL?05.Alessandro Moschitti and Cosmin Bejan.
2004.
A se-mantic kernel for predicate argument classification.
InCoNLL-2004, Boston, MA, USA.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2006.
Semantic role labeling via tree ker-nel joint inference.
In Proceedings of CoNLL-X, NewYork City.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2008.
Tree kernels for semantic role labeling.Computational Linguistics, 34(2):193?224.Alessandro Moschitti.
2006.
Making tree kernels prac-tical for natural language learning.
In Proccedings ofEACL?06.Julia Neumann, Christoph Schnorr, and Gabriele Steidl.2005.
Combined SVM-Based Feature Selection andClassification.
Machine Learning, 61(1-3):129?150.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpus ofsemantic roles.
Comput.
Linguist., 31(1):71?106.J.
Pei, J. Han, Mortazavi B. Asl, H. Pinto, Q. Chen, U.Dayal, and M. C. Hsu.
2001.
PrefixSpan Mining Se-quential Patterns Efficiently by Prefix Projected Pat-tern Growth.
In Proceedings of ICDE?01.Alain Rakotomamonjy.
2003.
Variable selection usingSVM based criteria.
Journal of Machine Learning Re-search, 3:1357?1370.Libin Shen, Anoop Sarkar, and Aravind k. Joshi.
2003.Using LTAG Based Features in Parse Reranking.
InProceedings of EMNLP?06.Jun Suzuki and Hideki Isozaki.
2005.
Sequence and TreeKernels with Statistical Feature Mining.
In Proceed-ings of the 19th Annual Conference on Neural Infor-mation Processing Systems (NIPS?05).Ivan Titov and James Henderson.
2006.
Porting statisti-cal parsers with data-defined kernels.
In Proceedingsof CoNLL-X.Kristina Toutanova, Penka Markova, and ChristopherManning.
2004.
The Leaf Path Projection View ofParse Trees: Exploring String Kernels for HPSG ParseSelection.
In Proceedings of EMNLP 2004.Vladimir N. Vapnik.
1998.
Statistical Learning Theory.Wiley-Interscience.Jason Weston, Sayan Mukherjee, Olivier Chapelle, Mas-similiano Pontil, Tomaso Poggio, and Vladimir Vap-nik.
2001.
Feature Selection for SVMs.
In Proceed-ings of NIPS?01.Jason Weston, Andre?
Elisseeff, Bernhard Scho?lkopf, andMike Tipping.
2003.
Use of the zero norm with lin-ear models and kernel methods.
J. Mach.
Learn.
Res.,3:1439?1461.Mohammed J Zaki.
2002.
Efficiently mining frequenttrees in a forest.
In Proceedings of KDD?02.38
