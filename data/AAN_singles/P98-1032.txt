Automated Scoring Using A Hybrid Feature Identification TechniqueJill Burstein, Karen Kukich, Susanne Wolff,  Chi LutMartin Chodorow~, Lisa Braden-Harder$:l:, and Mary Dee Harris:H:+iEducat ional  Testing Service, Princeton N J, SHunter College, New York City, NY,$1:Butler-Hill Group, Reston, VA, and l:$$Language Technology, Inc, Austin, TXAbstractThis study exploits statistical redundancyinherent in natural language to automaticallypredict scores for essays.
We use a hybridfeature identification method, includingsyntactic structure analysis, rhetorical structureanalysis, and topical analysis, to score essayresponses from test-takers of the GraduateManagement Admissions Test (GMAT) andthe Test of Written English (TWE).
For eachessay question, a stepwise linear regressionanalysis is run on a training set (sample ofhuman scored essay responses) to extract aweighted set of predictive features for each testquestion.
Score prediction for cross-validationsets is calculated from the set of predictivefeatures.
Exact or adjacent agreement betweenthe Electronic Essay Rater (e-rater) scorepredictions and human rater scores ranged from87% to 94% across the 15 test questions.1.
Introduct ionThis paper describes the development andevaluation of a prototype system designed forthe purpose of automatically scoring essayresponses.
The paper reports on evaluationresults from scoring 13 sets of essay data fromthe Analytical Writing Assessments of theGraduate Management Admissions Test(GMAT) (see the GMAT Web site athttp://www.gmat.org/ for sample questions)and 2 sets of essay data from the Test of WrittenEnglish (TWE) (see http://www.toefl.org/tstprpmt.html for sample TWE questions).Electronic Essay Rater (e-rater) was designed toautomatically analyze essay features based onwriting characteristics specified at each of sixscore points in the scoring guide used by humanraters for manual scoring (also available athttp://www.gmat.orff).
The scoring guideindicates that an essay that stays on the topic ofthe question has a strong, coherent and well-organized argument structure, and displays avariety of word use and syntactic structure willreceive a score at the higher end of the six-pointscale (5 or 6).
Lower scores are assigned toessays as these characteristics diminish.One of our main goals was to design a system thatcould score an essay based on features pecifiedin the scoring guide for manual scoring.
E-raterfeatures include rhetorical structure, syntacticstructure, and topical analysis.
For each essayquestion, a stepwise linear regression analysis isrun on a set of training data (human-scoredessay responses) to extract a weighted set ofpredictive features for each test question.Final score prediction for cross-validation usesthe weighted predictive feature set identifiedduring training.
Score prediction accuracy isdetermined by measuring agreement betweenhuman rater scores and e-rater scorepredictions.
In accordance with humaninterrater "agreement" standards, human and e-rater scores also "agree" if there is an exactmatch or if the scores differ by no more thanone point (adjacent agreement).2062.
Hybrid Feature MethodologyE-rater uses a hybrid feature methodology thatincorporates everal variables either derivedstatistically, or extracted through NLPtechniques.
The final linear regression modelused for predicting scores includes syntactic,rhetorical and topical features.
The next threesections present a conceptual rationale and adescription of feature identification in essayresponses.2.1 Syntactic FeaturesThe scoring guides indicate that one featureused to evaluate an essay is syntactic variety.All sentences in the essays were parsed usingthe Microsoft Natural Language Processingtool (MSNLP) (see MSNLP (1997)) so thatsyntactic structure information could beaccessed.
The identification of syntacticstructures in essay responses yields informationabout the syntactic variety in an essay withregard to the identification of clause or verbtypes.A program was implemented to identify thenumber of complement clauses, subordinateclauses, infinitive clauses, relative clauses andoccurrences of the subjunctive modal auxiliaryverbs, would, could, should, might and may, foreach sentence in an essay.
Ratios of syntacticstructure types per essay and per sentence werealso used as measures of syntactic variety.2.2 Rhetorical Structure AnalysisGMAT essay questions are of two types:Analysis of an Issue (issue) and Analysis of anArgument (argument).
The GMAT issue essayasks the writer to respond to a general questionand to provide "reasons and/or examples" tosupport his or her position on an issueintroduced by the test question.
The GMATargument essay focuses the writer on theargument in a given piece of text, using theterm argument in the sense of a rationalpresentation of points with the purpose ofpersuading the reader.
The scoring guidesindicate that an essay will receive a score basedon the examinee's demonstration of a well-developed essay.
In this study, we try to identifyorganization of an essay through automatedanalysis and identification of the rhetorical (orargument) structure of the essay.Argument structure in the rhetorical sense mayor may not correspond to paragraph divisions.One can make a point in a phrase, a sentence,two or three sentences, a paragraph, and so on.For automated argument identification, e-rateridentifies 'rhetorical' relations, such asParallelism and Contrast that can appear atalmost any level of discourse.
This is part of thereason that human readers must also rely on cuewords to identify new arguments in an essay.Literature in the field of discourse analysissupports our approach.
It points out thatrhetorical cue words and structures can beidentified and used for computer-baseddiscourse analysis (Cohen (1984), (Mann andThompson (1988), Hovy, et al(1992),Hirschberg and Litman (1993), Vander Lindenand Martin (1995), and Knott (1996)).
E-raterfollows this approach by using rhetorical cuewords and structure features, in addition to othertopical and syntactic information.
We adaptedthe conceptual framework of conjunctiverelations from Quirk, et ai (1985) in which cueterms, such as "In summary" and "Inconclusion," are classified as conjuncts used forsummarizing.
Cue words such as "perhaps,"and "possibly" are considered to be "belief"words used by the writer to express a belief indeveloping an argument in the essay.
Wordslike "this" and "these" may often be used to flagthat the writer has not changed topics (Sidner(1986)).
We also observed that in certaindiscourse contexts structures uch as infinitiveclauses mark the beginning of a new argument.E-rater's automated argument partitioning andannotation program (APA) outputs an annotatedversion of each essay in which the argumentunits of the essays are labeled with regard totheir status as "marking the beginning of anargument," or "marking argument207development."
APA also outputs a version ofthe essay that has been partitioned "byargument", instead of "by paragraph," as it wasoriginally partitioned by the test-taker.
APAuses rules tbr argument annotation andpartitioning based on syntactic and paragraph-based distribution of cue words, phrases andstructures to identify rhetorical structure.Relevant cue words and terms are stored in acue word lexicon.2.3 Topical AnalysisGood essays are relevant o the assigned topic.They also tend to use a more specialized andprecise vocabulary in discussing the topic thanpoorer essays do.
We should therefore xpecta good essay to resemble other good essays inits choice of words and, conversely, a pooressay to resemble other poor ones.
E-raterevaluates the lexical and topical content of anessay by cornparing the words it contains to thewords found in manually graded trainingexamples for each of the six score categories.Two programs were implemented that computemeasures of content similarity, one based onword frequency (EssayContent) and the otheron word weight (ArgContent), as ininformation retrieval applications (Salton(1988)).In EssayContent, the vocabulary of each scorecategory is converted to a single vector whoseelements represent the total frequency of eachword in the training essays for that category.
Ineffect, this merges the essays for each score.
(Astop list of some function words is removedprior to vector construction.)
The systemcomputes cosine correlations between thevector for a given test essay and the six vectorsrepresenting the trained categories; thecategory that is most similar to the test essay isassigned as the evaluation of its content.
Anadvantage of using the cosine correlation is thatit is not sensitive to essay length, which mayvary considerably.The other content similarity measure, iscomputed separately by ArgContent for eachargument in the test essay and is based on thekind of term weighting used in informationretrieval.
For this purpose, the word frequencyvectors for the six score categories, describedabove, are converted to vectors of word weights.The weight for word i in score category s is:Wi.
s =(freqi..~ / max_freq,) * log(n_essaystot~,l/n_essaysi)where freq,.,, is the frequency of word i incategory s, max_freq~ is the frequency of themost frequent word in s (after a stop list ofwords has been removed), n_essaystot,,i is thetotal number of training essays across all sixcategories, and n_essays~ is the number oftraining essays containing word i.The first part of the weight formula representsthe prominence of word i in the score category,and the second part is the log of the word'sinverse document frequency.
For each argumentin the test essay, a vector of word weights isalso constructed.
Each argument is evaluated bycomputing cosine correlations between itsweighted vector and those of the six scorecategories, and the most similar category isassigned to the argument.
As a result of thisanalysis, e-rater has a set of scores (one perargument) for each test essay.In a preliminary study, we looked at how wellthe minimum, maximum, mode, median, andmean of the set of argument scores agreed withthe judgments of human raters for the essay as awhole.
The greatest agreement was obtainedfrom an adjusted mean of the argument scoresthat compensated for an effect of the number ofarguments in the essay.
For example, essayswhich contained only one or two argumentstended to receive slightly lower scores from thehuman raters than the mean of the argumentscores, and essays which contained manyarguments tended to receive slightly higherscores than the mean of the argument scores.
Tocompensate for this, an adjusted mean is used ase-rater's ArgContent,208A rqContent =(Zarg_scores +n_args) / (n args + 1)3.
Training and TestingIn all, e-rater's syntactic, rhetorical, and topicalanalyses yielded a total of 57 features for eachessay.
The training sets for each test questionconsisted of 5 essays for score 0, 15 essays forscore 1, and 50 essays each for scores 2through 6.
To predict the score assigned byhuman raters, a stepwise linear regressionanalysis was used to compute the optimalweights for these predictors based on manuallyscored training essays.
For example, Figure 1,below, shows the predictive feature setgenerated for the ARGI test question (seeresults in Table 1).
The predictive feature setfor ARGI illustrates how criteria specified formanual scoring described earlier, such asargument opic and development (using theArgContent score and argument developmentterms), syntactic structure usage, and wordusage (using the EssayContent score), arerepresented by e-rarer.
After training, e-rateranalyzed new test essays, and the regressionweights were used to combine the measuresinto a predicted score for each one.
Thisprediction was then compared to the scoresassigned by two human raters to check forexact or acljacent agreement.I.
ArgContent Score2.
EssavContent Score3.
Total Argument DevelopmentWords/Phrases4.
Total Pronouns Beginning Arguments5.
Total Complement Clauses BeginningArguments6.
Total Summary Words BeginningArguments7.
Total Detail Words Beginning Arguments8.
Total Rhetorical Words DevelopingArguments9.
Subjunctive Modal VerbsFigure 1: Predictive Feature Set forARG1 Test Question3.1 ResultsTable 1 shows the overall results for 8 GMATargument questions, 5 GMAT issue questionsand 2 TWE questions.
There was an average of638 response ssays per test question.
E-raterand human rater mean agreement across the 15data sets was 89%.
In many cases, agreementwas as high as that found between the twohuman raters.The items that were tested represented a widevariety of topics (see http://www.gmat.org/ forGMAT sample questions andhttp://www.toetl.org/tstprpmt.htm!
for sample TWEquestions).
The data also represented a widevariety of English writing competency.
In fact,the majority of test-takers from the 2 TWE datasets were nonnative English speakers.
Despitethese differences in topic and writing skill e-rater performed consistently well across items.Table 1: Mean Percentage and StandardDeviation for E-rater (E) and Human Rater(H) Agreement & Human InterraterAgreement For 15 Cross-Validation TestsHI~H2 HI~E H2~EMean 90.4 89.1 89.0S.D 2.1 2.3 2.7To determine the features that were the mostreliable predictors of essay score, we examinedthe regression models built during training.
Afeature type was considered to be a reliablepredictor if it proved to be significant in at least12 of the 15 regression analyses.
Using thiscriterion, the most reliable predictors were theArgContent and EssayContent scores, thenumber of cue words or phrases indicating thedevelopment of an argument, the number ofsyntactic verb and clause types, and the numberof cue words or phrases indicating the beginningof an argument.2094.
Discussion and ConclusionsThis study showsprocessing methodscan be used for thestudy indicates thattopical informationextracted and usedprediction of essayhow natural languageand statistical techniquesevaluation of text.
Therhetorical, syntactic, andcan be automaticallyfor machine-based scoreresponses.
These threetypes of information model features pecifiedin the manual scoring guides.
This study alsoshows that e-rater adapts well to manydifferent topical domains and populations oftest-takers.The information used for automated scoreprediction by e-rater can also be used asbuilding blocks for automated generation ofdiagnostic and instructional summaries.Clauses and sentences annotated by APA as"the beginning of a new argument" might beused to identify main points of an essay (Marcu(1997)).
In turn, identifying the main points inthe text of an essay could be used to generatefeedback reflecting essay topic andorganization.
Other features could be used toautomatically generate statements thatexplicate the basis on which e-rater generatesscores.
Such statements could supplementmanually created qualitative feedback about anessay.6.
ReferencesCohen, Robin (1984).
"A computational theoryof the function of clue words in argumentunderstanding."
In Proceedings of 1984International Computational LinguisticsCol!(erence.
California, 251-255..Hirschberg, Julia and Diane Litman (1993).
"Empirical Studies on the Disambiguation ofCue Phrases."
Computational Linguistics( 19)3, 501-530.Hovy, Eduard, Julia Lavid, Elisabeth Maier,"Employing Knowledge Resources in a NewText Planner Architecture," In Aspects ofAutomated NL Generation, Dale, Hovy, Rosnerand Stoch (Eds), Springer-Verlag Lecture Notesin AI no.
587, 57-72.GMAT (1997).
http://www.gmat.org/Knott, Alistair.
(1996).
"A Data-DrivenMethodology for Motivating a Set of CoherenceRelations."
Ph.D. Dissertation, available atwww.cogsci.edu.ac.uk/~alik/publications.htmi,under the Heading, Unpublished Stuff.Mann, William C. and Sandra A. Thompson(1988).
"Rhetorical Structure Theory: Toward afunctional theory of text organization."
Text8(3), 243-28 !.Marcu, Daniel.
(1997).
"From DiscourseStructures to Text Summaries.
", In Proceedingsof the Intelligent Scalable Text SummarizationWorkshop, Association for ComputationalLinguistics, Universidad Nacionai deEducacion a Distancia, Madrid, Spain.MSNLP (1997) http://research.microsoft.com/nlp/Quirk, Randolph, Sidney Greenbaum, GeoffreyLeech, and Jan Svartik (1985).
AComprehensive Grammar of the EnglishLanguage.
Longman, New York.Sidner, Candace.
(1986).
Focusing in theComprehension of Definite Anaphora.
InReadings in Natural Language Processing,Barbara Grosz, Karen Sparck Jones, and BonnieLynn Webber (Eds.
), Morgan KaufmannPublishers, Los Altos, California, 363-394.Salton, Gerard.
(1988).
Automatic textprocessing : the transformation, analysis, andretrieval of information by computer.
Addison-Wesley, Reading, Mass.TOEFL (1997).
http://www.toefl.org/tstprpmt.htmiVander Linden, Keith and James H. Martin(1995).
"Expressing Rhetorical Relations inInstructional Text: A Case Study in PurposeRelation."
Computational Linguistics 2 / (1), 29-57.210
