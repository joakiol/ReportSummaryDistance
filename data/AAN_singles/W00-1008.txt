Using decision trees to selectthe gran natical relation of a noun phraseSimon CORSTON-OLIVERMicrosoft ResearchOne Microsoft WayRedmond WA 98052, USAsimonco@microsoft.comAbstractWe present a machine-learning approach tomodeling the distribution of noun phrases(NPs) within clauses with respect o a fine-grained taxonomy of grammatical relations.
Wedemonstrate that a cluster of superficiallinguistic features can function as a proxy formore abstract discourse features that are notobservable using state-of-the-art naturallanguage processing.
The models constructedfor actual texts can be used to select amongalternative linguistic expressions of the samepropositional content when generatingdiscourse.1.
IntroductionNatural language generation involves a number ofprocesses ranging from planning the content o beexpressed through making encoding decisionsinvolving syntax, the lexicon and morphology.
Thepresent study concerns decisions made about he formand distribution of each "mention" of a discourseentity: should reference be made with a lexical NP, apronominal NP or a zero anaphor (i.e.
an elidedmention)?
Should a given mention be expressed asthe subject of its clause or in some other grammaticalrelation?If all works well, a natural anguage generationsystem may end up proposing a mmaber of possiblewell-formed expressions of the same propositionalcontent.
Although these possible formulations wouldall be judged to be valid sentences of the targetlanguage, it is not the ease that they are all equallylikely to occur.Research in the area of Preferred ArgumentStructure (Corston 1996, Du Bois 1987) hasestablished that in discourse in many languages,including English, NPs are distributed acrossgrammatical relations in statistically significant ways.For example, transitive clauses tend not to containlexical NPs in both subject and object positions andsubjects of transitives tend not to be lexical NPs norto be discourse-new.Unfortunately, the models used in PAS haveinvolved only simple chi-squared tests to identifystatistically significant patterns in the distribution ofNPs with respect o pairs of features (e.g.
part ofspeech and grammatical relation).
A further problemfrom the point of view of computational discourseanalysis is that many of the features used in empiricalstudies are not observable in texts using state-of-theart natural anguage processing.
Such non-observablefeatures include animacy, the information status of areferent, and the identification of the gender of areferent based on world knowledge.In the present study, we treat the task ofdetermining the appropriate distribution of mentionsin text as a machine learning classification problem:what is the probability that a mention will have acertain grammatical relation given a deh set oflinguistic features?
In particular, how accurately canwe select appropriate grammatical relations usingonly superficial linguistic features?2.
DataA total of 5,252 mentions were annotated from theEncarta electronic encyclopedia and 4,937 mentionsfrom the Wall Street Journal (WSJ).
Sentences wereparsed using the Microsoft English Grammar(Heidorn 1999) to extract mentions and linguisticfeatures.
These analyses were then hand-corrected toeliminate noise in the training data caused byinaccurate parses, allowing us to determine the upperbound on accuracy for the classification task if thecomputational nalysis were perfect.
Zero anaphorswere annotated only when they occurred as subjectsof coordinated clauses.
They have been excluded66from the present study since they are invariablydiscourse-given subjects.3.
FeaturesNineteen linguistic features were annotated, alongwith information about the referent of each mention.On the basis of the reference information weextracted the feature \[InformationStatus\],distinguishing "discourse-new" versus "discourse-old".
All mentions without a prior coreferentialmention in the text were classified as discourse-new,even if  they would not traditionally be consideredreferential.
\[InformationStatus\] is not directlyobservable since it requires the analyst to makedecisions about he referent of a mention.In addition to the feature \[InformafionStatus\], thefollowing eighteen observable features wereannotated.
These are all features that we canreasonably expect syntactic parsers to extract withsufficient accuracy today or in the near future.?
\[ClausalStatus\]: Does the mention occur in amain clause ("M"), complement clause ("C"),or subordinate clause ("S")??
\[Coordinated\] The mention is coordinatedwith at least one sibling.?
\[Definite\] The mention is marked with thedefinite article or a demonstrative pronoun.\[Fem\] The mention is unambiguouslyfeminine.?
\[GrRel\] The grammatical relation of themention (see below, this section).?
\[HasPossessive\] Modified by a possessivepronoun or a possessive NP with the elit ic'sors'.?
\[HasPP\] Contains a postmodifying pre-positional phrase.?
\[HasRelC1\] Contains a postmodifying relativeclause.?
\[InQuotes\] The mention occurs in quotedmaterial.?
\[Lex\] The specific inflected form of apronoun, e.g.
he, him.?
\[Mase\] The mention is unambiguouslymasculine.?
\[NounClass\] We distinguish common nounsversus proper names.
Within proper names,we distinguish the name of a place ("Geo")versus other proper names ("ProperName").?
\[Plural\] The head of the mention ismorphologically marked as plural.?
\[POS\] The part of speech of the head of themention.?
\[Prep\] The governing preposition, if any.?
\[RelC1\] The mention is a child of a relativeclause.?
\[TopLevel\] The mention is not embeddedwithin another mention.?
\[Words\] The total number of words in themention, discretized to the following values:{0, 1, 2, 3, 4, 5, 6to10, 1 lto15, abovel5}.Gender (\[Fern\], \[Mast\]) was only annotated forcommon ouns whose default word sense is gendered(e.g.
"mother", "father"), for common nouns withspecific morphology (e.g.
with the -ess suffix) andfor gender-marked proper names (e.g.
"John","Mary").
Gender was not marked for pronouns, toavoid difficult encoding decisions uch as the use ofgenetic "he".
~ Gender was also not marked for casesthat would require world knowledge.The feature \[GrRel\] was given a much finer-grained analysis than is usual in computationallinguistics.
Studies in PAS have demonstrated theneed to distinguish finer-grained categories than thetraditional grammatical relations of English grammar("subject", "object" ere) in order to account fordistributional phenomena in discourse.
For example,subjects of intransitive verbs pattern with the directobjects of transitive verbs as being the preferred locusfor introducing new mentions.
Subjects of transitives,however, are strongly dispreferred slots for theexpression of new information.
The use of fine-grained grammatical relations enables us to makerather specific claims about the distribution ofmentions.
The taxonomy of fine-grained grammaticalrelations is given below in Figure 1.1 The feature \[Lex\] was sufficient for the decision tree toolsto learn idiosyncratic uses of gendered pronouns.67Subject oftransitive (S.r)SubjectSubject ofintransitiveObject of transitiveSubject of copula(Sc)Subject ofintransitive(non-copula) (Si)GrammaticalRelation/ / / L  (PN)Po so.Po    PCP e I, / / ' t ,  NPCPP.
I~!
adjective(PP,) }., ..
I ~ PP complement ofJ 'N?un (NA~=ve I \[ verb (PPv) IOther (Oth) JFigure 1 The taxonomy of grammatical relations4.
Decision treesFor a set of annotated examples, we used decision-tree tools to construct the conditional probability of aspecific grammatical relation, given other features inthe domain, zThe decision trees are constructed usinga Bayesian learning approach that identifies treestructures with high posterior probability (Chickefinget al 1997).
In particular, a candidate tree structure(S) is evaluated against data (D) using Bayes' rule asfollows:P(SID) = constant- p(DIS) ?
p(S)For simplicity, we specify a prior distributionover tree structures using a single parameter kappa(k).
Assuming that N(S) probabilities are needed toparameterize a tree with structure S, we use:p(S) = c .
k2 Comparison experiments were also done with SupportVector Machines (Platt 2000, Vapnik 1998) using awhere 0 < k _< 1, and c is a constant such that p(S)sums to one.
Note that smaller values of kappa causesimpler structures to be favored.
As kappa growscloser to one (k = 1 corresponds to a uniform priorover all possible tree structures), the learned ecisiontrees become more elaborate.
Decision trees werebuilt for k~ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9,0.95, 0.99, 0.999}.Having selected a decision tree, we use theposterior means of the parameters to specify aprobability distribution over the grammaticalrelations.
To avoid overfitting, nodes containingfewer than fifty examples were not split during thelearning process.
In building decision trees, 70% ofthe data was used for training and 30% for held-outevaluation.The decision trees constructed can be rather complex,making them difficult o present visually.
Figure 2gives a simpler decision tree that predicts thegrammatical relation of  a mention for Enearta tvariety of kernel functions.
The results obtained wereindistinguishable from those reported here.68,=:..,?IY"~)=o.~ I ,- "-.
I t~,te, n}=o.o~ .
_ .
.
.
.
.2  ?~'---.
It~=:,=o.o~p(sc)=O.09 I I p(na)=0.O~ \] I p(nay=u..26 J p(ppv)=O.~ I P(~) =0-07 1 y ~ -~,p(sc}=0.1~l P~PPW~.-3p(st)=o,os ~ .
;~..~ Zo I~(st)=o.o3 I I p(ppa)=o.o5 ~(poss)=o.
:p(srn)=O.Olp(so)=o.o2Figure 2 Decision tree for Enearta, at k=0.7k=0.7.
The tree was constructed using a subset of themorphological nd syntactic features: \[Coordinated\],\[HasPP\], \[Lex\], \[NounClass\], \[Plural\], \[POS\], \[Prep\],\[RelC1\], \[TopLevel\], [Words\].
Grammatical relationswith only a residual probability are omitted for thesake of clarity.
The top-ranked grammatical relationat each leaf node appears in bold type.
Selecting thetop-ranked grammatical relation at each node resultsin a correct decision 58.82% of the time in the held-out test data.
By way of comparison, the best decisiontree for Enema computed using all morphologicaland syntactic features yields 66.05% accuracy at k =0.999.The distributional facts about the pronoun "he"represented in Figure 2 illustrate the utility of the\[me-grained taxonomy of grammatical relations.
Thepronoun "he" in embedded NPs (\[Prep\] = "-",\[TopLevel\] = No) and when not in a relative clause(\[RelC1\] = No) favors ST and SI.
Other grammaticalrelations have only residual probabilities.
The use ofthe traditional notion of subject would fail to capturethe fact that, in this syntactic ontext, the pronoun"he" tends not to occur as Sc, the subject of a copula.5.
Evaluating decision treesDecision trees were constructed and evaluated foreach corpus.
We were particularly interested in theaccuracy of models built using only observablefeatures.
If accurate modeling were to require moreabstract discourse features such as\[InformationStatus\], a feature that is not directlyobservable, then a machine-learning approach tomodeling the distribution of mentions would not becomputationally feasible.
Also of interest was thegenerality of the models.5.1 Using Observable Features OnlyDecision trees were built for Encarta and the WallStreet Journal using all features except the non-observable discourse feature \[InformationStatus\].
Thebest accuracy when evaluated against held-out testdata and selecting the top-ranked grammaticalrelation at each leaf node was 66.05% for Encarta atk=0.999 and 65.18% for Wall Street Journal atk=0.99.
Previous studies in Preferred ArgumentStructure (Corston 1996, Du Bois 1987) have69Table 1 Accuracy using only morphological nd syntactic featuresGrammatical relations in Accuracy in held-out est datatraining data (decision tree accuracy in parentheses)Corpus Top-ranked i Top two Using top-ranked Using top twoiEncarta PPN PPN, PPv 20.88% (66.05%) 41.37% (81.92%)WSJ Or Or, PPN 19.91% (66.16%) 35.56% (80.70%)established pairings of free-grained grammaticalrelations with respect o abstract discourse factors.New mentions in discourse, for example, tend to beintroduced as the subjects of intransitive verbs or asdirect objects, and are extremely unlikely to occur asthe subj~ts of transitive verbs.
Some languages evengive the same morphological nd syntactic treatmentto subjects of intransitives and direct objects, markingthem (so called "absolutive" case marking) inopposition to subjects of transitives (so called"ergative" marking).
Human referents, on the otherhand, tend to occur as the subjects of transitive verbsand as the subjects of intransitive verbs, rather than asobjects.
Such discourse tendencies perhaps motivatethe Use of one set of pronouns (the so called"nominative" pronouns {"he", "she", "we", "r',"they"}) in a language like English :for subjects and adifferent set of pronouns for objects (the so called"accusative" set {"him", "her", "us", "me", "them"}).Thus, we can see that distributional facts aboutmentions in discourse sometimes cross-cut themorphological nd syntactic encoding strategies of alanguage.
With a free-grained set of grammaticalrelations, we can allow the decision trees to discoversuch groupings of relations, rather than attempting tospecify the groupings in advance.We evaluated the accuracy of the decision treesby counting as a correct decision a grammaticalrelation that matched the top-ranked grammaticalrelation for a leaf node or the second rankedgamrnatieal relation for that leaf node.
With thisevaluation criterion, the accuracy for Enearta is81.92% at k=0.999 and for Wall Street Journal,80.70% at k=0.9.It is clearly naive to assume a baseline forcomparison in which all grammatical relations havean equal probability of occurrence, i.e.
1/12 or 0.083.Rather, in Table 1 we compare the accuracy to thatobtained by predicting the most frequent grammaticalrelations observed in the training data.
The decisiontrees perform substantially above this baseline.
Thetop two grammatical relations in the two corpora donot form a natural class.
In the Wall Street Journaltexts, for example, the top two grammatical relationsare Or (object of transitive verb) and PPN(prepositional phrase complement of a NP).
It isdifficult to see how mentions in these twogrammatical relations might be related.
Objects oftransitive verbs, for example, are typically entitiesaffected by the action of the verb.
Prepositionalphrase complements of NPs, however, areprototypically used to express attributes of the NP,e.g.
"the man with the red hat".
The grammaticalrelations paired by taking the top two predictions ateach leaf node in the decision trees constructed forthe Wall Street Journal and Encarta, however,frequently correspond to classes that have beenpreviously observed in the literature on PreferredArgument Structure.
The groupings {Or, Si}, {Or,Sc} and {Si, ST}, for example, occur on multiple leafnodes in the decision trees for both corpora.5.2 Using All FeaturesDecision trees were built for Encarta and the WallStreet Journal using all features including thediscourse feature \[InformationStatus\].
As it turnedout, the feature \[InformationStatus\] wasnot selectedduring the automatic onstruction of the decision treefor the Wall Street Journal.
The performance of the70decision trees on held-out est data from the WallStreet Journal therefore remained the same as thatgiven in section 5.1.
For Encarta, the addition of\[InformationStatus\] yielded only a modestimprovement in accuracy.
Selecting the top-rankedgrammatical relation rose from 66.05% at k=0.999 to67.32% at k = 0.999.
Applying a paired t-test, this isstatistically significant at the 0.01 level.
Selecting thetop two grammatical relations caused accuracy to risefrom 81.92% at k=0.999 to 82.23% at k=0.999, not astatistically significant improvement.The fact that the discourse feature\[InformationStatus\] does not make a marked impacton accuracy is not surprising.
The information statusof an NP is an important factor in determiningelements of form, such as the decision to use apronoun versus a lexical NP, or the degree ofelaboration (e.g.
by means of adjectives, post-modifying PPs and relative clauses).
Those elementsof form can be viewed as proxies for the feature\[informationStatus\].
Pronouns and definite NPs, forexample, typically refer to given entities, andtherefore are compatible with the grammaticalrelation ST.
Similarly, long indefinite lexical NPs arelikely to be new mentions.In a separate set of experiments conducted on thesame data, we built decision trees to predict theinformation status of the referent of a noun phraseusing the other linguistic features (grammaticalrelation, clausal status, definiteness and so on.)
Zeroanaphors were excluded, yielding 4,996 noun phrasesfor Encarta and 4,758 noun phrases for the WallStreet Journal.
The accuracy of the decision trees was80.45% for Encarta and 78.36% for the Wall StreetJournal.
To exclude the strong associations betweenpersonal pronouns and information status, we alsobuilt decision trees for only the lexical noun phrasesin the two corpora, a total of 4,542 noun phrases forEnema and 4,153 noun phrases for the Wall StreetJournal.
The accuracy of the decision trees was78.14% for Encarta and 77.45% for the Wall StreetJournal.
The feature \[informationStatus\] can thus beseen to be highly inferrable given the other featuresused.5.3 Domain-specificity of the Decision TreesThe decision trees built for the Encarta and WallStreet Journal corpora differ considerably, as is to beexpected for such distinct genres.
To measure thespecificity of the decision trees, we built modelsusing all the data for one corpus and evaluated on allthe data in the other corpus, using all features except\[informationStatus\].
Table 2 gives the baselinefigures for this cross-domain evaluation, selecting themost frequent grammatical relations in the trainingdata.
The peak accuracy from the decision trees isgiven in parentheses for comparison.
The decisiontrees perform well above the baseline.Table 3 provides a comparison of the accuracy ofdecision trees applied across domains compared tothose constructed and evaluated within a givendomain.
The extremely specialized sublanguage ofEncarta does not generalize well to the Wall StreetJournal.
In particular, when selecting the top-rankedgrammatical relation, the most severe evaluation ofthe accuracy of the decision trees, training on Encartaand evaluating on the Wall Street Journal results in adrop in accuracy of 7.54% compared to the WallStreet Journal within-corpus model.
By way ofcontrast, decision trees built from the Wall StreetJournal data do generalize well to Enearta, evenyielding a modest 0.41% improvement in accuracyover the model built for Encarta.
Since the Encartadata contains more mentions (5,252 mentions) thanthe Wall Street Journal data (4,937 mentions), thiseffect is not simply due to differences in the size ofthe training set.71Table 2 Cross-domain evaluation of the decision treesTrain-TestWSJ-EncartaEncarta-Grammaticaltraining dataTop-rankedOrrelations inTop twoOT, PPNAccuracy in held-out est data(decision tree accuracy in parentheses)Using top-ranked15.90% (66.32%)Using top two36.58% (79.51%)PPN PPN, PPv 15.98% (61.17%) 31.90% (77.64%)WSJTable 3 Comparison of cross-domain accuracy to.
.
.
.
.
within-domain accuracyTop-rankedTrain on Encarta, evaluate on WSJ 61.17%Train on WSJ, evaluate on WSJ 66.16%Relative difference in accuracy -7.54%Train on WSJ, evaluate on EncartaTrain on Encarta, evaluate on EnemaRelative difference in accuracy66.32%66.05%+0.41%Top twoTrain on Encarta, evaluate on WSJTrain on WSJ, evaluate on WSJRelative difference in accuracy77.64%80.70%-3.74%Train on WSJ, evaluate on EncartaTrain on Enema, evaluate on EncartaRelative difference in accuracy79.51%81.92%-2.94%5.4 Combining the DataCombining the Wall Street Journal and Encarta datainto one dataset and using 70% of the data fortraining and 30% for testing yielded mixed results.Selecting the top-ranked grammatical relation for thecombined ata yielded 66.01% at lc~0.99, comparedto the Encarta-specific accuracy of 66.05% and theWall Street Journal-specific peak accuracy of66.16%.
Selecting the top two grammatical relations,the peak accuracy for the combined ata was 81.39%at k=0.99, a result approximately midway between thecorpus-specific results obtained in section 5.1,namely 81.92% for Encarta and 80.70% for WallStreet Journal.The Wall Street Journal corpus contains a diverserange of articles, including op-ed pieces, mundanefinancial reporting, and world news.
The addition ofthe relatively homogeneous Encarta articles appearsto result in models that are even more robust thanthose constructed solely on the basis of the WallStreet Journal data.
The addition of the heterogeneousWall Street Journal articles, however, dilutes thefocus of the model constructed for Encarta.
Thisperhaps explains the fact that the peak accuracy of thecombined model lies above that for the Wall StreetJournal but below that of Encarta.6.
ConclusionNatural language generation is typically done underone of two scenarios.
In the first scenario, language isgenerated eex nihilo: a planning component formulatespropositions on the basis of a database query, asystem event, or some other non-linguistic stimulus.Under such a scenario, the discourse status ofreferents is known, since the planning component hasselected the discourse ntities to be expressed.
Moreabstract discourse features like \[informationStatus\]can therefore be used to guide the linguistic encodingdecisions.In the second, more typical scenario, naturallanguage generation involves reformulating existingtext, e.g.
for summarization ormachine translation.
Inthis scenario, analysis of the linguistic stimulus willmost likely have resulted in only a partialunderstanding of the source text.
Corefereneerelations (e.g.
between a pronoun and its antecedent)72may not be fully resolved, discourse relations may beunspecified, and the information status of mentions iunlikely to have been determined.
As was shown insection 5.2, the accuracy of the decision treesconstructed without he feature \[InformationStatus\] iscomparable to the accuracy that results from usingthis feature, since superficial elements of thelinguistic form of a mention are motivated by theinformation status of the mention.The decision trees that were constructed tomodelthe distribution of NPs in real texts can be used toguide the generation of natural language, specially toguide the selection among alternative grammaticalways of expressing the same propositional content.Sentences in which mentions occur in positions thatare unlikely given a set of linguistic features houldbe avoided.One interesting problem remains for futureresearch: why do writers occasionally place mentionsin statistically unlikely positions?
One possibility isthat writers do so for stylistic variation.
Anotherintriguing possibility is that statistically unusualoccurrences reflect pragmatic markedness, i.e.
thatwriters place NPs in certain positions in order tosignal discourse information.
Fox (1987), forexample, demonstrates that lexical NPs may be usedfor previously mentioned iscourse ntities where apronoun might be expected instead if there is anepisode boundary in the discourse.
For example, aprot~igonist in a novel may be reintroduced by nameat the beginning of a chapter.
In future research wepropose to examine the mentions that occur in placesnot predicted by the models.
It may be that thisapproach to modeling the distribution of mentions,essentially a machine-learning approach that seeks tomine an abstract property of texts, will provide usefulinsights into issues of discourse structure.ReferencesChickering, D. M., D. Heckerman, and C. Meek, 1997, "ABayesian approach to learning Bayesian etworks withlocal structure," In Geiger, D. and P. Punadlik Shenoy(eds.
), Uncertainty inArtificial Intelligence: Proceedingsof the Thirteenth Conference, 80-89.Corston, S. H., 1996, Ergativity in Roviana, SolomonIslands, Pacific Linguistics, Series B-113, AustraliaNational University Press: Canberra.Du Bois, J. W., 1987, "The discourse basis of ergativity,"Language 63:805-855.Fox, B.A., 1987, Discourse structure and anaphora,Cambridge Studies in Linguistics 48, CambridgeUniversity Press, Cambridge.Heidorn, G., 1999, "Intelligent writing assistance," Toappear in Dale, R., H. Moisl and H. Somers (eds.
), AHandbook of Natural Language Processing Teclmiques,Marcel Dekker.Platt, J., N. Cfistianini, J. Shawe-Taylor, 2000, "Largemargin DAGs for multiclass classification," In Advancesin Neural Information Processing Systems 12, MIT Press.Vapnik, V., 1998, Statistical Learning Theory, Wiley-Interscience, New York.73
