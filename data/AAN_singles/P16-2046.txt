Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 281?286,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsA Neural Network based Approach to Automatic Post-EditingSantanu Pal1, Sudip Kumar Naskar3, Mihaela Vela1, Josef van Genabith1,21Saarland University, Saarbr?ucken, Germany2German Research Center for Artificial Intelligence (DFKI), Germany3Jadavpur University, Kolkata, India{santanu.pal, josef.vangenabith}@uni-saarland.desudip.naskar@cse.jdvu.ac.in, m.vela@mx.uni-saarland.deAbstractWe present a neural network based auto-matic post-editing (APE) system to im-prove raw machine translation (MT) out-put.
Our neural model of APE (NNAPE)is based on a bidirectional recurrent neu-ral network (RNN) model and consists ofan encoder that encodes an MT output intoa fixed-length vector from which a de-coder provides a post-edited (PE) trans-lation.
APE translations produced byNNAPE show statistically significant im-provements of 3.96, 2.68 and 1.35 BLEUpoints absolute over the original MT,phrase-based APE and hierarchical APEoutputs, respectively.
Furthermore, humanevaluation shows that the NNAPE gener-ated PE translations are much better thanthe original MT output.1 IntroductionFor many applications the performance of state-of-the-art MT systems is useful but often far fromperfect.
MT technologies have gained wide ac-ceptance in the localization industry.
Computeraided translation (CAT) has become the de-factostandard in large parts of the translation industrywhich has resulted in a surge of demand for pro-fessional post-editors.
This, in turn, has resultedin substantial quantities of PE data which can beused to develop APE systems.In the context of MT, ?post-editing?
(PE) is de-fined as the correction performed by humans overthe translations produced by an MT system (Vealeand Way, 1997), often with minimal amount ofmanual effort (TAUS Report, 2010) and as a pro-cess of modification rather than revision (Loffler-Laurian, 1985).MT systems primarily make two types of errors?
lexical and reordering errors.
However, due tothe statistical and probabilistic nature of modellingin statistical MT (SMT), the currently dominantMT technology, it is non-trivial to rectify these er-rors in the SMT models.
Post-edited data are of-ten used in incremental MT frameworks as addi-tional training material.
However, often this doesnot fully exploit the potential of these rich PE data:e.g., PE data may just be drowned out by a largeSMT model.
An APE system trained on humanpost-edited data can serve as a MT post-processingmodule which can improve overall performance.An APE system can be considered as an MT sys-tem, translating predictable error patterns in MToutput into their corresponding corrections.APE systems assume the availability of sourcelanguage input text (SLIP), target language MToutput (TLMT) and target language PE data(TLPE).
An APE system can be modelled asan MT system between SLIPTLMTand TLPE.However, if we do not have access to SLIP, buthave sufficiently large amounts of parallel TLMT-TLPEdata, we can still build an APE model be-tween TLMTand TLPE.Translations provided by state-of-the-art MTsystems suffer from a number of errors includingincorrect lexical choice, word ordering, word in-sertion, word deletion, etc.
The APE work pre-sented in this paper is an effort to improve theMT output by rectifying some of these errors.
Forthis purpose we use a deep neural network (DNN)based approach.
Neural MT (NMT) (Kalchbren-ner and Blunsom, 2013; Cho et al, 2014a; Choet al, 2014b) is a newly emerging approach toMT.
On the one hand DNNs represent language ina continuous vector space which eases the mod-elling of semantic similarities (or distance) be-tween phrases or sentences, and on the other handit can also consider contextual information, e.g.,281utilizing all available history information in decid-ing the next target word, which is not an easy taskto model with standard APE systems.Unlike phrase-based APE systems (Simard etal., 2007a; Simard et al, 2007b; Pal, 2015; Palet al, 2015), our NNAPE system builds andtrains a single, large neural network that acceptsa ?draft?
translation (TLMT) and outputs an im-proved translation (TLPE).The remainder of the paper is organized as fol-lows.
Section 2 gives an overview of relevant re-lated work.
The proposed NNAPE system is de-scribed in detail in Section 3.
We present the ex-perimental setup in Section 4.
Section 5 presentsthe results of automatic and human evaluation to-gether with some analysis.
Section 6 concludesthe paper and provides avenues for future work.2 Related WorkAPE has proved to be an effective remedy tosome of the inaccuracies in raw MT output.
APEapproaches cover a wide methodological range.Simard et al (2007a) and Simard et al (2007b)applied SMT for post-editing, handling the repeti-tive nature of errors typically made by rule-basedMT systems.
Lagarda et al (2009) used statis-tical information from the trained SMT modelsfor post-editing of rule-based MT output.
Rosaet al (2012) and Mare?cek et al (2011) applied arule-based approach to APE on the morphologi-cal level.
Denkowski (2015) developed a methodfor real time integration of post-edited MT outputinto the translation model by extracting a gram-mar for each input sentence.
Recent studies haveeven shown that the quality of MT plus PE canexceed the quality of human translation (Fiedererand OBrien, 2009; Koehn, 2009; DePalma andKelly, 2009) as well as the productivity (Zampieriand Vela, 2014) in some cases.Recently, a number of papers have presented theapplication of neural networks in MT (Kalchbren-ner and Blunsom, 2013; ?
; Cho et al, 2014b; Bah-danau et al, 2014).
These approaches typicallyconsist of two components: an encoder encodesa source sentence and a decoder decodes into atarget sentence.In this paper we present a neural network basedapproach to automatic PE (NNAPE).
Our NNAPEmodel is inspired by the MT work of Bah-danau et al (2014) which is based on bidirectionalrecurrent neural networks (RNN).
Unlike Bah-danau et al (2014), we use LSTMs rather thanGRUs as hidden units.
RNNs allow process-ing of arbitrary length sequences, however, theyare susceptible to the problem of vanishing andexploding gradients (Bengio et al, 1994).
Totackle vanishing gradients in RNNs, two archi-tectures are generally used: gated recurrent units(GRU) (Cho et al, 2014b) and long-short termmemory (LSTM) (Hochreiter and Schmidhuber,1997).
According to empirical studies (Chung etal., 2014; J?ozefowicz et al, 2015) both architec-tures yield comparable performance.
GRUs tendto train faster than LSTMs.
On the other hand,given sufficient amounts of training data, LSTMsmay lead to better results.
Since our task is mono-lingual and we have more than 200K sentencepairs for training, we use a full LSTM (as the hid-den units) to model our NNAPE system.The model takes TLMTas input and providesTLPEas output.
To the best of our knowledge thework presented in this paper is the first approachto APE using neural networks.3 Neural Network based APEThe NNAPE system is based on a bidirectional(forward-backward) RNN based encoder-decoder.3.1 A Bidirectional RNN APEEncoder-DecoderOur NNAPE model encodes a variable-length se-quence of TLMT(e.g.
x = x1, x2, x3...xm) intoa fixed-length vector representation and then de-codes a given fixed-length vector representationback into a variable-length sequence of TLPE(e.g.
y = y1, y2, y3...yn).
Input and output se-quence lengths, m and n, may differ.A Bidirectional RNN encoder consists of for-ward and backward RNNs.
The forward RNN en-coder reads in each x sequentially from x1to xmand at each time step t, the hidden state htof theRNN is updated by using a non-linear activationfunction f (Equation 1), an elementwise logisticsigmoid with an LSTM unit.ht= f(ht?1, xt) (1)Similarly, the backward RNN encoder reads theinput sequence and calculates hidden states in re-verse direction (i.e.
xmto x1and hmto h1respec-tively).
After reading the entire input sequence,the hidden state of the RNN is provided a sum-mary c context vector (?C?
in Figure 1) of thewhole input sequence.282Figure 1: Generating the tthTLPEword ytfor agiven TLMT(x) by our NNAPE System.The decoder is another RNN trained to generatethe output sequence by predicting the next word ytgiven the hidden state ?tand the context vector ct(c.f., Figure1).
The hidden state of the decoder attime t is computed as given below.P (yt|y1, ...yt?1, x) = f(?t, yt?1, ct) (2)?t= f(?t?1, yt?1, ct) (3)The context vector ctcan be computed asct=m?i=1?tihi(4)Here, ?ti, is the weight of each hiand can be com-puted as?ti=exp(eti)?mj=1exp(etj)(5)where eti= a(?t?1, hi) is an alignment modelwhich provides a matching score between the in-puts around position i and the output at positiont.
The alignment score is based on the ithannota-tion hiof the input sentence and the RNN hiddenstate ?t?1.
The alignment model itself is a feed-forward neural network which directly computes asoft alignment that allows the gradient of the costfunction to be backpropagated through.
The gra-dient is used to train the alignment model as wellas the TLMT?TLPEtranslation model jointly.The alignment model is computed m?
n timesas follows:a(?t?1, hi) = vTatanh(Wa?t?1+ Uahi) (6)where Wa?
Rnh?nh, Ua?
Rnh?2nhand va?Rnhare the weight matrices of nhhidden units.4 ExperimentsWe evaluate the model on an English?Italian APEtask, which is detailed in the following subsec-tions.4.1 DataThe training data used for the experiments was de-veloped in the MateCat1project and consists of312K TLMT?TLPEparallel sentences.
The par-allel sentences are (English to) Italian MT out-put and their corresponding (human) post-editedItalian translations.
Google Translate (GT) is theMT engine which provided the original ItalianTLMToutput.
The data includes sentences fromthe Europarl corpus as well as news commen-taries.
Since the data contains some non-Italiansentences, we applied automatic language identifi-cation (Shuyo, 2010) in order to select only Italiansentences.
Automatic cleaning and pre-processingof the data was carried out by sorting the entireparallel training corpus based on sentence length,filtering the parallel data on maximum allowablesentence length of 80 and sentence length ratioof 1:2 (either direction), removing duplicates andapplying tokenization and punctuation normaliza-tion using Moses (Koehn et al, 2007) scripts.
Af-ter cleaning the corpus we obtained a sentence-aligned TLMT?TLPEparallel corpus containing213,795 sentence pairs.
We randomly extracted1000 sentence pairs each for the development setand test set from the pre-processed parallel cor-pus and used the remaining (211,795) as the train-ing corpus.
The training data features 57,568 and61,582 unique word types in TLMTand TLPE,respectively.
We chose the 40,000 most frequentwords from both TLMTand TLPEto train ourNNAPE model.
The remaining words whichare not among the most frequent words are re-placed by a special token ([UNK]).
The model wastrained for approximately 35 days, which is equiv-alent to 2,000,000 updates with GPU settings.4.2 Experimental SettingsOur bidirectional RNN Encoder-Decoder contains1000 hidden units for the forward backward RNNencoder and 1000 hidden units for the decoder.1https://www.matecat.com/283The network is basically a multilateral neural net-work with a single maxout unit as hidden layer(Goodfellow et al, 2013) to compute the condi-tional probability of each target word.
The wordembedding vector dimension is 620 and the sizeof the maxout hidden layer in the deep output is500.
The number of hidden units in the alignmentmodel is 1000.
The model has been trained ona mini-batched stochastic gradient descent (SGD)with ?Adadelta?
(Zeiler, 2012).
The main rea-son behind the use of ?Adadelta?
is to automat-ically adapt the learning rate of each parameter( = 10?6and ?
= 0.95).
Each SGD update di-rection is computed using a mini-batch of 80 sen-tences.We compare our NNAPE system with state-of-the-art phrase-based (Simard et al, 2007b) as wellas hierarchical phrase-based APE (Pal, 2015) sys-tems.
We also compare the output provided byour system against the original GT output.
Forbuilding the phrase-based and hierarchical phrase-based APE systems, we set maximum phraselength to 7.
A 5-gram language model built usingKenLM (Heafield, 2011) was used for decoding.System tuning was carried out using both k-bestMIRA (Cherry and Foster, 2012) and MinimumError Rate Training (MERT) (Och, 2003) on theheld-out development set (devset).
After parame-ters were tuned, decoding was carried out on theheld out test set.5 EvaluationThe performance of the NNAPE system was eval-uated using both automatic and human evaluationmethods, as described below.5.1 Automatic EvaluationThe output of the NNAPE system on the 1000sentences testset was evaluated using three MTevaluation metrics: BLEU (Papineni et al, 2002),TER (Snover et al, 2006) and Meteor (Denkowskiand Lavie, 2011).
Table 1 provides a comparisonof our neural system performance against the base-line phrase-based APE (S1), baseline hierarchicalphrase-based APE (S2) and the original GT output.We use a, b, c, and d to indicate statistical signif-icance over GT, S1, S2and our NNAPE system(NN), respectively.
For example, the S2BLEUscore 63.87a,bin Table 1 means that the improve-ment provided by S2in BLEU is statistically sig-nificant over Google Translator and phrase-basedAPE.
Table 1 shows that S1provides statisticallysignificant (0.01 < p < 0.04) improvements overGT across all metrics.
Similarly S2yields statis-tically significant (p < 0.01) improvements overboth GT and S1across all metrics.
The NN sys-tem performs best and results in statistically sig-nificant (p < 0.01) improvements over all othersystems across all metrics.
A systematic trend(NN > S2> S1> GT ) can be observed in Ta-ble 1 and the improvements are consistent acrossthe different metrics.
The relative performancegain achieved by NN over GT is highest in TER.System BLEU TER METEORGT (a) 61.26 30.94 72.73S1(b) 62.54a29.49a73.21aS2(c) 63.87a,b28.67a,b73.63a,bNN (d) 65.22a,b,c27.56a,b,c74.59a,b,cTable 1: Automatic evaluation.5.2 Human EvaluationHuman evaluation was carried out by four profes-sional translators, native speakers of Italian, withprofessional translation experience between oneand two years.
Since human evaluation is verycostly and time consuming, it was carried out on asmall portion of the test set consisting of 145 ran-domly sampled sentences and only compared NNwith the original GT output.
We used a pollingscheme with three different options.
Translatorschoose which of the two (GT or NN) outputs isthe better translation or whether there is a tie (?un-certain?).
To avoid any bias towards any particularsystem, the order in which two system outputs arepresented is randomized so that the translators donot know which system they are contributing theirvotes to.We analyzed the outcome of the voting pro-cess (4 translators each giving 145 votes) andfound that the winning NN system received 285(49.13%) votes compared to 99 (17.07%) votesreceived by the GT system, while the rest of thevotes (196, 33.79%) go to the ?uncertain?
option.We measured pairwise inter-annotator agreementbetween the translators by computing Cohen?s ?coefficient (Cohen, 1960) reported in Table 2.
Theoverall ?
coefficient is 0.330.
According to (Lan-dis and Koch, 1977) this correlation coefficientcan be interpreted as fair.284Cohen?s ?
T1 T2 T3 T4T1 - 0.141 0.424 0.398T2 0.141 - 0.232 0.540T3 0.424 0.232 - 0.248T4 0.398 0.540 0.248 -Table 2: Pairwise correlation between translatorsin the evaluation process.5.3 AnalysisThe results of the automatic evaluation show thatNNAPE has advantages over the phrase-based andhierarchical APE approaches.
On manual inspec-tion we found that the NNAPE system drasticallyreduced the preposition insertion and deletion er-ror in Italian GT output and was also able to han-dle the improper use of prepositions and determin-ers (e.g.
?states?
?
?dei stati?, ?the states?
?
?gli stati?).
The use of a bidirectional RNN neu-ral model makes the model sensitive towards con-texts.
Moreover, NNAPE captures global reorder-ing by capturing contextual features which helpsto reduce word ordering errors to some extent.6 Conclusion and Future WorkThe NNAPE system provides statistically sig-nificant improvements over existing state-of-the-art APE models and produces significantly bet-ter translations than GT which is very difficultto beat.
This enhancement in translation qual-ity through APE should reduce human PE effort.Human evaluation revealed that the NNAPE gen-erated PE translations contain less lexical errors,NNAPE rectifies erroneous word insertions anddeletions, and improves word ordering.In future, we would like to test our system in areal-life translation scenario to analyze productiv-ity gains in a commercial environment.
We alsowant to extend the APE system by incorporatingsource language knowledge into the network andcompare LSTM against GRU hidden units.AcknowledgmentsWe would like to thank all the anonymous re-viewers for their feedback.
We are also thank-ful to Translated SRL, Rome, Italy.
They haveshared their data for the experiments and enabledthe manual evaluation of our system.Santanu Pal is supported by the People Pro-gramme (Marie Curie Actions) of the EuropeanUnion?s Framework Programme (FP7/2007-2013)under REA grant agreement no 317471.ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural Machine Translation by JointlyLearning to Align and Translate.
arXiv preprintarXiv:1409.0473.Yoshua Bengio, Patrice Simard, and Paolo Frasconi.1994.
Learning Long-Term Dependencies with Gra-dient Descent is Difficult.
IEEE Transactions onNeural Networks, 5(2):157?166.Colin Cherry and George Foster.
2012.
Batch Tun-ing Strategies for Statistical Machine Translation.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 427?436.KyungHyun Cho, Bart van Merrienboer, Dzmitry Bah-danau, and Yoshua Bengio.
2014a.
On the Prop-erties of Neural Machine Translation: Encoder-Decoder Approaches.
CoRR, abs/1409.1259.Kyunghyun Cho, Bart Van Merri?enboer, C?alarG?ulc?ehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio.
2014b.
Learn-ing Phrase Representations using RNN Encoder?Decoder for Statistical Machine Translation.
In Pro-ceedings of the 2014 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP),pages 1724?1734.Junyoung Chung, C?alar G?ulc?ehre, Kyunghyun Cho,and Yoshua Bengio.
2014.
Empirical Evalua-tion of Gated Recurrent Neural Networks on Se-quence Modeling.
Technical Report Arxiv report1412.3555, Universit?e de Montr?eal.Jacob Cohen.
1960.
A Coefficient of Agreement forNominal Scales.
Educational and PsychologicalMeasurement, 20(1):37?46, April.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic Metric for Reliable Optimizationand Evaluation of Machine Translation Systems.
InProceedings of the EMNLP 2011 Workshop on Sta-tistical Machine Translation, pages 85?91.Michael Denkowski.
2015.
Machine Translation forHuman Translators.
Ph.D. thesis, Carnegie MellonUniversity.Donald A. DePalma and Nataly Kelly.
2009.
ProjectManagement for Crowdsourced Translation: HowUser-Translated Content Projects Work in Real Life.Translation and Localization Project Management:The Art of the Possible, pages 379?408.Rebecca Fiederer and Sharon OBrien.
2009.
Qual-ity and Machine Translation: a Realistic Objective.Journal of Specialised Translation, 11:52?74.285Ian J Goodfellow, David Warde-Farley, Mehdi Mirza,Aaron Courville, and Yoshua Bengio.
2013.
Max-out networks.
arXiv preprint arXiv:1302.4389.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proceedings of theSixth Workshop on Statistical Machine Translation,pages 187?197.Sepp Hochreiter and J?urgen Schmidhuber.
1997.
LongShort-Term Memory.
Neural Comput., 9(8):1735?1780, November.Rafal J?ozefowicz, Wojciech Zaremba, and IlyaSutskever.
2015.
An Empirical Exploration ofRecurrent Network Architectures.
In Proceedingsof the 32nd International Conference on MachineLearning, pages 2342?2350.Nal Kalchbrenner and Phil Blunsom.
2013.
RecurrentContinuous Translation Models.
In Proceedings ofthe 2013 Conference on Empirical Methods in Nat-ural Language Processing, pages 1700?1709.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proceedings of the 45th Annual Meeting of theACL on Interactive Poster and Demonstration Ses-sions, pages 177?180.Philipp Koehn.
2009.
A Process Study of Computer-aided Translation.
Machine Translation, 23(4):241?263.Antonio Lagarda, Vicent Alabau, Francisco Casacu-berta, Roberto Silva, and Enrique D?
?az-de Lia?no.2009.
Statistical post-editing of a rule-based ma-chine translation system.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, Companion Vol-ume: Short Papers, NAACL-Short ?09, pages 217?220.J.
Richard Landis and Gary G. Koch.
1977.
TheMeasurement of Observer Agreement for Categor-ical Data.
Biometrics, 33(1):159?74.Anne-Marie Loffler-Laurian.
1985.
Traduction Au-tomatique et Style.
Babel, 31(2):70?76.David Mare?cek, Rudolf Rosa, Petra Galu?s?c?akov?a, andOnd?rej Bojar.
2011.
Two-step Translation withGrammatical Post-processing.
In Proceedings of theSixth Workshop on Statistical Machine Translation,pages 426?432.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedingsof the 41st Annual Meeting on Association for Com-putational Linguistics - Volume 1, pages 160?167.Santanu Pal, Mihaela Vela, Sudip Kumar Naskar, andJosef van Genabith.
2015.
USAAR-SAPE: AnEnglish?Spanish Statistical Automatic Post-EditingSystem.
In Proceedings of the Tenth Workshop onStatistical Machine Translation, pages 216?221.Santanu Pal.
2015.
Statistical Automatic Post Editing.In The Proceedings of the EXPERT Scientific andTechnological workshop.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318.Rudolf Rosa, David Mare?cek, and Ond?rej Du?sek.2012.
DEPFIX: A System for Automatic Correc-tion of Czech MT Outputs.
In Proceedings of theSeventh Workshop on Statistical Machine Transla-tion, pages 362?368.Nakatani Shuyo.
2010.
Language Detection Libraryfor Java.Michel Simard, Cyril Goutte, and Pierre Isabelle.2007a.
Statistical Phrase-Based Post-Editing.
InHuman Language Technologies 2007: The Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics; Proceedings ofthe Main Conference, pages 508?515.Michel Simard, Nicola Ueffing, Pierre Isabelle, andRoland Kuhn.
2007b.
Rule-Based Translation withStatistical Phrase-Based Post-Editing.
In Proceed-ings of the Second Workshop on Statistical MachineTranslation, pages 203?206.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A studyof Translation Edit Rate with Targeted Human An-notation.
In Proceedings of association for machinetranslation in the Americas, pages 223?231.TAUS Report.
2010.
Post editing in practice.
Techni-cal report, TAUS.Tony Veale and Andy Way.
1997.
Gaijin: A Bootstrap-ping, Template-driven Approach to Example-basedMT.
In Proceedings of the Recent Advances in Nat-ural Language Processing.Marcos Zampieri and Mihaela Vela.
2014.
Quantify-ing the Influence of MT Output in the TranslatorsPerformance: A Case Study in Technical Transla-tion.
In Proceedings of the EACL Workshop on Hu-mans and Computer-assisted Translation (HaCat),pages 93?98.Matthew D. Zeiler.
2012.
ADADELTA: An AdaptiveLearning Rate Method.
arXiv:1212.5701 [cs.LG].286
