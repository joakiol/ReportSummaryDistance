Proceedings of NAACL HLT 2007, pages 460?467,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsCombining Lexical and Grammatical Features to Improve ReadabilityMeasures for First and Second Language TextsMichael J. HeilmanKevyn Collins-ThompsonJamie CallanMaxine EskenaziLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon University4502 Newell Simon HallPittsburgh, PA 15213-8213{mheilman,kct,callan,max}@cs.cmu.eduAbstractThis work evaluates a system that uses in-terpolated predictions of reading difficultythat are based on both vocabulary andgrammatical features.
The combined ap-proach is compared to individual gram-mar- and language modeling-basedapproaches.
While the vocabulary-basedlanguage modeling approach outper-formed the grammar-based approach,grammar-based predictions can be com-bined using confidence scores with thevocabulary-based predictions to producemore accurate predictions of reading dif-ficulty for both first and second languagetexts.
The results also indicate that gram-matical features may play a more impor-tant role in second language readabilitythan in first language readability.1 IntroductionThe REAP tutoring system (Heilman, et al 2006),aims to provide authentic reading materials of theappropriate difficulty level, in terms of both vo-cabulary and grammar, for English as a SecondLanguage students.
An automatic measure of read-ability that incorporated both lexical and gram-matical features was thus needed.For first language (L1) learners (i.e., childrenlearning their native tongue), reading level hasbeen predicted using a variety of techniques, basedon models of a student?s lexicon, grammatical sur-face features such as sentence length (Flesch,1948), or combinations of such features (Schwarmand Ostendorf, 2005).
It was shown by Collins-Thompson and Callan (2004) that a vocabulary-based language modeling approach was effective atpredicting the readability of grades 1 to 12 of Webdocuments of varying length, even with high levelsof noise.Prior work on first language readability bySchwarm and Ostendorf (2005) incorporatedgrammatical surface features such as parse treedepth and average number of verb phrases.
Thiswork combining grammatical and lexical featureswas promising, but it was not clear to what extentthe grammatical features improved predictions.Also, discussions with L2 instructors suggestthat a more detailed grammatical analysis of textsthat examines features such as passive voice andvarious verb tenses can provide better features withwhich to predict reading difficulty.
One goal ofthis work is to show that the use of pedagogicallymotivated grammatical features (e.g., passivevoice, rather than the number of words per sen-tence) can improve readability measures based onlexical features alone.One of the differences between L1 and L2 read-ability is the timeline and processes by which firstand second languages are acquired.
First languageacquisition begins at infancy, and the primarygrammatical structures of the target language areacquired by age four in typically developing chil-460dren (Bates, 2003).
That is, most grammar is ac-quired prior to the beginning of a child?s formaleducation.
Therefore, most grammatical featuresseen at high reading levels such as high school arepresent with similar frequencies at low readinglevels such as grades 1-3 that correspond to ele-mentary school-age children.
It should be notedthat sentence length is one grammar-related differ-ence that can be observed as L1 reading level in-creases.
Sentences are kept short in texts for lowL1 reading levels in order to reduce the cognitiveload on child readers.
The average sentence lengthof texts increases with the age and reading level ofthe intended audience.
This phenomenon has beenutilized in early readability measures (Flesch,1948).
Vocabulary change, however, continueseven into adulthood, and has been shown to be amore effective predictor of L1 readability thansimpler measures such as sentence length (Collins-Thompson and Callan, 2005).Second language learners, unlike their L1 coun-terparts, are still very much in the process of ac-quiring the grammar of their target language.
Infact, even intermediate and advanced students ofsecond languages, who correspond to higher L2reading levels, often struggle with the grammaticalstructures of their target language.
This phenome-non suggests that grammatical features may play amore important role in predicting and measuringL2 readability.
That is not to say, however, thatvocabulary cannot be used to predict L2 readinglevels.
Second language learners are learning bothvocabulary and grammar concurrently, and readingmaterials for this population are chosen or au-thored according to both lexical and grammaticalcomplexity.
Therefore, the authors predict that areadability measure for texts intended for secondlanguage learners that incorporates both grammati-cal and lexical features could clearly outperform ameasure based on only one of these two types offeatures.This paper begins with descriptions of the lan-guage modeling and grammar-based predictionsystems.
A description of the experiments followsthat covers both the evaluation metrics and corporaused.
Experimental results are presented, followedby a discussion of these results, and a summary ofthe conclusions of this work.2 Language Model Readability Predictionfor First Language TextsStatistical language modeling exploits patterns ofuse in language.
To build a statistical model oftext, training examples are used to collect statisticssuch as word frequency and order.
Each trainingexample has a label that tells the model the ?true?category of the example.
In this approach, onestatistical model is built for each grade level to bepredicted.The statistical language modeling approach hasseveral advantages over traditional readabilityformulas, which are usually based on linear regres-sion with two or three variables.
First, a languagemodeling approach generally gives much betteraccuracy for Web documents and short passages(Collins-Thompson and Callan, 2004).
Second,language modeling provides a probability distribu-tion across all grade models, not just a single pre-diction.
Third, language modeling provides moredata on the relative difficulty of each word in thedocument.
This might allow an application, forexample, to provide more accurate vocabulary as-sistance.The statistical model used for this study isbased on a variation of the multinomial Na?veBayes classifier.
For a given text passage T, thesemantic difficulty of T relative to a specific gradelevel Gi is predicted by calculating the likelihoodthat the words of T were generated from a repre-sentative language model of Gi.
This likelihood iscalculated for each of a number of language mod-els, corresponding to reading difficulty levels.
Thereading difficulty of the passage is then estimatedas the grade level of the language model mostlikely to have generated the passage T.The language models employed in this work aresimple: they are based on unigrams and assumethat the probability of a token is independent of thesurrounding tokens.
A unigram language model issimply defined by a list of types (words) and theirindividual probabilities.
Although this is a weakmodel, it can be effectively trained from less la-beled data than more complex models, such as bi-gram or trigram models.
Additionally, higherorder n-gram models might capture grammatical aswell as lexical differences.
The relative contribu-tions of grammatical and lexical features were thusbetter distinguished by using unigram language461models that more exclusively focus on lexical dif-ferences.In this language modeling approach, a genera-tive model is assumed for a passage T, in which ahypothetical author generates the tokens of T by:1.
Choosing a grade language model, Gi,from the set G = {Gi} of 12 unigram languagemodels, according to a prior probability distri-bution P(Gi).2.
Choosing a passage length |T| in tokens ac-cording to a probability distribution P(|T|).3.
Sampling |T| tokens from Gi?s multinomialword distribution according to the ?na?ve?
as-sumption that each token is independent of allother tokens in the passage, given the languagemodel Gi.These assumptions lead to the following expres-sion for the probability of T being generated bylanguage model Gi according to a multinomial dis-tribution:?
?=VwwCiiwCGwPTTPGTP )!
()|(|!||)(|)|()(Next, according to Bayes?
Theorem:)()|()()|(TPGTPGPTGP iii = .Substituting (1) into (2), taking logarithms, andsimplifying produces:SRwCGwPwCTGPVwiVwiloglog)!(log)|(log)()|(log++?=?
?,where V is the list of all types in the passage T, w isa type in V, and C(w) is the number of tokens withtype w in T.  For simplicity, the factor R representsthe contribution of the prior P(Gi), and S representsthe contribution of the passage length |T|, given thegrade level.Two further assumptions are made to simplifythe illustration:1.
That all grades are equally likely a priori.That is,Gi NGP 1)( =  where NG is the numberof grade levels.
For example, if there are 12grade levels, then NG = 12.
This allows log R tobe ignored.2.
That all passage lengths (up to a maximumlength M) are equally likely.
This allows log Sto be ignored.These may be poor assumptions in a real appli-cation, but they can be easily included or excludedin the model as desired.
The log C(w)!
term canalso be ignored because it is constant across levels.Under these conditions, an extremely simple formfor the grade likelihood remains.
In order to findwhich model Gi maximizes Equation (3), themodel which Gi that maximizes the followingequation must be found:)|(log)()|( iVwi GwPwCGTL ?=This is straightforward to compute: for each tokenin the passage T, the log probability of the tokenaccording to the language model of Gi is calcu-lated.
Summing the log probabilities of all tokensproduces the overall likelihood of the passage,given the grade.
The grade level with the maxi-mum likelihood is then chosen as the final read-ability level prediction.This study employs a slightly more sophisti-cated extension of this model, in which a slidingwindow is moved across the text, with a grade pre-diction being made for each window.
This resultsin a distribution of grade predictions.
The gradelevel corresponding to a given percentile of thisdistribution is chosen as the prediction for the en-tire document.
The values used in these experi-ments for the percentile thresholds for L1 and L2were chosen by accuracy on held-out data.3 Grammatical Construction ReadabilityPrediction for Second Language TextsThe following sections describe the approach topredicting readability based on grammatical fea-tures.
As with any classifier, two components arerequired to classify texts by their reading level:first, a definition for and method of identifyingfeatures; second, an algorithm for using these fea-tures to classify a given text.
A third component,training data, is also necessary in this classification462task.
The corpus of materials used for training andtesting is discussed in a subsequent section.3.1 Features for Grammar-based PredictionL2 learners usually learn grammatical patterns ex-plicitly from grammar explanations in L2 text-books, unlike their L1 counterparts who learn themimplicitly through natural interactions.
Grammati-cal features would therefore seem to be an essentialcomponent of an automatic readability measure forL2 learners, who must actively acquire both thelexicon and grammar of their target language.The grammar-based readability measure relieson being able to automatically identify grammati-cal constructions in text.
Doing so is a multi-stepprocess that begins by syntactically parsing thedocument.
The Stanford Parser (Klein and Man-ning, 2002) was used to produce constituent struc-ture trees.
The choice of parser is not essential tothe approach, although the accuracy of parsingdoes play a role in successful identification of cer-tain grammatical patterns.
PCFG scores from theparser were also used to filter out some of the ill-formed text present in the test corpora.
The defaulttraining set of Penn Treebank (Marcus et al 1993)was used for the parser because the domain andstyle of those texts actually matches fairly wellwith the domain and style of the texts on which areading level predictor for second language learn-ers might be used.Once a document is parsed, the predictor usesTgrep2 (Rohde, 2005), a tree structure searchingtool, to identify instances of the target patterns.
ATgrep2 pattern defines dominance, sisterhood,precedence, and other relationships between nodesin the parse tree for a sentence.
A pattern can alsoplace constraints on the terminal symbols (e.g.,words and punctuation), such that a pattern mightrequire a form of the copula ?be?
to exist in a cer-tain position in the construction.
An example of aTGrep2 search pattern for the progressive verbtense is the following:?VP < /^VB/ < (VP < VBG)?Searching for this pattern returns sentences inwhich a verb phrase (VP) dominates an auxiliaryverb (whose symbol begins with VB) as well asanother verb phrase, which in turn dominates averb in gerund form (VBG).
An example of amatching sentence is, ?The student was reading abook,?
shown in Figure 2.Figure 2: The parse tree for an example sentencethat matches a pattern for progressive verb tense.A set of 22 relevant grammatical constructionswere identified from grammar textbooks for threedifferent ESL levels (Fuchs et al, 2005).
Thesegrammar textbooks had different authors and pub-lishers than the ones used in the evaluation corporain order to minimize the chance of experimentalresults not generalizing beyond the specific materi-als employed in this study.
The ESL levels corre-spond to the low-intermediate (hereafter, level 3),high-intermediate (level 4), and advanced (level 5)courses at the University of Pittsburgh?s EnglishLanguage Institute.
The constructions identified inthese grammar textbooks were then implementedin the form of Tgrep2 patterns.Feature  Lowest Level Highest LevelPassive Voice 0.11 0.71Past Participle 0.28 1.63Perfect Tense 0.01 0.33Relative Clause 0.54 0.60ContinuousTense0.19 0.27Modal 0.80 1.44Table 1: The rates of occurrence per 100 words ofa few of the features used by the grammar-basedpredictor.
Rates are shown for the lowest (2) andhighest (5) levels in the L2 corpus.The rate of occurrence of constructions wascalculated on a per word basis.
A per-word rathera bookThe studentSVPVBD VPVBGNPwasreadingNP463than a per-sentence measure was chosen because aper-sentence measure would depend too greatly onsentence length, which also varies by level.
It wasalso desirable to avoid having sentence length con-founded with other features.
Table 1 shows thatthe rates of occurrence of certain constructions be-come more frequent as level increases.
This sys-tematic variation across levels is the basis for thegrammar-based readability predictions.A second feature set was defined that consistedof 12 grammatical features that could easily beidentified without computationally intensive syn-tactic parsing.
These features included sentencelength, the various verb forms in English, includ-ing the present, progressive, past, perfect, continu-ous tenses, as well as part of speech labels forwords.
The goal of using a second feature set wasto examine how dependent prediction quality wason a specific set of features, as well as to test theextent to which the output of syntactic parsingmight improve prediction accuracy.3.2 Algorithm for Grammatical Feature-based ClassificationA k-Nearest Neighbor (kNN) algorithm is used forclassification based on the grammatical featuresdescribed above.
The kNN algorithm is an in-stance-based learning technique originally devel-oped by Cover and Hart (1967) by which a testinstance is classified according to the classifica-tions of a given number (k) of training instancesclosest to it.
Distance is defined in this work as theEuclidean distance of feature vectors.
Mitchell(1997) provides more details on the kNN algo-rithm.
This algorithm was chosen because it hasbeen shown to be effective in text classificationtasks when compared to other popular methods(Yang 1999).
A k value of 12 was chosen becauseit provided the best performance on held-out data.Additionally, it is straightforward to calculatea confidence measure with which kNN predictionscan be combined with predictions from other clas-sifiers?in this case with predictions from the uni-gram language modeling-based approach describedabove.
A confidence measure was important inthis task because it provided a means with which tocombine the grammar-based predictions with thepredictions from the language modeling-basedpredictor while maintaining separate models foreach type of feature.
These separate models weremaintained to better determine the relative contri-butions of grammatical and lexical features.A static linear interpolation of predictions us-ing the two approaches led to only minimal reduc-tions of prediction error, likely because predictionsfrom the poorer performing grammar-based classi-fier were always given the same weight.
However,with the confidence measures, predictions from thegrammar-based classifier could be given moreweight when the confidence measure was high, andless weight when the measure was low and thepredictions were likely to be inaccurate.
The case-dependent interpolation of prediction values al-lowed for the effective combination of languagemodeling- and grammar-based predictions.The confidence measure employed is the pro-portion of the k most similar training examples, ornearest neighbors, that agree with the final labelchosen for a given test document.
For example, ifseven of ten neighbors have the same label, thenthe confidence score will be 0.6.
The interpolatedreadability prediction value is calculated as fol-lows:LI = LLM + CkNN * LGR,where LLM is the language model-based prediction,LGR is the grammar-based prediction from the kNNalgorithm, and CkNN is the confidence value for thekNN prediction.
The language modeling approachis treated as a black box, but it would likely bebeneficial to have confidence measures for it aswell.4 Descriptions of ExperimentsThis section describes the experiments used to testthe hypothesis that grammar-based features canimprove readability measures for English, espe-cially for second language texts.
The measuresand cross-validation setup are described.
A de-scription of the evaluation corpora of labeled firstand second language texts follows.4.1 Experimental SetupTwo measurements were used in evaluating theeffectiveness of the reading level predictions.First, the correlation coefficient evaluated whetherthe trends of prediction values matched the trendsfor human-labeled texts.
Second, the meansquared error of prediction values provided a464measure of how correct each of the predictors wason average,  penalizing more severe errors moreheavily.
Mean square error was used rather thansimple accuracy (i.e., number correct divided bysample size) because the task of readability predic-tion is more akin to regression than classification.Evaluation measures such as accuracy, precision,and recall are thus less meaningful for readabilityprediction tasks because they do not capture thefact that an error of 4 levels is more costly than anerror of a single level.A nine-fold cross-validation was employed.The data was first split into ten sets.
One set wasused as held-out data for selecting the parameter kfor the kNN algorithm and the percentile value forthe language modeling predictor, and then the re-maining nine were used to evaluate the quality ofpredictions.
Each of these nine was in turn se-lected as the test set, and the other eight were usedas training data.4.2 Corpora of Labeled TextsTwo corpora of labeled texts were used in theevaluation.
The first corpus was from a set of textsgathered from the Web for a prior evaluation of thelanguage modeling approach.
The 362 texts hadbeen assigned L1 levels (1-12) by grade schoolteachers, and consisted of approximately 250,000words.
For more details on the L1 corpus, see(Collins-Thompson and Callan, 2005).The second corpora consisted of textbook mate-rials (Adelson-Goldstein and Howard, 2004, forlevel 2; Ediger and Pavlik, 2000, for levels 3 and 4;Silberstein, 2002, for level 5) from a series of Eng-lish as a Second Language reading courses at theEnglish Language Institute at the University ofPittsburgh.
The four reading practice textbooksthat constitute this corpus were from separate au-thors and publishers than the grammar textbooksused to select and define grammatical features.The reading textbooks in the corpus are used incourses intended for beginning (level 2) throughadvanced (level 5) students.
The textbooks werescanned into electronic format, and divided intofifty roughly equally sized files.
This second lan-guage corpus consisted of approximately 200,000words.Although the sources and formats of the twocorpora were different, they share a number ofcharacteristics.
Their size was roughly equal.
Thedocuments in both were also fairly but not per-fectly evenly distributed across the levels.
Bothcorpora also contained a significant amount ofnoise which made accurate prediction of readinglevel more challenging.
The L1 corpus was fromthe Web, and therefore contained navigationmenus, links, and the like.
The texts in the L2 cor-pus also contained significant levels of noise due tothe inclusion of directions preceding readings, ex-ercises and questions following readings, as well aslabels on figures and charts.
The scanned fileswere not hand-corrected in this study, in part to testthat the measures are robust to noise, which is pre-sent in the Web documents for which the readabil-ity measures are employed in the REAP tutoringsystem.The grammar-based prediction seems to bemore significantly negatively affected by the noisein the two corpora because the features rely moreon dependencies between different words in thetext.
For example, if a word happened to be part ofan image caption rather than a well-formed sen-tence, the unigram language modeling approachwould only be affected for that word, but thegrammar-based approach might be affected forfeatures spanning an entire clause or sentence.5 Results of ExperimentsThe results show that for both the first and sec-ond language corpora, the language modeling(LM) approach alone produced more accurate pre-dictions than the grammar-based approach alone.The mean squared error values (Table 2) werelower, and the correlation coefficients (Table 3)were higher for the LM predictor than the gram-mar-based predictor.The results also indicate that while grammar-based predictions are not as accurate as the vo-cabulary-based scores, they can be combined withvocabulary-based scores to produce more accurateinterpolated scores.
The interpolated predictionscombined by using the kNN confidence measurewere slightly and in most tests significantly moreaccurate in terms of mean squared error than thepredictions from either single measure.
Interpola-tion using the first set of grammatical features ledto 7% and 22% reductions in mean squared erroron the L1 and L2 corpora, respectively.
These re-sults were verified using a one-tailed paired t-test465of the squared error values of the predictions, andsignificance levels are indicated in Table 2.Mean Squared Error ValuesTest Set (Num.
Levels) L1(12) L2(4)Language Modeling 5.02 0.51Grammar 10.27 1.08Interpolation 4.65* 0.40**Grammar2 (feature set #2) 12.77 1.26Interp2.
(feature set #2) 4.73 0.43*Table 2.
Comparison of Mean Squared Error ofpredictions compared to human labels for differentmethods.
Interpolated values are significantly bet-ter compared to language modeling predictionswhere indicated (* = p<0.05, ** = p<0.01).Correlation CoefficientsTest Set (Num.
Levels) L1(12) L2(4)Language Modeling 0.71 0.80Grammar 0.46 0.55Interpolation 0.72 0.83Grammar2 (feature set #2) 0.34 0.48Interp2.
(feature set #2) 0.72 0.81Table 3.
Comparison of Correlation Coefficientsof prediction values to human labels for differentprediction methods.The trends were similar for both sets of gram-matical features.
However, the first set of featuresthat included complex syntactic constructs led tobetter performance than the second set, which in-cluded only verb tenses, part of speech labels, andsentence length.
Therefore, when syntactic parsingis not feasible because of corpora size, it seemsthat grammatical features requiring only part-of-speech tagging and word counts may still improvereadability predictions.
This is practically impor-tant because parsing can be too computationallyintensive for large corpora.All prediction methods performed better, interms of correlations, on the L2 corpus than on theL1 corpus.
The L2 corpus is somewhat smaller insize and should, if only on the basis of training ma-terial available to the prediction algorithms, actu-ally be more difficult to predict than the L1 corpus.To ensure that the range of levels was not causingthe four-level L2 corpus to have higher predictionsthan the twelve-level L1 corpus, the L1 corpus wasalso divided into four bins (grades 1-3, 4-6, 7-9,10-12).
The accuracy of predictions for the binnedversion of the L1 corpus was not substantially dif-ferent than for the 12-level version.6 DiscussionIn the experimental tests, the LM approach wasmore effective for measuring both L1 and L2 read-ability.
There are several potential causes of thiseffect.
First, the language modeling approach canutilize all the words as they appear in the text asfeatures, while the grammatical features were cho-sen and defined manually.
As a result, the LMapproach can make measurements on a text for asmany features as there are words in its lexicon.Additionally, the noise present in the corpora likelyaffected the grammar-based approach dispropor-tionately more because that method relies on accu-rate parsing of relationships between words.Additionally, English is a morphologically im-poverished language compared to most languages.Text classification, information retrieval, and manyother human language technology tasks can be ac-complished for English without accounting forgrammatical features such as morphological inflec-tions.
For example, an information retrieval sys-tem can perform reasonably well in Englishwithout performing stemming, which does notgreatly increase performance except when queriesand documents are short (Krovetz, 1993).However, most languages have a rich morphol-ogy by which a single root form may have thou-sands or perhaps millions of inflected or derivedforms.
Language technologies must account formorphological features in such languages or thevocabulary grows so large that it becomes unman-ageable.
Lee (2004), for example, showed thatmorphological analysis can improve the quality ofstatistical machine translation for Arabic.
Thus itseems that grammatical features could contributeeven more to measures of readability for texts inother languages.That said, the use of grammatical features ap-pears to play a more important role in readabilitymeasures for L2 than for L1.
When interpolatedwith grammar-based scores, the reduction of meansquared error over the language modeling approachfor L1 was only 7%, while for L2 the reduction orsquared error was 22%.
An evaluation on corporawith less noise would likely bring out these differ-466ences further and show grammar to be an evenmore important factor in second language readabil-ity.
This result is consistent with the fact that sec-ond language learners are still in the process ofacquiring the basic grammatical constructs of theirtarget language.7 ConclusionThe results of this work suggest that grammaticalfeatures can play a role in predicting reading diffi-culty levels for both first and second language textsin English.
Although a vocabulary-based languagemodeling approach outperformed the grammar-based predictor, an interpolated measure usingconfidence scores for the grammar-based predic-tions showed improvement over both individualmeasures.
Also, grammar appears to play a moreimportant role in second language readability thanin first language readability.
Ongoing work aimsto improve grammar-based readability by reducingnoise in training data, automatically creating largergrammar feature sets, and applying more sophisti-cated modeling techniques.8 AcknowledgementsWe would like to acknowledge Lori Levin for use-ful advice regarding grammatical constructions, aswell as the anonymous reviewers for their sugges-tions.This material is based on work supported byNSF grant IIS-0096139 and Dept.
of Educationgrant R305G03123.
Any opinions, findings, con-clusions or recommendations expressed in this ma-terial are the authors', and do not necessarily reflectthose of the sponsors.ReferencesJ.
Adelson-Goldstein and L. Howard.
2004.
Read andReflect 1.
Oxford University Press, USA.E.
Bates.
2003.
On the nature and nurture of language.In R. Levi-Montalcini, D. Baltimore, R. Dulbecco, F.Jacob, E. Bizzi, P. Calissano, & V. Volterra (Eds.
),Frontiers of biology: The brain of Homo sapiens (pp.241?265).
Rome: Istituto della Enciclopedia Italianafondata da Giovanni Trecanni.M.
Fuchs, M. Bonner, M. Westheimer.
2005.
Focus onGrammar, 3rd Edition.
Pearson ESL.K.
Collins-Thompson and J. Callan.
2004.
A languagemodeling approach to predicting reading difficulty.Proceedings of the HLT/NAACL Annual Conference.T.
Cover and P. Hart.
1967.
Nearest neighbor patternclassification.
IEEE Transactions on InformationTheory, 13, 21-27.A.
Ediger and C. Pavlik.
2000.
Reading ConnectionsIntermediate.
Oxford University Press, USA.A.
Ediger and C. Pavlik.
2000.
Reading ConnectionsHigh Intermediate.
Oxford University Press, USA.M.
Heilman, K. Collins-Thompson, J. Callan & M. Es-kenazi.
2006.
Classroom success of an Intelligent Tu-toring System for lexical practice and readingcomprehension.
Proceedings of the Ninth Interna-tional Conference on Spoken Language Processing.D.
Klein and C. D. Manning.
2002.
Fast Exact Inferencewith a Factored Model for Natural Language Parsing.Advances in Neural Information Processing Systems15 (NIPS 2002), December 2002.R.
Krovetz.
1993.
Viewing morphology as an inferenceprocess.
SIGIR-93, 191?202.Y.
Lee.
2004.
Morphological Analysis for StatisticalMachine Translation.
Proceedings of theHLT/NAACL Annual Conference.M.
Marcus, B. Santorini and M. Marcinkiewicz.
1993.
"Building a large annotated corpus of English: thePenn Treebank."
Computational Linguistics, 19(2).T.
Mitchell.
1997.
Machine Learning.
The McGraw-Hill Companies, Inc.  pp.
231-236.D.
Rohde.
2005.
Tgrep2 User Manual.http://tedlab.mit.edu/~dr/Tgrep2/tgrep2.pdf.S.
Schwarm, and M. Ostendorf.
2005.
Reading LevelAssessment Using Support Vector Machines and Sta-tistical Language Models.
Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics.S.
Silberstein, B. K. Dobson, and M. A. Clarke.
2002.Reader's Choice, 4th edition.
University of MichiganPress/ESL.Y.
Yang.
1999.
A re-examination of text categorizationmethods.
Proceedings of ACM SIGIR Conference onResearch and Development in Information Retrieval(SIGIR'99, pp 42--49).467
