Improvements in the Stochastic Segment Model for Phoneme RecognitionV.
Digalakis t M. Ostendofft  J .R .
Rohlicek:~tBoston University~BBN Systems and Technologies Corp.ABSTRACTThe heart of a speech recognition system is the acousticmodel of sub-word units (e.g., phonemes).
In this workwe discuss refinements of the stochastic segment model,an alternative to hidden Markov models for representa-tion of the acoustic variability of phonemes.
We con-centrate on mechanisms for better modelling time corre-lation of features across an entire segment.
Results arepresented for speaker-independent phoneme classifica-tion in continuous speech based on the 'lIMIT 0a!~base.INTRODUCTIONAlthough hidden Markov models (HMMs) are cur-rently one of the most successful approaches to acous-tic modelling for continuous peech recognition, theirperformance is limited in part became of the as-sumption that observation features at different imesare conditionally independent given the underlyingstate sequence and because the Markov assumptionon the state sequence may not adequately modeltime structure.
An alternative model, the stochas-tic segment model (SSlVl), was proposed to overcomesome of these deficiencies \[Roucos and Dunham 1987,Ostendorf and Roucos 1989, Roucos et al1988\].An observed segment of speech (e.g., a phomeme)is represented by a sequence of q-dimensional featurevectors Y = \[~ ~ ... yt\] T, where the length kis variable and T denotes block transposition.
Thestochastic segment model for Y has two components\[Roncos et al1988\]: 1) a time transformation Tk tomodel the variable-length observed segment, Y, interms of a fixed-length unobserved sequence, X ffi\[zl z2 ... z,,~\] a', as Y = TkX, and 2) a probabilis-tic representation f the unobserved feature sequence X.The conditional density of the observed segment Y givenphoneme a and observed length k is:fp(Yl, , k) = ?
pCXI, )dX.JX :Y= TaXAssuming the observed length is less than or equal to thelength of X, k _< m, T~ is a time-warping transformationwhich obtains Y by selecting a subset of elements of Xand the density p(Yla, k) is a qk-dimensional marginaldistribution of p(Xlc0.
In practice, we can accomodateobservations of length k > m by either tying distri-butions of X (so rn is effectively larger) or discardingsome of the observations inY.
In this work, as in previ-ous work, the time transformation, T~, is chosen to mapeach observed frame Vi to the nearest model sample zjaccording to a linear time-warping criterion.
The distri-bution p(Xla) for the segment X given the phoneme ais then modelled using an rnq-dirnensional multi-variateGaussian distribution.Algorithms for automatic recognition and training ofthe stochastic segment model are similar to those forhidden Markov modelling.
The maximum a posterioriprobability rule is used for classification of segmentswhen the phoneme segmentation is known:max p(Y lct, k )p( k la)p(~),Qwhere p(kla) is the probability that phoneme a haslength k. A Viterbi search over all possible segmen-tations is used for recognition with unknown segmen-tations.
The models are trained from known segmen-tations using maximum likelihood parameter stimation.When segmentations are unknown, an iterative algorithmbased on automatic segmentation and maximum likeli-hood parameter estimation exists for which increasingthe probability of the observations with each iteration isguaranteed.Initial results with segment-based models have beenencouraging.
A stochastic segment model has pre-viously been used for speaker-dependent phonemeand word recognition, demonstrating that a seg-ment model outperformed a discrete hidden Markovmodel when both models were context-independent\[Ostendorf and Roucos 1989\].
Other segment-basedmodels have also showed encouraging results in speaker-independent applications \[Bush and Kopec 1987,Bocchieri and Doddington 1986,332Makino and Kido 1986, Zue et al1989\].Although the previous results using the stochastic seg-ment model were encouraging, there were several limi-tations.
First, the comparison to HMMs did not clearlyshow the advantages of the segment model since theSSM and the HMM used disparate feature distribu-tions: the segment model used ten continuous distri-butions and the HMM used three discrete distributionsfor each phoneme.
Second, the flexibility of the seg-ment model was not fully exploited because time sam-pies within a segment were assumed independent dueto training data limitations in these speaker-dependentapplications.
Finally, results showed that the context-dependent HMM triphone models \[Schwartz et al1985\]outperformed the context-independent segment models.Again due to training data limitations, context-dependentsegment models were not effective.In this work we address the issues of 1) time correla-tion modelling and 2) meaningful comparisons of theSSM with HMMs in a speaker-independent phonemeclassification task.
In the next section, we describerefinements to the SSM which improve the time cor-relation modelling capability, including time-dependentparameter reduction and assumption of a Markov timecorrelation structure.
Then experimental results usingthe TIMIT database are described.
These results includecomparisons of HMM and SSM, as well as the effects ofmodelling time correlation.
Although the HMM perfor-mauce is similar to segment model performance whentime-sample independence is assumed for the segmentmodel, we demonstrate that the refinements improve per-formance of the stochastic segment model so that it out-performs the HMM for phoneme classification.TIME CORRELATIONMODELLINGPreliminary experiments using full, rru/-dimensional co-variance structure for p(Xla) did not improve perfor-rnance over the block-diagonal structure used when timesamples are assumed to be uncorrelated.
We believethat this was due to insufficient raining data since afull covariance model has roughly m times as many freeparameters a a block diagonal one and since the particu-lar task on which the SSM was tested had a very fimitedamount of training data.
Our efforts have focused on twoparallel approaches of handling the training data prob-lem: devising an effective parameter reduction methodand constraining the structure of the covariance matrixto further educe the number of parameters to estimate.PARAMETER REDUCTIONA first step towards the incorporation of time correlationin the SSM is parameter reduction; an obvious candi-date is the method of linear discriminants \[Wilks 1962\].Our intuition suggested that sample-dependent reductionwould outperform a single transformation for reduction.In fact, contrary to other results \[Brown 1987\], the sin-gle tranformation yielded poor performance (see Section3).Linear discriminant parameter eduction for theSSM was implemented using sample-dependent trans-formations as follows.
The speech segment X =\[Zl z2 .. .
zm\] ~r is substituted by a sequence of lin-ear transformations of the ra original cepstral vectors= \[zl ~2 .
.
.
Zm\] ~ = \[R(I)zI R(2)z2 .
.
.
R(ra)zm\] TThe transformation matrices, R(i), are obtained by solv-ing m generalized eigenvalue problems,S~3~=~4~,  i f f i l ,2 , .
.
.
,mThe classes used at each time instant from 1 to m arethe phonemes, but the between- and within-class cattermatrices (S~) and Sty, respectively) are computed usingonly the observations for that specific sample.MARKOVIAN ASSUMPTIONStructureIn order to obtain the advantages of parameterizationof time correlation without the m-fold increase in thenumber of parameters in the full-covariance case, weconsider a constrained structure for the covariance ma-trix.
More specifically, we assume that the density ofthe unobserved segment is that of a non-homogeneousMarkov processF(X) = i~ (zl)pz(z21zl)p3 (z3 I z2).., pm(zm I zm- I)Under this hypothesis, the number of parameters thathave to be estimated increases by less than a factor of 3over the block-diagonal case (see Table 1).
Furthermore,by introducing the Markov restriction to the covariancemarx ,  we shall also see in the following section that wecan simplify the reestimation formulas for the Estimate-Maximize (EM) algorithm.333Block Diagonal ma" + 3rod 2 2(3nt-2)d ~ 3ntd Markovian -'--'-3-'-- + "-Z--FuLl covariance ' ~  +Table 1: Number of parameters per phone modelParameter EstimationAs mentioned earlier, we assume that the observed seg-ment Y is a "downsampled" version of the underlyingfixed-length "hidden" segment, X.
We used two differ-ent approaches for the parameter stimation problem.Linear Time Upsampling.
The first, linear time up-sampling, interpolates an unobserved sample zi of theunderlying sequence X, by mapping that point to an ob-served frame, y~, with a linear-in-time warping trans-formation of the observed length k to the fixed lengthrn.
The disadvantage of this method is that linear timeupsampling introduces a correlation problem when mod-els with non-independent flames are assumed, and in\[Roucos et al1988\] better esults were reported whenthe parameter estimates were obtained with the EM al-gorithm.
However, in the case of frame dependent trans-formations, a missing observation is not interpolated byan adjacent one, but by a different ransformation ofthat observation.
This partially eliminates the correla-tion problem.Estimate-Maximize Algorithm for the MarkovianCase.
A second approach for the parameter stimationproblem is to use the Estimate-Maxitrdze (EM) algorithmto obtain a maximum likelihood solution under the as-sumptions given in Section 2.2.1.
As defined in Sec-tion 1, X represents he sequence of the incomplete d~!~and Y that of the observed ata.
In this case, the ob-served length k is mapped through a linear time warpingtransformation to the fixed length ra, and each observa-tion y~ is assigned to the closest in time zi.
Under theassumption that rn is always greater than k, there are cer-tain elements of X that have no elements of Y assignedto them, and we refer to them as "missing".
Let/~\[.
andC~ denote the estimates at the r-th iteration of the meanvector of the i-th sample and the cross-covariance b -tween the i-th and j-th samples respectively.
Then thesteps of the EM algorithm are:1.
Estimate step: Estimate the following completedam statistics for each frame or appropriate combinationof frames and each observation:ff zi is observed;ff =i is missing.ff both are observed;ff =~ is missing;ff both are missing.E"C=ilY) = ~ =~'L E'(=dY),\[ ='=~'where j = i or ../= i+ 1, ' denotes transposition and .E"(-)is the expectation operator using the r-th iteration densityestimates.
Under the assumption that the observationsform a Markov chain, we have thatW(=~IY) = W(=dz6,=D, k < i < twhere k and 1 are the immediately last and next non-missing elements of X to i and j.  Let/~ be the mean of=i and Ci~ be the covariance of zi and =$.
AssumingGaussian densities, the conditional expectations become~(=,  1=6, =0 = ~ + q6  v66 (=6 - ~D+,q6 Kz(=~ - ~D++c~vi~(=6 - ~I,) + c~i (=~ - ~)and~"(=,=} I=,,, =,) = q~ - {q6 v66 c;l + q6 ~,q ;+l l l l +~ vi~cT~ + ~ ~i  } + E"(=, I=6, =0E"(=i I=6, =0whereVkk Vk, \] r c~k c~, \] -1and the Vtk, V6z, ~i matrices are obtained from the ma-trix inversion by partitioning lemma.2.
Maximize step: The (r + 1)-th estimates are:XETIXET334where T is the set of all observations for a certainphoneme.
In order to simplify the reestimation formu-las, we also investigated a "forward prediction" type ap-proximation, where the expectations of the r-th step areconditioned only on the last observed sample instead ofboth the last and the next:~'(z~lz~) = ~'I + Cl'~CI~-l(zk - ~'D~ - - l fv r t .
I.~(Z iZ J  Izk)  = ~ ' i  --  ~k ' - '~k  ~kEXPERIMENTAL RESULTSIn this section we present experimental results forspeaker-independent phoneme recognition.
We per-formed experiments on the TIM1T database \[Lamel etal 1986\] for segment models and hidden Markov mod-els using known phonetic segmentations.
Mel-warpedcepstra nd their derivatives, together with the deriva-tive of log power, are used for recognition.
We used61 phonetic models.
However, in counting errors, dif-ferent phones representing the same English phonemecan be substituted for each other without causing an er-ror.
The set of 39 English phonemes that we used isthe same as the ones that the CMU SPHINX and MITSUMMIT systems reported phonetic recognition resultson \[Lee and Hon 1988, Zue et al1989\].The portion of the database that we have availableconsists of 420 speakers and 10 sentences per speaker,two of these, the "sa" sentences, are the same across allspeakers and were not used in either ecognition or train-ing because they would lead to optimistic results.
Wedesignated 219 male and 98 female speakers for training(a total of 2536 training sentences) and a second set of71 male and 32 female different speakers for testing (atotal of 824 test sentences with 31,990 phonemes).
Wedeliberately selected a large number of testing speak-ers to increase the confidence of performance estimates.The best-case results, reported at the end of this section,were obtained using all of the available training sen-tences (from both male and female speakers) and testingover the entire test set.
Most of the other results foralgorithm development and comparisons were obtainedby training over the male speakers only and testing onthe Western and N.Y. dialect male speakers (a total of219 training and 17 test speakers), a subset hat gives usgood estimates of the overall performance aswe can seefrom the global results.SSM w/o duration 67.0%SSM with duration 68.6%HMM 68.7%Table 2: HMM and Segment Comparison (rn = 5, 10cepstra nd deriv., no time correlation).The TIMIT database has also been used by other re-searchers for the evaluation of the phonetic accuracy oftheir speech recognizers.
Lee and Hon, 1988, reporteda phonetic accuracy for the SPHINX system of from58.8% with 12% insertions when context-independent(61) phone models were used.
Zue et al 1989 obtaineda 70% classification performance on the same databasefor unknown speakers.HMM/segment comparison.
We first evaluated therelative performance of the SSM to a continuous Gaus-sian distribution HMM \[Babl et al1981\].
In this ex-periment, he features were ten reel-warped cepstra ndtheir derivatives.
Both the SSM and the HMM had thesame number of distributions (SSM of length rn = 5and no time correlation versus a 7-state, 5 distributionsHMM), and the recognition algorithm for the HMM wasa full search along the observed length.
For the SSM,we obtained results for two cases: with and without us-ing the duration information p(kl~ ).
Note that, for afair comparison, the segment model should include du-ration information, which was not incorporated in ear-tier versions of the segment model.
With the durationinformation, both the SSM and the HMM gave simi-lar performance (68.6% and 68.7%, see Table 2).
Thesuperiority of the SSM becomes clearer after the timecorrelation and the parameter reduction methods are in-corporated, even though the SSM with lime correlationsuffered from limited training.Parameter Reduction.
We compared the single andmultiple transformation reduction methods on a single-speaker task, using 14 mel-warped cepstra (but not theirderivatives) as original features.
We evaluated the recog-nition performance of the SSM for 1) different numbersof the original cepstral coefficients (from 4 up to 14), 2)different number of linear discriminants obtained using asingle transformation a d 3) different numbers of linear33580 i i706 10 16Number of Featurea (q)VSdoQg~IlOI I I/ ?, _Markovian .umpUols0 lna*pmulmst Frmnmt i f f  , I i I , I i0 10 80 80 40Number of Linear Dl~rlmlnan~ (q)Figure 1: Evaluation of parameter reduction methods ona single-speaker task.Figure 2: Performance of different covariance structurefor a segment model length rn = 5 using linear upsam-piing parameter stimation.discriminants obtained from frame dependent transfor-mations (see Figure 1).
The frame dependent featuresgave the best performance of 78.2% when the featureswere reduced to 9 due to training problems, whereasthe single transformation features gave actually lowerperformance than the original features.
This can beexplained by the fact that there was a small between-class scatter for the single Wansformation, relative tothe sample-dependent transformations.
The eigenvaluespread for the single transformation was only 6.2 (ratioof largest o smallest eigenvalue) whereas in the case ofmultiple transformations this ratio ranged from 178.7 to318.3.
A larger ratio occurs at the middle frames sincethe effect of adjacent phonemes i smaller at the middleof a certain phoneme and is easier to discriminate.
Therecognition performance for the single speaker reportedhere was measured on a set of 61 phonemes countingmisrecognized allophones as errors.Time Correlation.
We performed a series of experi-ments on three different types of covatiance matrices forthe SSM.
The length of the SSM in this case was rn = 5.In Figure 2, we have plotted the phonetic accuracy ver-sus the number of features q for 1) a full covariance, 2) aMarkov structure and 3) for a block diagonal covariance(independent frames).
When the number of features issmall and there is enough data to estimate the param-eters, the full covariance model outperforms the othertwo.
It should be noted though that the performanceof the Markovian model is close to that of the full co-variance ven for a small number of features.
Hence,the Markov hypothesis represents well the structure ofthe covariance matrix, and as the number of featuresincreases the Markov model outperforms the full covari-ance model since it is more easily trainable.
In addition,we expect hat the curves for those two models would befurther separated for a bigger segment length m, sincethe number of parameters for the full model is quadraticin ra, whereas that of the Markov model is linear.
(Form = 5 the full model has almost wice as many param-eters as the Markov model).As the number of parameters increases, the inde-pendence assumption gives the best recognition perfor-mance.
However, with more training data the mod-els that use time correlation will outperform the modelwhich does not.
Furthermore, we were able to duplicatethe best case results using a Markov model for the firstfeatures and a second independent distribution for the"less significant" features.
(In this case, the correlation336Method 5-Cepstra \[5-DCepstraLin.Time Resampling 45.7%EM Algorithm 59.0%Forward Approxim.
58.7%10 LD67.3%62.8%60.5%Table 3: Parameter Estimation Algorithms (ra = 8).between the first and the last features i  lost, but the timecorrelation between successive frames compensates forthis).Parameter Est imation Algorithms.
We evaluated thedifferent methods of parameter estimation that we pre-sented in Section 2.
In this set of experiments, we onlyused 10 features (either 5 reel-warped cepstra nd theirderivatives, or 10 linear discriminants) due to limitationsin the available computer time.
A segment of lengthm = 8 rather than the usual rn = 5 was used, in order toobtain a better understanding of the interpolation poten-tial of each algorithm.
The comparative r sults are sum-marized in Table 3.
When cepstra nd their derivativesare used, the EM algorithm clearly gives better esultsthan the linear time upsampling method.
In addition,the "forward prediction" approximation gave us simi-lar recognition performance to the one obtained whenthe full reestirnation formulas were used.
However, thesituation is inverted when the features are linear dis-eriminants, and we refer the reader to Section 2 for anexplanation of those results.Global Results.
The best case system - based on in-dependent samples, m = 5, and 38 linear discriminants- was evaluated using the entire data set.
The classi-fier was trained on the whole training set of all maleand female speakers, and tested on 824 sentences from103 speakers.
As it can be seen in Table 4, where wepresent the results by region and sex, the phoneme clas-sification rate does not have large variations among dif-ferent regions, indicating the robustness of our classifier.The somewhat higher numbers on the male speakers canbe attributed to the fact that approximately 70% of ourtraining set consisted of sentences spoken by male speak-ers and the classifier was biased in this sense.
The re-suits were also consistent among different speakers.
TheRegion \] Males I Females \[ M + P INew England 72.4%Northern 73.5%North Midland 70.8%South Midland 72.4%Southern 73.2%N.Y.City 69.2%Western 72.8%Army Brat 73.8%70.1% 71.4%71.7% 73.1%70.0% 70.6%70.6% 71.8%70.7% 72.2%70.6% 69.7%75.9% 73.6%73.3% 73.7%Total I 72.3% \[ 71.4% \[ 72.1% \[Table 4: Phonetic Accuracyby Region and Sexrecognition rates for all speakers ranged from 59.9% to80.7%, with the median speakers at 72.7% for the maletest speakers, 71.9% for the female speakers and 72.3%for the whole test set.
Approximately 80% of all thetest speakers (82 out of 103) had a recognition perfor-mance over 69%, and only 8% of the speakers gaveperformance below 65%, including some "problematic"speakers.Our best cease result of 72% correct classification canbe compared to the SUMMIT 70% classification per-formance on the TIMIT data for unknown speakers\[Zue t al 1989\].
Although these results are based onknown segmentations, past work in segment modellingfor speaker-dependent phoneme recognition showedthat recognition with unknown segmentations yields asmall loss in recognition performance with a cost of10% phoneme insertion \[Ostendorf and Roucos 1989\].With this small loss in performance, the segmentmodels can still be expected to outperform HMMphoneme recognition performance of 59% on this task\[Lee and Hon 1988\].CONCLUSIONSIn summary, we have shown that with sufficient trainingdata, it is possible to model detailed time correlation ina segment-based model which can outperform HMMs incontext-independent phoneme classification tasks.
It re-mains to be shown that this result also holds for phonemerecognition, when phoneme segmentation boundaries arenot known.
In addition, the result should be extended to337new tasks, where automatic training will be required.There are several directions to further develop theSSM.
Since context-dependent models have been shownto give dramatic improvements in HMM word recog-nition, it is important to demonstrate similar results forsegment models.
This will require research in robust pa-raameter stimation techniques.
In addition, research onthe variable-to-fixed length transformation is also im-portant.
Although a constrained transformatiou is prob-ably an advantage of the segment model, it is not clearthat linear time warping is the best ransformation for allphonemes, and it may be useful to develop a mechanismfor estimating transformations.AcknowledgementThis work was jointly funded by NSF and DARPA underNSF grant number IRI-8902124.References\[Bahl et al1981\] L.R.
Bahl, R. Bakis, P.S.
Cohen, A.Cole, F. Jelinek, B.L.
Lewis, and R.L.
Mercer,"Continuous parameter acoustic processing forrecognition of a natural speech corpus," In IEEEInt.
Conf Acoust., Speech, Signal Processing,pages 1149-1152, Atlanta, GA, April 1981.\[Bocchieri and Doddington 1986\] E. Bocchieri and O.Doddington, "Frame-specific statistical featuresfor speaker-independent speech recognition,"IEEE Trans.
Acoust., Speech and Signal Proc.,ASSP-34(4):755-764, August 1986.\[Brown 1987\] P. F. Brown, The Acoustic-ModelingProbelm in Automatic Speech Recognition, PhDthesis, Carnegie-Mellon University, May 1987.IBM Technical Report RC12750.\[Bush and Kopec 1987/\] M.A.
Bush and G.E.
Kopec,"Network-based connected igit recognition,"IEEE Trans.
Acoust., Speech, Signal Processing,ASSP-35(10):1401-1413, October 1987.\[Lamel et al1986\] L.F. Lamel, R. H. Kassel and S.Seneff, "Speech Database Development: Designand Analysis of the Acoustic-Phonetic Corpus,"in Proc.
DARPA Speech Recognition Workshop,Report No.
SAIC-86/1546, pp.
100-109, Feb.1986.\[Lee and Hon 1988\] K.-F. Lee and H.-W. Hon,"Speaker-Independent Phone Recognition UsingHidden Markov Models," CMU TechnicalReport No.
CMU-CS-88-121.\[Makino and Kido 1986\] S. Makino and K. Kido,"Recognition of Phonemes Using Time-Spectrumpattern," Speech Communication, Vol.
5, No.
2,June 1986, pp.
225-238.\[Ostendorf and Roucos 1989\] M. Ostendorf and S.Roucos, "A stochastic segment model forphoneme-based continuous speech recognition,"to appear, IEEE Trans.
Acoustic Speech andSignal Processing, December 1989.\[Roucos and Dunlmm 1987\] S. Roucos and M.Ostendorf Dunham, "A stochastic segmentmodel for phoneme-based continuous speechrecognition," In IEEE Int.
Conf.
Acoust., Speech,Signal Processing, pages 73---89, Dallas, TX,April 1987/.
Paper No.
3.3.\[Roucos et al1988\] S. Roucos, M. Ostendorf, H. Gish,and A. Derr, "Stochastic segment modelingusing the estimate-maximize algorithm," In IEEEInt.
Conf.
Acoust., Speech, Signal Processing,pages 127-130, New York, New York, April1988.\[Schwartz et al1985\] R.M.
Schwartz, Y.L.
Chow,O.A.
Kimball, S. Roucos, M. Krasner, and J.Makhoul, "Context-dependent modeling foracoustic-phonetic recognition of continuousspeech," In IEEE Int.
Conf.
Acoust., Speech,Signal Processing, pages 1205-1208, Tampa, FL,March 1985.
Paper No.
31.3.\[Wilks 1962\] S.S. Wilks, Mathematical Statistics, JohnWiley & Sons, 1962.\[Zue t al 1989\] V. Zue, J.
Glass, M. Phillips and S.Seneff, "Acoustic Segmentation a d PhoneticClassification in the Summit System," in IEEEInt.
Conf.
Acoust., Speech, Signal Processing,pages 389-392, Glasgow, Scotland, May 1989.338
