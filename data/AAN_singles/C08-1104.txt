Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 825?832Manchester, August 2008From Words to Senses: A Case Study of Subjectivity RecognitionFangzhong SuSchool of ComputingUniversity of Leeds, UKfzsu@comp.leeds.ac.ukKatja MarkertSchool of ComputingUniversity of Leeds, UKmarkert@comp.leeds.ac.ukAbstractWe determine the subjectivity of wordsenses.
To avoid costly annotation, weevaluate how useful existing resources es-tablished in opinion mining are for thistask.
We show that results achieved withexisting resources that are not tailored to-wards word sense subjectivity classifica-tion can rival results achieved with super-vision on a manually annotated trainingset.
However, results with different re-sources vary substantially and are depen-dent on the different definitions of subjec-tivity used in the establishment of the re-sources.1 IntroductionIn recent years, subjectivity analysis and opinionmining have attracted considerable attention in theNLP community.
Unlike traditional informationextraction and document classification tasks whichusually focus on extracting facts or categorizingdocuments into topics (e.g., ?sports?, ?politics?,?medicine?
), subjectivity analysis focuses on de-termining whether a language unit (such as a word,sentence or document) expresses a private state,opinion or attitude and, if so, what polarity is ex-pressed, i.e.
a positive or negative attitude.Inspired by Esuli and Sebastiani (2006) andWiebe and Mihalcea (2006), we explore the auto-matic detection of the subjectivity of word senses,in contrast to the more frequently explored taskof determining the subjectivity of words (see Sec-tion 2).
This is motivated by many words beingc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.subjectivity-ambiguous, i.e.
having both subjec-tive and objective senses, such as the word positivewith its two example senses given below.1(1) positive, electropositive?having a positive electriccharge;?protons are positive?
(objective)(2) plus, positive?involving advantage or good; ?a plus(or positive) factor?
(subjective)Subjectivity labels for senses add an additionallayer of annotation to electronic lexica and allow togroup many fine-grained senses into higher-levelclasses based on subjectivity/objectivity.
This canincrease the lexica?s usability.
As an example,Wiebe and Mihalcea (2006) prove that subjectiv-ity information for WordNet senses can improveword sense disambiguation tasks for subjectivity-ambiguous words (such as positive).
In addition,Andreevskaia and Bergler (2006) show that theperformance of automatic annotation of subjectiv-ity at the word level can be hurt by the presence ofsubjectivity-ambiguous words in the training setsthey use.
Moreover, the prevalence of differentword senses in different domains also means thata subjective or an objective sense of a word mightbe dominant in different domains; thus, in a sci-ence text positive is likely not to have a subjectivereading.
The annotation of words as subjective andobjective or positive and negative independent ofsense or domain does not capture such distinctions.In this paper, we validate whether word sensesubjectivity labeling can be achieved with existingresources for subjectivity analysis at the word andsentence level without creating a dedicated, man-ually annotated training set of WordNet senses la-beled for subjectivity.2We show that such an ap-1All examples in this paper are from WordNet 2.0.2We use a subset of WordNet senses that are manually an-notated for subjectivity as test set (see Section 3).825proach ?
even using a simple rule-based unsu-pervised algorithm ?
can compete with a stan-dard supervised approach and also compares wellto prior research on word sense subjectivity label-ing.
However, success depends to a large degreeon the definition of subjectivity used in the estab-lishment of the prior resources.The remainder of this paper is organized as fol-lows.
Section 2 discusses previous work.
Sec-tion 3 introduces our human annotation schemefor word sense subjectivity and also shows thatsubjectivity-ambiguous words are frequent.
Sec-tion 4 describes our proposed classification algo-rithms in detail.
Section 5 presents the experimen-tal results and evaluation, followed by conclusionsand future work in Section 6.2 Related WorkThere has been extensive research in opinion min-ing at the document level, for example on productand movie reviews (Pang et al, 2002; Pang andLee, 2004; Dave et al, 2003; Popescu and Etzioni,2005).
Several other approaches focus on thesubjectivity classification of sentences (Kim andHovy, 2005; Kudo and Matsumoto, 2004; Riloffand Wiebe, 2003).
They often build on the pres-ence of subjective words in the sentence to be clas-sified.Closer to our work is the large body of workon the automatic, context-independent classifica-tion of words according to their polarity, i.e as pos-itive or negative (Hatzivassiloglou and McKeown,1997; Turney and Littman, 2003; Kim and Hovy,2004; Takamura et al, 2005).
They use eitherco-occurrence patterns in corpora or dictionary-based methods.
Many papers assume that sub-jectivity recognition, i.e.
separating subjectivefrom objective words, has already been achievedprior to polarity recognition and test against wordlists containing subjective words only (Hatzivas-siloglou and McKeown, 1997; Takamura et al,2005).
However, Kim and Hovy (2004) and An-dreevskaia and Bergler (2006) also address theclassification into subjective/objective words andshow this to be a potentially harder task than po-larity classification with lower human agreementand automatic performance.There are only two prior approaches address-ing word sense subjectivity or polarity classifi-cation.
Esuli and Sebastiani (2006) determinethe polarity of word senses in WordNet, distin-guishing among positive, negative and objective.They expand a small, manually determined seedset of strongly positive/negative WordNet sensesby following WordNet relations and use the result-ing larger training set for supervised classification.The resulting labeled WordNet gives three scoresfor each sense, representing the positive, negativeand objective score respectively.
However, thereis no evaluation as to the accuracy of their ap-proach.
They then extend their work (Esuli andSebastiani, 2007) by applying the Page Rank algo-rithm to ranking the WordNet senses in terms ofhow strongly a sense possesses a given semanticproperty (e.g., positive or negative).Wiebe and Mihalcea (2006) label word sensesin WordNet as subjective or objective.
They use amethod relying on distributional similarity as wellas an independent, large manually annotated opin-ion corpus (MPQA) (Wiebe et al, 2005) for deter-mining subjectivity.
One of the disadvantages oftheir algorithm is that it is restricted to senses thathave distributionally similar words in the MPQAcorpus, excluding 23.2% of their test data from au-tomatic classification.3 Human Annotation of Word SenseSubjectivity and PolarityIn contrast to other researchers (Hatzivassiloglouand McKeown, 1997; Takamura et al, 2005), wedo not see polarity as a category that is depen-dent on prior subjectivity assignment and thereforeapplicable to subjective senses only.
We followWiebe and Mihalcea (2006) in that we see subjec-tive expressions as private states ?that are not opento objective observation or verification?.
This in-cludes direct references to emotions, beliefs andjudgements (such as anger, criticise) as well as ex-pressions that let a private state be inferred, forexample by referring to a doctor as a quack.
Incontrast, polarity refers to positive or negative as-sociations of a word or sense.
Whereas there is adependency in that most subjective senses have arelatively clear polarity, polarity can be attached toobjective words/senses as well.
For example, tu-berculosis is not subjective ?
it does not describea private state, is objectively verifiable and wouldnot cause a sentence containing it to carry an opin-ion, but it does carry negative associations for thevast majority of people.We therefore annotate subjectivity of wordsenses similarly to Wiebe and Mihalcea (2006),826distinguishing between subjective (S), objective(O) or both (B).
Both is used if a literal andmetaphoric sense of a word are collapsed into oneWordNet synset or if a WordNet synset containsboth opinionated and objective expressions (suchas bastard and illegitimate child in Ex.
3 below).We expand their annotation scheme by also an-notating polarity, using the labels positive (P), neg-ative (N) and varied (V).
The latter is used when asense?s polarity varies strongly with the context,such as Example 8 below, where we would ex-pect uncompromising to be a judgement but thisjudgement will be positive or negative dependingon what a person is uncompromising about.
Toavoid prevalence of personalised associations, an-notators were told to only annotate polarity forsubjective senses, as well as objective senses thatcarry a strong association likely to be shared bymost people at least in Western culture (such as thenegative polarity for words referring to diseasesand crime).
Other objective senses would receivethe label O:NoPol.Therefore, we have 7 sub categories in total:O:NoPol, O:P, O:N, S:P, S:N, S:V, and B. Thenotation before and after the colon represents thesubjectivity and polarity label respectively.
We listsome annotated examples below.
(3) bastard, by-blow, love child, illegitimate child, illegiti-mate, whoreson?
the illegitimate offspring of unmar-ried parents (B)(4) atrophy?undergo atrophy; ?Muscles that are not usedwill atrophy?
(O:N)(5) guard, safety, safety device?a device designed to pre-vent injury (O:P)(6) nasty, awful?offensive or even (of persons) mali-cious;?in a nasty mood?
;?a nasty accident?
; ?a nastyshock?
(S:N)(7) happy?enjoying or showing or marked by joy or plea-sure or good fortune; ?a happy smile?
;?spent manyhappy days on the beach?
; ?a happy marriage?
(S:P)(8) uncompromising, inflexible?not making concessions;?took an uncompromising stance in the peace talks?
(S:V)As far as we are aware, this is the first annotationscheme for both subjectivity and polarity of wordsenses.
We believe both are relevant for opinionextraction: subjectivity for finding and analysingdirectly expressed opinions, and polarity for ei-ther classifying these further or extracting objec-tive words that, however, serve to ?colour?
a textor present bias rather than explicitly stated opin-ions.
Su and Markert (2008) describe the annota-tion scheme and agreement study in full.3.1 Agreement StudyWe used the Micro-WNOp corpus containing 1105WordNet synsets to test our annotation scheme.3The Micro-WNOp corpus is representative of thepart-of-speech distribution in WordNet.Two annotators (both near-native English speak-ers) independently annotated 606 synsets of theMicro-WNOp corpus for subjectivity and polarity.One annotator is the second author of this paperwhereas the other is not a linguist.
The overallagreement using all 7 categories is 84.6%, with akappa of 0.77, showing high reliability for a dif-ficult pragmatic task.
This at first seems at oddswith the notion of sentiment as a fuzzy category asexpressed in (Andreevskaia and Bergler, 2006) butwe believe is due to three factors:?
The annotation of senses instead of wordssplits most subjectivity-ambiguous wordsinto several senses, removing one source ofannotation difficulty.?
The annotation of senses in a dictionary pro-vided the annotators with sense descriptionsin form of Wordnet glosses as well as re-lated senses, providing more information thana pure word annotation task.?
The split of subjectivity and polarity annota-tion made the task clearer and the annotationof only very strong connotations for objectiveword senses ?de-individualized?
the task.As in this paper we are only interested in subjec-tivity recognition, we collapse S:V, S:P, and S:Ninto a single label S and O:NoPol, O:P, and O:Ninto a single label O. Label B remains unchanged.For this three-way annotation overall percentageagreement is 90.1%, with a kappa of 0.79.3.2 Gold StandardAfter cases with disagreement were negotiated be-tween the two annotators, a gold standard annota-tion was agreed upon.
Our test set consists of thisagreed set as well as the remainder of the Micro-WNOp corpus annotated by one of the annotatorsalone after agreement was established.
This set isavailable for research purposes at http://www.comp.leeds.ac.uk/markert/data.3The corpus has originally been annotated by theproviders (Esuli and Sebastiani, 2007) with scores for posi-tive, negative and objective/no polarity, thus a mixture of sub-jectivity and polarity annotation.
We re-annotated the corpuswith our annotation scheme.827How many words are subjectivity-ambiguous?As the number of senses increases with word fre-quency, we expect rare words to be less likelyto be subjectivity-ambiguous than frequent words.The Micro-WNOp corpus contains relatively fre-quent words so we will get an overestimation ofsubjective-ambiguous word types from this cor-pus, though not necessarily of word tokens.
It in-cludes 298 different words with all their synsetsin WordNet 2.0.
Of all words, 97 (32.5%) aresubjectivity-ambiguous, a substantial number.4 AlgorithmsIn this section, we present experiments using fivedifferent resources as training sets or clue sets forthis task.
The first is the Micro-WNOp corpus withour own dedicated word sense subjectivity anno-tation which is used in a standard supervised ap-proach as training and test set via 10-fold cross-validation.
This technique presupposes a man-ual annotation effort tailored directly to our taskto provide training data.
As it is costly to createsuch training sets, we investigate whether exist-ing resources such as two different subjective sen-tence lists (Section 4.2) and two different subjec-tive word lists (Section 4.3) can be adapted to pro-vide training data or clue sets although they do notprovide any information about word senses.
Allresources are used to create training data for su-pervised approaches; the subjective word lists arealso used in a simple rule-based unsupervised ap-proach.All algorithms were tested on the Micro-WNOpcorpus by comparing to the human gold stan-dard annotation.
However, we excluded all senseswith the label both from Micro-WNOp for test-ing the automatic algorithms, resulting in a final1061 senses, with 703 objective and 358 subjec-tive senses.
We also compare all algorithms to abaseline of always assigning the most frequent cat-egory (objective) to each sense, which results in anoverall accuracy of 66.3%.4.1 Standard Supervised Approach: 10-foldCross-validation (CV) on Micro-WNOpWe use 10-fold cross validation for training andtesting on the annotated synsets in the Micro-WNOp corpus.
We applied a Naive Bayes clas-sifier,4using the following three types of features:4We also experimented with KNN, Maximum Entropy,Rocchio and SVM algorithms and overall Naive Bayes per-Lexical Features: These are unigrams in theglosses.
We use a bag-of-words approach and filterout stop words.As glosses are usually quite short, using a bag-of-word feature representation will result in high-dimensional and sparse feature vectors, which of-ten deteriorate classification performance.
In orderto address this problem to some degree, we also ex-plored other features which are available as train-ing and test instances are WordNet synsets.Part-of-Speech (POS) Features: each sensegets its POS as a feature (adjective, noun, verb oradverb).Relation Features: WordNet relations are goodindicators for determining subjectivity as manyof them are subjectivity-preserving.
For exam-ple, if sense A is subjective, then its antonymsense B is likely to be subjective.
We employ8 relations here?antonym, similar-to, derived-from, attribute, also-see, direct-hyponym, direct-hypernym, and extended-antonym.
Each relationR leads to 2 features that describe for a sense Ahow many links of that type it has to synsets in thesubjective or the objective training set respectively.Finally, we represent the feature weightsthrough a TF*IDF measure.Considering the size of WordNet (115,424synsets inWordNet 2.0), the labeled Micro-WNOpcorpus is small.
Therefore, the question ariseswhether it is possible to adapt other data sourcesthat provide subjectivity information to our task.4.2 Sentence Collections: Movie and MPQAIt is reasonable to cast word sense subjectivityclassification as a sentence classification task, withthe glosses that WordNet provides for each senseas the sentences to be classified.
Then we can intheory feed any collection of annotated subjectiveand objective sentences as training data into ourclassifier while the annotated Micro-WNOp cor-pus is used as test data.
We experimented with twodifferent available data sets to test this assumption.Movie-domain Subjectivity Data Set (Movie):Pang and Lee (2004) used a collection of labeledsubjective and objective sentences in their workon review classification.5The data set contains5000 subjective sentences, extracted from moviereviews collected from the Rotten Tomatoes webformed best.5Available at http://www.cs.cornell.edu/People/pabo/movie-review-data/828site.6The 5000 objective sentences were col-lected from movie plot summaries from the In-ternet Movie Database (IMDB).
The assumptionis that all the snippets from the Rotten Tomatoespages are subjective (as they come from a reviewsite), while all the sentences from IMDB are ob-jective (as they focus on movie plot descriptions).The MPQA Corpus contains news articlesmanually annotated at the phrase level for opin-ions, their polarity and their strength.
The cor-pus (Version 1.2) contains 11,112 sentences.
Weconvert it into a corpus of subjective and objectivesentences following exactly the approach in (Riloffet al, 2003; Riloff and Wiebe, 2003) and obtain6127 subjective and 4985 objective sentences re-spectively.
Basically any sentence that contains atleast one strong subjective annotation at the phraselevel is seen as a subjective sentence.We again use a Naive Bayes algorithm with lex-ical unigram features.
Note that part-of-speechand relation features are not applicable here as thetraining set consists of corpus sentences, notWord-Net synsets.4.3 Word Lists: General Inquirer andSubjectivity ListSeveral word lists annotated for subjectivity or po-larity such as the General Inquirer (GI)7or the sub-jectivity clues list (SL) collated by Janyce Wiebeand her colleagues8are available.The General Inquirer (GI) was developed byPhilip Stone and colleagues in the 1960s.
It con-centrates on word polarity.
Here we make thesimple assumption that both positive and negativewords in the GI list are subjective clues whereasall other words are objective.The Subjectivity Lexicon (SL) centers onsubjectivity so that it is ideally suited for ourtask.
It provides fine-grained information for eachclue, such as part-of-speech, subjectivity strength(strong/weak), and prior polarity (positive, nega-tive, or neutral).
For example, object(verb) is asubjective clue whereas object(noun) is objective.Regarding strength, the adjective evil is marked asstrong subjective whereas the adjective exposed ismarked as a weak subjective clue.Both lexica do not include any informationabout word senses and therefore cannot be useddirectly for subjectivity assignment at the sense6http://www.rottentomatoes.com/7http://www.wjh.harvard.edu/?inquirer/8http://www.cs.pitt.edu/mpqa/level.
For example, at least one sense of anysubjectivity-ambiguous word will be labeled incor-rectly if we just adopt a word-based label.
In addi-tion, these lists are far from complete: compared tothe over 100,000 synsets in WordNet, GI contains11,788 words marked for polarity (1915 positive,2291 negative and 7582 no-polarity words) and theSL list contains about 8,000 subjective words.Still, it is a reasonable assumption that any glossthat contains several subjective words indicates asubjective sense overall.
This intuition is strength-ened by the characteristics of glosses.
They nor-mally are short and concise without a complex syn-tactic structure, thus the occurrence of subjectivewords in such a short string is likely to indicatea subjective sense overall.
This contrasts, for ex-ample, with sentences in newspapers where oneclause might express an opinion, whereas otherparts of the sentence are objective.Therefore, for the rule-based unsupervisedalgorithm we lemmatized and POS-tagged theglosses in the Micro-WNOp test set.
Then wecompute a subjectivity score S for each synset bysumming up the weight values of all subjectivityclues in its gloss.
If S is equal or higher than anagreed threshold T, then the synset is classified assubjective, otherwise as objective.
For the GI lexi-con, all subjectivity clues have the same weight 1,whereas for the SL list we assign a weight value2 to strongly subjective clues and 1 to weaklysubjective clues.
We experimented with severalthresholds T and report here the results for the bestthresholds, which were 2 for SL and 4 for the GIword list.
The corresponding methods are calledRule-SL and Rule-GI.This approach does not allow us to easily inte-grate relational WordNet features.
It might alsosuffer from the incompleteness of the lexica andthe fact that it has to make decisions for bor-derline cases (at the value of the threshold set).We therefore explored instead to generate larger,more reliable training data consisting of Word-Net synsets from the word lists.
To achieve this,we assign a subjectivity score S as above to allWordNet synsets (excluding synsets in the test set).If S is higher or equal to a threshold T1it is addedto the subjective training set, if it is lower or equalto T2it is added to the objective training set.
Thisallows us to choose quite clear thresholds so thatborderline cases with a score between T1and T2are not in the training set.
It also allows to use part-829of-speech and relational features as the training setthen consists of WordNet synsets.
In this way,we can automatically generate (potentially noisy)training data of WordNet senses marked for sub-jectivity without annotating any WordNet sensesmanually for subjectivity.We experimented with several different thresh-old sets but we found that they actually have a min-imal impact on the final results.
We report here thebest results for a threshold T1of 4 and T2of 2 forthe SL lexicon and of 3 and 1 respectively for theGI word list.5 Experiments and EvaluationWe measure the classification performance withoverall accuracy as well as precision, recall andbalanced F-score for both categories (objective andsubjective).
All results are summarised in Table 1.Results are compared to the baseline of majorityclassification using a McNemar test at the signifi-cance level of 5%.5.1 Experimental ResultsTable 1 shows that SL?performs best among allthe methodologies.
All CV, Rule-SL and SL meth-ods significantly beat the baseline.
In addition, ifwe compare the results of methods with and with-out additional parts-of-speech and WordNet rela-tion features, we see a small but consistent im-provement when we use additional features.
It isalso worthwhile to expand the rule-based unsuper-vised method into a method for generating train-ing data and use additional features as SL?signifi-cantly outperforms Rule-SL.5.2 DiscussionWord Lists.
Surprisingly, using SL greatly outper-forms GI, regardless of whether we use the super-vised or unsupervised method or whether we uselexical features only or the other features as well.9There are several reasons for this.
First, the GIlexicon is annotated for polarity, not subjectivity.More specifically, words that we see as objectivebut with a strong positive or negative association(such as words for crimes) and words that we seeas subjective are annotated with the same polar-ity label in the GI lexicon.
Therefore, the GI def-inition of subjectivity does not match ours.
Also,9This pattern is repeated for all threshold combinations,which are not reported here.the GI lexicon does not operate with a clearly ex-pressed polarity definition, leading to conflictingannotations and casting doubt on its widespreaduse in the opinion mining community as a goldstandard (Turney and Littman, 2003; Takamura etal., 2005; Andreevskaia and Bergler, 2006).
Forexample, amelioration is seen as non-polar in GIbut improvement is annotated with positive polar-ity.
Second, in contrast to SL, GI does not considerdifferent parts-of-speech of a word and subjectiv-ity strength (strong/weak subjectivity).
Third, GIcontains many fewer subjective clues than SL.Sentence Data.
When using the Movie datasetand MPQA corpus as training data, the resultsare not satisfactory.
We first checked the purityof these two datasets to see whether they are toonoisy.
For this purpose, we used a naive Bayesalgorithm with unigram features and conducted a10-fold cross validation experiment on recognizingsubjective/objective sentences within the Moviedataset and MPQA independently.
Interestingly,the accuracy for the Movie dataset and MPQA cor-pus achieved 91% and 76% respectively.
Consid-ering that they are balanced datasets with a mostfrequent category baseline of about 50%, this ac-curacy is high, especially for the Movie dataset.However, again the subjectivity definition in theMovie corpus does not seem to match ours.
Re-call that we see a word sense or a sentence as sub-jective if it expresses a private state (i.e., emotion,opinion, sentiment, etc.
), and objective otherwise.Inspecting the movie data set, we found that in-deed the sentences included in its subjective setwould mostly be seen as subjective in our senseas well as they contain opinions about the moviesuch as it desperately wants to be a wacky , screw-ball comedy , but the most screwy thing here is howso many talented people were convinced to wastetheir time.
It is also true that the sentences (plotdescriptions) in its ?objective?
data set relativelyrarely contain opinions about the movie.
How-ever, they still contain other opinionated contentlike opinions and emotions of the characters in themovie such as the obsession of a character withJohn Lennon in the beatles fan is a drama aboutAlbert, a psychotic prisoner who is a devoted fanof John Lennon and the beatles.
Since the dataset?s definition of subjective sentences is closerto ours than the one for objective sentences, weconducted a one-class learning approach (Li andLiu, 2003) using Movie?s subjective sentences as830Table 1: ResultsMethod Subjective Objective AccuracyPrecision Recall F-score Precision Recall F-scoreBaseline N/A 0 N/A 66.3% 100% 79.7% 66.3%CV 65.2% 52.8% 58.3% 78.1% 85.6% 81.7% 74.6%?CV?69.5% 55.3% 61.6% 79.4% 87.6% 83.3% 76.7%?Movie 43.8% 60.1% 50.6% 74.9% 60.7% 67.1% 60.5%MPQA 44.5% 78.5% 56.8% 82.1% 50.1% 62.2% 59.7%GI 50.4% 39.4% 44.2% 72.2% 80.2% 76.0% 66.4%GI?54.5% 33.5% 41.5% 71.7% 85.8% 78.1% 68.1%SL 64.3% 62.8% 63.6% 81.3% 82.2% 81.8% 75.7%?SL?66.2% 64.5% 65.3% 82.2% 83.2% 82.7% 76.9%?Rule-GI 38.5% 5.6% 9.8% 66.5% 95.4% 78.4% 65.1%Rule-SL 59.7% 70.4% 64.6% 83.4% 75.8% 79.4% 74.0%?1CV, GI and SL correspond to methods using lexical features only.2CV?, GI?and SL?correspond to methods using a feature combination of lexical,part-of-speech, and WordNet relations.3?
indicates results significantly better than the baseline.the only training data.
The algorithm10combinesExpectation Maximization and Naive Bayes algo-rithms, and we used randomly extracted 50,000unlabeled synsets in WordNet as the necessary un-labeled data.
This approach achieves an accuracyof 69.4% on Micro-WNOp, which is significantlybetter than the baseline.The subjectivity definition in the MPQA corpusis quite close to ours.
However, our mapping fromits phrase annotation to sentence annotation mightbe too coarse-grained as many sentences in the cor-pus span several clauses containing both opinionsand factual description.
We assume that this is pos-sibly also the reason why its purity is lower thanin the Movie dataset.
We therefore experimentedagain with a one-class learning approach using justthe subjective phrases in MPQA as training data.The accuracy does improve to 67.6% but is stillnot significantly higher than the baseline.5.3 Comparison to Prior ApproachesEsuli and Sebastiani (2006) make their labeledWordNet SentiWordNet 1.0 publically available.11Recall that they actually use polarity classification:however, as there is a dependency between po-larity and subjectivity classification for subjectivesenses, we map their polarity scores to our subjec-tivity labels as follows.
If the sum of positive and10Available at http://www.cs.uic.edu/?liub/LPU/.11Available at http://sentiwordnet.isti.cnr.it/negative scores of a sense in SentiWordNet is morethan or equal to 0.5, then it is subjective and other-wise objective.12Using this mapping, it achievesan accuracy of 75.3% on the Micro-WNOp cor-pus, compared to our gold standard.
Therefore ourmethods CV?and SL?perform slightly better thantheirs, although the improvement is not significant.The task definition in Wiebe and Mihal-cea (2006) is much more similar to ours but theyuse different annotated test data, which is not pub-lically available, so an exact comparison is not pos-sible.
Both data sets, however, seem to include rel-atively frequent words.
One disadvantage of theirmethod is that it is not applicable to all WordNetsenses as it is dependent on distributionally sim-ilar words being available in the MPQA.
Thus,23% of their test data is excluded from evaluation,whereas our methods can be used on any WordNetsense.
They measure precision and recall for sub-jective senses in a precision/recall curve: Precisionis about 48/9% at a recall of 60% for subjectivesenses whereas our best SL?method has a preci-sion of 66% at about the same recall.
Althoughthis suggests better performance of our method, itis not possible to draw final conclusions from thiscomparison due to the data set differences.12We experimented with slightly different mappings butthis mapping gave SentiWordNet the best possible result.There is a relatively large number of cases with a 0.5/0.5 splitin SentiWordNet, making it hard to decide between subjectiveand objective senses.8316 Conclusion and Future WorkWe proposed different ways of extracting trainingdata and clue sets for word sense subjectivity label-ing from existing opinion mining resources.
Theeffectiveness of the resulting algorithms dependsgreatly on the generated training data, more specif-ically on the different definitions of subjectivityused in resource creation.
However, we were ableto show that at least one of these methods (basedon the SL word list) resulted in a classifier that per-formed on a par with a supervised classifier thatused dedicated training data developed for this task(CV).
Thus, it is possible to avoid any manual an-notation for the subjectivity classification of wordsenses.Our future work will explore new methodolo-gies in feature representation by importing morebackground information (e.g., syntactic informa-tion).
Furthermore, our current method of integrat-ing the rich relation information in WordNet (us-ing them as standard features) does not use jointclassification of several senses.
Instead, we thinkit will be more promising to use the relations toconstruct graphs for semi-supervised graph-basedlearning of word sense subjectivity.
In addition, wewill also explore whether the derived sense labelsimprove applications such as sentence classifica-tion and clustering WordNet senses.ReferencesAndreevskaia, Alina and Sabine Bergler.
2006.
Min-ing WordNet for Fuzzy Sentiment: Sentiment TagExtraction from WordNet Glosses.
Proceedings ofEACL?06.Dave, Kushal, Steve Lawrence, and David Pennock.2003.
Mining the Peanut Gallery: Opinion Extrac-tion and Semantic Classification of Product Reviews.Proceedings of WWW?03.Esuli, Andrea and Fabrizio Sebastiani.
2006.
Senti-WordNet: A Publicly Available Lexical Resource forOpinion Mining.
Proceedings of LREC?06.Esuli, Andrea and Fabrizio Sebastiani.
2007.
PageR-anking WordNet Synsets: An application to OpinionMining.
Proceedings of ACL?07.Hatzivassiloglou, Vasileios and Kathleen McKeown.1997.
Predicting the Semantic Orientation of Ad-jectives.
Proceedings of ACL?97.Kim, Soo-Min and Eduard Hovy.
2004.
Determiningthe Sentiment of Opinions.
Proceedings of COL-ING?04.Kim, Soo-Min and Eduard Hovy.
2005.
AutomaticDetection of Opinion Bearing Words and Sentences.Proceedings of ICJNLP?05.Kudo, Taku and Yuji Matsumoto.
2004.
A BoostingAlgorithm for Classification of Semi-structured Text.Proceedings of EMNLP?04.Li, Xiaoli and Bing Liu.
2003.
Learning to classifytext using positive and unlabeled data.
Proceedingsof IJCAI?03.Pang, Bo and Lillian Lee.
2004.
A Sentiment Edu-cation: Sentiment Analysis Using Subjectivity sum-marization Based on Minimum Cuts.
Proceedings ofACL?04.Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment Classification us-ing Machine Learning Techniques.
Proceedings ofEMNLP?02.Popescu, Ana-Maria and Oren Etzioni.
2003.
Ex-tracting Product Fatures and Opinions from ReviewsProceedings of EMNLP?05.Riloff, Ellen, JanyceWiebe, and TheresaWilson.
2003.Learning Subjective Nouns using Extraction PatternBootstrapping.
Proceedings of CoNLL?03Riloff, Ellen and Janyce Wiebe.
2003.
Learning Ex-traction Patterns for Subjective Expressions.
Pro-ceedings of EMNLP?03.Su, Fangzhong and Katja Markert.
2008.
Elicit-ing Subjectivity and Polarity Judgements on WordSenses.
Proceedings of Coling?08 workshop of Hu-man Judgements in Computational Linguistics.Takamura, Hiroya, Takashi Inui, and Manabu Oku-mura.
2005.
Extracting Semantic Orientations ofWords using Spin Model.
Proceedings of ACL?05.Turney, Peter and Michael Littman.
2003.
MeasuringPraise and Criticism: Inference of Semantic Orien-tation from Association.
ACM Transaction on Infor-mation Systems.Wiebe, Janyce and Rada Micalcea.
2006.
Word Senseand Subjectivity.
Proceedings of ACL?06.Wiebe, Janyce, Theresa Wilson, and Claire Cardie.2005.
Annotating Expressions of Opinions andEmotions in Language.
Language Resources andEvaluation.832
