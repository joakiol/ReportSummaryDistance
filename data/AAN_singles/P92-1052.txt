SEXTANT: EXPLORING UNEXPLORED CONTEXTS FORSEMANTIC EXTRACTION FROM SYNTACTIC ANALYSISGregory Grefenstet teComputer Science Department, University of Pittsburgh, Pittsburgh, PA 15260grefen@cs.pitt.eduAbstractFor a very long time, it has been con-sidered that the only way of automati-cally extracting similar groups of wordsfrom a text collection for which no se-mantic information exists is to use docu-ment co-occurrence data.
But, with ro-bust syntactic parsers that are becom-ing more frequently available, syntacti-cally recognizable phenomena about wordusage can be confidently noted in largecollections of texts.
We present here anew system called SEXTANT which usesthese parsers and the finer-grained con-texts they produce to judge word similar-ity.BACKGROUNDMany machine-based approaches to term sim-ilarity, such as found in T I tUMP (Jacobsand Zernick 1988) and FERRET (Mauldin1991), can be characterized asknowledge-richin that they presuppose that known lexicalitems possess Conceptual Dependence(CD)-like descriptions.
Such an approach neces-sitates a great amount of manual encodingof semantic information and suffers from thedrawbacks of cost (in terms of initial coding,coherence checking, maintenance after modi-fications, and costs derivable from a host ofother software engineering concern); of do-ma in  dependence  (a semantic structure de-veloped for one domain would not be applica-ble to another.
For example, sugar would havevery different semantic relations in a medi-cal domain than in a commodities exchangedomain); and of r ig id i ty  (even within well-established omain, new subdomains pringup, e.g.
AIDS.
Can hand-coded systems keepup with new discoveries and new relationswith an acceptable latency?
)In the Information Retrieval community.researchers have consistently considered that324"the linguistic apparatus required for effec-tive domain-independent a alysis is not yetat hand," and have concentrated on countingdocument co-occurrence statistics (Peat andWillet 1991), based on the idea that wordsappearing in the same document must sharesome semantic similarity.
But document co-occurrence suffers from two problems: granu-laxity (every word in the document is consid-ered potentially related to every other word,no matter what the distance between them)and co -occur rence  (for two words to be seenas similar they must physically appear in thesame document.
As an illustration, considerthe words tumor and turnout.
These wordscertainly share the same contexts, but wouldnever appear in the same document.)
In gen-eral different words used to describe similarconcepts might not be used in the same doc-ument, and are missed by these methods.Recently, a middle ground between thesetwo approaches has begun to be broken.
Re-searchers uch as (Evans et al 1991) and(Church and Hanks 1990) have applied robustgrammars and statistical techniques over largecorpora to extract interesting noun phrasesand subject-verb, verb-object pairs.
(Hearst1992) has shown that certain lexical-syntactictemplates can reliably extract hyponym re-lations from text.
(Ruge 1991) shows thatmodifier-head relations in noun phrases ex-tracted from a large corpus provide a use-ful context for extracting similar words.
Thecommon thread of all these techniques i thatthey require no hand-coded omain knowl-edge, but they examine more cleanly definedcontexts than simple document co-occurrencemethods.Similarly, our SEXTANT 1 uses fine-grained syntactically derived contexts, but de-rives its measures of similarity from consider-I Semantic EXtraction from Text via Analyzed Net-works of Termsing not the co-occurrence of two words in thesame context, but rather the overlapping ofall the contexts associated with words over anentire corpus.
Calculation of the amount ofshared weighted contexts produces a similar-ity measure between two words.SEXTANTSEXTANT can be run on any English text,without any pre-coding of domain knowledgeor manual editing of the text.
The input textpasses through the following steps: (I) Mor-phological analysis.
Each word is morpholog-ically analyzed and looked up in a 100,000word dictionary to find its possible parts ofspeech.
(II) Grammatical Disambiguation.
Astochastic parser assigns one grammatical cat-egory to each word in the text.
These firsttwo steps use CLARIT programs (Evans et al1991).
(III) Noun and Verb Phrase Splitting.Each sentence is divided into verb and nounphrases by a simple regular grammar.
(IV)Syntagmatic Relation Extraction.
A four-pass algorithm attaches modifiers to nouns,noun phrases to noun phrases and verbs tonoun phrases.
(Grefenstette 1992a) (V) Con-text Isolation.
The modifying words attachedto each word in the text are isolated for allnouns.
Thus the context of each noun isgiven by all the words with which it is asso-ciated throughout the corpus.
(VI) Similaritymatching.
Contexts are compared by usingsimilarity measures developed in the SocialSciences, such as a weighted Jaccard measure.As an example, consider the following sen-tence extracted from a medical corpus.Cyclophosphamide markedly prolonged induct iontime and suppressed peak titer irrespective ofthe time of antigen administration.Each word is looked up in a online dictionary.After grammatical ambiguities are removedby the stochastic parser, the phrase is dividedinto noun phrases(NP) and verb phrases(VP),giving,NP cyclophosphamide (sn)- -  markedly (adv)VP prolong (v t -pas t )NP induction (sn) time (sn)-- and (cnj)VP suppress (v t -pas t )NP peak (sn) t i te r  (sn) i r respect ive -o f  (prep)the (d) time (sn) of (prep) antigen (en)administration (sn)Once each sentence in the text is divided intophrases, intra- and inter-phrase structural re-lations are extracted.
First noun phrasesare scanned from left to right(NPLR), hook-ing up articles, adjectives and modifier nounsto their head nouns.
Then, noun phrasesare scanned right to left(NPttL), connectingnouns over prepositions.
Then, starting fromverb phrases, phrases are scanned before theverb phrase for an unconnected head whichbecomes the subject(VPRL), and likewise tothe right of the verb for objects(VPLtt), pro-ducing for the example:VPRL cyclophosphamide , prolong < SUBJNPRL t ime , induct ion  < NNVPLR prolong , t ime < DOBJVPRL cyclophosphamide , suppress < SUBJNPRL t i te r  , peak < NNVPLR suppress , titer < DOBJNPLR t i te r  , t ime < NNPREPNPRL administration , antigen < NNNext SEXTANT extracts a user specified setof relations that are considered as each word'scontext for similarity calculations.
For exam-ple, one set of relations extracted by SEX-TANT for the above sentence can becyclophosphamide prolong-SUBJt ime induct iontime prolong-DOBJcyclophosphamide suppress-SUBJt i te r  peakt i te r  suppress-DOBJt i te r  t imeadmin is t ra t ion  ant igent ime admin is t ra t ionIn this example, the word time is found mod-ified by the words induction, prolong-DOBJand administration, while administration isonly considered by this set of relations to bemodified by antigen.
Over the whole corpusof 160,000 words, one can consider what mod-ifies administration.
Isolating these modifiersgives a list such asadministration androgenadministration antigenadministration aorticadministration examineadministration associate-DOBJadministration aseociate-SUBJadministration azathioprineadministration carbon-dioxideadministration caseadministration cause-SUBJ.
.
.At this point SEXTANT compares all theother words in the corpus, using a user-specified similarity measure such the Jaccardmeasure, to find which words are most simi-lar to which others.
For example, the wordsfound as most similar to administration i  thismedical corpus were the following words in or-der of most to least similar:325admin is t ra t ion  in jec t ion ,  t reatment ,  therapy,in fus ion ,  dose, response,  .
.
.As can be seen, the sense of administra-tion as in the "administration of drugs andmedicines" is clearly extracted here, since ad-ministration in this corpus is most similarlyused as other words such as injection and ther-apy having to do with dispensing drugs andmedicines.
One of the interesting aspects ofthis approach, contrary to the coarse-graineddocument co-occurrence approach, is that ad-ministration and injection need never appearin the same document for them to be recog-nized as semantically similar.
In the case ofthis corpus, administration and injection wereconsidered similar because they shared the fol-lowing modifiers:acid follow-DOBJ growth prior produce-IOBJdose  ext rac t  increase-SUBJ in t ravenoustreat-IOBJ associate-SUSJ associate-DOBJrapid cause-SUBJ antigen adrenalectomyaortic hormone subside-IOBJ alter-IOBJfolio-acid amd folateIt is hard to select any one word which wouldindicate that these two words were similar,but the fact that they do share so many words,and more so than other words, indicates thatthese words share close semantic characteris-tics in this corpus.When the same procedure is run over acorpus of library science abstracts, adminis-tration is recognized as closest toadministration graduate, office, campus,education, director, ...Similarly circulation was found to be closest oflow in the medical corpus and to date in thelibrary corpus.
Cause was found to be closestto etiology in the medical corpus and to deter-minant in the library corpus.
Frequently oc-curring words, possessing enough context, aregenerally ranked by SEXTANT with words in-tuitively related within the defining corpus.D ISCUSSIONWhile finding similar words in a corpus with-out any domain knowledge is interesting initself, such a tool is practically useful in anumber of areas.
A lexicographer building adomain-specific dictionary would find such atool invaluable, given a large corpus of rep-resentative text for that domain.
Similarly,a Knowledge Engineer creating a natural an-guage interface to an expert system could usethis system to cull similar terminology in afield.
We have shown elsewhere (Grefenstette1992b), in an Information itetrieval setting,that expanding queries using the closest ermsto query terms derived by SEXTANT can im-prove recall and precision.
We find that oneof the most interesting results from a linguis-tic point of view, is the possibility automati-caUy creating corpus defined thesauri, as canbe seen above in the differences between re-lations extracted from medical and from in-formation science corpora.
In conclusion, wefeel that this fine grained approach to contextextraction from large corpora, and similaritycalculation employing those contexts, even us-ing imperfect syntactic analysis tools, showsmuch promise for the future.Re ferences(Church and Hanks 1990) K.W.
Church andP.
Hanks.
Word association orms, mutualinformation, and lexicography.
Computa-tional Linguistics, 16(1), Mar 90.
(Evans et al 1991) D.A.
Evans, S.K.
Hender-son, R.G.
Lefferts, and I.A.
Monarch.
Asummary of the CLARIT project.
TitCMU-LCL-91-2, Carnegie-Mellon, Nov 91.
(Grefenstette 1992a) G. Grefenstette.
Sex-tant: Extracting semantics from raw text,implementation details.
Tit CS92-05, Uni-versity of Pittsburgh, Feb 92.
(Grefenstette 1992b) G. Grefenstette.
Use ofsyntactic context to produce term associ-ation lists for text retrieval.
SIGIR'9~,Copenhagen, June 21-24 1992.
ACM.
(Hearst 1992) M.A.
Hearst.
Automatic acqui-sition of hyponyms from large text corpora.COLING'92, Nantes, France, July 92.
(Jacobs and Zeruick 1988) P. S. Jacobs andU.
Zernick.
Acquiring lexical knowledgefrom text: A case study.
In ProceedingsSeventh National Conference on ArtificialIntelligence, 739-744, Morgan Kaufmann.
(Mauldin 1991) M. L. Mauldin.
ConceptualInformation Retrieval: A case study inadaptive parsing.
Kluwer, Norwell, 91.
(Peat and WiUet 1991) H.J.
Peat and P. Wil-let.
The limitations of term co-occurrencedata for query expansion in document re-trieval systems.
JASIS, 42(5), 1991.
(ituge 1991) G. ituge.
Experiments on lin-guistically based term associations.
InRIAO'91, 528-545, Barcelona, Apr 91.CID, Paris.326
