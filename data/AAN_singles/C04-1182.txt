Analysis and Detection of Reading Miscues for Interactive Literacy TutorsKatherine Lee, Andreas Hagen, Nicholas Romanyshyn, Sean Martin, Bryan PellomCenter for Spoken Language ResearchUniversity of Colorado at Boulderpellom@cslr.colorado.eduAbstractThe Colorado Literacy Tutor (CLT) is atechnology-based literacy program, designedon the basis of cognitive theory andscientifically motivated reading research,which aims to improve literacy and studentachievement in public schools.
One of thecritical components of the CLT is a speechrecognition system which is used to track thechild?s progress during oral reading and toprovide sufficient information to detectreading miscues.
In this paper, we extend onprior work by examining a novel labeling ofchildren?s oral reading audio data in order tobetter understand the factors that contributemost significantly to speech recognitionerrors.
While these events make up nearly 8%of the data, they are shown to account forapproximately 30% of the word errors in astate-of-the-art speech recognizer.
Next, weconsider the problem of detecting miscuesduring oral reading.
Using features derivedfrom the speech recognizer, we demonstratethat 67% of reading miscues can be detected ata false alarm rate of 3%.1 IntroductionPioneering research by MIT and CMU as well asmore recent work by the IBM Watch-me-Readproject has demonstrated human languagetechnologies can play an effective role in systemsdesigned to improve children?s reading abilities(McCandless, 1992; Mostow et al, 1994; Zue etal., 1996).
In CMU?s Project LISTEN, for example,the tutor operates by prompting children to readindividual sentences out loud.
The tutor listens tothe child using speech recognition and extractsfeatures that can be used to detect oral readingmiscues (Mostow et al, 2002; Tam et al 2003).The most common miscues that children makewhile reading out loud are word substitutions,repetitions, and self-corrections with wordomissions and insertions being less frequent(Fogarty et al 2001).Upon detecting such reading errors, the tutor mustprovide appropriate feedback to the child.
Whilethe type of feedback and level of feedback is thecurrent subject of much debate within the researchcommunity, recent results have shown thatautomated reading tutors can improve studentachievement (Mostow et al, 2003).
In fact,providing real time feedback by simplyhighlighting words as they are read out loud is thebasis of at least one commercial product today1.Cole et al (2003) and Wise et al (in press)describe a new scientifically-based literacyprogram, Foundations to Fluency, in which avirtual tutor?a lifelike 3D computer model?interacts with children in multimodal learning tasksto teach them to read.
A key component of thisprogram is the Interactive Book, which combinesreal-time multilingual speech recognition, facialanimation, and natural language understandingcapabilities to teach children to read andcomprehend text.
Within the context of thisreading program, Hagen et al (2003) demonstratedan initial speech recognition system that providesreal-time reading tracking for children.
This workwas later extended by Hagen et al (2004) toincorporate improved acoustic and languagemodeling strategies.
When tested on 106 children(ages 9-11) who were asked to read one of anumber of short age-appropriate stories, a finalsystem word error rate of 8.0% was demonstrated.While reporting raw word error rate is useful forcomparison purposes to prior research, we pointout that it does not provide any diagnosticinformation which can be used to understandfactors that contribute to speech recognition errorwithin such children?s literacy tutor programs.Therefore, this paper extends our earlier work intwo important ways.
First, in order to understandwhere future improvements can be obtained, weprovide a novel ?event?
labeling of our children?sspeech corpus and examine the performance of thecurrent speech recognition system under eachlabeled event condition.
Second, we describe theconstruction of an automated classifier which candetect reading miscues in children?s speech.This paper is organized as follows.
First,Section 2 provides an introduction and overview ofthe Colorado Literacy Tutor project.
Section 3describes the audio corpus used in the experiments1 http://www.soliloquy.comprovided in this paper and Section 4 describes ourbaseline speech recognition system.
Next, Section5 describes the event labeling methodology andword error analysis under each labeled eventcondition.
Finally Section 6 describes our initialwork towards developing a system to detectreading miscues based on the output of ourbaseline speech recognition system.
Conclusionsand future work are outlined in Section 7.2 The Colorado Literacy TutorThe Colorado Literacy Tutor (CLT)2 is atechnology-based literacy program, designed onthe basis of cognitive theory and scientificallymotivated reading research, which aims to improveliteracy and student achievement in public schools.The goal of the Colorado Literacy Tutor is toprovide computer-based learning tools that willimprove student achievement in any subject areaby helping students learn to read fluently, toacquire new knowledge through deepunderstanding of what they read, to makeconnections to other knowledge and experiences,and to express their ideas concisely and creativelythrough writing.
A second goal is to scale up theprogram to both state and national levels in theU.S.
by providing accessible, inexpensive andeffective computer-based learning tools.The CLT project consists of four tightlyintegrated components: Managed LearningEnvironment, Foundational Reading Skills Tutors,Interactive Books, and Latent-Semantic Analysis(LSA)-based comprehension training (Steinhart2001; Deerwester et al, 1990; Landauer andDumais, 1997).
A key feature of the project is theuse of leading edge human communicationtechnologies in learning tasks.
The project hasbecome a test bed for research and development ofperceptive animated agents that integrate auditoryand visual behaviors during face-to-faceconversational interaction with human learners.The project enables us to evaluate componenttechnologies with real users?students inclassrooms?and to evaluate how the technologyintegration affects learning using standardizedassessment tools.Within the CLT, Interactive Books are the mainplatform for research and development of naturallanguage technologies and perceptive animatedagents.
Figure 1 shows a page of an InteractiveBook.
Interactive Books incorporate speechrecognition, spoken dialogue, natural languageprocessing, and computer animation technologiesto enable natural face-to-face conversational2 http://www.colit.orginteraction with users.
The integration of thesetechnologies is performed using a client-serverarchitecture that provides a platform-independentuser interface for Web-based delivery ofmultimedia learning tools.
Interactive Bookauthoring tools are designed for easy use by projectstaff, teachers and students to enable authors todesign and format books by combining text,images, videos and animated characters.
Once textand illustrations have been imported or input intothe authoring environment, authors can orchestrateinteractions between users, animated charactersand media objects.
Developers can populateillustrations (digital images) with animatedcharacters, and cause them to converse with eachother, with the user, or speak their parts in thestories using naturally recorded or syntheticspeech.
A mark up language enables authors tocontrol characters?
facial expressions and gestureswhile speaking.
The authoring tools also enableauthors to pre-record sentences and/or individualwords in the text as well as utterances to beproduced by animated characters duringconversations.Figure 1: An example interactive bookInteractive Books enable a wide range of userand system behaviors.
These include having thestory narrated by animated characters, havingconversations with animated characters instructured or mixed-initiative dialogues, having thestudent read out loud while words are highlighted,enabling the student to click on words to havethem spoken by the agent or to have the agentinteract with the student to sound out the word,having the student respond to questions posed bythe agent either by clicking on objects in images orsaying or typing responses, and having the studentproduce typed or spoken story summaries whichcan be analyzed for content using natural languageprocessing techniques.3 CU Children?s Read Story CorpusWithin the context of the CLT project, we havecollected a corpus of audio data consisting of readstories spoken by children.
Known as the CUChildren?s Read Story Corpus3, the data currentlycontains speech and associated word-leveltranscriptions from 106 children who were askedto read a short age-appropriate story and to providea spontaneous spoken summary of the material.
Inaddition, each child was prompted to read 25phonetically balanced sentences for future use inexploring strategies for speaker adaptation.The data were collected from native Englishspeaking children in the Boulder Valley SchoolDistrict (Boulder, Colorado, USA).
We haveinitially collected and transcribed stories fromchildren in grades 3, 4, and 5 (grade 3: 17speakers, grade 4: 28 speakers, grade 5: 61speakers).
The data were originally collected in aquiet room using a commonly available head-mounted microphone.
The current 16 kHzsampled corpus consists of 10 different stories.Each story contains an average of 1054 words (min532 words / max 1926 words) with an average of413 unique words per story.
Note that while eachstory is accompanied by a spontaneous summaryproduced by the child, we do not consider thosedata for this paper.4 Baseline Speech Recognition SystemThe CLT uses the SONIC speech recognitionsystem as a basis for providing real-timerecognition of children?s speech (Pellom, 2001;Pellom and Hacioglu, 2003; Hagen et al 2004)4.The recognizer implements an efficient time-synchronous, beam-pruned Viterbi token-passingsearch through a static re-entrant lexical prefix treewhile utilizing continuous density mixtureGaussian Hidden Markov Models (HMMs).
Therecognizer uses PMVDR cepstral coefficients(Yapanel and Hansen, 2003) as its featurerepresentation.
Children?s acoustic models wereestimated from 46 hours of audio from the CURead and Prompted Children?s Speech Corpus(Hagen et al, 2003)5 and the OGI Kids?
speechcorpus (Shobaki et al, 2000).During oral reading, the speech recognizermodels the story text using statistical n-gramlanguage models.
This approach gives therecognizer flexibility to insert/delete/substitute3 The CU Children?s Read Story Corpus is madeavailable for research purposes (http://cslr.colorado.edu)4 SONIC is freely downloadable for research usefrom (http://cslr.colorado.edu)5 This corpus differs from the test corpus in Section 3.words based on acoustics and to provide accurateconfidence information from the word-lattice.
Therecognizer receives packets of audio andautomatically detects voice activity.
When thechild speaks, the partial hypotheses are sent to areading tracking module.
The reading trackingmodule determines the current reading location byaligning each partial hypothesis with the story textusing a Dynamic Programming search.
In order toallow for skipping of words or even skipping to adifferent place within the text, the search findswords that when strung together minimize aweighted cost function of adjacent word-proximityand distance from the reader's last active readinglocation.
The Dynamic Programming searchadditionally incorporates constraints to account forboundary effects at the ends of each partial phrase.Hagen et al (2004) describes more recentadvances made to both acoustic and languagemodeling for oral-reading recognition of children?sspeech.
Specifically, that work describes the useof cross-utterance word history modeling, position-sensitive dynamic n-gram language modeling, aswell as vocal tract length normalization, speaker-adaptive training, and iterative unsupervisedspeaker adaptation for improved recognition.
Thefinal system was shown to have an overall worderror rate of 8.0% on the speech corpus describedin Section 3.
This system serves as the baseline forour experiments in the remainder of the paper.5 Event-based Word Error AnalysisWhile our earlier work in Hagen et al (2003)and Hagen et al (2004) described consistentimprovements in speech recognition accuracy onchildren?s speech, the use of raw word error ratedoes not reveal much information in terms ofwhere future improvements in system performancemay be obtained.
Because of this, we annotatedthe CU Children?s Read Story Corpus in terms of aset of event labels which we feel might have mostrelation to speech recognition error rate.
Next, inSection 5.1, we describe the event labelingmethodology and then provide a detailed erroranalysis of our baseline system in Section 5.2.5.1 Event Labeling MethodologyThe event labels for this project were chosenbased on the most common types of errors childrenmake when reading aloud.
Also included in thelabels are other acoustic events that occurfrequently, such as breaths and pauses, which maycontribute to an error made by the speechrecognizer.
The event labels for this study aresummarized in Table 1.
Common errors as statedbefore are word repetitions, omissions,substitutions, insertions, and self-corrections.Although pauses (PS) are natural in speech, toomany can disrupt the fluency of the read story.
If apause is extended, the recognizer may potentiallyinsert a word (during the silence region).
Similarly,we marked breath placements (BR) if they wereaudible.
We hypothesize that words may beinserted during periods of breath if not properlyaccounted for by the speech recognizer.Mispronunciations (MP) tend to occur when achild is faced with word he/she is not familiar withand makes an attempt at either sounding it out (orspeak fluently with an inappropriate phoneticrealization).
The use of wrong words (WW) iscommonly a result of fast reading.
The child mayonly read the first part of the word and guess onthe rest replacing the word with one that isphonologically similar.
An interjection (IJ) is anyword inserted into a sentence that is not in theoriginal text (e.g., ?um?
or ?ah?).
Repetitions(REP) occur when the child realizes he/she hasmade a mistake and self corrects him/herselfusually by repeating the misread word or bybeginning the sentence over again.
In some casesthe child catches his/her error before finishing theword and thus creating a partial word, however,since it is a conscious act by the child the word ismarked as a repetition assuming he/she did repeatit to self correct.Other important factors to be tracked by therecognizer are over-articulations (OA), hesitations(HS), non-speech segments (NS) and backgroundnoises (BN).
An over-articulation is considered tobe a deliberate sounding out of the word whereeach sound may be heard separately.
A child mayadditionally hesitate on a word while lookingahead at the next word causing parts of the word tobe elongated (e.g., stretched vowels).
The non-speech sound and background noise labels aremeant to indicate any noise outside of the child?sreading such as a cough or a door closing.
Wealso considered including a label for head-colds(HC), but later removed this label due toinconsistencies and subjective assessments needed.These labels were applied to 106 read storiesfrom the audio corpus described in Section 3.Each file was analyzed by one of three listenersand marked using these labels.
Reliability betweenthe listeners was checked by overlapping the filesanalyzed and comparing mark ups.
The eventlabeling and word-level transcription of the audiocorpus were conducted using the freely availableTranscriber software6.6 http://www.etca.fr/CTA/gip/Projets/Transcriber/Event Label& Event DescriptionTotalWords(%)WordError(%)None No Labeled Event 92.26 5.7REP Word Repetition 2.46 22.4BR Breath 1.44 26.1PW Partial Word 0.70 49.6PS Pause 0.70 40.5HS Hesitation/Elongation 0.67 13.8WW Wrong Word 0.60 48.1MP Mispronunciation 0.36 36.2BN Background Noise 0.30 15.5IJ Interjection / Insertion 0.28 61.3NS Non-Speech Sound 0.27 58.8OA Over-articulation 0.10 38.3Table 1: Event labels used in speech recognitionerror analysis on the CU Children?s Read StoryCorpus.
Total words aligned to each condition areshown (in %) along with the average word errorrate of the baseline system under each condition.The baseline system has a word error rate of 8.0%.5.2 Speech Recognition Error AnalysisUsing the NIST Speech Recognition ScoringToolkit (SCTK)7 we obtained the alignments of thereference word-level transcription with thehypothesized string from our baseline speechrecognition system.
By using the associated timinginformation, each word was then marked asbelonging to one of the event classes shown inTable 1 (or possibly no class marking).
Each wordwas further marked as correctly or incorrectlyrecognized by the speech recognizer using thescoring software.
Based upon this analysis we areable to deduce the percentage of words that areoutput from the speech recognizer and associatedwith each event condition (column 2 of Table 1).We also can determine the average word-error ratefor each labeled event type (column 3 of Table 1).What is most striking from Table 1 is that theaverage system word error rate during non-eventlabeled conditions is 5.7% while the average worderror rate for words associated with the labeledevent conditions is 31.5%.
While the speechrecognizer output during the labeled events issmall (approximately 7.7% of the words), theevents contribute to nearly 30% of the word errorrate of the system.
Most troubling are instances ofrepeated words and breaths made by the childduring read-aloud.
We suggest that future progresscan be made by focusing on (1) flexible n-gramlanguage modeling which may take into accountthe problem of word-repetition, and (2) moreaccurate acoustic modeling and rejection of breathevents during oral reading.7 http://www.nist.gov/speech/tools/6 Automatic Detection of Reading MiscuesAn important aspect in an automated readingtutor is the capability of detecting reading miscuesand utilizing this knowledge to provide appropriatefeedback.
The level of detail present in thefeedback strongly depends on the event detectionaccuracy, which is investigated in this paper.
Weleave the problem of determining what feedback toprovide as an area of future work.
First, we defineour miscue detection problem and then provide adescription of the features and classifier utilized.Finally, we evaluate our miscue detection systemusing the baseline speech recognition systemdescribed in Section 4.6.1 Problem FormulationOur main criterion for detecting events in oursystem is based on word alignments whichcompare the reference transcription of the child?sspeech to the reference story text.
Similarly toTam et al (2003), in order to detect readingmiscues the speech recognizer's hypothesizedoutput is aligned against the target story text usingthe Viterbi algorithm (i.e., hypothesis-targetalignment).
Furthermore the alignment of thehuman-based transcription against the story text isneeded in the classification / evaluation process todetermine where reading miscues actually occur(i.e., transcription-target algnment).We define a reading miscue event as anyinstance in which the child inserts, deletes orsubstitutes a word during oral reading.
Thereforeeach word spoken by the child is associated withan event label (insertion, deletion or substitution)or non-event (i.e., correct word).Given this word-level miscue labeling of the datawe can propose a detection problem.
Here, eachrecognized word is submitted to a classifier.
Thisclassifier labels each output word as correct orincorrect (i.e., a miscue event).
By thresholdingthe classifier output we can determine a detectionrate for a given false alarm rate and thereforedescribe a Receiver Operating Characteristic(ROC).
The detection rate is defined as thenumber of times the hypothesis-target andtranscription-target algnments show miscues at thesame position divided by the number oftranscription-target miscues.
The false alarm rate isdefined as the number of times the hypothesis-target algnment shows a miscue at a positionwhere the transcription-target algnment does not.We stress that we are not interested in the exactreading miscue (wrong word, correct word butpronounced incorrectly, partial word, etc.)
thatoccurred, which would request too specificinformation for a current state of the art system togive reliable feedback.
Rather, we wish to designan indicator that can accurately report the detectionof a miscue event whenever the text was not readcorrectly.In order to be able to map one alignment to theother, the two alignments need to be synchronized.Our approach synchronizes the two alignmentsover the target words in the actual story text.Therefore each target word represents a uniqueposition within both alignments.
If one or moreinsertions occurred before a certain word in thetarget sentence this event is noted in a datastructure attached to the specific target wordstating the number of inserted words before theactual spoken word.
If the word was replaced withanother word in the hypothesis or transcription, thewrong word will be aligned with the actual targetword, if a word is left out, no word from thehypothesis or transcription will be aligned with thespecific deleted target word.
Therefore the numberof tokens with additional information aboutsubstitutions, deletions and insertions in thehypothesis-target algnment and transcription willbe the same for both alignments and thereforeword-based synchronization is ensured.
Toillustrate the process a short example is given.
Thetarget sentence,it was the first day of summer vacationmight be spoken by the child (and transcribed) as,it was  it was the third day of summer vacationand the recognition hypothesis might state,it it was the first day vacationTherefore this transcription would have twoinsertions and one substitution events.
Thehypothesis would have one insertion and twodeletions (?of summer?).
The alignments alongwith the attached information are shown in Table2.
The miscue columns indicate an event occurringat a specific position in the target text or rightbefore it in the case of one or more insertionsbefore a certain word.Story(Target)Trans.(Ref.)ActualMiscueRecognizer(Hyp.
)Hyp.Miscueit it  0-0-0 it  0-0-0was was 0-0-0 was 0-0-1the the 0-0-2 the 0-0-0first third 1-0-0 first 0-0-0day day 0-0-0 day  0-0-0of of 0-0-0 <no_word> 0-1-0summer summer 0-0-0 <no_word> 0-1-0vacation vacation 0-0-0 vacation 0-0-0Table 2: Transcription-target algnment andhypothesis-target algnment with substitution-deletion-insertion (s-d-i) miscue annotation.This setup enables us to compute the detectionand false alarm rates based on the synchronizedalignments.
Within the Viterbi-alignment process asoft decision is made whether to classify a word asa substitution or not.
If the phonemes of thehypothesized word match the phonemes of thetarget word by at least 75% (determined byphoneme alignment) the word is accepted ascorrect.
This softer decision overcomes lessimportant events like misses of an ?s/z?
sound atthe end of a word (e.g., ?piano?
vs.
?pianos?
).6.2 Features for Miscue DetectionThe alignment based miscue detection is onlycapable of providing a single operating point(detection rate / false alarm rate).
We nextintroduce additional features which allow us tothreshold the classifier output and allow the systemto operate at any point along the ROC curve.In order to be able to operate the detector atdifferent levels of sensitivity additional features tothe alignment used in a classifier are a usefulextension.
The features we chose are,?
the word alignment(either 1 if the hypothesized word aligns to thetarget story word or 0 otherwise)?
the speech recognizer language model score(computed per word)?
the speech recognizer acoustic score(per word, normalized by frame count)?
the length of the pause in seconds before thecurrent word (0 if no pause exists)?
the number of phonemes in the current wordThe alignment is obtained as discussed inSection 6.1.
The language model score and thenormalized acoustic score are indicators for thequality of the match between the hypothesizedword?s model and the observed features.
Thelength of the pause before a word indicates ahesitation that might be a hint for a readingirregularity.
The number of phonemes shouldreflect the assumption that longer words aregenerally harder to read, especially for bad readers.6.3 Classifier FormulationWe trained a linear classifier based on thefeatures discussed above.
The use of a linearclassifier was motivated by earlier work of Hazenet al (2001) which demonstrated that such aclassifier can generate acceptable performance forspeech recognizer confidence estimation given thatthe decision surface is relatively simple.
Theclassifier can be expressed as,fpr Tvv=where pv  is the trained classification vector andfvis the feature vector described above.
The finalclassification is based on a threshold value.
If r isgreater than the threshold value, the instance underinvestigation is classified as a miscue, otherwise asa non-event.
By varying r over a certain range thereceiver operating characteristic (ROC) curve canbe obtained.6.4 EvaluationThe data set used to train the classifier consistsof 50% of the CU Children?s Read Story Corpusrandomly chosen such that age and grade levels aredistributed similarly to the entire corpus.
Thetraining examples represent both miscue and non-miscue events.
The miscues are those examplesthat represent substitutions, deletions, or insertionswithin the transcription-target algnment.
Thenegative examples are chosen from the non-miscueexamples.
There are 4,875 miscue and 8,715 non-miscue examples used to train the classifier.We tested the classifier on the remaining 50% ofthe corpus.
There are approximately 5,000 miscuesin the test set.
The ROC curve resulting from theclassification system applied to the test set isshown in Figure 2.
It can be seen that the overallperformance has a relatively high detection rate of67% with a false alarm rate of less than 3.0%.
Withthe detection rate adjusted to 70% and higher thefalse alarm rate increases rapidly.DT (%) 55.0 60.0 65.0 70.0 75.0 80.0FA (%) 2.6 2.7 2.9 5.1 19.8 36.4Figure 2: Detection rate vs. false alarm rate ROCfor the CU Children?s Read Story corpus.Example operating points are shown below.7 ConclusionsIn this paper we have described the ColoradoLiteracy Tutor (CLT) which aims to improveliteracy and student achievement in public schools.We extended on our previous work in severalnovel aspects.
First, we have collected andannotated a children?s speech corpus in terms of aset of labeled event conditions which we believestrongly correlate to speech recognition error.
Infact while these events make up nearly 8% of thedata, they were shown to account forapproximately 30% of the word errors in a state-of-the-art speech recognition system.
To ourknowledge, previous work has not considered sucha detailed word error analysis on a children?sspeech corpus.
We then provided our initialframework for detecting oral reading miscues.Using a simple linear classifier and using featuresderived from a speech recognizer, wedemonstrated that 67% of reading miscues can bedetected at a false alarm rate of 3%.
While thissystem appears to outperform the previous resultspresented in Tam et al (2003), we point out thatthere is currently no standardized test set availableto directly compare those systems.
Therefore, theaudio corpus and event labeling presented in thispaper will be made available to researchers topromote community-wide benchmarking.
In thefuture we plan to correlate the miscue detectionperformance with the event labeling strategyoutlined in Section 5 of the paper.
We expect thatsuch an error analysis will continue to provideinsight to areas for system development.8 AcknowledgementsThis work was supported by grants from theNational Science Foundation's ITR and IERIPrograms under grants NSF/ITR: REC-0115419,NSF/IERI: EIA-0121201, NSF/ITR: IIS-0086107,NSF/IERI: 1R01HD-44276.01; and the ColemanInstitute for Cognitive Disabilities.
The viewsexpressed in this paper do not necessarily representthe views of the NSF.ReferencesR.
Cole, S. van Vuuren, B. Pellom, K. Hacioglu, J. Ma,J.
Movellan, S. Schwartz, D. Wade-Stein, W. Ward, J.Yan.
2003.
Perceptive Animated Interfaces: FirstSteps Toward a New Paradigm for Human ComputerInteraction.
Proceedings of the IEEE, Vol.
91, No.
9,pp.
1391-1405.S.
Deerwester, S. Dumais, T. Landauer, G. Furnas, andR.
Harshman.
1990.
Indexing by Latent SemanticAnalysis.
Journal of the Society for InformationScience, vol.
41, no.
6, pp.
391-407.J.
Fogarty, L. Dabbish, D. Steck, and J. Mostow.
2001.Mining a Database of Reading Mistakes: For Whatshould an Automated Reading Tutor Listen?
In J. D.Moore, C. L. Redfield, and W. L. Johnson (Eds.
),Artificial Intelligence in Education:  AI-ED in theWired and Wireless Future, pp.
422-433.A.
Hagen, B. Pellom, and R. Cole.
2003.
Children?sSpeech Recognition with Application to InteractiveBooks and Tutors.
ASRU-2003, St. Thomas, USA.A.
Hagen, B. Pellom, S. Van Vuuren, R. Cole.
2004.Advances in Children?s Speech Recognition within anInteractive Literacy Tutor.
HLT-NAACL, BostonMassachusetts, USA.T.
Hazen, S. Seneff, and J. Polifroni.
2002.
RecognitionConfidence Scoring and its Use in SpeechUnderstanding Systems.
Computer Speech andLanguage, Vol.
16,No.
1, pp.
49-67.E.
Kintsch, D. Steinhart, G. Stahl, C. Matthews, R.Lamb, and LRG.
2000.
Developing SummarizationSkills through the Use of LSA-based Feedback.Interactive Learning Environments, Vol.
8, pp.
87-109.T.
Landauer and S. Dumais.
1997.
A Solution to Plato'sProblem: The Latent Semantic Analysis Theory ofAcquisition, Induction and Representation ofKnowledge.
Psych.
Review, Vol.
104, pp.
211-240.M.
McCandless.
1992.
Word Rejection for a LiteracyTutor.
Bachelor of Science Thesis, MIT.J.
Mostow, G. Aist, P. Burkhead, A. Corbett, A. Cuneo,S.
Eitelman, C. Huang, B. Junker, M. B. Sklar, and B.Tobin.
2003.
Evaluation of an Automated ReadingTutor that Listens:  Comparison to Human Tutoringand Classroom Instruction.
Journal of EducationalComputing Research, 29(1), 61-117.J.
Mostow, J. Beck, S. Winter, S. Wang, and B. Tobin.2002.
Predicting Oral Reading Miscues.
ICSLP-02,Denver, Colorado.J.
Mostow, S. Roth, A. G. Hauptmann, and M. Kane.1994.
A Prototype Reading Coach that Listens.AAAI-94, Seattle, WA, pp.
785-792.B.
Pellom.
2001.
SONIC: The University of ColoradoContinuous Speech Recognizer.
Technical Report TR-CSLR-2001-01, University of Colorado.B.
Pellom, K. Hacioglu.
2003.
Recent Improvements inthe CU SONIC ASR System for Noisy Speech: TheSPINE Task.
Proc.
ICASSP, Hong Kong.K.
Shobaki, J.-P. Hosom, and R. Cole.
2000.
The OGIKids' Speech Corpus and Recognizers.
Proc.
ICSLP-2000, Beijing, China.D.
Steinhart.
2001.
Summary Street: An IntelligentTutoring System for Improving Student Writingthrough the Use of Latent Semantic Analysis.
Ph.D.Dissertation, Dept.
Psychology, Univ.
of Colorado,Boulder, CO.Y-C. Tam, J. Mostow, J. Beck, and S. Banerjee.
2003.Training a Confidence Measure for a Reading Tutorthat Listens.
Proc.
Eurospeech, Geneva, Switzerland,3161-3164.U.
Yapanel, J. H.L.
Hansen.
2003.
A New Perspectiveon Feature Extraction for Robust In-vehicle SpeechRecognition.
Proc.
Eurospeech, Geneva, Switzerland.V.
Zue, S. Seneff, J. Polifroni, H. Meng, J.
Glass.
1996.Multilingual Human-Computer Interactions: FromInformation Access to Language Learning.
ICSLP-96,Philadelphia, PA.
