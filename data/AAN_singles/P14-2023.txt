Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 136?142,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsDecoder Integration and Expected BLEU Trainingfor Recurrent Neural Network Language ModelsMichael AuliMicrosoft ResearchRedmond, WA, USAmichael.auli@microsoft.comJianfeng GaoMicrosoft ResearchRedmond, WA, USAjfgao@microsoft.comAbstractNeural network language models are oftentrained by optimizing likelihood, but wewould prefer to optimize for a task specificmetric, such as BLEU in machine trans-lation.
We show how a recurrent neuralnetwork language model can be optimizedtowards an expected BLEU loss insteadof the usual cross-entropy criterion.
Fur-thermore, we tackle the issue of directlyintegrating a recurrent network into first-pass decoding under an efficient approxi-mation.
Our best results improve a phrase-based statistical machine translation sys-tem trained on WMT 2012 French-Englishdata by up to 2.0 BLEU, and the expectedBLEU objective improves over a cross-entropy trained model by up to 0.6 BLEUin a single reference setup.1 IntroductionNeural network-based language and translationmodels have achieved impressive accuracy im-provements on statistical machine translation tasks(Allauzen et al, 2011; Le et al, 2012b; Schwenket al, 2012; Vaswani et al, 2013; Gao et al, 2014).In this paper we focus on recurrent neural networkarchitectures which have recently advanced thestate of the art in language modeling (Mikolov etal., 2010; Mikolov et al, 2011; Sundermeyer et al,2013) with several subsequent applications in ma-chine translation (Auli et al, 2013; Kalchbrennerand Blunsom, 2013; Hu et al, 2014).
Recurrentmodels have the potential to capture long-span de-pendencies since their predictions are based on anunbounded history of previous words (?2).In practice, neural network models for machinetranslation are usually trained by maximizing thelikelihood of the training data, either via a cross-entropy objective (Mikolov et al, 2010; Schwenket al, 2012) or more recently, noise-contrastive es-timation (Vaswani et al, 2013).
However, it iswidely appreciated that directly optimizing for atask-specific metric often leads to better perfor-mance (Goodman, 1996; Och, 2003; Auli andLopez, 2011).
The expected BLEU objective pro-vides an efficient way of achieving this for ma-chine translation (Rosti et al, 2010; Rosti et al,2011; He and Deng, 2012; Gao and He, 2013;Gao et al, 2014) instead of solely relying on tra-ditional optimizers such as Minimum Error RateTraining (MERT) that only adjust the weightingof entire component models within the log-linearframework of machine translation (?3).Most previous work on neural networks for ma-chine translation is based on a rescoring setup(Arisoy et al, 2012; Mikolov, 2012; Le et al,2012a; Auli et al, 2013), thereby side steppingthe algorithmic and engineering challenges of di-rect decoder-integration.
One recent exception isVaswani et al (2013) who demonstrated that feed-forward network-based language models are moreaccurate in first-pass decoding than in rescoring.Decoder integration has the advantage for the neu-ral network to directly influence search, unlikerescoring which is restricted to an n-best list or lat-tice.
Decoding with feed-forward architectures isstraightforward, since predictions are based on afixed size input, similar to n-gram language mod-els.
However, for recurrent networks we have todeal with the unbounded history, which breaks theusual dynamic programming assumptions for effi-cient search.
We show how a simple but effectiveapproximation can side step this issue and we em-pirically demonstrate its effectiveness (?4).We test the expected BLEU objective by train-ing a recurrent neural network language modeland obtain substantial improvements.
We also findthat our efficient approximation for decoder inte-gration is very accurate, clearly outperforming arescoring setup (?5).136wtht-1htytVWU001000Figure 1: Structure of the recurrent neural networklanguage model.2 Recurrent Neural Network LMsOur model has a similar structure to the recurrentneural network language model of Mikolov et al(2010) which is factored into an input layer, a hid-den layer with recurrent connections, and an out-put layer (Figure 1).
The input layer encodes theword at position t as a 1-of-N vector wt.
The out-put layer ytrepresents scores over possible nextwords; both the input and output layers are of size|V |, the size of the vocabulary.
The hidden layerstate htencodes the history of all words observedin the sequence up to time step t. The state ofthe hidden layer is determined by the input layerand the hidden layer configuration of the previoustime step ht?1.
The weights of the connectionsbetween the layers are summarized in a numberof matrices: U represents weights from the in-put layer to the hidden layer, and W representsconnections from the previous hidden layer to thecurrent hidden layer.
Matrix V contains weightsbetween the current hidden layer and the outputlayer.
The activations of the hidden and outputlayers are computed by:ht= tanh(Uwt+ Wht?1)yt= tanh(Vht)Different to previous work (Mikolov et al, 2010),we do not use the softmax activation function tooutput a probability over the next word, but in-stead just compute a single unnormalized score.This is computationally more efficient than sum-ming over all possible outputs such as requiredfor the cross-entropy error function (Bengio et al,2003; Mikolov et al, 2010; Schwenk et al, 2012).Training is based on the back propagation throughtime algorithm, which unrolls the network andthen computes error gradients over multiple timesteps (Rumelhart et al, 1986); we use the expectedBLEU loss (?3) to obtain the error with respect tothe output activations.
After training, the outputlayer represents scores s(wt+1|w1.
.
.
wt,ht) forthe next word given the previous t input words andthe current hidden layer configuration ht.3 Expected BLEU TrainingWe integrate the recurrent neural network lan-guage model as an additional feature into the stan-dard log-linear framework of translation (Och,2003).
Formally, our phrase-based model is pa-rameterized by M parameters ?
where each ?m?
?, m = 1 .
.
.M is the weight of an associatedfeature hm(f, e).
Function h(f, e) maps foreignsentences f and English sentences e to the vectorh1(f, e) .
.
.
(f, e), and the model chooses transla-tions according to the following decision rule:e?
= arg maxe?E(f)?Th(f, e)We summarize the weights of the recurrent neuralnetwork language model as ?
= {U,W,V} andadd the model as an additional feature to the log-linear translation model using the simplified nota-tion s?
(wt) = s(wt|w1.
.
.
wt?1,ht?1):hM+1(e) = s?
(e) =|e|?t=1log s?
(wt) (1)which computes a sentence-level language modelscore as the sum of individual word scores.
Thetranslation model is parameterized by ?
and ?which are learned as follows (Gao et al, 2014):1.
We generate an n-best list for each foreignsentence in the training data with the baselinetranslation system given ?
where ?M+1= 0using the settings described in ?5.
The n-bestlists serve as an approximation to E(f) usedin the next step for expected BLEU trainingof the recurrent neural network model (?3.1).2.
Next, we fix ?, set ?M+1= 1 and opti-mize ?
with respect to the loss function onthe training data using stochastic gradient de-scent (SGD).11We tuned ?M+1on the development set but found that?M+1= 1 resulted in faster training and equal accuracy.1373.
We fix ?
and re-optimize ?
in the presenceof the recurrent neural network model usingMinimum Error Rate Training (Och, 2003)on the development set (?5).3.1 Expected BLEU ObjectiveFormally, we define our loss function l(?)
asthe negative expected BLEU score, denoted asxBLEU(?)
for a given foreign sentence f :l(?)
=?
xBLEU(?)=?e?E(f)p?,?
(e|f)sBLEU(e, e(i)) (2)where sBLEU(e, e(i)) is a smoothed sentence-level BLEU score with respect to the referencetranslation e(i), and E(f) is the generation setgiven by an n-best list.2We use a sentence-levelBLEU approximation similar to He and Deng(2012).3The normalized probability p?,?
(e|f) ofa particular translation e given f is defined as:p?,?
(e|f) =exp{?
?Th(f, e)}?e??E(f)exp{?
?Th(f, e?
)}(3)where ?Th(f, e) includes the recurrent neural net-work hM+1(e), and ?
?
[0, inf) is a scaling factorthat flattens the distribution for ?
< 1 and sharp-ens it for ?
> 1 (Tromble et al, 2008).4Next, we define the gradient of the expectedBLEU loss function l(?)
using the observation thatthe loss does not explicitly depend on ?:?l(?)??=?e|e|?t=1?l(?)?s?(wt)?s?(wt)??=?e|e|?t=1??wt?s?(wt)?
?where ?wtis the error term for English word wt.5The error term indicates how the loss changes withthe translation probability which we derive next.62Our definitions do not take into account multiple derivationsfor the same translation because our n-best lists contain onlyunique entries which we obtain by choosing the highest scor-ing translation among string identical candidates.3In early experiments we found that the BLEU+1 approxi-mation used by Liang et al (2006) and Nakov et.
al (2012)worked equally well in our setting.4The ?
parameter is only used during expected BLEU trainingbut not for subsequent MERT tuning.5A sentence may contain the same word multiple times andwe compute the error term for each occurrence separatelysince the error depends on the individual history.6We omit the gradient of the recurrent neural network score?s?(wt)?
?since it follows the standard form (Mikolov, 2012).3.2 Derivation of the Error Term ?wtWe rewrite the loss function (2) using (3) and sep-arate it into two terms G(?)
and Z(?)
as follows:l(?)
= ?xBLEU(?)
= ?G(?)Z(?
)(4)= ??e?E(f)exp{?
?Th(f, e)} sBLEU(e, e(i))?e?E(f)exp{?
?Th(f, e)}Next, we apply the quotient rule of differentiation:?wt=?xBLEU(?)?s?(wt)=?(G(?)/Z(?))?s?(wt)=1Z(?)(?G(?)?s?(wt)??Z(?)?s?(wt)xBLEU(?
))Using the observation that ?
is only relevant to therecurrent neural network hM+1(e) (1) we have??
?Th(f, e)?s?
(wt)= ??M+1?hM+1(e)?s?(wt)=??M+1s?
(wt)which together with the chain rule, (3) and (4) al-lows us to rewrite ?wtas follows:?wt=1Z(?)?e?E(f),s.t.wt?e(?
exp{?
?Th(f, e)}?s?
(wt)U(?, e))=?e?E(f),s.t.wt?e(p?,?
(e|f)U(?, e)?M+1?s?
(wt))where U(?, e) = sBLEU(e, ei)?
xBLEU(?
).4 Decoder IntegrationDirectly integrating our recurrent neural networklanguage model into first-pass decoding enables usto search a much larger space than would be pos-sible in rescoring.Typically, phrase-based decoders maintain a setof states representing partial and complete transla-tion hypothesis that are scored by a set of features.Most features are local, meaning that all requiredinformation for them to assign a score is availablewithin the state.
One exception is the n-gram lan-guage model which requires the preceding n ?
1words as well.
In order to accommodate this fea-ture, each state usually keeps these words as con-text.
Unfortunately, a recurrent neural networkmakes even weaker independence assumptions so138that it depends on the entire left prefix of a sen-tence.
Furthermore, the weaker independence as-sumptions also dramatically reduce the effective-ness of dynamic programming by allowing muchfewer states to be recombined.7To solve this problem, we follow previous workon lattice rescoring with recurrent networks thatmaintained the usual n-gram context but kept abeam of hidden layer configurations at each state(Auli et al, 2013).
In fact, to make decoding asefficient as possible, we only keep the single bestscoring hidden layer configuration.
This approx-imation has been effective for lattice rescoring,since the translations represented by each state arein fact very similar: They share both the samesource words as well as the same n-gram contextwhich is likely to result in similar recurrent his-tories that can be safely pruned.
As future costestimate we score each phrase in isolation, reset-ting the hidden layer at the beginning of a phrase.While simple, we found our estimate to be moreaccurate than no future cost at all.5 ExperimentsBaseline.
We use a phrase-based system simi-lar to Moses (Koehn et al, 2007) based on a setof common features including maximum likeli-hood estimates pML(e|f) and pML(f |e), lexicallyweighted estimates pLW(e|f) and pLW(f |e),word and phrase-penalties, a hierarchical reorder-ing model (Galley and Manning, 2008), a lineardistortion feature, and a modified Kneser-Ney lan-guage model trained on the target-side of the paral-lel data.
Log-linear weights are tuned with MERT.Evaluation.
We use training and test data fromthe WMT 2012 campaign and report results onFrench-English and German-English.
Transla-tion models are estimated on 102M words of par-allel data for French-English, and 99M wordsfor German-English; about 6.5M words for eachlanguage pair are newswire, the remainder areparliamentary proceedings.
We evaluate on sixnewswire domain test sets from 2008 to 2013 con-taining between 2034 to 3003 sentences.
Log-linear weights are estimated on the 2009 data setcomprising 2525 sentences.
We evaluate accuracyin terms of BLEU with a single reference.Rescoring Setup.
For rescoring we use ei-7Recombination only retains the highest scoring state if thereare multiple identical states, that is, they cover the samesource span, the same translation phrase and contexts.ther lattices or the unique 100-best output ofthe phrase-based decoder and re-estimate the log-linear weights by running a further iteration ofMERT on the n-best list of the development set,augmented by scores corresponding to the neuralnetwork models.
At test time we rescore n-bestlists with the new weights.Neural Network Training.
All neural networkmodels are trained on the news portion of theparallel data, corresponding to 136K sentences,which we found to be most useful in initial exper-iments.
As training data we use unique 100-bestlists generated by the baseline system.
We use thesame data both for training the phrase-based sys-tem as well as the language model but find thatthe resulting bias did not hurt end-to-end accu-racy (Yu et al, 2013).
The vocabulary consists ofwords that occur in at least two different sentences,which is 31K words for both language pairs.
Wetuned the learning rate ?
of our mini-batch SGDtrainer as well as the probability scaling parameter?
(3) on a held-out set and found simple settings of?
= 0.1 and ?
= 1 to be good choices.
To preventover-fitting, we experimented with L2 regulariza-tion, but found no accuracy improvements, prob-ably because SGD regularizes enough.
We evalu-ate performance on a held-out set during trainingand stop whenever the objective changes less than0.0003.
The hidden layer uses 100 neurons unlessotherwise stated.5.1 Decoder IntegrationWe compare the effect of direct decoder integra-tion to rescoring with both lattices and n-best listswhen the model is trained with a cross-entropy ob-jective (Mikolov et al, 2010).
The results (Ta-ble 1 and Table 2) show that direct integration im-proves accuracy across all six test sets on both lan-guage pairs.
For French-English we improve overn-best rescoring by up to 1.1 BLEU and by up to0.5 BLEU for German-English.
We improve overlattice rescoring by up to 0.4 BLEU on French-English and by up to 0.3 BLEU on German-English.
Compared to the baseline, we achieveimprovements of up to 2.0 BLEU for French-English and up to 1.3 BLEU for German-English.The average improvement across all test sets is1.5 BLEU for French-English and 1.0 BLEU forGerman-English compared to the baseline.139dev 2008 2010 syscomb2010 2011 2012 2013 AllTestBaseline 24.11 20.73 24.68 24.59 25.62 24.85 25.54 24.53RNN n-best rescore 24.83 21.41 25.17 25.06 26.53 25.74 26.31 25.25RNN lattice rescore 24.91 21.73 25.56 25.43 27.04 26.43 26.75 25.72RNN decode 25.14 22.03 25.86 25.74 27.32 26.86 27.15 26.06Table 1: French-English accuracy of decoder integration of a recurrent neural network language model(RNN decode) compared to n-best and lattice rescoring as well as the output of a phrase-based systemusing an n-gram model (Baseline); Alltest is the corpus-weighted average BLEU across all test sets.dev 2008 2010 syscomb2010 2011 2012 2013 AllTestBaseline 19.35 19.96 20.87 20.66 19.60 19.80 22.48 20.58RNN n-best rescore 20.17 20.29 21.35 21.27 20.51 20.54 23.03 21.21RNN lattice rescore 20.24 20.38 21.55 21.43 20.77 20.63 23.23 21.38RNN decode 20.13 20.51 21.79 21.71 20.91 20.93 23.53 21.61Table 2: German-English results of direct decoder integration (cf.
Table 1).dev 2008 2010 syscomb2010 2011 2012 2013 AllTestBaseline 24.11 20.73 24.68 24.59 25.62 24.85 25.54 24.53CE RNN 24.80 21.15 25.14 25.06 26.45 25.83 26.69 25.29+ xBLEU RNN 25.11 21.74 25.52 25.42 27.06 26.42 26.72 25.71Table 3: French-English accuracy of a decoder integrated cross-entropy recurrent neural network model(CE RNN) and a combination with an expected BLEU trained model (xBLEU RNN).
Results are notcomparable to Table 1 since a smaller hidden layer was used to keep training times manageable (?5.2).5.2 Expected BLEU TrainingTraining with the expected BLEU loss is compu-tationally more expensive than with cross-entropysince each training example is an n-best list in-stead of a single sentence.
This increases the num-ber of words to be processed from 3.5M to 340M.To keep training times manageable, we reduce thehidden layer size to 30 neurons, thereby greatlyincreasing speed.
Despite slower training, the ac-tual scoring at test time of expected BLEU mod-els is about 5 times faster than for cross-entropymodels since we do not need to normalize the out-put layer anymore.
The results (Table 3) showimprovements of up to 0.6 BLEU when combin-ing a cross-entropy model with an expected BLEUvariant.
Average gains across all test sets are 0.4BLEU, demonstrating that the gains from the ex-pected BLEU loss are additive.6 Conclusion and Future WorkWe introduce an empirically effective approxima-tion to integrate a recurrent neural network modelinto first pass decoding, thereby extending pre-vious work on decoding with feed-forward neu-ral networks (Vaswani et al, 2013).
Our best re-sult improves the output of a phrase-based decoderby up to 2.0 BLEU on French-English translation,outperforming n-best rescoring by up to 1.1 BLEUand lattice rescoring by up to 0.4 BLEU.
Directlyoptimizing a recurrent neural network languagemodel towards an expected BLEU loss proves ef-fective, improving a cross-entropy trained variantby up 0.6 BLEU.
Despite higher training complex-ity, our expected BLEU trained model has fivetimes faster runtime than a cross-entropy modelsince it does not require normalization.In future work, we would like to scale up tolarger data sets and more complex models throughparallelization.
We would also like to experimentwith more elaborate future cost estimates, such asthe average score assigned to all occurrences of aphrase in a large corpus.7 AcknowledgmentsWe thank Michel Galley, Arul Menezes, ChrisQuirk and Geoffrey Zweig for helpful discussionsrelated to this work as well as the four anonymousreviewers for their comments.140ReferencesAlexandre Allauzen, H?el`ene Bonneau-Maynard, Hai-Son Le, Aur?elien Max, Guillaume Wisniewski,Franc?ois Yvon, Gilles Adda, Josep Maria Crego,Adrien Lardilleux, Thomas Lavergne, and ArtemSokolov.
2011.
LIMSI @ WMT11.
In Proc.
ofWMT, pages 309?315, Edinburgh, Scotland, July.Association for Computational Linguistics.Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, andBhuvana Ramabhadran.
2012.
Deep Neural Net-work Language Models.
In NAACL-HLT Work-shop on the Future of Language Modeling for HLT,pages 20?28, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Michael Auli and Adam Lopez.
2011.
Training aLog-Linear Parser with Loss Functions via Softmax-Margin.
In Proc.
of EMNLP, pages 333?343.
Asso-ciation for Computational Linguistics, July.Michael Auli, Michel Galley, Chris Quirk, and Geof-frey Zweig.
2013.
Joint Language and TranslationModeling with Recurrent Neural Networks.
In Proc.of EMNLP, October.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A Neural Probabilistic Lan-guage Model.
Journal of Machine Learning Re-search, 3:1137?1155.Michel Galley and Christopher D. Manning.
2008.
ASimple and Effective Hierarchical Phrase Reorder-ing Model.
In Proc.
of EMNLP, pages 848?856.Jianfeng Gao and Xiaodong He.
2013.
Training MRF-Based Phrase Translation Models using GradientAscent.
In Proc.
of NAACL-HLT, pages 450?459.Association for Computational Linguistics, June.Jianfeng Gao, Xiaodong He, Scott Wen tau Yih, andLi Deng.
2014.
Learning Continuous Phrase Rep-resentations for Translation Modeling.
In Proc.of ACL.
Association for Computational Linguistics,June.Joshua Goodman.
1996.
Parsing Algorithms and Met-rics.
In Proc.
of ACL, pages 177?183, Santa Cruz,CA, USA, June.Xiaodong He and Li Deng.
2012.
Maximum ExpectedBLEU Training of Phrase and Lexicon TranslationModels.
In Proc.
of ACL, pages 8?14.
Associationfor Computational Linguistics, July.Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.2014.
Minimum Translation Modeling with Recur-rent Neural Networks.
In Proc.
of EACL.
Associa-tion for Computational Linguistics, April.Nal Kalchbrenner and Phil Blunsom.
2013.
Re-current Continuous Translation Models.
In Proc.of EMNLP, pages 1700?1709, Seattle, Washington,USA, October.
Association for Computational Lin-guistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proc.
of ACL Demo and Poster Sessions, pages177?180, Prague, Czech Republic, Jun.Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012a.
Continuous Space Translation Models withNeural Networks.
In Proc.
of HLT-NAACL, pages39?48, Montr?eal, Canada.
Association for Compu-tational Linguistics.Hai-Son Le, Thomas Lavergne, Alexandre Al-lauzen, Marianna Apidianaki, Li Gong, Aur?elienMax, Artem Sokolov, Guillaume Wisniewski, andFranc?ois Yvon.
2012b.
LIMSI @ WMT12.
In Proc.of WMT, pages 330?337, Montr?eal, Canada, June.Association for Computational Linguistics.Percy Liang, Alexandre Bouchard-C?ot?e, Ben Taskar,and Dan Klein.
2006.
An end-to-end discriminativeapproach to machine translation.
In Proc.
of ACL-COLING, pages 761?768, Jul.Tom?a?s Mikolov, Karafi?at Martin, Luk?a?s Burget, JanCernock?y, and Sanjeev Khudanpur.
2010.
Recur-rent Neural Network based Language Model.
InProc.
of INTERSPEECH, pages 1045?1048.Tom?a?s Mikolov, Anoop Deoras, Daniel Povey, Luk?a?sBurget, and Jan?Cernock?y.
2011.
Strategies forTraining Large Scale Neural Network LanguageModels.
In Proc.
of ASRU, pages 196?201.Tom?a?s Mikolov.
2012.
Statistical Language Modelsbased on Neural Networks.
Ph.D. thesis, Brno Uni-versity of Technology.Preslav Nakov, Francisco Guzman, and Stephan Vo-gel.
2012.
Optimizing for Sentence-Level BLEU+1Yields Short Translations.
In Proc.
of COLING.
As-sociation for Computational Linguistics.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proc.
of ACL,pages 160?167, Sapporo, Japan, July.Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2010.
BBN System De-scription for WMT10 System Combination Task.In Proc.
of WMT, pages 321?326.
Association forComputational Linguistics, July.Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2011.
Expected BLEUTraining for Graphs: BBN System Description forWMT11 System Combination Task.
In Proc.
ofWMT, pages 159?165.
Association for Computa-tional Linguistics, July.David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.Williams.
1986.
Learning Internal Representationsby Error Propagation.
In Symposium on Parallel andDistributed Processing.141Holger Schwenk, Anthony Rousseau, and MohammedAttik.
2012.
Large, Pruned or Continuous SpaceLanguage Models on a GPU for Statistical MachineTranslation.
In NAACL-HLT Workshop on the Fu-ture of Language Modeling for HLT, pages 11?19.Association for Computational Linguistics.Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,Ben Freiberg, Ralf Schl?uter, and Hermann Ney.2013.
Comparison of Feedforward and RecurrentNeural Network Language Models.
In IEEE Inter-national Conference on Acoustics, Speech, and Sig-nal Processing, pages 8430?8434, May.Roy W. Tromble, Shankar Kumar, Franz Och, andWolfgang Macherey.
2008.
Lattice MinimumBayes-Risk Decoding for Statistical Machine Trans-lation.
In Proc.
of EMNLP, pages 620?629.
Associ-ation for Computational Linguistics, October.Ashish Vaswani, Yinggong Zhao, Victoria Fossum, andDavid Chiang.
2013.
Decoding with Large-scaleNeural Language Models improves Translation.
InProc.
of EMNLP.
Association for ComputationalLinguistics, October.Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.2013.
Max-Violation Perceptron and Forced Decod-ing for Scalable MT Training.
In Proc.
of EMNLP,pages 1112?1123.
Association for ComputationalLinguistics, October.142
