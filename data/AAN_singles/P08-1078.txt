Proceedings of ACL-08: HLT, pages 683?691,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsContextual PreferencesIdan Szpektor, Ido Dagan, Roy Bar-HaimDepartment of Computer ScienceBar-Ilan UniversityRamat Gan, Israel{szpekti,dagan,barhair}@cs.biu.ac.ilJacob GoldbergerSchool of EngineeringBar-Ilan UniversityRamat Gan, Israelgoldbej@eng.biu.ac.ilAbstractThe validity of semantic inferences dependson the contexts in which they are applied.We propose a generic framework for handlingcontextual considerations within applied in-ference, termed Contextual Preferences.
Thisframework defines the various context-awarecomponents needed for inference and theirrelationships.
Contextual preferences extendand generalize previous notions, such as se-lectional preferences, while experiments showthat the extended framework allows improvinginference quality on real application data.1 IntroductionApplied semantic inference is typically concernedwith inferring a target meaning from a given text.For example, to answer ?Who wrote Idomeneo?
?,Question Answering (QA) systems need to infer thetarget meaning ?Mozart wrote Idomeneo?
from agiven text ?Mozart composed Idomeneo?.
Followingcommon Textual Entailment terminology (Giampic-colo et al, 2007), we denote the target meaning by h(for hypothesis) and the given text by t.A typical applied inference operation is matching.Sometimes, h can be directly matched in t (in theexample above, if the given sentence would be liter-ally ?Mozart wrote Idomeneo?).
Generally, the tar-get meaning can be expressed in t in many differ-ent ways.
Indirect matching is then needed, usinginference knowledge that may be captured throughrules, termed here entailment rules.
In our exam-ple, ?Mozart wrote Idomeneo?
can be inferred usingthe rule ?X compose Y ?
X write Y ?.
Recently,several algorithms were proposed for automaticallylearning entailment rules and paraphrases (viewedas bi-directional entailment rules) (Lin and Pantel,2001; Ravichandran and Hovy, 2002; Shinyama etal., 2002; Szpektor et al, 2004; Sekine, 2005).A common practice is to try matching the struc-ture of h, or of the left-hand-side of a rule r, withint.
However, context should be considered to allowvalid matching.
For example, suppose that to findacquisitions of companies we specify the target tem-plate hypothesis (a hypothesis with variables) ?X ac-quire Y ?.
This h should not be matched in ?childrenacquire language quickly?, because in this contextY is not a company.
Similarly, the rule ?X chargeY ?
X accuse Y ?
should not be applied to ?Thisstore charged my account?, since the assumed senseof ?charge?
in the rule is different than its sense inthe text.
Thus, the intended contexts for h and rand the context within the given t should be prop-erly matched to verify valid inference.Context matching at inference time was of-ten approached in an application-specific manner(Harabagiu et al, 2003; Patwardhan and Riloff,2007).
Recently, some generic methods were pro-posed to handle context-sensitive inference (Daganet al, 2006; Pantel et al, 2007; Downey et al, 2007;Connor and Roth, 2007), but these usually treatonly a single aspect of context matching (see Sec-tion 6).
We propose a comprehensive framework forhandling various contextual considerations, termedContextual Preferences.
It extends and generalizesprevious work, defining the needed contextual com-ponents and their relationships.
We also present andimplement concrete representation models and un-683supervised matching methods for these components.While our presentation focuses on semantic infer-ence using lexical-syntactic structures, the proposedframework and models seem suitable for other com-mon types of representations as well.We applied our models to a test set derived fromthe ACE 2005 event detection task, a standard In-formation Extraction (IE) benchmark.
We show thebenefits of our extended framework for textual in-ference and present component-wise analysis of theresults.
To the best of our knowledge, these are alsothe first unsupervised results for event argument ex-traction in the ACE 2005 dataset.2 Contextual Preferences2.1 NotationAs mentioned above, we follow the generic Tex-tual Entailment (TE) setting, testing whether a targetmeaning hypothesis h can be inferred from a giventext t. We allow h to be either a text or a template,a text fragment with variables.
For example, ?Thestock rose 8%?
entails an instantiation of the tem-plate hypothesis ?X gain Y ?.
Typically, h representsan information need requested in some application,such as a target predicate in IE.In this paper, we focus on parse-based lexical-syntactic representation of texts and hypotheses, andon the basic inference operation of matching.
Fol-lowing common practice (de Salvo Braz et al, 2005;Romano et al, 2006; Bar-Haim et al, 2007), h issyntactically matched in t if it can be embedded int?s parse tree.
For template hypotheses, the matchinginduces a mapping between h?s variables and theirinstantiation in t.Matching h in t can be performed either directlyor indirectly using entailment rules.
An entailmentrule r: ?LHS ?
RHS?
is a directional entailmentrelation between two templates.
h is matched in t us-ing r if LHS is matched in t and h matches RHS.In the example above, r: ?X rise Y ?X gain Y ?
al-lows us to entail ?X gain Y ?, with ?stock?
and ?8%?instantiating h?s variables.
We denote vars(z) theset of variables of z, where z is a template or a rule.2.2 MotivationWhen matching considers only the structure of hy-potheses, texts and rules it may result in incorrectinference due to contextual mismatches.
For exam-ple, an IE system may identify mentions of publicdemonstrations using the hypothesis h: ?X demon-strate?.
However, h should not be matched in ?Engi-neers demonstrated the new system?, due to a mis-match between the intended sense of ?demonstrate?in h and its sense in t. Similarly, when looking forphysical attack mentions using the hypothesis ?X at-tack Y ?, we should not utilize the rule r: ?X accuseY ?X attack Y ?, due to a mismatch between a ver-bal attack in r and an intended physical attack in h.Finally, r: ?X produce Y ?
X lay Y ?
(applicablewhen X refers to poultry and Y to eggs) should notbe matched in t: ?Bugatti produce the fastest cars?,due to a mismatch between the meanings of ?pro-duce?
in r and t. Overall, such incorrect inferencesmay be avoided by considering contextual informa-tion for t, h and r during their matching process.2.3 The Contextual Preferences FrameworkWe propose the Contextual Preferences (CP) frame-work for addressing context at inference time.
In thisframework, the representation of an object z, wherez may be a text, a template or an entailment rule, isenriched with contextual information denoted cp(z).This information helps constraining or disambiguat-ing the meaning of z, and is used to validate propermatching between pairs of objects.We consider two components within cp(z): (a)a representation for the global (?topical?)
contextin which z typically occurs, denoted cpg(z); (b)a representation for the preferences and constraints(?hard?
preferences) on the possible terms that caninstantiate variables within z, denoted cpv(z).
Forexample, cpv(?X produce Y ?
X lay Y ?)
mayspecify that X?s instantiations should be similar to?chicken?
or ?duck?.Contextual Preferences are used when entailmentis assessed between a text t and a hypothesis h, ei-ther directly or by utilizing an entailment-rule r. Ontop of structural matching, we now require that theContextual Preferences of the participants in the in-ference will also match.
When h is directly matchedin t, we require that each component in cp(h) willbe matched with its counterpart in cp(t).
When r isutilized, we additionally require that cp(r) will bematched with both cp(t) and cp(h).
Figure 1 sum-marizes the matching relationships between the CP684Figure 1: The directional matching relationships betweena hypothesis (h), an entailment rule (r) and a text (t) in theContextual Preferences framework.components of h, t and r.Like Textual Entailment inference, ContextualPreferences matching is directional.
When matchingh with t we require that the global context prefer-ences specified by cpg(h) would subsume those in-duced by cpg(t), and that the instantiations of h?svariables in t would adhere to the preferences incpv(h) (since t should entail h, but not necessarilyvice versa).
For example, if the preferred global con-text of a hypothesis is sports, it would match a textthat discusses the more specific topic of basketball.To implement the CP framework, concrete modelsare needed for each component, specifying its repre-sentation, how it is constructed, and an appropriatematching procedure.
Section 3 describes the specificCP models that were implemented in this paper.The CP framework provides a generic view ofcontextual modeling in applied semantic inference.Mapping from a specific application to the genericframework follows the mappings assumed in theTextual Entailment paradigm.
For example, in QAthe hypothesis to be proved corresponds to the affir-mative template derived from the question (e.g.
h:?X invented the PC?
for ?Who invented the PC??
).Thus, cpg(h) can be constructed with respect tothe question?s focus while cpv(h) may be gener-ated from the expected answer type (Moldovan etal., 2000; Harabagiu et al, 2003).
Construction ofhypotheses?
CP for IE is demonstrated in Section 4.3 Contextual Preferences ModelsThis section presents the current models that we im-plemented for the various components of the CPframework.
For each component type we describeits representation, how it is constructed, and a cor-responding unsupervised match score.
Finally, thedifferent component scores are combined to yieldan overall match score, which is used in our exper-iments to rank inference instances by the likelihoodof their validity.
Our goal in this paper is to cover theentire scope of the CP framework by including spe-cific models that were proposed in previous work,where available, and elsewhere propose initial mod-els to complete the CP scope.3.1 Contextual Preferences for Global ContextTo represent the global context of an object z weutilize Latent Semantic Analysis (LSA) (Deerwesteret al, 1990), a well-known method for representingthe contextual-usage of words based on corpus sta-tistics.
We use LSA analysis of the BNC corpus1,in which every term is represented by a normalizedvector of the top 100 SVD dimensions, as describedin (Gliozzo, 2005).To construct cpg(z) we first collect a set of termsthat are representative for the preferred general con-text of z.
Then, the (single) vector which is the sumof the LSA vectors of the representative terms be-comes the representation of cpg(z).
This LSA vec-tor captures the ?average?
typical contexts in whichthe representative terms occur.The set of representative terms for a text t con-sists of all the nouns and verbs in it, representedby their lemma and part of speech.
For a rule r:?LHS ?
RHS?, the representative terms are thewords appearing in LHS and in RHS.
For exam-ple, the representative terms for ?X divorce Y ?
Xmarry Y ?
are {divorce:v, marry:v}.
As mentionedearlier, construction of hypotheses and their contex-tual preferences depends on the application at hand.In our experiments these are defined manually, asdescribed in Section 4, derived from the manual de-finitions of target meanings in the IE data.The score of matching the cpg components of twoobjects, denoted by mg(?, ?
), is the Cosine similarityof their LSA vectors.
Negative values are set to 0.3.2 Contextual Preferences for Variables3.2.1 RepresentationFor comparison with prior work, we follow (Pan-tel et al, 2007) and represent preferences for vari-1http://www.natcorp.ox.ac.uk/685able instantiations using a distributional approach,and in addition incorporate a standard specificationof named-entity types.
Thus, cpv is represented bytwo lists.
The first list, denoted cpv:e, contains ex-amples for valid instantiations of that variable.
Forexample, cpv:e(X kill Y ?
Y die of X) may be[X: {snakebite, disease}, Y : {man, patient}].
Thesecond list, denoted cpv:n, contains the variable?spreferred named-entity types (if any).
For exam-ple, cpv:n(X born in Y ) may be [X: {Person}, Y :{Location}].
We denote cpv:e(z)[j] and cpv:n(z)[j]as the lists for a specific variable j of the object z.For a text t, in which a template p is matched, thepreference cpv:e(t) for each template variable is sim-ply its instantiation in t. For example, when ?X eatY ?
is matched in t: ?Many Americans eat fish reg-ularly?, we construct cpv:e(t) = [X: {Many Ameri-cans}, Y : {fish}].
Similarly, cpv:n(t) for each vari-able is the named-entity type of its instantiation int (if it is a named entity).
We identify entity typesusing the default Lingpipe2 Named-Entity Recog-nizer (NER), which recognizes the types Location,Person and Organization.
In the above example,cpv:n(t)[X] would be {Person}.For a rule r: LHS ?
RHS, we automaticallyadd to cpv:e(r) all the variable instantiations thatwere found common for both LHS and RHS in acorpus (see Section 4), as in (Pantel et al, 2007; Pen-nacchiotti et al, 2007).
To construct cpv:n(r), wecurrently use a simple approach where each individ-ual term in cpv:e(r) is analyzed by the NER system,and its type (if any) is added to cpv:n(r).For a template hypothesis, we currently repre-sent cpv(h) only by its list of preferred named-entitytypes, cpv:n. Similarly to cpg(h), the preferred typesfor each template variable were adapted from thosedefined in our IE data (see Section 4).To allow compatible comparisons with previouswork (see Sections 5 and 6), we utilize in thispaper only cpv:e when matching between cpv(r)and cpv(t), as only this representation was exam-ined in prior work on context-sensitive rule applica-tions.
cpv:n is utilized for context matches involvingcpv(h).
We denote the score of matching two cpvcomponents by mv(?, ?
).2http://www.alias-i.com/lingpipe/3.2.2 Matching cpv:eOur primary matching method is based on repli-cating the best-performing method reported in (Pan-tel et al, 2007), which utilizes the CBC distribu-tional word clustering algorithm (Pantel, 2003).
Inshort, this method extends each cpv:e list with CBCclusters that contain at least one term in the list, scor-ing them according to their ?relevancy?.
The scoreof matching two cpv:e lists, denoted here SCBC(?, ?
),is the score of the highest scoring member that ap-pears in both lists.We applied the final binary match score presentedin (Pantel et al, 2007), denoted here binaryCBC:mv:e(r, t) is 1 if SCBC(r, t) is above a threshold and0 otherwise.
As a more natural ranking method, wealso utilize SCBC directly, denoted rankedCBC,having mv:e(r, t) = SCBC(r, t).In addition, we tried a simpler method that di-rectly compares the terms in two cpv:e lists, uti-lizing the commonly-used term similarity metric of(Lin, 1998a).
This method, denoted LIN , uses thesame raw distributional data as CBC but computesonly pair-wise similarities, without any clusteringphase.
We calculated the scores of the 1000 mostsimilar terms for every term in the Reuters RVC1corpus3.
Then, a directional similarity of term ato term b, s(a, b), is set to be their similarity scoreif a is in b?s 1000 most similar terms and 0 other-wise.
The final score of matching r with t is deter-mined by a nearest-neighbor approach, as the scoreof the most similar pair of terms in the correspond-ing two lists of the same variable: mv:e(r, t) =maxj?vars(r)[maxa?cpv:e(t)[j],b?cpv:e(r)[j][s(a, b)]].3.2.3 Matching cpv:nWe use a simple scoring mechanism for compar-ing between two named-entity types a and b, s(a, b):1 for identical types and 0.8 otherwise.A variable j has a single preferred entity typein cpv:n(t)[j], the type of its instantiation in t.However, it can have several preferred types for h.When matching h with t, j?s match score is thatof its highest scoring type, and the final score isthe product of all variable scores: mv:n(h, t) =?j?vars(h)(maxa?cpv:n(h)[j][s(a, cpv:n(t)[j])]).Variable j may also have several types in r, the3http://about.reuters.com/researchandstandards/corpus/686types of the common arguments in cpv:e(r).
Whenmatching h with r, s(a, cpv:n(t)[j]) is replaced withthe average score for a and each type in cpv:n(r)[j].3.3 Overall Score for a MatchA final score for a given match, denoted allCP, isobtained by the product of all six matching scoresof the various CP components (multiplying by 1if a component score is missing).
The six scoresare the results of matching any of the two compo-nents of h, t and r: mg(h, t), mv(h, t), mg(h, r),mv(h, r), mg(r, t) and mv(r, t) (as specified above,mv(r, t) is based on matching cpv:e while mv(h, r)and mv(h, t) are based on matching cpv:n).
We userankedCBC for calculating mv(r, t).Unlike previous work (e.g.
(Pantel et al, 2007)),we also utilize the prior score of a rule r, whichis provided by the rule-learning algorithm (see nextsection).
We denote by allCP+pr the final matchscore obtained by the product of the allCP scorewith the prior score of the matched rule.4 Experimental SettingsEvaluating the contribution of Contextual Prefer-ences models requires: (a) a sample of test hypothe-ses, and (b) a corresponding corpus that containssentences which entail these hypotheses, where allhypothesis matches (either direct or via rules) are an-notated.
We found that the available event mentionannotations in the ACE 2005 training set4 provide auseful test set that meets these generic criteria, withthe added value of a standard real-world dataset.The ACE annotation includes 33 types of events,for which all event mentions are annotated in thecorpus.
The annotation of each mention includes theinstantiated arguments for the predicates, which rep-resent the participants in the event, as well as generalattributes such as time and place.
ACE guidelinesspecify for each event type its possible arguments,where all arguments are optional.
Each argument isassociated with a semantic role and a list of possiblenamed-entity types.
For instance, an Injure eventmay have the arguments {Agent, Victim, Instrument,Time, Place}, where Victim should be a person.For each event type we manually created a smallset of template hypotheses that correspond to the4http://projects.ldc.upenn.edu/ace/given event predicate, and specified the appropri-ate semantic roles for each variable.
We consid-ered only binary hypotheses, due to the type ofavailable entailment rules (see below).
For In-jure, the set of hypotheses included ?A injure V?and ?injure V in T?
where role(A)={Agent, In-strument}, role(V)={Victim}, and role(T)={Time,Place}.
Thus, correct match of an argument corre-sponds to correct role identification.
The templateswere represented as Minipar (Lin, 1998b) depen-dency parse-trees.The Contextual Preferences for h were con-structed manually: the named-entity types forcpv:n(h) were set by adapting the entity types givenin the guidelines to the types supported by the Ling-pipe NER (described in Section 3.2).
cpg(h) wasgenerated from a short list of nouns and verbs thatwere extracted from the verbal event definition inthe ACE guidelines.
For Injure, this list included{injure:v, injury:n, wound:v}.
This assumes thatwhen writing down an event definition the userwould also specify such representative keywords.Entailment-rules for a given h (rules in whichRHS is equal to h) were learned automatically bythe DIRT algorithm (Lin and Pantel, 2001), whichalso produces a quality score for each rule.
We im-plemented a canonized version of DIRT (Szpektorand Dagan, 2007) on the Reuters corpus parsed byMinipar.
Each rule?s arguments for cpv(r) were alsocollected from this corpus.We assessed the CP framework by its ability tocorrectly rank, for each predicate (event), all thecandidate entailing mentions that are found for itin the test corpus.
Such ranking evaluation is suit-able for unsupervised settings, with a perfect rank-ing placing all correct mentions before any incor-rect ones.
The candidate mentions are found in theparsed test corpus by matching the specified eventhypotheses, either directly or via the given set of en-tailment rules, using a syntactic matcher similar tothe one in (Szpektor and Dagan, 2007).
Finally, thementions are ranked by their match scores, as de-scribed in Section 3.3.
As detailed in the next sec-tion, those candidate mentions which are also an-notated as mentions of the same event in ACE areconsidered correct.The evaluation aims to assess the correctness ofinferring a target semantic meaning, which is de-687noted by a specific predicate.
Therefore, we elim-inated four ACE event types that correspond to mul-tiple distinct predicates.
For instance, the Transfer-Money event refers to both donating and lendingmoney, which are not distinguished by the ACE an-notation.
We also omitted three events with less than10 mentions and two events for which the given setof learned rules could not match any mention.
Wewere left with 24 event types for evaluation, whichamount to 4085 event mentions in the dataset.
Out ofthese, our binary templates can correctly match onlymentions with at least two arguments, which appear2076 times in the dataset.Comparing with previous evaluation methodolo-gies, in (Szpektor et al, 2007; Pantel et al, 2007)proper context matching was evaluated by post-hocjudgment of a sample of rule applications for a sam-ple of rules.
Such annotation needs to be repeatedeach time the set of rules is changed.
In addition,since the corpus annotation is not exhaustive, re-call could not be computed.
By contrast, we use astandard real-world dataset, in which all mentionsare annotated.
This allows immediate comparisonof different rule sets and matching methods, withoutrequiring any additional (post-hoc) annotation.5 Results and AnalysisWe experimented with three rule setups over theACE dataset, in order to measure the contributionof the CP framework.
In the first setup no rules areused, applying only direct matches of template hy-potheses to identify event mentions.
In the other twosetups we also utilized DIRT?s top 50 or 100 rulesfor each hypothesis.A match is considered correct when all matchedarguments are extracted correctly according to theirannotated event roles.
This main measurement is de-noted All.
As an additional measurement, denotedAny, we consider a match as correct if at least oneargument is extracted correctly.Once event matches are extracted, we first mea-sure for each event its Recall, the number of correctmentions identified out of all annotated event men-tions5 and Precision, the number of correct matchesout of all extracted candidate matches.
These figures5For Recall, we ignored mentions with less than two argu-ments, as they cannot be correctly matched by binary templates.quantify the baseline performance of the DIRT ruleset used.
To assess our ranking quality, we measurefor each event the commonly used Average Preci-sion (AP) measure (Voorhees and Harmann, 1998),which is the area under the non-interpolated recall-precision curve, while considering for each setup allcorrect extracted matches as 100% Recall.
Overall,we report Mean Average Precision (MAP), macroaverage Precision and macro average Recall over theACE events.
Tables 1 and 2 summarize the main re-sults of our experiments.
As far as we know, theseare the first published unsupervised results for iden-tifying event arguments in the ACE 2005 dataset.Examining Recall, we see that it increases sub-stantially when rules are applied: by more than100% for the top 50 rules, and by about 150% forthe top 100, showing the benefit of entailment-rulesto covering language variability.
The difference be-tween All and Any results shows that about 65%of the rules that correctly match one argument alsomatch correctly both arguments.We use two baselines for measuring the CP rank-ing contribution: Precision, which corresponds tothe expected MAP of random ranking, and MAPof ranking using the prior rule score provided byDIRT.
Without rules, the baseline All Precision is34.1%, showing that even the manually constructedhypotheses, which correspond directly to the eventpredicate, extract event mentions with limited accu-racy when context is ignored.
When rules are ap-plied, Precision is very low.
But ranking is consider-ably improved using only the prior score (from 1.4%to 22.7% for 50 rules), showing that the prior is aninformative indicator for valid matches.Our main result is that the allCP and allCP+prmethods rank matches statistically significantly bet-ter than the baselines in all setups (according to theWilcoxon double-sided signed-ranks test at the levelof 0.01 (Wilcoxon, 1945)).
In the All setup, rankingis improved by 70% for direct matching (Table 1).When entailment-rules are also utilized, prior-onlyranking is improved by about 35% and 50% whenusing allCP and allCP+pr, respectively (Table 2).Figure 2 presents the average Recall-Precision curveof the ?50 Rules, All?
setup for applying allCP orallCP+pr, compared to prior-only ranking baseline(other setups behave similarly).
The improvementin ranking is evident: the drop in precision is signif-688R P MAP (%)(%) (%) cpv cpg allCPAll 14.0 34.1 46.5 52.2 60.2Any 21.8 66.0 72.2 80.5 84.1Table 1: Recall (R), Precision (P) and Mean Average Pre-cision (MAP) when only matching template hypothesesdirectly.# R P MAP (%)Rules (%) (%) prior allCP allCP+prAll 50 29.6 1.4 22.7 30.6 34.1100 34.9 0.7 20.5 26.3 30.2Any 50 46.5 3.5 41.2 43.7 48.6100 52.9 1.8 35.5 35.1 40.8Table 2: Recall (R), Precision (P) and Mean Average Pre-cision (MAP) when also using rules for matching.icantly slower when CP is used.
The behavior of CPwith and without the prior is largely the same up to50% Recall, but later on our implemented CP mod-els are noisier and should be combined with the priorrule score.Templates are incorrectly matched for several rea-sons.
First, there are context mismatches which arenot scored sufficiently low by our models.
Anothermain cause is incorrect learned rules in which LHSand RHS are topically related, e.g.
?X convict Y ?X arrest Y ?, or rules that are used in the wrong en-tailment direction, e.g.
?X marry Y ?X divorce Y ?
(DIRT does not learn rule direction).
As such rulesdo correspond to plausible contexts of the hypothe-sis, their matches obtain relatively high CP scores.In addition, some incorrect matches are caused byour syntactic matcher, which currently does not han-dle certain phenomena such as co-reference, modal-ity or negation, and due to Minipar parse errors.5.1 Component AnalysisTable 3 displays the contribution of different CPcomponents to ranking, when adding only that com-ponent?s match score to the baselines, and under ab-lation tests, when using all CP component scores ex-cept the tested component, with or without the prior.As it turns out, matching h with t (i.e.
cp(h, t),which combines cpg(h, t) and cpv(h, t)) is most use-ful.
With our current models, using only cp(h, t)along with the prior, while ignoring cp(r), achieves50 Rules  -  All01020304050607080901000 10 20 30 40 50 60 70 80 90 100Relative RecallPrecisionbaseline CP CP + priorFigure 2: Recall-Precision curves for ranking using: (a)only the prior (baseline); (b) allCP; (c) allCP+pr.the highest score in the table.
The strong impact ofmatching h and t?s preferences is also evident in Ta-ble 1, where ranking based on either cpg or cpv sub-stantially improves precision, while their combina-tion provides the best ranking.
These results indicatethat the two CP components capture complementaryinformation and both are needed to assess the cor-rectness of a match.When ignoring the prior rule score, cp(r, t) is themajor contributor over the baseline Precision.
Forcpv(r, t), this is in synch with the result in (Pantelet al, 2007), which is based on this single modelwithout utilizing prior rule scores.
On the otherhand, cpv(r, t) does not improve the ranking whenthe prior is used, suggesting that this contextualmodel for the rule?s variables is not stronger than thecontext-insensitive prior rule score.
Furthermore,relative to this cpv(r, t) model from (Pantel et al,2007), our combined allCP model, with or withoutthe prior (first row of Table 2), obtains statisticallysignificantly better ranking (at the level of 0.01).Comparing between the algorithms for match-ing cpv:e (Section 3.2.2) we found that whilerankedCBC is statistically significantly better thanbinaryCBC, rankedCBC and LIN generallyachieve the same results.
When considering thetradeoffs between the two, LIN is based on a muchsimpler learning algorithm while CBC?s output ismore compact and allows faster CP matches.689Addition To Ablation FromP prior allCP allCP+prBaseline 1.4 22.7 30.6 34.1cpg(h, t) ?10.4 ?35.4 32.4 33.7cpv(h, t) ?11.0 29.9 27.6 32.9cp(h, t) ?8.9 ?37.5 28.6 30.0cpg(r, t) ?4.2 ?30.6 32.5 35.4cpv(r, t) ?21.7 21.9 ?12.9 33.6cp(r, t) ?26.0 ?29.6 ?17.9 36.8cpg(h, r) ?8.1 22.4 31.9 34.3cpv(h, r) ?10.7 22.7 ?27.9 34.4cp(h, r) ?16.5 22.4 ?29.2 34.4cpg(h, r, t) ?7.7 ?30.2 ?27.5 ?29.2cpv(h, r, t) ?27.5 29.2 ?7.7 30.2?
Indicates statistically significant changes compared to the baseline,according to the Wilcoxon test at the level of 0.01.Table 3: MAP(%), under the ?50 rules, All?
setup, whenadding component match scores to Precision (P) or prior-only MAP baselines, and when ranking with allCP orallCP+pr methods but ignoring that component scores.Currently, some models do not improve the re-sults when the prior is used.
Yet, we would like tofurther weaken the dependency on the prior score,since it is biased towards frequent contexts.
Weaim to properly identify also infrequent contexts (ormeanings) at inference time, which may be achievedby better CP models.
More generally, when usedon top of all other components, some of the mod-els slightly degrade performance, as can be seen bythose figures in the ablation tests which are higherthan the corresponding baseline.
However, due totheir different roles, each of the matching compo-nents might capture some unique preferences.
Forexample, cp(h, r) should be useful to filter out rulesthat don?t match the intended meaning of the givenh.
Overall, this suggests that future research for bet-ter models should aim to obtain a marginal improve-ment by each component.6 Related WorkContext sensitive inference was mainly investigatedin an application-dependent manner.
For exam-ple, (Harabagiu et al, 2003) describe techniques foridentifying the question focus and the answer type inQA.
(Patwardhan and Riloff, 2007) propose a super-vised approach for IE, in which relevant text regionsfor a target relation are identified prior to applyingextraction rules.Recently, the need for context-aware inferencewas raised (Szpektor et al, 2007).
(Pantel et al,2007) propose to learn the preferred instantiations ofrule variables, termed Inferential Selectional Prefer-ences (ISP).
Their clustering-based model is the onewe implemented for mv(r, t).
A similar approachis taken in (Pennacchiotti et al, 2007), where LSAsimilarity is used to compare between the preferredvariable instantiations for a rule and their instanti-ations in the matched text.
(Downey et al, 2007)use HMM-based similarity for the same purpose.All these methods are analogous to matching cpv(r)with cpv(t) in the CP framework.
(Dagan et al, 2006; Connor and Roth, 2007) pro-posed generic approaches for identifying valid appli-cations of lexical rules by classifying the surround-ing global context of a word as valid or not for thatrule.
These approaches are analogous to matchingcpg(r) with cpg(t) in our framework.7 ConclusionsWe presented the Contextual Preferences (CP)framework for assessing the validity of inferencesin context.
CP enriches the representation of tex-tual objects with typical contextual information thatconstrains or disambiguates their meaning, and pro-vides matching functions that compare the prefer-ences of objects involved in the inference.
Experi-ments with our implemented CP models, over real-world IE data, show significant improvements rela-tive to baselines and some previous work.In future research we plan to investigate improvedmodels for representing and matching CP, and to ex-tend the experiments to additional applied datasets.We also plan to apply the framework to lexical infer-ence rules, for which it seems directly applicable.AcknowledgementsThe authors would like to thank Alfio MassimilianoGliozzo for valuable discussions.
This work waspartially supported by ISF grant 1095/05, the ISTProgramme of the European Community under thePASCAL Network of Excellence IST-2002-506778,the NEGEV project (www.negev-initiative.org) andthe FBK-irst/Bar-Ilan University collaboration.690ReferencesRoy Bar-Haim, Ido Dagan, Iddo Greental, and EyalShnarch.
2007.
Semantic inference at the lexical-syntactic level.
In Proceedings of AAAI.Michael Connor and Dan Roth.
2007.
Context sensitiveparaphrasing with a global unsupervised classifier.
InProceedings of the European Conference on MachineLearning (ECML).Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-morshtein, and Carlo Strapparava.
2006.
Direct wordsense matching for lexical substitution.
In Proceed-ings of the 21st International Conference on Compu-tational Linguistics and 44th Annual Meeting of ACL.Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-yakanok, Dan Roth, and Mark Sammons.
2005.
Aninference model for semantic entailment in natural lan-guage.
In Proceedings of the National Conference onArtificial Intelligence (AAAI).Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society of Information Science,41(6):391?407.Doug Downey, Stefan Schoenmackers, and Oren Etzioni.2007.
Sparse information extraction: Unsupervisedlanguage models to the rescue.
In Proceedings of the45th Annual Meeting of ACL.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, andBill Dolan.
2007.
The third pascal recognizing tex-tual entailment challenge.
In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Para-phrasing.Alfio Massimiliano Gliozzo.
2005.
Semantic Domainsin Computational Linguistics.
Ph.D. thesis.
Advisor-Carlo Strapparava.Sanda M. Harabagiu, Steven J. Maiorano, and Marius A.Pas?ca.
2003.
Open-domain textual question answer-ing techniques.
Nat.
Lang.
Eng., 9(3):231?267.Dekang Lin and Patrick Pantel.
2001.
Discovery of in-ference rules for question answering.
In Natural Lan-guage Engineering, volume 7(4), pages 343?360.Dekang Lin.
1998a.
Automatic retrieval and clusteringof similar words.
In Proceedings of COLING-ACL.Dekang Lin.
1998b.
Dependency-based evaluation ofminipar.
In Proceedings of the Workshop on Evalua-tion of Parsing Systems at LREC.Dan Moldovan, Sanda Harabagiu, Marius Pasca, RadaMihalcea, Roxana Girju, Richard Goodrum, andVasile Rus.
2000.
The structure and performance ofan open-domain question answering system.
In Pro-ceedings of the 38th Annual Meeting of ACL.Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,Timothy Chklovski, and Eduard Hovy.
2007.
ISP:Learning inferential selectional preferences.
In Hu-man Language Technologies 2007: The Conference ofNAACL; Proceedings of the Main Conference.Patrick Andre Pantel.
2003.
Clustering by committee.Ph.D.
thesis.
Advisor-Dekang Lin.Siddharth Patwardhan and Ellen Riloff.
2007.
Effec-tive information extraction with semantic affinity pat-terns and relevant regions.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL).Marco Pennacchiotti, Roberto Basili, Diego De Cao, andPaolo Marocco.
2007.
Learning selectional prefer-ences for entailment or paraphrasing rules.
In Pro-ceedings of RANLP.Deepak Ravichandran and Eduard Hovy.
2002.
Learningsurface text patterns for a question answering system.In Proceedings of the 40th Annual Meeting of ACL.Lorenza Romano, Milen Kouylekov, Idan Szpektor, IdoDagan, and Alberto Lavelli.
2006.
Investigating ageneric paraphrase-based approach for relation extrac-tion.
In Proceedings of the 11th Conference of theEACL.Satoshi Sekine.
2005.
Automatic paraphrase discoverybased on context and keywords between ne pairs.
InProceedings of IWP.Yusuke Shinyama, Satoshi Sekine, Sudo Kiyoshi, andRalph Grishman.
2002.
Automatic paraphrase acqui-sition from news articles.
In Proceedings of HumanLanguage Technology Conference.Idan Szpektor and Ido Dagan.
2007.
Learning canonicalforms of entailment rules.
In Proceedings of RANLP.Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-tura Coppola.
2004.
Scaling web-based acquisition ofentailment relations.
In Proceedings of EMNLP 2004,pages 41?48, Barcelona, Spain.Idan Szpektor, Eyal Shnarch, and Ido Dagan.
2007.Instance-based evaluation of entailment rule acquisi-tion.
In Proceedings of the 45th Annual Meeting ofACL.Ellen M. Voorhees and Donna Harmann.
1998.Overview of the seventh text retrieval conference(trec?7).
In The Seventh Text Retrieval Conference.Frank Wilcoxon.
1945.
Individual comparisons by rank-ing methods.
Biometrics Bulletin, 1(6):80?83.691
