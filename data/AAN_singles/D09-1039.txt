Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 371?380,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPAccuracy-Based Scoring for DOT: Towards Direct Error Minimization forData-Oriented TranslationDaniel GalronCIMSNew York Universitygalron@cs.nyu.eduSergio Penkale, Andy WayCNGLDublin City University{spenkale,away}@computing.dcu.ieI.
Dan MelamedAT&T Shannon Laboratory{lastname}@research.att.comAbstractIn this work we present a novel techniqueto rescore fragments in the Data-OrientedTranslation model based on their contri-bution to translation accuracy.
We de-scribe three new rescoring methods, andpresent the initial results of a pilot experi-ment on a small subset of the Europarl cor-pus.
This work is a proof-of-concept, andis the first step in directly optimizing trans-lation decisions solely on the hypothesizedaccuracy of potential translations resultingfrom those decisions.1 IntroductionThe Data-Oriented Translation (DOT) (Poutsma,2000) model is a tree-structured translation model,in which linked subtree fragments extracted froma parsed bitext are composed to cover a source-language sentence to be translated.
Each linkedfragment pair consists of a source-language sideand a target-language side, similar to (Wu, 1997).Translating a new sentence involves composingthe linked fragments into derivations so that anew source-language sentence is covered by thesource tree fragments of the linked pairs, wherethe yields of the target-side derivations are the can-didate translations.
Derivations are scored accord-ing to their likelihood, and the translation is se-lected from the derivation pair with the highestscore.
However, we have no reason to believe thatmaximizing likelihood is the best way to maxi-mize translation accuracy ?
likelihood and accu-racy do not necessarily correlate well.We can frame the problem as a search problem,where we are searching a space of derivations forthe one that yields the highest scoring translation.By putting weights on the derivations in the searchspace, we wish to point the decoder in the direc-tion of the optimal translation.
Since we wantthe decoder to find the translation with the high-est evaluation score, we would want to score thederivations with weights that correlate well withthe particular evaluation measure in mind.Much of the work in the MT literature hasfocused on the scoring of translation decisionsmade.
(Yamada and Knight, 2001) follow (Brownet al, 1993) in using the noisy channel model,by decomposing the translation decisions mod-eled by the translation model into different types,and inducing probability distributions via max-imum likelihood estimation over each decisiontype.
This model is then decoded as describedin (Yamada and Knight, 2002).
This type of ap-proach is also followed in (Galley et al, 2006).There has been some previous work onaccuracy-driven training techniques for SMT, suchas MERT (Och, 2003) and the Simplex ArmijoDownhill method (Zhao and Chen, 2009), whichtune the parameters in a linear combination of var-ious phrase scores according to a held-out tun-ing set.
While this does tune the relative weightsof the scores to maximize the accuracy of candi-dates in the tuning set, the scores themselves in thelinear combination are not necessarily correlatedwith the accuracy of the translation.
Tillmann andZhang (2006) present a procedure to directly opti-mize the global scoring function used by a phrase-based decoder on the accuracy of the translations.Similarly to MERT, Tillmann and Zhang estimatethe parameters of a weight vector on a linear com-bination of (binary) features using a global objec-tive function correlated with BLEU (Papineni etal., 2002).In this work, we prototype some methods formoving directly towards incorporating a measureof the translation quality of each fragment used,bringing DOT more into the mainstream of cur-rent SMT research.
In Section 2 we describeprobability-based DOT fragment scoring.
In Sec-tion 3 we describe our rescoring setup and the371(a)SNP VPVlikesNPSNP VPVpla?
?tPPPa`NP(b)NPJohnNPJohn(c)SNPJohnVPVlikesNPSNP VPVpla?
?tPPPa`NPJohn(d)NPMaryNPMaryFigure 1: Example DOT Fragments.three rescoring methods.
In Section 4, we describeour experiments.
In Section 5 we compare theresults of rescoring the fragments with the threemethods.
In Section 6 we discuss some of thedecisions that are affected by our rescoring meth-ods.
Finally, we discuss the next steps in trainingthe DOT system by optimizing over a translationaccuracy-based objective function in Section 7.2 DOT ScoringAs described in previous work (Poutsma, 2000;Hearne and Way, 2003), DOT scores translationsaccording to the probabilities of the derivations,which are in turn computed from the relative fre-quencies of linked tree fragments in a parallel tree-bank.
Linked fragment pairs are conditionally in-dependent, so the score of a derivation is the prod-uct of the probabilities of all the linked fragmentsused.
To find the probability of a translation,DOT marginalizes over the scores of all deriva-tions yielding the translation.From a parallel treebank aligned at the sub-sentential level, we extract all possible linked frag-ment pairs by first selecting all linked pairs ofnodes in the treebank to be the roots of a new sub-tree pair, and then selecting a (possibly empty) setof linked node pairs that are descendants of thenewly selected fragment roots and deleting all sub-tree pairs dominated by these nodes.
Leaves offragments can either be terminals, or non-terminalfrontier nodes where we can compose other frag-ments (c.f.
(Eisner, 2003)).
We give example DOTfragment pairs in Figure 1.Given two subtree pairs ?s1, t1?
and ?s2, t2?,we can compose them using the DOT composi-tion operator ?
if the leftmost non-terminal fron-tier node of s1is equal to the root node of s2,and the leftmost non-terminal frontier node of s1?slinked counterpart in t1is equal to the root nodeof t2.
The resulting tree pair consists of a copyof s1where s2has been inserted at the leftmostfrontier node, and a copy of t1where t2has beeninserted at the node linked to s1?s leftmost frontiernode (Hearne and Way, 2003).In Figure 1, fragment pair (a) is a fragment withtwo open substitution sites.
If we compose thisfragment pair with fragment pair (b), the sourceside composition must take place on the leftmostnon-terminal frontier node (the leftmost NP).
Onthe target side we compose on the frontier linkedto the leftmost source side non-terminal frontier.The result is fragment pair (c).
If we now com-pose the resulting fragment pair with fragment pair(d), we obtain a fragment pair with no open sub-stitution sites whose source-side yield is John likesMary and whose target-side yield is Mary pla?
?t a`John.
Note that there are two different derivationsusing the fragment pairs in Figure 1 that result inthe same fragment pair, namely (a) ?
(b) ?
(d), and(c) ?
(d).For a given linked fragment pair ?ds, dt?, theprobability assigned to it isP (?ds, dt?)
=|?ds, dt?|?r(us)=r(ds)?r(ut)=r(dt)|?us, ut?|(1)where |?ds, dt?| is the number of times the frag-ment pair ?ds, dt?
is found in the bitext, and r(d)is the root nonterminal of d. Essentially, the prob-ability assigned to the fragment pair is the relativefrequency of the fragment pair to the pair of non-terminals that root the fragments.Then, with the assumption that DOT fragmentsare conditionally independent, the probability of aderivation isP (d) = P (?ds, dt?1?
.
.
.
?
?ds, dt?N)=?iP (?ds, dt?i) (2)In the original DOT formulation, DOT disam-biguated translations according to their probabil-ities.
Since a translation can have many possiblederivations, to obtain the probability of a transla-tion it is necessary to marginalize over the distinctderivations yielding a translation.
The probabil-ity of a translation wtof a source sentence ws, is372given by (3):P (ws, wt) =?d?DP (d?ws,wt?)
(3)and the translation is chosen so as to maximize (4):w?t= argmaxwtP (ws, wt) (4)Hearne and Way (2006) examined alternative dis-ambiguation strategies.
They found that ratherthan disambiguating on the translation probability,the translation quality would improve by disam-biguating on the derivation probability, as in (5):w?t= argmaxdP (d) (5)Our analysis suggest that this is because manyderivations with very low probabilities generatethe same, poor translation.
When applying Equa-tion (3) to marginalize over those derivations, theresulting score is higher for the poor translationthan a better translation with fewer derivations butwhere the derivations had higher likelihood.Using the DOT model directly is difficult ?the number of fragments extracted from a paral-lel treebank is exponential in the size of the tree-bank.
Therefore we use the Goodman reductionof DOT (Hearne, 2005) to create an isomorphicPCFG representation of the DOT model that is lin-ear in the size of the treebank.
The idea behind theGoodman reduction is that rather than storing frag-ments in the grammar and translating via compo-sition, we simultaneously build up the fragmentsusing the PCFG reduction and compose them to-gether.
To perform the reduction, we first relabelthe two linked nodes (X, Y) with the new labelX=Y.
We then label each node in the parallel tree-bank with a unique Goodman index.
Each binary-branching node and its two children can be inter-nal or root/frontier.
We add rules to the grammarreflecting the role that each node can take, keepingunaligned nodes as fragment-internal nodes.
So inthe case where a node and both of its children arealigned, we commit 8 rules into the grammar, asfollows:LHS ?
RHS1 RHS2 LHS+a ?
RHS1 RHS2LHS ?
RHS1+b RHS2 LHS+a ?
RHS1+b RHS2LHS ?
RHS1 RHS2+c LHS+a ?
RHS1 RHS2+cLHS ?
RHS1+b RHS+c LHS+a ?
RHS1+b RHS2+cA category label which ends in a ?+?
symbol fol-lowed by a Goodman index is fragment-internaland all other nodes are either fragment roots orS=S1N=N3JohnVP2V4likesN=N5MaryS=S1N=N4MaryVP2V5pla?
?tPP3P6a`N=N7JohnSource PCFG Target PCFGS=S?
N=N VP+2 0.5 S=S?
N=N VP+2 0.5S=S?
N=N+3 VP+2 0.5 S=S?
N=N+4 VP+2 0.5S=S+1?
N=N VP+2 0.5 S=S+1?
N=N VP+2 0.5S=S+1?
N=N+3 VP+2 0.5 S=S+1?
N=N+4 VP+2 0.5N=N?
John 0.5 N=N?Mary 0.5N=N+3?
John 1 N=N+4?Mary 1VP+2?
V+4 N=N 0.5 VP+2?
V+5 PP+3 1VP+2?
V+4 N=N+5 0.5 V+5?
pla?
?t 1V+4?
likes 1 PP+3?
P+6 N=N 0.5N=N?Mary 0.5 PP+3?
P+6 N=N+7 0.5N=N+5?Mary 1 P+6?
a` 1N=N?
John 0.5N=N+7?
John 1Figure 2: A parallel tree and its corresponding Goodman re-duction.frontier nodes.
A fragment pair, then, is a pair ofsubtrees in which the root does not have an index,all internal nodes have indices, and all the leavesare either terminals or un-indexed nodes.
We givean example Goodman reduction in Figure 2.While we store the source grammar and the tar-get grammar separately, we also keep track of thecorrespondence between source and target Good-man indices and can easily identify the alignmentsaccording to the Goodman indices.
Probabilitiesfor the PCFG rules are computed monolinguallyas in the standard Goodman reduction for DOP(Goodman, 1996).
In decoding with the Goodmanreduction, we first find the n-best parses on thesource side, and for each source fragment, we con-struct the k-best fragments on the target side.
Wefinally compute the bilingual derivation probabil-ities by multiplying the source and target deriva-tion probabilities by the target fragment relativefrequencies conditioned on the source fragment.There are a few problems with a likelihood-based scoring scheme.
First, it is not clear thatif a fragment is more likely to be seen in trainingdata then it is more likely to be used in a correcttranslation of an unseen sentence.
In our analysisof the candidate translations of the DOT system,we observed that frequently, the highest-likelihoodcandidate translation output by the system was notthe highest-accuracy candidate inferred.
An addi-tional problem is that, as described in (Johnson,2002), the relative frequency estimator for DOP373(and by extension, DOT) is known to be biasedand inconsistent.3 Accuracy-Based Fragment ScoringIn our work, we wish to incorporate a measureof fragment accuracy into the scoring.
To do so,we reformulate the scoring of DOT as log-linearrather than probabilistic, in order to incorporatenon-likelihood features into the derivation scores.For all tree fragment pairs ?ds, dt?, letl(?ds, dt?)
= log(p(?ds, dt?))
(6)The general form of a rescored tree fragment willbes(?ds, dt?)
= ?0l(?ds, dt?)
+k?i=1?ifi(?ds, dt?
)(7)where each ?iis the weight of that term in the fi-nal score, and each fi(d) is a feature.
In this work,we only consider f1(d), an accuracy-based score,although in future work we will consider a widevariety of features in the scoring function, includ-ing combinations of the different scoring schemesdescribed below, binary lexical features, binarysource-side syntactic features, and local target sidefeatures.
The score of a derivation is now given by(8):s(d) = s(?ds, dt?1?
.
.
.
?
?ds, dt?N)=?is(?ds, dt?i) (8)In order to disambiguate between candidatetranslations, we follow (Hearne and Way, 2006)by using Equation (5).3.1 Structured Fragment RescoringIn all our approaches, we rescore fragments ac-cording to their contribution to the accuracy ofa translation.
We would like to give fragmentsthat contribute to good translations relatively highscores, and give fragments that contribute to badtranslations relatively low scores, so that duringdecoding fragments that are known to contribute togood translations would be chosen over those thatare known to contribute to bad translations.
Fur-thermore, we would like to score each fragment ina derivation independently, since bad translationsmay contain good fragments, and vice-versa.In practice, it is infeasible to rescore only thosefragments seen during the rescoring process, dueto the Goodman reduction for DOT.
If we were toproperly rescore each fragment, a new rule wouldneed to be added to the grammar for each rule ap-pearing in the fragment.
Since the number of frag-ments is exponential, this would lead to a substan-tial increase in grammar size.
Instead, we rescorethe individual rules in the fragments, by evenly di-viding the total amount of scoring mass among therules of the particular fragment, and then assigningthem the average of the rule scores over all frag-ments in which they appear.
That is for each ruler in a fragment f consisting of cf(r) rules withscore ?
(f), the score of the rule is given as:s(r) =?f :r?f?
(f)/cf(r)|f |(11)This has the further advantage that we are al-lowing fragments that were unseen during tuningto be rescored according to previously seen frag-ment substructures.To implement this scheme, we select a set of or-acle translations for each sentence in the tuningdata by evaluating all the candidate translationsagainst the gold standard translation using the F-score (Turian et al, 2003), and selecting thosewith the highest F1-measure, with exponent 1.
Weuse GTM, rather than BLEU, because BLEU isnot known to work well on a per-sentence level(Lavie et al, 2004) as needed for oracle selection.We then compare all the target-side fragments in-ferred in the translation process for each candidatetranslation against the fragments that yielded theoracles.
There are two relevant parts of the frag-ments ?
the internal yields (i.e.
the terminal leavesof the fragment) and the substitution sites (i.e.
thefrontiers where other fragments attach).
We scorethe fragments rooted at the substitution sites sepa-rately from the parent fragment.
We can uniquelyidentify the set of fragments that can be rooted atsubstitution sites by determining the span of thelinked source-side derivation.To compare two fragments, we define an editdistance between them.
For a given fragment d,let r(d) be the root of the fragment, let r(d) ?rhs1 be the left subtree of r(d), and let r(d) ?rhs2 be the right subtree.
The difference betweena candidate fragment dcand an oracle fragmentdgsis given by the equations in Table 1.These equations define a minimum edit dis-tance between two fragment trees, allowing sub-fragment order inversion, insertion, and deletion374?
(dc, dgs) =(0 if dc= dgs1 if dc6= dgsBase case: dcand dgsare unary subtrees or substitution sites (9)?
(dc, dgs) = min8>>>><>>>>:?(dc?
rhs1, dgs?
rhs1) + ?(dc?
rhs2, dgs?
rhs2),?(dc?
rhs2, dgs?
rhs1) + ?(dc?
rhs1, dgs?
rhs2) + 1,?
(dc, dgs?
rhs1) + |y(dgs?
rhs2)|,?
(dc, dgs?
rhs2) + |y(dgs?
rhs1)|,?(dc?
rhs1, dgs) + |y(dc?
rhs2)|,?(dc?
rhs2, dgs) + |y(dc?
rhs1)|(10)Table 1: The recursive relation defining the fragment difference between two fragments.
(a) ABbCc(b) ACcBb(c) DABbFfEeFigure 3: Comparing trees (a) and (b) with our distance met-ric yields a value of 1.
The difference between trees (a) and(c) is 2, and for trees (b) and (c) the distance is 3.as edit operations.
For example, the only dif-ference between trees (a) and (b) in Figure 3 isthat their children have been inverted.
To com-pare these trees using our distance metric, we firstcompute the first argument of the min function inEquation (10), directly comparing the structure ofeach immediate subtree.
We then compute the sec-ond argument, obtaining the cost of performing aninversion, and finally compute the remaining argu-ments, assessing the cost of allowing each tree tobe a direct subtree of the other.
The result of thiscomputation is 1, representing the inversion oper-ation required to transform tree (a) into tree (b).If we compare trees (a) and (c) in Figure 3, weobtain a value of 2, given that the minimum opera-tions required to transform tree (a) into tree (c) areinserting an additional subtree at the top level andthen substituting the subtree rooted by C for thesubtree rooted by F. If we compare tree (b) withtree (c) then the distance is 3, since we are nowrequired to also replace the subtree rooted by C bythe one rooted by B.Since it is not efficient to compute the differ-ences directly, we utilize common substructuresand derive a dynamic programming implementa-tion of the recursion.
We compare each fragmentagainst the set of oracle fragments for the samesource span, and select the lowest cost as the score,assigning the candidate the negative difference be-tween it and the oracle fragment it is most similarto, as in (12):f(?ds, dt?)
= max?dos,dot??Do:dos=ds??
(dt, dot) (12)In practice, given the Goodman reduction forDOT, we divide the fragment score by the numberof rules in the fragment, and assign the average ofthose scores for each rule instance across all frag-ments rescored.3.2 Normalized Structured FragmentRescoringIn the structured fragment rescoring scheme, thescores that the fragments are assigned are the un-normalized edit distances between the two frag-ments.
It may be better to normalize the fragmentscores, rather than using the minimum number oftree transformations to convert one fragment intothe other.
We would expect that when compar-ing larger fragments, on average there would bemore transformations needed to change one intothe other than when comparing small fragments.However in the previous scheme, small fragmentswould have higher scores than large fragments,since fewer differences would be observed.
Thenormalized score is given in (13):f(?ds, dt?)
= max?dos,dot?
?Do:dos=dslog(1 ?
?
(dt, dot)/max(|dt|, |dot|))(13)Essentially, we are normalizing the edit distanceby the maximum edit distance possible, namelythe size of the largest fragment of the two beingcompared.3.3 Fragment Surface RescoringThe disadvantage of the minimum tree fragmentedit approach is that it explicitly takes the internal375syntactic structure of the fragment into account.In comparing two fragments, they may have thesame (or very similar) surface yields, but differ-ent internal structures.
The previous approachwould penalize the candidate fragment, even if itsyield is quite close to the oracle.
In this rescor-ing method, we extract the leaves of the candi-date and oracle fragments, representing the substi-tution sites by the source span which their frag-ments cover.
We then compare them using theDamerau-Levenshtein distance ?dl(dc, dgs) (Dam-erau, 1964) between the two fragment yields, andscore them as in (14):f(?ds, dt?)
= max?dos,dot??Do:dos=ds?
?dl(dt, dot) (14)In Equation (14) we are selecting the maximalscore for ?ds, dt?
from its comparison to all thepossible corresponding oracle fragments.
In thisway, we are choosing to score ?ds, dt?
against theoracle fragment it is closest to.4 ExperimentsFor our pilot experiments, we tested all the rescor-ing methods in the previous section on Spanish-to-English translation against the relative-frequencybaseline.
We randomly selected 10,000 sentencesfrom the Europarl corpus (Koehn, 2005), andparsed and aligned the bitext as described in (Tins-ley et al, 2009).
From the parallel treebank, weextracted a Goodman reduction DOT grammar, asdescribed in (Hearne, 2005), although on an orderof magnitude greater amount of training data.
Un-like (Bod, 2007), we did not use the unsupervisedversion of DOT, and did not attempt to scale upour amount of training data to his levels, althoughin ongoing work we are optimizing our system tobe able to handle that amount of training data.
Toperform the rescoring, we randomly chose an ad-ditional 30K sentence pairs from the Spanish-to-English bitext.
We rescored the grammar by trans-lating the source side of the 10K training sentencepairs and 10K of the additional sentences, and us-ing the methods in Section 3 to score the frag-ments derived in the translation process.
We thenperformed the same experiment translating the full40K-sentence set.
Rules in the grammar that werenot used during tuning were rescored using a de-fault score defined to be the median of all scoresobserved.Our system performs translation by first obtain-ing the n-best parses for the source sentences andBLEU NIST F-SCOREBaseline 8.78 3.582 38.212-8 4-6 5-5 6-4 8-2BLEU SFR 10.30 10.31 10.32 10.27 10.08NSFR 8.31 9.37 9.53 9.66 9.90FSR 10.19 10.25 10.18 10.19 9.93NIST SFR 3.792 3.805 3.808 3.800 3.781NSFR 3.431 3.638 3.661 3.693 3.722FSR 3.784 3.799 3.792 3.795 3.764F-SCORE SFR 40.92 40.82 40.86 40.84 40.78NSFR 37.53 39.50 39.93 40.38 40.78FSR 40.83 40.85 40.87 40.91 40.67Table 2: Results on test set.
Rescoring on 20K sentences.SFR stands for Structured Fragment Rescoring, NSFR forNormalized SFR and FSR for Fragment Surface Rescoring.system-i-j represents the corresponding system with ?0= iand ?1= j. Underlined results are statistically significantlybetter than the baseline at p = 0.01.BLEU NIST F-SCOREBaseline 8.78 3.582 38.212-8 4-6 5-5 6-4 8-2BLEU SFR 10.59 10.58 10.41 10.38 10.08NSFR 8.61 9.71 9.90 9.96 9.93FSR 10.49 10.48 10.35 10.38 10.06NIST SFR 3.841 3.835 3.810 3.807 3.785NSFR 3.515 3.694 3.713 3.734 3.727FSR 3.834 3.833 3.820 3.816 3.784F-SCORE SFR 41.12 40.99 40.86 40.88 40.75NSFR 38.16 40.39 40.69 40.90 40.75FSR 41.03 41.02 41.01 40.98 40.72Table 3: Results on test set.
Rescoring on 40K sentences.
Un-derlined are statistically significantly better than the baselineat p = 0.01.then computing the k-best bilingual derivations foreach source parse.
In our experiments we usedbeams of n = 10, 000 and k = 5.
We also ex-perimented with different values of ?0and ?1inEquation (7).
We set these parameters manually,although in future work we will automatically tunethem, perhaps using a MERT-like algorithm.We tested our rescored grammars on a set of2,000 randomly chosen Europarl sentences, andused a set of 200 randomly chosen sentences asa development test set.
15 ResultsTranslation quality results can be found in Tables2 and 3.
In these tables, columns labeled i-j in-dicate that the corresponding system was trainedusing parameters ?0= i and ?1= j in Equa-tion 7.
Statistical significance tests for NIST andBLEU were performed using Bootstrap Resam-pling (Koehn, 2004).1All sentences, including the ones used for training, werelimited to a length of at most 20 words.376BLEU NIST F-SCOREBaseline 10.82 3.493 42.312-8 4-6 5-5 6-4 8-2BLEU SFR 11.34 12.12 11.94 11.97 11.78NSFR 9.68 10.99 11.38 11.63 11.30FSR 11.40 11.49 11.72 11.91 11.72NIST SFR 3.653 3.727 3.723 3.708 3.694NSFR 3.376 3.530 3.554 3.616 3.572FSR 3.655 3.675 3.698 3.701 3.675F-SCORE SFR 44.84 45.47 45.36 45.33 45.08NSFR 41.44 43.38 44.18 44.79 44.26FSR 44.68 44.91 45.15 45.19 44.82Table 4: Results on development test set.
Rescoring on 40Ksentences.As Table 2 indicates, all three rescoring meth-ods significantly outperform the relative frequencybaseline.
The unnormalized structured fragmentrescoring method performed the best, with thelargest improvement of 1.5 BLEU points, a 17.5%relative improvement.
We note that the BLEUscores for both the baseline and the experimentsare low.
This is to be expected, because the gram-mar is extracted from a very small bitext espe-cially when the heterogeneity of the Europarl cor-pus is considered.
In our analysis, only 32.5 per-cent of the test sentences had a complete source-side parse, meaning that a lot of structural infor-mation is lost contributing to arbitrary target-sideordering.
In these experiments we did not use anadditional language model.
DOT (and many othersyntax-based SMT systems) essentially have thetarget language model encoded within the trans-lation model, since the inferences derived dur-ing translations link source structures to targetstructures, so in principle, no additional languagemodel should be necessary.
Furthermore, we onlyevaluate against a single reference, which alsocontributes to the lowering of absolute scores.
Toprovide a sanity check against a state-of-the-artsystem, we trained the Moses phrase-based MTsystem (Koehn et al, 2007) using our trainingcorpus, using no language model and using uni-form feature weights, to provide a fair comparisonagainst our baseline.
We used this system to de-code our development test set, and as a result weobtained a BLEU score of 10.72, which is compa-rable to the score obtained by our baseline on thesame set.When we scale up to tuning on 40,000 sen-tences we see an improvement in BLEU scores aswell, as shown in Table 3.
When tuning on 40Ksentences, we observe an increase of 1.81 BLEUpoints on the best-performing system, which is a20.6% improvement over the baseline.
We notethat rescoring on 20K sentences rescores approxi-mately 275,000 rules out of 655,000 in the gram-mar, whereas rescoring on 40K sentences rescoresapproximately 280,000.To analyze the benefits of the rescored gram-mar, we set aside a separate development set thatwe decoded with the grammar trained on 40K sen-tences.
The results are presented in Table 4.
Theanalysis is presented in Section 6.Interestingly, there is a large difference betweenthe normalized and unnormalized versions of theSFR scoring scheme.
Our analysis suggests thatthe differences are mostly due to numerical issues,namely the difference in magnitude between theNSFR scores and the likelihood scores in the linearcombination, and the default value assigned whenthe NSFR score was zero.
In ongoing work, weare working to address these issues.For most configurations the difference betweenSFR and FSR was not statistically significant atp = 0.05.
Our analysis indicated that surface dif-ferences tended to co-occur with structural differ-ences.
We hypothesize that as we scale up to largerand more ambiguous grammars, the system willinfer more derivations with the same yields, ren-dering a larger difference between the quality ofthe two scoring mechanisms.6 DiscussionTo analyze the advantages and disadvantages ofour approach over the baseline, we closely ex-amined and compared the derivations made onthe devset translation by the SFR-scored gram-mar and the likelihood-scored grammar.
Althoughthe BLEU scores are rather low, there were sev-eral sentences in which the SFR-scored grammarshowed a marked improvement over the baseline.We observed two types of improvements.The first is where the rescored grammar gaveus translations that, while still generally bad, werecloser to the gold standard than the baseline trans-lation.
For example, the Spanish sentence ?Y entercer lugar , esta?
el problema de la aplicacio?n uni-forme del Derecho comunitario .?
translates intothe gold standard ?Thirdly , we have the problemof the uniform application of Community law .
?The baseline grammar translates the sentence as?on third place , Transport and Tourism .
I arethe problems of the implementation standardisedis the EU law .?
with a GTM F-Score of 0.378,377sn=NP+67600 ?1.97/?5.66NP+67608the rapporteursp=PP+67601s=IN ?0.48/?0.37insn=SBAR+165198 ?1.39/?1.90nc=TO+165203todn=VP 0/?0.49makesn=NP+36950 ?5.89/?5.09NP+36952the rapporteursp=PP+36951 ?4.28/?3.81s=IN ?0.48/?0.37insn=NP+36953dn=DT 0/?0.58bothnc=NNS ?1.03/?0.81questionsFigure 4: Target side of the highest-scoring translations for a sentence, according to the baseline system (left) and the SFRsystem (right).
Boxed nodes are substitution sites.
Scores in superscripts denote the score of the sub-derivation according tothe baseline and to the SFR system.and the rescored grammar outputs the translation?to there in the third place , I are the problem ofthe implementation standardised is the Commu-nity law .
?, with an F-Score of 0.5.
While many ofthe fragments in the derivations that yielded thesetwo translations differ, the ones we would like tofocus on are the fragments that yield the transla-tion of ?comunitario?.
The grammar contains sev-eral competing unary fragment pairs for ?comuni-taro?.
In the baseline grammar, the pair (aq=NNP?
comunitario, aq=NNP ?
EU) has a scoreof ?0.693147, whereas the pair (aq=NNP ?comunitario, aq=NNP?
Community) has ascore of ?1.38629.
In the rescored grammar how-ever, (aq=NNP ?
comunitario, aq=NNP ?EU) has a score of -0.762973, whereas (aq=NNP?
comunitario, aq=NNP ?
Community)has a score of -0.74399.
In effect, the rescoringscheme rescored the word alignment itself.
Thissuggests that in future work, it may be possibleto integrate a word aligner or fragment aligner di-rectly into the MT training method.The other improvement was where the baselineand the SFR-scored grammar output translationsof roughly the same quality according to the eval-uation measure, yet in terms of human evaluation,the SFR translation was much better than the base-line translation.
For instance, our devset containedthe Spanish sentence ?Estoy de acuerdo con el po-nente en dos cuestiones .?
The baseline transla-tion given is ?I agree with the rapporteur in tomake .
?, and the SFR-scored translation given is?I agree with the rapporteur in both questions .
?.While both translations have the same GTM scoreagainst the gold standard ?I agree with the rap-porteur on two issues .
?, clearly, the second oneis of far higher quality than the first.
As we cansee in Figure 4, the derivation over the substring?in both questions?
gets a higher score than ?into make?
when translated with the rescored gram-mar.
In the baseline, ?en dos cuestiones?
is nottranslated as a whole unit ?
rather, the derivation of?el ponente en dos cuestiones?
is decomposed intofour subderivations, yielding ?el?
?ponente?
?en?
?dos cuestiones?, where each of those is translatedseparately, into ???
?the rapporteur?
?in?
and ?tomake?.
The SFR-scored grammar, however, out-puts a different bilingual derivation.
The sourceis decomposed into five sub-derivations, one foreach word, and each word is translated separately.Then, the rescored target fragments set the propertarget-side word order and select the target-sidewords that maximize the score of the subderiva-tion covering the source span.
We note that in thisexample, the score of translating ?dos?
to ?make?was higher than the score of translating ?dos?
to?both?.
However, the higher level target frag-ment that composed the translation of ?dos?
to-gether with the translation of ?cuestiones?
yieldeda higher score when composing ?both questions?rather than ?to make?.7 Conclusions and Future WorkThe results presented above indicate that aug-menting the scoring mechanism with an accuracy-based measure is a promising direction for transla-tion quality improvement.
It gives us a statisticallysignificant improvement over the baseline, and ouranalysis has indicated that the system is indeedmaking better decisions, moving us a step closertowards the goal of making translation decisionsbased on the hypothesis of the resulting transla-378tion?s accuracy.Now that we have demonstrated that translationquality can be improved by incorporating a mea-sure of fragment quality into the scoring scheme,our immediate next step is to optimize our sys-tem so that we can scale up to significantly largertraining and tuning sets, and determine whetherthe improvements we have noted carry over whenthe likelihood is computed from more data.
Af-terwards, we will implement a training schemeto maximize an accuracy-based objective func-tion, for instance, by minimizing the differencebetween the scores of the highest-scoring deriva-tion and the oracle derivations, in effect maximiz-ing the score of the highest-scoring translation.The rescoring method presented in this paperneed not be limited to DOT.
Fragments can bethought of as analogous to phrases in Phrase-Based SMT systems ?
we could implement a sim-ilar rescoring system for phrase-based systems,where we generate several candidate translationsfor source sentences in a tuning set, and score eachphrase used against the phrases used in a set of or-acles.
More broadly, we could potentially take anystatistical MT system, and compare the featuresof all candidates generated against those of oracletranslations, and score those that are closer to theoracle higher than those further away.Finally, by explicitly framing the translationproblem as a search problem, where we are di-vorcing the inferences in the search space (i.e.the model) from the path we take to find the op-timal inference according to some criterion (i.e.the scoring scheme), we can remove some of thevariability when comparing two models or scoringmechanisms (Lopez, 2009).AcknowledgementsThis work is supported by Science Foundation Ire-land (Grant No.
07/CE/I1142).
We would like tothank the anonymous reviewers for their helpfulcomments and suggestions.ReferencesR.
Bod.
2007.
Unsupervised syntax-based ma-chine translation: The contribution of discontiguousphrases.
In Proceedings of the 11th Machine Trans-lation Summit, pages 51?57, Copenhagen, Den-mark.P.
F. Brown, S. Della Pietra, V. Della Pietra, andR.
Mercer.
1993.
The mathematics of statistical ma-chine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311.F.
J. Damerau.
1964.
A technique for computer de-tection and correction of spelling errors.
Commun.ACM, 7(3):171?176.J.
Eisner.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In Proceedings of the41st Annual Meeting of the Association for Com-putational Linguistics (ACL), Companion Volume,pages 205?208, Sapporo.M.
Galley, J. Graehl, K. Knight, D. Marcu, S. De-Neefe, W. Wang, and I. Thayer.
2006.
Scalable in-ference and training of context-rich syntactic trans-lation models.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Compu-tational Linguistics, pages 961?968, Sydney, Aus-tralia.J.
Goodman.
1996.
Efficient algorithms for parsing theDOP model.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, pages 143?152, Philadelphia, PA.M.
Hearne and A.
Way.
2003.
Seeing the wood for thetrees: Data-oriented translation.
In Proceedings ofthe Ninth Machine Translation Summit, pages 165?172, New Orleans, LA.M.
Hearne and A.
Way.
2006.
Disambiguation strate-gies for data-oriented translation.
In Proceedings ofthe 11th Conference of the European Association forMachine Translation, pages 59?68, Oslo, Norway.M.
Hearne.
2005.
Data-Oriented Models of Parsingand Translation.
Ph.D. thesis, Dublin City Univer-sity, Dublin, Ireland.M.
Johnson.
2002.
The DOP estimation method isbiased and inconsistent.
Computational Linguistics,28(1):71?76, March.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Proceedingsof the Annual Meeting of the Association for Com-putational Linguistics, demonstation session, pages177?180, Prague, Czech Republic.P.
Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings of379the Conference on Empirical Methods in NaturalLanguage Processing, pages 388?395, Barcelona,Spain.P.
Koehn.
2005.
Europarl: A Parallel Corpus for Sta-tistical Machine Translation.
In Machine Transla-tion Summit X, pages 79?86, Phuket, Thailand.A.
Lavie, K. Sagae, and S. Jayaraman.
2004.
The sig-nificance of recall in automatic metrics for MT eval-uation.
In Proceedings of the 6th Conference of theAssociation for Machine Translation in the Ameri-cas, pages 134?143, Washington, DC.A.
Lopez.
2009.
Translation as weighted deduction.
InProceedings of the 12th Conference of the EuropeanChapter of the ACL (EACL 2009), pages 532?540,Athens, Greece.F.
J. Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proceedings of the 41stAnnual Meeting of the Association for Computa-tional Linguistics, pages 160?167, Sapporo, Japan.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of 40th Annual Meet-ing of the Association for Computational Linguis-tics, pages 311?318, Philadelphia, PA.A.
Poutsma.
2000.
Data-oriented translation.
In The18th International Conference on ComputationalLinguistics, pages 635?641, Saarbru?cken, Germany.C.
Tillmann and T. Zhang.
2006.
A discrimina-tive global training algorithm for statistical MT.
InProceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, pages 721?728, Sydney, Australia.J.
Tinsley, M. Hearne, and A.
Way.
2009.
Parallel tree-banks in phrase-based statistical machine transla-tion.
In Proceedings of the Tenth International Con-ference on Intelligent Text Processing and Computa-tional Linguistics (CICLing), pages 318?331, Mex-ico City, Mexico.J.
Turian, L. Shen, and I. D. Melamed.
2003.
Eval-uation of machine translation and its evaluation.
InProceedings of the Ninth Machine Translation Sum-mit, pages 386?393, New Orleans, LA.D.
Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Com-putational Linguistics, 23(3):377?404.K.
Yamada and K. Knight.
2001.
A syntax-basedstatistical translation model.
In Proceedings of39th Annual Meeting of the Association for Com-putational Linguistics, pages 523?530, Toulouse,France.K.
Yamada and K. Knight.
2002.
A decoder forsyntax-based statistical MT.
In Proceedings of 40thAnnual Meeting of the Association for Computa-tional Linguistics, pages 303?310, Philadelphia, PA.B.
Zhao and S. Chen.
2009.
A simplex armijodownhill algorithm for optimizing statistical ma-chine translation decoding parameters.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguis-tics, Companion Volume: Short Papers, pages 21?24, Boulder, Colorado.380
