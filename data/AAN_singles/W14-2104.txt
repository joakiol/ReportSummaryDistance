Proceedings of the First Workshop on Argumentation Mining, pages 24?28,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsOntology-Based Argument Mining and Automatic Essay ScoringNathan Ong, Diane Litman, and Alexandra BrusilovskyDepartment of Computer Science, University of PittsburghPittsburgh, PA 15260 USAnro5,dlitman,apb27@pitt.eduAbstractEssays are frequently used as a mediumfor teaching and evaluating argumentationskills.
Recently, there has been interest indiagrammatic outlining as a replacementto the written outline that often precedesessay writing.
This paper presents a pre-liminary approach for automatically iden-tifying diagram ontology elements in es-says, and demonstrates its positive corre-lation with expert scores of essay quality.1 IntroductionEducators tend to favor students providing aminimal-writing structure, or an outline, beforewriting a paper.
This allows teachers to giveearly feedback to students to reduce the amountof structural editing that might be needed lateron.
However, there is evidence to suggest thatstandard text-based outlines do not necessarily im-prove writing quality (Torrance et al., 2000).
Re-cently, there has been growing interest in graph-ical outline representations, especially for argu-mentative essays in various domains (Scheuer etal., 2009; Scheuer et al., 2010; Peldszus and Stede,2013; Reed and Rowe, 2004; Reed et al., 2007).Not only do they provide a different outlining for-mat, but they also allow students to concretely vi-sualize their argumentation structure.
Our workis part of the ArgumentPeer project (Falakmassiret al., 2013), which combines computer-supportedargument diagramming and peer-review with thegoal of improving students?
writing skills.In this paper, we follow the lead of others in dis-course parsing for essay scoring (Burstein et al.,2001), and we preliminarily attempt to answer twoquestions: Q1) Can an argument mining systembe developed to automatically recognize the ar-gument ontology used during diagramming, whenprocessing a student?s later written essay?
Q2) Ifso, is the number of ontological elements that canbe recognized in a student?s essay correlated withthe essay?s argumentation quality?
Potentially, an-swering these questions in the affirmative wouldallow us to assist students with their writing by al-lowing computer tutors to label sentences with theontology, determine which elements are missing,and suggest adding these missing elements to im-prove essay quality.2 CorpusOur corpus for argument mining consists of 52 es-says written in two University of Pittsburgh un-dergraduate psychology courses.
In both courses,students were asked to write an argumentative es-say supporting two separate hypotheses that theycreated based on data they were given.
The aver-age essay contains 5.2 paragraphs, 28.6 sentences,and 592.1 words.Before writing the essay, students were first re-quired to generate an argument diagram justify-ing their hypotheses using the LASAD argumen-tation system1.
LASAD argument diagrams con-sist of nodes and arcs from an instructor-definedontology, as shown in Figure 1.
Next, studentswere required to turn their diagrams into writ-ten argumentative essays.
Automatically taggingthese essays according to the 4 node types (Cur-rent Study, Hypothesis, Claim, Citation) and 2arc types (Supports, Opposes) common to bothcourses is the argument mining goal of this pa-per.
The tagged essay corresponding to Figure 1 isshown in Table 1.2While the diagram is requiredto be completed by students, this work does notutilize the student diagrams.1http://lasad.dfki.de2Both diagrams and papers were distributed to other stu-dents in the class for peer review.
While the diagrams werenot required to be revised, students needed to revise their es-says to address peer feedback.
To maximize diagram and es-say similarity, here we work with only the first drafts.24Figure 1: An argument diagram from a research methods course.After the courses, expert graders were asked toscore all essays on a 5-point Likert scale (with 1being the lowest and 5 being the highest) withoutthe diagrams, using a rubric with multiple crite-ria.
For the essay as a whole, graders not onlychecked for correct grammar usage, but also forflow and organization.
In addition, essays weregraded based on the logic behind their argumen-tation of their hypotheses, as well as addressingclaims that both supported and opposed their hy-potheses.
While not an explicit category, many ofthe criteria required students to present multiplecitations backing their hypotheses.
The averageexpert score for the 52 essays is 3.03, and the me-dian is 3, with the scores distributed as shown incolumn four of Table 2.3 MethodologyEssay Discourse Processing.
Firstly, raw essaysare parsed for discourse connectives.
Explicit dis-course connectives are then tagged with their sense(i.e.
Expansion, Contingency, Comparison, orTemporal) using the Discourse Connectives Tag-ger3, as shown in Table 1.Mining the Argument Ontology.
We devel-oped a rule-based algorithm to label each sentence3http://www.cis.upenn.edu/?epitler/discourse.htmlin an essay with at most one label from our tar-get argument ontology.
Our rules were developedusing our intuition and informal examination of 9essays from the corpus of 52.
The algorithm con-sists of the following ordered4rules:Rule 1: If the sentence begins with a Compar-ison discourse connective, or if the sentence con-tains any string prefixes from {conflict, oppose}and a four-digit number (intended as a year for acitation), then tag with Opposes.Rule 2: If the sentence begins with a Contin-gency connective and does not contain a four-digitnumber, then tag with Supports.Rule 3: If the sentence contains a four-digitnumber, then tag with Citation.Rule 4: If the sentence contains string prefixesfrom {suggest, evidence, shows, Essentially, indi-cate} (case-sensitive), then tag with Claim.Rule 5: If the sentence is in the first, second, orlast paragraph, and contains string prefixes from{hypothes, predict}, or if the sentence contains theword ?should?
and contains no Contingency con-nectives, and does not contain a four-digit numberand does not contain string prefixes from {conflict,oppose}, then tag with Hypothesis.Rule 6: If the previous sentence was taggedwith Hypothesis, and this sentence begins with anExpansion connective and does not contain a four-4When multiple rules apply, the tag of the earliest is used.25# Essay Sentence Label Rule1 The ultimate goal of this study is to investigate the relationship betweenstop-sign violations and traffic activity.CurrentStudy72 To do this we analyzed two different variables on traffic activity: time of dayand location.None 8... ... ... ...6 Stop-signs indicate that the driver must come to a complete stop before thesign and check for oncoming and opposing traffic before[-Temporal] pro-ceeding on.Claim 47 For a stop to be considered complete the car must completely stop moving.
None 8... ... ... ...16 The first hypothesis was: If[-Contingency] it is a high activity time of day atan intersection then[-Contingency], there will be a higher ratio of completestops made than during a low activity time at the intersection.Hypothesis 517 The second hypothesis was: If[-Contingency] there is a busy intersectionthen[-Contingency], there will be a higher ratio of complete stops made thanat an intersection that is less busy.Hypothesis 518 So[-Contingency] essentially, it was expected that when[-Temporal] therewas a higher traffic activity level, either due to location or time of day, therewere to be less stop-sign violations.Supports 219 There have been many studies which indicate that people do drive differentlyat different times of day and[-Expansion] that it does have an impact ondriving risk.Claim 420 Reimer et al (2007) found that time of day did influence driving speed, reac-tion time, and speed variability measures.Citation 3... ... ... ...24 However[-Comparison], McGarva & Steiner (2000) oppose the second hy-pothesis because[-Contingency] they found that provoked driver aggressionthrough honking horns, increased the rate of acceleration at a stop sign.Opposes 1... ... ... ...Table 1: Essay sentences, their mined ontological labels, and rules used to determine the labels, for theessay associated with Figure 1.
Inferred discourse connective senses are italicized in square brackets.digit number, then tag with Hypothesis.Rule 7: If the sentence is in the first or last para-graph and contains at least one word from {study,research} and does not contain the words {past,previous, prior} (first letter case-insensitive) anddoes not contain string prefixes from {hypothes,predict} and does not contain a four-digit number,then tag with Current Study.Rule 8: Do not assign a tag to the sentence.Some sample output can be found on Table 1.Note that sentence 24 could have been tagged asCitation using Rule 3, but because it fits the crite-ria for Rule 1, it is tagged as Opposes.Ontology-Based Essay Scoring.
We also devel-oped a rule-based algorithm to score each essay inthe corpus.
These rules were developed using ourintuition in conjunction with the examination ofthe expert grading rubric.
These rules take a la-beled essay from the argument mining algorithmand outputs a score in the continuous range [0,5]using the following procedure:51: Assign one point to essays that have at leastone sentence tagged with Current Study (CS).2: Assign one point to essays that have at leastone sentence tagged with Hypothesis (H).3: Assign one point to essays that have at leastone sentence tagged with Opposes (O).4: Assign points based on the sum of the num-ber of sentences tagged with Claim (Cl) and thenumber of sentences tagged with Supports (S), alldivided by the number of paragraphs (#?).
If this5Score 0 occurs when no labels are assigned to the essay.26value exceeds 1, assign only one point.5: Assign points based on the number of sen-tences tagged with Citation (Ci) divided by thenumber of paragraphs (#?).
If this value exceeds1, assign only one point.6: Sum all of the previously computed points.For the three paragraph essay excerpted in Ta-ble 1 (assigned expert score 3), there were threesentences tagged with Current Study, three withHypothesis, one with Opposes, one with Sup-ports, two with Claim and three with Citation.The score is computed as follows:1CS+ 1H+ 1O+2Cl+ 1S3#?+3Ci3#?= 54 ResultsSince our essays do not have gold-standard on-tology labels yet, we cannot intrinsically evaluatethe argument mining algorithm.
We instead per-formed an extrinsic evaluation via our use of themined argument labels for essay scoring.The average automatic score for the corpus is3.42 and the median is 3.5, while the correspond-ing expert values are 3.03 and 3, respectively.
Apaired t-test of the means has a significance of p <0.01, suggesting that our algorithm over-scores theessays.
We also ran a one-sample t-test on each ex-pert score value to see if the automatic scores weresimilar to the expert scores.
We hypothesized thatwithin each expert score category predicted accu-rately, we should not see a significant difference (p?
0.05).
Table 2 shows that while the automaticscore is not significantly different for expert score4, the scores are significantly different for scores 2and 3.We also examined the Spearman?s rank corre-lation between the computed and expert scores.6We see that the Spearman?s rank correlation showssignificance of p < 0.0001 with a rho value of0.997.
Together these metrics suggest that our au-tomated scores are currently useful for ranking butnot for rating.5 Conclusion and Future WorkWe have presented simple rule-based algorithmsfor argumentation mining in student essays andessay scoring using argument mining.
Based onpreliminary extrinsic evaluation, our pattern-basedrecognition of a basic argumentation ontology6A Pearson correlation did not give significant results.expert avg.
auto t n pscore score1 4.33 ?
1 ?2 3.23 3.21 8 0.0133 3.30 2.10 31 0.0444 3.80 -1.00 12 0.337Table 2: One-sample t-test results for scores.seems to provide some insight into essay scoresacross two courses.
While the automatic scoresdid not necessarily reflect the expert scores, theranking correlation demonstrated that more argu-mentative elements were related to higher scores.Even with the limitations of this study (e.g.
no in-trinsic evaluation, a small essay corpus, a limitedargument ontology, a scoring algorithm using onlyontology features, application of discourse con-nector for a different genre), our results suggestthe promise of using argument mining to triggerfeedback in a writing tutoring system.To develop a more linguistically sophisticatedand accurate argument mining algorithm, our fu-ture plans include exploiting discourse informa-tion beyond connectives, e.g., by parsing our es-says in terms of PDTB (Lin et al., 2011) or RSTrelations (Feng and Hirst, 2012).
We also plan tolook at the helpfulness of argumentation schemes(Feng and Hirst, 2011), and other linguistic andessay features for automatic evaluation (Crossleyand McNamara, 2010).
In addition, our essaysare being annotated with diagram ontology labels,which will enable us to use machine learning toconduct intrinsic argument mining evaluations andto learn the weights for each rule or determine newrules.
Finally, we plan to explore using the dia-grams to bootstrap the essay annotation process.While some sentences in an essay can easily bemapped to the corresponding diagram (e.g.
sen-tence 1 in Table 1 to node 1 in Figure 1), the com-plication is that essays tend to be more fleshed-outthan diagrams, and at least in our corpus, also con-tain argument changes motivated by diagram peer-review.
While sentence 6 in Table 1 is correctlytagged as a Claim, this content is not in Figure 1.AcknowledgmentsThis work is supported by NSF Award 1122504.We thank Huy Nguyen, Wenting Xiong, andMichael Lipschultz.27References[Burstein et al.2001] Jill Burstein, Karen Kukich, Su-sanne Wolff, Ji Lu, and Martin Chodorow.
2001.Enriching automated essay scoring using discoursemarking.
ERIC Clearinghouse.
[Crossley and McNamara2010] Scott A Crossley andDanielle S McNamara.
2010.
Cohesion, coher-ence, and expert evaluations of writing proficiency.In Proceedings of the 32nd annual conference of theCognitive Science Society, pages 984?989.
Austin,TX: Cognitive ScienceSociety.
[Falakmassir et al.2013] Mohammad Falakmassir,Kevin Ashley, and Christian Schunn.
2013.
Usingargument diagramming to improve peer gradingof writing assignments.
In Proceedings of the 1stWorkshop on Massive Open Online Courses at the16th Annual Conference on Artificial Intelligence inEducation, Memphis, TN.
[Feng and Hirst2011] Vanessa Wei Feng and GraemeHirst.
2011.
Classifying arguments by scheme.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies-Volume 1, pages 987?996.
As-sociation for Computational Linguistics.
[Feng and Hirst2012] Vanessa Wei Feng and GraemeHirst.
2012.
Text-level discourse parsing with richlinguistic features.
In Proceedings of the 50th An-nual Meeting of the Association for ComputationalLinguistics: Long Papers-Volume 1, pages 60?68.Association for Computational Linguistics.
[Lin et al.2011] Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating textcoherence using discourse relations.
In Proceed-ings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies-Volume 1, pages 997?1006.
Associa-tion for Computational Linguistics.
[Peldszus and Stede2013] Andreas Peldszus and Man-fred Stede.
2013.
From argument diagrams to argu-mentation mining in texts: A survey.
InternationalJournal of Cognitive Informatics and Natural Intel-ligence (IJCINI), 7(1):1?31.
[Reed and Rowe2004] Chris Reed and Glenn Rowe.2004.
Araucaria: Software for argument analy-sis, diagramming and representation.
InternationalJournal on Artificial Intelligence Tools, 13(04):961?979.
[Reed et al.2007] Chris Reed, Douglas Walton, andFabrizio Macagno.
2007.
Argument diagrammingin logic, law and artificial intelligence.
The Knowl-edge Engineering Review, 22(01):87?109.
[Scheuer et al.2009] Oliver Scheuer, Bruce M.McLaren, Frank Loll, and Niels Pinkwart.
2009.An analysis and feedback infrastructure for argu-mentation learning systems.
In Proceedings of the2009 Conference on Artificial Intelligence in Educa-tion: Building Learning Systems That Care: FromKnowledge Representation to Affective Modelling,pages 629?631, Amsterdam, The Netherlands, TheNetherlands.
IOS Press.
[Scheuer et al.2010] Oliver Scheuer, Frank Loll, NielsPinkwart, and Bruce M. McLaren.
2010.
Computer-supported argumentation: A review of the stateof the art.
International Journal of Computer-Supported Collaborative Learning, 5(1):43?102.
[Torrance et al.2000] Mark Torrance, Glyn V. Thomas,and Elizabeth J. Robinson.
2000.
Individual differ-ences in undergraduate essay-writing strategies: Alongitudinal study.
Higher Education, 39(2):181?200.28
