Speech Annotation by Multi-sensory RecordingRobert LukDepartment ofComputing, Hong Kong Polytechnic UniversityEmaih csrluk@comp.polyu.edu.hkAbstractThis paper describes our effort to mark andannotate read Cantonese speech for both citationpronunciation and reading aloudsentences/phrases.
Four signals are recordedsimultaneously to assist marking and annotation:acoustic, laryngograph, nasal and air burstsignals.
A coarse match between voicedsegments of speech and voiced segments of thephonetic spelling of the utterance is executed bydynamic programming asfor approximate stringmatching.
Finally, we discuss general issues inthe design of our softxvare for annotation.
(a)1.
Introduction10 20 30 40 50This paper describes our effort to mark readspeech based on multi-sensor)' recording.Although speech data are available for manylanguages (e.g.
English, Putonghua, etc.
),Cantonese speech data are still rare.
Annotationof speech by hand is a tedious task, subject oerrors and consistency problems.
Therefore, weaim to annotate read Cantonese speechautomatically.
However, automatic annotationcan be difficult even though the pronunciation ofthe speech sound is known because certainphonetic events are difficult to detect (e.g.plosives).
We have adopted the multi-sensorytechnique developed for annotating English forCantonese, after Chan and Fourcin \[1\].Apart from annotation, marking speech data isalso important for general speech analysis.
Forexample, pitch synchronous Fourier transformcan take the advantages of both a wide-band anda narrow-band Fourier transform where finerdetails of the spectrum are more apparent withpitch synchronous transform (Figure 1).
(b)Figure 1: (a) Pitch Synchronous spectrum basedon 64-point FFT with autocorrelation at doublethe pitch period (b) 64-point FFT with fixedwindow size of 200 and overlap of 50 samplepoints.2.
Multi-sensoR' recordingIn this section, we describe the four signals thatare simultaneously recorded.
Next, we describethe physical set up for recording and therecording session.242.1 SensorsFour signals are received and recorded in amulti-sensor), recording session and these are theacoustic signal (Sp), laryngograph signal (Lx),plosive signal (Fx) and nasal signal (Nx).
Lxprovides information about vocal fold vibrationsand enables the identification ofvoiced/unvoiced segment as well as occurrenceof each epoch (i.e.
vocal fold closure).
The latteris important for pitch detection as well assubsequent signal processing that are pitchsynchronous.
Figure 2 shows the use of Lx todefine the voiced segment and epoch positions.~ :  " "  slwaO4C .
.
.
.
.
.
.
.
.
.
.
.
.l .
'~ ; : : : - '2?
: : - ' :~: : : ' - , ;~ ~',~.
- - - -  -- - .
.
- - -  , -  -~  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
- -  " i '~ l '~ l. .
.
.
.
.
- .
.
,  .
.1!1 ) i --.,:, .
: , .
:  : : : .?
.
.
.
.
.
.
.
.
.
.
.
: .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
: _ .
.
.
: : .
.
_ .
.
: .
.
_)..L_..:.._.L::..__.~.L.
_Z:.:.____:.
: .__.~.__:.
;_.:~Figure 2: The multi-sensory recording of thesyllable /pal The 4 channels from the top tobottom are: acoustic signal (Sp), laryngographsignal (Lx), turbulence signal (Fx) and nasalsignal (AS:), respectively.The *v signal is picked up by a high-frequencysensitive miniature microphone placed 1 to 2 cmnear the mouth.
The signal is drasticallyattenuated so that only a sudden burst of air canprovide sufficient excitation for recording.
Theburst of air is registered for aspiration orturbulence near the month (e.g.
fricative andaspirated voice stop) which may be undetectablein the acoustic signal (Sp).
Figure 3 shows theaspirated voice stop/p/that  is not registered inSp.
We anticipate that for continuous speech thistype of events occur often.A piezeo-ceramic transducer is placed near thenose bridge to detect nasal resonance.
The signalfrom this transducer is Nx and it is useful to25determine when nasalization occurs.
This wouldbe useful for detecting nasal consonants becauseit is simply an absorption of the vocalic energy,represented as a spectral zero.
Figure 4 showsthe recording of nasal resonance for the word/ma/......
!'
!!
':iy i l;i "~i\] i'~;:7' ~f"(~ !~'.
!~i':fi~(~i""~i ~i?.
t:~ ........ L...... ' ';iii  : : : :  " 'Figure 3: Multi-sensory recording of the syllable/ma/.
Note that nasal resonance occur at thebeginning in the bottom channel (Nx) and theamplitude is reduced in the top channel (Sp).Figure 4: A schematic diagram that shows thephysical set up of a multi-sensor5, recordingsensors.2.2.
Recording Set UpRecording is carried out in an anceohic hamberat City University.
of Hong Kong (Figure 5).
Tilemixer, sensor, laryngograph and microphonepower supply are placed inside the ancoehicchamber where as the recorder and PC areplaced outside because of noise from coolingfans.
The mixer provides amplification for theNx signal and attenuation for the Fx signal.Likewise, the microphone power supply and thelaryngograph provide amplification of the Spand Lx signals, respectively.
Four channel taperecordings are carried out first because they canserve as back up.
Afterwards, the recorded ataare transferred to the PC by the computerinterface under computer control via the RS232link.
It is possible to mark the beginning andending of each utterance using the DATrecorder.2.3 Recording SessionWe have carried out recording isolatedCantonese \[2\] speech sounds as well as readspeech of phrases and sentences.
For isolatedsyllables, subjects are asked to pronounce allcombinations of Cantonese initials, finals andtones which amounts to several thousandsyllables.
To save time and manual effort, thesubject reads aloud a page of syllables (about50) which are recorded on to the DAT tapebefore transfer to the PC.
To maintain someconsistency, subjects are asked to read aloud acarrier sentence by heart and pronounce only thetarget syllable.For continuous read speech, subjects are given alist of sentences or phrases to read aloud.
Thesesentences are selected from a corpus, thatmaximizes the coverage of Cantonese diphonesbased on a greed),' algorithm \[3\].
The 104sentences covered 348 Cantonese diphones.
Thecorpus is a collection of news articles from thePH corpus \[4\].3.
Isolated Syllable MarkingEach file contains a set of syllables read aloud ina recording session.
The 4 channels are sampledat 16kHz and quantized to 16 bits.
The first stepis to isolate the syllables from silence and labelthese syllables with the corresponding phoneticspelling augmented with a tone.
Next, the fourchannel data is compressed into a marked speechdata to save storage by a multiplicative factor of4.
The marked speech data uses the leastsignificant hree bits to encode where an epoch,some turbulence at the month, some nasalizationor silence have occurred, according to thescheme shown in Table 1.
Silence is alsoencoded because the recording will be carriedout for an utterance instead of isolated syllablesfor later work.Bit Pattern Meanin\[000 Silence Presence01 i Not Silencelxx Epoch Presence010 Plosive/Fricative Presence001 .
Nasality PresenceTable l" Bit pattern scheme for representing thedifferent marks of speech data.
Nasal andturbulence are assumed not to simultaneouslyoccur .The least significant three bits instead of themost significant three bits are chosen forencoding because of compatibility reasons.
Thethree bits can be considered as an additive noisecomponent of magnitude at most 3 bits (i.e.
8).Usually, speech signals are much larger than 8so that the noise due to the least 3 bits are almostnegligible based on this encoding.
We havefound no noticeable degradation in the markedspeech signal, which can be fed to othersoftware like MATLAB as binary data.energy > thres andenergy < thres and duration <glitchstartdUrati?n i ~"~.~..~..~.~..x.// e n e r g y ?-( 0 ) thresenergy < ~thres / mduration =glitch/.
~  energy ?thres / rnFigure 5: State transition diagram of the finitestate machine for speech segmentation.3.1 Speech SegmentationThe 4-channel recording is segmented based onthe running energy of the speech signal Sp.
Afnite-state machine (FSM) keeps track of thesegmentation decision (Figure 5).
At state 0, theFSM considers the speech as silence.
When therunning energy is beyond a threshold T, the FSMmakes a transition to state 1.
The FSM remainsin state 1 provided that the running energyremains beyond the threshold.
Otherwise, it willmake a transition back to state 0.
If tile FSM26remains in state 1 for a sufficiently long timethat the speech signal cannot be a glitch, theFSM makes a transition to state 2.
It will remainin state 2 if the running energy is beyond Tdivided by m. The multiplicative reduction maccounts for the steady reduction of speechenergy near the end.
Otherwise, the FSM makesa transition back to state 0.3.2 Phonet ic  Spel l ing Label ingEach segmented speech data corresponds to asyllable and the data has to be labeled with thecorresponding phonetic spelling.
Due to noise,sometimes glitches are mis-recognized asspeechdata and there are usually more segmentedspeech files than the amount of labels.
We useda simple strategy to sort the data by size anddelete the extra small files before labeling iscarried out.3.3 Epoch Detect ionThe detection of epoch is based on the Lx signal.The epoch is roughly located when the Lx signalis at the maximum near the largest change in theLx signal.
A simple detection strategy is todetermine the first order backward ifference:DLx\[i\] = Lx\[i\] - Lx\[i-2\]The detection selects those with a positive slope(i.e.
DLx\[i\] > 0).
A threshold T,, is set accordingto the following rule:T,, = 0.1 x max{DLx\[ i \ ]}!in order to decide those slopes which aredefinitely too small to consider for theidentification of the epoch.
Another threshold Tkis determined by the k-means algorithm whichdecides which of the remaining slopes are largeenough and which are too small.
Any remainingslopes, which are larger than T~ and whichoccurred consecutively, are deleted except at thelast position.
The remaining slopes positions arethen the epoch positions (Figure 6)..ii 1u0 2000 4000 6000 8000 10000 12000 '??
?/ _\]0 2000 4000 6000 8000 10000 12000Figure 6: Pitch detection from the Lx signalshown at the top.
The result is shown at thebottom where each spike represents the largestpositive slop found.i10000 .............. ~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.  "
.
,oooj\ = =,oo< iilo ~ - -~ , "  \ i i\i I1 , , ?
; i0 1000 2000 3000 4000 5000Figure 7: The frequency distribution of theamplitude of the positive backward ifference ofthe Lx signal.
The threshold was found to be1530 by the k-means algorithm which isreasonable.The k-means algorithm for determining Tkassumes there are two clusters: cl for slopes thatare significantly large and c2 for those slopeswhich are significantly small.
Initially, thealgorithm selects the two extreme slope values(i.e.
maximum and minimum) as the centriod ofthe two respective clusters.
A slope x israndomly selected and decided which cluster itbelongs based on the following rule:if d(cl,x) > d(c2,:U thenC2 := {X} LJC2elseCI := ~} L.)CI27where dO is the distance between the centroid ofa cluster and the slope x.
After each assignment,the centroid of the changed cluster is updated.Assignment of slopes to the two clusters; isrepeatedly carried out until no more slope valuesto assign (Figure 7).3.4 Plosive/Fricative DetectionCertain plosives (e.g./p/) and fricatives (e.g./f/)produces turbulence near the month (Figure 8).This sudden burst of air is registered in the Fxsignal as a sudden rise in magnitude.
We followChan and Fourcin \[1\] to find the envelop of theFx signal by first high-pass filtering (with sigmasmoothing) the signal at l kHz and smooth it by amedian filter of length 201.$15/a~4 T  - ?
I 1 ~Figure 8: Post-processing of a multi-sensoryrecording of the speech/ma/.3.5 NasalizationThe amount of nasality is computed based onboth the Nx signal and the Lx signal as in \[1\].Nasality is considered as the energy absorbed inthe nasal cavity, reflected by the amount of nasalresonance picked up by the peizeo-ceramictransducer.
The absolute value of A5.
wouldindicate the amount of energy in the vibrationbut this has to be summed over one pitch periodto indicate the amount of absorption for thepulse of air released in one vocal cord open-close cycle.
Thus, we compute A~ as the sum ofthe absolute value of the Nx signal in one pitchperiod between two consecutive epoches.The presence of nasality (Figure 8) isdetermined by a threshold Tx where an)' N,, valuelarger than Tx implies there exists somesignificant nasalization.
To decide a betterthreshold between significant nasalization andinsignificant nasalization, a different hresholdT,, is used, which is determined by the k-meansalgorithm.4.
Continuous Speech AnnotationAnnotation for a speaker-independentcontinuous peech is not an easy task withouttraining.
Our main idea is to find a reliablecoarse match between the available phoneticspelling of the speech and perform additionalprocessing to locate fine details.A reliable cue is voicing which is available fromthe Zx signal because it is decoupled from theacoustic environment, making voiceidentification under extreme noisy environmentpossible.
Also, since the Zx signal represents hesource signal without convolving with the vocaltrack, it is relatively easy and reliable to detectthe occurrence of pitch marks and thereforevoicing.
For matching phonetic spelling withspeech sound, usually a syllable corresponds toavoice segment because ach syllable must have apeak.
Thus, the voice segment can be used thebasic unit for finding the annotation of thespeech.4.1 Voiced segment identificationTo detect voicing, tile Lx signal is differencedand thresholded by the k-means algorithm, as inmarking speech data (Section 3.3).
In addition,the voice segment must have some continuity inthe vocal cord vibration which restricts theduration of the voice segment to have at least 2cycles.
Taking the range of pitch to be between 2and 20ms \[5\], the duration of voiced segmentmust be at least 40ms long.
Figure 9 shows anexample of fnding the voiced segments whenreading aloud a sentence.
The accuracy of the Lxsignal is usually within 1 Lx cycle.28, I fFigure 9: Detection of voiced segments using theLx signal.
The segments found are indicated by acolor (blue) ribbon at the top of the signal.4.2 Sentence/Phrase Boundary DetectionSentence and phrase boundary can be manuallymarked by the DAT recorder or from theannotation software.
The later is particularlytiresome because the amount of speech data islarge, b'pically around 100Mbytes.
Therefore,the visualization software takes time to scan anddisplay' the data.The alternative explored in here is toautomatically identify these sentence/phraseboundaries by measuring the duration betweentwo voiced segments.
If the duration is morethan 900ms, a sentence/phrase boundary isfound.
However, the subject has to be aware ofthis arrangement for sentence/phraseseparations.4.3 Unvoiced Context ComputationFor computational efficiency, unvoiced contextcomputation only identifies the existence orabsence of air burst, noise above 4kHz and nasalresonance.
The existence and absence of theseevents are used for coarse matching betweenidentified voice segment from speech and fromphonetic spelling (Figure 10).Figure 10: The detection of unvoiced contexts ofan utterance of 30 syllables.
Key: LB for theexistence of an air burst in the left context of avoiced segment and LF for the existence offricative noise in the left context.
Since there areno right context air burst of fricative noise, theywere not identified (as RB and RF respectively).4.
3.1 Air Bw'st DetectionAir burst is detected in the Fx signal.
For eachvoice segment, he left and right contexts for airburst detection are between 10ms and 40msaway from the voiced segment.
Within these twoportions of the speech data, we obtain themaximum absolute differenced Fx signal.
If thismaximum is larger than a threshold (set at 800),then air burst is detected.4.
3.2 Fricative-like NoiseFor fricative-like consonants, the turbulence isregistered as noise above 4kHz in tile Sp signal.Since these noise can extend quite far from thevoiced segment, Sp signal between 10ms and800 ms away from the voiced segment isexamined.
For each context, Sp signal is high-pass filtered at a cutoff of 4kHz.
The filteredsignal is differenced and the largest magnitude iscompared with a threshold.
If the signal is largerthan the threshold, than fricative noise ispresent.4.4 Coarse MatchingThe aim of coarse matching is to associate thevoiced segments of the phonetic spelling and29those identified in the speech signal.
The voicedsegment identified in the speech signal mayrepresent one or more voiced segment of thephonetic spelling because of co-articulation.
Forexample, the greeting sentence can have thefollowing phonetic spelling/li ho ma/.
The threevoiced segments of this phonetic spelling are/li/,/ho/ and /ma/.
However, in continuous peech,the voiced segment identified may be co-articulated to gather giving rise to only 2 voicedsegments:/li homa/since nasal /m/and vowels/o/ and /a/ are voiced.
Here, voicing has thespecial meaning that the vocal fold vibrates.Therefore, some voiced consonants likefricatives are not considered as voiced becausethe production does not involve vocal foldvibrations.The voiced segments in the phonetic spellingand found in speech are temporally ordered sothat these segments can be considered as stringswhere each character is a voiced segment.Coarse matching can be considered as a stringmatching problem but due to co-articulationapproximate string matching that caters formerging voiced segment in matching is needed(Figure 11).
;ii~ T i T i ' l l l lI, h l l  , ,m J , _  l.t,.,,t,~.
, ,~.
,~ .
.
.
.
~ IOll~ ~dt l~!IFigure 11: The labeled voiced speech segmentsby approximate string matching.4.4.1.
Problem FormulationLet s be the sequence of voiced segmentsidentified in the Sp signal.
Likewise, let p be thesequence of voiced segments in the phoneticspelling of Sp.
Let s\[i\] denote the /,h voicedsegment and likewise forp\[i\].The distance D(s,p) between s and p is theminimal number of edit operations thattransform s to p and vice versa.
The minimaldistance and the sequence of operations can befound by dynamic programming, using thefollowing rule:d\[i,j\] = mot { d\[i-l,j\] + hlsert(i-l,i,j),d\[ij-1\] + delete(j-l,j,O,d\[i-l,j-1\] + sub(i,j)}where d\[i,j\] is the minimal edit distance from(0,0) to position (i,j), representing the matchingof voiced segments s\[O,i\] in Sp with those p\[O.j\]in the phonetic spelling.4.4.2 Edit DistanceUnlike approximate string matching, the editdistance of insertion, deletion and substitutionsare determined ifferently.
For insertion, weconsider the two voiced speech segments at i andi -l are associated with a single voiced segmentof phonetic spelling.
Effectively, there is anerror in voice segmentation where one of thesegment (i or i + l ) is a spurious detection.For deletion, the voiced segments of thephonetic spelling is associated with one voicedspeech segment.
Effectively, this edit operationis accounting for the co-articulation of twovoiced segments as in/li homa/.Such co-articulation does not occur freely.
Forexample, if there are plosives or fricatives in theunvoiced context between the two voicedsegments (e.g.
co-articulation i  /li/ and /ho/),then it is very unlikely that the voiced segmentsare co-articulated together.
In addition, if thevoiced speech segment is very short, then it isalso unlikely that the two voiced segments of thephonetic spelling are read with the single voicedspeech segment.
Thus, both unvoiced contextconstraints and voiced segment duration areweighting factors of the deletion operation.30For substitution at position (i, j) , we considerwhether the voiced speech segment is the sameas the voiced segment of the phonetic spelling.
Ifthey have the same unvoiced context, thesubstitution cost would be low.
Otherwise, thecost would be based on the number ofmismatches in the unvoiced context.5.
Software Design and ImplementationInevitably, it is necessary to check and correctautomatic annotation.
Software tool developedfor this purposes needs to visualize a largevolume of data that runs into hundreds ofMbytes.
This is particularly the case for multi-sensor3' recording.1~e~84~ 56576t5~?
,I i , \] I.,.
II l ?
h,l.
\[ .t l .l.
~I ~.1"I'l' " ' ' I  " "  ~'I'" r " \['I 'I' l'Figure 12: Visualization of the decimatedversion of speech file of size I lOM bytes.
T~vomarkers define a region for magnification.;'!'ff~l!?
!Figure 13: Visualization of a magnified segmentof the speech data in Figure 12.
Pitch markidentification was carried out as shown in thesecond channel.For visualization, our software decimates tilegiven speech data since the resolution of thescreen is only 1024 (Figure 12).
This provides abird's eye view of the data.
For speech datadetails, the user can zoom (Figure 13) into aregion within two markers defined by clickingthe mouse at the appropriate screen location.Within the magnified scale, the user can move(Figure 14) the speech data to the left or right ofthe current magnified region of data.
The usercan also save the marked region directly into afile.
The name of the file can be automaticallygenerated or found from a list of labels in a file.
: Lebel Genere~on .........................LE-I~M~'ke,PO,i~O.
: ~ !
L~belCou~ ~0 ~ Le.F~ghtMeu.k~Pos~on" IT,H~EI , Lobel Jsp i ~gh?i Pre,r~ Zoom Out I L=e," \[13 ; t.~el:: Su~/u?
I Reto~:l J Co, or o!
Re,on: J!
67116~0 : Gen LabelFigure 14: A dialog for moving, saving andmanual labeling speech data.
This dialog box isinvoked when tile user double clicks betweentwo markers or within a voiced segment.Signal processing for visualization is carried outwith the data stored in tile buffer and it is notdirectly operating on the speech data in the file.The purpose is to visualize tile effect of settingparameters of certain signal processing function.Once the desired parameter values are found,signal processing is carried out for tile speechdata in the file.
Since tile buffer data is adecimated version of the data in tile file, thesignal processing parameters have to be scaledby the amount of decimation.
For example, a 16kHz signal may be decimated 4 times and thecutoff frequency of high-pass filtering at 4kHzhas to reduced to l kHz.The software also enable us to visualize tilemarked speech data for verification andmodification.
Non-silence is shown as a ribbonon the top of the view window.
Since nasal andair-burst do not occur simultaneously, they areshown as different color ribbons at tilehorizontal evel in the view window.
The pitch31epoches are displayed as vertical lines.
Due todecimation, most pitch epoches are notdisplayed (Figure 15).
They will appear againwhen a segment is magnified (Figure 16).Figure 15: Visualization of the marked speechdata.
The dark ribbon shows the non-silencecomponent.
The light band shows the nasalsegment.
Due to decimation, most of the pitchmarks are not displayed._~ ........... :',~w~-Ji IFigure 16: A magnified region of the markedspeech data showing the location of theidentified pitch epoches which were absent inFigure 15.6.
DiscussionWe have described how 4-channels of speechdata are recorded and transferred to thecomputer.
We demonstrated that marked speechdata provide important information for bothannotation and speech analysis.
Our markingscheme is space efficient and it is compatiblewith other speech processing software withoutregard to marking.
We have also described howto post-process the 4-channels of data to obtainthe marking information.
Although the markingprocess can be completely automatic, humanchecking is still necessary for full correctness.Many decisions are based on setting anappropriate threshold, which can be deternainedby the k-means algorithm.AcknowledgmentThis research is supported by CERG PolyU757/95H.
We thank Mark Huckvale of theDepartment of Phonetics and Linguistics,University College of London for providing theirspeech filing system.
We are grateful to the CityUniversity of Hong Kong for providing theaneohoic chamber and recording equipment.Finally, we thank Guo and Liu for providing thePH corpus.References1.
Chan, D.S.F.
and A.J.
Fourcin (1994)"Automatic Annotation using Multi-SensorData", Proceedings of ESCA Eurospeech'94, Germany, pp.
187-190.2.
Zee, Y.Y.
(1991) "Chinese (Hong KongCantonese)", Journal of the hTternationalPhonetic Association, 21 ( 1 ).3.
Shih, C. and B. Ao (1994) "Duration studyfor the AT&T Mandarin text-to-speechsystem", Conference Proceedings of theSecond ESCA/1EEE IVorkshop on SpeechSynthesis, Mohonk, pp.
29-32.4.
Guo, J. and H.C. Laln (1992) "PH: a Chinesecorpus for pinyin-hanzi transcription",Technical Report TR93-112-0, Institute ofSystems Sciences, National University ofSingapore.5.
O'Shaughnessy, D. (1987) Speechcommunication."
human and machine,Reading, Mass.,Addison-Wesley.32
