ABL: Alignment-Based LearningMenno van  ZaanenSchool of Computer  StudiesUniversity of LeedsLS2 9 JT  L(~edsUKmenno@scs, leeds ,  ac.
ukAbst rac tThis \])al)er introdu(:es a new tyl)e of grammarlearning algorithm, iilst)ired l)y sl;ring edit dis-tance (Wagner and Fis(:her, 1974).
The algo-rithm takes a (:ortms of tlat S(~lltell(:es as inputand returns a (:ortms of lat)elled, l)ra(:ket(~(1 sen-~ten(:(~s.
The method works on 1)airs of unstru(:-tllr(?
(l SelltellC(~,s that have one or more words in(:onunon.
W\]lc, ll two senten('es are (tivided intoparts that are the same in 1)oth s(mten(:es andparl;s tha|; are (litl'erent, this intbrmation is usedto lind l)arl;s that are interchangeal)le.
Theset)arts are taken as t)ossil)le (:onstituents of thesame tyl)e. After this alignment learning stel) ,the selection learning stc l) sel(~('ts the most l)rot)-at)le constituents from all 1)ossit)le (:onstituents.This m(;thod was used to t)ootstrat) structure(m the ATIS (:ortms (Mar(:us et al, 1f)93) andon the OVIS ~ (:ort)us (Bommma et ~d., 1997).While the results are en(:om:aging (we ol)|;ainedUl) to 89.25 % non-crossing l)ra(:kets precision),this 1)at)er will 1)oint out some of the shortcom-ings of our at)l)roa(:h and will suggest 1)ossiblesolul;ions.1 In t roduct ionUnsupervised learning of syntactic structure isone of the hardest 1)rol)lems in NLP.
Althoughpeople are adept at learning grammatical struc-ture, it is ditficult to model this 1)recess andtherefore it is hard to make a eomtmter learnstrllCtllre.We do not claim that the algorithm describedhere models the hmnan l)rocess of languagelearning.
Instead, the algorithm should, givenunstructured sentences, find the best structure.This means that the algorithm should assign1Opcnbam" Vcrvoer hfformatie Systeeln (OVIS)stands for Pul)lic Transt)ort hfformation System.sl;ru('ture to sentences whi(:h are similar to the,~;tru(:ture peot)le would give to sentences, lintnot ne(:essarily in the same |li l le or Sl);~(;e l'e-strictions.The algorithm (:onsists of two t)hases.
Thetirst t)hase is a constituent generator, whi(:\]l gen-erates a m()tiw~ted set of possible constituents1)y aligning sentenc(:s. The se(:ond i)hase re-stri(:ts tllis set l)y selecting the best constituentsfrom the set.The rest of this t)aper is organized as ibl-lows.
Firstly, we will start t)y describing l)revi-ous work in machine learning of language stru(:-ture and then we will give a descrit)tion of theABL algorithm.
Next, some results of al)t)lyingthe ABL algorithm to different corpora will 1)egiven, followed 1)y a discussion of the algorithmalia flltllre resear(;h.2 Prev ious  WorkI,e;wning metl,o(ls can t)e grouped into suitor-vised and unsut)ervised nmthods.
Sul)ervisedmethods are initial|seal with structured input(i.e.
stru(:ture(\] sent(m(:es for grannnar learningmethods), while mlsut)ervised methods learn l)yusing mlstru(:tured ata only.In 1)ractice, SUl)ervised methods outpertbrmmlsut)ervised methods, since they can adapttheir output based on the structured exami)lesin the initial|sat|on t)hase whereas unSUl)ervisedlnethods emmet.
However, it is worthwhileto investigate mlsupcrvised gramlnar learningmethods, since "the costs of annotation are pro-hibitively time and ext)ertise intensive, and theresulting corpora may 1)e too suscet)tible to re-stri(:tion to a particular domain, apt)lication, orgenre".
(Kehler and Stolcke, 1.999)There have 1)een several approaches to the un-supervised learning of syntactic structures.
Wewill give a short overview here.961Memory based learifing (MBL) keeps track ofpossible contexts and assigns word types basedon that information (Daelemans, 1995).
Red-ington et al (1998) present a method thatbootstraps yntactic categories using distribu-tional information and Magerman and Marcus(1990) describe a method that finds constituentboundaries using mutual information values ofthe part of speech n-grams within a sentence.Algorithms that use the minimmn descriptionlength (MDL) principle build grammars thatdescribe the input sentences using the minimalnunfl)er of bits.
This idea stems from intbrnm-tion theory.
Examples of these systems can befound in (Grfinwald, 1994) and (de Marcken,1996).The system by Wolff (1982) pertbrms aheuristic search while creating and Inergingsymbols directed by an evaluation function.Chen (1.995) presents a Bayesian grammar in-duction method, which is tbllowed by a post-pass using the inside-outside algorithm (Baker,1979; Lari and Young, 1990).Most work described here cmmot learn com-plex structures uch as recursion, while othersystems only use limited context to find con-stituents.
However, the two phases in ABLare closely related to some previous work.Tim alignment learning phase is etlb.ctively acompression technique comparat)le to MDL orBayesian grammar induction methods.
ABLremembers all possible constituents, buildinga search space.
The selection h;arning phasesearches this space, directed by a probabilisticevaluation function.3 Algor i thmWe will describe an algorithm that learns struc-ture using a corpus of plain (mlstructured) sen-tences.
It does not need a structured train-ing set to initialize, all structural informationis gathered from the unstructured sentences.The output of the algorithm is a labelled,bracketed version of the inlmt corpus.
Althoughthe algorithm does not generate a (context-fl'ee)grammar, it is trivial to deduce one from thestructured corpus.The algorithm builds on Harris's idea (1951)that states that constituents of the same typecan be replaced by each oth, er.
Consider the sen-Wh, at is a family fareWh, at is th, e payload of an African SwallowWh, at is & family fare)xWh, at is (the payload of an African Swallow)xFigure 1: Example bootstrapping structureFor each sentence sl in the corpus:For every other sentence s2 in the corpus:Align s~ to s2Find the identical and distinct partsbetween s~ and s2Assign non-terminals to the constituents(i.e.
distinct parts of s~ and s2)Figure 2: Alignment learning algorithmfences as shown in figure 1.
2 The constituents a.family fare and the payload of an African Swal-low both have the same syntactic type (theyare both NPs), so they can be replaced by eachother.
This means that when the constituent inthe first sentence is replaced by the constituentin the second sentence, the result is a wflid sen-tence in the language; it is the second sentence.The main goal of the algorithm is to estab-lish that a family .fare and the payload of art,African Swallow are constituents and have thesame type.
This is done by reversing Harris'sidea: 'i1" (a group o.f) words car-,, be; replaced byeach other, they are constituents and h.ave th, esame type.
So the algorithm now has to findgroups of words that can be replaced by eachother and after replacement s ill generate validsentences.The algorithm consists of two steps:1.
Alignment Leanfing2.
Selection Learning3.1 A l ignment  LearningThe model learns by comparing all sentencesin the intmt corpus to each other in pairs.
Anoverview of the algorithm can be tbund in fig-ure 2.Aligning sentences results in "linking" iden-tical words in the sentences.
Adjacent linkedwords are then grouped.
This process reveals2All sentences in the examlfles can be fbund in theATIS corlms.962.f,'o,,,.
Sa,,.
F,'a,.ci.,'co (to Dallas)../'rout (Dallas to)| San Francisco 02(Sa,, l.o)  Dallas 02O, DaUas #o Sa,,.
J';'a,.cisco)2?
\[;1"0 ~II,.fF()Ii't(San Francisco), to (Dallas)2(Dalla.gj to (Sa,,.Figure 3: Ambiguous al ignments1;t1(; groul)S of identical words, 1)ut it also llIlC()v-ers the groups of distinct wor(ls in the sentences.In figure 1 What is is the identical part  of thesentences and a fam, ily J'a~v, and the payload ofan A./ricau, Swallow are the dist inct l)arts.
Thedistinct parts are interchangeable, so they are(tetermilmd to 1)e const ituents o17 the same I;yl)e.We will now Cxl)lain the stel)s in the align-men|  learning i)hase in more de, tail.3.1.1 Ed i t  D is tanceq\[b find the identi(:al word grouI)S in |;he sen-tences, we use the edit; distan(:e a lgor i thm byWagner and Fischer (197d:), which finds themin imum nmnl)er of edit operat ions (insertion,(lelei;ion and sul)stii;ul;ion) l;o change one sen-te, nce into the other, ld(mti(:al wor(ts in the sen-t(races can 1)e t'(mnd at \])\]a(;es W\]l(~,l'e lie edit op-eration was al)plied.The insl;antia,tiol~ of the algoril;hm that  fin(isl;}le longest  COllllllOll Slll)S(}(\]ll(}ll(;( ~,ill two  Sell-tences sometimes "links" words that  are, toofar apart, in figure 3 when: 1)esides the o(:cm'-rences of.from,, the ocem:rences of San }4"au, ci.scoor Dallas are linked, this results in unintendedconstituents.
We woukt r;d;her have the lnodellinking to, result ing in a sl;1"u(;I;llre with the 1101111phrases groul)ed with the same type corre(:tly.Linking San Francisco or Dallas results i~lconst ituents that  vary widely in size.
This  stemsfrom the large distance between the linkedurords in the tirsi; sentence mid in th(; s(:condsentence.
This  type of al ignlnent can t)e ruledout by biasing the cost f imction using distancesbetween words.3.1.2 Group ingAn edit distance algor i thm links identical wordsin two sentences.
When adjacent wor(ls arelinked in l)oth sentences, they can l)e grouped.A groul) like this is a part  of a senten(:e that  canalso be tbmM in the other sentence.
(In figure 1,What is is a group like this.
)The rest of the sentences can also be grouped.The words in these grout)s arm words that  aredistinct in the two sentences.
When all of thesegroups fl:om sentence, one would 1)e relflaced bythe respective groups of sentence two, sentencetwo is generated.
(a family fare and th, c pay-load of an African Swallow art: of this type ofgroup in figure 1.)
Each pair of these distinctgroups consists of possilfle constil;uents Of thesame type.
:~As can be, seen in tigure 3, it is possible thatempty groups can lm learned.a .
l .a  Ex is t ing  Const i tuentsAt seine 1)oint it may be t)ossible that  the modellem'ns a co11stituent that  was already stored.This may hal)l)en when a new sentence is com-pared to a senlaen(;e in the part ial ly structuredcorpus.
In this case,, no new tyl)e, is intro(hu:ed~lint the, consti|;ucnl; in l;he new sentence gel;s l;hesame type of the const i tuent in the sentence inthe part ial ly structm:ed corpus.It may even t)e the case that  a part ial ly si;ruc-tured sentence is compared to another  part ial lysl;rtlctllre(1 selll;elR,e.
This occm:s whel~ a s(:n-fence that (;onl;ains some sl;ructure, which waslearner1 1)y COlnl)aring to a sentelme in the par-t;\]ally structure(l  (;Ol;pllS~ is (;Olllt)ar(~,(\] 1;o al l -other (t)art;ially stru(:ture(t) sente, n(:e. Whenthe ('omparison of these two se, nl;ence, s yieldsa const i tuent hai: was ah:ea(ly t)resent in bothsenten(:es, the tyl)es of these constitueld;S aremerged.
All const i tuents of these types are ut)-dated, so the, y have the same tyl)e.By merging tyl)es of const i tuents we make t;heassuml)tion that  co\]lstil;uents in a (:ertain con-text can only have one tyl)e. In section 5.2 wediscuss the, imt)li(:atiolls of this assmnpl;ion andpropose an alternative at)t)roach.3.2 Se lect ion  Learn ingThe first step in the algorithm may at somepoint generate COllstituents that overlap withother constituents, hi figure 4 Give me allflights .from Dallas to Boston receives two over-lal)ping structures.
One constituent is learned3Since the alger||Inn does not know any (linguist;|c)llalIICS for the types, the alger|finn chooses natural num-bers to denote different types.963( Book Delta 128 ) f lwn Dallas to Boston?
'Give m?
(all.fligh, ts)'f,'om Dallas to Boston)Give me ( help on classes )l?igure 4: Overlapping constituentsby comparing against Book Delta 128 f i rm Dal-las to Boston and the other (overlapl)ing) con-stituent is tbund by aligning with Give me helpon classes.The solution to this problem has to do withselecting the correct constituents (or at leastthe better constituents) out of the possible con-stitnents.
Selecting constituents can be done inseveral dittbrent ways.ABL : incr  Assume that the first constituentlearned is the correct one.
This means thatwhen a new constituent overlaps with olderconstituents, it can 1)e ignored (i.e.
they arenot stored in the cortms).ABL : lea f  The model corot)rites the probabil-ity of a constituent counting the nmnber oftimes the particular words of the constituenthave occurred in the learned text as a con-stituent, normalized by the total number ofconstituents.Ple,f(c) = \]c' C C:  yield(c') = yicld(c)lICIwhere C is the entire set: of constituents.ABL :braneh In addition to the words of thesentence delimited by the constituent, themodel computes the probability based on thepart of the sentence delimited by the wordsof the constituent and its non-terminal (i.e.a normalised probability of ABL:leaf).Pb, .~na, , (c l root(c  ) = r )  =e c :  y/el( l ( , - ' )  - -  y ie ld (c )  A ; "1Ic" c :  ,'oot(c") =The first method is non-probabilistic and maybe applied every time a constituent is found thatoverlaps with a known constituent (i.e.
whilelearning).The two other methods are probabilistic.
Themodel computes the probability of the con-stituents and then uses that probability to selectconstituents with the highest probability.
Thesemethods are ~pplied afl;er the aligmnent learn-ing phase, since more specific informatioil (inthe form of 1)etter counts) can be found at thattime.In section 4 we will ewfluate all three methodson the ATIS and OVIS corpus.3.2.1 Vi terb iSince more than just two constituents can over-lap, all possible combinations of overlappingconstitueni;s hould be considered when com-Imting the best combination of constituents,which is the product of the probabilities of theseparate constituents as in SCFGs (cf.
(Booth,1969)).
A Viterbi style algorithm optimization(1967) is used to etficiently select the best com-bination of constituents.When conll)uting the t)r()t)ability of a com-bination of constituents, multiplying the sepa-rate probabilities of the constituents biases to-wards a low nnmber of constituents.
Theretbre,we comtmte the probability of a set of con-stituents using a normalized version, the geo-metric mean 4, rather than its product.
(Cara-ballo and Charniak, 1998)4 Resu l tsThe three different ABL algorithms m~d two1)aseline systems have been tested on the ATISand OVIS corpora.The ATIS corlms ti'om the P(;nn Treebankconsists of 716 sentences containing 11,777 (:on-stituents.
The larger OVIS corpus is a Dutchcorpus containing sentences on travel intbrma-tion.
It consists of exactly 10,000 sentences.
Wehave removed all sentences containing only oneword, resulting in a corpus of 6,797 sentencesand 48,562 constituents.The sentences of the corpora are stript)edof their structures.
These plain sentences areused in the learning algorithms and the result-ing structure is compared to the structure of theoriginal corpus.All ABL methods are tested ten times.
Th(,ABL:incr method is applied to random orders ofthe input corpus.
The probabilistic ABL meth-ods select constituents at random when differ-ent combinations of constituents have the sameprobability.
The results in table 1 show the4The geometric mean of a set of constituents^.
.
.
A = VFI  =, P( d964LEFTI{IGI/TABL:INCllAI3L:LEAFABL:BI/ANCIINCBP32.6O82.7083.24 (1.17)81.42 (0.11)8, .31 (0.01)AT1S OVISNCBI{ ZCS NCB\] ) NCBR ZCS76.8292.9187.21 (0.67)86.27 (0.06)89.31 (0.01)1.J238.8318.56 (2.32)21.63 (0.5O)29.75 (0.00)51.2375.8588.71 (0.79)85.32 (0.02)89.2.5 (0.oo)73.1786.6684.36 (1.\]0)79.96 (0.03)8>o4 (0.0|))25.2248.0830.87 (0.09)42.20 (0.01)laJ)h, 1: Results of I;hemean ;rod standard deviations (between bra(:k-ets).The two base, line systcnis, left and right, ontyt)uiM left: mid right brnnching trees respectively.Three, metrics hnve been compnl;cd.
NCBPstmlds for Non-(\]rossing Bra.
(:kets Precision,which denotes the percentage, of learned (:on-stituents th~,t do not overlai) with any con-sl;it;uent;s in I;he m'igi'n, al (:orpus.
NCIH~ is theNon-Crossing Brackets ll.e(:all mid shows |;het)(;rt'ent~ge of constituents in the original co lt)us thai; (1o not overlap with :my constituentsin the learned (:oft)us.
Finnlly, Z(LS' strums ti)l'Zero-(Jrossing Sentences a,nd r(',l)reseuts he per-c(ml;age of sentence, s that (t(1 not have m~y over-lnt)l)ing constii;uenl;s.4.1 Eva luat ion'l-'tm incr modet 1)erfi)rms (tuii:e well (:onsi(hwingthe t'~mt hat it; (:;mnot re(:ov(w t'roln incorre(:t(:()nstituents, with a t)re(:ision a,nd re(:~dl of ()V(~l'8t) %.
The order of the senl;en(:es how(we, r isquite iml)orbmt , since |tie sl;ml(tard deviationof the inc'r model is quite high (est)e~(:ialty withthe ZCS, reaching 3.22 % on the OV!S (:orpus).We expected the prot)nl)ilistic nmtho(ts toi)erform t)o,l;ter, trot the lc((f modet performsslightly worse.
The, ZCS, however, is somewhatbetter, re, suiting in 21.63 % on the AT1S cor-pus.
Furthermore, d;he standard deviations ofthe le,:f model (&lid Of the branch, model) arec\]ose to 0 %.
The st;~tisti(:al methods generatemore precise, results.Ttm bra'n, ch, modet dearly outl)erfornl allo~,her models.
Using more Sl)e(:itic statistics gen-erate better results.Although the resull;s of the N FIS (:orpus mMOVlS corIms differ, the, conclusions that (:ml })ereached are similm:.ATIS and OVIS corpora4.2 ABL  Compared  to Other  MethodsIt; is difficult to corot)are the results of the ABLmodel ag~dnst other lnethods, since, often d i fthrent corpora or m(',trics m:e used.
The meth-ods describe, d by Pereira and Schabcs (1.9(.
)2)comes reasonably close to ours.
The unsuper-vised nmthod le~rns tructure on plain sentencesfrom the ATIS corlms resulting in 37.35 % pre-cision, while the "un.supcrvised ABL signili(:mltlyoutperforms this method, reaching 85.31% l)re-cision.
Only their s'uperviscd version results inn slightly higher pre('ision of 90.36 %.The syste, nl th;d; simt)ly buihts right branch-ins structures results in 82.70 % precision mid92.91% teeM1 on the ATIS cortms, where ABLgot 85.31% and 89.31%.
This wa,s expected,sin(:e English is a right |)rmmhing language; aleft branching sysl;Clll t)(~rff)l.'ltle(| lllllCh woFsc(32.60 % pre(:ision and 76.82 % rccnll).
C(m-versely, right branching wouht not do very wellon ~ ,l~q)mmse, corpus (~ left 1)r~m(:hing lan-gua.ge).
Sin(:e A\]31, does not have a 1)ref(~renc(~fi)r direction built; in, we exi)ect ABL to t)ertbrmsimilarly on n Ja,t)anese (:orpus.5 D iscuss ion  and  Future  Extens ions5.1 Recurs |onAll ABL methods des('ribed here can lem:n re-cursive structures and have been fomtd when~t)plying ABI, to the NIl?IS and OVIS (:orlms.As (:ml be sc(m in figure 5, the learned recur-sive structure, is similm: to the, original struc-ture.
Some structure has t)een removed to makeit easier to see where the recurs|on occurs.Roughly, recursive structures arc built in twosteps.
First, the algorithm generates the struc-ture with difl'cro, nt non-terminals.
Then, thetwo nonq;ermimds are merged as described inso, el;ion 3.1.3.
The merging of the non-terminalsm~y occur anywhere in the cortms , sin(:e allmerged non-terndnals are ut)dated.965learnedoriginallearnedoriginalPlease ezplain the (field FLT DAY in the (table)is)isPlease explain (the .field FLT DAY in (the table)NP)NpExplain classes QW and (QX and (Y)a2)~'eExplain classes ((QW)Np and (QX)NI, and (Y)NP)NPFignre 5: Recursive structures learned in the A TIS corpusShow me the ( morning )x flightsShow me the ( nonstop )x fli.qhtsFigure 6: Wrong syntactic type5.2 Wrong Syntactic TypeIn section 3.1.3 we made the assumt)tion that aconstituent in a certain context can only haveone type.
This assumption introduces omeproblems.The sentence John likes visiting relatives il-lustrates uch a problem.
The constituent vis-iting relatives can be a noun phrase or n verbphrase.Another prol)lem is ilhlstrated in figure 6.When applying the ABL learning algorithm tothese sentences, it will determine that morningand nonstop are of the same type.
Untbrtu-nately, morning is a noun, while nonstop is anadverb)A fixture extension will not only look at thetype of the constituents, lint also at the con-text; of the constituents.
Ii5 the example, theconstituent morning nlay also take the t)lace ofa subject position in other sentences~ 1)ut theconstituent nonstop never will.
This intbrnm-tion can be used to determine when to mergeconstituent types, efl'ectively loosening the as-sunlption.5.3 Weakening Exact MatchWhen the ABL algorithms try to learn with twoconlpletely distinct sentences, nothing can belearned.
If we weaken the exact match betweenwords in the alignment step of the algorithm, itis possible to learn structure ven with distinctsentences.Instead of linking exactly matching words,the algorithm should match words that areequivalent.
An obvious way of implementingthis is by making use of cquivalence classes.
(See5Harris's implication does hold in these sentences.nonstop can also be replaced by for example cheap (an-other adverb) and morning can be replaced by evenin.q(another noun).for example (Redington et al, 1998).)
The idea1)ehind equivalence classes is that words whichare closely related are grouped together.A big advantage of equivalence classes is thatthey can be learned in an unsupervised way, sothe resulting algorithm remains nnsui)ervised.Words that are in the same equivalence classare.
said to be sufficiently equivalent, so thealigmnent algoritlnn may assunm they are sin>ilar and may thus link them.
Now sentencesthat do not have words in common, but do havewords in the same equivalence class in common,can be used to learn structure.When using equivalence classes, more con-stituents are learned and more terminals in con-stitnents may l)e seen as similar (according tothe equivalence classes).
This results in a muchricher structm'ed corlms.5.4 Alternative StatisticsAt the moment we have tested two diflbrentways of computing the probal)ility of a con-stituent: ABL:leaf which computes  the t ) ro t )  -ability of the occurrence of the terminals in aconstituent, and ABL:b','anch which coml)utesthe probability of the occurrence of |;11(; termi-nals together with the root non-terminal in a(-onstitueut, based on the learned corpus.Of course, other models can bc imt)lemented.One interesting possibility takes a DOP-like ap-proach (Bod, 1998), which also takes into ac-count the inner structure of the constituents.6 Conc lus ionWc have introduced a new grammar learning al-gorithm based 055 c()mparing and aligning plainsentences; neither pre-labelled or bracketed sen-tences, nor pre-tagged sentences arc used.
Ituses distinctions between sentences to find pos-sible constituents and afterwards elects themost probable ones.
The output of the algo-rithm is a structured version of the corpus.By l;aking entire sentences into account, thecontext used by the model is not limited by win-dow size, instead arbitrarily large contexts are966used.
Furthermore, the model has the ability tolearn recursion.~\['ln'ee ditl'erent instances of the algorithmhave l)een al)t)lied to two corpora of differ-eat size, the ATIS corpus (716 sentences) andthe OVIS corpus (6,797 sentences), generatingpromising results.
Although t;he OVIS corpusis almost ten t;imes the size of the ATIS cor-pus, these corpora describe a small conceptualdomain.
We plan to ~l)t)ly the ~flgori~hms tolarger domain corpora in the near fllture.Re ferencesJ.
K. Barker.
1979.
Trainabh; grammars forspeech recognition.
In J. J. Wolf and 1).
H.Klatt, editors, Speech, Communication Papersfor the Ninety-seventh Meetin.q of the Acous-tical Society of America, pages 547-550.R,ens Bod.
1998.
Beyond Grammar An_F, zpcricncc-Bascd Th, eory of Language.
Stan-Jbrd, CA: CSLI Publications.R.. Bonnema, R. Bod, and R,.
Scha.
1997.
ADOP model for semantic iil|;ertn'el;ation.
InProceedings of the Association for Compu~tational Ling'aistics/Eurwpean Ch, apter of th, cAssociation for Computational Linguistics,Madrid, p~ges 159 167.
Sommerset, N J: A s-soci~tion tbr Compul;ational Linguistics.T.
Booth.
1969.
Probal)ilistic representation fformed languages.
In Co'~@',rcnce ll,cco'rd o/"1959 "lEnth, Annual Symposium on ,5'witcl~,in.qand Automata Theory, pages 74 8:1.Sharon A. Caraballo and Eugene Charniak.1998.
New figures of merit for best-first prol)-abilist;ie chart parsing.
Computational Lin-guistics, 24(2):275 298.Stanley F. Chert.
1995.
Bayesian gralmnar in-duction for language modeling.
\]:ll Proceed-ings of the Association J'or ComputationalLinguistics, pages 228 235.Walter Daelemmls.
1995.
Memory-based lexi-cal acquisition and 1)rocessing.
In P. Stefh;ns,editor, Mach, inc Translation and the Lexicon,vohmm 898 of Lecture Notes in Artificial In-telligence, pages 85 98.
Berlin: Springer Ver-lag.Carl G. de Marcken.
1996.
Unsupervised Lan-g'aage Acquisition.
I)h.D.
thesis, Departnmntof Electrical Engineering mid Comtmter Sci-ence, Massachusetts Institute of Technology,Cambridge, MA, sep.Peter Oriinwald.
1994.
A nfinimmn de, scriptionlengl;h approach to grammar inference.
InG.
Scheler, S. Wernter, and E. R,iloif, editors,Connectionist, Statistical and S!pnbolic Ap-proaches to Learning for Natural Language,vohnne 1004 of Lecture Notes in dl~ pages203-216.
Berlin: Springer Verlag.Zellig Harris.
1951.
Methods in Structural Lin-guistics.
Chicago, IL: University of ChicagoPress.Andrew Kehler and Andreas Stoleke.
1999.Preface.
In A. Kehler and A, Stolcke, edi-tors, Unsuper'viscd Learning in Natural Lan-guage Processing.
Association for Comlmta-tional Linguistics.
Proceedings of the work-shop.K.
Lari and S. J.
Young.
1990.
The estima-tion of stochastic ontext-free grammars us-ing the inside-outside ~dgorithm.
ComputerSpeech and Language, 4:35 56.\]).
Magerman and M. Marcus.
1990.
Pars-ing natural language using mutual intbrma-tion statistics.
In Pwcecdin.qs o,f th, e NationalCon.fcrcnce on Artificial Intelli.qence, p~ges984 989.
Cambridge, MA: MIT Press.M.
Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a large annotated corpus ofenglish: the Penn tr(,ebank.
ComputationalLinguistics, 19(2):31.3 330.F.
Pereira and Y. Schgd)e,s.
1992.
Inside-outsidereestimation fl:om pm:tially t)racketed cor-pora.
In l'rocccdings of th, c Association forComputational Lin.quistics, pages :128-135,Newark, Debm~are.Martin Redington, Nick Chater, and StevenFinch.
1998.
Distrilmtional information: Apowerflfl cue for acquiring synt;actic cate-gories.
Cwnitivc Science, 22(4):4:25 469.A.
Viterbi.
t967.
Error bmmds for convoh>tiona\] codes and an asymptotically ol)timumdecoding algorithm.
Institute of Electricaland Electronics Engineers Transactions onInformation Th, cory, 13:260 269.Robert A. Wagner and Michael J. Fischer.1974:.
The string-to-string correction prob-lem.
Journal of th, e Association for Comput-ing Machinery, 21(1):168-173, jmLJ.
G. Wollf.
1982.
Lmlguage acquisition, datacompression and generalization.
Langv, agc ~Communication, 2:57-89.967
