Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 294?299,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsContext-Dependent Multilingual Lexical Lookup for Under-ResourcedLanguagesLian Tze Lim*?
*SEST, KDU College PenangGeorgetown, Penang, Malaysialiantze@gmail.comLay-Ki Soon and Tek Yong Lim?FCI, Multimedia UniversityCyberjaya, Selangor, Malaysia{lksoon,tylim}@mmu.edu.myEnya Kong TangLinton University CollegeSeremban, Negeri Sembilan, Malaysiaenyakong1@gmail.comBali Ranaivo-Malan?onFCSIT, Universiti Malaysia Sarawak,Kota Samarahan, Sarawak, Malaysiambranaivo@fit.unimas.myAbstractCurrent approaches for word sense dis-ambiguation and translation selection typ-ically require lexical resources or largebilingual corpora with rich informationfields and annotations, which are ofteninfeasible for under-resourced languages.We extract translation context knowledgefrom a bilingual comparable corpora of aricher-resourced language pair, and injectit into a multilingual lexicon.
The multilin-gual lexicon can then be used to performcontext-dependent lexical lookup on textsof any language, including under-resourcedones.
Evaluations on a prototype lookuptool, trained on a English?Malay bilingualWikipedia corpus, show a precision scoreof 0.65 (baseline 0.55) and mean recip-rocal rank score of 0.81 (baseline 0.771).Based on the early encouraging results,the context-dependent lexical lookup toolmay be developed further into an intelligentreading aid, to help users grasp the gist ofa second or foreign language text.1 IntroductionWord sense disambiguation (WSD) is the task ofassigning sense tags to ambiguous lexical items(LIs) in a text.
Translation selection chooses targetlanguage items for translating ambiguous LIs in atext, and can therefore be viewed as a kind of WSDtask, with translations as the sense tags.
The trans-lation selection task may also be modified slightlyto output a ranked list of translations.
This then re-sembles a dictionary lookup process as performedby a human reader when reading or browsing a textwritten in a second or foreign language.
For conve-nience?s sake, we will call this task (as performedvia computational means) context-dependent lexi-cal lookup.
It can also be viewed as a simplifiedversion of the Cross-Lingual Lexical Substitution(Mihalcea et al, 2010) and Cross-Lingual WordSense Disambiguation (Lefever and Hoste, 2010)tasks, as defined in SemEval-2010.There is a large body of work around WSD andtranslation selection.
However, many of these ap-proaches require lexical resources or large bilin-gual corpora with rich information fields and an-notations, as reviewed in section 2.
Unfortunately,not all languages have equal amounts of digital re-sources for developing language technologies, andsuch requirements are often infeasible for under-resourced languages.We are interested in leveraging richer-resourcedlanguage pairs to enable context-dependent lexicallookup for under-resourced languages.
For this pur-pose, we model translation context knowledge as asecond-order co-occurrence bag-of-words model.We propose a rapid approach for acquiring themfrom an untagged, comparable bilingual corpusof a (richer-resourced) language pair in section 3.This information is then transferred into a multilin-gual lexicon to perform context-dependent lexicallookup on input texts, including those in an under-resourced language (section 4).
Section 5 describesa prototype implementation, where translation con-text knowledge is extracted from a English?Malaybilingual corpus to enrich a multilingual lexiconwith six languages.
Results from a small experi-ment are presented in 6 and discussed in section 7.The approach is briefly compared with some relatedwork in section 8, before concluding in section 9.2 Typical Resource Requirements forTranslation SelectionWSD and translation selection approaches may bebroadly classified into two categories depending294on the type of learning resources used: knowledge-and corpus-based.
Knowledge-based approachesmake use of various types of information fromexisting dictionaries, thesauri, or other lexical re-sources.
Possible knowledge sources include defi-nition or gloss text (Banerjee and Pedersen, 2003),subject codes (Magnini et al, 2001), semantic net-works (Shirai and Yagi, 2004; Mahapatra et al,2010) and others.Nevertheless, lexical resources of such rich con-tent types are usually available for medium- to rich-resourced languages only, and are costly to buildand verify by hand.
Some approaches thereforeturn to corpus-based approaches, use bilingual cor-pora as learning resources for translation selection.
(Ide et al, 2002; Ng et al, 2003) used aligned cor-pora in their work.
As it is not always possible toacquire parallel corpora, comparable corpora, oreven independent second-language corpora havealso been shown to be suitable for training pur-poses, either by purely numerical means (Li and Li,2004) or with the aid of syntactic relations (Zhouet al, 2001).
Vector-based models, which capturethe context of a translation or meaning, have alsobeen used (Sch?tze, 1998; Papp, 2009).
For under-resourced languages, however, bilingual corpora ofsufficient size may still be unavailable.3 Enriching Multilingual Lexicon withTranslation Context KnowledgeCorpus-driven translation selection approaches typ-ically derive supporting semantic information froman aligned corpus, where a text and its translationare aligned at the sentence, phrase and word level.However, aligned corpora can be difficult to ob-tain for under-resourced language pairs, and areexpensive to construct.On the other hand, documents in a comparablecorpus comprise bilingual or multilingual text ofa similar nature, and need not even be exact trans-lations of each other.
The texts are therefore un-aligned except at the document level.
Comparablecorpora are relatively easier to obtain, especiallyfor richer-resourced languages.3.1 Overview of Multilingual LexiconEntries in our multilingual lexicon are organised asmultilingual translation sets, each corresponding toa coarse-grained concept, and whose members areLIs from different languages {L1, .
.
.
, LN} con-veying the same concept.
We denote an LI as?item?, sometimes with the 3-letter ISO languagecode in underscript when necessary: ?item?eng.
Alist of 3-letter ISO language codes used in this pa-per is given in Appendix A.For example, following are two translation setscontaining different senses of English ?bank?
(?fi-nancial institution?
and ?riverside land?
):TS 1 = {?bank?eng, ?bank?msa, ???
?zho, .
.
.
}TS 2 = {?bank?eng, ?tebing?msa, ??
?zho, .
.
.
}.Multilingual lexicons with under-resourced lan-guages can be rapidly bootstrapped from simplebilingual translation lists (Lim et al, 2011).
Ourmultilingual lexicon currently contains 24371 En-glish, 13226 Chinese, 35640 Malay, 17063 French,14687 Thai and 5629 Iban LIs.3.2 Extracting Translation ContextKnowledge from Comparable CorpusWe model translation knowledge as a bag-of-wordsconsisting of the context of a translation equiva-lence in the corpus.
We then run latent seman-tic indexing (LSI) (Deerwester et al, 1990) on acomparable bilingual corpora.
A vector is then ob-tained for each LI in both languages, which maybe regarded as encoding some translation contextknowledge.While LSI is more frequently used in informa-tion retrieval, the translation knowledge acquisi-tion task can be recast as a cross-lingual indexingtask, following (Dumais et al, 1997).
The underly-ing intuition is that in a comparable bilingual cor-pus, a document pair about finance would be morelikely to contain English ?bank?eng and Malay?bank?msa (?financial institution?
), as opposed toMalay ?tebing?msa (?riverside?).
The words ap-pearing in this document pair would then be anindicative context for the translation equivalencebetween ?bank?eng and ?bank?msa.
In other words,the translation equivalents present serve as a kindof implicit sense tag.Briefly, a translation knowledge vector is ob-tained for each multilingual translation set from abilingual comparable corpus as follows:1.
Each bilingual pair of documents is mergedas one single document, with each LI taggedwith its respective language code.2.
Pre-process the corpus, e.g.
remove closed-class words, perform stemming or lemmati-sation, and word segmentation for languageswithout word boundaries (Chinese, Thai).2953.
Construct a term-document matrix (TDM), us-ing the frequency of terms (each made up bya LI and its language tag) in each document.Apply further weighting, e.g.
TF-IDF, if nec-essary.4.
Perform LSI on the TDM.
A vector is thenobtained for every LI in both languages.5.
Set the vector associated with each translationset to be the sum of all available vectors of itsmember LIs.4 Context-Dependent Lexical LookupGiven an input text in language Li (1 ?
i ?
N ),the lookup module should return a list of multilin-gual translation set entries, which would containL1, L2, .
.
.
, LN translation equivalents of LIs inthe input text, wherever available.
For polysemousLIs, the lookup module should return translationsets that convey the appropriate meaning in context.For each input text segment Q (typically a sen-tence), a ?query vector?, VQ is computed by takingthe vectorial sum of all open class LIs in the in-put Q.
For each LI l in the input, the list of alltranslation sets containing l, is retrieved into TS l.TS l is then sorted in descending order ofCSim(Vt, VQ) = Vt ?
VQ|Vt| ?
|VQ|(i.e.
the cosine similarity between the query vectorVQ and the translation set candidate t?s vector) forall t ?
TS l.If the language of input Q is not present inthe bilingual training corpus (e.g.
Iban, an under-resourced language spoken in Borneo), VQ is thencomputed as the sum of all vectors associated withall translation sets in TS l. For example, given theIban sentence ?Lelaki nya tikah enggau emperajaiya, siko dayang ke ligung?
(?he married his sweet-heart, a pretty girl?
), VQ would be computed asVQ =?V (lookup(?lelaki?iba))+?V (lookup(?tikah?iba))+?V (lookup(?emperaja?iba))+?V (lookup(?dayang?iba))+?V (lookup(?ligung?iba))where the function lookup(w) returns the transla-tion sets containing LI w.5 Prototype ImplementationWe have implemented LEXICALSELECTOR, a pro-totype context-dependent lexical lookup tool inJava, trained on a English?Malay bilingual cor-pus built from Wikipedia articles.
Wikipedia ar-ticles are freely available under a Creative Com-mons license, thus providing a convenient sourceof bilingual comparable corpus.
Note that whilethe training corpus is English?Malay, the trainedlookup tool can be applied to texts of any languageincluded in the multilingual dictionary.Malay Wikipedia articles1 and their correspond-ing English articles of the same topics2 were firstdownloaded.
To form the bilingual corpus, eachMalay article is concatenated with its correspond-ing English article as one document.The TDM constructed from this corpus con-tains 62 993 documents and 67 499 terms, includ-ing both English and Malay items.
The TDM isweighted by TF-IDF, then processed by LSI usingthe Gensim Python library3.
The indexing process,using 1000 factors, took about 45 minutes on aMacBook Pro with a 2.3 GHz processor and 4 GBRAM.
The vectors obtained for each English andMalay LIs were then used to populate the transla-tion context knowledge vectors of translation setin a multilingual lexicon, which comprise six lan-guages: English, Malay, Chinese, French, Thai andIban.As mentioned earlier, LEXICALSELECTOR canprocess texts in any member languages of the mul-tilingual lexicon, instead of only the languages ofthe training corpus (English and Malay).
Figure 1shows the context-depended lexical lookup out-puts for the Iban input ?Lelaki nya tikah enggauemperaja iya, siko dayang ke ligung?.
Note that?emperaja?
is polysemous (?rainbow?
or ?lover?
),but is successfully identified as meaning ?lover?
inthis sentence.6 Early Experimental Results80 input sentences containing LIs with translationambiguities were randomly selected from the Inter-net (English, Malay and Chinese) and contributedby a native speaker (Iban).
The test words are:?
English ?plant?
(vegetation or factory),1http://dumps.wikimedia.org/mswiki/2http://en.wikipedia.org/wiki/Special:Export3http://radimrehurek.com/gensim/296Figure 1: LEXICALSELECTOR output for Iban input ?Lelaki nya tikah enggau emperaja iya, siko dayangke ligung?.
Only top ranked translation sets are shown.?
English ?bank?
(financial institution or river-side land),?
Malay ?kabinet?
(governmental Cabinet orhousehold furniture),?
Malay ?mangga?
(mango or padlock),?
Chinese ???
(g?, valley or grain) and?
Iban ?emperaja?
(rainbow or lover).Each test sentence was first POS-tagged auto-matically based on the Penn Treebank tagset.
TheEnglish test sentences were lemmatised and POS-tagged with the Stanford Parser.4 The Chinese testsentences segmented with the Stanford ChineseWord Segmenter tool.5 For Malay POS-tagging,we trained the QTag tagger6 on a hand-taggedMalay corpus, and applied the trained tagger on ourtest sentences.
As we lacked a Iban POS-tagger,the Iban test sentences were tagged by hand.
LIsof each language and their associated vectors canthen be retrieved from the multilingual lexicon.The prototype tool LEXICALSELECTOR thencomputes the CSim score and ranks potential trans-lation sets for each LI in the input sentences(ranking strategy wiki-lsi).
The baseline strat-egy (base-freq) selects the translation set whosemembers occur most frequently in the bilingualWikipedia corpus.As a comparison, the English, Chinese andMalay test sentences were fed to Google Trans-late7 and translated into Chinese, Malay and En-glish.
(Google Translate does not support Ibancurrently.)
The Google Translate interface makesavailable the ranked list of translation candidatesfor each word in an input sentence, one language4http://www-nlp.stanford.edu/software/lex-parser.shtml5http://nlp.stanford.edu/software/segmenter.shtml6http://phrasys.net/uob/om/software7http://translate.google.com on 3 October 2012at a time.The translated word for each of the inputtest word can therefore be noted.
The highest rankof the correct translation for the test words in En-glish/Chinese/Malay are used to evaluate goog-tr.Two metrics were used in this quick evaluation.The first metric is by taking the precision of the firsttranslation set returned by each ranking strategy,i.e.
whether the top ranked translation set containsthe correct translation of the ambiguous item.
Theprecision metric is important for applications likemachine translation, where only the top-rankedmeaning or translation is considered.The results may also be evaluated similar to adocument retrieval task, i.e.
as a ranked lexicallookup for human consumption.
This is measuredby the mean reciprocal rank (MRR), the averageof the reciprocal ranks of the correct translation setfor each input sentence in the test set T :MRR = 1|T ||T |?i=11rankiThe results for the three ranking strategies aresummarised in Table 1.
For the precision metric,wiki-lsi scored 0.650 when all 80 input sen-tences are tested, while the base-freq baselinescored 0.550. goog-tr has the highest precisionat 0.797.
However, if only the Chinese and Malayinputs ?
which has less presence on the Inter-net and ?less resource-rich?
than English ?
weretested (since goog-tr cannot accept Iban inputs),wiki-lsi and goog-tr actually performs equallywell at 0.690 precision.In our evaluation, the MRR score of wiki-lsiis 0.810, while base-freq scored 0.771.wiki-lsi even outperforms goog-tr whenonly the Chinese and Malay test sentences areconsidered for the MRR metric, as goog-tr297Table 1: Precision and MRR scores of context-dependent lexical lookupIncl.
Eng.
& Iban W/o Eng.
& IbanStrategy Precision MRR Precision MRRwiki-lsi 0.650 0.810 0.690 0.845base-freq 0.550 0.771 0.524 0.762goog-tr 0.797 0.812 0.690 0.708did not present the correct translation in its listof alternative translation candidates for sometest sentences.
This suggests that the LSI-basedtranslation context knowledge vectors would behelpful in building an intelligent reading aid.7 Discussionwiki-lsi performed better than base-freq forboth the precision and the MRR metrics, althoughfurther tests is warranted, given the small size ofthe current test set.
While wiki-lsi is not yetsufficiently accurate to be used directly in an MTsystem, it is helpful in producing a list of rankedmultilingual translation sets depending on the inputcontext, as part of an intelligent reading aid.
Specif-ically, the lookup module would have benefited ifsyntactic information (e.g.
syntactic relations andparse trees) was incorporated during the trainingand testing phase.
This would require more timein parsing the training corpus, as well as assumingthat syntactic analysis tools are available to pro-cess test sentences of all languages, including theunder-resourced ones.Note that even though the translation contextknowledge vectors were extracted from an English?Malay corpus, the same vectors can be applied onChinese and Iban input sentences as well.
Thisis especially significant for Iban, which otherwiselacks resources from which a lookup or disambigua-tion tool can be trained.
Translation context knowl-edge vectors mined via LSI from a bilingual com-parable corpus, therefore offers a fast, low cost andefficient fallback strategy for acquiring multilin-gual translation equivalence context information.8 Related WorkBasile and Semeraro (2010) also used Wikipediaarticles as a parallel corpus for their participationin the SemEval 2010 Cross-Lingual Lexical Sub-stitution task.
Both training and test data were forEnglish?Spanish.
The idea behind their systemis to count, for each potential Spanish candidate,the number of documents in which the target En-glish word and the Spanish candidate occurs inan English?Spanish document pair.
In the task?s?best?
evaluation (which is comparable to our ?Preci-sion?
metric), Basile and Semeraro?s system scored26.39 precision on the trial data and 19.68 preci-sion on the SemEval test data.
This strategy ofselecting the most frequent translation is similar toour base-freq baseline strategy.Sarrafzadeh et al (2011) also tackled the prob-lem of cross-lingual disambiguation for under-resourced language pairs (English?Persian) usingWikipedia articles, by applying the one sense percollocation and one sense per discourse heuristicson a comparable corpus.
The authors incorporatedEnglish and Persian wordnets in their system, thusachieving 0.68 for the ?best sense?
(?Precision?)evaluation.
However, developing wordnets for newlanguages is no trivial effort, as acknowledged bythe authors.9 ConclusionWe extracted translation context knowledge from abilingual comparable corpus by running LSI on thecorpus.
A context-dependent multilingual lexicallookup module was implemented, using the cosinesimilarity score between the vector of the inputsentence and those of candidate translation sets torank the latter in order of relevance.
The precisionand MRR scores outperformed Google Translate?slexical selection for medium- and under-resourcedlanguage test inputs.
The LSI-backed translationcontext knowledge vectors, mined from bilingualcomparable corpora, thus provide an fast and af-fordable data source for building intelligent readingaids, especially for under-resourced languages.AcknowledgmentsThe authors thank Multimedia University and Uni-versiti Malaysia Sarawak for providing support andresources during the conduct of this study.
We alsothank Panceras Talita for helping to prepare theIban test sentences for context-dependent lookup.A 3-Letter ISO Language CodesCode Language Code Languageeng English msa Malayzho Chinese fra Frenchtha Thai iba Iban298ReferencesSatanjeev Banerjee and Ted Pedersen.
2003.
Extendedgloss overlaps as a measure of semantic relatedness.In Proceedings of the 18th International Joint Con-ference on Artificial Intelligence, pages 805?810.Pierpaolo Basile and Giovanni Semeraro.
2010.
UBA:Using automatic translation and Wikipedia for cross-lingual lexical substitution.
In Proceedings of the5th International Workshop on Semantic Evaluation(SemEval 2010), pages 242?247, Uppsala, Sweden.Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society for Information Science,41(6):391?407.Susan T. Dumais, Michael L. Littman, and Thomas K.Landauer.
1997.
Automatic cross-language re-trieval using latent semantic indexing.
In AAAI97Spring Symposium Series: Cross Language Text andSpeech Retrieval, pages 18?24, Stanford University.Nancy Ide, Tomaz Erjavec, and Dan Tufis?.
2002.Sense discrimination with parallel corpora.
In Pro-ceedings of the SIGLEX/SENSEVAL Workshop onWord Sense Disambiguation: Recent Successes andFuture Directions, pages 54?60, Philadelphia, USA.Els Lefever and V?ronique Hoste.
2010.
SemEval-2010 Task 3: Cross-lingual word sense disambigua-tion.
In Proceedings of the 5th International Work-shop on Semantic Evaluation (SemEval 2010), Upp-sala, Sweden.Hang Li and Cong Li.
2004.
Word translation disam-biguation using bilingual bootstrapping.
Computa-tional Linguistics, 30(1):1?22.Lian Tze Lim, Bali Ranaivo-Malan?on, and Enya KongTang.
2011.
Low cost construction of a multilinguallexicon from bilingual lists.
Polibits, 43:45?51.Bernardo Magnini, Carlo Strapparava, Giovanni Pez-zulo, and Alfio Gliozzo.
2001.
Using domaininformation for word sense disambiguation.
InProceedings of the 2nd International Workshop onEvaluating Word Sense Disambiguation Systems(SENSEVAL-2), pages 111?114, Toulouse, France.Lipta Mahapatra, Meera Mohan, Mitesh M. Khapra,and Pushpak Bhattacharyya.
2010.
OWNS: Cross-lingual word sense disambiguation using weightedoverlap counts and Wordnet based similarity mea-sures.
In Proceedings of the 5th International Work-shop on Semantic Evaluation (SemEval 2010), Upp-sala, Sweden.Rada Mihalcea, Ravi Sinha, and Diana McCarthy.2010.
SemEval-2010 Task 2: Cross-lingual lexicalsubstitution.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation (SemEval 2010),Uppsala, Sweden.Hwee Tou Ng, Bin Wang, and Yee Seng Chan.
2003.Exploiting parallel texts for word sense disambigua-tion: An empirical study.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics, pages 455?462, Sapporo, Japan.Gyula Papp.
2009.
Vector-based unsupervised wordsense disambiguation for large number of contexts.In V?clav Matou?ek and Pavel Mautner, editors,Text, Speech and Dialogue, volume 5729 of Lec-ture Notes in Computer Science, pages 109?115.Springer Berlin Heidelberg.Bahareh Sarrafzadeh, Nikolay Yakovets, Nick Cercone,and Aijun An.
2011.
Cross-lingual word sense dis-ambiguation for languages with scarce resources.
InProceedings of the 24th Canadian Conference onAdvances in Artificial Intelligence, pages 347?358,St.
John?s, Canada.Hinrich Sch?tze.
1998.
Automatic word sense discrim-ination.
Computational Linguistics, 24(1):97?123.Kiyoaki Shirai and Tsunekazu Yagi.
2004.
Learn-ing a robust word sense disambiguation model us-ing hypernyms in definition sentences.
In Proceed-ings of the 20th International Conference on Com-putational Linguistics (COLING 2004), pages 917?923, Geneva, Switzerland.
Association for Compu-tational Linguistics.Ming Zhou, Yuan Ding, and Changning Huang.
2001.Improviging translation selection with a new transla-tion model trained by independent monolingual cor-pora.
Computational Linguistics and Chinese lan-guage Processing, 6(1):1?26.299
