Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 189?192, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSemantic Role Lableing System using Maximum Entropy Classier ?Ting Liu, Wanxiang Che, Sheng Li, Yuxuan Hu and Huaijun LiuInformation Retrieval LabSchool of Computer Science and TechnologyHarbin Institute of TechnologyChina, 150001{tliu, car, ls, yxhu, hjliu}@ir.hit.edu.cnAbstractA maximum entropy classifier is used inour semantic role labeling system, whichtakes syntactic constituents as the labelingunits.
The maximum entropy classifier istrained to identify and classify the predi-cates?
semantic arguments together.
Onlythe constituents with the largest probabil-ity among embedding ones are kept.
Af-ter predicting all arguments which havematching constituents in full parsing trees,a simple rule-based post-processing is ap-plied to correct the arguments which haveno matching constituents in these trees.Some useful features and their combina-tions are evaluated.1 IntroductionThe semantic role labeling (SRL) is to assign syn-tactic constituents with semantic roles (arguments)of predicates (most frequently verbs) in sentences.A semantic role is the relationship that a syntacticconstituent has with a predicate.
Typical semanticarguments include Agent, Patient, Instrument, etc.and also adjunctive arguments indicating Locative,Temporal, Manner, Cause, etc.
It can be used inlots of natural language processing application sys-tems in which some kind of semantic interpretationis needed, such as question and answering, informa-tion extraction, machine translation, paraphrasing,and so on.
?This research was supported by National Natural ScienceFoundation of China via grant 60435020Last year, CoNLL-2004 hold a semantic role la-beling shared task (Carreras and Ma`rquez, 2004)to test the participant systems?
performance basedon shallow syntactic parser results.
In 2005, SRLshared task is continued (Carreras and Ma`rquez,2005), because it is a complex task and now it isfar from desired performance.In our SRL system, we select maximum en-tropy (Berger et al, 1996) as a classifier to im-plement the semantic role labeling system.
Dif-ferent from the best classifier reported in litera-tures (Pradhan et al, 2005) ?
support vector ma-chines (SVMs) (Vapnik, 1995), it is much eas-ier for maximum entropy classifier to handle themulti-class classification problem without additionalpost-processing steps.
The classifier is much fasterthan training SVMs classifiers.
In addition, max-imum entropy classifier can be tuned to minimizeover-fitting by adjusting gaussian prior.
Xue andPalmer (2004; 2005) and Kwon et al (2004) haveapplied the maximum entropy classifier to semanticrole labeling task successfully.In the following sections, we will describe oursystem and report our results on development andtest sets.2 System Description2.1 Constituent-by-ConstituentWe use syntactic constituent as the unit of labeling.However, it is impossible for each argument to findits matching constituent in all auto parsing trees.
Ac-cording to statistics, about 10% arguments have nomatching constituents in the training set of 245,353189constituents.
The top five arguments with no match-ing constituents are shown in Table 1.
Here, Char-niak parser got 10.08% no matching arguments andCollins parser got 11.89%.Table 1: The top five arguments with no matchingconstituents.Args Cha parser Col parser BothAM-MOD 9179 9205 9153A1 5496 7273 3822AM-NEG 3200 3217 3185AM-DIS 1451 1482 1404A0 1416 2811 925Therefore, we can see that Charniak parser got abetter result than Collins parser in the task of SRL.So we use the full analysis results created by Char-niak parser as our classifier?s inputs.
Assume thatwe could label all AM-MOD and AM-NEG argumentscorrectly with simple post processing rules, the up-per bound of performance could achieve about 95%recall.At the same time, we can see that for some ar-guments, both parsers got lots of no matchings suchas AM-MOD, AM-NEG, and so on.
After analyzingthe training data, we can recognize that the perfor-mance of these arguments can improve a lot afterusing some simple post processing rules only, how-ever other arguments?
no matching are caused pri-marily by parsing errors.
The comparison betweenusing and not using post processing rules is shownin Section 3.2.Because of the high speed and no affection in thenumber of classes with efficiency of maximum en-tropy classifier, we just use one stage to label all ar-guments of predicates.
It means that the ?NULL?tag of constituents is regarded as a class like ?ArgN?and ?ArgM?.2.2 FeaturesThe following features, which we refer to as thebasic features modified lightly from Pradhan etal.
(2005), are provided in the shared task data foreach constituent.?
Predicate lemma?
Path: The syntactic path through the parse tree from theparse constituent to the predicate.?
Phrase type?
Position: The position of the constituent with respect toits predicate.
It has two values, ?before?
and ?after?,for the predicate.
For the situation of ?cover?, we usea heuristic rule to ignore all of them because there is nochance for them to become an argument of the predicate.?
Voice: Whether the predicate is realized as an active orpassive construction.
We use a simple rule to recognizepassive voiced predicates which are labeled with part ofspeech ?
VBN and sequences with AUX.?
Head word stem: The stemming result of the con-stituent?s syntactic head.
A rule based stemming algo-rithm (Porter, 1980) is used.
Collins Ph.D thesis (Collins,1999)[Appendix.
A] describs some rules to identify thehead word of a constituent.
Especially for prepositionalphrase (PP) constituent, the normal head words are notvery discriminative.
So we use the last noun in the PPreplacing the traditional head word.?
Sub-categorizationWe also use the following additional features.?
Predicate POS?
Predicate suffix: The suffix of the predicate.
Here, weuse the last 3 characters as the feature.?
Named entity: The named entity?s type in the constituentif it ends with a named entity.
There are four types: LOC,ORG, PER and MISC.?
Path length: The length of the path between a constituentand its predicate.?
Partial path: The part of the path from the constituentto the lowest common ancestor of the predicate and theconstituent.?
Clause layer: The number of clauses on the path betweena constituent and its predicate.?
Head word POS?
Last word stem: The stemming result of the last word ofthe constituent.?
Last word POSWe also use some combinations of the above fea-tures to build some combinational features.
Lots ofcombinational features which were supposed to con-tribute the SRL task of added one by one.
At thesame time, we removed ones which made the per-formance decrease in practical experiments.
At last,we keep the following combinations:?
Position + Voice?
Path length + Clause layer?
Predicate + Path?
Path + Position + Voice?
Path + Position + Voice + Predicate?
Head word stem + Predicate?
Head word stem + Predicate + Path?
Head word stem + Phrase?
Clause layer + Position + PredicateAll of the features and their combinations are usedwithout feature filtering strategy.1902.3 ClassifierLe Zhang?s Maximum Entropy Modeling Toolkit 1,and the L-BFGS parameter estimation algorithmwith gaussian prior smoothing (Chen and Rosenfeld,1999) are used as the maximum entropy classifier.We set gaussian prior to be 2 and use 1,000 itera-tions in the toolkit to get an optimal result throughsome comparative experiments.2.4 No EmbeddingThe system described above might label two con-stituents even if one embeds in another, which is notallowed by the SRL rule.
So we keep only one ar-gument when more arguments embedding happens.Because it is easy for maximum entropy classifier tooutput each prediction?s probability, we can label theconstituent which has the largest probability amongthe embedding ones.2.5 Post Processing StageAfter labeling the arguments which are matchedwith constituents exactly, we have to handle the ar-guments, such as AM-MOD, AM-NEG and AM-DIS,which have few matching with the constituents de-scribed in Section 2.1.
So a post processing is givenby using some simply rules:?
Tag target verb and successive particles as V.?
Tag ?not?
and ?n?t?
in target verb chunk as AM-NEG.?
Tag modal verbs in target verb chunk, such as words withPOS of ?MD?, ?going to?, and so on, as AM-MOD.?
Tag the words with POS of ?CC?
and ?RB?
at the start ofa clause which include the target verb as AM-DIS.3 Experiments3.1 Data and Evaluation MetricsThe data provided for the shared task is a part ofPropBank corpus.
It consists of the sections fromthe Wall Street Journal part of Penn Treebank.
Sec-tions 02-21 are training sets, and Section 24 is devel-opment set.
The results are evaluated for precision,recall and F?=1 numbers using the srl-eval.pl scriptprovided by the shared task organizers.3.2 Post ProcessingAfter using post processing rules, the final F?=1 isimproved from 71.02% to 75.27%.1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html3.3 Performance CurveBecause the training corpus is substantially en-larged, this allows us to test the scalability oflearning-based SRL systems to large data set andcompute learning curves to see how many data arenecessary to train.
We divide the training set, 20sections Penn Treebank into 5 parts with 4 sectionsin each part.
There are about 8,000 sentences in eachpart.
Figure 1 shows the change of performance asa function of training set size.
When all of trainingdata are used, we get the best system performance asdescribed in Section 3.4.Figure 1: Our SRL system performance curve (ofF?=1) effecting of the training set size.We can see that as the training set becomes largerand larger, so does the performance of SRL system.However, the rate of increase slackens.
So we cansay that at present state, the larger training data hasfavorable effect on the improvement of SRL systemperformance.3.4 Best System ResultsIn all the experiments, all of the features and theircombinations described above are used in our sys-tem.
Table 2 presents our best system performanceon the development and test sets.From the test results, we can see that our systemgets much worse performance on Brown corpus thanWSJ corpus.
The reason is easy to be understoodfor the dropping of automatic syntactic parser per-formance on new corpus but WSJ corpus.The training time on PIV 2.4G CPU and 1G Memmachine is about 20 hours on all 20 sections, 39,832-191Precision Recall F?=1Development 79.65% 71.34% 75.27Test WSJ 80.48% 72.79% 76.44Test Brown 71.13% 59.99% 65.09Test WSJ+Brown 79.30% 71.08% 74.97Test WSJ Precision Recall F?=1Overall 80.48% 72.79% 76.44A0 88.14% 83.61% 85.81A1 79.62% 72.88% 76.10A2 73.67% 65.05% 69.09A3 76.03% 53.18% 62.59A4 78.02% 69.61% 73.58A5 100.00% 40.00% 57.14AM-ADV 59.85% 48.02% 53.29AM-CAU 68.18% 41.10% 51.28AM-DIR 56.60% 35.29% 43.48AM-DIS 76.32% 72.50% 74.36AM-EXT 83.33% 46.88% 60.00AM-LOC 65.31% 52.89% 58.45AM-MNR 58.28% 51.16% 54.49AM-MOD 98.52% 96.37% 97.43AM-NEG 97.79% 96.09% 96.93AM-PNC 43.68% 33.04% 37.62AM-PRD 50.00% 20.00% 28.57AM-REC 0.00% 0.00% 0.00AM-TMP 78.38% 66.70% 72.07R-A0 81.70% 85.71% 83.66R-A1 77.62% 71.15% 74.25R-A2 60.00% 37.50% 46.15R-A3 0.00% 0.00% 0.00R-A4 0.00% 0.00% 0.00R-AM-ADV 0.00% 0.00% 0.00R-AM-CAU 100.00% 25.00% 40.00R-AM-EXT 0.00% 0.00% 0.00R-AM-LOC 83.33% 47.62% 60.61R-AM-MNR 66.67% 33.33% 44.44R-AM-TMP 77.27% 65.38% 70.83V 98.71% 98.71% 98.71Table 2: Overall results (top) and detailed results onthe WSJ test (bottom).sentences training set with 1,000 iterations and morethan 1.5 million samples and 2 million features.The predicting time is about 160 seconds on 1,346-sentences development set.4 ConclusionsWe have described a maximum entropy classifieris our semantic role labeling system, which takessyntactic constituents as the labeling units.
Thefast training speed of the maximum entropy clas-sifier allows us just use one stage of argumentsidentification and classification to build the system.Some useful features and their combinations areevaluated.
Only the constituents with the largestprobability among embedding ones are kept.
Af-ter predicting all arguments which have matchingconstituents in full parsing trees, a simple rule-based post-processing is applied to correct the ar-guments which have no matching constituents.
Theconstituent-based method depends much on the syn-tactic parsing performance.
The comparison be-tween WSJ and Brown test sets results fully demon-strates the point of view.ReferencesAdam L. Berger, Stephen A. Della Pietra, and Vincent J.Della Pietra.
1996.
A maximum entropy approach tonatural language processing.
Computational Linguis-tics, 22(1):39?71.Xavier Carreras and Llu?
?s Ma`rquez.
2004.
Introductionto the conll-2004 shared task: Semantic role labeling.In Proceedings of CoNLL-2004, pages 89?97, Boston,MA, USA.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introductionto the CoNLL-2005 Shared Task: Semantic Role La-beling.
In Proceedings of CoNLL-2005.Stanley F. Chen and Ronald Rosenfeld.
1999.
A gaussianprior for smoothing maximum entropy models.
Tech-nical Report CMU-CS-99-108.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Pennsyl-vania University.Namhee Kwon, Michael Fleischman, and Eduard Hovy.2004.
Framenet-based semantic parsing using maxi-mum entropy models.
In Proc.
Coling 2004.Martin Porter.
1980.
An algorithm for suffix stripping.Program, 14(3).Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, WayneWard, James H. Martin, and Daniel Jurafsky.
2005.Support vector learning for semantic argument classi-fication.
Machine Learning Journal.Vladamir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer-Verlag, Berlin.Nianwen Xue and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
In Proc.
EMNLP2004.Nianwen Xue and Martha Palmer.
2005.
Automatic se-mantic role labeling for chinese verbs.
In Proc.
IJCAI2005.192
