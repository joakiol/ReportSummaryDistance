Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1535?1545,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsExploiting Source-side Monolingual Data in Neural Machine TranslationJiajun Zhang?
and Chengqing Zong??
?University of Chinese Academy of Sciences, Beijing, ChinaNational Laboratory of Pattern Recognition, CASIA, Beijing, China?CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai, China{jjzhang,cqzong}@nlpr.ia.ac.cnAbstractNeural Machine Translation (NMT) based onthe encoder-decoder architecture has recentlybecome a new paradigm.
Researchers haveproven that the target-side monolingual datacan greatly enhance the decoder model ofNMT.
However, the source-side monolingualdata is not fully explored although it shouldbe useful to strengthen the encoder model ofNMT, especially when the parallel corpus isfar from sufficient.
In this paper, we proposetwo approaches to make full use of the source-side monolingual data in NMT.
The first ap-proach employs the self-learning algorithm togenerate the synthetic large-scale parallel datafor NMT training.
The second approach ap-plies the multi-task learning framework usingtwo NMTs to predict the translation and thereordered source-side monolingual sentencessimultaneously.
The extensive experimentsdemonstrate that the proposed methods ob-tain significant improvements over the strongattention-based NMT.1 IntroductionNeural Machine Translation (NMT) following theencoder-decoder architecture proposed by (Kalch-brenner and Blunsom, 2013; Cho et al, 2014) hasbecome the novel paradigm and obtained state-of-the-art translation quality for several language pairs,such as English-to-French and English-to-German(Sutskever et al, 2014; Bahdanau et al, 2014; Lu-ong et al, 2015b; Sennrich et al, 2015).
This end-to-end NMT typically consists of two recurrent neu-ral networks.
The encoder network maps the sourcesentence of variable length into the context vectorrepresentation; and the decoder network generatesthe target translation word by word starting from thecontext vector.Currently, most NMT methods utilize only thesentence aligned parallel corpus for model train-ing, which limits the capacity of the model.
Re-cently, inspired by the successful application of tar-get monolingual data in conventional statistical ma-chine translation (SMT) (Koehn et al, 2007; Chi-ang, 2007), Gulcehre et al (2015) and Sennrichet al (2015) attempt to enhance the decoder net-work model of NMT by incorporating the target-side monolingual data so as to boost the transla-tion fluency.
They report promising improvementsby using the target-side monolingual data.
In con-trast, the source-side monolingual data is not fullyexplored.
Luong et al (2015a) adopt a simpleautoencoder or skip-thought method (Kiros et al,2015) to exploit the source-side monolingual data,but no significant BLEU gains are reported.
Notethat, in parallel to our efforts, Cheng et al (2016b)have explored the usage of both source and targetmonolingual data using a similar semi-supervisedreconstruction method, in which two NMTs are em-ployed.
One translates the source-side monolingualdata into target translations, and the other recon-structs the source-side monolingual data from thetarget translations.In this work, we investigate the usage of thesource-side large-scale monolingual data in NMTand aim at greatly enhancing its encoder network sothat we can obtain high quality context vector rep-resentations.
To achieve this goal, we propose two1535approaches.
Inspired by (Ueffing et al, 2007; Wuet al, 2008) handling source-side monolingual cor-pus in SMT and (Sennrich et al, 2015) exploitingtarget-side monolingual data in NMT, the first ap-proach adopts the self-learning algorithm to gener-ate adequate synthetic parallel data for NMT train-ing.
In this method, we first build the baseline ma-chine translation system with the available alignedsentence pairs, and then obtain more synthetic par-allel data by translating the source-side monolingualsentences with the baseline system.The proposed second approach applies the multi-task learning framework to predict the target trans-lation and the reordered source-side sentences atthe same time.
The main idea behind is that webuild two NMTs: one is trained on the aligned sen-tence pairs to predict the target sentence from thesource sentence, while the other is trained on thesource-side monolingual corpus to predict the re-orderd source sentence from original source sen-tences1.
It should be noted that the two NMTs sharethe same encoder network so that they can help eachother to strengthen the encoder model.In this paper, we make the following contribu-tions:?
To fully investigate the source-side monolin-gual data in NMT, we propose and comparetwo methods.
One attempts to enhance the en-coder network of NMT by producing rich syn-thetic parallel corpus using a self-learning algo-rithm, and the other tries to perform machinetranslation and source sentence reordering si-multaneously with a multi-task learning archi-tecture.?
The extensive experiments on Chinese-to-English translation show that our proposedmethods significantly outperform the strongNMT baseline augmented with the attentionmechanism.
We also find that the usage of thesource-side monolingual data in NMT is moreeffective than that in SMT.
Furthermore, wefind that more monolingual data does not al-ways improve the translation quality and onlyrelevant monolingual data helps.1We reorder all the source-side monolingual sentences so asto make them close to target language in word order.start ???1???1????
?1?1?1 ?2 ??????????2?2?1?1??
?1 ?
?2 ??????encoderdecoderattention???
?Figure 1: The encoder-decoder NMT with attention.2 Neural Machine TranslationOur approach on using source-side monolingual cor-pora can be applied in any neural machine trans-lation as long as it employs the encoder-decoderframework.
Without loss of generality, we use theattention-based NMT proposed by (Bahdanau et al,2014), which utilizes recurrent neural networks forboth encoder and decoder as illustrated in Fig.
1.The encoder-decoder NMT first encodes thesource sentence X = (x1, x2, ?
?
?
, xTx) into a se-quence of context vectors C = (h1,h2, ?
?
?
,hTx)whose size varies with respect to the source sen-tence length.
Then, the encoder-decoder NMT de-codes from the context vectors C and generates tar-get translation Y = (y1, y2, ?
?
?
, yTy) one word eachtime by maximizing the probability of p(yi|y<i, C).Note that xj (yi) is word embedding correspondingto the jth (ith) word in the source (target) sentence.Next, we briefly review the encoder introducing howto obtain C and the decoder addressing how to cal-culate p(yi|y<i, C).Encoder: The context vectors C are generatedby the encoder using a pair of recurrent neural net-works (RNN) which consists of a forward RNNand a backward RNN.
The forward RNN operatesleft-to-right over the source sentence from the firstword, resulting in the forward context vectors Cf =(?
?h 1,?
?h 2, ?
?
?
,?
?h Tx), in which?
?h j = RNN(?
?h j?1, xj) (1)?
?h j can be calculated similarly.1536RNN can be a Gated Recurrent Unit (GRU) (Choet al, 2014) or a Long Short-Term Memory Unit(LSTM) (Hochreiter and Schmidhuber, 1997).
Ateach position j of the source sentence, the contextvector hj is defined as the concatenation of the for-ward and backward context vectors.Decoder: The conditional probabilityp(yi|y<i, C) is computed in different ways ac-cording to the choice of the context C at time i.
In(Cho et al, 2014), the authors choose C = hTx ,while Bahdanau et al (2014) use different context ciat different time step and the conditional probabilitywill become:p(yi|y<i, C) = p(yi|y<i, ci) = g(yi?1, zi, ci) (2)where zi is the ith hidden state of the decoder andis calculated conditioning on the previous hiddenstate zi?1, previous output yi?1 and and the sourcecontext vector ci at time i:zi = RNN(zi?1, yi?1, ci) (3)In attention-based NMT, ci is computed as theweighted sum of the source-side context vectors, justas illustrated in the top half of Fig.
1.All the parameters of the encoder-decoder NMTare optimized to maximize the following condi-tional log-likelihood of the sentence aligned bilin-gual data:L(?)
= 1NN?n=1Ty?i=1logp(y(n)i |y(n)<i , X(n), ?)
(4)3 Incorporating Source-side MonolingualData in NMTWe can see from the above objective function that allthe network parameters are only optimized on thesentence aligned parallel corpus.
It is well knownthat more related data of high quality leads to betterand more robust network models.
However, bilin-gual data is scarce in many languages (or domains).It becomes a key issue how to improve the encoderand decoder networks using other data besides theparallel sentence pairs.
Gulcehre et al (2015) andSennrich et al (2015) have tried to fine-tune thedecoder neural network with target-side large-scalemonolingual data and they report remarkable perfor-mance improvement with the enhanced decoder.
Incontrast, we believe that the encoder part of NMTcan also be greatly strengthened with the source-sidemonolingual data.To investigate fully the source-side monolingualdata in improving the encoder network of NMT, wepropose two approaches: the first one employs theself-learning algorithm to provide synthetic paralleldata in which the target part is obtained through au-tomatically translating the source-side monolingualdata, which we refer to as self-learning method.
Thesecond one applies the multi-task learning frame-work that consists of two NMTs sharing the sameencoder network to simultaneously train one NMTmodel on bilingual data and the other sentencereordering NMT model2 on source-side monolin-gual data, which we refer to as sentence reorderingmethod.3.1 Self-learning MethodGiven the sentence aligned bitext Db ={(X(n)b , Y(n)b )}Nn=1 in which N is not big enough,we have the source-side large-scale monolingualdata Dsm = {Xmsm}Mm=1 which is related to thebitext and M  N .Our goal is to generate much more bilingual datausingDb andDsm.
From the view of machine learn-ing, we are equipped with some labelled dataDb andplenty of unlabelled data Dsm, and we aim to obtainmore labelled data for training better models.
Self-learning is a simple but effective algorithm to tacklethis issue.
It first establishes a baseline with labelleddata and then adopts the baseline to predict the la-bels of the unlabelled data.
Finally, the unlabelleddata together with the predicted labels become newlabelled data.In our scenario, the self-learning algorithm per-form the following three steps .
First, a baseline ma-chine translation (MT) system (can use any transla-tion model, SMT or NMT) is built with the givenbilingual data Db.
Second, the baseline MT sys-2NMT is essentially a sequence-to-sequence predictionmodel.
In most cases, the input sequence is different from theoutput sequence.
In the sentence reordering NMT, we requirethat output sequence to be the reordered input sentences whichare close to English word order.1537?1 ?2 ?????????
?2?2?1?1 ???
?1 ?
?2 ???????
1?
2?
?
?1 ?2 ??????
?2 ?1 ?reordering  translationreordered source - side monolingual data  s entence aligned bilingual dataFigure 2: Multi-task learning framework to use source-sidemonolingual data in NMT, which includes a translation modeland a sentence reordering model.tem automatically translates the source-side mono-lingual sentences Dsm into target translations Dtt ={(Y mtt )}Mm=1, and further pairs Dsm with Dtt re-sulting in the synthetic parallel corpus Dsyn ={(Xmsm, Y mtt )}Mm=1.
Third, the synthetic parallel cor-pus Dsyn plus the original bitext Db are combinedtogether to train the new NMT model.In principle, we can apply any MT system as thebaseline to generate the synthetic bilingual data.
Inaccordance with the translation model we focus onin this work, we employ NMT as the baseline MTsystem.
Note that the synthetic target parts may neg-atively influence the decoder model of NMT.
To ad-dress this problem, we can distinguish original bitextfrom the synthetic bilingual sentences during NMTtraining by freezing the parameters of the decodernetwork for the synthetic data.It is worthy to discuss why self-learning algo-rithm can improve the encoder model of NMT.
Eventhough we requireDsm to share the same source lan-guage vocabulary as Db and no new word transla-tions can be generated, the source-side monolingualdata provides much more permutations of words inthe vocabulary.
Our RNN encoder network modelwill be optimized to well explain all of the word per-mutations.
Thus, the encoder model of NMT can beenhanced for better generalization.3.2 Sentence Reordering MethodThe self-learning algorithm needs to translate firstthe large-scale source-side monolingual data.
A nat-ural question arises that whether can we improvethe encoder model of NMT using just source-sidemonolingual corpora rather than the synthetic par-allel data.
Luong et al (2015a) attempt to lever-age source-side monolingual data in NMT using asimple autoencoder and skip-thought vectors.
How-ever, no promising results are reported.
We believethat the reason lies in two aspects: 1) the large-scalemonolingual data is not carefully selected; and 2)the adopted model is relatively simple.
In this work,we propose to apply the multi-task learning methodwhich designs a parameter sharing neural networkframework to perform two tasks: machine transla-tion and source sentence reordering.
Fig.2 illus-trates the overview of our framework for source-sidemonolingual data usage.As shown in Fig.
2, our framework consists oftwo neural networks that shares the same encodermodel but employs two different decoder models formachine translation and sentence reordering respec-tively.
For the machine translation task trained onthe sentence aligned parallel data Db, the networkparameters are optimized to maximize the condi-tional probability of the target sentence Y (n)b given asource sentenceX(n)b , namely argmaxp(Y (n)b |X(n)b ).As for the sentence reordering task trained onsource-side monolingual data Dsm, we regard it as aspecial machine translation task in which the targetoutput is just the reordered source sentence, Y (m)sm =X?
(m)sm .
X ?
(m)sm is obtained from X(m)sm by using thepre-ordering rules proposed by (Wang et al, 2007),which can permutate the words of the source sen-tence so as to approximate the target language wordorder3.
In this way, the sentence reordering NMT ismore powerful than an autoencoder.
Using the NMTparadigm, the shared encoder network is leveragedto learn the deep representation C(n)sm of the sourcesentenceX(n)sm , and the decoder network is employedto predict the reordered source sentence from thedeep representation C(n)sm (here X(n)sm ?
Dsm) bymaximizing p(X ?
(n)sm |X(n)sm ).
Note that the above two3The pre-ordering rules are obtained from the parsed sourcetrees which heavily depend on the accuracy and efficiency of theparser.
In fact, it takes us lots of time (even longer than syntheticparallel data generation) to parse all the source-side monolin-gual data.
In the future, we attempt to design a more efficientpre-ordering method relying only on the bilingual training data.1538tasks share the same encoder model to obtain the en-coding of the source sentences.
Accordingly, theoverall objective function of this multi-task learn-ing is the summation of log probabilities of machinetranslation and sentence reordering:L(?)
= 1NN?n=1Ty?i=1logp(y(n)i |y(n)<i , X(n), ?
)+ 1MM?m=1TX?i=1logp(X?
(m)i |X?
(m)<i , X(m), ?
)(5)where (?
= ?enc, ?decT , ?decR).
?enc is the param-eter collection of source language encoder network,?decT denotes the parameter set of the decoder net-work for translation, and ?decR represents the param-eters of the decoder network for sentence reordering.Intuitively, the sentence reordering task is easierthan the translation task.
Furthermore, in this paper,we pay much more attention on the translation taskcompared to the sentence reordering task.
Consider-ing these, we distinguish these two tasks during theparameter optimization process.
It is performed us-ing an alternate iteration strategy.
For each iteration,we first optimize the encoder-decoder network pa-rameters in the reordering task for one epoch.
Thelearnt encoder network parameters are employed toinitialize the encoder model for the translation task.Then, we learn the encoder-decoder network param-eters in the translation task for several epochs4.
Thenew encoder parameters are then used to initializethe encoder model for the reordering task.
We con-tinue the iteration until the constraint (e.g.
iterationnumber or no parameter change) is satisfied.
Theweakness is that this method is less efficient than theself-learning approach.4 Experimental SettingsIn this section we describe the data set used in ourexperiments, data preprocessing, the training andevaluation details, and all the translation methods wecompare in experiments.4We rune four epochs for the translation task in each itera-tion.4.1 DatasetWe perform two tasks on Chinese-to-English trans-lation: one for small data set and the other forlarge-scale data set.
Our small training data in-cludes 0.63M sentence pairs (after data cleaning)extracted from LDC corpora5.
The large-scale dataset contains about 2.1M sentence pairs including thesmall training data.
For validation, we choose NIST2003 (MT03) dataset.
For testing, we use NIST2004 (MT04), NIST 2005 (MT05) and NIST 2006(MT06) datasets.
As for the source-side monolin-gual data, we collect about 20M Chinese sentencesfrom LDC and we retain the sentences in whichmore than 50% words should appear in the source-side portion of the bilingual training data, resultingin 6.5M monolingual sentences for small trainingdata set (12M for large-scale training data set) or-dered by the word hit rate.4.2 Data PreprocessingWe apply word-level translation in experiments.
TheChinese sentences are word segmented using Stan-ford Word Segmenter6.
To pre-order the Chinesesentences using the syntax-based reordering methodproposed by (Wang et al, 2007), we utilize theBerkeley parser (Petrov et al, 2006).
The Englishsentences are tokenized using the tokenizer scriptfrom the Moses decoder7.
To speed up the trainingprocedure, we clean the training data and remove allthe sentences of length over 50 words.
We limit thevocabulary in both Chinese and English to the most40K words and all the out-of-vocabulary words arereplaced with UNK.4.3 Training and Evaluation DetailsEach NMT model is trained on GPU K40 us-ing stochastic gradient decent algorithm AdaGrad(Duchi et al, 2011).
We use mini batch size of 32.The word embedding dimension of source and tar-get language is 500 and the size of hidden layer isset to 1024.
The training time for each model rangesfrom 5 days to 10 days for small training data set andranges from 8 days to 15 days for large training data5LDC2000T50, LDC2002L27, LDC2002T01,LDC2002E18, LDC2003E07, LDC2003E14, LDC2003T17,LDC2004T07.6http://nlp.stanford.edu/software/segmenter.shtml7http://www.statmt.org/moses/1539Method MT03 MT04 MT05 MT06Moses 30.30 31.04 28.19 30.04RNNSearch 28.38 30.85 26.78 29.27RNNSearch-Mono-SL (25%) 29.65 31.92 28.65 29.86RNNSearch-Mono-SL (50%) 32.43 33.16 30.43 32.35RNNSearch-Mono-SL (75%) 30.24 31.18 29.33 28.82RNNSearch-Mono-SL (100%) 29.97 30.78 26.45 28.06RNNSearch-Mono-MTL (25%) 31.68 32.51 29.8 31.29RNNSearch-Mono-MTL (50%) 33.38 34.30 31.57 33.40RNNSearch-Mono-MTL (75%) 31.69 32.83 28.17 30.26RNNSearch-Mono-MTL (100%) 30.31 30.62 27.23 28.85RNNSearch-Mono-Autoencoder (50%) 31.55 32.07 28.19 30.85RNNSearch-Mono-Autoencoder (100%) 27.81 30.32 25.84 27.73Table 1: Translation results (BLEU score) for different translation methods.
For our methods exploring the source-side monolingualdata, we investigate the performance change as we choose different scales of monolingual data (e.g.
from top 25% to 100%according to the word coverage of the monolingual sentence in source language vocabulary of bilingual training corpus).set8.
We use case-insensitive 4-gram BLEU score asthe evaluation metric (Papineni et al, 2002).4.4 Translation MethodsIn the experiments, we compare our method withconventional SMT model and a strong NMT model.We list all the translation methods as follows:?
Moses: It is the state-of-the-art phrase-basedSMT system (Koehn et al, 2007).
We use itsdefault configuration and train a 4-gram lan-guage model on the target portion of the bilin-gual training data.?
RNNSearch: It is an attention-based NMT sys-tem (Bahdanau et al, 2014).?
RNNSearch-Mono-SL: It is our NMT systemwhich makes use of the source-side large-scalemonolingual data by applying the self-learningalgorithm.?
RNNSearch-Mono-MTL: It is our NMT sys-tem that exploits the source-side monolingualdata by using our multi-task learning frame-work which performs machine translation andsentence reordering at the same time.8It needs another 5 to 10 days when adding millions ofmonolingual data.?
RNNSearch-Mono-Autoencoder: It also ap-plies the multi-task learning framework inwhich a simple autoencoder is adopted onsource-side monolingual data (Luong et al,2015a).5 Translation Results on Small DataFor translation quality evaluation, we attempt to fig-ure out four questions: 1) Can the source-side mono-lingual data improve the neural machine translation?2) Could the improved NMT outperform the state-of-the-art phrase-based SMT?
3) Whether it is truethat the more the source-side monolingual data thebetter the translation quality?
4) Which MT modelis more suitable to incorporate source-side monolin-gual data: SMT or NMT?5.1 Effects of Source-side Monolingual Data inNMTTable 1 reports the translation quality for differentmethods.
Comparing the first two lines in Table1, it is obvious that the NMT method RNNSearchperforms much worse than the SMT model Moseson Chinese-to-English translation.
The gap is aslarge as approximately 2.0 BLEU points (28.38 vs.30.30).
We speculate that the encoder-decoder net-work models of NMT are not well optimized due toinsufficient bilingual training data.The focus of this work is to figure out whether1540the encoder model of NMT can be improved usingsource-side monolingual data and further boost thetranslation quality.
The four lines (3-6 in Table 1)show the BLEU scores when applying self-learningalgorithm to incorporate the source-side monolin-gual data.
Clearly, RNNSearch-Mono-SL outper-forms RNNSearch in most cases.
The best perfor-mance is obtained if the top 50% monolingual data isused.
The biggest improvement is up to 4.05 BLEUpoints (32.43 vs. 28.38 on MT03) and it also signif-icantly outperforms Moses.When employing our multi-task learning frame-work to incorporate source-side monolingual data,the translation quality can be further improved(Lines 7-10 in Table 1).
For example, RNNSearch-Mono-MTL using the top 50% monolingual data canremarkably outperform the baseline RNNSearch,with an improvement up to 5.0 BLEU points (33.38vs.
28.38 on MT03).
Moreover, it also performssignificantly better than the state-of-the-art phrase-based SMT Moses by the largest gains of 3.38 BLEUpoints (31.57 vs. 28.19 on MT05).
The promis-ing results demonstrate that source-side monolin-gual data can improve neural machine translationand our multi-task learning is more effective.From the last two lines in Table 1, we can seethat RNNSearch-Mono-Autoencoder can also im-prove the translation quality by more than 1.0 BLEUpoints when using the most related monolingualdata.
However, it underperforms RNNSearch-Mono-MTL by a large gap.
It indicates that sentence re-ordering model is better than sentence reconstruc-tion model for exploiting the source-side monolin-gual data.Note that we sort the source-side monolingualdata according to the word coverage 9 in the bilin-gual training data.
Sentences in the front have moreshared words with the source-side vocabulary ofbilingual training data.
We can clearly see fromTable 1 that monolingual data cannot always im-prove NMT.
By adding closely related corpus (25%to 50%), the methods can achieve better and bet-ter performance.
However, when adding more unre-9In current work, the simple word coverage is applied toindicate the similarity.
In the future, we plan to use phrase em-bedding (Zhang et al, 2014) or sentence embedding (Zhang etal., 2015; Wang et al, 2016a; Wang et al, 2016b) to select therelevant monolingual data.0.0 0.2 0.4 0.6 0.8 1.0Top k*100% source-side monolingual data used29303132333435BLEUscore (%)Translation Quality on MT04MosesRNNSearchMoses_monoRNNSearch_SLRNNSearch_MTLFigure 3: Effects of source-side monolingual data on MT04.lated monolingual data (75% to 100%) which sharesfewer and fewer words in common with the bilin-gual data, the translation quality becomes worse andworse, and even worse than the baseline RNNSearch.Both self-learning algorithm RNNSearch-Mono-SLand multi-task learning framework RNNSearch-Mono-MTL have the same trend.
This indicates thatonly closely related source-side monolingual datacan lead to performance improvement.5.2 NMT vs. SMT on Using Source-sideMonolingual DataAlthough the proposed multi-task learning frame-work cannot fit SMT because of no shared deepinformation between the two tasks in SMT, self-learning algorithm can also be applied in SMT asdone by (Ueffing et al, 2007; Wu et al, 2008).
Wemay want to know whether NMT is more effectivein using source-side monolingual data than SMT.We apply the self-learning algorithm in SMT byincorporating top 25%, 50%, 75% and 100% syn-thetic sentence pairs to retrain baseline Moses.
Fig.3 shows the effect of source-side monolingual datain different methods on test set MT04.
The fig-ure reveals three similar phenomena.
First, relatedmonolingual data can boost the translation qualityno matter whether NMT or SMT is used, but mixingmore unrelated monolingual corpus will decreasethe performance.
Second, integrating closely relatedsource-side monolingual data in NMT (RNNSearch-SL and RNNSearch-MTL) is much more effectivethan that in SMT (e.g.
results for top 50%).
Itis because that SMT relies on the translation rules1541Method MT03 MT04 MT05 MT06RNNSearch 35.18 36.20 33.21 32.86RNNSearch-Mono-MTL (50%) 36.32 37.51 35.08 34.26RNNSearch-Mono-MTL (100%) 35.75 36.74 34.23 33.52Table 2: Translation results (BLEU score) for different translation methods in large-scale training data.learnt from the bilingual training data and the syn-thetic parallel data is obtained by these rules, andthus the synthetic parallel data cannot generate muchmore information.
In contrast, NMT provides aencoder-decoder mechanism and depends heavilyon the source language semantic vector representa-tions which facilitate the information sharing.
Third,the translation quality changes much more dramati-cally in NMT methods than that in SMT.
It indicatesthat the neural network models incline to be moreaffected by the quality of the training data.6 Translation Results on Large-scale DataA natural question arises that is the source-sidemonolingual data still very helpful when we havemuch more bilingual training data.
We conduct thelarge-scale experiments using our proposed multi-task framework RNNSearch-Mono-MTL.
Table 2 re-ports the results.We can see from the table that closely relatedsource-side monolingual data (the top 50%) canalso boost the translation quality on all of the testsets.
The performance improvement can be morethan 1.0 BLEU points.
Compared to the resultson small training data, the gains from source-sidemonolingual data are much smaller.
It is reasonablesince large-scale training data can make the param-eters of the encoder-decoder parameters much sta-ble.
We can also observe the similar phenomenonthat adding more unrelated monolingual data leadsto decreased translation quality.7 Related WorkAs a new paradigm for machine translation, theencoder-decoder based NMT has drawn more andmore attention.
Most of the existing methods mainlyfocus on designing better alignment mechanisms(attention model) for the decoder network (Chenget al, 2016a; Luong et al, 2015b; Cohn et al,2016; Feng et al, 2016; Tu et al, 2016; Mi et al,2016a; Mi et al, 2016b), better objective functionsfor BLEU evaluation (Shen et al, 2016) and betterstrategies for handling unknown words (Luong et al,2015c; Sennrich et al, 2015; Li et al, 2016) or largevocabularies (Jean et al, 2015; Mi et al, 2016c).Our focus in this work is aiming to make fulluse of the source-side large-scale monolingual datain NMT, which is not fully explored before.
Themost related works lie in three aspects: 1) apply-ing target-side monolingual data in NMT, 2) target-ing knowledge sharing with multi-task NMT, and 3)using source-side monolingual data in conventionalSMT and NMT.Gulcehre et al (2015) first investigate the target-side monolingual data in NMT.
They propose shal-low and deep fusion methods to enhance the decodernetwork by training a big language model on target-side large-scale monolingual data.
Sennrich et al(2015) further propose a new approach to use target-side monolingual data.
They generate the syntheticbilingual data by translating the target monolingualsentences to source language sentences and retrainNMT with the mixture of original bilingual data andthe synthetic parallel data.
It is similar to our self-learning algorithm in which we concern the source-side monolingual data.
Furthermore, their methodrequires to train an additional NMT from target lan-guage to source language, which may negatively in-fluence the attention model in the decoder network.Dong et al (2015) propose a multi-task learn-ing method for translating one source language intomultiple target languages in NMT so that the en-coder network can be shared when dealing with sev-eral sets of bilingual data.
Zoph et al (2016), Zophand Knight (2016) and Firat et al (2016) further dealwith more complicated cases (e.g.
multi-source lan-guages).
Note that all these methods require bilin-gual training corpus.
Instead, we adapt the multi-task learning framework to better accommodate thesource-side monolingual data.Ueffing et al (2007) and Wu et al (2008) explore1542the usage of source-side monolingual data in con-ventional SMT with a self-learning algorithm.
Al-though we apply self-learning in this work, we use itto enhance the encoder network in NMT rather thangenerating more translation rules in SMT and wealso adapt a multi-task learning framework to takefull advantage of the source-side monolingual data.Luong et al (2015a) also investigate the source-sidemonolingual data in the multi-task learning frame-work, in which a simple autoencoder or skip-thoughtvectors are employed to model the monolingualdata.
Our sentence reordering model is more pow-erful than simple autoencoder in encoder enhance-ment.
Furthermore, they do not carefully preparethe monolingual data for which we show that onlyrelated monolingual data leads to big improvements.In parallel to our work, Cheng et al (2016b) pro-pose a similar semi-supervised framework to handleboth source and target language monolingual data.If source-side monolingual data is considered, a re-construction framework including two NMTs is em-ployed.
One NMT translates the source-side mono-lingual data into target language translations, fromwhich the other NMT attempts to reconstruct theoriginal source-side monolingual data.
In contrastto their approach, we propose a sentence reorder-ing model rather than the sentence reconstructionmodel.
Furthermore, we carefully investigate the re-lationship between the monolingual data quality andthe translation performance improvement.8 Conclusions and Future WorkIn this paper, we propose a self-learning algo-rithm and a new multi-task learning framework touse source-side monolingual data so as to improvethe encoder network of the encoder-decoder basedNMT.
The self-learning algorithm generates the syn-thetic parallel corpus and enlarge the bilingual train-ing data to enhance the encoder model of NMT.The multi-task learning framework performs ma-chine translation on bilingual data and sentence re-ordering on source-side monolingual data by shar-ing the same encoder network.
The experimentsshow that our method can significantly outperformthe strong attention-based NMT baseline, and theproposed multi-task learning framework performsbetter than the self-learning algorithm at the expenseof low efficiency.
Furthermore, the experiments alsodemonstrate that NMT is more effective for incor-porating the source-side monolingual data than con-ventional SMT.
We also observe that more mono-lingual data does not always improve the translationquality and only relevant data does help.In the future, we would like to design smartermechanisms to distinguish real data from syntheticdata in self-learning algorithm, and attempt to pro-pose better models for handling source-side mono-lingual data.
We also plan to apply our methodsin other languages, especially for low-resource lan-guages.AcknowledgmentsWe thank the reviewers for their valuable com-ments and suggestions.
This research work hasbeen partially funded by the Natural Science Foun-dation of China under Grant No.
91520204 andNo.
61303181, and supported by the Strate-gic Priority Research Program of the CAS (GrantXDB02070007).ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
arXiv preprintarXiv:1409.0473.Yong Cheng, Shiqi Shen, Zhongjun He, Wei He, Hua Wu,Maosong Sun, and Yang Liu.
2016a.
Agreement-based joint training for bidirectional attention-basedneural machine translation.
In Proceedings of AAAI2016.Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu,Maosong Sun, and Yang Liu.
2016b.
Semi-supervisedlearning for neural machine translation.
In Proceed-ings of ACL 2016.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
computational linguistics, 33(2):201?228.Kyunghyun Cho, Bart Van Merrie?nboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, and Yoshua Bengio.
2014.
Learning phraserepresentations using rnn encoder-decoder for statis-tical machine translation.
In Proceedings of EMNLP2014.Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vymolova,Kaisheng Yao, Chris Dyer, and Gholamreza Haffari.2016.
Incorporating structural alignment biases intoan attentional neural translation model.
In Proceed-ings of NAACL 2016.1543Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, andHaifeng Wang.
2015.
Multi-task learning for multi-ple language translation.
In Proceedings of ACL 2015.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
The Journal of MachineLearning Research, 12:2121?2159.Shi Feng, Shujie Liu, Mu Li, and Ming Zhou.
2016.Implicit distortion and fertility models for attention-based encoder-decoder nmt model.
arXiv preprintarXiv:1601.03317.Orhan Firat, Kyunghyun Cho, and Yoshua Bengio.
2016.Multi-way, multilingual neural machine translationwith a shared attention mechanism.
arXiv preprintarXiv:1601.01073.Caglar Gulcehre, Orhan Firat, Kelvin Xu, KyunghyunCho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,Holger Schwenk, and Yoshua Bengio.
2015.
On us-ing monolingual corpora in neural machine translation.arXiv preprint arXiv:1503.03535.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Sebastien Jean, Kyunghyun Cho, Roland Memisevic, andYoshua Bengio.
2015.
On using very large target vo-cabulary for neural machine translation.
In Proceed-ings of ACL 2015.Nal Kalchbrenner and Phil Blunsom.
2013.
Recur-rent continuous translation models.
In Proceedings ofEMNLP 2013.Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, RichardS.
Zemel, Antonio Torralba, Raquel Urtasun, andSanja Fidler.
2015.
Skip-thought vectors.
In Pro-ceedings of NIPS 2015.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, et al 2007.
Moses: Open source toolkit forstatistical machine translation.
In Proceedings of ACL2007, pages 177?180.Xiaoqing Li, Jiajun Zhang, and Chengqing Zong.
2016.Towards zero unknown word in neural machine trans-lation.
In Proceedings of IJCAI 2016.Minh-Thang Luong, Quoc V Le, Ilya Sutskever, OriolVinyals, and Lukasz Kaiser.
2015a.
Multi-tasksequence to sequence learning.
arXiv preprintarXiv:1511.06114.Minh-Thang Luong, Hieu Pham, and Christopher DManning.
2015b.
Effective approaches to attention-based neural machine translation.
In Proceedings ofEMNLP 2015.Minh-Thang Luong, Ilya Sutskever, Quoc V Le, OriolVinyals, and Wojciech Zaremba.
2015c.
Addressingthe rare word problem in neural machine translation.In Proceedings of ACL 2015.Haitao Mi, Baskaran Sankaran, Zhiguo Wang, and AbeIttycheriah.
2016a.
A coverage embedding model forneural machine translation.
In Proceedings of EMNLP2016.Haitao Mi, Zhiguo Wang, Niyu Ge, and Abe Ittycheriah.2016b.
Supervised attentions for neural machinetranslation.
In Proceedings of EMNLP 2016.Haitao Mi, Zhiguo Wang, and Abe Ittycheriah.
2016c.Vocabulary manipulation for large vocabulary neuralmachine translation.
In Proceedings of ACL 2016.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of ACL2002, pages 311?318.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of COLING-ACL 2006.Rico Sennrich, Barry Haddow, and Alexandra Birch.2015.
Improving neural machine translationmodels with monolingual data.
arXiv preprintarXiv:1511.06709.Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu,Maosong Sun, and Yang Liu.
2016.
Minimum risktraining for neural machine translation.
In Proceed-ings of ACL 2016.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural networks.In Proceedings of NIPS 2014.Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,and Hang Li.
2016.
Coverage-based neural machinetranslation.
In Proceedings of ACL 2016.Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.2007.
Transductive learning for statistical machinetranslation.
In Proceedings of ACL 2007.Chao Wang, Michael Collins, and Philipp Koehn.
2007.Chinese syntactic reordering for statistical machinetranslation.
In Proceedings of EMNLP 2007.Zhiguo Wang, Haitao Mi, and Abe Ittycheriah.
2016a.Semi-supervised clustering for short text via deep rep-resentation learning.
In Proceedings of CoNLL 2016.Zhiguo Wang, Haitao Mi, and Abe Ittycheriah.
2016b.Sentence similarity learning by lexical decompositionand composition.
arXiv preprint arXiv:1602.07019.Hua Wu, Haifeng Wang, and Chengqing Zong.
2008.Domain adaptation for statistical machine translationwith domain dictionary and monolingual corpora.
InProceedings of COLING 2008, pages 993?1000.Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, andChengqing Zong.
2014.
Bilingually-constrainedphrase embeddings for machine translation.
In Pro-ceedings of ACL 2014.1544Jiajun Zhang, Dakun Zhang, and Jie Hao.
2015.
Localtranslation prediction with global sentence representa-tion.
In Proceedings of IJCAI 2015.Barret Zoph and Keven Knight.
2016.
Transfer learn-ing for low-resource neural machine translation.
arXivpreprint arXiv:1604.02201v1.Barret Zoph, Deniz Yuret, Jonathan May, and KevinKnight.
2016.
Multi-source neural translation.
InProceedings of NAACL 2016.1545
