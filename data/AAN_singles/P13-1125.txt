Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1275?1284,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsCut the noise: Mutually reinforcing reordering and alignments forimproved machine translationKarthik VisweswariahIBM Research Indiav-karthik@in.ibm.comMitesh M. KhapraIBM Research Indiamikhapra@in.ibm.comAnanthakrishnan RamanathanIBM Research Indiaanandr42@gmail.comAbstractPreordering of a source language sentenceto match target word order has proved tobe useful for improving machine transla-tion systems.
Previous work has shownthat a reordering model can be learnedfrom high quality manual word alignmentsto improve machine translation perfor-mance.
In this paper, we focus on furtherimproving the performance of the reorder-ing model (and thereby machine transla-tion) by using a larger corpus of sentencealigned data for which manual word align-ments are not available but automatic ma-chine generated alignments are available.The main challenge we tackle is to gen-erate quality data for training the reorder-ing model in spite of the machine align-ments being noisy.
To mitigate the effectof noisy machine alignments, we proposea novel approach that improves reorder-ings produced given noisy alignments andalso improves word alignments using in-formation from the reordering model.
Thisapproach generates alignments that are 2.6f-Measure points better than a baseline su-pervised aligner.
The data generated al-lows us to train a reordering model thatgives an improvement of 1.8 BLEU pointson the NIST MT-08 Urdu-English eval-uation set over a reordering model thatonly uses manual word alignments, and again of 5.2 BLEU points over a standardphrase-based baseline.1 IntroductionDealing with word order differences betweensource and target languages presents a significantchallenge for machine translation systems.
Failingto produce target words in the correct order resultsin machine translation output that is not fluent andis often very hard to understand.
These problemsare particularly severe when translating betweenlanguages which have very different structure.Phrase based systems (Koehn et al, 2003) uselexicalized distortion models (Al-Onaizan and Pa-pineni, 2006; Tillman, 2004) and scores from thetarget language model to produce words in the cor-rect order in the target language.
These systemstypically are only able to capture short range re-orderings and the amount of data required to po-tentially capture longer range reordering phenom-ena is prohibitively large.There has been a large body of work showingthe efficacy of preordering source sentences usinga source parser and applying hand written or auto-matically learned rules (Collins et al, 2005; Wanget al, 2007; Ramanathan et al, 2009; Xia and Mc-Cord, 2004; Genzel, 2010; Visweswariah et al,2010).
Recently, approaches that address the prob-lem of word order differences between the sourceand target language without requiring a high qual-ity source or target parser have been proposed(DeNero and Uszkoreit, 2011; Visweswariah etal., 2011; Neubig et al, 2012).
These methodsuse a small corpus of manual word alignments(where the words in the source sentence are man-ually aligned to the words in the target sentence)to learn a model to preorder the source sentence tomatch target order.In this paper, we build upon the approach in(Visweswariah et al, 2011) which uses manualword alignments for learning a reordering model.Specifically, we show that we can significantlyimprove reordering performance by using a largenumber of sentence pairs for which manual wordalignments are not available.
The motivation forgoing beyond manual word alignments is clear:the reordering model can have millions of featuresand estimating weights for the features on thou-sands of sentences of manual word alignments is1275likely to be inadequate.
One approach to deal withthis problem would be to use only part-of-speechtags as features for all but the most frequent words.This will cut down on the number of features andperhaps the model would be learnable with a smallset of manual word alignments.
Unfortunately, aswe will see in the experimental section, leavingout lexical information from the models hurts per-formance even with a relatively small set of man-ual word alignments.
Another option would be tocollect more manual word alignments but this isundesirable because it is time consuming and ex-pensive.The challenge in going beyond manual wordalignments and using machine alignments is thenoise in the machine alignments which affects theperformance of the reordering model (see Section5).
We illustrate this with the help of a motivatingexample.
Consider the example English sentenceand its translation shown in Figure 1.He went to the stadium to playvaha khelne keliye stadium ko gayaFigure 1: An example English sentence withits Urdu translation with alignment links.
Red(dotted) links are incorrect links while the blue(dashed) links are the corresponding correct links.A standard word alignment algorithm that weused (McCarley et al, 2011) made the mistake ofmis-aligning the Urdu ko and keliye (it switchedthe two).
Deriving reference reorderings fromthese wrong alignments would give us an incor-rect reordering.
A reordering model trained onsuch incorrect reorderings would obviously per-form poorly.
Our task is thus two-fold (i) im-prove the quality of machine alignments (ii) usethese less noisy alignments to derive cleaner train-ing data for a reordering model.Before proceeding, we first point out that thetwo tasks, viz., reordering and word alignmentare related: Having perfect reordering makes thealignment task easier while having perfect align-ments in turn makes the task of finding reorder-ings trivial.
Motivated by this fact, we introducemodels that allow us to connect the source/targetreordering and the word alignments and showthat these models help in mutually improving theperformance of word alignments and reordering.Specifically, we build two models: the first scoresreorderings given the source sentence and noisyalignments, the second scores alignments giventhe noisy source and target reorderings and thesource and target sentences themselves.
The sec-ond model helps produce better alignments, whilewe use the first model to help generate better ref-erence reordering given noisy alignments.
Theseimproved reference reorderings will then be usedto train a reordering model.Our experiments show that reordering modelstrained using these improved machine alignmentsperform significantly better than models trainedonly on manual word alignments.
This results ina 1.8 BLEU point gain in machine translation per-formance on an Urdu-English machine translationtask over a preordering model trained using onlymanual word alignments.
In all, this increasesthe gain in performance by using the preorderingmodel to 5.2 BLEU points over a standard phrase-based system with no preordering.The rest of this paper is structured as follows.Section 2 describes the main reordering issues inUrdu-English translation.
Section 3 introduces thereordering modeling framework that forms the ba-sis for our work.
Section 4 describes the two mod-els we use to tie together reordering and align-ments and how we use these models to generatetraining data for training our reordering model.Section 5 presents the experimental setup used forevaluating the models proposed in this paper onan Urdu-English machine translation task.
Sec-tion 6 presents the results of our experiments.We describe related work in Section 7 and finallypresent some concluding remarks and potential fu-ture work in Section 8.2 Reordering issues in Urdu-EnglishtranslationIn this section we describe the main sources ofword order differences between Urdu and Englishsince this is the language pair we experiment within this paper.The typical word order in Urdu is Subject-Object-Verb unlike English in which the order isSubject-Verb-Object.
Urdu has case markers thatsometimes (but not always) mark the subject andthe object of a sentence.
This difference in theplacement of verbs can often lead to movements ofverbs over long distances (depending on the num-ber of words in the object).
Phrase based systemsdo not capture such long distance movements well.1276Another difference is that Urdu uses post-positions unlike English which uses prepositions.This can also lead to long range movements de-pending on the length of the noun phrase that thepost-position follows.
The order of noun phrasesand prepositional phrases is also swapped in Urduas compared with English.3 Reordering modelIn this section we briefly describe the reorderingmodel (Visweswariah et al, 2011) that forms thebasis of our work.
We also describe an approx-imation we make in the training process that sig-nificantly speeds up the training without much lossof accuracy which enables training on much largerdata sets.
Consider a source sentence w that wewould like to reorder to match the target order.
Letpi represent a candidate permutation of the sourcesentence w. pii denotes the index of the word in thesource sentence that maps to position i in the can-didate reordering, thus reordering with this candi-date permutation pi we will reorder the sentencew to wpi1 , wpi2 , ..., wpin .
The reordering model weuse assigns costs to candidate permutations as:C(pi|w) =?ic(pii?1, pii).The costs c(m,n) are pairwise costs of puttingwm immediately before wn in the reordering.
Wereorder the sentence w according to the permu-tation pi that minimizes the cost C(pi|w).
Wefind the minimal cost permutation by convertingthe problem into a symmetric Travelling SalesmanProblem (TSP) and then using an implementationof the chained Lin-Kernighan heuristic (Applegateet al, 2003).
The costs in the reordering modelc(m,n) are parameterized by a linear model:c(m,n) = ?T?
(w,m, n)where ?
is a learned vector of weights and ?
is avector of binary feature functions that inspect thewords and POS tags of the source sentence at andaround positions m and n. We use the features(?)
described in Visweswariah et al (2011) thatwere based on features used in dependency pars-ing (McDonald et al, 2005a).To learn the weight vector ?
we require a cor-pus of sentences w with their desired reorderingspi?.
Past work Visweswariah et al (2011) usedhigh quality manual word alignments to derive thedesired reorderings pi?
as follows.
Given wordaligned source and target sentences, we drop thesource words that are not aligned1.
Let mi be themean of the target word positions that the sourceword at index i is aligned to.
We then sort thesource indices in increasing order ofmi (this orderdefines pi?).
If mi = mj (for example, because wiand wj are aligned to the same set of words) wekeep them in the same order that they occurred inthe source sentence.We used the single best Margin Infused RelaxedAlgorithm (MIRA) (McDonald et al (2005b),Crammer and Singer (2003)) with online updatesto our parameters given by:?i+1 = argmin?||?
?
?i||s.t.
C(pi?|w) < C(p?i|w) ?
L(pi?, p?i).In the equation above, p?i = argminpi C(pi|w) isthe best reordering based on the current parametervalue ?i and L is a loss function.
We take L to bethe number of words for which the hypothesizedpermutation p?i has a different preceding word ascompared with the reference permutation pi?.In this paper we focus on the case where in ad-dition to using a relatively small number of man-ual word aligned sentences to derive the refer-ence permutations pi?
used to train our model,we would like to use more abundant but nois-ier machine aligned sentence pairs.
To handlethe larger amount of training data we obtain frommachine alignments, we make an approximationin training that we found empirically to not af-fect performance but that makes training fasterby more than a factor of five.
This allows usto train the reordering model with roughly 150Ksentences in about two hours.
The approximationwe make is that instead of using the chained Lin-Kernighan heuristic to solve the TSP problem tofind p?i = argminpi C(pi|w), we select greedilyfor each word the preceding word that has the low-est cost2.
Using ?i to denote argminj c(j, i) andlettingC(?|w) =?ic(?i, i),1Note that the unaligned source words are dropped only atthe time of training.
At the time of testing all source words areretained as the alignment information is obviously not avail-able at test time.2It should be noted that this approximation was done onlyat the time of training.
At the time of testing we still use thechained Lin-Kernighan heuristic to solve the TSP problem.1277we do the update according to:?i+1 = argmin?||?
?
?i||s.t.
C(pi?|w) < C(?|w) ?
L(pi?,?
).Again the loss L(pi?,?)
is the number of positionsi for which pi?i?1 is different from ?i?1.4 Generating reference reordering fromparallel sentencesThe main aim of our work is to improve the re-ordering model by using parallel sentences forwhich manual word alignments are not avail-able.
In other words, we want to generate rel-atively clean reference reorderings from parallelsentences and use them for training a reorderingmodel.
A straightforward approach for this is touse a supervised aligner to align the words in thesentences and then derive the reference reorderingas we do for manual word alignments.
However,as we will see in the experimental results, the qual-ity of a reordering model trained from automaticalignments is very sensitive to the quality of align-ments.
This motivated us to explore if we can fur-ther improve our aligner and the method for gen-erating reference reorderings given alignments.We improve upon the above mentioned ba-sic approach by coupling the tasks of reorder-ing and word alignment.
We do this by build-ing a reordering model (C(pis|ws,wt,a)) thatscores reorderings pis given the source sentencews, target sentence wt and machine alignmentsa.
Complementing this model, we build an align-ment model (P (a|ws,wt,pis,pit)) that scoresalignments a given the source and target sen-tences and their predicted reorderings according tosource and target reordering models.
The model(C(pis|ws,wt,a)) helps to produce better refer-ence reorderings for training our final reorderingmodel given fixed machine alignments and thealignment model (P (a|ws,wt,pis,pit)) helps im-prove the machine alignments taking into accountinformation from reordering models.
In the fol-lowing sections, we describe our overall approachfollowed by a description of the two models.4.1 Overall approach to generating trainingdataWe first describe our overall approach to gen-erating training data for the reordering modelgiven a small corpus of sentences with manualC(pis|ws) C(pit|wt)Step 1: Train reordering modelsusing manual word alignmentsP (a|ws,wt, pis, pit)C(pis|ws,a) C(pit|wt,a)Step 2: Feed predictionsof the reordering modelsto the alignment modelStep 3: Feed predictionsof the alignment modelto the reordering modelsFigure 2: Overall approach: Building a sequenceof reordering and alignment models.word alignments (H) and a much larger corpus ofparallel sentences (U ) that are not word aligned.The basic idea is to chain together the two models,viz., reordering model and alignment model, asillustrated in Figure 2.
The steps involved are asdescribed below:Step 1: First, we use manual word alignments(H) to train source and target reordering modelsas described in (Visweswariah et al, 2011).Step 2: Next, we use the hand alignments to trainan alignment model P (a|ws,wt,pis,pit).
Inaddition to the original source and target sentence,we also feed the predictions of the reorderingmodel trained in Step 1 to this alignment model(see section 4.2 for details of the model itself).Step 3: Finally, we use the predictions of thealignment model trained in Step 2 to train reorder-ing models C(pis|ws,wt,a) (see section 4.3 fordetails on the reordering model itself).After building the sequence of models shown inFigure 2, we apply them in sequence on the un-aligned parallel data U , starting with the reorder-ing models C(pis|ws) and C(pit|wt).
The re-orderings obtained for the source side in U (afterapplying the final model C(pis|ws,a)) are usedalong with reference reorderings obtained fromthe manual word alignments to train our reorder-ing model.
Note that, in theory, we could iterateover steps 2 and 3 several times but, in practicewe did not see a benefit of going beyond one iter-1278ation in our experiments.
Also, since we are inter-ested only in the source side reorderings producedby the model C(pis|ws,a), the target reorderingmodel C(pit|wt,a) is needed only if we iterateover steps 2 and 3.We now point to some practical considerationsof our approach.
Consider the case when we aretraining an alignment model conditioned on re-orderings (P (a|ws,wt,pis,pit)).
If the reorder-ing model that generated these reorderings pis,pitwere trained on the same data that we are usingto train the alignment model, then the reorder-ings would be much better than we would ex-pect on unseen test data, and hence the align-ment model (P (a|ws,wt,pis,pit)) may learn tomake the alignment overly consistent with the re-orderings pis and pit.
To counter this problem,we divide the training data H into K parts andat each stage we apply a model (reordering oralignment) on part i that had not seen part i intraining.
This ensures that the alignment modeldoes not see very optimistic reorderings and viceversa.
We now describe the individual models,viz., P (a|ws,wt,pis,pit) and C(pis|ws,a).4.2 Modeling alignments given reorderingIn this section we describe how we fuse informa-tion from source and target reordering models toimprove word alignments.As a base model we use the correction modelfor word alignments proposed by McCarley etal.
(2011).
This model was significantly betterthan the MaxEnt aligner (Ittycheriah and Roukos,2005) and is also flexible in the sense that it allowsfor arbitrary features to be introduced while stillkeeping training and decoding tractable by using agreedy decoding algorithm that explores potentialalignments in a small neighborhood of the currentalignment.
The model thus needs a reasonablygood initial alignment to start with for which weuse the MaxEnt aligner (Ittycheriah and Roukos,2005) as in McCarley et al (2011).The correction model is a log-linear model:P (a|ws,wt) = exp(?T?
(a,ws,wt))Z(ws,wt) .The ?s are trained using the LBFGS algorithm(Liu et al, 1989) to maximize the log-likelihoodsmoothed with L2 regularization.
The featurefunctions ?
we start with are those used in Mc-Carley et al (2011) and include features encodingthe Model 1 probabilities between pairs of wordslinked in the alignment a, features that inspectsource and target POS tags and parses (if avail-able) and features that inspect the alignments ofadjacent words in the source and target sentence.To incorporate information from the reorder-ing model, we add features that use the predictedsource pis and target permutations pit.
We intro-duce some notation to describe these features.
LetSm and Sn be the set of indices of target wordsthatwsm andwsn are aligned to respectively.
We de-fine the minimum signed distance (msd) betweenthese two sets as:msd(Sm, Sn) = i?
?
j?where, (i?, j?)
= arg min(i,j)?Sm?Sn|i?
j|We quantize and encode with binary featuresthe minimum signed distance between the sets ofthe indices of the target words that source wordsadjacent in the reordering pis (wspisi and wspisi+1) arealigned to.
We instantiate similar features with theroles of source and target sentences reversed.
Withthis addition of features we use the same trainingand testing procedure as in McCarley et al (2011).If the reorderings pis were perfect we would learnto only allow alignments where wspisi and wspisi+1were aligned to adjacent words in the target sen-tence.
Although the reordering model is not per-fect, preferring alignments consistent with the re-ordering models improves the aligner.4.3 Modeling reordering given alignmentsTo model source permutations given source (ws)and target (wt) sentences, and alignments (a) wereuse the reordering model framework describedin Section 3 adding additional features capturingthe relation between a hypothesized permutationpi and alignments a.
To allow for searching viathe same TSP formulation we once again assigncosts to candidate permutations as:C(pis|ws,wt,a) =?ic(pii?1, pii|ws,a).Note that we introduce a dependence on the targetsentence wt only through the alignment a. Onceagain we parameterize the costs by a linear model:c(m,n) = ?T?
(ws,a,m, n).For the feature functions ?, in addition to thefeatures that only depend on ws,m, n (that we1279use in our standard reordering model) we addbinary indicator features based on msd(Sm, Sn)and msd(Sm, Sn) conjoined with POS(wsm) andPOS(wsn).Here, Sm and Sn are the set of indices of tar-get words that wsm and wsn are aligned to respec-tively.
We conjoin the msd (minimum signed dis-tance) with the POS tags to allow the model to cap-ture the fact that the alignment error rate maybehigher for some POS tags than others (e.g., wehave observed verbs have a higher error rate inUrdu-English alignments).Given these features we train the parameters ?using the MIRA algorithm as described in Sec-tion 3.
Using this model, we can find the low-est cost permutation C(pis|ws,a) using the Lin-Kernighan heuristic as described in Section 3.This model allows us to combine features fromthe original reordering model along with informa-tion coming from the alignments to find source re-orderings given a parallel corpus and alignments.We will see in the experimental section that thisimproves upon the simple heuristic for deriving re-orderings described in Section 3.5 Experimental setupIn this section we describe the experimental setupthat we used to evaluate the models proposed inthis paper.
All experiments were done on Urdu-English and we evaluate reordering in two ways:Firstly, we evaluate reordering performance di-rectly by comparing the reordered source sentencein Urdu with a reference reordering obtained fromthe manual word alignments using BLEU (Pap-ineni et al, 2002) (we call this measure monolin-gual BLEU or mBLEU).
All mBLEU results arereported on a small test set of about 400 sentencesset aside from our set of sentences with manualword alignments.
Additionally, we evaluate the ef-fect of reordering on our final systems for machinetranslation measured using BLEU.We use about 10K sentences (180K words) ofmanual word alignments which were created inhouse using part of the NIST MT-08 training data3to train our baseline reordering model and to trainour supervised machine aligners.
We use a parallelcorpus of 3.9M words consisting of 1.7M wordsfrom the NIST MT-08 training data set and 2.2Mwords extracted from parallel news stories on the3http://www.ldc.upenn.eduweb4.
The parallel corpus is used for building ourphrased based machine translation system and toadd training data for our reordering model.
Forour English language model, we use the GigawordEnglish corpus in addition to the English side ofour parallel corpus.
Our Part-of-Speech tagger isa Maximum Entropy Markov model tagger trainedon roughly fifty thousand words from the CRULPcorpus (Hussain, 2008).For our machine translation experiments, weused a standard phrase based system (Al-Onaizanand Papineni, 2006) with a lexicalized distortionmodel with a window size of +/-4 words5.
Toextract phrases we use HMM alignments alongwith higher quality alignments from a supervisedaligner (McCarley et al, 2011).
We report resultson the (four reference) NIST MT-08 evaluation setin Table 4 for the News and Web conditions.
TheNews and Web conditions each contain roughly20K words in the test set, with the Web conditioncontaining more informal text from the web.6 Results and DiscussionsWe now discuss the results of our experiments.Need for additional data: We first show the needfor additional data in Urdu-English reordering.Column 2 of Table 1 shows mBLEU as a functionof the number of sentences with manual wordalignments that are used to train the reorderingmodel.
We see a roughly 3 mBLEU points dropin performance per halving of data indicating apotential for improvement by adding more data.Using fewer features: We compare the perfor-mance of a model trained using lexical featuresfor all words (Column 2 of Table 1) with a modeltrained using lexical features only for the 1000most frequent words (Column 3 of Table 1).
Themotivation for this is to explore if a good modelcan be learned even from a small amount of data ifwe restrict the number of features in a reasonablemanner.
However, we see that even with only2.4K sentences with manual word alignments ourmodel benefits from lexical identities of morethan the 1000 most frequent words.Effect of quality of machine alignments: Wenext look at the use of automatically generated4http://centralasiaonline.com5Note that the same window size of +/-4 words was usedfor all the systems, i.e., the baseline system as well as thesystems using different preordering techniques.1280Data size All features Frequent lex only10K 52.5 50.85K 49.6 49.02.5K 46.6 46.2Table 1: mBLEU scores for Urdu to English re-ordering using different number of sentences ofmanually word aligned training data with all fea-tures and with lexical features instantiated only forthe 1000 most frequent words.machine alignments to train the reordering modeland see the effect of aligner quality on the re-ordering model generated using this data.
Theseexperiments also form the baseline for the mod-els we propose in this paper to clean up align-ments.
We experimented with two different super-vised aligners : a maximum entropy aligner (Itty-cheriah and Roukos, 2005) and an improved cor-rection model that corrects the maximum entropyalignments (McCarley et al, 2011).Aligner Train size mBLEUType f-Measure (words)None - 35.5Manual 180K 52.5MaxEnt 70.0 3.9M 49.5Correction model 78.1 3.9M 55.1Table 2: mBLEU scores for Urdu to English re-ordering using models trained on different datasources and tested on a development set of 8017Urdu tokens.Table 2 shows mBLEU scores when the re-ordering model is trained on reordering referencescreated from aligners with different quality.
Wesee that the quality of the alignments matter agreat deal to the reordering model; using MaxEntalignments cause a degradation in performanceover just using a small set of manual word align-ments.
The alignments obtained using the alignerof McCarley et al (2011) are of much betterquality and hence give higher reordering perfor-mance.
Note that this reordering performanceis much better than that obtained using manualword alignments because the size of machinealignments is much larger (3.9M v/s 180K words).Improvements in reordering performance us-ing the proposed models: Table 3 shows im-provements in the reordering model when usingthe models proposed in this paper.
We useH to re-fer to the manually word aligned data and U to re-fer to the additional sentence pairs for which man-ual word alignments are not available.
We reportthe following numbers :1.
Base correction model: This is the baselinewhere we use the correction model of McCar-ley et al (2011) for generating word alignments.The f-Measure of this aligner is 78.1% (see row1, column 2).
Corresponding to this, we also re-port the baseline for our reordering experimentsin the third column.
Here, we first generate wordalignments for U using the aligner of McCarley etal.
(2011) and then extract reference reorderingsfrom these alignments.
We then combine thesereference reorderings with the reference reorder-ings derived fromH and use this combined data totrain a reordering model which serves as the base-line (mBLEU = 55.1).2.
Correction model, C(pi|a): Here, once againwe generate alignments for U using the correc-tion model of McCarley et al (2011).
However,instead of using the basic approach of extractingreference reorderings, we use our improved modelC(pi|a) to generate reference reorderings from U .These reference reorderings are again combinedwith the reference reorderings derived fromH andused to train a reordering model (mBLEU = 56.4).3.
P (a|pi), C(pi|a): Here, we build the entire se-quence of models shown in Figure 2.
The align-ment model P (a|pi) is first improved by using pre-dictions from the reordering model.
These im-proved alignments are then used to extract betterreference reorderings from U using C(pi|a).We see substantial improvements over simplyadding in the data from the machine alignments.Improvements come roughly in equal parts fromthe two techniques we proposed in this paper : (i)using a model to generate reference reorderingsfrom noisy alignments and (ii) using reordering in-formation to improve the aligner.Method f-Measure mBLEUBase Correction model 78.1 55.1Correction model, C(pi|a) 78.1 56.4P (a|pi), C(pi|a) 80.7 57.6Table 3: mBLEU with different methods to gener-ate reordering model training data from a machinealigned parallel corpus in addition to manual wordalignments.Improvements in MT performance using theproposed models: We report results for a phrasebased system with different preordering tech-niques.
For results including a reordering model,we simply reorder the source side Urdu data bothwhile training and at test time.
In addition to1281phrase based systems with different preorderingmethods, we also report on a hierarchical phrasebased system for which we used Joshua 4.0 (Gan-itkevitch et al, 2012).
We see a significant gain of1.8 BLEU points in machine translation by goingbeyond manual word alignments using the best re-ordering model reported in Table 3.
We also note again of 2.0 BLEU points over a hierarchical phrasebased system.System type MT-08 evalWeb News AllBaseline (no preordering) 18.4 25.6 22.2Hierarchical phrase based 19.6 30.7 25.4Reordering: Manual alignments 20.7 30.0 25.6+ Machine alignments simple 21.3 30.9 26.4+ machine alignments, model based 22.1 32.2 27.4Table 4: MT performance without preordering(phrase based and hierarchical phrase based),and with reordering models using different datasources (phrase based).7 Related workDealing with the problem of handling word orderdifferences in machine translation has recently re-ceived much attention.
The approaches proposedfor solving this problem can be broadly dividedinto 3 sets as discussed below.The first set of approaches handle the reorder-ing problem as part of the decoding process.
Hier-archical models (Chiang, 2007) and syntax basedmodels (Yamada and Knight, 2002; Galley etal., 2006; Liu et al, 2006; Zollmann and Venu-gopal, 2006) improve upon the simpler phrasebased models but with significant additional com-putational cost (compared with phrase based sys-tems) due to the inclusion of chart based parsing inthe decoding process.
Syntax based models alsorequire a high quality source or target languageparser.The second set of approaches rely on a sourcelanguage parser and treat reordering as a separateprocess that is applied on the source language sen-tence at training and test time before using a stan-dard approach to machine translation.
Preorderingthe source data with hand written or automaticallylearned rules is effective and efficient (Collinset al, 2005; Wang et al, 2007; Ramanathan etal., 2009; Xia and McCord, 2004; Genzel, 2010;Visweswariah et al, 2010) but requires a sourcelanguage parser.Recent approaches that avoid the need for asource or target language parser and retain the ef-ficiency of preordering models were proposed in(Tromble and Eisner, 2009; DeNero and Uszko-reit, 2011; Visweswariah et al, 2011; Neubiget al, 2012).
(DeNero and Uszkoreit, 2011;Visweswariah et al, 2011; Neubig et al, 2012) fo-cus on the use of manual word alignments to learnpreordering models and in both cases no benefitwas obtained by using the parallel corpus in ad-dition to manual word alignments.
Our work isan extension of Visweswariah et al (2011) andwe focus on being able to incorporate relativelynoisy machine alignments to improve the reorder-ing model.In addition to being related to work in reorder-ing, our work is also more broadly related to sev-eral other efforts which we now outline.
Seti-awan et al (2010) proposed the use of functionword reordering to improve alignments.
Whilethis work is similar to one of our models (modelof alignments given reordering) we differ in us-ing a reordering model of all words (not just func-tion words) and both source and target sentences(not just the source sentence).
The task of directlylearning a reordering model for language pairs thatare very different is closely related to the task ofparsing and hence work on semi-supervised pars-ing (Koo et al, 2008; McClosky et al, 2006;Suzuki et al, 2009) is broadly related to our work.Our work coupling reordering and alignments isalso similar in spirit to approaches where parsingand alignment are coupled (Wu, 1997).8 ConclusionIn the paper we showed that a reordering modelcan benefit from data beyond a relatively smallcorpus of manual word alignments.
We proposeda model that scores reorderings given alignmentsand the source sentence that we use to gener-ate cleaner training data from noisy alignments.We also proposed a model that scores alignmentsgiven source and target sentence reorderings thatimproves a supervised alignment model by 2.6points in f-Measure.
While the improvement inalignment performance is modest, the improve-ment does result in improved reordering models.Cumulatively, we see a gain of 1.8 BLEU pointsover a baseline reordering model that only usesmanual word alignments, a gain of 2.0 BLEUpoints over a hierarchical phrase based system,and a gain of 5.2 BLEU points over a phrase based1282system that uses no source preordering on a pub-licly available Urdu-English test set.As future work we would like to evaluate ourmodels on other language pairs.
Another avenueof future work we would like to explore is the useof monolingual source and target data to furtherassist the reordering model.
We hope to be able tolearn lexical information such as how many argu-ments a verb takes, what nouns are potential sub-jects for a given verb by gathering statistics froman English parser and projecting to the source lan-guage via our word/phrase translation table.ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Dis-tortion models for statistical machine translation.
InProceedings of ACL, ACL-44, pages 529?536, Mor-ristown, NJ, USA.
Association for ComputationalLinguistics.David Applegate, William Cook, and Andre Rohe.2003.
Chained lin-kernighan for large travelingsalesman problems.
In INFORMS Journal On Com-puting.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Comput.
Linguist., 33(2):201?228, June.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL, pages 531?540,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Koby Crammer and Yoram Singer.
2003.
Ultraconser-vative online algorithms for multiclass problems.
J.Mach.
Learn.
Res., 3:951?991, March.John DeNero and Jakob Uszkoreit.
2011.
Inducingsentence structure from parallel corpora for reorder-ing.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?11, pages 193?203, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and trainingof context-rich syntactic translation models.
InProceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, ACL-44, pages 961?968, Stroudsburg, PA,USA.
Association for Computational Linguistics.Juri Ganitkevitch, Yuan Cao, Jonathan Weese, MattPost, and Chris Callison-Burch.
2012.
Joshua 4.0:Packing, pro, and paraphrases.
In Proceedings ofthe Seventh Workshop on Statistical Machine Trans-lation, pages 283?291, Montre?al, Canada, June.
As-sociation for Computational Linguistics.Dmitriy Genzel.
2010.
Automatically learning source-side reordering rules for large scale machine transla-tion.
In Proceedings of the 23rd International Con-ference on Computational Linguistics.Sarmad Hussain.
2008.
Resources for Urdu languageprocessing.
In Proceedings of the 6th Workshop onAsian Language Resources, IJCNLP?08.Abraham Ittycheriah and Salim Roukos.
2005.
A max-imum entropy word aligner for Arabic-English ma-chine translation.
In Proceedings of HLT/EMNLP,HLT ?05, pages 89?96, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of HLT-NAACL.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In ACL, pages 595?603.Dong C. Liu, Jorge Nocedal, and Dong C. 1989.
Onthe limited memory bfgs method for large scale op-timization.
Mathematical Programming, 45:503?528.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the44th annual meeting of the Association for Com-putational Linguistics, ACL-44, pages 609?616,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.J.
Scott McCarley, Abraham Ittycheriah, SalimRoukos, Bing Xiang, and Jian-ming Xu.
2011.
Acorrection model for word alignments.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing, EMNLP ?11, pages 889?898, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InHLT-NAACL.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005a.
Online large-margin training of de-pendency parsers.
In Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics, ACL ?05, pages 91?98, Stroudsburg, PA,USA.
Association for Computational Linguistics.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005b.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof HLT.Graham Neubig, Taro Watanabe, and Shinsuke Mori.2012.
Inducing a discriminative parser to optimizemachine translation reordering.
In Proceedings ofthe 2012 Joint Conference on Empirical Methodsin Natural Language Processing and Computational1283Natural Language Learning, pages 843?853, JejuIsland, Korea, July.
Association for ComputationalLinguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Ananthakrishnan Ramanathan, Hansraj Choudhary,Avishek Ghosh, and Pushpak Bhattacharyya.
2009.Case markers and morphology: addressing the cruxof the fluency problem in English-Hindi smt.
In Pro-ceedings of ACL-IJCNLP.Hendra Setiawan, Chris Dyer, and Philip Resnik.
2010.Discriminative word alignment with a function wordreordering model.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?10, pages 534?544, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Jun Suzuki, Hideki Isozaki, Xavier Carreras, andMichael Collins.
2009.
An empirical study of semi-supervised structured conditional models for depen-dency parsing.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 2 - Volume 2, EMNLP ?09,pages 551?560, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Christoph Tillman.
2004.
A unigram orientationmodel for statistical machine translation.
In Pro-ceedings of HLT-NAACL.Roy Tromble and Jason Eisner.
2009.
Learning linearordering problems for better translation.
In Proceed-ings of EMNLP.Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,Vijil Chenthamarakshan, and Nandakishore Kamb-hatla.
2010.
Syntax based reordering with automat-ically derived rules for improved statistical machinetranslation.
In Proceedings of the 23rd InternationalConference on Computational Linguistics.Karthik Visweswariah, Rajakrishnan Rajkumar, AnkurGandhe, Ananthakrishnan Ramanathan, and JiriNavratil.
2011.
A word reordering model for im-proved machine translation.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?11, pages 486?496,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Chao Wang, Michael Collins, and Philipp Koehn.2007.
Chinese syntactic reordering for statisticalmachine translation.
In Proceedings of EMNLP-CoNLL.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Comput.
Linguist., 23(3):377?403, September.Fei Xia and Michael McCord.
2004.
Improvinga statistical MT system with automatically learnedrewrite patterns.
In COLING.Kenji Yamada and Kevin Knight.
2002.
A decoder forsyntax-based statistical MT.
In Proceedings of ACL.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax augmented machine translation via chart parsing.In Proceedings on the Workshop on Statistical Ma-chine Translation.1284
