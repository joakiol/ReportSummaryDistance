Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 33?40,Portland, Oregon, June 23, 2011. c?2011 Association for Computational LinguisticsWikiTopics: What is Popular on Wikipedia and WhyByung Gyu Ahn1 and Benjamin Van Durme1,2 and Chris Callison-Burch11Center for Language and Speech Processing2Human Language Technology Center of ExcellenceJohns Hopkins UniversityAbstractWe establish a novel task in the spirit of news sum-marization and topic detection and tracking (TDT):daily determination of the topics newly popular withWikipedia readers.
Central to this effort is a newpublic dataset consisting of the hourly page viewstatistics of all Wikipedia articles over the last threeyears.
We give baseline results for the tasks of:discovering individual pages of interest, clusteringthese pages into coherent topics, and extracting themost relevant summarizing sentence for the reader.When compared to human judgements, our systemshows the viability of this task, and opens the doorto a range of exciting future work.1 IntroductionIn this paper we analyze a novel dataset: we havecollected the hourly page view statistics1 for everyWikipedia page in every language for the last three years.We show how these page view statistics, along with otherfeatures like article text and inter-page hyperlinks, canbe used to identify and explain popular trends, includingpopular films and music, sports championships, elections,natural disasters, etc.Our approach is to select a set of articles whose dailypageviews for the last fifteen days dramatically increaseabove those of the preceding fifteen day period.
Ratherthan simply selecting the most popular articles for a givenday, this selects articles whose popularity is rapidly in-creasing.
These popularity spikes tend to be due to sig-nificant current events in the real world.
We examine 100such articles for each of 5 randomly selected days in 2009and attempt to group the articles into clusters such thatthe clusters coherently correspond to current events andextract a summarizing sentence that best explains the rel-evant event.
Quantitative and qualitative analyses are pro-vided along with the evaluation dataset.1The data does not contain any identifying information about whoviewed the pages.
See http://dammit.lt/wikistatsBarack ObamaJoe BidenWhite HouseInauguration.
.
.US Airways Flight 1549Chesley SullenbergerHudson River.
.
.Super BowlArizona CardinalsFigure 1: Automatically selected articles for Jan 27, 2009.We compare our automatically collected articles tothose in the daily current events portal of Wikipediawhere Wikipedia editors manually chronicle currentevents, which comprise armed conflicts, international re-lations, law and crime, natural disasters, social, political,sports events, etc.
Each event is summarized with a sim-ple phrase or sentence that links to related articles.
Weview our work as an automatic mechanism that could po-tentially supplant this hand-curated method of selectingcurrent events by editors.Figure 1 shows examples of automatically selected ar-ticles for January 27, 2009.
We would group the arti-cles into 3 clusters, {Barack Obama, Joe Biden, WhiteHouse, Inauguration} which corresponds to the inaugu-ration of Barack Obama, {US Airways Flight 1549, Ches-ley Sullenburger, Hudson River} which corresponds tothe successful ditching of an airplane into the Hudsonriver without loss of life, and {Superbowl, Arizona Car-dinals} which corresponds to the then upcoming Super-bowl XLIII.We further try to explain the clusters by selecting sen-tences from the articles.
For the first cluster, a good se-lection would be ?the inauguration of Barack Obama asthe 44th president .
.
.
took place on January 20, 2009?.For the second cluster, ?Chesley Burnett ?Sully?
Sullen-berger III (born January 23, 1951) is an American com-33mercial airline pilot, .
.
.
, who successfully carried out theemergency water landing of US Airways Flight 1549 onthe Hudson River, offshore from Manhattan, New YorkCity, on January 15, 2009, .
.
.
?
would be a nice sum-mary, which also provides links to the other articles inthe same cluster.
For the third cluster, ?Superbowl XLIIIwill feature the American Football Conference championPittsburgh Steelers (14-4) and the National Football Con-ference champion Arizona Cardinals (12-7) .?
would bea good choice which delineates the association with Ari-zona Cardinals.Different clustering methods and sentence selectionfeatures are evaluated and results are compared.
Topicmodels, such as K-means (Manning et al, 2008) vectorspace clustering and latent Dirichlet alocation (Blei etal., 2003), are compared to clustering using Wikipedia?slink structure.
To select sentences we make use of NLPtechnologies such as coreference resolution, and namedentity and date taggers.
Note that the latest revision ofeach article on the day on which the article is selected isused in clustering and textualization to simulate the situa-tion where article selection, clustering, and textualizationare performed once every day.Figure 2 illustrates the pipeline of our WikiTopics sys-tem: article selection, clustering, and textualization.2 Article selectionWe would like to identify an uptrend in popularity of ar-ticles.
In an online encyclopedia such as Wikipedia, thepageviews for an article reflect its popularity.
Followingthe Trending Topics software2, WikiTopics?s articles se-lection algorithm determines each articles?
monthly trendvalue as increase in pageviews within last 30 days.
Themonthly trend value tk of an article k is defined as be-low:tk =15?i=1dki ?30?i=16dkiwheredki = daily pageviews i?
1 days ago for an article kWe selected 100 articles of the highest trend value foreach day in 2009.
We call the articles WikiTopics articles.We leave as future work other possibilities to determinethe trend value and choose articles3, and only briefly dis-cuss some alternatives in this section.Wikipedia has a portal page called ?current events?,in which significant current events are listed manu-ally by Wikipedia editors.
Figure 3 illustrates spikes in2http://www.trendingtopics.org3For example, one might leverage additional signals of real worldevents, such as Twitter feeds, etc.1001000100001000001e+061e+071e+08Dec 06 Dec 20 Jan 03 Jan 17 Jan 31PageviewsBarack ObamaUnited StatesList of Presidents of the United StatesPresident of the United StatesAfrican AmericanList of African-American firstsFigure 3: Pageviews for all the hand-curated articles relatedto the inauguration of Barack Obama.
Pageviews spike on thesame day as the event took place?January 20, 2009.pageviews of the hand-curated articles related to the in-auguration of Barack Obama, which shows clear correla-tion between the spikes and the day on which the relevantevent took place.
It is natural to contrast WikiTopics ar-ticles to this set of hand-curated articles.
We evaluatedWikiTopics articles against hand-curated articles as goldstandard and had negative results with precision of 0.13and recall of 0.28.There are a few reasons for this.
First, there aremuch fewer hand-curated articles than WikiTopics arti-cles: 17,253 hand-selected articles vs 36,4004 WikiTopicsarticles; so precision cannot be higher than 47%.
Second,many of the hand-selected articles turned out to have verylow pageviews: 6,294 articles (36.5%) have maximumdaily pageviews less than 1,000 whereas WikiTopics arti-cles have increase in pageviews of at least 10,000.
It is ex-tremely hard to predict the hand-curated articles based onpageviews.
Figure 4 further illustrates hand-curated arti-cles?
lack of increase in pageviews as opposed to Wiki-Topics articles.
On the contrary, nearly half of the hand-curated articles have decrease in pageviews.
For the hand-curated articles, it seems that spikes in pageviews arean exception rather than a commonality.
We thereforeconcluded that it is futile to predict hand-curated arti-cles based on pageviews.
The hand-curated articles sufferfrom low popularity and do not spike in pageviews often.Figure 5 contrasts the WikiTopics articles and the hand-curated articles.
The WikiTopics articles shown here donot appear in the hand-curated articles within fifteen daysbefore or after, and vice versa.
WikiTopics selected arti-cles about people who played a minor role in the relevantevent, recently released films, their protagonists, popularTV series, etc.
Wikipedia editors selected articles about4One day is missing from our 2009 pageviews statistics.34Daily Page Views Topic Selection Clustering TextualizationFigure 2: Process diagram: (a) Topic selection: select interesting articles based on increase in pageviews.
(b) Clustering: cluster thearticles according to relevant events using topic models or Wikipedia?s hyperlink structure.
(c) Textualization: select the sentencethat best summarizes the relevant event.-2024680  0.2  0.4  0.6  0.8  1logratioquantileWikiTopics articleshand-curated articlesFigure 4: Log ratio of the increase in pageviews:log?i = 115dik/?i = 1630.
Zero means no changein pageviews.
WikiTopics articles show pageviews increase ina few orders of magnitude as opposed to hand-curated articles.actions, things, geopolitical or organizational names inthe relevant event and their event description mentionsall of them.For this paper we introduce the problem of topic se-lection along with a baseline solution.
There are vari-ous viable alternatives to the monthly trend value.
Asone of them, we did some preliminary experiments withthe daily trend value, which is defined by dk1 ?
dk2 , i.e.the difference of the pageviews between the day and theprevious day: we found that articles selected using thedaily trend value have little overlap?less than half the ar-ticles overlapped with the monthly trend value.
Futurework will consider the addition of sources other thanpageviews, such as edit histories and Wikipedia categoryinformation, along with more intelligent techniques tocombine these different sources.3 ClusteringClustering plays a central role to identify current events;a group of coherently related articles corresponds to aWikiTopics articlesJoe BidenNotorious (2009 film)The Notorious B.I.G.Lost (TV series).
.
.hand-curated articlesFraudFloridaHedge fundArthur NadelFederal Bureau of InvestigationFigure 5: Illustrative articles for January 27, 2009.
WikiTopicsarticles here do not appear in hand-curated articles within fifteendays before or after, and vice versa.
The hand-curated articlesshown here are all linked from a single event ?Florida hedgefund manager Arthur Nadel is arrested by the United States Fed-eral Bureau of Investigation and charged with fraud.
?current event.
Clusters, in general, may have hierarchiesand an element may be a member of multiple clusters.Whereas Wikipedia?s current events are hierarchicallycompiled into different levels of events, we focus on flatclustering, leaving hierarchical clustering as future work,but allow multiple memberships.In addition to clustering using Wikipedia?s inter-pagehyperlink structure, we experimented with two familiesof clustering algorithms pertaining to topic models: theK-means clustering vector space model and the latentDirichlet alocation (LDA) probabilistic topic model.
Weused the Mallet software (McCallum, 2002) to run thesetopic models.
We retrieve the latest revision of each arti-cle on the day that WikiTopics selected it.
We strip unnec-essary HTML tags and Wiki templates with mwlib5 andsplit sentences with NLTK (Loper and Bird, 2002).
Nor-malization, tokenization, and stop words removal wereperformed, but no stemming was performed.
The uni-gram (bag-of-words) model was used and the number5http://code.pediapress.com/wiki/wiki/mwlib35Test set # Clusters B3 F-scoreHuman-1 48.6 0.70 ?
0.08Human-2 50.0 0.71 ?
0.11Human-3 53.8 0.74 ?
0.10ConComp 31.8 0.42 ?
0.18OneHop 45.2 0.58 ?
0.17K-means tf 50 0.52 ?
0.04K-means tf-idf 50 0.58 ?
0.09LDA 44.8 0.43 ?
0.08Table 1: Clustering evaluation: F-scores are averaged acrossgold standard datasets.
ConComp and OneHop are using thelink structure.
K-means clustering with tf-idf performs best.Manual clusters were evaluated against those of the other twoannotators to determine inter-annotator agreement.of clusters/topics K was set to 50, which is the averagenumber of clusters in the human clusters6.
For K-means,the common settings were used: tf and tf-idf weightingand cosine similarity (Allan et al, 2000).
For LDA, wechose the most probable topic for each article as the clus-ter ID.
Two different clustering schemes make use of theinter-page hyperlink structure: ConComp and OneHop.In these schemes, the link structure is treated as a graph,in which each page corresponds to a vertex and each linkto an undirected edge.
ConComp groups a set of arti-cles that are connected together.
OneHop chooses an ar-ticle and groups a set of articles that are directly linked.The number of resulting clusters depends on the orderin which you choose an article.
To find the minimum ormaximum number of such clusters would be computa-tionally expensive.
Instead of attempting to find the op-timal number of clusters, we take a greedy approach anditeratively create clusters that maximize the central nodeconnectivity, stopping when all nodes are in at least onecluster.
This allows for singleton clusters.Three annotators manually clustered WikiTopics arti-cles for five randomly selected days.
The three manualclusters were evaluated against each other to measureinter-annotator agreement, using the multiplicity B3 met-ric (Amigo?
et al, 2009).
Table 1 shows the results.
TheB3 metric is an extrinsic clustering evaluation metric andneeds a gold standard set of clusters to evaluate against.The multiplicity B3 works nicely for overlapping clus-ters: the metric does not need to match cluster IDs andonly considers the number of the clusters that a pair ofdata points shares.
For a pair of data points e and e?, letC(e) be the set of the test clusters that e belongs to, andL(e) be the set of e?s gold standard clusters.
The multi-6K=50 worked reasonably well for the most cases.
We are planningto explore a more principled way to set the number.Airbus A320 familyAir Force OneChesley SullenbergerUS Airways Flight 1549Super Bowl XLIIIArizona CardinalsSuper BowlKurt Warner2009 flu pandemic by countrySevere acute respiratory syndrome2009 flu pandemic in the United StatesFigure 6: Examples of clusters: K-means clustering on the arti-cles of January 27, 2009 and May 12, 2009.
The centroid articlefor each cluster, defined as the closest article to the center of thecluster in vector space, is in bold.plicity B3 scores are evaluated as follows:Prec(e, e?)
=min (|C(e) ?
C(e?
)|, |L(e) ?
L(e?
)|)|C(e) ?
C(e?
)|Recall(e, e?)
=min (|C(e) ?
C(e?
)|, |L(e) ?
L(e?
)|)|L(e) ?
L(e?
)|The overall B3 scores are evaluated as follows:Prec = AvgeAvge?.C(e)?C(e?
)6=0Prec(e, e?
)Recall = AvgeAvge?.L(e)?L(e?
)6=0Recall(e, e?
)The inter-annotator agreement in the B3 scores are in therange of 67%?74%.
K-means clustering performs best,achieving 79% precision compared to manual cluster-ing.
OneHop clustering using the link structure achievedcomparable performance.
LDA performed significantlyworse, comparable to ConComp clustering.Clustering the articles according to the relevance to re-cent popularity is not trivial even for humans.
In Wiki-Topics articles for February 10, 2009, Journey (band) andBruce Springsteen may seem to be relevant to GrammyAwards, but in fact they are relevant on this day becausethey performed the halftime show at the Super Bowl.
K-means fails to recognize this and put them into the clusterof Grammy Awards, while ConComp merged GrammyAwards and Super Bowl into the same cluster.
OneHopkept the two clusters intact and benefited from puttingBruce Springsteen into both the clusters.
LDA cluster-ing does not have such a benefit; its performance mighthave suffered from our allowing only a single member-ship for an article.
Clustering using the link structure per-forms comparably with other clustering algorithms with-out using topic models.
It is worth noting that there area few ?octopus?
articles that have links to many articles.The United States on January 27, 2009 was disastrous,with its links to 58 articles, causing ConComp clusteringto group 89 articles into a single cluster.
OneHop clus-tering?s condition that groups only articles that are onehop away alleviates the issue and it also benefited fromputting an article into multiple clusters.36To see if external source help better clustering, we ex-plored the use of news articles.
We included the news ar-ticles that we crawled from various news websites intothe same vector space as the Wikipedia articles, and ranK-means clustering with the same settings as before.
Foreach day, we experimented with news articles within dif-ferent numbers of past days.
The results did not showsignificant improvement over clustering without externalnews articles.
This needs further investigation7.4 TextualizationWe would like to generate textual descriptions for theclustered articles to explain why they are popular andwhat current event they are relevant to.
We started witha two-step approach similar to multi-document extrac-tive summarization approaches (Mckeown et al, 2005).The first step is sentence selection; we extract the bestsentence that describes the relevant event for each arti-cle.
The second step is combining the selected sentencesof a cluster into a coherent summary.
Here, we focus onthe first step of selecting a sentence and evaluate the se-lected sentences.
The selected sentences for each clus-ter are then put together without modification, where thequality of generated summary mainly depends on the ex-tracted sentences at the first step.
We consider each articleseparately, using as features only information such as dateexpressions and references to the topic of the article.
Fu-ture work will consider sentence extraction, aware of therelated articles in the same cluster, and better summariza-tion techniques, such as sentence fusion or paraphrasing.We preprocess the Wikipedia articles using the Serifsystem (Boschee et al, 2005) for date tagging and coref-erence resolution.
The identified temporal expressionsare in various formats such as exact date (?February 12,1809?
), a season (?spring?
), a month (?December 1808?
),a date without a specific year (?November 19?
), and evenrelative time (?now?, ?later that year?, ?The followingyear?).
Some examples are shown in Figure 7.
The en-tities mentioned in a given article are compiled into a listand the mentions of each entity, including pronouns, arelinked to the entity as a coreference chain.
Some exam-ples are shown in Figure 9.In our initial scheme, we picked the first sentenceof each article because the first sentence is usually anoverview of the topic of the article and often relevant tothe current event.
For example, a person?s article oftenhas the first line with one?s recent achievement or death.An article about an album or a film often begins with therelease date.
We call this First.7News articles tend to group with other news articles.
We are cur-rently experimenting with different filtering and parameters.
Also notethat we only experimented with all news articles on a given day.
Clus-tering with selective news articles might help.February 12, 18091860nowthe 17th centurysome timeDecember 180834 years oldspringSeptemberLater that yearNovember 19that same monthThe following winterThe following yearApril 1865late 1863Figure 7: Selected examples of temporal expressions identifiedby Serif from 247 such date and time expressions extracted fromthe article Abraham Lincoln.We also picked the sentence with the most recent dateto the day on which the article was selected.
Dates in thenear future are considered in the same way as the recentdates.
Dates may appear in various formats, so we make amore specific format take precedence, i.e.
?February 20,2009?
is selected over vaguer dates such as ?February2009?
or ?2009?.
We call this scheme Recent.As the third scheme, we picked the sentence with themost recent date among those with a reference to the ar-ticle?s title.
The reasoning behind this is if the sentencerefers to the title of the article, it is more likely to be rel-evant to the current event.
We call this scheme Self.After selecting a sentence for each cluster, we substi-tute personal pronouns in the sentence with their propernames.
This step enhances readability of the sentence,which often refers to people by a pronoun such as ?he?,?his?, ?she?, or ?her?.
The examples of substituted propernames appear in Figure 9 in bold.
The Serif system classi-fies which entity mentions are proper names for the sameperson, but choosing the best name among the names isnot a trivial task: proper names may vary from John toJohn Kennedy to John Fitzgerald ?Jack?
Kennedy.
Wechoose the most frequent proper name.For fifty randomly chosen articles over the five se-lected days, two annotators selected the sentences thatbest describes why an article gained popularity recently,among 289 sentences per each article on average fromthe article text.
For each article, annotators picked a sin-gle best sentence, and possibly multiple alternative sen-tences.
If there is no such single sentence that best de-scribes a relevant event, annotators marked none as thebest sentence and listed alternative sentences that par-tially explain the relevant event.
The evaluation resultsfor all the selection schemes are shown in Table 2.
Tosee inter-annotator agreement, two annotators?
selectionswere evaluated against each other.
The other selectionschemes are evaluated against both the two annotators?selection and their scores in the table are averaged acrossthe two.
The precision and recall score for best sentencesare determined by evaluating a scheme?s selection of the372009-01-27: Inauguration of Barack ObamaGold: The inauguration of Barack Obama as the forty-fourth Presidentof the United States took place on January 20, 2009.Alternatives: 1.
The inauguration, with a record attendance for anyevent held in Washington, D.C., marked the commencement of thefour-year term of Barack Obama as President and Joseph Biden asVice President.
2.
With his inauguration as President of the UnitedStates, Obama became the first African American to hold the officeand the first President born in Hawaii.
3.
Official events were held inWashington, D.C. from January 18 to 21, 2009, including the We AreOne: The Obama Inaugural Celebration at the Lincoln Memorial, a dayof service on the federal observance of the Martin Luther King, Jr. Day,a ?Kids?
Inaugural: We Are the Future?
concert event at the VerizonCenter, the inaugural ceremony at the U.S. Capitol, an inauguralluncheon at National Statuary Hall, a parade along PennsylvaniaAvenue, a series of inaugural balls at the Washington ConventionCenter and other locations, a private White House gala and an inauguralprayer service at the Washington National Cathedral.First: The inauguration of Barack Obama as the forty-fourth Presidentof the United States took place on January 20, 2009.Recent: On January 22, 2009, a spokesperson for the Joint Committeeon Inaugural Ceremonies also announced that holders of blue, purpleand silver tickets who were unable to enter the Capitol grounds to viewthe inaugural ceremony would receive commemorative items.Self: On January 21, 2009, President Obama, First Lady MichelleObama, Vice President Biden and Dr. Jill Biden attended an inauguralprayer service at the Washington National Cathedral.2009-02-10: February 2009 Great Britain and Ireland snowfallGold: The snowfall across Great Britain and Ireland in February 2009is a prolonged period of snowfall that began on 1 February 2009.Alternative: Many areas experienced their largest snowfall levels in 18years.First: The snowfall across Great Britain and Ireland in February 2009is a prolonged period of snowfall that began on 1 February 2009.Recent: BBC regional summary - 4 February 2009Self: The snowfall across Great Britain and Ireland in February 2009 isa prolonged period of snowfall that began on 1 February 2009.2009-04-19: Wilkins SoundGold: On 5 April 2009 the thin bridge of ice to the Wilkins Ice Shelfoff the coast of Antarctica splintered, and scientists expect it couldcause the collapse of the Shelf.Alternatives: 1.
There are reports the shelf has exploded into hundredsof small ice bergs.
2.
On 5 April 2009, the ice bridge connecting partof the ice shelf to Charcot Island collapsed.First: Wilkins Sound is a seaway in Antarctica that is largely occupiedby the Wilkins Ice Shelf.Recent: On 5 April 2009 the thin bridge of ice to the Wilkins Ice Shelfoff the coast of Antarctica splintered, and scientists expect it couldcause the collapse of the Shelf.Self: On 25 March 2008 a chunk of the Wilkins ice shelf disintegrated,putting an even larger portion of the glacial ice shelf at risk.Figure 8: Sentence selection: First selects the first sentence, andoften fails to relate the current event.
Recent tend to pinpoint theexact sentence that describes the relevant current event, but failswhen there are several sentences with a recent temporal expres-sion.
Self helps avoid sentences that does not refer to the topicof the article, but suffers from errors propagated from corefer-ence resolution.2009-01-27: Barack ObamaBefore: He was inaugurated as President on January 20, 2009.After: Obama was inaugurated as President on January 20,2009.Coref: {Barack Hussein Obama II (brk hsen obm; born August4,, Barack Obama, Barack Obama as the forty-fourth President,Barack Obama, Sr. , Crain?s Chicago Business naming Obama,Michelle Obama, Obama, Obama in Indonesian, SenatorObama,}2009-02-10: Rebirth (Lil Wayne album)Before: He also stated the album will be released on April 7,2009.After: Lil Wayne also stated the album will be released onApril 7, 2009.Coref: {American rapper Lil Wayne, Lil Wayne, Wayne}2009-04-19: Phil SpectorBefore: His second trial resulted in a conviction of seconddegree murder on April 13, 2009.After: Spector?s second trial resulted in a conviction of seconddegree murder on April 13, 2009.Coref: {Mr. Spector, Phil Spector, Phil Spector?
The characterof Ronnie ?Z, Spector, Spector-, Spector (as a producer),Spector himself, Spector of second-degree murder, Spector,who was conducting the band for all the acts,, Spektor, wifeRonnie Spector}2009-05-12: EminemBefore: He is planning on releasing his first album since 2004,Relapse, on May 15, 2009.After: Eminem is planning on releasing his first album since2004, Relapse, on May 15, 2009.Coref: {Eminem, Marshall Bruce Mathers, Marshall BruceMathers III, Marshall Bruce Mathers III (born October 17,,Mathers}2009-10-12: Brett FavreBefore: He came out of retirement for the second time andsigned with the Minnesota Vikings on August 18, 2009.After: Favre came out of retirement for the second time andsigned with the Minnesota Vikings on August 18, 2009.Coref: {Bonita Favre, Brett Favre, Brett Lorenzo Favre, Brett?sfather Irvin Favre, Deanna Favre, Favre, Favre,, Favre (ISBN978-1590710364) which discusses their personal family andGreen Bay Packers family, Irvin Favre, Southern Miss.
Favre,the Brett Favre, The following season Favre, the jersey Favre}Figure 9: Pronoun replacement: Personal pronouns are substi-tuted with their proper names, which are italicized.
The coref-erence chain for the entity is also shown; our method correctlyavoids names wrongly placed in the chain.
Note that unlike theother sentences, the last one is not related to the current event,Brett Favre?s victory against Green Bay Packers.38Single best AlternativesScheme Precision Recall Precision RecallHuman 0.50 0.55 0.85 0.75First 0.14 0.20 0.33 0.40Recent 0.31 0.44 0.51 0.60Self 0.31 0.36 0.49 0.48Self fallback 0.33 0.46 0.52 0.62Table 2: Textualization: evaluation results of sentence selectionschemes.
Self fallback scheme first tries to select the best sen-tence as the Self scheme, and if it fails to select one it falls backto the Recent scheme.best sentences against a gold standard?s selection.
Toevaluate alternative sentences, precision is measured asthe fraction of articles where the test and gold standardselections overlap (share at least one sentence), comparedto the total number of articles that have at least one sen-tence selected according to the test set.
Recall is definedby instead dividing by the number of articles that have atleast one sentence selected in the gold standard.The low inter-annotator agreement for selecting thebest sentence shows the difficulty of the task.
However,when their sentence selection is evaluated by allowingmultiple alternative gold standard sentences, the agree-ment is higher.
It seems that there are a set of articles forwhich it is easy to pick the best sentence that two anno-tators and automatic selection schemes easily agree on,and another set of articles for which it is difficult to findsuch a sentence.
In the easier articles, the best sentenceoften includes a recent date expression, which is easilypicked up by the Recent scheme.
Figure 8 illustrates suchcases.
In the more difficult articles, there are no such sen-tences with recent dates.
X2 (film) is such an example; itwas released in 2003.
The release of the prequel X-MenOrigins: Wolverine in 2009 renewed its popularity andthe X2 (film) article still does not have any recent dates.There is a more subtle case: the article Farrah Fawcettincludes many sentences with recent dates in a section,which describes the development of a recent event.
It ishard to pinpoint the best one among them.Sentence selection heavily depends on other NLP com-ponents, so errors in them could result in the error in sen-tence selection.
Serena Williams is an example where anerror in sentence splitting propagates to sentence selec-tion.
The best sentence manually selected was the firstsentence in the article ?Serena Jameka Williams .
.
.
, as ofFebruary 2, 2009, is ranked World No.
1 by the Women?sTennis Association .
.
.
.?
The sentence was disastrouslydivided into two sentences right after ?No.?
by NLTKduring preprocessing.
In other words, the gold standardsentence could not be selected no matter how well se-lection performs.
Another source of error propagation iscoreference resolution.
The Self scheme limits sentenceselection to the sentences with a reference to the articles?title, and it failed to improve over Recent.
In qualitativeanalysis, 3 out of 4 cases that made a worse choice re-sulted from failing to recognize a reference to the topicof the article.
By having it fall back to Recent?s selectionwhen it failed to find any best sentence, its performancemarginally improved.
Improvements of the componentswould result in better performance of sentence selection.WikiTopics?s current sentence extraction succeeded ingenerating the best or alternative sentences that summa-rizes the relevant current event for more than half of thearticles, in enhanced readability through coreference res-olution.
For the other difficult cases, it needs to take dif-ferent strategies rather than looking for the most recentdate expressions.
Alternatives may consider references toother related articles.
In future work, selected sentenceswill be combined to create summary of a current event,and will use sentence compression, fusion and paraphras-ing to create more succinct summaries.5 Related workWikiTopics?s pipeline architecture resembles that of newssummarization systems such as Columbia Newsblaster(McKeown et al, 2002).
Newsblaster?s pipeline is com-prised of components for performing web crawls, articletext extraction, clustering, classification, summarization,and web page generation.
The system processes a con-stant stream of newswire documents.
In contrast, Wiki-Topics analyzes a static set of articles.
Hierarchical clus-tering like three-level clustering of Newsblaster (Hatzi-vassiloglou et al, 2000) could be applied to WikiTopicsto organize current events hierarchically.
Summarizingmultiple sentences that are extracted from the articles inthe same cluster would provide a comprehensive descrip-tion about the current event.
Integer linear programming-based models (Woodsend and Lapata, 2010) may prove tobe useful to generate summaries while global constraintslike length, grammar, and coverage are met.The problem of Topic Detection and Tracking (TDT)is to identify and follow new events in newswire, andto detect the first story about a new event (Allan et al,1998).
Allan et al (2000) evaluated a variety of vectorspace clustering schemes, where the best settings fromthose experiments were then used in our work.
This wasfollowed recently by Petrovic?
et al (2010), who took anapproximate approach to first story detection, as appliedto Twitter in an on-line streaming setting.
Such a systemmight provide additional information to WikiTopics byhelping to identify and describe current events that haveyet to be explicitly described in a Wikipedia article.
Svoreet al (2007) explored enhancing single-document sum-mariation using news query logs, which may also be ap-plicable to WikiTopics.Wikipedia?s inter-article links have been utilized to39construct a topic ontology (Syed et al, 2008), word seg-mentation corpora (Gabay et al, 2008), or to computesemantic relatedness (Milne and Witten, 2008).
In ourwork, we found the link structure to be as useful to clustertopically related articles as well as the article text.
In fu-ture work, the text and the link structure will be combinedas Chaudhuri et al (2009) explored multi-view hierarchi-cal clustering for Wikipedia articles.6 ConclusionsWe have described a pipeline for article selection, clus-tering, and textualization in order to identify and describesignificant current events as according to Wikipedia con-tent, and metadata.
Similarly to Wikipedia editors main-taining that site?s ?current events?
pages, we are con-cerned with neatly collecting articles of daily relevance,only automatically, and more in line with expressed userinterest (through the use of regularly updated page viewlogs).
We have suggested that Wikipedia?s hand-curatedarticles cannot be predicted solely based on pageviews.Clustering methods based on topic models and inter-article link structure are shown to be useful to groupa set of articles that are coherently related to a currentevent.
Clustering based on only link structure achievedcomparable performance with clustering based on topicmodels.
In a third of cases, the sentence that best de-scribed a current event could be extracted from the ar-ticle text based on temporal expressions within an article.We employed a coreference resolution system assist intext generation, for improved readability.
As future work,sentence compression, fusion, and paraphrasing could beapplied to selected sentences with various strategies tomore succinctly summarize the current events.
Our ap-proach is language independent, and may be applied tomulti-lingual current event detection, exploiting furtherthe online encyclopedia?s cross-language references.
Fi-nally, we plan to leverage social media such as Twit-ter as an additional signal, especially in cases where es-sential descriptive information has yet to be added to aWikipedia article of interest.AcknowledgmentsWe appreciate Domas Mituzas and Fre?de?ric Schu?tz forthe pageviews statistics and Peter Skomoroch for theTrending Topics software.
We also thank three anony-mous reviewers for their thoughtful advice.
This re-search was supported in part by the NSF under grant IIS-0713448 and the EC through the EuroMatrixPlus project.The first author was funded by Samsung Scholarship.Opinions, interpretations, and conclusions are those ofthe authors and not necessarily endorsed by the sponsors.ReferencesJames Allan, Jaime Carbonell, George Doddington, JonathanYamron, and Yiming Yang.
1998.
Topic Detection andTracking Pilot Study Final Report.
In Proceedings of theDARPA Broadcast News Transcription and UnderstandingWorkshop.James Allan, Victor Lavrenko, Daniella Malin, and RussellSwan.
2000.
Detections, bounds, and timelines: UMassand TDT-3.
In Proceedings of Topic Detection and Track-ing Workshop.Enrique Amigo?, Julio Gonzalo, Javier Artiles, and FelisaVerdejo.
2009.
A comparison of extrinsic clusteringevaluation metrics based on formal constraints.
Inf.
Retr.,12(4):461?486.D.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
Latent Dirichletallocation.
Journal of Machine Learning Research.Elizabeth Boschee, Ralph Weischedel, and Alex Zamanian.2005.
Automatic information extraction.
In Proceedings ofIA.Kamalika Chaudhuri, Sham M. Kakade, Karen Livescu, andKarthik Sridharan.
2009.
Multi-view clustering via canoni-cal correlation analysis.
In Proceedings of ICML.David Gabay, Ziv Ben-Eliahu, and Michael Elhadad.
2008.
Us-ing wikipedia links to construct word segmentation corpora.In Proceedings of AAAI Workshops.Vasileios Hatzivassiloglou, Luis Gravano, and Ankineedu Ma-ganti.
2000.
An investigation of linguistic features and clus-tering algorithms for topical document clustering.
In Pro-ceedings of SIGIR.Edward Loper and Steven Bird.
2002.
NLTK: the Natural Lan-guage Toolkit.
In Proceedings of ACL.C.
Manning, P. Raghavan, and H. Schu?tze.
2008.
Introductionto information retrieval.
Cambridge University Press.Andrew Kachites McCallum.
2002.
MALLET: A MachineLearning for Language Toolkit.
http://mallet.cs.umass.edu.Kathleen R. McKeown, Regina Barzilay, David Evans,Vasileios Hatzivassiloglou, Judith L. Klavans, Ani Nenkova,Carl Sable, Barry Schiffman, and Sergey Sigelman.
2002.Tracking and summarizing news on a daily basis withColumbia?s Newsblaster.
In Proceedings of HLT.Kathleen Mckeown, Rebecca J. Passonneau, David K. Elson,Ani Nenkova, and Julia Hirschberg.
2005.
Do summarieshelp?
a task-based evaluation of multi-document summariza-tion.
In Proceedings of SIGIR.David Milne and Ian H. Witten.
2008.
An effective, low-costmeasure of semantic relatedness obtained from Wikipedialinks.
In Proceedings of AAAI Workshops.Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2010.Streaming first story dectection with application to Twitter.In Proceedings of NAACL.Krysta M. Svore, Lucy Vanderwende, and Christopher J.C.Burges.
2007.
Enhancing single-document summarizationby combining ranknet and third-party sources.
In Proceed-ings of EMNLP-CoLing.Zareen Saba Syed, Tim Finin, and Anupam Joshi.
2008.Wikipedia as an ontology for describing documents.
In Pro-ceedings of ICWSM.Kristian Woodsend and Mirella Lapata.
2010.
Automatic gen-eration of story highlights.
In Proceedings of ACL.40
