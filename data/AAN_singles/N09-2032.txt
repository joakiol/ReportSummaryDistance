Proceedings of NAACL HLT 2009: Short Papers, pages 125?128,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsDomain Adaptation with Artificial Data for Semantic Parsing of SpeechLonneke van der PlasDepartment of LinguisticsUniversity of GenevaGeneva, SwitzerlandJames HendersonDepartment of Computer ScienceUniversity of GenevaGeneva, Switzerland{Lonneke.vanderPlas,James.Henderson,Paola.Merlo}@unige.chPaola MerloDepartment of LinguisticsUniversity of GenevaGeneva, SwitzerlandAbstractWe adapt a semantic role parser to the do-main of goal-directed speech by creating anartificial treebank from an existing text tree-bank.
We use a three-component model thatincludes distributional models from both tar-get and source domains.
We show that we im-prove the parser?s performance on utterancescollected from human-machine dialogues bytraining on the artificially created data withoutloss of performance on the text treebank.1 IntroductionAs the quality of natural language parsing improvesand the sophistication of natural language under-standing applications increases, there are several do-mains where parsing, and especially semantic pars-ing, could be useful.
This is particularly true inadaptive systems for spoken language understand-ing, where complex utterances need to be translatedinto shallow semantic representation, such as dia-logue acts.The domain on which we are working is goal-directed system-driven dialogues, where a systemhelps the user to fulfil a certain goal, e.g.
booking ahotel room.
Typically, users respond with short an-swers to questions posed by the system.
For exam-ple In the South is an answer to the question Wherewould you like the hotel to be?
Parsing helps iden-tifying the components (In the South is a PP) andsemantic roles identify the PP as a locative, yield-ing the following slot-value pair for the dialogue act:area=South.
A PP such as in time is not identified asa locative, whereas keyword-spotting techniques asthose currently used in dialogue systems may pro-duce area=South and area=time indifferently.Statistical syntactic and semantic parsers needtreebanks.
Current available data is lacking in one ormore respects: Syntactic/semantic treebanks are de-veloped on text, while treebanks of speech corporaare not semantically annotated (e.g.
Switchboard).Moreover, the available human-human speech tree-banks do not exhibit the same properties as thesystem-driven speech on which we are focusing, inparticular in their proportion of non-sentential utter-ances (NSUs), utterances that are not full sentences.In a corpus study of a subset of the human-humandialogues in the BNC, Ferna?ndez (2006) found thatonly 9% of the total utterances are NSUs, whereaswe find 44% in our system-driven data.We illustrate a technique to adapt an exist-ing semantic parser trained on merged Penn Tree-bank/PropBank data to goal-directed system-drivendialogue by artificial data generation.
Our main con-tribution lies in the framework used to generate ar-tificial data for domain adaptation.
We mimic thedistributions over parse structures in the target do-main by combining the text treebank data and theartificially created NSUs, using a three-componentmodel.
The first component is a hand-crafted modelof NSUs.
The second component describes the dis-tribution over full sentences and types of NSUs asfound in a minimally annotated subset of the targetdomain.
The third component describes the distribu-tion over the internal parse structure of the generateddata and is taken from the source domain.Our approach differs from most approaches to do-main adaptation, which require some training onfully annotated target data (Nivre et al, 2007),whereas we use minimally annotated target dataonly to help determine the distributions in the ar-tificially created data.
It also differs from previ-125ous work in domain adaptation by Foster (2007),where similar proportions of ungrammatical andgrammatical data are combined to train a parseron ungrammatical written text, and by Weilhammeret al (2006), who use interpolation between twoseparately trained models, one on an artificial cor-pus of user utterances generated by a hand-codeddomain-specific grammar and one on available cor-pora.
Whereas much previous work on parsingspeech has focused on speech repairs, e.g.
Charniakand Johnson (2001), we focus on parsing NSUs.2 The first component: a model of NSUsTo construct a model of NSUs we studied a subset ofthe data under consideration: TownInfo.
This smallcorpus of transcribed spoken human-machine dia-logues in the domain of hotel/restaurant/bar searchis gathered using the TownInfo tourist informationsystem (Lemon et al, 2006).The NSUs we find in our data are mainly of thetype answers, according to the classification givenin Ferna?ndez (2006).
More specifically, we findshort answers, plain and repeated affirmative an-swers, plain and helpful rejections, but also greet-ings.Current linguistic theory provides several ap-proaches to dealing with NSUs (Merchant, 2004;Progovac et al, 2006; Ferna?ndez, 2006).
Follow-ing the linguistic analysis of NSUs as non-sententialsmall clauses (Progovac et al, 2006) that do not havetense or agreement functional nodes, we make theassumption that they are phrasal projections.
There-fore, we reason, we can create an artificial data setof NSUs by extracting phrasal projections from anannotated treebank.In the example given in the introduction, we sawa PP fragment, but fragments can be NPs, APs, etc.We define different types of NSUs based on the rootlabel of the phrasal projection and define rules thatallow us to extract NSUs (partial parse trees) fromthe source corpus.1 Because the target corpus alsocontains full sentences, we allow full sentences tobe taken without modification from the source tree-bank.1Not all of these rules are simple extractions of phrasal pro-jections, as described in section 4.3 The two distributional componentsThe distributional model consists of two compo-nents.
By applying the extraction rules to the sourcecorpus we build a large collection of both full sen-tences and NSUs.
The distributions in this collec-tion follow the distributions of trees in the source do-main (first distributional component).
We then sam-ple from this collection to generate our artificial cor-pus following distributions from the target domain(second distributional component).The probability of an artificial tree P (fi(cj)) gen-erated with an extraction rule fi applied to a con-stituent from the source corpus cj is defined asP (fi(cj)) = P (fi)P (cj |fi) ?
Pt(fi)Ps(cj |fi)The first distributional component originates fromthe source domain.
It is responsible for the internalstructure of the NSUs and full sentences extracted.Ps(cj |fi) is the probability of the constituent takenfrom the source treebank (cj), given that the rule fiis applicable to that constituent.Sampling is done according to distributions ofNSUs and full sentences found in the target corpus(Pt(fi)).
As explained in section 2, there are severaltypes of NSUs found in the target domain.
This sec-ond component describes the distributions of typesof NSUs (or full sentences) found in the target do-main.
It determines, for example, the proportion ofNP NSUs that will be added to the artificial corpus.To determine the target distribution we classified171 (approximately 5%) randomly selected utter-ances from the TownInfo data, that were used as adevelopment set.2 In Table 1 we can see that 15.2 %of the trees in the artificial corpus will be NP NSUs.34 Data generationWe constructed our artificial corpus from sections2 to 21 of the Wall Street Journal (WSJ) sectionof the Penn Treebank corpus (Marcus et al, 1993)2We discarded very short utterances (yes, no, and greetings)since they don?t need parsing.
We also do not consider incom-plete NSUs resulting from interruptions or recording problems.3Because NSUs can be interpreted only in context, the sameNSU can correspond to several syntactic categories: South forexample, can be an noun, an adverb, or an adjective.
In case ofambiguity, we divided the score up for the several possible tags.This accounts for the fractional counts.126Category # Occ.
Perc.
Category # Occ.
Perc.NP 19.0 15.2 RB 1.7 1.3JJ 12.7 10.1 DT 1.0 0.8PP 12.0 9.6 CD 1.0 0.8NN 11.7 9.3 Total frag.
70.0 56.0VP 11.0 8.8 Full sents 55.0 44.0Table 1: Distribution of types of NSUs and full sentencesin the TownInfo development set.merged with PropBank labels (Palmer et al, 2005).We included all the sentences from this dataset inour artificial corpus, giving us 39,832 full sentences.In accordance with the target distribution we added50,699 NSUs extracted from the same dataset.
Wesampled NSUs according to the distribution given inTable 1.
After the extraction we added a root FRAGnode to the extracted NSUs4 and we capitalised thefirst letter of each NSU to form an utterance.There are two additional pre-processing steps.First, for some types of NSUs maximal projectionsare added.
For example, in the subset from the tar-get source we saw many occurrences of nouns with-out determiners, such as Hotel or Bar.
These typesof NSUs would be missed if we just extracted NPsfrom the source data, since we assume that NSUs aremaximal projections.
Therefore, we extracted singlenouns as well and we added the NP phrasal projec-tions to these nouns in the constructed trees.
Sec-ond, not all extracted NSUs can keep their semanticroles.
Extracting part of the sentence often seversthe semantic role from the predicate of which it wasoriginally an argument.
An exception to this are VPNSUs and prepositional phrases that are modifiers,such as locative PPs, which are not dependent on theverb.
Hence, we removed the semantic roles fromthe generated NSUs except for VPs and modifiers.5 ExperimentsWe trained three parsing models on both the originalnon-augmented merged Penn Treebank/Propbankcorpus and the artificially generated augmented tree-bank including NSUs.
We ran a contrastive ex-periment to examine the usefulness of the three-component model by training two versions of the4The node FRAG exists in the Penn Treebank.
Our annota-tion does not introduce new labels, but only changes their dis-tribution.augmented model: One with and one without thetarget component.5These models were tested on two test sets: a smallcorpus of 150 transcribed utterances taken from theTownInfo corpus, annotated with gold syntactic andsemantic annotation by two of the authors6: theTownInfo test set.
The second test set is used tocompare the performance of the parser on WSJ-stylesentences and consists of section 23 of the mergedPenn Treebank/Propbank corpus.
We will refer tothis test set as the non-augmented test set.5.1 The statistical parserThe parsing model is the one proposed in Merloand Musillo (2008), which extends the syntacticparser of Henderson (2003) and Titov and Hender-son (2007) with annotations which identify seman-tic role labels, and has competitive performance.The parser uses a generative history-based proba-bility model for a binarised left-corner derivation.The probabilities of derivation decisions are mod-elled using the neural network approximation (Hen-derson, 2003) to a type of dynamic Bayesian Net-work called an Incremental Sigmoid Belief Network(ISBN) (Titov and Henderson, 2007).The ISBN models the derivation history with avector of binary latent variables.
These latent vari-ables learn to represent features of the parse historywhich are useful for making the current and subse-quent derivation decisions.
Induction of these fea-tures is biased towards features which are local inthe parse tree, but can find features which are passedarbitrarily far through the tree.
This flexible mecha-nism for feature induction allows the model to adaptto the parsing of NSUs without requiring any designchanges or feature engineering.5.2 ResultsIn Table 2, we report labelled constituent recall, pre-cision, and F-measure for the three trained parsers(rows) on the two test sets (columns).7 These mea-5The model without the target distribution has a uniform dis-tribution over full sentences and NSUs and within NSUs a uni-form distribution over the 8 types.6This test set was constructed separately and is completelydifferent from the development set used to determine the distri-butions in the target data.7Statistical significance is determined using a stratified shuf-fling method, using software available at http://www.cis.127Training TestingTownInfo PTB nonaugRec Prec F Rec Prec FPTB nonaug 69.4 76.7 72.9 81.4 82.1 81.7PTB aug(+t) 81.4 77.8 79.5 81.3 82.0 81.7PTB aug(?t) 62.6 64.3 63.4 81.2 81.9 81.6Table 2: Recall, precision, and F-measure for the two testsets, trained on non-augmented data and data augmentedwith and without the target distribution component.sures include both syntactic labels and semantic rolelabels.The results in the first two lines of the columnsheaded TownInfo indicate the performance on thereal data to which we are trying to adapt our parser:spoken data from human-machine dialogues.
Theparser does much better when trained on the aug-mented data.
The differences between training onnewspaper text and newspaper texts augmented withartificially created data are statistically significant(p < 0.001) and particularly large for recall: almost12%.The columns headed PTB nonaug show that theperformance on parsing WSJ texts is not hurt bytraining on data augmented with artificially cre-ated NSUs (first vs. second line).
The differencein performance compared to training on the non-augmented data is not statistically significant.The last two rows of the TownInfo data show theresults of our contrastive experiment.
It is clearthat the three-component model and in particular ourcareful characterisation of the target distribution isindispensable.
The F-measure drops from 79.5% to63.4% when we disregard the target distribution.6 ConclusionsWe have shown how a three-component model thatconsists of a model of the phenomenon being stud-ied and two distributional components, one from thesource data and one from the target data, allowsone to create data artificially for training a seman-tic parser.
Specifically, analysis and minimal anno-tation of only a small subset of utterances from thetarget domain of spoken dialogue systems sufficesto determine a model of NSUs as well as the nec-essary target distribution.
Following this frameworkupenn.edu/?dbikel/software.html.we were able to improve the performance of a statis-tical parser on goal-directed spoken data extractedfrom human-machine dialogues without degradingthe performance on full sentences.AcknowledgementsThe research leading to these results has receivedfunding from the EU FP7 programme (FP7/2007-2013) under grant agreement nr 216594 (CLASSICproject: www.classic-project.org).ReferencesE.
Charniak and M. Johnson.
2001.
Edit detection andparsing for transcribed speech.
In Procs.
NAACL.R.
Ferna?ndez.
2006.
Non-sentential utterances in dia-logue: classification resolution and use.
Ph.D. thesis,University of London.J.
Foster.
2007.
Treebanks gone bad: Parser evaluationand retraining using a treebank of ungrammatical sen-tences.
International Journal of Document Analysisand Recognition, 10:1?16.J.
Henderson.
2003.
Inducing history representations forbroad-coverage statistical parsing.
In Procs.
NAACL-HLT.O.
Lemon, K. Georgila, J. Henderson, and M. Stuttle.2006.
An ISU dialogue system exhibiting reinforce-ment learning of dialogue policies: generic slot-fillingin the TALK in-car system.
In Procs.
EACL.M.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1993.
Building a large annotated corpus of English:the Penn Treebank.
Comp.
Ling., 19:313?330.J.
Merchant.
2004.
Fragments and ellipsis.
Linguisticsand Philosophy, 27:661?738.P.
Merlo and G. Musillo.
2008.
Semantic parsingfor high-precision semantic role labelling.
In Procs.CONLL.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The CoNLL 2007shared task on dependency parsing.
In Procs.
EMNLP-CoNLL.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
TheProposition Bank: An annotated corpus of semanticroles.
Comp.
Ling., 31:71?105.L.
Progovac, K. Paesani, E. Casielles, and E. Barton.2006.
The Syntax of Nonsententials:MultidisciplinaryPerspectives.
John Benjamins.I Titov and J Henderson.
2007.
Constituent parsing withIncremental Sigmoid Belief Networks.
In Procs.
ACL.K.
Weilhammer, M. Stuttle, and S. Young.
2006.
Boot-strapping language models for dialogue systems.
InProcs.
Conf.
on Spoken Language Processing.128
