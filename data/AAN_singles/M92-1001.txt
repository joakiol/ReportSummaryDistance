OVERVIEW OF THE FOURTH MESSAGE UNDERSTANDIN GEVALUATION AND CONFERENC EBeth M. SundheimNaval Command, Control, and Ocean Surveillance Cente rRDT&E Division (NRaD) 1Decision Support and AI Technology Branc hSan Diego, CA 92152-5000sundheim@nosc .milINTRODUCTIO NThe Fourth Message Understanding Conference (MUC-4) is the latest in a serie sof conferences that concern the evaluation of natural language processing (NLP )systems.
These conferences have reported on progress being made both in th edevelopment of systems capable of analyzing relatively short English texts and inthe definition of a rigorous performance evaluation methodology .
MUC-4 waspreceded by a period of intensive system development by each of the participatingorganizations and blind testing using materials prepared by NRaD and SAIC thatare described in this paper, other papers in this volume, and the MUC- 3proceedings [1] .The overall objective of the evaluations is to advance our understanding of th emerits of current text analysis techniques, as applied to the performance of arealistic information extraction task.
As a task, information extraction require s"understanding" of the texts, but it presents a more limited challenge than would atask requiring production of an in-depth representation of the contents o fcomplete texts .
The inputs to the analysis/extraction process consist of naturally -occurring newswire texts that were obtained in electronic form.
The outputs of th eprocess are a set of templates or semantic frames resembling the contents of apartially formatted database .MUC-3 and MUC-4 offer benchmarks for the field of NLP in general and fo rinformation extraction technology in particular .
One of the fundamental ways i nwhich MUC-3 and MUC-4 are distinct from earlier efforts is in their choice of texts :MUC-3 and MUC-4 made use of news articles on the subject of Latin America nterrorism, whereas the previous conferences had made use of naval tactica lmessage narratives [2] .
The MUC-4 evaluation and conference featured a nenhanced evaluation methodology, greater participation, and significantly mor econclusive results than those recorded in the MUC-3 proceedings .Evaluating end-to-end systems in the context of a common task helps in severa lways to bridge the gap between research and technology.
First, it makes it easierfor both technology producers and technology consumers to understand an dappreciate the value of the methods that are being explored and applied .It alsoformerly the Naval Ocean Systems Center (NOSC) .3serves as an example of achievable results and inspires ideas for real-lifeapplications such as statistical analysis or trends analysis of world events ,routing/retrieval of texts of personal interest, and feeding of data to expert system sand database management systems .
Finally, it encourages the development oflarge, experimental testbed systems in 'which to conduct research, and th eevaluation results can provide insight into research bottlenecks that are impedin gthe development of full-scale, usable systems .This paper presents an overall view of the MUC-4 evaluation and, to a larg eextent, reflects the content of introductory presentations made at the conference.This paper is also an overview of the conference proceedings, which include spapers contributed by the organizations that participated in the evaluation (Part sII and III) and by individuals who were involved in designing aspects of th eevaluation (Part I) .
The ordering of papers does not necessarily correspond to th eorder in which the presentations were made during the conference .
Theproceedings also includes a number of appendices (Part IV) containing material spertinent to the evaluation .EVALUATION PARTICIPANTSSeventeen systems were evaluated for MUC-4, versus 15 for MUC-3 .
Nineteenorganizations participated in the development of the MUC-4 systems, including 1 2of the 17 MUC-3 participants .
These veteran groups are BBN Systems andTechnologies (Cambridge, MA), General Electric (Schenectady, NY), Hughe sResearch Laboratories (Malibu, CA), Language Systems, Inc .
(Woodland Hills, CA) ,McDonnell Douglas Electronic Systems (Santa Ana, CA), New York University (NewYork City, NY), Paramax Systems2 (Paoli, PA), PRC, Inc .
(McLean, VA), SRIInternational (Menlo Park, CA), the University of Maryland together withConQuest, Inc .
3 (Baltimore, MD), and the University of Massachusetts (Amherst ,MA).Participation in MUC demands a great commitment of resources over a nextended period of time .
The actual effort expended for MUC-4 ranged from lessthan two person-months to over twelve .
For the veteran groups, this effort was i naddition to the effort spent preparing for MUC-3 .
Given the commitment requiredand the limited amount of funding that was available to help support the efforts, i tis not surprising that several MUC-3 groups were unable to continue participatio nin MUC-44 .
What is surprising is that there were seven new MUC-4 participants.These include three organizations currently working under separate DARPAcontracts in the area of information extraction, namely Brandeis Universit y(Waltham, MA), Carnegie Mellon University (Pittsburgh, PA), and New Mexico Stat e2 formerly Unisys Center for Advanced Information Technolog y3 formerly Synchronetics, Inc .4 Those MUC-3 participants that were unable to participate in the MUC-4 evaluation areAdvanced Decision Systems (Mountain View, CA), General Telephone and Electronics (Mountai nView, CA), Intelligent Text Processing, Inc .
(Santa Monica, CA), the University of Nebraska(Lincoln, NE), and the University of Southwest Louisiana (Lafayette, LA) .4University (Las Cruces, N M ) .
~  New participants also included the MITRE Corp.(Bedford, MA), Systems Research and Applications (Arlington, VA), the Universityof Michigan (Ann Arbor, MI), and the University of Southern California (LosAngeles, CA).DIFFERENCES BETWEEN THE MUC-3 AND MUC-4 EVALUATIONSPreparations for MUC-4 were made starting in October, 1991, the call forparticipation was issued in December, and the system development phase was wellunderway by February, 1992.
A dry run of the evaluation was conducted in lateMarch, final testing was done in late May, and the conference was held in mid-~ u n e .
6  The program committee7 approved an ambitious plan for updating variousaspects of the MUC-3 evaluation design for use for MUC-4.
Changes to the taskdefinition, corpus, measures of performance, and test protocols were made in orderto provide* greater focus on the issue of spurious data generation;* isolation of text filtering performance;* better isolation of language analysis performance;* assessment of system independence from the training data;* assessment of system development progress since MUC-3;* more consistent scoring;* means to make valid score comparisons among systems.Greater Focus on the Issue of Spurious Data GenerationThe MUC-3 measures of performance implicitly encouraged participants tostrive to develop their systems to  achieve high recall at the expense of high~ v e r ~ e n e r a t i o n .
~  A few changes were made to the template scoring software tomake the generation of spurious data more apparent.
One of these changes focusesattention on overgeneration at the slot level (generating more slot values thanwere expected), while the others focus attention on overgeneration at the templatelevel (generating more templates than were expected).To address the spurious slot-value issue, an additional method of assessingpenalties for missing and spurious data (called the "Matched/Spurious" method)was incorporated, completing the picture provided by the three that had beendeveloped for MUC-3.
To address the spurious template issue, a preliminary step inthe alignment of response templates with key templates was implemented thatrequires that minimal "content-based mapping conditions" be met in order foralignment to occur.
Response templates that fail to meet these minimal conditionsNew Mexico State University teamed with Brandeis University for MUC-4, and CarnegieMellon University teamed with General Electric.6 The conference was hosted by PRC, Inc. at their conference center in McLean, VA.7 The MUC-4 program committee included B. Sundheim (NRaD), chair; N. Chinchor (SAIC); R.Grishman (NYU); J. Hobbs (SRI); D. Lewis (U Chicago); L. Rau (GE); C. Weir (Paramax).Readers unfamiliar with the usage of the terms "recall," "precision," and "overgeneration" asinformation extraction evaluation metrics should refer to [3].are scored as spurious ; if any unaligned key templates remain, the system get spenalized for missing them.
These changes are discussed further in [3] .One way in which the spurious template issue was addressed was to change th eway the scoring software does the mapping (or "alignment") of the system -generated "response" templates with the answer "key" templates .
A minimumdegree of match in the content of the key and response is required before mappin gis allowed ; if disallowed, the system is penalized for having produced one spuriou stemplate (and, under some circumstances, as many spurious slot values as there arevalues in the response) and for having missed one template (and, under som ecircumstances, as many slot values as there are values in the key).
When multipl etemplate mappings are possible, the scoring program chooses the mapping that i slikely to produce the best score.
The MUC-3 scoring program used only the latte rstrategy, the scoring optimization strategy .
Thus, no matter how bad the fit in th econtent of the template, a mapping would be permitted.
The MUC-3 metho dtherefore hid the fact that a response template and a key template wer erepresenting completely different incidents .In addition to these changes to the scoring software, the test protocol wa smodified so that the focus of most of the attention was shifted from th e"Matched/Missing" method of scoring, which penalizes at both the template an dslot-value level for missing information but only at the template level for spuriou sinformation, to the "All Templates" method, which penalizes at both levels for bot htypes of error.
Greater emphasis was also placed on viewing a system's recall an dprecision as a rectangular "region of performance", whose boundaries are define dby the four methods of assessing penalties .
This view of performance reflects theassumption that real-world aplications would vary according to their degree oftolerance of missing data versus spurious data .
See test procedure (appendix B) andscatter plots (appendix H) .Isolation of Text Filtering Performanc eOverall, approximately 50% of the texts in the MUC-3 and MUC-4 corpora areirrelevant to the information extraction evaluation task .Thus, a significan tsubtask is to discriminate between relevant and irrelevant texts .
MUC-3 scoreswere computed based on performance at the template level, rather than on themessage level, making it difficult to derive a text filtering score .
To measure thetext filtering capabilities of the MUC-4 systems directly, scores were assigned at themessage level and combined using a contingency table .This is discussed furtherin [3] and [4] .Better Isolation of Language Analysis Performanc eSeveral changes to the template design were made in order to better isolate th esystems' capabilities with respect to the kinds of text processing required to mee tthe differing information extraction requirements (appendix A) :1 .
Slots in the MUC-3 template that contained composite values were split int otwo slots .
Thus, a MUC-3 slot (TYPE OF INCIDENT) filled with the valueATTEMPTED BOMBING became two MUC-4 slots (INCIDENT : TYPE and INCIDENT :STAGE OF EXECUTION) filled with BOMBING and ATTEMPTED, respectively ;similarly, a MUC-3 slot (HUMAN TARGET : ID) filled with the value "MARI O6FLORES" ("STUDENT") became two MUC-4 slots (HUM TGT : NAME and HUM TGT :DESCRIPTION) filled with "MARIO FLORES" and "STUDENT" : "MARIO FLORES" ,respectively .2.
A new string-fill slot (INCIDENT : INSTRUMENT ID) was added foridentifying the instrument of an attack (e .g ., "CAR BOMB") ; this slot is paired wit hthe set-fill slot (INCIDENT : INSTRUMENT TYPE) that was used for MUC-3 an dthat now contains a cross-reference to the string-fill slot (e .g., VEHICLE BOMB :"CAR BOMB") .3.
New slots (PHYS TGT : NUMBER and HUM TGT : NUMBER) were added fo rthe number associated with each physical and human target (e .g., 3 : "POWERPYLONS" and 4 : "ENERGY TOWERS"), supplementing the information in the tota lnumber slots (PHYS TGT : TOTAL NUMBER and HUM TGT : TOTAL NUMBER) .The usage of the slots containing total numbers was restricted to cases where th einformation was not redundant (i .e., to cases where there is more than one suchtarget) and was explicitly mentioned in the text (i .e., cases where no computatio nby the system is required) .4.
The usage of the ATTACK incident type was extended to cover all murde rincidents ; cases were eliminated in which MURDER templates existed in th etraining set, either by deletion or by conversion to ATTACK, depending on th ecircumstances .
95.
The slot ordering was changed so that groups of dependent slots appea rtogether, and the scoring software was updated to compute subtotal scores for eac hgroup.
These groups were termed "pseudo-objects" since they were incorporated a sa compromise between retaining the flat template format and replacing it with a nobject-oriented format .
The experimental test designed and conducted by Genera lElectric [5] was an attempt to find out what would have happened if the templat eformat had been overhauled; the pseudo-object computations were essential fo rthat test .6.
The scoring software was updated to include a "STRING FILLS ONLY" row toshow how system performance on string-fill slots compares with performance o nset-fill slots, for which a "SET FILLS ONLY" row already existed .Assessment of System Independence from the Training Dat aThe reuse for MUC-4 of the same domain and fundamentally the same task asused for MUC-3 raised the concern that the "generality" of the systems would comeinto question .
To address these concerns, a controlled generality test was added t othe test protocol .
The MUC-3 corpus originated from an Foreign Broadcas tInformation Service (FBIS) archival database containing news articles (from "FBI S9 For MUC-3, any incident type other than ATTACK that resulted in the death of one of thehuman targets was represented in two templates, one of which was a MURDER template .
AnATTACK incident that resulted in death to only a subset of the targets was similarl yrepresented in two templates .
For MUC-4, these "dependent" MURDER templates were deleted .
"Stand-alone" MURDER templates, which were created when the result of an attack was death t oall targets, were converted to ATTACK templates .7Daily Reports") that had been disseminated as messages [1] .Nearly all thos earticles carried datelines from 1989 through early 1990 ; just a few were from 1988 .For the generality test, over 900 different articles of the same varieties as thos ecomprising the MUC-4 corpus were retrieved from a CD-ROM covering August -December 1988, and a sample of 100 was selected as test data and labeled TST4 .Sampling factors included the total number of texts for each month in the corpusand, as for the MUC-3 corpus, the total number of texts for each country of interes tin the corpus .
Thus, the two corpora, including the test sets, report somewha tdifferent incidents and show where the hotbeds of terrorist activity differ fro mthe one time span to the other.
l 0Assessment of System Development Progress since MUC- 3For MUC-3, a study was carried out to measure the complexity of the MUC- 3evaluation task vis-a-vis the previous evaluation, and the scores obtained in th eprevious evaluation were recomputed using the MUC-3 method of scoring [6] .
Theevidence was that the MUC-3 task was considerably more complex in most regard sand that the MUC-3 scores were about half as good (had twice the shortfall from theupper bound).
The conclusion was that the increase in difficulty in the task morethan offset the decrease in scores, showing that significant progress had beenmade .In the absence of an established, comprehensive methodology, this comparisonwas necessarily crude since the two evaluations were so different with respect t ocomplexity of the data, corpus dimensions, nature of the task, and scoring o fresults .
In contrast, the differences between MUC-3 and MUC-4 are much les sradical, and it was possible to design a controlled comparison between the two.
Infact, an attempt was made to neutralize the differences entirely by forward -converting the materials from the MUC-3 final test to the MUC-4 format .
Convertedmaterials include the TST2 key and response templates and the cumulative TST 2history file .
11 In addition, the scoring program was configured to disregard thos eslots in the template that were new for MUC-4 (INCIDENT : INSTRUMENT ID,PHYS TGT : NUMBER, HUM TGT : NUMBER) and those that had been incompatibl yredefined for MUC-4 (PHYS TGT : TOTAL NUMBER, HUM TGT : TOTAL NUMBER) .NRaD restored TST2 for the MUC-3 veteran sites ; the scoring was donenoninteractively, using the converted cumulative history file .
The MUC-4 testprotocol required that all MUC-4 participants do a comparable scoring of TST3, i .e .
,10 Other differences that existed between the corpora were eliminated .
For example, the ne wcorpus was obtained in mixed upper and lower ease .
TST4 was converted to all upper case inorder to be consistent with the original corpus .
Also, the new corpus was not stored in th eform of messages and, as a consequence, long articles appeared in their entirety rather tha nbeing broken up .
Any long texts that were selected for inclusion in TST4 were scanned fo rterrorism key words, and all but a one- to one-and-one-half-page section of text containing on eor more of those key words was thrown out.11 The history file contains a record of all interactive scoring decisions ; the cumulativ ehistory file is built up as NRaD scores each system .
The scoring program does not query theuser if the history covers the case in question .
This feature ensures consistency of scoringacross systems .8one in which all slots except those mentioned above are scored .
NRaD and the MUC -4 participants used the same version of the scoring program (version 3 .3) .More Consistent Scorin gThe scoring program was updated to further automate the scoring of set-fil lslots--the user is now queried only when a set-fill value is cross-referenced to astring-fill value that the scoring program cannot automatically score .It was alsoupdated to score some string fills automatically .
The coverage of the interactiv escoring guidelines (appendix C) was extended .
These updates were meant to ensur egreater consistency in template scoring among people and across scoring runs .The test protocol required that all participants score their own templates .NRaD subsequently rescored the basic test runs for the two new test sets, TST3 an dTST4; however, runs such as the one using TST3 to measure progress (describe dabove) were not rescored.
In terms of the overall scores for TST3, there was verylittle difference (0-2% in recall or precision) noted between those that the sitesreported and those that were produced when NRaD rescored the outputs .
For TST4,the differences ranged from 0-4% .The actual differences due to subjective scoring are much smaller, however .This is because the rescoring done at NRaD used a slightly updated version of th escoring program (version 3.4a) and a slightly updated version of the answer keys .With respect to the latter, there were more updates made to TST4 than to TST3 ;hence, the greater range in scoring differences for TST4 .
As another side note, i tis the case that the NRaD overall recall and precision scores are almost alwaysslightly higher than those the sites reported ; this is probably because NRaD was i na position to interpret the interactive scoring guidelines more liberally than thesites were, while maintaining consistency in subjective decisions across systems .Means to Make Valid Score Comparisons Among System sA well-defined set of evaluation metrics was used for MUC-3, and for the firs ttime, the metrics were implemented as software.
This enabled the production ofmeasures of performance at the slot and template levels and measures for subset sof the data (e .g., for only the set-fill slots, for only certain slots in certai ntemplates).
With this wealth of data, together with new confidence in the validit yof the scores and the maturing state of development of many of the systems unde revaluation, there was a growing need for a valid means of making direct cross -system performance comparisons .For MUC-3, there were no scientific grounds for saying that a systemperforming at 50% recall and 50% precision was doing "better" than on eperforming at 30% recall and 70% precision .
The only justification for such aclaim came from the test protocol, which specified that the run submitted by eachsite as the system's "required" run be one in which the recall and precision score swere optimized to be as similar as possible .
Furthermore, there were no ground sfor claiming that a system that got 50% recall and 50% precision was significantlybetter than one that got 48% recall and 48% precision.Two innovations in the area of scoring were made to address these issues .
First ,a scientifically sound, single-score measure was incorporated that enabled system s9to be ranked .
This measure, known as the F-measure, allows different weighting sof recall and precision .
When they are weighted equally, it does what was onl yimplied by the MUC-3 test protocol, i .e ., it would rank a system with 50% recall and50% precision higher than one 30% recall and 70% precision .
Second, a method ofdoing statistical significance testing was incorporated into the test protocol .
This isa computer-intensive method that uses an approximate randomization approach ;for MUC-4, it was used for TST3 and TST4 to determine the significance of th eoverall F-measure scores and All Templates scores .
These innovations are discusse dfurther in [3] and [7], respectively .Shortcomings in the EvaluationA number of shortcomings in the evaluation remain .In fact, one of theinteresting outcomes of MUC-4 was the extent to which the improved syste mperformance brought out the task deficiencies .
It is not difficult to define a ninformation extraction task but perhaps even more difficult to make neede dimprovements without jeopardizing the schedule, placing an undue burden on th eevaluation participants, or incurring large costs in terms of updating existin ganswer key templates and documentation.
The compromise reached for MUC-4 wa sto minimize the changes to the task definition and to focus instead on makin gimprovements to the evaluation metrics and scoring software .Among theremaining shortcomings of the evaluation are the following:1.
The flat template structure created problems as far as meaningfully an dconsistently expressing inherently recursive kinds of data such as levels o fdescription for perpetrators and human targets.
The perpetrator slots allowed for atwo-level distinction, with very poor conventions for deciding what to do if the tex tmade more levels of distinction than that, e .g., three levels in "Miguel	 Vasquez, amember of the Jacobo Carcomo Command of the FMLN".
The human target slots hadmore explicit but still inadequate conventions for entering whatever levels o fdescription were needed to correspond to fillers of other slots, e .g., "five peoplewere injured, including two security guards" .
Another consequence of the flattemplate structure was the requirement to encode explicit cross-references ,greatly complicating the scoring algorithms .2.
The definition of a "relevant terrorist incident" was inadequate in severa lrespects .
The distinction between a terrorist incident and other kinds of violen tevents -- guerrilla actions, shootings of drug traffickers, etc .
-- is a difficult one t oexpress in comprehensive, hard and fast rules .
It was also difficult to express therelevance criteria of "specificity" and "recency" in a way that could beconsistently applied.
The intent was to not do extraction unless some specifi cinformation was present that a database user would find useful ; for example ,extraction would not be done if no particular incident was being referred t o("terrorist bombings have been taking place with increasing frequency", "ove r100 bombings have taken place in the last two weeks") .
If an incident was reportedas having taken place more than two months prior to the date of the article, n oextraction was to be done unless the article gave "new" template-fillin ginformation, e .g., when a new suspect was being brought forth .
However, withoutprior knowledge of the actual incident, it was sometimes difficult to tell whethe rthe information that was being reported was new or not .
These problems ofdetermining relevance were partly due to the task definition and partly due to th einherent vagueness of the texts .103.
There were small gaps in the template fill rules .
For example, the rulesconcerning stories that give contradictory evidence about some of the facts wereinadequate .
A more frequent problem was that the set-fill lists for physical an dhuman target types were sparse and sometimes vaguely defined, and some of theseproblems had consequences for determining relevance at the template level .
Forexample, if a text describes the target of an incident only as a "naval attache", th eincident is relevant if the target is classified as DIPLOMAT but irrelevant if th etarget is classified as ACTIVE MILITARY .4.
In terms of the scoring, there were several relatively minor bu ttroublesome problems .
A bug in the scoring program was discovered just prior t ofinal testing, and a change was made to the scoring program and the interactiv escoring guidelines just prior to final testing that had to be retracted when NRa Drescored TST3 and TST4.
The largest number of problems were those that involve dmaking subjective judgments during interactive scoring .
For example, string fill sthat closely resembled the ones in the key but originated from remote places in th etexts had to be examined in context to determine whether they were "fortuitousl ycorrect" (as, perhaps, in the case of "urban guerrillas" as a substitute for "urba nterrorists") or "infortuitously incorrect" (as in the case of "11 peasants" as asubstitute for "3 peasants") .
Making principled decisions about awarding partialcredit was also difficult when the cases weren't specifically covered by th einteractive scoring guidelines .5.
The change in template mapping strategy described earlier as a nimprovement made for MUC-4 had one consequence that was at least potentiall yproblematic .
The problem is due to the inflexibility of one of the mappingconditions, namely the requirement that there be at least a partial match on th efiller for INCIDENT : TYPE .
A partial match existed when the response wasATTACK, and the key was any other value; this scoring is based on ATTACK being asupercategory of the other set-fill options .
In the reverse case, however, theresponse is scored incorrect, thereby disallowing the mapping and, as describe dearlier, resulting in penalties for having generated a spurious template and fo rhaving missed a template .
The disallowance of a mapping simply on the basis of a nincorrect incident type is probably too extreme .
(In practice, however, it appearsto have rarely had significant adverse consequences ; see UMass paper in Part II a sone example of it having apparently significantly affected their TST 4performance .
)At a higher level, there are shortcomings that are due to the choice of task .Information extraction has served as an excellent vehicle for elucidating theapplication potential of current technology ; however, its utility as a vehicle fo rfocusing attention on solving the "hard" problems of NLP is not as great .
Manyinsights have been gained into the nature of NLP by experience in developing thelarge-scale systems required to participate in the evaluation .
Nevertheless, somuch effort is involved simply to make it through the evaluation that it takes adisciplined effort to resist implementing quick solutions to all the major issue sinvolved, whether they are well understood problems or not .The attempts that have been made to use the information extraction task t oreveal language analysis capabilities specifically have so far met with limite dsuccess.
One of these examined the results of information extraction at the loca llevel of processing (apposition handling), and the other looked at the global leve l11of processing (discourse handling) .
The former was carried out for MUC-3 [8] an dthe latter for MUC-4 [9] .
The major conclusions of the apposition test were that thetest was isolating the phenomenon to some extent and that the systems as a grou pwere doing better on the cases that had been hypothesized as easier than on thosethat had been hypothesized as more difficult .
However, it also appears thatperformance on the apposition test may have reflected the systems' slot-fillin gcapabilities at least as much as their apposition analysis capabilities .
Appositionwas chosen as the subject of the test partly because of the relatively hig hfrequency of occurrence of the phenomen; however, a substantial portion of th ecases introduced confounding factors and had to be thrown out .
The majorconclusion of the discourse processing test was that the texts that were expected t obe "easy" were not and that there was something about the composition of the smal ltest samples that were used that was confounding the results.
Although thereseems to be no theoretical impediment to conducting successful fine-grained task -oriented tests, these two efforts seem to show that such tests cannot be designed a sadjuncts but rather require independent specification in order to ensure adequat etest samples and an appropriately designed information extraction task .DISCUSSION OF TEST SETS AND TEST RESULT SAppendix B describes the performance objectives of the evaluation, th ecomponents of the test, how the sites were to conduct the tests and score th eoutputs, and what files the sites were to submit to NRaD after finishing the tes tprocedure.
Appendix G contains summary score reports for the component tests ,and appendix H displays some of those results in the form of scatter plots.
Thediscussion below concerns the results for the basic test components, namely TST3 ,TST4, and the TST2/TST3 "progress" test .
The "adjunct" tests that are mentioned i nappendix B are reported on in [5] and [9] .TST2/TST3 "Progress" Tes tThe progress test made a controlled comparison between MUC-3 and MUC- 4performance .
The data points for MUC-3 were obtained using the templates tha tthe veteran participants' systems generated on the MUC-3 final test on TST2 .
Thedata points for MUC-4 were obtained for all MUC-4 sites; they were obtained usingthe templates generated on TST3 .
As described earlier, the TST2 test materials wereforward-converted to the MUC-4 format, and scoring included only those templat eslots whose MUC-3 and MUC-4 definitions were consistent .
1 212 The TST3 progress scores are generally slightly better (up to 2%) than TST3 "base" scores ;this difference is the result of having excluded the number slots and the instrument ID slotfrom the scoring on the progress test .The TST2 progress scores are generally substantially worse (at least 5% lower recall o rprecision) than the MUC-3 TST2 "base" scores reported on in [3] .
The changes (primaril ydecreases) are due to such factors as the following:1.
The manual clean-up of the automatic forward conversion of the templates is subjectto a small degree of error.
The elimination of some MURDER templates via conversion toATTACK templates could result in an underestimation of performance ; the splitting of th ehuman target ID information into two slots could result in an overestimation of performance.2.
Since scoring of the TST2 templates for the progress test was done in batch, withoutany manual template remappings, performance may be slightly underestimated for the few site s12Following are some of the hypotheses that were to be tested concerning theperformance of the MUC-3 veteran systems :1.
Most MUC-3 veteran systems would improve on at least one measure .2.
Systems that were at the leading edge of performance for MUC-3 might no tbe able to attain higher scores on one measure without sacrificing performance o nanother .3.
The limitations of some approaches might emerge .4.
The need for progress in certain research areas might become salient .5.
The fairest (and most generous) view of progress would come from th eMatched/Missing row, which was the focus of the MUC-3 test, rather than from th emore stringent, All Templates row, on which the MUC-4 TST3 and TST4 tests focused .A comparison of the tables in section 4 of appendix G shows improvement o none of the three primary measures (recall, precision, overgeneration) by all 1 1systems, given the Matched/Missing method, and by 10 of the 11 systems, given th eAll Templates method .
Improvements on all three measures were achieved b ythree systems on Matched/Missing (GE, LSI, NYU), including two of the leadingMUC-3 performers (GE, NYU), and by seven systems on All Templates (GE, LSI, NYU ,PRC, SRI, UMBC-ConQuest, UMass), including several of the leading performers (GE ,NYU, SRI, UMass) .
Tradeoffs resulting in improved recall at the expense of lowe rprecision are evident in the results for three systems on Matched/Missin g(Hughes, Paramax, UMass) and for one system on All Templates (Paramax) .Tradeoffs leading to improved precision at the expense of lower recall can be see nin the results for two systems on Matched/Missing (BBN, MDC) and one system onAll Templates (BBN) .M/M&AT REC M/M PRE AT PRE M/M OVG AT OV GMAX CHG FOR WORSE- 7 -10 -19 + 14 +2 6MAXCHGFORBETTER +22 +10 +34 -8 -4 2AVERAGE CHG +8 +2 + 11 +2 -12Table 1 .
Differences Between TST2 and TST3 Scores on Progress Tes tThe differences in scores between the two test sets are summarized in Table 1 .The differences are calculated as the TST3 progress score minus the TST2 progres sscore.
The first row shows the worst degradation among the 11 systems, the secondrow shows the most improvement, and the third row shows the average change .that made substantial use of this facility ; however, the need for this facility has declined a sthe template alignment capabilities of the scoring program have improved .3.
The elimination of some MURDER templates via deletion eliminated one source o finflation of scores .4.
The scoring program now uses more stringent criteria when aligning templates ; theimpact is generally a higher missing template count, which lowers recall, and a higher spuriou stemplate and slot-filler count, which lowers precision .1 3Note that there is only one column for recall, which is unaffected by the choice o fMatched/Missing (M/M) versus All Templates (AT) .Recall improved by an average of eight percentage points .
On average, therewas very little change in precision and overgeneration on Matched/Missing, bu tAll Templates shows dramatic improvement on both measures .
It is interesting thatthe progress is more evident on All Templates than on Matched/Missing : for nineof the eleven systems, the All Templates precision and overgeneration scores sho wa larger improvement from MUC-3 to MUC-4 than do the Matched/Missing scores .It appears that the new focus on the All Templates row caused developers to devotea great deal of attention to reducing overgeneration (thereby increasin gprecision), and that they succeeded .Furthermore, of these nine systems, eigh tshowed improved recall as well .The F-measures provide assistance in interpreting the results of this test ,especially for those systems that exhibited a recall-precision tradeoff.
The F-measure scores show whether or not the tradeoff paid off in terms of overal lperformance .
Figure 1 shows the MUC-3 veteran systems' All Templates F-measur escores (with recall and precision equally weighted) from the tables in appendix G ,section 3 .n TST2TST3Figure 1 .
All Templates F-Measure (P&R) for Progress Test on TST2 and TST 3Figure 1 shows that a "typical" increase in F-measure performance is aroun d10 points (BBN, GE, LSI, NYU, UMass), and two systems (PRC and SRI) show a muc hgreater performance improvement than that.
The SRI results are especiall yremarkable because of the radical differences between their MUC-3 and MUC-4systems.
The BBN results show that the tradeoff in performance they made fo rMUC-4 clearly paid off in terms of overall progress .14Two of the systems exhibiting only a slight performance increase (Hughes ,Paramax) do little or no linguistically-based processing ; by their developers' ownadmission, the systems are incapable of much higher performance unless they ar eaugmented by other types of processing.
The remaining two systems, MDESC andUMBC-ConQuest, were overhauled for MUC-4 .
In the case MDESC, this overhaulresulted in lower overall performance than what was achieved for MUC-3 ; in thecase of UMBC-ConQuest, it resulted in a modest increase but still very low overal lperformance .
It should be noted that the level of effort that .
could be afforded b yeach of these four sites was minimal and that this undoubtedly was a significan tlimiting factor.Systems representing organizations that are not veterans of the MUC- 3evaluation are not included in the above discussion .
They were tested on the TST3portion of the progress test .
Their scores are included in appendix G, section 4 .In summary, the progress test showed that higher levels of performance b ynearly all systems were achieved despite the relative difficulty of TST3 .
Progresswas more evident when the All Templates scores are considered; this is due to thesuccess of most systems in controlling overgeneration .
Most systems did not giv eevidence of a recall-precision tradeoff, which means that there is still a variety o ftechniques that exhibit potential for attaining even higher levels of performancein the future .
The few systems that exhibited a tradeoff clearly benefited from it interms of overall performance .
However, minimal improvement was shown b ysystems that do not use linguistically-based processing, and minimal progress o reven a degradation in performance were the result in a couple cases wher esystems were radically changed for MUC-4 .TST3 and TST4 Test sThis section describes the "base" MUC-4 tests, which used the TST3 and TST4 tes tsets.
As distinct from the progress test discussed in the previous section, the basetests scored the entire template rather than selected slots .
Thus, the TST3 scores fo rthe two tests can be different, but in reality differences turned out not to b euniversal .
Where differences do exist, they are fairly small -- the overall recall ,precision, and overgeneration base scores are at most three points lower than th eprogress scores .As described earlier in this paper, TST3 consists of a sample of 100 previousl yunseen texts from the corpus of FBIS texts that had been obtained prior to MUC-3 .The sampling method ensures that the test set contains the same percentage o ftexts by country as the corpus as a whole; aside from enforcing that constraint ,sampling is done blindly .
The TST4 test set consists of a sample of 100 texts from th enew corpus of FBIS texts that was obtained via CD-ROM specifically for MUC-4 .The density of relevant information in TST3 is relatively high, making it i nsome ways a more difficult test set than others .
The density of relevan tinformation in TST4 is much more similar to TST1 and the training set than it is t oTST2 and TST3, making it in some respects a relatively simple test .
Some of thedifferences between TST3 and TST4 are summarized below .1 .
Approximately two-thirds of the texts in TST3 (65 out of 100) fall in th e"definitely relevant" category, versus approximately one-half in TST4 (48 out 100) .152.
Almost one-half the texts in TST3 (30 out of 65) require the generation o fmore than one template, versus almost one-third in TST4 (15 out of 48) .3.
Many templates include a greater density of information than usual ,especially in such slots as HUM TGT : DESCRIPTION .In reality, TST3 is just a bit more difficult by each of these criteria than TST2 ,which was used for final MUC-3 testing .
13 However, TST2 was itself more difficul tthan TST1 and the training set .
1 4As mentioned earlier, the purpose of introducing the TST4 test was to learn t owhat extent system performance is independent of the training data .
The variableintroduced by TST4 was the time span covered by the texts .
The change in timespan meant that a somewhat different set of incidents would be reported -- n oincidents occuring later than 31 December, 1988 would be reported in TST4 ,whereas incidents up through early 1990 would be reported in TST3 .
It also meantthat the incidents would reflect a different world situation, resulting in a differentdistribution of articles among the countries of interest .The major differences i nthis respect were in the number of articles about El Salvador (down from 40% i nTST3 to 25% in TST4), Chile (up from 5% to 18%), and Peru (up from 6% to 19%) .The hypotheses to be tested were that systems would not perform as well o nTST4 as on TST3 and that systems that rely more heavily on corpus-based statistic swould suffer a greater hit in performance than other systems .
The results ,however, are mixed with respect to the first hypothesis and apparently negativ ewith respect to the second.
Table 2 presents a summary of the All Templates score sfor the base runs on TST3 and TST4 (appendix G, sections 1 and 2), including thefloating point F-measure with recall and precision equally weighted (appendix G ,section 5) .REC PRE OVG F-MEA S(R&P)TST3 BEST 5 8 5 5 26_ 5 6 .01_TST4 BEST 6 2 5 3 3 4 -5 7 .0 5TST3 WORST 2 8 9 0 4 .4 7TST4 WORST 3 10 8 7 5 .7 9TST3 AVG_ 31 34 55 31 .3 5TST4 AVG 35 33 57 32 .26 ,Table 2.
Summary of TST3 and TST4 All Templates Scores (Base Test )The TST3 scores are quite similar to the TST4 scores, despite the difference snoted between the test sets .Naturally, however, the degree of similarity varie s13 Several participants did not use TST2 to train on for MUC-4 ; instead, they reserved it fo ruse as blind test data for internal tests .
When reported in the papers in Part II (e.g., by SRAand SRI), the results seem to confirm the degree of similarity between TST2 and TST3, in th esense that the systems did just slightly worse on TST3 than on the last internal test run o nTST2 .14 A table of some summary statistics concerning all four test sets and the training set i sincluded in the BBN paper in Part II .1 6from one system to the next .
Inspection of the individual systems' scores show sthat only two systems (LSI, UMich) had both lower recall and lower precision o nTST4 than on TST3, and the degradation in recall for LSI is only one percentagepoint .
For two other systems (PRC, SRI) recall was the same on both test sets, whil eprecision was lower on TST4 .
Eleven systems showed higher recall and lowerprecision on TST4.
Two systems (USC, MITRE) scored higher recall and higherprecision on TST4 .
Where there was a difference in recall or precision, the degre eof difference is as great as 11 recall points and 13 precision points (cf appendix H ,figure H7) .The F-measure value (with recall equally weighted with precision) is highe ron TST4 than on TST3 for 10 of the 17 systems (BBN, GE, GE-CMU, Hughes, MDESC,MITRE, NYU, SRA, UMBC-ConQuest, USC), is less than two points lower on TST4 fo rfour others (NMSU-Brandeis, Paramax, PRC, SRI), and is more than two points loweron TST4 for only three systems (LSI, UMass, UMich) .
The absolute ranking s(without considering whether the differences are statistically significant) sho wsix systems ranked the same on both test sets, ten changing rank by just on eposition, and one changing rank by two positions .
Thus, in a very real sense, thedifferences in performance from a cross-system perspective are minimal, and i tcan be concluded that the two test sets are giving consistent results .The overall performance of more than half the systems was better on TST4than on TST3, as determined by the F-measures .
The relative straightforwardnessof the TST4 test set may have washed out or even reversed the predicted behaviorwith respect to recall .
15 The expected negative effect of using a corpus spanning adifferent period of time was not seen ; it would be necessary to place more control son the information density characteristics of the test sets in order to isolate such afactor .BBN, GE, NYU, SRI, and UMass submitted the results of optional tests conducte dusing TST3 or both TST3 and TST4 .
The optional tests explored ways of controllin gsystem behavior to produce recall-precision tradeoffs that were predicted to besuboptimal overall (compared to the base run) but distinctly better on one measureor the other.
These tests varied greatly in their design and in the performanceimpact; further information is available in appendices G (section 3) and H (figure sH3, H4, 119) and in the papers in Part II .A few general comments can be made on the basis of the scatter plots i nappendix H concerning the overall performance of the systems .
Figures H1 and H 2show that higher recall is usually correlated with higher precision, just as th eMUC-3 results showed.
Therefore, once again there is no reason not to beoptimistic about seeing continued improvement on both measures in the future .Figures H5 and H6 plot overall recall versus overgeneration ; they show that, to alarge extent, the overall precision scores seen in H1 and H2 are accounted for b ythe overgeneration factor .This shows that overgeneration is still a seriou sproblem, although MUC-4 clearly demonstrated that a great deal of progress ha dbeen made in this area .Clearly also, the problem of missing information is stil lserious, as witnessed by the fact that recall is still only moderate .15 With respect to precision, it should be noted that the two systems that showed better recal land precision on TST4 than on TST3 (USC and MITRE) are less mature than most, which ma ymake their performance less predictable .17The question of how to assess the state of the art has to be addressed in part b ycomparison to human capabilities, since the real-life challenge is still for system sto try to match the performance of well-trained people .
Although the humanperformance limits have not been scientifically determined, they are no westimated to be in the neighborhood of 75% recall and 85% precision, assuming th eAll Templates scoring method and a representative test set .
These figures may seemlow; however, the experience of generating the key templates for theseevaluations suggests that they are not .Human factors play a role in estimatin gthis limit; however, the major factors are the task deficiencies and the inheren tambiguity and vagueness of the texts .
These performance goals mean, therefore ,that the leading systems are falling perhaps 15% short of the recall target and 30 %short of the precision target .Figures 1110-H12 plot the "regions of performance" of the systems as defined b ythe overall Matched/Missing, Matched/Spurious, Matched Only, and All Template srecall and precision scores .
There are some interesting differences in the shape a swell as the size of those regions.
For the systems displaying the smallest regions ofperformance (H10), the shape is rather square, or it is elongated more horizontall ythan vertically .In contrast, the regions in H11 and to an even greater extent th eregions in H12 are distinctly rectangular and elongated vertically .
These shapesare evidence that the systems in H10 are least affected by overgeneration ; those in1112 are most affected .
There is some comparative proof that the MUC-3 veteran swere bringing overgeneration under control in the fact that H12 includes onl yone veteran systemFigures H15-H18 show that, as anticipated, system performance on slot srequiring string fills would be worse than on those requiring set fills .
Thedifferences would probably be more striking if it were not for the fact that th escoring of eight of the eleven set-fill slots is confounded by the cross-referencesattached to them.
(In contrast, just one of the six string-fill slots has a cross -reference requirement .)
Whether for this reason or not, it does not appear thatthe distinction in slot type serves as a discriminator among systems, since ther eare no dramatic differences in the relative position of the systems in th econtrasting graphs across both test sets .GENERAL OBSERVATION SThere are many ways in which MUC-4 has surpassed MUC-3 in bringin gvarious aspects of the evaluation into focus, including the deficiencies remainingin the task that were described earlier.
The challenge posed by the task appear sless imposing now -- it is now the rule rather than the exception to find systemscapable of exploiting the large training corpus of texts and templates for thepurposes of knowledge acquisition, automatic training, and internal testing .
Theinteraction between systems engineering concerns and theoretical concerns i sreceiving increasing attention .
In particular, scalability and robustness issue smust be addressed in order to take full advantage of the corpus for trainingpurposes and to perform as well as possible on new test data .Whereas the challenge posed by the task has come to be accepted more or les sas a matter of course, the burden of preparing for the evaluation is increasingl yfelt .Beneficial effects of the task challenge and evaluation burden are, amon g18other things, that the algorithms for dealing with large amounts of unrestricte dtext have become more robust, the development cycle has gotten shorter, and th eamount of automated knowledge acquisition has increased.
On the down side, th eevaluation burden is still such that quantifiable progress is slow ; there is still astrong sentiment that time is the primary limiting factor, not technology, and thattherefore level of effort is one of the most significant factors in predictin gperformance .However, even though one impediment to improved performance is th eamount of time that can be invested in just doing a lot of hard work, including agreat deal of knowledge engineering and system engineering, it is even moreapparent from MUC-4 than from MUC-3 that there are certain prevalent "har dproblems" posed by the task that require serious study .
One thing that has beennoted is how small problems in early stages of processing can have large negativ eeffects on the ability of later stages to do their job .
MUC-3 (and earlier evaluations )pressed the point of reducing the fragility of sentence-level processing, and th esentence analyzers were developed to produce output even when they didn't hav efull coverage .
MUC-4 has refocused attention on the sentence and the importanc eof doing more complete linguistic analysis at that level .Another thing that has become nearly universal experience is the inadequac yof current approaches to determine when and how to combine information fro mmultiple sentences into a single, coherent representation .
Although theapproaches are limited in effectiveness by the quality of the sentence-leve linterpretation, they are also inherently limited in their ability to incorporat einformation from sentences that lack domain-specific "key words", to incorporateinformation from anaphors (especially from definite noun phrases), and to dea lwith interruptions in the discourse .
Currently these discourse phenomena ar egenerally dealt with in terms of template "splitting" and "merging" based on th ecompatibility of data in the output representation rather than by trackin gdiscourse as part of the analysis process .
Some of these issues are apparent in theparticipants' discussion in Part III of the "system walkthrough" example (appendi xF) .The techniques that were used to improve performance above MUC-3 levelsstill vary greatly, but the emphasis on hybrid systems combining linguistic an dnonlinguistic processing has increased, and the limitations of the purel ynonlinguistic approaches are very evident .
As the viability of informationextraction as a useful application of NLP has increased, the idea of building system sspecifically for that purpose has emerged, and there is beginning to be a divisionbetween those who would insist that the most successful systems will be the mostgeneric ones with respect to application task and domain and those that believ ethat the most successful systems will take advantage of whatever reductions i nlevel of sophistication are permitted by the task of information extraction .
At thebottom is the question of what it will take to get from the current limit of about 60 %recall and 55% precision to the estimated upper limits of human performance .
Alsoat issue is the issue of portability in terms of system architecture and portability i nterms of cost.
Will it cost less to port a large, complicated system that has separat edomain-specific modules to a new domain and/or task, or will it cost less to port asmaller, simpler system to a new domain and to build a new system for a new task ?19CONCLUSIONSNew performance standards were set on the MUC-3 and MUC-4 informatio nextraction task.
Despite increased task difficulty and scoring stringency for MUC-4, the results of a MUC-4 test to measure progress since MUC-3 show substantiall yhigher overall performance for most systems (at least 10 points higher on the F -measure) .
It has now proven possible to achieve overall scores above 60% recal land 55% precision and an F-measure exceeding 55 .
The new challenge to contro lovergeneration was successfully met, although overgeneration is still hig henough that it exerts a major negative impact on precision .The results of a test to measure the generality of the MUC-4 algorithms sho wthat they were not overly tuned to the training set.
The usage of a test set from acorpus spanning a different period of time than that of the original corpus wasexpected to have a negative effect on performance, but this effect was not seen .
Itwould be necessary to place controls on the information density characteristics o fthe test sets in order to isolate the time factor.Upper limits on human performance of the task are estimated to be 75% recal land 85% precision, primarily due to deficiencies in the task definition an dexpressiveness of the formalism and to the inherent ambiguity and vagueness o fthe texts .
System performance falls short of these levels by at least 15 recall point sand 30 precision points.
However, some MUC-4 systems attained high enoug hperformance that task deficiencies account for a significant portion of th epenalties incurred by the scoring .Clearly, the performance envelope could have been pushed out even farther i fthe participants had had the opportunity to work on the systems steadily for theentire year.
The level of effort is reflected to some extent in the scores, and timewas again a limiting factor .
The differences in sophistication among the system smay be great, but these differences may not be so great in terms of the scores .However, it could well be that there is a great qualitative difference between an F -measure score of 45 and one of 55 .
Since the task deficiencies are being raised as alimiting factor and certain theoretical issues such as those involving sentence-and discourse-level analysis are becoming limiting factors as well, it may b epossible to conclude that the ceiling on performance is much more perceptibl ethan it was after MUC-3 and that major steps forward in the state of the art may no tbe easy to obtain.Error analyses point toward the critical need for research in areas such asdiscourse reference resolution and inferencing .
For example, the inability toreliably determine whether a description found in one part of the text refers o rdoes not refer to something previously described inhibits both recall and precisio nbecause it could result in either missed information or spurious information; theinability to pick up subtle cues to relevant information places a limitation on recal lbecause it results in missed information .
The ability to take advantage ofsophisticated approaches to discourse that have already received computationaltreatment is limited by their dependence on error-free outputs from earlier stage sof processing .There is a need for renewed attention to robust processing at th esentence level .20It is time to move on to a different information extraction task and domain i norder to make further progress in the evaluation methodology and to ensure tha tthe challenge to handle unrestricted text remains high .
MUC-4 has clarified manyof the issues pertaining to the definition of a performance evaluation using a ninformation extraction task ; at some point, it will be worthwhile to try to design amore comprehensive performance test of NLP capabilities than what th einformation extraction task covers .ACKNOWLEDGEMEN TAll the evaluation participants are to be congratulated for their efforts insupport of MUC-4.
I would like to thank Nancy Chinchor, Ralph Grishman, JerryHobbs, David Lewis, Lisa Rau, and Carl Weir for their commitment as programcommittee members to improving the evaluation methodology and for thei rattention to the deluge of email communications.
Thanks also to PRC, Inc ., forhosting the conference and to Richard Tong and Lynette Hirschman in their role sas "outside evaluation experts" .
The NRaD work was supported by DARPA/SIST Ounder ARPA order 6359.REFERENCE S[1] Proceedings of the Third Message Understanding Conference (MUC-3), May,1991, Morgan Kaufmann .
[2] Sundheim, B ., Plans for a Task-Oriented Evaluation of Natural Languag eUnderstanding Systems, in Proceedings of the Speech and Natural LanguageWorkshop, February, 1989, Morgan Kaufmann, pp .
197-202.
[3] Chinchor, N., MUC-4 Evaluation Metrics (in this volume) .
[4] Lewis, D. and Tong, R., Text Filtering in MUC-3 and MUC-4 (in this volume) .
[5] Krupka, G. and Rau, L ., GE Adjunct Test Report: Object-Oriented Design an dScoring for MUC-4 (in this volume) .
[6] Hirschman, L., Comparing MUCK-II and MUC-3 : Assessing the Difficulty ofDifferent Tasks, in Proceedings of the Third Message Understanding Conferenc e(MUC-3), May, 1991, Morgan Kaufmann, pp .
25-30 .
[7] Chinchor, N ., Statistical Significance of MUC-4 Results (in this volume) .
[8] Chinchor, N., MUC-3 Linguistic Phenomena Test Experiment, in Proceedings ofthe Third Message Understanding Conference (MUC-3), May, 1991, Morga nKaufmann, pp .
31-45.
[9] Hirschman, L., An Adjunct Test for Discourse Processing in MUC-4 (in thi svolume) .
[10] Sundheim, B ., Overview of the Third Message Understanding Evaluation an dConference, in Proceedings of the Third Message Understanding Conference (MUC -3), May, 1991, Morgan Kaufmann, pp .
3-16 .21
