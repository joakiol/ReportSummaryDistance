Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1192?1202,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsLeveraging Small Multilingual Corpora for SMT Using Many PivotLanguagesRaj DabreGraduate School of InformaticsKyoto UniversityKyoto 606-8501prajdabre@gmail.comFabien CromieresJapan Science and Technology AgencyKawaguchi-shiSaitama 332-0012fabien@pa.jst.jpSadao KurohashiGraduate School of InformaticsKyoto UniversityKyoto 606-8501kuro@i.kyoto-u.ac.jpPushpak BhattacharyyaCFILTIIT Bombay, PowaiIndia 400076pushpakbh@gmail.comAbstractWe present our work on leveraging multilin-gual parallel corpora of small sizes for Sta-tistical Machine Translation between Japaneseand Hindi using multiple pivot languages.
Inour setting, the source and target part of thecorpus remains the same, but we show thatusing several different pivot to extract phrasepairs from these source and target parts leadto large BLEU improvements.
We focus on avariety of ways to exploit phrase tables gener-ated using multiple pivots to support a directsource-target phrase table.
Our main methoduses the Multiple Decoding Paths (MDP) fea-ture of Moses, which we empirically verifyas the best compared to the other methods weused.
We compare and contrast our various re-sults to show that one can overcome the limita-tions of small corpora by using as many pivotlanguages as possible in a multilingual setting.Most importantly, we show that such pivot-ing aids in learning of additional phrase pairswhich are not learned when the direct source-target corpus is small.
We obtained improve-ments of up to 3 BLEU points using multiplepivots for Japanese to Hindi translation com-pared to when only one pivot is used.
To thebest of our knowledge, this work is also thefirst of its kind to attempt the simultaneous uti-lization of 7 pivot languages at decoding time.1 IntroductionWith the increasing size of parallel corpora it hasbecome possible to achieve very high quality trans-lation.
However, not all language pairs are blessedwith the availability of large parallel corpora in thesizes of millions of lines.
With the exception ofthe major European languages and a few Asian lan-guages like Chinese and Japanese, other languageshave parallel corpora in the sizes of a few thousandsof lines.
Since translation quality is related to thesize of the parallel corpus, it is impossible to achievethe same level of translation quality as that in thecase of resource rich languages.
To remedy this sce-nario, an intermediate resource rich language can beexploited.
Although, finding a direct parallel cor-pus between source and target languages might bedifficult, there are higher odds of finding a pair ofparallel corpora: one between the source languageand an intermediate resource rich language (hence-forth called pivot1) and one between that pivot andthe target language.Using the methods developed for Pivot BasedSMT (Wu and Wang, 2007) (Utiyama and Isahara,2007) one can use the source-pivot and pivot-targetparallel corpora to develop a source-target transla-tion system (henceforth called as pivot based system2) .
Moreover, if there exists a small source-targetparallel corpus then the resulting system (henceforthcalled as direct system3) can be supported by thepivot based source-target system to significantly im-prove the translation quality.
Note that in this paperwe use the terms ?translation system?
and ?phrasetable?
interchangeably since the phrase table is the1In most cases this is English.2The phrase table will be known as the pivot phrase table.3The phrase table will be called as direct phrase table andthe corpus will be the direct parallel corpus.1192main component of the translation system.
Reorder-ing tables are supplementary and can usually be re-placed by a simple distortion model.Major problems arise when source-pivot andpivot-target corpora belong to different domainsleading to rather poor quality translations.
Even ifthe individual corpora are large, one will run intodomain adaptation problems.
In such a scenario theavailability of a small size multilingual corpus of afew thousand lines belonging to a single domain canbe beneficial.
The setting of this paper is:1.
We suppose the existence of a multilingual cor-pus with sentences aligned across N4differentlanguages.2.
We show using the other languages as addi-tional pivots leads to the construction of betterphrase tables and better translation results.Note that this setting is realistic and differs fromthe majority of existing work on pivot languages,in which the source-pivot and pivot-target corporaare unrelated (or at least do not have equivalent sen-tences).
In addition to the well-known Europarl cor-pus, many other similar multilingual corpora exist.For example, a multilingual parallel corpus for 9 ma-jor Indian Languages belonging to the Health andTourism domain of approximately 50000 lines wasused to develop basic SMT systems (Kunchukuttanet al, 2014).
For our experiments we will use arecently released Bible domain multilingual paral-lel corpus (Resnik et al, 1999) for a large number(over 25) of languages (other than Indian) includ-ing Japanese and Hindi (Japanese to Hindi transla-tion being our focus) of approximately 30000 lines.We chose this setting because we feel that this mul-tilingual approach is especially important for low-resource language pairs.Typically system combination methods like lin-ear interpolation are used to combine the direct andpivot phrase tables by modifying the probabilities ofphrase pairs leading to the modification of the under-lying distribution which affects the resultant transla-tion quality.
The Multiple Decoding Paths (Birchand Osborne, 2007) (MDP) feature has been used4The construction of a multilingual corpus has already thebenefit that each new language added to it will allow directtranslation with a SMT system for N new language pairs.to combine two source-target phrase tables of dif-ferent domains for domain adaptation (Koehn andSchroeder, 2007) but not so extensively in a pivotlanguage scenario, especially when multiple pivotsare involved (7 in our case).
Our work is differentfrom other previous works in the following ways:?
We work on a realistic low resource settingfor translation between Japanese and Hindi inwhich we use small sized multilingual corporacontaining translations of a sentence in multi-ple languages.?
We focus on the impact of using a relativelylarge number of pivot languages (7 to be pre-cise) to improve the translation quality andcompare this to when only one pivot languageis used.?
Most works focus on obtaining pivot basedphrase tables on relatively larger corpora thanthe ones used for the direct phrase table.
Weuse the same corpora sizes for the pivot as wellas direct tables.?
We verify that Multiple Decoding Paths (MDP)feature of Moses is much more effectivethan plain linear interpolation, especially whenmore pivot languages are used together.?
We show that simply varying the pivot lan-guage leads to additional phrase pairs being ac-quired that impact translation quality.Section 2 contains the related work.
Section 3 be-gins with a basic description about the languages in-volved, followed by the corpora details and the ex-perimental methodology.
Section 4 consists of re-sults, observations and discussions.
The paper endswith conclusions and future work.2 Related WorkUtiyama and Isahara (2007) developed a method(sentence translation strategy) for cascading asource-pivot and a pivot-target system to translatefrom source to target using a pivot language.
Sincethis results in multiplicative error propagation Wuand Wang (2009) developed a method (triangu-lation) in which they combined the source-pivotand pivot-target phrase tables to get a source-target1193phrase table.
They then combine the pivoted anddirect tables by linear interpolation whose weightswere manually specified.
There is a method to au-tomatically learn the weights (Sennrich, 2012) but itrequires reference phrase pairs not easily availablein resource constrained scenarios like ours.
Work ontranslation from Indonesian to English using Malayand Spanish to English using Portuguese (Nakovand Ng, 2009) as pivot languages worked well sincethe pivots had substantial similarity to the sourcelanguages.
This is one of the first works to use MDPin the pivot based SMT scenario.
(Paul et al, 2013) and (Paul et al, 2009) showedthat English is not the best pivot language for manylanguage pairs, including Japanese and Hindi.
Thiswas reason enough for us to not consider Englishas a pivot in our experiments.
None of the aboveworks focus on the utilization and impact of morethan 2 pivots in their experiments which was one ofour main objectives.
Related to multilingual transla-tion are works by Habash and Hu (2009), El Kholyet al (2013), Salloum et al (2014) and Koehn etal.
(2009).
Work on multi source translation (Ochand Ney, 2001) which is complementary to our workmust also be noted.In the related field of information retrieval,pivot languages were employed to translate queriesin cross-language information retrieval (CLIR)(Gollins and Sanderson, 2001) (Kishida and Kando,2003).
Chinnakotla et al (2010) retrieved feed-back terms from documents written in the pivot lan-guages (after translating back from the pivot), andaugmented source queries leading to improvementsin information retrieval.
We now talk about the lan-guages, corpora and experiments conducted.3 Description of Languages, Corpora andExperimentsWe first describe the pivot languages and the cor-pora we use.
We follow this with a description ofthe triangulation method which we use to constructphrase tables using the pivot languages, the methodsused to combine the constructed tables and then theexperiments that use them.3.1 Languages involvedWe performed experiments on translation betweenJapanese and Hindi which do not belong to thesame language group but exhibit many similarities:Japanese (J) and Hindi (H) both have SOV orderand are morphologically rich.
For pivots we con-sidered languages like Chinese, Korean (East-Asianlanguages of which Korean is closer to source),Marathi, Kannada, Telugu (Indian languages closerto target), Paite (Sino-Tibetian) and Esperanto (rela-tively distant from both source and target).
Increas-ing the number of languages reduced the size of mul-tilingual parallel translations available5.
Our choiceof languages was initially random but led to interest-ing observations as will be seen later.3.2 Corpora DetailsThe corpora used comes from the freely availablemultilingual Bible corpus6stored in XML files.
Af-ter sentence aligning all 9 languages we got 29780sentence tuples.
A tuple contains 9 sentences: 1 foreach language.
This we divided into 29000 train-ing tuples, 280 tuning tuples and 500 testing tuples.The Japanese sentences were segmented using JU-MAN (Kurohashi et al, 1994).
The Chinese andKorean (Hangul blocks were space separated) sen-tences were directly available in their character seg-mented form.
The corpora of the other languageswere left morphologically and syntactically unpro-cessed.3.3 Phrase Table TriangulationWe implemented the phrase table triangulationmethod (Wu and Wang, 2007) using JAVA as theprogramming language.
The phrase table has 4main scores: forward and inverse phrase translationprobabilities (equations 1 and 2) accompanied byforward and inverse lexical translation probabilities(equations 3 and 4).
The formulae for generatingthem using pivots are:?
(f |e) =?pi?
(f |pi) ??
(pi|e) (1)5It must be noted that Hebrew and Greek are most likely thelanguages from which the Bible sentences were translated intothe other languages.6http://homepages.inf.ed.ac.uk/s0787820/bible/1194?
(e|f) =?pi?
(e|pi) ??
(pi|f) (2)Pw(f |e, a) =?piPw(f |pi, a1) ?
Pw(pi|e, a2) (3)Pw(e|f, a) =?piPw(e|pi, a2) ?
Pw(pi|f, a1) (4)Here a1is the alignment between phrases f(source) and pi(pivot), a2, the alignment betweenpiand e (target) and a the alignment between e andf.
Note that the lexical translation probabilities arecalculated in the same way as the phrase probabil-ities.
Our results might improve even more if weused more sophisticated approaches like crosslan-guage similarity method or the method which usespivot induced alignments (Wu and Wang, 2007).3.4 Phrase Table CombinationThere are 3 ways to combine phrase tables: linearinterpolation, fillup interpolation and multiple de-coding paths.
Linear interpolation is performed bymerging the tables and computing a weighted sum ofphrase pair probabilities from each phrase table giv-ing a final single table.
Typically, the direct phrasetable is given a significantly higher weight than thepivot based table.?
(f |e) = ?0?
?direct(f |e) +?li?li?
?li(f |e)subject to ?0+?li?li= 1 (5)Typically ?0is 0.9 (Wu and Wang, 2009) and thepivot languages are collectively given a weight of0.1.
?li(f |e) is the inverse translation probabilityfor language li.
In our experiments we set the ?
?s ac-cording to the ratio of the BLEU scores, on the testset, of the translations using the individual phrasetables.
It is possible to learn optimal weights butthis requires a collection of reference phrase pairswhich would not be readily available in a resourceconstrained scenario.Fillup interpolation does not modify phrase prob-abilities but selects phrase pair entries from the nexttable if they are not present in the current table.The priority of the phrase tables should be speci-fied which we do by ranking them according to theBLEU scores on a test set.Multiple Decoding Paths (MDP) method ofMoses which uses all the tables simultaneouslywhile decoding ensures that each pivot table is keptseparate and translation options are collected fromall the tables.
Increasing the number of pivot lan-guages slows the decoding process drastically butthe existence of powerful machines negates this lim-itation.
For the sake of completeness we also ex-perimented with a combination of both, linear andMDP, methods by: Firstly, combining the pivotbased phrase tables into a single table using equa-tion 5 (using the ratio of BLEU scores as interpola-tion weights) followed by using this table to supportthe direct phrase table by MDP.
Note that the rightway would be to use the BLEU scores on the tuningset but our objective was to show that even in thebest case scenario (also called Oracle7scenario) thismethod is still inferior compared to only using theMDP method.3.5 Descriptions of ExperimentsOur experiments were centered around Phrase BasedSMT (PBSMT).
We used the open source Mosesdecoder (Hoang et al, 2007) package (includingGiza++) for word alignment, phrase table extrac-tion and decoding for sentence translation.
We alsoused the Moses scripts for linear and fillup interpola-tion along with the multiple decoding paths (MDP)setting (by modifying the moses.ini files).
We per-formed MERT (Och, 2003) based tuning using theMIRA algorithm.
We used BLEU (Papineni et al,2002) as our evaluation criteria and the bootstrap-ping method (Koehn, 2004) for significance testing.For the sake of comparison with previous methods,we experimented with sentence translation strategy(Utiyama and Isahara, 2007) using 10 as the n-bestlist size for intermediate and target language transla-tions.
The experiments we performed are given be-low.
Each experiment involves either the creation ofa phrase tables or combination of phrase tables.
Wetune, test and evaluate these tables or combinations.1.
A src (source) to tgt (target) direct phrase table.2.
For piv in Pivot Languages Set; the set of pivotlanguages to be used (Tables 1 and 2):7By Oracle scenario we mean that we already know the per-formance on the test sets and exploit this information to ?un-fairly?
boost the translation scores.1195(a) src to piv and piv to tgt phrase tables.Translate the src test sentences to tgt usingthe sentence translation strategy and eval-uate.
(Column 2)(b) Triangulate the src-piv and piv-tgt phrasetables to get the src-piv-tgt phrase table.
(Column 3)(c) Perform linear interpolation of the src-tgtand src-piv-tgt table using 9:1 weight ra-tio in equation 5 to get a combined table.
(Column 4)(d) Perform linear interpolation of the src-tgtand src-piv-tgt table using the ratio of theirBLEU scores as weights in equation 5 toget a combined table.
(Column 5)(e) Perform fillup interpolation of the src-tgt(main) and src-piv-tgt table (secondary) toget a combined table.
(Column 6)(f) Combine the src-tgt and src-piv-tgt phrasetable using MDP (2 paths, 1 for direct and1 for pivot).
(Column 7)3.
Combine all the src-piv-tgt tables into a singletable using linear (weights are ratios of BLEUscores) and fillup interpolation independently,giving the phrase tables: linear interp all andfill interp all respectively.
Table 3, rows 4 and5.4.
Perform linear interpolation of the src-tgt andlinear interp all tables using 9:1 weight ratio inequation 5 to get a combined table.
Table 3,row 6.5.
Perform linear interpolation of the src-tgt andall src-piv-tgt phrase tables using the ratio oftheir BLEU scores as weights in equation 5 toget a combined table.
Table 3, row 7.6.
Perform fillup interpolation of the src-tgt andall src-piv-tgt phrase tables.
The priority ofthe tables is given by the descending order ofBLEU scores.
Table 3, row 8.7.
Combine the linear interp all with the src-tgt phrase table using MDP.
Repeat this forfill interp all.
Table 3, rows 9 and 10.8.
Combine all the src-piv-tgt phrase tables withthe src-tgt phrase table using MDP (8 paths, 1for direct and 1 for each of the 7 pivots).
Table3, row 11.9.
Combine the top 3 pivot phrase tables with thesrc-piv-tgt phrase tables with the src-tgt phrasetable using MDP (4 paths, 1 for direct and 1 foreach of the 3 pivots).
The pivot tables with the 3highest8standalone BLEU scores are selected.Table 3, row 12.4 Results and DiscussionsBLEU scores obtained after testing the tuned tablesare reported.
Scores in bold are statistically signifi-cant (p<0.05) over the baseline which is the systemtrained using a direct src-tgt parallel corpus.4.1 ResultsThe Japanese-Hindi direct translation system gavea BLEU of 33.86 whereas the Hindi-Japanese onegave 37.47.
For the rest of the paper these will bethe baselines, unless mentioned otherwise.The evaluation scores are split into 3 tables.
Ta-ble 1 contains the scores for Japanese to Hindi (Ta-ble 2 for Hindi to Japanese) translation using eachpivot separately and has 7 columns whose detailsare given in section 3.5 from 2.a to 2.f.
Table 3contains the scores for Japanese to Hindi (and viceversa) translation using all 7 pivots together in var-ious ways.
Each row is self explanatory.
In row 6,we mean that the direct phrase table has a weight of0.9 and the remainder 0.1 is distributed amongst thepivot phrase tables in the ratio of their standaloneBLEU scores which can be seen in column 3 of ta-bles 1 and 2.
It is quite clear that sentence translationstrategy is the most inferior technique.4.2 ObservationsBelow, we give an explanation of the observedscores from various points of views.4.2.1 On the Pivots UsedIt is logical to consider that the closeness of apivot language to the source or target is an impor-tant factor in the improvement of translation qual-ity, since Korean helps Japanese-Hindi translation.8We chose 3 since our evaluation showed that the BLEUscores for the 3 pivot languages were much larger than the re-maining ones.1196Pivot Sentence Standalone Linear Linear Fill MDPLanguage Strategy Interpolate (1) Interpolate (2) Interpolate WithWith Direct With Direct With Direct Direct1.
Direct 33.862.
Chinese 23.53 28.89 34.03 34.61 34.31 35.663.
Korean 26.30 28.92 34.65 34.18 34.64 35.604.
Esperanto 22.43 28.73 34.63 34.55 35.32 35.745.
Paite 19.40 26.64 34.17 34.40 34.66 35.226.
Marathi 15.68 21.80 33.88 33.80 33.83 34.037.
Kannada 16.94 24.15 33.74 34.13 34.87 35.528.
Telugu 14.15 21.31 33.81 33.85 34.04 34.57Table 1: Japanese-Hindi Results Using Single PivotsPivot Sentence Standalone Linear Linear Fill MDPLanguage Strategy Interpolate (1) Interpolate (2) Interpolate WithWith Direct With Direct With Direct Direct1.
Direct 37.472.
Chinese 27.93 30.97 35.90 38.47 38.41 39.493.
Korean 30.68 32.67 35.99 38.72 38.55 39.494.
Esperanto 26.67 30.80 36.07 37.82 37.85 39.145.
Paite 23.37 29.17 35.89 37.73 37.39 38.196.
Marathi 20.59 26.21 35.89 37.57 37.72 38.307.
Kannada 23.21 26.96 35.84 38.05 37.79 38.058.
Telugu 19.01 25.22 37.25 36.98 37.11 37.04Table 2: Hindi-Japanese Results Using Single PivotsOf all the scores, the ones obtained using Koreanand Chinese as pivots stand out as the best and it isknown that Korean and Japanese share many simi-larities.
Although this gives reason to believe thatlanguages belonging to the same language groupshould act as good choices of pivots, the languagesKannada, Telugu and (especially) Marathi shouldhave helped improve Hindi to Japanese translation.Moreover, languages like Paite and Esperanto whichare relatively distant from both Hindi and Japanesegave better performance than the Indian Languages.Remember that the Chinese and Korean corporawere character segmented9and that Esperanto andPaite are not so morphologically rich.
The Indianpivot languages have agglutinative features whichis one of the main causes of poor quality SMT.This clearly indicates that morphological similarityto source and target is another equally important as-9Hangul blocks were space separated in the Korean case.pect that affects the translation quality.
Had thisnot been the case, the Indian Languages would haveacted as good pivots.
This shows that experimentsinvolving forcing the morpheme to morpheme ratio,of the source to pivot to target sentences, to be thesame, must be conducted.
Henceforth, it is to be ex-pected that the most significant improvements willbe obtained when Chinese, Korean and Esperanto(in a number of cases) are used as pivots.4.2.2 On the Linear and Fill InterpolationMethodsSingle pivots: All the interpolation methods(columns 4, 5 and 6 of Tables 1 and 2) gave smallimprovements in BLEU in most cases compared tothe baselines for both language pairs.
The resultsdo not show drastic improvements, which is ex-pected since the baseline and pivots based phrasetables are constructed from the same multilingual1197Combination Type Jap-Hin Hin-Jap1.
Direct phrase table (baseline) 33.86 37.472.
Best result using single pivot 35.74 (Esp.)
39.49 (Kor.)3.
Combine All Pivots using MDP 34.49 37.024.
A - Linear Interpolate All Pivot tables (BLEU score ratio) 32.50 35.655.
B - Fill Interpolate All Pivot tables (Priority according to BLEU score) 32.12 34.446.
Linear Interpolate (9:1 ratio) Direct with All Pivot tables 34.56 38.607.
Linear Interpolate (BLEU score ratio) Direct with All Pivots 35.24 39.088.
Fill Interpolate Direct with All Pivots (Priority according to BLEU score) 35.28 38.709.
Combine Direct and A using MDP 36.40 39.8510.
Combine Direct and B using MDP 36.67 40.0711.
Combine Direct and All Pivots tables using MDP 38.42 40.1912.
Combine Direct and Top 3 (BLEU) pivot tables using MDP (Oracle) 38.22 41.09Table 3: Results Using Multiple Pivots With Different Combination Methodstraining instances (29000 tuples - see section 3.2).Typically the interpolation methods are shown togive substantial performance boosts when the di-rect source-target phrase table is obtained using rel-atively smaller corpora sizes compared to those usedfor the source-pivot and pivot-target tables.
In caseof linear interpolation with a 9:1 weight ratio, thescores improve slightly in some cases for Japanese-Hindi but degrade in case of Hindi-Japanese.
How-ever, in the case of linear interpolation where theBLEU score ratio is used as the weight ratio, the im-provements are much better10.Fill based interpolation also gives improvementsin some cases, mostly when Chinese and Korean areused as pivots.
An overall comparison shows thatthere is no consistency when a single pivot languageis used and no conclusive comment can be made onthe efficacy of these interpolation methods.Multiple Pivots: However in Table 3, rows 6 to8 show that using all the pivots together, result ina significant improvement over the direct phrase ta-bles.
Linear interpolation with BLEU score ratiogives 35.24 BLEU (33.86 for direct phrase table) forJapanese-Hindi and 39.08 BLEU (37.47 for directphrase table).
Rows 4 and 5 show the scores of thelinear and fill interpolation of only the pivot basedphrase tables.
It is interesting to see that in case ofJapanese-Hindi the BLEU scores rival that of the di-rect phrase table (32.50/32.12 v.s.
33.86).
This is10Expected as we use test set evaluation information.similar in the case of Hindi-Japanese: 35.65/34.44v.s.
37.47.
The following points must be noted:a.
Since the setting is multilingual and improve-ments, however slight, are observed in some casesit must be the case that, through pivoting, additional(and possibly improved) phrase pairs are inducedwhich are not extracted using the direct source-target parallel corpus.
This also gives reason tobelieve that every pivot induces a different set ofphrase pairs thereby overcoming the limitations ofpoor alignment (and effectively phrase extraction)on small corpora.
Even if there is no alignment er-ror, pivoting still introduces new phrase pairs whichimproves MT performance.b.
The pivot based phrase tables already have anincomplete probability space with respect to thephrase pair distribution.
Linear interpolation tendsto violate the overall probability mass since thephrase pair distribution gets changed.
Fill interpo-lation just adds additional phrase pairs from the nextphrase table when not available in the current onewhich leads to poor mixing of different probabilitymodels giving poorer performance in-spite of addi-tional phrase pairs being available.c.
Since some pivot languages are obviously bad,their probability scores would drastically affect theoverall probability mass.
They should be excludedor given low weights, which we do by consideringthe BLEU score ratio.
However, this is not a goodidea because the scores for Telugu, a bad pivot forHindi-Japanese translation, degraded to a lesser ex-1198tent when the Telugu based phrase table was lin-early combined with the direct phrase table.
Sen-nrich (2012) gave a method to learn these weights,but in a resource constrained scenario such a methodis difficult to apply.This motivated us to try the Multiple DecodingPaths (MDP) feature of Moses.4.2.3 On using MDPSingle pivots: Since log linear combination doesnot modify the probability space it should lead todefinitive increase in translation scores.
This claimis validated by the last columns of Tables 1 and 2.For Japanese-Hindi: barring Marathi, the combina-tion of the direct and pivot phrase table leads to sig-nificant improvement over the direct phrase tables.A similar situation occurs for Hindi-Japanese exceptthat Telugu behaves as a bad pivot.Multiple pivots: Row 3 of Table 3 indicatesthat the log linear combination of all the pivot ta-bles using MDP for Japanese-Hindi gives a BLEU of34.49, an improvement (p<0.05) over the direct ta-ble (BLEU 33.86).
For Hindi-Japanese, although theequivalent BLEU score (37.02) is not an improve-ment over that of the direct table (37.47), it doesshow that multiple pivots can be used to achievetranslation quality similar to the quality obtained bya direct table.Since it was observed that the interpolation of allthe pivot tables into a single one gave scores close tothe direct tables we decided to try the combination ofthe all pivots interpolated table with the direct tableusing MDP.
Rows 9 and 10 show that there is a sig-nificant improvement compared to the scores of thedirect tables alone.
But this method of linear + loglinear combination would still suffer from the limi-tation of linear interpolation which led to the final 2experiments which use only log linear combination.Row 11 shows that the method of combining thedirect and all the pivot tables using MDP (one foreach table) outperforms all the methods so far.
Thereason is simple: Only good translation options arecollected from all tables during hypothesis expan-sion, the bad ones are automatically pruned.
ForJapanese-Hindi the BLEU is 38.42 which is an im-provement of 4.56 (13% relative) over the BLEU ofthe direct phrase table (33.86).
For Hindi-Japanesethe BLEU of 40.19 is an improvement of 2.72(7.25% relative) over that of the direct table (37.47).The increment is lesser because of the premise weestablished in section 4.2.1.
This points to an inter-esting observation that pivot languages induce betterphrase pairs in a multilingual setting which are notpresent in the direct phrase table.
This is quite bene-ficial when the corpora sizes are small which lead topoor quality phrase tables.To test whether exclusion of bad performing piv-ots leads to improvements in BLEU we performedanother oracle experiment in which we only in-cluded the pivot phrase tables having significantstandalone BLEU difference compared to the oth-ers.
Korean, Chinese and Esperanto were the onesthat stood out.
The last row shows that for Japanese-Hindi the BLEU (38.22) does not significantly in-crease over the situation when all pivots are usedtogether (38.42).
However for Hindi-Japanese theBLEU is 41.09 which is a significant (p<0.05) in-crease compared to when all the pivots are used to-gether (40.19 - 2.2% relative).
Note that this leadsto an absolute BLEU difference of 3.62 (9.66% rel-ative) compared to the BLEU of the direct phrase ta-ble.
The improvements for Japanese-Hindi were al-ready so large (13%) that more significant improve-ments would need deeper inspection and improvedmethods.
We believe that further significant im-provements are possible and advanced methods toeffectively select multiple pivots need to be studiedand implemented.4.2.4 On the number of new phrase pairsinducedBased on the cutoff of 0.001 for the inverse trans-lation probability, Table 4 contains the statistics ofthe unique phrase pairs in each pivot table (Columns4 to 10) and the direct table (Column 3) along withthe number of phrase pairs common (Column 2) toall.
It is quite obvious that each pivot11induces itsown set of unique phrase pairs.4.2.5 On the improvement in translationsTable 5 gives the count of improved translations,out of 500 tested sentences, over the direct usingsentence level BLEU difference at various cutoffs.On an average 50% of the sentences showed increase11For each language we use their first 3 characters of theirnames as the shortened versions.1199Direction Common Direct Chi Kor Esp Pai Kan Mar Tel1.
Jap-Hin 0.032 1.404 20.74 18.65 16.06 23.85 26.56 30.92 26.842.
Hin-Jap 0.034 1.528 26.20 20.26 18.06 28.83 29.90 36.98 31.23Table 4: Unique phrase pairs in each table (in millions of pairs)Direction >0 >0.1 >0.2 >0.3 >0.4 >0.5 >0.6 >0.71.
Jap-Hin 267 108 36 12 6 4 2 02.
Hin-Jap 275 124 60 24 12 4 1 1Table 5: Number of improved translations (out of 500) using sentence level BLEU difference at various cutoffsin BLEU and the number of improved sentences de-creases with increasing cutoff.
We manually veri-fied a random sample and found that the improve-ments were commensurate with the reported differ-ences.
Finally, consider an example of improvementin Japanese to Hindi translation.Input: ??
??
?
???
?
??
??
?
??
?
??
?
????
??
?
???
?
???????????????
(Sore kara,Yohane no deshi-tachi ga kiru, shitai o hikitoru ho-muru.
Soshite, iesu no tokoro ni iku hokoku suru.
)English equivalent: After that, John?s disciplescame and took his corpse away, buried it and thenwent to Jesus to give him the news.Direct translation: tab y h?A k clo\ n aAkrus kF loT l gyA aOr usk pAs jAkr btaAEdyA (Tab yohanna ke chelo ne aakar uss ki lothale gaya; aura uske paas jaakar bata diya)Best translation using MDP: tab y h?A k clo\n aAkr us kF loT ko l jAkr gAY EdyAaOr jAkr yFf  ko smAcAr EdyA (Tab yohannake chelo ne aakar usa ki lotha ko le jaakar gaad diyaaura jakar yesu ko samachara diya)Analysis: Note that in the direct translation thepart about ?burying the corpse?
(gaad diya) and?Jesus?
(yesu) is missing which is present in theMDP translation.
Also the verb forms indicatingthe sequence of actions like ?came and?
(aakar) and?took his corpse away?
(usa ki lotha ko le jaakar)are much better in the MDP translation.
Instead of?samachara diya?
(gave news) the preferred transla-tion is ?samachara di?.5 Conclusions and Future WorkWe have presented our work on leveraging a smallsized multilingual parallel corpus using 7 pivot lan-guages for SMT between Japanese and Hindi.
Ourmain objective was to augment a phrase table on di-rect parallel corpus using many pivot language basedphrase tables constructed from the same multilin-gual corpus.
We confirm that this induces additionaland improved phrase pairs which, under the MultipleDecoding Paths setting (MDP), leads to substantialimprovements over the direct phrase tables.
Moreimportantly, we show that using multiple pivot lan-guages simultaneously lead to large improvementsin BLEU compared to the when a single pivot isused; which is the novel aspect of our work.
Thisopens up many further research directions like a.How can one choose a set of good pivot languagesamongst available choices?
b.
Does this multilin-gual leveraging help in a situation where we havelarge size corpora like Europarl corpora?
c. Howmuch of an impact can treatment (morphological orsyntactic) of the pivot language help in improvingtranslation quality?
d. Can good reordering in-formation be extracted by pivoting?
e. Can multisource and multi pivot setting further enhance qual-ity?
f. How can the noise induced by pivoting becontrolled by methods other than probability cut-offs?
and finally g. Can simpler but more effectivemethods compared to triangulation be exploited ina multilingual scenario?
The last 4 questions havelong been ignored and deserve to be answered.
Wewill pursue these directions in the future and attemptto provide satisfactory answers.1200ReferencesAlexandra Birch and Miles Osborne.
2007.
Ccg su-pertags in factored statistical machine translation.
InIn ACL Workshop on Statistical Machine Translation,pages 9?16.Manoj K. Chinnakotla, Karthik Raman, and PushpakBhattacharyya.
2010.
Multilingual pseudo-relevancefeedback: Performance study of assisting languages.In Proceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, ACL ?10, pages1346?1356, Stroudsburg, PA, USA.
Association forComputational Linguistics.Ahmed El Kholy, Nizar Habash, Gregor Leusch, EvgenyMatusov, and Hassan Sawaf.
2013.
Selective com-bination of pivot and direct statistical machine trans-lation models.
In Proceedings of the Sixth Interna-tional Joint Conference on Natural Language Process-ing, pages 1174?1180.
Asian Federation of NaturalLanguage Processing.Tim Gollins and Mark Sanderson.
2001.
Improvingcross language retrieval with triangulated translation.In Proceedings of the 24th Annual International ACMSIGIR Conference on Research and Development inInformation Retrieval, SIGIR ?01, pages 90?95, NewYork, NY, USA.
ACM.Nizar Habash and Jun Hu.
2009.
Improving arabic-chinese statistical machine translation using english aspivot language.Hieu Hoang, Alexandra Birch, Chris Callison-burch,Richard Zens, Marcello Federico, Nicola Bertoldi,Chris Dyer, Brooke Cowan, Wade Shen, ChristineMoran, Ond?rej Bojar, Alexandra Constantin, and EvanHerbst.
2007.
Moses: Open source toolkit for statisti-cal machine translation?.
acl-2007.Kazuaki Kishida and Noriko Kando.
2003.
Two-stagerefinement of query translation in a pivot language ap-proach to cross-lingual information retrieval: An ex-periment at clef 2003.
In Carol Peters, Julio Gonzalo,Martin Braschler, and Michael Kluck, editors, CLEF,volume 3237 of Lecture Notes in Computer Science,pages 253?262.
Springer.Philipp Koehn and Josh Schroeder.
2007.
Experimentsin domain adaptation for statistical machine transla-tion.
In Proceedings of the Second Workshop on Sta-tistical Machine Translation, StatMT ?07, pages 224?227, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Philipp Koehn, Alexandra Birch, and Ralf Steinberger,2009.
462 Machine Translation Systems for Europe,pages 65?72.
Association for Machine Translation inthe Americas, AMTA.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In EMNLP, pages388?395.Anoop Kunchukuttan, Abhijit Mishra, Rajen Chatter-jee, Ritesh Shah, and Pushpak Bhattacharyya.
2014.Shata-anuvadak: Tackling multiway translation of in-dian languages.
In Proceedings of the Ninth Interna-tional Conference on Language Resources and Eval-uation (LREC?14), pages 1781?1787, Reykjavik, Ice-land, May.
European Language Resources Association(ELRA).
ACL Anthology Identifier: L14-1355.Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,and Makoto Nagao.
1994.
Improvements of japanesemorphological analyzer juman.
In Proceedings ofThe International Workshop on Sharable Natural Lan-guage, pages 22?28.Preslav Nakov and Hwee Tou Ng.
2009.
Improved statis-tical machine translation for resource-poor languagesusing related resource-rich languages.
In Proceed-ings of the 2009 Conference on Empirical Methodsin Natural Language Processing: Volume 3 - Volume3, EMNLP ?09, pages 1358?1367, Stroudsburg, PA,USA.
Association for Computational Linguistics.Franz Josef Och and Hermann Ney.
2001.
Statisticalmulti-source translation.
In In MT Summit 2001, pages253?258.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics - Volume 1, ACL ?03, pages 160?167, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Michael Paul, Hirofumi Yamamoto, Eiichiro Sumita, andSatoshi Nakamura.
2009.
On the importance ofpivot language selection for statistical machine trans-lation.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, Companion Volume: Short Papers, NAACL-Short ?09, pages 221?224, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Michael Paul, Andrew Finch, and Eiichrio Sumita.
2013.How to choose the best pivot language for automatictranslation of low-resource languages.
12(4):14:1?14:17, October.Philip Resnik, MariBroman Olsen, and Mona Diab.1999.
The bible as a parallel corpus: Annotating the1201?book of 2000 tongues?.
Computers and the Humani-ties, 33(1-2):129?153.Wael Salloum, Heba Elfardy, Linda Alamir-Salloum,Nizar Habash, and Mona Diab.
2014.
Sentence leveldialect identification for machine translation systemselection.
In Proceedings of the 52nd Annual Meetingof the Association for Computational Linguistics (Vol-ume 2: Short Papers), pages 772?778.
Association forComputational Linguistics.Rico Sennrich.
2012.
Perplexity minimization for trans-lation model domain adaptation in statistical machinetranslation.
In Proceedings of the 13th Conferenceof the European Chapter of the Association for Com-putational Linguistics, EACL ?12, pages 539?549,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Masao Utiyama and Hitoshi Isahara.
2007.
A compar-ison of pivot methods for phrase-based statistical ma-chine translation.
In in Proceedings of the conferenceon Human Language Technology Conference of theNorth American Chapter of the Association of Com-putational Linguistics (NAACL-HLT, pages 484?491.Hua Wu and Haifeng Wang.
2007.
Pivot languageapproach for phrase-based statistical machine transla-tion.
Machine Translation, 21(3):165?181, Septem-ber.Hua Wu and Haifeng Wang.
2009.
Revisiting pivotlanguage approach for machine translation.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP: Volume 1 - Volume 1, ACL ?09, pages 154?162, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.1202
