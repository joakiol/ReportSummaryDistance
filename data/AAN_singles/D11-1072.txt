Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 782?792,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsRobust Disambiguation of Named Entities in TextJohannes Hoffart1, Mohamed Amir Yosef1, Ilaria Bordino2, Hagen Fu?rstenau3,Manfred Pinkal3, Marc Spaniol1, Bilyana Taneva1, Stefan Thater3, Gerhard Weikum11 Max Planck Institute for Informatics, Saarbru?cken, Germany2 Yahoo!
Research Lab, Barcelona, Spain3 Saarland University, Saarbru?cken, Germany{jhoffart,mamir,mspaniol,btaneva,weikum}@mpi-inf.mpg.debordino@yahoo-inc.com {hagenf,pinkal,stth}@coli.uni-sb.deAbstractDisambiguating named entities in natural-language text maps mentions of ambiguousnames onto canonical entities like people orplaces, registered in a knowledge base such asDBpedia or YAGO.
This paper presents a ro-bust method for collective disambiguation, byharnessing context from knowledge bases andusing a new form of coherence graph.
It unifiesprior approaches into a comprehensive frame-work that combines three measures: the priorprobability of an entity being mentioned, thesimilarity between the contexts of a mentionand a candidate entity, as well as the coherenceamong candidate entities for all mentions to-gether.
The method builds a weighted graph ofmentions and candidate entities, and computesa dense subgraph that approximates the bestjoint mention-entity mapping.
Experimentsshow that the new method significantly outper-forms prior methods in terms of accuracy, withrobust behavior across a variety of inputs.1 Introduction1.1 MotivationWeb pages, news articles, blog postings, and otherInternet data contain mentions of named entities suchas people, places, organizations, etc.
Names are oftenambiguous: the same name can have many differentmeanings.
For example, given a text like ?They per-formed Kashmir, written by Page and Plant.
Pageplayed unusual chords on his Gibson.
?, how can wetell that ?Kashmir?
denotes a song by Led Zeppelinand not the Himalaya region (and that Page refersto guitarist Jimmy Page and not to Google founderLarry Page, and that Gibson is a guitar model ratherthan the actor Mel Gibson)?Establishing these mappings between the mentionsand the actual entities is the problem of named-entitydisambiguation (NED).If the possible meanings of a name are known up-front - e.g., by using comprehensive gazetteers suchas GeoNames (www.geonames.org) or knowledgebases such as DBpedia (Auer07), Freebase (www.freebase.com), or YAGO (Suchanek07), whichhave harvested Wikipedia redirects and disambigua-tion pages - then the simplest heuristics for name res-olution is to choose the most prominent entity for agiven name.
This could be the entity with the longestWikipedia article or the largest number of incominglinks in Wikipedia; or the place with the most inhab-itants (for cities) or largest area, etc.
Alternatively,one could choose the entity that uses the mentionmost frequently as a hyperlink anchor text.
For theexample sentence given above, all these techniqueswould incorrectly map the mention ?Kashmir?
to theHimalaya region.
We refer to this suite of methodsas a popularity-based (mention-entity) prior.Key to improving the above approaches is to con-sider the context of the mention to be mapped, andcompare it - by some similarity measure - to contex-tual information about the potential target entities.For the example sentence, the mention ?Kashmir?has context words like ?performed?
and ?chords?
sothat we can compare a bag-of-words model againstcharacteristic words in the Wikipedia articles of thedifferent candidate entities (by measures such as co-sine similarity, weighted Jaccard distance, KL diver-gence, etc.).
The candidate entity with the highestsimilarity is chosen.
Alternatively, labeled trainingdata can be harnessed to learn a multi-way classifier,and additional features like entire phrases, part-of-speech tags, dependency-parsing paths, or nearby782hyperlinks can be leveraged as well.
These methodswork well for sufficiently long and relatively cleaninput texts such as predicting the link target of a Wi-kipedia anchor text (Milne08).
However, for short ormore demanding inputs like news, blogs, or arbitraryWeb pages, relying solely on context similarity can-not achieve near-human quality.
Similarity measuresbased on syntactically-informed distributional mod-els require minimal context only.
They have beendeveloped for common nouns and verbs (Thater10),but not applied to named entities.The key to further improvements is to jointly con-sider multiple mentions in an input and aim for a col-lective assignment onto entities (Kulkarni09).
Thisapproach should consider the coherence of the re-sulting entities, in the sense of semantic relatedness,and it should combine such measures with the con-text similarity scores of each mention-entity pair.
Inour example, one should treat ?Page?, ?Plant?
and?Gibson?
also as named-entity mentions and aim todisambiguate them together with ?Kashmir?.Collective disambiguation works very well when atext contains mentions of a sufficiently large numberof entities within a thematically homogeneous con-text.
If the text is very short or is about multiple, un-related or weakly related topics, collective mappingtends to produce errors by directing some mentionstowards entities that fit into a single coherent topicbut do not capture the given text.
For example, a textabout a football game between ?Manchester?
and?Barcelona?
that takes place in ?Madrid?
may end upmapping either all three of these mentions onto foot-ball clubs (i.e., Manchester United, FC Barcelona,Real Madrid) or all three of them onto cities.
Theconclusion here is that none of the prior methodsfor named-entity disambiguation is robust enough tocope with such difficult inputs.1.2 ContributionOur approach leverages recently developed knowl-edge bases like YAGO as an entity catalog and arich source of entity types and semantic relationshipsamong entities.
These are factored into new measuresfor the similarity and coherence parts of collectivelydisambiguating all mentions in an input text.
Forsimilarity, we also explore an approach that lever-ages co-occurrence information obtained from large,syntactically parsed corpora (Thater10).We cast the joint mapping into the following graphproblem: mentions from the input text and candidateentities define the node set, and we consider weightededges between mentions and entities, capturing con-text similarities, and weighted edges among entities,capturing coherence.
The goal on this combinedgraph is to identify a dense subgraph that containsexactly one mention-entity edge for each mention,yielding the most likely disambiguation.
Such graphproblems are NP-hard, as they generalize the well-studied Steiner-tree problem.
We develop a greedyalgorithm that provides high-quality approximations,and is customized to the properties of our mention-entity graph model.In addition to improving the above assets for theoverall disambiguation task, our approach gains inrobustness by using components selectively in a self-adapting manner.
To this end, we have devised thefollowing multi-stage procedure.?
For each mention, we compute popularity priorsand context similarities for all entity candidatesas input for our tests.?
We use a threshold test on the prior to decidewhether popularity should be used (for mentionswith a very high prior) or disregarded (for men-tions with several reasonable candidates).?
When both the entity priors and the context simi-larities are reasonably similar in distribution forall the entity candidates, we keep the best candi-date and remove all others, fixing this mentionbefore running the coherence graph algorithm.We then run the coherence graph algorithm on allthe mentions and their remaining entity candidates.This way, we restrict the coherence graph algorithmto the critical mentions, in situations where the goalof coherence may be misleading or would entail highrisk of degradation.The paper makes the following novel contribu-tions: 1) a framework for combining popularity pri-ors, similarity measures, and coherence into a robustdisambiguation method; 2) new measures for defin-ing mention-entity similarity; 3) a new algorithmfor computing dense subgraphs in a mention-entitygraph, which produces high-quality mention-entitymappings; 4) an empirical evaluation on a demand-ing corpus (based on additional annotations for thedataset of the CoNLL 2003 NER task), with signifi-783cant improvements over state-of-the-art opponents.2 State of the ArtRecognizing named entities (NER tagging) in natural-language text has been extensively addressed in NLPresearch.
The output is labeled noun phrases.
How-ever, these are not yet canonical entities, explicitlyand uniquely denoted in a knowledge repository.Approaches that use Wikipedia for explicit disam-biguation date back to (Bunescu06) and have beenfurther pursued by (Cucerzan07; Han09; Milne08;Nguyen08; Mihalcea07).
(Bunescu06) defined a sim-ilarity measure that compared the context of a men-tion to the Wikipedia categories of an entity candi-date.
(Cucerzan07; Milne08; Nguyen08) extendedthis framework by using richer features for the simi-larity comparison.
(Milne08) additionally introduceda supervised classifier for mapping mentions to en-tities, with learned feature weights rather than usingthe similarity function directly.
(Milne08) introduceda notion of semantic relatedness between a mention?scandidate entities and the unambiguous mentions inthe textual context.
The relatedness values are de-rived from the overlap of incoming links in Wikipediaarticles.
(Han09) considered another feature: the re-latedness of common noun phrases in a mention?scontext, matched against Wikipedia article names.While these features point towards semantic coher-ence, the approaches are still limited to mapping eachmention separately.
Nonetheless, this line of feature-rich similarity-driven methods achieved very goodresults in experiments, especially for the task of pre-dicting Wikipedia link targets for a given href anchortext.
On broader input classes such as news articles(called ?wikification in the wild?
in (Milne08)), theprecision was reported to be about 75 percent.The first work with an explicit collective-learningmodel for joint mapping of all mentions has been(Kulkarni09).
This method starts with a supervisedlearner for a similarity prior, and models the pair-wise coherence of entity candidates for two differentmentions as a probabilistic factor graph with all pairsas factors.
The MAP (maximum a posteriori) es-timator for the joint probability distribution of allmappings is shown to be an NP-hard optimizationproblem, so that (Kulkarni09) resorts to approxima-tions and heuristics like relaxing an integer linearprogram (ILP) into an LP with subsequent round-ing or hill-climbing techniques.
The experiments in(Kulkarni09) show that this method is superior to thebest prior approaches, most notably (Milne08).
How-ever, even approximate solving of the optimizationmodel has high computational costs.Coreference resolution is the task of mappingmentions like pronouns or short phrases to a pre-ceding, more explicit, mention.
Recently, interesthas arisen in cross-document coreference resolution(Mayfield09), which comes closer to NED, but doesnot aim at mapping names onto entities in a knowl-edge base.
Word sense disambiguation (McCarthy09;Navigli09) is the more general task of mapping con-tent words to a predefined inventory of word senses.While the NED problem is similar, it faces the chal-lenges that the ambiguity of entity names tends to bemuch higher (e.g., mentions of common lastnamesor firstname-only).Projects on automatically building knowledgebases (Doan08) from natural-language text includeKnowItAll (Banko07), YAGO and its tool SOFIE(Suchanek09; Nakashole11), StatSnowball (Zhu09),ReadTheWeb (Carlson10), and the factor-graph workby (Wick09).
Only SOFIE maps names onto canon-ical entities; the other projects produce output withambiguous names.
SOFIE folds the NED into itsMaxSat-based reasoning for fact extraction.
This ap-proach is computationally expensive and not intendedfor online disambiguation of entire texts.3 FrameworkMentions and Ambiguity: We consider an inputtext (Web page, news article, blog posting, etc.)
withmentions (i.e., surface forms) of named entities (peo-ple, music bands, songs, universities, etc.)
and aimto map them to their proper entries in a knowledgebase, thus giving a disambiguated meaning to entitymentions in the text.
We first identify noun phrasesthat potentially denote named entities.
We use theStanford NER Tagger (Finkel05) to discover theseand segment the text accordingly.Entity Candidates: For possible entities (withunique canonical names) that a mention could denote,we harness existing knowledge bases like DBpediaor YAGO.
For each entity they provide a set of shortnames (e.g., ?Apple?
for Apple Inc. and para-784phrases (e.g., ?Big Apple?
for New York City).In YAGO, these are available by the means relation,which in turn is harvested from Wikipedia disam-biguation pages, redirects, and links.Popularity Prior for Entities: Prominence orpopularity of entities can be seen as a probabilisticprior for mapping a name to an entity.
The most com-mon way of estimating this are the Wikipedia-basedfrequencies of particular names in link anchor textsreferring to specific entities, or number of inlinks.Context Similarity of Mentions and Entities:The key for mapping mentions onto entities are thecontexts on both sides of the mapping.
We considertwo different approaches.
First, for each mention,we construct a context from all words in the entireinput text.
This way, we can represent a mentionas a set of (weighted) words or phrases that it co-occurs with.
Second, we alternatively consider simi-larity scores based on syntactically-parsed contexts,based on (Thater10).
On the entity side of the map-ping, we associate each entity with characteristickeyphrases or salient words, precomputed from Wi-kipedia articles and similar sources.
For example,Larry Page would have keyphrases like ?Stan-ford?, ?search engine?, etc., whereas Jimmy Pagemay have keyphrases ?Gibson guitar?, ?hard rock?,etc.
Now we can define and compute similarity mea-sures between a mention and an entity candidate,e.g., the weighted word overlap, the KL divergence,n-gram-based measures, etc.
In addition, we mayuse syntactic contextualization techniques, based ondependency trees, that suggest phrases that are typi-cally used with the same verb that appears with themention in the input text (Thater10).Coherence among Entities: On the entity side,each entity has a context in the underlying knowl-edge base(s): other entities that are connected viasemantic relationships (e.g., memberOf) or have thesame semantic type (e.g., rock musician).
Anasset that knowledge bases like DBpedia and YAGOprovide us with is the same-as cross-referencing toWikipedia.
This way, we can quantify the coherencebetween two entities by the number of incoming linksthat their Wikipedia articles share.
When we considercandidate entities for different mentions, we can nowdefine and compute a notion of coherence among thecorresponding entities, e.g., by the overlap amongtheir related entities or some form of type distance.Coherence is a key asset because most texts deal witha single or a few semantically related topics such asrock music or Internet technology or global warming,but not everything together.Overall Objective Function: To aim for the bestdisambiguation mappings, our framework combinesprior, similarity, and coherence measures into acombined objective function: for each mention mi,i = 1..k, select entity candidates eji , one per men-tion, such that?
?
?i=1..kprior(mi, eji)+?
?
?i=1..ksim(cxt(mi), cxt(eji))+?
?
coh(ej1 ?
cnd(m1) .
.
.
ejk ?
cnd(mk)) = max!where ?
+ ?
+ ?
= 1, cnd(mi) is the set of pos-sible meanings of mi, cxt( ) denotes the context ofmentions and entities, respectively, and coh( ) is thecoherence function for a set of entities.Section 4 gives details on each of these three com-ponents.
For robustness, our solution selectively en-ables or disables the three components, based on testson the mentions of the input text; see Section 5.4 Features and Measures4.1 Popularity PriorAs mentioned above, our framework supports multi-ple forms of popularity-based priors, but we found amodel based on Wikipedia link anchors to be mosteffective: For each surface form that constitutes ananchor text, we count how often it refers to a partic-ular entity.
For each name, these counts provide uswith an estimate for a probability distribution overcandidate entities.
For example, ?Kashmir?
refers toKashmir (the region) in 90.91% of all occurrencesand in 5.45% to Kashmir (Song).4.2 Mention-Entity SimilarityKeyphrase-based Similarity: On the mention side,we use all tokens in the document (except stopwordsand the mention itself) as context.
We experimentedwith a distance discount to discount the weight oftokens that are further away, but this did not improvethe results for our test data.On the entity side, the knowledge base knows au-thoritative sources for each entity, for example, the785corresponding Wikipedia article or an organizationalor individual homepage.
These are the inputs foran offline data-mining step to determine character-istic keyphrases for each entity and their statisticalweights.
We describe this only for Wikipedia as in-put corpus, the approach extends to other inputs.
Askeyphrase candidates for an entity we consider itscorresponding Wikipedia article?s link anchors texts,including category names, citation titles, and externalreferences.
We extended this further by consideringalso the titles of articles linking to the entity?s article.All these phrases form the keyphrase set of an entity:KP (e).For each word w that occurs in a keyphrase, wecompute a specificity weight with regard to the givenentity: the MI (mutual information) between the en-tity e and the keyword w, calculating the joint proba-bilities for MI as follows:p(e, w) =?
?w ?
(KP (e) ??e?
?INe KP (e?))?
?Nreflecting if w is contained in the keyphrase set of eor any of the keyphrase sets of an entity linking to e,IN(e), with N denoting the total number of entities.The joint probabilities for the cases p(e, w?
), p(e?, w),p(e?, w?)
are calculated accordingly.Keyphrases may occur only partially in an inputtext.
For example, the phrase ?Grammy Award win-ner?
associated with entity Jimmy Page may oc-cur only in the form ?Grammy winner?
near somemention ?Page?.
Therefore, our algorithm for thesimilarity of mention m with regard to entity e com-putes partial matches of e?s keyphrases in the text.This is done by matching individual words and re-warding their proximity in an appropriate score.
Tothis end we compute, for each keyphrase, the shortestwindow of words that contains a maximal numberof words of the keyphrase.
We refer to this windowas the phrase?s cover (cf.
(Taneva11)).
For example,matching the text ?winner of many prizes includingthe Grammy?
results in a cover length of 7 for thekeyphrase ?Grammy award winner?.
By this ratio-nale, the score of partially matching phrase q in a textis set to:score(q) = z(?w?cover weight(w)?w?q weight(w))2where z = # matching wordslength of cover(q) andweight(w) is eitherthe MI weight (defined above) or the collection-wideIDF weight of the keyphrase word w. Note that thesecond factor is squared, so that there is a superlinearreduction of the score for each word that is missingin the cover.For the similarity of a mention m to candidateentity e, this score is aggregated over all keyphrasesof e and all their partial matches in the text, leadingto the similarity scoresimscore(m, e) =?q?KP (e)score(q)Syntax-based Similarity: In addition to surfacefeatures of words and phrases, we leverage informa-tion about the immediate syntactic context in whichan entity mention occurs.
For example, in the sen-tence ?Page played unusual chords?, we can extractthe fact that the mention ?Page?
is the subject of theverb ?play?.
Using a large text corpus for training,we collect statistics about what kinds of entities tendto occur as subjects of ?play?, and then rank the can-didate entities according to their compatibility withthe verb.Specifically, we employ the framework of(Thater10), which allows us to derive vector represen-tations of words in syntactic contexts (such as beingthe subject of a particular verb).
We do not directlyapply this model to derive contextualized representa-tions of entity mentions, as information about specificproper names is very sparse in corpora like GigaWordor Wikipedia.
Instead, we consider a set of substi-tutes for each possible entity e, which we take as itscontext cxt(e).
For this, we use the WordNet synsetsassociated with the entity?s YAGO types and all theirhypernyms.
For each substitute, we compute a stan-dard distributional vector and a contextualized vectoraccording to (Thater10).
Syntax-based similarity be-tween cxt(e) and the context cxt(m) of the mentionis then defined as the sum of the scalar-product simi-larity between these two vectors for each substitute.This results in high similarity if the syntactic contex-tualization only leads to small changes of the vectors,reflecting the compatibility of the entity?s substitutes.In our example, we compute a vector for ?gui-tarist?
as subject of ?play?, and another one for ?en-trepreneur?
in the same context.
The former is more786compatible with the given context than the latter, lead-ing to higher similarity for the entity Jimmy Page.4.3 Entity-Entity CoherenceAs all entities of interest are registered in a knowl-edge base (like YAGO), we can utilize the semantictype system, which is usually a DAG of classes.
Thesimples measure is the distance between two entitiesin terms of type and subclassOf edges.The knowledge bases also provide same-as cross-referencing to Wikipedia, amd we quantify the coher-ence between two entities by the number of incom-ing links that their Wikipedia articles share.
Thisapproach has been refined by Milne and Witten(Milne08), taking into account the total number N ofentities in the (Wikipedia) collection:mw coh(e1, e2) =1?
log (max(|INe1 |, |INe2 |))?
log(|INe1 ?
INe2 |)log(|N |)?
log (min(|INe1 |, |INe2 |))if > 0 and else set to 0.5 Graph Model and Algorithms5.1 Mention-Entity GraphFrom the popularity, similarity, and coherence mea-sures discussed in Section 4, we construct a weighted,undirected graph with mentions and candidate enti-ties as nodes.
As shown in the example of Figure 1,the graph has two kinds of edges:?
A mention-entity edge is weighted with a similar-ity measure or a combination of popularity andsimilarity measure.
Our experiments will use alinear combination with coefficients learned fromwithheld training data.?
An entity-entity edge is weighted based onWikipedia-link overlap, or type distance, or somecombination along these lines.Our experiments will focus on anchor-based pop-ularity, keyphrase-based and/or syntactic similarity,and link-based coherence (mw coh).
The mention-entity graph is dense on the entities side and often hashundreds or thousands of nodes, as the YAGO knowl-edge base offers many candidate entities for commonmentions (e.g., country names that could also denotesports teams, common lastnames, firstnames, etc.
).5.2 Graph AlgorithmGiven a mention-entity graph, our goal is to com-pute a dense subgraph that would ideally contain allmention nodes and exactly one mention-entity edgefor each mention, thus disambiguating all mentions.We face two main challenges here.
The first is howto specify a notion of density that is best suited forcapturing the coherence of the resulting entity nodes.The seemingly most natural approach would be tomeasure the density of a subgraph in terms of its totaledge weight.
Unfortunately, this will not work ro-bustly for the disambiguation problem.
The solutioncould be dominated by a few entity nodes with veryhigh weights of incident edges, so the approach couldwork for prominent targets, but it would not achievehigh accuracy also for the long tail of less prominentand more sparsely connected entities.
We need tocapture the weak links in the collective entity set ofthe desired subgraph.
For this purpose, we definethe weighted degree of a node in the graph to be thetotal weight of its incident edges.
We then define thedensity of a subgraph to be equal to the minimumweighted degree among its nodes.
Our goal is tocompute a subgraph with maximum density, whileobserving constraints on the subgraph structure.The second critical challenge that we need to faceis the computational complexity.
Dense-subgraphproblems are almost inevitably NP-hard as they gen-eralize the Steiner-tree problem.
Hence, exact algo-rithms on large input graphs are infeasible.To address this problem, we adopt and extend anapproximation algorithm of (Sozio10) for the prob-lem of finding strongly interconnected, size-limitedgroups in social networks.
The algorithm starts fromthe full mention-entity graph and iteratively removesthe entity node with the smallest weighted degree.Among the subgraphs obtained in the various steps,the one maximizing the minimum weighted degreewill be returned as output.
To guarantee that wearrive at a coherent mention-entity mapping for allmentions, we enforce each mention node to remainconnected to at least one entity.
However, this con-straint may lead to very suboptimal results.For this reason, we apply a pre-processing phase toprune the entities that are only remotely related to themention nodes.
For each entity node, we compute thedistance from the set of all mention nodes in terms787They performedKashmir,written byPageand Plant.Page playedunusual chordson his Gibson.??
Led Zeppelin??
Hard rock??
Electric guitar??
Session guitarist??
Led Zeppelin??
Gibson??
Jimmy Pagesignature model??
Hard rockKashmir (song)Kashmir (region)Larry PageJimmy PagePage, ArizonaRobert PlantGibson Les PaulGibson, MissouriFigure 1: Mention-Entity Graph Exampleof the sum of the corresponding squared shortest-path distances.
We then restrict the input graph tothe entity nodes that are closest to the mentions.
Anexperimentally determined good choice for the sizeof this set is five times the number of the mentionnodes.
Then the iterative greedy method is run onthis smaller subgraph.
Algorithm 1 summarizes thisprocedure, where an entity is taboo if it is thelast candidate for a mention it is connected to.Algorithm 1: Graph Disambiguation AlgorithmInput: weighted graph of mentions and entitiesOutput: result graph with one edge per mentionbeginpre?processing phase;foreach entity docalculate distance to all mentions;keep the closest (5?
mentions count)entities, drop the others;main loop;while graph has non-taboo entity dodetermine non-taboo entity nodewith lowest weighted degree, remove itand all its incident edges;if minimum weighted degree increasedthenset solution to current graph;post?processing phase;process solution by local search or fullenumeration for best configuration;The output of the main loop would often be closeto the desired result, but may still have more than onemention-entity edge for one or more mentions.
Atthis point, however, the subgraph is small enough toconsider an exhaustive enumeration and assessmentof all possible solutions.
This is one of the optionsthat we have implemented as post-processing step.Alternatively, we can perform a faster local-searchalgorithm.
Candidate entities are randomly selectedwith probabilities proportional to their weighted de-grees.
This step is repeated for a prespecified numberof iterations, and the best configuration with the high-est total edge-weight is used as final solution.5.3 Robustness TestsThe graph algorithm generally performs well.
How-ever, it may be misled in specific situations, namely,if the input text is very short, or if it is thematicallyheterogeneous.
To overcome these problems, we in-troduce two robustness tests for individual mentionsand, depending on the tests?
outcomes, use only asubset of our framework?s features and techniques.Prior test: Our first test ensures that the popularityprior does not unduly dominate the outcome if thetrue entities are dominated by false alternatives.
Wecheck, for each mention, whether the popularity priorfor the most likely candidate entity is above somethreshold ?, e. g. above 90% probability.
If this is notthe case, then the prior is completely disregarded forcomputing the mention-entity edge weights.
Other-wise, the prior is combined with the context-basedsimilarity computation to determine edge weights.788We never rely solely on the prior.Coherence test: As a test for whether the coher-ence part of our framework makes sense or not,we compare the popularity prior and the similarity-only measure, on a per-mention basis.
For eachmention, we compute the L1 distance between thepopularity-based vector of candidate probabilitiesand the similarity-only-based vector of candidateprobabilities:?i=1..k|prior(m, ei)?
simscore(m, ei)|This difference is always between 0 and 2.
If it ex-ceeds a specified threshold ?
(e.g., 1), the disagree-ment between popularity and similarity-only indi-cates that there is a situation that coherence may beable to fix.
If, on the other hand, there is hardly anydisagreement, using coherence as an additional as-pect would be risky for thematically heterogeneoustexts and should better be disabled.
In that case, wechoose an entity for the mention at hand, using thecombination of prior and similarity.
Only the win-ning entity is included in the mention-entity graph, allother candidates are omitted for the graph algorithm.The robustness tests and the resulting adaptation ofour method are fully automated.6 Experiments6.1 SetupSystem: All described methods are implemented ina prototype system called AIDA (Accurate OnlineDisambiguation of Named Entities).
We use the Stan-ford NER tagger (Finkel05) to identify mentions ininput texts, the YAGO2 knowledge base (Hoffart11)as a repository of entities, and the English Wikipe-dia edition (as of 2010-08-17) as a source of miningkeyphrases and various forms of weights.
The graphalgorithm makes use of Webgraph (Boldi04).Datasets: There is no established benchmark forNED.
The best prior work (Kulkarni09)) compiledits own hand-annotated dataset, sampled from onlinenews.
Unfortunately, this data set is fairly small (102short news articles, about 3,500 proper noun men-tions).
Moreover, its entity annotations refer to an oldversion of Wikipedia.
To avoid unfair comparisons,we created our own dataset based on CoNLL 2003articles 1,393mentions (total) 34,956mentions with no entity 7,136words per article (avg.)
216mentions per article (avg.)
25distinct mentions per article (avg.)
17mentions with candidate in KB (avg.)
21entities per mention (avg) 73initial annotator disagreement (%) 21.1Table 1: CoNLL Dataset Propertiesdata, extensively used in prior work on NER tagging(Sang03).This consists of proper noun annotations for 1393Reuters newswire articles.
We hand-annotated allthese proper nouns with corresponding entities inYAGO2.
Each mention was disambiguated by twostudents and resolved by us in case of conflict.
Thisdata set is referred to as CoNLL in the followingand fully available at http://www.mpi-inf.mpg.de/yago-naga/aida/.
Table 1 summarizes prop-erties of the dataset.Methods under comparison: Our framework in-cludes many variants of prior methods from the lit-erature.
We report experimental results for some ofthem.
AIDA?s parameters were tuned by line-searchon 216 withheld development documents.
We foundthe following to work best:?
threshold for prior test: ?
= 0.9?
weights for popularity, similarity, coherence:?
= 0.43, ?
= 0.47, ?
= 0.10?
initial number of entites in graph: 5 ?
#mentions?
threshold for coherence test: ?
= 0.9We checked the sensitivity of the hyper-parametersettings and found the influence of variations to besmall, e. g. when varying ?
within the range [0.5,1.3],the changes in precision@1.0 are within 1%.The baseline for our experiments is the collective-inference method of (Kulkarni09), which outper-forms simpler methods (such as (Milne08)).
Werefer to this method as Kul CI.
Since program codefor this method is not available, we re-implementedit using the LP solver CPLEX for the optimizationproblem with subsequent rounding, as described in(Kulkarni09).
In addition, we compare against (ourre-implementation of) the method of (Cucerzan07),789Our Methods Competitorssim-k priorsim-kpriorsim-ssim-ksim-sr-priorsim-kr-priorsim-kcohr-priorsim-kr-cohprior Cuc Kul s Kul sp Kul CIMacro P@1.0 76.53 75.75 71.43 76.40 80.71 80.73 81.91 71.24 43.74 58.06 76.74 76.74Micro P@1.0 76.09 70.72 66.09 76.13 79.57 81.77 81.82 65.84 51.03 63.42 72.31 72.87MAP 66.98 83.99 85.97 67.00 85.91 89.05 87.31 86.63 40.06 63.90 86.50 85.44Table 2: Experimental results on CoNLL (all values in %)referred to as Cuc.
For all methods, weights forcombining components were obtained by traininga SVM classifier on 946 withheld CoNLL trainingdocuments.Performance measures: The key measures in ourevaluation are precision and recall.
We considerthe precision-recall curve, as there is an inherenttrade-off between the two measures.
Precision is thefraction of mention-entity assignments that matchthe ground-truth assignment.
Recall is the fractionof the ground-truth assignments that our method(s)could compute.
Both measures can aggregate over ofall mentions (across all texts) or over all input texts(each with several mentions).
The former is calledmicro-averaging, the latter macro-averaging.As we use a knowledge base with millions of enti-ties, we decided to neglect the situation that a mentionmay refer to an unknown entity not registered in theknowledge base.
We consider only mention-entitypairs where the ground-truth gives a known entity,and thus ignore roughly 20% of the mentions withoutknown entity in the ground-truth.
This simplifies thecalculation of aggregated precision-recall measureslike (interpolated) MAP (mean average precision):MAP = 1m?i=1..mprecision@ imwhere precision@ im is the precision at a specificrecall level.
This measure is equivalent to the areaunder the precision-recall curve.For constructing the precision-recall curve, we sortthe mention-entity pairs in descending order of con-fidence, so that x% recall refers to the x% with thehighest confidence.
We use each method?s mention-entity similarity for the confidence values.6.2 ResultsThe results of AIDA vs. the collective-inferencemethod of (Kulkarni09) and the entity disambigua-tion method of (Cucerzan07) on 229 test documentsare shown in Table 21.
The table includes variantsof our framework, with different choices for the sim-ilarity and coherence computations.
The shorthandnotation for the combinations in the table is as fol-lows: prior: popularity prior; r-prior: popularityprior with robustness test; sim-k: keyphrase basedsimilarity measure; sim-s: syntax-based similarity;coh: graph coherence; r-coh: graph coherence withrobustness test.The shorthand names for competitors are: Cuc:(Cucerzan07) similarity measure; Kul s: (Kulka-rni09) similarity measure only; Kul sp: Kul s com-bined with plus popularity prior; Kul CI: Kul sp com-bined with coherence.
All coherence methods usethe Milne-Witten inlink overlap measure mw coh.The most important measure is macro/micro preci-son@1.0, which corresponds to the overall correct-ness of the methods for all mentions that are assignedto an entity in the ground-truth data.
Our sim-k pre-cision is already very good.
Combining it with thesyntax-based similarity improves micro-averaged pre-cision@1.0, but the macro-averaged results are a bitworse.
Thus, the more advanced configurations ofAIDA did not use syntax-based similarity.
Uncondi-tionally combining prior and sim-k degrades the qual-ity, but including the prior robustness test (r-priorsim-k) improves the results significantly.
The preci-sion for our best method, the prior- and coherence-tested Keyphrase-based mention-entity similarity (r-prior sim-k r-coh), significantly outperforms all com-petitors (with a p-value of a paired t-test< 0.01).
Ourmacro-averaged precision@1.0 is 81.91%, whereasKul CI only achieves 76.74%.
Even r-prior sim-k, without any coherence, significantly outperforms12 of the 231documents in the original test set could not beprocessed by Kul CI due to memory limitations.
All results aregiven for the subset, for the sake of comparability.
Results forthe complete set are available on our website.7900 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.70.80.9r-prior sim-k r-cohr-prior sim-kKul CI Kul sppriorrecallprecisionFigure 2: Experimental results on CoNLL: precision-recall curvesKul CI (with coherence) with a p-value of < 0.01.In micro-average precision@1.0, the differences areeven higher, showing that we perform better through-out all documents.The macro-averaged precision-recall curves in Fig-ure 2 show that the best AIDA method performsparticularly well in the tail of high recall values.
TheMAP underlines the robustness of our best methods.The high MAP for the prior method is becausewe rank by mention-entity edge weight; for priorthis is simply the prior probability.
As the prior ismost probably correct for mentions with a very highprior for their most popular entity (by definition), theinitial ranking of the prior is very good, but dropsmore sharply.
We believe that the main difficulty innamed entity disambiguation lies exactly in the ?longtail?
of not-so-prominent entities.We also tried the (Milne08) web service on a sub-set of our test collection, but this was obviouslygeared for Wikipedia linkage and performed poorly.6.3 DiscussionOur keyphrase-based similarity measure performsbetter than the Kul s measure, which is a combina-tion of 4 different entity contexts (abstract tokens,full text tokens, inlink anchor tokens, inlink anchortokens + surrounding tokens), 3 similarity measures(Jaccard, dot product, and tf.idf cosine similarity),and the popularity prior.
Adding the prior to oursimilarity measure by linear combination degradesthe performance.
We found that our measure alreadycaptures a notion of popularity because popular enti-ties have more keyphrases and can thus accumulatea higher total score.
The popularity should only beused when one entitiy has a very high probability, andintroducing the robustness test for the prior achievedthis, improving on both our similarity and Kul sp.Unconditionally adding the notion of coherenceamong entities improves the micro-average precision,but not the macro-average.
Investigating potentialproblems, we found that the coherence can be ledastray when parts of the document form a coherentcluster of entities, and other entities are then forcedto be coherent to this cluster.
To overcome this is-sue, we introduced the coherence robustness test,and the results with r-coh show that it makes senseto fix an entity for a mention when the prior andsimilarity are in reasonable agreement.
Adding thiscoherence test leads to a signigicant (p-value < 0.05)improvement over the non-coherence based measuresin both micro- and macro-average precision.
Our ex-periments showed that when adding this coherencetest, around 23 of the mentions are solved using localsimilarity only and are assigned an entity before run-ning the graph algorithm.
In summary, we observedthat the AIDA configuration with r-prior, keyphrase-based sim-k, and r-coh significantly outperformed allcompetitors.7 Conclusions and Future WorkThe AIDA system provides an integrated NEDmethod using popularity, similarity, and graph-basedcoherence, and includes robustness tests for self-adaptive behavior.
AIDA performed significantly bet-ter than state-of-the-art baselines.
The system is fullyimplemented and accessible online (http://www.mpi-inf.mpg.de/yago-naga/aida/).
Our fu-ture work will consider additional semantic proper-ties between entities (types, memberOf/partOf, etc.
)for further enhancing the coherence algorithm.AcknowledgementsThis work has been partially supported by the German Sci-ence Foundation (DFG) through the Cluster of Excellenceon ?Multimodal Computing and Interaction?
and the Eu-ropean Union through the 7th Framework IST IntegratedProject ?LivingKnowledge?
(no.
231126).
We also thankMauro Sozio for the discussion on the graph algorithm.791ReferencesSo?ren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann, Richard Cyganiak, Zachary G. Ives: DB-pedia: A Nucleus for a Web of Open Data.
ISWC 2007Michele Banko, Michael J. Cafarella, Stephen Soderland,Matthew Broadhead, Oren Etzioni: Open InformationExtraction from the Web.
IJCAI 2007Paolo Boldi and Sebastiano Vigna.
The WebGraph frame-work I: Compression techniques.
WWW 2004, soft-ware at http://webgraph.dsi.unimi.it/Razvan C. Bunescu, Marius Pasca: Using EncyclopedicKnowledge for Named entity Disambiguation.
EACL2006Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka Jr., Tom M. Mitchell.Toward an Architecture for Never-Ending LanguageLearning.
AAAI 2010Silviu Cucerzan: Large-Scale Named Entity Disambigua-tion Based on Wikipedia Data.
EMNLP-CoNLL 2007AnHai Doan, Luis Gravano, Raghu Ramakrishnan, Shiv-akumar Vaithyanathan.
(Eds.).
Special issue on infor-mation extraction.
SIGMOD Record, 37(4), 2008.Jenny Rose Finkel, Trond Grenager, Christopher Man-ning: Incorporating Non-local Information into Infor-mation Extraction Systems by Gibbs Sampling.
ACL2005, software at http://nlp.stanford.edu/software/CRF-NER.shtmlXianpei Han, Jun Zhao: Named entity disambiguationby leveraging wikipedia semantic knowledge.
CIKM2009.Johannes Hoffart, Fabian Suchanek, Klaus Berberich, Ed-win Lewis-Kelham, Gerard de Melo, Gerhard Weikum:YAGO2: Exploring and Querying World Knowledge inTime, Space, Context, and Many Languages.
Demo Pa-per, WWW 2011, data at http://www.mpi-inf.mpg.de/yago-naga/yago/Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,Soumen Chakrabarti: Collective annotation of Wikipe-dia entities in web text.
KDD 2009James Mayfield et al: Corss-Document Coreference Res-olution: A Key Technology for Learning by Reading.AAAI Spring Symposium on Learning by Reading andLearning to Read, 2009.Diane McCarthy.
Word Sense Disambiguation: AnOverview.
Language and Linguistics Compass 3(2):537-558, Wiley, 2009Rada Mihalcea, Andras Csomai: Wikify!
: Linking Docu-ments to Encyclopedic Knowledge.
CIKM 2007David N. Milne, Ian H. Witten: Learning to Link withWikipedia.
CIKM 2008Ndapandula Nakashole, Martin Theobald, GerhardWeikum: Scalable Knowledge Harvesting with HighPrecision and High Recall.
WSDM 2011Roberto Navigli: Word sense disambiguation: A survey.ACM Comput.
Surv., 41(2), 2009Hien T. Nguyen, Tru H. Cao: Named Entity Disambigua-tion on an Ontology Enriched by Wikipedia.
RIVF2008Erik F. Tjong Kim Sang, Fien De Meulder: Introduction tothe CoNLL-2003 Shared Task: Language-IndependentNamed Entity Recognition.
CoNLL 2003Mauro Sozio, Aristides Gionis: The Community-searchProblem and How to Plan a Successful Cocktail Party.KDD 2010Fabian M. Suchanek, Gjergji Kasneci, Gerhard Weikum:YAGO: a Core of Semantic Knowledge.
WWW 2007Fabian Suchanek, Mauro Sozio, Gerhard Weikum: SOFIE:a Self-Organizing Framework for Information Extrac-tion.
WWW 2009Bilyana Taneva, Mouna Kacimi, and Gerhard Weikum:Finding Images of Rare and Ambiguous Entities.
Tech-nical Report MPI-I-2011-5-002, Max Planck Institutefor Informatics, 2011.Stefan Thater, Hagen Fu?rstenau, Manfred Pinkal.
Contex-tualizing Semantic Representations using SyntacticallyEnriched Vector Models.
ACL 2010Michael L. Wick, Aron Culotta, Khashayar Rohani-manesh, Andrew McCallum: An Entity Based Modelfor Coreference Resolution.
SDM 2009: 365-376Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, Ji-RongWen: StatSnowball: a Statistical Approach to Extract-ing Entity Relationships.
WWW 2009792
