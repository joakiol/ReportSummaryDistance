Combining Multiple Knowledge Sources for DiscourseSegmentationDiane  J .
L i tmanAT&T Bell Laborator ies600 Mounta in  AvenueMurray  Hill, N J  07974d iane@research.at t .comRebecca  J .
Passonneau*Bel lcore445 South StreetMorr istown,  N J  07960beck~be l l core .comAbst rac tWe predict discourse segment boundariesfrom linguistic features of utterances, usinga corpus of spoken narratives as data.
Wepresent two methods for developing seg-mentation algorithms from training data:hand tuning and machine learning.
Whenmultiple types of features are used, resultsapproach uman performance on an inde-pendent est set (both methods), and usingcross-validation (machine learning).1 In t roduct ionMany have argued that discourse has a global struc-ture above the level of individual utterances, andthat linguistic phenomena like prosody, cue phra-ses, and nominal reference are partly conditioned byand reflect this structure (cf.
(Grosz and Hirschberg,1992; Grosz and Sidner, 1986; Hirschberg and Grosz,1992; Hirschberg and Litman, 1993; Hirschberg andPierrehumbert, 1986; Hobbs, 1979; Lascarides andOberlander, 1992; Linde, 1979; Mann and Thomp-son, 1988; Polanyi, 1988; Reichman, 1985; Webber,1991)).
However, an obstacle to exploiting the rela-tion between global structure and linguistic devicesin natural anguage systems is that there is too littledata about how they constrain one another.
Wehave been engaged in a study addressing this gap.In previous work (Passonneau and Litman, 1993),we reported on a method for empirically validatingglobal discourse units, and on our evaluation of algo-rithms to identify these units.
We found significantagreement among naive subjects on a discourse seg-mentation task, which suggests that global discourseunits have some objective reality.
However, we alsofound poor correlation of three untuned algorithms(based on features of referential noun phrases, cuewords, and pauses, respectively) with the subjects'segmentations.In this paper, we discuss two methods for develo-ping segmentation algorithms using multiple know-*Bellcore did not support he second author's work.ledge sources.
In section 2, we give a brief overviewof related work and summarize our previous results.In section 3, we discuss how linguistic features arecoded and describe our evaluation.
In section 4, wepresent our analysis of the errors made by the bestperforming untuned algorithm, and a new algorithmthat relies on enriched input features and multipleknowledge sources.
In section 5, we discuss our useof machine learning tools to automatically constructdecision trees for segmentation from a large set ofinput features.
Both the hand tuned and automa-tically derived algorithms improve over our previ-ous algorithms.
The primary benefit of the handtuning is to identify new input features for impro-ving performance.
Machine learning tools make itconvenient to perform numerous experiments, touselarge feature sets, and to evaluate results using cross-validation.
We discuss the significance of our resultsand briefly compare the two methods in section 6.2 D iscourse  Segmentat ion2.1 Re lated WorkSegmentation has played a significant role in muchwork on discourse.
The linguistic structure of Groszand Sidner's (1986) tri-partite discourse model con-sists of multi-utterance s gments whose hierarchicalrelations are isomorphic with intentional structure.In other work (e.g., (Hobbs, 1979; Polanyi, 1988)),segmental structure is an artifact of coherence re-lations among utterances, and few if any specificclaims are made regarding segmental structure perse.
Rhetorical Structure Theory (RST) (Mann andThompson, 1988) is another tradition of defining re-lations among utterances, and informs much workin generation.
In addition, recent work (Moore andParis, 1993; Moore and Pollack, 1992) has addressedthe integration of intentions and rhetorical relations.Although all of these approaches have involved de-tailed analyses of individual discourses or represen-tative corpora, we believe there is a need for morerigorous empirical studies.Researchers have begun to investigate the abilityof humans to agree with one another on segmen-108tation, and to propose methodologies for quantify-ing their findings.
Several studies have used expertcoders to locally and globally structure spoken dis-course according to the model of Grosz and Sid-net (1986), including (Grosz and Hirschberg, 1992;Hirschberg and Grosz, 1992; Nakatani et al, 1995;Stifleman, 1995).
Hearst (1994) asked subjectsto place boundaries between paragraphs of exposi-tory texts, to indicate topic changes.
Moser andMoore (1995) had an expert coder assign segmentsand various segment features and relations basedon RST.
To quantify their findings, these studiesuse notions of agreement (Gale et al, 1992; Mo-set and Moore, 1995) and/or reliability (Passonneauand Litman, 1993; Passonneau and Litman, to ap-pear; Isard and Carletta, 1995).By asking subjects to segment discourse using anon-linguistic riterion, the correlation of linguisticdevices with independently derived segments canthen be investigated in a way that avoids circularity.Together, (Grosz and Hirschberg, 1992; Hirschbergand Grosz, 1992; Nakatani et al, 1995) comprisean ongoing study using three corpora: professio-nally read AP news stories, spontaneous narrative,and read and spontaneous versions of task-orientedmonologues.
Discourse structures are derived fromsubjects' segmentations, then statistical measuresare used to characterize these structures in terms ofacoustic-prosodic features.
Grosz and Hirschberg'swork also used the classification and regression treesystem CART (Breiman et al, 1984) to automati-cally construct and evaluate decision trees for classi-fying aspects of discourse structure from intonatio-nal feature values.
Morris and Hirst (1991) structu-red a set of magazine texts using the theory of (Groszand Sidner, 1986), developed a thesaurus-based le-xical cohesion algorithm to segment text, then qua-litatively compared their segmentations with the re-sults.
Hearst (1994) presented two implemented seg-mentation algorithms based on term repetition, andcompared the boundaries produced to the bounda-ries marked by at least 3 of 7 subjects, using in-formation retrieval metrics.
Kozima (1993) had 16subjects egment a simplified short story, developedan algorithm based on lexical cohesion, and qualita-tively compared the results.
Reynar (1994) propo-sed an algorithm based on lexical cohesion in con-junction with a graphical technique, and used infor-mation retrieval metrics to evaluate the algorithm'sperformance in locating boundaries between conca-tenated news articles.2.2 Our Prev ious ResultsWe have been investigating a corpus of monologuescollected and transcribed by Chafe (1980), knownas the Pear stories.
As reported in (Passonneauand Litman, 1993), we first investigated whetherunits of global structure consisting of sequences ofutterances could be reliably identified by naive sub-jects.
We analyzed linear segmentations of 20 nar-ratives performed by naive subjects (7 new subjectsper narrative), where speaker intention was the seg-ment criterion.
Subjects were given transcripts, as-ked to place a new segment boundary between li-nes (prosodic phrases) 1 wherever the speaker hada new communicative goal, and to briefly describethe completed segment.
Subjects were free to as-sign any number of boundaries.
The qualitativeresults were that segments varied in size from 1to 49 phrases in length (Avg.-5.9), and the rateat which subjects assigned boundaries ranged from5.5% to 41.3%.
Despite this variation, we foundstatistically significant agreement among subjectsacross all narratives on location of segment boun-daries (.114 z 10 -6 < p < .6 z 10-9).We then looked at the predictive power of lin-guistic cues for identifying the segment boundariesagreed upon by a significant number of subjects.
Weused three distinct algorithms based on the distri-bution of referential noun phrases, cue words, andpauses, respectively.
Each algorithm (NP-A, CUE-A, PAUSE-A) was designed to replicate the subjects'segmentation task (break up a narrative into conti-guous egments, with segment breaks falling betweenprosodic phrases).
NP-A used three features, whileCUE-A and PAUSE-A each made use of a single fea-ture.
The features are a subset of those described insection 3.To evaluate how well an algorithm predicted seg-mental structure, we used the information retrie-val (IR) metrics described in section 3.
As repor-ted in (Passonneau and Litman, to appear), we alsoevaluated a simple additive method for combiningalgorithms in which a boundary is proposed if eachseparate algorithm proposes a boundary.
We testedall pairwise combinations, and the combination ofall three algorithms.
No algorithm or combinationof algorithms performed as well as humans.
NP-A performed better than the other unimodal algo-rithms, and a combination of NP-A and PAUSE-Aperformed best.
We felt that significant improve-ments could be gained by combining the input fea-tures in more complex ways rather than by simplycombining the outputs of independent algorithms.3 Methodo logy3.1 Boundary  C lass i f i ca t ionWe represent each narrative in our corpus as a se-quence of potential boundary sites, which occur bet-ween prosodic phrases.
We classify a potential boun-dary site as boundary if it was identified as suchby at least 3 of the 7 subjects in our earlier study.Otherwise it is classified as non-boundary.
Agree-ment among subjects on boundaries was significantat below the .02% level for values ofj ___ 3, where j is1 We used Chafe's (1980) prosodic analysis.109.
.Because he's looking at the girl.\]1 SUBJECT (non-boundary)\[\[.75\] Falls over,\[ 5 SUBJECTS (boundary) l\[1.35\] uh there's no conversation in this movie.\ [0  SUBJECTS (non-boundary)\[\[.6\] There 's  sounds,\ [0  SUBJECTS (.on-boundary)\]yOU know,I O SUBJECTS (non-boundary) llike the birds and stuff,10 SUBJECTS (non-boundary)\]but there., the humans  beings in it don ' t  say anything.17  SUBJECTS (boundary)\[ll.01 He falls over,Figure h Excerpt from narr.
6, with boundaries.the number of subjects (1 to 7), on all 20 narratives.
2Fig.
1 shows a typical segmentation of one of thenarratives in our corpus.
Each line corresponds toa prosodic phrase, and each space between the li-nes corresponds to a potential boundary site.
Thebracketed numbers will be explained below.
The bo-xes in the figure show the subjects' responses at eachpotential boundary site, and the resulting boundaryclassification.
Only 2 of the 7 possible boundary si-tes are classified as boundary.3.2 Cod ing  o f  L ingu is t ic  FeaturesGiven a narrative of n prosodic phrases, the n-1 po-tential boundary sites are between each pair of pros-odic phrases Pi and P/+I, i from 1 to n-1.
Eachpotential boundary site in our corpus is coded usingthe set of linguistic features hown in Fig.
2.Values for the prosodic features are obtained byautomatic analysis of the transcripts, whose con-ventions are defined in (Chafe, 1980) and illustra-ted in Fig.
h .... and "?"
indicate sentence-final intonational contours; "," indicates phrase-finalbut not sentence final intonation; "\[X\]" indicatesa pause lasting X seconds; ".." indicates a breakin timing too short to be measured.
The featu-res before and after depend on the final punctua-tion of the phrases Pi and Pi+I, respectively.
Thevalue is '+sentence.final.contour' if "."
or "?
", '-sentence.final.contour' if ",".
Pause is assigned 'true'if Pi+l begins with \[X\], 'false' otherwise.
Durationis assigned X if pause is 'true', 0 otherwise.The cue phrase features are also obtained by au-tomatic analysis of the transcripts.
Cue1 is assigned'true' if the first lexical item in PI+I is a member ofthe set of cue words summarized in (Hirschberg andLitman, 1993).
Word1 is assigned this lexical item if2We prev ious ly  used  agreement  by  4 sub jec ts  as  thethreshold for boundaries; for j > 4, agreement was signi-ficant at the .01~0 level.
(Passonneau and Litman, 1993)?
P rosod ic  Features- before:+sentence.f inal .contour,-sentence.f lnal .contour- after: +sentence.f inal .contour,-sentence.f lnal .contour.- pause: true, false.- durat ion:  continuous.?
Cue  Phrase  Features- cue1: true, false.- word1: also, and, anyway, basically, because, but,  fi-nally, first, like, meanwhile,  no, now, oh, okay, only,or, see, so, then,  well, where, NA.-- cue2: true, false.- word2: and, anyway, because, boy, but,  now, okay, or,right, so, still, then,  NA.?
Noun Phrase  Features- coref: +coref , -corer ,  NA.- infer: +infer, -infer, NA.- global.pro: +global .pro,  -global.pro, NA.?
Combined  Feature-- cue-prosody: complex, true, false.Figure 2: Features and their potential values.cuel is true, 'NA' (not applicable) otherwise, a Cue2is assigned 'true' if cue, is true and the second lexi-cal item is also a cue word.
Word2 is assigned thesecond lexical item if cue2 is true, 'NA' otherwise.Two of the noun phrase (NP) features are hand-coded, along with functionally independent clauses(FICs), following (Passonneau, 1994).
The two aut-hors coded independently and merged their results.The third feature, global.pro, is computed from thehand coding.
FICs are tensed clauses that are neit-her verb arguments nor restrictive relatives.
If a newFIC (C/) begins in prosodic phrase Pi+I, then NPsin Cj are compared with NPs in previous clauses andthe feature values assigned as follows4:1. corer = '+coref'  if Cj contains an NP that co-refers with an NP in Cj-1; else corer= ' -cord'2.
infer= '+infer' i fCj contains an NP whose refe-rent can be inferred from an NP in Cj-1 on thebasis of a pre-defined set of inference relations;else infer-  '-infer'3.
global.pro = '+global.pro' if Cj contains a defi-nite pronoun whose referent is mentioned in aprevious clause up to the last boundary assignedby the algorithm; else global.pro = '-global.pro'If a new FIC is not initiated in Pi+I, values for allthree features are 'NA'.Cue-prosody, which encodes a combination ofprosodic and cue word features, was motivated byan analysis of IR errors on our training data, as de-scribed in section 4.
Cue-prosody is 'complex' if:aThe cue phrases that occur in the corpus &re shownas potential values in Fig.
2.4The NP algorithm can assign multiple boundarieswithin one prosodic phrase if the phrase contains mul-tiple clauses; these very rare cases are normalized (Pas-sonneau and Litman, 1993).110.
.Because hei's looking at the girl.\[.75\] (Z IBRO-PRONOUNi )  Falls over,before after pause durat ion cue 1 word 1 cue~ word;~ coref infer E;lobal.pro cue-prosodic+s.f.c -s.f.c true .75 false NA fM~e NA + + trueFigure 3: Example feature coding of a potential boundary site.1.
before = '+sentence.final.contour'2.
pause = 'true'3.
And either:(a) cuet = 'true', wordt ~ 'and'(b) cuet = 'true', word1 = 'and', cue2 = 'true',word2 ?
'and'Else, cue-prosody has the same values as pause.Fig.
3 illustrates how the first boundary site inFig.
1 would be coded using the features in Fig.
2.The prosodic and cue phrase features were moti-vated by previous results in the literature.
For ex-ample, phrases beginning discourse segments werecorrelated with preceding pause duration in (Groszand Hirschberg, 1992; ttirschberg and Grosz, 1992).These and other studies (e.g.~ (iiirschberg and Lit-man, 1993)) also found it useful to distinguish bet-ween sentence and non-sentence final intonationalcontours.
Initial phrase position was correlated withdiscourse signaling uses of cue words in (Hirschbergand Litman, 1993); a potential correlation betweendiscourse signaling uses of cue words and adjacencypatterns between cue words was also suggested.
Fi-nally, (Litman, 1994) found that treating cue phra-ses individually rather than as a class enhanced theresults of (iiirschberg and Litman, 1993).Passonneau (to appear) examined some of the fewclaims relating discourse anaphoric noun phrases toglobal discourse structure in the Pear corpus.
Re-suits included an absence of correlation of segmentalstructure with centering (Grosz et al, 1983; Kamey-ama, 1986), and poor correlation with the contrastbetween full noun phrases and pronouns.
As notedin (Passonneau and Litman, 1993), the NP featureslargely reflect Passonneau's hypotheses that adja-cent utterances are more likely to contain expres-sions that corefer, or that are inferentially linked,if they occur within the same segment; and that adefinite pronoun is more likely than a full NP to re-fer to an entity that was mentioned in the currentsegment, if not in the previous utterance.3.3 Eva luat ionThe segmentation algorithms presented in the nexttwo sections were developed by examining only atraining set of narratives.
The algorithms are thenevaluated by examining their performance in pre-dicting segmentation a separate test set.
We cur-rently use 10 narratives for training and 5 narrativesfor testing.
(The remaining 5 narratives are reser-ved for future research.)
The 10 training narrativesTraininl~ Set .63 .72 .06 .12Test Set .64 .68 .07 .11Table 1: Average human performance.range in length from 51 to 162 phrases (Avg.=101.4),or from 38 to 121 clauses (Avg.=76.8).
The 5 testnarratives range in length from 47 to 113 phrases(Avg.=S7.4), or from 37 to 101 clauses (Avg.=69.0).The ratios of test to training data measured in narra-tives, prosodic phrases and clauses, respectively, are50.0%, 43.1% and 44.9%.
For the machine learningalgorithm we also estimate performance using cross-validation (Weiss and Kulikowski, 1991), as detailedin Section 5.To quantify algorithm performance, we use the in-formation retrieval metrics shown in Fig.
4.
Recallis the ratio of correctly hypothesized boundaries totarget boundaries.
Precision is the ratio of hypo-thesized boundaries that are correct o the total hy-pothesized boundaries.
(Cf.
Fig.
4 for fallout anderror.)
Ideal behavior would be to identify all andonly the target boundaries: the values for b and cin Fig.
4 would thus both equal O, representing noerrors.
The ideal values for recall, precision, fallout,and error are 1, 1, 0, and 0, while the worst valuesare 0, 0, 1, and 1.
To get an intuitive summary ofoverall performance, we also sum the deviation ofthe observed value from the ideal value for each me-tric: (1-recall) + (1-precision) + fallout + error.
Thesummed eviation for perfect performance is thus 0.Finally, to interpret our quantitative results, weuse the performance of our human subjects as a tar-get goal for the performance ofour algorithms (Galeet al, 1992).
Table 1 shows the average human per-formance for both the training and test sets of nar-ratives.
Note that human performance is basicallythe same for both sets of narratives.
However, twoSubjectsAlgorithm Boundary INon-DoundaryBoundary a bNon-Boundary c dRecall  =Precis ion =Fallout ---- bError ---- ~Figure 4: Information retrieval metrics.111factors prevent his performance from being closerto ideal (e.g., recall and precision of 1).
The first isthe wide variation in the number of boundaries thatsubjects used, as discussed above.
The second is theinherently fuzzy nature of boundary location.
Wediscuss this second issue at length in (Passonneanand Litman, to appear), and present relaxed IR me-trics that penalize near misses less heavily in (Lit-man and Passonneau, 1995).4 Hand Tun ingTo improve performance, we analyzed the two typesof IR errors made by the original NP algorithm (Pas-sonneau and Litman, 1993) on the training data.Type "b" errors (cf.
Fig.
4), mis-classification fnon-boundaries, were reduced by changing the co-ding features pertaining to clauses and NPs.
Most"b" errors correlated with two conditions used in theNP algorithm, identification of clauses and of infe-rential inks.
The revision led to fewer clauses (moreassignments of 'NA' for the three NP features) andmore inference relations.
One example of a changeto clause coding is that formulaic utterances havingthe structure of clauses, but which function like in-terjections, are no longer recognized as independentclauses.
These include the phrases let's see, let mesee, I don't know, you know when they occur with noverb phrase argument.
Other changes pertained tosentence fragments, unexpected clausal arguments,and embedded speech.Three types of inference relations linking succes-sive clauses (Ci-1, Ci) were added (originally therewere 5 types (Passonneau, 1994)).
Now, a pronoun(e.g., it, that, this) in Ci referring to an action, eventor fact inferrable from Ci-1 links the two clauses.
Sodoes an implicit argument, as in Fig.
5, where themissing argument of notice is inferred to be the eventof the pears falling.
The third case is where an NPin Ci is described as part of an event that resultsdirectly from an event mentioned in Ci-1.
"C" type errors (cf.
Fig.
4), mis-classificationof boundaries, often occurred where prosodic andcue features conflicted with NP features.
The origi-nal NP algorithm assigned boundaries wherever thethree values '-coref', '-infer', '-global.pro' (defined insection 3) co-occurred, represented as the first con-ditional statement of Fig.
6.
Experiments led to thehypothesis that the most improvement came by as-signing a boundary if the cue-prosody feature hadthe value 'complex', even if the algorithm would nototherwise assign a boundary, as shown in Fig.
6.CI.
Phr .6 3.0178 3.02\[1.1 \[.7\] A-nd\]  he's not  really., doesn ' t  seemto be pay ing  all that  much at tent ion\[.557 because  \[.45\]\] you know the pears fal l i ,and.
.
he doesn ' t  real ly not ice (O i ) ,Figure 5: Inferential link due to implicit argument.i f  (coref  = -coref  and  infer = - infer and  g loba l .p ro  = -g lobal .pro)then  boundarye lse | f  cue-prosody  ---- complex then  boundarye lse  non-boundaryFigure 6: Condition 2 algorithm.We refer to the original NP algorithm applied tothe initial coding as Condition 1, and the tuned al-gorithm applied to the enriched coding as Condition2.
Table 2 presents the average IR scores acrossthe narratives in the training set for both conditi-ons.
Reduction of "b" type errors raises precision,and lowers fallout and error rate.
Reduction of "c"type errors raises recall, and lowers fallout and errorrate.
All scores improve in Condition 2, with pre-cision and fallout showing the greatest relative im-provement.
The major difference from human per-formance is relatively poorer precision.The standard eviations in Table 2 are often closeto 1/4 or 1/3 of the reported averages.
This indicatesa large amount of variability in the data, reflectingwide differences across narratives (speakers) in thetraining set with respect o the distinctions recogni-zed by the algorithm.
Although the high standarddeviations show that the tuned algorithm is not wellfitted to each narrative, it is likely that it is overspe-cialized to the training sample in the sense that testnarratives are likely to exhibit further variation.Table 3 shows the results of the hand tuned al-gorithm on the 5 randomly selected test narrativeson both Conditions 1 and 2.
Condition 1 results,the untuned algorithm with the initial feature set,are very similar to the training set except for worseprecision.
Thus, despite the high standard evia-tions, 10 narratives eems to have been a sufficientsample size for evaluating the initial NP algorithm.Condition 2 results are better than condition 1 inTable 3, and condition 1 in Table 2.
This is strongevidence that the tuned algorithm is a better pre-dictor of segment boundaries than the original NPalgorithm.
Nevertheless, the test results of condition2 are much worse than the corresponding training re-sults, particularly for precision (.44 versus .62).
ThisAveralse Recal l  P rec  Fall E r ror  SumDevCond i t ion  1 .42 .40 .14 .22 1.54Std.
Dev.
.17 .12 .06 .07 .34Cond i t ion  2 .58 .62 .08 .14 1.02Std.
Dev.
.14 .10 .04 .05 .18Table 2: Performance on training set.Average  Recal l  P rec  Fall E r ror  SumDevCond i t ion  1 .44 .29 .16 .21 1.64Std.
Dev.
.18 .17 .07 .05 .32Cond i t ion  2 .50 .44 .11 .17 1.34Std.
Dev.
.21 .06 .03 .04 .29Table 3: Performance on test set.112confirms that the tuned algorithm is over calibratedto the training set.5 Mach ine  Learn ingWe use the machine learning program C4.5 (Quin-lan, 1993) to automatically develop segmentation al-gorithms from our corpus of coded narratives, whereeach potential boundary site has been classified andrepresented as a set of linguistic features.
The firstinput to C4.5 specifies the names of the classes tobe learned (boundary and non-boundary), and thenames and potential values of a fixed set of codingfeatures (Fig.
2).
The second input is the trainingdata, i.e., a set of examples for which the class andfeature values (as in Fig.
3) are specified.
Our trai-ning set of 10 narratives provides 1004 examples ofpotential boundary sites.
The output of C4.5 is aclassification algorithm expressed as a decision tree,which predicts the class of a potential boundary gi-ven its set of feature values.Because machine learning makes it convenient toinduce decision trees under a wide variety of con-ditions, we have performed numerous experiments,varying the number of features used to code the trai-ning data, the definitions used for classifying a po-tential boundary site as boundary or non-boundary 5and the options available for running the C4.5 pro-gram.
Fig.
7 shows one of the highest-performinglearned decision trees from our experiments.
Thisdecision tree was learned under the following condi-tions: all of the features hown in Fig.
2 were used tocode the training data, boundaries were classified asdiscussed in section 3, and C4.5 was run using onlythe default options.
The decision tree predicts theclass of a potential boundary site based on the featu-res before, after, duration, cuel, wordl, corer, infer,and global.pro.
Note that although not all availablefeatures are used in the tree, the included featuresrepresent 3of the 4 general types of knowledge (pros-ody, cue phrases and noun phrases).
Each level ofthe tree specifies a test on a single feature, with abranch for every possible outcome of the test.
6 Abranch can either lead to the assignment of a class,or to another test.
For example, the tree initiallybranches based on the value of the feature before.If the value is '-sentence.final.contour' then the firstbranch is taken and the potential boundary site is as-signed the class non-boundary.
If the value of beforeis 'q-sentence.final.contour' then the second branchis taken and the feature corer is tested.The performance of this learned decision tree ave-raged over the 10 training narratives is shown inTable 4, on the line labeled "Learning 1".
The linelabeled "Learning 2" shows the results from another5(Litman and Passonneau, 1995) varies the numberof subjects used to determine boundaries.eThe actual tree branches on every value of worda;the figure merges these branches for clarity.i f  before = -sentence.f inal .contour then  non.boundarye lae i f  before = +sentence.f inal .contour theni f core f  = NA then  non-boundarye lse i f  coref = +corer  theni f  after  ----.
+sentence.f inal .contour theni f  durat ion <__ 1.3 then  non-boundarye lse l f  durat ion > 1.3 then  boundarye lse i f  after  = -sentence.f inal .contour theni f  word 1 E {also,basically, because,finally, first,like,meanwhile,no,oh,okay, only, aee,so,well ,where,NA}then  non-boundarye lse | f  word 1 E {anyway, but ,now,or , then} then  boundarye lse | f  word I = and theni f  durat ion < 0.6 then  non-boundarye lse i fdurat~on > 0.6 then  boundarye lse i f  coref = -corer theni f  infer = +infer then  non-boundarye lse l f  infer = NA then  boundarye lse i f in fe r  = -infer theni f  after = -sentence.f inal .contour then  boundarye lse l f  after  = +sentence.f inal .contour theni f  cue 1 = true theni f  global.pro = NA then  boundarye lse i f  global.pro = -global.pro then  boundarye lse l f  global.pro = +global .pro theni f  durat ion < 0.65 then  non-boundaryelsei fdurat~'on > 0.65 then  boundarye lse i fcue  I = false theni f  durat ion > 0.5 then  non.boundarye lse l fdurat ion  <: 0.5 theni f  durat ion < 0.35 then  non-boundarye ise i fdurat~on > 0.35 then  boundaryFigure 7: Learned decision tree for segmentation.machine learning experiment, in which one of thedefault C4.5 options used in "Learning 1" is over-ridden.
The "Learning 2" tree (not shown due tospace restrictions) is more complex than the tree ofFig.
7, but has slightly better performance.
Notethat "Learning 1" performance is comparable to hu-man performance (Table 1), while "Learning 2" isslightly better than humans.
The results obtainedvia machine learning are also somewhat better thanthe results obtained using hand tuning--particularlywith respect o precision ("Condition 2" in Table 2),and are a great improvement over the original NPresults ("Condition 1" in Table 2).The performance of the learned decision trees ave-raged over the 5 test narratives is shown in Table 5.Comparison of Tables 4 and 5 shows that, as with thehand tuning results (and as expected), average per-formance is worse when applied to the testing ratherthan the training data particularly with respect toprecision.
However, performance is an improvementover our previous best results ("Condition 1" in Ta-ble 3), and is comparable to ("Learning 1") or veryslightly better than ("Learning 2") the hand tuningresults ("Condition 2" in Table 3).We also use the resampling method of cross-validation (Weiss and Kulikowski, 1991) to estimateperformance, which averages results over multiplepartitions of a sample into test versus training data.We performed 10 runs of the learning program, eachusing 9 of the 10 training narratives for that run's113Average  Recal l  P rec  Fall E r ror  SumDevLearn ing  1 .54 .76 .04 .11 .85Std.
Dev.
.18 .12 .02 .04 .28Learn ing  2 .59 .78 .03 .10 .76"Std.
Dev.
.22 .12 .02 .04 .29Table 4: Performance on training set.Average  Recal l  Prec  Fall E r ror  SumDevLearn ing  1 .43 .48 .08 .16 1.34Std.
Dev.
.21 .13 .03 .05 .36Learn ing  2 .47 .50 .09 .16 1.27Std.
Dev.
.18 .16 .04 .07 .42Table 5: Performance on test set.Average  Recal l  P rec  Fall E r ror  SumDevLearn ing  1 .43 .63 .05 .15 1.14'Std.
Dev, .19 .16 .03 .03 .24Learn ing  2 .46 .61 .07 .15 1.15Std.
Dev.
.20 .14 .04 .03 .21Table 6: Using 10-fold cross-validation.training set (for learning the tree) and the remainingnarrative for testing.
Note that for each iterationof the cross-validation, the learning process beginsfrom scratch and thus each training and testing setare still disjoint.
While this method does not makesense for humans, computers can truly ignore pre-vious iterations.
For sample sizes in the hundreds(our 10 narratives provide 1004 examples) 1O-foldcross-validation ften provides a better performanceestimate than the hold-out method (Weiss and Ku-likowski, 1991).
Results using cross-validation areshown in Table 6, and are better than the estimatesobtained using the hold-out method (Table 5), withthe major improvement coming from precision.
Bec-ause a different ree is learned on each iteration, thecross-validation evaluates the learning method, nota particular decision tree.6 Conc lus ionWe have presented two methods for developing seg-mentation hypotheses using multiple linguistic fea-tures.
The first method hand tunes features andalgorithms based on analysis of training errors.
Thesecond method, machine learning, automatically in-duces decision trees from coded corpora.
Both me-thods rely on an enriched set of input features com-pared to our previous work.
With each method, wehave achieved marked improvements in performancecompared to our previous work and are approachinghuman performance.
Note that quantitatively, themachine learning results are slightly better than thehand tuning results.
The main difference on averageperformance is the higher precision of the automatedalgorithm.
Furthermore, note that the machine lear-ning algorithm used the changes to the coding fea-tures that resulted from the tuning methods.
Thissuggests that hand tuning is a useful method forunderstanding how to best code the data, while ms-chine learning provides an effective (and automatic)way to produce an algorithm given a good featurerepresentation.Our results lend further support o the hypothesisthat linguistic devices correlate with discourse struc-ture (cf.
section 2.1), which itself has practical im-port.
Understanding systems could infer segmentsas a step towards producing summaries, while ge-neration systems could signal segments to increasecomprehensibility/Our results also suggest hat tobest identify or convey segment boundaries, ystemswill need to exploit multiple signals simultaneously.We plan to continue our experiments by furthermerging the automated and analytic techniques, andevaluating new algorithms on our final test corpus.Because we have already used cross-validation, wedo not anticipate significant degradation on new testnarratives.
An important area for future researchis to develop principled methods for identifying di-stinct speaker strategies pertaining to how they si-gnal segments.
Performance of individual speakersvaries widely as shown by the high standard eviati-ons in our tables.
The original NP, hand tuned, andmachine learning algorithms all do relatively poorlyon narrative 16 and relatively well on 11 (both inthe test set) under all conditions.
This lends sup-port to the hypothesis that there may be consistentdifferences among speakers regarding strategies forsignaling shifts in global discourse structure.Re ferencesLeo Breiman, Jerome Friedman, Richard Oishen,and C. Stone.
1984.
Classification and RegressionTrees.
Wadsworth and Brooks, Monterey, CA.Wallace L. Chafe.
1980.
The Pear Stories.
AblexPublishing Corporation, Norwood, NJ.William Gale, Ken W. Church, and David Yarow-sky.
1992.
Estimating upper and lower boundson the performance of word-sense disambiguationprograms.
In Proc.
of the 30th ACL, pages 249-256.Barbara Grosz and Julia Hirschberg.
1992.
Someintonational characteristics ofdiscourse structure.In Proc.
of the International Conference on Spo-ken Language Processing.Barbara Grosz and Candace Sidner.
1986.
Atten-tion, intentions and the structure of discourse.Computational Linguistics, 12:175-204.Barbara J. Grosz, Aaravind K. Joshi, and ScottWeinstein.
1983.
Providing a unified account ofdefinite noun phrases in discourse.
In Proc.
of the21st ACL, pages 44-50.rCf.
(Hirschberg a~d Pierrehumbert, 1986) who arguethat comprehensibility improves if units are prosodicallysignaled.114Marti A. Hearst.
1994.
Multi-paragraph segmenta-tion of expository text.
In Proc, of the 32nd A CL.Julia Hirschberg and Barbara Grosz.
1992.
Intona-tional features of local and global discourse struc-ture.
In Proc.
of the Darpa Workshop on SpokenLanguage.Julia Hirschberg and Diane Litman.
1993.
Empiri-cal studies on the disambiguation f cue phrases.Computational Linguistics, 19(3):501-530.Julia Hirschberg and Janet Pierrehumbert.
1986.The intonational structuring of discourse.
In Proc.of the 24th A CL.Jerry R. Hobbs.
1979.
Coherence and coreference.Cognitive Science, 3(1):67-90.Amy Isard and Jean Carletta.
1995.
Replicabi-lity of transaction and action coding in the maptask corpus.
In AAA1 1995 Spring SymposiumSeries: Empirical Methods in Discourse Interpre-tation and Generation, pages 60-66.Megumi Kameyama.
1986.
A property-sharingconstraint in centering.
In Proc.
of the 24th ACL,pages 200-206.H.
Kozima.
1993.
Text segmentation based on si-milarity between words.
In Proc.
of the 31st ACL(Student Session), pages 286-288.Alex Lascarides and Jon Oberlander.
1992.
Tempo-ral coherence and defeasible knowledge.
Theoreti-cal Linguistics.Charlotte Linde.
1979.
Focus of attention and thechoice of pronouns in discourse.
In Talmy Givon,editor, Syntax and Semantics: Discourse and Syn-tax, pages 337-354.
Academic Press, New York.Diane J. Litman and Rebecca J. Passonneau.
1995.Developing algorithms for discourse segmentation.In AAAI 1995 Spring Symposium Series: Empiri.cal Methods in Discourse Interpretation and Ge-neration, pages 85-91.Diane J. Litman.
1994.
Classifying cue phrases intext and speech using machine learning.
In Proc.of the 12th AAA1, pages 806-813.William C. Mann and Sandra Thompson.
1988.Rhetorical structure theory.
TEXT, pages 243-281.Johanna D. Moore and Cecile Paris.
1993.
Planningtext for advisory dialogues: Capturing intentionaland rhetorical information.
Computational Lin-guistics, 19:652-694.Johanna D. Moore and Martha E. Pollack.
1992.A problem for RST: The need for multi-leveldiscourse analysis.
Computational Linguistics,18:537-544.Jane Morris and Graeme ttirst.
1991.
Lexical co-hesion computed by thesaural relations as an in-dicator of the structure of text.
ComputationalLinguistics, 17:21-48.Megan Moser and Julia D. Moore.
1995.
Using dis-course analysis and automatic text generation tostudy discourse cue usage.
In AAAI 1995 SpringSymposium Series: Empirical Methods in Dis-course Interpretation and Generation, pages 92-98.Christine H. Nakatani, Julia Hirsehberg, and Bar-bara J. Grosz.
1995.
Discourse structure in spo-ken language: Studies on speech corpora.
InAAAI 1995 Spring Symposium Series: EmpiricalMethods in Discourse Interpretation and Genera-tion, pages 106-112.Rebecca J. Passonneau and Diane J. Litman.
1993.Intention-based segmentation: Human reliabilityand correlation with linguistic cues.
In Proc.
ofthe 31st ACL, pages 148-155.Rebecca J. Passonneau and D. Litman.
to appear.Empirical analysis of three dimensions of spokendiscourse.
In E. Hovy and D. Scott, editors, In-terdisciplinary Perspectives on Discourse.
Sprin-ger Verlag, Berlin.Rebecca J. Passonneau.
1994.
Protocol for codingdiscourse referential noun phrases and their ante-cedents.
Technical report, Columbia University.Rebecca J. Passonneau.
to appear.
Interaction ofthe segmental structure of discourse with explicit-ness of discourse anaphora.
In E. Prince, A. Joshi,and M. Walker, editors, Proc.
of the Workshopon Centering Theory in Naturally Occurring Dis-course.
Oxford University Press.Livya Polanyi.
1988.
A formal model of discoursestructure.
Journal of Pragmaties, pages 601-638.John K. Quinlan.
1993.
C4.5 : Programs for Ma-chine Learning.
Morgan Kaufmann Publishers,San Mates, Calif.Rachel Reichman.
1985.
Getting Computers to TalkLike You and Me: Discourse Contezt, Focus, andSemantics.
Bradford.
MIT, Cambridge.J.
C. Reynar.
1994.
An automatic method of fin-ding topic boundaries.
In Proc.
of the 3$nd ACL(Student Session), pages 331-333.Lisa J. Stifleman.
1995.
A discourse analysisapproach to structured speech.
In AAAI 1995Spring Symposium Series: Empirical Methods inDiscourse Interpretation and Generation, pages162-167.Bonnie L. Webber.
1991.
Structure and ostensionin the interpretation f discourse deixis.
Languageand Cognitive Processes, pages 107-135.Sholom M. Weiss and Casimir Kulikowski.
1991.Computer systems that learn: classification andprediction methods from statistics, neural nets,machine learning, and expert s~/stems.
MorganKaufmann.115
