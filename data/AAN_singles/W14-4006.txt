Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 51?56,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsReducing the Impact of Data Sparsity in Statistical Machine TranslationKaran Singla1, Kunal Sachdeva1, Diksha Yadav1, Srinivas Bangalore2, Dipti Misra Sharma11LTRC IIIT Hyderabad,2AT&T Labs-ResearchAbstractMorphologically rich languages generallyrequire large amounts of parallel data toadequately estimate parameters in a statis-tical Machine Translation(SMT) system.However, it is time consuming and expen-sive to create large collections of paralleldata.
In this paper, we explore two strate-gies for circumventing sparsity caused bylack of large parallel corpora.
First, we ex-plore the use of distributed representationsin an Recurrent Neural Network based lan-guage model with different morphologicalfeatures and second, we explore the use oflexical resources such as WordNet to over-come sparsity of content words.1 IntroductionStatistical machine translation (SMT) models es-timate parameters (lexical models, and distortionmodel) from parallel corpora.
The reliability ofthese parameter estimates is dependent on the sizeof the corpora.
In morphologically rich languages,this sparsity is compounded further due to lack oflarge parallel corpora.In this paper, we present two approaches thataddress the issue of sparsity in SMT models formorphologically rich languages.
First, we use anRecurrent Neural Network (RNN) based languagemodel (LM) to re-rank the output of a phrase-based SMT (PB-SMT) system and second we uselexical resources such as WordNet to minimize theimpact of Out-of-Vocabulary(OOV) words on MTquality.
We further improve the accuracy of MTusing a model combination approach.The rest of the paper is organized as follows.We first present our approach of training the base-line model and source side reordering.
In Section4, we present our experiments and results on re-ranking the MT output using RNNLM.
In Section5, we discuss our approach to increase the cover-age of the model by using synset ID?s from theEnglish WordNet (EWN).
Section 6 describes ourexperiments on combining the model with synsetID?s and baseline model to further improve thetranslation accuracy followed by results and obser-vations sections.We conclude the paper with futurework and conclusions.2 Related WorkIn this paper, we present our efforts of re-ranking the n-best hypotheses produced by a PB-MT (Phrase-Based MT) system using RNNLM(Mikolov et al., 2010) in the context of an English-Hindi SMT system.
The re-ranking task in ma-chine translation can be defined as re-scoring then-best list of translations, wherein a number oflanguage models are deployed along with fea-tures of source or target language.
(Dungarwalet al., 2014) described the benefits of re-rankingthe translation hypothesis using simple n-grambased language model.
In recent years, the useof RNNLM have shown significant improvementsover the traditional n-gram models (Sundermeyeret al., 2013).
(Mikolov et al., 2010) and (Liu etal., 2014) have shown significant improvements inspeech recognition accuracy using RNNLM .
Shi(2012) also showed the benefits of using RNNLMwith contextual and linguistic features.
We havealso explored the use of morphological features(Hindi being a morphologically rich language) inRNNLM and deduced that these features furtherimprove the baseline RNNLM in re-ranking the n-best hypothesis.Words in natural languages are richly diverseso it is not possible to cover all source languagewords when training an MT system.
Untranslatedout-of-vocabulary (OOV) words tend to degradethe accuracy of the output produced by an MTmodel.
Huang (2010) pointed to various typesof OOV words which occur in a data set ?
seg-51mentation error in source language, named enti-ties, combination forms (e.g.
widebody) and ab-breviations.
Apart from these issues, Hindi beinga low-resourced language in terms of parallel cor-pora suffers from data sparsity.In the second part of the paper, we address theproblem of data sparsity with the help of EnglishWordNet (EWN) for English-Hindi PB-SMT.
Weincrease the coverage of content words (exclud-ing Named-Entities) by incorporating sysnset in-formation in the source sentences.Combining Machine Translation (MT) systemshas become an important part of statistical MT inpast few years.
Works by (Razmara and Sarkar,2013; Cohn and Lapata, 2007) have shown thatthere is an increase in phrase coverage when com-bining different systems.
To get more coverage ofunigrams in phrase-table, we have explored sys-tem combination approaches to combine modelstrained with synset information and without synsetinformation.
We have explored two methodolo-gies for system combination based on confusionmatrix(dynamic) (Ghannay et al., 2014) and mix-ing models (Cohn and Lapata, 2007).3 Baseline Components3.1 Baseline Model and Corpus StatisticsWe have used the ILCI corpora (Choudhary andJha, 2011) for our experiments, which containsEnglish-Hindi parallel sentences from tourism andhealth domain.
We randomly divided the data intotraining (48970), development (500) and testing(500) sentences and for language modelling weused news corpus of English which is distributedas a part of WMT?14 translation task.
The data isabout 3 million sentences which also contains MTtraining data.We trained a phrase based (Koehn et al., 2003)MT system using the Moses toolkit with word-alignments extracted from GIZA++ (Och and Ney,2000).
We have used the SRILM (Stolcke andothers, 2002) with Kneser-Ney smoothing (Kneserand Ney, 1995) for training a language model forthe first stage of decoding.
The result of this base-line system is shown in Table 1.3.2 English Transformation ModuleHindi is a relatively free-word order language andgenerally tends to follow SOV (Subject-Object-Verb) order and English tends to follow SVO(Subject-Verb-Object) word order.
Research hasNumber of Number of Number ofTraining Development Evaluation BLEUSentences Sentences Sentences48970 500 500 20.04Table 1: Baseline Scores for Phrase-based MosesModelshown that pre-ordering source language to con-form to target language word order significantlyimproves translation quality (Collins et al., 2005).We created a re-ordering module for transform-ing an English sentence to be in the Hindi orderbased on reordering rules provided by Anusaaraka(Chaudhury et al., 2010).
The reordering rules arebased on parse output produced by the StanfordParser (Klein and Manning, 2003).The transformation module requires the text tocontain only surface form of words, however, weextended it to support surface form along with itsfactors such as lemma and Part of Speech (POS).Input : the girl in blue shirt is my sisterOutput : in blue shirt the girl is my sister.Hindi : neele shirt waali ladki meri bahen hai (blue) ( shirt) (Mod)(girl)(my)(sister)(Vaux)With this transformation, the English sentenceis structurally closer to the Hindi sentence whichleads to better phrase alignments.
The modeltrained with the transformed corpus produces anew baseline score of 21.84 BLEU score animprovement over the earlier baseline of 20.04BLEU points.4 Re-Ranking ExperimentsIn this section, we describe the results of re-ranking the output of the translation model us-ing Recurrent Neural Networks (RNN) based lan-guage models using the same data which is usedfor language modelling in the baseline models.Unlike traditional n-gram based discrete lan-guage models, RNN do not make the Markov as-sumption and potentially can take into accountlong-term dependencies between words.
Since thewords in RNNs are represented as continuous val-ued vectors in low dimensions allowing for thepossibility of smoothing using syntactic and se-mantic features.
In practice, however, learninglong-term dependencies with gradient descent isdifficult as described by (Bengio et al., 1994) dueto diminishing gradients.We have integrated the approach of re-scoring52100 200 300 400 5002224262830Number of HypothesesBLEUscoresBaselinePOSNONELemmaOracleAllFigure 1: BLEU Scores for Re-ranking experi-ments with RNNLM using different feature com-binations.n-best output using RNNLM which has also beenshown to be helpful by (Liu et al., 2014).
Shi(2012) also showed the benefits of using RNNLMwith contextual and linguistic features.
Follow-ing their work, we used three type of features forbuilding an RNNLM for Hindi : lemma (root),POS, NC (number-case).
The data used was aWikipedia dump, MT training data, news arti-cles which had approximately 500,000 Hindi sen-tences.
Features were extracted using paradigm-based Hindi Morphological Analyzer1Figure 1 illustrates the results of re-ranking per-formed using RNNLM trained with various fea-tures.
The Oracle score is the highest achievablescore in a re-ranking experiment.
This score iscomputed based on the best translation out of n-best translations.
The best translation is found us-ing the cosine similarity between the hypothesisand the reference translation.
It can be seen fromFigure 1, that the LM with only word and POS in-formation is inferior to all other models.
However,morphological features like lemma, number andcase information help in re-ranking the hypothesissignificantly.
The RNNLM which uses all the fea-tures performed the best for the re-ranking exper-iments achieving a BLEU score of 26.91, after re-scoring 500-best obtained from the pre-order SMTmodel.1We have used the HCU morph-analyzer.System BLEUBaseline 21.84Rescoring 500-best with RNNLMFeaturesNONE 25.77POS 24.36Lemma(root) 26.32ALL(POS+Lemma+NC) 26.91Table 2: Rescoring results of 500-best hypothesesusing RNNLM with different features5 Using WordNet to Reduce DataSparsityWe extend the coverage of our source data by us-ing synonyms from the English WordNet (EWN).Our main motivation is to reduce the impact ofOOV words on output quality by replacing wordsin a source sentence with their correspondingsynset IDs.
However, choosing the appropriatesynset ID based upon its context and morphologi-cal information is important.
For sense selection,we followed the approach used by (Tammewar etal., 2013), which is also described further in thissection in the context of our task.
We ignoredwords that are regarded as Named-Entities as in-dicated by Stanford NER tagger, as they shouldnot have synonyms in any case.5.1 Sense SelectionWords are ambiguous, independent of their sen-tence context.
To choose an appropriate sense ac-cording to the context for a lexical item is a chal-lenging task typically termed as word-sense dis-ambiguation.
However, the syntactic category ofa lexical item provides an initial cue for disam-biguating a lexical item.
Among the varied senses,we filter out the senses that are not the same POStag as the lexical item.
But words are not just am-biguous across different syntactic categories butare also ambiguous within a syntactic category.
Inthe following, we discuss our approaches to selectthe sense of a lexical item best suited in a givencontext within a given category.
Also categorieswere filtered so that only content words get re-placed with synset IDs.5.1.1 Intra-Category Sense SelectionFirst Sense: Among the different senses,we se-lect the first sense listed in EWN corresponding tothe POS-tag of a given lexical item.
The choice ismotivated by our observation that the senses of a53lexical item are ordered in the descending order oftheir frequencies of usage in the lexical resource.Merged Sense: In this approach, we merge allthe senses listed in EWN corresponding to thePOS-tag of the given lexical item.
The motivationbehind this strategy is that the senses in the EWNfor a particular word-POS pair are too finely clas-sified resulting in classification of words that mayrepresent the same concept, are classified into dif-ferent synsets.
For example : travel and go canmean the same concept in a similar context but thefirst sense given by EWN is different for these twowords.
Therefore, we merge all the senses for aword into a super sense ( synset ID of first wordoccurred in data), which is given to all its syn-onyms even if it occurs in different synset IDs.5.2 Factored ModelTechniques such as factored modelling (Koehnand Hoang, 2007) are quite beneficial for Trans-lation from English to Hindi language as shownby (Ramanathan et al., 2008).
When we replacewords in a source sentence with the synset ID?as,we tend to lose morphological information associ-ated with that word.
We add inflections as featuresin a factored SMT model to minimize the impactof this replacement.We show the results of the processing steps onan example sentence below.Original Sentence : Ram is going to market tobuy applesNew Sentence : Ram is Synset(go.v.1)to Synset(market.n.0) to Synset(buy.v.1)Synset(apple.n.1)Sentence with synset ID: Ram E is ESynset(go.v.1) ing to E Synset(market.n.0) Eto E Synset(buy.v.1) E Synset(apple.n.1) sThen English sentences were reordered to Hindiword-order using the module discussed in Section3.Reordered Sentence: Ram E Synset(apple.n.1) sSynset(buy.v.1) E to E Synset(market.n.0) E to ESynset(go.v.1) ing is EIn Table 3, the second row shows the BLEUscores for the models in which there are synset IDsfor the source side.
It can be seen that the factoredmodel also shows significant improvement in theresults.6 Combining MT ModelsCombining Machine translation (MT) systems hasbecome an important part of Statistical MT inthe past few years.
There are two dominant ap-proaches.
(1) a system combination approachbased on confusion networks (CN) (Rosti et al.,2007), which can work dynamically in combin-ing the systems.
(2) Combine the models by lin-early interpolating and then using MERT to tunethe combined system.6.1 Combination based on confusionnetworksWe used the tool MANY (Barrault, 2010) for sys-tem combination.
However, since the tool is con-figured to work with TERp evaluation metric, wemodified it to use METEOR (Gupta et al., 2010)metric since it has been shown by (Kalyani et al.,2014), that METEOR evaluation metric is bettercorrelated to human evaluation for morphologi-cally rich Indian Languages.6.2 Linearly Interpolated CombinationIn this approach, we combined phrase-tables ofthe two models (Eng (sysnset) - Hindi and Base-line) using linear interpolation.
We combined thetwo models with uniform weights ?
0.5 for eachmodel, in our case.
We again tuned this modelwith the new interpolated phrase-table using stan-dard algorithm MERT.7 Experiments and ResultsAs can be seen in Table 3, the model with synsetinformation led to reduction in OOV words.
Eventhough BLEU score decreased, but METEORscore improved for all the experiments based onusing synset IDs in the source sentence, but it hasbeen shown by (Gupta et al., 2010) that METEORis a better evaluation metrics for morphologicallyrich languages.
Also, when synset ID?as are usedinstead of words in the source language, the sys-tem makes incorrect morphological choices.
Ex-ample : going and goes will be replaced by samesynset ID ?aSynset(go.v.1)?a, so this has lead to lossof information in the phrase-table but METEORcatches these complexities as it considers featureslike stems, synonyms for its evaluation metricsand hence showed better improvements comparedto BLEU metric.
Last two rows of Table 3 showresults for combination experiments and MixtureModel (linearly interpolated model) showed best54System #OOV words BLEU MeteorBaseline 253 21.8 .492Eng(Synset ID)-HindiBaseline 237 19.2 .494*factor(inflections) 225 20.3 .506Ensembled Decoding 213 21.0 .511Mixture Model 210 21.2 .519Table 3: Results for the model in which there were Synset ID?s instead of word in English dataresults with significant reduction in OOV wordsand also some gains in METEOR score.8 ObservationsIn this section, we study the coverage of differentmodels by categorizing the OOV words into 5 cat-egories.?
NE(Named Entities) : As the data wasfrom Health & Tourism domain, these wordswere mainly the names of the places andmedicines.?
VB : types of verb forms?
NN : types of nouns and pronouns?
ADJ : all adjectives?
AD : adverbs?
OTH : there were some words which did notmean anything in English?
SM : There were some occasional spellingmistakes seen in the test data.Note : There were no function words seen in theOOV(un-translated) wordsCat.
Baseline Eng(synset)-Hin MixtureModelNE 120 121 115VB 47 37 27NN 76 60 47ADJ 22 15 12AD 5 5 4OTH 2 2 2SM 8 8 8Table 4: OOV words in Different ModelsAs this analysis was done on a small dataset andfor a fixed domain, the OOV words were few innumber as it can be seen in Table 4.
But the OOVwords across the different models reduced as ex-pected.
The NE words remained almost the samefor all the three models but OOV words from cate-gory VB,NN,ADJ decreased for Eng(synset)-Hinmodel and Mixture model significantly.9 Future WorkIn the future, we will work on using the two ap-proaches discussed: Re-Ranking & using lexicalresources to reduce sparsity together in a system.We will work on exploring syntax based featuresfor RNNLM and we are planning to use a bettermethod for sense selection and extending this con-cept for more language pairs.
Word-sense disam-biguation can be used for choosing more appro-priate sense when the translation model is trainedon a bigger data data set.
Also we are looking forunsupervised techniques to learn the replacementsfor words to reduce sparsity and ways to adapt oursystem to different domains.10 ConclusionsIn this paper, we have discussed two approachesto address sparsity issues encountered in trainingSMT models for morphologically rich languageswith limited amounts of parallel corpora.
In thefirst approach we used an RNNLM enriched withmorphological features of the target words andshow the BLEU score to improve by 5 points.
Inthe second approach we use lexical resource suchas WordNet to alleviate sparsity.ReferencesLo?
?c Barrault.
2010.
Many: Open source machinetranslation system combination.
The Prague Bul-letin of Mathematical Linguistics, 93:147?155.Yoshua Bengio, Patrice Simard, and Paolo Frasconi.1994.
Learning long-term dependencies with gra-dient descent is difficult.
Neural Networks, IEEETransactions on, 5(2):157?166.Sriram Chaudhury, Ankitha Rao, and Dipti M Sharma.2010.
Anusaaraka: An expert system based machinetranslation system.
In Natural Language Processing55and Knowledge Engineering (NLP-KE), 2010 Inter-national Conference on, pages 1?6.
IEEE.Narayan Choudhary and Girish Nath Jha.
2011.
Cre-ating multilingual parallel corpora in indian lan-guages.
In Proceedings of Language and Technol-ogy Conference.Trevor Cohn and Mirella Lapata.
2007.
Ma-chine translation by triangulation: Making ef-fective use of multi-parallel corpora.
In AN-NUAL MEETING-ASSOCIATION FOR COMPU-TATIONAL LINGUISTICS, volume 45, page 728.Citeseer.Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd annualmeeting on association for computational linguis-tics, pages 531?540.
Association for ComputationalLinguistics.Piyush Dungarwal, Rajen Chatterjee, Abhijit Mishra,Anoop Kunchukuttan, Ritesh Shah, and PushpakBhattacharyya.
2014.
The iit bombay hindi-englishtranslation system at wmt 2014.
In Proceedings ofthe Ninth Workshop on Statistical Machine Transla-tion, pages 90?96, Baltimore, Maryland, USA, June.Association for Computational Linguistics.Sahar Ghannay, France Le Mans, and Lo?c Barrault.2014.
Using hypothesis selection based features forconfusion network mt system combination.
In Pro-ceedings of the 3rd Workshop on Hybrid Approachesto Translation (HyTra)@ EACL, pages 1?5.Ankush Gupta, Sriram Venkatapathy, and Rajeev San-gal.
2010.
Meteor-hindi: Automatic mt evaluationmetric for hindi as a target language.
In Proceed-ings of ICON-2010: 8th International Conferenceon Natural Language Processing.Chung-chi Huang, Ho-ching Yen, and Jason S Chang.2010.
Using sublexical translations to handle theoov problem in mt.
In Proceedings of The NinthConference of the Association for Machine Transla-tion in the Americas (AMTA).Aditi Kalyani, Hemant Kamud, Sashi Pal Singh, andAjai Kumar.
2014.
Assessing the quality of mtsystems for hindi to english translation.
In In-ternational Journal of Computer Applications, vol-ume 89.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.
Asso-ciation for Computational Linguistics.Reinhard Kneser and Hermann Ney.
1995.
Im-proved backing-off for m-gram language modeling.In Acoustics, Speech, and Signal Processing, 1995.ICASSP-95., 1995 International Conference on, vol-ume 1, pages 181?184.
IEEE.Philipp Koehn and Hieu Hoang.
2007.
Factored trans-lation models.
In EMNLP-CoNLL, pages 868?876.Citeseer.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InProceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 48?54.
Association for Computa-tional Linguistics.X Liu, Y Wang, X Chen, MJF Gales, and PC Wood-land.
2014.
Efficient lattice rescoring using recur-rent neural network language models.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH, pages 1045?1048.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of the38th Annual Meeting on Association for Computa-tional Linguistics, pages 440?447.
Association forComputational Linguistics.Ananthakrishnan Ramanathan, Jayprasad Hegde,Ritesh M Shah, Pushpak Bhattacharyya, andM Sasikumar.
2008.
Simple syntactic and morpho-logical processing can help english-hindi statisticalmachine translation.
In IJCNLP, pages 513?520.Majid Razmara and Anoop Sarkar.
2013.
Ensembletriangulation for statistical machine translation.
InProceedings of the Sixth International Joint Confer-ence on Natural Language Processing, pages 252?260.Antti-Veikko I Rosti, Spyridon Matsoukas, andRichard Schwartz.
2007.
Improved word-level sys-tem combination for machine translation.
In AN-NUAL MEETING-ASSOCIATION FOR COMPU-TATIONAL LINGUISTICS, volume 45, page 312.Citeseer.Yangyang Shi, Pascal Wiggers, and Catholijn MJonker.
2012.
Towards recurrent neural networkslanguage models with linguistic and contextual fea-tures.
In INTERSPEECH.Andreas Stolcke et al.
2002.
Srilm-an extensible lan-guage modeling toolkit.
In INTERSPEECH.Martin Sundermeyer, Ilya Oparin, J-L Gauvain, BenFreiberg, R Schluter, and Hermann Ney.
2013.Comparison of feedforward and recurrent neuralnetwork language models.
In Acoustics, Speech andSignal Processing (ICASSP), 2013 IEEE Interna-tional Conference on, pages 8430?8434.
IEEE.Aniruddha Tammewar, Karan Singla, Srinivas Banga-lore, and Michael Carl.
2013.
Enhancing asr bymt using semantic information from hindiwordnet.In Proceedings of ICON-2013: 10th InternationalConference on Natural Language Processing.56
