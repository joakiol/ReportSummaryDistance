Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 705?712,Sydney, July 2006. c?2006 Association for Computational LinguisticsGuessing Parts-of-Speech of Unknown Words Using Global InformationTetsuji NakagawaCorporate R&D CenterOki Electric Industry Co., Ltd.2?5?7 Honmachi, Chuo-kuOsaka 541?0053, Japannakagawa378@oki.comYuji MatsumotoGraduate School of Information ScienceNara Institute of Science and Technology8916?5 Takayama, IkomaNara 630?0101, Japanmatsu@is.naist.jpAbstractIn this paper, we present a method forguessing POS tags of unknown words us-ing local and global information.
Al-though many existing methods use onlylocal information (i.e.
limited windowsize or intra-sentential features), global in-formation (extra-sentential features) pro-vides valuable clues for predicting POStags of unknown words.
We propose aprobabilistic model for POS guessing ofunknown words using global informationas well as local information, and estimateits parameters using Gibbs sampling.
Wealso attempt to apply the model to semi-supervised learning, and conduct experi-ments on multiple corpora.1 IntroductionPart-of-speech (POS) tagging is a fundamentallanguage analysis task.
In POS tagging, we fre-quently encounter words that do not exist in train-ing data.
Such words are called unknown words.They are usually handled by an exceptional pro-cess in POS tagging, because the tagging sys-tem does not have information about the words.Guessing the POS tags of such unknown words isa difficult task.
But it is an important issue bothfor conducting POS tagging accurately and forcreating word dictionaries automatically or semi-automatically.
There have been many studies onPOS guessing of unknown words (Mori and Na-gao, 1996; Mikheev, 1997; Chen et al, 1997; Na-gata, 1999; Orphanos and Christodoulakis, 1999).In most of these previous works, POS tags of un-known words were predicted using only local in-formation, such as lexical forms and POS tagsof surrounding words or word-internal features(e.g.
suffixes and character types) of the unknownwords.
However, this approach has limitationsin available information.
For example, commonnouns and proper nouns are sometimes difficultto distinguish with only the information of a sin-gle occurrence because their syntactic functionsare almost identical.
In English, proper nounsare capitalized and there is generally little ambi-guity between common nouns and proper nouns.In Chinese and Japanese, no such convention ex-ists and the problem of the ambiguity is serious.However, if an unknown word with the same lex-ical form appears in another part with informa-tive local features (e.g.
titles of persons), this willgive useful clues for guessing the part-of-speechof the ambiguous one, because unknown wordswith the same lexical form usually have the samepart-of-speech.
For another example, there is apart-of-speech named sahen-noun (verbal noun) inJapanese.
Verbal nouns behave as common nouns,except that they are used as verbs when they arefollowed by a verb ?suru?
; e.g., a verbal noun?dokusho?
means ?reading?
and ?dokusho-suru?is a verb meaning to ?read books?.
It is diffi-cult to distinguish a verbal noun from a commonnoun if it is used as a noun.
However, it willbe easy if we know that the word is followed by?suru?
in another part in the document.
This issuewas mentioned by Asahara (2003) as a problemof possibility-based POS tags.
A possibility-basedPOS tag is a POS tag that represents all the possi-ble properties of the word (e.g., a verbal noun isused as a noun or a verb), rather than a property ofeach instance of the word.
For example, a sahen-noun is actually a noun that can be used as a verbwhen it is followed by ?suru?.
This property can-not be confirmed without observing real usage ofthe word appearing with ?suru?.
Such POS tagsmay not be identified with only local informationof one instance, because the property that each in-stance has is only one among all the possible prop-erties.To cope with these issues, we propose a methodthat uses global information as well as local in-formation for guessing the parts-of-speech of un-known words.
With this method, all the occur-rences of the unknown words in a document1 aretaken into consideration at once, rather than thateach occurrence of the words is processed sepa-rately.
Thus, the method models the whole doc-ument and finds a set of parts-of-speech by max-imizing its conditional joint probability given thedocument, rather than independently maximizingthe probability of each part-of-speech given eachsentence.
Global information is known to be use-ful in other NLP tasks, especially in the named en-tity recognition task, and several studies success-fully used global features (Chieu and Ng, 2002;Finkel et al, 2005).One potential advantage of our method is its1In this paper, we use the word document to denote thewhole data consisting of multiple sentences (training corpusor test corpus).705ability to incorporate unlabeled data.
Global fea-tures can be increased by simply adding unlabeleddata into the test data.Models in which the whole document is takeninto consideration need a lot of computation com-pared to models with only local features.
Theyalso cannot process input data one-by-one.
In-stead, the entire document has to be read beforeprocessing.
We adopt Gibbs sampling in order tocompute the models efficiently, and these modelsare suitable for offline use such as creating dictio-naries from raw text where real-time processing isnot necessary but high-accuracy is needed to re-duce human labor required for revising automati-cally analyzed data.The rest of this paper is organized as follows:Section 2 describes a method for POS guessing ofunknown words which utilizes global information.Section 3 shows experimental results on multiplecorpora.
Section 4 discusses related work, andSection 5 gives conclusions.2 POS Guessing of Unknown Words withGlobal InformationWe handle POS guessing of unknown words as asub-task of POS tagging, in this paper.
We assumethat POS tags of known words are already deter-mined beforehand, and positions in the documentwhere unknown words appear are also identified.Thus, we focus only on prediction of the POS tagsof unknown words.In the rest of this section, we first present amodel for POS guessing of unknown words withglobal information.
Next, we show how the testdata is analyzed and how the parameters of themodel are estimated.
A method for incorporatingunlabeled data with the model is also discussed.2.1 Probabilistic Model Using GlobalInformationWe attempt to model the probability distributionof the parts-of-speech of all occurrences of theunknown words in a document which have thesame lexical form.
We suppose that such parts-of-speech have correlation, and the part-of-speechof each occurrence is also affected by its localcontext.
Similar situations to this are handled inphysics.
For example, let us consider a case wherea number of electrons with spins exist in a system.The spins interact with each other, and each spin isalso affected by the external magnetic field.
In thephysical model, if the state of the system is s andthe energy of the system is E(s), the probabilitydistribution of s is known to be represented by thefollowing Boltzmann distribution:P (s)= 1Z exp{?
?E(s)}, (1)where ?
is inverse temperature and Z is a normal-izing constant defined as follows:Z=?sexp{??E(s)}.
(2)Takamura et al (2005) applied this model to anNLP task, semantic orientation extraction, and weapply it to POS guessing of unknown words here.Suppose that unknown words with the same lex-ical form appear K times in a document.
Assumethat the number of possible POS tags for unknownwords is N , and they are represented by integersfrom 1 to N .
Let tk denote the POS tag of the kthoccurrence of the unknown words, let wk denotethe local context (e.g.
the lexical forms and thePOS tags of the surrounding words) of the kth oc-currence of the unknown words, and let w and tdenote the sets of wk and tk respectively:w={w1, ?
?
?
, wK}, t={t1, ?
?
?
, tK}, tk?
{1, ?
?
?
, N}.
?i,j is a weight which denotes strength of the in-teraction between parts-of-speech i and j, and issymmetric (?i,j = ?j,i).
We define the energywhere POS tags of unknown words given w aret as follows:E(t|w)=?{12K?k=1K?k?=1k?
6=k?tk,tk?
+K?k=1log p0(tk|wk)},(3)where p0(t|w) is an initial distribution (localmodel) of the part-of-speech t which is calculatedwith only the local context w, using arbitrary sta-tistical models such as maximum entropy models.The right hand side of the above equation consistsof two components; one represents global interac-tions between each pair of parts-of-speech, and theother represents the effects of local information.In this study, we fix the inverse temperature?
= 1.
The distribution of t is then obtained fromEquation (1), (2) and (3) as follows:P (t|w)= 1Z(w)p0(t|w) exp{12K?k=1K?k?=1k?
6=k?tk,tk?
}, (4)Z(w)=?t?T (w)p0(t|w) exp{12K?k=1K?k?=1k?
6=k?tk,tk?
}, (5)p0(t|w)?K?k=1p0(tk|wk), (6)where T (w) is the set of possible configurationsof POS tags given w. The size of T (w) is NK ,because there are K occurrences of the unknownwords and each unknown word can have one of NPOS tags.
The above equations can be rewritten asfollows by defining a function fi,j(t):fi,j(t)?12K?k=1K?k?=1k?
6=k?
(tk, i)?(tk?
, j), (7)P (t|w)= 1Z(w)p0(t|w) exp{ N?i=1N?j=1?i,jfi,j(t)}, (8)Z(w)=?t?T (w)p0(t|w) exp{ N?i=1N?j=1?i,jfi,j(t)}, (9)706where ?
(i, j) is the Kronecker delta:?
(i, j)={ 1 (i = j),0 (i 6= j).
(10)fi,j(t) represents the number of occurrences of thePOS tag pair i and j in the whole document (di-vided by 2), and the model in Equation (8) is es-sentially a maximum entropy model with the doc-ument level features.As shown above, we consider the conditionaljoint probability of all the occurrences of the un-known words with the same lexical form in thedocument given their local contexts, P (t|w), incontrast to conventional approaches which assumeindependence of the sentences in the documentand use the probabilities of all the words only ina sentence.
Note that we assume independencebetween the unknown words with different lexicalforms, and each set of the unknown words with thesame lexical form is processed separately from thesets of other unknown words.2.2 DecodingLet us consider how to find the optimal POS tags tbasing on the model, given K local contexts of theunknown words with the same lexical form (testdata) w, an initial distribution p0(t|w) and a setof model parameters ?
= {?1,1, ?
?
?
, ?N,N}.
Oneway to do this is to find a set of POS tags whichmaximizes P (t|w) among all possible candidatesof t. However, the number of all possible candi-dates of the POS tags is NK and the calculation isgenerally intractable.
Although HMMs, MEMMs,and CRFs use dynamic programming and somestudies with probabilistic models which have spe-cific structures use efficient algorithms (Wang etal., 2005), such methods cannot be applied herebecause we are considering interactions (depen-dencies) between all POS tags, and their joint dis-tribution cannot be decomposed.
Therefore, weuse a sampling technique and approximate the so-lution using samples obtained from the probabilitydistribution.We can obtain a solution t?
= {t?1, ?
?
?
, t?K} asfollows:t?k=argmaxtPk(t|w), (11)where Pk(t|w) is the marginal distribution of thepart-of-speech of the kth occurrence of the un-known words given a set of local contexts w, andis calculated as an expected value over the distri-bution of the unknown words as follows:Pk(t|w)=?t1,???,tk?1,tk+1,??
?,tKtk=tP (t|w),=?t?T (w)?
(tk, t)P (t|w).
(12)Expected values can be approximately calculatedusing enough number of samples generated fromthe distribution (MacKay, 2003).
Suppose thatA(x) is a function of a random variable x, P (x)initialize t(1)for m := 2 to Mfor k := 1 to Kt(m)k ?
P (tk|w, t(m)1 , ?
?
?
, t(m)k?1, t(m?1)k+1 , ?
?
?
, t(m?1)K )Figure 1: Gibbs Samplingis a distribution of x, and {x(1), ?
?
?
,x(M)} are Msamples generated from P (x).
Then, the expec-tation of A(x) over P (x) is approximated by thesamples: ?xA(x)P (x)' 1MM?m=1A(x(m)).
(13)Thus, if we have M samples {t(1), ?
?
?
, t(M)}generated from the conditional joint distributionP (t|w), the marginal distribution of each POS tagis approximated as follows:Pk(t|w)' 1MM?m=1?
(t(m)k , t).
(14)Next, we describe how to generate samplesfrom the distribution.
We use Gibbs samplingfor this purpose.
Gibbs sampling is one of theMarkov chain Monte Carlo (MCMC) methods,which can generate samples efficiently from high-dimensional probability distributions (Andrieu etal., 2003).
The algorithm is shown in Figure 1.The algorithm firstly set the initial state t(1), thenone new random variable is sampled at a timefrom the conditional distribution in which all othervariables are fixed, and new samples are cre-ated by repeating the process.
Gibbs sampling iseasy to implement and is guaranteed to convergeto the true distribution.
The conditional distri-bution P (tk|w, t1, ?
?
?
, tk?1, tk+1, ?
?
?
, tK) in Fig-ure 1 can be calculated simply as follows:P (tk|w, t1, ?
?
?
, tk?1, tk+1, ?
?
?
, tK)= P (t|w)P (t1, ?
?
?
, tk?1, tk+1, ?
?
?
, tK |w) ,=1Z(w)p0(t|w) exp{ 12?Kk?=1?Kk??=1k??
6=k??tk?
,tk??
}?Nt?k=1P (t1, ?
?
?
, tk?1, t?k, tk+1, ?
?
?
, tK |w),=p0(tk|wk) exp{?Kk?=1k?
6=k?tk?
,tk}?Nt?k=1p0(t?k|wk) exp{?Kk?=1k?
6=k?tk?
,t?k}, (15)where the last equation is obtained using the fol-lowing relation:12K?k?=1K?k??=1k??
6=k??tk?
,tk??=12K?k?=1k?
6=kK?k??=1k??
6=k,k??
6=k??tk?
,tk??
+K?k?=1k?
6=k?tk?
,tk .In later experiments, the number of samples M isset to 100, and the initial state t(1) is set to the POStags which maximize p0(t|w).The optimal solution obtained by Equation (11)maximizes the probability of each POS tag givenw, and this kind of approach is known as the maxi-mum posterior marginal (MPM) estimate (Marro-quin, 1985).
Finkel et al (2005) used simulatedannealing with Gibbs sampling to find a solutionin a similar situation.
Unlike simulated annealing,this approach does not need to define a cooling707schedule.
Furthermore, this approach can obtainnot only the best solution but also the second bestor the other solutions according to Pk(t|w), whichare useful when this method is applied to semi-automatic construction of dictionaries because hu-man annotators can check the ranked lists of can-didates.2.3 Parameter EstimationLet us consider how to estimate the param-eter ?
= {?1,1, ?
?
?
, ?N,N} in Equation (8)from training data consisting of L examples;{?w1, t1?, ?
?
?
, ?wL, tL?}
(i.e., the training datacontains L different lexical forms of unknownwords).
We define the following objective func-tion L?, and find ?
which maximizes L?
(the sub-script ?
denotes being parameterized by ?):L?
= logL?l=1P?
(tl|wl) + logP (?
),= logL?l=11Z?
(wl)p0(tl|wl) exp{ N?i=1N?j=1?i,jfi,j(tl)}+ logP (?),=L?l=1[?logZ?
(wl)+log p0(tl|wl)+N?i=1N?j=1?i,jfi,j(tl)]+ logP (?).
(16)The partial derivatives of the objective functionare:?L??
?i,j =L?l=1[fi,j(tl)?
??
?i,j logZ?
(wl)]+ ??
?i,j logP (?),=L?l=1[fi,j(tl)?
?t?T (wl)fi,j(t)P?
(t|wl)]+ ??
?i,j logP (?).
(17)We use Gaussian priors (Chen and Rosenfeld,1999) for P (?
):logP (?
)=?N?i=1N?j=1?2i,j2?2 + C,??
?i,j logP (?)
= ?
?i,j?2 .where C is a constant and ?
is set to 1 in laterexperiments.
The optimal ?
can be obtained byquasi-Newton methods using the above L?
and?L??
?i,j , and we use L-BFGS (Liu and Nocedal,1989) for this purpose2.
However, the calculationis intractable because Z?
(wl) (see Equation (9))in Equation (16) and a term in Equation (17) con-tain summations over all the possible POS tags.
Tocope with the problem, we use the sampling tech-nique again for the calculation, as suggested byRosenfeld et al (2001).
Z?
(wl) can be approx-imated using M samples {t(1), ?
?
?
, t(M)} gener-ated from p0(t|wl):Z?
(wl)=?t?T (wl)p0(t|wl) exp{ N?i=1N?j=1?i,jfi,j(t)},2In later experiments, L-BFGS often did not convergecompletely because we used approximation with Gibbs sam-pling, and we stopped iteration of L-BFGS in such cases.'
1MM?m=1exp{ N?i=1N?j=1?i,jfi,j(t(m))}.
(18)The term in Equation (17) can also be approxi-mated using M samples {t(1), ?
?
?
, t(M)} gener-ated from P?
(t|wl) with Gibbs sampling:?t?T (wl)fi,j(t)P?
(t|wl)' 1MM?m=1fi,j(t(m)).
(19)In later experiments, the initial state t(1) in Gibbssampling is set to the gold standard tags in thetraining data.2.4 Use of Unlabeled DataIn our model, unlabeled data can be easily usedby simply concatenating the test data and the unla-beled data, and decoding them in the testing phase.Intuitively, if we increase the amount of the testdata, test examples with informative local featuresmay increase.
The POS tags of such examples canbe easily predicted, and they are used as globalfeatures in prediction of other examples.
Thus,this method uses unlabeled data in only the test-ing phase, and the training phase is the same asthe case with no unlabeled data.3 Experiments3.1 Data and ProcedureWe use eight corpora for our experiments; thePenn Chinese Treebank corpus 2.0 (CTB), a partof the PFR corpus (PFR), the EDR corpus (EDR),the Kyoto University corpus version 2 (KUC), theRWCP corpus (RWC), the GENIA corpus 3.02p(GEN), the SUSANNE corpus (SUS) and the PennTreebank WSJ corpus (WSJ), (cf.
Table 1).
Allthe corpora are POS tagged corpora in Chinese(C),English(E) or Japanese(J), and they are split intothree portions; training data, test data and unla-beled data.
The unlabeled data is used in ex-periments of semi-supervised learning, and POStags of unknown words in the unlabeled data areeliminated.
Table 1 summarizes detailed informa-tion about the corpora we used: the language, thenumber of POS tags, the number of open classtags (POS tags that unknown words can have, de-scribed later), the sizes of training, test and un-labeled data, and the splitting method of them.For the test data and the unlabeled data, unknownwords are defined as words that do not appear inthe training data.
The number of unknown wordsin the test data of each corpus is shown in Ta-ble 1, parentheses.
Accuracy of POS guessing ofunknown words is calculated based on how manywords among them are correctly POS-guessed.Figure 2 shows the procedure of the experi-ments.
We split the training data into two parts;the first half as sub-training data 1 and the latterhalf as sub-training data 2 (Figure 2, *1).
Then,we check the words that appear in the sub-training708Corpus # of POS # of Tokens (# of Unknown Words) [partition in the corpus](Lang.)
(Open Class) Training Test UnlabeledCTB 34 84,937 7,980 (749) 6,801(C) (28) [sec.
1?270] [sec.
271?300] [sec.
301?325]PFR 42 304,125 370,627 (27,774) 445,969(C) (39) [Jan. 1?Jan.
9] [Jan. 10?Jan.
19] [Jan. 20?Jan.
31]EDR 15 2,550,532 1,280,057 (24,178) 1,274,458(J) (15) [id = 4n+ 0, id = 4n+ 1] [id = 4n+ 2] [id = 4n+ 3]KUC 40 198,514 31,302 (2,477) 41,227(J) (36) [Jan. 1?Jan.
8] [Jan. 9] [Jan. 10]RWC 66 487,333 190,571 (11,177) 210,096(J) (55) [1?10,000th sentences] [10,001?14,000th sentences] [14,001?18,672th sentences]GEN 47 243,180 123,386 (7,775) 134,380(E) (36) [1?10,000th sentences] [10,001?15,000th sentences] [15,001?20,546th sentences]SUS 125 74,902 37,931 (5,760) 37,593(E) (90) [sec.
A01?08, G01?08, [sec.
A09?12, G09?12, [sec.
A13?20, G13?22,J01?08, N01?08] J09?17, N09?12] J21?24, N13?18]WSJ 45 912,344 129,654 (4,253) 131,768(E) (33) [sec.
0?18] [sec.
22?24] [sec.
19?21]Table 1: Statistical Information of CorporaCorpus TrainingDataTestDataUnlabeledDataSub-Trainingdata 1(*1)Sub-Trainingdata 2(*1)Sub-Local Model 1(*3)Sub-Local Model 2(*3)Global ModelLocal Model(*2)(optional)TestResultData flow for trainingData flow for testingFigure 2: Experimental Proceduredata 1 but not in the sub-training data 2, or viceversa.
We handle these words as (pseudo) un-known words in the training data.
Such (two-fold)cross-validation is necessary to make training ex-amples that contain unknown words3.
POS tagsthat these pseudo unknown words have are definedas open class tags, and only the open class tagsare considered as candidate POS tags for unknownwords in the test data (i.e., N is equal to the num-ber of the open class tags).
In the training phase,we need to estimate two types of parameters; localmodel (parameters), which is necessary to calcu-late p0(t|w), and global model (parameters), i.e.,?i,j .
The local model parameters are estimatedusing all the training data (Figure 2, *2).
Local3A major method for generating such pseudo unknownwords is to collect the words that appear only once in a cor-pus (Nagata, 1999).
These words are called hapax legom-ena and known to have similar characteristics to real un-known words (Baayen and Sproat, 1996).
These words areinterpreted as being collected by the leave-one-out technique(which is a special case of cross-validation) as follows: Oneword is picked from the corpus and the rest of the corpusis considered as training data.
The picked word is regardedas an unknown word if it does not exist in the training data.This procedure is iterated for all the words in the corpus.However, this approach is not applicable to our experimentsbecause those words that appear only once in the corpus donot have global information and are useless for learning theglobal model, so we use the two-fold cross validation method.model parameters and training data are necessaryto estimate the global model parameters, but theglobal model parameters cannot be estimated fromthe same training data from which the local modelparameters are estimated.
In order to estimate theglobal model parameters, we firstly train sub-localmodels 1 and 2 from the sub-training data 1 and2 respectively (Figure 2, *3).
The sub-local mod-els 1 and 2 are used for calculating p0(t|w) of un-known words in the sub-training data 2 and 1 re-spectively, when the global model parameters areestimated from the entire training data.
In the test-ing phase, p0(t|w) of unknown words in the testdata are calculated using the local model param-eters which are estimated from the entire trainingdata, and test results are obtained using the globalmodel with the local model.Global information cannot be used for unknownwords whose lexical forms appear only once inthe training or test data, so we process only non-unique unknown words (unknown words whoselexical forms appear more than once) using theproposed model.
In the testing phase, POS tags ofunique unknown words are determined using onlythe local information, by choosing POS tags whichmaximize p0(t|w).Unlabeled data can be optionally used for semi-supervised learning.
In that case, the test data andthe unlabeled data are concatenated, and the bestPOS tags which maximize the probability of themixed data are searched.3.2 Initial DistributionIn our method, the initial distribution p0(t|w) isused for calculating the probability of t given lo-cal context w (Equation (8)).
We use maximumentropy (ME) models for the initial distribution.p0(t|w) is calculated by ME models as follows(Berger et al, 1996):p0(t|w)= 1Y (w) exp{ H?h=1?hgh(w, t)}, (20)709Language FeaturesEnglish Prefixes of ?0 up to four characters,suffixes of ?0 up to four characters,?0 contains Arabic numerals,?0 contains uppercase characters,?0 contains hyphens.Chinese Prefixes of ?0 up to two characters,Japanese suffixes of ?0 up to two characters,?1, ?|?0|, ?1 & ?|?0|,?|?0|i=1 {?i} (set of character types).
(common) |?0| (length of ?0),?
?1, ?+1, ?
?2 & ?
?1, ?+1 & ?+2,?
?1 & ?+1, ?
?1 & ?
?1, ?+1 & ?+1,?
?2 & ?
?2 & ?
?1 & ?
?1,?+1 & ?+1 & ?+2 & ?+2,?
?1 & ?
?1 & ?+1 & ?+1.Table 2: Features Used for Initial DistributionY (w)=N?t=1exp{ H?h=1?hgh(w, t)}, (21)where gh(w, t) is a binary feature function.
Weassume that each local context w contains the fol-lowing information about the unknown word:?
The POS tags of the two words on each sideof the unknown word: ?
?2, ?
?1, ?+1, ?+2.4?
The lexical forms of the unknown word itselfand the two words on each side of the un-known word: ?
?2, ?
?1, ?0, ?+1, ?+2.?
The character types of all the characters com-posing the unknown word: ?1, ?
?
?
, ?|?0|.We use six character types: alphabet, nu-meral (Arabic and Chinese numerals), sym-bol, Kanji (Chinese character), Hiragana(Japanese script) and Katakana (Japanesescript).A feature function gh(w, t) returns 1 if w and tsatisfy certain conditions, and otherwise 0; for ex-ample:g123(w, t)={ 1 (?
?1 =?President?
and ?
?1 =?NNP?
and t = 5),0 (otherwise).The features we use are shown in Table 2, whichare based on the features used by Ratnaparkhi(1996) and Uchimoto et al (2001).The parameters ?h in Equation (20) are esti-mated using all the words in the training datawhose POS tags are the open class tags.3.3 Experimental ResultsThe results are shown in Table 3.
In the table, lo-cal, local+global and local+global w/ unlabeledindicate that the results were obtained using onlylocal information, local and global information,and local and global information with the extra un-labeled data, respectively.
The results using onlylocal information were obtained by choosing POS4In both the training and the testing phases, POS tags ofknown words are given from the corpora.
When these sur-rounding words contain unknown words, their POS tags arerepresented by a special tag Unk.PFR (Chinese)+162 vn (verbal noun)+150 ns (place name)+86 nz (other proper noun)+85 j (abbreviation)+61 nr (personal name)?
?
?
?
?
?
?26 m (numeral)?100 v (verb)RWC (Japanese)+33 noun-proper noun-person name-family name+32 noun-proper noun-place name+28 noun-proper noun-organization name+17 noun-proper noun-person name-first name+6 noun-proper noun+4 noun-sahen noun?
?
?
?
?
?
?2 noun-proper noun-place name-country name?29 nounSUS (English)+13 NP (proper noun)+6 JJ (adjective)+2 VVD (past tense form of lexical verb)+2 NNL (locative noun)+2 NNJ (organization noun)?
?
?
?
?
?
?3 NN (common noun)?6 NNU (unit-of-measurement noun)Table 4: Ordered List of Increased/DecreasedNumber of Correctly Tagged Wordstags t?
= {t?1, ?
?
?
, t?K} which maximize the proba-bilities of the local model:t?k=argmaxtp0(t|wk).
(22)The table shows the accuracies, the numbers of er-rors, the p-values of McNemar?s test against theresults using only local information, and the num-bers of non-unique unknown words in the testdata.
On an Opteron 250 processor with 8GB ofRAM, model parameter estimation and decodingwithout unlabeled data for the eight corpora took117 minutes and 39 seconds in total, respectively.In the CTB, PFR, KUC, RWC and WSJ cor-pora, the accuracies were improved using globalinformation (statistically significant at p < 0.05),compared to the accuracies obtained using only lo-cal information.
The increases of the accuracies onthe English corpora (the GEN and SUS corpora)were small.
Table 4 shows the increased/decreasednumber of correctly tagged words using global in-formation in the PFR, RWC and SUS corpora.In the PFR (Chinese) and RWC (Japanese) cor-pora, many proper nouns were correctly tagged us-ing global information.
In Chinese and Japanese,proper nouns are not capitalized, therefore propernouns are difficult to distinguish from commonnouns with only local information.
One reasonthat only the small increases were obtained withglobal information in the English corpora seems tobe the low ambiguities of proper nouns.
Many ver-bal nouns in PFR and a few sahen-nouns (Japaneseverbal nouns) in RWC, which suffer from theproblem of possibility-based POS tags, were alsocorrectly tagged using global information.
Whenthe unlabeled data was used, the number of non-unique words in the test data increased.
Comparedwith the case without the unlabeled data, the accu-710Corpus Accuracy for Unknown Words (# of Errors)(Lang.)
[p-value] ?# of Non-unique Unknown Words?local local+global local+global w/ unlabeledCTB 0.7423 (193) 0.7717 (171) 0.7704 (172)(C) [0.0000] ?344?
[0.0001] ?361?PFR 0.6499 (9723) 0.6690 (9193) 0.6785 (8930)(C) [0.0000] ?16019?
[0.0000] ?18861?EDR 0.9639 (874) 0.9643 (863) 0.9651 (844)(J) [0.1775] ?4903?
[0.0034] ?7770?KUC 0.7501 (619) 0.7634 (586) 0.7562 (604)(J) [0.0000] ?788?
[0.0872] ?936?RWC 0.7699 (2572) 0.7785 (2476) 0.7787 (2474)(J) [0.0000] ?5044?
[0.0000] ?5878?GEN 0.8836 (905) 0.8837 (904) 0.8863 (884)(E) [1.0000] ?4094?
[0.0244] ?4515?SUS 0.7934 (1190) 0.7957 (1177) 0.7979 (1164)(E) [0.1878] ?3210?
[0.0116] ?3583?WSJ 0.8345 (704) 0.8368 (694) 0.8352 (701)(E) [0.0162] ?1412?
[0.7103] ?1627?Table 3: Results of POS Guessing of Unknown WordsCorpus Mean?Standard Deviation(Lang.)
Marginal S.A.CTB (C) 0.7696?0.0021 0.7682?0.0028PFR (C) 0.6707?0.0010 0.6712?0.0014EDR (J) 0.9644?0.0001 0.9645?0.0001KUC (J) 0.7595?0.0031 0.7612?0.0018RWC (J) 0.7777?0.0017 0.7772?0.0020GEN (E) 0.8841?0.0009 0.8840?0.0007SUS (E) 0.7997?0.0038 0.7995?0.0034WSJ (E) 0.8366?0.0013 0.8360?0.0021Table 5: Results of Multiple Trials and Compari-son to Simulated Annealingracies increased in several corpora but decreasedin the CTB, KUC and WSJ corpora.Since our method uses Gibbs sampling in thetraining and the testing phases, the results are af-fected by the sequences of random numbers usedin the sampling.
In order to investigate the influ-ence, we conduct 10 trials with different sequencesof pseudo random numbers.
We also conduct ex-periments using simulated annealing in decoding,as conducted by Finkel et al (2005) for informa-tion extraction.
We increase inverse temperature ?in Equation (1) from ?
= 1 to ?
?
?
with thelinear cooling schedule.
The results are shown inTable 5.
The table shows the mean values and thestandard deviations of the accuracies for the 10 tri-als, and Marginal and S.A. mean that decoding isconducted using Equation (11) and simulated an-nealing respectively.
The variances caused by ran-dom numbers and the differences of the accuraciesbetween Marginal and S.A. are relatively small.4 Related WorkSeveral studies concerning the use of global infor-mation have been conducted, especially in namedentity recognition, which is a similar task to POSguessing of unknown words.
Chieu and Ng (2002)conducted named entity recognition using globalfeatures as well as local features.
In their MEmodel-based method, some global features wereused such as ?when the word appeared first in aposition other than the beginning of sentences, theword was capitalized or not?.
These global fea-tures are static and can be handled in the samemanner as local features, therefore Viterbi decod-ing was used.
The method is efficient but does nothandle interactions between labels.Finkel et al (2005) proposed a method incorpo-rating non-local structure for information extrac-tion.
They attempted to use label consistency ofnamed entities, which is the property that namedentities with the same lexical form tend to havethe same label.
They defined two probabilis-tic models; a local model based on conditionalrandom fields and a global model based on log-linear models.
Then the final model was con-structed by multiplying these two models, whichcan be seen as unnormalized log-linear interpola-tion (Klakow, 1998) of the two models which areweighted equally.
In their method, interactions be-tween labels in the whole document were consid-ered, and they used Gibbs sampling and simulatedannealing for decoding.
Our model is largely sim-ilar to their model.
However, in their method, pa-rameters of the global model were estimated usingrelative frequencies of labels or were selected byhand, while in our method, global model parame-ters are estimated from training data so as to fit tothe data according to the objective function.One approach for incorporating global infor-mation in natural language processing is to uti-lize consistency of labels, and such an approachhave been used in other tasks.
Takamura et al(2005) proposed a method based on the spin mod-els in physics for extracting semantic orientationsof words.
In the spin models, each electron hasone of two states, up or down, and the models giveprobability distribution of the states.
The statesof electrons interact with each other and neighbor-ing electrons tend to have the same spin.
In their711method, semantic orientations (positive or nega-tive) of words are regarded as states of spins, inorder to model the property that the semantic ori-entation of a word tends to have the same orienta-tion as words in its gloss.
The mean field approxi-mation was used for inference in their method.Yarowsky (1995) studied a method for wordsense disambiguation using unlabeled data.
Al-though no probabilistic models were consideredexplicitly in the method, they used the property oflabel consistency named ?one sense per discourse?for unsupervised learning together with local in-formation named ?one sense per collocation?.There exist other approaches using global in-formation which do not necessarily aim to uselabel consistency.
Rosenfeld et al (2001) pro-posed whole-sentence exponential language mod-els.
The method calculates the probability of asentence s as follows:P (s)= 1Z p0(s) exp{?i?ifi(s)},where p0(s) is an initial distribution of s and anylanguage models such as trigram models can beused for this.
fi(s) is a feature function and canhandle sentence-wide features.
Note that if we re-gard fi,j(t) in our model (Equation (7)) as a fea-ture function, Equation (8) is essentially the sameform as the above model.
Their models can incor-porate any sentence-wide features including syn-tactic features obtained by shallow parsers.
Theyattempted to use Gibbs sampling and other sam-pling methods for inference, and model parame-ters were estimated from training data using thegeneralized iterative scaling algorithm with thesampling methods.
Although they addressed mod-eling of whole sentences, the method can be di-rectly applied to modeling of whole documentswhich allows us to incorporate unlabeled data eas-ily as we have discussed.
This approach, modelingwhole wide-scope contexts with log-linear modelsand using sampling methods for inference, givesus an expressive framework and will be applied toother tasks.5 ConclusionIn this paper, we presented a method for guessingparts-of-speech of unknown words using globalinformation as well as local information.
Themethod models a whole document by consider-ing interactions between POS tags of unknownwords with the same lexical form.
Parameters ofthe model are estimated from training data usingGibbs sampling.
Experimental results showed thatthe method improves accuracies of POS guess-ing of unknown words especially for Chinese andJapanese.
We also applied the method to semi-supervised learning, but the results were not con-sistent and there is some room for improvement.AcknowledgementsThis work was supported by a grant from the Na-tional Institute of Information and Communica-tions Technology of Japan.ReferencesChristophe Andrieu, Nando de Freitas, Arnaud Doucet, andMichael I. Jordan.
2003.
An introduction to MCMC for MachineLearning.
Machine Learning, 50:5?43.Masayuki Asahara.
2003.
Corpus-based Japanese morphologicalanalysis.
Nara Institute of Science and Technology, Doctor?sThesis.Harald Baayen and Richard Sproat.
1996.
Estimating Lexical Priorsfor Low-Frequency Morphologically Ambiguous Forms.
Com-putational Linguistics, 22(2):155?166.Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra.1996.
A Maximum Entropy Approach to Natural Language Pro-cessing.
Computational Linguistics, 22(1):39?71.Stanley Chen and Ronald Rosenfeld.
1999.
A Gaussian Priorfor Smoothing Maximum Entropy Models.
Technical ReportCMUCS-99-108, Carnegie Mellon University.Chao-jan Chen, Ming-hong Bai, and Keh-Jiann Chen.
1997.
Cate-gory Guessing for Chinese Unknown Words.
In Proceedings ofNLPRS ?97, pages 35?40.Hai Leong Chieu and Hwee Tou Ng.
2002.
Named Entity Recogni-tion: A Maximum Entropy Approach Using Global Information.In Proceedings of COLING 2002, pages 190?196.Jenny Rose Finkel, Trond Grenager, and Christopher Manning.2005.
Incorporating Non-local Information into Information Ex-traction Systems by Gibbs Sampling.
In Proceedings of ACL2005, pages 363?370.D.
Klakow.
1998.
Log-linear interpolation of language models.
InProceedings of ICSLP ?98, pages 1695?1699.Dong C. Liu and Jorge Nocedal.
1989.
On the limited memoryBFGS method for large scale optimization.
Mathematical Pro-gramming, 45(3):503?528.David J. C. MacKay.
2003.
Information Theory, Inference, andLearning Algorithms.
Cambridge University Press.Jose.
L. Marroquin.
1985.
Optimal Bayesian Estimators for ImageSegmentation and Surface Reconstruction.
A.I.
Memo 839, MIT.Andrei Mikheev.
1997.
Automatic Rule Induction for Unknown-Word Guessing.
Computational Linguistics, 23(3):405?423.Shinsuke Mori and Makoto Nagao.
1996.
Word Extraction fromCorpora and Its Part-of-Speech Estimation Using DistributionalAnalysis.
In Proceedings of COLING ?96, pages 1119?1122.Masaki Nagata.
1999.
A Part of Speech Estimation Method forJapanese Unknown Words using a Statistical Model of Morphol-ogy and Context.
In Proceedings of ACL ?99, pages 277?284.Giorgos S. Orphanos and Dimitris N. Christodoulakis.
1999.
POSDisambiguation and Unknown Word Guessing with DecisionTrees.
In Proceedings of EACL ?99, pages 134?141.Adwait Ratnaparkhi.
1996.
A Maximum Entropy Model for Part-of-Speech Tagging.
In Proceedings of EMNLP ?96, pages 133?142.Ronald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.
2001.Whole-Sentence Exponential Language Models: A Vehicle ForLinguistic-Statistical Integration.
Computers Speech and Lan-guage, 15(1):55?73.Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005.
Ex-tracting Semantic Orientations of Words using Spin Model.
InProceedings of ACL 2005, pages 133?140.Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
2001.
TheUnknown Word Problem: a Morphological Analysis of JapaneseUsing Maximum Entropy Aided by a Dictionary.
In Proceedingsof EMNLP 2001, pages 91?99.Shaojun Wang, Shaomin Wang, Russel Greiner, Dale Schuurmans,and Li Cheng.
2005.
Exploiting Syntactic, Semantic and LexicalRegularities in Language Modeling via Directed Markov RandomFields.
In Proceedings of ICML 2005, pages 948?955.David Yarowsky.
1995.
Unsupervised Word Sense DisambiguationRivaling Supervised Methods.
In Proceedings of ACL ?95, pages189?196.712
