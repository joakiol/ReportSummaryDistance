Named Entity Extraction from Noisy Input: Speech and OCRDavid Miller, Scan Boisen, Richard Schwartz, Rebecca Stone, Ralph WeischedelBBN Technologies70 Fawcett StreetCambridge, MA 02138dmiller@bbn.com, boisen@bbn.com, schwartz@bbn.com, rwstone@bbn.com, weischedel@bbn.comAbstractIn this paper, we analyze the performanceof name finding in the context of a varietyof automatic speech recognition (ASR)systems and in the context of one opticalcharacter recognition (OCR) system.
Weexplore the effects of word error rate fromASR and OCR, performance as a functionof the amount of training data, and forspeech, the effect of out-of-vocabularyerrors and the loss of punctuation and mixedcaseI IntroductionInformation extraction systems havetraditionally been evaluated on online text withrelatively few errors in the input.
For example,this description of the Nominator system(Wacholder et al 1997) would apply to severalother systems: "We chose The Wall StreetJournal corpus because it follows standardstylistic conventions, especially capitalization,which is essential for Nominator to work."
Thereal-world challenge, however, is pointed out inPalmer and Day (1997): "It is also unknownhow the existing high-scoring systems wouldperform on less well-behaved texts, such assingle-case texts, non-newswire texts, or textobtained via optical character recognition(OCR).
"In this paper we explore how performancedegrades on noisy input, in particular onbroadcast news (speech) and on newspaper(printed matter).
Error rates of automaticspeech recognizers (ASR) on broadcast newsare still very high, e.g., 14-28% word error.Though character error can be very low for laserprinter output, word error rates of 20% arepossible for OCR systems applied to newsprintor low-quality printed matter.In this paper, we evaluate a learning algorithm,a hidden Markov model (HM) ,  for namedentity extraction applied to human transcripts ofnews, to transcripts without case or punctuation(perfect speech output), to errorful ASR outputand to OCR output.
Extracting information fromnoisy sources poses the following challenges,which are addressed in the paper.?
Since speech recognizers do not generatemixed case nor punctuation, how much docase and punctuation contribute torecognizing names in English?
(Section 3.
)Note that these challenges also arise inlanguages without case to signal propernouns (e.g., Chinese, German, Japanese), inmono-case English or informal English(e.g., emails).?
How much will performance degrade withincreasing error in the input?
(Section 4.)?
How does closed vocabulary recognitionaffect information extraction performance?
(Section 5)?
For the learning algorithm employed, howmuch training and effort are required?
(Section 6)?
How much do lists of names contribute toperformance?
(Section 7)3162 Algorithms and Data2.1 Task Definition and DataThe named entity (NE) task used for thisevaluation requires the system to identify allnamed locations, named persons, namedorganizations, dates, times, monetary amounts,and percentages.
The task definition is given inChinchor, et al (1998).For speech recognition, roughly 175 hours ofnews broadcasts (roughly 1.2m words of audio)were available from the National Institute forScience and Technology (NIST) for training.All of that data includes both the audio and amanual transcription.
The test set consisted of 3hours of news (roughly 25k words).For the combined OCR/NE system, the OCRcomponent was trained on the University ofWashington English Image Database, which iscomprised primarily of technical journalarticles.
The NE system was trained separatelyon 690K words of 1993 Wall Street Journal(WSJ) data (roughly 1250 articles), includingdevelopment data from the Sixth MessageUnderstanding Conference (MUC-6) NamedEntity evaluation.
The test set wasapproximately 20K words of separate WSJ data(roughly 45 articles), also taken from the MUC-6 data set.
Both test and training texts wereoriginal text (no OCR errors) in mixed case withnormal punctuation.
Printing the on-line text,rather than using the original newsprint,produced the images for OCR, which were allscanned at 600 DPI.2.2 AlgorithmsThe information extraction system tested isIdentiFinder(TM), which has previously beendetailed in Bikel et al (1997, 1999).
In thatsystem, an HMM labels each word either withone of the desired classes (e.g., person,organization, etc.)
or with the label NOT-A-NAME (to represent "none of the desiredclasses").
The states of the HMM fall intoregions, one region for each desired class plusone for NOT-A-NAME.
(See Figure 2-1.)
TheHMM thus has a model of each desired classand of the other text.
Note that theimplementation is not confined to the sevenname classes used in the NE task; the particularclasses to be recognized can be easily changedvia a parameter.Within each of the regions, we use a statisticalbigram language model, and emit exactly oneword upon entering each state.
Therefore, thenumber of states in each of the name-classregions is equal to the vocabulary size.Additionally, there are two special states, theSTART-OF-SENTENCE and END-OF-SENTENCEstates.
In addition to generating the word, statesmay also generate f atures of that word.START-OF-SENTENCE END-OF SENTENCEFigure 2-1: Pictorial representation of conceptual model3173 Effect of Textual CluesThe output of each of the speech recognizers iin SNOR (speech normalized orthographicrepresentation) format, a format which is largelyunpunctuated and in all capital letters(apostrophes and periods after spoken letters arepreserved).
When a typical NE extractionsystem runs on ordinary English text, it usespunctuation and capitalization as features thatcontribute to its decisions.
In order to learn howmuch degradation i performance is caused bythe absence of these features from SNORformat, we performed the following experiment.We took a corpus that had full punctuation andmixed case and preprocessed it to make threenew versions: one with all upper case letters butpunctuation preserved, one with original casebut punctuation marks removed, and one withboth case and punctuation removed.
We thenpartitioned all four versions of the corpus into atraining set and a held-out est set, using thesame partition in all four versions, andmeasured I entiFinder's performance.The corpus we used for this experiment was thetranscriptions of the second 100 hours of theBroadcast News acoustic modelling data,comprising 114 episodes.
We partitioned thisdata to form a training set of 98 episodes(640,000 words) and a test set of 16 episodes(130,000 words).
Because the test transcriptionswere created by humans, they have a 0% worderror rate.
The results are shown in Table 3-1.The removal of case information has the greatereffect, reducing performance by 2.3 points,while the loss of punctuation reducesperformance by 1.4 points.
The loss fromremoving both features is 3.4 points, less thanthe sum of the individual degradations.
Thissuggests that there are some events where bothmixed case and punctuation are required to leadIdentiFinder to the correct answer.Mixed UpperCase CaseWith punctuation 92.4 90.1Without punctuation 91.0 89.0Table 3-1: Effect of case and punctuation onperformance(F-measure) on Broadcast NewsdataIt should be noted that because the data aretranscriptions of speech, no version of thecorpus contains all the textual clues that wouldappear in newspaper text like the MUC-7 NewYork Times data.
In particular, numbers arewritten out in words as they would be spoken,not represented using digits, and abbreviationssuch as "Dr.", "Jr." or "Sept." are expanded outto their full spoken word.
We conclude that thedegradation in performance going fromnewspaper text to SNOR recognizer output is atleast 3.4 points in the 0% WER case, andprobably more due to these other missing textclues.4 Effect of Word Errors4.1 Optical Character(OCR)RecognitionThe OCR experiments were performed using thesystem described in Makhoul et al (1998).Recognition was performed at the characterlevel, rather than the word level, so thevocabulary is not closed (unlike the ASR resultsdiscussed in subsequent sections).
Figure 4-1shows IdentiFinder's performance under 4conditions of varying word error ate (WER):1.
Original text (no OCR, 0% WER)2.
OCR from high-quality (laser-printed) textimages (2.7% WER)3.
OCR on degraded images (13.7% WER).4."
OCR on degraded images, processed with aweak character language model (19.1%WER)For the second and third conditions, 1.3Mcharacters of Wall Street Journal were used for318OCR language model training: the fourthcondition used a much weaker characterlanguage model, which accounts for the poorerperformance.The interpolated line has been fit to theperformance of the OCR-based systems, with aslope indicating 0.6 points of F-measure lost foreach percentage point increase in word error.The line has been extrapolated to 0% WER: theactual 0% WER condition is 95.4, which onlyslightly exceeds the projected value.10095 ~757O5 10 15 20 25 30Word Error RateFigure 4-1: IdentiFinder Named Entityperformance as a function of OCR worderror rate4.2 Automatic Speech Recognition(ASR)Figure 5-1 shows IdentiFinder's performance onall speech systems in the 1998 Hub-4evaluations (Przybocki, et al, 1999).
Theseexperiments were run in co-operation withNIST.
The interpolated line has been fit to theerrorful transcripts, and then extrapolated out to0% WER speech.
As can be seen, the line fitsthe data extremely well, and has a slope of 0.7points of F-measure lost for each additional 1%of word error rate.
The fact that the extrapolated' These figures do not reflect the best possibleperformance of the OCR system: for example, whentesting on degraded ata, it would be usual to includerepresentative data in training.
This was not aconcern for this experiment, however, which focussedon name finding performance.line slightly overestimates the actualperformance at 0% WER (given by a A)indicates that the degradation may be sub-linearin the range 0-15% WER.leO959oi= ,E~- 8075BE i i0 5 10 15 20 25Wo~ error r~e (Hub4 E~ 9e)Figure 4-2: IdentiFinder named-entityperformance as a function of word error rate(in cooperation with NIST)5 Out of Vocabulary Rates for NamesIt is generally agreed that out-of-vocabulary(OOV) words do not have a major impact on theword error rate achieved by large vocabularyspeech recognizers doing transcription.
Thereason is that speech lexicons are designed toinclude the most frequent words, thus ensuringthat OOV words will represent only a smallfraction of the words in any test set.
However,we have seen that the ,OOV rate for words thatare part of named-entities can be as much as afactor of ten greater than the baseline OOV fornon-name words.
This could make OOV amajor problem for NE extraction from speech.To explore this, we measured the percentage ofnames in the Broadcast News data that containat least one OOV word as a function of lexiconsize.
For this purpose, we built lexicons simplyby ordering the words of the 1998 Hub-4Language Modeling data according to30319Name Category Lexicon Size5K 10K 20K 40K 60K 80K 100K 120KPERSONORGANIZATIONLOCATIONTIMEMONEYDATEPERCENT34.7 52.7 69.9 85.1 89.4 91.1 91.9 93.973.2 90.2 94.2 97.5 98.2 98.5 98.7 98.876.6 87.1 92.2 96.2 97.5 98.0 98.8 99.197.0 97.0 99.0 100 100 100 100 10094.4 98.2 98.8 100 100 100 100 10096.1 99.3 99.8 100 100 100 100 10098.9 99.3 I00 100 100 100 100 100Table 5-1: Percentage of in-vocabulary events as a function of lexicon size.frequency, and truncating the list at variouslengths.
The percentage of in-vocabulary eventsof each type as a function of lexicon size isshown in Table 5-1.Most modem speech recognizers employ avocabulary of roughly 60,000 words; using alarger lexicon introduces more errors fromacoustic perplexity than it fixes throughenlarged vocabulary.
It is clear from the tablethat the only name category that might suffer asignificant OOV problem with a 60Kvocabulary is PERSONs.
One might imaginethat a more carefully constructed lexicon couldreduce the OOV rate for PERSONs while stillstaying within the 60,000 word limit.
However,even if a cleverly designed 60K lexiconsucceeded in having the name coverage of thefrequency-ordered 120K word lexicon (whichcontains roughly 40,000 more proper namesthan the 60K lexicon), it would reduce thePERSON OOV rate by only 4% absolute.6 Effect of training set size6.1 Automatic Speech RecognitionWe have measured NE performance in thecontext of speech as a function of training setsize and found that the performance increaseslogarithmically with the amount of training datafor 15% WER test data as well as for 0% WERinput.
However the growth rate is slower for15% WER test data.
We constructed smalltraining sets of various size by randomlyselecting sets of 6, 12, 25, and 49 episodes fromthe second 100 hours of annotated BroadcastNews training data.
We also defined a trainingset of 98 episodes from the second 100 hours, aswell as sets containing the full 98 episodes plussome or all of the first 100 hours of BroadcastNews training.
Our largest training set contained1.2 million words, and our smallest a mere30,000 words.
All training data were convertedto SNOR format.Given that PERSONs account for roughly 50%of the named-entities in broadcast news, themaximum gain in F measure available fordoubling the lexicon size is 2 points.
Moreover,this gain would require that every PERSONname added to the vocabulary be recognizedproperly -- an unlikely prospect, since most ofthese words will not appear in the acoustictraining for the recognizer.
For these reasons,we conclude that the OOV problem is not amajor factor in determining NE performancefrom speech.For each training set, we trained a separateIdentiFinder model and evaluated it on twoversions of the 1998 Hub4-IE data -- the 0%WER transcription created by a human, and anASR transcript with 15%.
The results areplotted in Figure 6-1.
The slopes of theinterpolated lines predict that IdentiFinder'sperformance on 15% WER speech will increaseby 1.5 points for each additional doubling of thetraining data, while performance goes up 1.8points per doubling of the training for perfectspeech input.320100950 L .$E9085807570v .
- -  "?
e.?
15% WER?
0% WERi \[10000 100000 1000000 10000000Number of training words (log scale)Figure 6-1: Performance as aPossibly, the difference in slope of the two linesis that the real value of increasing the trainingset lies in increasing the number of distinct rarenames that appear.
Once an example is in thetraining, IdentiFinder is able to extract it and useit in test.
However, when the test data isrecognizer output, the rare names are less likelyto appear in the test, either because they don'tappear in the speech lexicon or they are poorlytrained in the speech model and misrecognized.If they don't appear in the test, IdentiFinder can'tmake full use of the additional training, and thusperformance on errorful input increases moreslowly than it does on error-free input text.6.2 Optical Character RecognitionA similar relationship between training size andperformance is seen for the OCR test condition.function of training data for speech.The training was partitioned by documents intoequal sized sets:Partition size Training SizeEighth 77.5 K wordsQuarter 155 K wordsHalf 310 K wordsWhole 620 K wordsUsing the same test set, each partition was usedto train a separate model, which was thenevaluated on the different word error conditions:performance was then averaged across eachpartition size to produce the data points below.Input Word Error Rate (WER) Eighth Quarter Half Whole0% WER (Original text) 92.4 93.7 94.3 95.32.7% WER 90.0 90.8 91.6 92.513.7% WER 84.3 85.2 86.0 86.619.1% WER 79.6 80.4 80.8 82.5321100959O8O757010000Performance as a function of training sizen - - ' - ' '~ -~ ' '~  .....m ..--B--'--A0% WER\[\] 2.7% WERX 13.7% WERO19.1% WER100000 1000000Number of training words (log scala)Figure 6-2: Performance as a function of training data for OCR.While this graph of this data in Figure 6-2shows a logarithmic improvement, as with theASR experiments, the rate of improvement issubstantially less, roughly 0.9 increase in F-measure for doubling the training data.
Thismay be explained by the difference in difficultybetween the two tests: even with only 77.5kwords of training, the 0% WER performanceexceeds the ASR system trained on 1.2M words.full point, while on recognizer produced output,performance goes u~p by only 0.3 points./ 0% WER 15% WERWithout lists 89.5 81.9With lists 90.5 82.2Table 7-1: Effect of lists in the presence ofspeech errors.8 Related Work  and Future Work7 Effect of ListsLike most NE extraction systems, IdentiFindercan use lists of strings of known to be names toestimate the probability that a word will be aname, given that it appears on a particular list.We trained two models on 1.2 million words ofSNOR data, one with lists and one without.
Wetested on the human transcription (0% WER)and the ASR (15% WER) versions of the 1998evaluation transcripts.
Table 7-1 shows theresults.
We see that on human constructedtranscripts, lists improve the performance by aTo our knowledge, no other informationextraction technology has been applied to OCRmaterial.For audio materials, three related efforts werebenchmarked on NE extraction from broadcastnews.
Palmer, et al (1999) employs an HMMvery similar to that reported for IdentifFinder(Bikel et al, 1997,1999).
Renals et al (1999)reports on a rule-based system and an HMMintegrated with a speech recognizer.
Appelt andMartin (1999) report on the TEXTPRO system,which recognises names using manually writtenfinite state sales.322Of these, the Palmer system and TEXTPROreport results on five different word error rates.Both degrade linearly, about .7F, with each 1%increase in WER from ASR.
None report theeffect of training set size, capitalization,punctuation, or out-of-vocabulary items.Of the four systems, IdentiFinder epresentsstate-of-the-art performance.
Of all the systemsevaluated, those with the simple architecture ofASR followed by information extractionperformed markedly better than the systemwhere extraction was more integrated withASR.In general, these results compare favorably withresults reported in the Message UnderstandingConference (Chinchor, et al, 1998).
Thehighest NE score in MUC-7 was 93.39; for 0%WER, our best score was 90.5 without case andpunctuation which costs about 3.4 points.9 ConclusionsFirst and foremost, the hidden Markov model isquite robust in the face of errorful input.Performance on both speech and OCR inputdegrades linearly as a function of word error.Even, without case information or punctuationin the input, the performance on the broadcastnews task is above 90%, with only a 3.4 pointdegradation in performance due to missingtextual clues.
Performance even with 15% worderror degrades by only about 8 points of F forboth OCR and ASR systems.Second, because annotation can be performedquickly and inexpensively by non-experts,training-based systems like IdentiFinder offer apowerful advantage in moving to new languagesand new domains.
In our experience, annotationof English typically proceeds at 5k words perhour or more.
This means interestingperformance an be achieved with as little as 20hours of student annotation (i.e., at least 100kwords).
Increasing training continuallyimproves performance, generally as thelogarithm of the training set size.
Ontranscribed speech, performance is already good(89.3 on 0% WER) with only 100 hours or643K words of training data.Third, though errors due to words out of thevocabulary of the speech recognizer are aproblem, they represent only about 15% of theerrors made by the combined speech recognitionand named entity system.Fourth, we used exactly the same training data,modeling, and search algorithm for errorfulinput as we do for error-free input.
For OCR,we trained on correct newswire once only forboth correct text input 0% (WER) and for avariety of errorful text input conditions.
Forspeech, we simply transformed text training datainto SNOR format and retrained.
Using thisapproach, the only cost of handling errorfulinput from OCR or ASR was a small amount ofcomputing time.
There were no rules to rewrite,no lists to change, and no vocabularyadjustments.
Even so, the degradation inperformance on errorful input is no worse thanthe word error rate of the OCR/ASR system.AcknowledgmentsThe work reported here was supported in part bythe Defense Advanced Research ProjectsAgency.
Technical agent for part of this workwas NRaD under contract number N66001-97-D-8501.
The views and conclusions containedin this document are those of the authors andshould not be interpreted as necessarilyrepresenting the official policies, eitherexpressed or implied, of the Defense AdvancedResearch Projects Agency or the United StatesGovernment.ReferencesD.
E. Appelt, D. Martin, "Named Entity Extractionfrom Speech: Approach and Results Using theText.Pro System," Proceedings Of The DARPABroadcast News Workshop, February 28-March 3,Morgan Kaufmann Publishers, pp 51-54 (1999).D.
Bikel, S. Miller, R. Schwartz, and R. Weischedel,'Nymble: a High-Performance L arning Name-finder".
In Fifth Conference on Applied NaturalLanguage Processing, (published by ACL) pp 194-201 (1997).323D.
Bikel, R. Schwartz, and R. Weischedel, "AnAlgorithm that Learns What's in a Name,"Machine Learning 34, pp 211-231, (1999).N.
Chinchor, "MUC-7 Named Entity Task DefinitionVersion 3.5".
Available by ftp fromftp.muc.saic.com/pub/MUC/MUC7-guidelines.(1997).N.
Chincor, P. Robinson, E. Brown, "HUB-4 NamedEntity Task Definition Version 4.8".
Available byftp from www.nist.gov/speech/hub4_98.
(1998).J.
Makhoul, R. Schwartz, C. Lapre, and I. Bazzi, "AScript-Independent Methodology for OpticalCharacter Recognition,", Pattern Recognition, pp1285-1294 (1998).Z.
Lu, R. Schwartz, P. Natarajan, I. Bazzi, J.Makhoul, "Advances in the BBN BYBLOS OCRSystem," Proceedings of the InternationalConference on Document Analysis andRecognition, (1999).D.
D. Palmer, J. D. Burger, M. Ostendorf,"Information Extraction from Broadcast NewsSpeech Data," Proceedings Of The DARPABroadcast News Workshop, February 28-March 3,Morgan Kaufmann Publishers, pp 41-46 (1999).M.
A. Przybocki, J. G. Fiscus, J. S. Garofolo, D. S.Pallett, "1998 Hub-4 Information ExtractionEvaluation," Proceedings Of The DARPABroadcast News Workshop, February 28-March 3,Morgan Kaufmann Publishers, pp 13-18 (1999).S.
Renals, Y. Gotoh, R. Gaizauskas, M. Stevenson,"Baseline IE-NE Experiments Using theSPRACH/LASIE System," Proceedings Of TheDARPA Broadcast News Workshop, February 28-March 3, Morgan Kaufmann Publishers, pp 47-50(1999).324
