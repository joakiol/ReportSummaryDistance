Integration of Hand-Crafted and Statistical Resources in MeasuringWord SimilarityAtsushl FUJ I I  Toshlhlro HASEGAWA Takenobu TOKUNAGADepar tment  of  Computer  ScienceTokyo Ins t i tu te  of  Technology{fuj  i i ,  take,  t anaka}@cs, t ?tsch.
ac.
jpHozumi TANAKAAbst rac tThis paper proposes a new approach for wordsimilarity measurement.
The statistics-basedcomputation of word similarity has been pop-ular in recent research, but is associated witha significant computational cost.
On the otherhand, the use of hand-crafted thesauri as se-mantic resources is simple to implement, butlacks mathematical rigor.
To integrate the ad-vantages of these two approaches, we aim atcalculating a statistical weight for each branchof a thesaurus, so that we can measure wordsimilarity simply based on the length of thepath between two words in the thesaurus.
Ourexperiment on Japanese nouns shows that thisframework upheld the inequality of statistics-based word similarity with an accuracy of morethan 70%.
We also report on the effectivity ofour framework in the task of word sense disam-biguation.1 In t roduct ionThis paper proposes a new approach for word similaritymeasurement, as has been variously used in such NLPapplications as smoothing \[Dagan et al, 1994; Grishmanand Sterling, 1994\] and word clustering \[Charniak, 1993;Hindle, 1990; Pereira et al, 1993; Tokunaga et al, 1995\].Previous methods for word similarity measurementcan be divided into two categories: statistics-based ap-proaches and hand-crafted thesaurus-based approaches.In statistics-based approaches, and namely the "vectorspace model", each word is generally represented by avector consisting of co-occurrence statistics (such as fre-quency) with respect o other words \[Charniak, 1993\].The similarity between two given words is then compu-tationally measured using two vectors representing thosewords.
One typical implementation computes the rela-tive similarity as the cosine of the angle between twovectors, a method which is also commonly used in in-formation retrieval and text categorization systems tomeasure the similarity between documents \[Frankes andBaeza-Yates, 1992\].
Since it is based on mathematicalmethods, this type of similarity measurement has beenpopular.
Besides this, since the similarity is computedbased on given co-occurrence data, word similarity caneasily be adjusted according to the domain.
However,data sparseness is an inherent problem.
This fact wasobserved in our preliminary experiment, despite usingstatistical information taken from news articles as manyas 4 years.
Furthermore, in this approach, vectors re-quire O(N 2) memory space, given that N is the numberof words, and therefore, large data sizes can prove pro-hibitive.
Note that even if one statically stores possibleword similarity combinations, O(N 2) space is required.The other category of word similarity approaches u essemantic resources, that is, hand-cra/ted thesauri (suchas the Roget's thesaurus \[Chapman, 1984\] or Word-Net \[Miller et al, 1993\] in the case of English, and Bun-ruigoihyo \[National Language Research Institute, 1996\]or EDR \[EDR, 1995\] in the case of Japanese), based onthe intuitively feasible assumption that words locatednear each other within the structure of a thesaurus havesimilar meaning.
Therefore, the similarity between twogiven words is represented bythe length of the path be-tween them in the thesaurus structure \[Kurohashi andNagao, 1994; Li et al, 1995; Uramoto, 1994\].
Unlikethe former approach, the required memory space can berestricted to O(N) because only a list of semantic odesfor each word is required.
For example, the commonlyused Japanese Bunrsigoihyo thesaurus \[National Lan-guage Research Institute, 1996\] represents each seman-tic code with only 8 digits.
However, computationallyspeaking, the relation between the similarity (namely thesemantic length of the path), and the physical length ofthe path is not clear 1.
Furthermore, since most thesauriaim at a general word hierarchy, the similarity betweenwords used in specific domains (technical terms) cannotbe measured to the desired level of accuracy.IMost researchers heuristicallydefine functions between thesimilarity and physical path length \[Kurohashi and Nagao, 1994;Li et al, 1995; Uramoto, 1994\].45In this paper, we aim at intergrating the advantages ofthe two above methodological types, or more precisely,realizing statistics-based word similarity based on thelength of the thesaurus path.
The crucial concern in thisprocess is how to determine the statistics-based lengthof each branch in a thesanrus.
We tentatively use theBunruigoihyo thesaurus, inwhich each word correspondsto a leaf in the tree structure.
Let us take figure 1, whichshows a fragment of the thesaurus.
In this figure, w,'sdenote words and x,'s denote the statistics-based length(SBL, for short) of each branch i.
Let the statistics-based(vector space model) word similarity between wl and w2be vsm(wl, w2).
We hope to estimate this similarity bythe length of the path through branches 3 and 4, andderive an equation "xs + x4 = sirn(wl, w2)".
Intuitivelyspeaking, any combination of xs and x4 which satisfiesthis equation can constitute the SBLs for branches 3 and4.
Formalizing equations for other pairs of words in thesame manner, we can derive the simultaneous equationshown in figure 2.
That is, we can assign the SBL foreach branch by way of finding answers for each x~.
Thismethod is expected to excel in the following aspects.First, this method allows us to measure the statistics-based word similarity, while retaining the optimal re-quired memory space (O(N)).
One may argue thatstatistics-based automatic thesaurus construction (forexample, the method proposed by Tokunaga et al \[Toku-naga et al, 1995\]) can provide the same advantage, be-sides which there is no human overhead.
However, ithas been empirically observed that the topology of thestructure (especially at higher levels) is not necessarilyreasonable when based solely on statistics \[Frankes andBaeza-Yates, 1992\].
To avoid this problem, we wouldlike to introduce hand-crafted thesauri into our frame-work because the topology (such as MAMMAL is a hyperclass of HUMAN) allows for higher levels of sophistica-tion based on human knowledge.Second, since each SBL reflects the statistics takenfrom co-occurrence data ~f the whole word set, statisticsof each word can complement each other, and thus, thedata sparseness problem tends to be minimized.
Let ustake figure 1 again, and assume that the statistics for w4are sparse or completely missing.
In previous tatistics-based approaches, the similarity between w4 and otherwords cannot be reasonably measured, or not measuredat all.
However, in our method, similarity value such asvsm(wl, wa) can be reasonably measured because SBLsxl, x2 and x3 can be well-defined with sufficient statis-tics.In section 2, we elaborate on the methodology of ourword similarity measurement.
We then evaluate ourmethod by way of an experiment in section 3 and appliedthis method to the task of word sense disambiguation isection 4.6wl w2 w3 w4Figure 1: A fragment of the thesaurusX 1 "~X2 -~X 3 "~X 5 = vsm(wl ,w3)X l '~X2 +x3+x6 = vsm(wl ,w4)xl +x2+x4+x5 = vsm(w~,ws)Figure 2: A fragment of the simultaneous equation as-sociated with figure 12 Methodo logy2.1 Overv iewOur word similarity measurement proceeds in the follow-ing way:1. compute the statistics-based similarity of everycombination of given words,2.
set up a simultaneous equation through use of thethesaurus and previously computed word similar-ity, and find solutions for the statistics-based l ngth(SBL) of the corresponding thesaurus branch (seefigures 1 and 2),3. the similarity between two given words is measuredby the sum of SBLs included in the path betweenthose words.We will elaborate on each step in the following sections.2.2 Stat i s t i cs -based word  s imi lar i tyIn the vector space model, each word w~ is represented bya vector comprising statistical factors of co-occurrence.This can be expressed by equation (1), where ~z isthe vector for the word in question, and t,j is the co-occurrence statistics of w~ and w:.=< t,1, t,2, .
.
.
,  t,j, ... > (1)With regard to t~3, we adopted TF.IDF, commonly usedin information retrieval systems \[Frankes and Baeza-Yates, 1992\].
Based on this notion, t,~ is calculated as inequation (2), where \]~ is the frequency of w, collocating45with w3, f3 is the frequency of w3, and T is the totalnumber of collocations within the overall co-occurrencedata.t., = f., .
log (~)  (2)We then compute the similarity between words a and bbj  the cosine of the angle between the two vectors g andb.
This is realized by equation (3), where vsm is thesimilarity between a and b, based on the vector spacemodel.
~.~vsm(a, b) = i~llgl (3)It should be noted that our framework is indepen-dent of the implementation of the similarity computa-tion, which has been variously proposed by differentresearchers \[Charniak, 1993; Frankes and Baeza-Yates,1992\].2.3 Reso lu t ion  of  the  s imul taneous  equa-t ionThe simultaneous equation used in our method is ex-pressed by equation (4), where A is a matrix comprisingonly the values 0 and 1, and B is a list ofvsm's (see equa-tion (3)) for any possible combinations of given words.X is a list of variables, which represents the statistics-based length (SBL) for the corresponding branch in thethesaurus.AX = S (4)Here, let the i-th similarity in B be vsm(a,b), and letpath(a, b) denote the path between words a and b in thethesaurus.
Each equation contained in the simultaneousequation is represented by equation (5), where x~ is thestatistics-based length (SBL) for branch 3, and a, 3 iseither 0 or 1 as in equation (6).\[OQI O~z2 " ' "  ~3 " ' ' \ ]XlX2i = yam(a, b)X3(5)1 i f j  Epath(a,b)c% = 0 otherwise (6)By finding the solutions for X, we can assign SBLs tobranches.
However, the set of similarity values outnum-bers the variables.
For example, the Bunruigoihyo the-saurus contains about 55,000 noun entries, and therefore,the number of similarity values for those nouns becomesabout 1.5x109 (ss,000C2).
On the other hand, the num-ber of the branches i  only about 53,000.
As such, overlymany equations are redundant, and the time complex-ity to solve the simultaneous equation becomes a crucialproblem.
To counter this problem, we randomly dividethe overall equation set into equal parts, which can besolved reasonably.
Thereafter we approximate he solu-tion for x by averaging the solutions for x derived fromeach subset.
Let us take figure 3, in which the numberof subsets is given as two without loss of generality.
Inthis figure, x,1 and x~2 denote the answers for branchi individually derived from subsets 1 and 2, and x~ isapproximated by the average of xzl and x,2 (that is,x,l+x,2 ~ To generalize this notion, let x,j denote the 2 /"solution associated with branch i in subset j.
The ap-proximate solution for branch i is given by equation (7),where n is the number of divisions of the equation set.3=1I equation set I/l subset1 -i\lsubset2 1/ ,I +za)/2Figure 3: Approximation of the statistics-based lengthXs2.4 Word  s imi lar i ty using SBLLet us reconsider figure 1.
In this figure, the similaritybetween Wl and w2, for example, is measured by thesum of x3 and x4.
In general, the similarity betweenwords a and b using SBL (sbl(a, b), hereafter) is realizedby equation (8), where x~ is the SBL for branch i, andpath(a, b) is the path that includes thesaurus brancheslocated between a and b.sbl(a,b) = E x~ (8)~Epath(a,b)3 Exper imentat ionWe conducted experiments on noun entries in the Bun-ruigoihyo thesaurus.
Co-occurrence data was extractedfrom the RWC text base RWC-DB-TEXT-95-1 \[RealWorld Computing Partnership, 1995\].
This text baseconsists of 4 years worth of Mainichi Shimbun \[Mainichi47Shimbun, 1991-1994\] newspaper articles, which were au-tomatically annotated with morphological tags.
The to-tal number of morphemes i about 100 million.
Insteadof conducting full parsing on the texts, several heuris-tics were used in order to obtain dependencies betweennouns and verbs in the form of tuples (frequency, noun,postposition, verb).
Among these tuples, only thosewhich included the postposition wo (typically markingthe accusative case) were used.
Further, tuples withnouns appearing in the Bunruigoihyo thesaurus were se-lected.
When the noun comprised a compound noun,it was transformed into the maximal eftmost substringcontained in the Bunruigoihyo thesaurus.
As a result,419,132 tuples remained, consisting of 23,223 noun typesand 9,151 verb types.
In regard to resolving the simulta-neous equations, we used the mathematical nalysis tool,,MATLAB ,,2.What we evaluated here is the degree to which thesimultaneous equation was successfully approximatedthrough the use of the technique described in section 2.In other words, to what extent he (original) statistics-based word similarity can be realized by our frame-work.
We conducted this evaluation in the follow-ing way.
Let the statistics-based similarity betweenwords a and b be vsm(a,b), and the similarity basedon SBL be sbl(a, b).
Here, let us assume the inequal-ity "vsm(a, b) > vsm(c, d)" for words a, b, c and d. Ifthis inequality can be maintained for our method, thatis, "sbl(a, b) > sbl(c, d)", the similarity measurement istaken to be successful.
The accuracy is then estimatedby the ratio between the number of successful measure-ments and the total number of trials.
Since resolutionof equations is time-consuming, we tentatively general-ized 23,223 nouns into 303 semantic lasses (representedby the first 4 digits of the semantic ode given in theBunruigoihyo thesaurus), reducing the total number ofequations to 45,753.
Figure 4 shows the relation be-tween the number of equations used and the accuracy:we divided the overall equation set into n equal sub-sets 3 (see section 2.3), and progressively increased thenumber of subsets used in the computation.
When thewhole set of equations was provided, the accuracy be-came about 72%.
We also estimated the lower boundof this evaluation, that is, we also conducted the sametrials using the Bunruigoihyo thesaurus.
In this case,if word a is more closely located to b than c is to dand "vsm(a, b) > vsm(c,d)", that trial measurement istaken to be successful.
We found that the lower boundwas roughly 56%, and therefore, our framework outper-formed this method.2Cybernet System, Inc.3We arbitrarily set n = 15 so as to be able to resolve quationsreasonably.75~7065J~0 I I !
, I0 10000 20000 30000 40000 50000number of equations usedFigure 4: The relation between the number of equationsused and the accuracy4 An applicationWe further evaluated our word similarity technique inthe task of word sense disambiguation (WSD).
In thistask, the system is inputted with sentences containingsense ambiguous words, and interprets them by choos-ing the most plausible meaning for them based on thecontext 4.
The WSD technique used in this paper hasbeen proposed by Kurohashi et al \[Kurohashi and Na-gao, 1994\] and enhanced by Fujii et al \[Fujii et al, 1996\],and disambiguates Japanese sense ambiguous verbs byuse of an example-database 5.
Figure 5 shows a frag-ment of the database associated with the Japanese verbtsukau, some of which senses are "to employ", "to op-erate" and "to spend".
The database specifies the caseframe(s) associated with each verb sense.
In Japanese,a complement of a verb consists of a noun phrase (casefiller) and its case marker suffix, for example ga (nom-inative), ni (dative) or wo (accusative).
The databaselists several case filler examples for each case.
Given aninput, the system identifies the verb sense on the basis ofthe similarity between the input and examples for eachverb sense contained in the database.
Let us take thefollowing input:enjinia ga fakkusu wo tsukau.
(engineer-NOM) (facsimile-ACC) (?
)In this example, one may consider enjinia ("engineer")and \]akkusu ("facsimile") to be semantically similar to4In most WSD systems, candidates ofword sense are predefinedin a dictionary.SThere have been different approaches proposed for this task,based on statistics \[Charniak, 1993\].48gakusei ("student") and konpyuutaa ("computer"), re-spectively, from the "to operate" sense of tsukau.
Asa result, tsukau is interpreted as "to operate".
To for-realize this notion, the system computes the plausibil-ity score for each verb sense candidate, and chooses thesense that maximizes the score.
The score is computedby considering the weighted average of the similarity ofthe input case fillers with respect o each of the corre-sponding example case fillers listed in the database forthe sense under evaluation.
Formally, this is expressedby equation (9), where Score(s) is the score for verbsense s. nc denotes the case filler for case c, and gs,edenotes a set of case filler examples for each case c ofsense s (for example, ?
= {kate, kigyou} for the ga casein the "to employ" sense in figure 5).
sim(nc, e) standsfor the similarity between c and an example case fillere.Score(s) = ~ CCD(c).
max sim(nc, e) (9) C eE~s,cCCD(c) expresses the weight factor of case c using thenotion of case contribution to verb sense disambigua-tion (CCD) proposed by Fujii et al\[Fujii et al, 1996\].Intuitively, the CCD of a case becomes greater when ex-ample sets of the case fillers are disjunctive over differentverb senses.
In the case fillers of figure 5, for example,CCD(ACC) is greater than CCD(NOM) (see Fujii etal's paper for details).One may notice that the critical content of this taskis the computation of the similarity between case fillers(nouns) in equation (9).
This is exactly where our wordsimilarity measurement can be applied.
In this experi-ment, we compared the following three methods for wordsimilarity measure:* the Bunruigoihyo thesaurus (BGH): the similaritybetween case fillers is measured by a function be-tween the length of the path and the similarity.
Inthis experiment, we used the function proposed byKurohashi et ai.
\[Kurohashi and Nagao, 1994\] asshown in table 1.. vector space model (VSM): we replace s~m(nc, e)equation (9) with vsm(nc, e) computed by equa-tion (3)?
our method base on statistics-based length (SBL):we simply replace sim(nc, e) in equation (9) withsbl(nc, e) computed by equation (8).We collected sentences (as test/training data) fromthe EDR Japanese corpus \[EDR, 1995\] ~.
Since Japanesesentences have no lexical segmentation, the input hasto be both morphologically and syntactically analyzedprior to the sense disambiguation process.
We ex-perimentally used the Japanese morph/syntax parsereThe EDR corpus was originally collected from news articles.Table 1: The relation between the length of the pathbetween two nouns nl and n2 in the Bunruigoihyo the-saurus (len(nl, n2)) and their similarity (szm(nl, n2))\[ fen(hi,n2) I 0 2 4 6 8 10 12 14 I sire(hi,n2) 12 11 10 9 8 7 5 0"QJP" \[Kameda, 1996\] for this process.
Based on analy-sis by the QJP parser, we removed sentences with miss-ing verb complements (in most cases, due to ellipsis orzero aaaphora).
The EDR corpus also provides enseinformation for each The EDIt corpus provides senseinformation for each word based on the EDIt dictio-nary, which we used as a means of checking the cor-rect interpretation.
Our derived corpus contains tenverbs frequently appearing in the EDIt corpus, whichare summarized in table 2.
In table 2, the column of"English gloss" describes typical English translations ofthe Japanese verbs, the column of "# of sentences" de-notes the number of sentences in the corpus, while "#of senses" denotes the number of verb senses, based onthe EDIt dictionary.
For each of the ten verbs, we con-ducted four-fold cross validation: that is, we divided thecorpus into four equal parts, and conducted four trials,in each of which a different one of the four parts wasused as test data and the remaining parts were used astraining data (the database).
Table 2 also shows the pre-cision of each method.
The precision is the ratio of thenumber of correct interpretations, to the number of out-puts.
The column of "control" denotes the precision ofa naive WSD technique, in which the system systemat-icaily chooses the verb sense appearing most frequentlyin the database \[Gale et al, 1992\].The precision for each similarity calculation methoddid not differ greatly, and the use of the length of thepath in the Bunruigoihyo thesaurus (BGH) slightly out-performed other method on the whole.
However, sincethe overall precision is biased by frequently appearedverbs (such as tsukau and ukeru), our word similaritymeasurement is not necessarily inferior to other meth-ods.
In fact, disambiguation f verbs such as mo~orneru,in which BGH is surpassed by VSM, SBL maintains aprecision level relatively equivalent to that for VSM.
Be-sides this, as we pointed out in section 1, SBL allowsus to reduce the data size from O(N 2) to O(N) in ourframework, given that N is the number of word entries.5 Conc lus ionIn this paper, we proposed a new method for the mea-surement of word similarity.
Our method integratesthe statistics-based and thesanrus-based approaches.
Bythis, we can realize the statistical computation of wordsimilarity based on a thesaurus, with optimal computa-tion cost.
We showed the effectivity of our method by49I kare (he) } I " igyou company) ga { Idkaku (project) } m 3yu~gyouin (emplo~,~) sotsugyousei (graduate) ' wo tsukau (to employ)kanojo (she) " { shigoto (work) ~ konpyuutaa (computer)' gakuse: (student) ' ga kenkyuu (research) J ni k ikai  (machine) , wo tsukau (to operate){ kate (he) } f kuruma (car) } { nenry?u (fuel) !seifa (government) ga \[ flzkushi (welfare) ni shigen (resource) wo t~ukau (to spend) zetkin (tax)Figure 5: A fragment of the database associated with the Japanese verb tsukauTable 2: Precision of word sense disambiguation (the highestl English # ofverb gloss sentencestsukauukerumots uspendmoukeruto,  iireceive17291573hold 1471mwu see 1096rnotomeru request 1025dasu evict 872kuwaeru add 467okuru send 387kaku write 382establish 343-- \[ 9345I #"f  I sere es BGH7 58.810 80.212 72.117 49.15 67.45 65.94 68.79 58.42 74.53 67.1i - ii 664precision is typed in boldface)precision (%)VSM SBL \[ control55.0 52.8 27.880.9 75,5 38.470.1 71.3 37.546.5 49.8 22.771.4 71.0 48.863.4 63.4 42.367.7 69.4 58.556.8 58.4 28.973.0 73.3 48.765.6 64.7 51.065.2 64.5 { 37.4Iway of an experiment, and demonstrated itsapplicationto word sense disambiguation.
Future work will includehow to decrease the number of equations without degrad-ing the performance, and application of our frameworkto other NLP tasks for the further evaluation.AcknowledgmentsThe authors would like to thank Mr. Timothy Bald-win (TITECH, Japan) for his comments on the earlierversion of this paper, Mr. Masayuki Kameda (RICOHCo., Ltd., Japan) for his support with the QJP parser,and Mr. Akira Hirabayashi and Mr. Naoyuki Sakural(TITECH, Japan) for aiding with experiments.Re ferences\[Chapman, 1984\] Chapman, R. L. Roger's InternationalThesaurus (Fourth Edition).
Harper and Row.\[Charniak, 1993\] Charniak, E. Statistical LanguageLearning.
MIT Press.\[Dagan et al, 1994\] Dagan, I., Pereira, F., and Lee, L.Similarity-based stimation of word cooccurrenceprobabilities.
In Proceedings of ACL, pp.
272-278.\[EDR, 1995\] EDR.
EDR Electronic DictionaryTechnical Guide.
(In Japanese).\[Frankes and Baeza-Yates, 1992\] Ftankes, W. B., andBaeza-Yates, R. Information Retrieval: Data StructureAlgorithms.
PTR Prentice-Hall.\[Fujii et al, 1996\] Fujii, A., Inui, K., Tokunaga, T., andTanaka, H. To what extent does case contribute toverb sense disambiguation?
In Proceedings ofCOLING, pp.
59-64.\[Gale et al, 1992\] Gale, W., Church, K. W., andYarowsky, D. Estimating upper and lower bounds onthe performance of word-sense disambiguationprograms.
In Proceedings of ACL, pp.
249-256.\[Grishman and Sterling, 1994\] Grishman, R., andSterling, J. Generalizing automatically generatedselectional patterns.
In Proceedings ofCOLING, pp.742-747.\[Hindle, 1990\] Hindle, D. Noun classification frompredicate-argument structures.
In Proceedings o\] A CL,pp.
268-275.\[Kameda, 1996\] Kameda, M. A portable & quickJapanese parser : QJP.
In Proceedings of COLING, pp.616--621.\[Kurohashi and Nagao, 1994\] Kurohashi, S., andNagao, M. A method of case structure analysis forJapanese sentences based on examples in case frame50dictionary.
IEICE TRANSACTIONS on Informationand Systems, E77-D(2), 227-239.\[Li et al, 1995\] Li, X., Szpakowicz, S., and Matwin, S.A WordNet-based algorithm for word sensedisambiguation.
I  Proceedings of IJCAI, pp.1368-1374.\[Miller et al, 1993\] Miller, G. A., Bechwith, R.,Fellbanm, C., Gross, D., Miller, K., and Tengi, R. FivePapers on WordNet.
Tech.
rep. CSL Report 43,Cognitive Science Laboratory, Princeton University.Revised version.\[National Language Research Institute, 1996\] NationalLanguage Research Institute.
Bunruigoihyo (revisedand enlarged edition).
(In Japanese).\[Pereira et aL, 1993\] Pereira, F., Tishby, N., and Lee, L.Distributional clustering of English words.
InProceedings of ACL, pp.
183-190.\[Real World Computing Partnership, 1995\] Real WorldComputing Partnership.
RWC text database.http: //wm?.
rwcp.
or.
jp/wswg, html.\[Mainichi Shimbun, 1991-1994\] Mainichi ShimbunCD-ROM '91-'94.\[Tokunaga et aL, 1995\] Tokunaga, T., Iwayama, M.,and Tanaka, H. Automatic thesaurus constructionbased on grammatical relations.
In Proceedings ofIJCAI, pp.
1308-1313.\[Uramoto, 1994\] Uramoto, N. Example-basedword-sense disambiguation.
IEICE TRANSACTIONSon Information and Systems, E77-D(2), 240-246.51
