Proceedings of the 14th European Workshop on Natural Language Generation, pages 202?203,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsAn Automatic Method for Building a Data-to-Text GeneratorSina Zarrie?
Kyle RichardsonInstitut fu?r Maschinelle SprachverarbeitungUniversity of Stuttgart, Germanysina.zarriess,kyle@ims.uni-stuttgart.de1 IntroductionWe describe our contribution to the Generatingfrom Knowledge Bases (KBgen) challenge.
Oursystem is learned in a bottom-up fashion, by in-ducing a probabilistic grammar that representsalignments between strings and parts of a knowl-edge graph.
From these alignments, we extractinformation about the linearization and lexicalchoices associated with the target knowledge base,and build a simple generate-and-rank system in thestyle of (Langkilde and Knight, 1998).12 Semantic Parsing and AlignmentsA first step in building our generator involves find-ing alignments between phrases and their ground-ings in the target knowledge base.
Figure 1 showsan example sentence from training paired with thecorresponding triple relations.
A partial lexicon isprovided, indicating the relation between a subsetof words and their concepts.Using the triples, we automatically constructa probabilistic context-free grammar (PCFG) byconverting these triples to rewrite rules, usingideas from (Bo?rschinger et al 2011).
The righthand side of the rules represent the constituentsof the triples in all orders (initially with a uni-form probability) since the linear realization of atriple relation in the language might vary.
Thisis rewritten back to each of its constituents to al-low for interaction with other concepts that sat-isfy further domain relations.
Individual concepts,represented in the grammar as preterminals, areassigned to the associated words in the lexicon,while unknown words are mapped to all conceptswith equal probability.Following (Bo?rschinger et al 2011), sentencesin the training are restricted to analyses corre-sponding to their gold triple relations, and theinside-outside algorithm, a variant of EM, is ap-plied to learn the corresponding PCFG parame-ters.
In intuitive terms, the learning algorithm iter-1This work was supported by the Deutsche Forschungsge-meinschaft (German Research Foundation) in SFB 732 Incre-mental Specification in Context, project D2 (PI: Jonas Kuhn).We thank Thomas Mu?ller for help with the language models.atively maximizes the probability of rules that de-rive the correct triple relations in training, lookingover several examples.
For example, the unknownword are in Figure 1 is learned to indicate the re-lation object since it often occurs in training con-texts where this relation occurs between entitiessurrounding it.
The syntax of how triples are com-posed and ordered in the language is also learnedin an analogous way.We annotate the development data with the mostprobable trees predicted by the PCFG.
Figure 1shows the viterbi parse for the given sentence af-ter training.
Bascially, it defines a spanning treefor the knowledge graph given in the input.
Eachternary subtree indicates a triple relation detectedin the sentence, and the root node of this subtreespecifies the head (or first argument) of the triplerelation.
Note that some triple relations are notfound (e.g.
the base relation), since they are im-plicit in the language.3 Grammar and Lexicon ExtractionThe viterbi trees learned in the previous step forthe development set are used for constructing ageneration grammar that specifies the mapping be-tween triples and surface realizations.
The tree inFigure 1 indicates, for example, that the second ar-gument of an object relation can be realized to theleft of the relation and its first argument.
We alsolearn that the site relation can be lexicalized as thephrase in the.Grammar A non-lexical production in a treecorresponds to a surface realization of an inputtriple.
We iterate over all productions of the treesin the development data and aggregate counts ofconcept orderings over all instances of a relation.We distinguish preterminal concepts (preterm)that map to a lexical entry and nonterminal con-cepts (nonterm) that embed another subtree.
Ex-ample (1) and (2) illustrate rules that apply to thetree in Figure 1 for ordering the site and object re-lation.
The rule for object introduces ambiguity.Note that (2-a) deletes the object phrase.
(1) Input: (Anonterm,r-site,Bnonterm)202r-siter-site-head(left)e-lysosome-modifier]t2e-eukaryotic-cell]t3eukaryotic cellsr-has-partoft3[e-lysosomelysosomesr-sitetheint2[e-intracellular-digestion-heade-intracellular-digestion]t1digestedr-objectaret1[e-polymerpolymers:TRIPLES ( (|Intracellular-Digestion36204| |object| |Polymer36220|)(|Intracellular-Digestion36204| |base| |Eukaryotic-Cell36203|)(|Intracellular-Digestion36204| |site| |Lysosome36202|)(|Eukaryotic-Cell36203| |has-part| |Lysosome36202|))Figure 1: A semantic parse (top) learned from the the triples (bottom) provided during training.
Words/concepts in bold areknown from the lexicon, while the rest is learned along with the syntax of triple combination.
Triple instances in the tree aremarked with square brackets.a.
rhs: Anonterm r-site Bnonterm; 1.0(2) Input: (Apreterm,r-object,Bpreterm)a. rhs: Apreterm Bpreterm; 0.33b.
rhs: Bpreterm r-object Apreterm; 0.3c.
...Lexicon For each preterminal in the trees, weextract its lexical span in the surface sentence.
Forinstance, we extract 15 phrases as possible real-izations for the base relation (e.g.
?for the?, ?inthe?, ?of a?, ?from a?).
This is merged with theprovided lexicon, to create an expanded lexicon.4 Generation PipelineThe main idea of the generator is to produce a(possibly large) set of output candidates licensedby the grammar and the lexicon.
In a final step,these candidates are ranked with the help of alanguage model, a common approach in statisti-cal generation (Langkilde and Knight, 1998).
Wetrain our language model on the GENIA corpus(Ohta et al 2002).
Below is our overall pipeline.1.
compute all spanning trees licensed by the input triples2.
for each spanning tree from step 1, compute all surfacelinearizations licensed by the generation grammar3.
for each linearized tree from step 2, compute all surfacesentences licensed by the expanded lexicon4.
rank surface candidates with a language modelThe set of spanning trees produced in step 1 istypically small.
We prune the set of possible lin-earizations based on the counts in the generationgrammar, and consider only the two most likelyorderings for each input triple.
We also prune theset of possible lexicalizations and refine it withsome linguistic constraints described below.Linguistic Constraints The viterbi treeslearned in the alignment step do not capture anylinguistic properties of the sentences in terms ofmorpho-syntactic categories.
As a consequence,most of the output candidates coming from step 3are ungrammatical.
Ungrammatical sentences donot necessarily get low scores from the languagemodel as it captures local relations betweenneighbouring words.
We introduce some simplecandidate filters to ensure some basic linguisticconstraints.
With the help of the lexicon and someheuristics, we tag all lexical entries containing afinite verb.
In step 3, we filter all candidates thata) have no finite verb, b) have a finite verb as thefirst or last word, c) realize two finite verbs nextto each other.Conclusion We explore the use of SemanticParsing techniques, coupled with corpus-basedgeneration.
We expect that our prototype wouldbenefit from further development of the linguisticcomponents, given that it is built with minimal re-sources.ReferencesBo?rschinger, Benjamin, Jones, Bevan K, Johnson,Mark.
2011.
Reducing Grounded Learning toGrammatical Inference In Proc.
of EMNLP?11,pages 1416-1425.Irene Langkilde and Kevin Knight.
1998.
Generationthat exploits corpus-based statistical knowledge.
InProc.
of ACL 1998, pages 704?710.Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim.
2002.The genia corpus: an annotated research abstractcorpus in molecular biology domain.
In Proc.
ofHLT ?02, pages 82?86.203
