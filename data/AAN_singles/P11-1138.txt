Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1375?1384,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLocal and Global Algorithms for Disambiguation to WikipediaLev Ratinov 1 Dan Roth1 Doug Downey2 Mike Anderson31University of Illinois at Urbana-Champaign{ratinov2|danr}@uiuc.edu2Northwestern Universityddowney@eecs.northwestern.edu3Rexonomymrander@gmail.comAbstractDisambiguating concepts and entities in a con-text sensitive way is a fundamental problemin natural language processing.
The compre-hensiveness of Wikipedia has made the on-line encyclopedia an increasingly popular tar-get for disambiguation.
Disambiguation toWikipedia is similar to a traditional WordSense Disambiguation task, but distinct in thatthe Wikipedia link structure provides addi-tional information about which disambigua-tions are compatible.
In this work we analyzeapproaches that utilize this information to ar-rive at coherent sets of disambiguations for agiven document (which we call ?global?
ap-proaches), and compare them to more tradi-tional (local) approaches.
We show that previ-ous approaches for global disambiguation canbe improved, but even then the local disam-biguation provides a baseline which is veryhard to beat.1 IntroductionWikification is the task of identifying and link-ing expressions in text to their referent Wikipediapages.
Recently, Wikification has been shown toform a valuable component for numerous naturallanguage processing tasks including text classifica-tion (Gabrilovich and Markovitch, 2007b; Chang etal., 2008), measuring semantic similarity betweentexts (Gabrilovich and Markovitch, 2007a), cross-document co-reference resolution (Finin et al, 2009;Mayfield et al, 2009), and other tasks (Kulkarni etal., 2009).Previous studies on Wikification differ with re-spect to the corpora they address and the subsetof expressions they attempt to link.
For exam-ple, some studies focus on linking only named en-tities, whereas others attempt to link all ?interest-ing?
expressions, mimicking the link structure foundin Wikipedia.
Regardless, all Wikification systemsare faced with a key Disambiguation to Wikipedia(D2W) task.
In the D2W task, we?re given a textalong with explicitly identified substrings (calledmentions) to disambiguate, and the goal is to out-put the corresponding Wikipedia page, if any, foreach mention.
For example, given the input sen-tence ?I am visiting friends in <Chicago>,?
weoutput http://en.wikipedia.org/wiki/Chicago ?
theWikipedia page for the city of Chicago, Illinois, andnot (for example) the page for the 2002 film of thesame name.Local D2W approaches disambiguate each men-tion in a document separately, utilizing clues suchas the textual similarity between the document andeach candidate disambiguation?s Wikipedia page.Recent work on D2W has tended to focus on moresophisticated global approaches to the problem, inwhich all mentions in a document are disambiguatedsimultaneously to arrive at a coherent set of dis-ambiguations (Cucerzan, 2007; Milne and Witten,2008b; Han and Zhao, 2009).
For example, if amention of ?Michael Jordan?
refers to the computerscientist rather than the basketball player, then wewould expect a mention of ?Monte Carlo?
in thesame document to refer to the statistical techniquerather than the location.
Global approaches utilizethe Wikipedia link graph to estimate coherence.1375m1 = Taiwan m2 = China m3 = Jiangsu Province..............t1 = Taiwan t5 =People's Republic of China t7 = Jiangsu..............Document text  with mentionst2 = Chinese Taipei t3 =Republic of China t4 = China t6 = History of China?
(m1, t1)?
(m1, t2)?
(m1, t3)?
(t1, t7) ?
(t3, t7) ?
(t5, t7)Figure 1: Sample Disambiguation to Wikipedia problem with three mentions.
The mention ?Jiangsu?
is unambiguous.The correct mapping from mentions to titles is marked by heavy edgesIn this paper, we analyze global and local ap-proaches to the D2W task.
Our contributions areas follows: (1) We present a formulation of theD2W task as an optimization problem with local andglobal variants, and identify the strengths and theweaknesses of each, (2) Using this formulation, wepresent a new global D2W system, called GLOW.
Inexperiments on existing and novel D2W data sets,1GLOW is shown to outperform the previous state-of-the-art system of (Milne and Witten, 2008b), (3)We present an error analysis and identify the key re-maining challenge: determining when mentions re-fer to concepts not captured in Wikipedia.2 Problem Definition and ApproachWe formalize our Disambiguation to Wikipedia(D2W) task as follows.
We are given a documentd with a set of mentions M = {m1, .
.
.
,mN},and our goal is to produce a mapping from the setof mentions to the set of Wikipedia titles W ={t1, .
.
.
, t|W |}.
Often, mentions correspond to aconcept without a Wikipedia page; we treat this caseby adding a special null title to the set W .The D2W task can be visualized as finding amany-to-one matching on a bipartite graph, withmentions forming one partition and Wikipedia ti-tles the other (see Figure 1).
We denote the outputmatching as an N -tuple ?
= (t1, .
.
.
, tN ) where tiis the output disambiguation for mention mi.1The data sets are available for download athttp://cogcomp.cs.illinois.edu/Data2.1 Local and Global DisambiguationA local D2W approach disambiguates each men-tion mi separately.
Specifically, let ?
(mi, tj) be ascore function reflecting the likelihood that the can-didate title tj ?W is the correct disambiguation formi ?
M .
A local approach solves the followingoptimization problem:?
?local = argmax?N?i=1?
(mi, ti) (1)Local D2W approaches, exemplified by (Bunescuand Pasca, 2006) and (Mihalcea and Csomai, 2007),utilize ?
functions that assign higher scores to titleswith content similar to that of the input document.We expect, all else being equal, that the correctdisambiguations will form a ?coherent?
set of re-lated concepts.
Global approaches define a coher-ence function ?, and attempt to solve the followingdisambiguation problem:??
= argmax?[N?i=1?
(mi, ti) + ?(?)]
(2)The global optimization problem in Eq.
2 is NP-hard, and approximations are required (Cucerzan,2007).
The common approach is to utilize theWikipedia link graph to obtain an estimate pairwiserelatedness between titles ?
(ti, tj) and to efficientlygenerate a disambiguation context ?
?, a rough ap-proximation to the optimal ??.
We then solve theeasier problem:??
?
argmax?N?i=1[?
(mi, ti) +?tj????
(ti, tj)] (3)1376Eq.
3 can be solved by finding each ti and then map-ping mi independently as in a local approach, butstill enforces some degree of coherence among thedisambiguations.3 Related WorkWikipedia was first explored as an informationsource for named entity disambiguation and in-formation retrieval by Bunescu and Pasca (2006).There, disambiguation is performed using an SVMkernel that compares the lexical context around theambiguous named entity to the content of the can-didate disambiguation?s Wikipedia page.
However,since each ambiguous mention required a separateSVM model, the experiment was on a very limitedscale.
Mihalcea and Csomai applied Word SenseDisambiguation methods to the Disambiguation toWikipedia task (2007).
They experimented withtwo methods: (a) the lexical overlap between theWikipedia page of the candidate disambiguationsand the context of the ambiguous mention, and (b)training a Naive Bayes classiffier for each ambigu-ous mention, using the hyperlink information foundin Wikipedia as ground truth.
Both (Bunescu andPasca, 2006) and (Mihalcea and Csomai, 2007) fallinto the local framework.Subsequent work on Wikification has stressed thatassigned disambiguations for the same documentshould be related, introducing the global approach(Cucerzan, 2007; Milne and Witten, 2008b; Han andZhao, 2009; Ferragina and Scaiella, 2010).
The twocritical components of a global approach are the se-mantic relatedness function ?
between two titles,and the disambiguation context ??.
In (Milne andWitten, 2008b), the semantic context is defined tobe a set of ?unambiguous surface forms?
in the text,and the title relatedness ?
is computed as Normal-ized Google Distance (NGD) (Cilibrasi and Vitanyi,2007).2 On the other hand, in (Cucerzan, 2007) thedisambiguation context is taken to be all plausibledisambiguations of the named entities in the text,and title relatedness is based on the overlap in cat-egories and incoming links.
Both approaches havelimitations.
The first approach relies on the pres-2(Milne and Witten, 2008b) also weight each mention in ?
?by its estimated disambiguation utility, which can be modeledby augmenting ?
on per-problem basis.ence of unambiguous mentions in the input docu-ment, and the second approach inevitably adds ir-relevant titles to the disambiguation context.
As wedemonstrate in our experiments, by utilizing a moreaccurate disambiguation context, GLOW is able toachieve better performance.4 System ArchitectureIn this section, we present our global D2W system,which solves the optimization problem in Eq.
3.
Werefer to the system as GLOW, for Global Wikifica-tion.
We use GLOW as a test bed for evaluating localand global approaches for D2W.
GLOW combinesa powerful local model ?
with an novel methodfor choosing an accurate disambiguation context ?
?,which as we show in our experiments allows it tooutperform the previous state of the art.We represent the functions ?
and ?
as weightedsums of features.
Specifically, we set:?
(m, t) =?iwi?i(m, t) (4)where each feature ?i(m, t) captures some aspectof the relatedness between the mention m and theWikipedia title t. Feature functions ?i(t, t?)
are de-fined analogously.
We detail the specific featurefunctions utilized in GLOW in following sections.The coefficients wi are learned using a Support Vec-tor Machine over bootstrapped training data fromWikipedia, as described in Section 4.5.At a high level, the GLOW system optimizes theobjective function in Eq.
3 in a two-stage process.We first execute a ranker to obtain the best non-nulldisambiguation for each mention in the document,and then execute a linker that decides whether themention should be linked to Wikipedia, or whetherinstead switching the top-ranked disambiguation tonull improves the objective function.
As our exper-iments illustrate, the linking task is the more chal-lenging of the two by a significant margin.Figure 2 provides detailed pseudocode for GLOW.Given a document d and a set of mentions M , westart by augmenting the set of mentions with allphrases in the document that could be linked toWikipedia, but were not included in M .
Introducingthese additional mentions provides context that maybe informative for the global coherence computation(it has no effect on local approaches).
In the second1377Algorithm: Disambiguate to WikipediaInput: document d, Mentions M = {m1, .
.
.
,mN}Output: a disambiguation ?
= (t1, .
.
.
, tN ).1) Let M ?
= M?
{ Other potential mentions in d}2) For each mention m?i ?
M ?, construct a set of disam-biguation candidates Ti = {ti1, .
.
.
, tiki}, tij 6= null3) Ranker: Find a solution ?
= (t?1, .
.
.
, t?|M?|), wheret?i ?
Ti is the best non-null disambiguation of m?i.4) Linker: For each m?i, map t?i to null in ?
iff doing soimproves the objective function5) Return ?
entries for the original mentions M .Figure 2: High-level pseudocode for GLOW.step, we construct for each mention mi a limited setof candidate Wikipedia titles Ti thatmi may refer to.Considering only a small subset of Wikipedia titlesas potential disambiguations is crucial for tractabil-ity (we detail which titles are selected below).
In thethird step, the ranker outputs the most appropriatenon-null disambiguation ti for each mention mi.In the final step, the linker decides whether thetop-ranked disambiguation is correct.
The disam-biguation (mi, ti) may be incorrect for several rea-sons: (1) mention mi does not have a correspondingWikipedia page, (2) mi does have a correspondingWikipedia page, but it was not included in Ti, or(3) the ranker erroneously chose an incorrect disam-biguation over the correct one.In the below sections, we describe each step of theGLOW algorithm, and the local and global featuresutilized, in detail.
Because we desire a system thatcan process documents at scale, each step requirestrade-offs between accuracy and efficiency.4.1 Disambiguation Candidates GenerationThe first step in GLOW is to extract all mentions thatcan refer to Wikipedia titles, and to construct a setof disambiguation candidates for each mention.
Fol-lowing previous work, we use Wikipedia hyperlinksto perform these steps.
GLOW utilizes an anchor-title index, computed by crawling Wikipedia, thatmaps each distinct hyperlink anchor text to its tar-get Wikipedia titles.
For example, the anchor text?Chicago?
is used in Wikipedia to refer both to thecity in Illinois and to the movie.
Anchor texts in theindex that appear in document d are used to supple-ment the mention setM in Step 1 of the GLOW algo-rithm in Figure 2.
Because checking all substringsBaseline Feature: P (t|m), P (t)Local Features: ?i(t,m)cosine-sim(Text(t),Text(m)) : Naive/Reweightedcosine-sim(Text(t),Context(m)): Naive/Reweightedcosine-sim(Context(t),Text(m)): Naive/Reweightedcosine-sim(Context(t),Context(m)): Naive/ReweightedGlobal Features: ?i(ti, tj)I[ti?tj ]?PMI(InLinks(ti),InLinks(tj)) : avg/maxI[ti?tj ]?NGD(InLinks(ti),InLinks(tj)) : avg/maxI[ti?tj ]?PMI(OutLinks(ti),OutLinks(tj)) : avg/maxI[ti?tj ]?NGD(OutLinks(ti),OutLinks(tj)) : avg/maxI[ti?tj ] : avg/maxI[ti?tj ]?PMI(InLinks(ti),InLinks(tj)) : avg/maxI[ti?tj ]?NGD(InLinks(ti),InLinks(tj)) : avg/maxI[ti?tj ]?PMI(OutLinks(ti),OutLinks(tj)) : avg/maxI[ti?tj ]?NGD(OutLinks(ti),OutLinks(tj)) : avg/maxTable 1: Ranker features.
I[ti?tj ] is an indicator variablewhich is 1 iff ti links to tj or vise-versa.
I[ti?tj ] is 1 iffthe titles point to each other.in the input text against the index is computation-ally inefficient, we instead prune the search spaceby applying a publicly available shallow parser andnamed entity recognition system.3 We consider onlythe expressions marked as named entities by theNER tagger, the noun-phrase chunks extracted bythe shallow parser, and all sub-expressions of up to5 tokens of the noun-phrase chunks.To retrieve the disambiguation candidates Ti fora given mention mi in Step 2 of the algorithm, wequery the anchor-title index.
Ti is taken to be theset of titles most frequently linked to with anchortext mi in Wikipedia.
For computational efficiency,we utilize only the top 20 most frequent target pagesfor the anchor text; the accuracy impact of this opti-mization is analyzed in Section 6.From the anchor-title index, we compute two lo-cal features ?i(m, t).
The first, P (t|m), is the frac-tion of times the title t is the target page for an an-chor text m. This single feature is a very reliableindicator of the correct disambiguation (Fader et al,2009), and we use it as a baseline in our experiments.The second, P (t), gives the fraction of all Wikipediaarticles that link to t.4.2 Local Features ?In addition to the two baseline features mentioned inthe previous section, we compute a set of text-based3Available at http://cogcomp.cs.illinois.edu/page/software.1378local features ?(t,m).
These features capture the in-tuition that a given Wikipedia title t is more likely tobe referred to by mention m appearing in documentd if the Wikipedia page for t has high textual simi-larity to d, or if the context surrounding hyperlinksto t are similar to m?s context in d.For each Wikipedia title t, we construct a top-200 token TF-IDF summary of the Wikipedia paget, which we denote as Text(t) and a top-200 to-ken TF-IDF summary of the context within whicht was hyperlinked to in Wikipedia, which we denoteas Context(t).
We keep the IDF vector for all to-kens in Wikipedia, and given an input mention m ina document d, we extract the TF-IDF representationof d, which we denote Text(d), and a TF-IDF rep-resentation of a 100-token window around m, whichwe denote Context(m).
This allows us to definefour local features described in Table 1.We additionally compute weighted versions ofthe features described above.
Error analysis hasshown that in many cases the summaries of the dif-ferent disambiguation candidates for the same sur-face form s were very similar.
For example, con-sider the disambiguation candidates of ?China?
andtheir TF-IDF summaries in Figure 1.
The major-ity of the terms selected in all summaries refer tothe general issues related to China, such as ?legal-ism, reform, military, control, etc.
?, while a minorityof the terms actually allow disambiguation betweenthe candidates.
The problem stems from the factthat the TF-IDF summaries are constructed againstthe entire Wikipedia, and not against the confusionset of disambiguation candidates of m. Therefore,we re-weigh the TF-IDF vectors using the TF-IDFscheme on the disambiguation candidates as a ad-hoc document collection, similarly to an approachin (Joachims, 1997) for classifying documents.
Inour scenario, the TF of the a token is the originalTF-IDF summary score (a real number), and the IDFterm is the sum of all the TF-IDF scores for the to-ken within the set of disambiguation candidates form.
This adds 4 more ?reweighted local?
features inTable 1.4.3 Global Features ?Global approaches require a disambiguation context??
and a relatedness measure ?
in Eq.
3.
In this sec-tion, we describe our method for generating a dis-ambiguation context, and the set of global features?i(t, t?)
forming our relatedness measure.In previous work, Cucerzan defined the disam-biguation context as the union of disambiguationcandidates for all the named entity mentions in theinput document (2007).
The disadvantage of this ap-proach is that irrelevant titles are inevitably added tothe disambiguation context, creating noise.
Milneand Witten, on the other hand, use a set of un-ambiguous mentions (2008b).
This approach uti-lizes only a fraction of the available mentions forcontext, and relies on the presence of unambigu-ous mentions with high disambiguation utility.
InGLOW, we utilize a simple and efficient alternativeapproach: we first train a local disambiguation sys-tem, and then use the predictions of that system asthe disambiguation context.
The advantage of thisapproach is that unlike (Milne and Witten, 2008b)we use all the available mentions in the document,and unlike (Cucerzan, 2007) we reduce the amountof irrelevant titles in the disambiguation context bytaking only the top-ranked disambiguation per men-tion.Our global features are refinements of previouslyproposed semantic relatedness measures betweenWikipedia titles.
We are aware of two previousmethods for estimating the relatedness between twoWikipedia concepts: (Strube and Ponzetto, 2006),which uses category overlap, and (Milne and Wit-ten, 2008a), which uses the incoming link structure.Previous work experimented with two relatednessmeasures: NGD, and Specificity-weighted CosineSimilarity.
Consistent with previous work, we foundNGD to be the better-performing of the two.
Thuswe use only NGD along with a well-known Pon-twise Mutual Information (PMI) relatedness mea-sure.
Given a Wikipedia title collection W , titlest1 and t2 with a set of incoming links L1, and L2respectively, PMI and NGD are defined as follows:NGD(L1, L2) = Log(Max(|L1|, |L2|))?
Log(|L1 ?
L2|)Log(|W |)?
Log(Min(|L1|, |L2|))PMI(L1, L2) = |L1 ?
L2|/|W ||L1|/|W ||L2|/|W |The NGD and the PMI measures can also be com-puted over the set of outgoing links, and we includethese as features as well.
We also included a fea-ture indicating whether the articles each link to one1379another.
Lastly, rather than taking the sum of the re-latedness scores as suggested by Eq.
3, we use twofeatures: the average and the maximum relatednessto ??.
We expect the average to be informative formany documents.
The intuition for also includingthe maximum relatedness is that for longer docu-ments that may cover many different subtopics, themaximum may be more informative than the aver-age.We have experimented with other semantic fea-tures, such as category overlap or cosine similar-ity between the TF-IDF summaries of the titles, butthese did not improve performance in our experi-ments.
The complete set of global features used inGLOW is given in Table 1.4.4 Linker FeaturesGiven the mention m and the top-ranked disam-biguation t, the linker attempts to decide whether t isindeed the correct disambiguation of m. The linkerincludes the same features as the ranker, plus addi-tional features we expect to be particularly relevantto the task.
We include the confidence of the rankerin t with respect to second-best disambiguation t?,intended to estimate whether the ranker may havemade a mistake.
We also include several propertiesof the mention m: the entropy of the distributionP (t|m), the percent of Wikipedia titles in which mappears hyperlinked versus the percent of times mappears as plain text, whether m was detected byNER as a named entity, and a Good-Turing estimateof how likely m is to be out-of-Wikipedia conceptbased on the counts in P (t|m).4.5 Linker and Ranker TrainingWe train the coefficients for the ranker features us-ing a linear Ranking Support Vector Machine, usingtraining data gathered from Wikipedia.
Wikipedialinks are considered gold-standard links for thetraining process.
The methods for compiling theWikipedia training corpus are given in Section 5.We train the linker as a separate linear SupportVector Machine.
Training data for the linker is ob-tained by applying the ranker on the training set.
Thementions for which the top-ranked disambiguationdid not match the gold disambiguation are treatedas negative examples, while the mentions the rankergot correct serve as positive examples.Mentions/Distinct titlesdata set Gold Identified SolvableACE 257/255 213/212 185/184MSNBC 747/372 530/287 470/273AQUAINT 727/727 601/601 588/588Wikipedia 928/813 855/751 843/742Table 2: Number of mentions and corresponding dis-tinct titles by data set.
Listed are (number of men-tions)/(number of distinct titles) for each data set, for eachof three mention types.
Gold mentions include all dis-ambiguated mentions in the data set.
Identified mentionsare gold mentions whose correct disambiguations exist inGLOW?s author-title index.
Solvable mentions are identi-fied mentions whose correct disambiguations are amongthe candidates selected by GLOW (see Table 3).5 Data sets and Evaluation MethodologyWe evaluate GLOW on four data sets, of whichtwo are from previous work.
The first data set,from (Milne and Witten, 2008b), is a subset of theAQUAINT corpus of newswire text that is annotatedto mimic the hyperlink structure in Wikipedia.
Thatis, only the first mentions of ?important?
titles werehyperlinked.
Titles deemed uninteresting and re-dundant mentions of the same title are not linked.The second data set, from (Cucerzan, 2007), is takenfrom MSNBC news and focuses on disambiguatingnamed entities after running NER and co-referenceresolution systems on newsire text.
In this case,all mentions of all the detected named entities arelinked.We also constructed two additional data sets.
Thefirst is a subset of the ACE co-reference data set,which has the advantage that mentions and theirtypes are given, and the co-reference is resolved.
Weasked annotators on Amazon?s Mechanical Turk tolink the first nominal mention of each co-referencechain to Wikipedia, if possible.
Finding the accu-racy of a majority vote of these annotations to beapproximately 85%, we manually corrected the an-notations to obtain ground truth for our experiments.The second data set we constructed, Wiki, is a sam-ple of paragraphs from Wikipedia pages.
Mentionsin this data set correspond to existing hyperlinks inthe Wikipedia text.
Because Wikipedia editors ex-plicitly link mentions to Wikipedia pages, their an-chor text tends to match the title of the linked-to-page?as a result, in the overwhelming majority of1380cases, the disambiguation decision is as trivial asstring matching.
In an attempt to generate morechallenging data, we extracted 10,000 random para-graphs for which choosing the top disambiguationaccording to P (t|m) results in at least a 10% rankererror rate.
40 paragraphs of this data was utilized fortesting, while the remainder was used for training.The data sets are summarized in Table 2.
The ta-ble shows the number of annotated mentions whichwere hyperlinked to non-null Wikipedia pages, andthe number of titles in the documents (withoutcounting repetitions).
For example, the AQUAINTdata set contains 727 mentions,4 all of which referto distinct titles.
The MSNBC data set contains 747mentions mapped to non-null Wikipedia pages, butsome mentions within the same document refer tothe same titles.
There are 372 titles in the data set,when multiple instances of the same title within onedocument are not counted.To isolate the performance of the individual com-ponents of GLOW, we use multiple distinct metricsfor evaluation.
Ranker accuracy, which measuresthe performance of the ranker alone, is computedonly over those mentions with a non-null gold dis-ambiguation that appears in the candidate set.
It isequal to the fraction of these mentions for which theranker returns the correct disambiguation.
Thus, aperfect ranker should achieve a ranker accuracy of1.0, irrespective of limitations of the candidate gen-erator.
Linker accuracy is defined as the fraction ofall mentions for which the linker outputs the correctdisambiguation (note that, when the title producedby the ranker is incorrect, this penalizes linker accu-racy).
Lastly, we evaluate our whole system againstother baselines using a previously-employed ?bag oftitles?
(BOT) evaluation (Milne and Witten, 2008b).In BOT, we compare the set of titles output for a doc-ument with the gold set of titles for that document(ignoring duplicates), and utilize standard precision,recall, and F1 measures.In BOT, the set of titles is collected from the men-tions hyperlinked in the gold annotation.
That is,if the gold annotation is { (China, People?s Repub-lic of China), (Taiwan, Taiwan), (Jiangsu, Jiangsu)}4The data set contains votes on how important the mentionsare.
We believe that the results in (Milne and Witten, 2008b)were reported on mentions which the majority of annotatorsconsidered important.
In contrast, we used all the mentions.Generated data setsCandidates k ACE MSNBC AQUAINT Wiki1 81.69 72.26 91.01 84.793 85.44 86.22 96.83 94.735 86.38 87.35 97.17 96.3720 86.85 88.67 97.83 98.59Table 3: Percent of ?solvable?
mentions as a functionof the number of generated disambiguation candidates.Listed is the fraction of identified mentions m whosetarget disambiguation t is among the top k candidatesranked in descending order of P (t|m).and the predicted anotation is: { (China, People?sRepublic of China), (China, History of China), (Tai-wan, null), (Jiangsu, Jiangsu), (republic, Govern-ment)} , then the BOT for the gold annotation is:{People?s Republic of China, Taiwan, Jiangsu} , andthe BOT for the predicted annotation is: {People?sRepublic of China, History of China, Jiangsu} .
Thetitle Government is not included in the BOT for pre-dicted annotation, because its associate mention re-public did not appear as a mention in the gold anno-tation.
Both the precision and the recall of the aboveprediction is 0.66.
We note that in the BOT evalua-tion, following (Milne and Witten, 2008b) we con-sider all the titles within a document, even if somethe titles were due to mentions we failed to identify.56 Experiments and ResultsIn this section, we evaluate and analyze GLOW?sperformance on the D2W task.
We begin by eval-uating the mention detection component (Step 1 ofthe algorithm).
The second column of Table 2 showshow many of the ?non-null?
mentions and corre-sponding titles we could successfully identify (e.g.out of 747 mentions in the MSNBC data set, only530 appeared in our anchor-title index).
Missing en-tities were primarily due to especially rare surfaceforms, or sometimes due to idiosyncratic capitaliza-tion in the corpus.
Improving the number of iden-tified mentions substantially is non-trivial; (Zhou etal., 2010) managed to successfully identify only 59more entities than we do in the MSNBC data set, us-ing a much more powerful detection method basedon search engine query logs.We generate disambiguation candidates for a5We evaluate the mention identification stage in Section 6.1381Data setsFeatures ACE MSNBC AQUAINT WikiP (t|m) 94.05 81.91 93.19 85.88P (t|m)+LocalNaive 95.67 84.04 94.38 92.76Reweighted 96.21 85.10 95.57 93.59All above 95.67 84.68 95.40 93.59P (t|m)+GlobalNER 96.21 84.04 94.04 89.56Unambiguous 94.59 84.46 95.40 89.67Predictions 96.75 88.51 95.91 89.79P (t|m)+Local+GlobalAll features 97.83 87.02 94.38 94.18Table 4: Ranker Accuracy.
Bold values indicate thebest performance in each feature group.
The global ap-proaches marginally outperform the local approaches onranker accuracy , while combing the approaches leads tofurther marginal performance improvement.mention m using an anchor-title index, choosingthe 20 titles with maximal P (t|m).
Table 3 eval-uates the accuracy of this generation policy.
Wereport the percent of mentions for which the cor-rect disambiguation is generated in the top k can-didates (called ?solvable?
mentions).
We see thatthe baseline prediction of choosing the disambigua-tion t which maximizes P (t|m) is very strong (80%of the correct mentions have maximal P (t|m) in alldata sets except MSNBC).
The fraction of solvablementions increases until about five candidates permention are generated, after which the increase israther slow.
Thus, we believe choosing a limit of 20candidates per mention offers an attractive trade-offof accuracy and efficiency.
The last column of Ta-ble 2 reports the number of solvable mentions andthe corresponding number of titles with a cutoff of20 disambiguation candidates, which we use in ourexperiments.Next, we evaluate the accuracy of the ranker.
Ta-ble 4 compares the ranker performance with base-line, local and global features.
The reweighted lo-cal features outperform the unweighted (?Naive?
)version, and the global approach outperforms thelocal approach on all data sets except Wikipedia.As the table shows, our approach of defining thedisambiguation context to be the predicted dis-ambiguations of a simpler local model (?Predic-tions?)
performs better than using NER entities asin (Cucerzan, 2007), or only the unambiguous enti-Data set Local Global Local+GlobalACE 80.1 ?
82.8 80.6 ?
80.6 81.5 ?
85.1MSNBC 74.9 ?
76.0 77.9 ?
77.9 76.5 ?
76.9AQUAINT 93.5 ?
91.5 93.8 ?
92.1 92.3 ?
91.3Wiki 92.2 ?
92.0 88.5 ?
87.2 92.8 ?
92.6Table 5: Linker performance.
The notation X ?
Ymeans that when linking all mentions, the linking accu-racy is X , while when applying the trained linker, theperformance is Y .
The local approaches are better suitedfor linking than the global approaches.
The linking accu-racy is very sensitive to domain changes.System ACE MSNBC AQUAINT WikiBaseline: P (t|m) 69.52 72.83 82.67 81.77GLOW Local 75.60 74.39 84.52 90.20GLOW Global 74.73 74.58 84.37 86.62GLOW 77.25 74.88 83.94 90.54M&W 72.76 68.49 83.61 80.32Table 6: End systems performance - BOT F1.
The per-formance of the full system (GLOW) is similar to that ofthe local version.
GLOW outperforms (Milne and Witten,2008b) on all data sets.ties as in (Milne and Witten, 2008b).6 Combiningthe local and the global approaches typically resultsin minor improvements.While the global approaches are most effective forranking, the linking problem has different charac-teristics as shown in Table 5.
We can see that theglobal features are not helpful in general for predict-ing whether the top-ranked disambiguation is indeedthe correct one.Further, although the trained linker improves ac-curacy in some cases, the gains are marginal?andthe linker decreases performance on some data sets.One explanation for the decrease is that the linkeris trained on Wikipedia, but is being tested on non-Wikipedia text which has different characteristics.However, in separate experiments we found thattraining a linker on out-of-Wikipedia text only in-creased test set performance by approximately 3percentage points.
Clearly, while ranking accuracyis high overall, different strategies are needed toachieve consistently high linking performance.A few examples from the ACE data set help il-6In NER we used only the top prediction, because using allcandidates as in (Cucerzan, 2007) proved prohibitively ineffi-cient.1382lustrate the tradeoffs between local and global fea-tures in GLOW.
The global system mistakenly links?<Dorothy Byrne>, a state coordinator for theFlorida Green Party, said .
.
.
?
to the British jour-nalist, because the journalist sense has high coher-ence with other mentions in the newswire text.
How-ever, the local approach correctly maps the men-tion to null because of a lack of local contextualclues.
On the other hand, in the sentence ?In-stead of Los Angeles International, for example,consider flying into <Burbank> or John Wayne Air-port in Orange County, Calif.?, the local rankerlinks the mention Burbank to Burbank, California,while the global system correctly maps the entity toBob Hope Airport, because the three airports men-tioned in the sentence are highly related to one an-other.Lastly, in Table 6 we compare the end systemBOT F1 performance.
The local approach provesa very competitive baseline which is hard to beat.Combining the global and the local approach leadsto marginal improvements.
The full GLOW sys-tem outperforms the existing state-of-the-art systemfrom (Milne and Witten, 2008b), denoted as M&W,on all data sets.
We also compared our system withthe recent TAGME Wikification system (Ferraginaand Scaiella, 2010).
However, TAGME is designedfor a different setting than ours: extremely shorttexts, like Twitter posts.
The TAGME RESTful APIwas unable to process some of our documents atonce.
We attempted to input test documents one sen-tence at a time, disambiguating each sentence inde-pendently, which resulted in poor performance (0.07points in F1 lower than the P (t|m) baseline).
Thishappened mainly because the same mentions werelinked to different titles in different sentences, lead-ing to low precision.An important question is why M&W underper-forms the baseline on the MSNBC and Wikipediadata sets.
In an error analysis, M&W performedpoorly on the MSNBC data not due to poor disam-biguations, but instead because the data set containsonly named entities, which were often delimited in-correctly by M&W.
Wikipedia was challenging fora different reason: M&W performs less well on theshort (one paragraph) texts in that set, because theycontain relatively few of the unambiguous entitiesthe system relies on for disambiguation.7 ConclusionsWe have formalized the Disambiguation toWikipedia (D2W) task as an optimization problemwith local and global variants, and analyzed thestrengths and weaknesses of each.
Our experimentsrevealed that previous approaches for global disam-biguation can be improved, but even then the localdisambiguation provides a baseline which is veryhard to beat.As our error analysis illustrates, the primary re-maining challenge is determining when a mentiondoes not have a corresponding Wikipedia page.Wikipedia?s hyperlinks offer a wealth of disam-biguated mentions that can be leveraged to traina D2W system.
However, when compared withmentions from general text, Wikipedia mentionsare disproportionately likely to have correspondingWikipedia pages.
Our initial experiments suggestthat accounting for this bias requires more than sim-ply training a D2W system on a moderate num-ber of examples from non-Wikipedia text.
Apply-ing distinct semi-supervised and active learning ap-proaches to the task is a primary area of future work.AcknowledgmentsThis research supported by the Army ResearchLaboratory (ARL) under agreement W911NF-09-2-0053 and by the Defense Advanced ResearchProjects Agency (DARPA) Machine Reading Pro-gram under Air Force Research Laboratory (AFRL)prime contract no.
FA8750-09-C-0181.
The thirdauthor was supported by a Microsoft New FacultyFellowship.
Any opinions, findings, conclusions orrecommendations are those of the authors and do notnecessarily reflect the view of the ARL, DARPA,AFRL, or the US government.ReferencesR.
Bunescu and M. Pasca.
2006.
Using encyclope-dic knowledge for named entity disambiguation.
InProceedings of the 11th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL-06), Trento, Italy, pages 9?16, April.Ming-Wei Chang, Lev Ratinov, Dan Roth, and VivekSrikumar.
2008.
Importance of semantic represen-tation: dataless classification.
In Proceedings of the138323rd national conference on Artificial intelligence -Volume 2, pages 830?835.
AAAI Press.Rudi L. Cilibrasi and Paul M. B. Vitanyi.
2007.
Thegoogle similarity distance.
IEEE Trans.
on Knowl.
andData Eng., 19(3):370?383.Silviu Cucerzan.
2007.
Large-scale named entity dis-ambiguation based on Wikipedia data.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), pages708?716, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Anthony Fader, Stephen Soderland, and Oren Etzioni.2009.
Scaling wikipedia-based named entity disam-biguation to arbitrary web text.
In Proceedings ofthe WikiAI 09 - IJCAI Workshop: User ContributedKnowledge and Artificial Intelligence: An EvolvingSynergy, Pasadena, CA, USA, July.Paolo Ferragina and Ugo Scaiella.
2010.
Tagme: on-the-fly annotation of short text fragments (by wikipediaentities).
In Jimmy Huang, Nick Koudas, Gareth J. F.Jones, Xindong Wu, Kevyn Collins-Thompson, andAijun An, editors, Proceedings of the 19th ACM con-ference on Information and knowledge management,pages 1625?1628.
ACM.Tim Finin, Zareen Syed, James Mayfield, Paul Mc-Namee, and Christine Piatko.
2009.
Using Wikitol-ogy for Cross-Document Entity Coreference Resolu-tion.
In Proceedings of the AAAI Spring Symposiumon Learning by Reading and Learning to Read.
AAAIPress, March.Evgeniy Gabrilovich and Shaul Markovitch.
2007a.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
In Proceedings of the20th international joint conference on Artifical intel-ligence, pages 1606?1611, San Francisco, CA, USA.Morgan Kaufmann Publishers Inc.Evgeniy Gabrilovich and Shaul Markovitch.
2007b.Harnessing the expertise of 70,000 human editors:Knowledge-based feature generation for text catego-rization.
J. Mach.
Learn.
Res., 8:2297?2345, Decem-ber.Xianpei Han and Jun Zhao.
2009.
Named entity dis-ambiguation by leveraging wikipedia semantic knowl-edge.
In Proceeding of the 18th ACM conference onInformation and knowledge management, CIKM ?09,pages 215?224, New York, NY, USA.
ACM.Thorsten Joachims.
1997.
A probabilistic analysis ofthe rocchio algorithm with tfidf for text categoriza-tion.
In Proceedings of the Fourteenth InternationalConference on Machine Learning, ICML ?97, pages143?151, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, andSoumen Chakrabarti.
2009.
Collective annotationof wikipedia entities in web text.
In Proceedingsof the 15th ACM SIGKDD international conferenceon Knowledge discovery and data mining, KDD ?09,pages 457?466, New York, NY, USA.
ACM.James Mayfield, David Alexander, Bonnie Dorr, JasonEisner, Tamer Elsayed, Tim Finin, Clay Fink, Mar-jorie Freedman, Nikesh Garera, James Mayfield, PaulMcNamee, Saif Mohammad, Douglas Oard, Chris-tine Piatko, Asad Sayeed, Zareen Syed, and RalphWeischede.
2009.
Cross-Document Coreference Res-olution: A Key Technology for Learning by Reading.In Proceedings of the AAAI 2009 Spring Symposiumon Learning by Reading and Learning to Read.
AAAIPress, March.Rada Mihalcea and Andras Csomai.
2007.
Wikify!
: link-ing documents to encyclopedic knowledge.
In Pro-ceedings of the sixteenth ACM conference on Con-ference on information and knowledge management,CIKM ?07, pages 233?242, New York, NY, USA.ACM.David Milne and Ian H. Witten.
2008a.
An effec-tive, low-cost measure of semantic relatedness ob-tained from wikipedia links.
In In the Wikipedia andAI Workshop of AAAI.David Milne and Ian H. Witten.
2008b.
Learning to linkwith wikipedia.
In Proceedings of the 17th ACM con-ference on Information and knowledge management,CIKM ?08, pages 509?518, New York, NY, USA.ACM.Michael Strube and Simone Paolo Ponzetto.
2006.Wikirelate!
computing semantic relatedness usingwikipedia.
In proceedings of the 21st national confer-ence on Artificial intelligence - Volume 2, pages 1419?1424.
AAAI Press.Yiping Zhou, Lan Nie, Omid Rouhani-Kalleh, FlavianVasile, and Scott Gaffney.
2010.
Resolving surfaceforms to wikipedia topics.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics (Coling 2010), pages 1335?1343, Beijing, China,August.
Coling 2010 Organizing Committee.1384
