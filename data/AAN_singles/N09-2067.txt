Proceedings of NAACL HLT 2009: Short Papers, pages 265?268,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAnchored Speech Recognition for Question AnsweringSibel Yaman1, Gokhan Tur2, Dimitra Vergyri2, Dilek Hakkani-Tur1,Mary Harper3 and Wen Wang21 International Computer Science Institute2 SRI International3 Hopkins HLT Center of Excellence, University of Maryland{sibel,dilek}@icsi.berkeley.edu,{gokhan,dverg,wwang}@speech.sri.com,mharper@casl.umd.eduAbstractIn this paper, we propose a novel questionanswering system that searches for responsesfrom spoken documents such as broadcastnews stories and conversations.
We propose anovel two-step approach, which we refer to asanchored speech recognition, to improve thespeech recognition of the sentence that sup-ports the answer.
In the first step, the sen-tence that is highly likely to contain the an-swer is retrieved among the spoken data thathas been transcribed using a generic automaticspeech recognition (ASR) system.
This candi-date sentence is then re-recognized in the sec-ond step by constraining the ASR search spaceusing the lexical information in the question.Our analysis showed that ASR errors causeda 35% degradation in the performance of thequestion answering system.
Experiments withthe proposed anchored recognition approachindicated a significant improvement in the per-formance of the question answering module,recovering 30% of the answers erroneous dueto ASR.1 IntroductionIn this paper, we focus on finding answers to userquestions from spoken documents, such as broad-cast news stories and conversations.
In a typicalquestion answering system, the user query is firstprocessed by an information retrieval (IR) system,which finds out the most relevant documents amongmassive document collections.
Each sentence inthese relevant documents is processed to determinewhether or not it answers user questions.
Once acandidate sentence is determined, it is further pro-cessed to extract the exact answer.Answering factoid questions (i.e., questions like?What is the capital of France??)
using web makesuse of the redundancy of information (Whittaker etal., 2006).
However, when the document collectionis not large and when the queries are complex, asin the task we focus on in this paper, more sophis-ticated syntactic, semantic, and contextual process-ing of documents and queries is performed to ex-tract or construct the answer.
Although much of thework on question answering has been focused onwritten texts, many emerging systems also enableeither spoken queries or spoken document collec-tions (Lamel et al, 2008).
The work we describein this paper also uses spoken data collections toanswer user questions but our focus is on improv-ing speech recognition quality of the documents bymaking use of the wording in the queries.
Considerthe following example:Manual transcription: We understand from Greek of-ficials here that it was a Russian-made rocket which isavailable in many countries but certainly not a weaponused by the Greek militaryASR transcription: to stand firm greek officials here thathe was a a russian made rocket uh which is available inmany countries but certainly not a weapon used by hegreat momentsQuestion: What is certainly not a weapon used by theGreek military?Answer: a Russian-made rocketAnswering such questions requires as good ASRtranscriptions as possible.
In many cases, though,there is one generic ASR system and a generic lan-guage model to use.
The approach proposed in thispaper attempts to improve the ASR performance byre-recognizing the candidate sentence using lexicalinformation from the given question.
The motiva-265tion is that the question and the candidate sentenceshould share some common words, and thereforethe words of the answer sentence can be estimatedfrom the given question.
For example, given a fac-toid question such as: ?What is the tallest build-ing in the world?
?, the sentence containing its an-swer is highly likely to include word sequences suchas: ?The tallest building in the world is NAME?or ?NAME, the highest building in the world, ...?,where NAME is the exact answer.Once the sentence supporting the answer is lo-cated, it is re-recognized such that the candidate an-swer is constrained to include parts of the questionword sequence.
To achieve this, a word network isformed to match the answer sentence to the givenquestion.
Since the question words are taken as a ba-sis to re-recognize the best-candidate sentence, thequestion acts as an anchor, and therefore, we callthis approach anchored recognition.In this work, we restrict out attention to questionsabout the subject, the object and the locative, tempo-ral, and causative arguments.
For instance, the fol-lowings are the questions of interest for the sentenceObama invited Clinton to the White House to discussthe recent developments:Who invited Clinton to the White House?Who did Obama invite to the White House?Why did Obama invite Clinton to the White House?2 Sentence ExtractionThe goal in sentence extraction is determining thesentence that is most likely to contain the answerto the given question.
Our sentence extractor relieson non-stop word n-gram match between the ques-tion and the candidate sentence, and returns the sen-tence with the largest weighted average.
Since notall word n-grams have the same importance (e.g.function vs. content words), we perform a weightedsum as typically done in the IR literature, i.e., thematching n-grams are weighted with respect to theirinverse document frequency (IDF) and length.A major concern for accurate sentence extractionis the robustness to speech recognition errors.
An-other concern is dealing with alternative word se-quences for expressing the same meaning.
To tacklethe second challenge, one can also include syn-onyms, and compare paraphrases of the question andthe candidate answer.
Since our main focus is on ro-PredicateSentence ExtractionSemantic RolesAnswering SentenceAnswer Extraction Anchored RecognitionDocumentSpeech RecognitionQuestionSearched ArgumentBaselineProposedFigure 1: Conceptual scheme of the baseline and pro-posed information distillation system.bustness to speech recognition errors, our data setis limited to those questions that are worded verysimilarly to the candidate answers.
However, theapproach is more general, and can be extended totackle both challenges.3 Answer ExtractionWhen the answer is to be extracted from ASR out-put, the exact answers can be erroneous because (1)the exact answer phrase might be misrecognized, (2)other parts of the sentence might be misrecognized,so the exact answer cannot be extracted either be-cause parser fails or because the sentence cannotmatch the query.The question in the example in the Introductionsection is concerned with the object of the predicate?is?
rather than of the other predicates ?understand?or ?was?.
Therefore, a pre-processing step is neededto correctly identify the object (in this example) thatis being asked, which is described next.Once the best candidate sentence is estimated, asyntactic parser (Harper and Huang, ) that also out-puts function tags is used to parse both the ques-tion and candidate answering sentence.
The parseris trained on Fisher, Switchboard, and speechifiedBroadcast News, Brown, and Wall Street Journaltreebanks without punctuation and case to match in-put the evaluation conditions.An example of such a syntactic parse is given inFigure 2.
As shown there, the ?SBJ?
marks the sur-face subject of a given predicate, and the ?TMP?
tagmarks the temporal argument.
There are also the?DIR?
and ?LOC?
tags indicating the locative ar-gument and the ?PRP?
tag indicating the causal ar-gument.
Such parses not only provide a mechanismto extract information relating to the subject of thepredicate of interest, but also to extract the part of266Figure 2: The function tags assist in finding the subject,object, and arguments of a given predicate.the sentence that the question is about, in this ex-ample ?a Russian-made rocket [which] is certainlynot a weapon used by the Greek military?.
The ex-traction of the relevant part is achieved by matchingthe predicate of the question to the predicates of thesubsentences in the best candidate sentence.
Oncesuch syntactic parses are obtained for the part of thebest-candidate sentence that matches the question, aset of rules are used to extract the argument that cananswer the question.4 Anchored Speech RecognitionIn this study we employed a state-of-the-art broad-cast news and conversations speech recognitionsystem (Stolcke et al, 2006).
The recognizerperforms a total of seven decoding passes withalternating acoustic front-ends: one based onMel frequency cepstral coefficients (MFCCs) aug-mented with discriminatively estimated multilayer-perceptron (MLP) features, and one based on per-ceptual linear prediction (PLP) features.
Acousticmodels are cross-adapted during recognition to out-put from previous recognition stages, and the outputof the three final decoding steps are combined viaconfusion networks.Given a question whose answer we expect to findin a given sentence, we construct a re-decoding net-work to match that question.
We call this process an-chored speech recognition, where the anchor is thequestion text.
Note that this is different than forcedalignment, which enforces the recognition of an au-dio stream to align with some given sentence.
It isused for detecting the start times of individual wordsor for language learning applications to exploit theacoustic model scores, since there is no need for alanguage model.Our approach is also different than the so-calledflexible alignment (Finke and Waibel, 1997), whichis basically forced alignment that allows skippingany part of the given sentence, replacing it with a re-ject token, or inserting hesitations in between words.In our task, we require all the words in the ques-tion to be in the best-candidate sentence without anyskips or insertions.
If we allow flexible alignment,then any part of the question could be deleted.
In theproposed anchored speech recognition scheme, weallow only pauses and rejects between words, but donot allow any deletions or skips.The algorithm for extracting anchored recognitionhypotheses is as follows: (i) Construct new recogni-tion and rescoring language models (LMs) by inter-polating the baseline LMs with those trained fromonly the question sentences and use the new LMto generate lattices - this aims to bias the recogni-tion towards word phrases that are included in thequestions.
(ii) Construct for each question an ?an-chored?
word network that matches the word se-quence of the question, allowing any other word se-quence around it.
For example if the question isWHAT did Bruce Gordon say?, we construct a wordnetwork to match Bruce Gordon said ANYTHINGwhere ?ANYTHING?
is a filler that allows any word(a word loop).
(iii) Intersect the recognition lat-tices from step (i) with the anchored network foreach question in (ii), thus extracting from the latticeonly the paths that match as answers to the ques-tion.
Then rescore that new lattice with higher orderLM and cross-word adapted acoustic models to getthe best path.
(iv) If the intersection part in (iii) failsthen we use a more constrained recognition network:Starting with the anchored network in (ii) we firstlimit the vocabulary in the ANYTHING word-loopsub-network to only the words that were included inthe recognition lattice from step (i).
Then we com-pose this network with the bigram LM (from step (i))to add bigram probabilities to the network.
Vocab-ulary limitation is done for efficiency reasons.
Wealso allow optional filler words and pauses to thisnetwork to allow for hesitations, non-speech eventsand pauses within the utterance we are trying tomatch.
This may limit somewhat the potential im-provement from this approach and we are working267Question Type ASR Output Manual Trans.Subject 85% 98%Object 75% 90%Locative Arg.
81% 93%Temporal Arg.
94% 98%Reason 86% 100%Total 83% 95%Table 1: Performance figures for the sentence extractionsystem using automatic and manual transcriptions.Question ASR Manual AnchoredType Output Trans.
OutputSubject 51% 77% 61%Object 41% 73% 51%Locative Arg.
18% 22% 22%Temporal Arg.
55% 73% 63%Reason 26% 47% 26%Total 44% 68% 52%Table 2: Performance figures for the answer extractionsystem using automatic and manual transcriptions com-pared with anchored recognition outputs.towards enhancing the vocabulary with more candi-date words that could contain the spoken words inthe region.
(v) Then we perform recognition withthe new anchored network and extract the best paththrough it.
Thus we enforce partial alignment ofthe audio with the question given, while the regu-lar recognition LM is still used for the parts outsidethe question.5 Experiments and ResultsWe performed experiments using a set of questionsand broadcast audio documents released by LDC forthe DARPA-funded GALE project Phase 3.
In thisdataset we have 482 questions (177 subject, 160 ob-ject, 73 temporal argument, 49 locative argument,and 23 reason) from 90 documents.
The ASR worderror rate (WER) for the sentences from which thequestions are constructed is 37% with respect tonoisy closed captions.
To factor out IR noise we as-sumed that the target document is given.Table 1 presents the performance of the sentenceextraction system using manual and automatic tran-scriptions.
As seen, the system is almost perfectwhen there is no noise, however performance de-grades about 12% with the ASR output.The next set of experiments demonstrate the per-formance of the answer extraction system when thecorrect sentence is given using both automatic andmanual transcriptions.
As seen from Table 2, theanswer extraction performance degrades by about35% relative using the ASR output.
However, usingthe anchored recognition approach, this improves to23%, reducing the effect of the ASR noise signifi-cantly1 by more than 30% relative.
This is shownin the last column of this table, demonstrating theuse of the proposed approach.
We observe that theWER of the sentences for which we now get cor-rected answers is reduced from 45% to 28% withthis approach, a reduction of 37% relative.6 ConclusionsWe have presented a question answering systemfor querying spoken documents with a novel an-chored speech recognition approach, which aims tore-decode an utterance given the question.
The pro-posed approach significantly lowers the error rate foranswer extraction.
Our future work involves han-dling audio in foreign languages, that is robust toboth ASR and machine translation noise.Acknowledgments: This work was funded by DARPA un-der contract No.
HR0011-06-C-0023.
Any conclusions or rec-ommendations are those of the authors and do not necessarilyreflect the views of DARPA.ReferencesM.
Finke and A. Waibel.
1997.
Flexible transcriptionalignment.
In Proceedings of the IEEE ASRU Work-shop, Santa Barbara, CA.M.
Harper and Z. Huang.
Chinese Statistical Parsing,chapter To appear.L.
Lamel, S. Rosset, C. Ayache, D. Mostefa, J. Turmo,and P. Comas.
2008.
Question answering on speechtranscriptions: the qast evaluation in clef.
In Proceed-ings of the LREC, Marrakech, Morocco.A.
Stolcke, B. Chen, H. Franco, V. R. R. Gadde,M.
Graciarena, M.-Y.
Hwang, K. Kirchhoff, N. Mor-gan, X. Lin, T. Ng, M. Ostendorf, K. Sonmez,A.
Venkataraman, D. Vergyri, W. Wang, J. Zheng,and Q. Zhu.
2006.
Recent innovations in speech-to-text transcription at SRI-ICSI-UW.
IEEE Trans-actions on Audio, Speech, and Language Processing,14(5):1729?1744, September.E.
W. D. Whittaker, J. Mrozinski, and S. Furui.
2006.Factoid question answering with web, mobile andspeech interfaces.
In Proceedings of the NAACL/HLT,Morristown, NJ.1according to the Z-test with 0.95 confidence interval268
