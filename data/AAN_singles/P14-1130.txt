Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1381?1391,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLow-Rank Tensors for Scoring Dependency StructuresTao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi JaakkolaComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{taolei, yuxin, yuanzh, regina, tommi}@csail.mit.eduAbstractAccurate scoring of syntactic structuressuch as head-modifier arcs in dependencyparsing typically requires rich, high-dimensional feature representations.
Asmall subset of such features is often se-lected manually.
This is problematic whenfeatures lack clear linguistic meaning asin embeddings or when the information isblended across features.
In this paper, weuse tensors to map high-dimensional fea-ture vectors into low dimensional repre-sentations.
We explicitly maintain the pa-rameters as a low-rank tensor to obtain lowdimensional representations of words intheir syntactic roles, and to leverage mod-ularity in the tensor for easy training withonline algorithms.
Our parser consistentlyoutperforms the Turbo and MST parsersacross 14 different languages.
We also ob-tain the best published UAS results on 5languages.11 IntroductionFinding an expressive representation of input sen-tences is crucial for accurate parsing.
Syntac-tic relations manifest themselves in a broad rangeof surface indicators, ranging from morphologicalto lexical, including positional and part-of-speech(POS) tagging features.
Traditionally, parsing re-search has focused on modeling the direct connec-tion between the features and the predicted syntac-tic relations such as head-modifier (arc) relationsin dependency parsing.
Even in the case of first-order parsers, this results in a high-dimensionalvector representation of each arc.
Discrete fea-tures, and their cross products, can be further com-plemented with auxiliary information about words1Our code is available at https://github.com/taolei87/RBGParser.participating in an arc, such as continuous vectorrepresentations of words.
The exploding dimen-sionality of rich feature vectors must then be bal-anced with the difficulty of effectively learning theassociated parameters from limited training data.A predominant way to counter the high dimen-sionality of features is to manually design or selecta meaningful set of feature templates, which areused to generate different types of features (Mc-Donald et al, 2005a; Koo and Collins, 2010; Mar-tins et al, 2013).
Direct manual selection may beproblematic for two reasons.
First, features maylack clear linguistic interpretation as in distribu-tional features or continuous vector embeddings ofwords.
Second, designing a small subset of tem-plates (and features) is challenging when the rel-evant linguistic information is distributed acrossthe features.
For instance, morphological proper-ties are closely tied to part-of-speech tags, whichin turn relate to positional features.
These featuresare not redundant.
Therefore, we may suffer a per-formance loss if we select only a small subset ofthe features.
On the other hand, by including allthe rich features, we face over-fitting problems.We depart from this view and leverage high-dimensional feature vectors by mapping them intolow dimensional representations.
We begin byrepresenting high-dimensional feature vectors asmulti-way cross-products of smaller feature vec-tors that represent words and their syntactic rela-tions (arcs).
The associated parameters are viewedas a tensor (multi-way array) of low rank, and opti-mized for parsing performance.
By explicitly rep-resenting the tensor in a low-rank form, we havedirect control over the effective dimensionality ofthe set of parameters.
We obtain role-dependentlow-dimensional representations for words (head,modifier) that are specifically tailored for parsingaccuracy, and use standard online algorithms foroptimizing the low-rank tensor components.The overall approach has clear linguistic and1381computational advantages:?
Our low dimensional embeddings are tailoredto the syntactic context of words (head, modi-fier).
This low dimensional syntactic abstrac-tion can be thought of as a proxy to manuallyconstructed POS tags.?
By automatically selecting a small number ofdimensions useful for parsing, we can lever-age a wide array of (correlated) features.
Un-like parsers such as MST, we can easily bene-fit from auxiliary information (e.g., word vec-tors) appended as features.We implement the low-rank factorization modelin the context of first- and third-order depen-dency parsing.
The model was evaluated on 14languages, using dependency data from CoNLL2008 and CoNLL 2006.
We compare our resultsagainst the MST (McDonald et al, 2005a) andTurbo (Martins et al, 2013) parsers.
The low-rankparser achieves average performance of 89.08%across 14 languages, compared to 88.73% for theTurbo parser, and 87.19% for MST.
The power ofthe low-rank model becomes evident in the ab-sence of any part-of-speech tags.
For instance,on the English dataset, the low-rank model trainedwithout POS tags achieves 90.49% on first-orderparsing, while the baseline gets 86.70% if trainedunder the same conditions, and 90.58% if trainedwith 12 core POS tags.
Finally, we demonstratethat the model can successfully leverage word vec-tor representations, in contrast to the baselines.2 Related WorkSelecting Features for Dependency Parsing Agreat deal of parsing research has been dedicatedto feature engineering (Lazaridou et al, 2013;Marton et al, 2010; Marton et al, 2011).
Whilein most state-of-the-art parsers, features are se-lected manually (McDonald et al, 2005a; McDon-ald et al, 2005b; Koo and Collins, 2010; Mar-tins et al, 2013; Zhang and McDonald, 2012a;Rush and Petrov, 2012a), automatic feature selec-tion methods are gaining popularity (Martins et al,2011b; Ballesteros and Nivre, 2012; Nilsson andNugues, 2010; Ballesteros, 2013).
Following stan-dard machine learning practices, these algorithmsiteratively select a subset of features by optimizingparsing performance on a development set.
Thesefeature selection methods are particularly promis-ing in parsing scenarios where the optimal featureset is likely to be a small subset of the original setof candidate features.
Our technique, in contrast,is suitable for cases where the relevant informationis distributed across a larger set of related features.Embedding for Dependency Parsing A lot ofrecent work has been done on mapping words intovector spaces (Collobert and Weston, 2008; Turianet al, 2010; Dhillon et al, 2011; Mikolov et al,2013).
Traditionally, these vector representationshave been derived primarily from co-occurrencesof words within sentences, ignoring syntactic rolesof the co-occurring words.
Nevertheless, any suchword-level representation can be used to offset in-herent sparsity problems associated with full lexi-calization (Cirik and S?ensoy, 2013).
In this sensethey perform a role similar to POS tags.Word-level vector space embeddings have sofar had limited impact on parsing performance.From a computational perspective, adding non-sparse vectors directly as features, including theircombinations, can significantly increase the num-ber of active features for scoring syntactic struc-tures (e.g., dependency arc).
Because of this is-sue, Cirik and S?ensoy (2013) used word vectorsonly as unigram features (without combinations)as part of a shift reduce parser (Nivre et al, 2007).The improvement on the overall parsing perfor-mance was marginal.
Another application of wordvectors is compositional vector grammar (Socheret al, 2013).
While this method learns to mapword combinations into vectors, it builds on ex-isting word-level vector representations.
In con-trast, we represent words as vectors in a mannerthat is directly optimized for parsing.
This frame-work enables us to learn new syntactically guidedembeddings while also leveraging separately esti-mated word vectors as starting features, leading toimproved parsing performance.Dimensionality Reduction Many machinelearning problems can be cast as matrix problemswhere the matrix represents a set of co-varyingparameters.
Such problems include, for example,multi-task learning and collaborative filtering.Rather than assuming that each parameter can beset independently of others, it is helpful to assumethat the parameters vary in a low dimensionalsubspace that has to be estimated together with theparameters.
In terms of the parameter matrix, thiscorresponds to a low-rank assumption.
Low-rankconstraints are commonly used for improving1382generalization (Lee and Seung, 1999; Srebroet al, 2003; Srebro et al, 2004; Evgeniou andPontil, 2007)A strict low-rank assumption can be restrictive.Indeed, recent approaches to matrix problems de-compose the parameter matrix as a sum of low-rank and sparse matrices (Tao and Yuan, 2011;Zhou and Tao, 2011).
The sparse matrix is used tohighlight a small number of parameters that shouldvary independently even if most of them lie ona low-dimensional subspace (Waters et al, 2011;Chandrasekaran et al, 2011).
We follow this de-composition while extending the parameter matrixinto a tensor.Tensors are multi-way generalizations of ma-trices and possess an analogous notion of rank.Tensors are increasingly used as tools in spec-tral estimation (Hsu and Kakade, 2013), includ-ing in parsing (Cohen et al, 2012) and other NLPproblems (de Cruys et al, 2013), where the goalis to avoid local optima in maximum likelihoodestimation.
In contrast, we expand features forparsing into a multi-way tensor, and operate withan explicit low-rank representation of the associ-ated parameter tensor.
The explicit representa-tion sidesteps inherent complexity problems asso-ciated with the tensor rank (Hillar and Lim, 2009).Our parameters are divided into a sparse set corre-sponding to manually chosen MST or Turbo parserfeatures and a larger set governed by a low-ranktensor.3 Problem FormulationWe will commence here by casting first-order de-pendency parsing as a tensor estimation problem.We will start by introducing the notation used inthe paper, followed by a more formal descriptionof our dependency parsing task.3.1 Basic NotationsLet A ?
Rn?n?dbe a 3-dimensional tensor (a 3-way array).
We denote each element of the tensoras Ai,j,kwhere i ?
[n], j ?
[n], k ?
[d] and [n]is a shorthand for the set of integers {1, 2, ?
?
?
, n}.Similarly, we use Mi,jand uito represent the ele-ments of matrix M and vector u, respectively.We define the inner product of two tensors (ormatrices) as ?A,B?
= vec(A)Tvec(B), wherevec(?)
concatenates the tensor (or matrix) ele-ments into a column vector.
The squared normof a tensor/matrix is denoted by ?A?2= ?A,A?.The Kronecker product of three vectors is de-noted by u?v?w and forms a rank-1 tensor suchthat(u?
v ?
w)i,j,k= uivjwk.Note that the vectors u, v, and w may be columnor row vectors.
Their orientation is defined basedon usage.
For example, u ?
v is a rank-1 matrixuvTwhen u and v are column vectors (uTv if theyare row vectors).We say that tensor A is in Kruskal form ifA =r?i=1U(i, :)?
V (i, :)?W (i, :) (1)where U, V ?
Rr?n, W ?
Rr?dand U(i, :) is theithrow of matrix U .
We will directly learn a low-rank tensor A (because r is small) in this form asone of our model parameters.3.2 Dependency ParsingLet x be a sentence and Y(x) the set of possibledependency trees over the words in x.
We assumethat the score S(x, y) of each candidate depen-dency tree y ?
Y(x) decomposes into a sum of?local?
scores for arcs.
Specifically:S(x, y) =?h?m ?
ys(h?
m) ?y ?
Y(x)where h ?
m is the head-modifier dependencyarc in the tree y.
Each y is understood as a col-lection of arcs h ?
m where h and m indexwords in x.2For example, x(h) is the word cor-responding to h. We suppress the dependence onx whenever it is clear from context.
For exam-ple, s(h ?
m) can depend on x in complicatedways as discussed below.
The predicted parse isobtained as y?
= arg maxy?Y(x)S(x, y).A key problem is how we parameterize thearc scores s(h ?
m).
Following the MSTparser (McDonald et al, 2005a) we can definerich features characterizing each head-modifierarc, compiled into a sparse binary vector ?h?m?RLthat depends on the sentence x as well as thechosen arc h?
m (again, we suppress the depen-dence on x).
Based on this feature representation,we define the score of each arc as s?
(h ?
m) =2Note that in the case of high-order parsing, the sumS(x, y) may also include local scores for other syntacticstructures, such as grandhead-head-modifier score s(g ?h ?
m).
See (Martins et al, 2013) for a complete list ofthese structures.1383Unigram features:form form-p form-nlemma lemma-p lemma-npos pos-p pos-nmorph biasBigram features:pos-p, pospos, pos-npos, lemmamorph, lemmaTrigram features:pos-p, pos, pos-nTable 1: Word feature templates used by ourmodel.
pos, form, lemma and morph stand forthe fine POS tag, word form, word lemma and themorphology feature (provided in CoNLL formatfile) of the current word.
There is a bias term thatis always active for any word.
The suffixes -p and-n refer to the left and right of the current word re-spectively.
For example, pos-p means the POS tagto the left of the current word in the sentence.?
?, ?h?m?
where ?
?
RLrepresent adjustable pa-rameters to be learned, and L is the number of pa-rameters (and possible features in ?h?m).We can alternatively specify arc features interms of rank-1 tensors by taking the Kroneckerproduct of simpler feature vectors associated withthe head (vector ?h?
Rn), and modifier (vector?m?
Rn), as well as the arc itself (vector ?h,m?Rd).
Here ?h,mis much lower dimensional thanthe MST arc feature vector ?h?mdiscussed ear-lier.
For example, ?h,mmay be composed of onlyindicators for binned arc lengths3.
?hand ?m, onthe other hand, are built from features shown inTable 1.
By taking the cross-product of all thesecomponent feature vectors, we obtain the full fea-ture representation for arc h?
m as a rank-1 ten-sor?h?
?m?
?h,m?
Rn?n?dNote that elements of this rank-1 tensor includefeature combinations that are not part of the fea-ture crossings in ?h?m.
In this sense, the rank-1tensor represents a substantial feature expansion.The arc score stensor(h?
m) associated with the3In our current version, ?h,monly contains the binnedarc length.
Other possible features include, for example, thelabel of the arc h ?
m, the POS tags between the head andthe modifier, boolean flags which indicate the occurence ofin-between punctutations or conjunctions, etc.tensor representation is defined analogously asstensor(h?
m) = ?A, ?h?
?m?
?h,m?where the adjustable parametersA also form a ten-sor.
Given the typical dimensions of the compo-nent feature vectors, ?h, ?m, ?h,m, it is not evenpossible to store all the parameters in A. Indeed,in the full English training set of CoNLL-2008, thetensor involves around 8 ?
1011entries while theMST feature vector has approximately 1.5 ?
107features.
To counter this feature explosion, we re-strict the parameters A to have low rank.Low-Rank Dependency Scoring We can repre-sent a rank-r tensor A explicitly in terms of pa-rameter matrices U , V , and W as shown in Eq.
1.As a result, the arc score for the tensor reduces toevaluating U?h, V ?m, and W?h,mwhich are allr dimensional vectors and can be computed effi-ciently based on any sparse vectors ?h, ?m, and?h,m.
The resulting arc score stensor(h ?
m) isthenr?i=1[U?h]i[V ?m]i[W?h,m]i(2)By learning parameters U , V , andW that functionwell in dependency parsing, we also learn context-dependent embeddings for words and arcs.
Specif-ically,U?h(for a given sentence, suppressed) is anr dimensional vector representation of the wordcorresponding to h as a head word.
Similarly,V ?mprovides an analogous representation for amodifier m. Finally, W?h,mis a vector embed-ding of the supplemental arc-dependent informa-tion.
The resulting embedding is therefore tiedto the syntactic roles of the words (and arcs), andlearned in order to perform well in parsing.We expect a dependency parsing model to ben-efit from several aspects of the low-rank tensorscoring.
For example, we can easily incorpo-rate additional useful features in the feature vec-tors ?h, ?mand ?h,m, since the low-rank assump-tion (for small enough r) effectively counters theotherwise uncontrolled feature expansion.
More-over, by controlling the amount of informationwe can extract from each of the component fea-ture vectors (via rank r), the statistical estimationproblem does not scale dramatically with the di-mensions of ?h, ?mand ?h,m.
In particular, thelow-rank constraint can help generalize to unseenarcs.
Consider a feature ?
(x(h) = a) ?
?
(x(m) =1384b) ?
?
(dis(x, h,m) = c) which is non-zero onlyfor an arc a ?
b with distance c in sentence x.If the arc has not been seen in the available train-ing data, it does not contribute to the traditionalarc score s?(?).
In contrast, with the low-rank con-straint, the arc score in Eq.
2 would typically benon-zero.Combined Scoring Our parsing model aims tocombine the strengths of both traditional featuresfrom the MST/Turbo parser as well as the newlow-rank tensor features.
In this way, our modelis able to capture a wide range of information in-cluding the auxiliary features without having un-controlled feature explosion, while still having thefull accessibility to the manually engineered fea-tures that are proven useful.
Specifically, we de-fine the arc score s?(h?
m) as the combination(1?
?)stensor(h?
m) + ?s?(h?
m)= (1?
?
)r?i=1[U?h]i[V ?m]i[W?h,m]i+ ?
?
?, ?h?m?
(3)where ?
?
RL, U ?
Rr?n, V ?
Rr?n, and W ?Rr?dare the model parameters to be learned.
Therank r and ?
?
[0, 1] (balancing the two scores)represent hyper-parameters in our model.4 LearningThe training set D = {(x?i, y?i)}Ni=1consists of Npairs, where each pair consists of a sentence xiand the corresponding gold (target) parse yi.
Thegoal is to learn values for the parameters ?, U , Vand W that optimize the combined scoring func-tion S?
(x, y) =?h?m?ys?
(h ?
m), definedin Eq.
3, for parsing performance.
We adopt amaximum soft-margin framework for this learningproblem.
Specifically, we find parameters ?, U , V ,W , and {?i} that minimizeC?i?i+ ??
?2+ ?U?2+ ?V ?2+ ?W?2s.t.
S?
(x?i, y?i) ?
S?
(x?i, yi) + ?y?i?
yi?1?
?i?yi?
Y(x?i), ?i.
(4)where ?y?i?yi?1is the number of mismatched arcsbetween the two trees, and ?iis a non-negativeslack variable.
The constraints serve to separatethe gold tree from other alternatives in Y(x?i) witha margin that increases with distance.The objective as stated is not jointly convexwith respect to U , V and W due to our explicitrepresentation of the low-rank tensor.
However, ifwe fix any two sets of parameters, for example, ifwe fix V andW , then the combined score S?
(x, y)will be a linear function of both ?
and U .
As a re-sult, the objective will be jointly convex with re-spect to ?
and U and could be optimized usingstandard tools.
However, to accelerate learning,we adopt an online learning setup.
Specifically,we use the passive-aggressive learning algorithm(Crammer et al, 2006) tailored to our setting, up-dating pairs of parameter sets, (?, U), (?, V ) and(?,W ) in an alternating manner.
This method isdescribed below.Online Learning In an online learning setup,we update parameters successively based on eachsentence.
In order to apply the passive-aggressivealgorithm, we fix two of U , V and W (say, for ex-ample, V and W ) in an alternating manner, andapply a closed-form update to the remaining pa-rameters (here U and ?).
This is possible sincethe objective function with respect to (?, U) has asimilar form as in the original passive-aggressivealgorithm.
To illustrate this, consider a trainingsentence xi.
The update involves finding first thebest competing tree,y?i= arg maxyi?Y(x?i)S?
(x?i, yi) + ?y?i?
yi?1(5)which is the tree that violates the constraint inEq.
4 most (i.e.
maximizes the loss ?i).
We thenobtain parameter increments ??
and ?U by solv-ingmin?
?, ?U, ??012????2+12?
?U?2+ C?s.t.
S?
(x?i, y?i) ?
S?
(x?i, y?i) + ?y?i?
y?i?1?
?In this way, the optimization problem attempts tokeep the parameter change as small as possible,while forcing it to achieve mostly zero loss on thissingle instance.
This problem has a closed formsolution??
= min{C,loss?2?d?
?2+ (1?
?)2?du?2}?d?
?U = min{C,loss?2?d?
?2+ (1?
?)2?du?2}(1?
?
)du1385whereloss = S?
(x?i, y?i) + ?y?i?
y?i?1?
S?
(x?i, y?i)d?
=?h?m ?
y?i?h?m?
?h?m ?
y?i?h?mdu =?h?m ?
y?i[(V ?m) (W?h,m)]?
?h?
?h?m ?
y?i[(V ?m) (W?h,m)]?
?hwhere (u v)i= uiviis the Hadamard (element-wise) product.
The magnitude of change of ?
andU is controlled by the parameterC.
By varyingC,we can determine an appropriate step size for theonline updates.
The updates also illustrate how ?balances the effect of the MST component of thescore relative to the low-rank tensor score.
When?
= 0, the arc scores are entirely based on the low-rank tensor and ??
= 0.
Note that ?h, ?m, ?h,m,and ?h?mare typically very sparse for each wordor arc.
Therefore du and d?
are also sparse andcan be computed efficiently.Initialization The alternating online algorithmrelies on how we initializeU , V , andW since eachupdate is carried out in the context of the othertwo.
A random initialization of these parameters isunlikely to work well, both due to the dimensionsinvolved, and the nature of the alternating updates.We consider here instead a reasonable determinis-tic ?guess?
as the initialization method.We begin by training our model without anylow-rank parameters, and obtain parameters ?.The majority of features in this MST componentcan be expressed as elements of the feature ten-sor, i.e., as [?h?
?m?
?h,m]i,j,k.
We can there-fore create a tensor representation of ?
such thatBi,j,kequals the corresponding parameter valuein ?.
We use a low-rank version of B as the ini-tialization.
Specifically, we unfold the tensor Binto a matrix B(h)of dimensions n and nd, wheren = dim(?h) = dim(?m) and d = dim(?h,m).For instance, a rank-1 tensor can be unfolded asu ?
v ?
w = u ?
vec(v ?
w).
We compute thetop-r SVD of the resulting unfolded matrix suchthat B(h)= PTSQ.
U is initialized as P .
Eachright singular vector SiQ(i, :) is also a matrix inRn?d.
The leading left and right singular vectorsof this matrix are assigned to V (i, :) and W (i, :)respectively.
In our implementation, we run oneepoch of our model without low-rank parametersand initialize the tensor A.Parameter Averaging The passive-aggressivealgorithm regularizes the increments (e.g.
??
and?U ) during each update but does not include anyoverall regularization.
In other words, keeping up-dating the model may lead to large parameter val-ues and over-fitting.
To counter this effect, we useparameter averaging as used in the MST and Turboparsers.
The final parameters are those averagedacross all the iterations (cf.
(Collins, 2002)).
Forsimplicity, in our algorithm we average U , V , Wand ?
separately, which works well empirically.5 Experimental SetupDatasets We test our dependency model on 14languages, including the English dataset fromCoNLL 2008 shared tasks and all 13 datasets fromCoNLL 2006 shared tasks (Buchholz and Marsi,2006; Surdeanu et al, 2008).
These datasets in-clude manually annotated dependency trees, POStags and morphological information.
Followingstandard practices, we encode this information asfeatures.Methods We compare our model to MST andTurbo parsers on non-projective dependency pars-ing.
For our parser, we train both a first-orderparsing model (as described in Section 3 and 4)as well as a third-order model.
The third orderparser simply adds high-order features, those typ-ically used in MST and Turbo parsers, into ours?
(x, y) = ?
?, ?
(x, y)?
scoring component.
Thedecoding algorithm for the third-order parsing isbased on (Zhang et al, 2014).
For the Turboparser, we directly compare with the recent pub-lished results in (Martins et al, 2013).
For theMST parser, we train and test using the most re-cent version of the code.4In addition, we im-plemented two additional baselines, NT-1st (firstorder) and NT-3rd (third order), corresponding toour model without the tensor component.Features For the arc feature vector ?h?m, weuse the same set of feature templates as MSTv0.5.1.
For head/modifier vector ?hand ?m, weshow the complete set of feature templates usedby our model in Table 1.
Finally, we use a similarset of feature templates as Turbo v2.1 for 3rd orderparsing.To add auxiliary word vector representations,we use the publicly available word vectors (Cirik4http://sourceforge.net/projects/mstparser/1386First-order only High-orderOurs NT-1st MST Turbo Ours-3rd NT-3rd MST-2nd Turbo-3rd Best PublishedArabic 79.60 78.71 78.3 77.23 79.95 79.53 78.75 79.64 81.12 (Ma11)Bulgarian 92.30 91.14 90.98 91.76 93.50 92.79 91.56 93.1 94.02 (Zh13)Chinese 91.43 90.85 90.40 88.49 92.68 92.39 91.77 89.98 91.89 (Ma10)Czech 87.90 86.62 86.18 87.66 90.50 89.43 87.3 90.32 90.32 (Ma13)Danish 90.64 89.80 89.84 89.42 91.39 90.82 90.5 91.48 92.00 (Zh13)Dutch 84.81 83.77 82.89 83.61 86.41 86.08 84.11 86.19 86.19 (Ma13)English 91.84 91.40 90.59 91.21 93.02 92.82 91.54 93.22 93.22 (Ma13)German 90.24 89.70 89.54 90.52 91.97 92.26 90.14 92.41 92.41 (Ma13)Japanese 93.74 93.36 93.38 92.78 93.71 93.23 92.92 93.52 93.72 (Ma11)Portuguese 90.94 90.67 89.92 91.14 91.92 91.63 91.08 92.69 93.03 (Ko10)Slovene 84.25 83.15 82.09 82.81 86.24 86.07 83.25 86.01 86.95 (Ma11)Spanish 85.27 84.95 83.79 83.61 88.00 87.47 84.33 85.59 87.96 (Zh13)Swedish 89.86 89.66 88.27 89.36 91.00 90.83 89.05 91.14 91.62 (Zh13)Turkish 75.84 74.89 74.81 75.98 76.84 75.83 74.39 76.9 77.55 (Ko10)Average 87.76 87.05 86.5 86.83 89.08 88.66 87.19 88.73 89.43Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and theEnglish dataset of CoNLL-2008.
For our model, the experiments are ran with rank r = 50 and hyper-parameter ?
= 0.3.
To remove the tensor in our model, we ran experiments with ?
= 1, correspondingto columns NT-1st and NT-3rd.
The last column shows results of most accurate parsers among Nivre etal.
(2006), McDonald et al (2006), Martins et al (2010), Martins et al (2011a), Martins et al (2013),Koo et al (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al (2013).and S?ensoy, 2013), learned from raw data (Glober-son et al, 2007; Maron et al, 2010).
Threelanguages in our dataset ?
English, German andSwedish ?
have corresponding word vectors in thiscollection.5The dimensionality of this representa-tion varies by language: English has 50 dimen-sional word vectors, while German and Swedishhave 25 dimensional word vectors.
Each entry ofthe word vector is added as a feature value intofeature vectors ?hand ?m.
For each word in thesentence, we add its own word vector as well asthe vectors of its left and right words.We should note that since our model parameterA is represented and learned in the low-rank form,we only have to store and maintain the low-rankprojections U?h, V ?mandW?h,mrather than ex-plicitly calculate the feature tensor ?h??m?
?h,m.Therefore updating parameters and decoding asentence is still efficient, i.e., linear in the num-ber of values of the feature vector.
In contrast,assume we take the cross-product of the auxiliaryword vector values, POS tags and lexical items ofa word and its context, and add the crossed val-ues into a normal model (in ?h?m).
The numberof features for each arc would be at least quadratic,growing into thousands, and would be a significantimpediment to parsing efficiency.Evaluation Following standard practices, wetrain our full model and the baselines for 105https://github.com/wolet/sprml13-word-embeddingsepochs.
As the evaluation measure, we use un-labeled attachment scores (UAS) excluding punc-tuation.
In all the reported experiments, the hyper-parameters are set as follows: r = 50 (rank of thetensor), C = 1 for first-order model and C = 0.01for third-order model.6 ResultsOverall Performance Table 2 shows the per-formance of our model and the baselines on 14CoNLL datasets.
Our model outperforms Turboparser, MST parser, as well as its own variantswithout the tensor component.
The improvementsof our low-rank model are consistent across lan-guages: results for the first order parser are betteron 11 out of 14 languages.
By comparing NT-1stand NT-3rd (models without low-rank) with ourfull model (with low-rank), we obtain 0.7% abso-lute improvement on first-order parsing, and 0.3%improvement on third-order parsing.
Our modelalso achieves the best UAS on 5 languages.We next focus on the first-order model andgauge the impact of the tensor component.
First,we test our model by varying the hyper-parameter?
which balances the tensor score and the tradi-tional MST/Turbo score components.
Figure 1shows the average UAS on CoNLL test datasetsafter each training epoch.
We can see that the im-provement of adding the low-rank tensor is con-sistent across various choices of hyper parame-13872 4 6 8 1084.0%84.5%85.0%85.5%86.0%86.5%87.0%87.5%88.0%# Epochs?=0.0?=0.2?=0.3?=0.4NT?1stFigure 1: Average UAS on CoNLL testsets af-ter different epochs.
Our full model consistentlyperforms better than NT-1st (its variation withouttensor component) under different choices of thehyper-parameter ?.no word vector with word vectorEnglish 91.84 92.07German 90.24 90.48Swedish 89.86 90.38Table 3: Results of adding unsupervised word vec-tors to the tensor.
Adding this information yieldsconsistent improvement for all languages.ter ?.
When training with the tensor componentalone (?
= 0), the model converges more slowly.Learning of the tensor is harder because the scor-ing function is not linear (nor convex) with respectto parameters U , V and W .
However, the tensorscoring component achieves better generalizationon the test data, resulting in better UAS than NT-1st after 8 training epochs.To assess the ability of our model to incorpo-rate a range of features, we add unsupervised wordvectors to our model.
As described in previoussection, we do so by appending the values of dif-ferent coordinates in the word vector into ?hand?m.
As Table 3 shows, adding this information in-creases the parsing performance for all the threelanguages.
For instance, we obtain more than0.5% absolute improvement on Swedish.Syntactic Abstraction without POS Since ourmodel learns a compressed representation of fea-ture vectors, we are interested to measure its per-formance when part-of-speech tags are not pro-vided (See Table 4).
The rationale is that given allother features, the model would induce representa-tions that play a similar role to POS tags.
Note thatOur model NT-1st-POS +wv.
-POS +POSEnglish 88.89 90.49 86.70 90.58German 82.63 85.80 78.71 88.50Swedish 81.84 85.90 79.65 88.75Table 4: The first three columns show parsing re-sults when models are trained without POS tags.The last column gives the upper-bound, i.e.
theperformance of a parser trained with 12 Core POStags.
The low-rank model outperforms NT-1st bya large margin.
Adding word vector features fur-ther improves performance.the performance of traditional parsers drops whentags are not provided.
For example, the perfor-mance gap is 10% on German.
Our experimentsshow that low-rank parser operates effectively inthe absence of tags.
In fact, it nearly reaches theperformance of the original parser that used thetags on English.Examples of Derived Projections We manu-ally analyze low-dimensional projections to assesswhether they capture syntactic abstraction.
Forthis purpose, we train a model with only a ten-sor component (such that it has to learn an accu-rate tensor) on the English dataset and obtain lowdimensional embeddings U?wand V ?wfor eachword.
The two r-dimension vectors are concate-nated as an ?averaged?
vector.
We use this vectorto calculate the cosine similarity between words.Table 5 shows examples of five closest neighborsof queried words.
While these lists include somenoise, we can clearly see that the neighbors ex-hibit similar syntactic behavior.
For example, ?on?is close to other prepositions.
More interestingly,we can consider the impact of syntactic contexton the derived projections.
The bottom part ofTable 5 shows that the neighbors change substan-tially depending on the syntactic role of the word.For example, the closest words to the word ?in-crease?
are verbs in the context phrase ?will in-crease again?, while the closest words becomenouns given a different phrase ?an increase of?.Running Time Table 6 illustrates the impact ofestimating low-rank tensor parameters on the run-ning time of the algorithm.
For comparison, wealso show the NT-1st times across three typicallanguages.
The Arabic dataset has the longest av-erage sentence length, while the Chinese dataset1388greatly profit says on whenactively earnings adds with whereopenly franchisees predicts into whatsignificantly shares noted at whyoutright revenue wrote during whichsubstantially members contends over whoincrease will increase again an increase ofrise arguing gainadvance be pricescontest charging paymenthalt gone membersExchequer making subsidiaryhit attacks hit the hardest hit isshed distributes monopoliesrallied stayed pillstriggered sang sophisticationappeared removed venturesunderstate eased factorsTable 5: Five closest neighbors of the queriedwords (shown in bold).
The upper part shows ourlearned embeddings group words with similar syn-tactic behavior.
The two bottom parts of the tabledemonstrate that how the projections change de-pending on the syntactic context of the word.#Tok.
Len.Train.
Time (hour)NT-1st OursArabic 42K 32 0.13 0.22Chinese 337K 6 0.37 0.65English 958K 24 1.88 2.83Table 6: Comparison of training times across threetypical datasets.
The second column is the numberof tokens in each data set.
The third column showsthe average sentence length.
Both first-order mod-els are implemented in Java and run as a singleprocess.has the shortest sentence length in CoNLL 2006.Based on these results, estimating a rank-50 tensortogether with MST parameters only increases therunning time by a factor of 1.7.7 ConclusionsAccurate scoring of syntactic structures such ashead-modifier arcs in dependency parsing typi-cally requires rich, high-dimensional feature rep-resentations.
We introduce a low-rank factoriza-tion method that enables to map high dimensionalfeature vectors into low dimensional representa-tions.
Our method maintains the parameters as alow-rank tensor to obtain low dimensional repre-sentations of words in their syntactic roles, and toleverage modularity in the tensor for easy train-ing with online algorithms.
We implement theapproach on first-order to third-order dependencyparsing.
Our parser outperforms the Turbo andMST parsers across 14 languages.Future work involves extending the tensor com-ponent to capture higher-order structures.
In par-ticular, we would consider second-order structuressuch as grandparent-head-modifier by increasingthe dimensionality of the tensor.
This tensor willaccordingly be a four or five-way array.
The onlineupdate algorithm remains applicable since each di-mension is optimized in an alternating fashion.8 AcknowledgementsThe authors acknowledge the support of the MURIprogram (W911NF-10-1-0533) and the DARPABOLT program.
This research is developed in col-laboration with the Arabic Language Technoligies(ALT) group at Qatar Computing Research Insti-tute (QCRI) within the LYAS project.
We thankVolkan Cirik for sharing the unsupervised wordvector data.
Thanks to Amir Globerson, AndreeaGane, the members of the MIT NLP group andthe ACL reviewers for their suggestions and com-ments.
Any opinions, findings, conclusions, orrecommendations expressed in this paper are thoseof the authors, and do not necessarily reflect theviews of the funding organizations.ReferencesMiguel Ballesteros and Joakim Nivre.
2012.
Mal-tOptimizer: An optimization tool for MaltParser.
InEACL.
The Association for Computer Linguistics.Miguel Ballesteros.
2013.
Effective morpholog-ical feature selection with MaltOptimizer at theSPMRL 2013 shared task.
In Proceedings ofthe Fourth Workshop on Statistical Parsing ofMorphologically-Rich Languages.
Association forComputational Linguistics.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of the Tenth Conference on Computa-tional Natural Language Learning, CoNLL-X ?06.Association for Computational Linguistics.Venkat Chandrasekaran, Sujay Sanghavi, Pablo A Par-rilo, and Alan S Willsky.
2011.
Rank-sparsity in-coherence for matrix decomposition.
SIAM Journalon Optimization.Volkan Cirik and H?usn?u S?ensoy.
2013.
The AI-KUsystem at the SPMRL 2013 shared task : Unsuper-vised features for dependency parsing.
In Proceed-ings of the Fourth Workshop on Statistical Parsing ofMorphologically-Rich Languages.
Association forComputational Linguistics.1389Shay B Cohen, Karl Stratos, Michael Collins, Dean PFoster, and Lyle Ungar.
2012.
Spectral learning oflatent-variable PCFGs.
In Proceedings of the 50thAnnual Meeting of the Association for Computa-tional Linguistics: Long Papers-Volume 1.
Associ-ation for Computational Linguistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing - Volume 10, EMNLP ?02.
As-sociation for Computational Linguistics.R.
Collobert and J. Weston.
2008.
A unified architec-ture for natural language processing: Deep neuralnetworks with multitask learning.
In InternationalConference on Machine Learning, ICML.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
The Journal of Ma-chine Learning Research.Tim Van de Cruys, Thierry Poibeau, and Anna Korho-nen.
2013.
A tensor-based factorization model ofsemantic compositionality.
In HLT-NAACL.
The As-sociation for Computational Linguistics.Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.2011.
Multiview learning of word embeddings viaCCA.
In Advances in Neural Information Process-ing Systems.A Evgeniou and Massimiliano Pontil.
2007.
Multi-task feature learning.
In Advances in neural infor-mation processing systems: Proceedings of the 2006conference.
The MIT Press.Amir Globerson, Gal Chechik, Fernando Pereira, andNaftali Tishby.
2007.
Euclidean embedding of co-occurrence data.
Journal of Machine Learning Re-search.Christopher Hillar and Lek-Heng Lim.
2009.
Mosttensor problems are NP-hard.
arXiv preprintarXiv:0911.1393.Daniel Hsu and Sham M Kakade.
2013.
Learning mix-tures of spherical gaussians: moment methods andspectral decompositions.
In Proceedings of the 4thConference on Innovations in Theoretical ComputerScience.
ACM.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, ACL ?10.
Association for Com-putational Linguistics.Terry Koo, Alexander M Rush, Michael Collins,Tommi Jaakkola, and David Sontag.
2010.
Dualdecomposition for parsing with non-projective headautomata.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Process-ing.
Association for Computational Linguistics.Angeliki Lazaridou, Eva Maria Vecchi, and MarcoBaroni.
2013.
Fish transporters and miraclehomes: How compositional distributional semanticscan help NP parsing.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing.
Association for ComputationalLinguistics.Daniel D Lee and H Sebastian Seung.
1999.
Learningthe parts of objects by non-negative matrix factor-ization.
Nature.Yariv Maron, Michael Lamar, and Elie Bienenstock.2010.
Sphere embedding: An application to part-of-speech induction.
In Advances in Neural Infor-mation Processing Systems.Andr?e FT Martins, Noah A Smith, Eric P Xing, Pe-dro MQ Aguiar, and M?ario AT Figueiredo.
2010.Turbo parsers: Dependency parsing by approximatevariational inference.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing.
Association for ComputationalLinguistics.Andr?e F. T. Martins, Noah A. Smith, Pedro M. Q.Aguiar, and M?ario A. T. Figueiredo.
2011a.
Dualdecomposition with many overlapping components.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?11.
Association for Computational Linguistics.Andr?e FT Martins, Noah A Smith, Pedro MQ Aguiar,and M?ario AT Figueiredo.
2011b.
Structured spar-sity in structured prediction.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing.
Association for ComputationalLinguistics.Andr?e FT Martins, Miguel B Almeida, and Noah ASmith.
2013.
Turning on the turbo: Fast third-ordernon-projective turbo parsers.
In Proceedings of the51th Annual Meeting of the Association for Compu-tational Linguistics.
Association for ComputationalLinguistics.Yuval Marton, Nizar Habash, and Owen Rambow.2010.
Improving arabic dependency parsing withlexical and inflectional morphological features.
InProceedings of the NAACL HLT 2010 First Work-shop on Statistical Parsing of Morphologically-RichLanguages, SPMRL ?10.
Association for Computa-tional Linguistics.Yuval Marton, Nizar Habash, and Owen Rambow.2011.
Improving arabic dependency parsing withform-based and functional morphological features.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies.
Association for Computa-tional Linguistics.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005a.
Online large-margin training of de-pendency parsers.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics (ACL?05).1390Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Haji?c.
2005b.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof the conference on Human Language Technologyand Empirical Methods in Natural Language Pro-cessing.
Association for Computational Linguistics.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with atwo-stage discriminative parser.
In Proceedingsof the Tenth Conference on Computational NaturalLanguage Learning.
Association for ComputationalLinguistics.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
CoRR.Peter Nilsson and Pierre Nugues.
2010.
Automaticdiscovery of feature sets for dependency parsing.
InProceedings of the 23rd International Conferenceon Computational Linguistics (Coling 2010).
Coling2010 Organizing Committee.Joakim Nivre, Johan Hall, Jens Nilsson, G?uls?en Eryiit,and Svetoslav Marinov.
2006.
Labeled pseudo-projective dependency parsing with support vectormachines.
In Proceedings of the Tenth Conferenceon Computational Natural Language Learning.
As-sociation for Computational Linguistics.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
MaltParser: Alanguage-independent system for data-driven depen-dency parsing.
Natural Language Engineering.Alexander Rush and Slav Petrov.
2012a.
Vine pruningfor efficient multi-pass dependency parsing.
In The2012 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies (NAACL ?12).Alexander M Rush and Slav Petrov.
2012b.
Vine prun-ing for efficient multi-pass dependency parsing.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies.Association for Computational Linguistics.Richard Socher, John Bauer, Christopher D. Manning,and Andrew Y. Ng.
2013.
Parsing with compo-sitional vector grammars.
In Proceedings of the51th Annual Meeting of the Association for Compu-tational Linguistics.Nathan Srebro, Tommi Jaakkola, et al 2003.
Weightedlow-rank approximations.
In ICML.Nathan Srebro, Jason Rennie, and Tommi S Jaakkola.2004.
Maximum-margin matrix factorization.
InAdvances in neural information processing systems.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s M`arquez, and Joakim Nivre.
2008.
TheCoNLL-2008 shared task on joint parsing of syn-tactic and semantic dependencies.
In Proceedingsof the Twelfth Conference on Computational Natu-ral Language Learning, CoNLL ?08.
Association forComputational Linguistics.Min Tao and Xiaoming Yuan.
2011.
Recovering low-rank and sparse components of matrices from in-complete and noisy observations.
SIAM Journal onOptimization.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, ACL ?10.
Association for Com-putational Linguistics.Andrew E Waters, Aswin C Sankaranarayanan, andRichard Baraniuk.
2011.
SpaRCS: Recovering low-rank and sparse matrices from compressive mea-surements.
In Advances in Neural Information Pro-cessing Systems.Hao Zhang and Ryan McDonald.
2012a.
Generalizedhigher-order dependency parsing with cube prun-ing.
In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learn-ing, EMNLP-CoNLL ?12.
Association for Compu-tational Linguistics.Hao Zhang and Ryan McDonald.
2012b.
Generalizedhigher-order dependency parsing with cube prun-ing.
In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning.Association for Computational Linguistics.Hao Zhang, Liang Huang Kai Zhao, and Ryan McDon-ald.
2013.
Online learning for inexact hypergraphsearch.
In Proceedings of EMNLP.Yuan Zhang, Tao Lei, Regina Barzilay, TommiJaakkola, and Amir Globerson.
2014.
Steps to ex-cellence: Simple inference with refined scoring ofdependency trees.
In Proceedings of the 52th An-nual Meeting of the Association for ComputationalLinguistics.
Association for Computational Linguis-tics.Tianyi Zhou and Dacheng Tao.
2011.
Godec: Ran-domized low-rank & sparse matrix decomposition innoisy case.
In Proceedings of the 28th InternationalConference on Machine Learning (ICML-11).1391
