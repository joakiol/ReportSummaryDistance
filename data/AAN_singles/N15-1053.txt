Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 504?514,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsD?ej`a Image-Captions: A Corpus of Expressive Descriptions in RepetitionJianfu Chen?and Polina Kuznetsova?and David S. Warren?and Yejin Choi?Stony Brook University?University of Washington?
{jianchen,pkuznetsova,warren}@cs.stonybrook.edu?, yejin@cs.washington.edu?AbstractWe present a new approach to harvesting alarge-scale, high quality image-caption corpusthat makes a better use of already existing webdata with no additional human efforts.
Thekey idea is to focus on D?ej`a Image-Captions:naturally existing image descriptions that arerepeated almost verbatim ?
by more than oneindividual for different images.
The resultingcorpus provides association structure between4 million images with 180K unique captions,capturing a rich spectrum of everyday narra-tives including figurative and pragmatic lan-guage.
Exploring the use of the new corpus,we also present new conceptual tasks of visu-ally situated paraphrasing, creative image cap-tioning, and creative visual paraphrasing.1 IntroductionThe use of multimodal web data has been a recur-ring theme in many recent studies integrating lan-guage and vision, e.g., image captioning (Ordonezet al, 2011; Hodosh et al, 2013; Mason and Char-niak, 2014; Kuznetsova et al, 2014), text-based im-age retrieval (Rasiwasia et al, 2010; Rasiwasia etal., 2007), and entry-level categorization (Ordonezet al, 2013; Feng et al, 2015).However, much research integrating complex tex-tual descriptions to date has been based on datasetsthat rely on substantial human curation or annota-tion (Hodosh et al, 2013; Rashtchian et al, 2010;Lin et al, 2014), rather than using the web data inthe wild as is (Ordonez et al, 2011; Kuznetsova etal., 2014).
The need for human curation limits thepotential scale of the multimodal dataset.
Withouthuman curation, however, the web data introducessignificant noise.
In particular, everyday captionsoften contain extraneous information that is not di-rectly relevant to what the image shows (Kuznetsovaet al, 2013b; Hodosh et al, 2013).In this paper, we present a new approach to har-vesting a large-scale, high quality image-captioncorpus that makes a better use of already existingweb data with no additional human efforts.
Figure 1shows sample captions in the resulting corpus, e.g.,?butterfly resting on a flower?
and ?evening walkalong the beach?.
Notably, some of these are figu-rative, e.g., ?rippled sky?
and ?sun is going to bed.
?The key idea is to focus on D?ej`a Image-Captions,i.e., naturally existing image captions that are re-peated almost verbatim by more than one individ-ual for different images.
The hypothesis is that suchcaptions represent common visual content acrossmultiple images, hence are more likely to be freeof unwanted extraneous information (e.g., specificnames, time, or any other personal information) andbetter represent visual concepts.
A surprising as-pect of our study is that such a strict data filtrationscheme can still result in a large-scale corpus; siftingthrough 760 million image-caption pairs, we harvestas many as 4 million image-caption pairs with 180Kunique captions.The resulting corpus, D?ej`a Image Captions, pro-vides several unique properties that complementhuman-curated or crowd-sourced datasets.
First, asour approach is fully automated, it can be readilyapplied to harvesting a new dataset from the everchanging multimodal web data.
Indeed, a recentinternet report estimates that billions of new pho-tographs are being uploaded daily (Meeker, 2014).In contrast, human-annotated datasets are costly toscale to different domains.Second, datasets that are harvested from the web504Butterflies are self propelled flowers (198)butterfly resting on a flower (26)After the sun has set (9)Sun is going to bed (21)can you spot the butterfly (88)The sky looks like it is on fire (58)The sun sets for another day (12)Evening walk along the beach (9)Chillaxing at the beach (20)Walk by the beach (557)  Rippled sky (44)In the sky (1013)Figure 1: The image-caption association graph of D?ej`a Image-Captions.
Solid lines represent original captions anddotted lines represent paraphrase captions.
This corpus reflects a rich spectrum of everyday narratives people use inonline activities including figurative language (e.g., ?Sun is going to bed?
), casual language (e.g., Chillaxing at thebeach?
), and conversational language (e.g., ?Can you spot the butterfly?).
The numbers in the parenthesis show thecardinality of images associated with each caption.
Surprisingly, some of these descriptions are highly expressive,almost creative, and yet not unique ?
as all these captions are repeated almost verbatim by different individualsdescribing different images.can complement those based on prompted humanannotations.
The latter in general are literal andmechanical readings of the visual scenes, while theformer reflect a rich spectrum of natural languageutterances in everyday narratives, including figura-tive, pragmatic, and conversational language, e.g.,?can you spot the butterfly?
(Figure 1).
Therefore,this dataset offers unique opportunities for ground-ing figurative and metaphoric expressions using vi-sual context.In conjunction with the new corpus, pub-licly shared at http://www.cs.stonybrook.edu/?jianchen/deja.html, we also presentthree new tasks: visually situated paraphrases (?5);creative image captioning (?7), and creative visualparaphrasing (?7).
The central algorithm compo-nent in addressing all these tasks is a simple and yeteffective approach to image caption transfer that ex-ploits the unique association structure of the result-ing corpus (?3).Our empirical results collectively demonstratethat when the web data is available at such scale,it is possible to obtain a large-scale, high-qualitydataset with significantly less noise.
We hope thatour approach would be only one of the first attempts,and inspire future research to develop better ways ofmaking use of ever-growing multimodal web data.Although it is unlikely that the automatically gath-ered datasets can completely replace the curated de-scriptions written in a controlled setting, our hopeis to find ways to complement human annotateddatasets in terms of both the scale and also the di-versity of the domain and language.The remainder of this paper is organized as fol-lows.
First we describe the dataset collection proce-dure and insights (?2).
We then present a new ap-proach to image caption transfer based on the asso-ciation structure of the corpus (?3) followed by ex-perimental results (?4).
After then we present newconceptual tasks: visual paraphrasing (?5), creativeimage captioning, and creative visual paraphrasing(?7), interleaved with corresponding experimentalresults (?6, ?8).2 Dataset - Captions in RepetitionOur corpus consists of three components (Table 1):MAIN SET The first step is to crawl as manyimage-caption pairs as possible.
We use flickr.comsearch API to crawl 760 million pairs in total.
TheAPI allows searching images within a given timewindow, which enables exhaustive search over anytime span.
To ensure visual correspondence betweenimages and captions, we set query terms using 693most frequent nouns from the dataset of Ordonez etal.
(2011), and systematically slide time windowsover the year 2013.1For each image, we segmentits title and the first line of its description into sen-tences.The crawled dataset at this point includes a lot ofnoise in the captions.
Hence we apply initial filter-ing rules to reduce the noise.
We retain only thoseimage-sentence pairs in which the sentence containsthe query noun, and does not contain personal infor-mation indicators such as first-person pronouns.
We1To ensure enough number of images are associated witheach caption, we further search captions with no more than 10associated images across all years.505set # captions # imagesMAIN 176,780 3,967,524PARAPHRASE7,570 human-annotated triples353,560 auto-generated triplesFIGURATIVE6,088 quotations 180,18518,179 quotations +predicted figurative captions413,698Table 1: Corpus Statisticsmean std 25% 50% 75% max#imgs.
22.4 47.6 4 10 25 4617#tokens 4.9 3.3 3 4 5 178Table 2: Percentiles of the image count associated witheach caption and the number of tokens in each caption.want captions that are more than simple keywords,thus we discard trivial captions that do not includeat least one verb, preposition, or adjective.The next step is to find captions in repetition.
Forthis purpose, we transform captions into canonicalforms.
We lemmatize all words, convert prepositionsto a special token ?IN?2, and discard function words,numbers, and punctuations.
For instance, ?The birdflies in blue sky?
and ?A bird flying into the blue sky?have the same canonical form, ?bird fly IN blue sky?.We then retain only those captions that are repeatedwith respect to their canonical forms by more thanone user, and for distinctly different images to en-sure the generality of the captions.Retaining only captions that are repeated verba-tim may seem overly restrictive.
Nonetheless, be-cause we start with as many as 760 million pairs,this procedure yields nearly 180K unique captionsassociated with nearly 4M images.3What is moresurprising, as will be shown later, is that many ofthese captions are highly expressive.
Table 2 showsthe distribution of the number of images associatedwith each caption.4The median and mean are 10and 22.4 respectively, showing a high degree of con-nectivities between captions and images.PARAPHRASE SET Our dataset collection proce-dure finds one-to-many relations between captions2We do this transformation so as not to over-count uniquecaptions with trivial variations, but merging prepositions cansometimes combine prepositions that are not semantically com-patible.
We therefore also keep original captions with originalprepositions.3We also keep user annotated image tags if available.4Without counting additional edges created by visual para-phrasing (?5).- Hanging out with dad (*) - Snuggling with dad - Cuddles with dad- Life on the ocean waves (*) - Swimming in the ocean - Playing in the oceanPlaying(in(the(ocean(- Good morning sun (*) - Sun through the trees - Here comes the sunAutomatic Visual Paraphrases- Fly high in the sky (*) - Stretching to the sky - Reaching out to the sky!smiling(children(- Children see magic   because they look for it (*) - The soul is healed by being with childrenStretching(to(the(sky(.
(Reaching(out(to(the(sky(- A bee collecting pollen (*) - Bumble bee on purple flower - Working bee!Bumble(bee((on(purple(flower(!Working(bee(There(is(a(storm(rolling(in(Storm(clouds(coming(over(Big(storm(is(coming(!Big(storm(is(coming(!The(soul(is(healed(by(being(with(children((Crowd-sourced Visual Paraphrases!Storm(clouds((coming(over(Figure 2: Example visual paraphrases: automatic (left)and crowd-sourced (right).
The first caption marked with* indicates the original caption of the corresponding im-age.
Some paraphrases are not strictly equivalent to theoriginal caption if considered out of context, while theyare pragmatically adequate paraphrases given the image.figure of speech #caps.
example (#imgs.
)quotation&idiom 70 The early bird gets the worm (77)personification 43 Meditating cat (38)metaphor 24 Wine is the answer (7)question 18 Do you see the moon (82)dialog 11 Hello little flower (37)anaphora 6 Beads, beads and more beads (62)simile 5 The lake is like glass (23)hyperbole 1 In the land of a billion lights (3)Table 3: Distribution of figurative language out of 1000random captions (171 figurative captions in total)and images.
To extend these relations to many-to-many, we introduce visually-situated paraphrases(or visual paraphrases for shorthand) (?5).
A visualparaphrase relation is a triple (i, c, p), where imagei has an original caption c, caption p is the visualparaphrase for c situated in image i.
We collect vi-sual paraphrases for sample images in our dataset,using both crowd sourcing (7,570 triples) and an au-tomatic algorithm (353,560 triples) (see ?5 for de-tails).
Figure 2 shows example visual paraphrases.Formally, our corpus represents a bipartite graphG = (T, V,E), in which the set of captions T andthe set of images V are connected by typed edgese(c, i, t), where caption c ?
T , image i ?
V , andedge type t ?
{original, paraphrase}, which de-notes whether the image-caption association is givenby the original caption or by a visual paraphrase.FIGURATIVE SET We find that many repeatingcaptions are surprisingly lengthy and expressive,most of which turn out to be idiomatic expres-sions and quotations, e.g., ?faith is the bird thatfeels the light when the dawn is still dark?
fromTagore?s poem.
We look up goodreads.com506tree light flower home sun rain sky water beach girl020k40k60k80k100k#imagestree light flower home sun rain sky water beach girl01k2k3k4k #captionsFigure 3: Top 10 queries with the largest number of im-ages and unique captionspolarity% inall caps.mean/median#imgs.
per cap.example (#imgs)pos.
8% 20 / 8Happy bride and groom (282)The rock and pool,is nice and cool (4)neg.
2% 19.5 / 7Bad day at the office (269)Crying lightning (147)Table 4: Distribution of caption sentiment.
The polar-ity is determined by comparing number of positive wordsand negative words (>: positive; <: negative) accord-ing to a sentiment lexicon (Wilson et al, 2005) (countingonly words of strong polarity).and brainyquotes.com to identify 6K quota-tion captions illustrated by 180K images.
We alsopresent a manual labeling on a small subset of thedata (Table 3) to provide better insights into the de-gree and types of figurative speech used in naturalcaptions.
Using these labels we build a classifier(?7) to further detect 18K figurative captions asso-ciated with 410K images.INSIGHTS As additional insights into the dataset,Figure 3 shows statistics of the visual content, Ta-ble 5 shows syntactic types of the captions, and Ta-ble 4 shows positive and negative sentiment in cap-tions.3 Image Captioning using AssociationStructureWe demonstrate the usefulness of the association be-tween images and captions via retrieval-based imagecaptioning.
Given a query image q and the corpusG = (T, V,E), the task is to find a caption c ?
Tthat maximizes an affinity function A(q, c), whichmeasures how well the caption c fits the query im-age q,c?= argmaxc?T{A(q, c)} (1)Visual Neighborhood: Each textual description,e.g., ?reading a book?, can associate with many dif-type %caps.
%imgs.
mean #imgs.
std #imgs.verb45% 44% 22 9be, have, do, look,go, make, come, get,wait, take, love, play,walk, fly, see, watch,find, live, sleep, fallSky is the limit (3057)Home is where the heart is (2480)Lunch is served (2443)Let them eat cake (2193)Follow the yellow brick road (2077)prep 44% 41% 21 9in, of, on,at, with, for,from, by,over, throughOn the road (4617)After the rains (4450)Under the bridge (3443)At the beach (3203)adj 11% 15% 30 15old, little, new,red, blue, more,white, big, beautiful,blackHome sweet home (2398)Good morning sun (1122)Cabbage white butterfly (976)Next door neighbors (838)Table 5: Statistics on the syntactic composition of cap-tions.
verb: captions with at least one verb.
prep:prepositional phrases (without any verbs).
adj: adjectivephrases (without any verbs and prepositions).
For eachcaption type, we also show the top words that appear inthe most number of captions (left), and the top captionsthat are associated with largest number of images (right).ferent visual instantiations (Figure 4a).
Our datasetG = (T, V,E) serves as a database to navigate thepossible visual instantiations of descriptive captionsas observed in online photo sharing communities.Let Nc= {i|e(c, i, original) ?
E} denote theset of adjacent nodes (i.e., visual instantiations) of acaption c. To quantify how well a caption c describea query image q, we propose to examine caption c?svisual neighborhood Ncas provided in our dataset.Concretely, the affinityA(q, c) of a query image q toa caption c is a function ?
(q,Nc) of q and the visualneighborhood Ncdefined as:A(q, c) = ?
(q,Nc) =1??
?i=1sim(q,Nic) (2)where ?
is a parameter; sim(?, ?)
is a similarity func-tion of two images; and Nc= [N1c,N2c, ...,N|Nc|c]is sorted by sim(q,Nic) in descending order.Figure 4a illustrates the key insight: instead of di-rectly transferring the caption of the single imagewith the closest visual similarity to the query im-age (Ordonez et al, 2011), we propose to retrieve acaption based on the aggregated visual similarity be-tween its visual neighborhood and the query image.The idea is to prefer a caption for which the queryimage is likely to be a prototypical visual rendering(Ordonez et al, 2013; Deselaers and Ferrari, 2011),hence avoid an unusual association between the text507??
: Pelicans fly in formation??16??
: Pelicans fly in formation??
: Sunset over the seaQuery image qRank by image similarity?????
?,????
?,?????
: Sunset over the sea?Final ranking190Rerank by neighborhood-based affinity?????
?Original rankingReading a book1.
Can ?
describe ??2.
How well does ?
fits into ?????
: Visual Neighborhood of c (a) (b)?????
?Figure 4: (a) Using the association structure, we retrieve a caption for which the query image is likely to be aprototypical visual rendering.
We hypothesize that there can be multiple visual prototypes of a caption.
(b) Rerankingby visual neighborhood proximity.and the visual information.
Also, we hypothesizethat there could be several diverse visual prototypesof any given textual description c, so we focus ononly the top ?
nearest members of Nc.We apply the neighborhood-based affinity for im-age captioning via reranking (Figure 4b): first weretrieve a pool of K candidate captions by find-ing top K closest images based on their direct vi-sual similarity to the query image, then compute theneighborhood-based affinity to rerank the captions.5The proposed approach is similar in spirit to the non-parametric K nearest neighbor approach of (Boimanet al, 2008) in modeling image-to-concept similar-ity rather than image-to-image similarity, but differsin that our work is in the context of image descrip-tion generation rather than classification.4 Experiments: Association StructureImproves Image CaptioningBaselines: The proposed approach (to be referredas ASSOC) requires one-to-many mappings betweencaptions and images at scale ?
a unique prop-erty of our dataset.
We compare against two base-lines: instance-based retrieval of (Ordonez et al,2011) (INSTANCE) and Kernel Canonical Correla-tion Analysis (KCCA) (Hardoon et al, 2004; Hodoshet al, 2013).
We implement KCCA with Hardoon?scode6.
We use a linear kernel since non-linear ker-nels like RBF showed worse performance.5We set K = 100 and choose parameter ?
using a held-outdevelopment set of 300 images.
If there are less than ?
availableimages, we use them all.6http://www.davidroihardoon.com/Professional/Code_files/kcca_package.tar.gzmethod BLEU METEORINSTANCE 0.125 0.029KCCA 0.118 0.024?
?ASSOCgiw/ all 0.130 0.031ASSOCg+tw/ all 0.133 0.030ASSOCtiw/ all 0.126 0.029ASSOCgiw/ ?
0.172?
?0.033?ASSOCg+tw/ ?
0.159?
?0.033?ASSOCtiw/ ?
0.184??0.034?
?Table 6: Automatic evaluation for image captioning:The superscripts denote the image feature for reranking;gi: GIST; ti: Tinyimage; g+t:= gi + ti.
We report the bestsetting (gt) for INSTANCE and KCCA.
Results statisticallysignificant compared to INSTANCE with two-tailed t-testare indicated with * (p < 0.05) and ** (p < 0.005).Configurations: For image features, we follow(Ordonez et al, 2011) to experiment with two globalimage descriptors and their combination: a) the GISTfeature that represents the dominant spatial struc-ture of a scene (Oliva and Torralba, 2001); b) theTinyimage feature that represents the overall colorof an image (Torralba et al, 2008); c) a combi-nation of the two.
We compute the similarity assim(Q, I) = ??Q?
I?2.
The INSTANCE andthe KCCA approaches use the feature combination.The ASSOC approach also use the combination forpreparing candidate captions, but can use differentfeatures for reranking.Dataset: We randomly sample 1000 images withunique captions as test set.
The rest of the corpus isthe pool of caption retrieval after discarding: (1) theoriginal caption c and all of its associated images, toavoid potential unfair advantage toward ASSOC and(2) the 10K captions used for training KCCA and all508reranking feature INSTANCE ASSOCgi 42% 58%g+t 50% 50%ti 46% 54%Table 7: Human evaluation for image captioning: the% of cases judged as visually more relevant, in pairwisecomparisons.
gi: GIST; ti: Tinyimage; g+t:= gi+ti.of their associated images (about 280K).Evaluation.
Automatic evaluation remains to be achallenge (Elliott and Keller, 2014).
We report bothBLEU (Papineni et al, 2002) at 1 without brevitypenalty, and METEOR (Banerjee and Lavie, 2005)with balanced precision and recall.
Table 6 showsthe results: the ASSOC approach (w/ ?)
significantlyoutperforms the two baselines.
The largest improve-ment over INSTANCE is 60% higher in BLEU, and44% higher in METEOR, demonstrating the bene-fit of the innate association structure of our corpus.Using all visual neighborhood (ASSOC w/ all) doesnot yield as strong results as selective neighborhood(ASSOC w/ ?
), confirming our hypothesis that eachvisual concept can have diverse visual renderings.We also compute crowd-sourced evaluation on asubset (200 images) randomly sampled out of thetest set.
For each query image, we present two cap-tions generated by two competing methods in a ran-dom order.
Turkers choose the caption that is morerelevant to the visual content of the given image.
Weaggregate the choices of three turkers by majorityvoting.
As shown in Table 7, ASSOC shows overallimprovement over baselines, where the difference ismore pronounced when reranking is based on fea-ture sets that differ from the one used during the can-didate retrieval.5 Image Captioning using VisualParaphrasesWe present an exploration of visually situated para-phrase (or visual paraphrase in short hand), anddemonstrate their utility for image captioning.
For-mally, given our corpus G = (T, V,E), a visualparaphrase relation is a triple (i, c, p), where givenan image i ?
V and its original caption c ?
T (i.e.,e(c, i, original) ?
E), p ?
T is a visual paraphrasefor c situated in a visual context given by the imagei (i.e, e(p, i, paraphrase) ?
E).
We collect visualparaphrases using both human annotation and an au-tomatic algorithm.
(1) Visual Paraphrasing using Crowd-sourcing:We use Amazon Mechanical Turk to annotate vi-sual paraphrases for a subset of images in our cor-pus.
Given each image with its original caption,we showed 10 randomly sampled candidate captionsfrom our dataset that share at least one physical-object noun7with the original caption.
Turkerschoose all candidate captions that could also de-scribe the given image.
We collect 7,570 (i, c, p)paraphrase triples in total.
(2) Visual Paraphrasing using Associative Struc-ture: We also propose an algorithm for automaticvisual paraphrasing by adapting the ASSOC algo-rithm for image captioning (?3) as follows: givenan image-caption pair (i, c), it first prepares a set ofcandidate captions that share the largest number ofphysical-object nouns with c, which are likely to besemantically close to c; then we rerank the candidatecaptions using the same neighborhood-based affinityas described in ?3.We apply this algorithm to generate a large set ofvisual paraphrases.
For each caption in our corpus,we randomly sample two of its associated images,and generate one visual paraphrase for each image-caption pair, which yields 353,560 (i, c, p) triples.See Figure 2 for example paraphrases.5.1 Image Captioning using VisualParaphrasingWe propose to utilize automatically-generated visualparaphrases to improve the ASSOC approach (?3) forimage captioning.
One potential limitation of theASSOC approach is that for some captions, the num-ber of associated images might be too small for reli-able estimations of the neighborhood based affinity.We hypothesize that for a caption with a small visualneighborhood, merging its neighborhood with thoseassociated with its visual paraphrases will give amore reliable estimation of the affinity between aquery image and that caption.
Thus we modify theASSOC approach as follows.After preparing a pool of K candidate captions{c1, c2, .
.
.
, cK}, automatically generate a visualparaphrase (ii, ci, pi) for each (ii, ci); then rerankthe candidate captions by the following affinity func-tion that merges the visual neighborhood from the7under the WordNet ?physical entity.n.01?
synset509method BLEU METEOR AMTINSTANCE 0.125 0.029 N/AASSOCgi0.172 0.033 45%ASSOCgipara0.187 0.036 55%ASSOCti0.184 0.034 45%ASSOCtipara0.197 0.036 55%Table 8: Automatic and human evaluation of exploit-ing visual paraphrases for image captioning.
The super-scripts represent the image feature used in the rerank-ing step; gi: GIST; ti: Tinyimage.
The AMT columnshows the percentages of captions preferred by human asof better visual relevance, in pairwise comparisons.
Theimprovement of ASSOCparaover ASSOC is significant atp < 0.002 for BLEU, and p < 0.03 for METEOR with twotailed t-test.paraphrase,A(q, Ci) = ?
(q,Nci?Npi) (3)6 Experiments: Visual ParaphrasingImproves Image CaptioningThe experimental configuration basically follows ?4.We compare ASSOCpara, the visual-paraphrase aug-mented approach, to the vanilla ASSOC approach.The image feature setting is the one with which theASSOC approach performs best.
Both approachesuse the GIST+Tinyimage feature to prepare candi-date captions, then use either the GIST or Tinyimagefeature for reranking.Table 8 shows that the ASSOCparaapproach sig-nificantly improves the vanilla ASSOC method un-der both automatic and human evaluation.
As a ref-erence, the first row shows the performance of theINSTANCE method (?4).
The ASSOC method signif-icantly improves over the INSTANCE method.
On asimilar vein, the ASSOCparamethod further improvesover the ASSOC method, as automatic paraphrasesprovide a better visual neighborhood.
This improve-ment is remarkable since the paraphrasing associa-tion is added automatically without any supervisedtraining.
This demonstrates the usefulness of the bi-partite association structure of our corpus.7 Image Captioning with CreativityNaturally existing captions reflect everyday narra-tives, which in turn reflect figurative language usesuch as metaphor, simile, and personification.
Togain better insights, one of the authors manually cat-egorized a set of 1000 random captions.
About 17%are identified as figurative.
Table 3 shows the distri-bution over different types of figurative captions.Creative Language Classifier: Using the smallset of labels described above, we train a simple bi-nary classifier to identify captions with creative lan-guage.8Using this classifier, we can control the de-gree of literalness or creativity in generated captions.Based on 5-fold cross-validation, the classifier per-forms with 77% precision and 43% recall.Importantly, a high-precision and low-recall clas-sifier suffices our purpose.
It is because in the con-text of creative captioning and creative paraphrasingpresented below, we only need to detect some figu-rative captions, not all.7.1 Creative Image CaptioningGiven a query image q, we describe it with the mostappropriate figurative caption.
We propose the AS-SOCcreativeapproach that alters the ASSOC approach(?3) to return a figurative caption from the candidatepool, excluding literal captions.7.2 Creative Visual ParaphrasingGiven a query image q and its original caption c, werephrase c to a more creative and inspirational cap-tion that still describes q.
We use the PARAcreativeap-proach that changes our automatic visual paraphras-ing algorithm (?5), by retrieving only figurative cap-tions.8 Experiments: Creative ImageCaptioning and Paraphrasing8.1 Creative CaptioningWe compare the ASSOCcreativeapproach to thevanilla ASSOC approach.
With the ASSOC ap-proach, the top-rank caption is usually literal.
Bothapproaches use the GIST+Tinyimage feature forpreparing candidate captions, and the Tinyimagefeature for reranking, which is the best setting forthe ASSOC approach (?4).Similarly to ?4, we sample 200 test images fromour corpus, and use AMT to compare two algorithmsin terms of visual relevance and creativity sepa-rately.
For creativity, we ask turkers to choose one8We use a random forest classifier with features includingwords indicating reasoning (but, could, that), generality (never,always), caption length, abstract nouns (life, and hope), andwhether the caption is a known idiom or quotation.510method creativity relevanceASSOC 33% 41%ASSOCcreative67% 59%Table 9: Human evaluation for creative captioning: % ofcaptions preferred by judges in pairwise comparisonsof the two captions that is more creative and inspira-tional than the other to describe each given test im-age.
Results are shown in Table 9.
(1) Creativity.
For 2/3 of the query images,captions produced by the ASSOCcreativemethod arejudged as more creative than those produced by theASSOC method.
This result indirectly validates thatthe figurativeness classifier has a reasonable preci-sion to control the literalness of the system caption.
(2) Visual relevance.
Interestingly, not only thecaptions from the ASSOCcreativemethod are favoredas creative, they are also judged as visually more rel-evant than those from the ASSOC method, despitethat each figurative caption has lower neighborhood-based affinity than the literal counterpart.
We con-jecture that it is easier for human judges to be imag-inative and draw visual relevance between the queryimage and figurative captions than the literal coun-terparts.
This result also suggests that figurative lan-guage may be of practical use in image caption ap-plications as a means to smooth the potentially brit-tle system output.
Figure 5 shows example systemoutput.8.2 Creative Visual ParaphrasingWe test 200 images that are associated with literalcaptions as predicted by the figurativeness classi-fier.
The PARAcreativeapproach competes againsttwo baselines: 1) the ORIGINAL captions , and 2)a text-only variant of the PARAcaptionapproach sansvisual processing: it randomly chooses a figurativecaption that shares the largest number of physical-object nouns with the original caption, without look-ing at the query image.
This is for evaluating theeffect of visual context.In addition to the evaluations as in ?8.1, we alsouse a multiple-choice setting that allows a turker tochoose zero to two captions that are visually relevantto the query image.
See Table 10 for results, andFigure 5 for example outputs.method creativity relevancesingle multipleORIGINAL 32% 80% 87%PARAcreative68% 20% 60%PARAcaption56% 47% 63%PARAcreative44% 53% 74%Table 10: Human eval for creative visual paraphrasingI.
Comparing original captions with creativeparaphrases (ORIGINAL vs. PARAcreative): Theparaphrases are preferred over the original literalcaptions as more creative most of the time.
As forthe visual relevance, the original captions are fa-vored over the paraphrases most of the time in thesingle-choice competition.
However, when we usea multiple-choice setting, paraphrases has a reason-able relevance rate (60%), despite the simplicity ofthe algorithm.
The fact that the original captions hasa high relevance rate (87%) shows that in our cor-pus the captions have high visual relevance to theirassociated images most of the time.II.
Creative paraphrasing with and without thevisual context (PARAcaptionvs.
PARAcreative): Interms of creativity, the PARAcaptionmethod is pre-ferred over the PARAcreativemethod.
We conjec-ture that without conditioning on the visual con-tent, PARAcaptionmethod tends to retrieve more un-expected captions that make turkers think they aremore fun and creative.
As for the visual relevance,by conditioning on the visual context given by queryimages, the PARAcreativemethod significantly im-proves the visual relevance over the text-only coun-terpart, PARAcaptionmethod.
This result highlightsthe pragmatic differences between visually-situatedparaphrasing and text-based paraphrasing.9 Related workImage-caption corpus: Our work contributes tothe line of research that makes use of internet webimagery and text (Ordonez et al, 2011; Berg et al,2010) by detecting the visually relevant text (Dodgeet al, 2012) and reducing the noise (Kuznetsova etal., 2013b; Kuznetsova et al, 2014).
Compared todatasets with crowd-sourced captions (Hodosh et al,2013; Lin et al, 2014), in which each image is an-notated with several captions, our dataset presentsseveral images for each caption, a subset of whichalso includes visually situated paraphrases.
The as-511-Hood under a full moon (*) -Mirror, mirror on the lake -Sky on the way home(*) -Red sky at night,  Shepherd's delight-Bee on orange flowers(*) -When the flower looms,  the bees come uninvited -Lights in cave(*) -There is a light that never goes out -Sail on by (*) -Row, row, row your boat gently down the streamCreative Image Captioning Creative Visual Paraphrasing-City of lights (*) -Great balls of fire -Young roe deer(*) -The tree that looks like a deer    -The flight of the crane(*) -That?s a crane-long haired girl(*) -Diamonds are a girl's  best friend-Sky on the way home(*) -Go home, sky, you?re drunk-Falling water(*) -Can you see the dogs-Red Bean Pastries (*) -When life gives you lemons< Good >  < Bad >   < Bad >   < Good >Figure 5: Examples of creative captioning and creative visual paraphrasing.
The left column shows good examplesin blue, and the right column shows bad examples in red.
The captions marked with * are the original captions of thecorresponding query images.sociation structure of our dataset is analogous tothat of ImageNet (Deng et al, 2009).
Unlike Ima-geNet that is built for nouns (physical objects) listedunder WordNet (Miller, 1995), our corpus is builtfor expressive phrases and full sentences and con-structed without human curation.
Our corpus hasseveral unique properties to complement existingcorpora.
As explored in a very recent work of (Gonget al, 2014), we expect that it is possible to com-bine crowd-sourced and web-harvested datasets andachieve the best of both worlds.Image captioning: Our work contributes to theincreasing body of research on retrieval-based im-age captioning (Ordonez et al, 2011; Hodosh etal., 2013; Hodosh and Hockenmaier, 2013; Socheret al, 2014), by providing a new large-scale cor-pus with unique association structure between im-ages and captions, by proposing an algorithm thatexploits the structure, and by exploring two new di-mensions: (i) visually situated paraphrasing (and itsutility for retrieval-based image captioning), and (ii)creative image captioning.Paraphrasing: Most previous studies in para-phrasing have focused exclusively on text, and theprimary goal has been learning semantic equiva-lence of phrases that would be true out of context(e.g., (Barzilay and McKeown, 2001; Pang et al,2003; Dolan et al, 2004; Ganitkevitch et al, 2013)),rather than targeting situated or pragmatic equiva-lence given a context.
Emerging efforts began ex-ploring paraphrases that are situated in video con-tent (Chen and Dolan, 2011), news events (Zhangand Weld, 2013), and knowledge base (Berant andLiang, 2014).
Our work is the first to introduce vi-sually situated paraphrasing in which the task is tofind paraphrases that are conditioned on both the in-put text as well as the visual context.
(Chen andDolan, 2011) collected situated paraphrases onlythrough crowd sourcing, while we also explore auto-matic collection, and further test the quality of auto-matic paraphrases by using the learned paraphrasesin an extrinsic evaluation setting.Figurative language: There has been substantialwork for detecting and interpreting figurative lan-guage (Shutova, 2010; Li et al, 2013; Kuznetsovaet al, 2013a; Tsvetkov et al, 2014), while relativelyless work on generating creative or figurative lan-guage (Veale, 2011; Ozbal and Strapparava, 2012).We probe data-driven approaches to creative lan-guage generation in the context of image captioning.10 ConclusionTo conclude, we have provided insights into mak-ing a better use of multimodal web data in thewild, resulting in a large-scale corpus, Deja Image-Captions, with several unique properties to comple-ment datasets with crowdsourced captions.
To vali-date the usefulness of the corpus, we proposed newimage captioning algorithms using the associativestructure, which we extended to several related tasksranging from visually situated paraphrasing to en-hanced image captioning.
In the process we havealso explored several new tasks: visually situatedparaphrasing, creative image captioning, and cre-ative caption paraphrasing.Acknowledgement The research is supported inpart by NSF Award IIS 1447549 and IIS 1408287.512ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with improvedcorrelation with human judgments.
In Proceedings ofthe ACL Workshop on Intrinsic and Extrinsic Evalu-ation Measures for Machine Translation and/or Sum-marization, pages 65?72.Regina Barzilay and Kathleen R. McKeown.
2001.
Ex-tracting paraphrases from a parallel corpus.
In Pro-ceedings of the 39th Annual Meeting on Associationfor Computational Linguistics, pages 50?57.
Associa-tion for Computational Linguistics.Jonathan Berant and Percy Liang.
2014.
Semantic Pars-ing via Paraphrasing.
In Association for Computa-tional Linguistics (ACL).Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.2010.
Automatic attribute discovery and characteri-zation from noisy web data.
In ECCV 2010, pages663?676.
Springer.Oren Boiman, Eli Shechtman, and Michal Irani.
2008.In defense of Nearest-Neighbor based image classifi-cation.
In IEEE Conference on Computer Vision andPattern Recognition, 2008.
CVPR 2008, pages 1?8,June.David L. Chen and William B. Dolan.
2011.
Collect-ing highly parallel data for paraphrase evaluation.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies-Volume 1, pages 190?200.
Asso-ciation for Computational Linguistics.Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei.
2009.
Imagenet: A large-scale hier-archical image database.
In Computer Vision and Pat-tern Recognition, 2009.
CVPR 2009.
IEEE Conferenceon, pages 248?255.
IEEE.Thomas Deselaers and Vittorio Ferrari.
2011.
Visual andsemantic similarity in imagenet.
In Computer Visionand Pattern Recognition (CVPR), 2011 IEEE Confer-ence on, pages 1777?1784.
IEEE.Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-sch, Margaret Mitchell, Karl Stratos, Kota Yamaguchi,Yejin Choi, Hal Daum?e III, Alexander C. Berg, andothers.
2012.
Detecting visual text.
In Proceedingsof the 2012 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 762?772.
As-sociation for Computational Linguistics.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: Exploiting massively parallel news sources.
InProceedings of the 20th international conference onComputational Linguistics, page 350.
Association forComputational Linguistics.Desmond Elliott and Frank Keller.
2014.
Comparing au-tomatic evaluation measures for image description.
InProceedings of the 52nd Annual Meeting of the Associ-ation for Computational Linguistics, volume 2, pages452?457.Song Feng, Sujith Ravi, Ravi Kumar, Polina Kuznetsova,Wei Liu, Alexander C. Berg, Tamara L. Berg, andYejin Choi.
2015.
Refer-to-as Relations as SemanticKnowledge.
In AAAI Conference on Artificial Intelli-gence.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The ParaphraseDatabase.
In Proceedings of NAACL-HLT, pages 758?764, Atlanta, Georgia, June.
Association for Computa-tional Linguistics.Yunchao Gong, Liwei Wang, Micah Hodosh, Julia Hock-enmaier, and Svetlana Lazebnik.
2014.
Improv-ing Image-Sentence Embeddings Using Large WeaklyAnnotated Photo Collections.
In ECCV 2014, pages529?545.
Springer.David Hardoon, Sandor Szedmak, and John Shawe-Taylor.
2004.
Canonical correlation analysis: Anoverview with application to learning methods.
Neu-ral computation, 16(12):2639?2664.Micah Hodosh and Julia Hockenmaier.
2013.
Sentence-based image description with scalable, explicit mod-els.
In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition Workshops,pages 294?300.Micah Hodosh, Peter Young, and Julia Hockenmaier.2013.
Framing image description as a ranking task:data, models and evaluation metrics.
Journal of Artifi-cial Intelligence Research, 47(1):853?899.Polina Kuznetsova, Jianfu Chen, and Yejin Choi.
2013a.Understanding and Quantifying Creativity in LexicalComposition.
In EMNLP, pages 1246?1258.Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg,Tamara L. Berg, and Yejin Choi.
2013b.
General-izing Image Captions for Image-Text Parallel Corpus.In ACL (2), pages 790?796.Polina Kuznetsova, Vicente Ordonez, Tamara Berg, andYejin Choi.
2014.
TreeTalk: Composition and Com-pression of Trees for Image Descriptions.
Transac-tions of the Association for Computational Linguistics.Hongsong Li, Kenny Q. Zhu, and Haixun Wang.
2013.Data-Driven Metaphor Recognition and Explanation.TACL, 1:379?390.Tsung-Yi Lin, Michael Maire, Serge Belongie, JamesHays, Pietro Perona, Deva Ramanan, Piotr Doll?ar, andC.
Lawrence Zitnick.
2014.
Microsoft COCO: Com-mon Objects in Context.
In ECCV, Z?urich.Rebecca Mason and Eugene Charniak.
2014.
Nonpara-metric Method for Data-driven Image Captioning.
InNAACL.513Mary Meeker.
2014.
Internet Trends 2014.George A. Miller.
1995.
WordNet: a lexical database forEnglish.
Communications of the ACM, 38(11):39?41.Aude Oliva and Antonio Torralba.
2001.
Modeling theshape of the scene: A holistic representation of thespatial envelope.
International journal of computer vi-sion, 42(3):145?175.Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.2011.
Im2text: Describing Images Using 1 MillionCaptioned Photographs.
In NIPS, volume 1, page 4.Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C.Berg, and Tamara L. Berg.
2013.
From large scale im-age categorization to entry-level categories.
In Com-puter Vision (ICCV), 2013 IEEE International Confer-ence on, pages 2768?2775.
IEEE.Gozde Ozbal and Carlo Strapparava.
2012.
A Computa-tional Approach to the Automation of Creative Nam-ing.
In Proceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 703?711, Jeju Island, Korea, July.Association for Computational Linguistics.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations: Ex-tracting paraphrases and generating new sentences.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 102?109.
Association for Computa-tional Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th annual meeting on association for computationallinguistics, pages 311?318.
Association for Computa-tional Linguistics.Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-lia Hockenmaier.
2010.
Collecting Image Annota-tions Using Amazon?s Mechanical Turk.
In Proceed-ings of the NAACL HLT 2010 Workshop on CreatingSpeech and Language Data with Amazon?s Mechan-ical Turk, CSLDAMT ?10, pages 139?147, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Nikhil Rasiwasia, Pedro J. Moreno, and Nuno Vasconce-los.
2007.
Bridging the gap: Query by semantic ex-ample.
Multimedia, IEEE Transactions on, 9(5):923?938.Nikhil Rasiwasia, Jose Costa Pereira, EmanueleCoviello, Gabriel Doyle, Gert R.G.
Lanckriet, RogerLevy, and Nuno Vasconcelos.
2010.
A New Approachto Cross-modal Multimedia Retrieval.
In Proceedingsof the International Conference on Multimedia, MM?10, pages 251?260, New York, NY, USA.
ACM.Ekaterina Shutova.
2010.
Models of metaphor in NLP.In Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics, ACL ?10,pages 688?697, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-pher D. Manning, and Andrew Y. Ng.
2014.Grounded compositional semantics for finding anddescribing images with sentences.
Transactions ofthe Association for Computational Linguistics, 2:207?218.Antonio Torralba, Robert Fergus, and William T. Free-man.
2008.
80 million tiny images: A large data setfor nonparametric object and scene recognition.
Pat-tern Analysis and Machine Intelligence, IEEE Trans-actions on, 30(11):1958?1970.Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman, EricNyberg, and Chris Dyer.
2014.
Metaphor detectionwith cross-lingual model transfer.
In Proceedings ofACL.Tony Veale.
2011.
Creative Language Retrieval: A Ro-bust Hybrid of Information Retrieval and LinguisticCreativity.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 278?287, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of the conferenceon human language technology and empirical methodsin natural language processing, pages 347?354.
Asso-ciation for Computational Linguistics.Congle Zhang and Daniel S Weld.
2013.
Harvesting Par-allel News Streams to Generate Paraphrases of EventRelations.
In EMNLP, pages 1776?1786.514
