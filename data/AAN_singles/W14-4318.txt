Proceedings of the SIGDIAL 2014 Conference, pages 133?140,Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational LinguisticsExtractive Summarization and Dialogue Act Modeling on EmailThreads: An Integrated Probabilistic ApproachTatsuro Oya and Giuseppe CareniniDepartment of Computer ScienceUniversity of British ColumbiaVancouver, B.C.
Canada{toya, carenini}@cs.ubc.caAbstractIn this paper, we present a novel supervisedapproach to the problem of summarizingemail conversations and modeling dialogueacts.
We assume that there is a relationshipbetween dialogue acts and important sen-tences.
Based on this assumption, we intro-duce a sequential graphical model approachwhich simultaneously summarizes emailconversation and models dialogue acts.
Wecompare our model with sequential andnon-sequential models, which independent-ly conduct the tasks of extractive summari-zation and dialogue act modeling.
Anempirical evaluation shows that our ap-proach significantly outperforms all base-lines in classifying correct summarysentences without losing performance ondialogue act modeling task.1 IntroductionNowadays, an overwhelming amount of text in-formation can be found on the web.
Most of thisinformation is redundant and thus the task ofdocument summarization has attracted much at-tention.
Since emails in particular are used for awide variety of purposes, the process of automat-ically summarizing emails might be of greatbenefit in dealing with this excessive amount ofinformation.
Much work has already been con-ducted on email summarization.
The first re-search on this topic was conducted by Rambowet al.
(2004), who took a supervised learning ap-proach to extracting important sentences.
Astudy on the supervised summarization of emailthreads was also performed by Ulrich et al.(2009).
This study used the regression-basedmethod for classification.
There have been stud-ies on unsupervised summarization of emailthreads as well.
Zhou et al.
(2007, 2008) pro-posed a graph-based unsupervised approach toemail conversation summarization using cluewords, i.e., recurring words contained in replies.In addition, the task of labeling sentenceswith dialogue acts has become important and hasbeen employed in many conversation analysissystems.
For example, applications such as meet-ing summarization and collaborative task learn-ing agents use dialogue acts as their underlyingstructure (Allen et al., 2007; Murray et al.,2010).
In a previous work, Cohen et al.
(2004)defined a set of ?email acts?
and employed textclassification methods to detect these acts inemails.
Later, Carvalho et al.
(2006) employed acombination of n-gram sequences as features andthen used a supervised machine learning methodto improve the accuracy of this email act classifi-cation.
In addition, Shafiq et al.
(2011) presentedunsupervised dialogue act labeling methods.
Intheir work, they introduced a graph-based meth-od and two probabilistic sequence-labelingmethods for modeling dialogue acts.However, little work has been done on dis-covering the relationship between dialogue actsand extractive summaries.
If there is a relation-ship between them, combining these approachesso as to model both simultaneously will yieldbetter results.
In this paper, we investigate thishypothesis by introducing a new sequentialgraphical model approach that performs dialogueact modeling and extractive summarization joint-ly on email threads.2 Related WorkWhile email summarization and dialogue actmodeling have been effectively studied, in mostprevious work, these tasks were studied inde-pendently.
This section provides related work foreach task separately.1332.1 Extractive SummarizationRambow et al.
(2004) introduced sentence ex-traction techniques that work for email threads.In their work, they introduced email-specific fea-tures and used a machine learning method toclassify whether or not a sentence should be in-corporated into a summary.
Their experimentsdemonstrated that their features were highly ef-fective for email summarization.Ulrich et al.
(2009) proposed a regression-based machine learning approaches to emailthread summarization.
They compared regres-sion-based classifiers to binary classifiers andshowed that their approach significantly im-proves the summarization accuracy.
They em-ployed the feature set introduced by Rambow etal.
(2004) as their baseline and introduced newfeatures that are also effective for email summa-rization.
Some of their features refer to dialogueacts but the assumption is that they are computedbefore the summarization task is performed.
Ourwork is aimed at a much closer integration of thetwo tasks by modeling them simultaneously.Carenini et al.
(2007) developed a fragmentquotation graph that can capture a fine-grainconversation structure in email threads, whichwe will describe in detail in Section 3.
They thenintroduced a ClueWordSummarizer (CWS), agraph-based unsupervised summarization ap-proach based on the concept of clue words,which are recurring words found in email replies.Their experiment showed that the CWS performsbetter than the email summarization approach inRambow et al.
(2004).Extractive summarization using a sequentiallabeling technique has also been studied.
Whilethis is not an email summarization, Shen et al.
(2007) proposed a linear-chain Conditional Ran-dom Field (CRF) based approach for extractivedocument summarization.
In their work, theytreated the summarization task as a sequence la-beling problem to take advantage of interactionrelationships between sentences; their approachshowed significant improvement when comparedwith non-sequential classifiers.2.2 Dialogue Act ModelingThe first studies on the dialogue act modeling inemails were performed by Cohen et al.
(2004).They defined ?email speech acts?
(e.g., Request,Deliver, Propose, and Commit) and used ma-chine learning methods to classify emails accord-ing to the intent of the sender.Carvalho et al.
(2006) further developed thisinitial proposal by using contextual informationsuch as combinations of n-gram sequences inemails as their features for a supervised learningapproach.
The experiment showed that their ap-proach reduced classification error rates by26.4%.
Shafiq et al.
(2011) proposed unsuper-vised dialogue act modeling in email threads andon forums.
They introduced a graph-based andtwo probabilistic unsupervised approaches formodeling dialogue acts.
By comparing those ap-proaches, they demonstrated that the probabilis-tic approaches were quite effective andperformed better than the graph-based one.While the following work is not done on theemail domain, Kim et al.
(2010) introduced adialogue act classification on one-on-one onlinechat forums.
To be able to capture sequentialdialogue act dependency on chats, they applied aCRF model.
They demonstrated that, comparedwith other classifiers, their CRF model per-formed the best.
In their later work (Kim et al.,2012), they extended the domain to multi-partylive chats and proposed new features for thatdomain.3 Capturing Conversation Structure inEmail ThreadsIn this section, we describe how to build a frag-ment quotation graph which captures the conver-sation structure of any email thread at finergranularity.
This graph was developed andshown to be effective by Carenini et al.
(2011).A key assumption of this approach is that in or-der to effectively perform summarization anddialogue act modeling, a fine graph representa-tion of the underlying conversation structure isneeded.Here, we start with the sample email conver-sation shown in Figure 1 (a).
For convenience,the content of the emails is represented as a se-quence of fragments.First, we identify all new and quoted frag-ments.
For example, email E1 is composed ofone new fragment, ?b?, and one quoted fragment,?a?.
As for email E3, since we do not yet knowwhether or not ?d?
and ?e?
are different frag-ments, we consider E3 as being composed of onenew fragment, ?de?
and one quoted fragment, ?b?.Second, we identify distinct fragments.
To dothis, we first identify overlaps by comparingfragments with each other.
If necessary, we splitthe fragments and remove any duplicates fromthem.
For example, a fragment, ?de?, in E3 is134split into ?d?
and ?e?
after being compared withfragments in E4 and the duplicates are removed.By applying this process to all of the emails,seven distinct fragments, a, b ..., and, g remain inthis example.In the third step, edges which represent thereplying relationships among the fragments arecreated.
These edges are determined based on theassumption that any fragment is a reply to neigh-boring quotations (the quoted fragments immedi-ately preceding or following the current one).
Forexample, the neighboring nodes of ?f?
in E4 are?d?
and ?e?.
Thus, we create two edges from node?f?
in E4 to node ?d?
and ?e?
in E3.
In the sameway, we see that the neighboring node of ?g?
inE4 is ?e?.
Hence, there is one edge from node ?g?to ?e?.
If no quotation is contained in a replyemail, we connect the fragments in the email tofragments in emails to which it reply.In email threads, there are cases in which theoriginal email with its quotations is missing fromthe user?s folder, as in the case of ?a?
in Figure 1(a).
These types of emails are called hiddenemails.
Carenini et al.
(2005) studied in detailhow these email types might be treated and theirinfluence on email summarization.Figure 1 (b) shows the completed fragmentquotation graph of the email thread shown inFigure 1 (a).
In the fragment quotation graphstructure, all paths (e.g., a-b-c, a-b-d-f, a-b-e-f,and a-b-e-g in Figure 1 (b)) capture the adjacentrelationships between email fragments.
Hence,we use every path that can be derived from thegraph as our dataset.
However, in this case, whenwe run the labeling task on these paths, we ob-tain multiple labels for some of the sentencesbecause the sentences in fragments such as ?a?,?b?, and ?f?
in Figure 1 (b) are shared amongmultiple paths.
Therefore, to assign a label to oneof these sentences, we take the label more fre-quently assigned to that sentence when all itspaths are considered (i.e., the majority vote).4 FeaturesFor both dialogue act modeling and extractivesummarization, many effective sentence featureshave been discovered so far.
Interestingly, somecommon features are shown to be effective inboth tasks.
This section explains the featuresused in our model.
We begin with the featuresfor extractive summarization and then describehow we derive the features for dialogue act mod-eling.
All the features explained in this section,whether they belong to extractive summarizationor dialogue act modeling, are included in ourmodel.
(a) A possible configuration of an email conversation(E2 and E3 reply to E1, and E4 replies to E3)(b) An example of a fragment quotation graphFigure 1: A fragment quotation graph derived from apossible configuration of an email conversation4.1 Extractive Summarization FeaturesThe features we use for extractive summarizationare mostly from Carenini et al.
(2008) and Ram-bow et al.
(2004) and have proven to be effectiveon conversational data.
Details of these featuresare described below.
Note that all sentences in anemail thread are ordered based on paths derivedfrom a fragment quotation graph.Length Feature: The number of words ineach sentence.Relative Position Feature: The number ofsentences preceding the current divided bythe total number of sentences in one path.Thread Name Overlaps Feature: The num-ber of overlaps of the content words betweenthe email thread title and a sentence.Subject Name Overlaps Feature: The num-ber of overlaps of the content words betweenthe subject of the email and a sentence.Question Feature: A binary feature that in-dicates whether or not a sentence has a ques-tion mark.CC Feature: A binary feature that indicateswhether or not an email contains CC.135Participation Dominance Feature: Thenumber of utterances each person makes inone path.Finally, we also include a simplified version ofthe ClueWordScore (CWS) developed byCarenini et al.
(2007), which is listed below.Simplified CWS Feature: The number ofoverlaps of the content words that occur inboth the current and adjacent sentences in thepath, ignoring stopwords.4.2 Dialogue Act FeaturesThe relative positions and length features haveproven to be beneficial to both tasks (Jeong et al.,2009; Carenini et al., 2008).
Hence, these arecategorized as both dialogue acts and extractivesummarization features.
In addition, we use wordand POS n-grams as our features for dialogue actmodeling.
These features are extracted by thefollowing process explained in Carvalho et al.(2006).
However, we extend the original ap-proach in order to further abstract n-gram fea-tures to avoid making them too sparse to beeffective.
In this section, we describe the deriva-tion process in detail.A multi-step approach is used to generateword n-gram features.
First, all words are taggedwith the named entity using the Stanford NamedEntity Recognizer (Finkel et al., 2005), and arethen replaced with these tags.
Second, a se-quence of word-replacement tasks is applied toall email messages.
Initially, some types of punc-tuation marks (e.g., <>()[];:.
and ,) and extraspaces are removed.
Then, shortened phrasessuch as ?I?m?
and ?We?ll?
are substituted formore formal versions such as ?I am?
and ?Wewill?.
Next, other replacement tasks are per-formed.
Some of them are described in Table1.In the third step, unigrams and bigrams are ex-tracted.
In this paper, unigrams and bigrams referto all possible sequences of length one and twoterms.
After extracting all unigrams and bigramsfor each dialogue act, we then compute Infor-mation Gain Score (Forman, 2003) and select then-grams whose scores are in the top five greateston the training set.
In this way, we can automati-cally detect features that represent the character-istics of each dialogue act.
In addition to word n-grams, we also include POS n-grams in our fea-tures.
In a similar way, we first tag each word insentences with POS using the Stanford POS tag-ger (Toutanova et al., 2003).
Then, for each dia-logue act, we extract bigrams and trigrams, all ofwhich are scored by the Information Gain.
Basedon their scores, we select the POS bigram andtrigram features whose scores are within the topfive greatest.
One example of word n-gram fea-tures for a Question dialogue act selected by thisderivation method is shown in Table 2.Pattern Replacement?why?,  ?where?,  ?who?,  ?what?
?when?
[WWHH]nominative pronouns [I]objective pronouns [ME]'it',  'those',  'these',  'this',  'that' [IT]'will',  ?would',  'shall',  'should', 'must' [MODAL_STRONG]?can',  'could',  'may',  'might' [MODAL_WEAK]'do',  'does',  'did',  ?done' [DO]'is',  'was',  'were',  'are',  'been' 'be',  'am' [BE]'after' , 'before',  'during' [AAAFTER]?Jack?, ?Wendy?
[Personal_PRONOUN]?New York?
[LOCATION]?Acme Corp.?
[ORGANIZATION]Table 1: Some Preprocessing Replacement PatternWord Unigram Word Bigram?
[MODAL_STRONG] [I]anyone [IT] ?WWHH [DO] anyonedeny [WWHH] [BE][Personal _PRONOUN] [BE] [IT]Table 2: Sample word n-grams selected as the fea-tures for Question dialogue act5 The Sequential Labeling TaskWe use a Dynamic Conditional Random Field(DCRF) (Sutton et al., 2004) for labeling tasks.A DCRF is a generalization of a linear-chainCRF which allows us to represent complex inter-action between labels.
To be more precise, it is aconditionally-trained undirected graphical modelwhose structure and parameters are repeated overa sequence.
Hence, it is the most appropriatemethod for performing multiple labeling tasks onthe same sequence.136Our DCRF uses the graph structure shown inFigure 2 with one chain (the top X nodes) model-ing extractive summary and the other (the middleY nodes) modeling dialogue acts.
Each node inthe observation sequence (the bottom Z nodes)corresponds to each sentence in a path of thefragment quotation graph of the email thread.
Asshown in Figure 2, the graph structure capturesthe relationship between extractive summariesand dialogue acts by connecting their nodes.We use Mallet1 (McCallum, 2002) to implementour DCRF model.
It uses l2-based regularizationto avoid overfitting, and a limited BFGS fittingalgorithm to learn the DCRF model parameters.Also, it uses tree-based reparameterization(Wainwright et al., 2002) to compute the poste-rior marginal, or inference.Figure 2: The DCRF model used to create extractivesummaries and model dialogue acts6 Empirical Evaluations6.1 Dataset SetupIn our experiment, the publically available BC3corpus2 (Ulrich et al., 2008) is used for trainingand evaluation purposes.
The corpus containsemail threads from the World Wide Web Con-sortium (W3C) mailing list.
It consists of 40threads with an average of five emails per thread.The corpus provides extractive summaries ofeach email thread, all of which were annotatedby three annotators.
Hence, we use sentences thatare selected by more than one annotator as thegold standard summary for each conversation.In addition, all sentences in the 39 out of 40threads are annotated for dialogue act tags.
Thetagset consists of five general and 12 specifictags.
All of these tags are based on Jeong et al.(2009).
For our experiment, considering that ourdata is relatively small, we decide to use thecoarser five tag set.
The details are shown in Ta-ble 3.1 http://mallet.cs.umass.edu2 http://www.cs.ubc.ca/nest/lci/bc3.htmlTag Description Relative Frequency (%)S Statement 73.8Q Question 7.92R Reply 5.23Su Suggestion 5.62M Miscellaneous 7.46Table 3: Dialogue act tag categories and their relativefrequency in the BC3 corpusAfter removing quoted sentences and redundantinformation such as senders and addresses, 1300distinct sentences remain in the 39 email threads.The detailed content of the corpus is summarizedin Table 4.TotalDatasetNo.
of Threads 39No.
of Sentences 1300No.
of Extractive Summary Sentences 521No.
of S Sentences 959No.
of Q Sentences 103No.
of R Sentences 68No.
of Su Sentences 73No.
of M Sentences  97Table 4: Detailed content of the BC3 corpus6.2 Evaluation MetricsHere, we introduce evaluation metrics for ourjoint model of extractive summarization and dia-logue act recognition.The CRF model has been shown to be the ef-fective one in both dialogue act modeling andextractive summarization (Shen et al., 2007; Kimet al., 2010; Kim et al., 2012).
Hence, for com-parison, we implement two different CRFs, onefor extractive summarization and the other fordialogue act modeling.
When classifying extrac-tive summaries using the CRF, we only use itsextractive summarization features.
Similarly,when modeling dialogue acts, we only use itsdialogue act features.
In addition, we also com-137pare our system with a non-sequential classifier,a support vector machine (SVM), with the samesettings as those described above.
For these im-plementations, we use Mallet and SVM-lightpackage3 (Joachims, 1999).In our experiment, we first measure separate-ly the performance of extractive summarizationand dialogue act modeling.
The performance ofextractive summarization is measured by its av-eraged precision, recall, and F-measure.
For dia-logue acts, we report the averaged-micro andmacro accuracies as well as the averaged accura-cies of each dialogue act.Second, we evaluate the combined perfor-mance of extractive summarization and dialogueact modeling tasks.
In general, we are interestedin the dialogue acts in summary sentences be-cause they can be later used as input for othernatural language processing applications such asautomatic abstractive summarization (Murray etal., 2010).
Therefore, we measure the perfor-mance of our model with the following modifiedprecision (Pre?
), recall (Rec?
), and F-measure(F?
):{                                         }{                                              }(1){                                        }{                            }(2)(3)where a correctly classified sentence refers to atrue summary sentence that is classified as suchand whose dialogue acts are also correctly classi-fied.6.3 Experiment ProcedureFor all cases, we run five sets of 10-fold crossvalidation to train and test the classifiers on ashuffled dataset and calculate the average of theresults.
For each cross validation run, we extractall features following the process described inSection 4 on the training set.
When comparingthese two baselines with our model, we report p-values obtained from a student paired t-test onthe results to determine their significance.3 http://www.cs.cornell.edu/people/tj/svm_light6.4 ResultsThe performances of extractive summarizationand dialogue act modeling using the three meth-ods are summarized in Table 5 and 6, respective-ly.DCRF CRF SVMF-measure 0.485 0.428 0.397t-test?s  p-value   0.00046 2.5E-07Precision 0.562 0.591 0.675Recall 0.457 0.370 0.308Table 5: A comparison of the extractive summariza-tion performance of our DCRF model and the twobaselines based on precision, recall, and F-measureDCRF CRF SVMMicro Accuracy 0.785 0.779 0.775t-test?s p-value   0.116 0.036Macro Accuracy 0.516 0.516 0.304t-test?s p-value   0.950 5.2E-32S Accuracy 0.901 0.892 0.999Q Accuracy 0.832 0.809 0.465R Accuracy 0.580 0.575 0.05Su Accuracy 0.139 0.108 0.00M Accuracy 0.126 0.198 0.00Table 6: A comparison of the dialogue act modelingperformance of our DCRF model and the two base-lines based on averaged accuraciesFrom Table 5, we observe that, in terms ofextractive summarization results, our DCRFmodel significantly outperforms the two base-lines.
Noticeable improvements can be seen forthe recall and F-measure.
In terms of F-measure,compared with the CRF and SVM, our modelimproves by 5.7% and 8.8% respectively.
The p-values obtained from the t-test indicate that ourresults are statistically significantly different (p <0.05) from those of the two baselines.Regarding dialogue act modeling, the resultsare summarized in Table 6.
While no improve-ment is shown for the micro-averaged accuracy,our model and the CRF significantly outperformthe SVM in terms of the macro-averaged accura-138cy.
Both our model and the CRF consider thesequential structure of the conversation, which isnot captured in the SVM model.
Clearly, thisindicates that the sequential models are effectivein modeling dialogue acts due to their ability tocapture the inter-utterance relations of conversa-tions.Compared with the CRF, our DCRF modeloutperforms it in most cases except in classifyingthe ?M?
dialogue act.
However these improve-ments are not significant as t-test of both macroand micro-averaged accuracies indicate that thedifferences are not statistically significant (p >0.05).Another item to be mentioned here is that theaccuracies of classifying ?R?, ?Su?
and ?M?
dia-logue acts are relatively low.
This issue appliesto all classifiers and is plausibly due to the smalldataset.
There are only 68, 73 and 97 sentences,respectively, out of 1300 that are labeled as ?R?,?Su?
and ?M?
in the BC3 corpus.
Since our dia-logue act classifiers rely heavily on n-gram fea-tures, were the data small, these features wouldbe too sparse to effectively represent the charac-teristics of the dialogue acts.
However, comparedwith the SVM results, our joint model and theCRF perform significantly better in classifyingthese dialogue acts.
This also explains why thesequential model is preferable in dialogue actmodeling.Note that despite the small dataset, all theclassifiers are relatively accurate in classifying?Q?.
This is because n-gram features selected for?Q?
such as ???
and ?WWHH?
are very specific tothis dialogue act, which makes the task of ?Q?classification easier compared to those of others.Next, we discuss the result of the com-bined performance.
The performances of ourmodel and the two baselines are summarized inTable 7.DCRF CRF SVMF-measure?
0.352 0.324 0.292t-test?s  p-value  0.015 3.3E-05Precision?
0.407 0.450 0.501Recall?
0.335 0.280 0.227Table 7: A comparison of the overall performance ofour DCRF model and the two baselines based onmodified precision, recall and F-measureWe see that our DCRF model significantlyoutperforms the two baselines.
While our modelyields the lowest Pre?
of all, its Rec?
is muchgreater than the other two baselines and thisleads to its achieving the highest F?.
Comparedwith the CRF and SVM, the F?
obtained fromour system improves by 2.8% and 6% respec-tively.
In addition, the p-values show that theresults of our model are statistically significant(p < 0.05) compared with those of the two base-lines.Overall, these experiments clearly indicatethat our model is effective in classifying bothdialogue acts and summary sentences.7 Conclusions and Future WorkIn this work, we have explored a new automatedapproach for extractive summarization and dia-logue act modeling on email threads.
In particu-lar, we have presented a statistical approach forjointly modeling dialogue acts and extractivesummarization in a single DCRF.
The empiricalresults demonstrate that our approach outper-forms the two baselines on the summarizationtask without loss of performance on the dialogueact modeling one.
In the future, we would like toextend our approach by exploiting more effectivefeatures.
We also plan to apply our approach todifferent domains possessing large dataset.AcknowledgementsWe are grateful to Yashar Mehdad, Raimond Ng,Maryam Tavafi and Shafiq Joty for their com-ments and UBC LCI group and ICICS for finan-cial support.ReferencesJ.
Allen, N. Chambers, G. Ferguson, L. Galescu, H.Jung, and W. Taysom.
Plow: A collaborative tasklearning agent.
In AAAI-07, pages 22?26, 2007.Giuseppe Carenini, Gabriel Murray, and RaymondNg.
2011.
Methods for Mining and SummarizingText Conversations.
Morgan Claypool.Giuseppe Carenini, Raymond Ng, and XiaodongZhou.
2005.
Scalable discovery of hidden emailsfrom large folders.
In ACM SIGKDD?05, pages544?549.Giuseppe Carenini, Raymond Ng, and XiaodongZhou.
2008.
Summarizing Emails with Conversa-tional Cohesion and Subjectivity In proceeding46th Annual Meetint Assoc.for Computational Lin-guistics, page 353-361.139Giuseppe Carenini, Raymond Ng, and XiaodongZhou.
2007.
Summarizing email conversationswith clue words.
16th International World WideWeb Conference (ACM WWW?07).Vitor R. Carvalho and William W. Cohen.
2006.
Im-proving ?email speech acts?
analysis via n-gramselection.
In Proceedings of the HLT-NAACL 2006Workshop on Analyzing Conversations in Text andSpeech, ACTS ?09, pages 35?41, Stroudsburg, PA,USA.
Association for Computational Linguistics.William W. Cohen, Vitor R. Carvalho, and Tom M.Mitchell.
2004.
Learning to classify email into?speech acts?.
In Proceedings of Empirical Meth-ods in Natural Language Processing, pages 309?316, Barcelona, Spain, July.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating Non-local Infor-mation into Information Extraction Systems byGibbs Sampling.
In Proceedings of the 43nd An-nual Meeting of the Association for ComputationalLinguistics (ACL 2005), pp.
363-370.George Forman.
2003.
An extensive empirical studyof feature selection metrics for text classification.The Journal of Machine Learning Research,3:1289?1305.Minwoo Jeong, Chin-Yew Lin, and Gary GeunbaeLee.
2009.
Semi-supervised speech act recognitionin emails and forums.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing.Thorsten Joachims.
1999 Making large-Scale SVMLearning Practical.
Advances in Kernel Methods -Support Vector Learning, B. Sch?lkopf and C.Burges and A. Smola (ed.
), MIT-Press, 1999.Shafiq Joty, Giuseppe Carenini, and Lin, Chin-YewLin.
2011.
Unsupervised Modeling of Dialog Actsin Asynchronous Conversations.
In Proceedings ofthe twenty second International Joint Conferenceon Artificial Intelligence (IJCAI) 2011.
Barcelona,Spain.Shafiq Joty, Giuseppe Carenini, Gabriel Murray, andRaymond Ng.
2009 Finding Topics in Emails: IsLDA enough?
NIPS-2009 workshop on applica-tions for topic models: text and beyond.
Whistler,Canada.McCallum, A. Kachites, 2002.
MALLET: A MachineLearning for Language Toolkit.http://mallet.cs.umass.edu.Su Nam Kim, Lawrence Cavedon, and TimothyBaldwin.
2010a.
Classifying dialogue acts in 1-to-1live chats.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP 2010), pages 862?871, Boston,USA.Su Nam Kim, Lawrence Cavedon and Timothy Bald-win (2012) Classifying Dialogue Acts in Multi-party Live Chats, In Proceedings of the 26th Pacif-ic Asia Conference on Language, Information andComputation (PACLIC 26), Bali, Indonesia, pp.463?472.Gabriel Murray and Giuseppe Carenini.
2008.
Sum-marizing Spoken and Written Conversations.
Em-pirical Methods in NLP (EMNLP 2008), Waikiki,Hawaii, 2008.Gabriel Murray and Giuseppe Carenini.
2010.
Sum-marizing Spoken and Written Conversations.
Gen-erating and Validating Abstracts of MeetingConversations: a User study (INLG 2010), Dublin,Ireland, 2010.Gabriel Murray, Renals Steve, and Carletta Jean.2005a.
Extrative summarization of meeting record-ings.
In Proceeding of Interspeech 2005, Lisbon,Portugal, pages 593-596.Owen Rambow, Lokesh Shrestha, John Chen, andChirsty Lauridsen.
2004.
Summarizing emailthreads.
In Proceedings of HLTNAACL 2004.Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, andZheng Chen.
2007.
Document summarization us-ing conditional random fields.
In Proc.
of IJCAI,volume 7, 2862?2867.Charles Sutton, Khashayar Rohanimanesh, and An-drew McCallum.
2004.
Dynamic conditionalrandom fields: Factorized probabilistic models forlabeling and segmenting sequence data.
In Proc.ICML.Maryam Tavafi, Yashar Mehdad, Shafiq Joty,Giuseppe Carenini and Raymond Ng.
2013.
Dia-logue Act Recognition in Synchronous and Asyn-chronous Conversations.
In Proceedings of theSIGDIAL 2013 Conference, pages 117?121, Metz,France.
Association for Computational Linguistics.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Net-work.
In Proceedings of HLT-NAACL 2003, pp.252-259.Jan Ulrich, Giuseppe Carenini, Gabriel Murray,and Raymond T. Ng: Regression-Based Summari-zation of Email Conversations.
ICWSM 2009Jan Ulrich, Gabriel Murray, and Giuseppe Carenini.2008.
A publicly available annotated corpus forsupervised email summarization.
AAAI-2008EMAIL Workshop.Martin J. Wainwright, Tommi Jaakkola, and Alan S.Willsky.
2002.
Treebased Reparameterization forApproximate Inference on Loopy Graphs.
In Ad-vances in Neural Information Processing Systems14, pages 1001 1008.
MIT Press.140
