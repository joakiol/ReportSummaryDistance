Experiments in EvaluatingInteractive Spoken Language Systems 1Joseph Polifroni, Lynette Hirschman, Stephanie Seneff, and Victor ZueSpoken Language Systems GroupLaboratory  for  Computer  Sc ienceMassachuset ts  Ins t i tu te  of Techno logyCambr idge ,  Massachuset ts  02139ABSTRACTAs the DARPA spoken language community moves to-wards developing useful systems for interactive problem solv-ing, we must explore alternative valuation procedures thatmeasure whether these systems aid people in solving problemswithin the task domain.
In this paper, we describe severalexperiments exploring new evaluation procedures.
To look atend-to-end evaluation, we modified our data collection pro-cedure slightly in order to experiment with several objectivetask completion measures.
We found that the task completiontime is well correlated with the number of queries used.
Wealso explored log file evaluation, where evaluators were askedto judge the clarity of the query and the correctness of the re-sponse based on examination of the log file.
Our results howthat seven evaluators were unanimous on more than 80% ofthe queries, and that at least 6 out of 7 evaluators agreed over90% of the time.
Finally, we applied these new procedures tocompare two systems, one system requiring a complete parseand the other using the more flexible robust parsing mecha-nism.
We found that these metrics could distinguish betweenthese systems: there were significant differences in ability tocomplete the task, number of queries required to complete thetask, and score (as computed through a log file evaluation)between the robust and the non-robust modes.INTRODUCTIONFor the first two years of the DARPA Spoken Lan-guage Program, common evaluation in the ATIS domainhas been performed solely with the Common AnswerSpecification (CAS) protocol \[4\], whereby a system's per-formance is determined by comparing its output, ex-pressed as a set of database tuples, with one or morepredetermined reference answers \[1\].
The CAS protocolhas the advantage that system evaluation can be car-ried out automatically, once the principles for generatingthe reference answers have been established and a cor-pus has been annotated accordingly.
Since direct com-parison across systems can be performed relatively easilywith this procedure, we have been able to achieve crossfertilization of research ideas, leading to rapid researchprogress.1This research was supported by DARPA under ContractN000\] 4-89-J-1332, monitored through the Office of Naval Research.28QUERY 1:RESPONSE I:QUERY 2:RESPONSE 2:PLEASE LIST THE FLIGHT FROM PITTSBURGHTO BALTIMORE THAT WILL HE MADE BY SIXSEAT AIRPLANE ON JUNE TWENTIETHThere are no flights from pittsburgh toBaltimore leaving before 6:00 a.m. onSaturday June 20.LIST THE NUMBER OF FIRST CLASS FLIGHTSAVAILABLE ON DELTA AIRLINES279Table 1: Two examples of queries from the February '92Evaluation for which the CAS evaluation gives a misleadingassessment of the system's ability to understand.However, CAS evaluation is not without its shareof shortcomings.
One unfortunate outcome of evaluat-ing systems based on answers retrieved from the flightdatabase is that we cannot separately assess the system'sunderstanding of the query from its ability to retrievethe correct information and to conform to the prescribedoutput format.
In the best case, researchers may end upspending a considerable amount of time worrying aboutthe form and substance of the answer, which has littleto do with language understanding.
In the worst case,the results of the evaluation may be down right mislead-ing.
Consider the two examples from the February 1992test-set shown in Figure 1.
For Query 1, the systemmisunderstood the phrase "by six" as meaning "before6:00 a.m." Nonetheless, the answer is judged correct,because both the hypothesized and reference answers arethe NULL set, i.e., no flights satisfy the set of constraints.For Query 2, the system found 279 flights, but the cor-rect answer is 278.
The erroneous extra flight is the oneconnecting flight in the database shared by two airlines,Delta and USAIR.Another shortcoming of the present evaluation pro-cedure is that it has no place for interactive dialogue.In a realistic application, the user and the computer areoften partners in problem solving, in which the final so-lution may be best obtained by allowing both sides totake the initiative in the conversation.
Since the hu-man/computer dialogue can vary widely from system tosystem, it is impossible to use the data collected from onesystem to evaluate another system without making avail-able the computer's half of the conversation.
Even then,the system being tested becomes an observer analyzingtwo sides of a conversation rather than a participant.II Measurements I\[ Mean \[Std.
Dev.
I\[Total ~ of Queries Used 4.8 1.6# of Queries with Error Messages 1.0 1.4Time to Completion (S.) 166.1 66.0To be sure, the current evaluation protocol has servedthe community well.
The refinements made during thelast year have significantly improved its ability to pro-vide an objective benchmark.
However, as we continueto press forward in developing useful spoken languagesystems that can help us solve problems, we must cor-respondingly expand the battery of evaluation protocolsto measure the effectiveness of these systems in accom-plishing specific tasks.At the March 1991 meeting of the SLS CoordinatingCommittee, a working group was formed with the specificgoal of exploring methodologies that will help us evaluateif, and how welt, a spoken language system accomplishesits task in the ATIS domain.
The consensus of the work-ing group was that, while we may not have a clear ideaabout how to evaluate overall system performance, it isappropriate to conduct experiments in order to gain ex-perience.
The purpose of this paper is to describe threeexperiments conducted at MIT over the past few monthsrelated to this issue.
These experiments explored a num-ber of objective and subjective valuation metrics, andfound some of them to be potentially helpful in deter-mining overall system performance and usefulness.END-TO-END EVALUATIONIn order to carry out end-to-end evaluation, i.e., eval-uation of overall task completion effectiveness, we mustbe able to determine precisely the task being solved, thecorrect answer(s), and when thesubject  is done.
Oncethese factors have been specified, we can then computesome candidate measures and see if any of them areappropriate for characterizing end-to-end system perfor-mance.While true measures of system performance will re-quire a (near) real-time spoken language system, we feltthat some preliminary experiments could be conductedwithin the context of our ATIS data collection effort \[3,2\].In our data collection paradigm, a typist types in thesubject's queries verbatim, after removing disfluencies.All subsequent processing is done automatically by thesystem.
To collect data for end-to-end evaluation, wemodified our standard ata collection procedure slightly,by adding a specific scenario which has a unique answer.For this scenario, the subjects were asked to report theanswer explicitly.As a preliminary experiment, we used two simple sce-narios.
In one of them, subjects were asked to determineTable 2: Objective nd-to-end measures.the type of aircraft used on a flight from Philadelphia toDenver that makes a stop in Atlanta and serves break-fast.
Subjects were asked to end the scenario by saying"End scenario.
The answer is" followed by a statementof the answer, e.g., "End scenario.
The answer is Boe-ing 727."
From the log files associated with the sessionscenario, we computed a number of objective measures,including the success of task completion, task completiontime, the number of successful and the number of unsuc-cessful queries (producing a "no answer" message) 2.We collected ata from 29 subjects and analyzed thedata from 24 subjects 3.
All subjects were able to com-plete the task, and statistics on some of the objectivemeasures are shown in Table 2.Figure 1 displays catter plots of the number of queriesused by each subject as a function of the task completiontime.
A least-square fit of the data is superimposed.
Thenumber of queries used is well correlated with the taskcompletion time (R = 0.84), suggesting that this measuremay be appropriate for quantifying the usefulness of sys-tems, at least within the context of our experiment.
Alsoplotted are the number of queries that generated a "noanswer" message.
The correlation of this measure withtask completion time is not as good (R = 0.66), possiblydue to subjects' different problem solving strategies andabilities.LOG FILE EVALUATIONWe also conducted a different set of experiments toexplore subject-based evaluation metrics.
Specifically, weextracted from the log files pairs of subject queries andsystem responses in sequence, and asked evaluators tojudge the clarity of the query (i.e., clear, unclear, or un-intelligible) and the correctness of the response (correct,partially correct, incorrect, or "system generated an errormessage").
A program was written to enable evaluatorsto enter their answers on-line, and the results were tab-ulated automatically.
We used seven evaluators for thisexperiment, all people from within our group.
Four peo-ple had detailed knowledge of the system and the desig-2The system generates a range of diagnostic messages, reportingthat it cannot parse, or that it cannot formulate a retrieval query,etc.3Data from the remaining subjects were not analyzed, since theyhave been designated by NIST as test material.2910'2,?
# of queries ?n # of error  messages?
lid ?
?
@11 \[\]?
f ' .
:  .
.i 00  200  300  400T ime to  Complet ionFigure 1: Relationship beween task completion time and thetotal number of queries used, and the number of queries thatgenerated a "no answer" message.nated correct reference answers.
Three of the evaluatorswere familiar with the ATIS system capabilities, but didnot have a detailed knowledge of what constituted a cor-rect reference answer for the comparator.
Our anedyses,based on data from 7 evaluators, indicate that 82% of thetime there was unanimous agreement among the evalua-tors, and there were 1 or fewer disagreements 92% of thetime.ii!A2?
:0 I 2 3 4Number  of  Eva luators  Who D isagreeFigure 2: Consistency of the 7 evaluators' answers duringlog file evaluation.
The data are based on 115 query/answerpairs.These results suggest that reasonable agreement ispossible using humans to evaluate log files.
Such on-lineevaluation is also quite cost effective; the evaluators wereeach able to check the 115 query/answer pairs in 30-45minutes.SYSTEM COMPARISONEXPERIMENTBuilding on the results of the pilot experiments onend-to-end and log file evaluation, we designed an exper-iment to test whether these metrics would be useful indistinguishing the performance of two systems on a morecomplex set of tasks.30Exper imenta l  Des ignWe decided to compare the performance of two MITsystems: the "full parse" system and the "robust parse"system \[5\].
These two systems contrast a conservativeapproach that only answers when it is confident (the fullparse system) against a more aggressive approach that iswilling to make mistakes by answering much more often,based on partial understanding (the robust parse sys-tem).
These systems had very measurably different per-formance in terms of the CAS metric, and our hypothesiswas that the metrics would show that the robust-parsingsystem outperformed the full-parsing system.
To try tocapture a broader ange of user behavior, we decided tovary the difficulty of the scenarios; we used two pairs ofscenarios, where each pair consist of an "easy" scenariofollowed by a "hard" scenario.
The scenarios were chosento have a single correct answer.
The easy scenarios werescenarios adapted from our previous data collection andcould be solved with around three queries.
The more dif-ficult scenarios were constructed to require more queries(10-15) to solve them.
The four scenarios are shown inTable 3.The experiment used a within-subject design, witheach subject using both systems.
In order to neutral-ize the effects of the individual scenarios and the orderof scenarios, all subjects were presented with the samescenarios, in the same order.
We alternated sessions inwhich the robust parser was used for scenarios one andtwo with sessions in which the robust parser was used forscenarios three and four.
Subjects were given no priortraining or warm-up exercises.As of this writing, we have collected ata from fifteensubjects.
Eight of these subjects used a version of thesystem with the robust parser turned on for the first twoscenarios and turned off for the second two; seven usedthe opposite configuration of full-parsing followed by ro-bust parsing.
All but two of the subjects had not usedthe system before.We used our standard subject instructions, slightlymodified to inform the subject that s/he would be us-ing two distinct systems.
The subjects were drawn fromthe same pool as in our previous data collection efforts,namely MIT students and Staff.
Each subject was given a$10 gift certificate for a local store.
The subjects were notgiven any special incentive for getting correct answers,nor were they told that they would be timed.
Each sub-ject was asked to fill out a version of our debriefing ques-tionnaire, slightly modified to include a specific questionasking the subject which system s/he had preferred.We found that writing the scenarios was tricky, andwe had to iterate several times on the wording of thescenario descriptions; in particular, we found that it wasdifficult to elicit the desired answer.
Even when we al-tered the instructions to remind the subjects of what1.
Find a flight from Philadelphia to Dallas that makes a stop in Atlanta.
The flight should serve breakfast.
Identify thetype of aircraft hat is used on the flight to Dallas.
(Information requested: aircraft ype)2.
You want to fly from Boston to San Francisco n the last weekend in April (Saturday, April 25 or Sunday, April 26).You'd like to return to Boston on the following Wednesday in the evening, if possible.
Your main concern is that allflights be on Continental since you are trying to maximize your frequent flyer miles.
Identify one flight in each direction(by number) that you can take.
(Information requested: flight number)3.
Find a flight from Atlanta to Baltimore.
The flight should be on a Boeing 757 and arrive around 7:00 P.M. Identify theflight (by number) and what meal is served on this flight.
(Information requested: flight number, meal type)4.
You live in Pittsburgh.
You want to combine a vacation trip to Atlanta with business and take a friend along.
Youwill receive a fixed travel allowance, based on a first-class ticket.
Identify a coach class fare (dollar amount) that comesclosest to allowing you to cover the expenses of both you and your friend based on the regular first class fare.
Choosea date within the next seven days and make sure the fare does not have a restriction that disallows this.
(Informationrequested: fare amount (in dollars) for coach class fare)Table 3: The four scenarios used by subjects in the second MIT end-to-end experiment.kind of answer they should provide, subjects did not al-ways read or follow the scenarios carefully.
We wantedto avoid prompting the subjects with phrases that weknew the system understood.
We therefore tried to wordthe scenarios in such a way that subjects would not beable to read from their instructions verbatim and ob-tain a response from the system.
We also wanted tosee what problem-solving strategies ubjects would usewhen various options were presented to them, only oneof which could solve their scenario.
In Scenario 2, for ex-ample, there are no Continental f ights on the Saturdayor Wednesday evening in question.
There are, however,Continental f ights on Sunday and on Wednesday duringthe day.Resu l t s  and  Ana lysesFrom the collected data, we made a number of mea-surements for each scenario, and examined how the twosystems differed in terms of these measures.
The mea-surements that we computed are:?
Scenario completion time;?
Existence of a reported solution;?
Correctness of the reported solution;?
Number of queries;?
Number of queries answered; number esulting in a"no answer" message from the system;?
Logfile evaluation metrics, including queries judgedto be correctly answered, incorrectly answered, par-tially correct, and out of domain(class X); also score,defined as % Correct - % Incorrect;?
User satisfaction from debriefing questionnaire.Table 4 summarizes ome of the results comparing thetwo systems across all scenarios.
For the remainder ofthis section, we will try to analyze these results and reachsome tentative conclusions.31Task Completion The first column of Table 4 showsthat the subjects were able to provide an answer in allthescenarios when the system was in robust mode, whereasonly 83% of the scenarios were completed in non-robustmode.
Interestingly, a detailed examination of the datashows that, for the 5 cases in the non-robust mode whenusers gave up, there was never an incorrectly answeredquery, but the number of unanswered queries was ex-tremely high.
From a problem-solving standpoint, we cantentatively conclude that a system that takes chances andanswers more queries eems to be more successful than amore conservative one.Finding the Correct Solution Our experimental pa-radigm allowed us to determine automatically, by pro-cessing the log files, whether the subject solved the sce-nario correctly, incorrectly, or not at all.
A much largerpercentage of the scenarios were correctly answered withthe robust system than with the non-robust system (90%vs.
70%).
Measured in terms of the percent of scenar-ios correctly solved, the robust system outperformed thenon-robust system in all scenarios.Task Completion Time The task completion time issummarized in the third column of Table 4.
The resultsare somewhat inconclusive, due to a number of factors.Although we were interested in assessing how long it tookto solve a scenario, we did not inform our subjects of this.In part, this was because we didn't want to add morestress to the situation.
More than one subject inexpli-cably cleared the history after having nearly solved thescenario, and then essentially repeated, sometimes ver-batim, the same series of questions.
Had they thoughtthere was a time constraint, they probably would nothave done this.
We suspect hat because subjects werenot encouraged to proceed quickly, it is difficult to drawany conclusions from the results on time-to-completion.Another insidious factor was background network trafficand machine load, factors that would contribute to vari-ations in time-to-completion which we did not control for[ Scenario System % of Scenarios Solution \[ Completion Number w/Solution Correct \[ Time(s)1 Robust 100 1001 Full 86 712 Robust 100 882 Full 86 863 Robust 100 1003 Full 88 884 Robust 100 714 Full 75 38All Robust 100 90ALl FuU 83 70Number ofQueries% of QueriesCorrect% of QueriesIncorrect215 4.4 94 0215 4.7 70 0478 8.6 66 25483 10.6 39 4199 4.4 82 15376 8.0 42 0719 11.7 71 22643 9.8 51 039943475487.28.3181% of Queries \] DARPANo Answer \] Score6 9430 708 4156 353 6858 426 4949 516 5751 47Table 4: Mean metrics for robust and full parse systems, shown by scenarioin these experiments.The next column of the same table shows the averagenumber of queries for each scenario.
Since these numbersappear to be well correlated with task completion time,they suffer from some of the same deficiencies.Log File Score In order to measure the number ofqueries correctly answered by the system, two system de-velopers independently examined each query/answer pairand judged the answer as correct, partially correct, incor-rect, or unanswered, based on the evaluation program de-veloped for the logfile evaluation.
The system developerswere in complete agreement 92% of the time.
The casesof disagreement were examined to reach a compromiserating.
This provided a quick and reasonably accurateway to assess whether the subjects received the informa-tion they asked for.
The percentages of queries correctlyanswered, incorrectly answered, and unanswered, and theresulting DARPA score (i.e., % correct - % incorrect) areshown in the last four columns of Table 4.Although not shown in Table 4, the overall ratio ofcorrectly answered queries to those producing no an-swer was an order of magnitude higher for the robustparser (148:13) than for the non-robust parser (118:125).This was associated with an order-of-magnitude increasein the number of incorrect answers: 32 vs. 3 for thenon-robust parser.
However, the percentage of "no an-swer" queries seemed to be more critical in determiningwhether a subject succeeded with a scenario than thepercentage of incorrect queries.Debriefing Questionnaire Each subject received a de-briefing questionnaire, which included a question askingfor a comparison of the two systems used.
Unfortunately,data were not obtained from the first five subjects.
Ofthe ten subjects that responded, five preferred the ro-bust system, one preferred the non-robust system, andthe remaining ones expressed no preference.Difficulty of Scenarios There was considerable vari-ability among the scenarios in terms of difficulty.
Sce-nario 4 turned out to be by far the most difficult one to32solve, with only a little over half of the sessions beingsuccessfully completed 4.
Subjects were asked to "choosea date within the next week" and to be sure that therestrictions on their fare were acceptable.
We intention-ally did not expand the system to understand the phrase"within the next week" to mean "no seven-day advancepurchase requirement," but instead required the user todetermine that information through some other means.Also in Scenario 4, there were no available first class faresthat would exactly cover two coach class fares.
Scenarios2 and 4 were intended to be more difficult than 1 and3, and indeed they collectively had a substantially lowerpercentage of correct query answers than the other twoscenarios, reflecting the fact that subjects were gropingfor ways to ask for information that the system would beable to interpret.There was a wide variation across subjects in theirability to solve a given scenario, and in fact, subjects de-viated substantially from our expectations.
Several sub-jects did not read the instructions carefully and ignoredor misinterpreted key restrictions in the scenario.
For in-stance, one subject thought the "within the next week"requirement in Scenario 4 meant that he should returnwithin a week of his departure.
Some subjects had aweak knowledge of air travel; one subject assumed thatthe return trip would be on the same flight as the forwardleg, an assumption which caused considerable confusionfor the system.The full parser and robust parser showed differentstrengths and weaknesses in specific scenarios.
For ex-ample, in Scenario 3, the full parser often could not parsethe expression "Boeing 757", but the robust parser hadno trouble.
This accounts in part for the large "win" ofthe robust parser in this scenario.
Conversely, in Sce-nario 4, the robust parser misinterpreted expressions ofthe type "about two hundred dollars", treating "abouttwo" as a time expression.
This led the conversationbadly astray in these cases, and perhaps accounts for the4 The other three scenarios were solved successfully on averagenearly 90% of the time.fact that subjects took more time solving the scenario inrobust mode.
The lesson here is that different scenariosmay find different holes in the systems under compari-son, thus making the comparison extremely sensitive tothe exact choice and wording of the scenarios.Performance Comparison The robust parser performedbetter than the non-robust parser on all measures for allscenarios except in Scenario 4.
In Scenario 4, the per-centage of sessions resulting in a correct solution favoredrobust parsing by a large margin (71% vs. 38%), butthe robust parser had a longer time to completion andmore queries to completion than the non-robust system,as well as a worse DARPA score (51% to 49%).
The ro-bust parser gave a greater percentage of correct answers(71% vs. 51%), but its incorrect answers were signif-icant enough (22% to 0%) to reverse the outcome forthe DARPA score.
Thus DARPA score seems to be cor-related with time to completion, but percent of correctanswers eems to be correlated with getting a correct so-lution.We feel that the data for Scenario 4, when used tomake comparisons between the robust and non-robustparser, are anomalous for several reasons.
The scenarioitself confused subjects, some of whom incorrectly as-sumed that the correct fare was one which was exactlyone-half of the first class fare.
Furthermore, fare restric-tions are not as familiar to subjects as we previously as-sumed, leading to lengthy interactions with the system.These difficulties led to differences in performance acrosssystems that we feel are not necessarily linked directly tothe systems themselves but rather to the nature of thescenario being solved.
In summary, our data show thefollowing salient trends:1.
Subjects were always able to complete the scenariofor the robust system.2.
Successful task completion distinguished the twosystems: full parse system succeeded 70% of thetime, compared with 90% for the robust system.3.
Percent of correctly answered queries followed thesame trend as completion time and number of over-all queries; these may provide a rough measure oftask difficulty.4.
Scores for the performance on individual querieswere not necessarily consistent with overall successin solving the problem.5.
Users expressed a preference for the robust system.CONCLUSIONSThe results of these experiments are very encourag-ing.
We believe that it is possible to define metrics thatmeasure the performance of interactive systems in thecontext of interactive problem solving.
We have had con-siderable success indesigning end-to-end task completiontests.
We have shown that it is possible to design suchscenarios, that the subjects can successfully perform the33designated task in most cases, and that we can define ob-jective metrics, including time to task completion, num-ber of queries, and number of system non-responses.
Inaddition, these metrics appear to be correlated.
To as-sess correctness of system response, we have shown thatevaluators can produce better than 90% agreement eval-uating the correctness of response based on examinationof query/answer pairs from the log file.
We have im-plemented an interactive tool to support his evaluation,and have used it in two separate xperiments.
Finally,we demonstrated the utility of these metrics in charac-terizing two systems.
There was good correspondencebetween how effective the system was in helping the userarrive at a correct answer for a given task, and metricssuch as time to task completion, number of queries, andpercent of correctly answered queries (based on log fileevaluation).
These metrics also indicated that systembehavior may not be uniform over a range of scenarios- the robust parsing system performed better on threescenarios, but had a worse DARPA score on the fourth(and probably most difficult) scenario.
Based on theseexperiments, we believe that these metrics provide thebasis for evaluating spoken language systems in a realis-tic interactive problem solving context.ACKNOWLEDGEMENTS.~Ve would like to thank several people who made sig-nificant contributions to this work.
David Goodine de-signed and implemented the interactive log file evalua-tion interface which greatly facilitated running the logfile evaluation experiments.
Christie Winterton recruitedthe subjects for the various data collection experimentsand served as the wizard/transcriber fo many of the sub-jects in our end-to-end evaluation experiment.
We wouldalso like to thank Nancy Daly, James Glass, Rob Kasseland Victoria Palay for serving as evaluators in the logfile evaluation.REFERENCES\[1\] Bates, M., Boisen, S., and Makhoul, J., "Developing anEvaluation Methodology for Spoken Language Systems,"Proc.
DARPA Speech and Natural Language Workshop,pp.
102-108, June, 1990.\[2\] MADCOW, "Multi-Site Data Collection for a SpokenLanguage Corpus," MADCOW, These Proceedings.\[3\] Polifroni, J. Seneff, S., and Zue, V., "Collection ofSpontaneous Speech for the sc Atis Domain and Com-parative Analyses of Data Collected at MIT and TI,"Proc.
DARPA Speech and Natural Language Workshop,pp.360-365, February 1991.\[4\] Ramshaw, L. A. and S. Boisen, "An SLS Answer Com-parator," SLS Note 7, BBN Systems and TechnologiesCorporation, Cambridge, MA, May 1990.\[5\] Seneff, S., "A Relaxation Method for UnderstandingSpontaneous Speech Utterances," These Proceedings.
