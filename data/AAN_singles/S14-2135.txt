Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 761?767,Dublin, Ireland, August 23-24, 2014.UNITOR: Aspect Based Sentiment Analysis with Structured LearningGiuseppe Castellucci(?
), Simone Filice(?
), Danilo Croce(?
), Roberto Basili(?)(?)
Dept.
of Electronic Engineering(?)
Dept.
of Civil Engineering and Computer Science Engineering(?)
Dept.
of Enterprise EngineeringUniversity of Roma, Tor Vergata, Italy{castellucci,filice}@ing.uniroma2.it; {croce,basili}@info.uniroma2.itAbstractIn this paper, the UNITOR system partici-pating in the SemEval-2014 Aspect BasedSentiment Analysis competition is pre-sented.
The task is tackled exploiting Ker-nel Methods within the Support VectorMachine framework.
The Aspect TermExtraction is modeled as a sequential tag-ging task, tackled through SVMhmm.
TheAspect Term Polarity, Aspect Categoryand Aspect Category Polarity detection aretackled as a classification problem wheremultiple kernels are linearly combined togeneralize several linguistic information.In the challenge, UNITOR system achievesgood results, scoring in almost all rank-ings between the 2ndand the 8thpositionwithin about 30 competitors.1 IntroductionIn recent years, many websites started offering ahigh level interaction with users, who are no morea passive audience, but can actively produce newcontents.
For instance, platforms like Amazon1orTripAdvisor2allow people to express their opin-ions on products, such as food, electronic itemsor clothes.
Obviously, companies are interestedin understanding what customers think about theirbrands and products, in order to implement correc-tive strategies on products themselves or on mar-keting solutions.
Performing an automatic analy-sis of user opinions is then a very hot topic.
Theautomatic extraction of subjective information intext materials is generally referred as SentimentAnalysis or Opinion Mining and it is performedThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/1http://www.amazon.com2http://www.tripadvisor.comvia natural language processing, text analysis andcomputational linguistics techniques.
Task 4 inSemEval 2014 edition3(Pontiki et al., 2014) aimsat promoting research on Aspect Based OpinionMining (Liu, 2007), which is approached as a cas-cade of 4 subtasks.
For example, let us considerthe sentence:The fried rice is amazing here.
(1)The Aspect Term Extraction (ATE) subtask aimsat finding words suggesting the presence of as-pects on which an opinion is expressed, e.g.fried rice in sentence 1.
In the Aspect TermPolarity (ATP) task the polarity evoked for eachaspect is recognized, i.e.
a positive polarity isexpressed with respect to fried rice.
In theAspect Category Detection (ACD) task the cate-gory evoked in a sentence is identified, e.g.
thefood category in sentence 1).
In the Aspect Cat-egory Polarity (ACP) task the polarity of each ex-pressed category is recognized, e.g.
a positivecategory polarity is expressed in sentence 1.Different strategies have been experimented inrecent years.
Classical approaches are based onmachine learning techniques and rely on sim-ple representation features, such as unigrams, bi-grams, Part-Of-Speech (POS) tags (Pang et al.,2002; Pang and Lee, 2008; Wiebe et al., 1999).Other approaches adopt sentiment lexicons in or-der to exploit some sort of prior knowledge aboutthe polar orientation of words.
These resources areusually semi-automatically compiled and providescores associating individual words to sentimentsor polarity orientation.In this paper, the UNITOR system participat-ing to the SemEval-2014 Aspect Based SentimentAnalysis task (Pontiki et al., 2014) is presented.The ATE task is modeled as a sequential labelingproblem.
A sentence is considered as a sequenceof tokens: a Markovian algorithm is adopted in3http://alt.qcri.org/semeval2014/task4/761order to decide what is an aspect term .
All theremaining tasks are modeled as multi-kernel clas-sification problems based on Support Vector Ma-chines (SVMs).
Various representation have beenexploited using proper kernel functions (Shawe-Taylor and Cristianini, 2004a).
Tree Kernels(Collins and Duffy, 2001; Moschitti et al., 2008;Croce et al., 2011) are adopted in order to capturestructural sentence information derived from theparse tree.
Moreover, corpus-driven methods areused in order to acquire meaning generalizationsin an unsupervised fashion (e.g.
see (Pado and La-pata, 2007)) through the analysis of distributionsof word occurrences in texts.
It is obtained by theconstruction of a Word Space (Sahlgren, 2006),which provides a distributional model of lexicalsemantics.
Latent Semantic Kernel (Cristianini etal., 2002) is thus applied within such space.In the remaining, in Section 2 and 3 we will ex-plain our approach in more depth.
Section 4 dis-cusses the results in the SemEval-2014 challenge.2 Sequence Labeling for ATEThe Aspect Term Extraction (ATE) has been mod-eled as a sequential tagging process.
We con-sider each token representing the beginning (B),the inside (I) or the outside (O) of an argu-ment.
Following this IOB notation, the resultingATE representation of a sentence like ?The [friedrice]ASPECTTERMis amazing here?
can be expressedby labeling each word according to its relative po-sition, i.e.
: [The]O[fried]B[rice]I[is]O[amaz-ing]O[here]O.The ATE task is thus a labeling process thatdetermines the individual (correct IOB) class foreach token.
The labeling algorithm used isSVMhmm(Altun et al., 2003)4: it combinesboth a discriminative approach to estimate theprobabilities in the model and a generative ap-proach to retrieve the most likely sequence oftags that explains a sequence.
Given an inputsequence x = (x1.
.
.
xl) ?
X of feature vec-tors x1.
.
.
xl, the model predicts a tag sequencey = (y1.
.
.
yl) ?
Y after learning a linear dis-criminant function F : X ?
Y ?
R over input-output pairs.
The labeling f(x) is thus defined as:f(x) = arg maxy?YF (x,y;w) and it is obtainedby maximizing F over the response variable, y,for a specific given input x. F is linear in some4www.cs.cornell.edu/People/tj/svm light/svm hmm.htmlcombined feature representation of inputs and out-puts ?
(x,y), i.e.
F (x,y;w) = ?w,?
(x,y)?.In SVMhmmthe observations x1.
.
.
xlcan benaturally expressed in terms of feature vectors.
Inparticular, we modeled each word through a set oflexical and syntactic features, as described in thefollowing section.2.1 Modeling Features for ATEIn the discriminative view of SVMhmm, eachword is represented by a feature vector, describ-ing its different observable properties.
For in-stance, the word rice in the example 1 is modeledthrough the following features: Lexical features:its lemma (rice) and POS tag (NN); Prefixes andSuffixes: the first n and the last m characters ofthe word (n = m = 3) (e.g.
ric and ice); Con-textual features: the left and right lexical contextsrepresented by the 3 words before (BEGIN::BBthe::DT fried::JJ) and after (is::VBZ amazing::JJhere::RB); the left and right syntactic contexts asthe POS bi-grams and tri-grams occurring before(i.e.
BB DT DT JJ BB DT JJ) and after (i.e.VBZ JJ JJ RB VBZ JJ RB) the word; Gram-matical features: features derived from the de-pendency graph associated to the sentence, i.e.boolean indicators that capture if the token is in-volved in a Subj, Obj or Amod relation in the cor-responding graph.3 Multiple Kernel Approach for Polarityand Category DetectionWe approached the remaining three subtasks of thepipeline as classification problems with multiplekernels, in line with (Castellucci et al., 2013).
Weused Support Vector Machines (SVMs) (Joachims,1999), a maximum-margin classifier that realizesa linear discriminative model.
The kernelized ver-sion of SVM learns from instances xiexploitingrich similarity measures (i.e.the kernel functions)K(xi, xj) = ??
(xi) ?
?(xj)?.
In this way projec-tion functions ?(?)
can be implicitly used in orderto transform the initial feature space into a moreexpressive one, where a hyperplane that separatesthe data with the widest margin can be found.Kernels can directly operate on variegate formsof representation, such as feature vectors, trees,sequences or graphs.
Then, modeling instancesin different representations, specific kernels canbe defined in order to explore different linguis-tic information.
These variety of kernel functions762K1.
.
.Kncan be independently defined and thecombinations K1+ K2+ .
.
.
of multiple func-tions can be integrated into SVM as they are stillkernels.
The next section describes the represen-tations as well as the kernel functions.3.1 Representing Lexical InformationThe Bag of Word (BoW) is a simple repre-sentation reflecting the lexical information of thesentence.
Each text is represented as a vectorwhose dimensions correspond to different words,i.e.
they represent a boolean indicator of the pres-ence or not of a word in the text.
The resultingkernel function is the cosine similarity (or linearkernel) between vector pairs, i.e.
linBoW.
In linewith (Shawe-Taylor and Cristianini, 2004b) we in-vestigated the contribution of the Polynomial Ker-nel of degree 2, poly2BoWas it defines an implicitspace where also feature pairs, i.e.
words pairs,are considered.In the polarity detection tasks, several polaritylexicons have been exploited in order to have use-ful hints of the intrinsic polarity of words.
Weadopted MPQA Subjectivity Lexicon5(Wilson etal., 2005) and NRC Emotion Lexicon (Moham-mad and Turney, 2013): they are large collectionof words provided with the underlying emotionthey generally evoke.
While the former consid-ers only positive and negative sentiments, the lat-ter considers also eight primary emotions, orga-nized in four opposing pairs, joy-sadness, anger-fear, trust-disgust, and anticipation-surprise.
Wedefine the Lexicon Based (LB) vectors as follows.For each lexicon, let E = {e1, ..., e|E|} be theemotion vocabulary defined in it.
Let w ?
s bea word occurring in sentence s, with I(w, i) be-ing the indicator function whose output is 1 if wis associated to the emotion label ei, or 0 other-wise.
Then, given a sentence s, each ei, i.e.
a di-mension of the emotional vocabularyE, receives ascore si=?w?sI(w, i).
Each sentence producesa vector ~s ?
R|E|, for each lexicon, on which a lin-ear kernel linLBis applied.3.2 Generalizing Lexical InformationAnother representation is used to generalize thelexical information of each text, without exploit-ing any manually coded resource.
Basic lexicalinformation is obtained by a co-occurrence WordSpace (WS) built accordingly to the methodology5http://mpqa.cs.pitt.edu/lexicons/subj lexicondescribed in (Sahlgren, 2006) and (Croce and Pre-vitali, 2010).
A word-by-context matrix M is ob-tained through a large scale corpus analysis.
Thenthe Latent Semantic Analysis (Landauer and Du-mais, 1997) technique is applied as follows.
Thematrix M is decomposed through Singular ValueDecomposition (SVD) (Golub and Kahan, 1965)into the product of three new matrices: U , S, andV so that S is diagonal and M = USVT.
Mis then approximated by Mk= UkSkVTk, whereonly the first k columns of U and V are used,corresponding to the first k greatest singular val-ues.
This approximation supplies a way to projecta generic wordwiinto the k-dimensional space us-ing W = UkS1/2k, where each row corresponds tothe representation vector ~wi.
The result is that ev-ery word is projected in the reduced Word Spaceand a sentence is represented by applying an addi-tive model as an unbiased linear combination.
Weadopted these vector representations using a linearkernel, as in (Cristianini et al., 2002), i.e.
linWSand a Radial Basis Function Kernel rbfWS.In Aspect Category Detection, and more gen-erally in topic classification tasks, some specificwords can be an effective indicator of the under-lying topic.
For instance, in the restaurant do-main, the word tasty may refer to the quality offood.
These kind of word-topic relationships canbe automatically captured by a Bag-of-Word ap-proach, but with some limitations.
As an exam-ple, a BoW representation may not capture syn-onyms or semantically related terms.
This lackof word generalization is partially compensatedby the already discussed Word Space.
However,this last representation aims at capturing the senseof an overall sentence, and no particular rele-vance is given to individual words, even if theycan be strong topic indicators.
To apply a model-ing more focused on topics, we manually selectedm seed words {?1, .
.
.
, ?m} that we consider re-liable topic-indicators, for example spaghetti forfood.
Notice that for every seed ?i, as well as forevery word w the similarity function sim(?i, w)can be derived from the Word Space represen-tations ~?iand ~w, respectively.
What we needis a specific seed-based representation reflectingthe similarity between topic indicators and sen-tences s. Given the words w occurring in s, theSeed-Oriented (SO) representation of s is an m-dimensional vector ~so(s) whose components are:soi(s) = maxw?ssim(?i, w).
Alternatively, as763seeds ?
refer to a set of evoked topics (i.e.
as-pect categories such as food) ?1, ...,?t, we candefine a t-dimensional vector~to(s) called Topic-Oriented (TO) representation for s, whose fea-tures are: toi(s) = maxw?s,?k?
?isim(?k, w).The adopted word similarity function sim(?, ?
)over ~so(s) and~to(s) depends on the experiments.In the unconstrained setting, i.e.
the Word SpaceTopic Oriented WSTO system, sim(?, ?)
consistsin the dot product over the Word Space represen-tations ~?iand ~w.
In the constrained case sim(?, ?
)corresponds to the Wu & Palmer similarity basedon WordNet (Wu and Palmer, 1994), in the socalled WordNet Seed Oriented WNSO system.The Radial Basis Function (RBF) kernel is thenapplied onto the resulting feature vectors~to(s) and~so(s) in the rbfWSTOand rbfWNSO, respectively.3.3 Generalizing Syntactic InformationIn order to exploit the syntactic information, TreeKernel functions proposed in (Collins and Duffy,2001) are adopted.
Tree kernels exploit syntacticsimilarity through the idea of convolutions amongsyntactic tree substructures.
Any tree kernel evalu-ates the number of common substructures betweentwo trees T1and T2without explicitly consideringthe whole fragment space.
Many tree represen-tations can be derived to represent the syntacticinformation, according to different syntactic theo-ries.
For this experiment, dependency formalismof parse trees is employed to capture sentencessyntactic information.
As proposed in (Croce etal., 2011), the kernel function is applied to ex-amples modeled according the Grammatical Rela-tion Centered Tree representation from the orig-inal dependency parse structures, shown in Fig.1: non-terminal nodes reflect syntactic relations,such as NSUBJ, pre-terminals are the Part-Of-Speech tags, such as nouns, and leaves are lex-emes, such as rice::n and amazing::j6.
In each ex-ample, the aspect terms and the covering nodes areenriched with a a suffix and all lexical nodes areduplicated by the node asp in order to reduce datasparseness.
Moreover, prior information derivedby the lexicon can be injected in the tree, by du-plicating all lexical nodes annotated in the MPQASubjectivity Lexicon, e.g.
the adjective amazing,with a node expressing the polarity (pos).Given two tree structures T1and T2, the6Each word is lemmatized to reduce data sparseness, butthey are enriched with POS tags.ROOTADVMRBhere::rJJposamazing::jCOPVBZbe::vNSUBJaNNaasprice::nAMODaVBNaaspfry::vDETDTthe::dFigure 1: Tree representation of the sentence 1.Tree Kernel formulation is reported hereafter:TK(T1, T2) =?n1?NT1?n2?NT2?
(n1, n2)where NT1and NT2are the sets of the T1?s andT2?s nodes, respectively and ?
(n1, n2) is equal tothe number of common fragments rooted in the n1and n2nodes.
The function ?
determines the na-ture of the kernel space.
In the constrained case thePartial Tree Kernel formulation (Moschitti, 2006)is used, i.e.
ptkGRCT.
In the unconstrained set-ting the Smoothed Partial Tree Kernel formulation(Croce et al., 2011) is adopted to emphasizes thelexicon in the Word Space, i.e.
the sptkGRCT.
Itcomputes the similarity between lexical nodes asthe similarity between words in the Word Space.So, this kernel allows a generalization both from asyntactic and lexical point of view.4 ResultsIn this Section the experimental results of theUNITOR system in the four different subtasks ofSemeval 2014 competition are discussed.
Teamswere allowed to submit two different outcomes foreach task: constrained submissions (expressed bythe suffix C in all the tables) are intended to mea-sure systems ability to learn sentiment analysismodels only over the provided data; unconstrained(expressed by the suffix U in all the tables) sub-missions allows teams to exploit additional train-ing data.
The first two tasks, i.e.
ATE and ATP,are defined on the laptop and restaurant domains,while the last two tasks, i.e.
ACD and ACP, aredefined for the restaurant dataset only.The unconstrained versions are derived by ex-ploiting word vectors derived in an unsupervisedfashion through the analysis of large scale cor-pora.
All words in a corpus occurring more than100 times (i.e.
the targets) are represented throughvectors.
The original space dimensions are gen-erated from the set of the 20,000 most frequentwords (i.e.
features) in the corpus.
One dimensiondescribes the Point-wise Mutual Information scorebetween one feature, as it occurs on a left or rightwindow of 3 tokens around a target.
Left contextsof targets are distinguished from the right ones, inorder to capture asymmetric syntactic behaviors764(e.g., useful for verbs): 40,000 dimensional vec-tors are thus derived for each target.
The SingularValue Decomposition is applied and the space di-mensionality is reduced to k = 250.
Two corporaare used for generating two different Word Spaces,one for the laptop and one for the restaurant do-main.
The Opinosis dataset (Ganesan et al., 2010)is used to build the electronic domain Word Space,while the restaurant domain corpus adopted is theTripAdvisor dataset7.
Both provided data and in-domain data are first pre-processed through theStanford Parser (Klein and Manning, 2003) in or-der to obtain POS tags or Dependency Trees.A modified version of LibSVM has beenadopted to implement Tree Kernel.
Parameterssuch as the SVM regularization coefficient C, thekernel parameters (for instance the degree of thepolynomial kernel) have been selected after a tun-ing stage based on a 5-fold cross validation.4.1 Aspect Term ExtractionThe Aspect Term Extraction task is modeled as asequential labeling problem.
The feature represen-tation described in Section 2.1, where each tokenis associated to a specific target class according tothe IOB notation, is used in the SVMhmmlearn-ing algorithm.
In the constrained version of theUNITOR system only the training data are usedto derive features.
In the unconstrained case theUNITOR system exploits lexical vectors derivedfrom a Word Space.
Each token feature repre-sentation is, in this sense, augmented through dis-tributional vectors derived from the Word Spacesdescribed above.
Obviously, the Opinosis WordSpace is used in the laptop subtask, while the Tri-pAdvisor Word Space is used in the restaurant sub-task.
These allow the system to generalize the lex-ical information, enabling a smoother match be-tween words during training and test phases, hope-fully capturing similarity phenomena such as therelation between screen and monitor.In Table 1 results in the laptop case are reported.Our system performed quite well, and ranked in6thand 10thposition over 28 submitted systems.In this case, the use of the Word Space is effec-tive, as noticed by the 4 position gain in the finalranking (almost 2 points in F1-measure).
In Table2 results in the restaurant case are reported.
Here,the use of Word Space does not give an improve-ment in the final performance.7http://sifaka.cs.uiuc.edu/?wang296/Data/index.htmlTable 1: Aspect Term Extraction Results - Laptop.System (Rank) P R F1UNITOR-C (10/28) .7741 .5764 .6608UNITOR-U (6/28) .7575 .6162 .6795Best-System-C (1/28) .8479 .6651 .7455Best-System-U (2/28) .8251 .6712 .7403Table 2: Aspect Term Extraction - Restaurants.System (Rank) P R F1UNITOR-C (5/29) .8244 .7786 .8009UNITOR-U (6/29) .8131 .7865 .7996Best-System-C (2/29) .8624 .8183 .8398Best-System-U (1/29) .8535 .8271 .8401In both cases, we observed that most of theerrors were associated to aspect terms composedby multiple words.
For example, in the sen-tence The portions of the food that came out weremediocre the gold aspect term is portions ofthe food while our system was able only to re-trieve food as aspect term.
The system is mainlyable to recognize single word aspect terms and, inmost of the cases, double words aspect terms.4.2 Aspect Term PolarityThe Aspect Term Polarity subtask has been mod-eled as a multi-class classification problem: fora given set of aspect terms within a sentence, itaims at determining whether the polarity of eachaspect term is positive, negative, neutral or con-flict.
It has been tackled using multi-kernel SVMsin a One-vs-All Schema.
In the constrained set-ting, the linear combination of the following ker-nel functions have been used: ptkGRCT, poly2BoWthat consider all the lemmatized terms in the sen-tence, a poly2BoWthat considers only the aspectterms, poly2BoWof the terms around the aspectterms in a window of size 5, linLBderived fromthe Emolex lexicon.
In the unconstrained settingthe sptkGRCTreplaces the ptk counterpart andthe rbfWSis added by linearly combining WordSpace vectors for verbs, nouns adjective and ad-verbs.
Results in Table 3 show that the proposedkernel combination allows to achieve the 8thposi-tion with the unconstrained system in the restau-rant domain.
The differences with the constrainedsetting demonstrate the contribution of the WordSpace acquired from the TripAdvisor corpus.
Un-fortunately, it is not true in the laptop domain, asshown in Table 4.
The use of the Opinosis corpuslets to a performance drop of the unconstrainedsetting.
An error analysis shows that the main lim-765itation of the proposed model is the inability tocapture deep semantic phenomena such as irony,as in the negative sentence ?the two waitress?slooked like they had been sucking on lemons?.Table 3: Aspect Term Polarity Results - Restau-rant.System (Rank) AccuracyUNITOR-C (12/36) .7248UNITOR-U (8/36) .7495Best-System-C (1/36) .8095Best-System-U (5/36) .7768Table 4: Aspect Term Polarity Results - Laptop.System (Rank) AccuracyUNITOR-C (10/32) .6299UNITOR-U (17/32) .5856Best-System-C (1/32) .7048Best-System-U (5/32) .66664.3 Aspect Category DetectionThe Aspect Category Detection has been mod-eled as a multi-label classification task where 5categories (ambience, service, food, price, anec-dotes/miscellaneous) must be recognized.
In theconstrained version, each class has been tack-led using a binary multi-kernel SVM equippedwith a linear combination of poly2BoWandrbfWNSO.
A category is assigned if the SVMclassifiers provides a positive prediction.
Theanecdotes/miscellaneous acceptance threshold hasbeen set to 0.3 (it has been estimated over a de-velopment set) due to its poor precision observedduring the tuning phase.
Moreover, consideringeach sentence is always associated to at least onecategory, if no label has been assigned, then thesentence is labelled with the category associatedto the highest prediction.In the unconstrained case, each class has beentackled using an ensemble of a two binary SVM-based classifiers.
The first classifier is a multi-kernel SVM operating on a linear combination ofrbfWSand poly2BoW.
The second classifier is aSVM equipped with a rbfWSTO.
A sentence is la-belled with a category if at least one of the two cor-responding classifiers predicts that label.
The firstclassifier assigns a label if the corresponding pre-diction is positive.
A more conservative strategyis applied to the second classifier, and a categoryis selected if its corresponding prediction is higherthan 0.3; again this threshold has been estimatedover a development set.
As in the constrained ver-sion, we observed a poor precision in the anec-dotes/miscellaneous category, so we increased thefirst classifier acceptance threshold to 0.3, whilethe second classifier output is completely ignored.Finally, if no label has been assigned, the sentenceis labelled with the category associated to the high-est prediction of the first classifier.Table 5: Aspect Category Detection Results.System (Rank) P R F1UNITOR-C (6/21) .8368 .7804 .8076UNITOR-U (2/21) .8498 .8556 .8526Best-System-C (1/21) .9104 .8624 .8857Best-System-U (4/21) .8435 .7892 .8155Table 5 reports the achieved results.
Consider-ing the simplicity of the proposed approach, theresults are impressive.
The ensemble schema,adopted in the unconstrained version, is very use-ful in improving the recall and allows the systemto achieve the second position in the competition.4.4 Aspect Category PolarityThe Aspect Category Polarity subtask has beenmodeled as a multi-class classification problem:given a set of pre-identified aspect categories for asentence, it aims at determining the polarity (pos-itive, negative, neutral or conflict) of each cate-gory.
It has been tackled using multi-kernel SVMsin a One-vs-All Schema.
In the constrained set-ting, the linear combination of the following ker-nel functions has been used: ptkGRCT, poly2BoWthat consider all the lemmatized terms in the sen-tence, a poly2BoWthat considers only verbs, nounsadjective and adverbs in the sentence, linLBde-rived from the MPQA sentiment lexicon.
In theunconstrained case the sptkGRCTreplaces the ptkcounterpart and the rbfWSis added by linearlycombining Word Space vectors for verbs, nounsadjective and adverbs.Again, results shown in Table 6 suggest the pos-itive contribution of the lexical generalization pro-vided by the Word Space (in the sptkGRCTandrbfWS) allows to achieve a good rank, i.e.
the4thposition with the unconstrained system in therestaurant domain.
The error analysis underlinesthat the proposed features do not capture irony.Table 6: Aspect Category Polarity Results.System (Rank) AccuracyUNITOR-C (7/25) .7307UNITOR-U (4/25) .7629Best-System-C (1/25) .8292Best-System-U (9/25) .7278766ReferencesYasemin Altun, I. Tsochantaridis, and T. Hofmann.2003.
Hidden Markov support vector machines.
InProceedings of the International Conference on Ma-chine Learning.Giuseppe Castellucci, Simone Filice, Danilo Croce,and Roberto Basili.
2013.
Unitor: Combiningsyntactic and semantic kernels for twitter sentimentanalysis.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Pro-ceedings of the Seventh International Workshop onSemantic Evaluation (SemEval 2013), pages 369?374, Atlanta, Georgia, USA, June.
ACL.Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In Proceedings of Neu-ral Information Processing Systems (NIPS?2001),pages 625?632.Nello Cristianini, John Shawe-Taylor, and HumaLodhi.
2002.
Latent semantic kernels.
J. Intell.Inf.
Syst., 18(2-3):127?152.Danilo Croce and Daniele Previtali.
2010.
Mani-fold learning for the semi-supervised induction offramenet predicates: an empirical investigation.
InGEMS 2010, pages 7?16, Stroudsburg, PA, USA.ACL.Danilo Croce, Alessandro Moschitti, and RobertoBasili.
2011.
Structured lexical similarity via con-volution kernels on dependency trees.
In Proceed-ings of EMNLP, Edinburgh, Scotland, UK.Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.2010.
Opinosis: a graph-based approach to abstrac-tive summarization of highly redundant opinions.
InProceedings of the 23rd International Conference onComputational Linguistics, pages 340?348.
ACL.Gene Golub and W. Kahan.
1965.
Calculating the sin-gular values and pseudo-inverse of a matrix.
Journalof the Society for Industrial and Applied Mathemat-ics: Series B, Numerical Analysis, 2(2):pp.
205?224.Thorsten Joachims.
1999.
Making large-Scale SVMLearning Practical.
MIT Press, Cambridge, MA.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings ofACL?03, pages 423?430.Tom Landauer and Sue Dumais.
1997.
A solution toplato?s problem: The latent semantic analysis the-ory of acquisition, induction and representation ofknowledge.
Psychological Review, 104.Bing Liu.
2007.
Web data mining.
Springer.Saif Mohammad and Peter D. Turney.
2013.
Crowd-sourcing a word-emotion association lexicon.
Com-putational Intelligence, 29(3):436?465.Alessandro Moschitti, Daniele Pighin, and RobertBasili.
2008.
Tree kernels for semantic role label-ing.
Computational Linguistics, 34.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In ECML, Berlin, Germany, September.Sebastian Pado and Mirella Lapata.
2007.Dependency-based construction of semanticspace models.
Computational Linguistics, 33(2).Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135, January.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification us-ing machine learning techniques.
In EMNLP, vol-ume 10, pages 79?86, Stroudsburg, PA, USA.
ACL.Maria Pontiki, Dimitrios Galanis, John Pavlopou-los, Harris Papageorgiou, Ion Androutsopoulos, andSuresh Manandhar.
2014.
Semeval-2014 task 4:Aspect based sentiment analysis.
In Proceedings ofthe International Workshop on Semantic Evaluation(SemEval).Magnus Sahlgren.
2006.
The Word-Space Model.Ph.D.
thesis, Stockholm University.John Shawe-Taylor and Nello Cristianini.
2004a.
Ker-nel Methods for Pattern Analysis.
Cambridge Uni-versity Press, New York, NY, USA.John Shawe-Taylor and Nello Cristianini.
2004b.
Ker-nel Methods for Pattern Analysis.
Cambridge Uni-versity Press.Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P.O?Hara.
1999.
Development and use of a gold-standard data set for subjectivity classifications.
InProceedings of the 37th annual meeting of theACL on Computational Linguistics, pages 246?253,Stroudsburg, PA, USA.
ACL.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of HumanLanguage Technologies Conference/Conference onEmpirical Methods in Natural Language Processing(HLT/EMNLP 2005), Vancouver, CA.Zhibiao Wu and Martha Palmer.
1994.
Verbs seman-tics and lexical selection.
In Proceedings of the32Nd Annual Meeting of ACL, ACL ?94, pages 133?138, Stroudsburg, PA, USA.
ACL.767
