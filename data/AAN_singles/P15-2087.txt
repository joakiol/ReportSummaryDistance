Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 530?535,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsMT Quality Estimation for Computer-assisted Translation:Does it Really Help?Marco Turchi, Matteo Negri, Marcello FedericoFBK - Fondazione Bruno Kessler,Via Sommarive 18, 38123 Trento, Italy{turchi,negri,federico}@fbk.euAbstractThe usefulness of translation quality es-timation (QE) to increase productivityin a computer-assisted translation (CAT)framework is a widely held assumption(Specia, 2011; Huang et al., 2014).
So far,however, the validity of this assumptionhas not been yet demonstrated throughsound evaluations in realistic settings.
Tothis aim, we report on an evaluation in-volving professional translators operatingwith a CAT tool in controlled but naturalconditions.
Contrastive experiments arecarried out by measuring post-editing timedifferences when: i) translation sugges-tions are presented together with binaryquality estimates, and ii) the same sug-gestions are presented without quality in-dicators.
Translators?
productivity in thetwo conditions is analysed in a principledway, accounting for the main factors (e.g.differences in translators?
behaviour, qual-ity of the suggestions) that directly impacton time measurements.
While the gen-eral assumption about the usefulness ofQE is verified, significance testing resultsreveal that real productivity gains can beobserved only under specific conditions.1 IntroductionMachine translation (MT) quality estimation aimsto automatically predict the expected time (e.g.
inseconds) or effort (e.g.
number of editing opera-tions) required to correct machine-translated sen-tences into publishable translations (Specia et al.,2009; Mehdad et al., 2012; Turchi et al., 2014a;C. de Souza et al., 2015).
In principle, the taskhas a number of practical applications.
An intu-itive one is speeding-up the work of human trans-lators operating with a CAT tool, a software de-signed to support and facilitate the translation pro-cess by proposing suggestions that can be editedby the user.
The idea is that, since the suggestionscan be useful (good, hence post-editable) or use-less (poor, hence requiring complete re-writing),reliable quality indicators could help to reduce thetime spent by the user to decide which action totake (to correct or re-translate).So far, despite the potential practical benefits,the progress in QE research has not been followedby conclusive results that demonstrate whether theuse of quality labels can actually lead to noticeableproductivity gains in the CAT framework.
To thebest of our knowledge, most prior works limit theanalysis to the intrinsic evaluation of QE perfor-mance on gold-standard data (Callison-Burch etal., 2012; Bojar et al., 2013; Bojar et al., 2014).On-field evaluation is indeed a complex task, asit requires: i) the availability of a CAT tool ca-pable to integrate MT QE functionalities, ii) pro-fessional translators used to MT post-editing, iii)a sound evaluation protocol to perform between-subject comparisons,1and iv) robust analysis tech-niques to measure statistical significance undervariable conditions (e.g.
differences in users?
post-editing behavior).To bypass these issues, the works more closelyrelated to our investigation resort to controlled andsimplified evaluation protocols.
For instance, in(Specia, 2011) the impact of QE predictions ontranslators?
productivity is analysed by measuringthe number of words that can be post-edited in afixed amount of time.
The evaluation, however,only concentrates on the use of QE to rank MToutputs, and the gains in translation speed are mea-sured against the contrastive condition in which noQE-based ranking mechanism is used.
In this arti-ficial scenario, the analysis disregards the relation1Notice that the same sentence cannot be post-editedtwice (e.g.
with/without quality labels) by the same translatorwithout introducing a bias in the time measurements.530between the usefulness of QE and the intrinsic fea-tures of the top-ranked translations (e.g.
sentencelength, quality of the MT).
More recently, Huanget al.
(2014) claimed a 10% productivity increasewhen translation is supported by the estimates ofan adaptive QE model.
Their analysis, however,compares a condition in which MT suggestions arepresented with confidence labels (the two factorsare not decoupled) against the contrastive condi-tion in which no MT suggestion is presented at all.Significance testing, moreover, is not performed.The remainder of this work describes ouron-field evaluation addressing (through objectivemeasurements and robust significance tests) thetwo key questions:?
Does QE really help in the CAT scenario??
If yes, under what conditions?2 Experimental SetupOne of the key questions in utilising QE in theCAT scenario is how to relay QE information tothe user.
In our experiments, we evaluate a way ofvisualising MT quality estimates that is based on acolor-coded binary classification (green vs. red) asan alternative to real-valued quality labels.
In ourcontext, ?green?
means that post-editing the trans-lation is expected to be faster than translation fromscratch, while ?red?
means that post-editing thetranslation is expected to take longer than trans-lating from scratch.This decision rests on the assumption that thetwo-color scheme is more immediate than real-valued scores, which require some interpretationby the user.
Analysing the difference between al-ternative visualisation schemes, however, is cer-tainly an aspect that we want to explore in the fu-ture.2.1 The CAT FrameworkTo keep the experimental conditions as natural aspossible, we analyse the impact of QE labels ontranslators?
productivity in a real CAT environ-ment.
To this aim, we use the open-source Mate-Cat tool (Federico et al., 2014), which has beenslightly changed in two ways.
First, the tool hasbeen adapted to provide only one single transla-tion suggestion (MT output) per segment, insteadof the usual three (one MT suggestion plus twoTranslation Memory matches).
Second, each sug-gestion is presented with a colored flag (green forgood, red for bad), which indicates its expectedquality and usefulness to the post-editor.
In thecontrastive condition (no binary QE visualization),grey is used as the neutral and uniform flag color.2.2 Getting binary quality labels.The experiment is set up for a between-subjectcomparison on a single long document as follows.First, the document is split in two parts.
Thefirst part serves as the training portion for a bi-nary quality estimator; the second part is re-served for evaluation.
The training portion ismachine-translated with a state-of-the-art, phrase-based Moses system (Koehn et al., 2007)2andpost-edited under standard conditions (i.e.
with-out visualising QE information) by the same usersinvolved in the testing phase.
Based on their post-edits, the raw MT output samples are then la-beled as ?good?
or ?bad?
by considering the HTER(Snover et al., 2006) calculated between raw MToutput and its post-edited version.3Our labelingcriterion follows the empirical findings of (Turchiet al., 2013; Turchi et al., 2014b), which indicatean HTER value of 0.4 as boundary between post-editable (HTER ?
0.4) and useless suggestions(HTER> 0.4).Then, to model the subjective concept of qual-ity of different subjects, for of each translatorwe train a separate binary QE classifier on thelabeled samples.
For this purpose we use theScikit-learn implementation of support vector ma-chines (Pedregosa et al., 2011), training our mod-els with the 17 baseline features proposed by Spe-cia et al.
(2009).
This feature set mainly takesinto account the complexity of the source sentence(e.g.
number of tokens, number of translations persource word) and the fluency of the target trans-lation (e.g.
language model probabilities).
Thefeatures are extracted from the data available atprediction time (source text and raw MT output)by using an adapted version (Shah et al., 2014)of the open-source QuEst software (Specia et al.,2013).
The SVM parameters are optimized bycross-validation on the training set.With these classifiers, we finally assign qualityflags to the raw segment translations in the test2The system was trained with 60M running words fromthe same domain (Information Technology) of the input doc-ument.3HTER measures the minimum edit distance (# word In-sertions + Deletions + Substitutions + Shifts / # ReferenceWords) between the MT output and its manual post-edition.531Average PET(sec/word)coloredgrey8.0869.592p = 0.33% Winsof colored51.7 p = 0.039Table 1: Comparison (Avg.
PET and ranking) be-tween the two testing conditions (with and withoutQE labels).portion of the respective document, which is even-tually sent to each post-editor to collect time andproductivity measurements.2.3 Getting post-editing time measurements.While translating the test portion of the docu-ment, each translator is given an even and ran-dom distribution of segments labeled according tothe test condition (colored flags) and segments la-beled according to the baseline, contrastive condi-tion (uniform grey flags).
In the distribution of thedata, some constraints were identified to ensurethe soundness of the evaluation in the two condi-tions: i) each translator must post-edit all the seg-ments of the test portion of the document, ii) eachtranslator must post-edit the segments of the testset only once, iii) all translators must post-edit thesame amount of segments with colored and greylabels.
After post-editing, the post-editing timesare analysed to assess the impact of the binary col-oring scheme on translators?
productivity.3 ResultsWe applied our procedure on an English user man-ual (Information Technology domain) to be trans-lated into Italian.
Post-editing was performed in-dependently by four professional translators, sothat two measurements (post-editing time) foreach segment and condition could be collected.Training and and test respectively contained 542and 847 segments.
Half of the 847 test segmentswere presented with colored QE flags, with a ra-tio of green to red labels of about 75% ?good?
and25% ?bad?.3.1 Preliminary analysisBefore addressing our research questions, we per-formed a preliminary analysis aimed to verify thereliability of our experimental protocol and theconsequent findings.
Indeed, an inherent risk ofpresenting post-editors with an unbalanced distri-bution of colored flags is to incur in unexpectedsubconscious effects.
For instance, green flagscould be misinterpreted as a sort of pre-validation,and induce post-editors to spend less time onthe corresponding segments (by producing fewerchanges).
To check this hypothesis we comparedthe HTER scores obtained in the two conditions(colored vs. grey flags), assuming that noticeabledifferences would be evidence of unwanted psy-chological effects.
The very close values mea-sured in the two conditions (the average HTER isrespectively 23.9 and 24.1) indicate that the pro-fessional post-editors involved in the experimentdid what they were asked for, by always changingwhat had to be corrected in the proposed sugges-tions, independently from the color of the associ-ated flags.
In light of this, post-editing time varia-tions in different conditions can be reasonably as-cribed to the effect of QE labels on the time spentby the translators to decide whether correcting orre-translating a given suggestion.3.2 Does QE Really Help?To analyse the impact of our quality estimates ontranslators?
productivity, we first compared the av-erage post-editing time (PET ?
seconds per word)under the two conditions (colored vs. grey flags).The results of this rough, global analysis are re-ported in Table 1, first row.
As can be seen, the av-erage PET values indicate a productivity increaseof about 1.5 seconds per word when colored flagsare provided.
Significance tests, however, indicatethat such increase is not significant (p > 0.05,measured by approximate randomization (Noreen,1989; Riezler and Maxwell, 2005)).An analysis of the collected data to better un-derstand these results and the rather high averagePET values observed (8 to 9.5 secs.
per word) evi-denced both a large number of outliers, and a highPET variability across post-editors.4To checkwhether these factors make existing PET differ-ences opaque to our study, we performed furtheranalysis by normalizing the PET of each transla-tor with the robust z-score technique (Rousseeuwand Leroy, 1987).5The twofold advantage of4We consider as outliers the segments with a PET lowerthan 0.5 or higher than 30.
Segments with unrealisticallyshort post-editing times may not even have been read com-pletely, while very long post-editing times suggest that thepost-editor interrupted his/her work or got distracted.
Theaverage PET for the four post-editors ranges from 2.266 to13.783.
In total, 48 segments have a PET higher than 30, and6 segments were post-edited in more than 360 seconds.5For each post-editor, it is computed by removing from53246?47?48?49?50?51?52?53?54?55?56?0.1?
0.2?
0.3?
0.4?
0.5?
0.6?
0.7?
0.8?
0.9?
1?%?Wins?of?Colored?HTER?LONG?
MEDIUM?
SHORT?46?47?48?49?50?51?52?53?54?55?56?0.1?
0.2?
0.3?
0.4?
0.5?
0.6?
0.7?
0.8?
0.9?
1?%?Wins?of?Colored?HTER?LONG?
MEDIUM?
SHORT?Figure 1: % wins of colored with respect to length and quality of MT output.
Left: all pairs.
Right: onlypairs with correct color predictions.this method is to mitigate idiosyncratic differencesin translators?
behavior, and reduce the influenceof outliers.
To further limit the impact of out-liers, we also moved from a comparison basedon average PET measurements to a ranking-basedmethod in which we count the number of timesthe segments presented with colored flags werepost-edited faster than those presented with greyflags.
For each of the (PET colored, PET grey)pairs measured for the test segments, the percent-age of wins (i.e.
lower time) of PET colored iscalculated.
As shown in the second row of Ta-ble 1, a small but statistically significant differencebetween the two conditions indeed exists.Although the usefulness of QE in the CATframework seems hence to be verified, the extentof its contribution is rather small (51.7% of wins).This motivates an additional analysis, aimed toverify if such marginal global gains hide larger lo-cal productivity improvements under specific con-ditions.3.3 Under what Conditions does QE Help?To address this question, we analysed two im-portant factors that can influence translators?
pro-ductivity measurements: the length (number oftokens) of the source sentences and the quality(HTER) of the proposed MT suggestions.
Tothis aim, all the (PET colored, PET grey) pairswere assigned to three bins based on the length ofthe source sentences: short (length?5), medium(5<length?20), and long (length>20).
Then, ineach bin, ten levels of MT quality were identi-fied (HTER ?
0.1, 0.2, .
.
., 1).
Finally, for eachbin and HTER threshold, we applied the ranking-the PET of each segment the post-editor median and dividingby the post-editor median absolute deviation (MAD).based method described in the previous section.The left plot of Figure 1 shows how the ?% winsof colored?
varies depending on the two factors onall the collected pairs.
As can be seen, for MT sug-gestions of short and medium length the percent-age of wins is always above 50%, while its value issystematically lower for the long sentences whenHTER>0.1.
However, the differences are statis-tically significant only for medium-length sugges-tions, and when HTER>0.1.
Such condition, inparticular when 0.2<HTER?0.5, seems to rep-resent the ideal situation in which QE labels canactually contribute to speed-up translators?
work.Indeed, in terms of PET, the average productiv-ity gain of 0.663 secs.
per word measured in the[0.2 ?
0.5] HTER interval is statistically signifi-cant.Although our translator-specific binary QE clas-sifiers (see Section 2) have acceptable perfor-mance (on average 80% accuracy on the test datafor all post-editors),6to check the validity of ourconclusions we also investigated if, and to whatextent, our results are influenced by classificationerrors.
To this aim, we removed from the threebins those pairs that contain a misclassified in-stance (i.e.
the pairs in which there is a mismatchbetween the predicted label and the true HTERmeasured after post-editing).7The results obtained by applying our ranking-based method to the remaining pairs are shown inthe right plot of Figure 1.
In this ?ideal?, error-freescenario the situation slightly changes (unsurpris-ingly, the ?% wins of colored?
slightly increases,6Measured by comparing each predicted binary label withthe ?true?
label obtained applying the 0.4 HTER threshold asa separator between good and bad MT suggestions.7The three bins contained 502, 792, 214 pairs before mis-classification removal and 339, 604, 160 pairs after cleaning.533especially for long suggestions for which we havethe highest number of misclassifications), but theoverall conclusions remain the same.
In particular,the higher percentage of wins is statistically sig-nificant only for medium-length suggestions withHTER>0.1 and, in the best case (HTER?0.2) it isabout 56.0%.4 ConclusionWe presented the results of an on-field evalua-tion aimed to verify the widely held assumptionthat QE information can be useful to speed-upMT post-editing in the CAT scenario.
Our resultssuggest that this assumption should be put intoperspective.
On one side, global PET measure-ments do not necessarily show statistically signif-icant productivity gains,8indicating that the con-tribution of QE falls below expectations (our firstcontribution).
On the other side, an in-depth anal-ysis abstracting from the presence of outliers andthe high variability across post-editors, indicatesthat the usefulness of QE is verified, at least tosome extent (our second contribution).
Indeed,the marginal productivity gains observed with QEat a global level become statistically significant inspecific conditions, depending on the length (be-tween 5 and 20 words) of the source sentences andthe quality (0.2<HTER?0.5) of the proposed MTsuggestions (our third contribution).AcknowledgementsThis work has been partially supported by the EC-funded projects MateCat (FP7 grant agreementno.
287688) and QT21 (H2020 innovation pro-gramme, grant agreement no.
645452).ReferencesOndrej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Workshopon Statistical Machine Translation.
In Proceedingsof the 8thWorkshop on Statistical Machine Transla-tion, WMT-2013, pages 1?44, Sofia, Bulgaria.Ondrej Bojar, Christian Buck, Christian Federmann,Barry Haddow, Philipp Koehn, Johannes Leveling,Christof Monz, Pavel Pecina, Matt Post, HerveSaint-Amand, Radu Soricut, Lucia Specia, and Ale?sTamchyna.
2014.
Findings of the 2014 workshop8Unless, for instance, robust and non-arbitrary methods toidentify and remove outliers are applied.on statistical machine translation.
In Proceedings ofthe Ninth Workshop on Statistical Machine Transla-tion, pages 12?58, Baltimore, Maryland, USA.Jos?e G. C. de Souza, Matteo Negri, Marco Turchi, andElisa Ricci.
2015.
Online Multitask Learning ForMachine Translation Quality Estimation.
In Pro-ceedings of the 53rd Annual Meeting of the Associa-tion for Computational Linguistics), Beijing, China.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Proceedings of the 7thWork-shop on Statistical Machine Translation (WMT?12),pages 10?51, Montr?eal, Canada.Marcello Federico, Nicola Bertoldi, Mauro Cettolo,Matteo Negri, Marco Turchi, Marco Trombetti,Alessandro Cattelan, Antonio Farina, DomenicoLupinetti, Andrea Martines, Alberto Massidda, Hol-ger Schwenk, Lo?
?c Barrault, Frederic Blain, PhilippKoehn, Christian Buck, and Ulrich Germann.
2014.The MateCat tool.
In Proceedings of COLING 2014,the 25th International Conference on ComputationalLinguistics: System Demonstrations, pages 129?132, Dublin, Ireland.Fei Huang, Jian-Ming Xu, Abraham Ittycheriah, andSalim Roukos.
2014.
Adaptive HTER Estimationfor Document-Specific MT Post-Editing.
In Pro-ceedings of the 52ndAnnual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 861?870, Baltimore, Maryland.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proceedings of the 45thAnnual Meeting of theACL on Interactive Poster and Demonstration Ses-sions, pages 177?180, Stroudsburg, PA, USA.Yashar Mehdad, Matteo Negri, and Marcello Fed-erico.
2012.
Match without a Referee: EvaluatingMT Adequacy without Reference Translations.
InProceedings of the Machine Translation Workshop(WMT2012), pages 171?180, Montr?eal, Canada.Eric W. Noreen.
1989.
Computer-intensive methodsfor testing hypotheses: an introduction.
Wiley Inter-science.Fabian Pedregosa, Gal Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and douard Duchesnay.
2011.Scikit-learn: Machine Learning in Python.
Journalof Machine Learning Research, 12:2825?2830.534Stefan Riezler and John T Maxwell.
2005.
Onsome Pitfalls in Automatic Evaluation and Signifi-cance Testing for MT.
In Proceedings of the ACLworkshop on intrinsic and extrinsic evaluation mea-sures for machine translation and/or summarization,pages 57?64.Peter J Rousseeuw and Annick M Leroy.
1987.
Robustregression and outlier detection, volume 589.
JohnWiley & Sons.Kashif Shah, Marco Turchi, and Lucia Specia.
2014.An efficient and user-friendly tool for machine trans-lation quality estimation.
In Proceedings of theNinth International Conference on Language Re-sources and Evaluation (LREC?14), Reykjavik, Ice-land.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In Proceedings of Association for MachineTranslation in the Americas, pages 223?231, Cam-bridge, Massachusetts, USA.Lucia Specia, Nicola Cancedda, Marc Dymetman,Marco Turchi, and Nello Cristianini.
2009.
Es-timating the Sentence-Level Quality of MachineTranslation Systems.
In Proceedings of the 13thAnnual Conference of the European Associationfor Machine Translation (EAMT?09), pages 28?35,Barcelona, Spain.Lucia Specia, Kashif Shah, Jos?e G.C.
de Souza, andTrevor Cohn.
2013.
QuEst - A Translation Qual-ity Estimation Framework.
In Proceedings of the51stAnnual Meeting of the Association for Compu-tational Linguistics: System Demonstrations, ACL-2013, pages 79?84, Sofia, Bulgaria.Lucia Specia.
2011.
Exploiting Objective Annotationsfor Minimising Translation Post-editing Effort.
InProceedings of the 15thConference of the EuropeanAssociation for Machine Translation (EAMT 2011),pages 73?80, Leuven, Belgium.Marco Turchi, Matteo Negri, and Marcello Federico.2013.
Coping with the Subjectivity of HumanJudgements in MT Quality Estimation.
In Proceed-ings of the 8thWorkshop on Statistical MachineTranslation, pages 240?251, Sofia, Bulgaria.Marco Turchi, Antonios Anastasopoulos, Jos?e G. C. deSouza, and Matteo Negri.
2014a.
Adaptive Qual-ity Estimation for Machine Translation.
In Proceed-ings of the 52nd Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 710?720, Baltimore, Maryland, USA.Marco Turchi, Matteo Negri, and Marcello Federico.2014b.
Data-driven Annotation of Binary MTQuality Estimation Corpora Based on Human Post-editions.
Machine translation, 28(3-4):281?308.535
