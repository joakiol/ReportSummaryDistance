Extraction of Translation Unit from Chinese-English Parallel CorporaCHANG BaobaoInstitute of Computational LinguisticsPeking University,Beijing, P.R.China, 100871chbb@pku.edu.cnPernilla DANIELSSON andWolfgang TEUBERTCentre for Corpus LinguisticsBirmingham University,Birmingham, B15 2TT United Kingdompernilla@ccl.bham.ac.ukteubertw@hhs.bham.ac.ukAbstractMore and more researchers have recognizedthe potential value of the parallel corpus in theresearch on Machine Translation and MachineAided Translation.
This paper examines howChinese English translation units could beextracted from parallel corpus.
An iterativealgorithm based on degree of word association isproposed to identify the multiword units forChinese and English.
Then the Chinese-EnglishTranslation Equivalent Pairs.are extracted fromthe parallel corpus.
We also made comparisonbetween different statistical associationmeasurement in this paper.Keywords: Parallel Corpus, TranslationUnit , Automatic Extraction of TranslationunitIntroductionThe field of machine translation has changedremarkably little since its earliest days in thefifties.
So far, useful machine translation couldonly obtained in very restricted domain.
Webelieve one of the problems of traditionalmachine translation lies in how it deals with unitof translation.
Normally Rule-Based MachineTranslation system takes word as basictranslation unit.
However, word is normallypolysemous and therefore ambiguous, whichcauses many difficulties in selecting propertarget equivalent words in machine translation,especially in translation between unrelatedlanguage pairs, such as Chinese and English.
Onthe other hand, human translation is rarelyword-based.
Human translators always translategroup of words as a whole, which means humando not view words as the basic translation units,and it seems they view language expressions thatcan transfer meaning unambiguously as basictranslation units instead.
Following thisobservation, we believe translation unit shall benot only words but also words groups(Multi-Word Unit) and a collection of bilingualtranslation unit will be certainly a very usefulresource to machine translation.Manual compilation of such a database oftranslation unit is certainly labor intensive.
Butfollowing the recent progress in CorpusLinguistics, especially in parallel corpusresearch such as Gale,W.
(1991), Tufis,D.
(2001),Wu, D., Xia, X.(1994).
Automatic identification oftranslation unit and its target equivalents fromexisted authentic translation might be a feasiblesolution; at least it can be used to produce acandidate list of bilingual translation unit.As a first step towards building a database ofbilingual translation units, we selected the HongKong Legal Documents Corpus (HKLDC) as theparallel corpus for the feasibility study.
Thispaper elaborates the methods we adopted.
Wewill first give our model of (semi-) automaticacquisition of bilingual translation unit based onparallel corpora in section 1.
Then we will showhow the corpus could be preprocessed in section2.
In section 3, several statistic measurementswill be introduced which will serve as a basis forlate steps in extracting of bilingual translationunits.
Section 4 will focuses on identification ofmulti-word units.
Section 5 will describe howtranslation equivalents could be extracted.
Insection 6, we give some evaluation regarding tothe performance in extracting the translationequivalent pairs.1 Framework of automatic acquisition ofbilingual translation unitThe whole process of identification of bilingualtranslation unit could be further divided intothree major steps as depicted in Figure 1.
(1) Preprocessing of the parallel corporaFor the purpose of extracting bilingualtranslation unit, some prior processing of thecorpus is necessary.
These include alignment ofthe bilingual texts at sentence level andunilingual annotation of the Chinese and Englishtexts respectively.
(2)Identification of multi-word unit in thealigned the textsAs we mentioned before, translation unit shallnot be only single words, but also multi-wordunits.
In this step, Both the Chinese and Englishmulti-word units are identified separately fromthe corpus.
(3) Extraction of the bilingual translation unitsAfter identification of the multi-word units fortexts of both languages, this step tries to set thecorrespondence between Chinese and Englishtranslation units.
The result of this step will be alist of bilingual Translation Equivalent Pairs andevery TEP is composed of a Chinese Translationunit and an English Translation unit.Figure 1.
Framework of translation unitacquisition2 Preprocessing the corpusThe Hong Kong legal documents were collectedfrom Internet.
The corpus is composed of lawsand amendments issued by the Hong KongSpecial Administration Region (HKSAR) during.All the texts in it are in both Chinese andEnglish.
We selected about 6 million words ofboth Chinese texts and English words(6,833,762 Chinese words and 6,391,919English words).All the Chinese texts in the corpus areencoded with Big-5 code.
Since all our Chinesetools can only deal with Chinese GB code.
Wefirstly converted all the Chinese texts fromBig-5 code into GB code.
Then the Corpus wasaligned with a length-based sentence aligner.
Forthe legal documents have been already wellarranged with section by section, which makesthe sentence alignment much easier and theprecision is high.
The Chinese texts were thensegmented and pos-tagged with a programdeveloped by the institute of Computationallinguistics, Peking University.
And all theEnglish Texts were tokenized, lemmatized, andpos-tagged with a freely available tree-basedtagger.
Two tag sets were used for Chinese andEnglish respectively, ICL/PKU tag set forChinese texts and UPENN tag set for Englishtexts.
Figure 2. shows a sample of the corpusafter preprocessing.Chinese texts English Texts?<s id=5>?
r??
n?
d???
n??
n?
w?
w<s id=6>??
n?
w?
?<s_id=5>This  DT thisOrdinance NN ordinancemay  MD may...General  JJ generalClauses  NNS clauseOrdinance NN ordinance.
.
.<s_id=6>Remarks NNS remark: : :Figure 2.
Samples of the corpus afterpreprocessingIn Figure 2., both corpus was arranged one tokenper line.
The XML-like tag <s> marks the startof the sentence.
The single-letter tags right to theChinese tokens are their part of speech tags.
Thetwo columns right to the English tokens are partof speech tags and lemmas.3 Statistical measurement usedFour statistical measurements were used inidentification of unilingual multi-word units andthe correspondences of the bilingual translation(3) TEP Extractor(1)Sentence AlignmentChinese Annotation English Annotation(2)Chinese MWUIdentificationEnglish MWUIdentificationunits.
All four statistical formulas measures thedegree of association of two random events.Given two random events, X and Y, theymight be two Chinese words appears in theChinese texts and two translation units appearsin an aligned region of the corpus.
Thedistribution of the two events could be depictedby a 2 by 2 contingency table.Y  Y?X  a bX?
c dFigure 3.
A 2 by 2 contingency tableThe numbers in the four cells of the table has thefollowing meanings:a : all counts of the cases the two events Xand Y co-occur.b : all counts of the cases that X occurs butY does notc : all counts of the cases that X does notoccur but Y doesd : all counts of the cases that both X and Ydo not occurBased on the above-mentioned contingencytable, different kinds of measurements could beused.
We have tried four of them, namely,point-wise mutual information, DICE coefficient,2?
score and Log-likelihood score.
One othermeasurement used by Gale(1991) is 2?
score,which is equivalent to the 2?
score.
All thefour measurements could be easily calculatedusing the following formula.
(1) Point-wise mutual information)()(log),( 2 cabaanttstMI +?+?=(2) DICE coefficient)()(2),( cabaattstDICE +?+=(3) 2?
score)()()()()(),(?22dcdbcabacbdanttst +?+?+?+???
?=(4) Log-Likelihood score))()(log)()(log)()(log)()(log(2),(dbdcnddcadcnccdbbanbbcabanaattstLL+?+??++?+??++?+??++?+???=4.
Identification of multi-word unitsWhat might constitute multi-word units isprobably a question critical to identification ofthem.
It seems rational to assume Multi-wordunits are something between phrases and words,which might have the following properties:1) The component words of a multi-wordunit should tend to co-occur frequently.In the significance of statistics,multi-word unit should be word groupthat co-occur more frequently thanexpectation.2) Multi-words units are not arbitrarycombinations of arbitrary words; theyshall form valid syntactic structure inthe meaning of linguistics.Based on the above-mentioned observations,we used an iterative algorithm using bothstatistical and linguistics means.
The algorithmruns as follows: firstly the algorithm tries to findall word pairs that show strong coherence.
Thiscould be done using the measurements listed insection 3.
After this step, all the word pairs inboth of Chinese texts and English Texts whoseassociation value is greater than a predefinedthreshold are marked.
But this can only list ofword groups of length of 2.
Word groups oflength more than 3 words could not be found byonly one run of the algorithm.
But apparentlythey could be found by a series of runs untilthere are no word groups having greaterassociation value than the threshold anymore.The algorithm is designed as recursive structure,it marks longer word groups by viewing theshorter word group marked in the previous runas one word.It is no doubt that pure statistics cannotperform very reliable.
Some word groups foundby the algorithm are awkward to be accepted asmulti-word unit.
The result of the algorithmshall be viewed as a candidate list ofmulti-words units.
Some kind of refinement ofthe results might be required.
For thinking thatmulti-word unit shall form valid syntacticpattern, we use a filter module which check allthe word groups found and see if they fall into aset of predefined syntactic patterns."a+n","b+n","n+n",?"MWU+n","n+MWU","MWU+MWU""NN+NN","NN+NNS",??"NN+IN<of>""JJ+NN",?
"MWU+MWU"Figure 4.
Syntactic patternsFigure 4. shows some patterns used by thefilter.
Patterns in the left side are for Chinesewhile the right side for English.5.
Extracting of the bilingual translation unitsWe adopt the same hypothesis-testing approachto set the correspondence between theChinese-English translation units.
It follows theobservations that words are translation of eachother are more likely to appear in alignedregions(Gale,W.
(1991), Tufis,D.
(2001)).
But wealso take the multi-word units intoconsideration.The whole procedure could be dividedlogically into two phases.
The first phase couldbe called a generative phase, which lists allpossible translation equivalent pairs from thealigned corpus.
And the second phase can beviewed as a testing operation, which selects theTranslation Equivalent Correspondences showan association measure higher than expectedunder the independence assumption astranslation equivalence pairs.
Again we useDICE coefficient, point-wise mutual information,LL score and 2?
score to measure the degreeof association.One of problems of above-mentionedapproach is its inefficiency in processing largecorpus.
Because in the generative phase, theabove-mentioned approach will list alltranslation equivalent pairs and can lead to hugesearch space.
To make the approach moreefficient, we adopted the following assumption:Source translation units tend to be translated intotranslation units of the same syntactic categories.For example, English nouns tend to be translatedinto Chinese nouns, and English pattern?JJ+NN?
tend to be translated into Chinesepattern ?a+n?
or ?b+n?.
Apparently, thisassumption is not always true for translation ofChinese into English and vice versa.
But it reallymakes the algorithm much more efficient whilethe precision does not fall severely.6.
Experiments and ResultsWe have performed some preliminaryexperiments to test the performance of differentstatistic measurements, performance changewhen the categorial hypothesis is used.For the experiments, we used a very smallportion of the corpus of 500 sentence pairs.Figure 5. show the performance of ChineseMultiWord Unit Identification, we count howmany correct MWUs are there in the firsthundred of candidate MWUs produced by theprogram.MI DICE LL 2?Correct 63 31 76 74Incorrect 37 69 24 26Accuracy  63?
31?
76?
74%Figure 5.
Performance variations of differentstatistical measurements for identification ofMWUFigure 6 shows the performance of the TEPextraction using different statitical means.
wecount how many correct and partially correctcorrespondences there are in the first hundred oftranslation equivalent pairs produced by thealgorithm.MI DICE LL 2?Correct 39 5 70 75Partially correct 5 1 10 6Accuracy  44?
6?
80?
81%Figure 6.
Performance variations of differentstatistical measurements for TEP extractionBoth Figure 5. and Figure 6 shows LL score and2?
score achieves better accuracy over mutualinformation and DICE coefficient.Experiments also show the categorialhypothesis might lead to fall in accuracy, we didtests on the above-mention 500 sentence paircorpus using the hypothesis, the precision fall by4% but the efficiency improved by more than200%.Figure 7. shows a sample of extractedtranslation equivalent pair from the test corpus.Some of them are wrong(see no 2), but most ofthem are correct translation equivalent pairs.Thenumbers in the right are 2?
scores1.
?
see /* 496.471 */2.
???_?
see /* 496.471 */3.
?
subsection  /* 496.237 */4.
??
repeal /* 495.814 */5.
??
order /* 493.195 */7.
??
exemption /* 490.829 */25.
??_??
subsidiary_legislation /* 477.173 */26.
??_??
public_body /* 475.711 */28.
???_??
Financial_Secretary /* 475.711 */31.
??
ordinance /* 470.081 */34.
??_??
primary_instrument /* 468.068 */41.
??_??
health_officer /* 468.068 */42.
???
magistrate /* 468.068 */43.
??
discharge /* 468.068 */45.
??
contract /* 468.068 */46.
??_??_??_?
?Chief_Justice_of_Final Appeal /* 468.068 */53.
??_??_??
?Hong_Kong_Special_Administrative_region/* 448.576 */63.
???
tribunal /* 420.579 */64.
??
declare /* 420.579 */Figure 7. sample of results extracted from thecorpusConclusionAs we see in the last section, the approach usedin this paper does really list many realtranslation equivalent pairs from the corpus.
Itseems not all the results could be taken astranslation units, but it really offers a candidatelist from which useful translation unit could beselected by means of human validation.
For acomplete evaluation of the approach, large scaleexperiments are still needed, which are nowunderway.AcknowledgementsWe would like to give our thanks to ProfessorDan Tufis.
His help in the lexical alignment andsuggestions are very important for our work.
Wealso would like to give thanks to all ourcolleagues who help us in many kinds of forms.ReferencesTeubert,W.(1997).
Translation and the corpus,proceedings of the second TELRI seminar, 147-164.Gale,W.
(1991).
Identifying words correspondencesin parallel Texts, DARPA speech and Naturallanguage workshop.
Asilomar, CA.Tufis,D.
(2001), Computational bilinguallexicography: automatic extraction of translationdictionaries, In Journal of Information Science andTechnology, Romanian Academy, Vol.
4, No.
3Maynard, D., Term Recognition using CombinedKnowledge Sources, PH.
D. thesis, ManchesterUniversity, United Kingdom.Yu Shiwen, Specification of Chinese textsegmentation and POS tagging, see:http://www.icl.pku.edu.cn/research/corpus/coprus-annotation.htmManual of Upenn Tree bank tag set, see:http://www.cis.upenn.edu/~treebank/Wu, D., Xia, X.
(1994), Leaning an English-ChineseLexicon from a Parallel Corpus, in AMTA-94,Association for MT in the Americas, Columbia, MD
