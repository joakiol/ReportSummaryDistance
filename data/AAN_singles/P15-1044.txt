Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 451?461,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsTraining a Natural Language Generator From Unaligned DataOnd?rej Du?ek and Filip Jur?c?
?cekCharles University in Prague, Faculty of Mathematics and PhysicsInstitute of Formal and Applied LinguisticsMalostransk?
n?m?est?
25, CZ-11800 Prague, Czech Republic{odusek,jurcicek}@ufal.mff.cuni.czAbstractWe present a novel syntax-based naturallanguage generation system that is train-able from unaligned pairs of input mean-ing representations and output sentences.It is divided into sentence planning, whichincrementally builds deep-syntactic de-pendency trees, and surface realization.Sentence planner is based on A* searchwith a perceptron ranker that uses noveldiffering subtree updates and a simple fu-ture promise estimation; surface realiza-tion uses a rule-based pipeline from theTreex NLP toolkit.Our first results show that training fromunaligned data is feasible, the outputs ofour generator are mostly fluent and rele-vant.1 IntroductionWe present a novel approach to natural lan-guage generation (NLG) that does not require fine-grained alignment in training data and uses deepdependency syntax for sentence plans.
We includeour first results on the BAGEL restaurant recom-mendation data set of Mairesse et al (2010).In our setting, the task of a natural languagegenerator is that of converting an abstract meaningrepresentation (MR) into a natural language utter-ance.
This corresponds to the sentence planningand surface realization NLG stages as describedby Reiter and Dale (2000).
It also reflects the in-tended usage in a spoken dialogue system (SDS),where the NLG component is supposed to trans-late a system output action into a sentence.
Whilethe content planning NLG stage has been used inSDS (e.g., Rieser and Lemon (2010)), we believethat deciding upon the contents of the system?s ut-terance is generally a task for the dialogue man-ager.
We focus mainly on the sentence planningpart in this work, and reuse an existing rule-basedsurface realizer to test the capabilities of the gen-erator in an end-to-end setting.Current NLG systems usually require a sepa-rate training data alignment step (Mairesse et al,2010; Konstas and Lapata, 2013).
Many of themuse a CFG or operate in a phrase-based fashion(Angeli et al, 2010; Mairesse et al, 2010), whichlimits their ability to capture long-range syntacticdependencies.
Our generator includes alignmentlearning into sentence planner training and usesdeep-syntactic trees with a rule-based surface re-alization step, which ensures grammatical correct-ness of the outputs.
Unlike previous approachesto trainable sentence planning (e.g., Walker et al(2001); Stent et al (2004)), our generator does notrequire a handcrafted base sentence planner.This paper is structured as follows: in Section 2,we describe the architecture of our generator.
Sec-tions 3 and 4 then provide further details on itsmain components.
In Section 5, we describe ourexperiments on the BAGEL data set, followed byan analysis of the results in Section 6.
Section 7compares our generator to previous related worksand Section 8 concludes the paper.2 Generator ArchitectureOur generator (see Figure 1) operates in two stagesthat roughly correspond to the traditional NLGstages of sentence planning and surface realiza-tion.
In the first stage, a statistical sentenceplanner generates deep-syntactic dependency treesfrom the input meaning representation.
These areconverted into plain text sentences in the secondstage by the (mostly rule-based) surface realizer.We use deep-syntax dependency trees to repre-sent the sentence plan, i.e.
the intermediate datastructure between the two aforementioned stages.These are ordered dependency trees that only con-tain nodes for content words (nouns, full verbs, ad-jectives, adverbs) and coordinating conjunctions.451meaning representation (dialogue acts)Sentenceplanner A* searchcandidategeneratorscorerexpand candidatesentence plan treeinto new candidatesscore candidatesto select next oneto be expandedsentence plan (deep syntax tree)plain text sentenceSurfacerealizermostly rule-based pipeline(from TreexNLP toolkit)Word orderingAgreementCompoundverb formsGrammaticalwordsPunctuationWord InflectionPhonetic changesinform(name=X,  type=placetoeat,            eattype=restaurant,  area=riverside,            food=Italian)t-treeX-namen:subjbev:finitalianadj:attrrestaurantn:objrivern:by+XX is an italian restaurant by the river.Figure 1: Overall structure of our generatorEach node has a lemma and a formeme ?
a concisedescription of its surface morphosyntactic form,which may include prepositions and/or subordi-nate conjunctions (Du?ek et al, 2012).
This struc-ture is based on the deep-syntax trees of the Func-tional Generative Description (Sgall et al, 1986),but it has been simplified to fit our purposes (seeFigure 1 in the middle).There are several reasons for taking the tra-ditional two-step approach to generation (as op-posed to joint approaches, see Section 7) and us-ing deep syntax trees as the sentence plan format:First, generating into deep syntax simplifies thetask for the statistical sentence planner ?
the plan-ner does not need to handle surface morphologyand auxiliary words.
Second, a rule-based syntac-tic realizer allows us to ensure grammatical cor-rectness of the output sentences, which would bemore difficult in a sequence-based and/or statisti-cal approach.1And third, a rule-based surface re-alizer from our sentence plan format is relativelyeasy to implement and can be reused for any do-main within the same language.
As in our case, itis also possible to reuse and/or adapt an existingsurface realizer (see Section 4).Deep-syntax annotation of sentences in thetraining set is needed to train the sentence plan-ner, but we assume automatic annotation and reusean existing deep-syntactic analyzer from the TreexNLP framework (Popel and ?abokrtsk?, 2010).2We use dialogue acts (DA) as defined in theBAGEL restaurant data set of Mairesse et al(2010) as a MR in our experiments throughout thispaper.
Here, a DA consists of a dialogue act type,which is always ?inform?
in the set, and a list ofslot-value pairs (SVPs) that contain informationabout a restaurant, such as food type or location(see the top of Figure 1).
Our generator can beeasily adapted to a different MR, though.3 Sentence PlannerThe sentence planner is based on a variant of theA* algorithm (Hart et al, 1968; Och et al, 2001;Koehn et al, 2003).
It starts from an empty sen-tence plan tree and tries to find a path to the opti-mal sentence plan by iteratively adding nodes.
Itkeeps two sets of hypotheses, i.e., candidate sen-tence plan trees, sorted by their score ?
hypothesesto expand (open set) and already expanded (closedset).
It uses the following two subcomponents toguide the search:?
a candidate generator that is able to incre-mentally generate candidate sentence plantrees (see Section 3.1),?
a scorer/ranker that scores the appropriate-ness of these trees for the input MR (see Sec-tion 3.2).1This issue would become more pressing in languageswith richer morphology than English.2See http://ufal.mff.cuni.cz/treex.
Domain-independent deep syntax analysis for several languages isincluded in this framework; the English pipeline used hereinvolves a statistical part-of-speech tagger (Spoustov?
et al,2007) and a dependency parser (McDonald et al, 2005), fol-lowed by a rule-based conversion to deep syntax trees.452t-tree bev:fint-treerecommendv:fint-treeservev:fint-treebev:fint-treerestaurantn:objbev:fint-treebev:fint-treeX-namen:subjbev:fint-treerestaurantn:subjbev:fint-treeX-namen:subjbev:fint-treeX-namen:subj restaurantn:objbev:fint-treeX-namen:subj barn:objOriginal sentenceplan tree: Its successors (selection):Figure 2: Candidate generator example inputs andoutputsThe basic workflow of the sentence planner al-gorithm then looks as follows:Init: Start from an open set with a single emptysentence plan tree and an empty closed set.Loop: 1.
Select the best-scoring candidate Cfrom the open set.
Add C to closedset.2.
The candidate generator generates C,a set of possible successors to C.These are trees that have more nodesthan C and are deemed viable.
Notethat C may be empty.3.
The scorer scores all successors inC and if they are not already in theclosed set, it adds them to the openset.4.
Check if the best successor in theopen set scores better than the bestcandidate in the closed set.Stop: The algorithm finishes if the top score inthe open set is lower than the top score inthe closed set for d consecutive iterations,or if there are no more candidates in theopen set.
It returns the best-scoring candi-date from both sets.3.1 Generating Sentence Plan CandidatesGiven a sentence plan tree, which is typically in-complete and may be even empty, the candidategenerator generates its successors by adding onenew node in all possible positions and with all pos-sible lemmas and formemes (see Figure 2).
Whilea naive implementation ?
trying out any combina-tion of lemmas and formemes found in the trainingdata ?
works in principle, it leads to an unman-ageable number of candidate trees even for a verysmall domain.
Therefore, we include several rulesthat limit the number of trees generated:1.
Lemma-formeme compatibility ?
only nodeswith a combination of lemma and formemeseen in the training data are generated.2.
Syntactic viability ?
the new node mustbe compatible with its parent node (i.e.,this combination, including the dependencyleft/right direction, must be seen in the train-ing data).3.
Number of children ?
no node can have morechildren than the maximum for this lemma-formeme combination seen in the trainingdata.4.
Tree size ?
the generated tree cannot havemore nodes than trees seen in the trainingdata.
The same limitation applies to the in-dividual depth levels ?
the training data limitthe number of nodes on the n-th depth levelas well as the maximum depth of any tree.This is further conditioned on the input SVPs?
the maximums are only taken from trainingexamples that contain the same SVPs that ap-pear on the current input.5.
Weak semantic compatibility ?
we only in-clude nodes that appear in the training dataalongside the elements of the input DA, i.e.,nodes that appear in training examples con-taining SVPs from the current input,6.
Strong semantic compatibility ?
for eachnode (lemma and formeme), we make a?compatibility list?
of SVPs and slots that arepresent in all training data examples contain-ing this node.
We then only allow generatingthis node if all of them are present in the cur-rent input DA.
To allow for more generaliza-tion, this rule can be applied just to lemmas453(disregarding formemes), and a certain num-ber of SVPs/slots from the compatibility listmay be required at maximum.Only Rules 4 (partly), 5, and 6 depend on theformat of the input meaning representation.
Usinga different MR would require changing these rulesto work with atomic substructures of the new MRinstead of SVPs.While especially Rules 5 and 6 exclude a vastnumber of potential candidate trees, this limitationis still much weaker than using hard alignmentlinks between the elements of the MR and the out-put words or phrases.
It leaves enough room togenerate many combinations unseen in the train-ing data (cf.
Section 6) while keeping the searchspace manageable.
To limit the space of potentialtree candidates even further, one could also use au-tomatic alignment scores between the elements ofthe input MR and the tree nodes (obtained using atool such as GIZA++ (Och and Ney, 2003)).3.2 Scoring Sentence Plan TreesThe scorer for the individual sentence plan treecandidates is a function that maps global featuresfrom the whole sentence plan tree t and the inputMR m to a real-valued score that describes the fit-ness of t in the context of m.We first describe the basic version of the scorerand then our two improvements ?
differing subtreeupdates and future promise estimation.Basic perceptron scorerThe basic scorer is based on the linear percep-tron ranker of Collins and Duffy (2002), where thescore is computed as a simple dot product of thefeatures and the corresponding weight vector:score(t,m) = w>?
feat(t,m)In the training phase, the weights w are ini-tialized to one.
For each input MR, the systemtries to generate the best sentence plan tree givencurrent weights, ttop.
The score of this tree isthen compared to the score of the correct gold-standard tree tgold.3If ttop6= tgoldand thegold-standard tree ranks worse than the generatedone (score(ttop,m) > score(tgold,m)), the weightvector is updated by the feature value difference of3Note that the ?gold-standard?
sentence plan trees are ac-tually produced by automatic annotation.
For the purposes ofscoring, they are, however, treated as gold standard.the generated and the gold-standard tree:w = w + ?
?
(feat(tgold,m)?
feat(ttop,m))where ?
is a predefined learning rate.Differing subtree updatesIn the basic version described above, the scorer istrained to score full sentence plan trees.
However,it is also used to score incomplete sentence plansduring the decoding.
This leads to a bias towardsbigger trees regardless of their fitness for the inputMR.
Therefore, we introduced a novel modifica-tion of the perceptron updates to improve scoringof incomplete sentence plans: In addition to up-dating the weights using the top-scoring candidatettopand the gold-standard tree tgold(see above),we also use their differing subtrees titop, tigoldforadditional updates.Starting from the common subtree tcof ttopandtgold, pairs of differing subtrees titop, tigoldare cre-ated by gradually adding nodes from ttopinto titopand from tgoldinto tigold(see Figure 3).
To main-tain the symmetry of the updates in case that thesizes of ttopand tgolddiffer, more nodes may beadded in one step.4The additional updates thenlook as follows:t0top= t0gold= tcfor i in 1, .
.
.min{|ttop| ?
|tc|, |tgold| ?
|tc|} ?
1 :titop= ti?1top+ node(s) from ttoptigold= ti?1gold+ node(s) from tgoldw = w + ?
?
(feat(tigold,m)?
feat(titop,m))Future promise estimationTo further improve scoring of incomplete sentenceplan trees, we incorporate a simple future promiseestimation for the A* search intended to boostscores of sentence plans that are expected to fur-ther grow.5It is based on the expected numberof children Ec(n) of different node types (lemma-formeme pairs).6Given all nodes n1.
.
.
n|t|in a4For example, if tgoldhas 6 more nodes than tcand ttophas 4 more, there will be 3 pairs of differing subtrees, withtigoldhaving 2, 4, and 5 more nodes than tcand titophaving1, 2, and 3 more nodes than tc.We have also evaluated a variant where both sets of sub-trees tigold, titopwere not equal in size, but this resulted indegraded performance.5Note that this is not the same as future path cost inthe original A* path search, but it plays an analogous role:weighing hypotheses of different size.6Ec(n) is measured as the average number of childrenover all occurrences of the given node type in the trainingdata.
It is expected to be domain-specific.454t-treeXn:subjbev:finrestaurantn:objmoderateadj:attrpricen:attrrangen:in+XGold standard tgold:cheapadj:attr italianadj:attrt-treeXn:subjbev:finrestaurantn:objTop generated ttop:t-treeXn:subjbev:finrestaurantn:objCommon subtree tc: Differing subtrees for update:t-treeXn:subjbev:finrestaurantn:objpricen:attrrange+cheapadj:attrt-treeXn:subjbev:finrestaurantn:obj-t1gold t1topFigure 3: An example of differing subtreesThe gold standard tree tgoldhas three more nodes than the common subtree tc, while the top generated tree ttophas two more.Only one pair of differing subtrees t1gold, t1topis built, where two nodes are added into t1goldand one node into t1top.sentence plan tree t, the future promise is com-puted in the following way:fp = ?
?
?w ?|t|?i=1max{0, Ec(ni)?
c(ni)}where c(ni) is the current number of children ofnode ni, ?
is a preset weight parameter, and?wis the sum of the current perceptron weights.
Mul-tiplying by the weights sum makes future promisevalues comparable to trees scores.Future promise is added to tree scores through-out the tree generation process, but it is disre-garded for the termination criterion in the Stopstep of the generation algorithm and in perceptronweight updates.Averaging weights and parallel trainingTo speed up training using parallel processing, weuse the iterative parameter mixing approach ofMcDonald et al (2010), where training data aresplit into several parts and weight updates are av-eraged after each pass through the training data.Following Collins (2002), we record the weightsafter each training pass, take an average at the end,and use this as the final weights for prediction.4 Surface RealizerWe use the English surface realizer from the TreexNLP toolkit (cf.
Section 2 and (Pt?
?cek, 2008)).
Itis a simple pipeline of mostly rule-based blocksthat gradually change the deep-syntactic trees intosurface dependency trees, which are then lin-earized to sentences.
It includes the followingsteps:?
Agreement ?
morphological attributes ofsome nodes are deduced based on agreementwith other nodes (such as in subject-predicateagreement).?
Word ordering ?
the input trees are alreadyordered, so only a few rules for grammaticalwords are applied.?
Compound verb forms ?
additional verbalnodes are added for verbal particles (infini-tive or phrasal verbs) and for compound ex-pressions of tense, mood, and modality.?
Grammatical words ?
prepositions, subordi-nating conjunctions, negation particles, arti-cles, and other grammatical words are addedinto the sentence.?
Punctuation ?
nodes for commas, final punc-tuation, quotes, and brackets are introduced.?
Word Inflection ?
words are inflected accord-ing to the information from formemes andagreement.?
Phonetic changes ?
English ?a?
becomes?an?
based on the following word.The realizer is designed as domain-independentand handles most English grammatical phenom-ena.
A simple ?round-trip?
test ?
using au-tomatic analysis with subsequent generation ?reached a BLEU score (Papineni et al, 2002)of 89.79% against the original sentences on thewhole BAGEL data set, showing only minor dif-ferences between the input sentence and genera-tion output (mostly in punctuation).455restaurantn:objX-arean:in+XandxX-arean:in+Xrestaurantn:objX-arean:in+Xandx X-arean:in+XFigure 4: Coordination structures conversion:original (left) and our format (right).5 Experimental SetupHere we describe the data set used in our experi-ments, the needed preprocessing steps, and the set-tings of our generator specific to the data set.5.1 Data setWe performed our experiments on the BAGELdata set of Mairesse et al (2010), which fitsour usage scenario in a spoken dialogue sys-tem and is freely available.7It contains a to-tal of 404 sentences from a restaurant informa-tion domain (describing the restaurant location,food type, etc.
), which correspond to 202 dia-logue acts, i.e., each dialogue act has two para-phrases.
Restaurant names, phone numbers, andother ?non-enumerable?
properties are abstracted?
replaced by an ?X?
symbol ?
throughout the gen-eration process.
Note that while the data set con-tains alignment of source SVPs to target phrases,we do not use it in our experiments.For sentence planner training, we automaticallyannotate all the sentences using the Treex deepsyntactic analyzer (see Section 2).
The annotationobtained from the Treex analyzer is further simpli-fied for the sentence planner in two ways:?
Only lemmas and formemes are used in thesentence planner.
Other node attributes areadded in the surface realization step (see Sec-tion 5.2).?
We convert the representation of coordinationstructures into a format inspired by UniversalDependencies.8In the original Treex anno-tation style, the conjunction heads both con-juncts, whereas in our modification, the first7Available for download at: http://farm2.user.srcf.net/research/bagel/.8http://universaldependencies.github.ioconjunct is at the top, heading the coordina-tion and the second conjunct (see Figure 4).The coordinations can be easily converted back forthe surface realizer, and the change makes the taskeasier for the sentence planner: it may first gener-ate one node and then decide whether it will add aconjunction and a second conjunct.5.2 Generator settingsIn our candidate generator, we use all the limita-tion heuristics described in Section 3.1.
For strongsemantic compatibility (Rule 6), we use just lem-mas and require at most 5 SVPs/slots from thelemma?s compatibility list in the input DA.We use the following feature types for our sen-tence planner scorer:?
current tree properties ?
tree depth, totalnumber of nodes, number of repeated nodes?
tree and input DA ?
number of nodes per SVPand number of repeated nodes per repeatedSVP,?
node features ?
lemma, formeme, and num-ber of children of all nodes in the current tree,and combinations thereof,?
input features ?
whole SVPs (slot + value),just slots, and pairs of slots in the DA,?
combinations of node and input features,?
repeat features ?
occurrence of repeated lem-mas and/or formemes in the current tree com-bined with repeated slots in the input DA,?
dependency features ?
parent-child pairs forlemmas and/or formemes, including and ex-cluding their left-right order,?
sibling features ?
sibling pairs for lemmasand/or formemes, also combined with SVPs,?
bigram features ?
pairs of lemmas and/orformemes adjacent in the tree?s left-right or-der, also combined with SVPs.All feature values are normalized to have a meanof 0 and a standard deviation of 1, with normaliza-tion coefficients estimated from training data.The feature set can be adapted for a differentMR format ?
it only must capture all importantparts of the MR, e.g., for a tree-like MR, the nodesand edges, and possibly combinations thereof.456SetupBLEU for training portion NIST for training portion10% 20% 30% 50% 100% 10% 20% 30% 50% 100%Basic perc.
46.90 52.81 55.43 54.53 54.24 4.295 4.652 4.669 4.758 4.643+ Diff-tree upd.
44.16 50.86 53.61 55.71 58.70 3.846 4.406 4.532 4.674 4.876+ Future promise 37.25 53.57 53.80 58.15 59.89 3.331 4.549 4.607 5.071 5.231Table 1: Evaluation on the BAGEL data set (averaged over all ten cross-validation folds)?Training portion?
denotes the percentage of the training data used in the experiment.
?Basic perc.?
= basic perceptron updates,?+ Diff-tree upd.?
= with differing subtree perceptron updates, ?+ Future promise?
= with future promise estimation.
BLEUscores are shown as percentages.Based on our preliminary experiments, we use100 passes over the training data and limit thenumber of iterations d that do not improve scoreto 3 for training and 4 for testing.
We use a hardmaximum of 200 sentence planner iterations perinput DA.
The learning rate ?
is set to 0.1.
We usetraining data parts of 36 or 37 training examples(1/10th of the full training set) in parallel training.If future promise is used, its weight ?
is set to 0.3.The Treex English realizer expects not onlylemmas and formemes, but also additional gram-matical attributes for all nodes.
In our experi-ments, we simply use the most common valuesfound in the training data for the particular nodesas this is sufficient for our domain.
In larger do-mains, some of these attributes may have to be alsoincluded in sentence plans.6 ResultsSame as Mairesse et al (2010), we use 10-foldcross-validation where DAs seen at training timeare never used for testing, i.e., both paraphrases ornone of them are present in the full training set.We evaluate using BLEU and NIST scores (Pap-ineni et al, 2002; Doddington, 2002) against bothreference paraphrases for a given test DA.The results of our generator are shown in Ta-ble 1, both for standard perceptron updates and ourimprovements ?
differing subtree updates and fu-ture promise estimation (see Section 3.2).Our generator did not achieve the same perfor-mance as that of Mairesse et al (2010) (ca.
67%).9However, our task is substantially harder sincethe generator also needs to learn the alignmentof phrases to SVPs and determine whether all re-quired information is present on the output (seealso Section 7).
Our differing tree updates clearlybring a substantial improvement over standard per-9Mairesse et al (2010) do not give a precise BLEU scorenumber in their paper, they only show the values in a graph.ceptron updates, and scores keep increasing withbigger amounts of training data used, whereaswith plain perceptron updates, the scores stay flat.The increase with 100% is smaller since all train-ing DAs are in fact used twice, each time with adifferent paraphrase.10A larger training set withdifferent DAs should bring a bigger improvement.Using future promise estimation boosts the scoreseven further, by a smaller amount for BLEU butnoticeably for NIST.
Both improvements on thefull training set are considered statistically signif-icant at 95% confidence level by the paired boot-strap resampling test (Koehn, 2004).
A manual in-spection of a small sample of the results confirmedthat the automatic scores reflect the quality of thegenerated sentences well.If we look closer at the generated sentences (seeTable 2), it becomes clear that the generator learnsto produce meaningful utterances which mostlycorrespond well to the input DA.
It is able to pro-duce original paraphrases and generalizes to pre-viously unseen DAs.On the other hand, not all required informationis always present, and some facts are sometimesrepeated or irrelevant information appears.
Thismostly happens with input slot-value pairs that oc-cur only rarely in the training data; we believe thata larger training set will solve this problem.
Alter-natively, one could introduce additional scorer fea-tures to discourage conflicting information.
An-other problem is posed by repeated slots in the in-put DA, which are sometimes not reflected prop-erly in the generated sentence.
This suggests thata further refinement of the scorer feature set maybe needed.10We used the two paraphrases that come with each DAas independent training instances.
While having two differ-ent gold-standard outputs for a single input is admittedly notideal for a discriminative learner, it still brings an improve-ment in our case.457Input DA inform(name=X-name, type=placetoeat, eattype=restaurant, near=X-near, food=Continental, food=French)Reference X is a French and continental restaurant near X.Generated X is a French and continental restaurant near X.Input DA inform(name=X-name, type=placetoeat, area=riverside, near=X-near, eattype=restaurant)Reference X restaurant is near X on the riverside.Generated X is a restaurant in the riverside area near X.Input DA inform(name=X-name, type=placetoeat, area=X-area, pricerange=moderate, eattype=restaurant)Reference X is a moderately priced restaurant in X.Generated X is a restaurant in the X area.Input DA inform(name=X-name, type=placetoeat, eattype=restaurant, area=riverside, food=French)Reference X is a French restaurant on the riverside.Generated X is a French restaurant in the riverside area which serves French food.Input DA inform(name=X-name, type=placetoeat, eattype=restaurant, pricerange=moderate,area=X-area, food=Contemporary, food=English)Reference X is a moderately priced English contemporary restaurant in X.Generated X is an English restaurant in the X area which serves expensive food in the moderate price rangelocated in X.Input DA inform(name=X-name, type=placetoeat, eattype=restaurant, area=citycentre, near=X-near,food="Chinese takeaway", food=Japanese)Reference X is a Chinese takeaway and Japanese restaurant in the city centre near X.Generated X is a Japanese restaurant in the centre of town near X and X.Input DA inform(name=X-name, type=placetoeat, pricerange=moderate, eattype=restaurant)Reference X is a restaurant that offers moderate price range.Generated X is a restaurant in the moderate price range.Table 2: Example generated sentencesSentences generated when training on the full set and using differing subtree updates and future promise estimation.7 Related WorkPrevious trainable methods in sentence planninguse in principle two techniques: First, in the over-generation and ranking approach (Walker et al,2001; Stent et al, 2004), many sentence plans aregenerated using a rule-based planner and then thebest one is selected by a statistical ranker.
Second,parameter optimization trains adjustable parame-ters of a handcrafted generator to produce outputswith desired properties (Paiva and Evans, 2005;Mairesse and Walker, 2008).
As opposed to ourapproach, both methods require an existing hand-crafted sentence planner.Other previous works combine sentence plan-ning and surface realization into a single step anddo not require a handcrafted base module.
Wongand Mooney (2007) experiment with a phrase-based machine translation system, comparing andcombining it with an inverted semantic parserbased on synchronous context-free grammars.
Luet al (2009) use tree conditional random fieldsover hybrid trees that combine natural languagephrases with formal semantic expressions.
Angeliet al (2010) generate text from database recordsthrough a sequence of classifiers, gradually se-lecting database records, fields, and correspond-ing textual realizations to describe them.
Konstasand Lapata (2013) recast the whole NLG problemas parsing over a probabilistic context-free gram-mar estimated from database records and their de-scriptions.
Mairesse et al (2010) convert inputDAs into ?semantic stacks?, which correspond tonatural language phrases and contain slots andtheir values on top of each other.
Their genera-tion model uses two dynamic Bayesian networks:the first one performs an ordering of the input se-mantic stacks, inserting intermediary stacks whichcorrespond to grammatical phrases, the secondone then produces a concrete surface realization.Dethlefs et al (2013) approach generation as asequence labeling task and use a conditional ran-dom field classifier, assigning a word or a phraseto each input MR element.Unlike our work, the joint approaches typi-cally include the alignment of input MR elementsto output words in a separate preprocessing step(Wong and Mooney, 2007; Angeli et al, 2010), orrequire pre-aligned training data (Mairesse et al,2010; Dethlefs et al, 2013).
In addition, their ba-sic algorithm often requires a specific input MRformat, e.g., a tree (Wong and Mooney, 2007; Luet al, 2009) or a flat database (Angeli et al, 2010;Konstas and Lapata, 2013; Mairesse et al, 2010).While dependency-based deep syntax has beenused previously in statistical NLG, the approachesknown to us (Bohnet et al, 2010; Belz et al, 2012;Ballesteros et al, 2014) focus only on the surfacerealization step and do not include a sentence plan-458ner, whereas our work is mainly focused on statis-tical sentence planning and uses a rule-based real-izer.Our approach to sentence planning is most sim-ilar to Zettlemoyer and Collins (2007), which usea candidate generator and a perceptron ranker forCCG parsing.
Apart from proceeding in the in-verse direction and using dependency trees, we useonly very generic rules in our candidate generatorinstead of language-specific ones, and we incorpo-rate differing subtree updates and future promiseestimation into our ranker.8 Conclusions and Further WorkWe have presented a novel natural language gen-erator, capable of learning from unaligned pairsof input meaning representation and output utter-ances.
It consists of a novel, A*-search-based sen-tence planner and a largely rule-based surface re-alizer from the Treex NLP toolkit.
The sentenceplanner is, to our knowledge, first to use depen-dency syntax and learn alignment of semantic el-ements to words or phrases jointly with sentenceplanning.We tested our generator on the BAGEL restau-rant information data set of Mairesse et al (2010).We have achieved very promising results, the ut-terances produced by our generator are mostly flu-ent and relevant.
They did not surpass the BLEUscore of the original authors; however, our task issubstantially harder as our generator does not re-quire fine-grained alignments on the input.
Ournovel feature of the sentence planner ranker ?
us-ing differing subtrees for perceptron weight up-dates ?
has brought a significant performance im-provement.The generator source code, along with config-uration files for experiments on the BAGEL dataset, is available for download on Github.11In future work, we plan to evaluate our genera-tor on further domains, such as geographic infor-mation (Kate et al, 2005), weather reports (Lianget al, 2009), or flight information (Dahl et al,1994).
In order to improve the performance of ourgenerator and remove the dependency on domain-specific features, we plan to replace the percep-tron ranker with a neural network.
We also wantto experiment with removing the dependency onthe Treex surface realizer by generating directlyinto dependency trees or structures into which de-11https://github.com/UFAL-DSG/tgenpendency trees can be converted in a language-independent way.AcknowledgmentsThis work was funded by the Ministry of Edu-cation, Youth and Sports of the Czech Republicunder the grant agreement LK11221 and core re-search funding, SVV project 260 104, and GAUKgrant 2058214 of Charles University in Prague.
Itused language resources stored and distributed bythe LINDAT/CLARIN project of the Ministry ofEducation, Youth and Sports of the Czech Repub-lic (project LM2010013).The authors would like to thank Luk??
?ilka,Ond?rej Pl?tek, and the anonymous reviewers forhelpful comments on the draft.ReferencesG.
Angeli, P. Liang, and D. Klein.
2010.
A simpledomain-independent probabilistic approach to gen-eration.
In Proc.
of the 2010 Conference on Empir-ical Methods in Natural Language Processing, page502?512.M.
Ballesteros, S. Mille, and L. Wanner.
2014.Classifiers for data-driven deep sentence genera-tion.
In Proceedings of the 8th International NaturalLanguage Generation Conference, pages 108?112,Philadelphia.A.
Belz, B. Bohnet, S. Mille, L. Wanner, and M. White.2012.
The Surface Realisation Task: Recent De-velopments and Future Plans.
In INLG 2012, pages136?140.B.
Bohnet, L. Wanner, S. Mille, and A. Burga.
2010.Broad coverage multilingual deep sentence genera-tion with a stochastic multi-level realizer.
In Proc.of the 23rd International Conference on Computa-tional Linguistics, page 98?106.M.
Collins and N. Duffy.
2002.
New ranking algo-rithms for parsing and tagging: Kernels over discretestructures, and the voted perceptron.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, page 263?270, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.M.
Collins.
2002.
Discriminative training methodsfor hidden Markov models: Theory and experimentswith perceptron algorithms.
In Proceedings of theACL-02 conference on Empirical methods in naturallanguage processing-Volume 10, page 1?8.
Associa-tion for Computational Linguistics.D.
A. Dahl, M. Bates, M. Brown, W. Fisher,K.
Hunicke-Smith, D. Pallett, E. Rudnicky, andE.
Shriberg.
1994.
Expanding the scope of the ATIS459task: the ATIS-3 corpus.
In in Proc.
ARPA Hu-man Language Technology Workshop ?92, Plains-boro, NJ, pages 43?48.
Morgan Kaufmann.N.
Dethlefs, H. Hastie, H. Cuay?huitl, and O. Lemon.2013.
Conditional Random Fields for ResponsiveSurface Realisation using Global Features.
In Pro-ceedings of ACL, Sofia.G.
Doddington.
2002.
Automatic evaluationof machine translation quality using N-gram co-occurrence statistics.
In Proceedings of the Sec-ond International Conference on Human LanguageTechnology Research, pages 138?145, San Fran-cisco, CA, USA.
Morgan Kaufmann Publishers Inc.O.
Du?ek, Z.
?abokrtsk?, M. Popel, M. Majli?,M.
Nov?k, and D. Mare?cek.
2012.
Formemesin English-Czech deep syntactic MT.
In Proceed-ings of the Seventh Workshop on Statistical MachineTranslation, page 267?274, Montreal.P.
E. Hart, N. J. Nilsson, and B. Raphael.
1968.
Aformal basis for the heuristic determination of mini-mum cost paths.
IEEE Transactions on Systems Sci-ence and Cybernetics, 4(2):100?107.R.
J. Kate, Y. W. Wong, and R. J. Mooney.
2005.Learning to transform natural to formal languages.In Proceedings of the National Conference on Ar-tificial Intelligence, volume 20.
Menlo Park, CA;Cambridge, MA; London; AAAI Press; MIT Press;1999.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statis-tical phrase-based translation.
In Proceedings ofNAACL-HLT - Volume 1, page 48?54, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.P.
Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP, page 388?395.I.
Konstas and M. Lapata.
2013.
A global model forconcept-to-text generation.
Journal of Artificial In-telligence Research, 48:305?346.P.
Liang, M. I. Jordan, and D. Klein.
2009.
Learningsemantic correspondences with less supervision.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP: Volume 1-Volume 1, page 91?99.W.
Lu, H. T. Ng, and W. S. Lee.
2009.
Naturallanguage generation with tree conditional randomfields.
In Proc.
of the 2009 Conference on Empir-ical Methods in Natural Language Processing: Vol-ume 1-Volume 1, page 400?409.F.
Mairesse and M. Walker.
2008.
Trainable gen-eration of big-five personality styles through data-driven parameter estimation.
In Proc.
of the 46thAnnual Meeting of the ACL (ACL), page 165?173.F.
Mairesse, M. Ga?i?c, F.
Jur?c?
?cek, S. Keizer, B. Thom-son, K. Yu, and S. Young.
2010.
Phrase-based sta-tistical language generation using graphical modelsand active learning.
In Proc.
of the 48th AnnualMeeting of the ACL, page 1552?1561.R.
McDonald, F. Pereira, K. Ribarov, and J. Haji?c.2005.
Non-projective dependency parsing usingspanning tree algorithms.
In Proceedings of the con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,page 523?530.R.
McDonald, K. Hall, and G. Mann.
2010.
Dis-tributed training strategies for the structured percep-tron.
In Human Language Technologies: The 2010Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 456?464.
Association for Computational Lin-guistics.F.
J. Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.F.
J. Och, N. Ueffing, and H. Ney.
2001.
An efficientA* search algorithm for statistical machine trans-lation.
In Proceedings of the Workshop on Data-driven Methods in Machine Translation - Volume 14,page 1?8, Stroudsburg, PA, USA.
Association forComputational Linguistics.D.
S. Paiva and R. Evans.
2005.
Empirically-basedcontrol of natural language generation.
In Proc.of the 43rd Annual Meeting of ACL, page 58?65,Stroudsburg, PA, USA.
ACL.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In Proceedings of the 40th annualmeeting of the Association for Computational Lin-guistics, page 311?318.M.
Popel and Z.
?abokrtsk?.
2010.
TectoMT: modu-lar NLP framework.
In Proceedings of IceTAL, 7thInternational Conference on Natural Language Pro-cessing, page 293?304, Reykjav?k.J.
Pt??cek.
2008.
Two tectogrammatical realizers sideby side: Case of English and Czech.
In Fourth In-ternational Workshop on Human-Computer Conver-sation, Bellagio, Italy.E.
Reiter and R. Dale.
2000.
Building Natural Lan-guage Generation Systems.
Cambridge Univ.
Press.V.
Rieser and O.
Lemon.
2010.
Natural language gen-eration as planning under uncertainty for spoken dia-logue systems.
In Empirical methods in natural lan-guage generation, page 105?120.P.
Sgall, E.
Haji?cov?, and J. Panevov?.
1986.
Themeaning of the sentence in its semantic and prag-matic aspects.
D. Reidel, Dordrecht.460D.
J.
Spoustov?, J. Haji?c, J. Votrubec, P. Krbec, andP.
Kv?eto?n.
2007.
The Best of Two Worlds: Co-operation of Statistical and Rule-based Taggers forCzech.
In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing: Informa-tion Extraction and Enabling Technologies, pages67?74.
Association for Computational Linguistics.A.
Stent, R. Prasad, and M. Walker.
2004.
Trainablesentence planning for complex information presen-tation in spoken dialog systems.
In Proceedings ofthe 42nd Annual Meeting on Association for Com-putational Linguistics, pages 79?86.M.
A. Walker, O. Rambow, and M. Rogati.
2001.SPoT: a trainable sentence planner.
In Proc.
of2nd meeting of NAACL, page 1?8, Stroudsburg, PA,USA.
ACL.Y.
W. Wong and R. J. Mooney.
2007.
Generationby inverting a semantic parser that uses statisticalmachine translation.
In Proc.
of Human LanguageTechnologies: The Conference of the North Amer-ican Chapter of the ACL (NAACL-HLT-07), page172?179.L.
S. Zettlemoyer and M. Collins.
2007.
Online learn-ing of relaxed CCG grammars for parsing to logi-cal form.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 678?687, Prague.461
