An Experiment On Incremental AnalysisUsing Robust Parsing TechniquesKi l ian  Foth  and Wol fgang Menze l  and Hor ia  F .
Pop  and Ingo  Schr6derfoth  I menze l  I h fpop  I schroeder@nats .
in fonnat ik .mf i -hamburg .deFachbere ich  In fo rmat ik ,  Un ivers i tg t  HamburgVogt-K611n-Strage 30, 22527 Hamburg ,  GermanyAbst ractThe results of an experiment are presented in whichan approach for robust parsing has been applied in-crementally.
They confirm that due to the robust na-ture  of the underlying technology an arbitrary pre-fix of a sentence can be analysed into an interme-diate structural description which is able to directthe further analysis with a high degree of reliability.Most notably, this result can be achieved withoutadapting the gramrnar or the parsing algorithms tothe case of increinental processing.
The resultingincremental parsing procedure is significantly fasterif compared to a non-incremental best-first search.Additionally it turns out that longer sentences ben-efit most from this acceleration.1 I n t roduct ionNatural language utterances usually unfold overtime, i. e., both listening and reading are carriedout in an incremental left-to-right manner.
Model-ing a similar type of behaviour in computer-basedsolutions is a challenging aim particularly interest-ing task for a number of quite different reasons thatare most relevant in the context of spoken languagesystems:?
Without any external signals about the endof an utterance, incremental analysis is theonly means to segment the incoming stream ofspeech input.?
An incremental analysis mode provides for alnore natural (mixed-initiative) dialogue be-haviour because partial results are already avail-able well before the end of an utterance.?
Parsing may already take place in concurrencyto sentence production.
Therefore the speakingtime becomes available as computing time.?
Dynmnic expectations about the upcomingparts of the utterance might be derived right intime to provide guiding hints for other process-ing components, e. g., predictions about likelyword forms for a speech recognizer (Hauensteinand Weber, 1994).In principle, two alternative strategies can be pur-sued when designing all incremental parsing proce-dure:1.
To keel) open all necessary structural hypothe-ses required to accomodate every possible con-tinuation of an utterance.
This is tile strategyusually adopted in an incremental chart parser(WirSn, 1992).2.
To commit to one or a limited number of inter-pretations where(a) either this commitment is made ratherearly and a mechanisnl for partial re-analysis is provided (Lombardo, 1992) or(b) it is delayed until sufficient information iseventually available to take an ultimate de-cision (Marcus, 1987).The apparent efficiency of human language un-derstanding is usually attributed to an early com-mitment strategy.Our approach, in fact, represents an attempt ocombine these two strategies: On the one hand, itkeeps many of the available building blocks for theinitial part of an utterance and passes an ('updated)search space to the following processing step.
Ad-ditionally, the optimal structural description for thedata already available is determined.
This not onlymakes an actual interpretation (together with expec-tations for possible continuations) available to sub-sequent processing components, but also opens upthe possibility to use this information to effectivelyconstrain the set of new structural hypotheses.Determining the optimal interpretation for a yetincomplete sentence, however, requires a parsing ap-proach robust enough to analyse an arbitrary sen-tencc prefix into a meaningful structure.
Thereforetwo closely related questions need to be raised:1.
Can the necessary degree of robustness beachieved?2.
Is the information contained in the currentlyoptimal structure useful to guide the subsequentanalysis?1026To answer these questions and to estimate the po-tential for search space reductions against a 1)ossibleloss of accuracy, a series of experiments has beenconducted.
Sections 2 and 3 introduce and motivatethe framework of the exl)eriments.
Section 4 de-scribes a number of heuristics for re-use of previoussolutions and Section 5 1)resents the results.2 Robust  Parsing in a DependencyFrameworkOur grammar models utterances as depertdeTzcyt ryes, which consist of pairs of words so that onedepends directly on the other.
This subordinationrelation can be qualified by a label (e. g. to distin-guish conq)lements fl:om modifiers).
Since each wordcml only depend on one other word, a labeled treeis formed, usually with the finite verb as its root.The decision on which stru(:ture to 1)ostulate foran utterance is guided 1)y explicit constrai~,ts, whichare rel)resented as mfiversally quantified logical for-mulas about features of word tbrms and l)artial trees.t~r instance, one constraint might t)ostulate that arelation labeled as 'Subject' can only occur betweena noun and a finite verb to its right, or that two dif-ferent del)endencies of the same verb may not bothbe labeled 'Sul)ject'.
For efficiency reasons, theseformulas may constrain individual del)endency edgesor pairs of edges only.
The al)plication of constraintsCall I)egin as soou as the first word of an utteranceis read; no global information about the utterance isrequired for analysis of its beginning.Since natm:al language inl)ut will often exhibit ir-regularities such as restarts, repairs, hesitations andother gr~tmmatical errors, individual errors shouldnot make further analysis impossible.
Instead, a ro-bust 1)arser should continue to build a structure tbrthe utterance.
Ideally, this structure should be closeto that of a similar, but grammatical utterance.This goal is attained by annotating the constraintsthat constitute the grammar with scores rangingfrom 0 to 1.
A structure that violates one or moreconstraints i annotated with the product of the co lresponding scores, and the structure with the highestcombined score is defined as the solution of the pars-lug problem.
In general, the higher the scores of theconstraints, the more irregular constructions can 1)eanalysed.
Parsing an utterance with mmotated orsoft constraints thus mnounts to multi-dimensionaloptimization.
Both complete and heuristic searchmethods can be eml)loyed to solve such a t)roblem.Our robust al)proach also provides m~ easy wayto implement partial parsing.
If necessary, e. g., anisolated noun labeled as 'Subject' may form the rootof a del)endency tree, although this would violatethe first constraint lnentioned above.
If a finite verl)is available, however, subordinating the noun underthe verb will avoid the error and thus produce abetter structure.
This capability is crucial for theanalysis of incomplete utterances.Different lcvcls of analysis can be defined to modelsyntactic as well as semantic structures.
A depel>dency tree is constructed for each of these levels.Since constraints can relate the edges in parallel de-pendency trees to each other, having several treescontributes to the robustness of the approach.
Al-together, the gramlnar used in the experiments de-scribed comprises \]2 levels of analysis and 490 con-straints (SchrSder el, al., 2000).3 Pref ix Parsing wi th  WeightedConst ra in tsIn general, dependency analysis is well-suited forincremental analysis.
Since subordinations alwaysconcern two words rather than full constituents, eachword can be integrated into the analysis as soon as itis read, although not necessarily in the ol)timal way.Also, the 1)re-colnl)uted dependency links can easily1)e re-used in subsequent i erations.
Therefore, de-1)endency grammar allows a fine-grained incrementalanalysis (boxnbardo, 1992).mr c/"\, r2 " -  modo" ?
\o ?daal; lassen sie uns dochI hen lat you usz" c1.,.
I j "~0mod/ /  / .rloch einen Ten'nil;<pa~t> yet a meeting(a)-\c "~ mod\\ ~o  0 0 / ~ \',dana lassen sie uns dochThen let you us <part>Let's appoint yet another meeting then.O"cJ -" :i IB",,,oa ~ ~ i (b)o~ ~noch einoll Terrain ausmachenyel a meeting appointlPigure 1: An example for a prefix analysisWhen assigning a det)endency structure to incom-plete utterances, the problem arises how to analysewords whose governors or complements still lie be-yond the time horizon.
Two distinct alternatives are1)ossible:1.
The parser can establish a dependency betweentilt word and a special node representing a im-tative word that is assmned to follow in the re-maining input.
This explicitly models the ex-pectations that would be raised by the prefix.llowever, unifying new words with these under-specified nodes is difficult, particularly when1027multiple words have been conjectured.
Also,many constraints cannot be meaningfully ap-plied to words with unknown features.2.
An incomplete prefix can be analyzed directlyif a grammar is robust enough to allow par-tial parsing as discussed in the previous sec-tion: If the constraint that forbids multipletrees receives a severe but non-zero penalty,missing governors or complements are accept-able as long as no better structure is possible.Experiments in prefix parsing using a dependencygrammar of German have shown that even complexutterances with nested subclauses can be analysedin the second way.
Figure la provides an exampleof this: Because the infinitive verb 'ausmachen' isnot yet visible, its coinplement ~Terinin' is analysedas an isolated subtree, and the main verb 'lassen' islacking a comI)lement.
After the missing verb hasbeen read, two additional dependency edges sufficeto build the correct structure from the partial parse.This method allows direct comparison between in-cremental and non-incremental parser runs, sinceboth methods use tim same grammar.
Therefore,we will follow up on the second alternative only andconstruct extended structures guided by the struc-tures of prefixes, without explicitly modeling missingwords.4 Re-Use  of  Part ia l  Resu l tsWhile a prefix analysis can produce partial parsesand diagnoses, so far this inforlnation has not beenused in subsequent i erations.
In fact, after a newword has been read, another search is conducted onall words already available.
To reduce this duplica-tion of work, we wish to narrow down the problemspace for these words.
Therefore, at each iteration,the set of hypotheses has to be updated:?
By deciding which old dependency hypothesesshould be kept.?
By deciding which new dependency hypothesesshould be added to the search space in order toaccomodate the incoming word.For that purpose, several heuristics have been de-vised, based on the following principles:P red ic t ion  s t rength .
Restrict the search space asmuch as possible, while maintaining correct-ness.Economy.
Keep as nmch of the previous structureas possible.Rightmost attachment.
Attach the incomingword to the most recent words.The heuristics are presented here in increasing or-der of the size of the problem space they produce:A.
Keep all dependency edges from the previousoptimal solution.
Add all dependency edgeswhere the incoming word modifies, or is modi-fied by, another word.B.
As A, but also keep all links that differ h'om theprevious optimal solution only in their lexicalreadings.C.
As B, but also keep all links that differ fl'om theprevious optimal solution only in the subordi-nation of its last word.D.
As C, but also keep all links that differ from theprevious optimal solution only in the subordi-nation of all the words lying on the path h'omthe last word to the root of the solution tree.E.
As D, but for all trees in the previous solution.5 Resu l tsIn order to evaluate the potential of the heuristicsdescribed above, we have conducted a series of ex-periments using a grammar that was designed fornon-incremental, robust parsing.
We tested the in-crelnental against a non-incremental parser using222 utterances taken from the VERBMOBIL do-main (Wahlster, 1993).V:\] -o -.\[=\]1 r;A B C D EHeuristicsFigure 2: Solution quality and processing time fordifferent heuristicsFigure 2 compares the five heuristics with respectto tile following criteria: 1Accuracy.
The accuracy (gray bar) describes howmany edges of the solutions are correct.correct edgesaccuracy = ~ edges foundtNote that the heuristics provide at most one solution andmay fail to find any solution.1028Weak recall .
We base our recall measure - given asthe black bar - on the number of solutions foundnon-incrementally (which is less than 100%) be-cause we want focus on tile impact of our heuris-tics, not the coverage of tile grammar.weak recall = @ correct edges@ edges fouud non-incrementallyRe la t ive  run- t ime.
The run-time required by theincremental procedure as a percentage of thetime required by the non-iucremental search al-gorithm is given as the white bar.The difference between tile gray and the black baris due to errors of the heuristic method, i. e., ei-ther because of its incal)ability to find the correctsubordination or due to excessive resource demands(which lead to process abortion).2Nvo observations can be made: First, all buttt,e last heuristics need less time than the non-incremental algorithm to complete while maintain-ing a relative high degree of quality.
Second, themore elaborate the heuristics are, the longer theyneed to run (as expected) and the better are the re-sults for the accuracy measure.
However, the lmuris-tics D and E could not complete to parse all sen-tences because in some cases a pre-defined time limitwas exceeded; this leads to the observed decreasein weak recall when compared to heuristics C. Asexpected, a trade-off between computing time andquality can be found.
Overall, heuristics C seems tobe a good choice because it achieves all accuracy ofup to 93.7% in only one fifth of the run-time.250%:200%t50%~00%50%FigureFigureheuristics~ Time for incremen-tal compared to non-incremental methodD Absolute time for in-cremental (16...20 setto 100%)?
The gray bar presents tile normalized time withthe time for sentence length between 16 and 20set to 100%.Tile results show that the speedup observed inFigure 2 is not evenly distributed.
While the incre-mental analysis of the short sentences takes longer(2.5 times slower) than the non-incremental go-rithm, the opposite is true for longer sentences (10times faster).
However, this is welcome behavior:The incremental procedure takes longer only in thosecases that are solved very fast anyway; tim problem-atic cases are parsed more quickly.
This behavior isa first hint that the incremental nalysis with re-useof partial results is a step that alleviates the combi-natorial explosion of resource demands.-?
~ ~m ~ cd100%" c-i , ,80%.60%"40%'20%O%1.
.
.5  6 .
.
.10  11 .
.
.15  16 .
.
.20  overallSentence Length3: Processing time vs. sentence length3 coral)ares the time requirements ofC for different sentence lengths.?
The relative run-time (as in Figure 2) is givenas the wlfite bar.0%,1...5 6...10 11...15 16...20 overallSentence LengthFigure 4: Accuracy vs. sentence length (colors havetile same meaning as ill Figure 2)Finally, Figure 4 compares the quality resultingfrom heuristics C for different sentence lengths.
Itturns out that, although a slight decrease is observ-able, the accuracy is relatively independent of sen-tence length.I I?
~ r -q~ ~ I ~.1 I ~ ~ 6 Conc lus ions  co c,~m 6 ~ An apl)roach to the incremental parsing of naturallanguage utterances has been presented, which isbased on tlle idea to use robust parsing techniquesto deal with incomplete sentences.
It determines astructural description for arbitrary sentence prefixesby searching for the optimal combination of localhypotheses.
This search is conducted in a problemspace which is repeatedly narrowed down accordingto the optimal solution found in tile preceding stepof analysis.1029The results available so far confirm the initial ex-pectation that the grammar used is robust enoughto reliably carry out such a prefix analysis, al-though it has originally been developed for the non-incremental case.
The optimal structure as deter-mined by the parser obviously contains relevant in-formation about the sentence prefix, so that evenvery simple and cheap heuristics can achieve a con-siderable level of accuracy.
Therefore, large parts ofthe search space can be excluded fi'oln repeated re-analysis, which eventually makes it even faster thanits non-incremental counterpart.
Most importantly,the observed speedup grows with the length of theutterance.On the other hand, none of the used structure-based heuristics produces a significant iml)rovementof quality even if a large amount of computationalresources is spent.
Quite a number of cases canbe identified where even the most expensive of ourheuristics is not strong enough, e. g., the Germansentence with a topicalized irect object:DieNOM,ACe Frau sieht derNOM Mama.The woman sees tile man.The woman, the man sees.Here, when analysing the subsentence die Frausieht, the parser will wrongly consider die Frau asthe subject, because it appears to have the rightcase and there is a clear preference to do so.
Later,when the next word comes in, there is no way toallow for dic Frau to change its structural interpre-tation, because this is not licensed by any of thegiven heuristics.Therefore, substantially more i)roblenl-orielltedheuristics are required, which should take into ac-count not only the ol)timal structure, but also theconflicts caused by it.
Using a weak but cheapheuristics, a fast al)proximation of the optimal struc-ture can be obtained within a very restricted searchspace, and then refined by subsequent structuraltransformations (Foth et al, 2000).
To a certaindegree this resembles the idea of applying reasonmaintenance t chniques for conflict resolution in in-cremental parsing (Wir6n, 1990).
In deciding whichstrategy is good enough to find the necessary firstapproximation the results of this paper might play acrucial role, since the I)ossible contribution of indi-vidual heuristics in such all extended fi'amework canbe precisely estimated.AcknowledgementsThis research as been partly fimded by the GermanResearch Foundation "Deutsche Forschungsgemein-schaft" under grant no.
Me 1472/1-2.ReferencesKilian Foth, Wolfgang Menzel, and Ingo SchrSder.2000.
A transformation-based parsing techniquewith anytime property.
In Procecdings of theInternational Workshop on Parsing Technologies(IWPT-2000), pages 89-100, Trento, Italy.Andreas Hauenstein and Hans Weber.
1994.
An in-vestigation of tightly coupled time synchronousspeech language interfaces using a unificationgrammar.
In Proceedings of the i2th NationalConference on Artificial Intelligence: Workshopon the Integration of Natural Language and SpeechProcessing, pages 42-49, Seattle, Washington.Johannes Heinecke, J/irgen Kunze, Wolfgang Men-zel, and Ingo SchrSder.
1998.
Eliminative pars-ing with graded constraints.
In Proceedings ofthe Joint Conference COLING/ACL-98, Mon-trial, Canada.Vincenzo Lombardo.
1992.
Incremental dependencyparsing.
In P~vceedings of the Annual Meeting ofthe ACL, Delaware, Newark, USA.Mitchell P. Marcus.
1987.
Deterministic pars-ing and description theory.
In Peter Whitclock,Mary McGee Wood, Harold Seiners, Rod John-son, and Paul Bennett, editors, Linguistic Theoryand Computer Applications', pages 69-112.
Aca-demic Press, London, England.Wolfgang Menzel and ingo SchrSder.
1998.
Decisionprocedures for dependency parsing using gradedconstraints.
In Sylvain Kahane and Alain Pol-gu~re, editors, Proceedings of the Joint Confer-ence COLING/ACL-98 Workshop: Processing ofDependency-based Grammars, pages 78-87, Mon-trSal, Canada.Ingo SchrSder, Wolfgang Menzel, Kilian Foth,and Michael Schulz.
2000.
Modeling depen-dency grammar with restricted constraints.
In-ternational Journal J?
'aitemcnt Automatique desLangues: Grammaires de d@endance, 41(1).Wolfgang Wahlster.
1993.
Verbmobih Translationof face-to-face dialogs.
In Proceedings of the 3rdEuropean Conference on @eech Communicationand Technology, pages 29-38, Berlin, Germany.Itans Weber.
1995.
LR-inkrementelles proba-bilistisches Chartparsing von Worthypothesen-graphen mit Unifikationsgrammatiken: Eine engeKopplung von Suche und Analyse.
Verbmobil-Report 52, UniversitAt Erlangen-Niirnberg.Mats Wir6n.
1992.
Studies in Incremental Natural-Language Analysis.
Ph.D. tlmsis, Department ofComputer and Information Science, LinkSpingUniversity, LinkSping, Sweden.Mats WirSn.
1990.
Incremental parsing and reasonmaintenance.
In Proceedings of the i3th Interna-tional Conference on Computational Linguistics(COLING-90), pages 287-292, Helsinki, Finland.1030
