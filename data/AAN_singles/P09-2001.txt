Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 1?4,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPVariational Inference for Grammar Induction with Prior KnowledgeShay B. Cohen and Noah A. SmithLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{scohen,nasmith}@cs.cmu.eduAbstractVariational EM has become a populartechnique in probabilistic NLP with hid-den variables.
Commonly, for computa-tional tractability, we make strong inde-pendence assumptions, such as the mean-field assumption, in approximating pos-terior distributions over hidden variables.We show how a looser restriction on theapproximate posterior, requiring it to be amixture, can help inject prior knowledgeto exploit soft constraints during the varia-tional E-step.1 IntroductionLearning natural language in an unsupervised waycommonly involves the expectation-maximization(EM) algorithm to optimize the parameters of agenerative model, often a probabilistic grammar(Pereira and Schabes, 1992).
Later approaches in-clude variational EM in a Bayesian setting (Bealand Gharamani, 2003), which has been shown toobtain even better results for various natural lan-guage tasks over EM (e.g., Cohen et al, 2008).Variational EM usually makes the mean-fieldassumption, factoring the posterior over hiddenvariables into independent distributions.
Bishop etal.
(1998) showed how to use a less strict assump-tion: a mixture of factorized distributions.In other work, soft or hard constraints on theposterior during the E-step have been exploredin order to improve performance.
For example,Smith and Eisner (2006) have penalized the ap-proximate posterior over dependency structuresin a natural language grammar induction task toavoid long range dependencies between words.Grac?a et al (2007) added linear constraints on ex-pected values of features of the hidden variables inan alignment task.In this paper, we use posterior mixtures to injectbias or prior knowledge into a Bayesian model.We show that empirically, injecting prior knowl-edge improves performance on an unsupervisedChinese grammar induction task.2 Variational Mixtures with ConstraintsOur EM variant encodes prior knowledge in an ap-proximate posterior by constraining it to be froma mixture family of distributions.
We will use x todenote observable random variables, y to denotehidden structure, and ?
to denote the to-be-learnedparameters of the model (coming from a subset ofR`for some `).
?
will denote the parameters ofa prior over ?.
The mean-field assumption in theBayesian setting assumes that the posterior has afactored form:q(?,y) = q(?
)q(y) (1)Traditionally, variational inference with the mean-field assumption alternates between an E-stepwhich optimizes q(y) and then an M-step whichoptimizes q(?
).1The mean-field assumptionmakes inference feasible, at the expense of op-timizing a looser lower bound on the likelihood(Bishop, 2006).
The lower bound that the algo-rithm optimizes is the following:F (q(?,y),?)
= Eq(?,y)[log p(x,y,?
| ?
)]+H(q)(2)where H(q) denotes the entropy of distribution q.We focus on changing the E-step and as a result,changing the underlying bound, F (q(?,y),?
).Similarly to Bishop et al (1998), instead of mak-ing the strict mean-field assumption, we assumethat the variational model is a mixture.
One com-ponent of the mixture might take the traditionalform, but others will be used to encourage certain1This optimization can be nested inside another EM al-gorithm that optimizes ?
; this is our approach.
q(?)
is tra-ditionally conjugate to the likelihood for computational rea-sons, but our method is not limited to that kind of prior, asseen in the experiments.1tendencies considered a priori to be appropriate.Denoting the probability simplex of dimension r4r= {?
?1, ..., ?r?
?
Rr: ?i?
0,?ri=1?i=1}, we require that:q(?,y | ?)
=?ri=1?iqi(y)qi(?)
(3)for ?
?
4r.
Qiwill denote the family of distri-butions for the ith mixture component, and Q(4r)will denote the family implied by the mixture ofQ1, .
.
.
,Qrwhere the mixture coefficients ?
?4r.
?
comprise r additional variational param-eters, in addition to parameters for each qi(y) andqi(?
).When one of the mixture components qiis suf-ficiently expressive, ?
will tend toward a degener-ate solution.
In order to force all mixture compo-nents to play a role?even at the expense of thetightness of the variational bound?we will im-pose hard constraints on ?
: ?
??4r?
4r.
Inour experiments (?3),?4rwill be mostly a line seg-ment corresponding to two mixture coefficients.The role of the variational EM algorithm is tooptimize the variational bound in Eq.
2 with re-spect to q(y), q(?
), and ?.
Keeping this intentionin mind, we can replace the E-step and M-step inthe original variational EM algorithm with 2r + 1coordinate ascent steps, for 1 ?
i ?
r:E-step: For each i ?
{1, ..., r}, optimize thebound given ?
and qi?(y)|i??{1,...,r}\{i}andqi?(?)|i??
{1,...,r}by selecting a new distributionqi(y).M-step: For each i ?
{1, ..., r}, optimize thebound given ?
and qi?(?)|i??{1,...,r}\{i}andqi?(y)|i??
{1,...,r}by selecting a new distributionqi(?
).C-step: Optimize the bound by selecting a new setof coefficients ?
?
?4rin order to optimize thebound with respect to the mixture coefficients.We call the revised algorithm constrained mix-ture variational EM.For a distribution r(h), we denote by KL(Qi?r)the following:KL(Qi?r) = minq?QiKL(q(h)?r)) (4)where KL(???)
denotes the Kullback-Leibler di-vergence.The next proposition, which is based on a resultin Grac?a et al (2007), gives an intuition of howmodifying the variational EM algorithm with Q =Q(?4r) affects the solution:Proposition 1.
Constrained mixture variationalEM finds local maxima for a function G(q,?
)such thatlog p(x | ?)?
min???4rL(?,?)
?
G(q,?)
?
log p(x | ?
)(5)where L(?,?)
=r?i=1?iKL(Qi?p(?,y | x,?
)).We can understand mixture variational EM aspenalizing the likelihood with a term bounded bya linear function of the ?, minimized over?4r.
Wewill exploit that bound in ?2.2 for computationaltractability.2.1 Simplex AnnealingThe variational EM algorithm still identifies onlylocal maxima.
Different proposals have been forpushing EM toward a global maximum.
In manycases, these methods are based on choosing dif-ferent initializations for the EM algorithm (e.g.,repeated random initializations or a single care-fully designed initializer) such that it eventuallygets closer to a global maximum.We follow the idea of annealing proposed inRose et al (1990) and Smith and Eisner (2006) forthe ?
by gradually loosening hard constraints on ?as the variational EM algorithm proceeds.
We de-fine a sequence of?4r(t) for t = 0, 1, ... such that?4r(t) ??4r(t+1).
First, we have the inequality:KL(Q(?4r(t))?p(?,y | x,?)
(6)?
KL(Q(?4r(t + 1))?p(?,y | x,?
))We say that the annealing schedule is ?
-separatedif we have for any ?
:KL(Q(?4r(t))?p(?,y | x,?))
(7)?
KL(Q(?4r(t + 1))?p(?,y | x,?))
??2(t+1)?
-separation requires consecutive familiesQ(?4r(t)) and Q(?4r(t + 1)) to be similar.Proposition 1 stated the bound we optimize,which penalizes the likelihood by subtracting apositive KL divergence from it.
With the ?
-separation condition we can show that even thoughwe penalize likelihood, the variational EM algo-rithm will still increase likelihood by a certainamount.
Full details are omitted for space and canbe found in ?
).2Input: initial parameters ?
(0), observed data x,annealing schedule?4r: N?
24rOutput: learned parameters ?
and approximateposterior q(?,y)t?
1;repeatE-step: repeatE-step: forall i ?
[r] do: q(t+1)i(y)?
argmaxq(y)?QiF?
(Pj 6=i?jq(t)i(?
)q(y) + ?iq(t)iq(y),?
(t))M-step: forall i ?
[r] do: q(t+1)i(?)?
argmaxq(?)?QiF?
(Pj 6=i?jq(?
)q(t)i(y) + ?iq(t)iq(y),?
(t))C-step: ?(t+1)?argmax???4r(t)F?(Prj=1?jq(t)i(?)q(t)i(y),?
(t))until convergence ;M-step: ?(t+1)?argmax?F?(Pri=1?iq(t+1)i(?)q(t+1)i(y),?)t?
t + 1;until convergence ;return ?(t),Pri=1?iq(t)i(?
)q(t)i(y)Figure 1: The constrained variational mixture EM algorithm.
[n] denotes {1, ..., n}.2.2 TractabilityWe now turn to further alterations of the bound inEq.
2 to make it more tractable.
The main problemis the entropy term which is not easy to compute,because it includes a log term over a mixture ofdistributions from Qi.
We require the distributionsin Qito factorize over the hidden structure y, butthis only helps with the first term in Eq.
2.We note that because the entropy function isconvex, we can get a lower bound on H(q):H(q) ?
?ri=1?iH(qi) =?ri=1?iH(qi(?,y))Substituting the modified entropy term intoEq.
2 still yields a lower bound on the likeli-hood.
This change makes the E-step tractable,because each distribution qi(y) can be computedseparately by optimizing a bound which dependsonly on the variational parameters in that distribu-tion.
In fact, the bound on the left hand side inProposition 1 becomes the function that we opti-mize instead of G(q,?
).Without proper constraints, the ?
update can beintractable as well.
It requires maximizing a lin-ear objective (in ?)
while constraining the ?
tobe from a particular subspace of the probabilitysimplex,?4r(t).
To solve this issue, we requirethat?4r(t) is polyhedral, making it possible to ap-ply linear programming (Boyd and Vandenberghe,2004).The bound we optimize is:2F?(r?i=1?iqi(?,y),?
)(8)=r?i=1?i(Eqi(?,y)[log p(?,y,x | m)] + H(qi(?,y)))with ?
?
?4r(tfinal) and (qi(?,y)) ?
Qi.
Thealgorithm for optimizing this bound is in Fig.
1,which includes an extra M-step to optimize?
(seeextended report).3 ExperimentsWe tested our method on the unsupervised learn-ing problem of dependency grammar induction.For the generative model, we used the dependencymodel with valence as it appears in Klein andMan-ning (2004).
We used the data from the Chi-nese treebank (Xue et al, 2004).
Following stan-dard practice, sentences were stripped of wordsand punctuation, leaving part-of-speech tags forthe unsupervised induction of dependency struc-ture, and sentences of length more than 10 wereremoved from the set.
We experimented witha Dirichlet prior over the parameters and logis-tic normal priors over the parameters, and foundthe latter to still be favorable with our method, asin Cohen et al (2008).
We therefore report resultswith our method only for the logistic normal prior.We do inference on sections 1?270 and 301?1151of CTB10 (4,909 sentences) by running the EM al-gorithm for 20 iterations, for which all algorithmshave their variational bound converge.To evaluate performance, we report the fractionof words whose predicted parent matches the goldstandard (attachment accuracy).
For parsing, weuse the minimum Bayes risk parse.Our mixture componentsQiare based on simplelinguistic tendencies of Chinese syntax.
These ob-servations include the tendency of dependencies to(a) emanate from the right of the current positionand (b) connect words which are nearby (in stringdistance).
We experiment with six mixture com-ponents: (1) RIGHTATTACH: Each word?s parentis to the word?s right.
The root, therefore, is al-ways the rightmost word; (2) ALLRIGHT: Therightmost word is the parent of all positions in thesentence (there is only one such tree); (3) LEFT-CHAIN: The tree forms a chain, such that each2This is a less tight bound than the one in Bishop et al(1998), but it is easier to handle computationally.3learningsettingLEFTCHAIN 34.9vanilla EM 38.3LN, mean-field 48.9This paper: I II IIIRIGHTATTACH 49.1 47.1 49.8ALLRIGHT 49.4 49.4 48.4LEFTCHAIN 47.9 46.5 49.9VERBASROOT 50.5 50.2 49.4NOUNSEQUENCE 48.9 48.9 49.9SHORTDEP 49.5 48.4 48.4RA+VAR+SD 50.5 50.6 50.1Table 1: Results (attachment accuracy).
The baselines areLEFTCHAIN as a parsing model (attaches each word to theword on its right), non-Bayesian EM, and mean-field vari-ational EM without any constraints.
These are comparedagainst the six mixture components mentioned in the text.
(I)corresponds to simplex annealing experiments (?
(0)1= 0.85);(II?III) correspond to fixed values, 0.85 and 0.95, for themixture coefficients.
With the last row, ?2to ?4are always(1?
?1)/3.
Boldface denotes the best result in each row.word is governed by the word to its right; (4) VER-BASROOT: Only verbs can attach to the wall node$; (5) NOUNSEQUENCE: Every sequence of nNN(nouns) is assumed to be a noun phrase, hence thefirst n?1 NNs are attached to the last NN; and (6)SHORTDEP: Allow only dependencies of lengthfour or less.
This is a strict model reminiscentof the successful application of structural bias togrammar induction (Smith and Eisner, 2006).These components are added to a variationalDMV model without the sum-to-1 constraint on?.
This complements variational techniques whichstate that the optimal solution during the E-stepfor the mean-field variational EM algorithm is aweighted grammar of the same form of p(x,y | ?
)(DMV in our case).
Using the mixture compo-nents this way has the effect of smoothing the esti-mated grammar event counts during the E-step, inthe direction of some prior expectations.Let ?1correspond to the component of the orig-inal DMV model, and let ?2correspond to one ofthe components from the above list.
Variationaltechniques show that if we let ?1obtain the value1, then the optimal solution will be ?1= 1 and?2= 0.
We therefore restrict ?1to be smaller than1.
More specifically, we use an annealing processwhich starts by limiting ?1to be ?
s = 0.85 (andhence limits ?2to be ?
0.15) and increases s ateach step by 1% until s reaches 0.95.
In addition,we also ran the algorithm with ?1fixed at 0.85 and?1fixed at 0.95 to check the effectiveness of an-nealing on the simplex.Table 1 describes the results of our experi-ments.
In general, using additional mixture com-ponents has a clear advantage over the mean-fieldassumption.
The best result with a single mix-ture is achieved with annealing, and the VERBAS-ROOT component.
A combination of the mix-tures (RIGHTATTACH) together with VERBAS-ROOT and SHORTDEP led to an additional im-provement, implying that proper selection of sev-eral mixture components together can achieve aperformance gain.4 ConclusionWe described a variational EM algorithm that usesa mixture model for the variational model.
Werefined the algorithm with an annealing mecha-nism to avoid local maxima.
We demonstratedthe effectiveness of the algorithm on a dependencygrammar induction task.
Our results show thatwith a good choice of mixture components andannealing schedule, we achieve improvements forthis task over mean-field variational inference.ReferencesM.
J. Beal and Z. Gharamani.
2003.
The variationalBayesian EM algorithm for incomplete data: with appli-cation to scoring graphical model structures.
In Proc.
ofBayesian Statistics.C.
Bishop, N. Lawrence, T. S. Jaakkola, and M. I. Jordan.1998.
Approximating posterior distributions in belief net-works using mixtures.
In Advances in NIPS.C.
M. Bishop.
2006.
Pattern Recognition and MachineLearning.
Springer.S.
Boyd and L. Vandenberghe.
2004.
Convex Optimization.Cambridge Press.S.
B. Cohen and N. A. Smith.
2009.
Variational inferencewith prior knowledge.
Technical report, Carnegie MellonUniversity.S.
B. Cohen, K. Gimpel, and N. A. Smith.
2008.
Logis-tic normal priors for unsupervised probabilistic grammarinduction.
In Advances in NIPS.J.
V. Grac?a, K. Ganchev, and B. Taskar.
2007.
Expectationmaximization and posterior constraints.
In Advances inNIPS.D.
Klein and C. D. Manning.
2004.
Corpus-based inductionof syntactic structure: Models of dependency and con-stituency.
In Proc.
of ACL.F.
C. N. Pereira and Y. Schabes.
1992.
Inside-outside reesti-mation from partially bracketed corpora.
In Proc.
of ACL.K.
Rose, E. Gurewitz, and G. C. Fox.
1990.
Statistical me-chanics and phrase transitions in clustering.
Physical Re-view Letters, 65(8):945?948.N.
A. Smith and J. Eisner.
2006.
Annealing structural biasin multilingual weighted grammar induction.
In Proc.
ofCOLING-ACL.N.
Xue, F. Xia, F.-D. Chiou, and M. Palmer.
2004.
The PennChinese Treebank: Phrase structure annotation of a largecorpus.
Natural Language Engineering, 10(4):1?30.4
