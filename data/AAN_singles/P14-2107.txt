Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 656?661,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsEnforcing Structural Diversity in Cube-pruned Dependency ParsingHao Zhang Ryan McDonaldGoogle, Inc.{haozhang,ryanmcd}@google.comAbstractIn this paper we extend the cube-pruneddependency parsing framework of Zhanget al (2012; 2013) by forcing inference tomaintain both label and structural ambigu-ity.
The resulting parser achieves state-of-the-art accuracies, in particular on datasetswith a large set of dependency labels.1 IntroductionDependency parsers assign a syntactic depen-dency tree to an input sentence (K?ubler et al,2009), as exemplified in Figure 1.
Graph-baseddependency parsers parameterize models directlyover substructures of the tree, including singlearcs (McDonald et al, 2005), sibling or grand-parent arcs (McDonald and Pereira, 2006; Car-reras, 2007) or higher-order substructures (Kooand Collins, 2010; Ma and Zhao, 2012).
As thescope of each feature function increases so doesparsing complexity, e.g., o(n5) for fourth-orderdependency parsing (Ma and Zhao, 2012).
Thishas led to work on approximate inference, typ-ically via pruning (Bergsma and Cherry, 2010;Rush and Petrov, 2012; He et al, 2013)Recently, it has been shown that cube-pruning(Chiang, 2007) can efficiently introduce higher-order dependencies in graph-based parsing (Zhangand McDonald, 2012).
Cube-pruned dependencyparsing runs standard bottom-up chart parsing us-ing the lower-order algorithms.
Similar to k-bestinference, each chart cell maintains a beam of k-best partial dependency structures.
Higher-orderfeatures are scored when combining beams duringinference.
Cube-pruning is an approximation, asthe highest scoring tree may fall out of the beambefore being fully scored with higher-order fea-tures.
However, Zhang et al (2013) observe state-of-the-art results when training accounts for errorsthat arise due to such approximations.John emailed April about one month agoNSUBJIOBJADVMODQUANTMODNUMNPADVMODFigure 1: A sample dependency parse.In this work we extend the cube-pruning frame-work of Zhang et al by observing that dependencyparsing has two fundamental sources of ambiguity.The first, structural ambiguity, pertains to confu-sions about the unlabeled structure of the tree, e.g.,the classic prepositional phrase attachment prob-lem.
The second, label ambiguity, pertains to sim-ple label confusions, e.g., whether a verbal objectis direct or indirect.Distinctions between arc labels are frequentlyfine-grained and easily confused by parsing mod-els.
For example, in the Stanford dependencylabel set (De Marneffe et al, 2006), the labelsTMOD (temporal modifier), NPADVMOD (noun-phrase adverbial modifier), IOBJ (indirect object)and DOBJ (direct object) can all be noun phrasesthat modify verbs to their right.
In the context ofcube-pruning, during inference, the system opts tomaintain a large amount of label ambiguity at theexpense of structural ambiguity.
Frequently, thebeam stores only label ambiguities and the result-ing set of trees have identical unlabeled structure.For example, in Figure 1, the aforementioned la-bel ambiguity around noun objects to the right ofthe verb (DOBJ vs. IOBJ vs. TMP) could lead oneor more of the structural ambiguities falling out ofthe beam, especially if the beam is small.To combat this, we introduce a secondary beamfor each unique unlabeled structure.
That is,we partition the primary (entire) beam into dis-joint groups according to the identity of unla-beled structure.
By limiting the size of the sec-ondary beam, we restrict label ambiguity and en-force structural diversity within the primary beam.The resulting parser consistently improves on thestate-of-the-art parser of Zhang et al (2013).
In656(a)l=l+l1(b)l=l1+l2Figure 2: Structures and rules for parsing with the(Eisner, 1996) algorithm.
Solid lines show onlythe construction of right-pointing first-order de-pendencies.
l is the predicted arc label.
Dashedlines are the additional sibling modifier signaturesin a generalized algorithm, specifically the previ-ous modifier in complete chart items.particular, data sets with large label sets (and thusa large number of label confusions) typically seethe largest jumps in accuracy.
Finally, we showthat the same result cannot be achieved by simplyincreasing the size of the beam, but requires ex-plicit enforcing of beam diversity.2 Structural Diversity in Cube-PruningOur starting point is the cube-pruned dependencyparsing model of Zhang and McDonald (2012).
Inthat work, as here, inference is simply the Eis-ner first-order parsing model (Eisner, 1996) shownin Figure 2.
In order to score higher-order fea-tures, each chart item maintains a list of signa-tures, which represent subtrees consistent with thechart item.
The stored signatures are the relevantportions of the subtrees that will be part of higher-order feature calculations.
For example, to scorefeatures over adjacent arcs, we might maintain ad-ditional signatures, again shown in Figure 2.The scope of the signature adds asymptoticcomplexity to parsing.
Even for second-order sib-lings, there will now be O(n) possible signaturesper chart item.
The result is that parsing com-plexity increases from O(n3) to O(n5).
Insteadof storing all signatures, Zhang and McDonald(2012) store the current k-best in a beam.
This re-sults in approximate inference, as some signaturesmay fall out of the beam before higher-order fea-tures can be scored.
This general trick is known ascube-pruning and is a common approach to deal-ing with large hypergraph search spaces in ma-chine translation (Chiang, 2007).Cube-pruned parsing is analogous to k-bestparsing algorithmically.
But there is a fundamen-tal difference.
In k-best parsing, if two subtreestaand tbbelong to the same chart item, with tal=0 : 1 : 2 :+l1l2l10 :l11 :l22 :l3l=0 : 1 : 2 :+l1l2l10 :l11 :l22 :l1Figure 3: Merging procedure in cube pruning.
Thebottom shows that enforcing diversity in the k-bestlists can give chance to a good structure at (2, 2).ranking higher than tb, then an extension of tathrough combing with a subtree tcfrom anotherchart item must also score higher than that of tb.This property is called the monotonicity property.Based on it, k-best parsing merges k-best subtreesin the following way: given two chart items withk-best lists to be combined, it proceeds on the twosorted lists monotonically from beginning to endto generate combinations.
Cube pruning followsthe merging procedure despite the loss of mono-tonicity due to the addition of higher-order featurefunctions over the signatures of the subtrees.
Theunderlying assumption of cube pruning is that thetrue k-best results are likely in the cross-productspace of top-ranked component subtrees.
Figure 3shows that the space is the top-left corner of thegrid in the binary branching cases.As mentioned earlier, the elements in chart itemk-best lists are feature signatures of subtrees.
Wemake a distinction between labeled signatures andunlabeled signatures.
As feature functions are de-fined on sub-graphs of the dependency trees, a fea-ture signature is labeled if and only if feature func-tions draw information from both the arcs in thesub-graph and the labels on the arcs.
Every la-beled signature projects to an unlabeled signature657that ignores the arc labels.The motivation for introducing unlabeled signa-tures for labeled parsing is to enforce structural di-versity.
Figure 3 illustrates the idea.
In the topdiagram, there is only one unlabeled signature inone of the two lists.
This is likely to happen whenthere is label ambiguity so that all three labels havesimilar scores.
In such cases, alternative tree struc-tures further down in the list that have the poten-tial to be scored higher when incorporating higher-order features, lose this opportunity due to prun-ing.
By contrast, if we introduce structural diver-sity by limiting the number of label variants, suchalternative structures can come out on top.More formally, when the feature signatures ofthe subtrees include arc labels, the cardinality ofthe set of all possible signatures grows by a poly-nomial of the size of the label set.
This factor has adiluting effect on the diversity of unlabeled signa-tures within the beam.
The larger the label set is,the greater the chance label ambiguity will dom-inate the beam.
Therefore, we introduce a sec-ond level of beam specifically for labeled signa-tures.
We call it the secondary beam, relative tothe primary beam, i.e., the entire beam.
The sec-ondary beam limits the number of labeled signa-tures for each unlabeled signature, a projection oflabeled signature, while the primary beam limitsthe total number of labeled signatures.
To illus-trate this, consider an original primary beam oflength b and a secondary beam length of sb.
Lettjirepresent the ithhighest scoring labeled variantof unlabeled structure j.
The table below shows aspecific example of beam configurations for b = 4for all possible values of sb.
The original beam isthe pathological case where all signatures have thesame unlabeled projection.
When sb= 1, all sig-natures in the beam now have a different unlabeledprojection.
When sb= 4, the beam reverts to theoriginal without any structural diversity.
Valuesbetween balance structural and label diversity.beam original b = 4 b = 4 b = 4 b = 4rank b=4 sb= 1 sb= 2 sb= 3 sb= 41 t11t11t11t11t112 t12t21t12t12t123 t13t31t21t13t134 t14t41t31t21t14?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
beam cut-off ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?5 t21.
.
.
.
.
.
.
.
.
.
.
.6 t31.
.
.
.
.
.
.
.
.
.
.
.7 t22.
.
.
.
.
.
.
.
.
.
.
.8 t32.
.
.
.
.
.
.
.
.
.
.
.9 t41.
.
.
.
.
.
.
.
.
.
.
.To achieve this in cube pruning, deeper explo-ration in the merging procedure becomes neces-sary.
In this example, originally the merging pro-cedure stops when t14has been explored.
Whensb= 1, the exploration needs to go further fromrank 4 to 9.
When sb= 2, it needs to go from 4to 6.
When sb= 3, only one more step to rank5 is necessary.
The amount of additional compu-tation depends on the value of sb, the composi-tion of the incoming k-best lists, and the featurefunctions which determine feature signatures.
Toaccount for this we also compare to baselines sys-tems that simply increase the size of the beam to acomparable run-time.In our experiments we found that sb= b/2 istypically a good choice.
As in most parsing sys-tems, beams are applied consistently during learn-ing and testing because feature weights will be ad-justed according to the diversity of the beam.3 ExperimentsWe use the cube-pruned dependency parser ofZhang et al (2013) as our baseline system.
Tomake an apples-to-apples comparison, we use thesame online learning algorithm and the same fea-ture templates.
The feature templates include first-to-third-order labeled features and valency fea-tures.
More details of these features are describedin Zhang and McDonald (2012).
For online learn-ing, we apply the same violation-fixing strategy(so-called single-node max-violation) on MIRAand run 8 epochs of training for all experiments.For English, we conduct experiments onthe commonly-used constituency-to-dependency-converted Penn Treebank data sets.
The first one,Penn-YM, was created by the Penn2Malt1soft-ware.
The second one, Penn-S-2.0.5, used theStanford dependency framework (De Marneffe etal., 2006) by applying version 2.0.5 of the Stan-ford parser.
The third one, Penn-S-3.3.0 was con-verted by version 3.3.0 of the Stanford parser.
Thetrain/dev/test split was standard: sections 2-21 fortraining; 22 for validation; and 23 for evaluation.Automatic POS tags for Penn-YM and Penn-S-2.0.5 are provided by TurboTagger (Martins et al,2013) with an accuracy of 97.3% on section 23.For Chinese, we use the CTB-5 dependency tree-bank which was converted from the original con-stituent treebank by Zhang and Nivre (2011) anduse gold-standard POS tags as is standard.1http://stp.lingfil.uu.se/?nivre/research/Penn2Malt.html658Berkeley Parser TurboParser Cube-pruned w/o diversity Cube-pruned w/ diversityUAS LAS UAS LAS UAS LAS UAS LASPENN-YM - - 93.07 - 93.50 92.41 93.57 92.48PENN-S-2.0.5 - - 92.82 - 93.59 91.17 93.71 91.37PENN-S-3.3.0 93.31 91.01 92.20 89.67 92.91 90.52 93.01 90.64PENN-S-3.3.0-GOLD 93.65 92.05 93.56 91.99 94.32 92.90 94.40 93.02CTB-5 - - - - 87.78 86.13 87.96 86.34Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement ofstructural diversity.
PENN-S and CTB-5 are significant at p < 0.05.
Penn-S-2.0.5 TurboParser result isfrom Martins et al (2013).
Following Kong and Smith (2014), we trained our models on Penn-S-3.3.0with gold POS tags and evaluated with both non-gold (Stanford tagger) and gold tags.Table 1 shows the main results of the paper.Both the baseline and the new system keep a beamof size 6 for each chart cell.
The difference isthat the new system enforces structural diversitywith the introduction of a secondary beam for la-bel variants.
We choose the secondary beam thatyields the highest LAS on the development datasets for Penn-YM, Penn-S-2.0.5 and CTB-5.
In-deed we observe larger improvements for the datasets with larger label sets.
Penn-S-2.0.5 has 49 la-bels and observes a 0.2% absolute improvement inLAS.
Although CTB-5 has a small label set (18),we do see similar improvements for both UAS andLAS.
There is a slight improvement for Penn-YMdespite the fact that Penn-YM has the most com-pact label set (12).
These results are the highestknown in the literature.
For the Penn-S-3.3.0 re-sults we can see that our model outperforms Tur-boPaser and is competitive with the Berkeley con-stituency parser (Petrov et al, 2006).
In particu-lar, if gold tags are assumed, cube-pruning signif-icantly outperforms Berkeley.
This suggests thatjoint tagging and parsing should improve perfor-mance further in the non-gold tag setting, as thatis a differentiating characteristic of constituencyparsers.
Table 2 shows the results on the CoNLL2006/2007 data sets (Buchholz and Marsi, 2006;Nivre et al, 2007).
For simplicity, we set the sec-ondary beam to 3 for all.
We can see that over-all there is an improvement in accuracy and this ishighly correlated with the size of the label set.In order to examine the importance of balancingstructural diversity and labeled diversity, we let thesize of the secondary beam vary from one to thesize of the primary beam.
In Table 3, we show theresults of all combinations of beam settings of pri-mary beam sizes 4 and 6 for three data sets: Penn-YM, Penn-S-2.0.5, and CTB-5 respectively.
In thetable, we highlight the best results for each beamsize and data set on the development data.
For 5of the total of 6 comparison groups ?
three lan-w/o diversity w/ diversityLanguage(labels) UAS LAS UAS LASCZECH(82) 88.36 82.16 88.36 82.02SWEDISH(64) 91.62 85.08 91.85 85.26PORTUGUESE(55) 92.07 88.30 92.23 88.50DANISH(53) 91.88 86.95 91.78 86.93HUNGARIAN(49) 85.85 81.02 86.55 81.79GREEK(46) 86.14 78.20 86.21 78.45GERMAN(46) 92.03 89.44 92.01 89.52CATALAN(42) 94.58 89.05 94.91 89.54BASQUE(35) 79.59 71.52 80.14 71.94ARABIC(27) 80.48 69.68 80.56 69.98TURKISH(26) 76.94 66.80 77.14 67.00SLOVENE(26) 86.01 77.14 86.27 77.44DUTCH(26) 83.57 80.29 83.39 80.19ITALIAN(22) 87.57 83.22 87.38 82.95SPANISH(21) 87.96 84.95 87.98 84.79BULGARIAN(19) 94.02 89.87 93.88 89.63JAPANESE(8) 93.26 91.67 93.16 91.51AVG 87.76 82.08 87.87 82.20Table 2: Results for languages from CoNLL2006/2007 shared tasks.
When a language is inboth years, the 2006 set is used.
Languages aresorted by the number of unique arc labels.guages times two primary beams ?
the best resultis obtained by choosing a secondary beam size thatis close to one half the size of the primary beam.Contrasting Table 1 and Table 3, the accuracy im-provements are consistent across the developmentset and the test set for all three data sets.A reasonable question is whether such improve-ments could be obtained by simply enlarging thebeam in the baseline parser.
The bottom row ofTable 3 shows the parsing results for the three datasets when the beam is enlarged to 16.
On Penn-S-2.0.5, the baseline with beam 16 is at roughlythe same speed as the highlighted best system withprimary beam 6 and secondary beam 3.
On CTB-5, the beam 16 baseline is 30% slower.
Table 3indicates that simply enlarging the beam ?
rela-tive to parsing speed ?
does not recover the winsof structural diversity on Penn-S-2.0.5 and CTB-5,though it does reduce the gap on Penn-S-2.0.5.
OnPenn-YM, the beam 16 baseline is slightly betterthan the new system, but 90% slower.659primary secondary PENN-YM PENN-S-2.0.5 CTB-5beam beam UAS LAS UAS LAS UAS LAS41 93.67 92.64 93.65 91.04 87.53 85.852 93.79 92.68 93.77 91.30 87.62 85.963 93.80 92.66 93.69 91.23 87.48 85.914 93.75 92.63 93.62 91.11 87.68 86.0861 93.65 92.46 93.76 91.15 87.72 86.052 93.80 92.69 93.80 91.35 87.61 85.963 93.75 92.64 93.99 91.55 87.80 86.184 93.82 92.74 93.84 91.40 87.91 86.285 93.82 92.71 93.71 91.26 87.75 86.126 93.74 92.61 93.70 91.21 87.66 86.0516 16 93.87 92.75 93.77 91.35 87.59 85.86Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, withdifferent primary beams.
When the size of the secondary beam is equal to the primary beam, the parserdegenerates to not enforcing structural diversity.
In the opposite, when the secondary beam is smaller,there is more structural diversity and less label diversity.
Results are on development sets.To better understand the behaviour of structuraldiversity pruning relative to increasing the beam,we looked at the unlabeled attachment F-score perdependency label in the Penn-S-2.0.5 developmentset2.
Table 4 shows the 10 labels with the largestincrease in attachment scores for structural diver-sity pruning relative to standard pruning.
Impor-tantly, the biggest wins are primarily for labels inwhich unlabeled attachment is lower than average(93.99, 8 out of 10).
Thus, diversity pruning getsmost of its wins on difficult attachment decisions.Indeed, many of the relations represent clausaldependencies that are frequently structurally am-biguous.
There are also cases of relatively shortdependencies that can be difficult to attach.
Forinstance, quantmod dependencies are typically ad-verbs occurring after verbs that modify quantitiesto their right.
But these can be confused as ad-verbial modifiers of the verb to the left.
These re-sults support our hypothesis that label ambiguityis causing hard attachment decisions to be prunedand that structural diversity can ameliorate this.4 DiscussionKeeping multiple beams in approximate searchhas been explored in the past.
In machine transla-tion, multiple beams are used to prune translationhypotheses at different levels of granularity (Zensand Ney, 2008).
However, the focus is improvingthe speed of translation decoder rather than im-proving translation quality through enforcementof hypothesis diversity.
In parsing, Bohnet andNivre (2012) and Bohnet et al (2013) propose amodel for joint morphological analysis, part-of-speech tagging and dependency parsing using a2Using eval.pl from Buchholz and Marsi (2006).w/o diversity w/ diversityLabel large beam small beam diffquantmod 86.65 88.06 1.41partmod 83.63 85.02 1.39xcomp 87.76 88.74 0.98tmod 89.75 90.72 0.97appos 88.89 89.84 0.95nsubjpass 92.53 93.31 0.78complm 94.50 95.15 0.64advcl 81.10 81.74 0.63ccomp 82.64 83.17 0.54number 96.86 97.39 0.53Table 4: Unlabeled attachment F-score per de-pendency relation.
The top 10 score increasesfor structural diversity pruning (beam 6 and la-bel beam of 3) over basic pruning (beam 16) areshown.
Only labels with more than 100 instancesin the development data are considered.left-to-right beam.
With a single beam, token levelambiguities (morphology and tags) dominate anddependency level ambiguity is suppressed.
This isaddressed by essentially keeping two beams.
Thefirst forces every tree to be different at the depen-dency level and the second stores the remaininghighest scoring options, which can include outputsthat differ only at the token level.The present work looks at beam diversity ingraph-based dependency parsing, in particular la-bel versus structural diversity.
It was shown thatby keeping a diverse beam significant improve-ments could be achieved on standard benchmarks,in particular with respect to difficult attachmentdecisions.
It is worth pointing out that otherdependency parsing frameworks (e.g., transition-based parsing (Zhang and Clark, 2008; Zhang andNivre, 2011)) could also benefit from modelingstructural diversity in search.660ReferencesS.
Bergsma and C. Cherry.
2010.
Fast and accurate arcfiltering for dependency parsing.
In Proc.
of COL-ING.B.
Bohnet and J. Nivre.
2012.
A transition-basedsystem for joint part-of-speech tagging and labelednon-projective dependency parsing.
In Proc.
ofEMNLP/CoNLL.B.
Bohnet, J. Nivre, I. Boguslavsky, F. Ginter, Rich?ardF., and J. Hajic.
2013.
Joint morphological and syn-tactic analysis for richly inflected languages.
TACL,1.S.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proc.of CoNLL.X.
Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In Proc.
of the CoNLLShared Task Session of EMNLP-CoNLL.D.
Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2).M.
De Marneffe, B. MacCartney, and C.D.
Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In Proc.
of LREC.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: an exploration.
In Proc.
of COL-ING.H.
He, H. Daum?e III, and J. Eisner.
2013.
Dynamicfeature selection for dependency parsing.
In Proc.of EMNLP.L.
Kong and N. A. Smith.
2014.
An empirical compar-ison of parsing methods for stanford dependencies.In ArXiv:1404.4314.T.
Koo and M. Collins.
2010.
Efficient third-order de-pendency parsers.
In Proc.
of ACL.S.
K?ubler, R. McDonald, and J. Nivre.
2009.
Depen-dency parsing.
Morgan & Claypool Publishers.X.
Ma and H. Zhao.
2012.
Fourth-order dependencyparsing.
In Proc.
of COLING.A.
F. T. Martins, M. B. Almeida, and N. A. Smith.2013.
Turning on the turbo: Fast third-order non-projective turbo parsers.
In Proc.
of ACL.R.
McDonald and F. Pereira.
2006.
Online learningof approximate dependency parsing algorithms.
InProc.
of EACL.R.
McDonald, K. Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
InProc.
of ACL.J.
Nivre, J.
Hall, S. K?ubler, R. McDonald, J. Nils-son, S. Riedel, and D. Yuret.
2007.
The CoNLL2007 shared task on dependency parsing.
In Proc.of EMNLP-CoNLL.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable treeannotation.
In Proc.
of ACL.A.
Rush and S. Petrov.
2012.
Efficient multi-pass de-pendency pruning with vine parsing.
In Proc.
ofNAACL.R.
Zens and H. Ney.
2008.
Improvements in dynamicprogramming beam search for phrase-based statisti-cal machine translation.
In Proc.
IWSLT.Y.
Zhang and S. Clark.
2008.
A Tale of TwoParsers: Investigating and Combining Graph-basedand Transition-based Dependency Parsing.
In Proc.of EMNLP.H.
Zhang and R. McDonald.
2012.
Generalizedhigher-order dependency parsing with cube pruning.In Proc.
of EMNLP.Y.
Zhang and J. Nivre.
2011.
Transition-based depen-dency parsing with rich non-local features.
In Proc.of ACL-HLT, volume 2.H.
Zhang, L. Huang, K.Zhao, and R. McDonald.
2013.Online learning for inexact hypergraph search.
InProc.
of EMNLP.661
