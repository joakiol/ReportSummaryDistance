Transactions of the Association for Computational Linguistics, 1 (2013) 1?12.
Action Editor: Sharon Goldwater.Submitted 11/2012; Revised 1/2013; Published 3/2013.
c?2013 Association for Computational Linguistics.Token and Type Constraints for Cross-Lingual Part-of-Speech TaggingOscar Ta?ckstro?m??
Dipanjan Das?
Slav Petrov?
Ryan McDonald?
Joakim Nivre?
? Swedish Institute of Computer Science?Department of Linguistics and Philology, Uppsala University?Google Research, New Yorkoscar@sics.se{dipanjand|slav|ryanmcd}@google.comjoakim.nivre@lingfil.uu.seAbstractWe consider the construction of part-of-speechtaggers for resource-poor languages.
Recently,manually constructed tag dictionaries fromWiktionary and dictionaries projected via bitexthave been used as type constraints to overcomethe scarcity of annotated data in this setting.In this paper, we show that additional tokenconstraints can be projected from a resource-rich source language to a resource-poor targetlanguage via word-aligned bitext.
We presentseveral models to this end; in particular a par-tially observed conditional random field model,where coupled token and type constraints pro-vide a partial signal for training.
Averagedacross eight previously studied Indo-Europeanlanguages, our model achieves a 25% relativeerror reduction over the prior state of the art.We further present successful results on sevenadditional languages from different families,empirically demonstrating the applicability ofcoupled token and type constraints across adiverse set of languages.1 IntroductionSupervised part-of-speech (POS) taggers are avail-able for more than twenty languages and achieve ac-curacies of around 95% on in-domain data (Petrov etal., 2012).
Thanks to their efficiency and robustness,supervised taggers are routinely employed in manynatural language processing applications, such as syn-tactic and semantic parsing, named-entity recognitionand machine translation.
Unfortunately, the resourcesrequired to train supervised taggers are expensive tocreate and unlikely to exist for the majority of written?Work primarily carried out while at Google Research.languages.
The necessity of building NLP tools forthese resource-poor languages has been part of themotivation for research on unsupervised learning ofPOS taggers (Christodoulopoulos et al 2010).In this paper, we instead take a weakly supervisedapproach towards this problem.
Recently, learningPOS taggers with type-level tag dictionary constraintshas gained popularity.
Tag dictionaries, noisily pro-jected via word-aligned bitext, have bridged the gapbetween purely unsupervised and fully supervisedtaggers, resulting in an average accuracy of over 83%on a benchmark of eight Indo-European languages(Das and Petrov, 2011).
Li et al(2012) further im-proved upon this result by employing Wiktionary1 asa tag dictionary source, resulting in the hitherto bestpublished result of almost 85% on the same setup.Although the aforementioned weakly supervisedapproaches have resulted in significant improvementsover fully unsupervised approaches, they have notexploited the benefits of token-level cross-lingualprojection methods, which are possible with word-aligned bitext between a target language of interestand a resource-rich source language, such as English.This is the setting we consider in this paper (?2).While prior work has successfully considered bothtoken- and type-level projection across word-alignedbitext for estimating the model parameters of genera-tive tagging models (Yarowsky and Ngai, 2001; Xiand Hwa, 2005, inter alia), a key observation under-lying the present work is that token- and type-levelinformation offer different and complementary sig-nals.
On the one hand, high confidence token-levelprojections offer precise constraints on a tag in aparticular context.
On the other hand, manually cre-1http://www.wiktionary.org/.1ated type-level dictionaries can have broad coverageand do not suffer from word-alignment errors; theycan therefore be used to filter systematic as well asrandom noise in token-level projections.In order to reap these potential benefits, we pro-pose a partially observed conditional random field(CRF) model (Lafferty et al 2001) that couples to-ken and type constraints in order to guide learning(?3).
In essence, the model is given the freedom topush probability mass towards hypotheses consistentwith both types of information.
This approach is flex-ible: we can use either noisy projected or manuallyconstructed dictionaries to generate type constraints;furthermore, we can incorporate arbitrary featuresover the input.
In addition to standard (contextual)lexical features and transition features, we observethat adding features from a monolingual word cluster-ing (Uszkoreit and Brants, 2008) can significantly im-prove accuracy.
While most of these features can alsobe used in a generative feature-based hidden Markovmodel (HMM) (Berg-Kirkpatrick et al 2010), weachieve the best accuracy with a globally normalizeddiscriminative CRF model.To evaluate our approach, we present extensiveresults on standard publicly available datasets for 15languages: the eight Indo-European languages pre-viously studied in this context by Das and Petrov(2011) and Li et al(2012), and seven additional lan-guages from different families, for which no compa-rable study exists.
In ?4 we compare various features,constraints and model types.
Our best model usestype constraints derived from Wiktionary, togetherwith token constraints derived from high-confidenceword alignments.
When averaged across the eightlanguages studied by Das and Petrov (2011) and Liet al(2012), we achieve an accuracy of 88.8%.
Thisis a 25% relative error reduction over the previousstate of the art.
Averaged across all 15 languages,our model obtains an accuracy of 84.5% compared to78.5% obtained by a strong generative baseline.
Fi-nally, we provide an in depth analysis of the relativecontributions of the two types of constraints in ?5.2 Coupling Token and Type ConstraintsType-level information has been amply used inweakly supervised POS induction, either via puremanually crafted tag dictionaries (Smith and Eisner,2005; Ravi and Knight, 2009; Garrette and Baldridge,2012), noisily projected tag dictionaries (Das andPetrov, 2011) or through crowdsourced lexica, suchas Wiktionary (Li et al 2012).
At the other endof the spectrum, there have been efforts that projecttoken-level information across word-aligned bitext(Yarowsky and Ngai, 2001; Xi and Hwa, 2005).
How-ever, systems that combine both sources of informa-tion in a single model have yet to be fully explored.The following three subsections outline our overallapproach for coupling these two types of informationto build robust POS taggers that do not require anydirect supervision in the target language.2.1 Token ConstraintsFor the majority of resource-poor languages, thereis at least some bitext with a resource-rich sourcelanguage; for simplicity, we choose English as oursource language in all experiments.
It is then nat-ural to consider using a supervised part-of-speechtagger to predict part-of-speech tags for the Englishside of the bitext.
These predicted tags can subse-quently be projected to the target side via automaticword alignments.
This approach was pioneered byYarowsky and Ngai (2001), who used the resultingpartial target annotation to estimate the parametersof an HMM.
However, due to the automatic natureof the word alignments and the POS tags, there willbe significant noise in the projected tags.
To conquerthis noise, they used very aggressive smoothing tech-niques when training the HMM.
Fossum and Abney(2005) used similar token-level projections, but in-stead combined projections from multiple source lan-guages to filter out random projection noise as wellas the systematic noise arising from different sourcelanguage annotations and syntactic divergences.2.2 Type ConstraintsIt is well known that given a tag dictionary, even ifit is incomplete, it is possible to learn accurate POStaggers (Smith and Eisner, 2005; Goldberg et al2008; Ravi and Knight, 2009; Naseem et al 2009).While widely differing in the specific model struc-ture and learning objective, all of these approachesachieve excellent results.
Unfortunately, they relyon tag dictionaries extracted directly from the un-derlying treebank data.
Such dictionaries provide indepth coverage of the test domain and also list all2	       	  	 	  Figure 1: Lattice representation of the inference search space Y(x) for an authentic sentence in Swedish (?The farmingproducts must be pure and must not contain any additives?
), after pruning with Wiktionary type constraints.
Thecorrect parts of speech are listed underneath each word.
Bold nodes show projected token constraints y?.
Underlinedtext indicates incorrect tags.
The coupled constraints lattice Y?
(x, y?)
consists of the bold nodes together with nodes forwords that are lacking token constraints; in this case, the coupled constraints lattice thus defines exactly one valid path.inflected forms ?
both of which are difficult to obtainand unrealistic to expect for resource-poor languages.In contrast, Das and Petrov (2011) automaticallycreate type-level tag dictionaries by aggregating overprojected token-level information extracted from bi-text.
To handle the noise in these automatic dictionar-ies, they use label propagation on a similarity graphto smooth (and also expand) the label distributions.While their approach produces good results and isapplicable to resource-poor languages, it requires acomplex multi-stage training procedure including theconstruction of a large distributional similarity graph.Recently, Li et al(2012) presented a simple andviable alternative: crowdsourced dictionaries fromWiktionary.
While noisy and sparse in nature, Wik-tionary dictionaries are available for 170 languages.2Furthermore, their quality and coverage is growingcontinuously (Li et al 2012).
By incorporating typeconstraints from Wiktionary into the feature-basedHMM of Berg-Kirkpatrick et al(2010), Li et alwereable to obtain the best published results in this setting,surpassing the results of Das and Petrov (2011) oneight Indo-European languages.2.3 Coupled ConstraintsRather than relying exclusively on either token ortype constraints, we propose to complement the onewith the other during training.
For each sentence inour training set, a partially constrained lattice of tagsequences is constructed as follows:2http://meta.wikimedia.org/wiki/Wiktionary ?
October 2012.1.
For each token whose type is not in the tag dic-tionary, we allow the entire tag set.2.
For each token whose type is in the tag dictio-nary, we prune all tags not licensed by the dictio-nary and mark the token as dictionary-pruned.3.
For each token that has a tag projected via ahigh-confidence bidirectional word alignment:if the projected tag is still present in the lattice,then we prune every tag but the projected tag forthat token; if the projected tag is not present inthe lattice, which can only happen for dictionary-pruned tokens, then we ignore the projected tag.Figure 1 provides a running example.
The latticeshows tags permitted after constraining the wordsto tags licensed by the dictionary (up until Step 2from above).
There is only a single token ?Jordbruk-sprodukterna?
(?the farming products?)
not in thedictionary; in this case the lattice permits the fullset of tags.
With token-level projections (Step 3;nodes with bold border in Figure 1), the lattice canbe further pruned.
In most cases, the projected tagis both correct and is in the dictionary-pruned lattice.We thus successfully disambiguate such tokens andshrink the search space substantially.There are two cases we highlight in order to showwhere our model can break.
First, for the token?Jordbruksprodukterna?, the erroneously projectedtag ADJ will eliminate all other tags from the lattice,including the correct tag NOUN.
Second, the token?na?gra?
(?any?)
has a single dictionary entry PRONand is missing the correct tag DET.
In the case where3DET is the projected tag, we will not add it to thelattice and simply ignore it.
This is because we hy-pothesize that the tag dictionary can be trusted morethan the tags projected via noisy word alignments.
Aswe will see in ?4, taking the union of tags performsworse, which supports this hypothesis.For generative models, such as HMMs (?3.1), weneed to define only one lattice.
For our best gen-erative model this is the coupled token- and type-constrained lattice.3 At prediction time, in both thediscriminative and the generative cases, we find themost likely label sequence using Viterbi decoding.For discriminative models, such as CRFs (?3.2),we need to define two lattices: one that the modelmoves probability mass towards and another onedefining the overall search space (or partition func-tion).
In traditional supervised learning without adictionary, the former is a trivial lattice containingthe gold standard tag sequence and the latter is theset of all possible tag sequences spanning the tokens.With our best model, we will move mass towards thecoupled token- and type-constrained lattice, such thatthe model can freely distribute mass across all pathsconsistent with these constraints.
The lattice definingthe partition function will be the full set of possibletag sequences when no dictionary is used; when adictionary is used it will consist of all dictionary-pruned tag sequences (sans Step 3 above; the full setof possibilities shown in Figure 1 for our runningexample).Figures 2 and 3 provide statistics regarding thesupervision coverage and remaining ambiguity.
Fig-ure 2 shows that more than two thirds of all tokens inour training data are in Wiktionary.
However, there isconsiderable variation between languages: Spanishhas the highest coverage with over 90%, while Turk-ish, an agglutinative language with a vast numberof word forms, has less than 50% coverage.
Fig-ure 3 shows that there is substantial uncertainty leftafter pruning with Wiktionary, since tokens are rarelyfully disambiguated: 1.3 tags per token are allowedon average for types in Wiktionary.Figure 2 further shows that high-confidence align-ments are available for about half of the tokens formost languages (Japanese is a notable exception with3Other training methods exist as well, for example, con-trastive estimation (Smith and Eisner, 2005).0255075100avg bg cs da de el es fr it ja nl pt sl sv tr zhPercent of tokenscoveredTokencoverage Wiktionary Projected Projected+FilteredFigure 2: Wiktionary and projection dictionary coverage.Shown is the percentage of tokens in the target side of thebitext that are covered by Wiktionary, that have a projectedtag, and that have a projected tag after intersecting the two.0.00.51.01.5avg bg cs da de el es fr it ja nl pt sl sv tr zhNumberoftags pertokenFigure 3: Average number of licensed tags per token onthe target side of the bitext, for types in Wiktionary.less than 30% of the tokens covered).
Intersecting theWiktionary tags and the projected tags (Step 2 and 3above) filters out some of the potentially erroneoustags, but preserves the majority of the projected tags;the remaining, presumably more accurate projectedtags cover almost half of all tokens, greatly reducingthe search space that the learner needs to explore.3 Models with Coupled ConstraintsWe now formally present how we couple token andtype constraints and how we use these coupled con-straints to train probabilistic tagging models.
Letx = (x1x2 .
.
.
x|x|) ?
X denote a sentence, whereeach token xi ?
V is an instance of a word type fromthe vocabulary V and let y = (y1y2 .
.
.
y|x|) ?
Y de-note a tag sequence, where yi ?
T is the tag assignedto token xi and T denotes the set of all possible part-of-speech tags.
We denote the lattice of all admissibletag sequences for the sentence x by Y(x).
This is the4inference search space in which the tagger operates.As we shall see, it is crucial to constrain the size ofthis lattice in order to simplify learning when onlyincomplete supervision is available.A tag dictionary maps a word type xj ?
V toa set of admissible tags T (xj) ?
T .
For wordtypes not in the dictionary we allow the full set oftags T (while possible, in this paper we do not at-tempt to distinguish closed-class versus open-classwords).
When provided with a tag dictionary, thelattice of admissible tag sequences for a sentence xis Y(x) = T (x1) ?
T (x2) ?
.
.
.
?
T (x|x|).
Whenno tag dictionary is available, we simply have the fulllattice Y(x) = T |x|.Let y?
= (y?1y?2 .
.
.
y?|x|) be the projected tags forthe sentence x.
Note that {y?i} = ?
for tokens withouta projected tag.
Next, we define a piecewise operator_ that couples y?
and Y(x) with respect to everysentence index, which results in a token- and type-constrained lattice.
The operator behaves as follows,coherent with the high level description in ?2.3:T?
(xi, y?i) = y?i _ T (xi) ={{y?i} if y?i ?
T (xi)T (xi) otherwise .We denote the token- and type-constrained lattice asY?
(x, y?)
= T?
(x1, y?1)?T?
(x2, y?2)?.
.
.?T?
(x|x|, y?|x|).Note that when token-level projections are not used,the dictionary-pruned lattice and the lattice with cou-pled constraints are identical, that is Y?
(x, y?)
= Y(x).3.1 HMMs with Coupled ConstraintsA first-order hidden Markov model (HMM) specifiesthe joint distribution of a sentence x ?
X and atag-sequence y ?
Y(x) as:p?
(x, y) =|x|?i=1p?
(xi | yi)?
??
?emissionp?
(yi | yi?1)?
??
?transition.We follow the recent trend of using a log-linearparametrization of the emission and the transitiondistributions, instead of a multinomial parametriza-tion (Chen, 2003).
This allows model parameters ?to be shared across categorical events, which hasbeen shown to give superior performance (Berg-Kirkpatrick et al 2010).
The categorical emissionand transition events are represented by feature vec-tors ?
(xi, yi) and ?
(yi, yi?1).
Each element of theparameter vector ?
corresponds to a particular fea-ture; the component log-linear distributions are:p?
(xi | yi) =exp(?>?
(xi, yi))?x?i?V exp (?>?
(x?i, yi)),andp?
(yi | yi?1) =exp(?>?
(yi, yi?1))?y?i?T exp (?>?
(y?i, yi?1)).In maximum-likelihood estimation of the parameters,we seek to maximize the likelihood of the observedparts of the data.
For this we need the joint marginaldistribution p?
(x, Y?
(x, y?))
of a sentence x, and itscoupled constraints lattice Y?
(x, y?
), which is obtainedby marginalizing over all consistent outputs:p?
(x, Y?
(x, y?))
=?y?Y?(x,y?)p?
(x, y) .If there are no projections and no tag dictionary, thenY?
(x, y?)
= T |x|, and thus p?
(x, Y?
(x, y?))
= p?
(x),which reduces to fully unsupervised learning.
The`2-regularized marginal joint log-likelihood of theconstrained training data D = {(x(i), y?
(i))}ni=1 is:L(?
;D) =n?i=1log p?
(x(i), Y?
(x(i), y?(i)))??
??
?22 .
(1)We follow Berg-Kirkpatrick et al(2010) and take adirect gradient approach for optimizing Eq.
1 withL-BFGS (Liu and Nocedal, 1989).
We set ?
= 1 andrun 100 iterations of L-BFGS.
One could also em-ploy the Expectation-Maximization (EM) algorithm(Dempster et al 1977) to optimize this objective, al-though the relative merits of EM versus direct gradi-ent training for these models is still a topic of debate(Berg-Kirkpatrick et al 2010; Li et al 2012).4 Notethat since the marginal likelihood is non-concave, weare only guaranteed to find a local maximum of Eq.
1.After estimating the model parameters ?, the tag-sequence y?
?
Y(x) for a sentence x ?
X is pre-dicted by choosing the one with maximal joint prob-ability:y?
?
arg maxy?Y(x)p?
(x, y) .4We trained the HMM with EM as well, but achieved betterresults with direct gradient training and hence omit those results.53.2 CRFs with Coupled ConstraintsWhereas an HMM models the joint probability ofthe input x ?
X and output y ?
Y(x), using locallynormalized component distributions, a conditionalrandom field (CRF) instead models the probability ofthe output conditioned on the input as a globally nor-malized log-linear distribution (Lafferty et al 2001):p?
(y | x) =exp(?>?
(x, y))?y?
?Y(x) exp (?>?
(x, y?
)),where ?
is a parameter vector.
As for the HMM,Y(x) is not necessarily the full space of possibletag-sequences; specifically, for us, it is the dictionary-pruned lattice without the token constraints.With a first-order Markov assumption, the featurefunction factors as:?
(x, y) =|x|?i=1?
(x, yi, yi?1) .This model is more powerful than the HMM in thatit can use richer feature definitions, such as joint in-put/transition features and features over a wider inputcontext.
We model a marginal conditional probabil-ity, given by the total probability of all tag sequencesconsistent with the lattice Y?
(x, y?):p?(Y?
(x, y?)
| x) =?y?Y?(x,y?)p?
(y | x) .The parameters of this constrained CRF are estimatedby maximizing the `2-regularized marginal condi-tional log-likelihood of the constrained data (Riezleret al 2002):L(?
;D) =n?i=1log p?(Y?
(x(i), y?
(i)) | x(i))?
???
?22 .
(2)As with Eq.
1, we maximize Eq.
2 with 100 itera-tions of L-BFGS and set ?
= 1.
In contrast to theHMM, after estimating the model parameters ?, thetag-sequence y?
?
Y(x) for a sentence x ?
X ischosen as the sequence with the maximal conditionalprobability:y?
?
arg maxy?Y(x)p?
(y | x) .4 Empirical StudyWe now present a detailed empirical study of the mod-els proposed in the previous sections.
In addition tocomparing with the state of the art in Das and Petrov(2011) and Li et al(2012), we present models withseveral combinations of token and type constraints,additional features incorporating word clusters.
Bothgenerative and discriminative models are explored.4.1 Experimental SetupBefore delving into the experimental details, wepresent our setup and datasets.Languages.
We evaluate on eight target languagesused in previous work (Das and Petrov, 2011; Li etal., 2012) and on seven additional languages (see Ta-ble 1).
While the former eight languages all belong tothe Indo-European family, we broaden the coverageto language families more distant from the sourcelanguage (for example, Chinese, Japanese and Turk-ish).
We use the treebanks from the CoNLL sharedtasks on dependency parsing (Buchholz and Marsi,2006; Nivre et al 2007) for evaluation.5 The two-letter abbreviations from the ISO 639-1 standard areused when referring to these languages in tables andfigures.Tagset.
In all cases, we map the language-specificPOS tags to universal POS tags using the mappingof Petrov et al(2012).6 Since we use indirect super-vision via projected tags or Wiktionary, the modelstates induced by all models correspond directly toPOS tags, enabling us to compute tagging accuracywithout a greedy 1-to-1 or many-to-1 mapping.Bitext.
For all experiments, we use English as thesource language.
Depending on availability, thereare between 1M and 5M parallel sentences for eachlanguage.
The majority of the parallel data is gath-ered automatically from the web using the methodof Uszkoreit et al(2010).
We further include datafrom Europarl (Koehn, 2005) and from the UN par-allel corpus (UN, 2006), for languages covered bythese corpora.
The English side of the bitext isPOS tagged with a standard supervised CRF tagger,trained on the Penn Treebank (Marcus et al 1993),with tags mapped to universal tags.
The parallel sen-5For French we use the treebank of Abeille?
et al(2003).6We use version 1.03 of the mappings available at http://code.google.com/p/universal-pos-tags/.6tences are word aligned with the aligner of DeNeroand Macherey (2011).
Intersected high-confidencealignments (confidence >0.95) are extracted and ag-gregated into projected type-level dictionaries.
Forpurely practical reasons, the training data with token-level projections is created by randomly samplingtarget-side sentences with a total of 500K tokens.Wiktionary.
We use a snapshot of the Wiktionaryword definitions, and follow the heuristics of Li etal.
(2012) for creating the Wiktionary dictionary bymapping the Wiktionary tags to universal POS tags.7Features.
For all models, we use only an identityfeature for tag-pair transitions.
We use five featuresthat couple the current tag and the observed word(analogous to the emission in an HMM): word iden-tity, suffixes of up to length 3, and three indicatorfeatures that fire when the word starts with a capitalletter, contains a hyphen or contains a digit.
These arethe same features as those used by Das and Petrov(2011).
Finally, for some models we add a wordcluster feature that couples the current tag and theword cluster identity of the word.
These (monolin-gual) word clusters are induced with the exchangealgorithm (Uszkoreit and Brants, 2008).
We set thenumber of clusters to 256 across all languages, as thishas previously been shown to produce robust resultsfor similar tasks (Turian et al 2010; Ta?ckstro?m etal., 2012).
The clusters for each language are learnedon a large monolingual newswire corpus.4.2 Models with Type ConstraintsTo examine the sole effect of type constraints, weexperiment with the HMM, drawing constraints fromthree different dictionaries.
Table 1 compares the per-formance of our models with the best results of Dasand Petrov (2011, D&P) and Li et al(2012, LG&T).As in previous work, training is done exclusively onthe training portion of each treebank, stripped of anymanual linguistic annotation.We first use all of our parallel data to generateprojected tag dictionaries: the English POS tags areprojected across word alignments and aggregated totag distributions for each word type.
As in Das andPetrov (2011), the distributions are then filtered witha threshold of 0.2 to remove noisy tags and to create7The definitions were downloaded on August 31, 2012 fromhttp://toolserver.org/?enwikt/definitions/.This snapshot is more recent than that used by Li et alPrior work HMM with type constraintsLang.
D&P LG&T YHMMproj.
YHMMwik.
YHMMunion YHMMunion +Cbg ?
?
84.2 68.1 87.2 87.9cs ?
?
75.4 70.2 75.4 79.2da 83.2 83.3 87.7 82.0 78.4 89.5de 82.8 85.8 86.6 85.1 80.0 88.3el 82.5 79.2 83.3 83.8 86.0 83.2es 84.2 86.4 83.9 83.7 88.3 87.3fr ?
?
88.4 75.7 75.6 86.6it 86.8 86.5 89.0 85.4 89.9 90.6ja ?
?
45.2 76.9 74.4 73.7nl 79.5 86.3 81.7 79.1 83.8 82.7pt 87.9 84.5 86.7 79.0 83.8 90.4sl ?
?
78.7 64.8 82.8 83.4sv 80.5 86.1 80.6 85.9 85.9 86.7tr ?
?
66.2 44.1 65.1 65.7zh ?
?
59.2 73.9 63.2 73.0avg (8) 83.4 84.8 84.9 83.0 84.5 87.3avg ?
?
78.5 75.9 80.0 83.2Table 1: Tagging accuracies for type-constrained HMMmodels.
D&P is the ?With LP?
model in Table 2 ofDas and Petrov (2011), while LG&T is the ?SHMM-ME?model in Table 2 of Li et al(2012).
YHMMproj.
, YHMMwik.
andYHMMunion are HMMs trained solely with type constraintsderived from the projected dictionary, Wiktionary andthe union of these dictionaries, respectively.
YHMMunion +C isequivalent to YHMMunion with additional cluster features.
Allmodels are trained on the treebank of each language,stripped of gold labels.
Results are averaged over the8 languages from Das and Petrov (2011), denoted avg (8),as well as over the full set of 15 languages, denoted avg.an unweighted tag dictionary.
We call this modelYHMMproj.
; its average accuracy of 84.9% on the eightlanguages is higher than the 83.4% of D&P and onpar with LG&T (84.8%).8 Our next model (YHMMwik.
)simply draws type constraints from Wiktionary.
Itslightly underperforms LG&T (83.0%), presumablybecause they used a second-order HMM.
As a simpleextension to these two models, we take the unionof the projected dictionary and Wiktionary to con-strain an HMM, which we name YHMMunion .
This modelperforms a little worse on the eight Indo-Europeanlanguages (84.5), but gives an improvement over theprojected dictionary when evaluated across all 15languages (80.0% vs. 78.5%).8Our model corresponds to the weaker, ?No LP?
projectionof Das and Petrov (2011).
We found that label propagation wasonly beneficial when small amounts of bitext were available.7Token constraints HMM with coupled constraints CRF with coupled constraintsLang.
YHMMunion +C+L y?HMM+C+L y?CRF+C+L Y?HMMproj.
+C+L Y?HMMwik.
+C+L Y?HMMunion +C+L Y?CRFproj.
+C+L Y?CRFwik.
+C+L Y?CRFunion+C+Lbg 87.7 77.9 84.1 84.5 83.9 86.7 86.0 87.8 85.4cs 78.3 65.4 74.9 74.8 81.1 76.9 74.7 80.3** 75.0da 87.3 80.9 85.1 87.2 85.6 88.1 85.5 88.2* 86.0de 87.7 81.4 83.3 85.0 89.3 86.7 84.4 90.5** 85.5el 85.9 81.1 77.8 80.1 87.0 83.9 79.6 89.5** 79.7es 89.1** 84.1 85.5 83.7 85.9 88.0 85.7 87.1 86.0fr 88.4** 83.5 84.7 85.9 86.4 87.4 84.9 87.2 85.6it 89.6 85.2 88.5 88.7 87.6 89.8 88.3 89.3 89.4ja 72.8 47.6 54.2 43.2 76.1 70.5 44.9 81.0** 68.0nl 83.1 78.4 82.4 82.3 84.2 83.2 83.1 85.9** 83.2pt 89.1 84.7 87.0 86.6 88.7 88.0 87.9 91.0** 88.3sl 82.4 69.8 78.2 78.5 81.8 80.1 79.7 82.3 80.0sv 86.1 80.1 84.2 82.3 87.9 86.9 84.4 88.9** 85.5tr 62.4 58.1 64.5 64.6 61.8 64.8 65.0 64.1** 65.2zh 72.6 52.7 39.5 56.0 74.1 73.3 59.7 74.4** 73.4avg (8) 87.2 82.0 84.2 84.5 87.0 86.8 84.9 88.8 85.4avg 82.8 74.1 76.9 77.6 82.8 82.3 78.2 84.5 81.1Table 2: Tagging accuracies for models with token constraints and coupled token and type constraints.
All models usecluster features (.
.
.
+C) and are trained on large training sets each containing 500k tokens with (partial) token-levelprojections (.
.
.
+L).
The best type-constrained model, trained on the larger datasets, YHMMunion +C+L, is included forcomparison.
The remaining columns correspond to HMM and CRF models trained only with token constraints (y?
.
.
.
)and with coupled token and type constraints (Y?
.
.
.).
The latter are trained using the projected dictionary (?proj.
),Wiktionary (?wik.)
and the union of these dictionaries (?union), respectively.
The search spaces of the models trained withcoupled constraints (Y?
.
.
.)
are each pruned with the respective tag dictionary used to derive the coupled constraints.The observed difference between Y?CRFwik.
+C+L and YHMMunion +C+L is statistically significant at p < 0.01 (**) and p < 0.015(*) according to a paired bootstrap test (Efron and Tibshirani, 1993).
Significance was not assessed for avg or avg (8).We next add monolingual cluster features tothe model with the union dictionary.
This model,YHMMunion +C, significantly outperforms all other type-constrained models, demonstrating the utility ofword-cluster features.9 For further exploration, wetrain the same model on the datasets containing 500Ktokens sampled from the target side of the paralleldata (YHMMunion +C+L); this is done to explore the effectsof large data during training.
We find that trainingon these datasets result in an average accuracy of87.2% which is comparable to the 87.3% reportedfor YHMMunion +C in Table 1.
This shows that the differentsource domain and amount of training data does notinfluence the performance of the HMM significantly.Finally, we train CRF models where we treat typeconstraints as a partially observed lattice and use thefull unpruned lattice for computing the partition func-9These are monolingual clusters.
Bilingual clusters as intro-duced in Ta?ckstro?m et al(2012) might bring additional benefits.tion (?3.2).
Due to space considerations, the resultsof these experiments are not shown in table 1.
We ob-serve similar trends in these results, but on average,accuracies are much lower compared to the type-constrained HMM models; the CRF model with theunion dictionary along with cluster features achievesan average accuracy of 79.3% when trained on samedata.
This result is not unsurprising.
First, the CRF?ssearch space is fully unconstrained.
Second, the dic-tionary only provides a weak set of observation con-straints, which do not provide sufficient informationto successfully train a discriminative model.
How-ever, as we will observe next, coupling the dictionaryconstraints with token-level information solves thisproblem.4.3 Models with Token and Type ConstraintsWe now proceed to add token-level information,focusing in particular on coupled token and type8constraints.
Since it is not possible to generateprojected token constraints for our monolingualtreebanks, we train all models in this subsectionon the 500K-tokens datasets sampled from the bi-text.
As a baseline, we first train HMM and CRFmodels that use only projected token constraints(y?HMM+C+L and y?CRF+C+L).
As shown in Table 2,these models underperform the best type-level model(YHMMunion +C+L),10 which confirms that projected to-ken constraints are not reliable on their own.
Thisis in line with similar projection models previouslyexamined by Das and Petrov (2011).We then study models with coupled token and typeconstraints.
These models use the same three dictio-naries as used in ?4.2, but additionally couple thederived type constraints with projected token con-straints; see the caption of Table 2 for a list of thesemodels.
Note that since we only allow projected tagsthat are licensed by the dictionary (Step 3 of the trans-fer, ?2.3), the actual token constraints used in thesemodels vary with the different dictionaries.From Table 2, we see that coupled constraints aresuperior to token constraints, when used both withthe HMM and the CRF.
However, for the HMM, cou-pled constraints do not provide any benefit over typeconstraints alone, in particular when the projecteddictionary or the union dictionary is used to derive thecoupled constraints (Y?HMMproj.
+C+L and Y?HMMunion +C+L).We hypothesize that this is because these dictionar-ies (in particular the former) have the same bias asthe token-level tag projections, so that the dictionaryis unable to correct the systematic errors in the pro-jections (see ?2.1).
Since the token constraints arestronger than the type constraints in the coupled mod-els, this bias may have a substantial impact.
Withthe Wiktionary dictionary, the difference between thetype-constrained and the coupled-constrained HMMis negligible: YHMMunion +C+L and Y?HMMwik.
+C+L both av-erage at an accuracy of 82.8%.The CRF model, on the other hand, is able to takeadvantage of the complementary information in thecoupled constraints, provided that the dictionary isable to filter out the systematic token-level errors.With a dictionary derived from Wiktionary and pro-jected token-level constraints, Y?CRFwik.
+C+L performs10To make the comparison fair vis-a-vis potential divergencesin training domains, we compare to the best type-constrainedmodel trained on the same 500K tokens training sets.0 1 2 302550751000 1 10 100 0 1 10 100 0 1 10 100 0 1 10 100Number of token?level projectionsTaggingaccuracyNumber of tags listed in WiktionaryFigure 4: Relative influence of token and type constraintson tagging accuracy in the Y?CRFwik.
+C+L model.
Word typesare categorized according to a) their number of Wiktionarytags (0,1,2 or 3+ tags, with 0 representing no Wiktionaryentry; top-axis) and b) the number of times they are token-constrained in the training set (divided into buckets of0, 1-9, 10-99 and 100+ occurrences; x-axis).
The boxessummarize the accuracy distributions across languagesfor each word type category as defined by a) and b).
Thehorizontal line in each box marks the median accuracy,the top and bottom mark the first and third quantile, re-spectively, while the whiskers mark the minimum andmaximum values of the accuracy distribution.better than all the remaining models, with an averageaccuracy of 88.8% across the eight Indo-Europeanlanguages available to D&P and LG&T.
Averagedover all 15 languages, its accuracy is 84.5%.5 Further AnalysisIn this section we provide a detailed analysis of theimpact of token versus type constraints and we studythe pruning and filtering mistakes resulting from in-complete Wiktionary entries in detail.
This analysisis based on the training portion of each treebank.5.1 Influence of Token and Type ConstraintsThe empirical success of the model trained with cou-pled token and type constraints confirms that theseconstraints indeed provide complementary signals.Figure 4 provides a more detailed view of the rela-tive benefits of each type of constraint.
We observeseveral interesting trends.First, word types that occur with more token con-straints during training are generally tagged moreaccurately, regardless of whether these types occur990.092.595.097.5100.00 50 100 150 200 250Number of corrected Wiktionary entriesPruningaccuracyFigure 5: Average pruning accuracy (line) across lan-guages (dots) as a function of the number of hypotheti-cally corrected Wiktionary entries for the k most frequentword types.
For example, position 100 on the x-axis cor-responds to manually correcting the entries for the 100most frequent types, while position 0 corresponds to ex-perimental conditions.in Wiktionary.
The most common scenario is for aword type to have exactly one tag in Wiktionary andto occur with this projected tag over 100 times inthe training set (facet 1, rightmost box).
These com-mon word types are typically tagged very accuratelyacross all languages.Second, the word types that are ambiguous accord-ing to Wiktionary (facets 2 and 3) are predominantlyfrequent ones.
The accuracy is typically lower forthese words compared to the unambiguous words.However, as the number of projected token con-straints is increased from zero to 100+ observations,the ambiguous words are effectively disambiguatedby the token constraints.
This shows the advantageof intersecting token and type constraints.Finally, projection generally helps for words thatare not in Wiktionary, although the accuracy for thesewords never reach the accuracy of the words withonly one tag in Wiktionary.
Interestingly, words thatoccur with a projected tag constraint less than 100times are tagged more accurately for types not in thedictionary compared to ambiguous word types withthe same number of projected constraints.
A possibleexplanation for this is that the ambiguous words areinherently more difficult to predict and that most ofthe words that are not in Wiktionary are less commonwords that tend to also be less ambiguous.zhtrsvslptnljaitfreseldedacsbgavg0 25 50 75 100Proportion of pruning errorsPRONNOUNDETADPPRTADVNUMCONJADJVERBX.Figure 6: Prevalence of pruning mistakes per POS tag,when pruning the inference search space with Wiktionary.5.2 Wiktionary Pruning MistakesThe error analysis by Li et al(2012) showed that thetags licensed by Wiktionary are often valid.
Whenusing Wiktionary to prune the search space of ourconstrained models and to filter token-level projec-tions, it is also important that correct tags are notmistakenly pruned because they are missing fromWiktionary.
While the accuracy of filtering is moredifficult to study, due to the lack of a gold standardtagging of the bitext, Figure 5 (position 0 on the x-axis) shows that search space pruning errors are nota major issue for most languages; on average thepruning accuracy is almost 95%.
However, for somelanguages such as Chinese and Czech the correct tagis pruned from the search space for nearly 10% of alltokens.
When using Wiktionary as a pruner, the upperbound on accuracy for these languages is thereforeonly around 90%.
However, Figure 5 also shows thatwith some manual effort we might be able to remedymany of these errors.
For example, by adding miss-ing valid tags to the 250 most common word types inthe worst language, the minimum pruning accuracywould rise above 95% from below 90%.
If the samewas to be done for all of the studied languages, themean pruning accuracy would reach over 97%.Figure 6 breaks down pruning errors resulting fromincorrect or incomplete Wiktionary entries acrossthe correct POS tags.
From this we observe that,for many languages, the pruning errors are highlyskewed towards specific tags.
For example, for Czechover 80% of the pruning errors are caused by mistak-enly pruned pronouns.106 ConclusionsWe considered the problem of constructing multilin-gual POS taggers for resource-poor languages.
Tothis end, we explored a number of different modelsthat combine token constraints with type constraintsfrom different sources.
The best results were ob-tained with a partially observed CRF model that ef-fectively integrates these complementary constraints.In an extensive empirical study, we showed that thisapproach substantially improves on the state of theart in this context.
Our best model significantly out-performed the second-best model on 10 out of 15evaluated languages, when trained on identical datasets, with an insignificant difference on 3 languages.Compared to the prior state of the art (Li et al 2012),we observed a relative reduction in error by 25%,averaged over the eight languages common to ourstudies.AcknowledgmentsWe thank Alexander Rush for help with the hyper-graph framework that was used to implement ourmodels and Klaus Macherey for help with the bi-text extraction.
This work benefited from many dis-cussions with Yoav Goldberg, Keith Hall, KuzmanGanchev and Hao Zhang.
We also thank the editorand the three anonymous reviewers for their valuablefeedback.
The first author is grateful for the financialsupport from the Swedish National Graduate Schoolof Language Technology (GSLT).ReferencesAnne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.2003.
Building a Treebank for French.
In A. Abeille?,editor, Treebanks: Building and Using Parsed Corpora,chapter 10.
Kluwer.Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?, JohnDeNero, and Dan Klein.
2010.
Painless unsupervisedlearning with features.
In Proceedings of NAACL-HLT.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of CoNLL.Stanley F Chen.
2003.
Conditional and joint models forgrapheme-to-phoneme conversion.
In Proceedings ofEurospeech.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2010.
Two decades of unsupervisedPOS induction: How far have we come?
In Proceed-ings of EMNLP.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proceedings of ACL-HLT.Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin.1977.
Maximum likelihood from incomplete data viathe EM algorithm.
Journal of the Royal StatisticalSociety, Series B, 39.John DeNero and Klaus Macherey.
2011.
Model-basedaligner combination using dual decomposition.
In Pro-ceedings of ACL-HLT.Brad Efron and Robert J. Tibshirani.
1993.
An Introduc-tion to the Bootstrap.
Chapman & Hall, New York, NY,USA.Victoria Fossum and Steven Abney.
2005.
Automaticallyinducing a part-of-speech tagger by projecting frommultiple source languages across aligned corpora.
InProceedings of IJCNLP.Dan Garrette and Jason Baldridge.
2012.
Type-supervisedhidden markov models for part-of-speech tagging withincomplete tag dictionaries.
In Proceedings of EMNLP-CoNLL.Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008.EM can find pretty good HMM POS-taggers (whengiven a good start).
In Proceedings of ACL-HLT.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT Summit.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilisticmodels for segmenting and labeling sequence data.
InProceedings of ICML.Shen Li, Joa?o Grac?a, and Ben Taskar.
2012.
Wiki-lysupervised part-of-speech tagging.
In Proceedings ofEMNLP-CoNLL.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical Programming, 45.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated corpusof English: the Penn treebank.
Computational Linguis-tics, 19(2).Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, andRegina Barzilay.
2009.
Multilingual part-of-speechtagging: Two unsupervised approaches.
JAIR, 36.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007.
The CoNLL 2007 shared task on dependencyparsing.
In Proceedings of EMNLP-CoNLL.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In Proceedings ofLREC.Sujith Ravi and Kevin Knight.
2009.
Minimized modelsfor unsupervised part-of-speech tagging.
In Proceed-ings of ACL-IJCNLP.11Stefan Riezler, Tracy H. King, Ronald M. Kaplan, RichardCrouch, John T. Maxwell, III, and Mark Johnson.
2002.Parsing the wall street journal using a lexical-functionalgrammar and discriminative estimation techniques.
InProceedings of ACL.Noah Smith and Jason Eisner.
2005.
Contrastive estima-tion: Training log-linear models on unlabeled data.
InProceedings of ACL.Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.2012.
Cross-lingual word clusters for direct transfer oflinguistic structure.
In Proceedings of NAACL-HLT.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceedingsof ACL.UN.
2006.
ODS UN parallel corpus.Jakob Uszkoreit and Thorsten Brants.
2008.
Distributedword clustering for large scale class-based languagemodeling in machine translation.
In Proceedings ofACL-HLT.Jakob Uszkoreit, Jay Ponte, Ashok Popat, and MosheDubiner.
2010.
Large scale parallel document miningfor machine translation.
In Proceedings of COLING.Chenhai Xi and Rebecca Hwa.
2005.
A backoff modelfor bootstrapping resources for non-English languages.In Proceedings of HLT-EMNLP.David Yarowsky and Grace Ngai.
2001.
Inducing mul-tilingual POS taggers and NP bracketers via robustprojection across aligned corpora.
In Proceedings ofNAACL.12
