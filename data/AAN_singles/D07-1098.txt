Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp.
940?946,Prague, June 2007. c?2007 Association for Computational LinguisticsProbabilistic Parsing Action Models for Multi-lingual DependencyParsingXiangyu DuanInstitute of Automation, Chi-nese Academy of Sciencesxyduan@nlpr.ia.ac.cnJun ZhaoInstitute of Automation, Chi-nese Academy of Sciencesjzhao@nlpr.ia.ac.cnBo XuInstitute of Automation, Chi-nese Academy of Sciencesxubo@hitic.ia.ac.cnAbstractDeterministic dependency parsers use pars-ing actions to construct dependencies.These parsers do not compute the probabil-ity of the whole dependency tree.
Theyonly determine parsing actions stepwiselyby a trained classifier.
To globally modelparsing actions of all steps that are taken onthe input sentence, we propose two kindsof probabilistic parsing action models thatcan compute the probability of the wholedependency tree.
The tree with the maxi-mal probability is outputted.
The experi-ments are carried on 10 languages, and theresults show that our probabilistic parsingaction models outperform the original de-terministic dependency parser.1 IntroductionThe target of CoNLL 2007 shared task (Nivre et al,2007) is to parse texts in multiple languages byusing a single dependency parser that has the ca-pacity to learn from treebank data.
Among parsersparticipating in CoNLL 2006 shared task(Buchholz et al, 2006), deterministic dependencyparser shows great efficiency in time and compa-rable performances for multi-lingual dependencyparsing (Nivre et al, 2006).
Deterministic parserregards parsing as a sequence of parsing actionsthat are taken step by step on the input sentence.Parsing actions construct dependency relations be-tween words.Deterministic dependency parser does not scorethe entire dependency tree as most of state-of-the-art parsers.
They only stepwisely choose the mostprobable parsing action.
In this paper, to globallymodel parsing actions of all steps that are taken onthe input sentence, we propose two kinds of prob-abilistic parsing action models that can computethe entire dependency tree?s probability.
Experi-ments are evaluated on diverse data set of 10 lan-guages provided by CoNLL 2007 shared-task(Nivre et al, 2007).
Results show that our prob-abilistic parsing action models outperform theoriginal deterministic dependency parser.
We alsopresent a general error analysis across a wide set oflanguages plus a detailed error analysis of Chinese.Next we briefly introduce the original determi-nistic dependency parsing algorithm that is a basiccomponent of our models.2 Introduction of Deterministic Depend-ency ParsingThere are mainly two representative deterministicdependency parsing algorithms proposed respec-tively by Nivre (2003), Yamada and Matsumoto(2003).
Here we briefly introduce Yamada andMatsumoto?s algorithm, which is adopted by ourmodels, to illustrate deterministic dependencyparsing.
The other representative method of Nivrealso parses sentences in a similar deterministicmanner except different data structure and parsingactions.Yamada?s method originally focuses on unla-beled dependency parsing.
Three kinds of parsingactions are applied to construct the dependencybetween two focus words.
The two focus words arethe current sub tree?s root and the succeeding (right)sub tree?s root given the current parsing state.Every parsing step results in a new parsing state,which includes all elements of the current partiallybuilt tree.
Features are extracted about these twofocus words.
In the training phase, features and thecorresponding parsing action compose the training940He provides confirming evidence RIGHTHeprovides confirming evidenceSHIFTLEFTRIGHTconfirmingHeprovides evidence provides evidenceHe confirmingprovidesHe evidenceconfirmingFigure 1.
The example of the parsing process of Yamada and Matsumoto?s method.
The input sentenceis ?He provides confirming evidence.?data.
In the testing phase, the classifier determineswhich parsing action should be taken based on thefeatures.
The parsing algorithm ends when there isno further dependency relation can be made on thewhole sentence.
The details of the three parsingactions are as follows:LEFT: it constructs the dependency that theright focus word depends on the left focus word.RIGHT: it constructs the dependency that theleft focus word depends on the right focus word.SHIFT: it does not construct dependency, justmoves the parsing focus.
That is, the new left focusword is the previous right focus word, whose suc-ceeding sub tree?s root is the new right focus word.The illustration of these three actions and theparsing process is presented in figure 1.
Note thatthe focus words are shown as bold black box.We extend the set of parsing actions to do la-beled dependency parsing.
LEFT and RIGHT areconcatenated by dependency labels, while SHIFTremains the same.
For example in figure 1, theoriginal action sequence ?RIGHT -> SHIFT ->RIGHT -> LEFT?
becomes ?RIGHT-SBJ ->SHIFT -> RIGHT-NMOD -> LEFT-OBJ?.3 Probabilistic Parsing Action ModelsDeterministic dependency parsing algorithms aregreedy.
They choose the most probable parsingaction at every parsing step given the current pars-ing state, and do not score the entire dependencytree.
To compute the probability of whole depend-ency tree, we propose two kinds of probabilisticmodels that are defined on parsing actins: parsingaction chain model (PACM) and parsing actionphrase model (PAPM).3.1 Parsing Action Chain Model (PACM)The parsing process can be viewed as a MarkovChain.
At every parsing step, there are several can-didate parsing actions.
The objective of this modelis to find the most probable sequence of parsingactions by taking the Markov assumption.
Asshown in figure 1, the action sequence ?RIGHT-SBJ -> SHIFT -> RIGHT-NMOD -> LEFT-OBJ?
constructs the right dependency tree of theexample sentence.
Choosing this action sequenceamong all candidate sequences is the objective ofthis model.Firstly, we should define the probability of thedependency tree conditioned on the input sentence.
)1(),...|()|(...110?=?=niii SdddPSTPWhere T denotes the dependency tree, S denotesthe original input sentence,  denotes the parsingaction at time step i.
We add an artificial parsingaction  as initial action.id0dWe introduce a variable  to denote theresulting parsing state when the action  is takenon .
is the original input sen-tence.idcontextid1?idcontext 0dcontextSuppose  are taken sequentially on theinput sentence S, and result in a sequence of pars-ing states , then P(T|S) de-fined in equation (1) becomes as below:ndd ...0nddcontextcontext ...0941)4()|()3()|()2(),...,|(...1...1...11110???===??
?=?nidiniddnidddiiiiicontextdPcontextcontextPcontextcontextcontextPFormula (3) comes from formula (2) by obeyingthe Markov assumption.
Note that formula (4) isabout the classifier of parsing actions.
It denotesthe probability of the parsing action given theparsing state .
If we train a classifierthat can predict with probability output, then wecan compute P(T|S) by computing the product ofthe probabilities of parsing actions.
The classifierwe use throughout this paper is SVM (Vapnik,1995).
We adopt Libsvm (Chang and Lin, 2005),which can train multi-class classifier and supporttraining and predicting with probability output(Chang and Lin, 2005).id1?idcontextFor this model, the objective is to choose theparsing action sequence that constructs the de-pendency tree with the maximal probability.)5()|(max)|(max...1...
11?= ?=nididd incontextdPSTPBecause this model chooses the most probablesequence, not the most probable parsing action atonly one step, it avoids the greedy property of theoriginal deterministic parsers.We use beam search for the decoding of thismodel.
We use m to denote the beam size.
Thenbeam search is carried out as follows.
At everyparsing step, all parsing states are ordered (or par-tially m ordered) according to their probabilities.Probability of a parsing state is determined bymultiplying the probabilities of actions that gener-ate that state.
Then we choose m best parsingstates for this step, and next parsing step only con-sider these m best parsing states.
Parsing termi-nates when the first entire dependency tree is con-structed.
To obtain a list of n-best parses, we sim-ply continue parsing until either n trees are found,or no further parsing can be fulfilled.3.2 Parsing Action Phrase Model (PAPM)In the Parsing Action Chain Model (PACM), ac-tions are competing at every parsing step.
Only mbest parsing states resulted by the correspondingactions are kept for every step.
But for the parsingproblem, it is reasonable that actions are competingfor which phrase should be built.
For dependencysyntax, one phrase consists of the head word andall its children.
Based on this motivation, we pro-pose Parsing Action Phrase Model (PAPM), whichdivides parsing actions into two classes: construct-ing action and shifting action.If a phrase is built after an action is performed,the action is called constructing action.
In originalYamada?s algorithm, constructing actions areLEFT and RIGHT.
For example, if LEFT is taken,it indicates that the right focus word has found allits children and becomes the head of this newphrase.
Note that one word with no children canalso be viewed as a phrase if its dependency onother word is constructed.
In the extended set ofparsing actions for labeled parsing, compound ac-tions, which consist of LEFT and RIGHT con-catenated by dependency labels, are constructingactions.If no phrase is built after an action is performed,the action is called shifting action.
Such action isSHIFT.We denote  as constructing action and  asshifting action.
j indexes the time step.
Then weintroduce a new concept: parsing action phrase.We use  to denote the ith parsing action phrase.It can be expanded as .
That is,parsing action phrase  is a sequence of parsingactions that constructs the next syntactic phrase.ja jbiAjjkji abbA 1...
??
?iAFor example, consider the parsing process infigure 1,  is ?RIGHT-SBJ?,  is ?SHIFT,RIGHT-NMOD?,  is ?LEFT-OBJ?.
Note thatconsists of a constructing action,  consistsof a shifting action and a constructing action,consists of a constructing action.1A 2A3A1A 2A3AThe indexes are different for both sides of theexpansion ,  is the ith parsingaction phrase corresponding to both constructingaction  at time step j and all its preceding shift-ing actions.
Note that on the right side of the ex-pansion, only one constructing action is allowedand is always at the last position, while shiftingaction can occur several times or does not occur atall.
It is parsing action phrases, i.e.
sequences ofparsing actions, that are competing for which nextphrase should be built.jjkji abbA 1...
???
iAja942The probability of the dependency tree given theinput sentence is redefined as:)|())|(()|()|...()|()|()...|()6(),...|()|(1111111...21...1...11...1...1...1...111?????????===?==???????=+?=?=?
?====?jtjiiiiiiibjktbtjniAkjniAjjkjniAiniAAniAAAniiicontextaPcontextbPcontextbPcontextabbPcontextAPcontextcontextPcontextcontextcontextPSAAAPSTPWhere k represents the number of steps that shift-ing action can be taken.
is the parsingstate resulting from a sequence of actionstaken on .iAcontextjjkj abb 1...
??
1?iAcontextThe objective in this model is to find the mostprobable sequence of parsing action phrases.)7()|(max)|(max...1...
11?= ?=niAiAA incontextAPSTPSimilar with parsing action chain model(PACM), we use beam search for the decoding ofparsing action phrase model (PAPM).
The differ-ence is that PAPM do not keep m best parsingstates at every parsing step.
Instead, PAPM keep mbest states which are corresponding to m best cur-rent parsing action phrases (several steps ofSHIFT and the last step of a constructing action).4 Experiments and ResultsExperiments are carried on 10 languages providedby CoNLL 2007 shared-task organizers (Nivre etal., 2007).
Among these languages, Chinese (Chenet al, 2003), Catalan (Mart?
et al, 2007) and Eng-lish (Johansson and Nugues, 2007) have low per-centage of non-projective relations, which are0.0%, 0.1% and 0.3% respectively.
Except thesethree languages, we use software of projectiviza-tion/deprojectivization provided by Nivre andNilsson (2005) for other languages.
Because ouralgorithm only deals with projective parsing, weshould projectivize training data at first to preparefor the following training of our algorithm.
Duringtesting, deprojectivization is applied to the outputof the parser.Considering the classifier of Libsvm (Chang andLin, 2005), the features are extracted from the fol-lowing fields of the data representation: FORM,LEMMA, CPOSTAG, POSTAG, FEATS and DE-PREL.
We split values of FEATS field into itsatomic components.
We only use available featuresof DEPREL field during deterministic parsing.
Weuse similar feature context window as used in Ya-mada?s algorithm (Yamada and Matsumoto, 2003).In detail, the size of feature context window is six,which consists of left two sub trees, two focuswords related sub trees and right two sub trees.This feature template is used for all 10 languages.4.1 Results of PACM and Yamada?s MethodAfter submitting the testing results of Parsing Ac-tion Chain Model (PACM), we also perform origi-nal deterministic parsing proposed by Yamada andMatsumoto (2003).
The total results are shown intable 1.
The experimental results are mainly evalu-ated by labeled attachment score (LAS), unlabeledattachment score (UAS) and labeled accuracy (LA).Table 1 shows that Parsing Action Chain Model(PACM) outperform original Yamada?s parsingmethod for all languages.
The LAS improvementsrange from 0.60 percentage points to 1.71 percent-age points.
Note that the original Yamada?smethod still gives testing results above the officialreported average performance of all languages.Ara Bas Cat Chi Cze Eng Gre Hun Ita TurYamLAS  69.31 69.67 83.26 81.88 74.63 84.81 72.75 76.24 80.08 73.94YamUAS  78.93 75.86 88.53 86.17 80.11 85.83 79.45 79.97 83.69 79.79YamLA  81.13 75.71 88.36 84.56 82.10 89.71 82.58 88.37 86.93 80.81PACMLAS  69.91 71.26 84.95 82.58 75.34 85.83 74.29 77.06 80.75 75.03PACMUAS  79.04 77.57 89.71 86.88 80.82 86.97 80.77 80.66 84.20 81.03PACMLA  81.40 77.35 89.55 85.35 83.17 90.57 83.87 88.92 87.32 81.17Table 1.
The performances of Yamada?s method (Yam) and Parsing Action Chain Model (PACM).9434.2 Results of PAPMNot all languages have only one root node of asentence.
Since Parsing Action Phrase Model(PAPM) only builds dependencies, and shiftingaction is not the ending action of a parsing actionphrase, PAPM always ends with one root word.This property makes PAPM only suitable forCatalan, Chinese, English and Hungarian, whichare unary root languages.
PAPM result of Catalanwas not submitted before deadline due to theshortage of time and computing resources.
Wereport Catalan?s PAPM result together with that ofother three languages in table 2.Cat Chi Eng HunPAPMLAS  87.26 82.64 86.69 76.89PAPMUAS  92.07 86.94 87.87 80.53PAPMLA  91.89 85.41 92.04 89.73Table 2.
The performance of Parsing ActionPhrase Model (PAPM) for Catalan, Chinese, Eng-lish and Hungarian.Compared with the results of PACM shown intable 1, the performance of PAPM differs amongdifferent languages.
Catalan and English showthat PAPM improves 2.31% and 0.86% respec-tively over PACM, while the improvement of Chi-nese is marginal, and there is a little decrease ofHungarian.
Hungarian has relatively high percent-age of non-projective relations.
If phrase consistsof head word and its non-projective children, theconstructing actions that are main actions inPAPM will be very difficult to be learned becausesome non-projective children together with theirheads have no chance to be simultaneously as fo-cus words.
Although projectivization is also per-formed for Hungarian, the built-in non-projectiveproperty still has negative influence on the per-formance.5 Error AnalysisIn the following we provide a general error analy-sis across a wide set of languages plus a detailedanalysis of Chinese.5.1 General Error AnalysisOne of the main difficulties in dependency parsingis the determination of long distance dependencies.Although all kinds of evaluation scores differdramatically among different languages, 69.91%to 85.83% regarding LAS, there are some generalobservations reflecting the difficulty of long dis-tance dependency parsing.
We study this difficultyfrom two aspects about our full submission ofPACM: precision of dependencies of different arclengths and precision of root nodes.For arcs of length 1, all languages give highperformances with lowest 91.62% of Czech(B?hmova et al, 2003) to highest 96.8% of Cata-lan (Mart?
et al, 2007).
As arcs lengths growlonger, various degradations are caused.
For Cata-lan, score of arc length 2 is similar with that of arclength 1, but there are dramatic degradations forlonger arc lengths, from 94.94% of arc length 2 to85.22% of length 3-6.
For English (Johansson andNugues, 2007) and Italian (Montemagni et al,2003), there are graceful degradation for arcs oflength 1,2 and 3-6, with 96-91-85 of English and95-85-75 of Italian.
For other languages, long arcsalso give remarkable degradations that pull downthe performance.Precision of root nodes also reflects the per-formance of long arc dependencies because thearc between the root and its children are oftenlong arcs.
In fact, it is the precision of roots andarcs longer than 7 that mainly pull down the over-all performance.
Yamada?s method is a bottom-upparsing algorithm that builds short distance de-pendencies at first.
The difficulty of building longarc dependencies may partially be resulted fromthe errors of short distance dependencies.
The de-terministic manner causes error propagation, andit indirectly indicates that the errors of roots arethe final results of error propagation of short dis-tance dependencies.
But there is an exception oc-curred in Chinese.
The root precision is 90.48%,only below the precision of arcs of length 1.
Thisphenomenon exists because the sentences in Chi-nese data set (Chen et al, 2003) are in fact clauseswith average length of 5.9 rather than entire sen-tences.
The root words are heads of clauses.Both Parsing Action Chain Model (PACM) andParsing Action Phrase Model (PAPM) avoidgreedy property of original Yamada?s method.
Itcan be expected that there will be a precision im-provement of long distance dependencies overoriginal Yamada?s method.
For PACM, the resultsof Basque (Aduriz et al, 2003), Catalan (Mart?
etal., 2007), Chinese (Chen et al, 2003), English(Johansson and Nugues, 2007) and Greek (Pro-944kopidis et al, 2005) show that the root precisionimprovement over Yamada?s method is more con-spicuous than that of other long distance depend-encies.
The largest improvement of roots precisionis 10.7% of Greek.
While for Arabic (Hajic et al,2004), Czech (B?hmova et al, 2003), Hungarian(Csendes et al, 2005), Italian (Montemagni et al,2003) and Turkish (Oflazer et al, 2003), the im-provement of root precision is small, but depend-encies of arcs longer than 1 give better scores.
ForPAPM, good performances of Catalan and Englishalso give significant improvements of root preci-sion over PACM.
For Catalan, the root precisionimprovement is from 63.86% to 95.21%; for Eng-lish, the root precision improvement is from62.03% to 89.25%.5.2 Error Analysis of ChineseThere are mainly two sources of errors regardingLAS in Chinese dependency parsing.One is from conjunction words (C) that have arelatively high percentage of wrong heads (about20%), and therefore 19% wrong dependency la-bels.
In Chinese, conjunction words often con-catenate clauses.
Long distance dependencies be-tween clauses are bridged by conjunction words.It is difficult for conjunction words to find theirheads.The other source of errors comes from auxiliarywords (DE) and preposition words (P).
Unlikeconjunction words, auxiliary words and preposi-tion words have high performance of finding righthead, but label accuracy (LA) decrease signifi-cantly.
The reason may lie in the large depend-ency label set consisting of 57 kinds of depend-ency labels in Chinese.
Moreover, auxiliary words(DE) and preposition words (P) have more possi-ble dependency labels than other coarse POS have.This introduces ambiguity for parsers.Most common POS including noun and verbcontribute much to the overall performance of83% Labeled Attachment Scores (LAS).
Adverbsobtain top score while adjectives give the worst.6 ConclusionWe propose two kinds of probabilistic modelsdefined on parsing actions to compute the prob-ability of entire sentence.
Compared with originalYamada and Matsumoto?s deterministic depend-ency method which stepwisely chooses mostprobable parsing action, the two probabilisticmodels improve the performance regarding all 10languages in CoNLL 2007 shared task.
Throughthe study of parsing results, we find that long dis-tance dependencies are hard to be determined forall 10 languages.
Further analysis about this diffi-culty is needed to guide the research direction.Feature exploration is also necessary to providemore informative features for hard problems.AckowledgementsThis work was supported by Hi-tech Research andDevelopment Program of China under grant No.2006AA01Z144, the Natural Sciences Foundationof China under grant No.
60673042, and the Natu-ral Science Foundation of Beijing under grant No.4052027, 4073043.ReferencesS.
Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.2006.
CoNLL-X shared task on multilingual de-pendency parsing.
SIGNLL.Chih-Chung Chang and Chih-Jen Lin.
2005.
LIBSVM:A library for support vector machines.J.
Nivre.
2003.
An efficient algorithm for projectivedependency parsing.
In Proceedings of the 8th In-ternational Workshop on Parsing Technologies(IWPT).J.
Nivre and J. Nilsson.
2005.
Pseudo-projective de-pendency parsing.
In Proc.
of ACL-2005, pages 99?106.J.
Nivre, J.
Hall, J. Nilsson, G. Eryigit, S. Marinov.2006.
Labeled Pseudo-Projective DependencyParsing with Support Vector Machines.
In Proc.
ofthe Tenth Conference on Computational NaturalLanguage Learning (CoNLL).J.
Nivre, J.
Hall, S. K?bler, R. McDonald, J. Nilsson, S.Riedel, and D. Yuret.
2007.
The CoNLL 2007shared task on dependency parsing.
In Proc.
of theJoint Conf.
on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL).H.
Yamada and Y. Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.
InProceedings of the 8th International Workshop onParsing Technologies (IWPT).V.
Vapnik.
1995.
The Nature of StatisticalLearningTheory.
Springer.945A.
Abeill?, editor.
2003.
Treebanks: Building andUsing Parsed Corpora.
Kluwer.I.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A.Diaz de Ilarraza, A. Garmendia and M. Oronoz.2003.
Construction of a Basque Dependency Tree-bank.
In Proc.
of the 2nd Workshop on Treebanksand Linguistic Theories (TLT), pages 201?204.A.
B?hmov?, J. Hajic, E. Hajicov?
and B. Hladk?.2003.
The PDT: a 3-level annotation scenario.
InAbeill?
(2003), chapter 7, 103?127.K.
Chen, C. Luo, M. Chang, F. Chen, C. Chen, C.Huang and Z. Gao.
2003.
Sinica Treebank: DesignCriteria, Representational Issues and Implementa-tion.
In Abeill?
(2003), chapter 13, pages 231?248.D.
Csendes, J. Csirik, T. Gyim?thy, and A. Kocsor.2005.
The Szeged Treebank.
Springer.J.
Hajic, O. Smrz, P. Zem?nek, J. Snaidauf and E.Beska.
2004.
Prague Arabic Dependency Treebank:Development in Data and Tools.
In Proc.
of theNEMLAR Intern.
Conf.
on Arabic Language Re-sources and Tools, pages 110?117.R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.In Proc.
of the 16th Nordic Conference onComputational Linguistics (NODALIDA).M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: thePenn Treebank.
Computational Linguistics,19(2):313?330.M.
A.
Mart?, M.
Taul?, L. M?rquez and M. Bertran.2007.
CESS-ECE: A Multilingual and MultilevelAnnotated Corpus.
Available for download from:http://www.lsi.upc.edu/~mbertran/cess-ece/.S.
Montemagni, F. Barsotti, M. Battista, N. Calzolari,O.
Corazzari, A. Lenci, A. Zampolli, F. Fanciulli, M.Massetani, R. Raffaelli, R. Basili, M. T. Pazienza, D.Saracino, F. Zanzotto, N. Nana, F. Pianesi, and R.Delmonte.
2003.
Building the Italian Syntactic-Semantic Treebank.
In Abeill?
(2003), chapter 11,pages 189?210.J.
Nivre, J.
Hall, S. K?bler, R. McDonald, J. Nilsson, S.Riedel, and D. Yuret.
2007.
The CoNLL 2007shared task on dependency parsing.
In Proc.
of theCoNLL 2007 Shared Task.
Joint Conf.
on EmpiricalMethods in Natural Language Processing andComputational Natural Language Learning(EMNLP-CoNLL).K.
Oflazer, B.
Say, D. Zeynep Hakkani-T?r, and G.T?r.
2003.
Building a Turkish treebank.
In Abeill?
(2003), chapter 15, pages 261?277.P.
Prokopidis, E. Desypri, M. Koutsombogera, H.Papageorgiou, and S. Piperidis.
2005.
Theoreticaland practical issues in the construction of a Greekdepen- dency treebank.
In Proc.
of the 4thWorkshop on Treebanks and Linguistic Theories(TLT), pages 149?160.946
