Improving Word Alignment Quality using Morpho-syntacticInformationMaja Popovic?
and Hermann NeyLehrstuhl fu?r Informatik VI - Computer Science DepartmentRWTH Aachen UniversityAhornstrasse 5552056 AachenGermany{popovic,ney}@cs.rwth-aachen.deAbstractIn this paper, we present an approach toinclude morpho-syntactic dependencies intothe training of the statistical alignmentmodels.
Existing statistical translation sys-tems usually treat different derivations ofthe same base form as they were indepen-dent of each other.
We propose a methodwhich explicitly takes into account such in-terdependencies during the EM training ofthe statistical alignment models.
The eval-uation is done by comparing the obtainedViterbi alignments with a manually anno-tated reference alignment.
The improve-ments of the alignment quality comparedto the, to our knowledge, best system arereported on the German-English Verbmobilcorpus.1 IntroductionIn statistical machine translation, a translationmodel Pr(fJ1 |eI1) describes the correspondencesbetween the words in the source language sen-tence fJ1 and the words in the target languagesentence eI1.
Statistical alignment models arecreated by introducing a hidden variable aJ1representing a mapping from the source wordfj into the target word eaj .
So far, most ofthe statistical machine translation systems arebased on the single-word alignment models asdescribed in (Brown et al, 1993) as well as theHidden Markov alignment model (Vogel et al,1996).
The lexicon models used in these systemstypically do not include any linguistic or con-textual information which often results in inad-equate alignments between the sentence pairs.In this work, we propose an approach to im-prove the quality of the statistical alignmentsby taking into account the interdependencies ofdifferent derivations of the words.
We are get-ting use of the hierarchical representation of thestatistical lexicon model as proposed in (Nie?enand Ney, 2001) for the conventional EM trainingprocedure.
Experimental results are reportedfor the German-English Verbmobil corpus andthe evaluation is done by comparing the ob-tained Viterbi alignments after the training ofconventional models and models which are usingmorpho-syntactic information with a manuallyannotated reference alignment.2 Related WorkThe popular IBM models for statistical ma-chine translation are described in (Brown etal., 1993) and the HMM-based alignment modelwas introduced in (Vogel et al, 1996).
A goodoverview of all these models is given in (Ochand Ney, 2003) where the model IBM-6 is alsointroduced as the log-linear interpolation of theother models.Context dependencies have been introducedinto the training of alignments in (Varea et al,2002), but they do not take any linguistic infor-mation into account.Some recent publications have proposed theuse of morpho-syntactic knowledge for statisti-cal machine translation, but mostly only for thepreprocessing step whereas training procedureof the statistical models remains the same (e.g.
(Nie?en and Ney, 2001a)).Incorporation of the morpho-syntactic knowl-egde into statistical models has been dealtin (Nie?en and Ney, 2001): hierarchical lexi-con models containing base forms and set ofmorpho-syntactic tags are proposed for thetranslation from German into English.
How-ever, these lexicon models are not used for thetraining but have been created from the Viterbialignment obtained after the usual training pro-cedure.The use of POS information for improvingstatistical alignment quality of the HMM-basedmodel is described in (Toutanova et al, 2002).They introduce additional lexicon probabilityfor POS tags in both languages, but actuallyare not going beyond full forms.3 Statistical Alignment ModelsThe goal of statistical machine translation is totranslate an input word sequence f1, .
.
.
, fJ inthe source language into a target language wordsequence e1, .
.
.
, eI .
Given the source languagesequence, we have to choose the target languagesequence that maximises the product of the lan-guage model probability Pr(eI1) and the trans-lation model probability Pr(fJ1 |eI1).
The trans-lation model describes the correspondence be-tween the words in the source and the targetsequence whereas the language model describeswell-formedness of a produced target sequence.The translation model can be rewritten in thefollowing way:Pr(fJ1 |eI1) =?aJ1Pr(fJ1 , aJ1 |eI1)where aJ1 are called alignments and representa mapping from the source word position j tothe target word position i = aj .
Alignmentsare introduced into translation model as a hid-den variable, similar to the concept of HiddenMarkov Models (HMM) in speech recognition.The translation probability Pr(fJ1 , aJ1 |eI1) canbe further rewritten as follows:Pr(fJ1 , aJ1 |eI1) =J?j=1Pr(fj , aj |f j?11 , aj?11 , eI1)=J?j=1Pr(aj |f j?11 , aj?11 , eI1) ?
?Pr(fj |f j?11 , aj1, eI1)where Pr(aj |f j?11 , aj?11 , eI1) is called alignmentprobability and Pr(fj |f j?11 , aj1, eI1) is lexiconprobability.In all popular translation models IBM-1 toIBM-5 as well as in HMM translation model,the lexicon probability Pr(fj |f j?11 , aj1, eI1) is ap-proximated with the simple single-word lexi-con probability p(fj |eaj ) which takes into ac-count only full forms of the words fj andeaj .
The difference between these models isbased on the definition of alignment modelPr(aj |f j?11 , aj?11 , eI1).
Detailed description ofthose models can be found in (Brown et al,1993), (Vogel et al, 1996) and (Och and Ney,2003).4 Hierarchical Representation of theLexicon ModelTypically, the statistical lexicon model is basedonly on the full forms of the words and does nothave any information about the fact that somedifferent full forms are actually derivations ofthe same base form.
For highly inflected lan-guages like German this might cause problemsbecause the coverage of the lexicon might below since the token/type ratio for German istypically much lower than for English (e.g.
forVerbmobil: English 99.4, German 56.3).To take these interdependencies into account,we use the hierarchical representation of the sta-tistical lexicon model as proposed in (Nie?enand Ney, 2001).
A constraint grammar parserGERCG for lexical analysis and morphologicaland syntactic disambiguation for German lan-guage is used to obtain morpho-syntactic infor-mation.
For each German word, this tool pro-vides its base form and the sequence of morpho-syntactic tags, and this information is thenadded into the original corpus.
For example,the German word ?gehe?
(go), a verb in theindicative mood and present tense which is de-rived from the base form ?gehen?
is annotatedas ?gehe#gehen-V-IND-PRES#gehen?.This new representation of the corpus wherefull word forms are enriched with its base formsand tags enables gradual accessing of informa-tion with different levels of abstraction.
Con-sider for example the above mentioned Germanword ?gehe?
which can be translated into theEnglish word ?go?.
Another derivation of thesame base form ?gehen?
is ?gehst?
which alsocan be translated by ?go?.
Existing statisticaltranslation models cannot handle the fact that?gehe?
and ?gehst?
are derivatives of the samebase form and both can be translated into thesame English word ?go?, whereas the hierarchi-cal representation makes it possible to take suchinterdependencies into account.5 EM Training5.1 Standard EM training (review)In this section, we will briefly review the stan-dard EM algorithm for the training of the lexi-con model.In the E-step the lexical counts are collectedover all sentences in the corpus:C(f, e) =?s?ap(a|f s, es)?i,j?
(f, fjs)?
(e, eis)In the M-step the lexicon probabilities are cal-culated:p(f |e) = C(f, e)?f?C(f?
, e)The procedure is similar for the other modelparameters, i.e.
alignment and fertility proba-bilities.For models IBM-1, IBM-2 and HMM, an ef-ficient computation of the sum over all align-ments is possible.
For the other models, thesum is approximated using an appropriately de-fined neighbourhood of the Viterbi alignment(see (Och and Ney, 2003) for details).5.2 EM training using hierarchicalcountsIn this section we describe the EM training ofthe lexicon model using so-called hierarchicalcounts which are collected from the hierarchi-caly annotated corpus.In the E-step the following types of counts arecollected:?
full form counts:C(f, e) =?s?ap(a|f s, es) ???i,j?
(f, fjs)?
(e, eis)where f is the full form of the word, e.g.?gehe?;?
base form+tag counts:C(fbt, e) =?s?ap(a|f s, es) ???i,j?
(fbt, fbtjs)?
(e, eis)where fbt represents the base form of theword f with sequence of correspondingtags, e.g.
?gehen-V-IND-PRES?;?
base form counts:C(fb, e) =?s?ap(a|f s, es) ???i,j?
(fb, fbjs)?
(e, eis)where fb is the base form of the word f ,e.g.
?gehen?.For each full form, refined hierarchical countsare obtained in the following way:Chier(f, e) = C(f, e) + C(fbt, e) + C(fb, e)and the M-step is then performed using hier-archical counts:p(f |e) = Chier(f, e)?f?Chier(f?
, e)The training procedure for the other modelparameters remains unchanged.6 Experimental ResultsWe performed our experiments on the Verbmo-bil corpus.
The Verbmobil task (W. Wahlster,editor, 2000) is a speech translation task in thedomain of appointment scheduling, travel plan-ning and hotel reservation.
The corpus statis-tics is shown in Table 1.
The number of sure andpossible alignments in the manual reference isgiven as well.
We also used a small training cor-pus consisting of only 500 sentences randomlychosen from the main corpus.We carried out the training scheme14H5334365 using the toolkit GIZA++.The scheme is defined according to the numberof iterations for each model.
For example, 43means three iterations of the model IBM-4.
Wetrained the IBM-1 and HMM model using hier-archical lexicon counts, and the parameters ofthe other models were also indirectly improvedthanks to the refined parameters of the initialmodels.German EnglishTrain Sentences 34446Words 329625 343076Vocabulary 5936 3505Singletons 2600 1305Test Sentences 354Words 3233 3109S relations 2559P relations 4596Table 1: Corpus statistics for Verbmobil task6.1 Evaluation MethodWe use the evaluation criterion described in(Och and Ney, 2000).
The obtained wordalignment is compared to a reference alignmentproduced by human experts.
The annotationscheme explicitly takes into account the ambi-guity of the word alignment.
The unambiguousalignments are annotated as sure alignments (S)and the ambiguous ones as possible alignments(P ).
The set of possible alignments P is usedespecially for idiomatic expressions, free trans-lations and missing function words.
The set Sis subset of the set P (S ?
P ).The quality of an alignment A is computedas appropriately redefined precision and recallmeasures.
Additionally, we use the alignmenterror rate (AER) which is derived from the well-known F-measure.recall = |A ?
S||S| , precision =|A ?
P ||A|AER = 1?
|A ?
S|+ |A ?
P ||A|+ |S|Thus, a recall error can only occur if a S(ure)alignment is not found and a precision errorcan only occur if a found alignment is not evenP (ossible).6.2 Alignment Quality ResultsTable 2 shows the alignment quality for the twocorpus sizes of the Verbmobil task.
Resultsare presented for the Viterbi alignments fromboth translation directions (German?Englishand English?German) as well as for combina-tion of those two alignments.The table shows the baseline AER for dif-ferent training schemes and the correspondingAER when the hierarchical counts are used.
Wesee that there is a consistent decrease in AERfor all training schemes, especially for the smalltraining corpus.
It can be also seen that greaterimprovements are yielded for the simpler mod-els.7 ConclusionsIn this work we have presented an approachfor including morpho-syntactic knowledge intoa maximum likelihood training of statisticaltranslation models.
As can be seen in Section5, going beyond full forms during the trainingby taking into account the interdependencies ofthe different derivations of the same base formresults in the improvements of the alignmentcorpus size = 0.5kTraining Model D ?
E E ?
D combined14 ibm1 27.5 33.4 22.7+hier 24.8 30.3 20.514H5 hmm 18.8 24.0 16.9+hier 16.9 21.5 14.814H533 ibm3 18.4 22.8 17.0+hier 16.7 22.1 15.514H53343 ibm4 16.9 21.5 16.2+hier 15.8 20.7 14.914H5334365 ibm6 16.7 21.1 15.9+hier 15.6 20.9 14.8corpus size = 34kTraining Model D ?
E E ?
D combined14 ibm1 17.6 24.1 14.1+hier 16.8 21.8 13.714H5 hmm 8.9 14.9 7.9+hier 8.4 13.7 7.314H533 ibm3 8.4 12.8 7.7+hier 8.2 12.7 7.414H53343 ibm4 6.3 10.9 6.0+hier 6.1 10.8 5.714H5334365 ibm6 5.7 10.0 5.5+hier 5.5 9.7 5.0Table 2: AER [%] for Verbmobil corpus for thebaseline system (name of the model) and thesystem using hierarchical method (+hier)quality, especially for the small training corpus.We assume that the method can be very effec-tive for cases where only small amount of data isavailable.
We also expect further improvementsby performing a special modelling for the rarewords.We are planning to investigate possibilitiesof improving the alignment quality for differentlanguage pairs using different types of morpho-syntactic information, like for example to useword stems and suffixes for morphologicaly richlanguages where some parts of the words haveto be aligned to the whole English words (e.g.Spanish verbs, Finnish in general, etc.)
We arealso planning to use the refined alignments forthe translation process.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vin-cent J. Della Pietra and Robert L. Mercer.1993.
The mathematics of statistical machinetranslation: Parameter estimation.
Compu-tational Linguistics, 19(2):263?311Ismael Garc?
?a Varea, Franz Josef Och, Her-mann Ney and Francisco Casacuberta.
2002.Improving alignment quality in statisticalmachine translation using context-dependentmaximum entropy models.
In Proc.
of the19th International Conference on Computa-tional Linguistics (COLING), pages 1051?1057, Taipei, Taiwan, August.Sonja Nie?en and Hermann Ney.
2001a.Morpho-syntactic analysis for reordering instatistical machine translation.
In Proc.
MTSummit VIII, pages 247?252, Santiago deCompostela, Galicia, Spain, September.Sonja Nie?en and Hermann Ney.
2001.
Towardhierarchical models for statistical machinetranslation of inflected languages.
In 39thAnnual Meeting of the Assoc.
for Computa-tional Linguistics - joint with EACL 2001:Proc.
Workshop on Data-Driven MachineTranslation, pages 47?54, Toulouse, France,July.Franz Josef Och and Hermann Ney.
2000.
Im-proved statistical alignment models.
In Proc.of the 38th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages440?447, Hong Kong, October.Franz Josef Och and Hermann Ney.
2003.
Asystematic comparison of various statisticalalignment models.
Computational Linguis-tics, 29(1):19?51Kristina Toutanova, H. Tolga Ilhan andChristopher D. Manning.
2002.
Extensionsto HMM-based statistical word alignmentmodels.
In Proc.
Conf.
on Empirical Methodsfor Natural Language Processing (EMNLP),pages 87?94, Philadelphia, PA, July.Stephan Vogel, Hermann Ney and Cristoph Till-mann.
1996.
HMM-based word alignmentin statistical translation.
In Proc.
of the16th International Conference on Computa-tional Linguistics (COLING) , pages 836?841, Copenhagen, Denmark, August.W.
Wahlster, editor 2000.
Verbmobil: Foun-dations of speech-to-speech translations.Springer Verlag, Berlin, Germany, July.
