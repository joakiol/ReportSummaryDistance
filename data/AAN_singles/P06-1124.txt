Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 985?992,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Hierarchical Bayesian Language Model based on Pitman-Yor ProcessesYee Whye TehSchool of Computing,National University of Singapore,3 Science Drive 2, Singapore 117543.tehyw@comp.nus.edu.sgAbstractWe propose a new hierarchical Bayesiann-gram model of natural languages.
Ourmodel makes use of a generalization ofthe commonly used Dirichlet distributionscalled Pitman-Yor processes which pro-duce power-law distributions more closelyresembling those in natural languages.
Weshow that an approximation to the hier-archical Pitman-Yor language model re-covers the exact formulation of interpo-lated Kneser-Ney, one of the best smooth-ing methods for n-gram language models.Experiments verify that our model givescross entropy results superior to interpo-lated Kneser-Ney and comparable to mod-ified Kneser-Ney.1 IntroductionProbabilistic language models are used exten-sively in a variety of linguistic applications, in-cluding speech recognition, handwriting recogni-tion, optical character recognition, and machinetranslation.
Most language models fall into theclass of n-gram models, which approximate thedistribution over sentences using the conditionaldistribution of each word given a context consist-ing of only the previous n?
1 words,P (sentence) ?T?i=1P (wordi |wordi?1i?n+1) (1)with n = 3 (trigram models) being typical.
Evenfor such a modest value of n the number of param-eters is still tremendous due to the large vocabu-lary size.
As a result direct maximum-likelihoodparameter fitting severely overfits to the trainingdata, and smoothing methods are indispensible forproper training of n-gram models.A large number of smoothing methods havebeen proposed in the literature (see (Chen andGoodman, 1998; Goodman, 2001; Rosenfeld,2000) for good overviews).
Most methods take arather ad hoc approach, where n-gram probabili-ties for various values of n are combined together,using either interpolation or back-off schemes.Though some of these methods are intuitively ap-pealing, the main justification has always beenempirical?better perplexities or error rates on testdata.
Though arguably this should be the onlyreal justification, it only answers the question ofwhether a method performs better, not how norwhy it performs better.
This is unavoidable giventhat most of these methods are not based on in-ternally coherent Bayesian probabilistic models,which have explicitly declared prior assumptionsand whose merits can be argued in terms of howclosely these fit in with the known properties ofnatural languages.
Bayesian probabilistic mod-els also have additional advantages?it is rela-tively straightforward to improve these models byincorporating additional knowledge sources andto include them in larger models in a principledmanner.
Unfortunately the performance of pre-viously proposed Bayesian language models hadbeen dismal compared to other smoothing meth-ods (Nadas, 1984; MacKay and Peto, 1994).In this paper, we propose a novel languagemodel based on a hierarchical Bayesian model(Gelman et al, 1995) where each hidden variableis distributed according to a Pitman-Yor process, anonparametric generalization of the Dirichlet dis-tribution that is widely studied in the statistics andprobability theory communities (Pitman and Yor,1997; Ishwaran and James, 2001; Pitman, 2002).985Our model is a direct generalization of the hierar-chical Dirichlet language model of (MacKay andPeto, 1994).
Inference in our model is howevernot as straightforward and we propose an efficientMarkov chain Monte Carlo sampling scheme.Pitman-Yor processes produce power-law dis-tributions that more closely resemble those seenin natural languages, and it has been argued thatas a result they are more suited to applicationsin natural language processing (Goldwater et al,2006).
We show experimentally that our hierarchi-cal Pitman-Yor language model does indeed pro-duce results superior to interpolated Kneser-Neyand comparable to modified Kneser-Ney, two ofthe currently best performing smoothing methods(Chen and Goodman, 1998).
In fact we show astronger result?that interpolated Kneser-Ney canbe interpreted as a particular approximate infer-ence scheme in the hierarchical Pitman-Yor lan-guage model.
Our interpretation is more usefulthan past interpretations involving marginal con-straints (Kneser and Ney, 1995; Chen and Good-man, 1998) or maximum-entropy models (Good-man, 2004) as it can recover the exact formulationof interpolated Kneser-Ney, and actually producessuperior results.
(Goldwater et al, 2006) has inde-pendently noted the correspondence between thehierarchical Pitman-Yor language model and in-terpolated Kneser-Ney, and conjectured improvedperformance in the hierarchical Pitman-Yor lan-guage model, which we verify here.Thus the contributions of this paper are three-fold: in proposing a langauge model with excel-lent performance and the accompanying advan-tages of Bayesian probabilistic models, in propos-ing a novel and efficient inference scheme for themodel, and in establishing the direct correspon-dence between interpolated Kneser-Ney and theBayesian approach.We describe the Pitman-Yor process in Sec-tion 2, and propose the hierarchical Pitman-Yorlanguage model in Section 3.
In Sections 4 and5 we give a high level description of our samplingbased inference scheme, leaving the details to atechnical report (Teh, 2006).
We also show howinterpolated Kneser-Ney can be interpreted as ap-proximate inference in the model.
We show ex-perimental comparisons to interpolated and mod-ified Kneser-Ney, and the hierarchical Dirichletlanguage model in Section 6 and conclude in Sec-tion 7.2 Pitman-Yor ProcessPitman-Yor processes are examples of nonpara-metric Bayesian models.
Here we give a quick de-scription of the Pitman-Yor process in the contextof a unigram language model; good tutorials onsuch models are provided in (Ghahramani, 2005;Jordan, 2005).
Let W be a fixed and finite vocabu-lary of V words.
For each word w ?
W let G(w)be the (to be estimated) probability of w, and letG = [G(w)]w?W be the vector of word probabili-ties.
We place a Pitman-Yor process prior on G:G ?
PY(d, ?,G0) (2)where the three parameters are: a discount param-eter 0 ?
d < 1, a strength parameter ?
> ?d anda mean vector G0 = [G0(w)]w?W .
G0(w) is thea priori probability of word w: before observingany data, we believe word w should occur withprobability G0(w).
In practice this is usually setuniformly G0(w) = 1/V for all w ?
W .
Both ?and d can be understood as controlling the amountof variability around G0 in different ways.
Whend = 0 the Pitman-Yor process reduces to a Dirich-let distribution with parameters ?G0.There is in general no known analytic form forthe density of PY(d, ?,G0) when the vocabularyis finite.
However this need not deter us as wewill instead work with the distribution over se-quences of words induced by the Pitman-Yor pro-cess, which has a nice tractable form and is suffi-cient for our purpose of language modelling.
Tobe precise, notice that we can treat both G andG0 as distributions over W , where word w ?
Whas probability G(w) (respectively G0(w)).
Letx1, x2, .
.
.
be a sequence of words drawn inde-pendently and identically (i.i.d.)
from G. Weshall describe the Pitman-Yor process in terms ofa generative procedure that produces x1, x2, .
.
.
it-eratively with G marginalized out.
This can beachieved by relating x1, x2, .
.
.
to a separate se-quence of i.i.d.
draws y1, y2, .
.
.
from the meandistribution G0 as follows.
The first word x1 isassigned the value of the first draw y1 from G0.Let t be the current number of draws from G0(currently t = 1), ck be the number of words as-signed the value of draw yk (currently c1 = 1),and c?
=?tk=1 ck be the current number of drawsfrom G. For each subsequent word xc?+1, we ei-ther assign it the value of a previous draw yk withprobability ck?d?+c?
(increment ck; set xc?+1 ?
yk),or we assign it the value of a new draw from G0986100 101 102 103 104 105 106100101102103104105100 101 102 103 104 105 106100101102103104105100 101 102 103 104 105 10600.20.40.60.81100 101 102 103 104 105 10600.20.40.60.81Figure 1: First panel: number of unique words as a function of the number of words drawn on a log-logscale, with d = .5 and ?
= 1 (bottom), 10 (middle) and 100 (top).
Second panel: same, with ?
= 10and d = 0 (bottom), .5 (middle) and .9 (top).
Third panel: proportion of words appearing only once, asa function of the number of words drawn, with d = .5 and ?
= 1 (bottom), 10 (middle), 100 (top).
Lastpanel: same, with ?
= 10 and d = 0 (bottom), .5 (middle) and .9 (top).with probability ?+dt?+c?
(increment t; set ct = 1;draw yt ?
G0; set xc?+1 ?
yt).The above generative procedure produces a se-quence of words drawn i.i.d.
from G, with Gmarginalized out.
It is informative to study thePitman-Yor process in terms of the behaviour itinduces on this sequence of words.
Firstly, no-tice the rich-gets-richer clustering property: themore words have been assigned to a draw fromG0, the more likely subsequent words will be as-signed to the draw.
Secondly, the more we drawfrom G0, the more likely a new word will be as-signed to a new draw from G0.
These two ef-fects together produce a power-law distributionwhere many unique words are observed, most ofthem rarely.
In particular, for a vocabulary of un-bounded size and for d > 0, the number of uniquewords scales as O(?T d) where T is the total num-ber of words.
For d = 0, we have a Dirichlet dis-tribution and the number of unique words growsmore slowly at O(?
log T ).Figure 1 demonstrates the power-law behaviourof the Pitman-Yor process and how this dependson d and ?.
In the first two panels we show theaverage number of unique words among 10 se-quences of T words drawn from G, as a func-tion of T , for various values of ?
and d. Wesee that ?
controls the overall number of uniquewords, while d controls the asymptotic growth ofthe number of unique words.
In the last two pan-els, we show the proportion of words appearingonly once among the unique words; this gives anindication of the proportion of words that occurrarely.
We see that the asymptotic behaviour de-pends on d but not on ?, with larger d?s producingmore rare words.This procedure for generating words drawnfrom G is often referred to as the Chinese restau-rant process (Pitman, 2002).
The metaphor is asfollows.
Consider a sequence of customers (cor-responding to the words draws from G) visiting aChinese restaurant with an unbounded number oftables (corresponding to the draws from G0), eachof which can accommodate an unbounded numberof customers.
The first customer sits at the first ta-ble, and each subsequent customer either joins analready occupied table (assign the word to the cor-responding draw from G0), or sits at a new table(assign the word to a new draw from G0).3 Hierarchical Pitman-Yor LanguageModelsWe describe an n-gram language model based on ahierarchical extension of the Pitman-Yor process.An n-gram language model defines probabilitiesover the current word given various contexts con-sisting of up to n ?
1 words.
Given a context u,let Gu(w) be the probability of the current wordtaking on value w. We use a Pitman-Yor processas the prior for Gu[Gu(w)]w?W , in particular,Gu ?
PY(d|u|, ?|u|, Gpi(u)) (3)where pi(u) is the suffix of u consisting of all butthe earliest word.
The strength and discount pa-rameters are functions of the length |u| of the con-text, while the mean vector is Gpi(u), the vectorof probabilities of the current word given all butthe earliest word in the context.
Since we do notknow Gpi(u) either, We recursively place a priorover Gpi(u) using (3), but now with parameters?|pi(u)|, d|pi(u)| and mean vector Gpi(pi(u)) instead.This is repeated until we get to G?, the vectorof probabilities over the current word given the987empty context ?.
Finally we place a prior on G?:G?
?
PY(d0, ?0, G0) (4)where G0 is the global mean vector, given a uni-form value of G0(w) = 1/V for all w ?
W .
Fi-nally, we place a uniform prior on the discount pa-rameters and a Gamma(1, 1) prior on the strengthparameters.
The total number of parameters in themodel is 2n.The structure of the prior is that of a suffix treeof depth n, where each node corresponds to a con-text consisting of up to n?1 words, and each childcorresponds to adding a different word to the be-ginning of the context.
This choice of the priorstructure expresses our belief that words appearingearlier in a context have (a priori) the least impor-tance in modelling the probability of the currentword, which is why they are dropped first at suc-cessively higher levels of the model.4 Hierarchical Chinese RestaurantProcessesWe describe a generative procedure analogousto the Chinese restaurant process of Section 2for drawing words from the hierarchical Pitman-Yor language model with all Gu?s marginalizedout.
This gives us an alternative representation ofthe hierarchical Pitman-Yor language model thatis amenable to efficient inference using Markovchain Monte Carlo sampling and easy computa-tion of the predictive probabilities for test words.The correspondence between interpolated Kneser-Ney and the hierarchical Pitman-Yor languagemodel is also apparent in this representation.Again we may treat each Gu as a distributionover the current word.
The basic observation isthat since Gu is Pitman-Yor process distributed,we can draw words from it using the Chineserestaurant process given in Section 2.
Further, theonly operation we need of its parent distributionGpi(u) is to draw words from it too.
Since Gpi(u)is itself distributed according to a Pitman-Yor pro-cess, we can use another Chinese restaurant pro-cess to draw words from that.
This is recursivelyapplied until we need draws from the global meandistribution G0, which is easy since it is just uni-form.
We refer to this as the hierarchical Chineserestaurant process.Let us introduce some notations.
For each con-text u we have a sequence of words xu1, xu2, .
.
.drawn i.i.d.
from Gu and another sequence ofwords yu1, yu2, .
.
.
drawn i.i.d.
from the parentdistribution Gpi(u).
We use l to index draws fromGu and k to index the draws from Gpi(u).
Definetuwk = 1 if yuk takes on value w, and tuwk = 0otherwise.
Each word xul is assigned to one ofthe draws yuk from Gpi(u).
If yuk takes on valuew define cuwk as the number of words xul drawnfrom Gu assigned to yuk, otherwise let cuwk = 0.Finally we denote marginal counts by dots.
Forexample, cu?k is the number of xul?s assigned thevalue of yuk, cuw?
is the number of xul?s withvalue w, and tu??
is the current number of drawsyuk from Gpi(u).
Notice that we have the follow-ing relationships among the cuw?
?s and tuw?:{tuw?
= 0 if cuw?
= 0;1 ?
tuw?
?
cuw?
if cuw?
> 0;(5)cuw?
=?u?:pi(u?)=utu?w?
(6)Pseudo-code for drawing words using the hier-archical Chinese restaurant process is given as arecursive function DrawWord(u), while pseudo-code for computing the probability that the nextword drawn from Gu will be w is given inWordProb(u,w).
The counts are initialized at allcuwk = tuwk = 0.Function DrawWord(u):Returns a new word drawn from Gu.If u = 0, return w ?W with probability G0(w).Else with probabilities proportional to:cuwk ?
d|u|tuwk: assign the new word to yuk.Increment cuwk; return w.?|u| + d|u|tu??
: assign the new word to a newdraw yuknew from Gpi(u).Let w ?
DrawWord(pi(u));set tuwknew = cuwknew = 1; return w.Function WordProb(u,w):Returns the probability that the next word aftercontext u will be w.If u = 0, return G0(w).
Else returncuw??d|u|tuw??|u|+cu??
+?|u|+d|u|tu???|u|+cu??
WordProb(pi(u),w).Notice the self-reinforcing property of the hi-erarchical Pitman-Yor language model: the morea word w has been drawn in context u, the morelikely will we draw w again in context u.
In factword w will be reinforced for other contexts thatshare a common suffix with u, with the probabil-ity of drawing w increasing as the length of the988common suffix increases.
This is because w willbe more likely under the context of the commonsuffix as well.The hierarchical Chinese restaurant process isequivalent to the hierarchical Pitman-Yor languagemodel insofar as the distribution induced on wordsdrawn from them are exactly equal.
However, theprobability vectors Gu?s have been marginalizedout in the procedure, replaced instead by the as-signments of words xul to draws yuk from theparent distribution, i.e.
the seating arrangement ofcustomers around tables in the Chinese restaurantprocess corresponding to Gu.
In the next sectionwe derive tractable inference schemes for the hi-erarchical Pitman-Yor language model based onthese seating arrangements.5 Inference SchemesIn this section we give a high level descriptionof a Markov chain Monte Carlo sampling basedinference scheme for the hierarchical Pitman-Yor language model.
Further details can be ob-tained at (Teh, 2006).
We also relate interpolatedKneser-Ney to the hierarchical Pitman-Yor lan-guage model.Our training data D consists of the number ofoccurrences cuw?
of each word w after each con-text u of length exactly n ?
1.
This correspondsto observing word w drawn cuw?
times from Gu.Given the training data D, we are interested inthe posterior distribution over the latent vectorsG = {Gv : all contexts v} and parameters ?
={?m, dm : 0 ?
m ?
n?
1}:p(G,?|D) = p(G,?,D)/p(D) (7)As mentioned previously, the hierarchical Chineserestaurant process marginalizes out each Gu, re-placing it with the seating arrangement in the cor-responding restaurant, which we shall denote bySu.
Let S = {Sv : all contexts v}.
We are thusinterested in the equivalent posterior over seatingarrangements instead:p(S,?|D) = p(S,?,D)/p(D) (8)The most important quantities we need for lan-guage modelling are the predictive probabilities:what is the probability of a test word w after a con-text u?
This is given byp(w|u,D) =?p(w|u,S,?
)p(S,?|D) d(S,?
)(9)where the first probability on the right is the pre-dictive probability under a particular setting ofseating arrangements S and parameters ?, and theoverall predictive probability is obtained by aver-aging this with respect to the posterior over S and?
(second probability on right).
We approximatethe integral with samples {S(i),?
(i)}Ii=1 drawnfrom p(S,?|D):p(w|u,D) ?I?i=1p(w|u,S(i),?
(i)) (10)while p(w|u,S,?)
is given by the functionWordProb(u,w):p(w | 0,S,?)
= 1/V (11)p(w |u,S,?)
= cuw?
?
d|u|tuw?
?|u| + cu?
?+?|u| + d|u|tu??
?|u| + cu?
?p(w |pi(u),S,?)
(12)where the counts are obtained from the seating ar-rangement Su in the Chinese restaurant processcorresponding to Gu.We use Gibbs sampling to obtain the posteriorsamples {S,?}
(Neal, 1993).
Gibbs samplingkeeps track of the current state of each variableof interest in the model, and iteratively resamplesthe state of each variable given the current states ofall other variables.
It can be shown that the statesof variables will converge to the required samplesfrom the posterior distribution after a sufficientnumber of iterations.
Specifically for the hierar-chical Pitman-Yor language model, the variablesconsist of, for each u and each word xul drawnfrom Gu, the index kul of the draw from Gpi(u)assigned xul.
In the Chinese restaurant metaphor,this is the index of the table which the lth customersat at in the restaurant corresponding to Gu.
If xulhas value w, it can only be assigned to draws fromGpi(u) that has value w as well.
This can either bea preexisting draw with value w, or it can be a newdraw taking on value w. The relevant probabili-ties are given in the functions DrawWord(u) andWordProb(u,w), where we treat xul as the lastword drawn from Gu.
This gives:p(kul = k|S?ul,?)
?max(0, c?uluxulk ?
d)?
+ c?ulu??
(13)p(kul = knew with yuknew = xul|S?ul,?)
??
+ dt?ulu???
+ c?ulu??p(xul|pi(u),S?ul,?)
(14)989where the superscript ?ul means the correspond-ing set of variables or counts with xul excluded.The parameters ?
are sampled using an auxiliaryvariable sampler as detailed in (Teh, 2006).
Theoverall sampling scheme for an n-gram hierarchi-cal Pitman-Yor language model takes O(nT ) timeand requires O(M) space per iteration, where T isthe number of words in the training set, and M isthe number of unique n-grams.
During test time,the computational cost is O(nI), since the predic-tive probabilities (12) require O(n) time to calcu-late for each of I samples.The hierarchical Pitman-Yor language modelproduces discounts that grow gradually as a func-tion of n-gram counts.
Notice that although eachPitman-Yor process Gu only has one discount pa-rameter, the predictive probabilities (12) producedifferent discount values since tuw?
can take ondifferent values for different words w. In fact tuw?will on average be larger if cuw?
is larger; averagedover the posterior, the actual amount of discountwill grow slowly as the count cuw?
grows.
Thisis shown in Figure 2 (left), where we see that thegrowth of discounts is sublinear.The correspondence to interpolated Kneser-Neyis now straightforward.
If we restrict tuw?
to be atmost 1, that is,tuw?
= min(1, cuw?)
(15)cuw?
=?u?:pi(u?)=utu?w?
(16)we will get the same discount value so long ascuw?
> 0, i.e.
absolute discounting.
Further sup-posing that the strength parameters are all ?|u| =0, the predictive probabilities (12) now directly re-duces to the predictive probabilities given by inter-polated Kneser-Ney.
Thus we can interpret inter-polated Kneser-Ney as the approximate inferencescheme (15,16) in the hierarchical Pitman-Yor lan-guage model.Modified Kneser-Ney uses the same values forthe counts as in (15,16), but uses a different val-ued discount for each value of cuw?
up to a maxi-mum of c(max).
Since the discounts in a hierarchi-cal Pitman-Yor language model are limited to be-tween 0 and 1, we see that modified Kneser-Ney isnot an approximation of the hierarchical Pitman-Yor language model.6 Experimental ResultsWe performed experiments on the hierarchicalPitman-Yor language model on a 16 million wordcorpus derived from APNews.
This is the samedataset as in (Bengio et al, 2003).
The training,validation and test sets consist of about 14 mil-lion, 1 million and 1 million words respectively,while the vocabulary size is 17964.
For trigramswith n = 3, we varied the training set size betweenapproximately 2 million and 14 million words bysix equal increments, while we also experimentedwith n = 2 and 4 on the full 14 million word train-ing set.
We compared the hierarchical Pitman-Yorlanguage model trained using the proposed Gibbssampler (HPYLM) against interpolated Kneser-Ney (IKN), modified Kneser-Ney (MKN) withmaximum discount cut-off c(max) = 3 as recom-mended in (Chen and Goodman, 1998), and thehierarchical Dirichlet language model (HDLM).For the various variants of Kneser-Ney, we firstdetermined the parameters by conjugate gradientdescent in the cross-entropy on the validation set.At the optimal values, we folded the validationset into the training set to obtain the final n-gramprobability estimates.
This procedure is as recom-mended in (Chen and Goodman, 1998), and takesapproximately 10 minutes on the full training setwith n = 3 on a 1.4 Ghz PIII.
For HPYLM weinferred the posterior distribution over the latentvariables and parameters given both the trainingand validation sets using the proposed Gibbs sam-pler.
Since the posterior is well-behaved and thesampler converges quickly, we only used 125 it-erations for burn-in, and 175 iterations to collectposterior samples.
On the full training set withn = 3 this took about 1.5 hours.Perplexities on the test set are given in Table 1.As expected, HDLM gives the worst performance,while HPYLM performs better than IKN.
Perhapssurprisingly HPYLM performs slightly worse thanMKN.
We believe this is because HPYLM is not aperfect model for languages and as a result poste-rior estimates of the parameters are not optimizedfor predictive performance.
On the other handparameters in the Kneser-Ney variants are opti-mized using cross-validation, so are given opti-mal values for prediction.
To validate this con-jecture, we also experimented with HPYCV, a hi-erarchical Pitman-Yor language model where theparameters are obtained by fitting them in a slightgeneralization of IKN where the strength param-990T n IKN MKN HPYLM HPYCV HDLM2e6 3 148.8 144.1 145.7 144.3 191.24e6 3 137.1 132.7 134.3 132.7 172.76e6 3 130.6 126.7 127.9 126.4 162.38e6 3 125.9 122.3 123.2 121.9 154.710e6 3 122.0 118.6 119.4 118.2 148.712e6 3 119.0 115.8 116.5 115.4 144.014e6 3 116.7 113.6 114.3 113.2 140.514e6 2 169.9 169.2 169.6 169.3 180.614e6 4 106.1 102.4 103.8 101.9 136.6Table 1: Perplexities of various methods and forvarious sizes of training set T and length of n-grams.eters ?|u|?s are allowed to be positive and opti-mized over along with the discount parametersusing cross-validation.
Seating arrangements areGibbs sampled as in Section 5 with the parame-ter values fixed.
We find that HPYCV performsbetter than MKN (except marginally worse onsmall problems), and has best performance over-all.
Note that the parameter values in HPYCV arestill not the optimal ones since they are obtainedby cross-validation using IKN, an approximationto a hierarchical Pitman-Yor language model.
Un-fortunately cross-validation using a hierarchicalPitman-Yor language model inferred using Gibbssampling is currently too costly to be practical.In Figure 2 (right) we broke down the contribu-tions to the cross-entropies in terms of how manytimes each word appears in the test set.
We seethat most of the differences between the methodsappear as differences among rare words, with thecontribution of more common words being neg-ligible.
HPYLM performs worse than MKN onwords that occurred only once (on average) andbetter on other words, while HPYCV is reversedand performs better than MKN on words that oc-curred only once or twice and worse on otherwords.7 DiscussionWe have described using a hierarchical Pitman-Yor process as a language model and shown thatit gives performance superior to state-of-the-artmethods.
In addition, we have shown that thestate-of-the-art method of interpolated Kneser-Ney can be interpreted as approximate inferencein the hierarchical Pitman-Yor language model.In the future we plan to study in more detailthe differences between our model and the vari-ants of Kneser-Ney, to consider other approximateinference schemes, and to test the model on largerdata sets and on speech recognition.
The hierarchi-cal Pitman-Yor language model is a fully Bayesianmodel, thus we can also reap other benefits of theparadigm, including having a coherent probabilis-tic model, ease of improvements by building inprior knowledge, and ease in using as part of morecomplex models; we plan to look into these possi-ble improvements and extensions.The hierarchical Dirichlet language model of(MacKay and Peto, 1994) was an inspiration forour work.
Though (MacKay and Peto, 1994) hadthe right intuition to look at smoothing techniquesas the outcome of hierarchical Bayesian models,the use of the Dirichlet distribution as a prior wasshown to lead to non-competitive cross-entropy re-sults.
Our model is a nontrivial but direct gen-eralization of the hierarchical Dirichlet languagemodel that gives state-of-the-art performance.
Wehave shown that with a suitable choice of priors(namely the Pitman-Yor process), Bayesian meth-ods can be competitive with the best smoothingtechniques.The hierarchical Pitman-Yor process is a naturalgeneralization of the recently proposed hierarchi-cal Dirichlet process (Teh et al, 2006).
The hier-archical Dirichlet process was proposed to solvea different problem?that of clustering, and it isinteresting to note that such a direct generaliza-tion leads us to a good language model.
Both thehierarchical Dirichlet process and the hierarchi-cal Pitman-Yor process are examples of Bayesiannonparametric processes.
These have recently re-ceived much attention in the statistics and ma-chine learning communities because they can re-lax previously strong assumptions on the paramet-ric forms of Bayesian models yet retain computa-tional efficiency, and because of the elegant wayin which they handle the issues of model selectionand structure learning in graphical models.AcknowledgementI wish to thank the Lee Kuan Yew EndowmentFund for funding, Joshua Goodman for answer-ing many questions regarding interpolated Kneser-Ney and smoothing techniques, John Blitzer andYoshua Bengio for help with datasets, AnoopSarkar for interesting discussion, and Hal DaumeIII, Min Yen Kan and the anonymous reviewers for9910 10 20 30 40 500123456Count of n?gramsAverageDiscountIKNMKNHPYLM2 4 6 8 10?0.01?0.00500.0050.010.0150.020.0250.03Cross?EntropyDifferences fromMKNCount of words in test setIKNMKNHPYLMHPYCVFigure 2: Left: Average discounts as a function of n-gram counts in IKN (bottom line), MKN (middlestep function), and HPYLM (top curve).
Right: Break down of cross-entropy on test set as a functionof the number of occurrences of test words.
Plotted is the sum over test words which occurred c timesof cross-entropies of IKN, MKN, HPYLM and HPYCV, where c is as given on the x-axis and MKN isused as a baseline.
Lower is better.
Both panels are for the full training set and n = 3.helpful comments.ReferencesY.
Bengio, R. Ducharme, P. Vincent, and C. Jauvin.2003.
A neural probabilistic language model.
Jour-nal of Machine Learning Research, 3:1137?1155.S.F.
Chen and J.T Goodman.
1998.
An empiricalstudy of smoothing techniques for language model-ing.
Technical Report TR-10-98, Computer ScienceGroup, Harvard University.A.
Gelman, J. Carlin, H. Stern, and D. Rubin.
1995.Bayesian data analysis.
Chapman & Hall, London.Z.
Ghahramani.
2005.
Nonparametric Bayesian meth-ods.
Tutorial presentation at the UAI Conference.S.
Goldwater, T.L.
Griffiths, and M. Johnson.
2006.Interpolating between types and tokens by estimat-ing power-law generators.
In Advances in NeuralInformation Processing Systems, volume 18.J.T.
Goodman.
2001.
A bit of progress in languagemodeling.
Technical Report MSR-TR-2001-72, Mi-crosoft Research.J.T.
Goodman.
2004.
Exponential priors for maximumentropy models.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguis-tics.H.
Ishwaran and L.F. James.
2001.
Gibbs samplingmethods for stick-breaking priors.
Journal of theAmerican Statistical Association, 96(453):161?173.M.I.
Jordan.
2005.
Dirichlet processes, Chineserestaurant processes and all that.
Tutorial presen-tation at the NIPS Conference.R.
Kneser and H. Ney.
1995.
Improved backing-off for m-gram language modeling.
In Proceedingsof the IEEE International Conference on Acoustics,Speech and Signal Processing, volume 1.D.J.C.
MacKay and L.C.B.
Peto.
1994.
A hierarchicalDirichlet language model.
Natural Language Engi-neering.A.
Nadas.
1984.
Estimation of probabilities in the lan-guage model of the IBM speach recognition system.IEEE Transaction on Acoustics, Speech and SignalProcessing, 32(4):859?861.R.M.
Neal.
1993.
Probabilistic inference usingMarkov chain Monte Carlo methods.
Technical Re-port CRG-TR-93-1, Department of Computer Sci-ence, University of Toronto.J.
Pitman and M. Yor.
1997.
The two-parameterPoisson-Dirichlet distribution derived from a stablesubordinator.
Annals of Probability, 25:855?900.J.
Pitman.
2002.
Combinatorial stochastic processes.Technical Report 621, Department of Statistics, Uni-versity of California at Berkeley.
Lecture notes forSt.
Flour Summer School.R.
Rosenfeld.
2000.
Two decades of statistical lan-guage modeling: Where do we go from here?
Pro-ceedings of the IEEE, 88(8).Y.W.
Teh, M.I.
Jordan, M.J. Beal, and D.M.
Blei.
2006.Hierarchical Dirichlet processes.
To appear in Jour-nal of the American Statistical Association.Y.
W. Teh.
2006.
A Bayesian interpretation of in-terpolated Kneser-Ney.
Technical Report TRA2/06,School of Computing, National University of Singa-pore.992
