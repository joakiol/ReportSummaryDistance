Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 174?179,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsDependency-based Convolutional Neural Networksfor Sentence Embedding?Mingbo Ma?Liang Huang?
?
?Graduate Center & Queens CollegeCity University of New York{mma2,lhuang}gc.cuny.eduBing Xiang?Bowen Zhou?
?IBM Watson GroupT.
J. Watson Research Center, IBM{lhuang,bingxia,zhou}@us.ibm.comAbstractIn sentence modeling and classification,convolutional neural network approacheshave recently achieved state-of-the-art re-sults, but all such efforts process word vec-tors sequentially and neglect long-distancedependencies.
To combine deep learn-ing with linguistic structures, we pro-pose a dependency-based convolution ap-proach, making use of tree-based n-gramsrather than surface ones, thus utlizing non-local interactions between words.
Ourmodel improves sequential baselines on allfour sentiment and question classificationtasks, and achieves the highest publishedaccuracy on TREC.1 IntroductionConvolutional neural networks (CNNs), originallyinvented in computer vision (LeCun et al, 1995),has recently attracted much attention in naturallanguage processing (NLP) on problems such assequence labeling (Collobert et al, 2011), seman-tic parsing (Yih et al, 2014), and search queryretrieval (Shen et al, 2014).
In particular, recentwork on CNN-based sentence modeling (Kalch-brenner et al, 2014; Kim, 2014) has achieved ex-cellent, often state-of-the-art, results on variousclassification tasks such as sentiment, subjectivity,and question-type classification.
However, despitetheir celebrated success, there remains a majorlimitation from the linguistics perspective: CNNs,being invented on pixel matrices in image process-ing, only consider sequential n-grams that are con-secutive on the surface string and neglect long-distance dependencies, while the latter play an im-portant role in many linguistic phenomena such asnegation, subordination, and wh-extraction, all ofwhich might dully affect the sentiment, subjectiv-ity, or other categorization of the sentence.
?This work was done at both IBM and CUNY, and was supported inpart by DARPA FA8750-13-2-0041 (DEFT), and NSF IIS-1449278.
We thankYoon Kim for sharing his code, and James Cross and Kai Zhao for discussions.Indeed, in the sentiment analysis literature, re-searchers have incorporated long-distance infor-mation from syntactic parse trees, but the resultsare somewhat inconsistent: some reported smallimprovements (Gamon, 2004; Matsumoto et al,2005), while some otherwise (Dave et al, 2003;Kudo and Matsumoto, 2004).
As a result, syn-tactic features have yet to become popular in thesentiment analysis community.
We suspect oneof the reasons for this is data sparsity (accordingto our experiments, tree n-grams are significantlysparser than surface n-grams), but this problemhas largely been alleviated by the recent advancesin word embedding.
Can we combine the advan-tages of both worlds?So we propose a very simple dependency-basedconvolutional neural networks (DCNNs).
Ourmodel is similar to Kim (2014), but while his se-quential CNNs put a word in its sequential con-text, ours considers a word and its parent, grand-parent, great-grand-parent, and siblings on the de-pendency tree.
This way we incorporate long-distance information that are otherwise unavail-able on the surface string.Experiments on three classification tasksdemonstrate the superior performance of ourDCNNs over the baseline sequential CNNs.
Inparticular, our accuracy on the TREC datasetoutperforms all previously published resultsin the literature, including those with heavyhand-engineered features.Independently of this work, Mou et al (2015,unpublished) reported related efforts; see Sec.
3.3.2 Dependency-based ConvolutionThe original CNN, first proposed by LeCun etal.
(1995), applies convolution kernels on a se-ries of continuous areas of given images, and wasadapted to NLP by Collobert et al (2011).
Fol-lowing Kim (2014), one dimensional convolutionoperates the convolution kernel in sequential orderin Equation 1, where xi?
Rdrepresents the d di-mensional word representation for the i-th word in174Despite the film ?s shortcomings the stories are quietly moving .ROOTFigure 1: Running example from Movie Reviews dataset.mensional word representation for the i-th word inthe sentence, and ?
is the concatenation operator.Therefore?xi,jrefers to concatenated word vectorfrom the i-th word to the (i+ j)-th word:?xi,j= xi?
xi+1?
?
?
?
?
xi+j(1)Sequential word concatenation?xi,jworks asn-gram models which feeds local information intoconvolution operations.
However, this setting cannot capture long-distance relationships unless weenlarge the window indefinitely which would in-evitably cause the data sparsity problem.In order to capture the long-distance dependen-cies we propose the dependency tree-based con-volution model (DTCNN).
Figure 1 illustrates anexample from the Movie Reviews (MR) dataset(Pang and Lee, 2005).
The sentiment of this sen-tence is obviously positive, but this is quite dif-ficult for sequential CNNs because many n-gramwindows would include the highly negative word?shortcomings?, and the distance between ?De-spite?
and ?shortcomings?
is quite long.
DTCNN,however, could capture the tree-based bigram?Despite ?
shortcomings?, thus flipping the senti-ment, and the tree-based trigram ?ROOT ?
moving?
stories?, which is highly positive.2.1 Convolution on Ancestor PathsWe define our concatenation based on the depen-dency tree for a given modifier xi:xi,k= xi?
xp(i)?
?
?
?
?
xpk?1(i)(2)where function pk(i) returns the i-th word?s k-thancestor index, which is recursively defined as:pk(i) ={p(pk?1(i)) if k > 0i if k = 0(3)Figure 2 (left) illustrates ancestor paths patternswith various orders.
We always start the convo-lution with xiand concatenate with its ancestors.If the root node is reached, we add ?ROOT?
asdummy ancestors (vertical padding).For a given tree-based concatenated word se-quence xi,k, the convolution operation applies afilter w ?
Rk?dto xi,kwith a bias term b de-scribed in equation 4:ci= f(w ?
xi,k+ b) (4)where f is a non-linear activation function such asrectified linear unit (ReLu) or sigmoid function.The filter w is applied to each word in the sen-tence, generating the feature map c ?
Rl:c = [c1, c2, ?
?
?
, cl] (5)where l is the length of the sentence.2.2 Max-Over-Tree Pooling and DropoutThe filters convolve with different word concate-nation in Eq.
4 can be regarded as pattern detec-tion: only the most similar pattern between thewords and the filter could return the maximum ac-tivation.
In sequential CNNs, max-over-time pool-ing (Collobert et al, 2011; Kim, 2014) operatesover the feature map to get the maximum acti-vation c?
= max c representing the entire featuremap.
Our DTCNNs also pool the maximum ac-tivation from feature map to detect the strongestactivation over the whole tree (i.e., over the wholesentence).
Since the tree no longer defines a se-quential ?time?
direction, we refer to our poolingas ?max-over-tree?
pooling.In order to capture enough variations, we ran-domly initialize the set of filters to detect differentstructure patterns.
Each filter?s height is the num-ber of words considered and the width is alwaysequal to the dimensionality d of word representa-tion.
Each filter will be represented by only onefeature after max-over-tree pooling.
After a seriesof convolution with different filter with differentheights, multiple features carry different structuralinformation become the final representation of theinput sentence.
Then, this sentence representationis passed to a fully connected soft-max layer andoutputs a distribution over different labels.Neural networks often suffer from overtrain-ing.
Following Kim (2014), we employ randomdropout on penultimate layer (Hinton et al, 2012).in order to prevent co-adaptation of hidden units.In our experiments, we set our drop out rate as 0.5and learning rate as 0.95 by default.
FollowingKim (2014), training is done through stochasticgradient descent over shuffled mini-batches withthe Adadelta update rule (Zeiler, 2012).2.3 Convolution on SiblingsAncestor paths alone is not enough to capturemany linguistic phenomena such as conjunction.Figure 1: Dependency tree of an example sentence from the Movie Reviews dataset.the sentence, and ?
is the concatenation operator.Therefore?xi,jrefers to concatenated word vectorfrom the i-th word to the (i+ j)-th word:?xi,j= xi?
xi+1?
?
?
?
?
xi+j(1)Sequential word concat nati n?xi,jworks asn-gram models which feeds local information intoconvolution operations.
However, this setting cannot capture long-distance relationships unless weenlarge the window indefinitely which would in-evitably caus the data sparsity problem.In order t capture the long-distance dependen-cies we propose the dependency-based convolu-tion model (DCNN).
Figure 1 illustrates an exam-ple from the Movie Reviews (MR) dataset (Pangand Lee, 2005).
The sentiment of this sentenceis obviously positive, but is i quite difficult forsequential CNNs because many n-gram windowswould include the highly negative word ?short-comings?, and the distance between ?Despite?
and?shortcomings?
is quite long.
DCNN, however,could capture the tree-based bigram ?Despite ?shortcomings?, thus flipping the sentiment, andthe tree-based trigram ?ROOT ?
moving ?
sto-ries?, which is highly positive.2.1 Convolution on Ancestor PathsWe define our concatenation based on the depen-dency tree for a given modifier xi:xi,k= xi?
xp(i)?
?
?
?
?
xpk?1(i)(2)where function pk(i) returns the i-th word?s k-thancestor index, which is recursively defined as:pk(i) ={p(pk?1(i)) if k > 0i if k = 0(3)Figure 2 (left) illustrates ancestor paths patternswith various orders.
We always start the convo-lution with xiand concatenate with its ancestors.If the root node is reached, we add ?ROOT?
asdummy ancestors (vertical padding).For a given tree-based concatenated word se-quence xi,k, the convolution operation applies afilter w ?
Rk?dto xi,kwith a bias term b de-scribed in equation 4:ci= f(w ?
xi,k+ b) (4)where f is a non-linear activation function such asrectified linear unit (ReLu) or sigmoid function.The filter w is applied to each word in the sen-tence, generating the feature map c ?
Rl:c = [c1, c2, ?
?
?
, cl] (5)where l is the length of the sentence.2.2 Max-Over-Tree Pooling and DropoutThe filters convolve with different word concate-nation in Eq.
4 can be regarded as pattern detec-tion: only the most similar pattern between thewords and the filter could return the maximum ac-tivati .
In sequential CNNs max-over-time pool-i g (Collobert et al, 2011; Kim, 2014) operatesover the feature map to get the maximum acti-vation c?
= max c representing the entire featuremap.
Our DCNNs also pool the maximum activa-tion from feature map to detect the strongest ac-tivation over the whole tree ( .e., over the wholesentence).
Since the tree no longer defines a se-quential ?time?
direction, we refer to our poolingas ?max-over-tree?
pooling.In order to capture enough variations, we ran-d mly initialize the set of filters to detect differentstructure patterns.
Each filter?s height is the num-ber of words considered and the width is alwaysequal to the dimensionality d of word representa-tion.
Each filter will be represented by only onefeature after max-over-tree pooling.
After a seriesof conv lutio with different filter with differentheights, multiple features carry different structuralinformation become the final representation of theinput sentence.
Then, this sentence representationis passed to a fully connected soft-max layer andoutputs a distribution over different labels.Neural networks often suffer from overtrain-ing.
Following Ki (2014), we empl y randomdropout on penultimate layer (Hinton et al, 2014).in order to prevent co-adaptation of hidden units.In our experiments, we set our drop out rate as 0.5and learning rate as 0.95 by default.
FollowingKim (2014), training is done through stochasticgradient descent over shuffled mini-batches withthe Adadelta update rule (Zeiler, 2012).175ancestor paths siblingsn pattern(s) n pattern(s)3m h g2s m4m h g g23s m h t s m5m h g g2g34t s m h s m h gTable 1: Tree-based convolution patterns.
Word concatenation always starts with m, while h, g, and g2denote parent, grand parent, and great-grand parent, etc., and ?
?
denotes words excluded in convolution.2.3 Convolution on SiblingsAncestor paths alone is not enough to capturemany linguistic phenomena such as conjunction.Inspired by higher-order dependency parsing (Mc-Donald and Pereira, 2006; Koo and Collins, 2010),we also incorporate siblings for a given word invarious ways.
See Table 1 (right) for details.2.4 Combined ModelPowerful as it is, structural information still doesnot fully cover sequential information.
Also, pars-ing errors (which are common especially for in-formal text such as online reviews) directly affectDTCNN performance while sequential n-gramsare always correctly observed.
To best exploitboth information, we want to combine both mod-els.
The easiest way of combination is to con-catenate these representations together, then feedinto fully connected soft-max neural networks.
Inthese cases, combine with different feature fromdifferent type of sources could stabilize the perfor-mance.
The final sentence representation is thus:?c = [c?
(1)a, ..., c?(Na)a?
??
?ancestors; c?
(1)s, ..., c?(Ns)s?
??
?siblings; c?
(1), ..., c?(N)?
??
?sequential]where Na, Ns, and N are the number of ancestor,sibling, and sequential filters.
In practice, we use100 filters for each template in Table 1.
The fullycombined representation is 1100-dimensional bycontrast to 300-dimensional for sequential CNN.3 ExperimentsWe implement our DTCNN on top of the opensource CNN code by Kim (2014).1Table 2summarizes our results in the context of otherhigh-performing efforts in the literature.
We usethree benchmark datasets in two categories: senti-ment analysis on both Movie Review (MR) (Pangand Lee, 2005) and Stanford Sentiment Treebank1https://github.com/yoonkim/CNN sentence(SST-1) (Socher et al, 2013) datasets, and ques-tion classification on TREC (Li and Roth, 2002).For all datasets, we first obtain the dependencyparse tree from Stanford parser (Manning et al,2014).2Different window size for different choiceof convolution are shown in Table 1.
For thedataset without a development set (MR), we ran-domly choose 10% of the training data to indicateearly stopping.
In order to have a fare compari-son with baseline CNN, we also use 3 to 5 as ourwindow size.
Most of our results are generatedby GPU due to its efficiency, however CPU poten-tially could generate better results.3Our imple-mentation can be found on Github.43.1 Sentiment AnalysisBoth sentiment analysis datasets (MR and SST-1) are based on movie reviews.
The differencesbetween them are mainly in the different num-bers of categories and whether the standard splitis given.
There are 10,662 sentences in the MRdataset.
Each instance is labeled positive or neg-ative, and in most cases contains one sentence.Since no standard data split is given, following theliterature we use 10 fold cross validation to includeevery sentence in training and testing at least once.Concatenating with sibling and sequential infor-mation obviously improves tree-based CNNs, andthe final model outperforms the baseline sequen-tial CNNs by 0.4, and ties with Zhu et al (2015).Different from MR, the Stanford SentimentTreebank (SST-1) annotates finer-grained labels,very positive, positive, neutral, negative and verynegative, on an extension of the MR dataset.
Thereare 11,855 sentences with standard split.
Ourmodel achieves an accuracy of 49.5 which is sec-ond only to Irsoy and Cardie (2014).
We set batchsize to 100 for this task.2The phrase-structure trees in SST-1 are actually automat-ically parsed, and thus can not be used as gold-standard trees.3GPU only supports float32 while CPU supports float64.4https://github.com/cosmmb/DTCNNFigure 2: Convolution patterns on trees.
Word concatenation always starts with m, while h, g, and g2denote parent, grand parent, and great-grand parent, etc., and ?
?
denotes words excluded in convolution.2.3 Convolution on SiblingsAncestor paths alone is not enough to capturemany linguistic phenomena such as conjunction.Inspired by igher-order dependency parsing ( c-Don ld and Pe eira, 2006; Koo and Collins, 2010),we also incorporate siblings for a given word invarious ways.
See Figure 2 (right) for details.2.4 Combined ModelPowerful as it is, structural information still doesnot fully cover sequential information.
Also, pars-ing errors (which are common especially for in-formal text such as online reviews) directly affectDCNN performance while sequential n-grams arealways correctly observed.
To best exploit both in-formation, we want to combine both models.
Theeasiest way of combination is to concatenate theserepresentations together, then feed into fully con-nected soft-max neural networks.
In these cases,combine with different feature from different typeof sources could stabilize the performance.
Thefinal sent nce rep esentation is thus:?c = [c?
(1)a, ..., c?(Na)a?
??
?an estors; c?
(1)s, ..., c?(Ns)s?
??
?siblings; c?
(1), ..., c?(N)?
??
?sequential]where Na, Ns, and N are the number of ancestor,sibling, and sequential filters.
In practice, we use100 filters for each template in Figure 2 .
The fullycombined representation is 1,100-dimensional bycontrast to 300-dimensional for sequential CNN.3 ExperimentsTable 1 summarizes results in the context of otherhigh-performing efforts in the literature.
We usethree benchmark datasets in two categories: senti-m nt analysis on both Movie Review (MR) (Pangand Le , 2005) and Stanford Sentiment Treebank(SST-1) (Socher et al, 2013) datasets, and ques-tion classification on TREC (Li and Roth, 2002).For all datasets, we first obtain the dependencyparse tree from Stanford parser (Manning et al,2014).1Different window ize for different choiceof convolution are shown in Figure 2.
For thedataset without a development set (MR), we ran-domly choose 10% of t e training data to indicateearly stopping.
In order to have a fare compari-son with baseline CNN, we also use 3 to 5 as ourwindow size.
Most of our results are generated byGPU due to its efficiency, however CPU could po-tentially get better results.2Our implementation,on top of Kim (2014)?s code,3will be released.43.1 Sentiment AnalysisBoth sentiment analysis datasets (MR and SST-1) are based on movie reviews.
The differencesbetween them are mainly in the different num-bers of categories and whether the standard splitis given.
There are 10,662 sentences in the MRdataset.
Each instance is labeled positive or neg-ative, and in most cases contains one sentence.Since no standard data split is given, following theliterature we use 10 fold cross validation to includeevery se tence in training and testing at least o c .Concatenating with sibling and sequential infor-mation obviously improves DCNNs, nd the finalmodel outperforms the baseline sequential CNNsby 0.4, and ties with Zhu et al (2015).Different from MR, the Stanford SentimentTreebank (SST-1) annotates finer-grained labels,very positive, positive, neutral, negative and verynegative, on an extension of the MR dataset.
Thereare 11,855 sentences with standard split.
Ourmodel achieves an accuracy of 49.5 which is sec-ond only to Irsoy and Cardie (2014).1The phrase-structure trees in SST-1 are actually automatically parsed,and thus can not be used as gold-standard trees.2GPU only supports float32 while CPU supports float64.3https://github.comw/yoonkim/CNN_sentence4https://github.com/cosmmb/DCNN176Category Model MR SST-1 TREC TREC-2This workDCNNs: ancestor 80.4?47.7?95.4?88.4?DCNNs: ancestor+sibling 81.7?48.3?95.6?89.0?DCNNs: ancestor+sibling+sequential 81.9 49.5 95.4?88.8?CNNsCNNs-non-static (Kim, 2014) ?
baseline 81.5 48.0 93.6 86.4?CNNs-multichannel (Kim, 2014) 81.1 47.4 92.2 86.0?Deep CNNs (Kalchbrenner et al, 2014) - 48.5 93.0 -Recursive NNsRecursive Autoencoder (Socher et al, 2011) 77.7 43.2 - -Recursive Neural Tensor (Socher et al, 2013) - 45.7 - -Deep Recursive NNs (Irsoy and Cardie, 2014) - 49.8 - -Recurrent NNs LSTM on tree (Zhu et al, 2015) 81.9 48.0 - -Other deep learning Paragraph-Vec (Le and Mikolov, 2014) - 48.7 - -Hand-coded rules SVMS(Silva et al, 2011) - 95.0 90.8Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets.TREC-2 is TREC with fine grained labels.
?Results generated by GPU (all others generated by CPU).
?Results generated from Kim (2014)?s implementation.3.2 Question ClassificationIn the TREC dataset, the entire dataset of 5,952sentences are classified into the following 6 cate-gories: abbreviation, entity, description, locationand numeric.
In this experiment, DCNNs easilyoutperform any other methods even with ancestorconvolution only.
DCNNs with sibling achieve thebest performance in the published literature.
DC-NNs combined with sibling and sequential infor-mation might suffer from overfitting on the train-ing data based on our observation.
One thingto note here is that our best result even exceedsSVMS(Silva et al, 2011) with 60 hand-codedrules.The TREC dataset alo provides subcategoriessuch as numeric:temperature, numeric:distance,and entity:vehicle.
To make our task more real-istic and challenging, we also test the proposedmodel with respect to the 50 subcategories.
Thereare obvious improvements over sequential CNNsfrom the last column of Table 1.
Like ours, Silvaet al (2011) is a tree-based system but it usesconstituency trees compared to ours dependencytrees.
They report a higher fine-grained accuracyof 90.8 but their parser is trained only on the Ques-tionBank (Judge et al, 2006) while we used thestandard Stanford parser trained on both the PennTreebank and QuestionBank.
Moreover, as men-tioned above, their approach is rule-based whileours is automatically learned.3.3 Discussions and ExamplesCompared with sentiment analysis, the advantageof our proposed model is obviously more substan-tial on the TREC dataset.
Based on our error anal-ysis, we conclude that this is mainly due to theCategory Model MR SST-1 TREC TREC-2This workDTCNNs: ancestor 80.4?47.7?95.4?88.4?DTCNNs: ancestor+sibling 81.7?48.3?95.6?89.0?DTCNNs: ancestor+sibling+sequential 81.9 49.5 95.4?88.8?CNNsCNNs-non-static (Kim, 2014) ?
baseline 81.5 48.0 93.6 86.4?CNNs-multichannel (Kim, 2014) 81.1 47.4 92.2 86.0?Deep CNNs (Kalchbrenner et al, 2014) - 48.5 93.0 -Recursive NNsRecursive Autoencoder (Socher et al, 2011) 77.7 43.2 - -Recursive Neural Tensor (Socher et al, 2013) - 45.7 - -Deep Recursive NNs (Irsoy and Cardie, 2014) - 49.8 - -Recurrent NNs LSTM on tree (Zhu et al, 2015) 81.9 48.0 - -Other deep learning Paragraph-Vec (Le and Mikolov, 2014) - 48.7 - -Hand-coded rules SVMS(Silva et al, 2011) - 95.0 90.8Table 2: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets.TREC-2 is TREC with fine grained labels.
?Results generated by GPU (all others generated by CPU).
?Results generated from Kim (2014)?s implementation.What is Hawaii ?s state flower ?root(a) enty?
locWhat is natural gas composed of ?root(b) enty?
descWhat does a defibrillator do ?root(c) desc?
entyNothing plot wise is worth emailing home aboutroot(d) mild negative?
mild positiveWhat is the temperature at the center of the earth ?root(e) NUM:temp?
NUM:distWhat were Christopher Columbus ?
three ships ?root(f) ENTY:veh?
LOC:otherFigure 2: Examples from TREC (a?c), SST-1 (d)and TREC with fine-grained label (e?f) that aremisclassified by the baseline CNN but correctlylabeled by our DTCNN.
For example, (a) shouldbe entity but is labeled location by CNN.3.2 Question ClassificationIn the TREC dataset, the entire dataset of 5,952sentences are classified into the following 6 cate-gories: abbreviation, entity, description, locationand numeric.
In this experiment, DTCNNs eas-ily outperform any other methods even with an-cestor convolution only.
DTCNNs with siblingachieve the best performance in the published lit-erature.
DTCNNs combined with sibling and se-quential information might suffer from overfittingon the training data based on our observation.
Onething to note here is that our best result even ex-ceeds SVMS(Silva et al, 2011) with 60 hand-coded rules.
We set batch size to 210 for this task.The TREC dataset alo provides subcategoriessuch as numeric:temperature, numeric:distance,and entity:vehicle.
To make our task more real-istic and challenging, we also test the proposedmodel with respect to the 50 subcategories.
Thereare obvious improvements over sequential CNNsfrom the last column of Table 2.
Like ours, Silvaet al (2011) is a tree-based system but it usesconstituency trees compared to ours dependencytrees.
They report a higher fine-grained accuracyof 90.8 but their parser is trained only on the Ques-tionBank (Judge et al, 2006) while we used thestandard Stanford parser trained on both the PennTreebank and QuestionBank.
Moreover, as men-tioned above, their approach is rule-based whileours is automatically learned.
For this task, we setbatch size to 30.3.3 Discussions and ExamplesCompared with sentiment analysis, the advantageof our proposed model is obviously more substan-tial on the TREC dataset.
Based on our error anal-Figure 3: Examples from TREC (a?c), SST-1 (d)and TREC with fine-grained label (e?f) that aremisclassified by the baseline CNN but correctlylabel d b our DCNN.
For example, (a) should beentity but is labeled location by CNN.177What is the speed hummingbirds fly ?
(noun)root(a) num?
entyWhat body of water are the Canary Islands in ?root(b) loc?
entyWhat position did Willie Davis play in baseball ?root(c) hum?
entyFigure 3: Examples from TREC datasets that aremisclassified by DTCNN but correctly labeled bybaseline CNN.
For example, (a) should be numer-ical but is labeled entity by DTCNN.ysis, we conclude that this is mainly due to thedifference of the parse tree quality between thetwo tasks.
In sentiment analysis, the dataset iscollected from the Rotten Tomatoes website whichincludes many irregular usage of language.
Someof the sentences even come from languages otherthan English.
The errors in parse trees inevitablyaffect the classification accuracy.
However, theparser works substantially better on the TRECdataset since all questions are in formal writtenEnglish, and the training set for Stanford parser5already includes the QuestionBank (Judge et al,2006) which includes 2,000 TREC sentences.Figure 2 visualizes examples where CNN errswhile DTCNN does not.
For example, CNN la-bels (a) as location due to ?Hawaii?
and ?state?,while the long-distance backbone ?What ?
flower?is clearly asking for an entity.
Similarly, in (d),DTCNN captures the obviously negative tree-based trigram ?Nothing ?
worth ?
emailing?.
Notethat our model also works with non-projective de-pendency trees such as the one in (b).
The lasttwo examples in Figure 2 visualize cases whereDTCNN outperforms the baseline CNNs in fine-grained TREC.
In example (e), the word ?temper-ature?
is at second from the top and is root of a8 word span ?the ... earth?.
When we use a win-dow of size 5 for tree convolution, every wordsin that span get convolved with ?temperature?
andthis should be the reason why DTCNN get correct.5http://nlp.stanford.edu/software/parser-faq.shtmlWhat is the melting point of copper ?root(a) num?
enty and descWhat did Jesse Jackson organize ?root(b) hum?
enty and entyWhat is the electrical output in Madrid , Spain ?root(c) enty?
num and numFigure 4: Examples from TREC datasets that aremisclassified by both DTCNN and baseline CNN.For example, (a) should be numerical but is la-beled entity by DTCNN and description by CNN.Figure 3 showcases examples where baselineCNNs get better results than DTCNNs.
Exam-ple (a) is misclassified as entity by DTCNN dueto parsing/tagging error (the Stanford parser per-forms its own part-of-speech tagging).
The word?fly?
at the end of the sentence should be a verbinstead of noun, and ?hummingbirds fly?
shouldbe a relative clause modifying ?speed?.There are some sentences that are misclassifiedby both the baseline CNN and DTCNN.
Figure 4shows three such examples.
Example (a) is notclassified as numerical by both methods due to theambiguous meaning of the word ?point?
which isdifficult to capture by word embedding.
This wordcan mean location, opinion, etc.
Apparently, thenumerical aspect is not captured by word embed-ding.
Example (c) might be an annotation error.From the mistakes made by DTCNNs, we findthe performance of DTCNN is mainly limited bytwo factors: the accuracy of the parser and thequality of word embedding.
Future work will fo-cus on these two issues.4 Conclusions and Future WorkWe have presented a very simple dependency tree-based convolution framework which outperformssequential CNN baselines on various classificationtasks.
Extensions of this model would considerdependency labels and constituency trees.
Also,we would evaluate on gold-standard parse trees.Figure 4: Examples from TREC datasets that aremisclassifi d by DCNN but correctly labeled bybaseline CNN.
For example, (a) should be numer-ical but is labeled entity by DCNN.difference of the pars tree quality between thtwo tasks.
In sentiment analysis, the dataset iscollected from the Rotten Tomatoes website whichincludes many irregular usage of language.
Someof the s ntences even come from languages oth rthan English.
The errors in parse trees inevitablyaffect the classification accuracy.
However, theparser works substantially better on the TRECdataset since all qu stions are in formal writtenEnglish, and the training set for Stanford parser5already includes the QuestionBank (Judge et al,2006) which i cludes 2,000 TREC sentences.Figure 3 visualizes examples where CNN errswhile DCNN does not.
For example, CNN la-bels (a) as location due to ?Hawaii?
and ?state?,while the long-distance backbone ?What ?
flower?is clearly asking for an entity.
Similarly, in (d),DCNN captures the obviously negative tree-basedtrigram ?Nothing ?
w rth ?
em iling?.
Note thatour model also works with non-projective depen-dency trees such as the one in (b).
The last two ex-amples in Figure 3 visualize cases where DCNNoutperforms the baseline CNNs in fine-grainedTREC.
In example (e), the word ?temperature?
isat second from the top and is root of a 8 word span?the ... earth?.
When we use a window of size 5for tree convolution, every words in that span getconvolved with ?temperature?
and this should bethe reason why DCNN get correct.Figure 4 showcases examples where baselineCNNs get better results than DCNNs.
Example(a) is misclassified as entity by DCNN due to pars-ing/tagging error (the Stanford parser performs its5http://nlp.stanford.edu/software/parser-faq.shtmlWhat is the speed hummingbirds fly ?
(noun)root(a) num?
entyWhat body of water are the Canary Islands in ?root(b) loc?
entyWhat position did Willie Davis play in baseball ?root(c) hum?
entyFigure 3: Examples from TREC dat sets that aremisclassified y DTCNN but correctly label d byas line CNN.
For example, (a) should numer-ical but is labeled entity by DTCNN.ysis, we conclude that this is mainly due to thedifference of the parse tree quality between thetwo tasks.
In sentiment analysis, th dataset iscollected from the Rotten Tomatoes ebsite whichincludes many irregular usage of language.
Someof the sentences even come from languages otherthan English.
The errors in parse trees inevitablyaffect the classification accuracy.
However, theparser works substantially bett r on the TRECdataset since all questions are in formal writtenEnglish, and the training set for Stanford parser5already includes the QuestionBank (Judge et al,2006) which includes 2,000 TREC sentences.Figure 2 visualizes examples wh re CNN errswhile DTCNN does not.
For example, CNN la-bels (a) as location due to ?Hawaii?
and ?state?,whil the long-distance backbone ?What ?
flower?is clearly asking for an entity Similarly, in (d),DTCNN captures the obviously negative tree-ba ed trigram ?Nothi g ?
worth emailing?.
Notthat our model also works with non-projective de-pendency trees such as the one in (b).
The lasttwo examples in Figure 2 visualize cases wh reDTCNN outperforms the baseline CNNs in fine-grained TREC.
I example (e), the word ?temper-ature?
is at second from the top and is root of a8 word span ?the ... earth?.
When we use a win-dow of size 5 for tree convolution, every wordsin that span get convolved with ?temperature?
andthis should be the reason why DTCNN get correct.5http://nlp.stanford.edu/software/parser-faq.shtmlWhat is the melting point of copper ?root(a) num?
enty and descWhat did Jesse Jackson organize ?root(b) hum?
enty and entyWhat is the electrical output in Madrid , Spain ?root(c) enty?
num and numFigure 4: Examples from TREC datasets that aremisclassified by both DTCNN and baseline CNN.For example, (a) should be numerical but is la-beled entity by DTCNN and description by CNN.Fig re 3 showcases examples where baselineCNNs get better results than DTCNNs.
Exam-ple (a) is misclassified as entity by DTCNN dueto parsing/tagging error (the Stanford parser per-forms its own part-of-speech tagging).
The word?fly?
at the end of the sentence should be a verbinstead of noun, and ?hummingbirds fly?
shouldbe a relative clause modifying ?speed?.There are some sentences that are misclassifiedby both the baseline CNN and DTCNN.
Figure 4shows three such examples.
Example (a) is notclassified as numerical by both methods due to theambiguous meaning of the word ?point?
which isdifficult to capture by word embedding.
This wordcan ean location, opinion, etc.
Apparently, thenumerical aspect is not captured by word embed-ding.
Example (c) might be an annotation error.From the mistakes made by DTCNNs, we findth performance of DTCNN is mainly limited bytwo factors: the accuracy of the parser and thequality of word embedding.
Future work will fo-cus on these two issues.4 Conclusions and Future WorkWe have presented a very simple dependency tree-based convolution framework which outperformssequential CNN baselines on various classificationtasks.
Extensions of this model would considerdependency labels and constituency trees.
Also,we would evaluate on gold-standard parse trees.Figure 5: Examples from TREC datasets that aremisclassified by both DCNN and baseline CNN.For example, (a) should be numerical but is la-beled entity by DCNN and description by CNN.own part-of-sp ech tagging).
he word ?fly?
atthe end of the sentence should be a verb instead ofnoun, and ?hummingbirds fly?
should be a relativeclause modifying ?speed?.There re some sentences that are misclassifiedby both the baseline CNN and DCNN.
Figure 5shows three such examples.
Example (a) is notclassified as numerical by both methods due to theambiguous meaning of the word ?point?
which isdifficult to capture by word embedd g. This wordcan mean lo a ion, opinion, etc.
Apparently, thenumerical aspect is not captured by word embed-ding.
Example (c) might be an annotation error.Shortly before sub itting to ACL 2015 welearned Mou et al (2015, unpublished) have inde-pendently reported concurrent and related efforts.The r constituency model, based on their unpub-lished work in programming languages (Mou etal., 2014),6performs convolution on pretrained re-cursive node presentations rath r tha word em-beddings, thus baring little, if any, resemblance toour dependency-based mod l. Their dependencymodel is related, but always includes a node andall its children (resembling Iyyer et al (2014)),which is a variant of our sibling model and alwaysflat.
By contrast, our ancestor model looks at thevertical path from any word to its ancestors, beinglinguistically motivated (Shen et al, 2008).4 ConclusionsWe have presented a very simple dependency-based convolution framework which outperformssequential CNN baselines on modeling sentences.6Both their 2014 and 2015 reports proposed (independently of each otherand independently of our work) the term ?tree-based convolution?
(TBCNN).178ReferencesR.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
Journal ofMachine Learning Research, 12.Kushal Dave, Steve Lawrence, and David M Pennock.2003.
Mining the peanut gallery: Opinion extractionand semantic classification of product reviews.
InProceedings of World Wide Web.Michael Gamon.
2004.
Sentiment classification oncustomer feedback data: noisy data, large featurevectors, and the role of linguistic analysis.
In Pro-ceedings of COLING.Geoffrey E. Hinton, Nitish Srivastava, AlexKrizhevsky, Ilya Sutskever, and Ruslan Salakhut-dinov.
2014.
Improving neural networks bypreventing co-adaptation of feature detectors.Journal of Machine Learning Research, 15.Ozan Irsoy and Claire Cardie.
2014.
Deep recursiveneural networks for compositionality in language.In Advances in Neural Information Processing Sys-tems, pages 2096?2104.Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,Richard Socher, and Hal Daum?e III.
2014.
A neuralnetwork for factoid question answering over para-graphs.
In Proceedings of EMNLP.John Judge, Aoife Cahill, and Josef van Genabith.2006.
Questionbank: Creating a corpus of parse-annotated questions.
In Proceedings of COLING.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network formodelling sentences.
In Proceedings of ACL.Yoon Kim.
2014.
Convolutional neural networks forsentence classification.
In Proceedings of EMNLP.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of ACL.Taku Kudo and Yuji Matsumoto.
2004.
A boostingalgorithm for classification of semi-structured text.In Proceedings of EMNLP.Quoc V Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
In Pro-ceedings of ICML.Y.
LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes,J.
Denker, H. Drucker, I. Guyon, U. Mller,E.
Sckinger, P. Simard, and V. Vapnik.
1995.
Com-parison of learning algorithms for handwritten digitrecognition.
In Int?l Conf.
on Artificial Neural Nets.Xin Li and Dan Roth.
2002.
Learning question classi-fiers.
In Proceedings of COLING.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP natural lan-guage processing toolkit.
In Proceedings of ACL:Demonstrations, pages 55?60.Shotaro Matsumoto, Hiroya Takamura, and ManabuOkumura.
2005.
Sentiment classification usingword sub-sequences and dependency sub-trees.
InProceedings of PA-KDD.Ryan McDonald and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proceedings of EACL.Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang.2014.
TBCNN: A tree-based convolutional neu-ral network for programming language processing.Unpublished manuscript: http://arxiv.org/abs/1409.5718.Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, andZhi Jin.
2015.
Discriminative neural sentencemodeling by tree-based convolution.
Unpublishedmanuscript: http://arxiv.org/abs/1504.01106v5.
Version 5 dated June 2, 2015; Version 1(?Tree-based Convolution: A New Architecture forSentence Modeling?)
dated Apr 5, 2015.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploit-ing class relationships for sentiment categorizationwith respect to rating scales.
In Proceedings of ACL,pages 115?124.Libin Shen, Lucas Champollion, and Aravind K Joshi.2008.
LTAG-spinal and the treebank.
Language Re-sources and Evaluation, 42(1):1?19.Yelong Shen, Xiaodong he, Jianfeng Gao, Li Deng, andGregoire Mesnil.
2014.
Learning semantic repre-sentations using convolutional neural networks forweb search.
In Proceedings of WWW.J.
Silva, L. Coheur, A. C. Mendes, and AndreasWichert.
2011.
From symbolic to sub-symbolic in-formation in question classification.
Artificial Intel-ligence Review, 35.Richard Socher, Jeffrey Pennington, Eric H. Huang,Andrew Y. Ng, and Christopher D. Manning.
2011.Semi-Supervised Recursive Autoencoders for Pre-dicting Sentiment Distributions.
In Proceedings ofEMNLP 2011.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of EMNLP 2013.Wen-tau Yih, Xiaodong He, and Christopher Meek.2014.
Semantic parsing for single-relation questionanswering.
In Proceedings of ACL.Mattgew Zeiler.
2012.
Adadelta: An adaptive learningrate method.
Unpublished manuscript: http://arxiv.org/abs/1212.5701.Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.2015.
Long short-term memory over tree structures.In Proceedings of ICML.179
