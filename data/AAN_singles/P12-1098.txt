Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 930?939,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsPORT:  a Precision-Order-Recall MT Evaluation Metric for TuningBoxing Chen, Roland Kuhn and Samuel LarkinNational Research Council Canada283 Alexandre-Tach?
Boulevard, Gatineau (Qu?bec), Canada J8X 3X7{Boxing.Chen, Roland.Kuhn, Samuel.Larkin}@nrc.caAbstractMany machine translation (MT) evaluationmetrics have been shown to correlate betterwith human judgment than BLEU.
Inprinciple, tuning on these metrics shouldyield better systems than tuning on BLEU.However, due to issues such as speed,requirements for linguistic resources, andoptimization difficulty, they have not beenwidely adopted for tuning.
This paperpresents PORT 1 , a new MT  evaluationmetric which combines precision, recalland an ordering metric and which isprimarily designed for tuning MT systems.PORT does not require external resourcesand is quick to compute.
It has a bettercorrelation with human judgment thanBLEU.
We compare PORT-tuned MTsystems to BLEU-tuned baselines in fiveexperimental conditions involving fourlanguage pairs.
PORT tuning achievesconsistently better performance than BLEUtuning, according to four automatedmetrics (including BLEU) and to humanevaluation: in comparisons of outputs from300 source sentences, human judgespreferred the PORT-tuned output 45.3% ofthe time (vs. 32.7% BLEU tuningpreferences and 22.0% ties).1 IntroductionAutomatic evaluation metrics for machinetranslation (MT) quality are a key part of buildingstatistical MT (SMT) systems.
They play two1PORT: Precision-Order-Recall Tunable metric.roles: to allow rapid (though sometimes inaccurate)comparisons between different systems or betweendifferent versions of the same system, and toperform tuning of parameter values during systemtraining.
The latter has become important since theinvention of minimum error rate training (MERT)(Och, 2003) and related tuning methods.
Thesemethods perform repeated decoding runs withdifferent system parameter values, which are tunedto optimize the value of the evaluation metric overa development set with reference translations.MT evaluation metrics fall into three groups:?
BLEU (Papineni et al, 2002), NIST(Doddington, 2002), WER, PER, TER(Snover et al, 2006), and LRscore (Birch andOsborne, 2011) do not use external linguisticinformation; they are fast to compute (exceptTER).?
METEOR (Banerjee and Lavie, 2005),METEOR-NEXT (Denkowski and Lavie2010), TER-Plus (Snover et al, 2009),MaxSim (Chan and Ng, 2008), TESLA (Liuet al, 2010), AMBER (Chen and Kuhn, 2011)and MTeRater (Parton et al, 2011) exploitsome limited linguistic resources, such assynonym dictionaries, part-of-speech tagging,paraphrasing tables or word root lists.?
More sophisticated metrics such as RTE(Pado et al, 2009), DCU-LFG (He et al,2010) and MEANT (Lo and Wu, 2011) usehigher level syntactic or semantic analysis toscore translations.Among these metrics, BLEU is the most widelyused for both evaluation and tuning.
Many of themetrics correlate better with human judgments oftranslation quality than BLEU, as shown in recentWMT Evaluation Task reports (Callison-Burch et930al., 2010; Callison-Burch et al, 2011).
However,BLEU remains the de facto standard tuning metric,for two reasons.
First, there is no evidence that anyother tuning metric yields better MT systems.
Ceret al (2010) showed that BLEU tuning is morerobust than tuning with other metrics (METEOR,TER, etc.
), as gauged by both automatic andhuman evaluation.
Second, though a tuning metricshould correlate strongly with human judgment,MERT (and similar algorithms) invoke the chosenmetric so often that it must be computed quickly.Liu et al (2011) claimed that TESLA tuningperformed better than BLEU tuning according tohuman judgment.
However, in the WMT 2011?tunable metrics?
shared pilot task, this did nothold (Callison-Burch et al, 2011).
In (Birch andOsborne, 2011), humans preferred the output fromLRscore-tuned systems 52.5% of the time, versusBLEU-tuned system outputs 43.9% of the time.In this work, our goal is to devise a metric that,like BLEU, is computationally cheap andlanguage-independent, but that yields better MTsystems than BLEU when used for tuning.
Wetried out different combinations of statistics beforesettling on the final definition of our metric.
Thefinal version, PORT, combines precision, recall,strict brevity penalty (Chiang et al, 2008) andstrict redundancy penalty (Chen and Kuhn, 2011)in a quadratic mean expression.
This expression isthen further combined with a new measure of wordordering, v, designed to reflect long-distance aswell as short-distance word reordering (BLEU onlyreflects short-distance reordering).
In a latersection, 3.3, we describe experiments that varyparts of the definition of PORT.Results given below show that PORT correlatesbetter with human judgments of translation qualitythan BLEU does, and sometimes outperformsMETEOR in this respect, based on data fromWMT (2008-2010).
However, since PORT isdesigned for tuning, the most important results arethose showing that PORT tuning yields systemswith better translations than those produced byBLEU tuning ?
both as determined by automaticmetrics (including BLEU), and according tohuman judgment, as applied to five data conditionsinvolving four language pairs.2 BLEU and PORTFirst, define n-gram precision p(n) and recall r(n):)(grams-n#)(grams-n#)(TRTnp ?=                 (1))(grams-n#)(grams-n#)(RRTnr?=              (2)where T = translation, R = reference.
Both BLEUand PORT are defined on the document-level, i.e.T and R are whole texts.
If there are multiplereferences, we use closest reference length for eachtranslation hypothesis to compute the numbers ofthe reference n-grams.2.1 BLEUBLEU is composed of precision Pg(N) and brevitypenalty BP:BPNPBLEU g ?= )(                 (3)where Pg(N) is the geometric average of n-gramprecisionsNNng npNP11)()( ???????
?= ?=(4)The BLEU brevity penalty punishes the score ifthe translation length len(T) is shorter than thereference length len(R); it is: ( ))(/)(1,0.1min TlenRleneBP ?=         (5)2.2 PORTPORT has five components: precision, recall, strictbrevity penalty (Chiang et al, 2008), strictredundancy penalty (Chen and Kuhn, 2011) and anordering measure v. The design of PORT is basedon exhaustive experiments on a development dataset.
We do not have room here to give a rationalefor all the choices we made when we designedPORT.
However, a later section (3.3) reconsiderssome of these design decisions.2.2.1 Precision and RecallThe average precision and average recall used inPORT (unlike those used in BLEU) are thearithmetic average of n-gram precisions Pa(N) andrecalls Ra(N):?==Nna npNNP1)(1)(                 (6)?==Nna nrNNR1)(1)(                   (7)931We use two penalties to avoid too long or tooshort MT outputs.
The first, the strict brevitypenalty (SBP), is proposed in (Chiang et al, 2008).Let ti be the translation of input sentence i, and letri be its reference.
Set????????
?= ?
?i iii irtrSBP |}||,min{|||1exp         (8)The second is the strict redundancy penalty (SRP),proposed in (Chen and Kuhn, 2011):????????
?= ?
?i ii iirrtSRP |||}||,max{|1exp         (9)To combine precision and recall, we tried fouraveraging methods: arithmetic (A), geometric (G),harmonic (H), and quadratic (Q) mean.
If all of thevalues to be averaged are positive, the order ismaxQAGHmin ?????
, with equalityholding if and only if all the values being averagedare equal.
We chose the quadratic mean tocombine precision and recall, as follows:2))(())(()(22 SRPNRSBPNPNQmean aa ?+?=   (10)2.2.2 Ordering MeasureWord ordering measures for MT compare twopermutations of the original source-language wordsequence: the permutation represented by thesequence of corresponding words in the MToutput, and the permutation in the reference.Several ordering measures have been integratedinto MT evaluation metrics recently.
Birch andOsborne (2011) use either Hamming Distance orKendall?s ?
Distance (Kendall, 1938) in theirmetric LRscore, thus obtaining two versions ofLRscore.
Similarly, Isozaki et al (2011) adopteither Kendall?s ?
Distance or Spearman?s ?
(Spearman, 1904) distance in their metrics.Our measure, v, is different from all of these.We use word alignment to compute the twopermutations (LRscore also uses word alignment).The word alignment between the source input andreference is computed using GIZA++ (Och andNey, 2003) beforehand with the default settings,then is refined with the heuristic grow-diag-final-and; the word alignment between the source inputand the translation is generated by the decoder withthe help of word alignment inside each phrase pair.PORT uses permutations.
These encode one-to-one relations but not one-to-many, many-to-one,many-to-many or null relations, all of which canoccur in word alignments.
We constrain theforbidden types of relation to become one-to-one,as in (Birch and Osborne, 2011).
Thus, in a one-to-many alignment, the single source word is forcedto align with the first target word; in a many-to-onealignment, monotone order is assumed for thetarget words; and source words originally alignedto null are aligned to the target word position justafter the previous source word?s target position.After the normalization above, suppose we havetwo permutations for the same source n-wordinput.
E.g., let P1 = reference, P2 = hypothesis:P1: 11p21p31p41p  ?ip1  ?np1P2: 12p22p32p42p  ?ip2  ?np2Here, each jip is an integer denoting position in theoriginal source (e.g., 11p = 7 means that the firstword in P1 is the 7th source word).The ordering metric v is computed from twodistance measures.
The first is absolutepermutation distance:?=?=niii ppPPDIST121211 ||),(               (11)Let2/)1(),(1 2111 +?= nnPPDIST?
(12)v1 ranges from 0 to 1; a larger value means moresimilarity between the two permutations.
Thismetric is similar to Spearman?s ?
(Spearman,1904).
However, we have found that ?
punisheslong-distance reorderings too heavily.
For instance,1?
is more tolerant than ?
of the movement of?recently?
in this example:Ref: Recently, I visited ParisHyp: I visited Paris recentlyInspired by HMM word alignment (Vogel et al,1996), our second distance measure is based onjump width.
This punishes a sequence of wordsthat moves a long distance with its internal orderconserved, only once rather than on every word.
Inthe following, only two groups of words havemoved, so the jump width punishment is light:Ref: In the winter of 2010, I visited ParisHyp: I visited Paris in the winter of 2010So the second distance measure is932?=????
?=niiiii ppppPPDIST1122111212 |)()(|),(   (13)where we set 001 =p  and 002 =p .
Let1),(1 2 2122?
?=nPPDISTv                     (14)As with v1, v2 is also from 0 to 1, and larger valuesindicate more similar permutations.
The orderingmeasure vs is the harmonic mean of v1 and v2:)/1/1/(2 21 vvvs +=.
(15)vs in (15) is computed at segment level.
Formultiple references, we compute vs for each, andthen choose the biggest one as the segment levelordering similarity.
We compute document levelordering with a weighted arithmetic mean:?
?==?= ls sls ssRlenRlenvv11)()((16)where l is the number of segments of thedocument, and len(R) is the length of the reference.2.2.3 Combined MetricFinally, Qmean(N) (Eq.
(10) and the word orderingmeasure v are combined in a harmonic mean:?vNQmeanPORT /1)(/12+=           (17)Here ?
is a free parameter that is tuned on held-out data.
As it increases, the importance of theordering measure v goes up.
For our experiments,we tuned ?
on Chinese-English data, setting it to0.25 and keeping this value for the other languagepairs.
The use of v means that unlike BLEU, PORTrequires word alignment information.3 Experiments3.1 PORT as an Evaluation MetricWe studied PORT as an evaluation metric onWMT data; test sets include WMT 2008, WMT2009, and WMT 2010 all-to-English, plus 2009,2010 English-to-all submissions.
The languages?all?
(?xx?
in Table 1) include French, Spanish,German and Czech.
Table 1 summarizes the testset statistics.
In order to compute the v part ofPORT, we require source-target word alignmentsfor the references and MT outputs.
These aren?tincluded in WMT data, so we compute them withGIZA++.We used Spearman?s rank correlation coefficient?
to measure correlation of the metric with system-level human judgments of translation.
The humanjudgment score is based on the ?Rank?
only, i.e.,how often the translations of the system were ratedas better than those from other systems (Callison-Burch et al, 2008).
Thus, BLEU, METEOR, andPORT were evaluated on how well their rankingscorrelated with the human ones.
For the segmentlevel, we follow (Callison-Burch et al, 2010) inusing Kendall?s rank correlation coefficient ?.As shown in Table 2, we compared PORT withsmoothed BLEU (mteval-v13a), and METEORv1.0.
Both BLEU and PORT perform matching ofn-grams up to n = 4.Set Year Lang.
#system #sent-pairTest1 2008 xx-en 43 7,804Test2 2009 xx-en 45 15,087Test3 2009 en-xx 40 14,563Test4 2010 xx-en 53 15,964Test5 2010 en-xx 32 18,508Table 1: Statistics of the WMT dev and test sets.MetricInto-En Out-of-Ensys.
seg.
sys.
seg.BLEU 0.792 0.215 0.777 0.240METEOR 0.834 0.231 0.835 0.225PORT 0.801 0.236 0.804 0.242Table 2: Correlations with human judgment on WMTPORT achieved the best segment levelcorrelation with human judgment on both the ?intoEnglish?
and ?out of English?
tasks.
At the systemlevel, PORT is better than BLEU, but not as goodas METEOR.
This is because we designed PORTto carry out tuning; we did not optimize itsperformance as an evaluation metric, but rather, tooptimize system tuning performance.
There aresome other possible reasons why PORT did notoutperform METEOR v1.0 at system level.
MostWMT submissions involve language pairs withsimilar word order, so the ordering factor v inPORT won?t play a big role.
Also, v depends onsource-target word alignments for reference andtest sets.
These alignments were performed byGIZA++ models trained on the test data only.9333.2 PORT as a Metric for Tuning3.2.1 Experimental detailsThe first set of experiments to study PORT as atuning metric involved Chinese-to-English (zh-en);there were two data conditions.
The first is thesmall data condition where FBIS2 is used to trainthe translation and reordering models.
It contains10.5M target word tokens.
We trained twolanguage models (LMs), which were combinedloglinearly.
The first is a 4-gram LM which isestimated on the target side of the texts used in thelarge data condition (below).
The second is a 5-gram LM estimated on English Gigaword.The large data condition uses training data fromNIST3 2009 (Chinese-English track).
All allowedbilingual corpora except UN, Hong Kong Laws andHong Kong Hansard were used to train thetranslation model and reordering models.
There areabout 62.6M target word tokens.
The same twoLMs are used for large data as for small data, andthe same development (?dev?)
and test sets are alsoused.
The dev set comprised mainly data from theNIST 2005 test set, and also some balanced-genreweb-text from NIST.
Evaluation was performed onNIST 2006 and 2008.
Four references wereprovided for all dev and test sets.The third data condition is a French-to-English(fr-en).
The parallel training data is from CanadianHansard data, containing 59.3M word tokens.
Weused two LMs in loglinear combination: a 4-gramLM trained on the target side of the paralleltraining data, and the English Gigaword 5-gramLM.
The dev set has 1992 sentences; the two testsets have 2140 and 2164 sentences respectively.There is one reference for all dev and test sets.The fourth and fifth conditions involve German--English Europarl data.
This parallel corpuscontains 48.5M German tokens and 50.8M Englishtokens.
We translate both German-to-English (de-en) and English-to-German (en-de).
The twoconditions both use an LM trained on the targetside of the parallel training data, and de-en alsouses the English Gigaword 5-gram LM.
News test2008 set is used as dev set; News test 2009, 2010,2011 are used as test sets.
One reference isprovided for all dev and test sets.2LDC2003E143http://www.nist.gov/speech/tests/mtAll experiments were carried out with ?
in Eq.
(17) set to 0.25, and involved only lowercaseEuropean-language text.
They were performedwith MOSES (Koehn et al, 2007), whose decoderincludes lexicalized reordering, translation models,language models, and word and phrase penalties.Tuning was done with n-best MERT, which isavailable in MOSES.
In all tuning experiments,both BLEU and PORT performed lower casematching of n-grams up to n = 4.
We alsoconducted experiments with tuning on a version ofBLEU that incorporates SBP (Chiang et al, 2008)as a baseline.
The results of original IBM BLEUand BLEU with SBP were tied; to save space, weonly report results for original IBM BLEU here.3.2.2 Comparisons with automatic metricsFirst, let us see if BLEU-tuning and PORT-tuningyield systems with different translations for thesame input.
The first row of Table 3 shows thepercentage of identical sentence outputs for thetwo tuning types on test data.
The second rowshows the similarity of the two outputs at word-level (as measured by 1-TER): e.g., for the two zh-en tasks, the two tuning types give systems whoseoutputs are about 25-30% different at the wordlevel.
By contrast, only about 10% of output wordsfor fr-en differ for BLEU vs.
PORT tuning.zh-ensmallzh-enlargefr-enHansde-enWMTen-deWMTSame sent.
17.7% 13.5% 56.6% 23.7% 26.1%1-TER 74.2 70.9 91.6 87.1 86.6Table 3: Similarity of BLEU-tuned and PORT-tunedsystem outputs on test data.TaskTuneEvaluation metrics (%)BLEU MTR 1-TER PORTzh-ensmallBLEUPORT26.827.2*55.255.738.038.049.750.0zh-enlargeBLEUPORT29.930.3*58.459.041.242.053.053.2fr-enHansBLEUPORT38.838.869.869.654.254.657.157.1de-enWMTBLEUPORT20.120.355.656.038.438.439.639.7en-deWMTBLEUPORT13.613.643.343.330.130.731.731.7Table 4: Automatic evaluation scores on test data.
* indicates the results are significantly better than thebaseline (p<0.05).934Table 4 shows translation quality for BLEU- andPORT-tuned systems, as assessed by automaticmetrics.
We employed BLEU4, METEOR (v1.0),TER (v0.7.25), and the new metric PORT.
In thetable, TER scores are presented as 1-TER to ensurethat for all metrics, higher scores mean higherquality.
All scores are averages over the relevanttest sets.
There are twenty comparisons in thetable.
Among these, there is one case (French-English assessed with METEOR) where BLEUoutperforms PORT, there are seven ties, and thereare twelve cases where PORT is better.
Table 3shows that fr-en outputs are very similar for bothtuning types, so the fr-en results are perhaps lessinformative than the others.
Overall, PORT tuninghas a striking advantage over BLEU tuning.Both (Liu et al, 2011) and (Cer et al, 2011)showed that with MERT, if you want the bestpossible score for a system?s translations accordingto metric M, then you should tune with M. Thisdoesn?t appear to be true when PORT and BLEUtuning are compared in Table 4.
For the twoChinese-to-English tasks in the table, PORT tuningyields a better BLEU score than BLEU tuning,with significance at p < 0.05.
We are currentlyinvestigating why PORT tuning gives higherBLEU scores than BLEU tuning for Chinese-English and German-English.
In internal tests wehave found no systematic difference in dev-setBLEUs, so we speculate that PORT?s emphasis onreordering yields models that generalize better forthese two language pairs.3.2.3 Human EvaluationWe conducted a human evaluation on outputs fromBLEU- and PORT-tuned systems.
The examplesare randomly picked from all ?to-English?conditions shown in Tables 3 & 4 (i.e., allconditions except English-to-German).We performed pairwise comparison of thetranslations produced by the system types as in(Callison-Burch et al, 2010; Callison-Burch et al,2011).
First, we eliminated examples where thereference had fewer than 10 words or more than 50words, or where outputs of the BLEU-tuned andPORT-tuned systems were identical.
Theevaluators (colleagues not involved with thispaper) objected to comparing two bad translations,so we then selected for human evaluation onlytranslations that had high sentence-level (1-TER)scores.
To be fair to both metrics, for eachcondition, we took the union of examples whoseBLEU-tuned output was in the top n% of BLEUoutputs and those whose PORT-tuned output wasin the top n% of PORT outputs (based on (1-TER)).
The value of n varied by condition: wechose the top 20% of zh-en small, top 20% of en-de, top 50% of fr-en and top 40% of zh-en large.We then randomly picked 450 of these examples toform the manual evaluation set.
This set was splitinto 15 subsets, each containing 30 sentences.
Thefirst subset was used as a common set; each of theother 14 subsets was put in a separate file, to whichthe common set is added.
Each of the 14evaluators received one of these files, containing60 examples (30 unique examples and 30 examplesshared with the other evaluators).
Within eachexample, BLEU-tuned and PORT-tuned outputswere presented in random order.After receiving the 14 annotated files, wecomputed Fleiss?s Kappa (Fleiss, 1971) on thecommon set to measure inter-annotator agreement,all?
.
Then, we excluded annotators one at a timeto compute i?
(Kappa score without i-th annotator,i.e., from the other 13).
Finally, we filtered out thefiles from the 4 annotators whose answers weremost different from everybody else?s: i.e.,annotators with the biggest iall ??
?
values.This left 10 files from 10 evaluators.
We threwaway the common set in each file, leaving 300pairwise comparisons.
Table 5 shows that theevaluators preferred the output from the PORT-tuned system 136 times, the output from theBLEU-tuned one 98 times, and had no preferencethe other 66 times.
This indicates that there is ahuman preference for outputs from the PORT-tuned system over those from the BLEU-tunedsystem at the p<0.01 significance level (in caseswhere people prefer one of them).PORT tuning seems to have a bigger advantageover BLEU tuning when the translation task ishard.
Of the Table 5 language pairs, the one wherePORT tuning helps most has the lowest BLEU inTable 4 (German-English); the one where it helpsleast in Table 5 has the highest BLEU in Table 4(French-English).
(Table 5 does not prove BLEU issuperior to PORT for French-English tuning:statistically, the difference between 14 and 17 hereis a tie).
Maybe by picking examples for eachcondition that were the easiest for the system totranslate (to make human evaluation easier), we935mildly biased the results in Table 5 against PORTtuning.
Another possible factor is reordering.PORT differs from BLEU partly in modeling long-distance reordering more accurately; English andFrench have similar word order, but the other twolanguage pairs don?t.
The results in section 3.3(below) for Qmean, a version of PORT withoutword ordering factor v, suggest v may be definedsuboptimally for French-English.PORT win BLEU win equal totalzh-ensmall1938.8%1836.7%1224.5%49zh-enlarge6945.7%4630.5%3623.8%151fr-enHans1432.6%1739.5%1227.9%43de-enWMT3459.7%1729.8%610.5%57All 13645.3%9832.7%6622.0%300Table 5: Human preference for outputs from PORT-tuned vs. BLEU-tuned system.3.2.4 Computation timeA good tuning metric should run very fast; this isone of the advantages of BLEU.
Table 6 shows thetime required to score the 100-best hypotheses forthe dev set for each data condition during MERTfor BLEU and PORT in similar implementations.The average time of each iteration, includingmodel loading, decoding, scoring and runningMERT4, is in brackets.
PORT takes roughly 1.5 ?2.5 as long to compute as BLEU, which isreasonable for a tuning metric.zh-ensmallzh-enlargefr-enHansde-enWMTen-deWMTBLEU 3 (13)  3 (17) 2 (19) 2 (20) 2 (11)PORT 5 (21) 5 (24) 4 (28) 5 (28) 4 (15)Table 6: Time to score 100-best hypotheses (averagetime per iteration) in minutes.3.2.5 Robustness to word alignment errorsPORT, unlike BLEU, depends on wordalignments.
How does quality of word alignmentbetween source and reference affect PORT tuning?We created a dev set from Chinese Tree Bank4Our experiments are run on a cluster.
The average time foran iteration includes queuing, and the speed of each node isslightly different, so bracketed times are only for reference.
(CTB) hand-aligned data.
It contains 588 sentences(13K target words), with one reference.
We alsoran GIZA++ to obtain its automatic wordalignment, computed on CTB and FBIS.
The AERof the GIZA++ word alignment on CTB is 0.32.In Table 7, CTB is the dev set.
The table showstuning with BLEU, PORT with human wordalignment (PORT + HWA), and PORT withGIZA++ word alignment (PORT + GWA); thecondition is zh-en small.
Despite the AER of 0.32for automatic word alignment, PORT tuning worksabout as well with this alignment as for the goldstandard CTB one.
(The BLEU baseline in Table 7differs from the Table 4 BLEU baseline becausethe dev sets differ).Tune BLEU MTR 1-TER PORTBLEU 25.1 53.7 36.4 47.8PORT + HWA 25.3 54.4 37.0 48.2PORT + GWA 25.3 54.6 36.4 48.1Table 7: PORT tuning - human & GIZA++ alignmentTask Tune BLEU MTR 1-TER PORTzh-ensmallBLEUPORTQmean26.827.226.855.255.755.338.038.038.249.750.049.8zh-enlargeBLEUPORTQmean29.930.330.258.459.058.541.242.041.853.053.253.1fr-enHansBLEUPORTQmean38.838.838.869.869.669.854.254.654.657.157.157.1de-enWMTBLEUPORTQmean20.120.320.355.656.056.338.438.438.139.639.739.7en-deWMTBLEUPORTQmean13.613.613.643.343.343.430.130.730.331.731.731.7Table 8: Impact of ordering measure v on PORT3.3 AnalysisNow, we look at the details of PORT to see whichof them are the most important.
We do not havespace here to describe all the details we studied,but we can describe some of them.
E.g., does theordering measure v help tuning performance?
Toanswer this, we introduce an intermediate metric.This is Qmean as in Eq.
(10): PORT without theordering measure.
Table 8 compares tuning withBLEU, PORT, and Qmean.
PORT outperformsQmean on seven of the eight automatic scoresshown for small and large Chinese-English.936However, for the European language pairs, PORTand Qmean seem to be tied.
This may be becausewe optimized ?
in Eq.
(18) for Chinese-English,making the influence of word ordering measure vin PORT too strong for the European pairs, whichhave similar word order.Measure v seems to help Chinese-Englishtuning.
What would results be on that languagepair if we were to replace v in PORT with anotherordering measure?
Table 9 gives a partial answer,with Spearman?s ?
and Kendall?s ?
replacing vwith ?
or ?
in PORT for the zh-en small condition(CTB with human word alignment is the dev set).The original definition of PORT seems preferable.Tune BLEU METEOR 1-TERBLEU 25.1 53.7 36.4PORT(v) 25.3 54.4 37.0PORT(?)
25.1 54.2 36.3PORT(?)
25.1 54.0 36.0Table 9: Comparison of the ordering measure: replacing?
with ?
or ?
in PORT.TaskTuneordering measures?
?
vNIST06 BLEUPORT0.9790.9790.9260.9280.9150.917NIST08 BLEUPORT0.9800.9810.9260.9290.9160.918CTB BLEUPORT0.9730.9750.8600.8660.8470.853Table 10: Ordering scores (?, ?
and v) for test sets NIST2006, 2008 and CTB.A related question is how much word orderingimprovement we obtained from tuning with PORT.We evaluate Chinese-English word ordering withthree measures: Spearman?s ?, Kendall?s ?
distanceas applied to two permutations (see section 2.2.2)and our own measure v. Table 10 shows the effectsof BLEU and PORT tuning on these threemeasures, for three test sets in the zh-en largecondition.
Reference alignments for CTB werecreated by humans, while the NIST06 and NIST08reference alignments were produced with GIZA++.A large value of ?, ?, or v implies outputs haveordering similar to that in the reference.
From thetable, we see that the PORT-tuned system yieldedbetter word order than the BLEU-tuned system inall nine combinations of test sets and orderingmeasures.
The advantage of PORT tuning isparticularly noticeable on the most reliable test set:the hand-aligned CTB data.What is the impact of the strict redundancypenalty on PORT?
Note that in Table 8, eventhough Qmean has no ordering measure, itoutperforms BLEU.
Table 11 shows the BLEUbrevity penalty (BP) and (number of matching 1-& 4- grams)/(number of total 1- & 4- grams) forthe translations.
The BLEU-tuned and Qmean-tuned systems generate similar numbers ofmatching n-grams, but Qmean-tuned systemsproduce fewer n-grams (thus, shorter translations).E.g., for zh-en small, the BLEU-tuned systemproduced 44,677 1-grams (words), while theQmean-trained system one produced 43,555 1-grams; both have about 32,000 1-grams matchingthe references.
Thus, the Qmean translations havehigher precision.
We believe this is because of thestrict redundancy penalty in Qmean.
As usual,French-English is the outlier: the two outputs hereare typically so similar that BLEU and Qmeantuning yield very similar n-gram statistics.Task Tune 1-gram 4-gram BPzh-ensmallBLEUQmean32055/4467731996/435554603/397164617/385950.9670.962zh-enlargeBLEUQmean34583/4537034369/442295954/404105987/392710.9720.959fr-enHansBLEUQmean28141/4052528167/407988654/342248695/344950.9830.990de-enWMTBLEUQmean42380/7542842173/724035151/664255203/634011.0000.968en-deWMTBLEUQmean30326/6236730343/620922261/548122298/545371.0000.997Table 11: #matching-ngram/#total-ngram and BP score4 ConclusionsIn this paper, we have proposed a new tuningmetric for SMT systems.
PORT incorporatesprecision, recall, strict brevity penalty and strictredundancy penalty, plus a new word orderingmeasure v.  As an evaluation metric, PORTperformed better than BLEU at the system leveland the segment level, and it was competitive withor slightly superior to METEOR at the segmentlevel.
Most important, our results show that PORT-tuned MT systems yield better translations  thanBLEU-tuned systems on several language pairs,according both to automatic metrics and humanevaluations.
In future work, we plan to tune thefree parameter ?
for each language pair.937ReferencesS.
Banerjee and A. Lavie.
2005.
METEOR: Anautomatic metric for MT evaluation with improvedcorrelation with human judgments.
In Proceedings ofACL Workshop on Intrinsic & Extrinsic EvaluationMeasures for Machine Translation and/orSummarization.A.
Birch and M. Osborne.
2011.
Reordering Metrics forMT.
In Proceedings of ACL.C.
Callison-Burch, C. Fordyce, P. Koehn, C. Monz andJ.
Schroeder.
2008.
Further Meta-Evaluation ofMachine Translation.
In Proceedings of WMT.C.
Callison-Burch, M. Osborne, and P. Koehn.
2006.Re-evaluating the role of BLEU in machinetranslation research.
In Proceedings of EACL.C.
Callison-Burch, P. Koehn, C. Monz, K. Peterson, M.Przybocki and O. Zaidan.
2010.
Findings of the 2010Joint Workshop on Statistical Machine Translationand Metrics for Machine Translation.
In Proceedingsof WMT.C.
Callison-Burch, P. Koehn, C. Monz and O. Zaidan.2011.
Findings of the 2011 Workshop on StatisticalMachine Translation.
In Proceedings of WMT.D.
Cer, D. Jurafsky and C. Manning.
2010.
The BestLexical Metric for Phrase-Based Statistical MTSystem Optimization.
In Proceedings of NAACL.Y.
S. Chan and H. T. Ng.
2008.
MAXSIM: A maximumsimilarity metric for machine translation evaluation.In Proceedings of ACL.B.
Chen and R. Kuhn.
2011.
AMBER: A ModifiedBLEU, Enhanced Ranking Metric.
In: Proceedings ofWMT.
Edinburgh, UK.
July.D.
Chiang, S. DeNeefe, Y. S. Chan, and H. T. Ng.
2008.Decomposability of translation metrics for improvedevaluation and efficient algorithms.
In Proceedings ofEMNLP, pages 610?619.M.
Denkowski and A. Lavie.
2010.
Meteor-next and themeteor paraphrase tables: Improved evaluationsupport for five target languages.
In Proceedings ofthe Joint Fifth Workshop on SMT andMetricsMATR, pages 314?317.G.
Doddington.
2002.
Automatic evaluation of machinetranslation quality using n-gram co-occurrencestatistics.
In Proceedings of HLT.J.
L. Fleiss.
1971.
Measuring nominal scale agreementamong many raters.
In Psychological Bulletin, Vol.76, No.
5 pp.
378?382.Y.
He, J.
Du, A.
Way and J. van Genabith.
2010.
TheDCU dependency-based metric in WMT-MetricsMATR 2010.
In Proceedings of the JointFifth Workshop on Statistical Machine Translationand MetricsMATR, pages 324?328.H.
Isozaki, T. Hirao, K. Duh, K. Sudoh, H. Tsukada.2010.
Automatic Evaluation of Translation Qualityfor Distant Language Pairs.
In Proceedings ofEMNLP.M.
Kendall.
1938.
A New Measure of Rank Correlation.In Biometrika, 30 (1?2), pp.
81?89.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,R.
Zens, C. Dyer, O. Bojar, A. Constantin and E.Herbst.
2007.
Moses: Open Source Toolkit for Statis-tical Machine Translation.
In Proceedings of ACL,pp.
177-180, Prague, Czech Republic.A.
Lavie and M. J. Denkowski.
2009.
The METEORmetric for automatic evaluation of machinetranslation.
Machine Translation, 23.C.
Liu, D. Dahlmeier, and H. T. Ng.
2010.
TESLA:Translation evaluation of sentences with linear-programming-based analysis.
In Proceedings of theJoint Fifth Workshop on Statistical MachineTranslation and MetricsMATR, pages 329?334.C.
Liu, D. Dahlmeier, and H. T. Ng.
2011.
Betterevaluation metrics lead to better machine translation.In Proceedings of EMNLP.C.
Lo and D. Wu.
2011.
MEANT: An inexpensive,high-accuracy, semi-automatic metric for evaluatingtranslation utility based on semantic roles.
InProceedings of ACL.F.
J. Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings ofACL-2003.
Sapporo, Japan.F.
J. Och and H. Ney.
2003.
A Systematic Comparisonof Various Statistical Alignment Models.
InComputational Linguistics, 29, pp.
19?51.S.
Pado, M. Galley, D. Jurafsky, and C.D.
Manning.2009.
Robust machine translation evaluation withentailment features.
In Proceedings of ACL-IJCNLP.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation ofmachine translation.
In Proceedings of ACL.K.
Parton, J. Tetreault, N. Madnani and M. Chodorow.2011.
E-rating Machine Translation.
In Proceedingsof WMT.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, and J.Makhoul.
2006.
A Study of Translation Edit Rate938with Targeted Human Annotation.
In Proceedings ofAssociation for Machine Translation in the Americas.M.
Snover, N. Madnani, B. Dorr, and R. Schwartz.2009.
Fluency, Adequacy, or HTER?
ExploringDifferent Human Judgments with a Tunable MTMetric.
In Proceedings of the Fourth Workshop onStatistical Machine Translation, Athens, Greece.C.
Spearman.
1904.
The proof and measurement ofassociation between two things.
In American Journalof Psychology, 15, pp.
72?101.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM basedword alignment in statistical translation.
InProceedings of COLING.939
