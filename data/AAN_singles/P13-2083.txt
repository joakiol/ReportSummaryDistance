Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 467?473,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Structured Distributional Semantic Model for Event Co-referenceKartik Goyal?
Sujay Kumar Jauhar?
Huiying Li?Mrinmaya Sachan?
Shashank Srivastava?
Eduard HovyLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon University{kartikgo,sjauhar,huiyingl,mrinmays,shashans,hovy}@cs.cmu.eduAbstractIn this paper we present a novel ap-proach to modelling distributional seman-tics that represents meaning as distribu-tions over relations in syntactic neighbor-hoods.
We argue that our model approxi-mates meaning in compositional configu-rations more effectively than standard dis-tributional vectors or bag-of-words mod-els.
We test our hypothesis on the problemof judging event coreferentiality, which in-volves compositional interactions in thepredicate-argument structure of sentences,and demonstrate that our model outper-forms both state-of-the-art window-basedword embeddings as well as simple ap-proaches to compositional semantics pre-viously employed in the literature.1 IntroductionDistributional Semantic Models (DSM) are popu-lar in computational semantics.
DSMs are basedon the hypothesis that the meaning of a word orphrase can be effectively captured by the distribu-tion of words in its neighborhood.
They have beensuccessfully used in a variety of NLP tasks includ-ing information retrieval (Manning et al, 2008),question answering (Tellex et al, 2003), word-sense discrimination (Sch?tze, 1998) and disam-biguation (McCarthy et al, 2004), semantic sim-ilarity computation (Wong and Raghavan, 1984;McCarthy and Carroll, 2003) and selectional pref-erence modeling (Erk, 2007).A shortcoming of DSMs is that they ignore thesyntax within the context, thereby reducing thedistribution to a bag of words.
Composing the?
*Equally contributing authorsdistributions for ?Lincoln?, ?Booth?, and ?killed?gives the same result regardless of whether the in-put is ?Booth killed Lincoln?
or ?Lincoln killedBooth?.
But as suggested by Pantel and Lin (2000)and others, modeling the distribution over prefer-ential attachments for each syntactic relation sep-arately yields greater expressive power.
Thus, toremedy the bag-of-words failing, we extend thegeneric DSM model to several relation-specificdistributions over syntactic neighborhoods.
Inother words, one can think of the Structured DSM(SDSM) representation of a word/phrase as sev-eral vectors defined over the same vocabulary,each vector representing the word?s selectionalpreferences for its various syntactic arguments.We argue that this representation not only cap-tures individual word semantics more effectivelythan the standard DSM, but is also better able toexpress the semantics of compositional units.
Weprove this on the task of judging event coreference.Experimental results indicate that our modelachieves greater predictive accuracy on the taskthan models that employ weaker forms of compo-sition, as well as a baseline that relies on state-of-the-art window based word embeddings.
Thissuggests that our formalism holds the potential ofgreater expressive power in problems that involveunderlying semantic compositionality.2 Related WorkNext, we relate and contrast our work to prior re-search in the fields of Distributional Vector SpaceModels, Semantic Compositionality and EventCo-reference Resolution.2.1 DSMs and CompositionalityThe underlying idea that ?a word is characterizedby the company it keeps?
was expressed by Firth467(1957).
Several works have defined approaches tomodelling context-word distributions anchored ona target word, topic, or sentence position.
Collec-tively these approaches are called DistributionalSemantic Models (DSMs).While DSMs have been very successful on a va-riety of tasks, they are not an effective model ofsemantics as they lack properties such as compo-sitionality or the ability to handle operators suchas negation.
In order to model a stronger form ofsemantics, there has been a recent surge in stud-ies that phrase the problem of DSM composition-ality as one of vector composition.
These tech-niques derive the meaning of the combination oftwo words a and b by a single vector c = f(a, b).Mitchell and Lapata (2008) propose a frameworkto define the composition c = f(a, b, r,K) wherer is the relation between a and b, and K is theadditional knowledge used to define composition.While this framework is quite general, the actualmodels considered in the literature tend to disre-gard K and r and mostly perform component-wiseaddition and multiplication, with slight variations,of the two vectors.
To the best of our knowledgethe formulation of composition we propose is thefirst to account for both K and r within this com-positional framework.Dinu and Lapata (2010) and S?aghdha and Ko-rhonen (2011) introduced a probabilistic modelto represent word meanings by a latent variablemodel.
Subsequently, other high-dimensional ex-tensions by Rudolph and Giesbrecht (2010), Ba-roni and Zamparelli (2010) and Grefenstette etal.
(2011), regression models by Guevara (2010),and recursive neural network based solutions bySocher et al (2012) and Collobert et al (2011)have been proposed.
However, these models donot efficiently account for structure.Pantel and Lin (2000) and Erk and Pad?
(2008)attempt to include syntactic context in distribu-tional models.
A quasi-compositional approachwas attempted in Thater et al (2010) by a com-bination of first and second order context vectors.But they do not explicitly construct phrase-levelmeaning from words which limits their applicabil-ity to real world problems.
Furthermore, we alsoinclude structure into our method of composition.Prior work in structure aware methods to the bestof our knowledge are (Weisman et al, 2012) and(Baroni and Lenci, 2010).
However, these meth-ods do not explicitly model composition.2.2 Event Co-reference ResolutionWhile automated resolution of entity coreferencehas been an actively researched area (Haghighiand Klein, 2009; Stoyanov et al, 2009; Raghu-nathan et al, 2010), there has been relatively lit-tle work on event coreference resolution.
Leeet al (2012) perform joint cross-document entityand event coreference resolution using the two-way feedback between events and their arguments.We, on the other hand, attempt a slightly differentproblem of making co-referentiality judgementson event-coreference candidate pairs.3 Structured Distributional SemanticsIn this paper, we propose an approach to incorpo-rate structure into distributional semantics (moredetails in Goyal et al (2013)).
The word distribu-tions drawn from the context defined by a set ofrelations anchored on the target word (or phrase)form a set of vectors, namely a matrix for the tar-get word.
One axis of the matrix runs over allthe relations and the other axis is over the distri-butional word vocabulary.
The cells store wordcounts (or PMI scores, or other measures of wordassociation).
Note that collapsing the rows of thematrix provides the standard dependency baseddistributional representation.3.1 Building Representation: The PropStoreTo build a lexicon of SDSM matrices for a givenvocabulary we first construct a proposition knowl-edge base (the PropStore) created by parsing theSimple English Wikipedia.
Dependency arcs arestored as 3-tuples of the form ?w1, r, w2?, denot-ing an occurrence of words w1, word w2 relatedby r. We also store sentence indices for triplesas this allows us to achieve an intuitive techniqueto achieve compositionality.
In addition to thewords?
surface-forms, the PropStore also storestheir POS tags, lemmas, and Wordnet supersenses.This helps to generalize our representation whensurface-form distributions are sparse.The PropStore can be used to query for the ex-pectations of words, supersenses, relations, etc.,around a given word.
In the example in Figure 1,the query (SST(W1) = verb.consumption, ?, dobj)i.e.
?what is consumed?
might return expectations[pasta:1, spaghetti:1, mice:1 .
.
.
].
Relations andPOS tags are obtained using a dependency parserTratz and Hovy (2011), supersense tags using sst-light Ciaramita and Altun (2006), and lemmas us-468Figure 1: Sample sentences & triplesing Wordnet Fellbaum (1998).3.2 Mimicking CompositionalityFor representing intermediate multi-word phrases,we extend the above word-relation matrix symbol-ism in a bottom-up fashion using the PropStore.The combination hinges on the intuition that whenlexical units combine to form a larger syntacticallyconnected phrase, the representation of the phraseis given by its own distributional neighborhoodwithin the embedded parse tree.
The distributionalneighborhood of the net phrase can be computedusing the PropStore given syntactic relations an-chored on its parts.
For the example in Figure1, we can compose SST(w1) = Noun.person andLemma(W1) = eat appearing together with a nsubjrelation to obtain expectations around ?people eat?yielding [pasta:1, spaghetti:1 .
.
. ]
for the objectrelation, [room:2, restaurant:1 .
.
.]
for the locationrelation, etc.
Larger phrasal queries can be built toanswer queries like ?What do people in China eatwith?
?, ?What do cows do?
?, etc.
All of this helpsus to account for both relation r and knowledge Kobtained from the PropStore within the composi-tional framework c = f(a, b, r,K).The general outline to obtain a compositionof two words is given in Algorithm 1, whichreturns the distributional expectation around thecomposed unit.
Note that the entire algorithm canconveniently be written in the form of databasequeries to our PropStore.Algorithm 1 ComposePair(w1, r, w2)M1 ?
queryMatrix(w1) (1)M2 ?
queryMatrix(w2) (2)SentIDs?M1(r) ?M2(r) (3)return ((M1?
SentIDs) ?
(M2?
SentIDs)) (4)For the example ?noun.person nsubj eat?, steps(1) and (2) involve querying the PropStore for theindividual tokens, noun.person and eat.
Let the re-sulting matrices be M1 and M2, respectively.
Instep (3), SentIDs (sentences where the two wordsappear with the specified relation) are obtained bytaking the intersection between the nsubj compo-nent vectors of the two matrices M1 and M2.
Instep (4), the entries of the original matrices M1and M2 are intersected with this list of commonSentIDs.
Finally, the resulting matrix for the com-position of the two words is simply the union ofall the relationwise intersected sentence IDs.
Intu-itively, through this procedure, we have computedthe expectation around the words w1 and w2 whenthey are connected by the relation ?r?.Similar to the two-word composition process,given a parse subtree T of a phrase, we obtainits matrix representation of empirical counts overword-relation contexts (described in Algorithm 2).Let the E = {e1 .
.
.
en} be the set of edges in T ,ei = (wi1, ri, wi2)?i = 1 .
.
.
n.Algorithm 2 ComposePhrase(T )SentIDs?
All Sentences in corpusfor i = 1?
n doMi1 ?
queryMatrix(wi1)Mi2 ?
queryMatrix(wi2)SentIDs?
SentIDs ?
(M1(ri) ?M2(ri))end forreturn ((M11?
SentIDs) ?
(M12?
SentIDs)?
?
?
?
(Mn1?
SentIDs) ?
(Mn2?
SentIDs))The phrase representations becomes sparser asphrase length increases.
For this study, we restrictphrasal query length to a maximum of three words.3.3 Event CoreferentialityGiven the SDSM formulation and assuming nosparsity constraints, it is possible to calculate469SDSM matrices for composed concepts.
However,are these correct?
Intuitively, if they truly capturesemantics, the two SDSM matrix representationsfor ?Booth assassinated Lincoln?
and ?Booth shotLincoln with a gun" should be (almost) the same.To test this hypothesis we turn to the task of pre-dicting whether two event mentions are coreferentor not, even if their surface forms differ.
It may benoted that this task is different from the task of fullevent coreference and hence is not directly compa-rable to previous experimental results in the liter-ature.
Two mentions generally refer to the sameevent when their respective actions, agents, pa-tients, locations, and times are (almost) the same.Given the non-compositional nature of determin-ing equality of locations and times, we representeach event mention by a triple E = (e, a, p) forthe event, agent, and patient.In our corpus, most event mentions are verbs.However, when nominalized events are encoun-tered, we replace them by their verbal forms.
Weuse SRL Collobert et al (2011) to determine theagent and patient arguments of an event mention.When SRL fails to determine either role, its empir-ical substitutes are obtained by querying the Prop-Store for the most likely word expectations forthe role.
It may be noted that the SDSM repre-sentation relies on syntactic dependancy relations.Hence, to bridge the gap between these relationsand the composition of semantic role participantsof event mentions we empirically determine thosesyntactic relations which most strongly co-occurwith the semantic relations connecting events,agents and patients.
The triple (e, a, p) is thus thecomposition of the triples (a, relationsetagent, e)and (p, relationsetpatient, e), and hence a com-plex object.
To determine equality of this complexcomposed representation we generate three levelsof progressively simplified event constituents forcomparison:Level 1: Full Composition:Mfull = ComposePhrase(e, a, p).Level 2: Partial Composition:Mpart:EA = ComposePair(e, r, a)Mpart:EP = ComposePair(e, r, p).Level 3: No Composition:ME = queryMatrix(e)MA = queryMatrix(a)MP = queryMatrix(p)To judge coreference betweenevents E1 and E2, we compute pair-wise similarities Sim(M1full,M2full),Sim(M1part:EA,M2part:EA), etc., for eachlevel of the composed triple representation.
Fur-thermore, we vary the computation of similarityby considering different levels of granularity(lemma, SST), various choices of distancemetric (Euclidean, Cityblock, Cosine), andscore normalization techniques (Row-wise, Full,Column-collapsed).
This results in 159 similarity-based features for every pair of events, which areused to train a classifier to decide conference.4 ExperimentsWe evaluate our method on two datasets and com-pare it against four baselines, two of which usewindow based distributional vectors and two thatemploy weaker forms of composition.4.1 DatasetsIC Event Coreference Corpus: The dataset(Hovy et al, 2013), drawn from 100 news articlesabout violent events, contains manually createdannotations for 2214 pairs of co-referent and non-coreferent events each.
Where available, events?semantic role-fillers for agent and patient are an-notated as well.
When missing, empirical substi-tutes were obtained by querying the PropStore forthe preferred word attachments.EventCorefBank (ECB) corpus: This corpus(Bejan and Harabagiu, 2010) of 482 documentsfrom Google News is clustered into 45 topics,with event coreference chains annotated over eachtopic.
The event mentions are enriched with se-mantic roles to obtain the canonical event struc-ture described above.
Positive instances are ob-tained by taking pairwise event mentions withineach chain, and negative instances are generatedfrom pairwise event mentions across chains, butwithin the same topic.
This results in 11039 posi-tive instances and 33459 negative instances.4.2 BaselinesTo establish the efficacy of our model, we compareSDSM against a purely window-based baseline(DSM) trained on the same corpus.
In our exper-iments we set a window size of seven words.
Wealso compare SDSM against the window-basedembeddings trained using a recursive neural net-work (SENNA) (Collobert et al, 2011) on bothdatsets.
SENNA embeddings are state-of-the-artfor many NLP tasks.
The second baseline uses470IC Corpus ECB CorpusPrec Rec F-1 Acc Prec Rec F-1 AccSDSM 0.916 0.929 0.922 0.906 0.901 0.401 0.564 0.843Senna 0.850 0.881 0.865 0.835 0.616 0.408 0.505 0.791DSM 0.743 0.843 0.790 0.740 0.854 0.378 0.524 0.830MVC 0.756 0.961 0.846 0.787 0.914 0.353 0.510 0.831AVC 0.753 0.941 0.837 0.777 0.901 0.373 0.528 0.834Table 1: Cross-validation Performance on IC and ECB datasetSENNA to generate level 3 similarity features forevents?
individual words (agent, patient and ac-tion).
As our final set of baselines, we extend twosimple techniques proposed by (Mitchell and Lap-ata, 2008) that use element-wise addition and mul-tiplication operators to perform composition.
Weextend it to our matrix representation and buildtwo baselines AVC (element-wise addition) andMVC (element-wise multiplication).4.3 DiscussionAmong common classifiers, decision-trees (J48)yielded best results in our experiments.
Table 1summarizes our results on both datasets.The results reveal that the SDSM model con-sistently outperforms DSM, SENNA embeddings,and the MVC and AVC models, both in termsof F-1 score and accuracy.
The IC corpus com-prises of domain specific texts, resulting in highlexical overlap between event mentions.
Hence,the scores on the IC corpus are consistently higherthan those on the ECB corpus.The improvements over DSM and SENNA em-beddings, support our hypothesis that syntax lendsgreater expressive power to distributional seman-tics in compositional configurations.
Furthermore,the increase in predictive accuracy over MVC andAVC shows that our formulation of compositionof two words based on the relation binding themyields a stronger form of compositionality thansimple additive and multiplicative models.Next, we perform an ablation study to deter-mine the most predictive features for the task ofevent coreferentiality.
The forward selection pro-cedure reveals that the most informative attributesare the level 2 compositional features involvingthe agent and the action, as well as their individ-ual level 3 features.
This corresponds to the in-tuition that the agent and the action are the prin-cipal determiners for identifying events.
Featuresinvolving the patient and level 1 features are leastuseful.
This is probably because features involv-ing full composition are sparse, and not as likelyto provide statistically significant evidence.
Thismay change as our PropStore grows in size.5 Conclusion and Future WorkWe outlined an approach that introduces structureinto distributed semantic representations gives usan ability to compare the identity of two repre-sentations derived from supposedly semanticallyidentical phrases with different surface realiza-tions.
We employed the task of event coreferenceto validate our representation and achieved sig-nificantly higher predictive accuracy than severalbaselines.In the future, we would like to extend our modelto other semantic tasks such as paraphrase detec-tion, lexical substitution and recognizing textualentailment.
We would also like to replace our syn-tactic relations to semantic relations and explorevarious ways of dimensionality reduction to solvethis problem.AcknowledgmentsThe authors would like to thank the anonymous re-viewers for their valuable comments and sugges-tions to improve the quality of the paper.
Thiswork was supported in part by the followinggrants: NSF grant IIS-1143703, NSF award IIS-1147810, DARPA grant FA87501220342.ReferencesMarco Baroni and Alessandro Lenci.
2010.
Distri-butional memory: A general framework for corpus-based semantics.
Comput.
Linguist., 36(4):673?721,December.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on Empirical471Methods in Natural Language Processing, EMNLP?10, pages 1183?1193, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Cosmin Adrian Bejan and Sanda Harabagiu.
2010.Unsupervised event coreference resolution with richlinguistic features.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 1412?1422, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Massimiliano Ciaramita and Yasemin Altun.
2006.Broad-coverage sense disambiguation and informa-tion extraction with a supersense sequence tagger.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?06, pages 594?602, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Ronan Collobert, Jason Weston, L?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
J. Mach.
Learn.
Res., 999888:2493?2537,November.Georgiana Dinu and Mirella Lapata.
2010.
Measuringdistributional similarity in context.
In Proceedingsof the 2010 Conference on Empirical Methods inNatural Language Processing, EMNLP ?10, pages1162?1172, Stroudsburg, PA, USA.
Association forComputational Linguistics.Katrin Erk and Sebastian Pad?.
2008.
A structuredvector space model for word meaning in context.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?08,pages 897?906, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Katrin Erk.
2007.
A simple, similarity-based modelfor selectional preferences.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.John R. Firth.
1957.
A Synopsis of Linguistic Theory,1930-1955.
Studies in Linguistic Analysis, pages 1?32.Kartik.
Goyal, Sujay Kumar Jauhar, Mrinmaya Sachan,Shashank Srivastava, Huiying Li, and Eduard Hovy.2013.
A structured distributional semantic model: Integrating structure with semantics.
In Proceed-ings of the 1st Continuous Vector Space Models andtheir Compositionality Workshop at the conferenceof ACL 2013.Edward Grefenstette, Mehrnoosh Sadrzadeh, StephenClark, Bob Coecke, and Stephen Pulman.
2011.Concrete sentence spaces for compositional distri-butional models of meaning.
In Proceedings of theNinth International Conference on ComputationalSemantics, IWCS ?11, pages 125?134, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of the 2010 Workshop onGEometrical Models of Natural Language Seman-tics, GEMS ?10, pages 33?37, Stroudsburg, PA,USA.
Association for Computational Linguistics.Aria Haghighi and Dan Klein.
2009.
Simple coref-erence resolution with rich syntactic and semanticfeatures.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Process-ing: Volume 3 - Volume 3, EMNLP ?09, pages 1152?1161, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.E.H.
Hovy, T. Mitamura, M.F.
Verdejo, J. Araki, andA.
Philpot.
2013.
Events are not simple: Iden-tity, non-identity, and quasi-identity.
In Proceedingsof the 1st Events Workshop at the conference of theHLT-NAACL 2013.Heeyoung Lee, Marta Recasens, Angel Chang, MihaiSurdeanu, and Dan Jurafsky.
2012.
Joint entityand event coreference resolution across documents.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,EMNLP-CoNLL ?12, pages 489?500, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Christopher D. Manning, Prabhakar Raghavan, andHinrich Sch?tze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press, New York,NY, USA.Diana McCarthy and John Carroll.
2003.
Disam-biguating nouns, verbs, and adjectives using auto-matically acquired selectional preferences.
Comput.Linguist., 29(4):639?654, December.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant word senses inuntagged text.
In Proceedings of the 42nd AnnualMeeting on Association for Computational Linguis-tics, ACL ?04, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL-08: HLT, pages 236?244.Patrick Pantel and Dekang Lin.
2000.
Word-for-wordglossing with contextually similar words.
In Pro-ceedings of the 1st North American chapter of theAssociation for Computational Linguistics confer-ence, NAACL 2000, pages 78?85, Stroudsburg, PA,USA.
Association for Computational Linguistics.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
A multi-pass sieve for coreference resolution.
In Proceed-ings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?10,472pages 492?501, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Sebastian Rudolph and Eugenie Giesbrecht.
2010.Compositional matrix-space models of language.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, ACL ?10,pages 907?916, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Hinrich Sch?tze.
1998.
Automatic word sense dis-crimination.
Comput.
Linguist., 24(1):97?123.Diarmuid ?
S?aghdha and Anna Korhonen.
2011.Probabilistic models of similarity in syntactic con-text.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?11, pages 1047?1057, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic com-positionality through recursive matrix-vector spaces.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,EMNLP-CoNLL ?12, pages 1201?1211, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrasecoreference resolution: making sense of the state-of-the-art.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 2 - Volume 2,ACL ?09, pages 656?664, Stroudsburg, PA, USA.Association for Computational Linguistics.Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fer-nandes, and Gregory Marton.
2003.
Quantitativeevaluation of passage retrieval algorithms for ques-tion answering.
In SIGIR, pages 41?47.Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations us-ing syntactically enriched vector models.
In Pro-ceedings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics, ACL ?10, pages948?957, Stroudsburg, PA, USA.
Association forComputational Linguistics.Stephen Tratz and Eduard Hovy.
2011.
A fast, ac-curate, non-projective, semantically-enriched parser.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?11, pages 1257?1268, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Hila Weisman, Jonathan Berant, Idan Szpektor, and IdoDagan.
2012.
Learning verb inference rules fromlinguistically-motivated evidence.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, EMNLP-CoNLL ?12,pages 194?204, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.S.
K. M. Wong and Vijay V. Raghavan.
1984.
Vectorspace model of information retrieval: a reevaluation.In Proceedings of the 7th annual international ACMSIGIR conference on Research and development ininformation retrieval, SIGIR ?84, pages 167?185,Swinton, UK.
British Computer Society.473
