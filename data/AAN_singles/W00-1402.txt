A Task-based Framework to Evaluate Evaluative ArgumentsGiuseppe CareniniIntelligent Systems ProgramUniversity of Pittsburgh, Pittsburgh, PA 15260, USAcarenini@cs.pitt.eduAbstractWe present an evaluation framework inwhich the effectiveness of evaluativearguments can be measured with real users.The framework is based on the task-efficacyevaluation method.
An evaluative argumentis presented in the context of a decision taskand measures related to its effectiveness areassessed.
Within this framework, we arecurrently running a formal experiment overify whether argument effectiveness canbe increased by tailoring the argument to theuser and by varying the degree of argumentconciseness.IntroductionEmpirical methods are fundamental in anyscientific endeavour to assess progress and tostimulate new research questions.
As the field ofNLG matures, we are witnessing a growinginterest in studying empirical methods toevaluate computational models of discoursegeneration (Dale, Eugenio et al 1998).However, with the exception of (Chu-Carrolland Carberry 1998), little attention as been paidto the evaluation of systems generatingevaluative arguments, communicative acts thatattempt o affect the addressee's attitudes (i.e.evaluative tendencies typically phrased in termsof like and dislike or favor and disfavor).The ability to generate valuative arguments icritical in an increasing number of onlinesystems that serve as .personal :assistants,advisors, or sales asslstants ~.For instance, atravel assistant may need to compare twovacation packages and argue that its current usershould like one more than the other.i See for instance www.activebuyersguide.comIn this paper, we present an evaluationframework in which the effectiveness ofevaluative arguments can be measured with realusers.
The measures of argument effectivenessused in our framework are based on principlesdeveloped in social psychology to studypersuasion (Miller and Levine 1996).
We arecurrently applying the framework to evaluatearguments generated by an argument generatorwe have developed (Carenini 2000).
To facilitatethe evaluation of specific aspects of thegeneration process, the argument generator hasbeen designed so that its functional componentscan be easily turned-offor changed.In the remainder of the paper, we first describeour argument generator.
Then, we summarizeliterature on persuasion from social psychology.Next, we discuss previous work on evaluatingNLG models.
Finally, we describe ourevaluation framework and the design of anexperiment we are currently running.1 The Argument GeneratorThe architecture of the argument generator is atypical pipelined architecture comprising adiscourse planner, a microplanner and a sentencereal izer.The input to the planning process is an abstractevaluative communicative action expressing:- The subject of the evaluation, which can bean entity or a comparison between twoentities in the~.domain of interest (e.g., ahouse or a comparison between two housesin the real-estate domain).An evaluation, which is a number in theinterval \[0,1\] where, depending on thesubject, 0 means "terrible" or "much worse"and 1 means "excellent" or "much better").Given an abstract communicative action, thediscourse planner (Young and Moore 1994)selects and arranges the content of the argument3.17 Is an Interesting house.
In fact.
Ihe quality of house 3-17 Is,.louse 3-17 offers a beaull/tJI view.
And also it looks wonderful.rrnore, house 3-17 has a convenient location in the Weslendorhood.
Even Ihough It Is somewhat ar from lhe pan (1.8 miles).3-17 Is close Io work (1.7 miles).
And also the Iraffic Is moderate on3.-17 iS on interesllng house.
In fact.
n has a reasonable location in ~i~.L~lle.~estend .nelglilbomoodz.~wenllaor, lghl~ouse :.3ot~'- ts',aomewhM,tarltom -~I::~he park (1.8 miles) and far fi-om shops (4 mlles), tl Is close to work (1.7 miles) (~\[,-iland a rapid Iranspodatlon slop ( 1 miles).
And also the traffic is moderate on', 3rd street.
Fudhermore.
amenities are attractive, nhas a very spaciousi garden (2000 sqll.
), a spac ous porch (250 sqll.)
and a large deck (220 sqh.).IFIna,y.
Ihe qualm of house 3-17 Is good.
Allhough house 3-17 is In the ~ ;-i~~?torian style.
N offers a beaut~l v|ew.
And also it loom wonderful ~I~iHouse 3- t 7 is an interesting house.
House 3-17 has e reasonable location.House 3-17 Issomewhat far hom the park i l .8 mllesl.
House 1-17 Isfar i~i~!~om shops (4. mUesl.
Howae 3--17 Is close Io a rapid transportation stop (1 :mtles).House 3-17 is?loseloworktl.7ralles).Thelratllclsmoderateon3rd street.
House 3- t 7 has a location In the Weslend neighborhood.
House3`17 hes excellent amenities.
House 3-17 has a very spaclous garden(2000 sqfl.).
House 3-17 has a spacious porch (250 sqft.
), House 3,-17 hasa large deck (220 sqlt.
), The quality of house 3-17 is good.
House 3-.17 is inIhe viclorian slyte.
House 3- t 7 offers a beautiful view.
House 3-17 looks ~iwondelflll.Figure 1 Sampleby decomposing abstract communicative actionsinto primitive ones and by imposing appropriateordering constraints among communicativeactions.
Two knowledge sources are involved inthis process:- A complex model of the user's preferencesbased on multiattribute utilility theory(MAUT)(Clemen 1996).- A set of plan operators, implementingguidelines for content selection andorgan(sat(on from argumentation theory(Carenini and Moore 2000).By using these two knowledge sources, thediscourse planner produces a text plan for anargument whose content and organization aretailored to the user according to argumentationtheory.Next, the text plan is passed tO the"microptannerwhich performs aggregation, pronominalizationand makes decisions about cue phrases.Aggregation is performed according to heuristicssimilar to the ones proposed in (Shaw 1998).
Forpronominalization, simple rules based oncentering are applied (Grosz.
Josh( et al 1995).arguments in order of decreasing expected effectiveness for the target user SUB J-26Finally, decisions about cue phrases are madeaccording to a decision tree based onsuggestions from (Knott 1996; di Eugenio,Moore et el.
1997) .
The sentence realizerextends previous work on realizing evaluativestatements (Elhadad 1995).The argument generator has been designed tofacilitate the testing of the effectiveness ofdifferent aspects of the generation process.
Theexperimenter can easily vary the expectedeffectiveness of the generated arguments bycontrolling whether the generator tailors theargument o the current user, the degree ofconciseness of the generated arguments andwhat microplanning tasks are performed.Figure 1 shows three arguments generated by theargument generator that clearly illustrate thisfeature.
We-expect the first-argument to be veryeffective for the target user.
Its content andorganization has been tailored to herpreferences.
Also, the argument is reasonablyfluent because of aggregation, pronominalizationand cue phrases.
In contrast, we expect thesecond argument to be less effective with our10target user, because it is not tailored to herpreferences 2, and it appears to be somewhat tooverbose 3.Finally, we expect he third argumentsnot to be effective at all.
It suffers from all theshortcomings of the second argument, with theadditional weakness of not being fluent (nomicroplannig tasks were performed).2A final note-on the evaluation of arguments.
An ........argument can also be evaluated by the argumentaddressee with respect o several dimensions ofquality, such as coherence, content,organization, writing style and convincingness.However, evaluations based on judgementsalong these dimensions are clearly weaker thanevaluations measuring actual attitudinal andArguing an evaluation involves an intentionalcommunicative act that attempts to affect thecurrent or future behavior of the addressees bycreating, changing or reinforcing the addressees'attitudes.
It follows that the effectiveness of anevaluative argument can be tested by comparingmeasurements of subjects' attitudes or behaviorbefore and after their exposure to the argument.In many experimental situations, however,measuring effects on overt behavior can beproblematic (Miller and Levine 1996), thereforemost research on persuasion has been basedeither on measurements of attitudes or ondeclaration of behavioral intentions.
The mostcommon technique to measure attitudes issubject self-report (Miller and Levine 1996).Typically, self-report measurements involve theuse of a scale that consists of two "'bipolar"terms (e.g., good-choice vs. bad-choice), usuallyseparated by seven or nine equal spaces thatparticipants use to evaluate an attitude or beliefstatement (see Figure 4 for examples).Research in persuasion suggests that someindividuals may be naturally more resistant opersuasion than others (Miller and Levine 1996).Individual features that seem to matter are:argumentativeness (tendency to argue)(lnfanteand Rancer 1982), intelligence, self-esteem andneed for cognition (tendency to engage in and toenjoy effortful cognitive endeavours)(Cacioppo,Petty et al 1983).
Any experiment in persuasionshould control for these variables.. Research  .......
An=,~:x~s~P~sychotogy., :~.
:~-.on.
~, :.=~boh~viomLchanges:(Qlso;amt~anna 199 !-).
?Persuas ion3 Eva luat ion  o f  NLG Mode ls.
.
.
.
.
_2 This argument .was tailored"to" a .default averageuser, for whom all aspects of a house are equallyimportant.
With respect o the first argument, noticethe different evaluation for the location and thedifferent order between the two text segments aboutlocation and quality.3 A threshold controlling verbosity was set to itsmaximum value.Several empirical methods have been proposedand applied in the literature for evaluating NLGmodels.
We discuss now why, among the threemain evaluation methods (i.e., human judges,corpus-based and task efficacy), task efficacyappears to be the most appropriate for testing theeffectiveness of evaluative arguments that aretailored to a complex model of the user'spreferences.The human judges evaluation method requires apanel of judges to score outputs of generationmodels (Chu-Carroll and Carberry 1998; Lesterand Porter March 1997).
The main limitation ofthis approach is that the input of the generationprocess needs to be simple enough to be easilyunderstood by judges 4.
Unfortunately, this is notthe case for our argument generator, where theinput consists of a possibly complex and novelargument subject (e.g., a new house with a largenumber of features), and a complex model of theuser's preferences.The corpus-based valuation method (Robin andMcKeown 1996) can be applied only when acorpus of input/output pairs is available.
Aportion of the corpus (the training set) is used todevelop a computational model of how theoutput can be generated from the input.
The restof the corpus (the testing set) is used to evaluatethe model.
Unfortunately, a corpus for ourgenerator does.
not exist.
Furthermore, it wouldbe difficult and extremely time-consuming toobtain and analyze such a corpus given thecomplexity of our.
generator..input/output pairs.4 See (Chu-Carroll and Carberry 1998) tbr anillustration of how the specification of the contextcan become xtremely complex when human judgesare used to evaluate content selection strategies for adialog system.11?
When a generator-is designed to generate outputfor users engaged in certain tasks, a natural wayto evaluate its effectiveness is by experimentingwith users performing those tasks.
For instance,in (Young, to appear) different models forgenerating natural language descriptions of plansare evaluated by measuring how effectivelyusers execute those plans given the descriptions.main sub-systems: the'IDEA system,- a Usei- ......Model Refiner and the Argument Generator.
Theframework assumes that a model of the user'spreferences based on MAUT has beenpreviously acquired using traditional methodsfrom decision theory (Edwards and Barron1994), to assure a reliable initial model.At the onset, the user is assigned the task to.This evaluatiort~ ,method,.,,~cal!ed~.
:task.~.~effiea~y,,~ ~.
, z,select~.?rara~,~\]~,:dataset..
:.the:zfaur.most p~e?eazed .....allows one to evaluate a generation modelwithout explicitly evaluating its output but bymeasuring the output's effects on user'sbehaviors, beliefs and attitudes in the context ofthe task.
The only requirement for this method isthe specification of a sensible task.Task efficacy is the method we have adopted inour evaluation framework.4 The Evaluation Framework4.1 The taskAiming at general results, we chose a ratherbasic and frequent task that has been extensivelystudied in decision analysis: the selection of asubset of preferred objects (e.g., houses) out of aset of possible alternatives by considering trade-offs among multiple objectives (e.g., houselocation, house quality).
The selection isperformed by evaluating objects with respect totheir values for a set of primitive attributes (e.g.,house distance form the park, size of thegarden).
In the evaluation framework we havedeveloped, the user performs this task by using acomputer environment (shown in Figure 3) thatsupports interactive data exploration andanalysis (IDEA) (Roth, Chuah et al 1997).
TheIDEA environment provides the user with a setof powerful visualization and directmanipulation techniques that facilitate user'sautonomous exploration of the set of alternativesand the selection of the preferred alternatives.Let's examine now how the argument generator,that we described in Section 1, can be evaluatedin the ,context of the ~selection =ta.sk, by gging.through the architecture of the evaluationframework.4.2 The f ramework  architecturealternatives and to place them in the Hot List(see Figure 3 upper right comer) ordered bypreference.
The IDEA system supports the userin this task (Figure 2 (I)).
As the interactionunfolds, all user actions are monitored andcollected in the User's Action History (Figure 2(2a)).
Whenever the user feels that she hasaccomplished the task, the ordered list ofpreferred alternatives is saved as her PreliminaryDecision (Figure 2 (2b)).
After that, this list, theUser's Action History and the initial Model ofUser's Preferences are analysed by the UserModel Refiner (Figure 2 (3)) to produce aRefined Model of the User's Preferences (Figure2 (4)).At this point, the stage is set for argumentgeneration.
Given the Refined Model of theUser's Preferences for the target selection task,the Argument Generator produces an evaluativeargument tailored to the user's model (Figure 2(5-6)).
Finally, the argument is presented to theuser by the IDEA system (Figure 2 (7)).The argument goal is to introduce a newalternative (not included in the dataset initiallypresented to the user) and to persuade the userthat the alternative is worth being considered.The new alternative is designed on the fly to bepreferable for the user given her preferencemodel.
Once the argument is presented, the usermay (a) decide to introduce the new alternativein her Hot List, or (b) decide to further explorethe dataset, possibly making changes to the HotList and introducing the new instance in the HotList, or (c) do nothing.
Figure 3 shows thedisplay at the end of the interaction, when theuser; after reading the argument, has decided tointroduce the new alternative in the firstposition.Figure 2 shows the architecture of the evaluationframework.
The framework consists of three12.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
L .
  ?
.
.
.
.
.
.
.
.
.
.
, .MeasmeJ of sxgmmmqs _ .
efe?live~m arewxxxSnaS~ned / Da~e~ ~ 5\].
3 4 ~  User, Pre~ere~User's Actions ~~ ~  Pre~ren~a- ~  J Cna,ed o~ MAWSub-sy~em.~ Inpu~sK3utputsFigure 2 The evaluation f ramework  architectureWhenever the user decides to stop exploring andis satisfied and confident with her finalselections, measures related to argument'seffectiveness can be assessed (Figure 2 (8)).These measures are obtained either from therecord of the user interaction with the system orfrom user self-reports (see Section 2).First, and most important, are measures ofbehavioral intentions and attitude change: (a)whether or not the user adopts the new proposedalternative, (b) in which position in the Hot Listshe places it, (c) how much she likes it, (d)whether or not the user revises the Hot List and(e) how much the user likes the objects in theHot List.
Second, a measure can be obtained ofthe user's confidence that she has selected thebest for her in the set of alternatives.
Third, ameasure of argument effectiveness can also bederived by explicitly questioning the user at theend of the interaction, about the rationale for herdecision.
This can be done either by asking theuser to justify her decision in a written.paragraph, or by asking,the user to :self-reportfor each attribute of the new house howimportant the attribute was in her decision (Olsoand Zanna 1991).
Both methods can providevaluable information on what aspects of theargument were more influential (i.e., betterunderstood and accepted by the user).A fourth measure of argument effectiveness is toexplicitly ask the user at the end of theinteraction to judge the argument with respect toseveral dimensions of quality, such as content,organization, writing style and convincigness.Evaluations based on judgments along thesedimensions are clearly weaker than evaluationsmeasuring actual behavioural and attitudinalchanges (Olso and Zanna 1991).
However, thesejudgments may provide more information thanjudgments from independent judges (as in the"human judges" method iscussed in Section 3),because they are performed by the addressee ofthe argument, when the experience of the task isstill vivid in her memory.To summarize, the evaluation framework justdescribed supports users in performing arealistic task at their own pace by-interactingwith an IDEA system.
In the context of this task,an evaluative argument is generated andtneasurements ,related to its effectiveness can be.
'performed.In the next section, we discuss an experimentthat we are currently running by using theevaluation framework.13? '
" , ' -  '.
: / : - . '
" " ; ,  ' : ~ i l i ~z ~ L .
_ - - .
- - - - - .
- -3  I.,.,-7...2,.....,.1141~nl  I IGlwden_glze P ~?=h_Sl:r~ Deck?
m ~.
?I m lie, /  - .
. "
" ' ,  Norghs lde-4 ~f .%'~"  " \ ~o .
.
.
.
.
.2~d , .
- .
>% 2rld............... ~ ~.. T~ .
~A University \[8 "\:~ ~I " i 8h,~pplng \]3rdWes~end i: :~ ~Eas  tend,?PIO ?.rcf~ r,~.
: ;,, C ~'Io ~nrnR rtt~,z ,i he safe Eastend neigflbOdl0od.
Even |hough house 3-,26 is somewhat faram a rapid i ranspod~idn  stop !
!
.6 miles).
It is ?idso towork  {1.8 miles ).
AndIhe traffic Is modera le  on ~d streel.
Furlherrnore.
the quality of houseis good.
Houso ~ offecz ?
be~lh~ ~ o( Iho  rlrver Anti aItsoooks beautiful,Figure 3 The IDEA environment display at the end of the interaction5 The Exper imentAs explained in Section 1, the argumentgenerator has been designed to facilitate testingof the effectiveness of different aspects of thegeneration process.
The experimenter can easilycontrol whether the generator tailors theargument to the current user, the degree ofconciseness of the argument, and whatmicroplanning tasks are performed.
In our initialexperiment, because of  limited financial andhuman resources, we focus on the first twoaspects for arguments about a single entity.
Notbecause we =are not int~ested in  effecti.veness ofperforming microplanning tasks, but because weconsider effectiveness of  tailoring andconciseness somewhat more difficult, andtherefore more interesting to prove.Thus, we designed a between-subjectsexperiment with four experimental conditions:No-Argument  - subjects are simply informed thata new house came on the market.Ta i lo red-Conc ise  - subjects are presented withan evaluation of  the new house tailored to theirpreferences and at a level of conciseness that wehypothesize to be optimal.Non-Ta i lo red-Conc ise  - subjects are presentedwith an evaluation of the new house which is nottailored to their preferences 5, but is at a level ofconciseness that we hypothesize to be optimal.Ta i lo red-Verbose  - subjects are presented withan evaluation,of the .new- house..tailored to theirpreferences, but at a level of conciseness that wehypothesize to be too low.s The evaluative argument is tailored to a defaultaverage user, for whom all aspects of a house areequally important.14a) How would you judge the houses in your Hot List?The more you like the house the closer you should put a cross to "good cho ice"I st housebad cho ice  : " " : : _ _  : _ _  : _ _  : _ _  : : good  cho ice2 nd housebad cho ice  :3 rd housebad cho ice  :4 th housebad cho ice  :_ _  : " " : : _ _  " : _ _  : : good  cho ice: : : : : : _ _  : : : good  cho ice: : " " : _ _  : _ _  "' " : :: _ _  i good  cho ice  "b) How sure are you that you have selected the four best houses among the ones available?Unsure  : .
.
.
.
: : : : : : Sure.
.
.
.  "
/>  Z .
- '~ .
~ i ? '
.~~Figure 4 Excerpt from questionnaire that subjects fill out at the end of-the interactionIn the four conditions, all the information aboutthe new house is also presented graphically.
Ourhypotheses on the outcomes of the experimentcan be summarized as follows.
We expectarguments generated for the Tailored-Concisecondition to be more effective than argumentsgenerated for both the Non-Tailored-Conciseand Tailored-Verbose conditions.
We alsoexpect the Tailored-Concise condition to besomewhat better than the No-Argumentcondititon, but to a lesser extent, becausesubjects, in the absence of any argument, mayspend more time further exploring the dataset,therefore reaching a more informed andbalanced decision.
Finally, we do not havestrong hypotheses on comparisons of argumenteffectiveness among the No-Argument, Non-Tailored-Concise and Tailored-Verboseconditions.The design of our evaluation framework andconsequently the design of this experiment takeinto account hat the effectiveness of argumentsis determined not only by the argument itselKbut also by user's traits such asargumentativeness, need for cognition, self-esteem and intelligence (as described in Section2).
Furthermore, we assume that argumenteffectiveness can be measured by means of thebehavioral intentions and self-reports describedin Section 4.2.The experiment is organized in two phases.-Inthe first phase, the subject fills out threequestionnaires on the Web.
One questionnaireimplements a method from decision theory toacquire a model of the subject's preferences(Edwards and Barton 1994).
The secondquestionnaire assesses the subject'sargumentativeness (lnfante and Rancer 1982).The last one assesses the subject's need forcognition (Cacioppo, PeW et al 1984).
In thesecond phase of the experiment, o control forother possible confounding variables (includingintelligence and self-esteem), the subject israndomly assigned to one of the four conditions.Then, the subject interacts with the evaluationframework and at the end of the interactionmeasures of the argument effectiveness arecollected.
Some details on measures based onsubjects' self-reports can be examined in Figure4, which shows an excerpt from the finalquestionnaire that subjects are asked to fill out atthe end of the interaction.After running the experiment with 8 pilotsubjects to refine and improve the experimentalprocedure, we are currently running a formalexperiment involving 40 subjects, I0 in eachexperimental conditions.Future  WorkIn this paper, we propose a task-basedframework to evaluate evaluative arguments.We are currently using this framework to run aformal experiment to evaluate arguments about asingle entity.
However, this is only a first step.The power of the framework is that it enablesthe design and execution of many different:experiments .
about- evaluative .arguments.
Thegoal of our current experiment is to verifywhether tailoring an evaluative argument to theuser and varying the degree of argumentconciseness influence argument effectiveness.We envision further experiments along thefollowing lines.15In the short "term, we plan to stcidy morecomplex arguments, including comparisonsbetween two entities, as well as comparisonsbetween mixtures of  entities and set of  entities.One experiment could assess the influence oftailoring and conciseness on the effectiveness ofthese more complex arguments.
Anotherpossible experiment could compare different.
argumentative, strategies for:~ selecting ~andorganizing the content of these arguments.
In thelong term, we intend to evaluate techniques togenerate evaluative arguments that combinenatural anguage and information graphics (e.g.,maps, tables, charts).AcknowledgementsMy thanks go to the members of the Autobriefproject: J. Moore, S. Roth, N. Green, S.Kerpedjiev and J. Mattis.
I also thank C. Conatifor comments on drafts of  this paper.
This workwas supported by grant number DAA-1593K0005 from the Advanced ResearchProjects Agency (ARPA).
Its contents are solelyresponsibility of the author.ReferencesCacioppo, J. T., R. E. Petty, et al (1984).
Theefficient Assessment ofneed for Cognition.
Journalof Personality Assessment 48(3): 306-307.Cacioppo, J. T., R. E. Petty, et al (1983).
Effects ofNeed for Cognition on Message Evaluation, Recall,and Persuasion.
Journal of Personality and SocialPsychology 45(4): 805-8 l8.Carenini, G. (2000).
Evaluating MultimediaInteractive Arguments in the Context of DataExploration Tasks.
PhD Thesis, Intelligent SystemProgram, University of Pittsburgh.Carenini, G. and J. Moore (2000).
A Strategy forEvaluating Evaluative arguments Int.
Conferenceon NLG, Mitzpe Ramon, Israel.C, hu-Carroll, J. and S. Carberry (1998).
CollaborativeResponse Generation in Planning Dialogues.Computational Linguistics 24(2): 355-400.Clemen, R. T. (1996).=Making l-lard Decisions: anintroduction to decision analysis.
Belmont,California, Duxbury Press.Dale, R., B. di Eugenio, et al (1998).
hm'oduction tothe Special Issue on NLG.
ComputationalLinguistics 24(3): 345-353.Edwards, W. ~afid F: H. Barron '( 1994 ).:~MJJ?
'?s~n~a :-::~':~2~-:-SMARTER: Improved Simple Methods forMultiattribute Utility Measurements.Organizational Behavior and Human DecisionProcesses 60: 306-325.Elhadad, M. (1995).
Using argumentation in textgeneration.
Journal of Pragmatics 24:189-220.Eugenio, B. D., J. Moore, et al (1997).
Learning_ ..PTeatures;~:~hat.
:Br.edicts:.
~Cue.
: Usage.
ACL97,Madrid, Spain.Grosz, B. J., A. K. Joshi, et al (1995).
Centering: AFramework for Modelling the Local Coherence ofDiscourse.
Computational Linguistics 21(2):203-226.Infante, D. A. and A. S. Rancer (1982).
AConceptualization and Measure ofArgumentativeness.
Journal of Personal ityAssessment 46: 72-80.Knott, A.
(1996).
A Data-Driven Methodology forMotivating a Set of Coherence Relations,University of Edinburgh.Lester, J. C. and B. W. Porter (I 997).
Developing andEmpirically Evaluating Robust ExplanationGenerators: The KNIGHT Experiments.Computational Linguistics 23(1): 65-101.Miller, M. D. and T. R. Levine (1996).
Persuasion.An Integrated Approach to Communication Theoryand Research.
M. B. Salwen and D. W. Stack.Mahwah, New Jersey: 261-276.Olso, J. M. and M. P. Zanna (1991).
Attitudes andbeliefs; Attitude change and attitude-behaviorconsistency.
Social Psychology.
R. M. Baron andW.
G. Graziano.Robin, J. and K. McKeown (1996).
EmpiricallyDesigning and Evaluating a New Revision-BasedModel for Summary Generation.
ArtificialIntelligence Journal, 85, 135-I 79.Roth, S. F., M. C. Chuah, et al (1997).
Towards anInformation Visualization Workspace: CombiningMultiple Means of ?vpression.
Human-ComputerInteraction Journal.Vol.
12, No.
1 & 2, pp.
131-185Shaw, J.
(I 998).
Clause Aggregation UsingLinguistic Knowledge.
9th Int.
Workshop on NLG,Niagara-on-the-Lake, Canada.Young, M. R. Using Grice's McLrim of Quantity toSelecr the Corrtent o f  Plan Descriptions: ArtificialIntelligence Journal, to appear.Young, M. R. and J. D. Moore (1994).
DoesDiscourse Planning Require a Special-PurposePlanner?
Proceedings of the AAAI-94 Workshopon planning for Interagent Communication.
Seattle,WA.16
