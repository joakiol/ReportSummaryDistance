Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 833?841,Beijing, August 2010Evaluation of Dependency Parsers on Unbounded DependenciesJoakim Nivre Laura Rimell Ryan McDonald Carlos Go?mez-Rodr?
?guezUppsala University Univ.
of Cambridge Google Inc. Universidade da Corun?ajoakim.nivre@lingfil.uu.se laura.rimell@cl.cam.ac.uk ryanmcd@google.com cgomezr@udc.esAbstractWe evaluate two dependency parsers,MSTParser and MaltParser, with respectto their capacity to recover unbounded de-pendencies in English, a type of evalu-ation that has been applied to grammar-based parsers and statistical phrase struc-ture parsers but not to dependency parsers.The evaluation shows that when combinedwith simple post-processing heuristics,the parsers correctly recall unboundeddependencies roughly 50% of the time,which is only slightly worse than twogrammar-based parsers specifically de-signed to cope with such dependencies.1 IntroductionThough syntactic parsers for English are re-ported to have accuracies over 90% on the WallStreet Journal (WSJ) section of the Penn Tree-bank (PTB) (McDonald et al, 2005; Sagae andLavie, 2006; Huang, 2008; Carreras et al, 2008),broad-coverage parsing is still far from being asolved problem.
In particular, metrics like attach-ment score for dependency parsers (Buchholz andMarsi, 2006) and Parseval for constituency parsers(Black et al, 1991) suffer from being an aver-age over a highly skewed distribution of differ-ent grammatical constructions.
As a result, in-frequent yet semantically important constructiontypes could be parsed with accuracies far belowwhat one might expect.This shortcoming of aggregate parsing met-rics was highlighted in a recent study by Rimellet al (2009), introducing a new parser evalua-tion corpus containing around 700 sentences an-notated with unbounded dependencies in sevendifferent grammatical constructions.
This corpuswas used to evaluate five state-of-the-art parsersfor English, focusing on grammar-based and sta-tistical phrase structure parsers.
For example, inthe sentence By Monday, they hope to have asheaf of documents both sides can trust., parsersshould recognize that there is a dependency be-tween trust and documents, an instance of objectextraction out of a (reduced) relative clause.
In theevaluation, the recall of state-of-the-art parsers onthis kind of dependency varies from a high of 65%to a low of 1%.
When averaging over the sevenconstructions in the corpus, none of the parsershad an accuracy higher than 61%.In this paper, we extend the evaluation ofRimell et al (2009) to two dependency parsers,MSTParser (McDonald, 2006) and MaltParser(Nivre et al, 2006a), trained on data from thePTB, converted to Stanford typed dependencies(de Marneffe et al, 2006), and combined with asimple post-processor to extract unbounded de-pendencies from the basic dependency tree.
Ex-tending the evaluation to dependency parsers is ofinterest because it sheds light on whether highlytuned grammars or computationally expensiveparsing formalisms are necessary for extractingcomplex linguistic phenomena in practice.
Unlikethe best performing grammar-based parsers stud-ied in Rimell et al (2009), neither MSTParser norMaltParser was developed specifically as a parserfor English, and neither has any special mecha-nism for dealing with unbounded dependencies.Dependency parsers are also often asymptoticallyfaster than grammar-based or constituent parsers,e.g., MaltParser parses sentences in linear time.Our evaluation ultimately shows that the re-call of MSTParser and MaltParser on unboundeddependencies is much lower than the average(un)labeled attachment score for each system.Nevertheless, the two dependency parsers arefound to perform only slightly worse than the bestgrammar-based parsers evaluated in Rimell et al833Each must match Wisman 's "pie" with the fragment that he carries with himnsubj dobjprepaux pobjposs'veprep rcmoddobjnsubjdetpobjpossdobja: ObRCFive things you can do for $ 15,000  or lesspobjnsubjauxrcmodnum numprep cc conjdobjb: ObRedThey will remain on a lower-priority list that includes 17 other countriespobjnsubjauxrcmodnsubjnumprep amoddetnsubjc: SbRCamoddobjWhat you see are self-help projectsnsubjcsubj copamoddobjdobj d: FreeWhat effect does a prism have on lightpobjnsubjauxdetdet prepdobjdobje: ObQNow he felt ready for the many actions he saw spreading out before himpobj rcmodprtmoddetprepacompnsubjnsubjamod xcompnsubjpreppobjg: SbEmadvThe men were at first puzz led, then angered by the aimless tackingpobjcop conjadvmoddetadvmodprepdetprepamodpobjnsubjpassf : RNRFigure 1: Examples of seven unbounded dependency constructions (a?g).
Arcs drawn below each sentence represent thedependencies scored in the evaluation, while the tree above each sentence is the Stanford basic dependency representation,with solid arcs indicating crucial dependencies (cf.
Section 4).
All examples are from the development sets.
(2009) and considerably better than the other sta-tistical parsers in that evaluation.
Interestingly,though the two systems have similar accuraciesoverall, there is a clear distinction between thekinds of errors each system makes, which we ar-gue is consistent with observations by McDonaldand Nivre (2007).2 Unbounded Dependency EvaluationAn unbounded dependency involves a word orphrase interpreted at a distance from its surfaceposition, where an unlimited number of clauseboundaries may in principle intervene.
Theunbounded dependency corpus of Rimell et al(2009) includes seven grammatical constructions:object extraction from a relative clause (ObRC),object extraction from a reduced relative clause(ObRed), subject extraction from a relative clause(SbRC), free relatives (Free), object questions(ObQ), right node raising (RNR), and subject ex-traction from an embedded clause (SbEm), allchosen for being relatively frequent and easy toidentify in PTB trees.
Examples of the con-structions can be seen in Figure 1.
The evalu-ation set contains 80 sentences per construction(which may translate into more than 80 depen-dencies, since sentences containing coordinationsmay have more than one gold-standard depen-dency), while the development set contains be-tween 13 and 37 sentences per construction.
Thedata for ObQ sentences was obtained from variousyears of TREC, and for the rest of the construc-tions from the WSJ (0-1 and 22-24) and Brownsections of the PTB.Each sentence is annotated with one or moregold-standard dependency relations representingthe relevant unbounded dependency.
The gold-standard dependencies are shown as arcs belowthe sentences in Figure 1.
The format of the de-pendencies in the corpus is loosely based on theStanford typed dependency scheme, although theevaluation procedure permits alternative represen-tations and does not require that the parser out-put match the gold-standard exactly, as long as the?spirit?
of the construction is correct.The ability to recover unbounded dependenciesis important because they frequently form part ofthe basic predicate-argument structure of a sen-tence.
Subject and object dependencies in par-ticular are crucial for a number of tasks, includ-ing information extraction and question answer-ing.
Moreover, Rimell et al (2009) show that,although individual types of unbounded depen-dencies may be rare, the unbounded dependencytypes in the corpus, considered as a class, occur inas many as 10% of sentences in the PTB.In Rimell et al (2009), five state-of-the-artparsers were evaluated for their recall on the gold-standard dependencies.
Three of the parsers werebased on grammars automatically extracted fromthe PTB: the C&C CCG parser (Clark and Curran,2007), the Enju HPSG parser (Miyao and Tsujii,2005), and the Stanford parser (Klein and Man-ning, 2003).
The two remaining systems were the834RASP parser (Briscoe et al, 2006), using a man-ually constructed grammar and a statistical parseselection component, and the DCU post-processorof PTB parsers (Cahill et al, 2004) using the out-put of the Charniak and Johnson reranking parser(Charniak and Johnson, 2005).
Because of thewide variation in parser output representations, amostly manual evaluation was performed to en-sure that each parser got credit for the construc-tions it recovered correctly.
The parsers were runessentially ?out of the box?, meaning that the de-velopment set was used to confirm input and out-put formats, but no real tuning was performed.
Inaddition, since a separate question model is avail-able for C&C, this was also evaluated on ObQsentences.
The best overall performers were C&Cand Enju, which is unsurprising since they aredeep parsers based on grammar formalisms de-signed to recover just such dependencies.
TheDCU post-processor performed somewhat worsethan expected, often identifying the existence ofan unbounded dependency but failing to iden-tify the grammatical class (subject, object, etc.
).RASP and Stanford, although not designed to re-cover such dependencies, nevertheless recovereda subset of them.
Performance of the parsers alsovaried widely across the different constructions.3 Dependency ParsersIn this paper we repeat the study of Rimell et al(2009) for two dependency parsers, with the goalof evaluating how parsers based on dependencygrammars perform on unbounded dependencies.MSTParser1 is a freely available implementa-tion of the parsing models described in McDon-ald (2006).
According to the categorization ofparsers in Ku?bler et al (2008) it is a graph-basedparsing system in that core parsing algorithms canbe equated to finding directed maximum span-ning trees (either projective or non-projective)from a dense graph representation of the sentence.Graph-based parsers typically rely on global train-ing and inference algorithms, where the goal is tolearn models in which the weight/probability ofcorrect trees is higher than that of incorrect trees.At inference time a global search is run to find the1http://mstparser.sourceforge.nethighest weighted dependency tree.
Unfortunately,global inference and learning for graph-based de-pendency parsing is typically NP-hard (McDonaldand Satta, 2007).
As a result, graph-based parsers(including MSTParser) often limit the scope oftheir features to a small number of adjacent arcs(usually two) and/or resort to approximate infer-ence (McDonald and Pereira, 2006).MaltParser2 is a freely available implementa-tion of the parsing models described in Nivre etal.
(2006a) and Nivre et al (2006b).
MaltParser iscategorized as a transition-based parsing system,characterized by parsing algorithms that producedependency trees by transitioning through abstractstate machines (Ku?bler et al, 2008).
Transition-based parsers learn models that predict the nextstate given the current state of the system as wellas features over the history of parsing decisionsand the input sentence.
At inference time, theparser starts in an initial state, then greedily movesto subsequent states ?
based on the predictions ofthe model ?
until a termination state is reached.Transition-based parsing is highly efficient, withrun-times often linear in sentence length.
Further-more, transition-based parsers can easily incorpo-rate arbitrary non-local features, since the currentparse structure is fixed by the state.
However, thegreedy nature of these systems can lead to errorpropagation if early predictions place the parserin incorrect states.McDonald and Nivre (2007) compared the ac-curacy of MSTParser and MaltParser along anumber of structural and linguistic dimensions.They observed that, though the two parsers ex-hibit indistinguishable accuracies overall, MST-Parser tends to outperform MaltParser on longerdependencies as well as those dependencies closerto the root of the tree (e.g., verb, conjunction andpreposition dependencies), whereas MaltParserperforms better on short dependencies and thosefurther from the root (e.g., pronouns and noun de-pendencies).
Since long dependencies and thosenear to the root are typically the last constructedin transition-based parsing systems, it was con-cluded that MaltParser does suffer from someform of error propagation.
On the other hand, the2http://www.maltparser.org835richer feature representations of MaltParser led toimproved performance in cases where error prop-agation has not occurred.
However, that study didnot investigate unbounded dependencies.4 MethodologyIn this section, we describe the methodologicalsetup for the evaluation, including parser training,post-processing, and evaluation.34.1 Parser TrainingOne important difference between MSTParser andMaltParser, on the one hand, and the best perform-ing parsers evaluated in Rimell et al (2009), onthe other, is that the former were never developedspecifically as parsers for English.
Instead, theyare best understood as data-driven parser gener-ators, that is, tools for generating a parser givena training set of sentences annotated with de-pendency structures.
Over the years, both sys-tems have been applied to a wide range of lan-guages (see, e.g., McDonald et al (2006), Mc-Donald (2006), Nivre et al (2006b), Hall et al(2007), Nivre et al (2007)), but they come withno language-specific enhancements and are notequipped specifically to deal with unbounded de-pendencies.Since the dependency representation used inthe evaluation corpus is based on the Stanfordtyped dependency scheme (de Marneffe et al,2006), we opted for using the WSJ section ofthe PTB, converted to Stanford dependencies, asour primary source of training data.
Thus, bothparsers were trained on section 2?21 of the WSJdata, which we converted to Stanford dependen-cies using the Stanford parser (Klein and Man-ning, 2003).
The Stanford scheme comes in sev-eral varieties, but because both parsers require thedependency structure for each sentence to be atree, we had to use the so-called basic variety (deMarneffe et al, 2006).It is well known that questions are very rarein the WSJ data, and Rimell et al (2009) foundthat parsers trained only on WSJ data generallyperformed badly on the questions included in the3To ensure replicability, we provide all experimentalsettings, post-processing scripts and additional informationabout the evaluation at http://stp.ling.uu.se/?nivre/exp/.evaluation corpus, while the C&C parser equippedwith a model trained on a combination of WSJand question data had much better performance.To investigate whether the performance of MST-Parser and MaltParser on questions could also beimproved by adding more questions to the train-ing data, we trained one variant of each parserusing data that was extended with 3924 ques-tions taken from QuestionBank (QB) (Judge et al,2006).4 Since the QB sentences are annotated inPTB style, it was possible to use the same conver-sion procedure as for the WSJ data.
However, it isclear that the conversion did not always produceadequate dependency structures for the questions,an observation that we will return to in the erroranalysis below.In comparison to the five parsers evaluated inRimell et al (2009), it is worth noting that MST-Parser and MaltParser were trained on the samebasic data as four of the five, but with a differ-ent kind of syntactic representation ?
dependencytrees instead of phrase structure trees or theory-specific representations from CCG and HPSG.
Itis especially interesting to compare MSTParserand MaltParser to the Stanford parser, which es-sentially produces the same kind of dependencystructures as output but uses the original phrasestructure trees from the PTB as input to training.For our experiments we used MSTParser withthe same parsing algorithms and features as re-ported in McDonald et al (2006).
However, un-like that work we used an atomic maximum en-tropy model as the second stage arc predictor asopposed to the more time consuming sequence la-beler.
McDonald et al (2006) showed that there isnegligible accuracy loss when using atomic ratherthan structured labeling.
For MaltParser we usedthe projective Stack algorithm (Nivre, 2009) withdefault settings and a slightly enriched featuremodel.
All parsing was projective because theStanford dependency trees are strictly projective.4QB contains 4000 questions, but we removed all ques-tions that also occurred in the test or development set ofRimell et al (2009), who sampled their questions from thesame TREC QA test sets.8364.2 Post-ProcessingAll the development and test sets in the corpusof Rimell et al (2009) were parsed using MST-Parser and MaltParser after part-of-speech taggingthe input using SVMTool (Gime?nez and Ma`rquez,2004) trained on section 2?21 of the WSJ data inStanford basic dependency format.
The Stanfordparser has an internal module that converts thebasic dependency representation to the collapsedrepresentation, which explicitly represents addi-tional dependencies, including unbounded depen-dencies, that can be inferred from the basic rep-resentation (de Marneffe et al, 2006).
We per-formed a similar conversion using our own tool.Broadly speaking, there are three ways in whichunbounded dependencies can be inferred from theStanford basic dependency trees, which we willrefer to as simple, complex, and indirect.
In thesimple case, the dependency coincides with a sin-gle, direct dependency relation in the tree.
Thisis the case, for example, in Figure 1d?e, whereall that is required is that the parser identifiesthe dependency relation from a governor to anargument (dobj(see, What), dobj(have,effect)), which we call the Arg relation; nopost-processing is needed.In the complex case, the dependency is repre-sented by a path of direct dependencies in the tree,as exemplified in Figure 1a.
In this case, it isnot enough that the parser correctly identifies theArg relation dobj(carries, that); it mustalso find the dependency rcmod(fragment,carries).
We call this the Link relation, be-cause it links the argument role inside the relativeclause to an element outside the clause.
Other ex-amples of the complex case are found in Figure 1cand in Figure 1f.In the indirect case, finally, the dependencycannot be defined by a path of labeled depen-dencies, whether simple or complex, but mustbe inferred from a larger context of the tree us-ing heuristics.
Consider Figure 1b, where thereis a Link relation (rcmod(things, do)), butno corresponding Arg relation inside the relativeclause (because there is no overt relative pro-noun).
However, given the other dependencies,we can infer with high probability that the im-plicit relation is dobj.
Another example of theindirect case is in Figure 1g.
Our post-processingtool performs more heuristic inference for the in-direct case than the Stanford parser does (cf.
Sec-tion 4.3).In order to handle the complex and indirectcases, our post-processor is triggered by the oc-currence of a Link relation (rcmod or conj) andfirst tries to add dependencies that are directly im-plied by a single Arg relation (relations involvingrelative pronouns for rcmod, shared heads anddependents for conj).
If there is no overt rela-tive pronoun, or the function of the relative pro-noun is underspecified, the post-processor relieson the obliqueness hierarchy subj < dobj <pobj and simply picks the first ?missing func-tion?, unless it finds a clausal complement (indi-cated by the labels ccomp and xcomp), in whichcase it descends to the lower clause and restartsthe search there.4.3 Parser EvaluationThe evaluation was performed using the same cri-teria as in Rimell et al (2009).
A dependencywas considered correctly recovered if the gold-standard head and dependent were correct andthe label was an ?acceptable match?
to the gold-standard label, indicating the grammatical func-tion of the extracted element at least to the levelof subject, passive subject, object, or adjunct.The evaluation in Rimell et al (2009) tookinto account a wide variety of parser output for-mats, some of which differed significantly fromthe gold-standard.
Since MSTParser and Malt-Parser produced Stanford dependencies for thisexperiment, evaluation required less manual ex-amination than for some of the other parsers, aswas also the case for the output of the Stanfordparser in the original evaluation.
However, a man-ual evaluation was still performed in order to re-solve questionable cases.5 ResultsThe results are shown in Table 1, where the ac-curacy for each construction is the percentage ofgold-standard dependencies recovered correctly.The Avg column represents a macroaverage, i.e.the average of the individual scores on the sevenconstructions, while the WAvg column represents837Parser ObRC ObRed SbRC Free ObQ RNR SbEm Avg WAvgMST 34.1 47.3 78.9 65.5 13.8 45.4 37.6 46.1 63.4Malt 40.7 50.5 84.2 70.2 16.2 39.7 23.5 46.4 66.9MST-Q 41.2 50.0Malt-Q 31.2 48.5Table 1: Parser accuracy on the unbounded dependency corpus.Parser ObRC ObRed SbRC Free ObQ RNR SbEm Avg WAvgC&C 59.3 62.6 80.0 72.6 81.2 49.4 22.4 61.1 69.9Enju 47.3 65.9 82.1 76.2 32.5 47.1 32.9 54.9 70.9MST 34.1 47.3 78.9 65.5 41.2 45.4 37.6 50.0 63.4Malt 40.7 50.5 84.2 70.2 31.2 39.7 23.5 48.5 66.9DCU 23.1 41.8 56.8 46.4 27.5 40.8 5.9 34.6 47.0RASP 16.5 1.1 53.7 17.9 27.5 34.5 15.3 23.8 34.1Stanford 22.0 1.1 74.7 64.3 41.2 45.4 10.6 37.0 50.3Table 2: Parser accuracy on the unbounded dependency corpus.
The ObQ score for C&C, MSTParser, and MaltParser is fora model trained with additional questions (without this C&C scored 27.5; MSTParser and MaltParser as in Table 1).a weighted macroaverage, where the construc-tions are weighted proportionally to their relativefrequency in the PTB.
WAvg excludes ObQ sen-tences, since frequency statistics were not avail-able for this construction in Rimell et al (2009).Our first observation is that the accuracies forboth systems are considerably below the ?90%unlabeled and ?88% labeled attachment scoresfor English that have been reported previously(McDonald and Pereira, 2006; Hall et al, 2006).Comparing the two parsers, we see that Malt-Parser is more accurate on dependencies in rela-tive clause constructions (ObRC, ObRed, SbRC,and Free), where argument relations tend to berelatively local, while MSTParser is more accu-rate on dependencies in RNR and SbEm, whichinvolve more distant relations.
Without the ad-ditional QB training data, the average scores forthe two parsers are indistinguishable, but MST-Parser appears to have been better able to takeadvantage of the question training, since MST-Qperforms better than Malt-Q on ObQ sentences.On the weighted average MaltParser scores 3.5points higher, because the constructions on whichit outperforms MSTParser are more frequent inthe PTB, and because WAvg excludes ObQ, whereMSTParser is more accurate.Table 2 shows the results for MSTParser andMaltParser in the context of the other parsers eval-uated in Rimell et al (2009).5 For the parsers5The average scores reported differ slightly from those inwhich have a model trained on questions, namelyC&C, MSTParser, and MaltParser, the figureshown for ObQ sentences is that of the questionmodel.
It can be seen that MSTParser and Malt-Parser perform below C&C and Enju, but abovethe other parsers, and that MSTParser achieves thehighest score on SbEm sentences and MaltParseron SbRC sentences.
It should be noted, however,that Table 2 does not represent a direct compar-ison across all parsers, since most of the otherparsers would have benefited from heuristic post-processing of the kind implemented here for MST-Parser and MaltParser.
This is especially true forRASP, where the grammar explicitly leaves sometypes of attachment decisions for post-processing.For DCU, improved labeling heuristics would sig-nificantly improve performance.
It is instructive tocompare the dependency parsers to the Stanfordparser, which uses the same output representationand has been used to prepare the training data forour experiments.
Stanford has very low recall onObRed and SbEm, the categories where heuristicinference plays the largest role, but mirrors MST-Parser for most other categories.6 Error AnalysisWe now proceed to a more detailed error analy-sis, based on the development sets, and classifyRimell et al (2009), where a microaverage (i.e., average overall dependencies in the corpus, regardless of construction)was reported.838the errors made by the parsers into three cate-gories: A global error is one where the parsercompletely fails to build the relevant clausal struc-ture ?
the relative clause in ObRC, ObRed, SbRC,Free, SbEmb; the interrogative clause in ObQ; andthe clause headed by the higher conjunct in RNR?
often as a result of surrounding parsing errors.When a global error occurs, it is usually mean-ingless to further classify the error, which meansthat this category excludes the other two.
An Argerror is one where the parser has constructed therelevant clausal structure but fails to find the Argrelation ?
in the simple and complex cases ?
or theset of surrounding Arg relations needed to inferan implicit Arg relation ?
in the indirect case (cf.Section 4.2).
A Link error is one where the parserfails to find the crucial Link relation ?
rcmodin ObRC, ObRed, SbRC, SbEmb; conj in RNR(cf.
Section 4.2).
Link errors are not relevant forFree and ObQ, where all the crucial relations areclause-internal.Table 3 shows the frequency of different errortypes for MSTParser (first) and MaltParser (sec-ond) in the seven development sets.
First of all,we can see that the overall error distribution isvery similar for the two parsers, which is proba-bly due to the fact that they have been trained onexactly the same data with exactly the same an-notation (unlike the five parsers previously eval-uated).
However, there is a tendency for MST-Parser to make fewer Link errors, especially inthe relative clause categories ObRC, ObRed andSbRC, which is compatible with the observationfrom the test results that MSTParser does betteron more global dependencies, while MaltParserhas an advantage on more local dependencies, al-though this is not evident from the statistics fromthe relatively small development set.Comparing the different grammatical construc-tions, we see that Link errors dominate for the rel-ative clause categories ObRC, ObRed and SbRC,where the parsers make very few errors withrespect to the internal structure of the relativeclauses (in fact, no errors at all for MaltParseron SbRC).
This is different for SbEm, where theanalysis of the argument structure is more com-plex, both because there are (at least) two clausesinvolved and because the unbounded dependencyType GlobalArg LinkA+LErrors# DepsObRC 0/1 1/1 7/11 5/3 13/16 20ObRed 0/1 0/1 6/7 3/4 9/13 23SbRC 2/1 1/0 7/13 0/0 10/14 43Free 2/1 3/5 ?
?
5/6 22ObQ 4/7 13/13 ?
?
17/20 25RNR 6/4 4/6 0/0 4/5 14/15 28SbEm 3/4 3/2 0/0 3/3 9/9 13Table 3: Distribution of error types in the developmentsets; frequencies for MSTParser listed first and MaltParsersecond.
The columns Arg and Link give frequencies forArg/Link errors occurring without the other error type, whileA+L give frequencies for joint Arg and Link errors.can only be inferred indirectly from the basic de-pendency representation (cf.
Section 4.2).
An-other category where Arg errors are frequent isRNR, where all such errors consist in attachingthe relevant dependent to the second conjunct in-stead of to the first.6 Thus, in the example in Fig-ure 1f, both parsers found the conj relation be-tween puzzled and angered but attached by to thesecond verb.Global errors are most frequent for RNR, prob-ably indicating that coordinate structures are diffi-cult to parse in general, and for ObQ (especiallyfor MaltParser), probably indicating that ques-tions are not well represented in the training seteven after the addition of QB data.7 As notedin Section 4.1, this may be partly due to the factthat conversion to Stanford dependencies did notseem to work as well for QB as for the WSJ data.Another problem is that the part-of-speech taggerused was trained on WSJ data only and did notperform as well on the ObQ data.
Uses of What asa determiner were consistently mistagged as pro-nouns, which led to errors in parsing.
Thus, forthe example in Figure 1e, both parsers producedthe correct analysis except that, because of the tag-ging error, they treated What rather than effect asthe head of the wh-phrase, which counts as an er-ror in the evaluation.In order to get a closer look specifically at theArg errors, Table 4 gives the confusion matrix6In the Stanford scheme, an argument or adjunct must beattached to the first conjunct in a coordination to indicate thatit belongs to both conjuncts.7Parsers trained without QB had twice as many globalerrors.839Sb Ob POb EmSb EmOb Other TotalSb ?
0/0 0/0 0/0 0/0 2/1 2/1Ob 2/3 ?
0/0 0/1 0/0 4/2 6/6POb 2/0 7/5 ?
0/0 0/0 5/8 14/13EmSb 1/1 4/2 0/0 ?
0/0 1/2 6/5EmOb 0/0 3/1 0/0 0/0 ?
1/6 4/7Total 5/4 14/8 0/0 0/1 0/0 13/19 32/32Table 4: Confusion matrix for Arg errors (excluding RNRand using parsers trained on QB for ObQ); frequencies forMSTParser listed first and MaltParser second.
The columnOther covers errors where the function is left unspecified orthe argument is attached to the wrong head.for such errors, showing which grammatical func-tions are mistaken for each other, with an extracategory Other for cases where the function is leftunspecified by the parser or the error is an attach-ment error rather than a labeling error (and ex-cluding the RNR category because of the specialnature of the Arg errors in this category).
Theresults again confirm that the two parsers makevery few errors on subjects and objects clause-internally.
The few cases where an object ismistaken as a subject occur in ObQ, where bothparsers perform rather poorly in general.
By con-trast, there are many more errors on prepositionalobjects and on embedded subjects and objects.
Webelieve an important part of the explanation forthis pattern is to be found in the Stanford depen-dency representation, where subjects and objectsare marked as such but all other functions real-ized by wh elements are left unspecified (using thegeneric rel dependency), which means that the re-covery of these functions currently has to rely onheuristic rules as described in Section 4.2.
Finally,we think it is possible to observe the tendency forMaltParser to be more accurate at local labelingdecisions ?
reflected in fewer cross-label confu-sions ?
and for MSTParser to perform better onmore distant attachment decisions ?
reflected infewer errors in the Other category (and in fewerLink errors).7 ConclusionIn conclusion, the capacity of MSTParser andMaltParser to recover unbounded dependencies isvery similar on the macro and weighted macrolevel, but there is a clear distinction in theirstrengths ?
constructions involving more distantdependencies such as ObQ, RNR and SbEm forMSTParser and constructions with more locallydefined configurations such as ObRC, ObRed,SbRC and Free for MaltParser.
This is a patternthat has been observed in previous evaluations ofthe parsers and can be explained by the globallearning and inference strategy of MSTParser andthe richer feature space of MaltParser (McDonaldand Nivre, 2007).Perhaps more interestingly, the accuracies ofMSTParser and MaltParser are only slightly be-low the best performing systems in Rimell et al(2009) ?
C&C and Enju.
This is true even thoughMSTParser and MaltParser have not been engi-neered specifically for English and lack specialmechanisms for handling unbounded dependen-cies, beyond the simple post-processing heuristicused to extract them from the output trees.
Thus,it is reasonable to speculate that the addition ofsuch mechanisms could lead to computationallylightweight parsers with the ability to extract un-bounded dependencies with high accuracy.AcknowledgmentsWe thank Marie-Catherine de Marneffe for greathelp with the Stanford parser and dependencyscheme, Llu?
?s Ma`rquez and Jesu?s Gime?nez forgreat support with SVMTool, Josef van Gen-abith for sharing the QuestionBank data, andStephen Clark and Mark Steedman for helpfulcomments on the evaluation process and the pa-per.
Laura Rimell was supported by EPSRC grantEP/E035698/1 and Carlos Go?mez-Rodr?
?guezby MEC/FEDER (HUM2007-66607-C04) andXunta de Galicia (PGIDIT07SIN005206PR, Re-des Galegas de PL e RI e de Ling.
de Corpus,Bolsas Estadas INCITE/FSE cofinanced).ReferencesBlack, E., S. Abney, D. Flickinger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, S. Roukos, B. Santorini,and T. Strzalkowski.
1991.
A procedure for quanti-tatively comparing the syntactic coverage of Englishgrammars.
In Proceedings of 4th DARPAWorkshop,306?311.Briscoe, T., J. Carroll, and R. Watson.
2006.
The sec-ond release of the RASP system.
In Proceedings840of the COLING/ACL 2006 Interactive PresentationSessions, 77?80.Buchholz, S. and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Pro-ceedings of CoNLL, 149?164.Cahill, A., M. Burke, R. O?Donovan, J.
Van Genabith,and A.
Way.
2004.
Long-distance dependencyresolution in automatically acquired wide-coveragePCFG-based LFG approximations.
In Proceedingsof ACL, 320?327.Carreras, X., M. Collins, and T. Koo.
2008.
TAG,dynamic programming, and the perceptron for ef-ficient, feature-rich parsing.
In Proceedings ofCoNLL, 9?16.Charniak, E. and M. Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.In Proceedings of ACL, 173?180.Clark, S. and J. R. Curran.
2007.
Wide-coverage ef-ficient statistical parsing with CCG and log-linearmodels.
Computational Linguistics, 33:493?552.de Marneffe, M.-C., B. MacCartney, and C. D. Man-ning.
2006.
Generating typed dependency parsesfrom phrase structure parses.
In Proceedings ofLREC.Gime?nez, J. and L. Ma`rquez.
2004.
SVMTool: A gen-eral POS tagger generator based on support vectormachines.
In Proceedings of LREC.Hall, J., J. Nivre, and J. Nilsson.
2006.
Discriminativeclassifiers for deterministic dependency parsing.
InProceedings of the COLING/ACL 2006 Main Con-ference Poster Sessions, 316?323.Hall, J., J. Nilsson, J. Nivre, G. Eryig?it, B. Megyesi,M.
Nilsson, and M. Saers.
2007.
Single malt orblended?
A study in multilingual parser optimiza-tion.
In Proceedings of the CoNLL Shared Task.Huang, L. 2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL, 586?594.Judge, J., A. Cahill, and J. van Genabith.
2006.
Ques-tionBank: Creating a corpus of parse-annotatedquestions.
In Proceedings of COLING-ACL, 497?504.Klein, D. and C. D. Manning.
2003.
Accurate unlexi-calized parsing.
In Proceedings of ACL, 423?430.Ku?bler, S., R. McDonald, and J. Nivre.
2008.
Depen-dency Parsing.
Morgan and Claypool.McDonald, R. and J. Nivre.
2007.
Characterizingthe errors of data-driven dependency parsing mod-els.
In Proceedings of EMNLP-CoNLL, 122?131.McDonald, R. and F. Pereira.
2006.
Online learningof approximate dependency parsing algorithms.
InProceedings of EACL, 81?88.McDonald, R. and G. Satta.
2007.
On the complexityof non-projective data-driven dependency parsing.In Proceedings of IWPT, 122?131.McDonald, R., K. Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
InProceedings of ACL, 91?98.McDonald, R., K. Lerman, and F. Pereira.
2006.
Mul-tilingual dependency analysis with a two-stage dis-criminative parser.
In Proceedings of CoNLL, 216?220.McDonald, R.. 2006.
Discriminative Learning andSpanning Tree Algorithms for Dependency Parsing.Ph.D.
thesis, University of Pennsylvania.Miyao, Y. and J. Tsujii.
2005.
Probabilistic disam-biguation models for wide-coverage HPSG parsing.In Proceedings of ACL, 83?90.Nivre, J., J.
Hall, and J. Nilsson.
2006a.
MaltParser:A data-driven parser-generator for dependency pars-ing.
In Proceedings of LREC, 2216?2219.Nivre, J., J.
Hall, J. Nilsson, G. Eryig?it, and S. Mari-nov. 2006b.
Labeled pseudo-projective dependencyparsing with support vector machines.
In Proceed-ings of CoNLL, 221?225.Nivre, J., J.
Hall, J. Nilsson, A. Chanev, G. Eryig?it,S.
Ku?bler, S. Marinov, and E. Marsi.
2007.
Malt-parser: A language-independent system for data-driven dependency parsing.
Natural Language En-gineering, 13:95?135.Nivre, J.
2009.
Non-projective dependency parsingin expected linear time.
In Proceedings of ACL-IJCNLP, 351?359.Rimell, L., S. Clark, and M. Steedman.
2009.
Un-bounded dependency recovery for parser evaluation.In Proceedings EMNLP, 813?821.Sagae, K. and A. Lavie.
2006.
Parser combinationby reparsing.
In Proceedings of NAACL HLT: ShortPapers, 129?132.841
