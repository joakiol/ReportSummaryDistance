Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 409?413,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsTolerant BLEU: a Submission to the WMT14 Metrics TaskJind?rich Libovick?y and Pavel PecinaCharles University in Prague, Faculty of Mathematics and PhysicsInstitute of Formal ad Applied Linguistics{libovicky, pecina}@ufal.mff.cuni.czAbstractThis paper describes a machine translationmetric submitted to the WMT14 MetricsTask.
It is a simple modification of thestandard BLEU metric using a monolin-gual alignment of reference and test sen-tences.
The alignment is computed asa minimum weighted maximum bipartitematching of the translated and the refer-ence sentence words with respect to therelative edit distance of the word prefixesand suffixes.
The aligned words are in-cluded in the n-gram precision compu-tation with a penalty proportional to thematching distance.
The proposed tBLEUmetric is designed to be more tolerant toerrors in inflection, which usually does noteffect the understandability of a sentence,and therefore be more suitable for measur-ing quality of translation into morphologi-cally richer languages.1 IntroductionAutomatic evaluation of machine translation (MT)quality is an important part of the machine trans-lation pipeline.
The possibility to run an evalua-tion algorithm many times while training a systemenables the system to be optimized with respect tosuch a metric (e.g., by Minimum Error Rate Train-ing (Och, 2003)).
By achieving a high correlationof the metric with human judgment, we expect thesystem performance to be optimized also with re-spect to the human perception of translation qual-ity.In this paper, we propose an MT metric calledtBLEU (tolerant BLEU) that is based on the stan-dard BLEU (Papineni et al., 2002) and designed tosuit better when translation into morphologicallyricher languages.
We aim to have a simple lan-guage independent metric that correlates with hu-man judgment better than the standard BLEU.Several metrics try to address this problemas well and usually succeed to gain a highercorrelation with human judgment (e.g.
ME-TEOR (Denkowski and Lavie, 2011), TerrorCat(Fishel et al., 2012)).
However, they usuallyuse some language-dependent tools and resources(METEOR uses stemmer and parahprasing tables,TerrorCat uses lemmatization and needs trainingdata for each language pair) which prevent themfrom being widely adopted.In the next section, the previous work is brieflysummarized.
Section 3 describes the metric in de-tail.
The experiments with the metric are describedin Section 4 and their results are summarized inSection 5.2 Previous WorkBLEU (Papineni et al., 2002) is an established andthe most widely used automatic metric for evalua-tion of MT quality.
It is computed as a harmonicmean of the n-gram precisions multiplied by thebrevity penalty coefficient which ensures also highrecall.
Formally:BLEU = BP ?
exp(4?n=114log pn),where BP is the brevity penaly defined as follows:BP ={1 if c > re1?rcotherwise,c is the length of the test sentence (number of to-kens), r is the length of the reference sentence, andpnis the proportion of n-grams from the test sen-tence found in the reference translations.The original experiments with the English toChinese translation (Papineni et al., 2002) re-ported very high correlation of BLEU with humanjudgments.
However, these scores were computedusing multiple reference translations (to capturetranslation variability) but in practice, only one409Reference:Source:Translation:I(am(driving(a(new(red(carJedu(nov?m(?erven?m(autemJedu(s(nov?m(?erven?m(autob7 3Corrected(and(wighted(translation: UJeduE(1p(UsE(1p(Unov?mE(243p(U?erven?mE(546p(UautemE(143pUnigram(precisionJedusnov?m?erven?mautemtBLEU(unigram(precision(=11243546143Bigram(precisionJedu(ss(nov?mnov?m(?erven?m?erven?m(autemtBLEU(bigram(precision(=15463447412116 5(?
(7b367avgU1E1p(=avgU1E(243p(=avgU243E(546p(=avgU546E143p(=BLEU(unigram(precision(=(1(4(5(=(7b2Jedusnov?m?erven?mautoJedu(ss(nov?mnov?m(?erven?m?erven?m(auto1612 4(?
(7b333BLEU(bigram(precision(=(7(4(4(=(726131Figure 1: An example of the unigram and bigram precision computation for translation from English toCzech with the test sentence having minor inflection errors and an additional preposition.
The first twolines contain the source sentence in English and a correct reference translation in Czech.
On the thirdline, there is an incorrectly translated sentence with errors in inflection.
Between the second and thethird line, the matching with respect to the affix distance is shown.
The fourth line contains the correctedtest sentence with the words weights.
The bottom part of the figure shows computation of the unigramand bigram precisions.
The first column contains the original translation n-grams, the second one thecorrected n-grams, the third one the n-gram weights and the last one indicates whether a matching n-gram is contained in the reference sentence.reference translation is usually available and there-fore the BLEU scores are often underestimated.The main disadvantage of BLEU is the fact thatit treats words as atomic units and does not allowany partial matches.
Therefore, words which areinflectional variants of each other are treated ascompletely different words although their mean-ing is similar (e.g.
work, works, worked, working).Further, the n-gram precision for n> 1 penalizesdifference in word order between the reference andthe test sentences even though in languages withfree word order both sentences can be correct (Bo-jar et al., 2010; Condon et al., 2009).There are also other widely recognized MTevaluation metrics: The NIST score (Dodding-ton, 2002) is also an n-gram based metric, butin addition it reflects how informative particularn-grams are.
A metric that achieves a very highcorrelation with human judgment is METEOR(Denkowski and Lavie, 2011).
It creates a mono-lingual alignment using language dependent toolsas stemmers and synonyms dictionaries and com-putes weighted harmonic mean of precision andrecall based on the matching.Some metrics are based on measuring theedit distance between the reference and test sen-tences.
The Position-Independent Error Rate(PER) (Leusch et al., 2003) is computed asa length-normalized edit distance of sentencestreated as bags of words.
The Translation EditRate (TER) (Snover et al., 2006) is a number ofedit operation needed to change the test sentenceto the most similar reference sentence.
In thiscase, the allowed editing operations are insertions,deletions and substitutions and also shifting wordswithin a sentence.A different approach is used in TerrorCat(Fishel et al., 2012).
It uses frequencies of auto-matically obtained translation error categories asbase for machine-learned pairwise comparison oftranslation hypotheses.In the Workshop of Machine Translation(WMT) Metrics Task, several new MT metricscompete annually (Mach?a?cek and Bojar, 2013).
Inthe comptetition, METEOR and TerrorCat scoredbetter that the other mentioned metrics.4103 Metric DescriptiontBLEU is computed in in two steps.
Similarly tothe METEOR score, we first make a monolingualalignment between the reference and the test sen-tences and then apply an algorithm similar to thestandard BLEU but with modified n-gram preci-sions.The monolingual alignment is computed as aminimum weighted maximum bipartite matchingbetween words in a reference sentence and a trans-lation sentence1using the Munkres assignment al-gorithm (Munkres, 1957).We define a weight of an alignment link as theaffix distance of the test sentence word wtiand thereference sentence word wrj: Let S be the longestcommon substring of wtiand wri.
We can rewritethe strings as a concatenation of a prefix, the com-mon substring and a suffix:wt= wti,pSwti,swr= wrj,pSwrj,sFurther, we define the affix distance as:AD(wr, wt)= max{1,L(wrj,p,wti,p)+L(wrs,j,wts,i)|S|}if |S| > 0 and AD(wr, wt) = 1 otherwise.
L is theLevensthein distance between two strings.For example the affix distance of two Czechwords vzpomenou and zapomenout (differentforms of verbs remember and forget) is computedin the following way: The longest common sub-string is pomenou which has a length of 7.
Theprefixes are vz and za and their edit distance is 2.The suffixes are an empty string and t which withthe edit distance 1.
The total edit distance of pre-fixes and suffixes is 3.
By dividing the total editdistance by the length of the longest common sub-string, we get the affix distance37?
0.43.We denote the resulting set of matching pairsof words as M = {(wri, wti)}mi=1and for each testsentence St= (wt1, ..., wtm) we create a correctedsentence?St= (w?t1, ..., w?tm) such thatw?ti={wrif ?wt: (wr, wt)?M & AD(wr, wt) ?
wtiotherwise.This means that the words from the test sen-tence which were matched with the affix distance1The matching is always one-to-one which means thatsome words remain unmatched if the sentences have differ-ent number of words.0 0.2 0.4 0.6 0.8 10.70.750.80.850.90.95en-csen-deen-esen-frAffix distance thresholdPearson's correlationcoeffitient0 0.2 0.4 0.6 0.8 10.910.920.930.940.950.960.97cs-ende-enes-enfr-enAffix distance thresholdPearson's correlationcoefficientFigure 2: Dependence of the Pearson?s correlationof tBLEU with the WMT13 human judgments onthe affix distance threshold for translations fromEnglish and to English.smaller than  are ?corrected?
by substituting themby the matching words from the reference sen-tence.
The threshold  is a free parameter of themetric.
When the threshold is set to zero, nocorrections are made and therefore the metric isequivalent to the standard BLEU.The words in the corrected sentence are as-signed the weights as follows:v(w?ti) ={1?AD(w?ti, wti) if w?ti6= wti1 otherwise.In other words, the weights penalize the correctedwords proportionally to the affix distance from theoriginal words.While computing the n-gram precision, twomatching n-grams (w?t1, .
.
.
w?tn) and (wr1, .
.
.
wrn)contribute to the n-gram precision with a score ofs(wt1, .
.
.
, wtn) =n?i=1v(w?ti) / ninstead of one as it is in the standard BLEU.
Therest of the BLEU score computation remains un-changed.
While using multiple reference transla-tion, the matching is done for each of the refer-ence sentence, and while computing the n-gramprecision, the reference sentences with the highestweight is chosen.
The computation of the n-gramprecision is illustrated in Figure 1.411direction BLEU METEOR tBLEUen-cs .781 .860 .787en-de .835 .868 .850en-es .875 .878 .884en-fr .887 .906 .906from English .844 .878 .857Table 1: System level Pearson?s correlation withthe human judgment for systems translating fromEnglish computed on the WMT13 dataset.4 EvaluationWe evaluated the proposed metric on the datasetused for the WMT13 Metrics Task (Mach?a?cek andBojar, 2013).
The dataset consists of 135 systems?outputs in 10 directions (5 into English 5 out ofEnglish).
Each system?s output and the referencetranslation contain 3000 sentences.
According tothe WMT14 guidelines, we report the the Pear-son?s correlation coefficient instead of the Spear-man?s coefficient that was used in the last years.Twenty values of the affix distance thresholdwere tested in order to estimate what is the mostsuitable threshold setting.
We report only the sys-tem level correlation because the metric is de-signed to compare only the whole system outputs.5 ResultsThe tBLEU metric generally improves the cor-relation with human judgment over the standardBLEU metric for directions from English to lan-guages with richer inflection.Examining the various threshold values showedthat dependence between the affix distance thresh-old and the correlation with the human judgmentvaries for different language pairs (Figure 2).
Fortranslation from English to morphologically richerlanguages than English ?
Czech, German, Spanishand French ?
using the tBLEU metric increasedthe correlation over the standard BLEU.
For Czechthe correlation quickly decreases for threshold val-ues bigger than 0.1, whereas for the other lan-guages it still grows.
We hypothesize this becausethe big morphological changes in Czech can en-tirely change the meaning.For translation to English, the correlationslightly increases with the increasing thresholdvalue for translation from French and Spanish, butdecreases for Czech and German.There are different optimal affix distancedirection BLEU METEOR tBLEUcs-en .925 .985 .927de-en .916 .962 .917es-en .957 .968 .953fr-en .940 .983 .933to English .923 .974 .935Table 2: System level Pearson?s correlation withthe human judgment for systems translating to En-glish computed on the WMT13 dataset.thresholds for different language pairs.
However,the threshold of 0.05 was used for our WMT14submission because it had the best average cor-relation on the WMT13 data set.
Tables 1 and2 show the results of the tBLEU for the particu-lar language pairs for threshold 0.05.
While com-pared to the BLEU score, the correlation is slightlyhigher for translation from English and approxi-mately the same for translation to English.The results on the WMT14 dataset did not showany improvement over the BLEU metric.
The rea-son of the results will be further examined.6 Conclusion and Future WorkWe presented tBLEU, a language-independent MTmetric based on the standard BLEU metric.
It in-troduced the affix distance ?
relative edit distancesof prefixes and suffixes of two string after remov-ing their longest common substring.
Finding amatching between translation and reference sen-tences with respect to this matching allows a pe-nalized substitution of words which has been mostlikely wrongly inflected and therefore less penal-izes errors in inflection.This metric achieves a higher correlation withthe human judgment than the standard BLEUscore for translation to morphological richer lan-guages without the necessity to employ any lan-guage specific tools.In future work, we would like to improve wordalignment between test and reference translationsby introducing word position and potentially otherfeatures, and implement tBLEU in MERT to ex-amine its impact on system tuning.7 AcknowledgementsThis research has been funded by the Czech Sci-ence Foundation (grant n. P103/12/G084) and theEU FP7 project Khresmoi (contract no.
257528).412ReferencesOnd?rej Bojar, Kamil Kos, and David Mare?cek.
2010.Tackling sparse data issue in machine translationevaluation.
In Proceedings of the ACL 2010 Con-ference Short Papers, pages 86?91.
Association forComputational Linguistics.Sherri Condon, Gregory A Sanders, Dan Parvaz, AlanRubenstein, Christy Doran, John Aberdeen, andBeatrice Oshika.
2009.
Normalization for auto-mated metrics: English and arabic speech transla-tion.
Proceedings of MT Summit XII.
Associationfor Machine Translation in the Americas, Ottawa,ON, Canada.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic metric for reliable optimization andevaluation of machine translation systems.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, WMT ?11, pages 85?91, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the Sec-ond International Conference on Human LanguageTechnology Research, HLT ?02, pages 138?145, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.Mark Fishel, Rico Sennrich, Maja Popovi?c, and Ond?rejBojar.
2012.
Terrorcat: a translation errorcategorization-based mt quality metric.
In Proceed-ings of the Seventh Workshop on Statistical MachineTranslation, pages 64?70.
Association for Compu-tational Linguistics.Gregor Leusch, Nicola Ueffing, Hermann Ney, et al.2003.
A novel string-to-string distance measurewith applications to machine translation evaluation.In Proceedings of MT Summit IX, pages 240?247.Citeseer.Matou?s Mach?a?cek and Ond?rej Bojar.
2013.
Results ofthe WMT13 metrics shared task.
In Proceedings ofthe Eighth Workshop on Statistical Machine Trans-lation, pages 45?51, Sofia, Bulgaria, August.
Asso-ciation for Computational Linguistics.James Munkres.
1957.
Algorithms for the assignmentand transportation problems.
Journal of the Societyfor Industrial & Applied Mathematics, 5(1):32?38.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics, pages 160?167, Sapporo, Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th annual meeting on association for compu-tational linguistics, pages 311?318.
Association forComputational Linguistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of association for machine transla-tion in the Americas, pages 223?231.413
