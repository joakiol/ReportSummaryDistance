Automatic Interpretation of Noun CompoundsUsing WordNet SimilaritySu Nam Kim1,2 and Timothy Baldwin2,31 Computer Science, University of Illinois, Chicago, IL 60607 USAsunamkim@gmail.com2 Computer Science and Software Engineering,University of Melbourne, Victoria 3010 Australia3 NICTA Victoria Lab, University of Melbourne, Victoria 3010 Australiatim@csse.unimelb.edu.auAbstract.
The paper introduces a method for interpreting novel noun compoundswith semantic relations.
The method is built around word similarity with pre-tagged noun compounds, based on WordNet::Similarity.
Over 1,088training instances and 1,081 test instances from the Wall Street Journal in thePenn Treebank, the proposed method was able to correctly classify 53.3% of thetest noun compounds.
We also investigated the relative contribution of the modi-fier and the head noun in noun compounds of different semantic types.1 IntroductionA noun compound (NC) is an ?N made up of two or more nouns, such as golf club orpaper submission; we will refer to the rightmost noun as the head noun and the re-mainder of nouns in the NC as modifiers.
The interpretation of noun compounds is awell-researched area in natural language processing, and has been applied in applica-tions such as question answering and machine translation [1,2,3].
Three basic propertiesmake the interpretation of NCs difficult [4]: (1) the compounding process is extremelyproductive; (2) the semantic relationship between head noun and modifier in the nouncompounds is implicit; and (3) the interpretation can be influenced by contextual andpragmatic factors.In this paper, we are interested in recognizing the semantic relationship between thehead noun and modifier(s) of noun compounds.
We introduce a method based on wordsimilarity between the component nouns in an unseen test instance NC and annotatedtraining instance NCs.
Due to its simplicity, our method is able to interpret NCs withsignificantly reduced cost.
We also investigate the relative contribution of the head nounand modifier in determining the semantic relation.For the purposes of this paper, we focus exclusively on binary NCs, that is NCsmade up of two nouns.
This is partly an empirical decision, in that the majority ofNCs occurring in unrestricted text are binary,1 and also partly due to there being ex-isting methods for disambiguating the syntactic structure of higher-arity NCs, effec-tively decomposing them into multiple binary NCs [3].
Note also that in this paper, we1 We estimate that 88.4% of NCs in the Wall Street Journal section of the Penn Treebank and90.6% of NCs in the British National Corpus are binary.R.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
945?956, 2005.c?
Springer-Verlag Berlin Heidelberg 2005946 S.N.
Kim and T. Baldwindistinguish semantic relations from semantic roles.
The semantic relation in an NC isthe underlying relation between the head noun and its modifier, whereas its semanticrole is an indication of its relation to the governing verb and other constituents in thesentence context.There is a significant body of closely-related research on interpreting semantic rela-tions in NCs which relies on hand-written rules.
[5] examined the problem of interpre-tation of NCs and constructed a set of hand-written rules.
[6] automatically extractedsemantic information from an on-line dictionary and manipulated a set of hand-writtenrules to assign weights to semantic relations.
Recently, there has been work on the auto-matic (or semi-automatic) interpretation of NCs [4,7,8].
However, most of this work isbased on a simplifying assumption as to the scope of semantic relations or the domainof interpretation, making it difficult to compare the performance of NC interpretationin a broader context.In the remainder of the paper, we detail the motivation for our work (Section 2),introduce the WordNet::Similarity system which we use to calculate word sim-ilarity (Section 3), outline the set of semantic relations used (Section 4), detail how wecollected the data (Section 5), introduce the proposed method (Section 6), and describeexperimental results (Section 7).2 MotivationMost work related to interpreting NCs depends on hand-coded rules [5].
The first at-tempt at automatic interpretation by [6] showed that it was possible to successfullyinterpret NCs.
However, the system involved costly hand-written rules involving man-ual intervention.
[9] estimated the amount of world knowledge required to interpretNCs and claimed that the high cost of data acquisition offsets the benefits of automaticinterpretation of NCs.Recent work [4,7,8] has investigated methods for interpreting NCs automaticallywith minimal human effort.
[10] introduced a semi-automatic method for recogniz-ing noun?modifier relations.
[4] examined nominalizations (a proper subset of NCs) interms of whether the modifier is a subject or object of the verb the head noun is derivedfrom (e.g.
language understanding = understand language).
[7] assigned hierarchicaltags to nouns in medical texts and classified them according to their semantic relationsusing neural networks.
[8] used the word senses of nouns to classify the semantic re-lations of NCs.
However, in all this work, there has been some underlying simplifyingassumption, in terms of the domain or range of interpretations an NC can occur with,leading to questions of scalability and portability to novel domains/NC types.In this paper, we introduce a method which uses word similarity based on WordNet.Word similarity has been used previously in various lexical semantic tasks, includingword sense disambiguation [11,12].
[11] showed that term-to-term similarity in a con-text space can be used to disambiguate word senses.
[12] measured the relatedness ofconcepts using similarity based on WordNet.
[13] examined the task of disambiguatingnoun groupings with respect to word senses using similarity between nouns in NCs.Our research uses similarities between nouns in the training and test data to interpretthe semantic relations of novel NCs.Automatic Interpretation of Noun Compounds 947apple juice morning milkchocolate milkMATERIAL TIMEs12s11 s21 s22Fig.
1.
Similarity between test NC chocolate milk and training NCs apple juice and morning milkTable 1.
WordNet-based similarities for component nouns in the training and test dataTraining noun Test noun Sijt1 apple chocolate 0.71t2 juice milk 0.83t1 morning chocolate 0.27t2 milk milk 1.00Figure 1 shows the correspondences between two training NCs, apple juice andmorning milk, and a test NC, chocolate milk; Table 1 lists the noun pairings and noun?noun similarities based on WordNet.
Each training noun is a component noun fromthe training data, each test noun is a component noun in the input, and Sij providesa measure of the noun?noun similarity in training and test, where t1 is the modifierand t2 is the head noun in the NC in question.
The similarities in Table 1 were com-puted by the WUP method [14] as implemented in WordNet::Similarity (seeSection 3).The simple product of the individual similarities (of each modifier and head noun,respectively) gives the similarity of the NC pairing.
For example, the similarity betweenchocolate milk and apple juice is 0.60, while that between chocolate milk and morningmilk is 0.27.
Note that although milk in the input NC also occurs in a training exemplar,the semantic relations for the individual NCs differ.
That is, while apple juice is juicemade from apples (MATERIAL), morning milk is milk served in the morning (TIME).By comparing the similarity of both elements of the input NC, we are able to arriveat the conclusion that chocolate milk is more closely related to chocolate milk, whichprovides the correct semantic relation of MATERIAL (i.e.
milk made from/flavored withchocolate).
Unlike word sense disambiguation systems, our method does not need todetermine the particular sense in which each noun is used.
The next example (Table 2)shows how our method interprets NCs containing ambiguous nouns correctly.One potential pitfall when dealing with WordNet is the high level of polysemy formany lexemes.
We analyze the effects of polysemy with respect to interest.
Assume thatwe have the two NCs personal interest (POSSESSION) and bank interest (CAUSE/TOPIC)in the training data.
Both contain the noun interest, with the meaning of a state of cu-948 S.N.
Kim and T. BaldwinTable 2.
The effects of polysemy on the similarities between nouns in the training and test dataTraining noun Test noun Sijt1 personal loan 0.32t2 interest rate 0.84t1 bank loan 0.75t2 interest rate 0.84Table 3.
Varying contribution of the head noun and modifier in predicting the semantic relationRelative contribution of modifier/head noun Relation Examplemodifier < head noun PROPERTY elephant sealmodifier = head noun EQUATIVE composer arrangermodifier > head noun TIME morning classriosity or concern about something in personal interest, and an excess or bonus beyondwhat is expected or due in bank interest.
Given the test NC loan rate, we would get thedesired result of bank interest being the training instance of highest similarity, leadingto loan rate being classified with the semantic relation of CAUSE/TOPIC.
The similar-ity between the head nouns interest and rate for each pairing of training and test NC isidentical, as the proposed method makes no attempt to disambiguate the sense of a nounin each NC context, and instead aggregates the overall word-to-word similarity acrossthe different sense pairings.
The determining factor is therefore the similarity betweenthe different modifier pairings, and the fact that bank is more similar to loan than is thecase for personal.We also investigate the weight of the head noun and the modifier in determiningoverall similarity.
We expect for different relations, the weight of the head noun andthe modifier will be different.
In the relation EQUATIVE, e.g., we would expect thesignificance of the head noun to be the same as that of the modifier.
In relations such asPROPERTY, on the other hand, we would expect the head noun to play a more importantrole than the modifier.
Conversely, with relations such as TIME, we would expect themodifier to be more important, as detailed in Table 3.3 WordNet::SimilarityWordNet::Similarity2 [12] is an open source software package developed atthe University of Minnesota.
It allows the user to measure the semantic similarity orrelatedness between a pair of concepts (or word senses), and by extension, between apair of words.
The system provides six measures of similarity and three measures ofrelatedness based on the WordNet lexical database [15].
The measures of similarity arebased on analysis of the WordNet isa hierarchy.2 www.d.umn.edu/?tpederse/similarity.htmlAutomatic Interpretation of Noun Compounds 949The measures of similarity are divided into two groups: path-based and informationcontent-based.
We chose four of the similarity measures in WordNet::Similarityfor our experiments: WUP and LCH as path-based similarity measures, and JCN andLIN as information content-based similarity measures.
LCH finds the shortest path be-tween nouns [16]; WUP finds the path length to the root node from the least com-mon subsumer (LCS) of the two word senses that is the most specific word sensethey share as an ancestor [14]; JCN subtracts the information content of the LCSfrom the sum [17]; and LIN scales the information content of the LCS relative to thesum [18].In WordNet::Similarity, relatedness goes beyond concepts being similar toeach other.
That is, WordNet provides additional (non-hierarchical) relations such ashas-part and made-of.
It supports our idea of interpretation of NCs by similarity.However, as [19] point out, information on relatedness has not been developed as ac-tively as conceptual similarity.
Besides, the speed of simulating these relatedness effectsis too slow to use in practice.
Hence, we did not use any of the relatedness measures inthis paper.4 Semantic RelationsA semantic relation in the context of NC interpretation is the relation between the mod-ifier and the head noun.
For instance, family car relates to POSSESSION whereas sportscar relates to PURPOSE.
[20] defined complex nominals as expressions that have ahead noun preceded by one or more modifying nouns or denominal adjectives, andoffered nine semantic labels after removing opaque compounds and adding nominalnon-predicating adjectives.
[5] produced a diverse set of NC interpretations.
Other re-searchers have identified alternate sets of semantic relations, or conversely cast doubtson the possibility of devising an all-purpose system of NC interpretations [21].
For ourwork, we do not intend to create a new set of semantic relations.
Based on our data,we chose a pre-existing set of semantic relations that had previously been used for au-tomatic (or semi-automatic) NC interpretation, namely the 20-member classification of[10] (see Appendix).
Other notable classifications include that of [6] which contains 13relations based on WH questions, making it ideally suited to question answering appli-cations.
However, some relations such as TOPIC are absent.
[7] proposed 38 relations forthe medical domain.
Such relations are too highly specialized to this domain, and notsuitable for more general applications.
[8] defined 35 semantic relations for complexnominals and adjective phrases.5 Data CollectionWe retrieved binary NCs from the Wall Street Journal component of the Penn treebank.We excluded proper nouns since WordNet does not contain even high-frequency propernouns such as Honda.
We also excluded binary NCs that are part of larger NCs.
Intagging the semantic relations of noun compounds, we hired two annotators: two com-puter science Ph.D students.
In many cases, even human annotators disagree on the tagallocation.
For NCs containing more than one semantic relation, the annotators were950 S.N.
Kim and T. Baldwinjudged to have agreed is there was overlap in at least one of the relations specified bythem for a given NC.
The initial agreement for the two annotators was 52.31%.
Fromthe disagreement of tagged relations, we observed that decisions between SOURCE andCAUSE, PURPOSE and TOPIC, and OBJECT and TOPIC frequently have lower agree-ment.
For the NCs where there was no agreement, the annotators decided on a set ofrelations through consultation.
The distribution of semantic relations is shown in theAppendix.
Overall, we used 1,088 NCs for the training data and 1,081 NCs for thetest data.6 MethodFigure 2 shows how to compute the similarity between the ith NC in the test dataand jth NC in the training data.
We calculate similarities for the component nouns ofthe ith NC in the test data with all NCs in the training data.
As a result, the modifierand head noun in the ith test NC are each associated with a total of m similarities,where m is the number of NCs in the training data.
The second step is to multiplythe similarities of the modifier and head noun for all NCs in the training data; we ex-periment with two methods for calculating the combined similarity.
The third step isto choose the NC in the training data which is most similar to the test instance, andtag the test instance according to the semantic relation associated with that traininginstance.Formally, SA is the similarity between NCs (Ni,1, Ni,2) and (Bj,1, Bj,2):SA((Ni,1, Ni,2), (Bj,1, Bj,2)) =((?S1 + S1) ?
((1 ?
?
)S2 + S2))2(1)where S1 is the modifier similarity (i.e.
S(Ni,1, Bj1)) and S2 is head noun similarity(i.e.
S(Ni,2, Bj2)); ?
?
[0, 1] is a weighting factor.SB is an analogous similarity function, based on the F-score:Bj1   Bj2Bm1 Bm2B31  B32B21  B22B11  B12Relation2Relation3Relation19Relation_kRelation3Ni1   Ni2Nn1  Nn2S(Ni1,B11)S(Ni1,B21)S(Ni1,Bj1)S(Ni1,Bm1)S(Ni2,B12)S(Ni2,B22)S(Ni2,Bj2)S(Ni2,Bm2)RELATIONNNN11  N12N21  N22 Similarity in detailFig.
2.
Similarity between the ith NC in the test data and jth NC in the training dataAutomatic Interpretation of Noun Compounds 951SB((Ni,1, Ni,2), B(j,1, Bj,2)) = 2 ?
(S1 + ?S1) ?
(S2 + (1 ?
?
)S2)(S1 + ?S1) + (S2 + (1 ?
?S2)) (2)The semantic relation is determined by rel:rel(Ni,1, Ni,2) = rel(Bm,1, Bm,2) (3)where m = argmaxjS((Ni,1, Ni,2), (Bj,1, Bj,2))7 Experimental Results7.1 Automatic Tagging Using SimilarityIn our first experiment, we tag the test NCs with semantic relations using four differentmeasures of noun similarity, assuming for the time being that the contribution of themodifier and head noun is equal (i.e.
?
= 0.5).
The baseline for this experiment is amajority-class classifier, in which all NCs are tagged according to the TOPIC class.Table 4.
Accuracy of NC interpretation for the different WordNet-based similarity measuresBasis Method SA SBmajority class Baseline 465 (43.0%) 465 (43.0%)path-based WUP 576 (53.3%) 557 (51.5%)path-based LCH 572 (52.9%) 565 (52.3%)information content-based JCN 505 (46.7%) 470 (43.5%)information content-based LIN 512 (47.4%) 455 (42.1%)human annotation Inter-annotator agreement 565 (52.3%) 565 (52.3%)Table 4 shows that WUP, using the SA multiplicative method of combination, pro-vides the highest NC interpretation accuracy, significantly above the majority-classbaseline.
It is particularly encouraging to see that WUP performs at or above the levelof inter-annotator agreement (52.3%), which could be construed as a theoretical upperbound for the task as defined here.
Using the F-score measure of similarity, LCH hasnearly the same performance as WUP.
Among the four measures of similarity used inthis first experiment, the path-based similarity measures have higher performance thanthe information content-based methods over both similarity combination methods.Compared to prior work on the automatic interpretation of NCs, our methodachieves relatively good results.
[7] achieved about 60% performance over the medicaldomain.
[8] used a word sense disambiguation system to achieve around 43% accuracyinterpreting NCs in the open domain.
Our accuracy of 53% compares favourably to bothof these sets of results, given that we are operating over open domain data.7.2 Relative Contribution of Modifier and Head NounIn the second experiment, we investigated the relative impact of the modifier and headnoun in determining the overall similarity of the NC.
While tagging the NCs, we got952 S.N.
Kim and T. Baldwin(accuracy %)alpha value48.54949.55050.55151.5520.0 0.2 0.4 0.6 0.8 1.0% w/ different weightFig.
3.
Classifier accuracy at different ?
valuesbeneficiarya ge ntcausecontainercontentdestinationequativeinstrumentlocatedlocationmaterialobjectpossessorproductpropertyresultpurposesourcetimetopic(accuracy %)(relation) 0204060801000?wup 5:5?wup 8:2?wup 2:8Fig.
4.
Classification accuracy for each semantic relation at different ?
valuesa sense of modifiers and head nouns having variable impact on the determination ofthe overall NC semantic relation.
For this test, we used the WUP method based on ourresults from above and also because it operates over the scale [0, 1], removing any needfor normalization.
In this experiment, modifiers and head nouns were assigned weights(?
in Equations 1 and 2) in the range 0.0, 0.1, ...1.0.Figure 3 shows the relative contribution of the modifier and head noun in the over-all NC interpretation process.
Interestingly, the head noun seems to be a more reliablepredictor of the overall NC interpretation than the modifier, and yet the best accuracyis achieved when each noun makes an equal contribution to the overall interpretation(i.e.
?
= 0.5).
Thus suggests that, despite any localized biases for individual NC inter-pretation types, the modifier and head noun have an equal impact on NC interpretationoverall.Automatic Interpretation of Noun Compounds 953Bm1 Bm2Bn1 Bn2Bn1 Bn2Bm1 Bm2Ni1 Ni2 CorrectAnswer0.820.79Ni1 Ni20.45 IncorrectAnswerNj1 Nj2Nj1 Nj20.790.45CorrectAnswerith step (i+1)th stepFig.
5.
Accumulating correctly tagged dataFigure 4 shows a breakdown of accuracy across the different semantic relation typesfor different weights.
In Figure 4, we have shown only the weights 0.2, 0.5 and 0.8 (toshow the general effect of variation in ?).
The dashed line shows the performance whenthe weight of modifiers and head nouns is the same (?
= 0.5).
The ?
symbol shows theresults of modifier-biased interpretation (?
= 0.8) and the + symbol shows the resultsof head noun-biased interpretation (?
= 0.2).
From Figure 4, we can see that for rela-tions such as CAUSE and INSTRUMENT, the modifier plays a more important role in thedetermination of the semantic relation of the NC.
On the other hand, for the CONTENTand PROPERTY relations, the head noun contributes more to NC interpretation.
Unex-pectedly, for EQUATIVE, the head noun contributes more than the modifier, althoughonly 9 examples were tagged with EQUATIVE, such that the result shown may not bevery representative of the general behavior.8 DiscussionWe have presented a method for interpreting the semantic relations of novel NCs usingword similarity.
We achieved about 53% interpretation accuracy using a path-basedmeasure of similarity.
Since our system was tested over raw test data from a generaldomain, we demonstrated that word similarity has surprising potential for interpretingthe semantic relations of NCs.
We also investigated using different weights for the headnoun and modifier to find out how much the modifier and head noun contributes in NCinterpretation and found that, with the exception of some isolated semantic relations,their relative contribution is equal.Our method has advantages such its relative simplicity and ability to run over smallamounts of training data, but there are also a few weaknesses.
The main bottleneck isthe availability of training data to use in classifying test instances.
We suggest that wecould use a bootstrap method to overcome this problem: in each step of classification,NCs which are highly similar to training instances, as determined by some threshold onsimilarity, are added to the training data to use in the next iteration of classification.
Oneway to arrive at such a threshold is to analyze the relative proportion of correctly- andincorrectly-classified instances at different similarity levels, through cross-validationover the training data.
We generate such a curve for the test data, as detailed in Fig-ure 6.If we were to use the crossover point (similarity ?
0.57), we would clearly ?infect?the training data with a significant number of misclassified instances, namely 30.69%of the new training instances; this would have an unpredictable impact on classifica-tion performance.
On the other hand, if we were to select a higher threshold based ona higher estimated proportion of correctly-classified instances (e.g.
70%), the relative954 S.N.
Kim and T. Baldwin(accuracy %)(similarity)ErrorSimilarity=0.57THRESHOLD(a) error rate with similarity 0.570204060801000  0.2  0.4  0.6  0.8  1?correct.percent??incorrect.percent?Fig.
6.
The relative proportion of correctly- and incorrectly-classified NCs at different similarityvalues, and the estimated impact of threshold-based bootstrappingincrease in training examples would be slight, and there would be little hope for muchimpact on the overall classifier accuracy.
Clearly, therefore, there is a trade-off here be-tween how much training data we wish to acquire automatically and whether this willimpact negatively or positively on classification performance.
We leave investigationof this trade-off as an item for future research.
Interestingly, in Figure 6 the propor-tion of misclassified examples is monotonically decreasing, providing evidence for thesoundness of the proposed similarity-based model.In the first experiment (where the weight of the modifier and head noun was thesame), we observed that some of the test NCs matched with several training NCs withhigh similarity.
However, since we chose only the NC with the highest similarity, weignored any insight other closely-matching training NCs may have provided into thesemantics of the test NC.
One possible workaround here would be to employ a votingstrategy, for example, in taking the k most-similar training instances and determin-ing the majority class amongst them.
Once again, we leave this as an item for futureresearch.AcknowledgementsWe would like to express our thanks to Bharaneedharan Rathnasabapathy for helpingto tag the noun compound semantic relations, and the anonymous reviewers for theircomments and suggestions.References1.
Cao, Y., Li, H.: Base noun phrase translation using web data and the em algorithm.
In:COLING2002.
(2002)2.
Baldwin, T., Tanaka, T.: Translation by machine of compound nominals: Getting it right.
In:ACL2004-MWE, Barcelona, Spain (2004) 24?31Automatic Interpretation of Noun Compounds 9553.
Lauer, M.: Designing Statistical Language Learners: Experiments on Noun Compounds.PhD thesis, Macquarie University (1995)4.
Lapata, M.: The disambiguation of nominalizations.
Comput.
Linguist.
28 (2002) 357?3885.
Finin, T.W.
: The semantic interpretation of compound nominals.
PhD thesis, University ofIllinois, Urbana, Illinois, USA (1980)6.
Vanderwende, L.: Algorithm for automatic interpretation of noun sequences.
In: Proceedingsof the 15th conference on Computational linguistics.
(1994) 782?7887.
Rosario, B., Marti, H.: Classifying the semantic relations in noun compounds via a domain-specific lexical hierarchy.
In: Proceedings of the 2001 Conference on Empirical Methods inNatural Language Processing.
(2001) 82?908.
Moldovan, D., Badulescu, A., Tatu, M., Antohe, D., Girju, R.: Models for the semanticclassification of noun phrases.
HLT-NAACL 2004: Workshop on Computational LexicalSemantics (2004) 60?679.
Fan, J., Barker, K., Porter, B.W.
: The knowledge required to interpret noun com-pounds.
In: Seventh International Joint Conference on Artificial Intelligence.
(2003)1483?148510.
Barker, K., Szpakowicz, S.: Semi-automatic recognition of noun modifier relationships.
In:Proceedings of the 17th international conference on Computational linguistics.
(1998) 96?10211.
Artiles, J., Penas, A., Verdejo, F.: Word sense disambiguation based on term to term simi-larity in a context space.
In: Senseval-3: Third International Workshop on the Evaluation ofSystems for the Semantic Analysis of Text.
(2004) 58?6312.
Patwardhan, S., Banerjee, S., Pedersen, T.: Using measures of semantic relatedness for wordsense disambiguation.
In: Proceedings of the Fourth International Conference on IntelligentText Processing and Computational Linguistics.
(2003)13.
Resnik, P.: Disambiguating noun groupings with respect to wordnet senses.
In: Proceedingsof the 3rd Workship on Very Large Corpus.
(1995) 77?9814.
Wu, Z., Palmer, M.: Verb semantics and lexical selection.
In: 32nd.
Annual Meeting of theAssociation for Computational Linguistics.
(1994) 133 ?13815.
Fellbaum, C., ed.
: WordNet: An Electronic Lexical Database.
MIT Press, Cambridge, USA(1998)16.
Leacock, C., Chodorow, N.: Combining local context and wordnet similarity for word senseidentification.
[15]17.
Jiang, J., Conrath, D.: Semantic similarity based on corpus statistics and lexical taxon-omy.
In: Proceedings on International Conference on Research in Computational Linguistics.
(1998) 19?3318.
Lin, D.: An information-theoretic definition of similarity.
In: Proceedings of the InternationalConference on Machine Learning.
(1998)19.
Banerjee, S., Pedersen, T.: Extended gloss overlaps as a measure of semantic relatedness.In: Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence.
(2003) 805?81020.
Levi, J.: The syntax and semantics of complex nominals.
In: New York:Academic Press.(1979)21.
Downing, P.: On the creation and use of English compound nouns.
Language 53 (1977)810?42956 S.N.
Kim and T. BaldwinAppendixTable 5.
The Semantic Relations in Noun Compounds (N1 = modifier, N2 = head noun)Relation Definition Example # of test/trainingAGENT N2 is performed by N1 student protest, band concert 10(2)/5BENEFICIARY N1 benefits from N2 student price, charitable compound 10(1)/7(1)CAUSE N1 causes N2 printer tray, flood water 54(10)/74(11)CONTAINER N1 contains N2 exam anxiety 13(6)/19(5)CONTENT N1 is contained in N2 paper tray, eviction notice 40(5)/34(7)DESTINATION N1 is destination of N2 game bus, exit route 2(1)/2EQUATIVE N1 is also head composer arranger, player coach 9/17(3)INSTRUMENT N1 is used in N2 electron microscope, diesel engine 6/11(2)LOCATED N1 is located at N2 building site, home town 12(2)/16(4)LOCATION N1 is the location of N2 lab printer, desert storm 29(10)/24(5)MATERIAL N2 is made of N1 carbon deposit, gingerbread man 12(1)/15(2)OBJECT N1 is acted on by N2 engine repair, horse doctor 88(16)/88(21)POSSESSOR N1 has N2 student loan, company car 32(3)/22(4)PRODUCT N1 is a product of N2 automobile factory, light bulb 27(1)/32(9)PROPERTY N2 is N1 elephant seal 76(5)/85(7)PURPOSE N2 is meant for N1 concert hall, soup pot 160(23)/160(23)RESULT N1 is a result of N2 storm cloud, cold virus 7(4)/8(1)SOURCE N1 is the source of N2 chest pain, north wind 86(21)/99(18)TIME N1 is the time of N2 winter semester, morning class 26(2)/19TOPIC N2 is concerned with N1 computer expert, safety standard 465(51)/446(60)The 4thcolumn gives us the number of words tagged with the corresponding relation in the1stcolumn.
The numbers within the parenthesis gives us the number of words that are taggedwith multiple relations( i.e.
those that are tagged with the relation in the 1stcolumn and otherrelations as well).
In the training data, 94 NCs have multiple relations and in test data, 81 NCshave multiple relations.
