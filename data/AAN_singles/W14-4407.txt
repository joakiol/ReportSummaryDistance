Proceedings of the 8th International Natural Language Generation Conference, pages 45?53,Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational LinguisticsA Template-based Abstractive Meeting Summarization: LeveragingSummary and Source Text RelationshipsTatsuro Oya, Yashar Mehdad, Giuseppe Carenini, Raymond NgDepartment of Computer ScienceUniversity of British Columbia, Vancouver, Canada{toya, mehdad, carenini, rng}@cs.ubc.caAbstractIn this paper, we present an automaticabstractive summarization system ofmeeting conversations.
Our system ex-tends a novel multi-sentence fusion algo-rithm in order to generate abstract tem-plates.
It also leverages the relationshipbetween summaries and their sourcemeeting transcripts to select the besttemplates for generating abstractivesummaries of meetings.
Our manual andautomatic evaluation results demonstratethe success of our system in achievinghigher scores both in readability and in-formativeness.1.
IntroductionPeople spend a vast amount of time in meetingsand these meetings play a prominent role in theirlives.
Consequently, study of automatic meetingsummarization has been attracting peoples?
atten-tion as it can save a great deal of their time andincrease their productivity.The most common approaches to automaticmeeting summarization have been extractive.Since extractive approaches do not require natu-ral language generation techniques, they are ar-guably simpler to apply and have been extensive-ly investigated.
However, a user study conductedby Murray et al.
(2010) indicates that users pre-fer abstractive summaries to extractive ones.Thereafter, more attention has been paid to ab-stractive meeting summarization systems (Me-hdad et al.
2013; Murray et al.
2010; Wang andCardie 2013).
However, the approaches intro-duced in previous studies create summaries byeither heavily relying on annotated data or byfusing human utterances which may containgrammatical mistakes.
In this paper, we addressthese issues by introducing a novel summariza-tion approach that can create readable summarieswith less need for annotated data.
Our systemfirst acquires templates from human-authoredsummaries using a clustering and multi-sentencefusion algorithm.
It then takes a meeting tran-script to be summarized, segments the transcriptbased on topics, and extracts important phrasesfrom it.
Finally, our system selects templates byreferring to the relationship between human-authored summaries and their sources and fillsthe templates with the phrases to create summar-ies.The main contributions of this paper are: 1)The successful adaptation of a word graph algo-rithm to generate templates from human-authored summaries; 2) The implementation of anovel template selection algorithm that effective-ly leverages the relationship between human-authored summary sentences and their sourcetranscripts; and 3) A comprehensive testing ofour approach, comprising both automatic andmanual evaluations.We instantiate our framework on the AMIcorpus (Carletta et al., 2005) and compare oursummaries with those created from a state-of-the-art systems.
The evaluation results demon-strate that our system successfully creates in-formative and readable summaries.2.
Related WorkSeveral studies have been conducted on creatingautomatic abstractive meeting summarizationsystems.
One of them includes the system pro-posed by Mehdad et al., (2013).
Their approachfirst clusters human utterances into communities(Murray et al., 2012) and then builds an entail-ment graph over each of the latter in order to se-lect the salient utterances.
It then applies a se-mantic word graph algorithm to them and createsabstractive summaries.
Their results show someimprovement in creating informative summaries.45However, since they create these summaries bymerging human utterances, their summaries arestill partially extractive.Recently, there have been some studies oncreating abstract summaries of specific aspects ofmeetings such as decisions, actions and problems(Murray et al.
2010; Wang and Cardie, 2013).These summaries are called the Focused MeetingSummaries (Carenini et al., 2011).The system introduced by Murray et al.
firstclassifies human utterances into specific aspectsof meetings, e.g.
decisions, problem, and action,and then maps them onto ontologies.
It then se-lects the most informative subsets from these on-tologies and finally generates abstractive sum-maries of them, utilizing a natural language gen-eration tool, simpleNLG (Gatt and Reiter, 2009).Although their approach is essentially focusedmeeting summarization, after creating summariesof specific aspects, they aggregate them into onesingle summary covering the whole meeting.Wang and Cardie introduced a template-basedfocused abstractive meeting summarization sys-tem.
Their system first clusters human-authoredsummary sentences and applies a Multiple-Sequence Alignment algorithm to them to gener-ate templates.
Then, given a meeting transcript tobe summarized, it identifies a human utterancecluster describing a specific aspect and extractsall summary-worthy relation instances, i.e.
indi-cator-argument pairs, from it.
Finally, the tem-plates are filled with these relation instances andranked accordingly, to generate summaries of aspecific aspect of the meeting.Although the two approaches above are bothsuccessful in creating readable summaries, theyrely on much annotated information, such as dia-log act and sentiment types, and also require theaccurate classification of human utterances thatcontain much noise and much ill-structuredgrammar.Our approach is inspired by the works intro-duced here but improves on their shortcomings.Unlike those of Murray et al.
(2010) and Wangand Cardie (2013), our system relies less on an-notated training data and does not require a clas-sifier.
In addition, our evaluation indicates thatour system can create summaries of the entireconversations that are more informative andreadable than those of Mehdad et al.(2013).3.
FrameworkIn order for summaries to be readable and in-formative, they should be grammatically correctand contain important information in meetings.To this end, we have created our framework con-sisting of the following two components: 1) Anoff-line template generation module, which gen-eralizes collected human-authored summariesand creates templates from them; and 2) An on-line summary generation module, which seg-ments meeting transcripts based on the topicsdiscussed, extracts the important phrases fromthese segments, and generate abstractive sum-maries of them by filling the phrases into the ap-propriate templates.
Figure 1 depicts our frame-work.
In the following sections, we describe eachof the two components in detail.Figure 1: Our meeting summarization framework.
Top: off-line Template generation module.
Bottom: on-lineSummary Generation module.463.1 Template Generation ModuleOur template generation module attempts to sat-isfy two possibly conflicting objectives.
First,templates should be quite specific such that theyaccept only the relevant fillers.
Second, ourmodule should generate generalized templatesthat can be used in many situations.
We assumethat the former is achieved by labeling phraseswith their hypernyms that are not too general andthe latter by merging related templates.
Based onthese assumptions, we divide our module into thethree tasks: 1) Hypernym labeling; 2) Clustering;and 3) Template fusion.3.1.1 Hypernym LabelingTemplates are derived from human-authoredmeeting summaries in the training data.
We firstcollect sentences whose subjects are meeting par-ticipant(s) and that contain active root verbs,from the summaries.
This is achieved by utilizingmeeting participant information provided in thecorpus and parsing sentences with the StanfordParser (Marneffe et al., 2006).
The motivationbehind this process is to collect sentences that aresyntactically similar.
We then identify all nounphrases in these sentences using the IllinoisChunker (Punyakanok and Roth, 2001).
Thischunker extracts all noun phrases as well as partof speech (POS) for all words.
To add further in-formation on each noun phrase, we label the rightmost nouns (the head nouns) in each phrase withtheir hypernyms using WordNet (Fellbaum,1998).
In WordNet, hypernyms are organized in-to hierarchies ranging from the most abstract tothe most specific.
For our work, we utilize thefourth most abstract hypernyms in light of thefirst goal discussed at the beginning of Section3.1, i.e.
not too general.
For disambiguating thesense of the nouns, we simply select the sensethat has the highest frequency in WordNet.At this stage, all noun phrases in sentencesare tagged with their hypernyms defined inWordNet, such as ?artifact.n.01?, and ?act.n.02?,where n?s stands for nouns and the two digitnumbers represent their sense numbers.
We treatthese hypernym-labeled sentences as templatesand the phrases as blanks.In addition, we also create two additionalrules for tagging noun phrases: 1) Since the sub-jects of all collected sentences are meeting par-ticipant(s), we label all subject noun phrases as?speaker?
; and 2) If the noun phrases consist ofmeeting specific terms such as ?the meeting?
or?the group?, we do not convert them into blanks.These two rules guarantee the creation of tem-plates suitable for meetings.Figure 2: Some examples of hypernym labeling task3.1.2 ClusteringNext, we cluster the templates into similargroups.
We utilize root verb information for thisprocess assuming that these verbs such as ?dis-cuss?
and ?suggest?
that appear in summaries arethe most informative factors in describing meet-ings.
Therefore, after extracting root verbs insummary sentences, we create fully connectedgraphs where each node represents the root verbsand each edge represents a score denoting howsimilar the two word senses are.
To measure thesimilarity of two verbs, we first identify the verbsenses based on their frequency in WordNet andcompute the similarity score based on the short-est path that connects the senses in the hypernymtaxonomy.
We then convert the graph into asimilarity matrix and apply a Normalized Cutsmethod (Shi and Malik, 2000) to cluster the rootverbs.
Finally, all templates are organized intothe groups created by their root verbs.Figure 3: A word graph generated from related templates and the highest scored path (shown in bold)473.1.3 Template FusionWe further generalize the clustered templates byapplying a word graph algorithm.
The algorithmwas originally proven to be effective in summa-rizing a cluster of related sentences (Boudin andMorin, 2013; Filippova, 2010; Mehdad et al.,2013).
We extend it so that it can be applied totemplates.Word Graph ConstructionIn our system, a word graph is a directed graphwith words or blanks serving as nodes and edgesrepresenting adjacency relations.Given a set of related templates in a group,the graph is constructed by first creating a startand end node, and then iteratively adding tem-plates to it.
When adding a new template, the al-gorithm first checks each word in the template tosee if it can be mapped onto existing nodes in thegraph.
The word is mapped onto a node if thenode consists of the same word and the samePOS tag, and no word from this template hasbeen mapped onto this node yet.
Then, it checkseach blank in the template and maps it onto anode if the node consists of the same hypernym-labeled blank and no blank from this templatehas been mapped onto this node yet.When more than one node refer to the sameword or blank in the template, or when more thanone word or blank in the template can be mappedto the same node in the graph, the algorithmchecks the neighboring nodes in the currentgraph as well as the preceding and the subse-quent words or blanks in the template.
Then,those word-node or blank-node pairs with higheroverlap in the context are selected for mapping.Otherwise, a new node is created and added tothe graph.
As a simplified illustration, we show aword graph in Figure 3 obtained from the follow-ing four templates.?
After introducing [situation.n.01], [speaker] then dis-cussed [content.n.05] .?
Before beginning [act.n.02] of [artifact.n.01], [speaker]discussed [act.n.02] and [content.n.05] for [arti-fact.n.01] .?
[speaker] discussed [content.n.05] of [artifact.n.01] and[material.n.01] .?
[speaker] discussed [act.n.02] and [asset.n.01] in attract-ing [living_thing.n.01] .Path SelectionThe word graph generates many paths connect-ing its start and end nodes, not all of which arereadable and cannot be used as templates.
Ouraim is to create concise and generalized tem-plates.
Therefore, we create the following rank-ing strategy to be able to select the ideal paths.First, to filter ungrammatical or complex tem-plates, the algorithm prunes away the paths hav-ing more than three blanks; having subordinateclauses; containing no verb; having two consecu-tive blanks; containing blanks which are not la-beled by any hypernym; or whose length areshorter than three words.
Note that these rules,which were defined based on close observationof the results obtained from our development set,greatly reduce the chance of selecting ill-structured templates.
Second, the remainingpaths are reranked by 1) A normalized pathweight and 2) A language model learned fromhypernym-labeled human-authored summaries inour training data, each of which is described be-low.1) Normalized Path WeightWe adapt Filippova (2010)?s approach to com-pute the edge weight.
The formula is shown as:?where ei,j  is an edge that connects the nodes iand j in a graph, freq(i) is the number of wordsand blanks in the templates that are mapped tonode i and diff(p,i,j) is the distance between theoffset positions of nodes i and j in path p. Thisweight is defined so that the paths that are in-formative and that contain salient (frequent)words are selected.
To calculate a path score,W(p), all the edge weights on the path aresummed and normalized by its length.2) Language ModelAlthough the goal is to create concise templates,these templates must be grammatically correct.Hence, we train an n-gram language model usingall templates generated from the training data inthe hypernym labeling stage.
Then for each path,we compute a sum of negative log probabilitiesof n-gram occurrences and normalize the scoreby its length, which is represented as H(p).The final score of each path is calculated asfollows:where ?
and ?
are the coefficient factors whichare tuned using our development set.
For each48group of clusters, the top ten best scored pathsare selected as templates and added to its group.As an illustration, the path shown in bold inFigure 3 is the highest scored path obtained fromthis path ranking strategy.3.2 Summary Generation ModuleThis section explains our summary generationmodule consisting of four tasks: 1) Topic seg-mentation; 2) Phrase and speaker extraction; 3)Template selection and filling; and 4) Sentenceranking.3.2.1 Topic SegmentationIt is important for a summary to cover all topicsdiscussed in the meeting.
Therefore, given ameeting transcript to be summarized, after re-moving speech disfluencies such as ?uh?, and?ah?, we employ a topic segmenter, LCSeg (Gal-ley et al., 2003) which create topic segments byobserving word repetitions.One shortcoming of LCSeg is that it ignoresspeaker information when segmenting transcripts.Important topics are often discussed by one ortwo speakers.
Therefore, in order to take ad-vantage of the speaker information, we extendLCSeg by adding the following post-processstep: If a topic segment contains more than 25 ut-terances, we subdivide the segment based on thespeakers.
These subsegments are then comparedwith one another using cosine similarity, and ifthe similarity score is greater than that of thethreshold (0.05), they are merged.
The two num-bers, i.e.
25 and 0.05, were selected based on thedevelopment set so that, when segmenting a tran-script, the system can effectively take into ac-count speaker information without creating toomany segments.3.2.2 Phrase And Speaker ExtractionAll salient phrases are then extracted from eachtopic segment in the same manner as performedin the template generation module in Section 3.1,by: 1) Extracting all noun phrases; and 2) Label-ing each phrase with the hypernym of its headnoun.
Furthermore, to be able to select salientphrases, these phrases are subsequently scoredand ranked based on the sum of the frequency ofeach word in the segment.
Finally, to handle re-dundancy, we remove phrases that are subsets ofothers.In addition, for each utterance in the meeting,the transcript contains its speaker?s name.
There-fore, we extract the most dominant speakers?name(s) for each topic segment and label them as?speaker?.
These phrases and this speaker infor-mation will later be used in the template fillingprocess.
Table 1 below shows an example ofdominant speakers and high scored phrases ex-tracted from a topic segment.Dominant speakersProject Manager (speaker)Industrial Designer (speaker)High scored phrases (hypernyms)the whole look (appearance.n.01)the company logo (symbol.n.01)the product (artifact.n.01)the outside (region.n.01)electronics (content.n.05)the fashion (manner.n.01)Table 1: Dominant speakers and high scoredphrases extracted from a topic segment3.2.3 Template Selection and FillingIn terms of our training data, all human-authoredabstractive summary sentences have links to thesubsets of their source transcripts which supportand convey the information in the abstractivesentences as illustrated in Figure 4.
These subsetsare called communities.
Since each community isused to create one summary sentence, we hy-pothesize that each community covers one spe-cific topic.Thus, to find the best templates for each topicsegment, we refer to our training data.
In particu-lar, we first find communities in the training setthat are similar to the topic segment and identifythe templates derived from the summary sen-tences linked to these communities.Figure 4: A link from an abstractive summary sentence to a subset of a meeting transcript that conveys or sup-ports the information in the abstractive sentence49This process is done in two steps, by: 1) As-sociating the communities in the training datawith the groups containing templates that werecreated in our template generation module; and2) Finding templates for each topic segment bycomparing the similarities between the segmentsand all sets of communities associated with thetemplate groups.
Below, we describe the twosteps in detail.1) Recall that in the template generationmodule in Section 3.1, we label human-authoredsummary sentences in training data with hyper-nyms and cluster them into similar groups.
Thus,as shown in Figure 5, we first associate all sets ofcommunities in the training data into thesegroups by determining to which groups thesummary sentences linked by these communitiesbelong.Figure 5: An example demonstrating how each com-munity in training data is associated with a group con-taining templates2) Next, for each topic segment, we computeaverage cosine similarity between the segmentand all communities in all of the groups.Figure 6: Computing the average cosine similaritiesbetween a topic segment and all sets of com munitiesin each groupAt this stage, each community is already as-sociated with a group that contains ranked tem-plates.
In addition, each segment has a list of av-erage-scores that measures how similar the seg-ment is to the communities in each group.
Hence,the templates used for each segment are decidedby selecting the ones from the groups with higherscores.Our system now contains for each segment aset of phrases and ideal templates, both of whichare scored, as well as the most dominant speakers?name(s).
Thus, candidate sentences are generatedfor each segment by: first, selecting speakers?name(s), then selecting phrases and templatesbased on their scores; and finally filling the tem-plates with matching labels.
Here, we limit themaximum number of sentences created for eachtopic segment to 30.
This number is defined sothat the system can avoid generating sentencesconsisting of low scored phrases and templates.Finally, these candidate sentences are passed toour sentence ranking module.3.2.4 Sentence RankingOur system will create many candidate sentenc-es, and most of them will be redundant.
Hence,to be able to select the most fluent, informativeand appropriate sentences, we create a sentenceranking model considering 1) Fluency, 2) Cover-age, and 3) The characteristics of the meeting,each of which are summarized below:1) FluencyWe estimate the fluency of the generated sen-tences in the same manner as in Section 3.1.3.That is, we train a language model on human-authored abstract summaries from the trainingportions of meeting data and then compute anormalized sum of negative log probabilities ofn-gram occurrences in the sentence.
The fluencyscore is represented as H(s) in the equation be-low.2) CoverageTo select sentences that cover important topics,we give special rewards to the sentences thatcontain the top five ranked phrases.3) The Characteristics of the MeetingWe also add three additional scoring rules thatare specific to the meeting summaries.
In particu-lar, these three rules are created based on phrasesoften used in the opening and closing of meet-ings in a development set: 1) If sentences derived50from the first segment contain the words ?open?or ?meeting?, they will be rewarded; 2) If sen-tences derived from the last segment contain thewords ?close?
or ?meeting?, the sentences willagain be rewarded; and 3)  If sentences not de-rived  from the first or last segment contains thewords ?open?
or ?close?,  they will be penalized.The final ranking score of the candidate sen-tences is computed using the follow formula:s  ?
s  ?
?ii 1  i s  ?
ii 1  i swhere, Ri (s) is a binary that indicates whetherthe top i ranked phrase exists in sentence s; Mi (s)is also a binary that indicates whether the i thmeeting specific rule can be met for sentence s;and ?, ?
i and ?
i are the coefficient factors to tunethe ranking score, all of which are tuned usingour development set.Finally, the sentence ranked the highest ineach segment is selected as the summary sen-tence, and the entire meeting summary is createdby collecting these sentences and sorting them bythe chronological order of the topic segments.4.
EvaluationIn this section, we describe an evaluation of oursystem.
First, we describe the corpus data.
Next,the results of the automatic and manual evalua-tions of our system against various baseline ap-proaches are discussed.4.1 DataFor our meeting summarization experiments, weuse manually transcripted meeting records andtheir human-authored summaries in the AMIcorpus.
The corpus contains 139 meeting recordsin which groups of four people play differentroles in a fictitious team.
We reserved 20 meet-ings for development and implemented a three-fold cross-validation using the remaining data.4.2 Automatic EvaluationWe report the F1-measure of ROUGE-1,ROUGE-2 and ROUGE-SU4 (Lin and Hovy,2003) to assess the performance of our system.The scores of automatically generated summariesare calculated by comparing them with human-authored ones.For our baselines, we use the system intro-duced by Mehdad et al.
(2013) (FUSION), whichcreates abstractive summaries from extractedsentences and was proven to be effective in cre-ating abstractive meeting summaries; and Tex-tRank (Mihalcea and Tarau, 2004), a graph basedsentence ranker that is suitable for creating ex-tractive summaries.
Our system can create sum-maries of any length by adjusting the number ofsegments to be created by LCSeg.
Thus, we cre-ate summaries of three different lengths (10, 15,and 20 topic segments) with the average numberof words being 100, 137, and 173, respectively.These numbers generally corresponds to human-authored summary length in the corpus whichvaries from 82 to 200 words.Table 2 shows the results of our system incomparison with those of the two baselines.
Theresults show that our model significantly outper-forms the two baselines.
Compared with FU-SION, our system with 20 segments achievesabout 3 % of improvement in all ROUGE scores.This indicates that our system creates summariesthat are more lexically similar to human-authoredones.
Surprisingly, there was not a significantchange in our ROUGE scores over the three dif-ferent summary lengths.
This indicates that oursystem can create summaries of any length with-out losing its content.Models Rouge-1 Rouge-2 Rouge-SU4TextRank 21.7 2.5 6.5FUSION 27.9 4.0 8.1Our System 10 Seg.
28.4 6.7 10.1Our System 15 Seg.
30.6 6.8 10.9Our System 20 Seg.
31.5 6.7 11.4Table 2: An evaluation of summarization performanceusing the F1 measure of ROUGE-1 2, and SU44.3 Manual EvaluationWe also conduct manual evaluations utilizing acrowdsourcing tool1.
In this experiment, our sys-tem with 15 segments is compared with FUSION,human-authored summaries (ABS) and, human-annotated extractive summaries (EXT).After randomly selecting 10 meetings, 10 par-ticipants were selected for each meeting and giv-en instructions to browse the transcription of themeeting so as to understand its gist.
They werethen asked to read all different types of summar-ies described above and rate each of them on a 1-5 scale for the following three items: 1) Thesummary?s overall quality, with ?5?
being thebest and ?1?
being the worst possible quality; 2)The summary?s fluency, ignoring the capitaliza-tion or punctuation, with ?5?
indicating nogrammatical mistakes and ?1?
indicating toomany; and 3) The summary?s informativeness,with ?5?
indicating that the summary covers allmeeting content and ?1?
indicating that the1 http://www.crowdflower.com/51summary does not cover the content at all.The results are described in Table 3.
Overall,58 people worldwide, who are among the mostreliable contributors accounting for 7 % of over-all members and who maintain the highest levelsof accuracy on test questions provided in pervi-ous crowd sourcing jobs, participated in this rat-ing task.
As to statistical significance, we use the2-tail pairwise t-test to compare our system withthe other three approaches.
The results are sum-marized in Table 4.Models Quality Fluency InformativenessOur System  3.52 3.69 3.54ABS 3.96 4.03 3.87EXT 3.02 3.16 3.30FUSION 3.16 3.14 3.05Table 3: Average rating scores.ModelsComparedQuality(P-value)Fluency(P-value)Informativeness(P-value)Our Systemvs.
ABS0.000162 0.000437 0.00211Our Systemvs.
FUSION0.00142 0.0000135 0.000151Our Systemvs.
EXT.0.000124 0.0000509 0.0621Table 4: T-test results of manual evaluationAs expected, for all of the three items, ABSreceived the highest of all ratings, while our sys-tem received the second highest.
The t-test re-sults indicate that the difference in the rating datais statistically significant for all cases except thatof informativeness between ours and the extrac-tive summaries.
This can be understood becausethe extractive summaries were manually createdby an annotator and contain all of the importantinformation in the meetings.From this observation, we can conclude thatusers prefer our template-based summaries overhuman-annotated extractive summaries and ab-stractive summaries created from extracted sali-ent sentences.
Furthermore, it demonstrates thatour summaries are as informative as human-annotated extractive ones.Finally, we show in Figure 7 one of the sum-maries created by our system in line-with a hu-man-authored one.5.
Conclusion and Future WorkIn this paper, we have demonstrated a robust ab-stractive meeting summarization system.
Our ap-proach makes three main contributions.
First, wehave proposed a novel approach for generatingtemplates leveraging a multi-sentence fusion al-gorithm and lexico-semantic information.
Sec-ond, we have introduced an effective templateselection method, which utilize the relationshipbetween human-authored summaries and theirsource transcripts.
Finally, comprehensive evalu-ation demonstrated that summaries created byour system are preferred over human-annotatedextractive ones as well as those created from astate-of-the-art meeting summarization system.The current version of our system uses onlyhypernym information in WordNet to labelphrases.
Considering limited coverage in Word-Net, future work includes extending our frame-work by applying a more sophisticated labelingtask utilizing a richer knowledge base (e.g., YA-GO).
Also, we plan to apply our framework todifferent multi-party conversational domainssuch as chat logs and forum discussions.Human-Authored SummaryThe project manager opened the meeting and had theteam members introduce themselves and describe theirroles in the upcoming project.
The project manager thendescribed the upcoming project.
The team then discussedtheir experiences with remote controls.
They alsodiscussed the project budget and which features theywould like to see in the remote control they are to create.The team discussed universal usage, how to find remoteswhen misplaced, shapes and colors, ball shaped remotes,marketing strategies, keyboards on remotes, and remotesizes.
team then discussed various features to consider inmaking the remote.Summary Created by Our System with 15 Segmentproject manager summarized their role of the meeting .user interface expert and project manager talks about auniversal remote .
the group recommended using theInternational Remote Control Association rather than aremote control .
project manager offered the ballidea .user interface expert suggested few buttons .
userinterface expert and industrial designer then asked amember about a nice idea for The idea .
project managerwent over a weak point .
the group announced the one-handed design .
project manager and industrial designerwent over their remote control idea .
project managerinstructed a member to research the ball function .industrial designer went over stability point .industrialdesigner went over definite points .Figure 7: A comparison between a human-authoredsummary and a summary created by our systemAcknowledgementsWe would like to thank all members in UBCNLP group for their comments and UBC LCIgroup and ICICS for financial support.ReferencesFlorian Boudin and Emmanuel Morin.
2013.Keyphrase Extraction for N-best Reranking in Mul-ti-Sentence Compression.
In  Proceedings of the522013 Conference of the North American Chapter ofthe Association for Computational Linguistics:Human Language Technologies (NAACL-HLT2013), 2013.Giuseppe Carenini, Gabriel Murray, and Raymond Ng.2011.
Methods for Mining and Summarizing TextConversations.
Morgan Claypool.J.
Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guil-lemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,M.
Kronenthal, G. Lathoud, M. Lincoln, A.Lisowska, I. McCowan, W. Post, D. Reidsma, andP.
Wellner.
2005.
The AMI meeting corpus: A pre-announcement.
In Proceeding of MLMI 2005, Ed-inburgh, UK, pages 28?39.Christiane Fellbaum 1998.
WordNet, An ElectronicLexical Database.
The MIT Press.
Cambridge, MA.Katja Filippova.
2010.
Multi-sentence compression:finding shortest paths in word graphs.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics, COLING ?10, pages 322?330, Stroudsburg, PA, USA.
Association for Com-putational LinguisticMichel Galley, Kathleen McKeown, Eric Fosler-Lussier and Hongyan Jing.
2003.
Discourse seg-mentation of multi-party conversation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics (ACL 2003).
562?569.
Sapporo, Japan.Albert Gatt and Ehud Reiter.
2009.
SimpleNLG: aRealisation Engine for Practical Applications.
InENLG?09: Proceedings of the 12th EuropeanWorkshop on Natural Language Generation, pages90?93, Morristown, NJ, USA.
Association forComputational Linguistics.Ravi Kondadadi, Blake Howald and Frank Schilder.2013.
A Statistical NLG Framework for Aggregat-ed Planning and Realization.
In Proceeding of theAnnual Conferene for the Association of Computa-tional Linguistic (ACL 2013).Marie-Catherine de Marneffe, Bill MacCartney andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.In Proceedings of the Fifth International Confer-ence on Language Resources and Evaluation(LREC'06).Yashar Mehdad, Giuseppe Carenini, Frank Tompa.2013.
Abstractive Meeting Summarization with En-tailment and Fusion.
In Proceedings of the 14th Eu-ropean Natural Language Generation (ENLG -SIGGEN 2013), Sofia, Bulgaria.Rata Mihalcea and Paul Tarau 2004.
TextRank:Bringing order into texts.
In Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing, July.Gabriel Murray, Giuseppe Carenini, and Raymond T.Ng.
2010.
Generating and validating abstracts ofmeeting conversations: a user study.
In INLG 2010.Gabriel Murray, Giuseppe Carenini and Raymond Ng.2012.
Using the Omega Index for Evaluating Ab-stractive Community Detection, NAACL 2012,Workshop on Evaluation Metrics and System Com-parison for Automatic Summarization, Montreal,Canada.Vasin Punyakanok and Dan Roth.
2001.
The Use ofClassifiers in Sequential Inference.
NIPS (2001) pp.995-1001.Jianbo Shi and Jitendra Malik.
2000.
Normalized Cuts& Image Segmentation.
IEEE Trans.
of PAMI, Aug2000.David C. Uthus and David W. Aha.
2011.
Plans to-ward automated chat summarization.
In Proceed-ings of the Workshop on Automatic Summarizationfor Different Genres, Media, and Languages,WASDGML?11, pages 1-7, Stroudsburg, PA, USA.Association for Computational Linguistics.Lu Wang and Claire Cardie.
2013.
Domain-Independent Abstract Generation for FocusedMeeting Summarization.
In ACL 2013.Liang Zhou and Eduard Hovy.
2005.
Digesting virtual?geek?
culture: The summarization of technical in-ternet relay chats.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics (ACL?05), pages 298-305, Ann Arbor,Michigan, June.
Association for ComputationalLinguistics.53
