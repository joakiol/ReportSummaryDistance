Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 564?574,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLearning to Explain Entity Relationships in Knowledge GraphsNikos Voskarides?University of Amsterdamn.voskarides@uva.nlEdgar MeijYahoo Labs, Londonemeij@yahoo-inc.comManos Tsagkias904Labs, Amsterdammanos@904labs.comMaarten de RijkeUniversity of Amsterdamderijke@uva.nlWouter Weerkamp904Labs, Amsterdamwouter@904labs.comAbstractWe study the problem of explaining re-lationships between pairs of knowledgegraph entities with human-readable de-scriptions.
Our method extracts and en-riches sentences that refer to an entity pairfrom a corpus and ranks the sentences ac-cording to how well they describe the re-lationship between the entities.
We modelthis task as a learning to rank problem forsentences and employ a rich set of fea-tures.
When evaluated on a large set ofmanually annotated sentences, we find thatour method significantly improves overstate-of-the-art baseline models.1 IntroductionKnowledge graphs are a powerful tool for support-ing a large spectrum of search applications includ-ing ranking, recommendation, exploratory search,and web search (Dong et al, 2014).
A knowl-edge graph aggregates information around enti-ties across multiple content sources and links theseentities together, while at the same time provid-ing entity-specific properties (such as age or em-ployer) and types (such as actor or movie).Although there is a growing interest in au-tomatically constructing knowledge graphs, e.g.,from unstructured web data (Weston et al, 2013;Craven et al, 2000; Fan et al, 2012), the prob-lem of providing evidence on why two entitiesare related in a knowledge graph remains largelyunaddressed.
Extracting and presenting evidencefor linking two entities, however, is an impor-tant aspect of knowledge graphs, as it can enforcetrust between the user and a search engine, whichin turn can improve long-term user engagement,e.g., in the context of related entity recommenda-tion (Blanco et al, 2013).
Although knowledge?This work was carried out while this author was visitingYahoo Labs.graphs exist that provide this functionality to acertain degree (e.g., when hovering over Google?ssuggested entities, see Figure 1), to the best ofour knowledge there is no previously published re-search on methods for entity relationship explana-tion.Figure 1: Part of Google?s search result page forthe query ?barack obama?.
When hovering overthe related entity ?Michelle Obama?, an explana-tion of the relationship between her and ?BarackObama?
is shown.In this paper we propose a method for explain-ing the relationship between two entities, whichwe evaluate on a newly constructed annotateddataset that we make publicly available.
In par-ticular, we consider the task of explaining rela-tionships between pairs of Wikipedia entities.
Weaim to infer a human-readable description for anentity pair given a relationship between the twoentities.
Since Wikipedia does not explicitly de-fine relationships between entities we use a knowl-edge graph to obtain these relations.
We cast ourtask as a sentence ranking problem: we automat-ically extract sentences from a corpus and rank564them according to how well they describe a givenrelationship between a pair of entities.
For rank-ing purposes, we extract a rich set of features anduse learning to rank to effectively combine them.Our feature set includes both traditional informa-tion retrieval and natural language processing fea-tures that we augment with entity-dependent fea-tures.
These features leverage information fromthe structure of the knowledge graph.
On top ofthis, we use features that capture the presence ina sentence of the relationship of interest.
For ourevaluation we focus on ?people?
entities and weuse a large, manually annotated dataset of sen-tences.The research questions we address are the fol-lowing.
First, we ask what the effectiveness ofstate-of-the-art sentence retrieval models is forexplaining a relationship between two entities(RQ1).
Second, we consider whether we can im-prove over sentence retrieval models by casting thetask in a learning to rank framework (RQ2).
Third,we examine whether we can further improve per-formance by using relationship-dependent modelsinstead of a relationship-independent one (RQ3).We complement these research questions with anerror and feature analysis.Our main contributions are a robust and effec-tive method for explaining entity relationships, de-tailed insights into the performance of our methodand features, and a manually annotated dataset.2 Related WorkWe combine ideas from sentence retrieval, learn-ing to rank, and question answering to address thetask of explaining relationships between entities.Previous work that is closest to the task we ad-dress in this paper is that of Blanco and Zaragoza(2010) and Fang et al (2011).
First, Blanco andZaragoza (2010) focus on finding and ranking sen-tences that explain the relationship between an en-tity and a query.
Our work is different in that wewant to explain the relationship between two enti-ties, rather than a query.
Fang et al (2011) explorethe generation of a ranked list of knowledge baserelationships for an entity pair.
Instead, we try toselect sentences that describe a particular relation-ship, assuming that this is given.Our approach builds on sentence retrieval,where one retrieves sentences rather than docu-ments that answer an information need.
Docu-ment retrieval models such as tf-idf, BM25, andlanguage modeling (Baeza-Yates et al, 1999) havebeen extended to tackle sentence retrieval.
Threeof the most successful sentence retrieval methodsare TFISF (Allan et al, 2003), which is a vari-ant of the vector space model with tf-idf weight-ing, language modeling with local context (Mur-dock, 2006; Fern?andez et al, 2011), and a recur-sive version of TFISF that accounts for local con-text (Doko et al, 2013).
TFISF is very competi-tive compared to document retrieval models tunedspecifically for sentence retrieval (e.g., BM25 andlanguage modeling (Losada, 2008)) and we there-fore include it as a baseline.Sentences that are suitable for explaining rela-tionships can have attributes that are important forranking but cannot be captured by term-based re-trieval models.
One way to combine a wide rangeof ranking features is learning to rank (LTR).
Re-cent years have witnessed a rapid increase in thework on learning to rank, and it has proven to be avery successful method for combining large num-bers of ranking features, for web search, but alsoother information retrieval applications (Burges etal., 2011; Surdeanu et al, 2011; Agarwal et al,2012).
We use learning to rank and represent eachsentence with a set of features that aim to capturedifferent dimensions of the sentence.Question answering (QA) is the task of provid-ing direct and concise answers to questions formedin natural language (Hirschman and Gaizauskas,2001).
QA can be regarded as a similar task toours, assuming that the combination of entity pairand relationship form the ?question?
and that the?answer?
is the sentence describing the relation-ship of interest.
Even though we do not follow theQA paradigm in this paper, some of the featureswe use are inspired by QA systems.
In addition,we employ learning to rank to combine the devisedfeatures, which has recently been successfully ap-plied for QA (Surdeanu et al, 2011; Agarwal etal., 2012).3 Problem StatementWe address the problem of explaining relation-ships between pairs of entities in a knowledgegraph.
We operationalize the problem as a prob-lem of ranking sentences from documents in acorpus that is related to the knowledge graph.More specifically, given two entities eiand ejthatform an entity pair ?ei, ej?, and a relation r be-tween them, the task is to extract a set of can-565didate sentences Sij= {sij1, .
.
.
, sijk} that referto ?ei, ej?
and to impose a ranking on the sen-tences in Sij.
The relation r has the general form?type(ei), terms(r), type(ej)?, where type(e) isthe type of the entity e (e.g., Person or Actor)and terms(r) are the terms of the relation (e.g.,CoCastsWith or IsSpouseOf).We are left with two specific tasks: (1) extract-ing candidate sentences Sij, and (2) ranking Sij,where the goal is to have sentences that providea perfect explanation of the relationship at the topposition of the ranking.
The next section describesour methods for both tasks.4 Explaining Entity RelationshipsWe follow a two-step approach for automaticallyexplaining relationships between entity pairs.First, in Section 4.1, we extract and enrich sen-tences that refer to an entity pair ?ei, ej?
from acorpus in order to construct a set of candidate sen-tences.
Second, in Section 4.2, we extract a richset of features describing the entities?
relationshipr and use supervised machine learning in order torank the sentences in Sijaccording to how wellthey describe the relationship r.4.1 Extracting candidate sentencesTo create a set of candidate sentences for a givenentity pair and relationship, we require a corpus ofdocuments that is pertinent to the entities at hand.Although any kind of document collection can beused, we focus on Wikipedia in this paper, as itprovides good coverage for the majority of entitiesin our knowledge graph.First, we extract surface forms for the given en-tities: the title of the entity?s Wikipedia article(e.g., ?Barack Obama?
), the titles of all redirectpages linking to that article (e.g., ?Obama?
), andall anchor text associated with hyperlinks to the ar-ticle within Wikipedia (e.g., ?president obama?
).We then split all Wikipedia articles into sentencesand consider a sentence as a candidate if (i) thesentence is part of either entities?
Wikipedia arti-cle and contains a surface form of, or a link to,the other entity; or (ii) the sentence contains sur-face forms of, or links to, both entities in the entitypair.Next, we apply two sentence enrichment stepsfor (i) making sentences self-contained and read-able outside the context of the source documentand (ii) linking the sentences to entities.
For (i),we replace pronouns in candidate sentences withthe title of the entity.
We apply a simple heuristicfor the people entities, inspired by (Wu and Weld,2010):1we count the frequency of the terms ?he?and ?she?
in the article for determining the genderof the entity, and we replace the first appearanceof ?he?
or ?she?
in each sentence with the entity?stitle.
We skip this step if any surface form of theentity occurs in the sentence.For (ii), we apply entity linking to provide linksfrom the sentence to additional entities (Milne andWitten, 2008).
This need arises from the factthat not every sentence in an article contains ex-plicit links to the entities it mentions, as Wikipediaguidelines only allow one link to another article inthe article?s text.2The algorithm takes a sentenceas input and iterates over n-grams that are not yetlinked to an entity.
If an n-gram matches a surfaceform of an entity, we establish a link between then-gram and the entity.
We restrict our search spaceto entities that are linked from within the sourcearticle of the sentence and from within articles towhich the source article links.
This way, our entitylinking method achieves high precision as almostno disambiguation is necessary.As an example, consider the sentence ?Hegave critically acclaimed performances in thecrime thriller Seven.
.
.
?
on the Wikipedia pagefor Brad Pitt.
After applying our enrichmentsteps, we obtain ?Brad Pitt gave criticallyacclaimed performances in the crime thrillerSeven.
.
.
?, making the sentence human read-able and link to the entities Brad Pitt andSeven (1995 film).4.2 Ranking sentencesAfter extracting candidate sentences, we rankthem by how well they describe the relationshipof interest r between entities eiand ej.
Thereare many signals beyond simple term statistics thatcan indicate relevance.
Automatically construct-ing a ranking model using supervised machinelearning techniques is therefore an obvious choice.For ranking we use learning to rank (LTR) and rep-resent each sentence with a rich set of features.
Ta-ble 1 lists the features we use.
Below we provide1We experimented with the Stanford co-reference reso-lution system (Lee et al, 2011) and Apache OpenNLP andfound that they were not able to consistently achieve the levelof effectiveness that we require.2http://en.Wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking566# Name GlossText features1 Sentence length Length of s in words2 Sum of idf Sum of IDF of terms of s in Wikipedia3 Average idf Average IDF of terms of s in Wikipedia4 Sentence density Lexical density of s, see Equation 1 (Lee et al, 2001)5?8 POS fractions Fraction of verbs, nouns, adjectives, others in s (Mintz et al, 2009)Entity features9 #entities Total number of entities in s10 Link to eiWhether s contains a link to the entity ei11 Link to ejWhether s contains a link to the entity ej12 Links to eiand ejWhether s contains links to both entities eiand ej13 Entity first Is eior ejthe first entity in the sentence?14 Spread of ei, ejDistance between the last match of eiand ejin s (Blanco and Zaragoza, 2010)15?22 POS fractions left/right Fraction of verbs, nouns, adjectives, others to the left/right window of eiand ejins (Mintz et al, 2009)23?25 #entities left/right/between Number of entities to the left/right or between entities eiand ejin s26 common links ei, ejWhether s contains any common link of eiand ej27 #common links The number of common links of eiand ejin s28 Score common links ei, ejSum of the scores of the common links of eiand ejin s29?30 #common links prev/next The number of common links of eiand ejin previous/next sentence of sRelationship features31 Match terms(r)?
Whether s contains any term in terms(r)32 Match wordnet(r)?
Whether s contains any phrase in wordnet(r)33 Match word2vec(r)?
Whether s contains any phrase in word2vec(r)34?36 or?s Boolean OR of feature 31 and one or both of features 32 and 3337?38 or(31, 32, 33) prev/next Boolean OR of features 31, 32, 33 for the previous/next sentence of s39 Average word2vec(r) Average cosine similarity of phrases in word2vec(r) that are matched in s40 Maximum word2vec(r) Maximum cosine similarity of phrases in word2vec(r) that are matched in s41 Sum word2vec(r) Sum of cosine similarity of phrases in word2vec(r) that are matched in s42 Score LC Lucene score of s with titles(ei, ej), terms(r), wordnet(r), word2vec(r) asquery43 Score R-TFISF R-TFISF score of s with queries constructed as aboveSource features44 Sentence position Position of s in document from which it originates45 From eior ej?
Does s originate from the Wikipedia article of eior ej?46 #(eior ej) Number of occurrences of eior ejin document from which s originates, inspiredby document smoothing for sentence retrieval (Murdock and Croft, 2005)Table 1: Features used for sentence ranking.a brief description of the more complex ones.Text features This feature type regards the im-portance of the sentence s at the term level.
Wecompute the density of s (feature 4) as:density(s) =1K ?
(K + 1)n?j=1idf(tj) ?
idf(tj+1)distance(tj, tj+1)2, (1)where K is the number of keyword terms ins and distance(tj, tj+1) is the number of non-keyword terms between keyword terms tjandtj+1.
We treat stop words and numbers in s as non-keywords and the remaining terms as keywords.Features 5?8 capture the distribution of part-of-speech tags in the sentence.Entity features These features partly buildon (Tsagkias et al, 2011; Meij et al, 2012) and de-scribe the entities and are dependent on the knowl-edge graph.
Whether eior ejis the first appearingentity in a sentence might be an indicator of impor-tance (feature 13).
The spread of eiand ejin thesentence (feature 14) might be an indicator of theircentrality in the sentence (Blanco and Zaragoza,2010).
Features 15?22 capture the distribution ofpart-of-speech tags in the sentence in a window offour words around eior ejin s (Mintz et al, 2009),complemented by the number of entities between,to the left of, and to the right of the entity pair(features 23?25).We assume that two articles that have manycommon articles that point to them are stronglyrelated (Witten and Milne, 2008).
We hypothesizethat, if a sentence contains common inlinks fromeiand ej, the sentence might contain important in-formation about their relationship.
Hence, we addwhether the sentence contains a common link (fea-567ture 26) and the number of common links (feature27) as features.
We score a common link l betweeneiand ejusing:score(l, ei, ej) = sim(l, ei) ?
sim(l, ej), (2)where sim(?, ?)
is defined as the similarity betweentwo Wikipedia articles, computed using a vari-ant of Normalized Google Distance (Witten andMilne, 2008).
Feature 28 then measures the sumof the scores of the common links.Previous research shows that using surroundingsentences is beneficial for sentence retrieval (Dokoet al, 2013).
We therefore consider the number ofcommon links in the previous and next sentence(features 29?30).Relationship features Feature 31 indicateswhether any of the relationship-specific terms oc-curs in the sentence.
Only matching the termsin the relationship may have low coverage sinceterms such as ?spouse?
may have many synonymsand/or highly related terms, e.g., ?husband?
or?married?.
Therefore, we use WordNet to findsynonym phrases of r (feature 32); we refer to thismethod as wordnet(r).Alternatively, we use word embeddings to findsuch similar phrases (Mikolov et al, 2013).
Suchembeddings take a text corpus as input and learnvector representations of words and phrases con-sisting of real numbers.
Given the set Vrconsist-ing of the vector representations of all the relation-ship terms and the set V which consists of the vec-tor representations of all the candidate phrases inthe data, we calculate the distance between a can-didate phrase represented by a vector vi?
V andthe vectors in Vras:distance(vi, V ) = cos?
?vi, ?vj?Vrvj?
?, (3)where ?vj?Vrvjis the element-wise sum of thevectors in Vrand the distance between two vec-tors v1and v2is measured using cosine similarity.The candidate phrases in V are then ranked usingEquation 3 and the top-m phrases are selected, re-sulting in features 33, 39, 40, and 41; we refer tothe ranked set of phrases that are selected usingthis procedure as word2vec(r).In addition, we employ state-of-the-art retrievalfunctions and include the scores for queries thatare constructed using the entities eiand ej, the re-lation r, wordnet(r), and word2vec(r).
We usethe titles of the entity articles titles(e) to repre-sent the entities in the query and two ranking func-tions, Recursive TFISF (R-TFISF) and LC,3(fea-tures 42?43).
TFISF is a sentence retrieval modelthat determines the level of relevance of a sentences given a query q as:R(s, q) =?t?qlog(tft,q+ 1)?log(tft,s+ 1) ?
log(n + 10.5 + sft) , (4)where tft,qand tft,sare the number of occur-rences of term t in the query q and the sentences respectively, sftis the number of sentences inwhich t appears, and n is the number of sentencesin the collection.
R-TFISF is an improved ex-tension of the TFISF method (Doko et al, 2013),which incorporates context from neighboring sen-tences in the ranking function:Rc(s, q) = (1 ?
?
)R(s, q)+ (5)?
[Rc(sprev(s), q) +Rc(snext(s), q)],where ?
is a free parameter and sprev(s) andsnext(s) indicate functions to retrieve the previousand next sentence, respectively.
We use a maxi-mum of three recursive calls.Source features Here, we refer to features thatare dependent on the source document of the sen-tences.
We have three such features.5 Experimental setupIn this section we describe the dataset, manual an-notations, learning to rank algorithm, and evalu-ation metrics that we use to answer our researchquestions.5.1 DatasetWe draw entities and their relationships from aproprietary knowledge graph that is created fromWikipedia, Freebase, IMDB, and other sources,and that is used by the Yahoo web search engine.We focus on ?people?
entities and relationshipsbetween them.4For our experiments we need toselect a manageable set of entities, which we ob-tain as follows.
We consider a year of query logs3In preliminary experiments R-TFISF and LC were thebest performing among a pool of sentence retrieval methods.4Note that, except for the co-reference resolution step de-scribed in Section 4.1, our method does not depend on thisrestriction.568from a large commercial search engine, count thenumber of times a user clicks on a Wikipedia ar-ticle of an entity in the results page and performstratified sampling of entities according to this dis-tribution.
As we are bounded by limited resourcesfor our manual assessments, we sample 1 476 en-tity pairs that together with nine unique relation-ship types form our experimental dataset.We use an English Wikipedia dump dated July8, 2013, containing approximately 4M articles, ofwhich 50 638 belong to ?people?
entities that arealso in our knowledge graph.
We extract sentencesusing the approach described in Section 4.1, re-sulting in 36 823 candidate sentences for our enti-ties.
On average we have 24.94 sentences per en-tity pair (maximum 423 and minimum 0).
Becauseof the large variance, it is not feasible to obtain ex-haustive annotations for all sentences.
We rank thesentences using R-TFISF and keep the top-10 sen-tences per entity pair for annotation.
This resultsin a total of 5 689 sentences.Five human annotators provided relevance judg-ments, manually judging sentences based on howwell they describe the relationship for an entitypair, for which we use a five-level graded rele-vance scale (perfect, excellent, good, fair, bad).5Of all relevance grades 8.1% is perfect, 15.69%excellent, 19.98% good, 8.05% fair, and 48.15%bad.
Out of 1 476 entity pairs, 1 093 have at leastone sentence annotated as fair.
As is common ininformation retrieval evaluation, we discard entitypairs that have only ?bad?
sentences.
We examinethe difficulty of the task for human annotators bymeasuring inter-annotator agreement on a subsetof 105 sentences that are judged by 3 annotators.Fleiss?
kappa is k = 0.449, which is considered tobe moderate agreement.5.2 Machine learningFor ranking sentences we use a Random Forest(RF) classifier (Breiman, 2001).6We set the num-ber of iterations to 300 and the sampling rate to0.3.
Experiments with varying these two parame-ters did not show any significant differences.
Wealso tried several feature normalization methods,none of them being able to significantly outper-5https://github.com/nickvosk/acl2015-dataset-learning-to-explain-entity-relationships6In preliminary experiments, we contrasted RF with gra-dient boosted regression trees and LambdaMART and foundthat RF consistently outperformed other methods.Baseline NDCG@1 NDCG@10 ERR@1 ERR@10B1 0.7508 0.8961 0.3577 0.4531B2 0.7511 0.8958 0.3584 0.4530B3 0.7595 0.8997 0.3696 0.4600B4 0.7767 0.9070 0.3774 0.4672B5 0.7801 0.9093 0.3787 0.4682Table 2: Results for five baseline variants.
See textfor their description and significant differences.form the runs without feature normalization.We obtain POS tags using the Stanford part-of-speech tagger and filter out a standard list of 33English stopwords.
For the word embeddings weuse word2vec and train our model on all text inWikipedia using negative sampling and the con-tinuous bag of words architecture.
We set the sizeof the phrase vectors to 500 and m = 30.5.3 Evaluation metricsWe employ two main evaluation metrics in ourexperiments, NDCG (J?arvelin and Kek?al?ainen,2002) and ERR (Chapelle et al, 2009).
The for-mer measures the total accumulated gain fromthe top of the ranking that is discounted at lowerranks and is normalized by the ideal cumulativegain.
The latter models user behavior and mea-sures the expected reciprocal rank at which a userwill stop her search.
We consider these ranking-based graded evaluation metrics at two cut-offpoints: position 1, corresponding to showing a sin-gle sentence to a user, and 10, which accounts forusers who might look at more results.
We reporton NDCG@1, NDCG@10, ERR@1, ERR@10,and Exc@1, which indicates whether we have an?excellent?
or ?perfect?
sentence at the top of theranking.
Likewise, Per@1 indicates whether wehave a ?perfect?
sentence at the top of the ranking(not all entity pairs have an excellent or a perfectsentence).We perform 5-fold cross validation and test forstatistical significance using a paired two-tailed t-test.
We depict a significant difference in perfor-mance for p < 0.01 with ?
(gain) and ?
(loss) andfor p < 0.05 with ?
(gain) and ?
(loss).
Boldfaceindicates the best score for a metric.6 Results and AnalysisWe compare the performance of typical docu-ment retrieval models and state-of-the-art sentenceretrieval models in order to answer RQ1.
Weconsider five sentence retrieval models: Luceneranking (LC), language modeling with Dirichlet569Has one # pairs # sentences Method NDCG@1 NDCG@10 ERR@1 ERR@10 Exc@1 Per@1fair 1 093 4 435 B5 0.7801 0.9093 0.3787 0.4682 ?
?LTR 0.8489?0.9375?0.4242?0.4980??
?good 1 038 4 285 B5 0.7742 0.9078 0.3958 0.4894 ?
?LTR 0.8486?0.9374?0.4438?0.5208??
?excellent 752 3 387 B5 0.7455 0.8999 0.4858 0.5981 0.7314 ?LTR 0.8372?0.9340?0.5500?0.6391?0.8298?
?perfect 339 1 687 B5 0.7082 0.8805 0.6639 0.7878 0.7729 0.6136LTR 0.8150?0.9245?0.7640?0.8518?0.8909?0.7227?Table 3: Results for the best baseline (B5) and the learning to rank method (LTR).smoothing (LM), BM25, TFISF, and RecursiveTF-ISF (R-TFISF).
We follow related work andset ?
= 0.1 for R-TFISF, k = 1 and b = 0 for BM25and ?
= 250 for LM (Fern?andez et al, 2011).In our experiments, a query q is constructed us-ing various combinations of surface forms of thetwo entities eiand ejand the relationship r. Eachentity in the entity pair can be represented by itstitle, the titles of any redirect pages pointing tothe entity?s article, the n-grams used as anchors inWikipedia to link to the article of the entity, or theunion of them all.
The relationship r can be repre-sented by the terms in the relationship, synonymsin wordnet(r), or by phrases in word2vec(r).First, we fix the way we represent r. Base-line B1 does not include any representation of rin the query.
B2 includes the relationship termsof r, while B3 includes the relationship terms of rand the synonyms in wordnet(r).
B4 includes theterms of r and the phrases in word2vec(r), and B5includes the relationship terms of r, the synonymsin wordnet(r) and the phrases in word2vec(r).Combining these variations with the entity repre-sentations, we find that all combinations that usethe titles as representation and R-TFISF as theretrieval function outperform all other combina-tions.7This can be explained by the fact that titlesare least ambiguous, thus reducing the possibilityof accidentally referring to other entities.
BM25and LC perform almost as well as R-TFISF, withonly insignificant differences in performance.Table 2 shows the best performing combinationof each baseline, i.e., varying the representationof r and using titles and R-TFISF.
B4 and B5are the best performing baselines, suggesting thatword2vec(r) and wordnet(r) are beneficial.
B5significantly outperforms all baselines except B4.We also experiment with a supervised combina-7We omit a full table of results due to space constraints.tion of the baseline rankers using LTR.
Here, weconsider each baseline ranker as a separate featureand train a ranking model.
The trained model isnot able to outperform the best individual baseline,however.6.1 Learning to rank sentencesNext, we provide the results of our method us-ing the features described in Section 4.2, exploringwhether our learning to rank (LTR) approach im-proves over sentence retrieval models (RQ2).
Wecompare an LTR model using Table 1?s featuresagainst the best baseline (B5).
Table 3 shows theresults.
Each group in the table contains the resultsfor the entity pairs that have at least one candidatesentence of that relevance grade for B5 and LTR.We find that LTR significantly outperforms B5by a large margin.
The absolute performance dif-ference between LTR and B5 becomes larger forall metrics as we move from ?fair?
to ?perfect,?which shows that LTR is more robust than thebaseline for entity pairs that have at least one highquality candidate sentence.
LTR ranks the bestpossible sentence at the top of the ranking for?83% of the cases for entity pairs that contain an?excellent?
sentence and for ?72% of the cases forentity pairs that contain a ?perfect?
sentence.Note that, as indicated in Section 5.1, we dis-card entity pairs that have only ?bad?
sentencesin our experiments.
For the sake of complete-ness, we report on the results for all entity pairs inour dataset?including those without any relevantsentences?in Table 4.6.2 Relationship-dependent modelsRelevant sentences may have different propertiesfor different relationship types.
For example, asentence describing two entities being partnerswould have a different form than one describingthat two entities costar in a movie.
A similar570Has one # pairs # sentences Method NDCG@1 NDCG@10 ERR@1 ERR@10 Exc@1 Per@1- 1 476 5 689 B5 0.5776 0.6733 0.2804 0.3467 ?
?LTR 0.6285?0.6940?0.3155?0.3694??
?Table 4: Results for the best baseline (B5) and the learning to rank method (LTR), using all entity pairsin the dataset, including those without any relevant sentences.Relationship # pairs # sentences NDCG@1 NDCG@10 ERR@1 ERR@10?MovieActor ,CoCastsWith,MovieActor?
410 1 403 0.8604 0.9436 0.3809 0.4546?TvActor ,CoCastsWith,TvActor?
210 626 0.8729 0.9482 0.3271 0.3845?MovieActor , IsDirectedBy ,MovieDirector?
?MovieDirector ,Directs,MovieActor?
112 492 0.8795 0.9396 0.4709 0.5261?Person, isChildOf ,Person?
?Person, isParentOf ,Person?
108 716 0.8428 0.9081 0.6395 0.7136?Person, isPartnerOf ,Person?
?Person, isSpouseOf ,Person?
155 877 0.8623 0.9441 0.6153 0.6939?Athlete,PlaysSameSportTeamAs,Athlete?
98 321 0.8787 0.9535 0.3350 0.3996Average results over all data 1 093 4 435 0.8661 0.9395 0.4615 0.5287LTR (Table 3; fair) 0.8489 0.9375 0.4242 0.4980Table 5: Results for relationship-dependent models.
Similar relationships are grouped together.idea was investigated in the context of QA for as-sociating question and answer types (Yao et al,2013).
To answer (RQ3) we examine whetherlearning a relationship-dependent model improvesover learning a single model for all types.
We splitour dataset per relationship type and train a modelper type using 5-fold cross-validation within each.Table 5 shows the results.8Our method is ro-bust across different relationships in terms ofNDCG.
However, we observe some variation inERR as this metric is more sensitive to the distri-bution of relevant items than NDCG?the distri-bution over relevance grades varies per relation-ship type.
For example, it is much more likely tofind candidate sentences that have a high relevancegrade for ?Person , isSpouseOf , Person?
than for?Athlete , PlaysSameSportTeamAs , Athlete?
inour dataset.
We plan to address this issue by ex-ploring other corpora in the future.The second-to-last row in Table 5 shows the av-eraged results over the different relationship types,which is a significant improvement over LTR atp < 0.01 for all metrics.
This method ranks thebest possible sentence at the top of the ranking for?85% of the cases for entity pairs that contain an?excellent?
sentence (?2% absolute improvementover LTR) and for ?75% of the cases for entitypairs that contain a ?perfect?
sentence (?3% abso-lute improvement over LTR).8We omit Exc@1 and Per@1 due to space constraints.6.3 Feature type analysisNext, we analyze the impact of the feature types.Table 6 shows how performance varies when re-moving one feature type at a time from the fullfeature set.
Relationship type features are the mostimportant, although entity type features are impor-tant as well.
This indicates that introducing fea-tures based on entities identified in the sentencesand the relationship is beneficial for this task.
Fur-thermore, the limited dependency on the sourcefeature type indicates that our method might beable to generalize in other domains.
Finally, texttype features do contribute to retrieval effective-ness, although not significantly.
Note that calcu-lating the sentence features is straightforward, asnone of our features requires heavy linguistic anal-ysis.Features NDCG@1 NDCG@10 ERR@1 ERR@10All 0.8661 0.9395 0.4615 0.5287All?text 0.8620 0.9372 0.4606 0.5274All?source 0.8598 0.9372 0.4582 0.5261All?entity 0.8421?
0.9282?
0.4497 0.5202?All?relation 0.8183?
0.9201?
0.4352?
0.5112?Table 6: Results using relationship-dependentmodels, removing individual feature types.6.4 Error analysisWhen looking at errors made by the system, wefind that some are due to the fact that entity pairsmight have more than one relationship (e.g., ac-571tors that costar in movies also being partners) butthe selected sentence covers only one of the re-lationships.9For example, Liza Minnelli isthe daughter of Judy Garland, but they havealso costarred in a movie, which is the relationshipof interest.
The model ranks the sentence ?LizaMinnelli is the daughter of singer and actress JudyGarland.
.
.
?
at the top, while the most relevantsentence is: ?Judy Garland performed at the Lon-don Palladium with her then 18-year-old daughterLiza Minnelli in November 1964.?Sentences that contain the relationship in whichwe are interested, but for which this cannot bedirectly inferred, are another source of error.Consider, for example, the following sentence,which explains director Christopher Nolandirected actor Christian Bale: ?Jackmanstarred in the 2006 film The Prestige, directed byChristopher Nolan and costarring Christian Bale,Michael Caine, and Scarlett Johansson?.
Eventhough the sentence contains the relationship of in-terest, it focuses on actor Hugh Jackman.
Thesentence ?In 2004, after completing filming forThe Machinist, Bale won the coveted role of Bat-man and his alter ego Bruce Wayne in ChristopherNolan?s Batman Begins.
.
.
?, in contrast, refers tothe two entities and the relationship of interest di-rectly, resulting in a higher relevance grade.
Ourmethod, however, ranks the first sentence on top,as it contains more phrases that refer to the rela-tionship.7 Conclusions and Future WorkWe have presented a method for explaining rela-tionships between knowledge graph entities withhuman-readable descriptions.
We first extract andenrich sentences that refer to an entity pair andthen rank the sentences according to how wellthey describe the relationship.
For ranking, weuse learning to rank with a diverse set of fea-tures.
Evaluation on a manually annotated datasetof ?people?
entities shows that our method sig-nificantly outperforms state-of-the-art sentence re-trieval models for this task.
Experimental resultsalso show that using relationship-dependent mod-els is beneficial.In future work we aim to evaluate how ourmethod performs on entities and relationships of9The annotators marked sentences that do not refer to therelationship of interest as ?bad?
but indicated whether theydescribe another relationship or not.
We plan to account forsuch cases in future work.any type and popularity, including tail entities andmiscellaneous relationships.
We also want to in-vestigate moving beyond Wikipedia and extractcandidate sentences from documents that are notrelated to the knowledge graph, such as web pagesor news articles.
Employing such documents alsoimplies an investigation into more advanced co-reference resolution methods.Our analysis showed that sentences may coverdifferent relationships between entities or differ-ent aspects of a single relationship?we aim to ac-count for such cases in follow-up work.
Further-more, sentences may contain unnecessary infor-mation for explaining the relation of interest be-tween two entities.
Especially when we want toshow the obtained results to end users, we mayneed to apply further processing of the sentencesto improve their quality and readability.
We wouldlike to explore sentence compression techniquesto address this.
Finally, relationships between en-tities have an inherit temporal nature and we aimto explore ways to explain entity relationships andtheir changes over time.AcknowledgmentsThis research was partially supported by the Eu-ropean Community?s Seventh Framework Pro-gramme (FP7/2007-2013) under grant agreementnr 312827 (VOX-Pol), the Netherlands Organi-sation for Scientific Research (NWO) under pro-ject nrs 727.011.005, 612.001.116, HOR-11-10,640.006.013, 612.066.930, CI-14-25, SH-322-15,Amsterdam Data Science, the Dutch national pro-gram COMMIT, the ESF Research Network Pro-gram ELIAS, the Elite Network Shifts projectfunded by the Royal Dutch Academy of Sciences(KNAW), the Netherlands eScience Center underproject nr 027.012.105, the Yahoo!
Faculty Re-search and Engagement Program, the MicrosoftResearch PhD program, and the HPC Fund.ReferencesArvind Agarwal, Hema Raghavan, Karthik Subbian,Prem Melville, Richard D. Lawrence, David C.Gondek, and James Fan.
2012.
Learning to rank forrobust question answering.
In Proceedings of the21st ACM international conference on Informationand knowledge management, pages 833?842.
ACM.James Allan, Courtney Wade, and Alvaro Bolivar.2003.
Retrieval and novelty detection at the sen-tence level.
In Proceedings of the 26th annual inter-national ACM SIGIR conference on Research and572development in informaion retrieval, pages 314?321.
ACM.Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al1999.
Modern information retrieval, volume 463.ACM press New York.Roi Blanco and Hugo Zaragoza.
2010.
Finding sup-port sentences for entities.
In Proceedings of the33rd international ACM SIGIR conference on Re-search and development in information retrieval,pages 339?346.
ACM.Roi Blanco, Berkant Barla Cambazoglu, Peter Mika,and Nicolas Torzec.
2013.
Entity recommendationsin web search.
In The Semantic Web?ISWC 2013,pages 33?48.
Springer.Leo Breiman.
2001.
Random forests.
Mach.
Learn.,45(1):5?32.Christopher J.C. Burges, Krysta Marie Svore, Paul N.Bennett, Andrzej Pastusiak, and Qiang Wu.
2011.Learning to rank using an ensemble of lambda-gradient models.
In Yahoo!
Learning to Rank Chal-lenge, pages 25?35.Olivier Chapelle, Donald Metzler, Ya Zhang, andPierre Grinspan.
2009.
Expected reciprocal rank forgraded relevance.
In Proceedings of the 18th ACMconference on Information and knowledge manage-ment, pages 621?630.
ACM.Mark Craven, Dan DiPasquo, Dayne Freitag, AndrewMcCallum, Tom Mitchell, Kamal Nigam, and Se?anSlattery.
2000.
Learning to construct knowledgebases from the world wide web.
Artificial Intelli-gence, 118(1?2):69?113.Alen Doko, Maja?Stula, and Darko Stipani?cev.
2013.A recursive TF-ISF based sentence retrieval methodwith local context.
International Journal of Ma-chine Learning and Computing, 3(2):195?200.Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, WilkoHorn, Ni Lao, Kevin Murphy, Thomas Strohmann,Shaohua Sun, and Wei Zhang.
2014.
Knowl-edge vault: A web-scale approach to probabilisticknowledge fusion.
In Proceedings of the 20th ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, KDD ?14, pages 601?610.
ACM.James Fan, Raphael Hoffman, Aditya Kalyanpur, Se-bastian Riedel, Fabian Suchanek, and Pratim ParthaTalukdar, 2012.
Proceedings of the Joint Workshopon Automatic Knowledge Base Construction andWeb-scale Knowledge Extraction (AKBC-WEKEX),chapter Proceedings of the Joint Workshop on Auto-matic Knowledge Base Construction and Web-scaleKnowledge Extraction (AKBC-WEKEX).
Associa-tion for Computational Linguistics.Lujun Fang, Anish Das Sarma, Cong Yu, and PhilipBohannon.
2011.
Rex: explaining relationships be-tween entity pairs.
Proceedings of the VLDB En-dowment, 5(3):241?252.Ronald T Fern?andez, David E. Losada, and Leif Az-zopardi.
2011.
Extending the language model-ing framework for sentence retrieval to include localcontext.
Information Retrieval, 14(4):355?389.Lynette Hirschman and Robert Gaizauskas.
2001.
Nat-ural language question answering: the view fromhere.
Natural Language Engineering, 7(04):275?300.Kalervo J?arvelin and Jaana Kek?al?ainen.
2002.
Cu-mulated gain-based evaluation of IR techniques.ACM Transactions on Information Systems (TOIS),20(4):422?446.Gary Geunbae Lee, Jungyun Seo, Seungwoo Lee, Han-min Jung, Bong-Hyun Cho, Changki Lee, Byung-Kwan Kwak, Jeongwon Cha, Dongseok Kim,JooHui An, et al 2001.
SiteQ: Engineering highperformance QA system using lexico-semantic pat-tern matching and shallow NLP.
In TREC.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2011.
Stanford?s multi-pass sieve corefer-ence resolution system at the CoNLL-2011 sharedtask.
In Proceedings of the Fifteenth Conference onComputational Natural Language Learning: SharedTask, pages 28?34.
Association for ComputationalLinguistics.David E. Losada.
2008.
A study of statistical queryexpansion strategies for sentence retrieval.
In Pro-ceedings of the SIGIR 2008 Workshop on FocusedRetrieval, pages 37?44.Edgar Meij, Wouter Weerkamp, and Maarten de Rijke.2012.
Adding semantics to microblog posts.
InProceedings of the fifth ACM international confer-ence on Web search and data mining, pages 563?572.
ACM.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.David Milne and Ian H. Witten.
2008.
Learning to linkwith Wikipedia.
In Proceedings of the 17th ACMconference on Information and knowledge manage-ment, pages 509?518.
ACM.Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 2-Volume 2, pages 1003?1011.
Association forComputational Linguistics.Vanessa Murdock and W. Bruce Croft.
2005.
A trans-lation model for sentence retrieval.
In Proceedingsof the conference on Human Language Technology573and Empirical Methods in Natural Language Pro-cessing, pages 684?691.
Association for Computa-tional Linguistics.Vanessa Graham Murdock.
2006.
Aspects of SentenceRetrieval.
Ph.D. thesis, University of MassachusettsAmherst.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2011.
Learning to rank answers to non-factoid questions from web collections.
Computa-tional Linguistics, 37(2):351?383.Manos Tsagkias, Maarten de Rijke, and WouterWeerkamp.
2011.
Linking online news and socialmedia.
In WSDM 2011: Fourth ACM InternationalConference on Web Search and Data Mining.
ACM,February.Jason Weston, Antoine Bordes, Oksana Yakhnenko,and Nicolas Usunier.
2013.
Connecting languageand knowledge bases with embedding models for re-lation extraction.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP 2013.Ian Witten and David Milne.
2008.
An effective, low-cost measure of semantic relatedness obtained fromwikipedia links.
In Proceeding of AAAI Workshopon Wikipedia and Artificial Intelligence: an Evolv-ing Synergy, AAAI Press, Chicago, USA, pages 25?30.Fei Wu and Daniel S Weld.
2010.
Open informationextraction using wikipedia.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 118?127.
Association forComputational Linguistics.Xuchen Yao, Benjamin Van Durme, and Peter Clark.2013.
Automatic coupling of answer extraction andinformation retrieval.
In Proceedings of the 51st An-nual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), pages 159?165.
Association for Computational Linguistics.574
