An Efficient Text Summarizer Using Lexical ChainsH.Gregory  SHber  and  Kath leen  F.  McCoyComputer  and in fo rmat ion  SciencesUnivers i ty of DelawareNewark,  DE  19711{ silber, mccoy} @cis.
udel.
edu?
A b s t r a c t  .
.
.
.
.
.
.
.We present a system which uses lexical chains as anintermediate r presentation for automatic text sum-marization.
This system builds on previous researchby implementing a lexical chain extraction algorithmin linear time.
The system is reasonably domain in-dependent and takes as input any text or HTMLdocument.
The system outputs a short summarybased on the most salient concepts from the origi-nal document.
The length of the extracted summarycan be either controlled automatically, or manuallybased on length or percentage ofcompression.
Whilestill under development, the system provides usefulsummaries which compare well in information con-tent to human generated summaries.
Additionally,the system provides a robust est bed for future sum-mary generation research.1 In t roduct ionAutomatic text summarization has long been viewedas a two-step process.
First, an intermediate r pre-sentation of the summary must be created.
Second,a natural language representation f the summarymust be generated using the intermediate r presen-tation(Sparek Jones, 1993).
Much of the early re-search in automatic text summarization has involvedgeneration of the intermediate representation.
Thenatural language generation problem has only re-cently received substantial attention in the contextof summarization.1.1 Mot ivat ionIn order to consider methods for generating naturaltext summaries from large documents, everal issuesmust be examined in detail.
First, an analysis of thequality of the intermediate r presentation for use ingeneration must be examined.
Second, a detailedexamination of the processes which link the inter-mediate representation to a potential final summarymust be undertaken.The system presented here provides a useful firststep towards these ends.
By developing a robust andefficient tool to generate these intermediate repre-sentations, we can both evaluate the representation......... andcormider the difficult problem of generatiiig nat-ural language texts from the representation.1.2 Background ResearchMuch research as been conducted in the area of au-tomatic text summarization.
Specifically, researchusing lexical chains and related techniques has re-ceived much attention.Early methods using word frequency counts didnot consider the relations between similar words.Finding the aboutness of a document requires find-ing these relations.
How these relations occur withina document is referred to as cohesion (Hallidayand Hasan, 1976).
First introduced by Morris andHirst (1991), lexical chains represent lexical cohe-sion among related terms within a corpus.
Theserelations can be recognized by identifying arbitrarysize sets of words which are semantically related (i.e.,have a sense flow).
These lexical chains provide aninteresting method for summarization because theirrecognition is easy within the source text and vastknowledge sources are not required in order to con>pure them.Later work using lexical chains was conducted byHirst and St-Onge (1997) using lexical chains to cor-rect malapropisms.
They used WordNet, a lexicaldatabase which contains ome semantic information(http://www.cs.princeton.edu/wn).Also using WordNet in their implenmntation.Barzilay and Elhadad (1997) dealt with some of tilelimitations in Hirst and St-Onge's algorithm by ex-amining every possible lexical chain which could becomputed, not just those possible at a given pointin the text.
That is to say, while Hirst and St.Ongewould compute the chain in which a word shouldbe placed when a word was first encountered, Barzi-lay and Elhadad computed ever:,' possible chain aword could become a member of when the word wasencountered, and later determined the best interpre-tation.2682 A Linear T ime A lgor i thm for Intra Intra Adjacent OtherComput ing  Lexical  Chains Pgrph.
Segment Segment2.1 Overv iew Same 1 1 1 1Our research on lexical chains as an intermediate Synonym 1 1 0 "O "representation forautomatic text summarization fol- Hypernym I 1 0 0lows the research of Barzilay and Elhadad (1997).
Hyponym 1 1 0 0We use their results as a basis for the utility of Sibling 1 0 0 0the methodology.
The most substantial difference isthat Barzi lay and Elhadad create all possible chainsexplicit ly and then choose the best possible chain,whereas we compute them implicitly.Table 1: Dynamic Scoring Metrics Set to MimicB+E's  Algorithmthe word itself.
These scores are dynamic and can .
.
.
.
.
.
.
2~2As mentioned above, WordNet is a lexical databasethat contains substantial semantic information.
Inorder to  facilitate fficient access, the WordNet noundatabase was re-indexed by line number as opposedto file position and the file was saved in a binary in-dexed format.
The database access tools were thenrewritten to take advantage of this new structure.The result  of this work is that  accesses to the Word-Net noun database can be accomplished an orderof magnitude faster than with the original imple-mentation.
No additional changes to the WordNetdatabases were made.
The re-indexing also provideda zero-based continuous numbering scheme that isimportant  o our linear time algorithm.
This impor-tance will be noted below.Modifications ~to.
Word.Net .
.
.
.
.
.
.
.
.
.
.
.
.
.
be set ~ased ,on:segmentation information, dista.nce,2.3 Our  A lgor i thmStep 1 For each word instance that is a nounFor every sense of that wordCompute all scored "meta-chains"Step 2 For each word instanceFigure out which "meta-chain"it contributes most toKeep the word instance in that chainand remove it from all otherChains updating the scoresof each "meta-chain"Figure 1: Basic linear time Algorithm for Comput-ing Lexical ChainsOur basic lexical chain algorithm is describedbriefly in Figure 1.
The algorithm takes a part ofspeech tagged corpus and extracts the nouns.
Us-ing WordNet to collect sense information for each ofthese noun instances, the algorithm then computesscored "nmta-chains" based on the collected infor-mation.
A "meta-chain" is a representation f everypossible lexical chain that can be computed start-ing with a word of a given sense.
These meta-chainsare scored in the following manner.
As each word in-stance is added, its contribution, which is dependenton the scoring metrics used, is added to the "meta-chain" score.
The contribution is then stored withinand type of relation.Currently, segmentation is accomplished prior tousing our algorithm by executing Hearst's text tiler(Hearst, 1994).
The sentence numbers of each seg-ment boundary are stored for use by our algorithm.These sentence numbers are used in conjunctionwith relation type as keys into a table of potentialscores.
Table 1 denotes ample metrics tuned to sim-ulate the system devised by Barzilay and Elhadad(1997).At this point, the collection of "meta-chains" con-talns all possible interpretations of the source doc-ument.
The problem is that  in our final represen-tation, each word instance can exist in only onechain.
To figure out which chain is the correct one,each word is examined.using the score contributionstored in Step 1 to determine which chain the givenword instance contributes to most.
By deleting theword instance from all the other chains, a represen-tation where each word instance exists in preciselyone chain remains.
Consequently, the sum of thescores of all the chains is maximal.
This method isanalogous to finding a maximal spanning tree in agraph of noun senses.
These noun senses are all ofthe senses of each noun instance in the document.From this representation, the highest scoredchains correspond to the important concepts in theoriginal document.
These important concepts canbe used to generate a summary from the sourcetext.
Barzilay and Elhadad use the notion of strongchains (i.e., chains whose scores are in excess of twostandard eviations above the mean of all scores) todetermine which chains to include in a summary.Our system can use this method, as well as sev-eral other methods including percentage compres-sion and number of sentences.For a more detailed description of our algorithmplease consult our previous work (Silber and McCoy,2000).2.4 Runt ime Ana lys i sIn this analysis, we will not consider the computa-tional complexity of part of speech tagging, as that isnot the focus of this research.
Also, because the size269Worst AverageCase CaseC1 =No.
of senses 30 2C2 =Parent/chi ld isa relations ,45147 t4Ca =No.
of nouns in WordNet 94474 94474C4 =No.
of synsets in WordNet 66025 66025C5 =No.
of siblings 397 39C6 =Chains word can belong to 45474 55Table 2: Constants from WordNetand structure of WordNet does not change from ex-ecution to execution of.aJae.algorit, hm, we shall takethese aspects of WordNet to be constant.
We willexamine each phase of our algorithm to show thatthe extraction of these lexical chains can indeed bedone in linear time.
For this analysis, we define con-stants from WordNet 1.6 as denoted in Table 2.Extracting information from WordNet entailslooking up each noun and extracting all synset, Hy-ponym/Hypernym, and sibling information.
Theruntime of these lookups over the entire documentis:n * (log(Ca) + Cl * C2 + Cl * C5)When building the graph of all possible chains, wesimply insert the word into all chains where a rela-tion exists, which is clearly bounded by a constant(C6).
The only consideration is the computationof the chain score.
Since we store paragraph num-bers represented within the chain as well as segmentboundaries, we can quickly determine whether therelations are intra-paragraph, intra-segment, or ad-jacent segment.
We then look up the appropriatescore contribution from the table of metrics.
There-fore, computing the score contribution of a givenword is constant.
The runtime of building the graphof all possible chains is:n*C6 .5Finding the best chain is equally efficient.
Foreach word, each chain to which it belongs is exam-ined.
Then, the word is marked as deleted fromall but the single chain whose score the word con-tributes to most.
In the case of a tie, the lower sensenmnber from WordNet is used, since this denotes amore general concept.
The runtime for this step is:n*C6 .4This analysis gives an overall worst case runtimeof:n * 1548216 + log(94474 ) + 227370and an average case runtime of:n ?
326 + log(94474) + 275While the constants are quite large, the algorithmis clearly O(n) in the number of nouns in the originaldocument.A t  "first glance, "the'constants ~involved seem pro-hibitively large.
Upon further analysis, however, wesee that most synsets have very few parent child re-lations.
Thus the worst case values maynot  reflectthe actual performance of our application.
In ad-dition, the synsets with many parent child relationstend to represent extremely general concepts.
Thesesynsets will most likely not appear very often as adirect synset for words appearing in a document.
"2,;5 User  ~InterfaceOur system currently can be used as a commandline utility.
The arguments allow the user to specifyscoring metrics, summary length, and whether ornot to search for collocations.
Additionally, a webCGI interface has been added as a front end whichallows a user to specify not just text documents, buthtml documents as well, and summarize them fromthe Internet.
Finally, our system has been attachedto a search engine.
The search engine uses data fromexisting search engines on the Internet o downloadand summarize ach page from the results.
Thesesummaries are then compiled and returned to theuser on a single page.
The final result is that asearch results page is returned with automaticallygenerated summaries.2.6 Compar i son  w i th  P rev ious  WorkAs mentioned above, this research is based on thework of Barzilay and Elhadad (1997) on lexicalchains.
Several differences exist between our methodand theirs.
First and foremost, the linear run-timeof our algorithm allows documents to be summarizedmuch faster.
Our algorithm can summarize a 40,000word document in eleven seconds on a Sun SPARCUltra10 Creator.
By comparison, our first versionof the algorithm which computed lexical chains bybuilding every possible interpretation like Barzilayand Elhadad took sLx minutes to extract chains from5,000 word documents.The linear nature of our algorithm also has sev-eral other advantages.
Since our algorithm is alsolinear in space requirements, we can consider all pos-sible chains.
Barzilay and Elhadad had to prune in-terpretations (enid thus chains) which did not seempromising.
Our algorithm does not require pruningof chains.Our algorithm also allows us to analyze the iin-portance of segmentation.
Barzilay and Elhadadused segmentation to reduce the complexity of theproblem of extracting chains.
They basically builtchains within a segment and combined these chainslater when chains across segment boundaries hareda word in the same sense in common.
While we in-clude segmentation i formation in our algorithm, it270is merely because it might prove useful in disam-biguating chains.
The fact that we can use it or notallows our algorithm to test the importance of seg-mentation to proper-word ~ense disambiguation.
Itis important o note that on short documents, likethose analyzed by Barzilay and Elhadad, segmen-tation appears to have little effect.
There is somelinguistic justification for this fact.
Segmentationis generally computed using word frequencies, andour lexical chains algorithm generally captures thesame type of information.
On longer documents,our research as shown segmentation to have a muchgreater effect.3 Cur rent  Research  and  FutureD i rec t ionsSome issues which are not currently addressed bythis research are proper name disambiguation andanaphora resolution.
Further, while we attempt olocate two-word collocations using WordNet, a morerobust collocation extraction technique iswarranted.One of the goals of this research is to eventuallycreate a system which generates natural languagesummaries.
Currently, the system uses sentence se-lection as its method of generation.
It is our con-tention that regardless of how well an algorithm forextracting sentences may be, it cannot possibly cre-ate quality summaries.
It seems obvious that sen-tence selection will not create fluent, coherent text.Further, our research shows that completeness is aproblem.
Because information extraction is only atthe sentence boundary, information which may bevery important may be left out if a highly com-pressed summary is required.Our current research is examining methods of us-ing all of the important sentences determined by ourlexical chains algorithm as a basis for a generationsystem.
Our intent is to use the lexical chains algo-rithm to determine what to summarize, and then amore classical generation system to present he in-formation as coherent text.
The goal is to combineand condense all significant information pertainingto a given concept which can then be used in gener-ation.4 Conc lus ions\Ve have described a domain independent summa-rization engine which allows for efficient summariza-tion of large documents.
The algorithm described isclearly O(n) in the number of nouns in the originaldocument.In their research, Barzilay and Elhadad showedthat lexieal chains could be an effective tool forautomatic text summarization (Barzilay and EI-hadad, 1997).
By developing a linear time al-gorithm to compute these chains, we have pro-dueed a front end to a summarization system whichcan be implemented efficiently.
An operationalsample of this demo is available on the web athttp://www.eecis.udel.edu/- silber/research.htm......
While.
,usable currenlfly, the-system provides aplatform for generation research on automatic textsummarization by providing an intermediate r pre-sentation which has been shown to capture impor-tant concepts from the source text (Barzilay andElhadad, 1997).
The algorithm's peed and effec-tiveness allows research into summarization f largerdocuments.
Moreover, its domain independence al-lows for research into the inherent differences be-tween domains.5 AcknowledgementsThe authors wish to thank the Korean Government,Ministry of Science and Technology, whose funding,as part of the Bilingual Internet Search MachineProject, has made this research possible.
Addition-ally, special thanks to Michael Elhadad and ReginaBarzilay for their advice, and for generously makingtheir data and results available.Re ferencesRegina Barzilay and Michael Elhadad.
1997.
Us-ing lexical chains for text summarization.
Pro-ceedings of the Intelligent Scalable Text Summa-rization Workshop, A CL Madrid.Michael Halliday and Ruqaiya Hasan.
1976.
Cohe-sion in English.
Longman, London.Marti A. Hearst.
1994.
Multi-paragraph segmenta-tion of expository text.
Proceedings o\] the 32ndAnnual Meeting of the ACL.Gramme Hirst and David St-Onge.
1997.
Lexicalchains as representation f context for the detec-tion and correction of malapropisms.
Wordnet:An electronic lexical database and some of its ap-plications.J.
Morris and G. Hirst.
1991.
Lexical cohesion com-puted by thesanral relations an an indecator ofthe structure of text.
Computational Linguistics,18:21--45.H.
Gregory Silber and Kathleen F. McCoy.
2000.Efficient text summarization using lexical chains.Conference on bztelligent User b~terfaces 2000.271
