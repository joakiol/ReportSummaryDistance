Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 619?623,Dublin, Ireland, August 23-24, 2014.TCDSCSS: Dimensionality Reduction to Evaluate Texts of VaryingLengths - an IR ApproachArun JayapalDept of Computer ScienceTrinity College Dublinjayapala@cs.tcd.ieMartin EmmsDept of Computer ScienceTrinity College Dublinmartin.emms@cs.tcd.ieJohn D.KelleherSchool of ComputingDublin Institute of Technologyjohn.d.kelleher@dit.ieAbstractThis paper provides system description ofthe cross-level semantic similarity task forthe SEMEVAL-2014 workshop.
Cross-level semantic similarity measures the de-gree of relatedness between texts of vary-ing lengths such as Paragraph to Sen-tence and Sentence to Phrase.
Latent Se-mantic Analysis was used to evaluate thecross-level semantic relatedness betweenthe texts to achieve above baseline scores,tested on the training and test datasets.
Wealso tried using a bag-of-vectors approachto evaluate the semantic relatedness.
Thisbag-of-vectors approach however did notproduced encouraging results.1 IntroductionSemantic relatedness between texts have beendealt with in multiple situations earlier.
But it isnot usual to measure the semantic relatedness oftexts of varying lengths such as Paragraph to Sen-tence (P2S) and Sentence to Phrase (S2P).
Thistask will be useful in natural language process-ing applications such as paraphrasing and summa-rization.
The working principle of information re-trieval system is the motivation for this task, wherethe queries are not of equal lengths compared tothe documents in the index.
We attempted twoways to measure the semantic similarity for P2Sand S2P in a scale of 0 to 4, 4 meaning both textsare similar and 0 being dissimilar.
The first oneis Latent Semantic Analysis (LSA) and second, abag-of-vecors (BV) approach.
An example of tar-get similarity ratings for comparison type S2P isprovided in table 1.This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence de-tails: http://creativecommons.org/licenses/by/4.0/Sentence: Schumacher was undoubtedly one ofthe very greatest racing drivers there has everbeen, a man who was routinely, on every lap, ableto dance on a limit accessible to almost no-oneelse.Score Phrase4 the unparalleled greatnessof Schumachers driving abilities3 driving abilities2 formula one racing1 north-south highway0 orthodontic insuranceTable 1: An Example - Sentence to Phrase simi-larity ratings for each scale2 DataThe task organizers provided training data, whichincluded 500 pairs of P2S, S2P, Phrase to Word(P2W) and their similarity scores.
The trainingdata for P2S and S2P included text from differentgenres such as Newswire, Travel, Metaphoric andReviews.
In the training data for P2S, newswiretext constituted 36% of the data, while reviewsconstituted 10% of the data and rest of the threegenres shared 54% of the data.Considering the different genres provided in thetraining data, a chunk of data provided for NISTTAC?s Knowledge Base Population was used forbuilding a term-by-document matrix on whichto base the LSA method.
The data includednewswire text and web-text, where the web-textincluded data mostly from blogs.
We used 2343documents from the NIST dataset1, which wereavailable in eXtended Markup Language format.Further to the NIST dataset, all the paragraphsin the training data2of paragraph to sentence wereadded to the dataset.
To add these paragraphs tothe dataset, we converted each paragraph into a1Distributed by LDC (Linguistic Data Consortium)2provided by the SEMEVAL task-3 organizers619new document and the documents were added tothe corpus.
The unique number of words identi-fied in the corpus were approximately 40000.3 System descriptionWe tried two different approaches for evaluatingthe P2S and S2P.
Latent Semantic Analysis (LSA)using SVD worked better than the Bag-of-Vectors(BV) approach.
The description of both the ap-proaches are discussed in this section.3.1 Latent Semantic AnalysisLSA has been used for information retrieval al-lowing retrieval via vectors over latent, arguablyconceptual, dimensions, rather than over surfaceword dimensions (Deerwester et al., 1990).
It wasthought this would be of advantage for comparisonof texts of varying length.3.1.1 RepresentationThe data corpus was converted into a mxn term-by-document matrix, A, where the counts (cm,n)of all terms (wm) in the corpus are representedin rows and the respective documents (dn) incolumns:A =?????d1d2?
?
?
dnw1c1,1c1,2?
?
?
c1,nw2c2,1c2,2?
?
?
c2,n...............wmcm,1cm,2?
?
?
cm,n????
?The document indexing rules such as text tok-enization, case standardization, stop words re-moval, token stemming, and special characters andpunctuations removal were followed to get the ma-trix A.Singular Value Decomposition (SVD) decom-poses the matrix into U , ?
and V matrices (ie.,A = U?VT) such that U and V are orthonormalmatrices and ?
is a diagonal matrix with singularvalues.
Retaining just the first k columns of U andV , gives an approximation of AA ?
Ak= Uk?kVTk(1)According to LSA, the columns of Ukare thoughtof as representing latent, semantic dimensions,and an arbitrary m-dimensional vector#?v can beprojected onto this semantic space by taking thedot-product with each column of Uk; we will callthe result#      ?vsem.In the experiments reported later, the m-dimensional vector#?v is sometimes a vector ofword counts, and sometimes a thresholded or?boolean?
version, mapping all non-zero numbersto 1.3.1.2 Similarity CalculationTo evaluate the similarity of a paragraph, p, and asentence, s, first these are represented as vectors ofword counts,#?p and#?s , then these are projected inthe latent semantic space, to give#      ?psemand#      ?ssem,and then between these the cosine similarity met-ric is calculated:cos(#      ?psem.#      ?ssem) =#      ?psem.#      ?ssem|#      ?psem|.|#      ?ssem|(2)The cosine similarity metric provides a similarityvalue in the range of 0 to 1, so to match the targetrange of 0 to 4, the cosine values were multipliedby 4.
Exactly the same procedure is used for thesentence to phrase comparison.Further, the number of retained dimensions ofUkwas varied, giving different dimensionalitiesof the LSA space.
The results of testing at the re-duced dimensions are discussed in 4.13.2 Bag-of-VectorsAnother method we experimented on could betermed a ?bag-of-vectors?
(BV) approach: eachword in an item to be compared is replaced by avector representing its co-occurrence behavior andthe obtained bags of vectors enter into the compar-ison process.3.2.1 RepresentationFor the BV approach, the same data sources as wasused for the LSA approach is turned into a m?mterm-by-term co-occurrence matrix C:C =???????w1w2?
?
?
wmw1c1,1c1,2?
?
?
c1,mw2c2,1c2,2?
?
?
c2,m...............wmcm,1cm,2?
?
?
cm,m??????
?The same preprocessing steps as for the LSA ap-proach applied (text tokenization, case standard-ization, stop words removal, special characters andpunctuations removal).
Via C, if one has a bag-of-words representing a paragraph, sentence orphrase, one can replace it by a bag-of-vectors, re-placing each word wiby the corresponding row ofC ?
we will call these rows word-vectors.6203.2.2 Similarity CalculationFor calculating P2S similarity, the procedure isas follows.
The paragraph and sentence are tok-enized, and stop-words were removed and are rep-resented as two vectors#?p and#?s .For each word pifrom#?p , its word vector fromC is found, and this is compared to the word vectorfor each word siin#?s , via the cosine measure.
Thehighest similarity score for each word piin#?p isstored in a vector# ?Spshown in (3).
The overallsemantic similarity score between paragraph andsentence is then the mean value of the vector# ?Sp?4 ?
see (4).Sp=[Sp1Sp2?
?
?
Spi](3)Ssim=?ni=1Spin?
4 (4)Exactly corresponding steps are carried out for theS2P similarity.
Although experiments were car-ried out this particular BV approach, the resultswere not encouraging.
Details of the experimentscarried out are explained in 4.2.4 ExperimentsDifferent experiments were carried out using LSAand BV systems described in sections 3.1 and 3.2on the dataset described in section 2.
Pearsoncorrelation and Spearman?s rank correlation werethe metrics used to evaluate the performance ofthe systems.
Pearson correlation provides the de-gree of similarity between the system?s score foreach pair and the gold standard?s score for the saidpair while Spearman?s rank correlation providesthe degree of similarity between the rankings ofthe pairs according to similarity.4.1 LSAThe LSA model was used to evaluate the semanticsimilarity between P2S and S2P.4.1.1 Paragraph to SentenceAn initial word-document matrix A was built byextracting tokens just based on spaces, stop wordsremoved and tokens sorted in alphabetical order.As described in 3.1.1, via the SVD of A, a ma-trix Ukis obtained which can be used to project anm dimensional vector into a k dimensional one.In one setting the paragraph and sentence vec-tors which are projected into the LSA space haveunique word counts for their dimensions.
In an-other setting before projection, these vectors areDimensions 100% 90% 50% 30% 10%Basic word-doc representation 0.499 - 0.494 0.484 0.426Evaluation-boolean counts 0.548 - 0.533 0.511 0.420Constrained tokenization 0.368 0.564 0.540 0.516 0.480Added data 0.461 0.602 0.568 0.517 0.522Table 2: Pearson scores at different dimensions -Paragraph to Sentencethresholded into ?boolean?
versions, with 1 for ev-ery non-zero count.The Pearson scores for these settings are in thefirst and second rows of table 2.
They show thevariation with the number of dimensions of theLSA representation (that is the number of columnsof U that are kept)3.
An observation is that theusage of boolean values instead of word countsshowed improved results.Further experiments were conducted, retainingthe boolean treatment of the vectors to be pro-jected.
In a new setting, further improvementswere made to the pre-processing step, creating anew word-document matrix A using constrainedtokenization rules, removing unnecessary spacesand tabs, and tokens stemmed4.
The performanceof the similarity calculation is shown as the thirdrow of Table 2: there is a trend of increase in cor-relation scores with respect to the increase in di-mensionality up to a maximum of 0.564, reachedat 90% dimension.0 20 40 60 80 1000.350.40.450.50.550.60.650.7Percent Dimensions maintainedSemantic similarityBasic word?doc representationEvaluation with Boolean valuesConstrained TokenizationAdded data representationFigure 1: Paragraph to Sentence - Pearson corre-lation scores for four different experiments at dif-ferent dimensions3(represented in percent) of UkNot convinced with the pearson scores, more3Here, the dimension X% means k = (X/100) ?
N ,whereN is the total number of columns in A in the unreducedSVD.4Stemmed using Porter Stemmer module availabe fromhttp://tartarus.org/?martin/PorterStemmer/621documents were added to the dataset to build anew word-document matrix representation A. Thedocuments included all the paragraphs from thetraining set.
Each paragraph provided in the train-ing set was added to the dataset as a separate docu-ment.
The experiment was performed maintainingthe settings from the previous experiment and theresults are shown in the fourth row of table 2.
Theincrease in trend of correlation scores with respectto the increase in dimensionality is followed by thenew U produced from A after applying SVD.
Fig-ure 2 provides the distribution of similarity scoresevaluated at 90% dimension of the model with re-spect to the gold standard.Further to compare the performance of differentexperiments, all the experiment results are plottedin Figure 1.
It can be observed that every subse-quent model built has shown improvements in per-formance.
The first two experiments shown in thefirst two rows of table 2 are shown in red and bluelines in the figure.
It can be observed that in boththe settings, the pearson correlation scores wereincreasing as the the number of dimensions main-tained also increased, whereas in the other two set-tings, the pearson correlation scores reached theirmaximum at 90% and came down at 100% di-mension, which is unexpected and so is not jus-tified.
It is observed from Figure 2 that the scores0 100 200 300 400 50000.511.522.533.54Training data ExamplesSimilarity scoresFigure 2: Semantic similarity scores - Gold stan-dard (Line plot) vs System scores (Scatter plot) forexamples in training dataof the system in scatter plot are not always clus-tered around the gold standard scores, plotted as aline.
As the gold standard score goes up, the sys-tem prediction accuracy has come down.
One rea-son for this pattern can be attributed to the train-ing set which had data mostly data from NewswireDimensions 100% 90% 70% 50% 30% 10%Basic word-docrepresentation 0.493 - - 0.435 0.423 0.366Evaluationboolean counts 0.472 - - 0.449 0.430 0.363Constrainedtokenization 0.498 0.494 0.517 0.485 0.470 0.434Addeddata 0.493 0.504 0.498 0.498 0.488 0.460Table 3: Pearson scores at different dimensions3-Sentence to Phraseand webtext.
Therefore, during evaluation all thewords from paragraph and/or sentence would nothave got a position while getting projected on thelatent semantic space, which we believe has pulleddown the accuracy.4.1.2 Sentence to PhraseThe experiments carried out for P2S provided in4.1.1 were conducted for S2P examples as well.The pearson scores produced by different experi-ments at different dimensions are provided in ta-ble 3.
This table shows that the latest word-document representation made with added docu-ments, did not have any impact on the correlationscores, while the earlier word-document represen-tation provided in 3rdrow, which used the originaldataset preprocessed with constrained tokeniza-tion rules, removing unnecessary spaces and tabs,and tokens stemmed, provided better correlationscore at 70% dimension.
Further the comparisonof different experiments carried out at differentsettings are plotted in Figure 3.0 20 40 60 80 1000.350.40.450.50.55Percent Dimensions maintainedSemantic similarityBasic word?doc representationEvaluation with Boolean valuesConstrained TokenizationAdded data representationFigure 3: Sentence to Phrase - Pearson correlationscores for four different experiments at differentdimensions3(represented in percentage) of Uk6224.2 Bag of VectorsBV was tested in two different settings.
Thefirst representation was created with bi-gram co-occurance count as mentioned in section 3.2.1 andexperiments were carried out as mentioned in sec-tion 3.2.2.
This produced negative Pearson corre-lation scores for P2S and S2P.
Then we created an-other representation by getting co-occurance countin a window of 6 words in a sentence, on evalua-tion produced correlation scores of 0.094 for P2Sand 0.145 for S2P.
As BV showed strong negativeresults, we did not continue using the method forevaluating the test data.
But we strongly believethat the BV approach can produce better results ifwe could compare the sentence to the paragraphrather than the paragraph to the sentence as men-tioned in section 3.2.2.
During similarity calcula-tion, when comparing sentence to the paragraph,for each word in the sentence, we look for the bestsemantic match from the paragraph, which wouldincrease the mean value by reducing the number ofdivisions representing the number of words in thesentence.
In the current setting, it is believed thatwhile computing the similarity for the paragraphto sentence, the words in the paragraph (longertext) will consider a few words in the sentence tobe similar multiple times.
This could not be rightwhen we compare the texts of varying lengths.5 Conclusion and DiscussionOn manual verification, it was identified that thedataset used to build the representation did nothave documents related to the genres Metaphoric,CQA and Travel.
The original dataset mostly haddocuments from Newswire text and blogs whichincluded reviews as well.
Further, it can be identi-fied from tables 2 and 3, the word-document rep-resentation with added documents from the train-ing set improved Pearson scores.
This allowed toassume that the dataset did not have completelyrelevant set of documents to evaluate the trainingset which included data from different genres.
Forevaluation of the model on test data, we submittedtwo runs and best of them reported Pearson scoreof 0.607 and 0.552 on P2S and S2P respectively.In the future work, we should be able to experi-ment with more relevant data to build the modelusing LSI and also use statistically strong unsu-pervised classifier pLSI (Hofmann T, 2001) for thesame task.
Further to this, as discussed in 4.2 wewould be able to experiment with the BV approachby comparing the sentence to the paragraph, whichwe believe will yield promising results to comparethe texts of varying lengths.ReferencesScott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer and Richard Harshman1990.
Indexing by latent semantic analysis Jour-nal of the American society for information science,41(6):391?401Thomas Hofmann 2001.
Unsupervised Learningby Probabilistic Latent Semantic Analysis JournalMachine Learning, Volume 42 Issue 1-2, January-February 2001 Pages 177 - 196623
