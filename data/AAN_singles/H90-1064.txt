The LincolnContinuousTied-Mixture HMMSpeech Recognizer*Douglas B. PaulLincoln Laboratory, MITLexington, Ma.
02173AbstractThe Lincoln robust HMM recognizer has been con-verted from a single Ganssian or Gaussian mixture pdfper state to tied mixtures in which a single set of Gaus-sians is shared between all states.
There were someinitial difficulties caused by the use of mixture pruning\[12\] but these were cured by using observation pruning.Fixed weight smoothing of the mixture weights allowedthe use of word-boundary-context-dependent triphonemodels for both speaker-dependent (SD) and speaker-independent (SI) recognition.
A second-differential ob-servation stream further improved SI performance butnot SD performance.
The overall recognition perfor-mance for both SI and SD training is equivalent othe best reported according to the October 89 ResourceManagement test set.
A new form of phonetic contextmodel, the semiphone, is also introduced.
This newmodel significantly reduces the number of states requiredto model a vocabulary.IntroductionTied mixture (TM) HMM systems \[3, 6\] use a Gaus-sian mixture pdf per state in which a single set of Gaus-sians is shared among all states:Pi (0) ~- E Ci,j Gj (o) (1)1ci,j > O, E cij = lJwhere i is the arc or state, G i is the jth Gaussian, and ois the observation vector.
This form of continuous obser-vation pdf shares the generality of discrete observationpdfs (histograms) with the absence of quantization errorfound in continuous density pdfs.
Unlike the non-TMcontinuous pdfs, TM pdfs are easily smoothed with otherpdfs by combining the mixture weights.
Unlike discreteobservation HMM systems, the Gaussians (analogous tothe vector quantizer codebook of a discrete observationsystem) can be optimized simultaneously with the mix-ture weights.
The training algorithms are identical to*This work was sponsored by the Defense Advanced ResearchProjects Agency.the algorithms for training a Gaussian mixture systemexcept he Gaussians are tied across all arcs.Mixture and Observation PruningComputing the full sum of equation 1is expensive dur-ing training and prohibitively expensive during recogni-tion since it must be computed for each active state ateach time step.
(Because the word sequence is unknown,recognition has many more active states than does train-ing.)
Ideally, one would only compute the terms whichdominate the sum.
However, it requires more computa-tion to find these terms than it does to simply sum them.Two faster approximate methods for reducing the com-putation exist: mixture and observation pruning.Mixture pruning simply drops terms that fall belowa threshold during training.
The weights may then bestored as a sparse array which also saves space.
Thecomputational savings are limited during the early it-erations of training since only a few terms have beendropped.
The final SD distributions are quite sharp (i.e.have only a few terms), but the final SI distributions arequite broad (i.e.
have many terms).
Thus the savingsare limited for SI systems.
When the distributions aresmoothed with less specific models, they become quitebroad again.
These difficulties are just computational--there is an even greater difficulty.
During training, theparameters of the Gaussians are also optimized whichcauses them to "move" in the observation space.
Withmixture pruning, a "lost" Gaussian cannot be recovered.
(This was the fundamental difficulty with the earlier ver-sion of the system reported in Reference \[12\].
)Instead of reducing the mixture order, observationpruning reduces the computation by computing the sumsfor all Gaussians whose output probability is above athreshold times the probability of the most probableGanssian.
(Some other sites have used the "top-N"Ganssians \[3, 7\].
In our system, it gives inferior recog-nition performance compared to the threshold method.
)All of the Gaussians must now be computed, but thisis a significant proportion of the computation only intraining.
(Some pruning is possible.
Our exploration oftree-structured search methods howed them to be in-effective because the number of Gaussians is too small332and the observation order is too large.)
The amount ofcomputation is now dependent upon the separations ofthe Gaussian means relative to their covariances and thestatistics of the observations.
The computational savingswere very significant except for the SI second-differentialobservation stream (discussed later).Observation pruning does not save space for severalreasons.
The observation pruned TM systems sufferfrom the same "missing observation" problem as do thediscrete observation systems and therefore no mixtureweight can be allowed to become zero.
Similarly, re-cruitment of "new" Ganssians (due to their movement)during training also requires that no mixture weight beallowed to become zero.
Both can be accomplished byusing full size weight arrays and lower bounding all en-tries by a small value.
Smoothing now causes no orga-nizational difficulty or increase in computation since allmixture weight arrays are full order.The TM CSR Deve lopmentThe following development tests were performed usingthe entire (12 speakers x 100 sentences, 10242 words) SDdevelopment-test portion of the Resource Management-1 (RM1) database.
Three training conditions were used:speaker-dependent with 600 sentences per speaker (SD),speaker-independent with 2880 sentences from 72 speak-ers (SI-72) and speaker-independent with 3990 sentencesfrom 109 speakers (SI-109).
All tests were performedwith the perplexity 60 word-pair grammar (WPG).
Theword error rate was used to evaluate the systems:substitutions + insertions + deletionscorrect nr o f  words (2)Line 1 of Table 1 gives the best results obtainedfrom the non-TM Gaussian (SD) and Gaussian mix-ture (SI) systems \[10\].
The SD system used word-boundary-context-dependent (WBCD or cross-word) tri-phone models and the SI systems used word-boundary-context-free (WBCF) triphone models.The TM HMM systems were trained by a modificationof the unsupervised bootstrapping procedure used in thenon-TM systems:1.
Train an initial set of Gaussians using a binary-splitting EM to form a Gaussian mixture model forall of the speech data.2.
Train monophone models from a flat start (all mix-ture weights equal).3.
Initialize the triphone models with the correspond-ing monophone models.4.
Train the triphone models.All of the systems described here use centisecondmel-cepstral first observation and time-differential mel-cepstral second observation streams.
The Gaussians usea tied (grand) variance (vector) per stream.
Each obser-vation stream is assumed to be statistically independentof the other streams.
Each phone model is a three statelinear HMM.
The triphone dictionary also included worddependent phones for some common function words.
Allstages of training use the Baum-Welch reestimation algo-rithm to optimize all parameters (the transition proba-bilities, mixture weights, Gaussian means, and tied vari-ances) simultaneously.
The lower bound on the mixtureweights was chosen empirically.The initial observation pruned TM system was derivedfrom the mixture pruned systems described in \[12\] andgave the performance shown in line 2 of Table 1.
Itused WBCF triphone models because there was insuffi-cient training data to adequately train WBCD models.Fixed-weight smoothing \[15\] and deleted interpolation\[2\] of the mixture weights were tested and the fixed-weight smoothing was found to be equal to or betterthan the deleted interpolation.
(Bugs have been foundin both implementations and the smoothing algorithmswill require more investigation.)
The fixed smoothingweights were computed as a function of the state (left,center, or right), the context (triphone, left-diphone,right-diphone, or monophone) and the number of in-stances of each phone.
The TM system with smoothedWBCF triphone models howed a performance improve-ment for both the SD and SI trained systems.
An ad-ditional improvement for both SD and SI systems wasobtained by adding WBCD models (table 1, line 3).
Un-til the smoothing was added, we had been able to ob-tain only slight improvements in the SD systems and noimprovement in the SI systems by adding the WBCDmodels.Finally, a third observation stream was tested.
Thisstream is a second-differential mel-cepstrum obtained byfitting a parabola to the data within =t=30 msec.
of thecurrent frame.
It produced no improvement for the SDsystem, but improved all of the SI systems (table 1, line4).
However, there was a significant computational costto this stream.
Unlike the other observation streams, thenumber of Gaussians which pass the observation pruningthreshold is quite large which slowed the system signif-icantly due to the cost of computing the mixture sums.Increasing the number of iterations of the EM Gaussianinitialization algorithm reduced the number of activeGaussians and simultaneously improved results slightly.The computational cost of this stream is still quite largeand methods to reduce the cost without damaging per-formance are still under investigation.The best systems (starred in table 1) were also testedon the Resource Management-2 (RM2) database.
(Thisdatabase is similar to the SD portion of RM1, exceptthat it contains only four speakers.
However, there are2400 training sentences available for each speaker.
Thetwo training conditions are SD-600 (600 sentences) andSD-2400 (2400 sentences).
The development tests used120 sentences per speaker for a total of 4114 words.
TheRM2 tests (Table 2) showed the SD systems to performbetter when trained on more data.
One of the speakers(bjw) and possibly a second (lpn) obtained performancewhich, in this author's opinion, is adequate for opera-333tional use.
This is the first time we have observed thislevel of performance on an RM task.
There is still, how-ever, wide performance variation across peakers.SemiphonesThe above best systems all use WBCD triphones.
Ascan of the 20,000 word Merriam-Webster pocket dictio-nary yields the following numbers of phones:Word Word WordInternal Beginning Endingmonophones 43 41 38diphones 1268 602 645triphones 10788CrossWord155849321(All stress and syllable markings were removed and allpossible word combinations were allowed for the cross-word numbers.)
This suggests that a large vocabularysystem using WBCD triphone models will require on theorder of 60K phone models.
(Even if the triphones areclustered to reduce the final number \[8, 13\], all triphonesmust trained before the clustering process.)
These num-bers assume no function word or stress dependencies.
(Avariety of other context factors have also been found toaffect he acoustic realization of phones \[4\].)
While thisnumber is not impossible--the Lincoln SI-109 WBCDsystem has about 10K triphones and CMU used up to38K triphones in their vocabulary independent train-ing experiments \[5\]--it is rather unwieldy and wouldrequire large amounts of data to train the models effec-tively.
(60K triphones would require about 280M mix-ture weights and accumulation variables in the LincolnSI system.
)One possible method of reducing the number of modelsis the semiphone, a class of phone model which includesclassic diphones and triphones as special cases.
(A classicdiphone extends from the center of one phone to thecenter of the next phone.
In a triphone based system, adiphone is a left or right phone-context sensitive phonemodel.)
The center phone of a three section semiphonemodel of a word with the phonetic transcription /abe/would be:ar-bz-bm bt-bm-br bm-b~-czwhere 1 denotes the left part, m the middle part, andr the right part.
As shown here, each section is writ-ten as a left and right context dependent section (i.e.a "tri-section").
Thus the middle part always has thesame contexts and is therefore only monophone depen-dent.
The left (and right) sections are dependent uponthe middle part, which is always the same, and a sec-tion of the adjacent phone.
Thus the left part is similarto the second half of a classic diphone, the center partis monophone dependent, and the right part is similarto the first half of a classic diphone.
(In fact, we im-plemented the scheme using the current riphone basedsystems imply by manipulating the dictionary.)
If themiddle part is dropped, this scheme implements a clas-sic diphone system and if the left and right parts areeliminated it reverts to the standard triphone scheme.One of the advantages of this scheme is a great reduc-tion in the number of models.
For the above dictionary,the three section model has 5695 phones.
(This num-ber was derived from the above table and is thereforenot quite correct since the single phone words were nottreated properly.
However, the number is sufficiently ac-curate to support he following conclusions.)
If the semi-phone system has one state per phone and the triphonesystem has three states per phone, each word model willhave the same number of states (for a given left and rightword context), but the semiphone system will have 5695unique states to train and the triphone system will have180K unique states to train.Semiphones avoid one of the difficult aspects of cross-word triphones--the single phone word.
A single phoneword requires a full crossbar of triphones in the recog-nition network \[11\].
The semiphone approach splits thesingle phone into a sequence of two or more semiphonesand simply joins the apexes of a left fan and a right fanfor a two semiphone model or places the middle semi-phone between the fans for a three semiphone model\[11\].A final advantage of the semiphone approach over theclassic diphone approach is the organization.
The unitsare organized by the phone.
This is a more convenientorganization for smoothing and also makes the word end-points explicitly available for word endpointing or anyword based organization of the recognizer.Our current implementation f this scheme has not yetaddressed smoothing the mixture weights of the semi-phones, so the results-to-date can only compare un-smoothed semiphone systems with smoothed triphonesystems.
Line 1 of Table 3 repeats the correspondingentries for two smoothed triphone systems from Table1 for comparison with the semiphone systems.
Line 2 isan unsmoothed three-section semiphone system with onestate per semiphone.
For both training conditions, thenumber of unique states was reduced by about a factor offive.
The difference in performance between the systemsis commensurate with the difference between smoothedand unsmoothed triphone systems.
Line 3 is equivalentto a classic diphone system with two states per semi-phone and thus four states per phone rather than threestates per phone as in the preceding systems.
This sys-tem has twice as many states as the other semiphonesystem and yields equivalent performance.
While thesemiphone systems do not currently outperform the tri-phone systems, they bear further investigation.The October 89 Evaluat ion TestSetAt the time of the October 89 meeting, the mixturepruned systems were not showing improved performanceover the best non-TM systems and therefore non-TM334systems were used in the evaluation tests.
The best ob-servation pruned systems (starred in Table 1) were testedusing the October 89 test set in order to compare themto the results obtained at the other DARPA sites.
Theresults are shown in Table 4.
These results are not sta-tistically distinguishable from best results reported byany site at the October 89 meeting \[14\].The June  90 Eva luat ion  TestsThe best TM triphone systems (starred in Table 1)were used to perform the evaluation tests.
Both systemsused WBCD triphones with fixed weight smoothing.
TheSD systems used two observation streams and the SI-109system used three observation streams.
The results areshown in Table 5.Conc lus ionThe change from mixture pruning to observationpruning has eliminated the Gaussian recruitment prob-lem.
The change increased the data space requirements,but provided a better environment for mixture weightsmoothing and reduced the computational requirementsfor both training and recognition.
Including fixed-smoothing-weight mixture-weight smoothing improvedperformance on both SD and SI trained systems andallowed the use of WBCD (cross-word) triphone models.Testing on the RM2 database showed that our systemsdeveloped on the RM1 database transferred without dif-ficulty to another database of the same form.
It alsoshowed that our SD systems will provide better perfor-mance when given more training data (2400 sentences)than is available in the RM1 database (600 sentences).Operational performance l vels were obtained on one ortwo of the (four) speakers.We found a simpler context-sensitive model--thesemiphone--to produce similar recognition performanceto the (by now) traditional triphone systems.
Thesemodels, which include the classical diphone as a specialcase, significantly reduce the number of states (or ob-servation pdfs) which must be trained.
The semiphonemodel will require further development and verificationbut it may be one way of simplifying our systems.
Sincethe number of semiphones required to cover a 20,000word dictionary is significantly less than the number oftriphones required to cover the same dictionary, theymay be a more practical route to vocabulary indepen-dent phone modeling than one based upon triphones.References\[1\] S. Austin, C. Barry, Y. L. Chow, A. Deft, O. Kim-ball, F. Kubala, J. Makhoul, P. Placeway, W. Rus-sell, R. Schwartz, and G. Yu, "Improved HMMModels for High Performance Speech Recognition,"Proceedings DARPA Speech and Natural LanguageWorkshop, Morgan Kaufmann Publishers, October,1989.\[2\]\[4\]\[5\]\[4\[7\]\[9\]\[10\]\[11\]\[12\]\[13\]\[14\]\[15\]L. R. Bahl, F. Jelinek, and R. L. Mercer, "A Max-imum Likelihood Approach to Continuous SpeechRecognition," IEEE Trans.
Pattern Analysis andMachine Intelligence, PAMI-5, March 1983.J.R.
Bellagarda nd D.H. Nahamoo, "Tied MixtureContinuous Parameter Models for Large Vocabu-lary Isolated Speech Recognition," Proc.
ICASSP89, Glasgow, May 1989.F.
R. Chen, "Identification of Contextual Factor ForPronunciation Networks," Proc.
ICASSP90, Albu-querque, New Mexico, April 1990.H.
W. Hon and K. F. Lee, "On Vocabulary-Independent Speech Modeling," Proc.
ICASSP90,Albuquerque, New Mexico, April 1990.X.
D. Huang and M.A.
Jack, "Semi-continuous Hid-den Markov Models for Speech Recognition," Com-puter Speech and Language, Vol.
3, 1989.X.
Huang, K. F. Lee, and H. W. Hon, "OnSemi-Continuous Hidden Markov Modeling," Proc.ICASSP90, Albuquerque, New Mexico, April 1990.K.
F. Lee, Automatic Speech Recognition: The De-velopment of the SPHINX System, Kluwer Aca-demic Publishers, 1989.K.
F. Lee, Presentation at DARPA Speech and Nat-ural Language Workshop, October 1989.D.
B. Paul, "The Lincoln Continuous Speech Recog-nition System: Recent Developments and Results,"Proceedings DARPA Speech and Natural LanguageWorkshop, February 1989, Morgan Kaufmann Pub-lishers, February 1989.D.
B. Paul, "The Lincoln Robust ContinuousSpeech Recognizer," Proc.
ICASSP 89., Glasgow,Scotland, May 1989.D.
B. Paul, "Tied Mixtures in the Lincoln RobustCSR," Proceedings DARPA Speech and NaturalLanguage Workshop, Morgan Kaufmann Publish-ers, October, 1989.D.B.
Paul and E. A. Martin, "Speaker Stress-Resistant Continuous Speech Recognition," Proc.ICASSP 88, New York, NY, April 1988.Proceedings DARPA Speech and Natural LanguageWorkshop, October 1989, Morgan Kaufmann Pub-lishers, October, 1989.R.
Schwartz, Y. Chow, O. Kimball, S. Roucos,M.
Krasner, and J. Makhoul, "Context-DependentModeling for Acoustic-Phonetic Recognition ofContinuous Speech," Proc.
ICASSP 85, Tampa, FL,April 1985.335Table 1.
RM1 Development Test Results using triphone models.
The standard eviations are computed for the bestresult in each column.% Word Error Rates with WPGSystem1.
Non-TM2.
TM-23.
TM-24.
TM-3Ganssians Smoothingmany2x2572x2573x257Binomial standard eviationsnnYYSDWBCF WBCD5.2 ~ 3.04.33.3 1.7"- 1.8( .18)  ( .13)*--evaluation test (best) systemsSI-72WBCF WBCD12.9 -14.0 -11.3 9.09.5 7.2(.29) \[ (.26) LSI-I09WBCF WBCD10.110.4 7.88.5 5.6*(.27) : (.23)Table 2.
RM2 Development test results using the best (starred) systems of Table 1.% Error Rates with WPGSD-2400 SD-600Speaker word (sd) sentence word (sd)bjw .2 1.7 1.4jls .7 5.0 3.8jrm 2.8 16.7 4.33.3 lpnavg.41.0 (.16) 6.72.53.O (.27)System: TM2, Gaussians: 2x257, Smoothedsentence10.021.726.715.018.3Triphone modelsTable 3.
Semiphone development tests using the RM1 database.
The standard eviations are computed for the bestresult in each column.
Line 1 is the best (starred) results from Table 1.% Word Error Rates with WPGSystem Gaussians SmoothingSectionsper PhoneStates perSection1.
TM-2,triphone 2x257 y 1 32.
TM-2,semiphone 2x257 n 3 13.
TM-2,semiphone 2x257 n 2 2Binomial standard eviationsSD SI-109States Errors States Errors17979 1.7 24201 7.83793 2.2 4372 9.57286 2.3(.13) (.27)Table 4.
Results for the best (starred) systems of Table 1 using the October 89 evaluation test (RM1) data.% Word Error Rates (std dev) with WPGSystem Gaussians Smoothing SD SI-109TM-2 2x257 y 2.6 (.31)TM-3 3x257 y - 5.9 (.45)Best from any site 2.5 \[1\] 6.0 \[9\]Table 5.
The June 1990 Evaluation test results using triphone based systems on the RM2 database.
The systemsare the best (starred) systems of Table 1.% Word Error Rates (std dev)Word-pair Grammar (p=60) No Grammar (p=991)*Training sub ins idel word(sd) sent sub ins del word(sd) sentTM-2 SD-2400 .9 .2 .4 1.51 (.19) 11.0 3.3 .8 .9 4.89 (.34) 28.8TM-2 SD-600 1.7 .5 .9 3.09 (.27) 20.0 8.3 2.2 2.2 12.66 (.52) 58.3TM-3 SI-109 l 3.8 .7 \] 1.3 5.86(.37) 31.9 16.5 2.1 4.4 22.92(.66) 74.6* Homonyms equivalent336
