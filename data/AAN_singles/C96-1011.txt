Unsuperv ised  Learn ing  o f  a Ru le -based  Span ish  Par t  o fSpeech  TaggerChinatsu  Aone  a, nd Kev in  HausmanSysl, c lns  l{.cs(;eu'<:t~ al (l Al)l)li<:{d;ions (~orpora.1;ioll (SIIA)4300 Vair l ,akcs (\]ottt%l<'a, it 'fax, VA 22033}Lo I1 cc((i\]Sl'{i,.
(:o i i i ~ h ~1,1 is i / 1311 ((1~ s l-iL co l i lAbst ractThis I)at)cr d(~scrihcs a, SI);ulish l'arl.-ol:Speech ( I ' ()S) l~a.gger which al,i)lies aadextends Ih'ill's a, lguril;hn~ for 'u,.super-vised learuing of ruh',- based l,aggcrs (llrill,1995).
First, we discuss our general ;ttt-proach including extensions we rim(It 1;~)the algor ithm in order t,o hamllc mlkllOWl~ w(H'(IH a,l~d pa.ra, ll~(d,crize \[ea.l'llillg;and ta.gging ot)l;ions.
Ncx L we rCl)orl,and amdyzc our eXl)cri~mm,al rcsull;s us-iug dill'crenl, l)ara~ilel:crs.
Thcl l ,  wc (h~s('ril)e our "hyhri(l" a.t)l)roach which wasll(~C{~Hs;-tl'y ill Ol'(l(w 1,o oV(~l'(:(Hl~l(~ ;1 \['un(lamcni, al li~lit;al, ion in Ih'ill's origiual al-goril, hnL Finally, wc cOral)at(> our tag-ger wil;h Ilid(Icn Mark()v Model (I lMM)-based (,aggers.
:l IntroductionWe have develol)ed a. Spanish l'arb-(,l:-HI)('(~ch( I ) ( )S)  'l'a.g:g(~r which al)l)lies and extends Ih'ill'salg(,rilJ~u for unSUl)crvised l('a.rniug ( l lr i l l ,  l.q!
),5)to cr(~a.l;e a. set of rules (;hal, r(~(luce the aml)iguil.yof I'()S tags on words.
We have ch()scu an un-supervised Ica','ning algori/,hn~ l)(~ca.u,s(~ il, does notrequire a. larg;(' I)()S-l;agged t,raining (-orl)us.
Sincethere was n() I)()S-t.agged Spanish c(,rt)us avail-abh' 1;() us and since creating a large hand-l,;tgp;(xlcorltus is both cosl, ly aud I)r()ne l,o inconsislamcy,Gc decision was also a l)ra, ci, ical one.
Wc have de-cided 1;o develop a rule-based I\[,;xggcr l}(!causc sucha.
tagger lea.rus a sel, of declarative rules m~d alsobecause we wautt'd 1,o c(tml)are it, with Ili(Id(:nM arkov M odd (I 1M M)-/)ascd 1;aggers.We extcude<l Ih'ill's algol'itlnu in scwwal ways.l"irsl,, we cxtcnd(;d it, (,o Imn(Ih~ unknowu words inthe training and test texl,s.
Scc(m(l, w(., i)aram(~terized learniug and t;ag,ghw; ol)tions.
Finally, wccxp(;rinmnl, ed wil;h a "hyhrid" solul, io,, where wetls(~d a. v(;ry sinful\[ Iltlllll)cr o\[' hm~(I-(lis:nnhigual,(~dtexi;s during training to overcom(~ a tiu~(lan~(ml, a.ll imitat ion in tit(: learning algorithm.2 Con ipo l len l ; s()ur Spa, ish l'()N l,agg(w consisLs of l, hre(~ (:()lHl)(~nenl;s: the lnil, ial St, al,e Anuol, a l.or, the l,carncr,;Ul(l l, he l{.uh" 'l';tggcr, c;tch o\[' which is d(~scrih~'(lhelow.2.1 In i t ia l  S ta te  AnnotatorThis conq)onent is used t()assign all I>ossil)le I>()Sl.ag~ 1.o a. given Spanish word.
It consisl;s ofh~xicon h>()kul) , nl(>rphol()gical nalysis, a.nd ult.k.
(.wn word ha, ndlin~,,.
'l'he Spanish I'()S l,ag s.~l,us(~d in 1,his w()rk c(msisl,s ()f l,hc l'olh,wi.v; l;~gs:AI)J. AI)V, BI+, (form of scr ()r ~s/ar), (II,()(~1('I ' IME, (;()I,()N, (;()MMA, (;ON.l, I)ATE, I)1,'/1',HAVE (form uf ha&', 9 , I IY I ' I IEN,  LET'I'I,~I{,,I , I 'AREN, M()I)EL, MUI,TII'IAEH,, N, NUMBI{,,1 ), I)EI{,I()I), I)I{,EI~'JX, PI{,(), J)I{,()PN, QIJEH-.MAll, I(, QIJ()TE, IlL)MAN, I{.I'AI{EN, SEMI-(;()l,()N, SI,ASll, SIJIRX)N.I, S//FVIX, TI IEILE(hater used iu "l,h(~re" collsl, ructi()ns), WI I \ [ )ET(,:,.
*l/il,,',,), WIJN J' (q,t,r), Wt\[l ' l '  ( da,~d~O, ;.,d V(S(> 'l'ahl(' ;/).2 .
\ ] .1  L(;xic(m Lookup and  Morl)h() logi(-alAna lys i sIinlil{e Ib'ill's English l;aggcr cxl)(wint(mt; (lescribed in (l+rill, 1!)!
),~), ,~o large I)()H-1;ag;gc(ISpanish c(,rpus was a.vailable Lo us Dotal which ;thtrge h~xicon cau be (\]eriv(~(l. As a resull,, we (le-ci(h~d 1;o pars(" l,h('.
(m-line ( kdlins Span ish--I!
',uglishl)icl,ionary t, ~tlt(\[ d(',riv(',d a, large h',xicon from it;(al)()ul, 45,000 cnl;rics).
W(' used only the ()penclass entries from {;Iris l<~xi('on, and then aug-m(~nl,c(I il, with irr(~gular verb forms and ;t nullll)crof closcd-(-iass words.
()ur nlorl)hological analyzeruses a sel; of rewrite rules to sl, ril> off all(l/or mo( lif','/ word endings I;o lind root forms uf words.2.1 .2  ( Jnknown Word  I lan( | l ingHittce l, hc lexicon and JUUrl)hological almlysiswill not cov('r cv('ry single wor(l I, ha/, cmi apl)Caritl a, 1,exl,, ;-mal, t,::nlpl, is iwulc ;/l, l, his ,'-;l, ag:c I,{iclassify ItllkltowII WOl'dS.
Any word which didnoL gel, ;tssigned one or more p;trl, s-o\[Lslme.ch inl W(: h;~v(', obta,in(:d ~ license to the, dictionary.53the lookup/morphology phase is examined for cer-tain traits often indicative of particular parts-of-speech.
This task is similar to what was doneby the guessers for the HMM-based French andGerman taggers (Chanod and Tapanainen, 1995;Feldweg, 1995).For example, words ending in the letters"mente" are assigned the tag of ADV (adverb).Those words ending in "ando" or "endo" are as-signed the tag V-CONTINUOUS-NIL (continuousform of the verb).
Table 1 shows a list of unknownword handling rules.Table 1: Unknown Word Handling RulesIteuristics POS tagnum > 1600 & < 2100roman numeral 1-9-and,-endo-ido,-ado,-ida,-ada-er,-ir,-ar-erse,-irse,-arse-cidn,-idad,-izaje-mente-ablecapitalizedDATEROMANV-CONTINUOUS-NILV-PERFECT-NILV-NIL-NILV-NIL-NIL-CLITIC ~NADVADJPROPNPerforming these simple checks reduces thenumber of unknowns in our test set of 17,639words from 737 (4.2%) to la8 (0.9%).
The re-maining unknowns are assigned a set of ambigu-ous open-class tags of N, V, ADJ, and ADV sothat they can be disambiguated by the Learner.2.2 LearnerThe Learner takes as input ambiguously taggedtexts produced by the Initial State Annotator, andtries to learn a set of rules that will reduce theambiguity of the tags.
Output is a file of rules inthe following form:context = C: P~\] ... \]1~ I ...
I P,~ --+ t}, wherecontext is one of PREVWORI) ,NEXTWORD, PREVTA(~ or NEXTTAG, 2Cis a word or tag,P1,..., t~,.. .
,  Pn are the ambiguousparts-of-speech to be reduced,Pi is the part-of-speech t at replacesP1,..., Pi, .
.
.
,  Pn.llere are some examples taken from the actuallearned rules:* NEXTWORD = DE : PIN --~ N. PREVWOI{D = EN : DETIADV --+ DET* PREVTAG = DET : VIN ---+ N* NEXTTAG = SUBCONJ  : BEIV --+ V2PREVWORD = previous word, PREVTAG =previous tag.The Learner applies Brill's algorithm for unsu-pervised learning to try to reduce the ambiguity ofthe tags in the input corpus.
The following stepsare taken:1.
The Learner examines each ambiguouslytagged word and creates a set of contexts forthe word.
Two of these contexts will be PRE-VWORD and NEXTWORD.
The remainderconsist of PREVTAG and NEXTTAG con-texts as required by the tag(s) on the preced-ing and following words.
For example, if theword preceding the ambiguously tagged wordis ambiguously tagged with two tags, then theLearner must generate two PREVrI'AG con-texts.2.
An attempt is made to tind unambiguouslytagged words in the corpus that are taggedwith one and only one of the tags on the am-biguously tagged word.
For example, if theword in question has both N and V tags, thenthe Learner would search for words with onlyan N tag or only a V tag.3.
If such a word is found, the contexts of thatword are examined to determine if there is anoverlap between them and the contexts gen-erated for the ambiguously tagged word.
Oneissue for this determination is how nmch am-biguity should be tolerated in the context ofthe unambiguously tagged word.
For exam-ple, if one of the possible contexts is PRE-VTA( I=N and the word preceding the unambiguously tagged word has both N and Vtags, should the context apply?
To permitvarious approaches to be tried, we extendedthe Learner to accept a parameter (i.e., free-dora) that determines how nmch ambiguitywill be accepted on the context words for thecontext to match.4.
If a context matches for this unambignouslytagged word, the count of unambignouslytagged words with the particular part ofspeech occurring in that context is incre-mented.5.
After the entire corpus is examined, each ofthese possible reduction rules (of the form"Change the tag of a word from X to Y inthe context C where Y C X") is ranked ac-cording to the following.
First, for each tagZ C ;g, Z ?
Y, the Learner computes:?
incontcxt(Z, C), wherefreq(Y)= number of occurrences of wordsunambiguously tagged with Y,freq(Z)= number of occurrences of wordsunambiguously tagged with Z,inconlext(Z, C)= nmnber of times a wordunambiguously tagged with Zoccurs in context (I.54The tag Z that gives the highest score fromthis formula is saved as R. Then the score fora particular transforrnatiotr is(:) - * te, (:)6.
If the highest-ranked transfornlation is trotpositive, the Learner is done.
()therwise, thehighest-ranked transformation is appendedto the list of transformations learned.
TheLearner then searches this list for the trans-forlnation that will result in the most reduc-tion of ambiguity (whi,;h will always l)e thelatest rule h:arned) and applies it;.
This pro-tess continues until no further reduction ofambiguity is possil)h;, lh;re, we also extendedtire l,earner to +~ccept a different parameter(i.e., l-ta.qJ'reedom) that deterntines how tHlt(:hambiguity will I)e accepted on a word that isused for ('onte+xt during ambiguity rcducliou,that is, when the l+earn(>r has tbund a ruh~ andis apl)lying it to the trMning text.
Note thatsl)ecifying too small a value for this t)arame-ter can cause the \].,e&r'lr(-:,i' to  go irrto ,:tIl etrd-less loop, as restricting the valid r:ollt(:xts Itrayhave the effect of nullifyiug the just-learnedrule.7.
The Learner thee returns to step 1 to beginthe process again.2.3 R,ule Tagg('+:t'This c+otnl)otmnt reads tagged texts l)ro(h,:ed bythe lnitiM State Atmotator and rules produced bythe Learner and applies the learned rules to timta,gged texts to reduce the aml)iguity of the tags.We extended the+ l{,ule Tagger to haw~ two i)os-sible modes of operation (i.e., best-rule-first andlearued-sequcuce mo(les controlled by t, hc scq pa-rameter) for using the, learned rules to reduce am-biguity:I.
The Rule Tagger can use an algorithm similarto that used in step 7 of the l,earner.
Eachpossible reduction rule is examined againstthe text to determine whid~ ruh', results inthe greatest reduction of aml)iguity.2.
The R, ule Tagger can use a sequeutial appli-cation of the learned rules in the order tha.tthe rules were learned.
After each rule hasbeen applied in sequence, all of the rules pre-ceding it are re-applied to take adwml.age ofambiguity reductions made by the latest ruleapl)lied.The R, ule Tagger allows one to specify, as in the/,earner, how much ambiguity will be, toleratedfor a context to match.
For example, one canbe very restrictive and require that a tag context(e.g., PREVTAG=N) thatch only an unambigu-ously tagged word (in this ease, a word with onlyan N tag).
This parameter (i.e,., r-lagJi'ccdom)Sl)eeifies the maximunl ambiguity Mlowed on acontext word for a (:ontext tag to llrateh: I re-quires that the context word be unatnbiguouslyl,agg('.
(l, 2 requires that tlmrc be no more than twotags on the word, and so on.3 Exper iments  and  Resu l tsI"or training and testing of the tagger, we haverandomly l)icked articles from a large (274MB)"H Norte" Mexican newspaper corl)uS, and sel)-arated tlwm into the training and test s(+ts.
Thetest set; (17,639 words) was t, ngged matmally forcomparison agaittst he system-tagged texts.
Fortraining, wc partitioned the de, velopment set intosev(:ral dilt'erent-sized sets in order to st(: the el-feels of training corpus sizes.
The 1)reakdown canI)e Found in Table 2.
'l'al)h~ 2: Ambiguously tagged Training setsS0t; WordsTiny 1322 wordsSmall 3066 wordsM(~dittm 5591 wordsFull 12795 wordsIf one randomly picks one of the possible tags(+t+ each word in the test set, the accuracy is 78+0%(78.0% with the simple verh tag set).
The awwageI'()S amhiguity per word is 1.52 (1.49) includingt)unctuation tags arr(I 1.58 (1.56) excluding l)Unc-tuat, ion tags.
For co,nparison, the accuracy oflh'ill's unsupervised English tagger was 95.1% us-ing 120,000-word Penn Treel)ank texts.
Ills initialstate tagging accuracy was 90,7%, whictl is con-siderably higher than our Sl)a, ish case (78.6%:).3.1 Eth;ct of  Tag SetOur tirst set of experiments tests the etDct of theI'()S tag eomt)lexity.
We used both the Siml)leverl) tag set (5 tags) and the c, otnplex verb tag set(42 tags), which is shown in "l'~l)le 3, where * canbe either IS(l, 2S(~, 3S(;, IPL, 2PI+, or 3PI+.
Intim case of siml)le verb tag set, tense, person andnuml)er information is discarded, leaving only a"V" tag and the lower four tags in the table.The scores witlr the siml)le verb tag set fur dif-ferent sizes of training sets are found in Tabh~ 4,and those with the complex verb tag set in 'l'able 5.
For these two experiments, ( he Learner wasset to have a tight restriction on using contextfor learning (i.c, the freedom parameter was set to1) and a loose restriction on context tbr applyingthe learned rules (i.e., l-lagfrecdom 10).
q'he l{,uleTagger was given a moderately-tight restrictiotb onusing context for reduction rule application (i.e.,r-lagJ'rccdom 2).In goner'M, the scores are slightly higher usingthe siml)le verb t~g set over the complex verb55Table 3: (Jomplex Verb Tag SetV-(~()N I) ITIONA L-*V-FUTUI{E-*V-IM PERFECT-*V-IM P EI~F ECT-S U 13.1 U NCTIV E- f{A-*V-IM PER, FECT-SUI\]J  UNCTIVE-S E-*V-PRESENT-*V-P RES ENT-S U BJ UN( JTIV E-*V- P R ET E RI T-*V-NIL-NIl,V-C()NTINU()US-NII~V-I'EI{,FECT-NILV-NIL-N1L-(;LITICTable d: Ambiguously tagged texts, Sirnple VerbsSet # of rules learnedTinySmallMediumFull(none)~core131 82.5%211 91.5%287 91.8%434 83.0%0 78.6%This rule was learned h~te in tile learning processwhen most I'/SU1KJONJ pairs had already beenreduced, llowever, as ol le Call see froll l  t\] le COil-text of the rule, it will apply in a large numberof eases in a text.
The Rule Tagger notes thisand applies the rule early, thus incorrectly chang-ing many P/SUI~C()NJ pairs to SUBC()NJ andreducing the accuracy of t, he tagging.
Since thisphenomenon ever occurred in any of the otherlearning rims, one can see that the learning proeess can be heavily influenced by the choice of it,put texts.3.2 Effeet of Ru le  App l i ca t ionParametersThe next tests performed involved using rules gen-erated above and changing 1)arameters to the Rule'l'agger to see how the scores wouhl be influenced.In the following test, we used tile simi)le verb tagset rules but varied the r-tagfrccdom parameterand the scq parameter.
The results can be foundin Table 6.tag set (91.8% vs. 90.3% for the "Medimn" cor-pus).
This behavior is most likely due to thefact that, some verb tense/person/number combi-nations e~mnot easily be distinguished from con-text, so the Learner was unable to find a rtfle thatwould disambiguate them.As can be seen from the tables, performanceincreased as the size of the learning set incre, asedup to the "Medium" set, where the score levelledotf.
With very small learning sets, the system wasunable to tlnd sulticient examples of phenomenato produce reduction rules with good coverage.One surprising data t)oint in the simpleverb tag set experiments was the "Full" score,which dropped Mmost 9% fi'om the "Medium"score.
After analyzing the results more closely,it was found that the l,earner had learneda very spec, i\[ie rule regarding tile reductionof prel)osition/subordinate~-conjunction eombina-tions late in the learning process.
The learnedrule was:I'I{I'3V'I'A(~ = N : I '\]SUBCONJ -~ S\[IB(ff)NJ'Fable 5:VerbsAmbiguously tagged texts, (~omplexSet # of rules learned ScoreTiny 125 81.7%Small 212 89.6%Medium 323 90.3%Full 564 90.2%(none) 0 78.0%Table 6: Ambiguously tagged texts, Simple VerbsSet R,-Tag-freedomTiny 1234Small 1234Medium 1234Full 1234Score(best-rule-first)82.7%82.5%82.1%81.9%90.
l%91.5%91.5%91.5%90.5%91.8%91.8%91.8%82.4%83.0%81.7%81.5%~eore(learned-seqt le l lce)80.2%80.6%80.5%80.5%89.8%89.9%89.9%89.9%90.6%90.5%90.5%90.5%79.8%80.0%80.0%8O.0%Although the wu'iations are slight, the bestvalue for the r'-lagfl'c, edom l)arameter seems to beat an ambiguity level of 2.
It seellls that the strat-egy of reducing the ambiguity as quickly as pos-sible (best-rule-first) is better than following theordering of the rules by the l,earner.
This \[naywell be due to the fact that the ordering of therules as produced by the Learner is dependent onthe training texts.
Since the test set was a differ-eat set of texts, the ordering of the rules was notas applicable to them as to the training texts, andso the tagging performance suffered.563.3 Etfe, r t  (ff Hand+tagged Tex(;sAfl?er ex+ttnining l?h(; result;s fi'om l?he aJ)(~v(~ expcrimcnl?s, wc rea,lized l?hal, sonm of (,he (:h~scd-cl;usswords in Spanish ;~re a, ltnosl, always amhiguous(e.g., preposil?ions are usually ~unl~ig;uous bel,we(m1 ) I{EP a, nd S U B(:( )NJ ,  a, nd del?errnine, rs bel;we('nI) 1'3'1' a, ud 1' R()).
This m('aus (;hal, l;h(~ l,ea, rncr will?~,ever \[ea, rn a rule I?o dismul)igu;tl,e l,hcs(~ clos(:d-class (:~+ses I)e(:+mse l,here will r;u'ely he ulmml)i,gtt-otis C()ll(;c:xl?s ill I, he l?raining I,ex(,s (,agge(\[ hy 1;11('ini(;iaJ Si;al,e A lttlO(,&(;or.
'l'ha(, is, un\]i\],m ()\[)('II-(:\[&SSw(,'ds, wc will no(, littd .cw ltJta,ntl)iguotts ch,s('dclass words i ,  l?exl?s prccis(~ly I)(:(:;mse there is oil\[ya closed set; of t;hcm+ 'l'hus, wc decided I?o illl, ro-(bite a, st~la\[\] tltll\]t\])(:r of' \]la, lt(\[-(,~Lgg(x\] l;exl;s illl,o (,lie1;l:a, ining sel; given (;o the l,earner.
Since t, he }l;m(\[t~tgg(;(\[ 1;exI;s \[l&Ve ~?corI'(~C(;" (~X&III\[)I0S ()\[+ V,~LI'IOIlSl)h(:notn(',ua,, I, he l.eartmr s\]toul(l \])e a\])\[e (,?)
lin(Igood exa, nq)les in t;h(,~+ I?
(, learn l'ro~+.For our t,esl?s, wc (h~litmd four set,s o\[" hat .
l -t, agged texts t,h;U, wc a, dded t+()the "Sttmll" (306(~wor(\[s) set, o\[' at~tbigu()us\[y l,aggcd l,exl,s.
The1)reakdown is in Table T.Ta, hh' 7: Ila, tM-l?a+gg(+d ' l ' ra in i ,g s(:l~sSet, WordsSnta, I1 218 wor(IsMe(liutn 588 wor(lsl.a, rgc !
)()(i wordsFull 1791 wordsAgain, (,he l,e~rner w~m ,'set l,o have a. l?i,ghl, rc-sl?rici;ion on using cont, exl?
for h+arn\]ttg (,fr'ccdom l);m(I a loose restric(,ion on col|l?ex(, \['or ;t.ppJyill,g l?hc\],:,a, rn(;(\[ rul(;s (la.qJ}'ecdom, 10).
'l'h(> I{,ulc Taggerwa, s giw~n a itlodera,l,ely-t, ighl?
rest, riot,ion on using(:OIl\[,(;Xl?
\['or t':(lll('l,iotl rule a,i)l+lit:al, i()t~ (J:r'rcdom 2).The bcst-rul(',-Jir,sl mode of I, he l{mle Tagg(:r wasI l scd,The resull,~, ~s shown iu Table 8, a+rc sligi~l, lybelA,('r l, han wh(;n using only ;m~l)igttously Laggedl?eXl,S, It is inl;eresl, it~g I?o note tl\];d, l,\]m higher~-tc(:tu'a, cy w:-ts achieved wit, h fewer ruh'.s.
Itl fact,;d\[ expe, rimcnl,s resull;ed iu \[ea, rnhtg a lil,l?h' (~ver200 tithes.
'I'M)h; 8: Atnl) iguous/ l \]t~;tnd, igu.us 'l'cxts, Sit~tpleVerbsSet, # of rul(':s h'artmd S(:(,'c10Medium 211 92.1%I,argc 205 I i%l"ull  202 L~.q2.
1%( .o, ,(  9 _ :2+t Ilu +ul(lil?i()t~ t,o Lira (+?l)('rhu(mts ;Ll)ov(>, wcwa, l|l?
(xI l,o knuw i\[" (,he itfl;r()(lucl, ion o\[' ha, rid-ULgg;cd texts into t im "Fu l l "  aud~iguo,.tsly 1,a~g,.
'dset would improw~ il,s r,M;h.er low score (or.
'l'a-hie 4).
Wc performed an experilJtcnl, using sitnplcw~rb tags, the "l,'ull" ambiguous ly  tagged text;s,~md the "Full" ha, nd-t;agged l?exts.
Tim resu\[l?swere d22 rules learned with :-~ score of 92.1%, whichtied with (,he "Sm'MI" ambiguously  l,agged set, forachieving l,he highest, .,tccura.
('y of all o\[" the lem',-i ,g/ta,ggine; runs, a~ full 13.5% higher than using,o  I,.~;-u'nittg.4 Prob lems and  Poss ib lehnprovementsAll;hough our Sl);mish P()S l;aggcr l)er\['orn,ed rcason;dfly well, ~whieving ~Llt it ,q)rovcment of 13.,5%ill ;tc(:tlra, cy over r&llC\[()ttlly picking tags, l?hcre wcrosewwaJ lwol)lcms t, lt~ui; ln'evenl, e,,l the sysl?cln I'ronllre;whiug an cwm higher score.4.11.
Learn ing  Proi~h' , lnAs discussed iu Sccl?ion 3.3, ~u,,l)iguous closedclass words (e.g., prep()sil, ions, det,crminers, etc.
)ca,nnol; bc reduced when l, here a, re no unaatll~iguous exa.nlples o\[' l?heull in l, he l,i'n, iui|lg; t,exl, s. Thisis prev;flent, in Slmnish, where most I)reposil?ionsC~ll| &\]SO ~)0 :-;tl\]){H'dill;t\[,o (; II j/LIICI, ioIIS~ dc(,orll l i l lOl'Sc;u| be prol|otlllS, (':l;c. A \['ew h~tll(I (,;Lgged l,exl?s;~rc required l,o Ic,,rn goud rules for reducing l,he;uld~iguil?y un I hesc words.
It ix l>ossihle, t,~w-(wcr, t, hM such l?exts c.%tl I)e dis:-md)igutd, ed onlyfor t, heir :-~lways ambiguous ch)scd-ck-tss words bul?llol, tlllaJlli)igtlOtlS clos('(l-cla,'-.
;s words or o\])0,11-classwords.
Such an cxperim(ml; similar 1,o seleclivo,samplin.q (.\[isctlsscd in l )agan and lengels(m (1)a-g;ul ~-md l",l+gelscm, 1+)/)5) wo.h l  I',e useful in the\['ul, tll'(: \[)c:c;~llse, il' it, is t, ruc, it; will reduce t, hc cost;or tll:-I,tlll;-t\] l,~+gging (-onsidcr+flfly.4.2 Lex icon  Prol) le,  nJ.I ) rohlems l;ha,t; \]m('anlc a, ppar(ml?
.a,s we ra, lt lll()r(:t,csl?s were (,he incotnl)l(~l?en(~ss ~tl~(I t\[tisl, a, kes int, he lexic(m. Whi le I,h(+ lexicotl, derived l'r()lll t h(~(',ollins Spa,nish-lgnglish dict, iot\]&ry, w~s quit,(' richin w(~r(ls, il?s l;:-tg set, ,.lid uol?
a, lwa+ys tmd, ch l;he t, agdcliuit,ions we ('ml)loyc(t. l,'or (~xampl(~, our l,ag sol,(\]isl, inguislms pr(>l)(:r n()uus (I)I{.
()I>N) &lid It()/lllS(N), whereas the (~ollins di(:l?ionvxy t~ark(+d h(>l;has nout+s (N).
We have a(lded ottr existing 1)t't)l)twha.rim lisl?s 1,o t, he lexicon t,o t>+u'l;bdly solve /,hisi)rol+lem, Iml, the list, s +u'e currenl;ly limil,cd I,o h>(:~l,l?i(lli II;_I, llI(~S (tll(i lmol)hCs lh'sl; n,~tln(>s.We a, lso l~'Otlltd s(w(q'0,1 mis takes  ht late ( \ ]o l l i , sdclinil,i(,ns (e.g., severed adverbs ending " ltt(~llt,( "?,wcr(" classified a(Ij(~cl;ives).
All, lt()ug\]~ we fixedl?hese mist, akcs as we t~ol, iccd (hcltl, it, ix diflicult,1,O kllOW h(lw nl,~uly sucll (Wl'ors sl,ill remain in tim\[(~xic:()n.It l?urtt(xt out; I;hal, the h+complcle, nc,s.s o1' the lex-ic.n was auol, h(w funda+m(ml,a,l l)rol)h~::~ I?o I~rill'sunsul>erviscd h'~-u'ning algoril:Inm ThaJ, is, when57the lexicon does not list all the possible tags for aword, the tagger is very likely to make a mistake.This is because the learner is trained to reducethe ambiguity of possible tags of a word (say N,V, ADJ tags), but if the lexicon lists only a sub-set of the possible tags (say N and V tags), thesystem will never learn to assign an ADJ tag evenwhen the word is used as an adjective.This type of problem was observed frequentlywhen words are ambiguous between proper nounsand some other parts-of-speech such as "Flo-,'es (ADJ /PROPN),"  "Lozano (ADJ /PROPN),""van (V/PP~OPN) ''a, "Serra (V/l'i{,OPN)," etc.because not all the proper nouns are in the lexi-COI l .The problems described above did not occur inBrill's experiments because he derived the lexiconfi'om a POS-tagged corpus and used the untaggedversion of the same corpus for training and test-ing.
Thus, he used an "optimal" lexicon whichcontains all the words with only parts-of-speechwhich appeared in the corpus.
In addition, insuch a corpus, rarely used POS tags of a wordare less likely to occur, and words are less likelyto be ambiguous.
Thus, in a sense, his "unsuper-vised learning" experiments did take advantage ofa large POS-tagged corpns.5 Re lated WorksIt is very ditIicult to compare performances be-tween taggers when accuracy depends on qualityof corpora and lexicons, and maybe on character-is,its of languages.
But in this section, we corn-pare our tagger with Hidden Markov Model-basedtaggers.A more widely used algorithnl for unsuper-vised learning of a POS tagger is Hidden MarkovModel (I1MM).
Cutting el al.
((hitting et al,1992) and Melialdo (Merialdo, 1994) used IIMMto learn English POS taggers while Chanod and'I'apanainen (Chanod and Tapanainen, 1995),Feldweg (Feldweg, 1995), and Ledn and Ser-rano (l,e6n and Serrano, 1995) ported tile Xeroxtagger (Cutting et al, 1992) to French, German,and Spanish respectively.
One of tile drawbacks ofan tlMM-based approach is that laborious man-ual tuning of symbol and transition biases is nec:essary to achieve high accuracy.
Without tunedbiases, the C, erman Xerox tagger achieved 85.89%while the French Xerox tagger achieved 87% accu-racy.
After one man-month of tuning biases, theaccuracy of the French tagger increased to 96.8%.One could derive such biases fronl a corpus, as dis-cussed in (Merialdo, 199d), but it unfortunatelyrequires a tagged cort/us.
'Fhe best accuracy of the Spanish Xerox tag:ger was 91.51% for the reduced tag set (174 tags)lit can be a part of a last name as it, "van Mahler",but also is an inflected form of "it".with the hase accuracy (i.e.
no training) of 88.98%while the best accuracy of our tagger is currently92.1% for the simple tag set (39 tags) with thebase accuracy of 78.6%.
The lower base accuracyin our exl>eriment is probably due to the largenumber of entries in the Collins dictionary.6 SummaryOur Spanish Part of Speech Tagger is a successfulimplementation and extension of Brill's unsuper:vised learning algorithm that reduces the ambi-guity of part-of-speech tags on words in Spanishtexts.The system requires few, if any, hand-taggedtexts to bootstrap itself.
Rather, it merely re-quires a Spanish lexicon and morphological an-alyzer that can tag words with all their possi-ble parts-of-speech.
(liven that the system per-forms at approximately 92% accuracy even withthe aforementioned problems and with the inch>sion of unknown words, we would expect that thissystem could achieve better results, approachingthose of similar English-language POS taggers,when these problems are rectitied.ReferencesEric Brill.
1995.
/Jnupervised learning of disam-biguation rules for part of speech tagging, hiProceedings of the 3rd Workshop on Very LargeCorpora.Jean-l)ierre Chanod and Pasi Tal)anainen.
1995.Tagging French - (~omparing statistical and aconstraint-based method.
In Proceedings of theI,/A CL - 95.D.
Cutting, J. Kupiec, J. Pedersen, and P. Sibun.1992.
A Practical Part-of-Speech Tagger.
hiProceedings of the 7'hird Conference ou AppliedNatural Language Processing.Ido Dagan and Scan I ).
Engelson.
1995.
SelectiveSampling in Natural I,anguage Learning.
InProceedings of the IJCAI Workshop on Nero Ap-proach to Lcarning for" Natural Language Pro-cessing.llelnlut Feldweg.
1995.
Implementation a d Eval-uation of a. German ll M M for POS Disambigua-lion.
In Proceedings of lhe Is"ACL ,91C1)A7'Workshop.Fernando S{mchez I,edn and Amalio F. Nieto Ser-ran().
1995. l)evelot>ment of a spanish versionof the xerox tagger.
Ill l'roceedings of the XICongrcso de la ,5'ocicdad I,,'spar~ola para el Proce=samiento dcl Lenguaje.
Nalural (,gEl'LN '95).Bernard Merialdo.
1994.
Tagging English Textwith a Probabilistic Model.
Compnialional Lin-guislics, 20(2).53
