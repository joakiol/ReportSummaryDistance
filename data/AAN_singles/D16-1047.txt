Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 490?500,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsMorphological Priors for Probabilistic Neural Word EmbeddingsParminder Bhatia?Yik Yak, Inc.3525 Piedmont Rd NE, Building 6, Suite 500Atlanta, GAparminder@yikyakapp.comRobert Guthrie?
and Jacob EisensteinSchool of Interactive ComputingGeorgia Institute of TechnologyAtlanta, GA 30312 USA{rguthrie3 + jacobe}@gatech.eduAbstractWord embeddings allow natural language pro-cessing systems to share statistical informationacross related words.
These embeddings aretypically based on distributional statistics, mak-ing it difficult for them to generalize to rare orunseen words.
We propose to improve wordembeddings by incorporating morphologicalinformation, capturing shared sub-word fea-tures.
Unlike previous work that constructsword embeddings directly from morphemes,we combine morphological and distributionalinformation in a unified probabilistic frame-work, in which the word embedding is a latentvariable.
The morphological information pro-vides a prior distribution on the latent word em-beddings, which in turn condition a likelihoodfunction over an observed corpus.
This ap-proach yields improvements on intrinsic wordsimilarity evaluations, and also in the down-stream task of part-of-speech tagging.1 IntroductionWord embeddings have been shown to improve manynatural language processing applications, from lan-guage models (Mikolov et al, 2010) to informationextraction (Collobert and Weston, 2008), and fromparsing (Chen and Manning, 2014) to machine trans-lation (Cho et al, 2014).
Word embeddings leveragea classical idea in natural language processing: usedistributional statistics from large amounts of unla-beled data to learn representations that allow sharing?The first two authors contributed equally.
Codeis available at https://github.com/rguthrie3/MorphologicalPriorsForWordEmbeddings.across related words (Brown et al, 1992).
While thisapproach is undeniably effective, the long-tail natureof linguistic data ensures that there will always bewords that are not observed in even the largest cor-pus (Zipf, 1949).
There will be many other wordswhich are observed only a handful of times, makingthe distributional statistics too sparse to accuratelyestimate the 100- or 1000-dimensional dense vectorsthat are typically used for word embeddings.
Theseproblems are particularly acute in morphologicallyrich languages like German and Turkish, where eachword may have dozens of possible inflections.Recent work has proposed to address this issue byreplacing word-level embeddings with embeddingsbased on subword units: morphemes (Luong et al,2013; Botha and Blunsom, 2014) or individual char-acters (Santos and Zadrozny, 2014; Ling et al, 2015;Kim et al, 2016).
Such models leverage the fact thatword meaning is often compositional, arising fromsubword components.
By learning representations ofsubword units, it is possible to generalize to rare andunseen words.But while morphology and orthography are some-times a signal of semantics, there are also many casessimilar spellings do not imply similar meanings: bet-ter-batter, melon-felon, dessert-desert, etc.
If eachword?s embedding is constrained to be a determinis-tic function of its characters, as in prior work, thenit will be difficult to learn appropriately distinct em-beddings for such pairs.
Automated morphologicalanalysis may be incorrect: for example, really maybe segmented into re+ally, incorrectly suggesting asimilarity to revise and review.
Even correct morpho-logical segmentation may be misleading.
Consider490that incredible and inflammable share a prefix in-,which exerts the opposite effect in these two cases.1Overall, a word?s observed internal structure givesevidence about its meaning, but it must be possible tooverride this evidence when the distributional factspoint in another direction.We formalize this idea using the machinery ofprobabilistic graphical models.
We treat word em-beddings as latent variables (Vilnis and McCallum,2014), which are conditioned on a prior distributionthat is based on word morphology.
We then maximizea variational approximation to the expected likeli-hood of an observed corpus of text, fitting variationalparameters over latent binary word embeddings.
Forcommon words, the expected word embeddings arelargely determined by the expected corpus likelihood,and thus, by the distributional statistics.
For rarewords, the prior plays a larger role.
Since the priordistribution is a function of the morphology, it is pos-sible to impute embeddings for unseen words aftertraining the model.We model word embeddings as latent binary vec-tors.
This choice is based on linguistic theories oflexical semantics and morphology.
Morphemes areviewed as adding morphosyntactic features to words:for example, in English, un- adds a negation feature(unbelievable), -s adds a plural feature, and -ed addsa past tense feature (Halle and Marantz, 1993).
Sim-ilarly, the lexicon is often viewed as organized interms of features: for example, the word bachelorcarries the features HUMAN, MALE, and UNMAR-RIED (Katz and Fodor, 1963).
Each word?s semanticrole within a sentence can also be characterized interms of binary features (Dowty, 1991; Reisinger etal., 2015).
Our approach is more amenable to suchtheoretical models than traditional distributed wordembeddings.
However, we can also work with the ex-pected word embeddings, which are vectors of prob-abilities, and can therefore be expected to hold theadvantages of dense distributed representations (Ben-gio et al, 2013).1The confusion is resolved by morphologically analyzing thesecond example as (in+flame)+able, but this requires hierarchi-cal morphological parsing, not just segmentation.2 ModelThe modeling framework is illustrated in Figure 1,focusing on the word sesquipedalianism.
This wordis rare, but its morphology indicates several of itsproperties: the -ism suffix suggests that the word is anoun, likely describing some abstract property; thesesqui- prefix refers to one and a half, and so on.
Ifthe word is unknown, we must lean heavily on theseintuitions, but if the word is well attested then we canrely instead on its examples in use.It is this reasoning that our modeling frameworkaims to formalize.
We treat word embeddings as la-tent variables in a joint probabilistic model.
The priordistribution over a word?s embedding is conditionedon its morphological structure.
The embedding it-self then participates, as a latent variable, in a neuralsequence model over a corpus, contributing to theoverall corpus likelihood.
If the word appears fre-quently, then the corpus likelihood dominates theprior ?
which is equivalent to relying on the word?sdistributional properties.
If the word appears rarely,then the prior distribution steps in, and gives a bestguess as to the word?s meaning.Before describing these component pieces in detail,we first introduce some notation.
The representationof word w is a latent binary vector bw ?
{0, 1}k,where k is the size of each word embedding.
Asnoted in the introduction, this binary representationis motivated by feature-based theories of lexical se-mantics (Katz and Fodor, 1963).
Each word w isconstructed from a set of Mw observed morphemes,Mw = (mw,1,mw,2, .
.
.
,mw,Mw).
Each morphemeis in turn drawn from a finite vocabulary of sizevm, so that mw,i ?
{1, 2, .
.
.
, vm}.
Morphemesare obtained from an unsupervised morphologicalsegmenter, which is treated as a black box.
Fi-nally, we are given a corpus, which is a sequenceof words, x = (x1, x2, .
.
.
, xN ), where each wordxt ?
{1, 2, .
.
.
, vw}, with vw equal to the size of thevocabulary, including the token ?UNK?
for unknownwords.2.1 Prior distributionThe key differentiating property of this model is thatrather than estimating word embeddings directly, wetreat them as a latent variable, with a prior distri-bution reflecting the word?s morphological proper-491plagued by sesquipedalianism .
.
.h1 h2 h3bplagued bby bsesquipedalianismuplague uby usesquiued upedaluianuismFigure 1: Model architecture, applied to the example sequence .
.
.
plagued by sesquipedalianism .
.
.
.
Blue solid arrows indicatedirect computation, red dashed arrows indicate probabilistic dependency.
For simplicity, we present our models as recurrent neuralnetworks rather than long short-term memories (LSTMs).ties.
To characterize this prior distribution, each mor-pheme m is associated with an embedding of its own,um ?
Rk, where k is again the embedding size.
Thenfor position i of the word embedding bw, we havethe following prior,bw,i ?
Bernoulli(?
(?m?Mwum,i)), (1)where ?(?)
indicates the sigmoid function.
The priorlog-likelihood for a set of word embeddings is,logP (b;M,u) (2)=Vw?wlogP (bw;Mw,u) (3)=Vw?wk?ilogP (bw,i;Mw,u) (4)=Vw?wk?ibw,i log ?
( ?m?Mwum,i)(5)+ (1?
bw,i) log(1?
?
( ?m?Mwum,i)).2.2 Expected likelihoodThe corpus likelihood is computed via a recurrentneural network language model (Mikolov et al, 2010,RNNLM), which is a generative model of sequencesof tokens.
In the RNNLM, the probability of eachword is conditioned on all preceding words througha recurrently updated state vector.
This state vectorin turn depends on the embeddings of the previouswords, through the following update equations:ht =f(bxt ,ht?1) (6)xt+1 ?Multinomial (Softmax [Vht]) .
(7)The function f(?)
is a recurrent update equation; inthe RNN, it corresponds to ?
(?ht?1 + bxt), where?(?)
is the elementwise sigmoid function.
The matrixV ?
Rv?k contains the ?output embeddings?
of eachword in the vocabulary.
We can then define the condi-tional log-likelihood of a corpusx = (x1, x2, .
.
.
xN )as,logP (x | b) =N?tlogP (xt | ht?1, b).
(8)Since ht?1 is deterministically computed fromx1:t?1 (conditioned on b), we can equivalently writethe log-likelihood as,logP (x | b) =?tlogP (xt | x1:t?1, b).
(9)This same notation can be applied to compute thelikelihood under a long-short term memory (LSTM)language model (Sundermeyer et al, 2012).
The onlydifference is that the recurrence function f(?)
fromEquation 6 now becomes more complex, includingthe input, output, and forget gates, and the recurrentstate ht now includes the memory cell.
As the LSTM492update equations are well known, we focus on themore concise RNN notation, but we employ LSTMsin all experiments due to their better ability to capturelong-range dependencies.2.3 Variational approximationInference on the marginal likelihood P (x1:N ) =?P (x1:N , b)db is intractable.
We address this is-sue by making a variational approximation,logP (x) = log?bP (x | b)P (b) (10)= log?bQ(b)Q(b)P (x | b)P (b) (11)= logEq[P (x | b)P (b)Q(b)](12)?Eq[logP (x | b) + logP (b)?
logQ(b)](13)The variational distribution Q(b) is defined using afully factorized mean field approximation,Q(b;?)
=vw?wk?iq(bw,i; ?w,i).
(14)The variational distribution is a product of Bernoullis,with parameters ?w,j ?
[0, 1].
In the evaluationsthat follow, we use the expected word embeddingsq(bw), which are dense vectors in [0, 1]k. We canthen use Q(?)
to place a variational lower bound onthe expected conditional likelihood,Even with this variational approximation, the ex-pected log-likelihood is still intractable to compute.In recurrent neural network language models, eachword xt is conditioned on the entire prior history,x1:t?1 ?
indeed, this is one of the key advantagesover fixed-length n-gram models.
However, thismeans that the individual expected log probabilitiesinvolve not just the word embedding of xt and itsimmediate predecessor, but rather, the embeddingsof all words in the sequence x1:t:Eq [logP (x | b)] (15)=N?tEq [logP (xt | x1:t?1, b)] (16)=N?t?
{bw:w?x1:t}Q({bw : w ?
x1:t})?
logP (xt | x1:t?1, b).
(17)We therefore make a further approximation by tak-ing a local expectation over the recurrent state,Eq [ht] ?
f(Eq [bxt ] , Eq [ht?1])(18)Eq [logP (xt | x1:t?1, b)] ?
log Softmax (VEq [ht]) .
(19)This approximation means that we do not propa-gate uncertainty about ht through the recurrent up-date or through the likelihood function, but rather, weuse local point estimates.
Alternative methods suchas variational autoencoders (Chung et al, 2015) orsequential Monte Carlo (de Freitas et al, 2000) mightprovide better and more principled approximations,but this direction is left for future work.Variational bounds in the form of Equation 13can generally be expressed as a difference betweenan expected log-likelihood term and a term for theKullback-Leibler (KL) divergence between the priordistribution P (b) and the variational distributionQ(b) (Wainwright and Jordan, 2008).
Incorporat-ing the approximation in Equation 19, the resultingobjective is,L =N?tlogP (xt | x1:t?1;Eq[b])?DKL(Q(b) ?
P (b)).
(20)493The KL divergence is equal to,DKL(Q(b) ?
P (b)) (21)=vw?wk?iDKL(q(bw,i) ?
P (bw,i)) (22)=vw?wk?i?w,i log ?
(?m?Mwum,i)+ (1?
?w,i) log(1?
?(?m?Mwum,i))?
?w,i log ?w,i ?
(1?
?w,i) log(1?
?w,i).
(23)Each term in the variational bound can be easilyconstructed in a computation graph, enabling auto-matic differentiation and the application of standardstochastic optimization techniques.3 ImplementationThe objective function is given by the variationallower bound in Equation 20, using the approxima-tion to the conditional likelihood described in Equa-tion 19.
This function is optimized in terms of severalparameters:?
the morpheme embeddings, {um}m?1...vm ;?
the variational parameters on the word embed-dings, {?
}w?1...vw ;?
the output word embeddings V;?
the parameter of the recurrence function, ?.Each of these parameters is updated via theRMSProp online learning algorithm (Tieleman andHinton, 2012).
The model and baseline (described be-low) are implemented in blocks (van Merrie?nboeret al, 2015).
In the remainder of the paper, we referto our model as VAREMBED.3.1 Data and preprocessingAll embeddings are trained on 22 million tokensfrom the the North American News Text (NANT)corpus (Graff, 1995).
We use an initial vocabu-lary of 50,000 words, with a special ?UNK?
tokenfor words that are not among the 50,000 most com-mon.
We then perform downcasing and convert allnumeric tokens to a special ?NUM?
token.
After thesesteps, the vocabulary size decreases to 48,986.
Notethat the method can impute word embeddings forout-of-vocabulary words under the prior distributionP (b;M,u); however, it is still necessary to decideon a vocabulary size to determine the number ofvariational parameters ?
and output embeddings toestimate.Unsupervised morphological segmentation is per-formed using Morfessor (Creutz and Lagus, 2002),with a maximum of sixteen morphemes per word.This results in a total of 14,000 morphemes, whichincludes stems for monomorphemic words.
We donot rely on any labeled information about morpho-logical structure, although the incorporation of goldmorphological analysis is a promising topic for futurework.3.2 Learning detailsThe LSTM parameters are initialized uniformly inthe range [?0.08, 0.08].
The word embeddings areinitialized using pre-trained word2vec embeddings.We train the model for 15 epochs, with an initiallearning rate of 0.01, a decay of 0.97 per epoch, andminibatches of size 25.
We clip the norm of thegradients (normalized by minibatch size) at 1, usingthe default settings in the RMSprop implementationin blocks.
These choices are motivated by priorwork (Zaremba et al, 2014).
After each iteration, wecompute the objective function on the developmentset; when the objective does not improve beyond asmall threshold, we halve the learning rate.Training takes roughly one hour per iteration us-ing an NVIDIA 670 GTX, which is a commoditygraphics processing unit (GPU) for gaming.
Thisis nearly identical to the training time required forour reimplementation of the algorithm of Botha andBlunsom (2014), described below.3.3 BaselineThe most comparable approach is that of Botha andBlunsom (2014).
In their work, embeddings are es-timated for each morpheme, as well as for each in-vocabulary word.
The final embedding for a word isthen the sum of these embeddings, e.g.,greenhouse = greenhouse + green + house, (24)where the italicized elements represent learned em-beddings.494We build a baseline that is closely inspired by thisapproach, which we call SUMEMBED.
The key differ-ence is that while Botha and Blunsom (2014) build onthe log-bilinear language model (Mnih and Hinton,2007), we use the same LSTM-based architecture asin our own model implementation.
This enables ourevaluation to focus on the critical difference betweenthe two approaches: the use of latent variables ratherthan summation to model the word embeddings.
Aswith our method, we used pre-trained word2vecembeddings to initialize the model.3.4 Number of parametersThe dominant terms in the overall number of parame-ters are the (expected) word embeddings themselves.The variational parameters of the input word em-beddings, ?, are of size k ?
vw.
The output wordembeddings are of size #|h| ?
vw.
The morphemeembeddings are of size k ?
vm, with vm  vw.
Inour main experiments, we set vw = 48, 896 (seeabove), k = 128, and #|h| = 128.
After includingthe character embeddings and the parameters of therecurrent models, the total number of parameters isroughly 12.8 million.
This is identical to number ofparameters in the SUMEMBED baseline.4 EvaluationOur evaluation compares the following embeddings:WORD2VEC We train the popular word2vecCBOW (continuous bag of words)model (Mikolov et al, 2013), using thegensim implementation.SUMEMBED We compare against the baseline de-scribed in ?
3.3, which can be viewed as areimplementation of the compositional modelof Botha and Blunsom (2014).VAREMBED For our model, we take the expectedembeddings Eq[b], and then pass them throughan inverse sigmoid function to obtain valuesover the entire real line.4.1 Word similarityOur first evaluation is based on two classical wordsimilarity datasets: Wordsim353 (Finkelstein et al,2001) and the Stanford ?rare words?
(rw) dataset (Lu-ong et al, 2013).
We report Spearmann?s ?, a mea-sure of rank correlation, evaluating on both the entirevocabulary as well as the subset of in-vocabularywords.As shown in Table 1, VAREMBED consistentlyoutperforms SUMEMBED on both datasets.
On thesubset of in-vocabulary words, WORD2VEC givesslightly better results on the wordsim words that arein the NANT vocabulary, but is not applicable tothe complete dataset.
On the rare words dataset,WORD2VEC performs considerably worse than bothmorphology-based models, matching the findings ofLuong et al (2013) and Botha and Blunsom (2014)regarding the importance of morphology for doingwell on this dataset.4.2 Alignment with lexical semantic featuresRecent work questions whether these word similar-ity metrics are predictive of performance on down-stream tasks (Faruqui et al, 2016).
The QVEC statis-tic is another intrinsic evaluation method, which hasbeen shown to be better correlated with downstreamtasks (Tsvetkov et al, 2015).
This metric measuresthe alignment between word embeddings and a setof lexical semantic features.
Specifically, we use thesemcor noun verb supersenses oracle provided at theqvec github repository.2As shown in Table 2, VAREMBED outperformsSUMEMBED on the full lexicon, and gives simi-lar performance to WORD2VEC on the set of in-vocabulary words.
We also consider the morphemeembeddings alone.
For SUMEMBED, this means thatwe construct the word embedding from the sum ofthe embeddings for its morphemes, without the ad-ditional embedding per word.
For VAREMBED, weuse the expected embedding under the prior distribu-tion E[b | c].
The results for these representationsare shown in the bottom half of Table 2, revealingthat VAREMBED learns much more meaningful em-beddings at the morpheme level, while much of thepower of SUMEMBED seems to come from the wordembeddings.2https://github.com/ytsvetko/qvec495WORD2VEC SUMEMBED VAREMBEDWordsim353all words (353) n/a 42.9 48.8in-vocab (348) 51.4 45.9 51.3rare words (rw)all words (2034) n/a 23.0 24.0in-vocab (715) 33.6 37.3 44.1Table 1: Word similarity evaluation results, as measured by Spearmann?s ??
100.
WORD2VEC cannot be evaluated on all words,because embeddings are not available for out-of-vocabulary words.
The total number of words in each dataset is indicated inparentheses.all words(4199)in vocab(3997)WORD2VEC n/a 34.8SUMEMBED 32.8 33.5VAREMBED 33.6 34.7morphemes onlySUMEMBED 24.7 25.1VAREMBED 30.2 31.0Table 2: Alignment with lexical semantic features, as measuredby QVEC.
Higher scores are better, with a maximum possiblescore of 100.4.3 Part-of-speech taggingOur final evaluation is on the downstream task ofpart-of-speech tagging, using the Penn Treebank.We build a simple classification-based tagger, us-ing a feedforward neural network.
(This is not in-tended as an alternative to state-of-the-art taggingalgorithms, but as a comparison of the syntacticutility of the information encoded in the word em-beddings.)
The inputs to the network are the con-catenated embeddings of the five word neighbor-hood (xt?2, xt?1, xt, xt+1, xt+2); as in all evalua-tions, 128-dimensional embeddings are used, so thetotal size of the input is 640.
This input is fed intoa network with two hidden layers of size 625, anda softmax output layer over all tags.
We train usingRMSProp (Tieleman and Hinton, 2012).Results are shown in Table 3.
Bothmorphologically-informed embeddings aresignificantly better to WORD2VEC (p < .01,two-tailed binomial test), but the difference betweenSUMEMBED and VAREMBED is not significantdev testWORD2VEC 92.42 92.40SUMEMBED 93.26 93.26VAREMBED 93.05 93.09Table 3: Part-of-speech tagging accuracies0-100 100-1000 1000-10000 10000-100000word frequency in NANT0.000.050.100.150.200.250.300.35errorrateembeddingVarEmbedSumEmbedWord2VecFigure 2: Error rates by word frequency.at p < .05.
Figure 2 breaks down the errors byword frequency.
As shown in the figure, the taggerbased on WORD2VEC performs poorly for rarewords, which is expected because these embeddingsare estimated from sparse distributional statistics.SUMEMBED is slightly better on the rarest words(the 0 ?
100 group accounts for roughly 10% ofall tokens).
In this case, it appears that this simpleadditive model is better, since the distributionalstatistics are too sparse to offer much improvement.The probabilistic VAREMBED embeddings arebest for all other frequency groups, showing that iteffectively combines morphology and distributionalstatistics.5 Related workAdding side information to word embeddingsAn alternative approach to incorporating additional496information into word embeddings is to constrain theembeddings of semantically-related words to be sim-ilar.
Such work typically draws on existing lexicalsemantic resources such as WordNet.
For example,Yu and Dredze (2014) define a joint training objec-tive, in which the word embedding must predict notonly neighboring word tokens in a corpus, but alsorelated word types in a semantic resource; a similarapproach is taken by Bian et al (2014).
Alternatively,Faruqui et al (2015) propose to ?retrofit?
pre-trainedword embeddings over a semantic network.
Bothretrofitting and our own approach treat the true wordembeddings as latent variables, from which the pre-trained word embeddings are stochastically emitted.However, a key difference from our approach is thatthe underlying representation in these prior works isrelational, and not generative.
These methods cancapture similarity between words in a relational lex-icon such as WordNet, but they do not offer a gen-erative account of how (approximate) meaning isconstructed from orthography or morphology.Word embeddings and morphology TheSUMEMBED baseline is based on the work of Bothaand Blunsom (2014), in which words are segmentedinto morphemes using MORFESSOR (Creutz andLagus, 2002), and then word representations arecomputed through addition of morpheme represen-tations.
A key modeling difference from this priorwork is that rather than computing word embeddingsdirectly and deterministically from subcomponentembeddings (morphemes or characters, as in (Linget al, 2015; Kim et al, 2016)), we use thesesubcomponents to define a prior distribution, whichcan be overridden by distributional statistics forcommon words.
Other work exploits morphologyby training word embeddings to optimize a jointobjective over distributional statistics and rich,morphologically-augmented part of speech tags (Cot-terell and Schu?tze, 2015).
This can yield better wordembeddings, but does not provide a way to computeembeddings for unseen words, as our approach does.Recent work by Cotterell et al (2016) extends theidea of retrofitting, which was based on semanticsimilarity, to a morphological framework.
In thismodel, embeddings are learned for morphemes aswell as for words, and each word embedding is con-ditioned on the sum of the morpheme embeddings,using a multivariate Gaussian.
The covariance of thisGaussian prior is set to the inverse of the number ofexamples in the training corpus, which has the effectof letting the morphology play a larger role for rareor unseen words.
Like all retrofitting approaches, thismethod is applied in a pipeline fashion after trainingword embeddings on a large corpus; in contrast, ourapproach is a joint model over the morphology andcorpus.
Another practical difference is that Cotterellet al (2016) use gold morphological features, whilewe use an automated morphological segmentation.Latent word embeddings Word embeddings aretypically treated as a parameter, and are optimizedthrough point estimation (Bengio et al, 2003; Col-lobert and Weston, 2008; Mikolov et al, 2010).
Cur-rent models use word embeddings with hundreds oreven thousands of parameters per word, yet manywords are observed only a handful of times.
It istherefore natural to consider whether it might bebeneficial to model uncertainty over word embed-dings.
Vilnis and McCallum (2014) propose to modelGaussian densities over dense vector word embed-dings.
They estimate the parameters of the Gaussiandirectly, and, unlike our work, do not consider us-ing orthographic information as a prior distribution.This is easy to do in the latent binary frameworkproposed here, which is also a better fit for some the-oretical models of lexical semantics (Katz and Fodor,1963; Reisinger et al, 2015).
This view is shared byKruszewski et al (2015), who induce binary wordrepresentations using labeled data of lexical seman-tic entailment relations, and by Henderson and Popa(2016), who take a mean field approximation overbinary representations of lexical semantic features toinduce hyponymy relations.More broadly, our work is inspired by recent ef-forts to combine directed graphical models with dis-criminatively trained ?deep learning?
architectures.The variational autoencoder (Kingma and Welling,2014), neural variational inference (Mnih and Gregor,2014; Miao et al, 2016), and black box variationalinference (Ranganath et al, 2014) all propose to usea neural network to compute the variational approx-imation.
These ideas are employed by Chung et al(2015) in the variational recurrent neural network,which places a latent continuous variable at each timestep.
In contrast, we have a dictionary of latent vari-497ables ?
the word embeddings ?
which introduceuncertainty over the hidden state ht in a standard re-current neural network or LSTM.
We train this modelby employing a mean field approximation, but thesemore recent techniques for neural variational infer-ence may also be applicable.
We plan to explore thispossibility in future work.6 Conclusion and future workWe present a model that unifies compositionaland distributional perspectives on lexical semantics,through the machinery of Bayesian latent variablemodels.
In this framework, our prior expectationsof word meaning are based on internal structure, butthese expectations can be overridden by distributionalstatistics.
The model is based on the very successfullong-short term memory (LSTM) for sequence mod-eling, and while it employs a Bayesian justification,its inference and estimation are little more compli-cated than a standard LSTM.
This demonstrates theadvantages of reasoning about uncertainty even whenworking in a ?neural?
paradigm.This work represents a first step, and we see manypossibilities for improving performance by extendingit.
Clearly we would expect this model to be more ef-fective in languages with richer morphological struc-ture than English, and we plan to explore this possi-bility in future work.
From a modeling perspective,our prior distribution merely sums the morpheme em-beddings, but a more accurate model might accountfor sequential or combinatorial structure, througha recurrent (Ling et al, 2015), recursive (Luong etal., 2013), or convolutional architecture (Kim et al,2016).
There appears to be no technical obstacleto imposing such structure in the prior distribution.Furthermore, while we build the prior distributionfrom morphemes, it is natural to ask whether char-acters might be a better underlying representation:character-based models may generalize well to non-word tokens such as names and abbreviations, theydo not require morphological segmentation, and theyrequire a much smaller number of underlying em-beddings.
On the other hand, morphemes encoderich regularities across words, which may make amorphologically-informed prior easier to learn thana prior which works directly at the character level.It is possible that this tradeoff could be transcendedby combining characters and morphemes in a singlemodel.Another advantage of latent variable models is thatthey admit partial supervision.
If we follow Tsvetkovet al (2015) in the argument that word embeddingsshould correspond to lexical semantic features, thenan inventory of such features could be used as asource of partial supervision, thus locking dimen-sions of the word embeddings to specific semanticproperties.
This would complement the graph-based?retrofitting?
supervision proposed by Faruqui et al(2015), by instead placing supervision at the level ofindividual words.AcknowledgmentsThanks to Erica Briscoe, Martin Hyatt, Yangfeng Ji,Bryan Leslie Lee, and Yi Yang for helpful discussionof this work.
Thanks also the EMNLP reviewers forconstructive feedback.
This research is supported bythe Defense Threat Reduction Agency under awardHDTRA1-15-1-0019.ReferencesYoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic languagemodel.
The Journal of Machine Learning Research,3:1137?1155.Yoshua Bengio, Aaron Courville, and Pascal Vincent.2013.
Representation learning: A review and new per-spectives.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 35(8):1798?1828.Jiang Bian, Bin Gao, and Tie-Yan Liu.
2014.
Knowledge-powered deep learning for word embedding.
InMachine Learning and Knowledge Discovery inDatabases, pages 132?148.
Springer.Jan A Botha and Phil Blunsom.
2014.
Compositional mor-phology for word representations and language mod-elling.
In Proceedings of the International Conferenceon Machine Learning (ICML).Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-cent J Della Pietra, and Jenifer C Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional linguistics, 18(4):467?479.Danqi Chen and Christopher D Manning.
2014.
A fastand accurate dependency parser using neural networks.In Proceedings of Empirical Methods for Natural Lan-guage Processing (EMNLP), pages 740?750.Kyunghyun Cho, Bart Van Merrie?nboer, Caglar Gulcehre,Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,498and Yoshua Bengio.
2014.
Learning phrase representa-tions using rnn encoder-decoder for statistical machinetranslation.
In Proceedings of Empirical Methods forNatural Language Processing (EMNLP).Junyoung Chung, Kyle Kastner, Laurent Dinh, KratarthGoel, Aaron Courville, and Yoshua Bengio.
2015.A recurrent latent variable model for sequential data.In Neural Information Processing Systems (NIPS),Montre?al.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceedingsof the International Conference on Machine Learning(ICML), pages 160?167.Ryan Cotterell and Hinrich Schu?tze.
2015.
Morpho-logical word-embeddings.
In Proceedings of the NorthAmerican Chapter of the Association for ComputationalLinguistics (NAACL), Denver, CO, May.Ryan Cotterell, Hinrich Schu?tze, and Jason Eisner.
2016.Morphological smoothing and extrapolation of wordembeddings.
In Proceedings of the Association forComputational Linguistics (ACL), Berlin, August.Mathias Creutz and Krista Lagus.
2002.
Unsuper-vised discovery of morphemes.
In Proceedings of theACL-02 workshop on Morphological and phonologi-cal learning-Volume 6, pages 21?30.
Association forComputational Linguistics.Joa?o FG de Freitas, Mahesan Niranjan, Andrew H. Gee,and Arnaud Doucet.
2000.
Sequential monte carlomethods to train neural network models.
Neural com-putation, 12(4):955?993.David Dowty.
1991.
Thematic proto-roles and argumentselection.
Language, pages 547?619.Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris Dyer,Eduard Hovy, and Noah A Smith.
2015.
Retrofittingword vectors to semantic lexicons.
In Proceedingsof the North American Chapter of the Association forComputational Linguistics (NAACL), Denver, CO, May.Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi,and Chris Dyer.
2016.
Problems with evaluation ofword embeddings using word similarity tasks.
arxiv,1605.02276.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, EhudRivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.2001.
Placing search in context: The concept revisited.In WWW, pages 406?414.
ACM.David Graff.
1995.
North american news text corpus.Morris Halle and Alec Marantz.
1993.
Distributed mor-phology and the pieces of inflection.
In Kenneth L.Hale and Samuel J. Keyser, editors, The view frombuilding 20.
MIT Press, Cambridge, MA.James Henderson and Diana Nicoleta Popa.
2016.
Avector space for distributional semantics for entailment.In Proceedings of the Association for ComputationalLinguistics (ACL), Berlin, August.Jerrold J Katz and Jerry A Fodor.
1963.
The structure ofa semantic theory.
Language, pages 170?210.Yoon Kim, Yacine Jernite, David Sontag, and Alexan-der M Rush.
2016.
Character-aware neural languagemodels.
In Proceedings of the National Conference onArtificial Intelligence (AAAI).Diederik P Kingma and Max Welling.
2014.
Auto-encoding variational bayes.
In Proceedings of the In-ternational Conference on Learning Representations(ICLR).German Kruszewski, Denis Paperno, and Marco Baroni.2015.
Deriving boolean structures from distributionalvectors.
Transactions of the Association for Computa-tional Linguistics, 3:375?388.Wang Ling, Tiago Lu?
?s, Lu?
?s Marujo, Ramo?n FernandezAstudillo, Silvio Amir, Chris Dyer, Alan W Black, andIsabel Trancoso.
2015.
Finding function in form: Com-positional character models for open vocabulary wordrepresentation.
In Proceedings of Empirical Methodsfor Natural Language Processing (EMNLP), Lisbon,September.Minh-Thang Luong, Richard Socher, and Christopher DManning.
2013.
Better word representations withrecursive neural networks for morphology.
In Pro-ceedings of the Conference on Computational NaturalLanguage Learning (CoNLL).Yishu Miao, Lei Yu, and Phil Blunsom.
2016.
Neural vari-ational inference for text processing.
In Proceedingsof the International Conference on Machine Learning(ICML).Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan Cer-nocky`, and Sanjeev Khudanpur.
2010.
Recurrent neu-ral network based language model.
In INTERSPEECH,pages 1045?1048.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representationsof words and phrases and their compositionality.
InAdvances in Neural Information Processing Systems,pages 3111?3119.Andriy Mnih and Karol Gregor.
2014.
Neural varia-tional inference and learning in belief networks.
InProceedings of the International Conference on Ma-chine Learning (ICML), pages 1791?1799.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.
InProceedings of the International Conference on Ma-chine Learning (ICML).Rajesh Ranganath, Sean Gerrish, and David Blei.
2014.Black box variational inference.
In Proceedings ofthe Seventeenth International Conference on ArtificialIntelligence and Statistics, pages 814?822.499Drew Reisinger, Rachel Rudinger, Francis Ferraro, CraigHarman, Kyle Rawlins, and Benjamin Van Durme.2015.
Semantic proto-roles.
Transactions of the Asso-ciation for Computational Linguistics, 3:475?488.Cicero D. Santos and Bianca Zadrozny.
2014.
Learningcharacter-level representations for part-of-speech tag-ging.
In Proceedings of the International Conferenceon Machine Learning (ICML), pages 1818?1826.Martin Sundermeyer, Ralf Schlu?ter, and Hermann Ney.2012.
LSTM neural networks for language modeling.In Proceedings of INTERSPEECH.Tijman Tieleman and Geoffrey Hinton.
2012.
Lecture 6.5:Rmsprop.
Technical report, Coursera Neural Networksfor Machine Learning.Yulia Tsvetkov, Manaal Faruqui, Wang Ling, GuillaumeLample, and Chris Dyer.
2015.
Evaluation of wordvector representations by subspace alignment.
In Pro-ceedings of Empirical Methods for Natural LanguageProcessing (EMNLP), Lisbon, September.Bart van Merrie?nboer, Dzmitry Bahdanau, Vincent Du-moulin, Dmitriy Serdyuk, David Warde-Farley, JanChorowski, and Yoshua Bengio.
2015.
Blocks and fuel:Frameworks for deep learning.
CoRR, abs/1506.00619.Luke Vilnis and Andrew McCallum.
2014.
Wordrepresentations via gaussian embedding.
CoRR,abs/1412.6623.Martin J. Wainwright and Michael I. Jordan.
2008.
Graph-ical models, exponential families, and variational infer-ence.
Foundations and Trends in Machine Learning,1(1-2):1?305.Mo Yu and Mark Dredze.
2014.
Improving lexical em-beddings with semantic knowledge.
In Proceedings ofthe Association for Computational Linguistics (ACL),pages 545?550, Baltimore, MD.Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.2014.
Recurrent neural network regularization.
arXivpreprint arXiv:1409.2329.George Kingsley Zipf.
1949.
Human behavior and theprinciple of least effort.
Addison-Wesley.500
