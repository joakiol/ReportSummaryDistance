Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 357?361,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsCharacter-based Neural Machine TranslationMarta R. Costa-juss`a and Jos?e A. R. FonollosaTALP Research CenterUniversitat Polit`ecnica de Catalunya, Barcelona{marta.ruiz,jose.fonollosa}@upc.eduAbstractNeural Machine Translation (MT) hasreached state-of-the-art results.
However,one of the main challenges that neural MTstill faces is dealing with very large vo-cabularies and morphologically rich lan-guages.In this paper, we propose a neural MTsystem using character-based embeddingsin combination with convolutional andhighway layers to replace the standardlookup-based word representations.
Theresulting unlimited-vocabulary and affix-aware source word embeddings are testedin a state-of-the-art neural MT based onan attention-based bidirectional recurrentneural network.
The proposed MT schemeprovides improved results even when thesource language is not morphologicallyrich.
Improvements up to 3 BLEU pointsare obtained in the German-English WMTtask.1 IntroductionMachine Translation (MT) is the set of algorithmsthat aim at transforming a source language intoa target language.
For the last 20 years, one ofthe most popular approaches has been statisticalphrase-based MT, which uses a combination offeatures to maximise the probability of the tar-get sentence given the source sentence (Koehn etal., 2003).
Just recently, the neural MT approachhas appeared (Kalchbrenner and Blunsom, 2013;Sutskever et al, 2014; Cho et al, 2014; Bahdanauet al, 2015) and obtained state-of-the-art results.Among its different strengths neural MT doesnot need to pre-design feature functions before-hand; optimizes the entire system at once becauseit provides a fully trainable model; uses word em-beddings (Sutskever et al, 2014) so that words (orminimal units) are not independent anymore; andis easily extendable to multimodal sources of in-formation (Elliott et al, 2015).
As for weaknesses,neural MT has a strong limitation in vocabularydue to its architecture and it is difficult and com-putationally expensive to tune all parameters in thedeep learning structure.In this paper, we use the neural MT baselinesystem from (Bahdanau et al, 2015), which fol-lows an encoder-decoder architecture with atten-tion, and introduce elements from the character-based neural language model (Kim et al, 2016).The translation unit continues to be the word, andwe continue using word embeddings related toeach word as an input vector to the bidirectionalrecurrent neural network (attention-based mecha-nism).
The difference is that now the embeddingsof each word are no longer an independent vec-tor, but are computed from the characters of thecorresponding word.
The system architecture haschanged in that we are using a convolutional neu-ral network (CNN) and a highway network overcharacters before the attention-based mechanismof the encoder.
This is a significant differencefrom previous work (Sennrich et al, 2015) whichuses the neural MT architecture from (Bahdanauet al, 2015) without modification to deal with sub-word units (but not including unigram characters).Subword-based representations have alreadybeen explored in Natural Language Process-ing (NLP), e.g.
for POS tagging (Santos andZadrozny, 2014), name entity recognition (San-tos and aes, 2015), parsing (Ballesteros et al,2015), normalization (Chrupala, 2014) or learningword representations (Botha and Blunsom, 2014;Chen et al, 2015).
These previous works showdifferent advantages of using character-level in-formation.
In our case, with the new character-357based neural MT architecture, we take advantageof intra-word information, which is proven to beextremely useful in other NLP applications (San-tos and Zadrozny, 2014; Ling et al, 2015a), es-pecially when dealing with morphologically richlanguages.
When using the character-based sourceword embeddings in MT, there ceases to be un-known words in the source input, while the sizeof the target vocabulary remains unchanged.
Al-though the target vocabulary continues with thesame limitation as in the standard neural MT sys-tem, the fact that there are no unknown wordsin the source helps to reduce the number of un-knowns in the target.
Moreover, the remaining un-known target words can now be more successfullyreplaced with the corresponding source-alignedwords.
As a consequence, we obtain a significantimprovement in terms of translation quality (up to3 BLEU points).The rest of the paper is organized as follows.Section 2 briefly explains the architecture of theneural MT that we are using as a baseline sys-tem.
Section 3 describes the changes introduced inthe baseline architecture in order to use character-based embeddings instead of the standard lookup-based word representations.
Section 4 reports theexperimental framework and the results obtainedin the German-English WMT task.
Finally, sec-tion 5 concludes with the contributions of the pa-per and further work.2 Neural Machine TranslationNeural MT uses a neural network approach tocompute the conditional probability of the tar-get sentence given the source sentence (Cho etal., 2014; Bahdanau et al, 2015).
The approachused in this work (Bahdanau et al, 2015) fol-lows the encoder-decoder architecture.First, theencoder reads the source sentence s = (s1, ..sI)and encodes it into a sequence of hidden statesh = (h1, ..hI).
Then, the decoder generates acorresponding translation t = t1, ..., tJbased onthe encoded sequence of hidden states h. Both en-coder and decoder are jointly trained to maximizethe conditional log-probability of the correct trans-lation.This baseline autoencoder architecture is im-proved with a attention-based mechanism (Bah-danau et al, 2015), in which the encoder usesa bi-directional gated recurrent unit (GRU).
ThisGRU allows for a better performance with longsentences.
The decoder also becomes a GRU andeach word tjis predicted based on a recurrent hid-den state, the previously predicted word tj?1, anda context vector.
This context vector is obtainedfrom the weighted sum of the annotations hk,which in turn, is computed through an alignmentmodel ?jk(a feedforward neural network).
Thisneural MT approach has achieved competitive re-sults against the standard phrase-based system inthe WMT 2015 evaluation (Jean et al, 2015).3 Character-based Machine TranslationWord embeddings have been shown to boost theperformance in many NLP tasks, including ma-chine translation.
However, the standard lookup-based embeddings are limited to a finite-size vo-cabulary for both computational and sparsity rea-sons.
Moreover, the orthographic representationof the words is completely ignored.
The standardlearning process is blind to the presence of stems,prefixes, suffixes and any other kind of affixes inwords.As a solution to those drawbacks, new alterna-tive character-based word embeddings have beenrecently proposed for tasks such as language mod-eling (Kim et al, 2016; Ling et al, 2015a), pars-ing (Ballesteros et al, 2015) or POS tagging (Linget al, 2015a; Santos and Zadrozny, 2014).
Evenin MT (Ling et al, 2015b), where authors use thecharacter transformation presented in (Ballesteroset al, 2015; Ling et al, 2015a) both in the sourceand target.
However, they do not seem to get clearimprovements.
Recently, (Luong and Manning,2016) propose a combination of word and char-acters in neural MT.For our experiments in neural MT, we selectedthe best character-based embedding architectureproposed by Kim et al (Kim et al, 2016) for lan-guage modeling.
As the Figure 1 shows, the com-putation of the representation of each word startswith a character-based embedding layer that as-sociates each word (sequence of characters) witha sequence of vectors.
This sequence of vectorsis then processed with a set of 1D convolutionfilters of different lengths (from 1 to 7 charac-ters) followed with a max pooling layer.
For eachconvolutional filter, we keep only the output withthe maximum value.
The concatenation of thesemax values already provides us with a representa-tion of each word as a vector with a fixed lengthequal to the total number of convolutional ker-358nels.
However, the addition of two highway layerswas shown to improve the quality of the languagemodel in (Kim et al, 2016) so we also kept theseadditional layers in our case.
The output of thesecond Highway layer will give us the final vec-tor representation of each source word, replacingthe standard source word embedding in the neuralmachine translation system.! "
# $ % & ' ( )*)+,)$-)./0.-1&"&-2)".)3')!!#$456,(2#7().-/$8/(,2#/$.0#(2)"5./0.!#00)")$2.()$41256&9./,27,2./0.
)&-1.0#(2)":#41;&<.=&<)":#41;&<.=&<)":#41;&<.=&<)"5Figure 1: Character-based word embeddingIn the target size we are still limited in vocabu-lary by the softmax layer at the output of the net-work and we kept the standard target word em-beddings in our experiments.
However, the resultsseem to show that the affix-aware representation ofthe source words has a positive influence on all thecomponents of the network.
The global optimiza-tion of the integrated model forces the translationmodel and the internal vector representation of thetarget words to follow the affix-aware codificationof the source words.4 Experimental frameworkThis section reports the data used, its preprocess-ing, baseline details and results with the enhancedcharacter-based neural MT system.4.1 DataWe used the German-English WMT data1includ-ing the EPPS, NEWS and Commoncrawl.
Pre-processing consisted of tokenizing, truecasing,normalizing punctuation and filtering sentenceswith more than 5% of their words in a language1http://www.statmt.org/wmt15/translation-task.htmlother than German or English.
Statistics are shownin Table 1.L Set S W V OOVDe Train 3.5M 77.7M 1.6M -Dev 3k 63.1k 13.6k 1.7kTest 2.2k 44.1k 9.8k 1.3kEn Train 3.5M 81.2M 0.8M -Dev 3k 67.6k 10.1k 0.8kTest 2.2k 46.8k 7.8k 0.6kTable 1: Corpus details.
Number of sentences (S),words (W), vocabulary (V) and out-of-vocabulary-words (OOV) per set and language (L).
M standingfor millions, k standing for thousands.4.2 Baseline systemsThe phrase-based system was built using Moses(Koehn et al, 2007), with standard parameterssuch as grow-final-diag for alignment, Good-Turing smoothing of the relative frequencies, 5-gram language modeling using Kneser-Ney dis-counting, and lexicalized reordering, among oth-ers.
The neural-based system was built using thesoftware from DL4MT2available in github.
Wegenerally used settings from previous work (Jeanet al, 2015): networks have an embedding of 620and a dimension of 1024, a batch size of 32, andno dropout.
We used a vocabulary size of 90 thou-sand words in German-English.
Also, as proposedin (Jean et al, 2015) we replaced unknown words(UNKs) with the corresponding source word usingthe alignment information.4.3 ResultsTable 3 shows the BLEU results for the baselinesystems (including phrase and neural-based, NN)and the character-based neural MT (CHAR).
Wealso include the results for the CHAR and NNsystems with post-processing of unknown words,which consists in replacing the UNKs with the cor-responding source word (+Src), as suggested in(Jean et al, 2015).
BLEU results improve by al-most 1.5 points in German-to-English and by morethan 3 points in English-to-German.
The reductionin the number of unknown words (after postpro-cessing) goes from 1491 (NN) to 1260 (CHAR)in the direction from German-to-English and from3148 to 2640 in the opposite direction.
Note the2http://dl4mt.computing.dcu.ie/3591 SRC Berichten zufolge hofft Indien darber hinaus auf einen Vertrag zur Verteidigungszusammenarbeit zwischen den beiden Nationen .Phrase reportedly hopes India , in addition to a contract for the defence cooperation between the two nations .NN according to reports , India also hopes to establish a contract for the UNK between the two nations .CHAR according to reports , India hopes to see a Treaty of Defence Cooperation between the two nations .REF India is also reportedly hoping for a deal on defence collaboration between the two nations .2 SRC der durchtrainierte Mainzer sagt von sich , dass er ein ?
ambitionierter Rennradler ?
ist .Phrase the will of Mainz says that he a more ambitious .NN the UNK Mainz says that he is a ?
ambitious , .
?CHAR the UNK in Mainz says that he is a ?
ambitious racer ?
.REF the well-conditioned man from Mainz said he was an ?
ambitious racing cyclist .
?3 SRC die GDL habe jedoch nicht gesagt , wo sie streiken wolle , so dass es schwer sei , die Folgen konkret vorherzusehen .Phrase the GDL have , however , not to say , where they strike , so that it is difficult to predict the consequences of concrete .NN however , the UNK did not tell which they wanted to UNK , so it is difficult to predict the consequences .CHAR however , the UNK did not say where they wanted to strike , so it is difficult to predict the consequences .REF the GDL have not said , however , where they will strike , making it difficult to predict exactly what the consequences will be .4 SRC die Premierminister Indiens und Japans trafen sich in Tokio .Phrase the Prime Minister of India and Japan in Tokyo .NN the Prime Minister of India and Japan met in TokyoCHAR the Prime Ministers of India and Japan met in TokyoREF India and Japan prime ministers meet in Tokyo5 SRC wo die Beamten es aus den Augen verloren .Phrase where the officials lost sight ofNN where the officials lost it out of the eyesCHAR where officials lose sight of itREF causing the officers to lose sight of itTable 2: Translation examples.De->En En->DePhrase 20.99 17.04NN 18.83 16.47NN+Src 20.64 17.15CHAR 21.40 19.53CHAR+Src 22.10 20.22Table 3: De-En BLEU results.number of out-of-vocabulary words of the test setis shown in Table 1.The character-based embedding has an impactin learning a better translation model at variouslevels, which seems to include better alignment,reordering, morphological generation and disam-biguation.
Table 2 shows some examples of thekind of improvements that the character-basedneural MT system is capable of achieving com-pared to baseline systems.
Examples 1 and 2 showhow the reduction of source unknowns improvesthe adequacy of the translation.
Examples 3 and 4show how the character-based approach is able tohandle morphological variations.
Finally, example5 shows an appropriate semantic disambiguation.5 ConclusionsNeural MT offers a new perspective in the wayMT is managed.
Its main advantages when com-pared with previous approaches, e.g.
statisticalphrase-based, are that the translation is faced withtrainable features and optimized in an end-to-endscheme.
However, there still remain many chal-lenges left to solve, such as dealing with the limi-tation in vocabulary size.In this paper we have proposed a modification tothe standard encoder/decoder neural MT architec-ture to use unlimited-vocabulary character-basedsource word embeddings.
The improvement inBLEU is about 1.5 points in German-to-Englishand more than 3 points in English-to-German.As further work, we are currently studying dif-ferent alternatives (Chung et al, 2016) to extendthe character-based approach to the target side ofthe neural MT system.AcknowledgementsThis work is supported by the 7th Framework Pro-gram of the European Commission through the In-ternational Outgoing Fellowship Marie Curie Ac-tion (IMTraP-2011-29951) and also by the Span-ish Ministerio de Econom?
?a y Competitividad andEuropean Regional Developmend Fund, contractTEC2015-69266-P (MINECO/FEDER, UE).ReferencesDimitry Bahdanau, Kyunghyun Cho, and YoshuaBengio.
2015.
Neural machine translation byjointly learning to align and translate.
CoRR,abs/1409.0473.Miguel Ballesteros, Chris Dyer, and Noah A. Smith.2015.
Improved transition-based parsing by model-ing characters instead of words with lstms.
In Pro-ceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing, pages 349?359, Lisbon, Portugal, September.
Association forComputational Linguistics.360Jan A. Botha and Phil Blunsom.
2014.
CompositionalMorphology for Word Representations and Lan-guage Modelling.
In Proceedings of the 31st Inter-national Conference on Machine Learning (ICML),Beijing, China, jun.
*Award for best application pa-per*.Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun,and Huan-Bo Luan.
2015.
Joint learning of char-acter and word embeddings.
In Qiang Yang andMichael Wooldridge, editors, IJCAI, pages 1236?1242.
AAAI Press.Kyunghyun Cho, Bart van van Merrienboer, DzmitryBahdanau, and Yoshua Bengio.
2014.
On theproperties of neural machine translation: Encoder?decoder approaches.
In Proc.
of the Eighth Work-shop on Syntax, Semantics and Structure in Statisti-cal Translation, Doha.Grzegorz Chrupala.
2014.
Normalizing tweets withedit scripts and recurrent neural embeddings.
InProceedings of the 52nd Annual Meeting of the As-sociation for Computational Linguistics, ACL 2014,June 22-27, 2014, Baltimore, MD, USA, Volume 2:Short Papers, pages 680?686.Junyoung Chung, Kyunghyun Cho, and Yoshua Ben-gio.
2016.
A character-level decoder without ex-plicit segmentation for neural machine translation.CoRR, abs/1603.06147.Desmond Elliott, Stella Frank, and Eva Hasler.
2015.Multi-language image description with neural se-quence models.
CoRR, abs/1510.04709.Sebastien Jean, Orhan Firat, Kyunghun Cho, RolandMemisevic, and Yoshua Bengio.
2015.
Montrealneural machine translation systems for wmt15.
InProc.
of the 10th Workshop on Statistical MachineTranslation, Lisbon.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentcontinuous translation models.
In Proc.
of the Con-ference on Empirical Methods in Natural LanguageProcessing, Seattle.Yoon Kim, Yacine Jernite, David Sontag, and Alexan-der M. Rush.
2016.
Character-aware neural lan-guage models.
In Proceedings of the 30th AAAIConference on Artificial Intelligence (AAAI?16).Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Proc.of the 41th Annual Meeting of the Association forComputational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolasBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open Source Toolkit for Statistical MachineTranslation.
In Proc.
of the 45th Annual Meetingof the Association for Computational Linguistics,pages 177?180.Wang Ling, Chris Dyer, Alan W Black, Isabel Tran-coso, Ramon Fermandez, Silvio Amir, Luis Marujo,and Tiago Luis.
2015a.
Finding function in form:Compositional character models for open vocabu-lary word representation.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing, pages 1520?1530, Lisbon, Portu-gal, September.
Association for Computational Lin-guistics.Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W.Black.
2015b.
Character-based neural machinetranslation.
CoRR, abs/1511.04586.Min-Thang Luong and Cristopher D. Manning.
2016.Character-based neural machine translation.
CoRR,abs/1511.04586.Cicero D. Santos and Victor Guimar aes.
2015.
Boost-ing named entity recognition with neural characterembeddings.
In Proceedings of the Fifth Named En-tity Workshop, pages 25?33, Beijing, China, July.Association for Computational Linguistics.Cicero D. Santos and Bianca Zadrozny.
2014.Learning character-level representations for part-of-speech tagging.
In Tony Jebara and Eric P. Xing, ed-itors, Proceedings of the 31st International Confer-ence on Machine Learning (ICML-14), pages 1818?1826.Rico Sennrich, Barry Haddow, and Alexandra Birch.2015.
Neural machine translation of rare words withsubword units.
CoRR, abs/1508.07909.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to sequence learning with neural net-works.
In Z. Ghahramani, M. Welling, C. Cortes,N.
D. Lawrence, and K. Q. Weinberger, editors, Ad-vances in Neural Information Processing Systems27, pages 3104?3112.
Curran Associates, Inc.361
