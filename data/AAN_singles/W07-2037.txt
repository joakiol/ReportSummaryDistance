Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 177?182,Prague, June 2007. c?2007 Association for Computational LinguisticsI2R: Three Systems for Word Sense Discrimination, Chinese Word SenseDisambiguation, and English Word Sense DisambiguationZheng-Yu Niu, Dong-Hong JiInstitute for Infocomm Research21 Heng Mui Keng Terrace119613 Singaporeniu zy@hotmail.comdhji@i2r.a-star.edu.sgChew-Lim TanDepartment of Computer ScienceNational University of Singapore3 Science Drive 2117543 Singaporetancl@comp.nus.edu.sgAbstractThis paper describes the implementationof our three systems at SemEval-2007, fortask 2 (word sense discrimination), task 5(Chinese word sense disambiguation), andthe first subtask in task 17 (English wordsense disambiguation).
For task 2, we ap-plied a cluster validation method to esti-mate the number of senses of a target wordin untagged data, and then grouped the in-stances of this target word into the esti-mated number of clusters.
For both task 5and task 17, We used the label propagationalgorithm as the classifier for sense disam-biguation.
Our system at task 2 achieved63.9% F-score under unsupervised evalua-tion, and 71.9% supervised recall with su-pervised evaluation.
For task 5, our sys-tem obtained 71.2% micro-average preci-sion and 74.7% macro-average precision.For the lexical sample subtask for task17, our system achieved 86.4% coarse-grained precision and recall.1 IntroductionSemEval-2007 launches totally 18 tasks for evalua-tion exercise, covering word sense disambiguation,word sense discrimination, semantic role labeling,and sense disambiguation for information retrieval,and other topics in NLP.
We participated three tasksin SemEval-2007, which are task 2 (EvaluatingWord Sense Induction and Discrimination Systems),task 5 (Multilingual Chinese-English Lexical Sam-ple Task) and the first subtask at task 17 (EnglishLexical Sample, English Semantic Role Labelingand English All-Words Tasks).The goal for SemEval-2007 task 2 (Evaluat-ing Word Sense Induction and Discrimination Sys-tems)(Agirre and Soroa, 2007) is to automaticallydiscriminate the senses of English target words bythe use of only untagged data.
Here we address thisword sense discrimination problem by (1) estimat-ing the number of word senses of a target word inuntagged data using a stability criterion, and then (2)grouping the instances of this target word into theestimated number of clusters according to the simi-larity of contexts of the instances.
No sense-taggeddata is used to help the clustering process.The goal of task 5 (Chinese Word Sense Disam-biguation) is to create a framework for the evaluationof word sense disambiguation in Chinese-Englishmachine translation systems.
Each participates ofthis task will be provided with sense tagged train-ing data and untagged test data for 40 Chinese pol-ysemous words.
The ?sense tags?
for the ambigu-ous Chinese target words are given in the form oftheir English translations.
Here we used a semi-supervised classification algorithm (label propaga-tion algorithm) (Niu, et al, 2005) to address thisChinese word sense disambiguation problem.The lexical sample subtask of task 17 (EnglishWord Sense Disambiguation) provides sense-taggedtraining data and untagged test data for 35 nouns and65 verbs.
This data includes, for each target word:OntoNotes sense tags (these are groupings of Word-Net senses that are more coarse-grained than tradi-177tional WN entries), as well as the sense inventory forthese lemmas.
Here we used only the training datasupplied in this subtask for sense disambiguation intest set.
The label propagation algorithm (Niu, et al,2005) was used to perform sense disambiguation bythe use of both training data and test data.This paper will be organized as follows.
First, wewill provide the feature set used for task 2, task 5and task 17 in section 2.
Secondly, we will presentthe word sense discrimination method used for task2 in section 3.
Then, we will give the label propa-gation algorithm for task 5 and task 17 in section 4.Section 5 will provide the description of data sets attask 2, task 5 and task 17.
Then, we will present theexperimental results of our systems at the three tasksin section 6.
Finally we will give a conclusion of ourwork in section 7.2 Feature SetIn task 2, task 5 and task 17, we used three types offeatures to capture contextual information: part-of-speech of neighboring words (no more than three-word distance) with position information, unorderedsingle words in topical context (all the contextualsentences), and local collocations (including 11 col-locations).
The feature set used here is as same asthe feature set used in (Lee and Ng, 2002) exceptthat we did not use syntactic relations.3 The Word Sense Discrimination Methodfor Task 2Word sense discrimination is to automatically dis-criminate the senses of target words by the use ofonly untagged data.
So we can employ clusteringalgorithms to address this problem.
Another prob-lem is that there is no sense inventories for targetwords.
So the clustering algorithms should have theability to automatically estimate the sense numberof a target word.Here we used the sequential Information Bottle-neck algorithm (sIB) (Slonim, et al, 2002) to esti-mate cluster structure, which measures the similarityof contexts of instances of target words according tothe similarity of their contextual feature conditionaldistribution.
But sIB requires the number of clus-ters as input.
So we used a cluster validation methodto automatically estimate the sense number of a tar-Table 1: Sense number estimation procedure forword sense discrimination.1 Set lower bound Kmin and upper bound Kmaxfor sense number k;2 Set k = Kmin;3 Conduct the cluster validation processpresented in Table 2 to evaluate the merit of k;4 Record k and the value of Mk;5 Set k = k + 1.
If k ?
Kmax, go to step 3,otherwise go to step 6;6 Choose the value k?
that maximizes Mk,where k?
is the estimated sense number.get word before clustering analysis.
Cluster valida-tion (or stability based approach)is a commonly usedmethod to the problem of model order identification(or cluster number estimation) (Lange, et al, 2002;Levine and Domany, 2001).
The assumption of thismethod is that if the model order is identical with thetrue value, then the cluster structure estimated fromthe data is stable against resampling, otherwise, it ismore likely to be the artifact of sampled data.3.1 The Sense Number Estimation ProcedureTable 1 presents the sense number estimation pro-cedure.
Kmin was set as 2, and Kmax was set as 5 inour system.
The evaluation function Mk (describedin Table 2) is relevant with the sense number k. qis set as 20 here.
Clustering solution which is stableagainst resampling will give rise to a local optimumof Mk, which indicates the true value of sense num-ber.
In the cluster validation procedure, we used thesIB algorithm to perform clustering analysis (de-scribed in section 3.2).The function M(C?, C) in Table 2 is given by(Levine and Domany, 2001):M(C?, C) =?i,j 1{C?i,j = Ci,j = 1, di ?
D?, dj ?
D?
}?i,j 1{Ci,j = 1, di ?
D?, dj ?
D?
},(1)where D?
is a subset with size ?|D| sampled fromfull data set D, C and C?
are |D|?
|D| connectivitymatrixes based on clustering solutions computed onD and D?
respectively, and 0 ?
?
?
1.
The con-nectivity matrix C is defined as: Ci,j = 1 if di anddj belong to the same cluster, otherwise Ci,j = 0.C?
is calculated in the same way.
?
is set as 0.90 inthis paper.178Table 2: The cluster validation method for evalua-tion of values of sense number k.Function: Cluster Validation(k, D, q)Input: cluster number k, data set D,and sampling frequency q;Output: the score of the merit of k;1 Perform clustering analysis using sIB ondata set D with k as input;2 Construct connectivity matrix Ck based onabove clustering solution on D;3 Use a random predictor ?k to assignuniformly drawn labels to instances in D;4 Construct connectivity matrix C?kusing above clustering solution on D;5 For ?
= 1 to q do5.1 Randomly sample a subset (D?)
with size?|D| from D, 0 ?
?
?
1;5.2 Perform clustering analysis using sIB on(D?)
with k as input;5.3 Construct connectivity matrix C?k usingabove clustering solution on (D?
);5.4 Use ?k to assign uniformly drawn labelsto instances in (D?
);5.5 Construct connectivity matrix C?
?kusing above clustering solution on (D?
);Endfor6 Evaluate the merit of k using followingobjective function:Mk = 1q??
M(C?k , Ck) ?
1q??
M(C?
?k , C?k),where M(C?, C) is given by equation (1);7 Return Mk;M(C?, C) measures the proportion of documentpairs in each cluster computed on D that are also as-signed into the same cluster by clustering solutionon D?.
Clearly, 0 ?
M ?
1.
Intuitively, if clus-ter number k is identical with the true value, thenclustering results on different subsets generated bysampling should be similar with that on full data set,which gives rise to a local optimum of M(C?, C).In our algorithm, we normalize M(C?F,k, CF,k)using the equation in step 6 of Table 2, whichmakes our objective function different from the fig-ure of merit (equation ( 1)) proposed in (Levineand Domany, 2001).
The reason to normalizeM(C?F,k, CF,k) is that M(C?F,k, CF,k) tends to de-crease when increasing the value of k. Therefore foravoiding the bias that smaller value of k is to be se-lected as cluster number, we use the cluster validityof a random predictor to normalize M(C?F,k, CF,k).3.2 The sIB Clustering AlgorithmHere we used the sIB algorithm (Slonim, et al,2002) to estimate cluster structure, which measuresthe similarity of contexts of instances according tothe similarity of their feature conditional distribu-tion.
sIB is a simplified ?hard?
variant of informa-tion bottleneck method (Tishby, et al, 1999).Let d represent a document, and w represent a fea-ture word, d ?
D, w ?
F .
Given the joint distri-bution p(d,w), the document clustering problem isformulated as looking for a compact representationT for D, which preserves as much information aspossible about F .
T is the document clustering so-lution.
For solving this optimization problem, sIBalgorithm was proposed in (Slonim, et al, 2002),which found a local maximum of I(T, F ) by: givenan initial partition T , iteratively drawing a d ?
Dout of its cluster t(d), t ?
T , and merging it intotnew such that tnew = argmaxt?Td(d, t).
d(d, t) isthe change of I(T, F ) due to merging d into clustertnew, which is given byd(d, t) = (p(d) + p(t))JS(p(w|d), p(w|t)).
(2)JS(p, q) is the Jensen-Shannon divergence, whichis defined asJS(p, q) = pipDKL(p?p) + piqDKL(q?p), (3)DKL(p?p) =?yplog pp, (4)DKL(q?p) =?yqlog qp, (5){p, q} ?
{p(w|d), p(w|t)}, (6){pip, piq} ?
{p(d)p(d) + p(t) ,p(t)p(d) + p(t)}, (7)p = pipp(w|d) + piqp(w|t).
(8)1794 The Label Propagation Algorithm forTask 5 and Task 17In the label propagation algorithm (LP) (Zhu andGhahramani, 2002), label information of any ver-tex in a graph is propagated to nearby verticesthrough weighted edges until a global stable stageis achieved.
Larger edge weights allow labels totravel through easier.
Thus the closer the examples,more likely they have similar labels (the global con-sistency assumption).In label propagation process, the soft label of eachinitial labeled example is clamped in each iterationto replenish label sources from these labeled data.Thus the labeled data act like sources to push out la-bels through unlabeled data.
With this push from la-beled examples, the class boundaries will be pushedthrough edges with large weights and settle in gapsalong edges with small weights.
If the data structurefits the classification goal, then LP algorithm can usethese unlabeled data to help learning classificationplane.Let Y 0 ?
Nn?c represent initial soft labels at-tached to vertices, where Y 0ij = 1 if yi is sj and 0otherwise.
Let Y 0L be the top l rows of Y 0 and Y 0Ube the remaining u rows.
Y 0L is consistent with thelabeling in labeled data, and the initialization of Y 0Ucan be arbitrary.Optimally we expect that the value of Wij acrossdifferent classes is as small as possible and the valueof Wij within same class is as large as possible.This will make label propagation to stay within sameclass.
In later experiments, we set ?
as the aver-age distance between labeled examples from differ-ent classes.Define n ?
n probability transition matrix Tij =P (j ?
i) = Wij?nk=1 Wkj, where Tij is the probabilityto jump from example xj to example xi.Compute the row-normalized matrix T by T ij =Tij/?nk=1 Tik.
This normalization is to maintainthe class probability interpretation of Y .Then LP algorithm is defined as follows:1.
Initially set t=0, where t is iteration index;2.
Propagate the label by Y t+1 = TY t;3.
Clamp labeled data by replacing the top l rowof Y t+1 with Y 0L .
Repeat from step 2 until Y t con-verges;4.
Assign xh(l + 1 ?
h ?
n) with a label sj?
,where j?
= argmaxjYhj .This algorithm has been shown to converge toa unique solution, which is Y?U = limt??
Y tU =(I ?
T uu)?1T ulY 0L (Zhu and Ghahramani, 2002).We can see that this solution can be obtained with-out iteration and the initialization of Y 0U is not im-portant, since Y 0U does not affect the estimation ofY?U .
I is u ?
u identity matrix.
T uu and T ul areacquired by splitting matrix T after the l-th row andthe l-th column into 4 sub-matrices.For task 5 and 17, we constructed connectedgraphs as follows: two instances u, v will be con-nected by an edge if u is among v?s k nearest neigh-bors, or if v is among u?s k nearest neighbors as mea-sured by cosine or JS distance measure.
k is set 10in our system implementation.5 Data Sets of Task 2, Task 5 and Task 17The test data for task 2 includes totally 27132 un-tagged instances for 100 ambiguous English words.There is no training data for task 2.There are 40 ambiguous Chinese words in task5.
The training data for this task consists of 2686instances, while the test data includes 935 instances.There are 100 ambiguous English words in thefirst subtask of task 17.
The training data for thistask consists of 22281 instances, while the test dataincludes 4851 instances.6 Experimental Results of Our Systems atTask 2, Task 5 and Task 17Table 3: The best/worst/average F-score of all thesystems at task 2 and the F-score of our system attask 2 for all target words, nouns and verbs with un-supervised evaluation.All words Nouns VerbsBest 78.7% 80.8% 76.3%Worst 56.1% 65.8% 45.1%Average 65.4% 69.0% 61.4%Our system 63.9% 68.0% 59.3%Table 3 lists the best/worst/average F-score of allthe systems at task 2 and the F-score of our systemat task 2 for all target words, nouns and verbs with180Table 4: The best/worst/average supervised recall ofall the systems at task 2 and the supervised recall ofour system at task 2 for all target words, nouns andverbs with supervised evaluation.All words Nouns VerbsBest 81.6% 86.8% 75.7%Worst 78.5% 81.4% 75.2%Average 79.6% 83.0% 75.7%Our system 81.6% 86.8% 75.7%Table 5: The best/worst/average micro-average pre-cision and macro-average precision of all the sys-tems at task 5 and the micro-average precision andmacro-average precision of our system at task 5.Micro-average Macro-averageBest 71.7% 74.9%Worst 33.7% 39.6%Average 58.5% 62.7%Our system 71.2% 74.7%unsupervised evaluation.
Our system obtained thefourth place among six systems with unsupervisedevaluation.
Table 4 shows the best/worst/averagesupervised recall of all the systems at task 2 and thesupervised recall of our system at task 2 for all tar-get words, nouns and verbs with supervised evalu-ation.
Our system is ranked as the first among sixsystems with supervised evaluation.
Table 7 liststhe estimated sense numbers by our system for allthe words at task 2.
The average of all the estimatedsense numbers is 3.1, while the average of all theground-truth sense numbers is 3.6 if we consider thesense inventories provided in task 17 as the answer.It seems that our estimated sense numbers are closeto the ground-truth ones.Table 5 provides the best/worst/average micro-average precision and macro-average precision of allthe systems at task 5 and the micro-average preci-sion and macro-average precision of our system attask 5.
Our system obtained the second place amongsix systems for task 5.Table 6 shows the best/worst/average coarse-grained score (precision) of all the systems the lexi-cal sample subtask of task 17 and the coarse-grainedscore (precision) of our system at the lexical sampleTable 6: The best/worst/average coarse-grainedscore (precision) of all the systems at the lexicalsample subtask of task 17 and the coarse-grainedscore (precision) of our system at the lexical sam-ple subtask of task 17.Coarse-grained score (precision)Best 88.7%Worst 52.1%Average 70.0%Our system 86.4%subtask of task 17.
The attempted rate of all the sys-tems is 100%.
So the precision value is equal to therecall value for all the systems.
Here we listed onlythe precision for the 13 systems at this subtask.
Oursystem is ranked as the third one among 13 systems.7 ConclusionIn this paper, we described the implementation ofour I2R systems that participated in task 2, task 5,and task 17 at SemEval-2007.
Our systems achieved63.9% F-score and 81.6% supervised recall for task2, 71.2% micro-average precision and 74.7% macro-average precision for task 5, and 86.4% coarse-grained precision and recall for the lexical samplesubtask of task 17.
The performance of our systemis very good under supervised evaluation.
It maybe explained by that our system has the ability tofind some minor senses so that it can outperformsthe baseline system that always uses the most fre-quent sense as the answer.ReferencesAgirre E. , & Soroa A.
2007.
SemEval-2007 Task 2:Evaluating Word Sense Induction and DiscriminationSystems.
Proceedings of SemEval-2007, Associationfor Computational Linguistics.Lange, T., Braun, M., Roth, V., & Buhmann, J. M. 2002.Stability-Based Model Selection.
Advances in NeuralInformation Processing Systems 15.Lee, Y.K., & Ng, H.T.
2002.
An Empirical Evalua-tion of Knowledge Sources and Learning Algorithmsfor Word Sense Disambiguation.
Proceedings of the2002 Conference on Empirical Methods in NaturalLanguage Processing, (pp.
41-48).181Levine, E., & Domany, E. 2001.
Resampling Method forUnsupervised Estimation of Cluster Validity.
NeuralComputation, Vol.
13, 2573?2593.Niu, Z.Y., Ji, D.H., & Tan, C.L.
2005.
Word SenseDisambiguation Using Label Propagation Based Semi-Supervised Learning.
Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguis-tics.Slonim, N., Friedman, N., & Tishby, N. 2002.
Un-supervised Document Classification Using SequentialInformation Maximization.
Proceedings of the 25thAnnual International ACM SIGIR Conference on Re-search and Development in Information Retrieval.Tishby, N., Pereira, F., & Bialek, W. (1999) The Infor-mation Bottleneck Method.
Proc.
of the 37th AllertonConference on Communication, Control and Comput-ing.Zhu, X.
& Ghahramani, Z.. 2002.
Learning from La-beled and Unlabeled Data with Label Propagation.CMU CALD tech report CMU-CALD-02-107.Table 7: The estimated sense numbers by our systemfor all the words at task 2.explain 2 move 3position 3 express 4buy 2 begin 2hope 3 prepare 3feel 5 policy 2hold 2 attempt 2work 5 recall 3people 4 find 2system 2 join 2bill 2 build 2hour 5 base 3value 4 management 2job 5 turn 4rush 2 kill 2ask 2 area 5approve 4 affect 4capital 4 keep 5purchase 2 improve 2propose 2 do 2see 3 drug 5president 3 come 5power 3 disclose 4effect 2 avoid 3part 5 plant 2exchange 4 share 2state 2 carrier 2care 5 complete 2promise 3 maintain 3estimate 2 development 4rate 2 space 5say 2 raise 3remove 5 future 3grant 4 network 3remember 3 announce 5cause 2 start 3point 5 order 2occur 4 defense 5authority 3 set 3regard 2 chance 2go 3 produce 2allow 4 negotiate 2describe 2 enjoy 4prove 3 exist 4claim 4 replace 3fix 2 examine 3end 5 lead 3receive 3 source 2complain 3 report 2need 2 believe 2condition 2 contribute 3182
