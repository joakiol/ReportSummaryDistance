Selecting the ?Right?
Number of SensesBased on Clustering Criterion FunctionsTed Pedersen and Anagha KulkarniDepartment of Computer ScienceUniversity of Minnesota, DuluthDuluth, MN 55812 USA{tpederse,kulka020}@d.umn.eduhttp://senseclusters.sourceforge.netAbstractThis paper describes an unsupervisedknowledge?lean methodology for auto-matically determining the number ofsenses in which an ambiguous word isused in a large corpus.
It is based on theuse of global criterion functions that assessthe quality of a clustering solution.1 IntroductionThe goal of word sense discrimination is to clusterthe occurrences of a word in context based on itsunderlying meaning.
This is often approached as aproblem in unsupervised learning, where the onlyinformation available is a large corpus of text (e.g.,(Pedersen and Bruce, 1997), (Schu?tze, 1998), (Pu-randare and Pedersen, 2004)).
These methods usu-ally require that the number of clusters to be dis-covered (k) be specified ahead of time.
However,in most realistic settings, the value of k is unknownto the user.Word sense discrimination seeks to cluster Ncontexts, each of which contain a particular tar-get word, into k clusters, where we would likethe value of k to be automatically selected.
Eachcontext consists of approximately a paragraph ofsurrounding text, where the word to be discrimi-nated (the target word) is found approximately inthe middle of the context.
We present a methodol-ogy that automatically selects an appropriate valuefor k. Our strategy is to perform clustering for suc-cessive values of k, and evaluate the resulting solu-tions with a criterion function.
We select the valueof k that is immediately prior to the point at whichclustering does not improve significantly.Clustering methods are typically either parti-tional or agglomerative.
The main difference isthat agglomerative methods start with 1 or N clus-ters and then iteratively arrive at a pre?specifiednumber (k) of clusters, while partitional methodsstart by randomly dividing the contexts into k clus-ters and then iteratively rearranging the membersof the k clusters until the selected criterion func-tion is maximized.
In this work we have used K-means clustering, which is a partitional method,and the H2 criterion function, which is the ratioof within cluster similarity to between cluster sim-ilarity.
However, our approach can be used withany clustering algorithm and global criterion func-tion, meaning that the criterion function should ar-rive at a single value that assesses the quality of theclustering for each value of k under consideration.2 MethodologyIn word sense discrimination, the number of con-texts (N) to cluster is usually very large, and con-sidering all possible values of k from 1...N wouldbe inefficient.
As the value of k increases, the cri-terion function will reach a plateau, indicating thatdividing the contexts into more and more clustersdoes not improve the quality of the solution.
Thus,we identify an upper bound to k that we refer to asdeltaK by finding the point at which the criterionfunction only changes to a small degree as k in-creases.According to the H2 criterion function, thehigher its ratio of within cluster similarity to be-tween cluster similarity, the better the clustering.A large value indicates that the clusters have highinternal similarity, and are clearly separated fromeach other.
Intuitively then, one solution to select-ing k might be to examine the trend of H2 scores,and look for the smallest k that results in a nearlymaximum H2 value.However, a graph of H2 values for a clustering111of the 4 sense verb serve as shown in Figure 1 (top)reveals the difficulties of such an approach.
Thereis a gradual curve in this graph and the maximumvalue (plateau) is not reached until k values greaterthan 100.We have developed three methods that take asinput the H2 values generated from 1...deltaKand automatically determine the ?right?
value ofk, based on finding when the changes in H2 as kincreases are no longer significant.2.1 PK1The PK1 measure is based on (Mojena, 1977),which finds clustering solutions for all values ofk from 1..N , and then determines the mean andstandard deviation of the criterion function.
Then,a score is computed for each value of k by sub-tracting the mean from the criterion function, anddividing by the standard deviation.
We adapt thistechnique by using the H2 criterion function, andlimit k from 1...deltaK:PK1(k) = H2(k)?mean(H2[1...deltaK])std(H2[1...deltaK])(1)To select a value of k, a threshold must be set.Then, as soon as PK1(k) exceeds this threshold,k-1 is selected as the appropriate number of clus-ters.
We have considered setting this threshold us-ing the normal distribution based on interpretingPK1 as a z-score, although Mojena makes it clearthat he views this method as an ?operational rule?that is not based on any distributional assumptions.He suggests values of 2.75 to 3.50, but also statesthey would need to be adjusted for different datasets.
We have arrived at an empirically determinedvalue of -0.70, which coincides with the point inthe standard normal distribution where 75% of theprobability mass is associated with values greaterthan this.We observe that the distribution of PK1 scorestends to change with different data sets, making ithard to apply a single threshold.
The graph of thePK1 scores shown in Figure 1 illustrates the dif-ficulty - the slope of these scores is nearly linear,and as such the threshold (as shown by the hori-zontal line) is a somewhat arbitrary cutoff.2.2 PK2PK2 is similar to (Hartigan, 1975), in that bothtake the ratio of a criterion function at k and k-1,0.0010.0020.0030.0040.0050.0060.0070.0080.0090 50 100 150 200H2 vs ks4r-2.000-1.500-1.000-0.5000.0000.5001.0001.5002 3 4 5 6 7 8 9 1011121314151617PK1 vs krrrrrrrrrrrrrrr r240.9001.0001.1001.2001.3001.4001.5001.6001.7001.8001.9002 3 4 5 6 7 8 9 1011121314 151617PK2 vs krrrrrrr r r rr r r r rr240.9900.9951.0001.0051.0101.0151.0201.0251.0301.0351.0402 3 4 5 6 7 8 9 1011121314 151617PK3 vs krrrrrrrrrrrrrrr24Figure 1: Graphs of H2 (top) and PK 1-3 forserve: Actual number of senses (4) shown as trian-gle (all), predicted number as square (PK1-3), anddeltaK (17) shown as dot (H2) and upper limit ofk (PK1-3).112in order to assess the relative improvement whenincreasing the number of clusters.PK2(k) = H2(k)H2(k ?
1) (2)When this ratio approaches 1, the clustering hasreached a plateau, and increasing k will have nobenefit.
If PK2 is greater than 1, then an addi-tional cluster improves the solution and we shouldincrease k. We compute the standard deviation ofPK2 and use that to establish a boundary as towhat it means to be ?close enough?
to 1 to considerthat we have reached a plateau.
Thus, PK2 willselect k where PK2(k) is the closest to (but notless than) 1 + standard deviation(PK2[1...deltaK]).The graph of PK2 in Figure 1 shows an el-bow that is near the actual number of senses.
Thecritical region defined by the standard deviation isshaded, and note that PK2 selected the value ofk that was outside of (but closest to) that region.This is interpreted as being the last value of k thatresulted in a significant improvement in cluster-ing quality.
Note that here PK2 predicts 3 senses(square) while in fact there are 4 actual senses (tri-angle).
It is significant that the graph of PK2 pro-vides a clearer representation of the plateau thandoes that of H2.2.3 PK3PK3 utilizes three k values, in an attempt to finda point at which the criterion function increasesand then suddenly decreases.
Thus, for a givenvalue of k we compare its criterion function to thepreceding and following value of k:PK3(k) = 2?H2(k)H2(k ?
1) + H2(k + 1) (3)PK3 is close to 1 if the three H2 values forma line, meaning that they are either ascending, orthey are on the plateau.
However, our use ofdeltaK eliminates the plateau, so in our case valuesof 1 show that k is resulting in consistent improve-ments to clustering quality, and that we shouldcontinue.
When PK3 rises significantly above 1,we know that k+1 is not climbing as quickly, andwe have reached a point where additional clus-tering may not be helpful.
To select k we chosethe largest value of PK3(k) that is closest to (butstill greater than) the critical region defined by thestandard deviation of PK3.
This is the last pointwhere a significant increase in H2 was observed.Note that the graph of PK3 in Figure 1 shows thevalue of PK3 rising and falling dramatically inthe critical region, suggesting a need for additionalpoints to make it less localized.PK3 is similar in spirit to (Salvador and Chan,2004), which introduces the L measure.
This triesto find the point of maximum curvature in the cri-terion function graph, by fitting a pair of lines tothe curve (where the intersection of these lines rep-resents the selected k).3 Experimental ResultsWe conducted experiments with words that have 2,3, 4, and 6 actual senses.
We used three words thathad been manually sense tagged, including the 3sense adjective hard, the 4 sense verb serve, andthe 6 sense noun line.
We also created 19 nameconflations where sets of 2, 3, 4, and 6 names ofpersons, places, or organizations that are includedin the English GigaWord corpus (and that are typ-ically unambiguous) are replaced with a singlename to create pseudo or false ambiguities.
Forexample, we replaced all mentions of Bill Clintonand Tony Blair with a single name that can referto either of them.
In general the names we usedin these sets are fairly well known and occur hun-dreds or even thousands of times.We clustered each word or name using four dif-ferent configurations of our clustering approach,in order to determine how consistent the selectedvalue of k is in the face of changing feature setsand context representations.
The four configura-tions are first order feature vectors made up of un-igrams that occurred 5 or more times, with andwithout singular value decomposition, and thensecond order feature vectors based on bigrams thatoccurred 5 or more times and had a log?likelihoodscore of 3.841 or greater, with and without sin-gular value decomposition.
Details on these ap-proaches can be found in (Purandare and Peder-sen, 2004).Thus, in total there are 22 words to be discrim-inated, 7 with 2 senses, 6 words with 3 senses, 6with 4 senses, and 3 words with 6 senses.
Fourdifferent configurations of clustering are run foreach word, leading to a total of 88 experiments.The results are shown in Tables 1, 2, and 3.
Inthese tables, the actual numbers of senses are inthe columns, and the predicted number of sensesare in the rows.We see that the predicted value of PK1 agreed113Table 1: k Predicted by PK1 vs Actual k2 3 4 61 6 6 3 3 182 5 5 1 3 143 4 1 7 2 144 6 5 7 1 195 4 2 1 76 2 3 3 2 107 1 1 28 1 19 1 1 211 1 128 24 24 12 88Table 2: k Predicted by PK2 vs Actual k2 3 4 61 3 1 42 8 5 7 6 263 8 10 8 2 304 4 2 3 95 1 3 2 66 1 2 1 47 2 29 1 1 210 1 2 311 1 112 1 117 2 228 24 24 12 88with the actual value in 15 cases, whereas PK3agreed in 17 cases, and PK2 agreed in 22 cases.We observe that PK1 and PK3 also experiencedconsiderable confusion, in that their predictionswere in many cases several clusters off of the cor-rect value.
While PK2 made various mistakes,it was generally closer to the correct values, andhad fewer spurious responses (very large or verysmall predictions).
We note that the distributionof PK2?s predictions were most like those of theactual senses.4 ConclusionsThis paper shows how to use clustering criterionfunctions as a means of automatically selecting thenumber of senses k in an ambiguous word.
Wehave found that PK2, a ratio of the criterion func-tions for the current and previous value of k, isTable 3: k Predicted by PK3 vs Actual k2 3 4 61 3 4 1 1 92 13 9 12 4 383 4 3 4 4 154 2 2 1 1 65 2 1 1 1 56 1 2 3 67 1 1 1 39 1 110 1 111 2 212 1 113 1 128 24 24 12 88most effective, although there are many opportu-nities for future improvements to these techniques.5 AcknowledgmentsThis research is supported by a National ScienceFoundation Faculty Early CAREER DevelopmentAward (#0092784).
All of the experiments inthis paper were carried out with the SenseClusterspackage, which is freely available from the URLon the title page.ReferencesJ.
Hartigan.
1975.
Clustering Algorithms.
Wiley, NewYork.R.
Mojena.
1977.
Hierarchical grouping methods andstopping rules: An evaluation.
The Computer Jour-nal, 20(4):359?363.T.
Pedersen and R. Bruce.
1997.
Distinguishing wordsenses in untagged text.
In Proceedings of the Sec-ond Conference on Empirical Methods in NaturalLanguage Processing, pages 197?207, Providence,RI, August.A.
Purandare and T. Pedersen.
2004.
Word sense dis-crimination by clustering contexts in vector and sim-ilarity spaces.
In Proceedings of the Conference onComputational Natural Language Learning, pages41?48, Boston, MA.S.
Salvador and P. Chan.
2004.
Determining thenumber of clusters/segments in hierarchical cluster-ing/segmentation algorithms.
In Proceedings of the16th IEEE International Conference on Tools withAI, pages 576?584.H.
Schu?tze.
1998.
Automatic word sense discrimina-tion.
Computational Linguistics, 24(1):97?123.114
