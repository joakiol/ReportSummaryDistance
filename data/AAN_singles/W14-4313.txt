Proceedings of the SIGDIAL 2014 Conference, pages 89?97,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsBack to the Blocks World: Learning New Actions through SituatedHuman-Robot DialogueLanbo She1, Shaohua Yang1, Yu Cheng2, Yunyi Jia2, Joyce Y. Chai1, Ning Xi21Department of Computer Science and EngineeringMichigan State UniversityEast Lansing, MI 48824, USA{shelanbo, jchai, yangshao}@cse.msu.edu2Department of Electrical and Computer EngineeringMichigan State UniversityEast Lansing, MI 48824, USA{chengyu9, jiayunyi, xin}@egr.msu.eduAbstractThis paper describes an approach for arobotic arm to learn new actions throughdialogue in a simplified blocks world.
Inparticular, we have developed a three-tier action knowledge representation thaton one hand, supports the connection be-tween symbolic representations of lan-guage and continuous sensorimotor repre-sentations of the robot; and on the otherhand, supports the application of existingplanning algorithms to address novel situ-ations.
Our empirical studies have shownthat, based on this representation the robotwas able to learn and execute basic actionsin the blocks world.
When a human isengaged in a dialogue to teach the robotnew actions, step-by-step instructions leadto better learning performance comparedto one-shot instructions.1 IntroductionWhen a new generation of robots start to workside-by-side with their human partners in jointtasks (Christensen et al., 2010), they will oftenencounter new objects or are required to performnew actions.
It is important for the robots to au-tomatically learn new knowledge about the en-vironment and the tasks from their human part-ners.
To address this issue, this paper describesour recent work on action learning through dia-logue.
As a first step, we limit our investigation toa simple blocks world motivated by Terry Wino-grad?s early work (Winograd, 1972).
By usingan industrial robotic arm (SCHUNK) in this smallworld, we are interested in addressing the follow-ing questions.
First, human language has a dis-crete and symbolic representation, but the robotarm has a continuous representation for its move-ments.
Where should the connections between thesymbolic representation and the continuous repre-sentation take place so that human language canbe used to direct the robot?s movements?
Second,when the robot learns new tasks from its humanpartner, how to represent the acquired knowledgeeffectively so that it can be applied in novel situa-tions?
Third, during human-robot dialogue, whenthe robot fails to perform the expected actions dueto the lack of knowledge, how should the humanteach the robot new actions?
through step-by-stepinstructions or one-shot instructions?With these questions in mind, we have devel-oped a three-tier action knowledge representationfor the robotic arm.
The lower level connects tothe physical arm and defines the trajectories ofexecuting three atomic actions supported by thearm (i.e., open gripper, close gripper, move).
Themiddle level defines primitive operators such asOpen Grip, Close Grip and MoveTo in the fash-ion of the traditional AI planner (Fikes and Nils-son, 1971) and directly links to the lower level.The upper-level captures the high-level actions ac-quired by learning from the human.
These high-level actions are represented as the desired goalstates of the environment as a result of these ac-tions.
This three-tier representation allows therobot to automatically come up with a sequence oflower-level actions by applying existing planningalgorithms.Based on this representation, we implementeda dialogue system for action learning and furtherconducted an empirical study with human sub-jects.
In particular, we compared the dialogue89Figure 1: An example setup and dialogue.
Objectsare marked with labels only for the illustration pur-pose.based on the step-by-step instructions (i.e., onestep at a time and wait for the robot?s responseat each step before going to the next step) withthe one-shot instructions (i.e., give the instructionwith all steps at once).
Our empirical results haveshown that the three-tier knowledge representationcan capture the learned new action and apply itto novel situations.
Although the step-by-step in-structions resulted in a lengthier teaching processcompared to the one-shot instructions, they led tobetter learning performance for the robot.2 Related WorkOver forty years ago, Terry Winograd developedSHRDLU (Winograd, 1972) to demonstrate nat-ural language understanding using a simulatedblock-moving arm.
One aspect he did not address,but mentioned in his thesis (Winograd, 1972) asan important aspect, was learning new actionsthrough natural language.
Motivated by Wino-grad?s early work, we start our initial investigationon action learning in a physical blocks world andwith a physical robotic arm.
The blocks world isthe most famous domain used for planning in ar-tificial intelligence.
Thus it allows us to focus onmechanisms that, on one hand, connect symbolicrepresentations of language with lower-level con-tinuous sensorimotor representations of the robot;and on the other hand, support the use of the plan-ning algorithms to address novel situations.Most previous work on following human in-structions are based on supervised learning (Kol-lar et al., 2010; Tellex et al., 2011; Chen et al.,2010) or reinforcement learning (Branavan et al.,2012; Branavan et al., 2010).
These types of learn-ing may not be adequate in time-critical situationswhere only resources available to the robot is itshuman partners.
Thus it is desirable that humanscan engage in a natural language dialogue to teachrobots new skills.
Using natural language dialogueto learn new skills have been explored previouslyby (Allen et al., 2007) where an artificial agent wasdeveloped to acquire skills through natural lan-guage instructions (i.e., find restaurant).
But thiswork only grounds language to symbolic interfacewidgets on web pages.In the robotics community, previous work hasapplied learning by demonstration to teach robotsnew skills (Cakmak et al., 2010).
To potentiallyallow natural language instructions, previous workhas also explored connecting language with lower-level control systems (Kress-Gazit et al., 2008;Siskind, 1999; Matuszek et al., 2012).
Differentfrom these previous works, here we investigate theuse of natural language dialogue for learning ac-tions.
Previous work described in (Cantrell et al.,2012; Mohan et al., 2013) is most similar to ourwork.
Here we focus on both grounded learningand the use of planning for action learning.3 Dialogue SystemFigure 2: System ArchitectureWe developed a dialogue system to supportlearning new actions.
An example setup is shownin Figure 1, in which a SCHUNK arm is used tomanipulate blocks placed on a surface.
In H1,the human starts to ask the robot to stack the blueblock (i.e., B1) on top of the red block (i.e., R1).The robot does not understand the action ?stack?,so it asks the human for instructions.
Then the hu-90Figure 3: Example semantic representation andaction frame for the human utterance ?stack theblue block on the red block on your right.
?man provides detailed steps to accomplish this ac-tion (i.e., H2to H8) and also observes the robot?sresponse in each step.
Note that during this pro-cess, another unknown action (i.e., ?grab?
as inH2) is encountered.
The robot thus needs to learnthis action first.
The robot is able to keep trackof the dialogue structure so that actions and sub-actions can be learned accordingly.
Once the robotreceives a confirmation from the human that thecorresponding action is successfully performed(i.e., H6and H9), it acquires the new action andexplicitly represents it in its knowledge base forfuture use.
Instead of representing the acquiredknowledge as specific steps as illustrated by thehuman, the acquired action is represented by theexpected final state, which represents the changesof environment as a result of the action.
The newaction can be directly applied to novel situationsby applying planning algorithms.
Figure 2 showsthe system structure.
Next we explain main systemmodules in detail.Natural Languge Processing: Natural languageprocessing modules capture semantic informationfrom human language inputs.
In particular, theIntention Recognizer is used to recognizehuman intent (e.g., Command and Confirmation).The Semantic Processor, implemented asCombinatory Categorial Grammar (CCG)1, isused to generate semantic representation.
Currentsemantic information includes the actions (e.g.,stack) and their roles (e.g., Theme and Destina-tion).
The roles are further represented by objects?properties (Color, Location and Spatial Relation).An example semantic representation of ?H1: Stackthe blue block on the red block on your right.?
isshown in Figure 3.1We utilized OpenCCG, which could be found at:http://openccg.sourceforge.net/Perception Modules: Besides interpreting humanlanguage, the robot also continuously perceivesthe shared environment with its camera.
Ob-jects in video frames are recognized through vi-sion system (Collet et al., 2011), and further repre-sented as a Vision Graph (computed by VisionGraph Builder), which captures objects andtheir properties (in the numerical form).
The robotcan also access to its own internal status, such asthe location of the gripper and whether it?s openor closed.
Combining the robot?s state and en-vironment information, the Discrete StateBuilder can represent the entire environment asa conjunction of predicates, which will be laterused for action planning.Referential Grounding: To make the semanticrepresentation meaningful, it must be grounded tothe robot?s representation of perception.
We usethe graph-based approach for referential ground-ing as described in (Liu et al., 2012)(Liu et al.,2013).
Once the references are grounded, the se-mantic representation becomes a Grounded ActionFrame.
For example, as shown in Figure 3, ?theblue block?
refers to B1 and ?the red block on yourright?
refers to R1.Dialogue Manager: The Dialogue Manageris used to decide what dialog acts the systemshould perform give a situation.
It is composed by:a representation of dialogue state, a space of sys-tem activity and a dialogue policy.
The dialoguestatus is computed based on the human intention adialogue state captures (from semantic representa-tion) and the Grounded Action Frame.
Thecurrent space of system activities includes askingfor instructions, confirming, executing actions andupdating its action knowledge base with new ac-tions.
The dialogue policy stores the (dialoguestate, system activities) pairs.
During interaction,the Dialogue Manager will first identify thecurrent dialogue state and then apply the dialogueacts associated with that state as specified in thedialogue policy.Action Modules: The Action Modules areused to realize a high-level action from theGrounded Action Frame with the physi-cal arm and to learn new actions.
For re-alizing high-level actions, if the action in theGrounded Action Frame has a record inthe Action Knowledge, which keeps trackof all the knowledge about various actions, the91Discrete Planner will do planning to find asequence of primitive actions to achieve the high-level action.
Then these primitive actions will se-quentially go through Continuous Plannerand be translated to the trajectories of arm motors.By following these trajectories, the arm can per-form the high-level action.
For learning new ac-tions, these modules will calculate state changesbefore and after applying the action on the focusobject.
Such changes of the state are generalizedand stored as knowledge representation of the newaction.Response Generator: Currently, the ResponseGenerator is responsible for language genera-tion to realize the detail sentence.
In our currentinvestigation, the speech feedback is simple, so wejust used a set of pre-defined templates to do lan-guage generation.
And the parameters in the tem-plates will be realized during run time.4 Action Learning through DialogueTo realize the action learning functionality wehave developed a set of action related processesincluding an action knowledge base, action execu-tion processes and action learning processes.
Nextwe give detailed explanations.4.1 Action ModulesFigure 4: Execution example for ?Pick up the blueblock?.As shown in Figure 4, the action knowledgebase is a three-level structure, which consists ofHigh-level action Knowledge, Discrete Plannerand Continuous Planner.4.1.1 Continuous PlannerThis lowest level planner defines three primitiveactions: open (i.e., open gripper), close (i.e., closegripper) and move (i.e., move to the destination).Each primitive action is defined as a trajectorycomputing function, implemented as inverse kine-matics.
The outputs of these functions are controlcommands sendt to each arm motor to keep thearm following the trajectory.4.1.2 Discrete PlannerThe Discrete Planner is used to decompose ahigh-level action into a sequence of primitive ac-tions.
In our system, it is implemented as aSTRIPS (Fikes and Nilsson, 1971) planner, whichis defined as a quadruple ?P,O, I,G?:?
P: Set of predicates describing a domain.?
O: Set of operators.
Each is specified by a setof preconditions and effects.
An operator isapplicable only when its preconditions couldbe entailed in a state.?
I: Initial state, the starting point of a problem.?
G: Goal state, which should be achieved if theproblem is solved.In our system, O set includes Open Gripper,Close Gripper and 8 different kinds ofMoveTo (She et al., 2014).
And the P setconsists of two dimensions of the environment:?
Arm States: G Open/Close (i.e., whether thegripper is open or closed), G Full/Empty(i.e., whether the gripper has an object in it)and G At(x) (i.e, location of the arm).?
Object States: Top Uclr/Clr(o) (i.e., whetherthe block o has another block on its top),In/Out G(o) (i.e., whether o is within thegripper fingers or not) and On(o,x) (i.e., o issupported by x).The I and G are captured real-time during thedialogue interaction.4.1.3 High-level action KnowledgeThe high-level actions represent actions specifiedby the human partner.
They are modeled as de-sired goal states rather than the action sequencetaught by human.
For example, the ?Stack(x,y)?could be represented as ?On(x,y)?G Open?.
If thehuman specifies a high-level action out of the ac-tion knowledge base, the dialogue manager willverbally request for instructions to learn the action.92Figure 5: Learning process illustration.
After hearing the stack action, the robot cannot perform.
So thehuman gives step by step instruction.
When the instruction is completed, new knowledge of Grab(x) andStack(x,y) are learned in the high-level action knowledge base as the combination of the goal state of therobotic arm and the changes of the state for the involved objects.4.2 Action ExecutionGiven a Grounded Action Frame, it isfirstly checked with the high-level action knowl-edge base.
If the knowledge base has its record(e.g., the Pickup and ClearTop in Figure 4.
), a goalstate describing the action effect will be retrieved.This goal state, together with the initial state cap-tured from the current environment, will be sentto the Discrete Planner.
And, through au-tomated planning, a sequence of primitive actionswill be generated to complete the task, which canbe immediately executed by the arm.Take the ?Pick up?
action frame in Figure 4as an example.
By checking the grounded ac-tion frame with the high-level action knowledge,a related goal state (i.e., ?G Close?Top Clr(B1)?In G(B1)?On(B1,air)?)
can be retrieved.
Atthe same time, the Discrete Evn Buildertranslates the real world environment as a con-junction of predicates, which serves as the ini-tial state.
Given the combination of initial stateand goal state, the STRIPS planner can search fora path of primitive actions to solve the problem.For example, the PickUp(B1) in Figure 4 can besolved by Open Grip, MoveTo(B1), Close Gripand MoveTo(air).The primitive actions are executed by the con-tinuous planner and control process in the lowerrobotic system.
For the ?open?
and ?close?, theyare executed by controlling the position of thegripper fingers.
For the ?move?, a task-space tra-jectory is first planned based on the minimum-timemotion planning algorithm to move the robot end-effector from the current position to the final posi-tion.
A kinematic controller with redundancy res-olution (Zhang et al., 2012) is then used to gener-ate the joint movements for the robot to track theplanned trajectory.
Achieving the end of the tra-jectory indicates the action completion.4.3 Action LearningFigure 5 illustrates the system internal process ofacquiring action knowledge from the dialogue inFigure 1.At the beginning of the dialogue, the groundedaction frame Stack(B1, R1) captured from the firsthuman utterance is not in the action knowledge,so it will be pushed to the top of the unknown ac-tion stack as a new action waiting to be learned.The environment state at this point is calculated asshown in the figure.
Then the robot will verballyrequest instructions.
During the instruction, it?spossible that another unknown action Grab(B1) isreferred.
The same as the Stack action, it will bepushed to the top of unknown action stack waitingto be learned.In the next instruction, the human says ?Openyour gripper?.
This sentence can be translated asaction frame Open and the goal state ?G Open?can be retrieved from the action knowledge base.After executing the action sequence, the grip-per state will be changed from ?G Close?
to?G Open?, as shown in Figure 5.
In the follow-ing two instructions, the human says ?Move to theblue block?
and ?Close gripper?.
Similarly, thesetwo instructions are translated as action framesMove(B1) and Close, then are executed accord-93ingly.
After executing these two steps, the state ofB1 is changed from ?Out G(B1)?
to ?In G(B1)?.At this point, the previous unknown actionGrab(B1) is achieved, so the human says ?Nowyou achieve the grab action?
as a signal of teach-ing completion.
After acknowledging the teach-ing completion, the action learning module willlearn the new action representation by combiningthe arm state with the state changes of the argu-ment objects in the unknown action frame.
Forexample, the argument object of unknown actionGrab(B1) is B1.
By comparing the original stateof B1, [(Out G B1)?
(Top Clr B1)?
(On B1 table)]with the final state, [(In G B1)?
(Top Clr B1)?
(OnB1 table)], B1 is changed from (Out G B1) to(In G B1).
So, the learning module will gener-alize such state changes and acquire the knowl-edge representation of the new action Grab(x) asG Close?In G(x).5 Empirical StudiesThe objectives of our empirical studies are twofolds.
First, we aim to exam whether the currentrepresentation can support planning algorithmsand execute the learned actions in novel situations.Second, we aim to evaluate how extra effort fromthe human partner through step-by-step instruc-tions may affect the robot?s learning performance.5.1 Instruction EffortPrevious work on mediating perceptual differ-ences between humans and robots have shown thata high collaborative effort from the robot leads tobetter referential grounding (Chai et al., 2014).Motivated by this previous work, we are inter-ested in examining how different levels of effortfrom human partners may affect the robot?s learn-ing performance.
More specifically, we model twolevels of variations:?
Collaborative Interaction: In this setting, ahuman partner provides step-by-step instruc-tions.
At each step, the human will observethe the robot?s response (i.e., arm movement)before moving to the next step.
For exam-ple, to teach ?stack?, the human would is-sue ?pick up the blue block?, observe therobot?s movement, then issue ?put it on thered block?
and observe the robot movement.By this fashion, the human makes extra effortto make sure the robot follows every step cor-rectly before moving on.
The human partnercan detect potential problems and respond toimmediate feedback from the robot.?
Non-Collaborative Interaction: In this set-ting, the human only provides a one-shot in-struction.
For example, to teach ?stack?,the human first issues a complete instruction?pick up the blue block and put it on top ofthe red block?
and then observes the robot?sresponses.
Compared to the collaborative set-ting, the non-collaborative setting is poten-tially more efficient.5.2 Experimental TasksSimilar to the setup shown in Figure 1, in thestudy, we have multiple blocks with different col-ors and sizes placed on a flat surface, with aSCHUNK arm positioned on one side of the sur-face and the human subject seated on the oppositeside.
The video stream of the environment is sentto the vision system (Collet et al., 2011).
With thepre-trained object model of each block, the visionsystem could capture blocks?
3D positions fromeach frame.
Five human subjects participated inour experiments2.
During the study, each sub-ject was informed about the basic actions the robotcan perform (i.e., open gripper, close gripper, andmove to) and was instructed to teach the robot sev-eral new actions through dialogue.
Each subjectwould go through the following two phases:5.2.1 Teaching/Learning PhaseEach subject was asked to teach the following fivenew actions under the two strategies (i.e., step-by-step instructions vs. one-shot instructions):{Pickup, Grab, Drop, ClearTop, Stack} Each time,the subject can choose any blocks they think areuseful to teach the action.
After finishing teachingone action (either under step-by-step instructionsor under one-shot instructions), we would surveythe subject whether he/she thinks the teaching iscompleted and the corresponding action is suc-cessfully performed by the robot.
We record theteaching duration and then re-arrange the table topsetting to move to the next action.For the teaching/learning phase, we use twometrics for evaluation: 1) Teaching CompletionRate(Rt) which stands for the number of actionssuccessfully taught and performed by the robot;2)Teaching Completion Duration (Dtwhich mea-sures the amount of time taken to teach an action.2More human subjects will be recruited to participate inour studies.945.2.2 Execution PhaseThe goal of learning is to be able to apply thelearned knowledge in novel situations.
To evalu-ate such capability, for each action, we designed10 additional setups of the environment whichare different from the environment where the ac-tion was learned.
For example, as illustrated inFigure 6, the human teaches the pick Up actionby instructing the robot how to perform ?pick upthe blue block(i.e., B1)?
under the environmentin 6(a).
Once the knowledge is acquired about theaction ?pick up?, we will test the acquired knowl-edge in a novel situation by instructing the robot toexecute ?pick up the green block(i.e., G1)?
in theenvironment shown in 6(b).
(a) Learning: the humanteaches the robot how to?pick up the blue block(i.e., B1)?
during the learn-ing phase(b) Execution: the humanasks the robot to ?pick upthe green block (i.e., G1)?after the robot acquires theknowledge about ?pick up?Figure 6: Examples of a learning and an executionsetup.For the execution phase, we also usedtwo factors to evaluate: 1) Action SequenceGeneration(Rg) which measures how many high-level actions among the 10 execution scenarioswhere the corresponding lower-level action se-quences are correctly generated; 2) Action Se-quence Execution(Rge) which measures the num-ber of high level actions that are correctly executedbased on the lower level action sequences.5.3 Empirical ResultsOur experiments resulted in a total of 50 actionteaching dialogues.
Half of these are under thestep-by-step instructions (i.e., collaborative inter-action) and half are under one-shot instructions(i.e., non-collaborative).
As shown in Figure 7,5 out of the 50 teaching dialogues were consid-ered as incomplete by the human subjects and allof them are from the Non-Collaborative setting.For each of the 45 successful dialogues, an actionwould be learned and acquired.
For each of theseacquired actions, we further tested its executionunder 10 different setups.Figure 7: The teaching completion result of the50 teaching dialogues.
?1?
stands for the dialoguewhere the subject considers the teaching/learningas complete since the robot performs the corre-sponding action correctly; and ?0?
indicates a fail-ure in learning.
The total numbers of teachingcompletion are listed in the bottom row.Figure 8: The teaching completion duration re-sults.
The durations under the non-collaborativestrategy are smaller than the collaborative strategyin most cases.5.3.1 Teaching PerformanceThe result of teaching completion is shown in Fig-ure 7.
Each subject contributes two columns: the?Non?
stands for the Non-Collaborative strategyand the ?Col?
column refers to the Collaborativestrategy.
As the table shows, all the 5 uncom-pleted teaching are from the Non-Collaborativestrategy.
In most of these 5 cases, the subjectsthought the actual performed actions were differ-ent from their expectations.
For example, in one ofthe ?stack?
failures, the human one-shot instruc-tion was ?move the blue block to the red block onthe left.?.
She thought the arm would put the blueblock on the top of red block, open gripper andthen move away.
However, based on the robot?sknowledge, it just moved the blue block abovethe red block and stopped there.
So the subjectconsidered this teaching as incomplete.
On theother hand, in the Collaborative interactions, therobot?s actual actions could also be different fromthe subject?s expectation.
But, as the instruction95Figure 9: Each bar represents the number of suc-cessfully generated action sequences during test-ing.
The solid portion of each bar represents thenumber of successfully executed action sequences.The number of successfully execution is alwayssmaller than or equal to the generation.
This is be-cause we are dealing with dynamic environment,and the inaccurate real-time localization will makesome correct action sequence fail to be executed.was given step-by-step, the instructors could no-tice the difference from the immediate feedbackand adjust their follow-up steps, which contributedto a higher completion rate.The duration of each teaching task is shown inFigure 8.
Bar heights represent average teachingduration, the ranges stand for standard error ofthe mean (SEM).
The 5 actions are representedby different groups.
As shown in the figure, theteaching duration under the Collaborative strategytends to take more time.
Because in the Collab-orative case, the human needs to plan next stepafter observing the robot?s response to a previousstep.
If an exception happens, a sub-dialogue isoften arranged to do correction.
But in the Non-Collaborative case, the human comes up with anentire instruction at the beginning, which appearsmore efficient.5.3.2 Execution PerformanceFigure 9 illustrates the action sequence generationand execution results in the execution phase.As shown in Figure 9, testing results of actionslearned under the Collaborative strategy are higherthan the ones using Non-Collaborative, this is be-cause teaching under the Collaborative strategy ismore likely to be successful.
One exception is theClear Top action, which has lower generation rateunder the Col setting.
By examining the collecteddata, we noticed that our system failed to learn theknowledge of Clear Top in one of the 5 teachingphases using Col setting, although the human sub-ject labeled it as successful.
Another phenomenonshown in Figure 9 is that the generation results arealways larger than or equal with the correspond-ing execution results.
This is caused by inaccuratelocalization and camera calibration, which intro-duced exceptions during executing the action se-quence.6 ConclusionThis paper describes an approach to robot actionlearning in a simplified blocks world.
The sim-plifications of the environment and the tasks allowus to explore connections between symbolic repre-sentations of natural language and continuous sen-sorimotor representations of the robot which cansupport automated planning for novel situations.This investigation is only our first step.
Many is-sues have not been addressed.
For example, theworld is full of uncertainties.
Our current ap-proach can only either succeed or fail executingan action based on the acquired knowledge.
Thereis no approximation or reasoning of the uncertainstates which may affect potential execution.
Also,when the robot fails to execute an action, there isno explanation why it fails.
If the robot can artic-ulate its internal representations regarding wherethe problem occurs, the human can provide betterhelp or targeted teaching.
These are the directionswe will pursue in our future work.7 AcknowledgmentThis work was supported by IIS-1208390 from theNational Science Foundation and N00014-11-1-0410 from the Office of Naval Research.ReferencesJames F. Allen, Nathanael Chambers, George Fergu-son, Lucian Galescu, Hyuckchul Jung, Mary D.Swift, and William Taysom.
2007.
Plow: A collab-orative task learning agent.
In AAAI, pages 1514?1519.
AAAI Press.S.
R. K. Branavan, Luke S. Zettlemoyer, and ReginaBarzilay.
2010.
Reading between the lines: Learn-ing to map high-level instructions to commands.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, ACL ?10,pages 1268?1277, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.S.R.K.
Branavan, Nate Kushman, Tao Lei, and ReginaBarzilay.
2012.
Learning high-level planning from96text.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 126?135, Jeju Island,Korea, July.
Association for Computational Linguis-tics.Maya Cakmak, Crystal Chao, and Andrea LockerdThomaz.
2010.
Designing interactions for robot ac-tive learners.
IEEE T. Autonomous Mental Develop-ment, 2(2):108?118.R.
Cantrell, K. Talamadupula, P. Schermerhorn, J. Ben-ton, S. Kambhampati, and M. Scheutz.
2012.
Tellme when and why to do it!
run-time planner modelupdates via natural language instruction.
In Human-Robot Interaction (HRI), 2012 7th ACM/IEEE Inter-national Conference on, pages 471?478, March.Joyce Y. Chai, Lanbo She, Rui Fang, Spencer Ottarson,Cody Littley, Changsong Liu, and Kenneth Han-son.
2014.
Collaborative effort towards commonground in situated human robot dialogue.
In Pro-ceedings of 9th ACM/IEEE International Confer-ence on Human-Robot Interaction, Bielefeld, Ger-many.David L. Chen, Joohyun Kim, and Raymond J.Mooney.
2010.
Training a multilingual sportscaster:Using perceptual context to learn language.
J. Artif.Int.
Res., 37(1):397?436, January.H.
I. Christensen, G. M. Kruijff, and J. Wyatt, editors.2010.
Cognitive Systems.
Springer.Alvaro Collet, Manuel Martinez, and Siddhartha S.Srinivasa.
2011.
The MOPED framework: ObjectRecognition and Pose Estimation for Manipulation.Richard E. Fikes and Nils J. Nilsson.
1971.
Strips: Anew approach to the application of theorem provingto problem solving.
In Proceedings of the 2Nd Inter-national Joint Conference on Artificial Intelligence,IJCAI?71, pages 608?620, San Francisco, CA, USA.Morgan Kaufmann Publishers Inc.Thomas Kollar, Stefanie Tellex, Deb Roy, and NicholasRoy.
2010.
Toward understanding natural languagedirections.
In Proceedings of the 5th ACM/IEEEInternational Conference on Human-robot Interac-tion, HRI ?10, pages 259?266, Piscataway, NJ, USA.IEEE Press.Hadas Kress-Gazit, Georgios E. Fainekos, andGeorge J. Pappas.
2008.
Translating structuredenglish to robot controllers.
Advanced Robotics,22(12):1343?1359.Changsong Liu, Rui Fang, and Joyce Chai.
2012.
To-wards mediating shared perceptual basis in situateddialogue.
In Proceedings of the 13th Annual Meet-ing of the Special Interest Group on Discourse andDialogue, pages 140?149, Seoul, South Korea.Changsong Liu, Rui Fang, Lanbo She, and Joyce Chai.2013.
Modeling collaborative referring for situatedreferential grounding.
In Proceedings of the SIG-DIAL 2013 Conference, pages 78?86, Metz, France.Cynthia Matuszek, Evan Herbst, Luke S. Zettlemoyer,and Dieter Fox.
2012.
Learning to parse nat-ural language commands to a robot control sys-tem.
In Jaydev P. Desai, Gregory Dudek, Ous-sama Khatib, and Vijay Kumar, editors, ISER, vol-ume 88 of Springer Tracts in Advanced Robotics,pages 403?415.
Springer.Shiwali Mohan, James Kirk, and John Laird.
2013.
Acomputational model for situated task learning withinteractive instruction.
In Proceedings of ICCM2013 - 12th International Conference on CognitiveModeling.Lanbo She, Yu Cheng, Joyce Chai, Yunyi Jia, ShaohuaYang, and Ning Xi.
2014.
Teaching robots new ac-tions through natural language instructions.
In RO-MAN.Jeffrey Mark Siskind.
1999.
Grounding the lexical se-mantics of verbs in visual perception using force dy-namics and event logic.
J. Artif.
Int.
Res., 15(1):31?90, February.Stefanie Tellex, Thomas Kollar, Steven Dickerson,Matthew R. Walter, Ashis Gopal Banerjee, Seth J.Teller, and Nicholas Roy.
2011.
Understanding nat-ural language commands for robotic navigation andmobile manipulation.
In Wolfram Burgard and DanRoth, editors, AAAI.
AAAI Press.T.
Winograd.
1972.
Procedures as a representation fordata in a computer program for understanding natu-ral language.
Cognitive Psychology, 3(1):1?191.Huatao Zhang, Yunyi Jia, and Ning Xi.
2012.
Sensor-based redundancy resolution for a nonholonomicmobile manipulator.
In IROS, pages 5327?5332.97
