Proceedings of the ACL 2010 Conference Short Papers, pages 38?42,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsAuthorship Attribution Using Probabilistic Context-Free GrammarsSindhu Raghavan Adriana Kovashka Raymond MooneyDepartment of Computer ScienceThe University of Texas at Austin1 University Station C0500Austin, TX 78712-0233, USA{sindhu,adriana,mooney}@cs.utexas.eduAbstractIn this paper, we present a novel approachfor authorship attribution, the task of iden-tifying the author of a document, usingprobabilistic context-free grammars.
Ourapproach involves building a probabilisticcontext-free grammar for each author andusing this grammar as a language modelfor classification.
We evaluate the perfor-mance of our method on a wide range ofdatasets to demonstrate its efficacy.1 IntroductionNatural language processing allows us to buildlanguage models, and these models can be usedto distinguish between languages.
In the con-text of written text, such as newspaper articles orshort stories, the author?s style could be consid-ered a distinct ?language.?
Authorship attribution,also referred to as authorship identification or pre-diction, studies strategies for discriminating be-tween the styles of different authors.
These strate-gies have numerous applications, including set-tling disputes regarding the authorship of old andhistorically important documents (Mosteller andWallace, 1984), automatic plagiarism detection,determination of document authenticity in court(Juola and Sofko, 2004), cyber crime investiga-tion (Zheng et al, 2009), and forensics (Luyckxand Daelemans, 2008).The general approach to authorship attributionis to extract a number of style markers from thetext and use these style markers as features to traina classifier (Burrows, 1987; Binongo and Smith,1999; Diederich et al, 2000; Holmes and Forsyth,1995; Joachims, 1998; Mosteller and Wallace,1984).
These style markers could include thefrequencies of certain characters, function words,phrases or sentences.
Peng et al (2003) build acharacter-level n-gram model for each author.
Sta-matatos et al (1999) and Luyckx and Daelemans(2008) use a combination of word-level statisticsand part-of-speech counts or n-grams.
Baayen etal.
(1996) demonstrate that the use of syntacticfeatures from parse trees can improve the accu-racy of authorship attribution.
While there havebeen several approaches proposed for authorshipattribution, it is not clear if the performance of oneis better than the other.
Further, it is difficult tocompare the performance of these algorithms be-cause they were primarily evaluated on differentdatasets.
For more information on the current stateof the art for authorship attribution, we refer thereader to a detailed survey by Stamatatos (2009).We further investigate the use of syntactic infor-mation by building complete models of each au-thor?s syntax to distinguish between authors.
Ourapproach involves building a probabilistic context-free grammar (PCFG) for each author and usingthis grammar as a language model for classifica-tion.
Experiments on a variety of corpora includ-ing poetry and newspaper articles on a number oftopics demonstrate that our PCFG approach per-forms fairly well, but it only outperforms a bi-gram language model on a couple of datasets (e.g.poetry).
However, combining our approach withother methods results in an ensemble that performsthe best on most datasets.2 Authorship Attribution using PCFGWe now describe our approach to authorship at-tribution.
Given a training set of documents fromdifferent authors, we build a PCFG for each authorbased on the documents they have written.
Givena test document, we parse it using each author?sgrammar and assign it to the author whose PCFGproduced the highest likelihood for the document.In order to build a PCFG, a standard statisticalparser takes a corpus of parse trees of sentencesas training input.
Since we do not have access toauthors?
documents annotated with parse trees,we use a statistical parser trained on a generic38corpus like the Wall Street Journal (WSJ) orBrown corpus from the Penn Treebank (http://www.cis.upenn.edu/?treebank/)to automatically annotate (i.e.
treebank) thetraining documents for each author.
In ourexperiments, we used the Stanford Parser (Kleinand Manning, 2003b; Klein and Manning,2003a) and the OpenNLP sentence segmenter(http://opennlp.sourceforge.net/).Our approach is summarized below:Input ?
A training set of documents labeledwith author names and a test set of documents withunknown authors.1.
Train a statistical parser on a generic corpuslike the WSJ or Brown corpus.2.
Treebank each training document using theparser trained in Step 1.3.
Train a PCFG Gi for each author Ai using thetreebanked documents for that author.4.
For each test document, compute its likeli-hood for each grammar Gi by multiplying theprobability of the top PCFG parse for eachsentence.5.
For each test document, find the author Aiwhose grammar Gi results in the highest like-lihood score.Output ?
A label (author name) for each docu-ment in the test set.3 Experimental ComparisonThis section describes experiments evaluating ourapproach on several real-world datasets.3.1 DataWe collected a variety of documents with knownauthors including news articles on a wide range oftopics and literary works like poetry.
We down-loaded all texts from the Internet and manually re-moved extraneous information as well as titles, au-thor names, and chapter headings.
We collectedseveral news articles from the New York Timesonline journal (http://global.nytimes.com/) on topics related to business, travel, andfootball.
We also collected news articles oncricket from the ESPN cricinfo website (http://www.cricinfo.com).
In addition, we col-lected poems from the Project Gutenberg web-site (http://www.gutenberg.org/wiki/Main_Page).
We attempted to collect sets ofdocuments on a shared topic written by multipleauthors.
This was done to ensure that the datasetstruly tested authorship attribution as opposed totopic identification.
However, since it is very dif-ficult to find authors that write literary works onthe same topic, the Poetry dataset exhibits highertopic variability than our news datasets.
We had5 different datasets in total ?
Football, Business,Travel, Cricket, and Poetry.
The number of au-thors in our datasets ranged from 3 to 6.For each dataset, we split the documents intotraining and test sets.
Previous studies (Stamatatoset al, 1999) have observed that having unequalnumber of words per author in the training setleads to poor performance for the authors withfewer words.
Therefore, we ensured that, in thetraining set, the total number of words per authorwas roughly the same.
We would like to note thatwe could have also selected the training set suchthat the total number of sentences per author wasroughly the same.
However, since we would liketo compare the performance of the PCFG-basedapproach with a bag-of-words baseline, we de-cided to normalize the training set based on thenumber of words, rather than sentences.
For test-ing, we used 15 documents per author for datasetswith news articles and 5 or 10 documents per au-thor for the Poetry dataset.
More details about thedatasets can be found in Table 1.Dataset # authors # words/auth # docs/auth # sent/authFootball 3 14374.67 17.3 786.3Business 6 11215.5 14.16 543.6Travel 4 23765.75 28 1086Cricket 4 23357.25 24.5 1189.5Poetry 6 7261.83 24.16 329Table 1: Statistics for the training datasets used inour experiments.
The numbers in columns 3, 4 and5 are averages.3.2 MethodologyWe evaluated our approach to authorship predic-tion on the five datasets described above.
For newsarticles, we used the first 10 sections of the WSJcorpus, which consists of annotated news articleson finance, to build the initial statistical parser in39Step 1.
For Poetry, we used 7 sections of theBrown corpus which consists of annotated docu-ments from different areas of literature.In the basic approach, we trained a PCFG modelfor each author based solely on the documentswritten by that author.
However, since the num-ber of documents per author is relatively low, thisleads to very sparse training data.
Therefore, wealso augmented the training data by adding one,two or three sections of the WSJ or Brown corpusto each training set, and up-sampling (replicating)the data from the original author.
We refer to thismodel as ?PCFG-I?, where I stands for interpo-lation since this effectively exploits linear interpo-lation with the base corpus to smooth parameters.Based on our preliminary experiments, we repli-cated the original data three or four times.We compared the performance of our approachto bag-of-words classification and n-gram lan-guage models.
When using bag-of-words, onegenerally removes commonly occurring ?stopwords.?
However, for the task of authorship pre-diction, we hypothesized that the frequency ofspecific stop words could provide useful infor-mation about the author?s writing style.
Prelim-inary experiments verified that eliminating stopwords degraded performance; therefore, we didnot remove them.
We used the Maximum Entropy(MaxEnt) and Naive Bayes classifiers in the MAL-LET software package (McCallum, 2002) as ini-tial baselines.
We surmised that a discriminativeclassifier like MaxEnt might perform better thana generative classifier like Naive Bayes.
How-ever, when sufficient training data is not available,generative models are known to perform betterthan discriminative models (Ng and Jordan, 2001).Hence, we chose to compare our method to bothNaive Bayes and MaxEnt.We also compared the performance of thePCFG approach against n-gram language models.Specifically, we tried unigram, bigram and trigrammodels.
We used the same background corpusmixing method used for the PCFG-I model to ef-fectively smooth the n-gram models.
Since a gen-erative model like Naive Bayes that uses n-gramfrequencies is equivalent to an n-gram languagemodel, we also used the Naive Bayes classifier inMALLET to implement the n-gram models.
Notethat a Naive-Bayes bag-of-words model is equiva-lent to a unigram language model.While the PCFG model captures the author?swriting style at the syntactic level, it may not accu-rately capture lexical information.
Since both syn-tactic and lexical information is presumably usefulin capturing the author?s overall writing style, wealso developed an ensemble using a PCFG model,the bag-of-words MaxEnt classifier, and an n-gram language model.
We linearly combined theconfidence scores assigned by each model to eachauthor, and used the combined score for the finalclassification.
We refer to this model as ?PCFG-E?, where E stands for ensemble.
We also de-veloped another ensemble based on MaxEnt andn-gram language models to demonstrate the con-tribution of the PCFG model to the overall per-formance of PCFG-E. For each dataset, we reportaccuracy, the fraction of the test documents whoseauthors were correctly identified.3.3 Results and DiscussionTable 2 shows the accuracy of authorship predic-tion on different datasets.
For the n-gram mod-els, we only report the results for the bigrammodel with smoothing (Bigram-I) as it was thebest performing model for most datasets (exceptfor Cricket and Poetry).
For the Cricket dataset,the trigram-I model was the best performing n-gram model with an accuracy of 98.34%.
Gener-ally, a higher order n-gram model (n = 3 or higher)performs poorly as it requires a fair amount ofsmoothing due to the exponential increase in allpossible n-gram combinations.
Hence, the supe-rior performance of the trigram-I model on theCricket dataset was a surprising result.
For thePoetry dataset, the unigram-I model performedbest among the smoothed n-gram models at 81.8%accuracy.
This is unsurprising because as men-tioned above, topic information is strongest inthe Poetry dataset, and it is captured well in theunigram model.
For bag-of-words methods, wefind that the generatively trained Naive Bayesmodel (unigram language model) performs bet-ter than or equal to the discriminatively trainedMaxEnt model on most datasets (except for Busi-ness).
This result is not suprising since ourdatasets are limited in size, and generative modelstend to perform better than discriminative meth-ods when there is very little training data available.Amongst the different baseline models (MaxEnt,Naive Bayes, Bigram-I), we find Bigram-I to bethe best performing model (except for Cricket andPoetry).
For both Cricket and Poetry, Naive Bayes40Dataset MaxEnt Naive Bayes Bigram-I PCFG PCFG-I PCFG-E MaxEnt+Bigram-IFootball 84.45 86.67 86.67 93.34 80 91.11 86.67Business 83.34 77.78 90.00 77.78 85.56 91.11 92.22Travel 83.34 83.34 91.67 81.67 86.67 91.67 90.00Cricket 91.67 95.00 91.67 86.67 91.67 95.00 93.34Poetry 56.36 78.18 70.90 78.18 83.63 87.27 76.36Table 2: Accuracy in % for authorship prediction on different datasets.
Bigram-I refers to the bigramlanguage model with smoothing.
PCFG-E refers to the ensemble based on MaxEnt, Bigram-I , andPCFG-I .
MaxEnt+Bigram-I refers to the ensemble based on MaxEnt and Bigram-I .is the best performing baseline model.
While dis-cussing the performance of the PCFG model andits variants, we consider the best performing base-line model.We observe that the basic PCFG model and thePCFG-I model do not usually outperform the bestbaseline method (except for Football and Poetry,as discussed below).
For Football, the basic PCFGmodel outperforms the best baseline, while forPoetry, the PCFG-I model outperforms the bestbaseline.
Further, the performance of the basicPCFG model is inferior to that of PCFG-I for mostdatasets, likely due to the insufficient training dataused in the basic model.
Ideally one would usemore training documents, but in many domainsit is impossible to obtain a large corpus of doc-uments written by a single author.
For example,as Luyckx and Daelemans (2008) argue, in foren-sics one would like to identify the authorship ofdocuments based on a limited number of docu-ments written by the author.
Hence, we investi-gated smoothing techniques to improve the perfor-mance of the basic PCFG model.
We found thatthe interpolation approach resulted in a substan-tial improvement in the performance of the PCFGmodel for all but the Football dataset (discussedbelow).
However, for some datasets, even thisimprovement was not sufficient to outperform thebest baseline.The results for PCFG and PCFG-I demon-strate that syntactic information alone is gener-ally a bit less accurate than using n-grams.
In or-der to utilize both syntactic and lexical informa-tion, we developed PCFG-E as described above.We combined the best n-gram model (Bigram-I)and PCFG model (PCFG-I) with MaxEnt to buildPCFG-E. For the Travel dataset, we find that theperformance of the PCFG-E model is equal to thatof the best constituent model (Bigram-I).
For theremaining datasets, the performance of PCFG-Eis better than the best constituent model.
Further-more, for the Football, Cricket and Poetry datasetsthis improvement is quite substantial.
We nowfind that the performance of some variant of PCFGis always better than or equal to that of the bestbaseline.
While the basic PCFG model outper-forms the baseline for the Football dataset, PCFG-E outperforms the best baseline for the Poetryand Business datasets.
For the Cricket and Traveldatasets, the performance of the PCFG-E modelequals that of the best baseline.
In order to as-sess the statistical significance of any performancedifference between the best PCFG model and thebest baseline, we performed the McNemar?s test,a non-parametric test for binomial variables (Ros-ner, 2005).
We found that the difference in theperformance of the two methods was not statisti-cally significant at .05 significance level for any ofthe datasets, probably due to the small number oftest samples.The performance of PCFG and PCFG-I is par-ticularly impressive on the Football and Poetrydatasets.
For the Football dataset, the basic PCFGmodel is the best performing PCFG model and itperforms much better than other methods.
It is sur-prising that smoothing using PCFG-I actually re-sults in a drop in performance on this dataset.
Wehypothesize that the authors in the Football datasetmay have very different syntactic writing stylesthat are effectively captured by the basic PCFGmodel.
Smoothing the data apparently weakensthis signal, hence causing a drop in performance.For Poetry, PCFG-I achieves much higher accu-racy than the baselines.
This is impressive giventhe much looser syntactic structure of poetry com-pared to news articles, and it indicates the value ofsyntactic information for distinguishing betweenliterary authors.Finally, we consider the specific contribution ofthe PCFG-I model towards the performance of41the PCFG-E ensemble.
Based on comparing theresults for PCFG-E and MaxEnt+Bigram-I , wefind that there is a drop in performance for mostdatasets when removing PCFG-I from the ensem-ble.
This drop is quite substantial for the Footballand Poetry datasets.
This indicates that PCFG-Iis contributing substantially to the performance ofPCFG-E.
Thus, it further illustrates the impor-tance of broader syntactic information for the taskof authorship attribution.4 Future Work and ConclusionsIn this paper, we have presented our ongoing workon authorship attribution, describing a novel ap-proach that uses probabilistic context-free gram-mars.
We have demonstrated that both syntac-tic and lexical information are useful in effec-tively capturing authors?
overall writing style.
Tothis end, we have developed an ensemble ap-proach that performs better than the baseline mod-els on several datasets.
An interesting extensionof our current approach is to consider discrimina-tive training of PCFGs for each author.
Finally,we would like to compare the performance of ourmethod to other state-of-the-art approaches to au-thorship prediction.AcknowledgmentsExperiments were run on the Mastodon Cluster,provided by NSF Grant EIA-0303609.ReferencesH.
Baayen, H. van Halteren, and F. Tweedie.
1996.Outside the cave of shadows: using syntactic annota-tion to enhance authorship attribution.
Literary andLinguistic Computing, 11(3):121?132, September.Binongo and Smith.
1999.
A Study of Oscar Wilde?sWritings.
Journal of Applied Statistics, 26:781.J Burrows.
1987.
Word-patterns and Story-shapes:The Statistical Analysis of Narrative Style.Joachim Diederich, Jo?rg Kindermann, Edda Leopold,and Gerhard Paass.
2000.
Authorship Attribu-tion with Support Vector Machines.
Applied Intel-ligence, 19:2003.D.
I. Holmes and R. S. Forsyth.
1995.
The Federal-ist Revisited: New Directions in Authorship Attri-bution.
Literary and Linguistic Computing, 10:111?127.Thorsten Joachims.
1998.
Text categorization withSupport Vector Machines: Learning with many rel-evant features.
In Proceedings of the 10th EuropeanConference on Machine Learning (ECML), pages137?142, Berlin, Heidelberg.
Springer-Verlag.Patrick Juola and John Sofko.
2004.
Proving andImproving Authorship Attribution Technologies.
InProceedings of Canadian Symposium for Text Anal-ysis (CaSTA).Dan Klein and Christopher D. Manning.
2003a.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics (ACL), pages 423?430, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Dan Klein and Christopher D. Manning.
2003b.
FastExact Inference with a Factored Model for NaturalLanguage Parsing.
In Advances in Neural Infor-mation Processing Systems 15 (NIPS), pages 3?10.MIT Press.Kim Luyckx and Walter Daelemans.
2008.
Author-ship Attribution and Verification with Many Authorsand Limited Data.
In Proceedings of the 22nd Inter-national Conference on Computational Linguistics(COLING), pages 513?520, August.Andrew Kachites McCallum.
2002.
MAL-LET: A Machine Learning for Language Toolkit.http://mallet.cs.umass.edu.Frederick Mosteller and David L. Wallace.
1984.
Ap-plied Bayesian and Classical Inference: The Case ofthe Federalist Papers.
Springer-Verlag.Andrew Y. Ng and Michael I. Jordan.
2001.
On Dis-criminative vs. Generative classifiers: A compari-son of logistic regression and naive Bayes.
In Ad-vances in Neural Information Processing Systems 14(NIPS), pages 841?848.Fuchun Peng, Dale Schuurmans, Viado Keselj, andShaojun Wang.
2003.
Language IndependentAuthorship Attribution using Character Level Lan-guage Models.
In Proceedings of the 10th Confer-ence of the European Chapter of the Association forComputational Linguistics (EACL).Bernard Rosner.
2005.
Fundamentals of Biostatistics.Duxbury Press.E.
Stamatatos, N. Fakotakis, and G. Kokkinakis.
1999.Automatic Authorship Attribution.
In Proceedingsof the 9th Conference of the European Chapter of theAssociation for Computational Linguistics (EACL),pages 158?164, Morristown, NJ, USA.
Associationfor Computational Linguistics.E.
Stamatatos.
2009.
A Survey of Modern Author-ship Attribution Methods.
Journal of the Ameri-can Society for Information Science and Technology,60(3):538?556.Rong Zheng, Yi Qin, Zan Huang, and HsinchunChen.
2009.
Authorship Analysis in CybercrimeInvestigation.
Lecture Notes in Computer Science,2665/2009:959.42
