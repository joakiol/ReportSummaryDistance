The SAMMIE Multimodal Dialogue Corpus Meets the Nite XML ToolkitIvana Kruijff-Korbayova?, Verena Rieser,Ciprian GerstenbergerSaarland University, Saarbru?cken, Germanyvrieser@coli.uni-sb.deJan Schehl, Tilman BeckerDFKI, Saarbru?cken, Germanyjan.schehl@dfki.deAbstractWe demonstrate work in progress1 us-ing the Nite XML Toolkit on a cor-pus of multimodal dialogues with anMP3 player collected in a Wizard-of-Oz(WOZ) experiments and annotated witha rich feature set at several layers.
Wedesigned an NXT data model, convertedexperiment log file data and manual tran-scriptions into NXT, and are building an-notation tools using NXT libraries.1 IntroductionIn the TALK project2 we are developing a mul-timodal dialogue system for an MP3 applicationfor in-car and in-home use.
The system shouldsupport natural, flexible interaction and collabo-rative behavior.
To achieve this, it needs to pro-vide advanced adaptive multimodal output.To determine the interaction strategies andrange of linguistic behavior naturally occurringin this scenario, we conducted two WOZ exper-iments: SAMMIE-1 involved only spoken inter-action, SAMMIE-2 was multimodal, with speechand screen input and output.3We have been annotating the corpus on sev-eral layers, representing linguistic, multimodaland context information.
The annotated corpuswill be used (i) to investigate various aspects of1Our demonstration results from the efforts of a largerteam including also N. Blaylock, B. Fromkorth, M. Gra?c,M.
Kai?er, A. Moos, P. Poller and M. Wirth.2TALK (Talk and Look: Tools for Ambient Linguis-tic Knowledge; http://www.talk-project.org), funded by theEU 6th Framework Program, project No.
IST-507802.3SAMMIE stands for Saarbru?cken Multimodal MP3Player Interaction Experiment.multimodal presentation and interaction strate-gies both within and across the annotation lay-ers; (ii) to design an initial policy for reinforce-ment learning of multimodal clarifications.4 Weuse the Nite XML Toolkit (NXT) (Carletta et al,2003) to represent and browse the data and to de-velop annotation tools.Below we briefly describe our experimentsetup, the collected data and the annotation lay-ers; we comment on methods and tools for datarepresentation and annotation, and then presentour NXT data model.2 Experiment Setup24 subjects in SAMMIE-1 and 35 in SAMMIE-2performed several tasks with an MP3 player ap-plication simulated by a wizard.
For SAMMIE-1 we had two, for SAMMIE-2 six wizards.
Thetasks involved searching for titles and buildingplaylists satisfying various constraints.
Each ses-sion was 30 minutes long.
Both users and wiz-ards could speak freely.
The interactions werein German (although most of the titles and artistnames in the database were English).SAMMIE-2 had a more complex setup.
Thetasks the subjects had to fulfill were divided intwo classes: with vs. without operating a driv-ing simulator.
When presenting the search re-sults, the wizards were free to produce mono-or multimodal output as they saw fit; they couldspeak freely and/or select one of four automati-cally generated screen outputs, which containedtables and lists of found songs/albums.
Theusers also had free choice between unconstrained4See (Kruijff-Korbayova?
et al, 2006) for more detailsabout the annotation goals and further usage of the corpus.69natural language and/or selecting items on thescreen.
Both wizard and user utterances were im-mediately transcribed.
The wizard?s utteranceswere presented to the user via a speech synthe-sizer.
To simulate acoustic understanding prob-lems, the wizard sometimes received only partof the transcribed user?s utterance, to elicit CRs.
(See (Kruijff-Korbayova?
et al, 2005) for details.
)3 Collected DataThe SAMMIE-2 data for each session consists ofa video and audio recording and a log file.5 Thegathered logging information per session con-sists of Open Agent Architecture (Martin et al,1999) (OAA) messages in chronological order,each marked by a timestamp.
The log files con-tain various information, e.g., the transcriptionsof the spoken utterances, the wizard?s databasequery and the number of results, the screen op-tion chosen by the wizard, classification of clari-fication requests (CRs), etc.4 Annotation Methods and ToolsThe rich set of features we are interested in nat-urally gives rise to a multi-layered view of thecorpus, where each layer is to be annotated inde-pendently, but subsequent investigations involveexploration and automatic processing of the inte-grated data across layers.There are two crucial technical requirementsthat must be satisfied to make this possible: (i)stand-off annotation at each layer and (ii) align-ment of base data across layers.
Without the for-mer, we could not keep the layers separate, with-out the latter we would not be able to align theseparate layers.
An additional equally importantrequirement is that elements at different layersof annotation should be allowed to have overlap-ping spans; this is crucial because, e.g., prosodicunits and syntactic phrases need not coincide.Among the existing toolkits that supportmulti-layer annotation, it was decided to useNXT (Carletta et al, 2003)6 in the TALKproject.
The NXT-based SAMMIE-2 corpus we5For 19 sessions the full set of data files exists.6http://www.ltg.ed.ac.uk/NITE/are demonstrating has been created in severalsteps: (1) The speech data was manually tran-scribed using the Transcriber tool.7 (2) We auto-matically extracted features at various annotationlayers by parsing the OAA messages in the logfiles.
(3) We automatically converted the tran-scriptions and the information from the log filesinto our NXT-based data representation format;features annotated in the transcriptions and fea-tures automatically extracted from the log fileswere assigned to elements at the appropriate lay-ers of representation in this step.Manual annotation: We use tools specifi-cally designed to support the particular annota-tion tasks.
We describe them below.As already mentioned, we used Transcriber forthe manual transcriptions.
We also performedcertain relatively simple annotations directly onthe transcriptions and coded them in-line by us-ing special notation.
This includes the identifica-tion of self-speech, the identification of expres-sions referring to domain objects (e.g., songs,artists and albums) and the identification of utter-ances that convey the results of database queries.For other manual annotation tasks (the annota-tion of CRs, task segmentation and completion,referring expressions and the relations betweenthem) we have been building specialized toolsbased on the NXT library of routines for build-ing displays and interfaces based on Java Swing(Carletta et al, 2003).
Although NXT comeswith a number of example applications, these aretightly coupled with the architecture of the cor-pora they were built for.
We therefore developeda core basic tool for our own corpus; we mod-ify this tool to suite each annotation task.
To fa-cilitate tool development, NXT provides GUI el-ements linked directly to corpora elements andsupport for handling complex multi-layer cor-pora.
This proved very helpful.Figure 4 shows a screenshot of our CR anno-tation tool.
It allows one to select an utterancein the left-hand side of the display by clickingon it, and then choose the attribute values fromthe pop-down lists on the right-hand side.
Cre-7http://trans.sourceforge.net/70ating relations between elements and creating el-ements on top of other elements (e.g., words orutterances) are extensions we are currently im-plementing (and will complete by the time of theworkshop).
First experiences using the tool toidentify CRs are promising.8 When demonstrat-ing the system we will report the reliability ofother manual annotation tasks.Automatic annotation using indexing: NXTalso provides a facility for automatic annotationbased on NiteQL query matches (Carletta et al,2003).
Some of our features, e.g., the dialoguehistory ones, can be easily derived via queries.5 The SAMMIE NXT Data ModelNXT uses a stand-off XML data format that con-sist of several XML files that point to each other.The NXT data model is a multi-rooted tree witharbitrary graph structure.
Each node has one setof children, and can have multiple parents.Our corpus consists of the following layers.Two base layers: words and graphical outputevents; both are time-aligned.
On top of these,structural layers correspond to one session persubject, divided into task sections, which con-sist of turns, and these consist of individual ut-terances, containing words.
Graphical outputevents will be linked to turns at a featural layer.Further structural layers are defined for CRsand dialogue acts (units are utterances), domainobjects and discourse entities (units are expres-sions consisting of words).
We keep independentlayers of annotation separate, even when they canin principle be merged into a single hierarchy.Figure 2 shows a screenshot made with Ami-gram (Lauer et al, 2005), a generic tool forbrowsing and searching NXT data.
On the left-hand side one can see the dependencies betweenthe layers.
The elements at the respective layersare displayed on the right-hand side.Below we indicate the features per layer:?
Words: Time-stamped words and othersounds; we mark self-speech, pronuncia-tion, deletion status, lemma and POS.8Inter-annotator agreement of 0.788 (?
corrected forprevalence).?
Graphical output: The type and amount ofinformation displayed, the option selectedby the wizard, and the user?s choices.?
Utterances: Error rates due to word dele-tion, and various features describing thesyntactic structure, e.g., mood, polarity,diathesis, complexity and taxis, the pres-ence of marked syntactic constructions suchas ellipsis, fronting, extraposition, cleft, etc.?
Turns: Time delay, dialogue duration sofar, and other dialogue history features, i.e.values which accumulate over time.?
Domain objects and discourse entities:Properties of referring expressions reflect-ing the type and information status of dis-course entities, and coreference/bridginglinks between them.?
Dialogue acts: DAs based on an agent-based approach to dialogue as collaborativeproblem-solving (Blaylock et al, 2003),e.g., determining joint objectives, find-ing and instantiating recipes to accomplishthem, executing recipes and monitoring forsuccess.
We also annotate propositionalcontent and the database queries.?
CRs: Additional features including thesource and degree of uncertainty, and char-acteristics of the CRs strategy.?
Tasks: A set of features for estimating usersatisfaction online for reinforcement learn-ing (Rieser et al, 2005).?
Session: Subject and wizard information,user questionnaire aswers, and accumulat-ing attribute values from other layers.6 SummaryWe described a multi-layered corpus of multi-modal dialogues represented and annotated us-ing NXT-based tools.
Our data model relates lin-guistic and graphical realization to a rich set ofcontext features and represents structural, hierar-chical interactions between different annotationlayers.
We combined different annotation meth-ods to construct the corpus.
Manual annotationand annotation evaluation is on-going.
The cor-pus will be used (i) investigate multimodal pre-sentation and interaction strategies with respect71Figure 1: NXT-based tool for annotating CRsFigure 2: SAMMIE-2 corpus displayed in Amigramto dialogue context and (ii) to design an initialpolicy for reinforcement learning of multimodalclarification strategies.References[Blaylock et al2003] N. Blaylock, J. Allen, and G. Fergu-son.
2003.
Managing communicative intentions withcollaborative problem solving.
In Current and NewDirections in Discourse and Dialogue, pages 63?84.Kluwer, Dordrecht.
[Carletta et al2003] J. Carletta, S. Evert, U. Heid, J. Kil-gour, J. Robertson, and H. Voormann.
2003.
The NITEXML Toolkit: flexible annotation for multi-modal lan-guage data.
Behavior Research Methods, Instruments,and Computers, special issue on Measuring Behavior.Submitted.[Kruijff-Korbayova?
et al2005] I. Kruijff-Korbayova?,T.
Becker, N. Blaylock, C. Gerstenberger, M. Kai?er,P.
Poller, J. Schehl, and V. Rieser.
2005.
An experimentsetup for collecting data for adaptive output planning ina multimodal dialogue system.
In Proc.
of ENLG.[Kruijff-Korbayova?
et al2006] I. Kruijff-Korbayova?,T.
Becker, N. Blaylock, C. Gerstenberger, M. Kai?er,P.
Poller, V. Rieser, and J. Schehl.
2006.
The SAMMIEcorpus of multimodal dialogues with an mp3 player.
InProc.
of LREC (to appear).
[Lauer et al2005] C. Lauer, J. Frey, B. Lang, T. Becker,T.
Kleinbauer, and J. Alexandersson.
2005.
Amigram- a general-purpose tool for multimodal corpus annota-tion.
In Proc.
of MLMI.
[Martin et al1999] D. L. Martin, A. J. Cheyer, and D. B.Moran.
1999.
The open agent architecture: A frame-work for building distributed software systems.
AppliedArtificial Intelligence: An International Journal, 13(1?2):91?128, Jan?Mar.
[Rieser et al2005] V. Rieser, I.
Kruijff-Korbayova?, andO.
Lemon.
2005.
A corpus collection and annotationframework for learning multimodal clarification strate-gies.
In Proc.
of SIGdial.72
