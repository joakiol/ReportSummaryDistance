Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 210?214,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsFinding middle ground?
Multi-objective Natural Language Generationfrom time-series dataDimitra Gkatzia, Helen Hastie, and Oliver LemonSchool of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh{dg106, h.hastie, o.lemon}@hw.ac.ukAbstractA Natural Language Generation (NLG)system is able to generate text from non-linguistic data, ideally personalising thecontent to a user?s specific needs.
In somecases, however, there are multiple stake-holders with their own individual goals,needs and preferences.
In this paper, weexplore the feasibility of combining thepreferences of two different user groups,lecturers and students, when generatingsummaries in the context of student feed-back generation.
The preferences of eachuser group are modelled as a multivariateoptimisation function, therefore the taskof generation is seen as a multi-objective(MO) optimisation task, where the twofunctions are combined into one.
This ini-tial study shows that treating the prefer-ences of each user group equally smoothsthe weights of the MO function, in a waythat preferred content of the user groups isnot presented in the generated summary.1 IntroductionSummarisation of time-series data refers to thetask of automatically generating summaries fromattributes whose values change over time.
Contentselection is the task of choosing what to say, i.e.what information to be included in a report (Re-iter and Dale, 2000).
Here, we consider the taskof automatically generating feedback summariesfor students describing their performance duringthe lab of a computer science module over thesemester.
This work is motivated by the fact thatdifferent user groups have different preferences ofthe content that should be conveyed in a summary,as shown by Gkatzia et al.
(2013).Various factors can influence students?
learning,such as difficulty of the material (Person et al.,1995), workload (Craig et al., 2004), attendancein lectures (Ames, 1992) etc.
These factors changeover time and can be interdependent.
The differentstakeholders (i.e.
lecturers and students) have dif-ferent perceptions regarding what constitutes goodfeedback.
Therefore, when generating feedback,we should take into account all preferences in or-der to be able to produce feedback summaries thatare acceptable by both user groups.Stakeholders often have conflicting goals, needsand preferences, for example managers with em-ployees or doctors with patients and relatives.
Inour data, for instance, lecturers tend to commenton the hours that a student studied, whereas thestudents disprefer this content.
Generating thesame summary for both groups allows for mean-ingful further discussion with common ground.Previous work on NLG systems that addressmore than one user group use different versions ofa system for each different user group (Gatt et al.,2009) or make use of User Models (Janarthanamand Lemon, 2010; Thompson et al., 2004; Zuk-erman and Litman, 2001).
Here, we explore amethod that adapts to both expert preferences andusers simultaneously (i.e.
lecturer and studentspreferences), by applying Multi-Objective opti-misation (MOO).
MOO can be applied to situa-tions where optimal decisions are sought in thepresence of trade-offs between conflicting objec-tives (Chankong and Haimes, 1983).
We explorewhether balancing the preferences of two usergroups can result in an adaptive system that is ac-ceptable by all users.
At the same time, the pro-gramming effort is reduced as only one systemneeds to be developed.
Moreover, by pooling allavailable data together, there is less need for anextensive data collection.In the next section, we present three systems:one tuned for lecturers, one for students, and onethat attempts to find middle ground.
In Section 3,we describe an evaluation of these three systemsand in Section 4 we discuss the results.
Finally, in210Section 5, directions for future work are discussed.2 MethodologyReinforcement Learning (RL) is a machine learn-ing technique that defines how an agent learnsto take optimal sequences of actions so as tomaximize a cumulative reward (Sutton and Barto,1998).
Here we extend the framework proposedby Gkatzia et al.
(2013) whereby the content selec-tion is seen as a Markov Decision problem and thegoal of the agent is to learn to take the sequenceof actions that leads to optimal content selection.A Temporal Difference learning method (Suttonand Barto, 1998) was used to train an agent forcontent selection.
Firstly, we will describe thedata in general.
Secondly, we refer to the RLsystem that adapts to lecturers?
preferences as de-scribed by Gkatzia et al.
(2013).
Thirdly, we willdescribe how we collected data and developed amethodology that adapts to students?
preferencesand finally how we combined the knowledge ofboth steps to develop an MO system.
The threesystems (Lecturer-adapted, Student-adapted, MO)share the same architecture but the difference liesin the reward functions used for training.2.1 The DataFor this study, the dataset described by Gkatziaet al.
(2013) was used.
Table 1 shows an exam-ple of this dataset that describes a student?s learn-ing habits and a corresponding feedback summaryprovided by a lecturer.
The dataset is composedof 37 similar instances.
Each instance consists oftime-series information about the student?s learn-ing routine and the selected templates that lectur-ers used to provide feedback to this student.
Atemplate is a quadruple consisting of an id, a fac-tor (Table 1), a reference type (trend, weeks, aver-age, other) and surface text.
For instance, a tem-plate can be (1, marks, trend, ?Your marks were<trend>over the semester?).
The lexical choicefor <trend>(i.e.
increasing or decreasing) de-pends on the values of time-series data.
Thereis a direct mapping between the values of factorand reference type and the surface text.
The time-series attributes are listed in Table 1 (bottom left).2.2 Time-series summarisation systemsActions and states: The state consists of the time-series data and the selected templates.
In order toexplore the state space the agent selects a time-series attribute (e.g.
marks, deadlines etc.)
andthen decides whether to talk about it or not.
Thestates and actions are similar for all systems.Lecturer-adapted reward functionThe reward function is derived from analysis withlinear regression of the provided dataset and is thefollowing cumulative multivariate function:RewardLECT= a+n?i=1bi?
xi+ c ?
lengthwhere X = {x1, x2, ..., xn} is the vector ofcombinations of the data trends observed in thetime-series data and a particular reference type ofthe factor.
The value of xiis given by the function:xi=??????????
?1, if the combination of a factor trendand a particular reference type isincluded in the feedback0, if not.The coefficients represent the preference level ofa factor to be selected and how to be conveyedin the summary.
Important factors are associatedwith high positive coefficients and the unimpor-tant ones with negative coefficients.
In the train-ing phase, the agent selects a factor and then de-cides whether to talk about it or not.
If it decidesto refer to a factor, the selection of the template isperformed deterministically, i.e.
it selects the tem-plate that results in higher reward.
Length rep-resents the number of factors selected for gener-ation.Student-adapted reward functionThe Student-adapted system uses the same RL al-gorithm as the Lecturer-adapted one.
The differ-ence lies in the reward function.
The reward func-tion used for training is of a similar style as theLecturer-adapted reward function.
This functionwas derived by manipulating the student ratings ina previous experiment and estimating the weightsusing linear regression in a similar way as Walkeret al.
(1997) and Rieser et al.
(2010).Multi-objective functionThe function used for the multi-objective methodis derived by weighting the sum of the individualreward functions.RMO= 0.5 ?
RLECT+ 0.5 ?
RSTUDENTTo reduce the confounding variables, we keptthe ordering of content in all systems the same.3 EvaluationThe output of the above-mentioned three systemswere evaluated both in simulation and with real211Raw Datafactors week 2 week 3 ... week 10marks 5 4 ... 5hours studied 1 2 ... 3... ... ... ... ...Trends from Datafactors factor trend(1) marks trend other(2) hours studied trend increasing(3) understandability trend decreasing(4) difficulty trend decreasing(5) deadlines trend increasing(6) health issues trend other(7) personal issues trend decreasing(8) lectures attended trend other(9) revision trend decreasingSummaryYour overall performance was excellentduring the semester.
Keep up the goodwork and maybe try some more challeng-ing exercises.
Your attendance was vary-ing over the semester.
Have a think abouthow to use time in lectures to improve yourunderstanding of the material.
You spent 2hours studying the lecture material onaverage.
You should dedicate more timeto study.
You seem to find the materialeasier to understand compared to thebeginning of the semester.
Keep up thegood work!
You revised part of the learn-ing material.
Have a think about whetherrevising has improved your performance.Table 1: Top left: example of the time-series raw data for feedback generation.
Bottom left: example ofdescribed trends.
Right box: a target summary generated by an expert (bold signifies the chosen content).users.
Example summaries of all systems are pre-sented in Table 2.3.1 Evaluation in Simulation26 summaries were produced by each system.
Theoutput of each system was evaluated with the threereward functions.
Table 3 shows the results.As expected, all systems score highly whenevaluated with the reward function for whichthey were trained, with the second highest rewardscored from the MO function.
Table 2 illustratesthis with the MO Policy clearly between the othertwo policies.
Moreover, the MO function reducesthe variability between summaries as is also re-flected in the standard deviation given in Table 3.We used BLEU (4-grams) (Papineni et al.,2002) to measure the similarities between thefeedback summaries generated by the three sys-tems.
BLEU score is between 0-1 with valuescloser to 1 indicating texts are more similar.
Ourresults demonstrate that the summaries generatedby the three systems are quite different (BLEUscore between 0.33 and 0.36).
This shows that theframework presented here is capable of producingquite different summaries based on the various re-ward functions.3.2 Evaluation with real usersThe goal of the evaluation is to determine whetherthe end-user can pick up on the above-mentioneddifferences in the feedback and rank them accord-ing to their preferences.
The output of the threesystems was ranked by 19 lecturers and 48 first-year Computer Science students.
Time-series dataof three students were presented on graphs to eachparticipant.
They were also shown 3 feedbacksummaries and they were asked to rank them interms of preference.As we can see from Table 4, the two user groupssignificantly preferred the output of the systemwhich was trained for their preferences (Mann-Whitney U test, p < 0.05).
Interestingly, lecturersfound both the outputs produced by the Lecturer-adapted system and the Student-adapted systemsignificantly preferable (p < 0.05) to the outputproduced by the MO system.
In contrast, studentssignificantly preferred the output generated by theStudent-adapted system over the other two.
Fi-nally, both user groups rated the MO system 3rd,but there is not a significant difference betweenthe student ratings for the MO system and theLecturer-adapted system.4 DiscussionIt is interesting to examine the weights derivedfrom the multiple-linear regression to determinethe preferences of the different user groups.
Forinstance, lecturers?
most preferred content ishours studied, therefore the reward function giveshigh scores to summaries that mention the hours212Lecturer-adapted Student-adapted Multi-objectiveMake sure you revise the learningmaterial and try to do the lab ex-ercises again.
You dedicated moretime studying the lecture material inthe beginning of the semester com-pared to the end of the semester.Have a think about what is prevent-ing you from studying.
Your under-standing of the material could beimproved.
Try going over the teach-ing material again.
You have hadother deadlines during weeks 5, 6,8, 9 and 10.
You may want to planyour studying and work ahead.
Youdid not face any health problemsduring the semester.You found the lab exercises verychallenging.
Make sure that youhave understood the taught materialand don?t hesitate to ask for clari-fication.
You dedicated more timestudying the lecture material inthe beginning of the semester com-pared to the end of the semester.Have a think about what is prevent-ing you from studying.
Your un-derstanding of the material couldbe improved.
Try going over theteaching material again.
Revisingmaterial during the semester willimprove your performance in thelab.Your attendance was varying over thesemester.
Have a think about how touse time in lectures to improve your un-derstanding of the material.
You foundthe lab exercises very challenging.
Makesure that you have understood the taughtmaterial and don?t hesitate to ask forclarification.
You dedicated more timestudying the lecture material in the be-ginning of the semester compared to theend of the semester.
Have a think aboutwhat is preventing you from studying.You did not face any health problemsduring the semester.
You revised partof the learning material.
Have a thinkwhether revising has improved your per-formance.Table 2: Example outputs from the three different systems (bold signifies the chosen content).Time-Series Summarisation Systems Lecturer Function Student Function MO FunctionLecturer-adapted system 243.82 (70.35) 51.99 (89.87) 114.12 (49.58)Student-adapted system 72.54 (106.97) 213.75 (59.45) 127.76 (52.09)MO system 123.67 (72.66) 153.79 (56.61) 164.84 (83.89)Table 3: Average rewards (and standard deviation) assigned to summaries produced by the 3 systems.Bold signifies higher reward.SummarisationSystemsLecturer?s Rat-ingStudent?sRatingLecturer-adapted 1st (2.15)* 3rd (1.97)Student-adapted 1st (2.01)* 1st* (2.22)MO 2nd, 3rd (1.81) 3rd (1.79)Table 4: Mode of the ratings for each user group(*Mann-Whitney U test, p < 0.05, when compar-ing each system to the MO system).that a student studied in all cases (i.e.
when thehours studied increased, decreased, or remainedstable).
This, however, does not factor heavily intothe student?s reward function.Secondly, lecturers find it useful to give someadvice to students who faced personal issues dur-ing the semester, such as advising them to talk totheir mentor.
Students, on the other hand, likereading about personal issues only when the num-ber of issues they faced was increasing over thesemester, perhaps as this is the only trend that mayaffect their performance.
Students seem to mostlyprefer a feedback summary that mentions the un-derstandability of the material when it increaseswhich is positive feedback.
Finally, the only factorthat both groups agree on is that health issues isnegatively weighted and therefore not mentioned.The MO reward function attempts to balancethe preferences of the two user groups.
Therefore,for this function, the coefficient for mentioninghealth issues is also negative, however the othercoefficients are smoothed providing neither strongnegative or positive coefficients.
This means thatthere is less variability (see Table 3) but that per-haps this function meets neither group?s criteria.5 Conclusion and Future WorkIn conclusion, we presented a framework for de-veloping and evaluating various reward functionsfor time-series summarisation of feedback.
Thisframework has been validated in that both simula-tion and subjective studies show that each groupdoes indeed prefer feedback generated using ahighly tuned reward function, with lecturers beingslightly more open to variation.
Further investiga-tion is required as to whether it is indeed possibleto find middle ground between these two groups.Choices for one group may be negatively ratedby the other and it might not be possible to findmiddle ground but it is worth investigating furtherother methods of reward function derivation usingstronger feature selection methods, such as Princi-pal Component Analysis.213ReferencesCarole Ames.
1992.
Classrooms: Goals, structures,and student motivation.
Journal of Educational Psy-chology, 84(3):p261?71.Chankong and Haimes.
1983.
Multiobjective decisionmaking theory and methodology.
In New York: El-sevier Science Publishing.Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,and Barry Gholson.
2004.
Affect and learning: anexploratory look into the role of affect in learningwith autotutor.
In Journal of Educational Media,29:241-250.Albert Gatt, Francois Portet, Ehud Reiter, JamesHunter, Saad Mahamood, Wendy Moncur, and So-mayajulu Sripada.
2009.
From data to text in theneonatal intensive care unit: Using NLG technologyfor decision support and information management.In Journal of AI Communications, 22:153-186.Dimitra Gkatzia, Helen Hastie, Srinivasan Ja-narthanam, and Oliver Lemon.
2013.
Generatingstudent feedback from time-series data using Rein-forcement Learning.
In 14th European Workshop inNatural Language Generation.Srinivasan Janarthanam and Oliver Lemon.
2010.Adaptive referring expression generation in spokendialogue systems: Evaluation with real users.
In11th Annual Meeting of the Special Interest Groupon Discourse and Dialogue.K Papineni, S Roukos, T. Ward, and W. J Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In 40th Annual meeting of the As-sociation for Computational Linguistics.Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, andArthur C. Graesser.
1995.
Pragmatics and peda-gogy: Conversational rules and politeness strategiesmay inhibit effective tutoring.
In Journal of Cogni-tion and Instruction, 13(2):161-188.Ehud Reiter and Robert Dale.
2000.
Building naturallanguage generation systems.
InCambridge Univer-sity Press.Verena Rieser, Oliver Lemon, and Xingkun Liu.
2010.Optimising information presentation for spoken dia-logue systems.
In 48th Annual Meeting of the Asso-ciation for Computational Linguistics.Richart Sutton and Andrew Barto.
1998.
Reinforce-ment learning.
In MIT Press.Cynthia A. Thompson, Mehmet H. Goker, and Pat Lan-gley.
2004.
A personalised system for conversa-tional recommendations.
In Journal of Artificial In-telligence Research 21, 333-428.Marilyn Walker, Diane Litman, Candace Kamm, andAlicia Abella.
1997.
PARADISE: A framework forevaluating spoken dialogue agents.
In 35th Annualmeeting of the Association for Computational Lin-guistics.Ingrid Zukerman and Diane Litman.
2001.
Natu-ral language processing and user modeling: Syner-gies and limitations.
In User Modeling and User-Adapted Interaction, 11(1-2), 129-158.214
