2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182?190,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsRe-examining Machine Translation Metrics for Paraphrase IdentificationNitin Madnani Joel TetreaultEducational Testing ServicePrinceton, NJ, USA{nmadnani,jtetreault}@ets.orgMartin ChodorowHunter College of CUNYNew York, NY, USAmartin.chodorow@hunter.cuny.eduAbstractWe propose to re-examine the hypothesis thatautomated metrics developed for MT evalu-ation can prove useful for paraphrase iden-tification in light of the significant work onthe development of new MT metrics over thelast 4 years.
We show that a meta-classifiertrained using nothing but recent MT metricsoutperforms all previous paraphrase identifi-cation approaches on the Microsoft ResearchParaphrase corpus.
In addition, we apply oursystem to a second corpus developed for thetask of plagiarism detection and obtain ex-tremely positive results.
Finally, we conductextensive error analysis and uncover the topsystematic sources of error for a paraphraseidentification approach relying solely on MTmetrics.
We release both the new dataset andthe error analysis annotations for use by thecommunity.1 IntroductionOne of the most important reasons for the recentadvances made in Statistical Machine Translation(SMT) has been the development of automated met-rics for evaluation of translation quality.
The goalof any such metric is to assess whether the trans-lation hypothesis produced by a system is seman-tically equivalent to the source sentence that wastranslated.
However, cross-lingual semantic equiv-alence is even harder to assess than monolingual,therefore, most MT metrics instead try to measurewhether the hypothesis is semantically equivalent toa human-authored reference translation of the samesource sentence.
Using such automated metrics asproxies for human judgments can provide a quick as-sessment of system performance and allow for shortfeature and system development cycles, which areimportant for evaluating research ideas.In the last 5 years, several shared tasks and com-petitions have led to the development of increasinglysophisticated metrics that go beyond the computa-tion of n-gram overlaps (BLEU, NIST) or edit dis-tances (TER, WER, PER etc.).
Note that the taskof an MT metric is essentially one of identifyingwhether the translation produced by a system is aparaphrase of the reference translation.
Althoughthe notion of using MT metrics for the task of para-phrase identification is not novel (Finch et al, 2005;Wan et al, 2006), it merits a re-examination in thelight of the development of these novel MT metricsfor which we can ask ?How much better, if at all,do these newer metrics perform for the task of para-phrase identification?
?This paper describes such a re-examination.
Weemploy 8 different MT metrics for identifyingparaphrases across two different datasets - thewell-known Microsoft Research paraphrase corpus(MSRP) (Dolan et al, 2004) and the plagiarismdetection corpus (PAN) from the 2010 UncoveringPlagiarism, Authorship and Social Software Misuseshared task (Potthast et al, 2010).
We include bothMSRP and PAN in our study because they representtwo very different sources of paraphrased text.
Thecreation of MSRP relied on the massive redundancyof news articles on the web and extracted senten-tial paraphrases from different stories written aboutthe same topic.
In the case of PAN, humans con-sciously paraphrased existing text to generate new,182plagiarized text.In the next section, we discuss previous work onparaphrase identification.
In ?3, we describe our ap-proach to paraphrase identification using MT met-rics as features.
Our approach yields impressive re-sults ?
the current state of the art for MSRP and ex-tremely positive for PAN.
In the same section, weexamine whether each metric?s purported strength isdemonstrated in our datasets.
Next, in ?4 we con-duct an analysis of our system?s misclassificationsfor both datasets and outline a taxonomy of errorsthat our system makes.
We also look at annotationerrors in the datasets themselves.
We discuss thefindings of the error analysis in ?5 and conclude in?6.2 Related Work & Our ContributionsOur goal in this paper is to examine the utility of aparaphrase identification approach that relies solelyon MT evaluation metrics and no other evidence ofsemantic equivalence.
Given this setup, the most rel-evant previous work is by Finch et al (2005) whichuses BLEU, NIST, WER and PER as features fora supervised classification approach using SVMs.In addition, they also incorporate part-of-speech in-formation as well as the Jiang-Conrath WordNet-based lexical relatedness measure (Jiang and Con-rath, 1997) into their edit distance calculations.
Inthe first part of our paper, we present classificationexperiments with newer MT metrics not available in2005, a worthwhile exercise in itself.
However, wego much further in our study:?
We apply our approach to two different para-phrase datasets (MSRP and PAN) that were cre-ated via different processes.?
We attempt to find evidence of each metric?spurported strength in both datasets.?
We conduct an extensive error analysis to findtypes of errors that a system based solely onMT metrics is likely to make.
In addition, wealso discover interesting paraphrase pairs in thedatasets.?
We release our sentence-level PAN dataset (see?3.3.2) which contains more realistic exam-ples of paraphrase and can prove useful to thecommunity for future evaluations of paraphraseidentification.BLEU-based features were also employed byWan et al (2006) who use them in combination withseveral other features based on dependency relationsand tree edit-distance inside an SVM.There are several other supervised approaches toparaphrase identification that do not use any featuresbased on MT metrics.
Mihalcea et al (2006) com-bine pointwise mutual information, latent semanticanalysis and WordNet-based measures of word se-mantic similarity into an arbitrary text-to-text sim-ilarity metric.
Qiu et al (2006) build a frame-work that detects dissimilarities between sentencesand makes its paraphrase judgment based on thesignificance of such dissimilarities.
Kozareva andMontoyo (2006) use features based on LCS, skipn-grams and WordNet with a meta-classifier com-posed of SVM, k-nearest neighbor and maximumentropy classifiers.
Islam and Inkpen (2007) mea-sure semantic similarity using a corpus-based mea-sure and a modified version of the Longest CommonSubsequence (LCS) algorithm.
Rus et al (2008)take a graph-based approach originally developedfor recognizing textual entailment and adapt it forparaphrase identification.
Fernando and Stevenson(2008) construct a matrix of word similarities be-tween all pairs of words in both sentences insteadof relying only on the maximal similarities.
Das andSmith (2009) use an explicit model of alignment be-tween the corresponding parts of two paraphrasticsentences and combine it with a logistic regressionclassifier built from n-gram overlap features.
Mostrecently, Socher et al (2011) employ a joint modelthat incorporates the similarities between both sin-gle word features as well as multi-word phrases ex-tracted from the parse trees of the two sentences.We compare our results to those from all the ap-proaches described in this section later in ?3.4.3 Classifying with MT MetricsIn this section, we first describe our overall approachto paraphrase identification that utilizes only MTmetrics.
We then discuss the actual MT metrics weused.
Finally, we describe the datasets on which weevaluated our approach and present our results.183MSRPThey had published an advertisement on the Internet on June 10,offering the cargo for sale, he added.On June 10, the ship?s owners had published an advertisement on theInternet, offering the explosives for sale.Security lights have also been installed and police have sweptthe grounds for booby traps.Security lights have also been installed on a barn near the front gate.PANDense fogs wrapped the mountains that shut in the little hamlet,but overhead the stars were shining in the near heaven.The hamlet is surrounded by mountains which is wrapped with densefogs, though above it, near heaven, the stars were shining.In still other places, the strong winds carry soil over longdistances to be mixed with other soils.In other places, where strong winds blow with frequent regularity,sharp soil grains are picked up by the air and hurled against therocks, which, under this action, are carved into fantastic forms.Table 1: Examples of paraphrases and non-paraphrases (in italics) from the MSRP and PAN corpora.3.1 ClassifierOur best system utilized a classifier combination ap-proach.
We used a simple meta-classifier that usesthe average of the unweighted probability estimatesfrom the constituent classifiers to make its final de-cision.
We used three constituent classifiers: Logis-tic regression, the SMO implementation of a supportvector machine (Platt, 1999; Keerthi et al, 2001)and a lazy, instance-based classifier that extends thenearest neighbor algorithm (Aha et al, 1991).
Weused the WEKA machine learning toolkit to performour experiments (Hall et al, 2009).
13.2 MT metrics used1.
BLEU (Papineni et al, 2002) is the most com-monly used metric for MT evaluation.
It iscomputed as the amount of n-gram overlap?for different values of n?between the systemoutput and the reference translation, temperedby a penalty for translations that might be tooshort.
BLEU relies on exact matching and hasno concept of synonymy or paraphrasing.
Weuse BLEU1 through BLEU4 as 4 different fea-1These constituent classifiers were chosen since they werethe top 3 performers in 5-fold cross-validation experimentsconducted on both MSRP and PAN training sets.
The meta-classifier was chosen similarly once the constituent classifiershad been chosen.tures for our classifier (hereafter BLEU(1-4)).2.
NIST (Doddington, 2002) is a variant of BLEUthat uses the arithmetic mean of n-gram over-laps, rather than the geometric mean.
It alsoweights each n-gram according to its informa-tiveness as indicated by its frequency.
We useNIST1 through NIST5 as 5 different featuresfor our classifier (hereafter NIST(1-5)).3.
TER (Snover et al, 2006) is defined as thenumber of edits needed to ?fix?
the translationoutput so that it matches the reference.
TERdiffers from WER in that it includes a heuris-tic algorithm to deal with shifts in addition toinsertions, deletions and substitutions.4.
TERp (TER-Plus) (Snover et al, 2009) buildsupon the core TER algorithm by providing ad-ditional edit operations based on stemming,synonymy and paraphrase.5.
METEOR (Denkowski and Lavie, 2010) usesa combination of both precision and recall un-like BLEU which focuses on precision.
Fur-thermore, it incorporates stemming, synonymy(via WordNet) and paraphrase (via a lookup ta-ble).6.
SEPIA (Habash and El Kholy, 2008) is asyntactically-aware metric designed to focus on184structural n-grams with long surface spans thatcannot be captured efficiently with surface n-gram metrics.
Like BLEU, it is a precision-based metric and requires a length penalty tominimize the effects of length.7.
BADGER (Parker, 2008) is a language inde-pendent metric based on compression and in-formation theory.
It computes a compressiondistance between the two sentences that utilizesthe Burrows Wheeler Transformation (BWT).The BWT enables taking into account commonsentence contexts with no limit on the size ofthese contexts.8.
MAXSIM (Chan and Ng, 2008) treats theproblem as one of bipartite graph matching andmaps each word in one sentence to at most oneword in the other sentence.
It allows the use ofarbitrary similarity functions between words.2Our choice of metrics was based on their popular-ity in the MT community, their performance in opencompetitions such as the NIST MetricsMATR chal-lenge (NIST, 2008) and the WMT shared evaluationtask (Callison-Burch et al, 2010), their availability,and their relative complementarity.3.3 DatasetsIn this section, we describe the two datasets that weused to evaluate our approach.3.3.1 Microsoft Research Paraphrase CorpusThe MSRP corpus was created by mining newsarticles on the web for topically similar articles andthen extracting potential sentential paraphrases us-ing a set of heuristics.
Extracted pairs were thenshown to two human judges with disagreementshandled by a third adjudicator.
The kappa was re-ported as 0.62, which indicates moderate to highagreement.
We used the pre-stipulated train-testsplits (4,076 sentence pairs in training and 1,725 intest) to train and test our classifier.2We also experimented with TESLA?a variant ofMAXSIM that performs better for MT evaluation?in our pre-liminary experiments However, both MAXSIM and TESLAperformed almost identically in our cross-validation experi-ments.
Therefore, we only retained MAXSIM in our final ex-periment since it was significantly faster to run than the versionof TESLA we had.3.3.2 Plagiarism Detection Corpus (PAN)We wanted to evaluate our approach on a set ofparaphrases where the semantic similarity was notsimply an accidental by-product of topical similaritybut rather consciously generated.
We used the testcollection from the PAN 2010 plagiarism detectioncompetition.
This dataset consists of 41,233 textdocuments from Project Gutenberg in which 94,202cases of plagiarism have been inserted.
The pla-giarism was created either by using an algorithm orby explicitly asking Turkers to paraphrase passagesfrom the original text.
We focus only on the human-created plagiarism instances.Note also that although the original PAN datasethas been used in plagiarism detection shared tasks,those tasks are generally formulated differently inthat the goal is to find all potentially plagiarized pas-sages in a given set of documents along with the cor-responding source passages from other documents.In this paper, we wanted to focus on the task of iden-tifying whether two given sentences can be consid-ered paraphrases.To generate a sentence-level PAN dataset, wewrote a heuristic alignment algorithm to find cor-responding pairs of sentences within a passage pairlinked by the plagiarism relationship.
The align-ment algorithm utilized only bag-of-words overlapand length ratios and no MT metrics.
For our nega-tive evidence, we sampled sentences from the samedocument and extracted sentence pairs that have atleast 4 content words in common.
We then sampledrandomly from both the positive and negative evi-dence files to create a training set of 10,000 sentencepairs and a test set of 3,000 sentence pairs.Table 1 shows examples of paraphrastic and non-paraphrastic sentence pairs from both the MSRP andPAN datasets.3.4 ResultsBefore presenting the results of experiments thatused multiple metrics as features, we wanted to de-termine how well each metric performs on its ownwhen used for paraphrase identification.
Table 2shows the classification results on both the MSRPand PAN datasets using each metric as the only fea-ture.
Although previously explored metrics such asBLEU and NIST perform reasonably well, they are185MSRP PANMetric Acc.
F1 Acc.
F1MAXSIM 67.2 79.4 84.7 83.4BADGER 67.6 79.9 88.5 87.9SEPIA 68.1 79.8 87.7 86.8TER 69.9 80.9 85.7 83.8BLEU(1-4) 72.3 80.9 87.9 87.1NIST(1-5) 72.8 81.2 88.2 87.3METEOR 73.1 81.0 89.5 88.9TERp 74.3 81.8 91.2 90.9Table 2: Classification results for MSRP and PAN withindividual metrics as features.
Entries are sorted by accu-racies on MSRP.clearly outperformed by some of the more robustmetrics such as TERp and METEOR.Table 3 shows the results of our experiments em-ploying multiple metrics as features, for both MSRPand PAN.
The final row in the table shows the resultsof our best system.
The remaining rows of this tableshow the top performing metrics for both datasets;we treat BLEU, NIST and TER as our baseline met-rics since they are not new and are not the primaryfocus of our investigation.
In terms of novel met-rics, we find that the top 3 metrics for both datasetswere TERp, METEOR and BADGER respectivelyas shown.
Combining all 8 metrics led to the bestperformance for MSRP but showed no performanceincrease for PAN.MSRP PANFeatures Acc.
F1 Acc.
F1Base Metrics 74.1 81.5 88.6 87.8+ TERp 75.6 82.5 91.5 91.2+ METEOR 76.6 83.2 92.0 91.8+ BADGER 77.0 83.7 92.3 92.1+ Others 77.4 84.1 92.3 92.1Table 3: The top 3 performing MT metrics for bothMSRP and PAN datasets as identified by ablation stud-ies.
BLEU(1-4), NIST(1-5) and TER were used as the 10base features in the classifiers.Our results for the PAN dataset are much better thanthose for MSRP since:(a) It is likely that our negative evidence is too easyfor most MT metrics.
(b) Many plagiarized pairs are linked simply vialexical synonymy which can be easily capturedby metrics like METEOR and TERp, e.g., thesentence ?Young?s main contention is that in lit-erature genius must make rules for itself, andthat imitation is suicidal?
is simply plagiarizedas ?Young?s major argument is that in litera-ture intellect must make rules for itself, andthat replication is dangerous.?
However, thePAN corpus does contains some very challeng-ing and interesting examples of paraphrases?even more so than MSRP?which we describein ?4.Finally, Table 4 shows that the results from ourbest system are the best ever reported on the MSRPtest set when compared to all previously publishedwork.
Furthermore, the single best performing met-ric (TERp)?also shown in the table?outperforms,by itself, many previous approaches utilizing multi-ple, complex features.Model Acc.
F1All Paraphrase Baseline 66.5 79.9(Mihalcea et al, 2006) 70.3 81.3(Rus et al, 2008) 70.6 80.5(Qiu et al, 2006) 72.0 81.6(Islam and Inkpen, 2007) 72.6 81.3(Fernando and Stevenson, 2008) 74.1 82.4TERp 74.3 81.8(Finch et al, 2005) 75.0 82.7(Wan et al, 2006) 75.6 83.0(Das and Smith, 2009) 76.1 82.7(Kozareva and Montoyo, 2006) 76.6 79.6(Socher et al, 2011) 76.8 83.6Best MT Metrics 77.4 84.1Table 4: Comparing the accuracy and F -score for the sin-gle best performing MT metric TERp (in gray) as well asthe best metric combination system (in gray and bold)with previously reported results on the MSRP test set(N = 1, 752).
Entries are sorted by accuracy.3.5 Metric ContributionsIn addition to quantitative results, we also wanted tohighlight specific examples from our datasets thatcan demonstrate the strength of the new metricsover simple n-gram overlap and edit-distance basedmetrics.
Below we present examples for the 4 best186metrics across both datasets:?
TERp uses stemming and phrasal paraphraserecognition to accurately classify the sentencepair ?For the weekend, the top 12 moviesgrossed $157.1 million, up 52 percent fromthe same weekend a year earlier.?
and ?Theoverall box office soared, with the top 12movies grossing $157.1 million, up 52 percentfrom a year ago.?
from MSRP as paraphrases.?
METEOR uses synonymy and stemmingto accurately classify the sentence pair ?Herletters at this time exhibited the two extremes offeeling in a marked degree.?
and ?Her lettersat this time showed two extremes of feelings.
?from PAN as plagiarized.?
BADGER uses unsupervised contextualsimilarity detection to accurately classify thesentence pair ?Otherwise they were false ormistaken reactions?
and ?Otherwise, were falseor wrong responses?
from PAN as plagiarized.?
SEPIA uses structural n-grams via dependencytrees to accurately classify the sentence pair?At his sentencing, Avants had tubes in hisnose and a portable oxygen tank beside him.
?and ?Avants, wearing a light brown jumpsuit,had tubes in his nose and a portable oxygentank beside him.?
from MSRP as paraphrases.4 Error AnalysisIn this section, we conduct an analysis of themisclassifications that our system makes on bothdatasets.
Our analyses consisted of finding the sen-tences pairs from the test set for each dataset whichnone of our systems (not just the best one) ever clas-sified correctly and inspecting a random sample of100 of these.
This inspection yields not only the topsources of error for an approach that relies solely onMT metrics but also uncovers sources of annotationerrors in both datasets themselves.4.1 MSRPIn their paper describing the creation of the MSRPcorpus, Dolan et al (2004) clearly state that ?the de-gree of mismatch allowed before the pair was judgednon-equivalent was left to the discretion of the indi-vidual rater?
and that ?many of the 33% of sentencepairs judged to be not equivalent still overlap signif-icantly in information content and even wording?.We found evidence that the raters were not alwaysconsistent in applying the annotation guidelines.
Forexample, in some cases the lack of attribution for aquotation led the raters to label a pair as paraphrasticwhereas in other cases it did not.
For example, thepair ?These are real crimes that hurt a lot of people.
?and ?
?These are real crimes that disrupt the lives ofreal people,?
Smith said.?
was not marked as para-phrastic.
Furthermore, even though the guidelinesinstruct the raters to ?treat anaphors and their fullforms as equivalent, regardless of how great the dis-parity in length or lexical content between the twosentences?, we found pairs of sentences marked asnon-paraphrastic which only differed in anaphora.However, the primary goal of this analysis is to findsources of errors in an MT-metric driven approachand below we present the top 5 such sources:1.
Misleading Lexical Overlap.
Non-paraphrastic pairs where there is largelexical overlap of secondary material betweenthe two sentences but the primary semanticcontent is different.
For example, ?GyorgyHeizler, head of the local disaster unit, said thecoach had been carrying 38 passengers.
?and ?The head of the local disasterunit, Gyorgy Heizler, said the coachdriver had failed to heed red stop lights.?.2.
Lack of World Knowledge.
Paraphrasticpairs that require world knowledge.
For ex-ample, ?Security experts are warning that anew mass-mailing worm is spreading widelyacross the Internet, sometimes posing as e-mail from the Microsoft founder.?
and ?Anew worm has been spreading rapidly acrossthe Internet, sometimes pretending to bean e-mail from Microsoft Chairman Bill Gates,antivirus vendors said Monday.?.3.
Tricky Phrasal Paraphrases.
Paraphras-187tic pairs that contain domain-dependent se-mantic alternations.
For example, ?Theleading actress nod went to energetic new-comer Marissa Jaret Winokur as Edna?sdaughter Tracy.?
and ?Marissa Jaret Winokur,as Tracy, won for best actress in a musical.?.4.
Date, Time and Currency Differences.
Para-phrastic pairs that contain different temporalor currency references.
These references werenormalized to generic tokens (e.g., $NUMBER)before being shown to MSRP raters but are re-tained in the released dataset.
For example,?Expenses are expected to be approximately$2.3 billion, at the high end of the previous ex-pectation of $2.2-to-$2.3 billion.?
and ?Spend-ing on research and development is expected tobe $4.4 billion for the year, compared with theprevious expectation of $4.3 billion.?.5.
Anaphoric References.
Paraphrastic pairswherein one member of the pair containsanaphora and the other doesn?t (these are con-sidered paraphrases according to MSRP guide-lines).
For example, ?They certainly reveal avery close relationship between Boeing and se-nior Washington officials.?
and ?The e-mailsreveal the close relationship between Boeingand the Air Force.
?.Note that most misclassified sentence pairs can becategorized into more than one of the above cate-gories.4.2 PANFor the PAN corpus, the only real source of error inthe dataset itself was the sentence alignment algo-rithm.
There were many sentence pairs that wereerroneously linked as paraphrases.
Leaving asidesuch pairs, the 3 largest sources of error for our MT-metric based approach were:1.
Complex Sentential Paraphrases.
By far,most of the misclassified pairs were paraphras-tic pairs that could be categorized as real worldplagiarism, i.e., where the plagiarizer copiesthe idea from the source but makes severalcomplex transformations, e.g., sentence split-ting, structural paraphrasing etc.
so as to ren-der an MT-metric based approach powerless.For example, consider the pair ?The schoolbears the honored name of one who, in the longyears of the anti-slavery agitation, was knownas an uncompromising friend of human free-dom.?
and ?The school is named after a manwho defended the right of all men and womento be free, all through the years when peoplecampaigned against slavery.?
Another inter-esting example is the pair ?The most unpromis-ing weakly-looking creatures sometimes live toninety while strong robust men are carried offin their prime.?
and ?Sometimes the strong per-sonalities live shorter than those who are unex-pected.?.2.
Misleading Lexical Overlap.
Similar toMSRP.
For example, ?Here was the second pe-riod of Hebraic influence, an influence whollymoral and religious.?
and ?This was the sec-ond period of Hellenic influence, an influencewholly intellectual and artistic.?.3.
Typographical and Spelling Errors.
Para-phrastic pairs where the Turkers creating theplagiarism also introduced other typos andspelling errors.
For example, ?The boat thenhad on board over 1,000 souls in all?
and?1000 people where on board at that tim?.5 DiscussionThe misses due to ?Date, Time, and Currency Dif-ferences?
are really just the result of an artifact inthe testing.
It is possible that an MT metrics basedapproach could accurately predict these cases if thereferences to dates etc.
were replaced with generictokens as was done for the human raters.
In asimilar vein, some of the misses that are due to alack of world knowledge might become hits if anamed entity recognizer could discover that ?Mi-crosoft founder?
is the same as ?Microsoft Chair-man?.
Similarly, some of the cases of anaphoric ref-erence might be recognized with an anaphora res-olution system.
And the problem of misspelling inPAN could be remedied with automatic spelling cor-rection.
Therefore, it is possible to improve the MTmetrics based approach further by utilizing certainNLP systems as pre-processing modules for the text.The only error category in MSRP and PAN188that caused false positives was ?Misleading LexicalOverlap?.
Here, the take-away message is that notevery part of a sentence is equally important for rec-ognizing semantic equivalence or non-equivalence.In a sentence that describes what someone commu-nicated, the content of what was said is crucial.
Forexample, despite lexical matches everywhere else,the mismatch of ?the coach had been carrying 38passengers?
and ?the driver had failed to heed thered stop lights?
disqualifies the respective sentencesfrom being paraphrases.
Along the same line, dif-ferences in proper names and their variants shouldreceive more weight than other words.
A sentenceabout ?Hebraic influence?
on a period in history isnot the same as a sentence which matches in ev-ery other way but is instead about ?Hellenic influ-ence?.
These sentences represent a bigger chal-lenge for an approach based solely on MT metrics.Given enough pairs of ?near-miss?
non-paraphrases,our system might be able to figure this out, but thiswould require a large amount of annotated data.6 ConclusionsIn this paper, we re-examined the idea that automaticmetrics used for evaluating translation quality canperform well explicitly for the task of paraphraserecognition.
The goal of our paper was to deter-mine whether approaches developed for the relatedbut different task of MT evaluation can be as com-petitive as approaches developed specifically for thetask of paraphrase identification.
While we do treatthe metrics as black boxes to an extent, we explic-itly chose metrics that were high performing but alsocomplementary in nature.Specifically, our re-examination focused on themore sophisticated MT metrics of the last few yearsthat claim to go beyond simple n-gram overlap andedit distance.
We found that a meta-classifier trainedusing only MT metrics outperforms all previous ap-proaches for the MSRP corpus.
Unlike previousstudies, we also applied our approach to a new pla-giarism dataset and obtained extremely positive re-sults.
We examined both datasets not only to findpairs that demonstrated the strength of each met-ric but also to conduct an error analysis to discoverthe top sources of errors that an MT metric basedapproach is susceptible to.
Finally, we discoveredthat using the TERp metric by itself provides fairlygood performance and can outperform many othersupervised classification approaches utilizing multi-ple, complex features.We also have two specific suggestions that we be-lieve can benefit the community.
First, we believethat binary indicators of semantic equivalence arenot ideal and a continuous value between 0 and 1indicating the degree to which two pairs are para-phrastic is more suitable for most approaches.
How-ever, rather than asking annotators to rate pairs ona scale, a better idea might be to show the sentencepairs to a large number of Turkers (?
20) on Ama-zon Mechanical Turk and ask them to classify it aseither a paraphrase or a non-paraphrase.
A simpleestimate of the degree of semantic equivalence ofthe pair is simply the proportion of the Turkers whoclassified the pair as paraphrastic.
An example ofsuch an approach, as applied to the task of grammat-ical error detection, can be found in (Madnani et al,2011).3 Second, we believe that the PAN corpus?with Turker simulated plagiarism?contains muchmore realistic examples of paraphrase and shouldbe incorporated into future evaluations of paraphraseidentification.
In order to encourage this, we are re-leasing our PAN dataset containing 13,000 sentencepairs.We are also releasing our error analysis data (100pairs for MSRP and 100 pairs for PAN) since theymight prove useful to other researchers as well.
Notethat the annotations for this analysis were producedby the authors themselves and, although, they at-tempted to accurately identify all error categories formost sentence pairs, it is possible that the errors insome sentence pairs were not comprehensively iden-tified.4AcknowledgmentsWe would like to thank Aoife Cahill, Michael Heil-man and the three anonymous reviewers for theiruseful comments and suggestions.3A good approximation is to use an ordinal scale for thehuman judgments as in the Semantic Textual Similarity taskof SemEval 2012.
See http://www.cs.york.ac.uk/semeval-2012/task6/ for more details.4The data is available at http://bit.ly/mt-para.189ReferencesD.
W. Aha, D. Kibler, and M. K. Albert.
1991.
Instance-based learning algorithms.
Mach.
Learn., 6:37?66.C.
Callison-Burch, P. Koehn, C. Monz, K. Peterson, andO.
Zaidan, editors.
2010.
Proceedings of the JointFifth Workshop on Statistical Machine Translation andMetricsMATR.Y.
S. Chan and H. T. Ng.
2008.
MAXSIM: A maxi-mum similarity metric for machine translation evalua-tion.
In Proceedings of ACL-HLT, pages 55?62.D.
Das and N.A.
Smith.
2009.
Paraphrase Identifica-tion as Probabilistic Quasi-synchronous Recognition.In Proceedings of ACL-IJCNLP, pages 468?476.M.
Denkowski and M. Lavie.
2010.
Extending theMETEOR Machine Translation Metric to the PhraseLevel.
In Proceedings of NAACL.G.
Doddington.
2002.
Automatic Evaluation of MachineTranslation Quality using N-gram Co-occurrenceStatistics.
In Proceedings of HLT, pages 138?145.B.
Dolan, C. Quirk, and C. Brockett.
2004.
Unsuper-vised Construction of Large Paraphrase Corpora: Ex-ploiting Massively Parallel News Sources.
In Proceed-ings of COLING, pages 350?356, Geneva, Switzer-land.S.
Fernando and M. Stevenson.
2008.
A Semantic Simi-larity Approach to Paraphrase Detection.
In Proceed-ings of the Computational Linguistics UK (CLUK)11th Annual Research Colloquium.A.
Finch, Y.S.
Hwang, and E. Sumita.
2005.
Using Ma-chine Translation Evaluation Techniques to DetermineSentence-level Semantic Equivalence.
In Proceedingsof the Third International Workshop on Paraphrasing,pages 17?24.N.
Habash and A. El Kholy.
2008.
SEPIA: SurfaceSpan Extension to Syntactic Dependency Precision-based MT Evaluation.
In Proceedings of the Workshopon Metrics for Machine Translation at AMTA.M.
Hall, E. Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: An Update.SIGKDD Explorations, 11.A.
Islam and D. Inkpen.
2007.
Semantic Similarity ofShort Texts.
In Proceedings of RANLP, pages 291?297.J.
J. Jiang and D. W. Conrath.
1997.
Semantic Similar-ity Based on Corpus Statistics and Lexical Taxonomy.CoRR, cmp-lg/9709008.S.
S. Keerthi, S. K. Shevade, C. Bhattacharyya, andK.
R. K. Murthy.
2001.
Improvements to Platt?s SMOAlgorithm for SVM Classifier Design.
Neural Com-put., 13(3):637?649.Z.
Kozareva and A. Montoyo.
2006.
Paraphrase Identi-fication on the Basis of Supervised Machine LearningTechniques.
In Proceedings of FinTAL, pages 524?233.N.
Madnani, J. Tetreault, M. Chodorow, and A. Ro-zovskaya.
2011.
They Can Help: Using Crowdsourc-ing to Improve the Evaluation of Grammatical ErrorDetection Systems.
In Proceedings of ACL (Short Pa-pers).R.
Mihalcea, C. Corley, and C. Strapparava.
2006.Corpus-based and Knowledge-based Measures OfText Semantic Similarity.
In Proceedings of AAAI,pages 775?780.NIST.
2008.
NIST MetricsMATR Challenge.
Informa-tion Access Division.
http://www.itl.nist.gov/iad/mig/tests/metricsmatr/.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.BLEU: A Method for Automatic Evaluation of Ma-chine Translation.
In Proceedings of ACL.S.
Parker.
2008.
BADGER: A New Machine TranslationMetric.
In Proceedings of the Workshop on Metricsfor Machine Translation at AMTA.John C. Platt.
1999.
Advances in kernel methods.
chap-ter Fast Training of Support Vector Machines using Se-quential Minimal Optimization, pages 185?208.
MITPress.M.
Potthast, B. Stein, A. Barro?n-Ceden?o, and P. Rosso.2010.
An Evaluation Framework for Plagiarism De-tection.
In Proceedings of COLING, pages 997?1005.L.
Qiu, M. Y. Kan, and T. S. Chua.
2006.
ParaphraseRecognition via Dissimilarity Significance Classifica-tion.
In Proceedings of the EMNLP, pages 18?26.V.
Rus, P.M. McCarthy, M.C.
Lintean, D.S.
McNamara,and A.C. Graesser.
2008.
Paraphrase Identificationwith Lexico-Syntactic Graph Subsumption.
In Pro-ceedings of FLAIRS, pages 201?206.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A Study of Translation Edit Ratewith Targeted Human Annotation.
In Proceedings ofAMTA.M.
Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009.TER-Plus: Paraphrase, Semantic, and Alignment En-hancements to Translation Edit Rate.
Machine Trans-lation, 23(2?3):117?127.R.
Socher, E.H. Huang, J. Pennington, A.Y.
Ng, and C.D.Manning.
2011.
Dynamic Pooling and UnfoldingRecursive Autoencoders for Paraphrase Detection.
InAdvances in Neural Information Processing Systems24 (NIPS).S.
Wan, R. Dras, M. Dale, and C. Paris.
2006.
UsingDependency-based Features to Take the ?para-farce?Out of Paraphrase.
In Proceedings of the AustralasianLanguage Technology Workshop (ALTW), pages 131?138.190
