Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 37?43,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsA Platform for Automated Acoustic Analysis for Assistive Technology   Suzanne Boyce Harriet Fell Department of Communication Sciences and Disorders College of Computer and Information Science University of Cincinnati Northeastern University Cincinnati, Ohio, 45267, USA Boston, Massachusetts, 02115, USA boycese@ucmail.uc.edu fell@ccs.neu.edu   Joel MacAuslan Lorin Wilde Speech Technology and Applied Research Boston University 54 Middlesex Turnpike  Bedford, Massachusetts,  , USA Boston, Massachusetts, 02180, USA joelm@staranalyticalservices.com wildercom@gmail.comAbstractThe use of speech production data has been limited by a steep learning curve and the need for laborious hand measurement.
We are building a tool set that provides summary sta-tistics for measures designed by clinicians to screen, diagnose or provide training to assis-tive technology users.
This will be achieved by extending an existing shareware software platform with ?plug-ins?
that perform specific measures and report results to the user.
The common underlying basis for this tool set is a Stevens?
paradigm of landmarks, points in an utterance around which information about ar-ticulatory events can be extracted.1 Introduction  To date, the use of speech production data has been limited by a steep learning curve and the need for laborious hand measurement.
Many speech-related studies result in voluminous acoustic data.
Many clinicians who design and use assistive technology would like to incorporate acoustic analysis, but have been discouraged because of these technical challenges.
We are in the process of developing a set of tools that considerably streamlines the proc-ess of analyzing speech production details.We are building a tool set to provide summary sta-tistics for measures designed by clinicians to screen, diagnose or provide training to patients.
This will be achieved by extending an existing shareware software platform with ?plug-ins?
that perform specific measures and report results to the user.
At present, our goal is to use the existing shareware software tool Wavesurfer (Wavesurfer, 2005).
The new modules will be set up to report data from a single audio file, or groups of audio files in a standard table format, for easy input to statistical or other analysis software.
For example, the data may be imported into a program that cor-relates speech data with scalp electrode and medi-cation data.
Our tool will include alternative and independently tested algorithms for clinically relevant measures, as well as guidance as to what the speech data may mean.
The common underlying basis for this tool set is a focused set of landmarks derived from Stevens?
Lexical Access from Features (LAFF) paradigm (Stevens, 1992, 2002; Liu, 1995; Slifka et al, 2004).
In this approach, landmarks are points in an utterance around which information about articula-tory events can be extracted.37In what follows, we will describe (1) the theoreti-cal rationale of landmarks, (2) the general utility of landmark processing and several examples of clin-ically related measures, and (3) our current work on developing tools to make landmark analysis more widely available.
2 Landmarks reflect articulation Landmark analysis is based on the fact that differ-ent sounds produce different patterns of abrupt changes in the acoustic signal simultaneously across wide frequency ranges.
For instance, the abrupt increase in amplitude for a broad range of frequencies above 3 kHz can be used to indicate the onset of bursts.
Likewise, an abrupt decrease in the same frequency bands can be used to indi-cate the end of frication.
The use of onset and off-set data in other frequency bands can be used to indicate sonorancy; i.e., intervals when the oral cavity is relatively unconstricted.
Examples based on Liu [1995] are listed below.
g(lottis): marks the onset (+g) or offset (-g) of voicing.
s(yllabicity): marks the onset (+s) or offset (-s) of syllabicity, i.e.
onsets and releases of voiced sono-rant consonants such as /l/ or /r/, vocal tract clo-sures due to voiced stop consonants such as /b/ or /d/.
b(urst): marks the onset (+b) of the burst of air following stop or affricate consonant release, or the onset of frication noise for fricative consonants.
Offsets (-b) mark points where aspiration or frica-tion noise ends abruptly due to a stop closure.
V(owel):  marks points of peak amplitude in a so-norant region?that is, a region where voicing is evident [Howitt, 2000].
Although much of the past work using landmark processing has been focused on employing a wide variety of landmarks to recognize the lexical con-tent of speech [Juneja and Espy-Wilson 2003, Slif-ka, et al 2004], the power of these measures is even more apparent when applied to non-lexical attributes.3 Applications of Landmark Analysis to Assistive Technology3.1 Tracking Articulatory Precision Measuring articulatory precision is important to evaluating efficacy of a treatment or in monitoring disease progression, e.g.
in Parkinson?s disease.
Given that landmarks reflect articulation, tools based on landmarks may be useful for measuring and monitoring articulatory precision [Boyce et al 2005, 2007].
The technique relies on setting em-pirically derived thresholds for the detection of abrupt acoustic changes in specified frequency bands.
Recall that changes in the acoustic signal occur simultaneously across wide frequency ranges.
When the onset of energy does not exceed threshold in a particular frequency band, i.e., not quite abrupt enough to trigger the detection of a landmark, then no landmark may be assigned.
However, since different sounds produce different patterns, changes detected in other bands at that point in time are either a) assigned to a different landmark, or b) considered to be extraneous.
Thus, small acoustic differences in the way speech is produced can be tracked as different patterns of landmarks.
In addition to requirements that a tool for general clinical use must be fast and robust, it must be able to handle a wide variety of speaking styles, dia-lects, and voices.
By focusing on landmarks that specify syllable structure and broad phoneme classes, distinctive differences between phonemes can be ignored.
Therefore, the tool is less likely to break down due to problems recognizing specific vocabulary while remaining sensitive to changes in the acoustic signal that reflect articulatory preci-sion of speech.
3.2 Evaluating phonological complexity Development of speech in early infancy includes the ability to produce increasingly complex phonological structure.
Patterns of syllable struc-ture in speech output can be tracked using land-marks, again without reference to specific phonemes or words.
In Fell et al [2002], land-marks were grouped into standard syllable patterns and syllables were grouped into utterances.
Statis-38tics based on these patterns were then reported to the clinician for various uses in training, screening or diagnosis.
Patterns of syllable complexity were used to compute a "vocalization age."
This was used in turn to derive screening rules that clinically distinguish infants who may be at risk for later communication or other developmental problems from typically developing infants.
3.3 Measuring and Evaluating ?Clear Speech?
?Clear Speech?
is an intelligibility-enhancing style of speech that is used to improve communication outcomes.
Listeners with hearing impairment de-rive significant benefit from being addressed with clearly articulated speech.
Speech that is more clearly articulated contains more abrupt acoustic changes.
The result is that speech with different levels of intelligibility shows different numbers and combinations of landmarks [Boyce et al 2005, 2007].
3.4 Other Applications In the UCARE project [1995], Cress reported ana-lyzing 40 hours of pre-existing [2005] videotaped sessions of children with physical or neurological impairments using landmark-based tools.
Fell et al [2004] reported using landmark analysis to follow the progress of several children with se-vere speech delays.
In this project, 10-minute, in-home audio recordings were processed in real-time on a 2002-era PC laptop.
Wade and M?bius [2007] used automated land-mark analysis to study speaking rate effects as a measure of disease progression in Parkinson's dis-ease.
DiCicco and Patel [2008] used automatic landmark analysis on dysarthric speech.
This study provides quantitative support for the hypothesis [Deller 1991] that dysarthric speech includes erroneous additional acoustic cues, not only malformed or missing ones.4 Potential Benefits of Landmark Appli-cations In a small study, Warner-Czyz and Davis [2010] compared consonant?vowel syllable accuracy in early words of children with normal hearing and children with hearing loss who received cochlear implantation.
They found and evaluated, via man-ual coding, approximately 4000 syllables from 48 hours of recordings.
This is a project where auto-matic landmark analysis might have greatly re-duced the effort.
Similarly, in a study on tongue-twisters, Matthew Goldrick (Northwestern University) collected 100 hours of data comprising 20,000 tokens in less than three weeks, but found that it required another 600 hours merely to segment and label the data for fur-ther analysis.
In personal correspondence about another study on single words, he stated:  A major ?choke point?
for speech production research is the need to manually analyze speech data.
Given that many thousands of data points are typically required to gain accu-rate estimates of probability density functions along phonetic dimensions, hundreds of per-son-hours are typically required to analyze da-ta from a single simple experiment?.
If we could gain access to reliable, highly accurate automated tools, we could change the speed of research by an order of magnitude.
Researchers who currently want to use speech analysis as a tool must accept long periods of hand measurements.
This discourages researchers who may be more interested in a particular neurological disease or process than in speech research per se.
It is notoriously difficult to quantify projects not undertaken, or papers not written, but it is telling that, although each of the studies cited above re-ported positive results from a study of speech ar-ticulation, they exist as relative islands in their respective disciplines.
We contend that this situa-tion exists largely because of barriers to entry; that is, we believe that many scientists would like to use speech assessment as part of their research, but elect not to do for lack of a convenient tool.
The existence of a convenient tool to detect, measure and track subtle changes in speech articulation would constitute an enabling technology.395 Tools5.1 Description In our own work, we have developed an automatic tool for detecting, counting and analyzing acoustic events in the speech signal that are commonly used by scientists to measure differences in speech ar-ticulation.
We are now integrating our system with Wave-surfer for certain researchers (linguists, speech-language pathologists, certain engineering and cognitive-science researchers) with a primary in-terest in inspecting and interpreting the articula-tion-related features in the waveforms of a corpus: e.g., the placement of landmarks of each type, pat-terns of clustering, or identification of non-speech sounds to be excised.
(See Figure 1).
)For this version, we are implementing user controls (?widgets?)
to produce automated measures or types of analyses for speech research such as: ?
Voice-onset time, VOT.
?
Detection of non-harmonic (and harmonic) voicing.
?
Identification and suppression or removal of stray sounds, i.e., non-speech.
?
Grouping of landmarks into syllable-like clus-ters.
(Note that Wavesurfer already provides a general pitch-tracking capability for harmonic voicing.)
The Wavesurfer plug-in will also allow the user to output information about an audio file or a direc-tory of audio files, e.g.
all the recordings of a child.
This information will be in a tab-delimited text file or a spreadsheet.
This will allow the speech scien-tistFigure 1: Wavesurfer with landmarks/waveform pane filtered to show only +/-g landmarks, and tran-scription pane (top) with +/-g and +/-s landmarks40This information will be in a tab-delimited text file or a spreadsheet.
This will allow the speech scien-tist to analyze the output and, for example, to summarize and compare the typically developing children to those diagnosed with autism.
5.2 User Testing We are currently recruiting potential users to test the system including graduate students and senior researchers in neurosciences and speech-related sciences.
So that these users can test the system on a realistic problem, we will provide them with a corpus of annotated, de-identified recordings of children with and without a diagnosis of autism.
This will provide context for specific training tasks that we ask of the users and enable them to formu-late their own appropriate, if small, research ques-tions that the system can help to answer.
We will probe their experiences by logging the questions they have about the system, watching their actions as they attempt to answer the research questions, and asking their opinions of the experience after-ward.
6 Requested Features In an early trial of our Waversurfer plug-in, a user requested the VOT (voice-onset time) measure.
In response to this request, we are now adding a VOT-transcription pane to display the automati-cally computed voice onset times aligned with the waveform, spectrogram, and displayed informa-tion.
The information in this pane is also auto-matically saved to a text file that can be analyzed with other software.
This request also led us to include a popup window to show the vowel-space in a recording.
Vowel-space measures are conventionally labor-intensive, thus limited to a few instances of specific vowels, and require that the researcher first identify spe-cific instances of these vowels.
On the other hand,  vocalic landmarks identify the instants where for-mant frequencies may be reliably estimated, so our tools can quickly and automatically evaluate the full vowel space of a passage.
(See Figure 2.
)Figure 2: Automatic Vowel-Space Evaluation.
Com-puting the resonant frequencies (formants) at vowel landmarks allows plotting the vowel space, i.e., the scat-ter of the first two formants against each other.
In this case, a female read the complete Rainbow Passage (a standard passage of 3 paragraphs, approx.
90 sec of reading).
The system automatically identified all the consonantal and vocalic landmarks, evaluated the for-mants at ~ 140 stressed vowels, and computed the con-vex hull (?rubber-band?)
area, 0.88 kHz2.
Total computation time on a commodity 3 GHz PC was 143 sec (and is directly proportional to the duration of the passage).
7 Challenges for Software development, Challenges for availability Our algorithms are implemented in MATLAB.
Though toolkits that run in MATLAB might be available free, or for a modest price, the MATLAB platform itself is costly, especially for non-academic users.
On the other hand, shareware or freeware may have minimal documentation; sup-port that depends entirely on the presence (or ab-sence!)
of a knowledgeable user community; and variable standards for testing, correctness, and per-formance.
A critical hidden cost for any system is the learn-ing curve.
For those systems with little documen-tation and training, this can dwarf the overt costs.
Our goal is to make learning easier by creating landmark-processing plug-ins that people can use within software that they already employ.41Such a plan requires a careful balance between the flexibility of a general, extensible system and the simplicity of a small, fixed set of easily docu-mented plug-in capabilities.
Our project therefore includes both a small set of simple functions, such as VOT, and software design centered on the needs identified by users from the appropriate research communities.
Our design relies on an iterative process of structured interviews and web-based surveys, combined with observations of user expe-riences with our plug-ins.
This user study extends beyond the matter of func-tionality and documentation.
It also addresses the expectations or requirements for convenient avail-ability, training, and support, and the costs that these imply.
8 Future work8.1 R ?
statistical analysis system We will integrate our software with R (http://www.r-project.org/) for those with a pri-mary interest instead in the derived articulatory-precision information: e.g., syllable production rate, fraction of syllables of a given complexity, or range of vowels.
For this platform, we will implement further user-level functions, with corresponding graphical user interfaces as appropriate, to produce:  ?
Number of landmarks, optionally excluding those that are automatically detected as noise-related.
?
Syllable complexity and statistics of same.
?
Utterance complexity.
?
Syllable production rate.
?
Articulatory precision.
?
Vowel space measures.8.2 Other Platforms We plan to expand our work to include plugins or packages for integration with a wider (and more powerful) collection of research tools, for example PRAAT, CSL, or even Excel.8.3 Other Features We are soliciting input from user communities about the features they would like to see in these tools.
Acknowledgments This work was funded in part by NIH grant R43 DC010104.
References  Suzanne Boyce, Joel MacAuslan, Ann Bradlow, and Rajka Smiljani?.
2007.
Automatic Detection of Dif-ferences Between Clear & Conversational Speech, poster presented at American Speech-Language-Hearing Convention.
Suzanne Boyce, Ann Bradlow, and Joel MacAuslan.
2005.
Landmark analysis of clear and conversational speaking styles, 150th meeting of the Acoustical So-ciety of America.
Thomas DiCicco and Rupal Patel.
2008.
Automatic Landmark Analysis of Dysarthric Speech, Journal of Medical Speech-Language Pathology, 16(4):213-219.
Cynthia J. Cress, S. Unrein, A. Weber, S. Krings, H. Fell, J. MacAuslan, and J. Gong.
2005.
Vocal Devel-opment Patterns in Children at Risk for Being Non-speaking.
ASHA 2005.
Cynthia J. Cress.
1995.
Communicative and symbolic precursors of AAC, Unpublished NIH CIDA Grant: University of Nebraska-Lincoln.
Jack R. Deller,  D. Hsu, and Linda J. Ferrier.
1991.
On the Use of Hidden Markov Modeling for Recognition of Dysarthric Speech, Computer Methods and Pro-grams in Biomedicine.
(35)2:125-139.
Harriet J.
Fell, Joel MacAuslan, Linda J. Ferrier, Susan G. Worst, and Karen Chenausky.
2002.
Vocalization Age as a Clinical Tool.
Procroceedings of the Inter-national Conference on Speech and Language Proc-essing.
Harriet J.
Fell, Joel MacAuslan, Cynthia.
Cress, Linda J. Ferrier.
2004. visiBabble for Reinforcement of Early Vocalization, Proceedings of ASSETS 2004.
161-168.
Wilson Howitt.
2000.
Unpublished Ph.D. dissertation, Massachusetts Institute of Technology.
Amit Juneja and Carol Espy-Wilson.
2003, Speech Segmentation Using Probabilistic Phonetic Feature Hierarchy and Support Vector Machines.
Proceed-ings of the International Joint Conference on Neural Networks.
Sharlene A. Liu.
1995.
Landmark Detection for Distinc-tive Feature-Hyphen Based Speech Recognition, M.I.T.
Doctoral Thesis.
R, http://www.r-project.org/42Janet Slifka, Kenneth N. Stevens, Sharon Manuel, and Stefanie Shattuck-Hufnagel.
2004.
A Landmark-Based Model of Speech Perception: History and Re-cent Developments.
From Sound to Sense, 85-90.
Kenneth N. Stevens, 2000.
Acoustic Phonetics, The MIT Press, Cambridge, Massachusetts.
Kenneth N. Stevens.
2002.
Toward a model for lexical access based on acoustic landmarks and distinctive features, Journal of the Acoustic Society of America.
111(4):1872-1891.
Kenneth N. Stevens, Sharon Manuel, Stefanie Shattuck-Hufnagel, and Sharlene Liu.
1992.
Implementation of a model for lexical access based on features,  Proced-ings ICSLP (Int.
Conf.
on Speech & Language Proc-essing).
499-502.
Travis Wade, Bernd M?bius.
2007.
Speaking rate ef-fects in a landmark-based phonetic exemplar model, Interspeech 2007.
402-405.
Wavesurfer.2005.
http://www.speech.kth.se/wavesurfer/43
