Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 755?762,Sydney, July 2006. c?2006 Association for Computational LinguisticsUnsupervised Topic Identification by Integrating Linguistic andVisual Information Based on Hidden Markov ModelsTomohide ShibataGraduate School of Information Scienceand Technology, University of Tokyo7-3-1 Hongo, Bunkyo-ku,Tokyo, 113-8656, Japanshibata@kc.t.u-tokyo.ac.jpSadao KurohashiGraduate School of Informatics,Kyoto UniversityYoshida-honmachi, Sakyo-ku,Kyoto, 606-8501, Japankuro@i.kyoto-u.ac.jpAbstractThis paper presents an unsupervised topicidentification method integrating linguis-tic and visual information based on Hid-den Markov Models (HMMs).
We employHMMs for topic identification, wherein astate corresponds to a topic and variousfeatures including linguistic, visual andaudio information are observed.
Our ex-periments on two kinds of cooking TVprograms show the effectiveness of ourproposed method.1 IntroductionRecent years have seen the rapid increase of mul-timedia contents with the continuing advance ofinformation technology.
To make the best useof multimedia contents, it is necessary to seg-ment them into meaningful segments and annotatethem.
Because manual annotation is extremely ex-pensive and time consuming, automatic annotationtechnique is required.In the field of video analysis, there have beena number of studies on shot analysis for videoretrieval or summarization (highlight extraction)using Hidden Markov Models (HMMs) (e.g.,(Chang et al, 2002; Nguyen et al, 2005; Q.Phunget al, 2005)).
These studies first segmented videosinto shots, within which the camera motion is con-tinuous, and extracted features such as color his-tograms and motion vectors.
Then, they classi-fied the shots based on HMMs into several classes(for baseball sports video, for example, pitch view,running overview or audience view).
In thesestudies, to achieve high accuracy, they relied onhandmade domain-specific knowledge or trainedHMMs with manually labeled data.
Therefore,they cannot be easily extended to new domainson a large scale.
In addition, although linguisticinformation, such as narration, speech of charac-ters, and commentary, is intuitively useful for shotanalysis, it is not utilized by many of the previousstudies.
Although some studies attempted to uti-lize linguistic information (Jasinschi et al, 2001;Babaguchi and Nitta, 2003), it was just keywords.In the field of Natural Language Processing,Barzilay and Lee have recently proposed a prob-abilistic content model for representing topics andtopic shifts (Barzilay and Lee, 2004).
This contentmodel is based on HMMs wherein a state corre-sponds to a topic and generates sentences relevantto that topic according to a state-specific languagemodel, which are learned from raw texts via anal-ysis of word distribution patterns.In this paper, we describe an unsupervised topicidentification method integrating linguistic and vi-sual information using HMMs.
Among severaltypes of videos, in which instruction videos (how-to videos) about sports, cooking, D.I.Y., and oth-ers are the most valuable, we focus on cookingTV programs.
In an example shown in Figure 1,preparation, sauteing, and dishing up are automat-ically labeled in sequence.
Identified topics lead tovideo segmentation and can be utilized for videosummarization.Inspired by Barzilay?s work, we employ HMMsfor topic identification, wherein a state corre-sponds to a topic, like preparation and frying, andvarious features, which include visual and audioinformation as well as linguistic information (in-structor?s utterances), are observed.
This studyconsiders a clause as an unit of analysis and thefollowing eight topics as a set of states: prepara-tion, sauteing, frying, baking, simmering, boiling,dishing up, steaming.In Barzilay?s model, although domain-specific755cut:1 saute:1 add:3 put:2preparation sauteing dishing up???????
?preparationsauteingdishing upsilencecue phrase?then?tCut an avocado.
We?ll saute.
Add spices.identifiedtopic:hiddenstatesobserveddatautterancecase frameimagePut cheese betweenslices of bread.Figure 1: Topic identification with Hidden Markov Models.word distribution can be learned from raw texts,their model cannot utilize discourse features, suchas cue phrases and lexical chains.
We incorpo-rate domain-independent discourse features suchas cue phrases, noun/verb chaining, which indicatetopic change/persistence, into the domain-specificword distribution.Our main claim is that we utilize visual and au-dio information to achieve robust topic identifi-cation.
As for visual information, we can utilizebackground color distribution of the image.
Forexample, frying and boiling are usually performedon a gas range and preparation and dishing up areusually performed on a cutting board.
This infor-mation can be an aid to topic identification.
As foraudio information, silence can be utilized as a clueto a topic shift.2 Related WorkIn Natural Language Processing, text segmenta-tion tasks have been actively studied for infor-mation retrieval and summarization.
Hearst pro-posed a technique called TextTiling for subdivid-ing texts into sub-topics (Hearst.M, 1997).
Thismethod is based on lexical co-occurrence.
Galleyet al presented a domain-independent topic seg-mentation algorithm for multi-party speech (Gal-ley et al, 2003).
This segmentation algorithmuses automatically induced decision rules to com-bine linguistic features (lexical cohesion and cuephrases) and speech features (silences, overlapsand speaker change).
These studies aim just atsegmenting a given text, not at identifying topicsof segmented texts.Marcu performed rhetorical parsing in theframework of Rhetorical Structure Theory (RST)based on a discourse-annotated corpus (Marcu,2000).
Although this model is suitable for ana-lyzing local modification in a text, it is difficult forthis model to capture the structure of topic transi-tion in the whole text.In contrast, Barzilay and Lee modeled a con-tent structure of texts within specific domains,such as earthquake and finance (Barzilay and Lee,2004).
They used HMMs wherein each state cor-responds to a distinct topic (e.g., in earthquakedomain, earthquake magnitude or previous earth-quake occurrences) and generates sentences rel-evant to that topic according to a state-specificlanguage model.
Their method first create clus-ters via complete-link clustering, measuring sen-tence similarity by the cosine metric using wordbigrams as features.
They calculate initial proba-bilities: state si specific language model psi(w?|w)756?????????
(Cut a Chinese cabbage.)????????????????
(Cut off its root and wash it.)???????????????
(A Japanese radish would taste delicious.)??3????????
(Divide it into three equal parts.)??????????
(Now, we'll  saute.)??
[individual action][individual action] [individual action][substitution][individual action][action declaration]????????????????????
(Just a little more and go for it!
)[small talk][small talk]cut:1cut off:1 wash:1divide:3saute:1Figure 2: An example of closed captions.
(The phrase sandwiched by a square bracket means an utterancetype and the word surrounded by a rectangle means an extracted utterance referring to an action.
Thebold word means a case frame assigned to the verb.
)and state-transition probability p(sj|si) from statesi to state sj .
Then, they continue to estimateHMM parameters with the Viterbi algorithm un-til the clustering stabilizes.
They applied the con-structed content model to two tasks: informationordering and summarization.
We differ from thisstudy in that we utilize multimodal features anddomain-independent discourse features to achieverobust topic identification.In the field of video analysis, there have beena number of studies on shot analysis with HMMs.Chang et al described a method for classifyingshots into several classes for highlight extractionin baseball games (Chang et al, 2002).
Nguyenet al proposed a robust statistical framework toextract highlights from a baseball video (Nguyenet al, 2005).
They applied multi-stream HMMsto control the weight among different features,such as principal component features capturingcolor information and frame-difference featuresfor moving objects.
Phung et al proposed a prob-abilistic framework to exploit hierarchy structurefor topic transition detection in educational videos(Q.Phung et al, 2005).Some studies attempted to utilize linguisticinformation in shot analysis (Jasinschi et al,2001; Babaguchi and Nitta, 2003).
For exam-ple, Babaguchi and Nitta segmented closed cap-tion text into meaningful units and linked them tovideo streams in sports video.
However, linguisticinformation they utilized was just keywords.3 Features for Topic IdentificationFirst, we?ll describe the features that we use fortopic identification, which are listed in Table 1.They consist of three modalities: linguistic, visualand audio modality.We utilize as linguistic information the instruc-tor?s utterances in video, which can be divided intovarious types such as actions, tips, and even smalltalk.
Among them, actions, such as cut, peel andgrease a pan, are dominant and supposed to be use-ful for topic identification and others can be noise.In the case of analyzing utterances in video, itis natural to utilize visual information as well aslinguistic information for robust analysis.
We uti-lize background image as visual information.
Forexample, frying and boiling are usually performedon a gas range and preparation and dishing up areusually performed on a cutting board.Furthermore, we utilize cue phrases and silenceas a clue to a topic shift, and noun/verb chainingas a clue to a topic persistence.We describe these features in detail in the fol-lowing sections.3.1 Linguistic FeaturesClosed captions of Japanese cooking TV programsare used as a source for extracting linguistic fea-757Table 1: Features for topic identification.Modality Feature Domain dependent Domain independentlinguistic case frame utterance generalizationcue phrases topic changenoun chaining topic persistenceverb chaining topic persistencevisual background image bottom of imageaudio silence topic changeTable 2: Utterance-type classification.
(An underlined phrase means a pattern for recognizing utterancetype.
)[action declaration]ex.
????????????????
(Then, we ?ll cook a steak)?????????????
(OK, we?ll fry.
)[individual action]ex.
???????????
(Cut off a step of this eggplant.)???????????
(Pour water into a pan.
)[food state]ex.
????????????????
(There is no water in the carrot.)[note]ex.
???????????
(Don?t cut this core off.)[substitution]ex.
??????????
(You may use a leek.
)[food/tool presentation]ex.
??????????????????
Today, we use this handy mixer.
)[small talk]ex.
??????
(Hello.)tures.
An example of closed captions is shown inFigure 2.
We first process them with the Japanesemorphological analyzer, JUMAN (Kurohashi etal., 1994), and make syntactic/case analysis andanaphora resolution with the Japanese analyzer,KNP (Kurohashi and Nagao, 1994).
Then, weperform the following process to extract linguis-tic features.3.1.1 Extracting Utterances Referring toActionsConsidering a clause as a basic unit, utterancesreferring to an action are extracted in the formof case frame, which is assigned by case analy-sis.
This procedure is performed for generaliza-tion and word sense disambiguation.
For exam-ple, ??????
(add salt)?
and ?????????
(add sugar into a pan)?
are assigned to caseframe ireru:1 (add) and ???????
(carve witha knife)?
is assigned to case frame ireru:2 (carve).We describe this procedure in detail below.Utterance-type recognitionTo extract utterances referring to actions, weclassify utterances into several types listed in Ta-ble 21.
Note that actions are supposed to have twolevels: [action declaration] means a declaration ofbeginning a series of actions and [individual ac-tion] means an action that is the finest one.1In this paper, [ ] means an utterance type.Input sentences are first segmented intoclauses and their utterance type is recognized.Among several utterance types, [individual ac-tion], [food/tool presentation], [substitution],[note], and [small talk] can be recognized byclause-end patterns.
We prepare approximately500 patterns for recognizing the utterance type.
Asfor [individual action] and [food state], consider-ing the portability of our system, we use generalrules regarding intransitive verbs or adjective + ???
(become)?
as [food state], and others as [in-dividual action].Action extractionWe extract utterances whose utterance type isrecognized as action ([action declaration] or [indi-vidual action]).
For example, ???
(peel)?
and ???
(cut)?
are extracted from the following sen-tence.
(1) ?????????
[individual action]??????????
[individual action]?
(Wepeel this carrot and cut it in half.
)We make two exceptions to reduce noises.
Oneis that clauses are not extracted from the sen-tence in which sentence-end clause?s utterance-type is not recognized as an action.
In the fol-lowing example, ???
(simmer)?
and ???
(cut)?are not extracted because the utterance type of758Table 3: An example of the automatically con-structed case frame.Verb Casemarker Exampleskiru:1 ga <agent>(cut) wo pork, carrot, vegetable, ?
?
?ni rectangle, diamonds, ?
?
?kiru:2 ga <agent>(drain) wo damp ?
?
?no eggplant, bean curd, ?
?
?ireru:1 ga <agent>(add) wo salt, oil, vegetable, ?
?
?ni pan, bowl, ?
?
?ireru:2 ga <agent>(carve) wo knife ?
?
?ni fish ?
?
?the sentence-end clause is recognized as [substi-tution].
(2) ????
[individual action]????
[indi-vidual action]?????
[substitution]?
(Itdoesn?t matter if you cut it after simmering.
)The other is that conditional/causal clauses arenot extracted because they sometimes refer to theprevious/next topic.
(3) ??????
????????
(After wefinish cutting it, we?ll fry.
)(4) ????????
????????????????
(We cut in this cherry tomato,because we?ll fry it in oil.
)Note that relations between clauses are recognizedby clause-end patterns.Verb sense disambiguation by assigning to acase frameIn general, a verb has multiple mean-ings/usages.
For example, ?????
has multipleusages, ??????
(add salt)?
and ???????
(carve with a knife)?
, which appear indifferent topics.
We do not extract a surface formof verb but a case frame, which is assigned bycase analysis.
Case frames are automaticallyconstructed from Web cooking texts (12 millionsentences) by clustering similar verb usages(Kawahara and Kurohashi, 2002).
An example ofthe automatically constructed case frame is shownin Table 3.
For example, ??????
(add salt)?is assigned to ireru:1 (add) and ???????
(carve with a knife)?
is assigned to case frameireru:2 (carve).3.1.2 Cue phrasesAs Grosz and Sidner (Grosz and Sidner, 1986)pointed out, cue phrases such as now and wellserve to indicate a topic change.
We use approx-imately 20 domain-independent cue phrases, suchas ???
(then)?, ???
(next)?
and ????????
(then)?.3.1.3 Noun ChainingIn text segmentation algorithms such as Text-Tiling (Hearst.M, 1997), lexical chains are widelyutilized for detecting a topic shift.
We utilize sucha feature as a clue to topic persistence.When two continuous actions are performed tothe same ingredient, their topics are often identi-cal.
For example, because ????
(grate)?
and ????
(raise)?
are performed to the same ingredi-ent ????
(turnip)?
, the topics (in this instance,preparation) in the two utterances are identical.
(5) a.
??????????????????
(We?ll grate a turnip.)b.
????????????????
(Raise this turnip on this basket.
)However, in the case of spoken language, be-cause there exist many omissions, it is often thecase that noun chaining cannot be detected withsurface word matching.
Therefore, we detectnoun chaining by using the anaphora resolutionresult2 of verbs (ex.
(6)) and nouns (ex.(7)).
Theverb, noun anaphora resolution is conducted bythe method proposed by (Kawahara and Kuro-hashi, 2004), (Sasano et al, 2004), respectively.
(6) a.
??????????
(Cut a cabbage.)b.
??
[?????]
?????
(Wash itonce.
)(7) a.
??????????????????
(Slice a carrot into 4-cm pieces.)b.
[?????]
???????????
(Peel its skin.
)3.1.4 Verb ChainingWhen a verb of a clause is identical with thatof the previous clause, they are likely to have thesame topic.
We utilize the fact that the adjoiningtwo clauses contain an identical verbs or not as anobserved feature.
(8) a.
?????????????
(Add somered peppers.
)2[ ] indicates an element complemented with anaphoraresolution.759b.
?????????
(Add chickenwings.
)3.2 Image FeaturesIt is difficult for the current image processing tech-nique to extract what object appears or what ac-tion is performing in video unless a detailed ob-ject/action model for a specific domain is con-structed by hand.
Therefore, referring to (Hamadaet al, 2000), we focus our attention on color dis-tribution at the bottom of the image, which is com-paratively easy to exploit.
As shown in Figure 1,we utilize the mass point of RGB in the bottom ofthe image at each clause.3.3 Audio FeaturesA cooking video contains various types of audioinformation, such as instructor?s speech, cuttingsounds and frizzling sound.
If cutting sound orfrizzling sound could be distinguished from othersounds, they could be an aid to topic identification,but it is difficult to recognize them.As Galley et al (Galley et al, 2003) pointedout, a longer silence often appears when topicchanges, and we can utilize it as a clue to topicchange.
In this study, silence is automatically ex-tracted by finding duration below a certain ampli-tude level which lasts more than one second.4 Topic Identification based on HMMsWe employ HMMs for topic identification, wherea hidden state corresponds to a topic and vari-ous features described in Section 3 are observed.In our model, considering the case frame as abasic unit, the case frame and background im-age are observed from the state, and discoursefeatures indicating to topic shift/persistence (cuephrases, noun/verb chaining and silence) are ob-served when the state transits.4.1 ParametersHMM parameters are as follows:?
initial state distribution ?i : the probabilitythat state si is a start state.?
state transition probability aij : the probabil-ity that state si transits to state sj .?
observation probability bij(ot) : the proba-bility that symbol ot is emitted when state sitransits to state sj .
This probability is givenby the following equation:bij(ot) = bj(cfk) ?
bj(R,G,B)?
bij(discourse features) (1)- case frame bj(cfk): the probability thatcase frame cfk is emitted by state sj .- background image bj(R,G,B): the prob-ability that background image bj(R,G,B) isemitted by state sj .
The emission probabilityis modeled by a single Gaussian distributionwith mean (Rj ,Gj ,Bj) and variance ?j .- discourse features : the probability thatdiscourse features are emitted when state sitransits to state sj .
This probability is definedas multiplication of the observation probabil-ity of each feature (cue phrase, noun chain-ing, verb chaining, silence).
The observationprobability of each feature does not dependon state si and sj , but on whether si and sjare the same or different.
For example, in thecase of cue phrase (c), the probability is givenby the following equation:bij(c) ={psame(c)(i = j)pdiff (c)(i 6= j)(2)4.2 Parameters EstimationWe apply the Baum-Welch algorithm for esti-mating these parameters.
To achieve high accu-racy with the Baum-Welch algorithm, which isan unsupervised learning method, some labeleddata have been required or proper initial param-eters have been set depending on domain-specificknowledge.
These requirements, however, makeit difficult to extend to other domains.
We auto-matically extract ?pseudo-labeled?
data focusingon the following linguistic expressions: if a clausehas the utterance-type [action declaration] and anoriginal form of its verb corresponds to a topic, itstopic is set to that topic.
Remind that [action dec-laration] is a kind of declaration of starting a seriesof actions.
For example, in Figure 1, the topic ofthe clause ?We?ll saute.?
is set to sauteing becauseits utterance-type is recognized as [action decla-ration] and the original form of its verb is topicsauteing.By using a small amounts of ?pseudo-labeled?data as well as unlabeled data, we train theHMM parameters.
Once the HMM parameters aretrained, the topic identification is performed usingthe standard Viterbi algorithm.5 Experiments and Discussion5.1 DataTo demonstrate the effectiveness of our proposedmethod, we made experiments on two kinds ofcooking TV programs: NHK ?Today?s Cooking?760Table 5: Experimental result of topic identification.Features Accuracycase frame background image discourse features silence ?Today?s Cooking?
?Kewpie 3-Min Cooking?
?61.7% 66.4%?56.8% 72.9%?
?69.9% 77.1%?
?
?70.5% 82.9%?
?
?
?70.5% 82.9%Table 4: Characteristics of the two cooking pro-grams we used for our experiments.Program Today?s Cooking Kewpie 3-Min CookingVideos 200 70Duration 25min 10min# of utterancesper video 249.4 183.4and NTV ?Kewpie 3-Min Cooking?.
Table 4presents the characteristics of the two programs.Note that time stamps of closed captions syn-chronize themselves with the video stream.
Ex-tracted ?pseudo-labeled?
data by the expressionmentioned in Section 4.2 are 525 clauses out of13564 (3.87%) in ?Today?s Cooking?, and 107clauses out of 1865 (5.74%) in ?Kewpie 3-MinCooking?.5.2 Experiments and DiscussionWe conducted the experiment of the topic iden-tification.
We first trained HMM parameters foreach program, and then applied the trained modelto five videos each, in which, we manually as-signed appropriate topics to clauses.
Table 5gives the evaluation results.
The unit of evalua-tion was a clause.
The accuracy was improvedby integrating linguistic and visual informationcompared to using linguistic / visual informa-tion alone.
(Note that ?visual information?
usespseudo-labeled data.)
In addition, the accuracywas improved by using various discourse features.The reason why silence did not contribute to ac-curacy improvement is supposed to be that closedcaptions and video streams were not synchronizedprecisely due to time lagging of closed captions.To deal with this problem, an automatic closedcaption alignment technique (Huang et al, 2003)will be applied or automatic speech recognitionwill be used as texts instead of closed captionswith the advance of speech recognition technol-ogy.Figure 3 illustrates an improved example byadding visual information.
In the case of usingonly linguistic information, this topic was rec-First, saute andbody.Chop a garlicnoisely.Let?s start cookedvegitable.preparation sauteingsauteinglinguisticlinguistic+ visualFigure 3: An improved example by adding visualinformation.ognized as sauteing, but this topic was actuallypreparation, which referred to the next topic.
Byusing the visual information that background colorwas white, this topic was correctly recognized aspreparation.We conducted another experiment to demon-strate the validity of several linguistic processes,such as utterance-type recognition and word sensedisambiguation with case frames, for extractinglinguistic information from closed captions de-scribed in Section 3.1.1.
We compared our methodto three methods: a method that does not per-form word sense disambiguation with case frames(w/o cf), a method that does not perform utterance-type recognition for extracting actions (uses allutterance-type texts) (w/o utype), a method, inwhich a sentence is emitted according to a state-specific language model (bigram) as Barzilay andLee adopted (bigram).
Figure 6 gives the exper-imental result, which demonstrates our method isappropriate.One cause of errors in topic identification is thatsome case frames are incorrectly constructed.
Forexample, kiru:1 (cut) contains ??????
(cuta vegetable)?
and ?????
(drain oil)?.
Thisleads to incorrect parameter training.
Other causeis that some verbs are assigned to an inaccuratecase frame by the failure of case analysis.6 ConclusionsThis paper has described an unsupervised topicidentification method integrating linguistic and vi-sual information based on Hidden Markov Mod-761Table 6: Results of the experiment that compares our method to three methods.Method Accuracy?Today?s Cooking?
?Kewpie 3-Min Cooking?proposed method 61.7% 66.4%w/o cf 57.1% 60.0%w/o utype 61.7% 62.1%bigram 54.7% 59.3%els.
Our experiments on the two kinds of cookingTV programs showed the effectiveness of integra-tion of linguistic and visual information and in-corporation of domain-independent discourse fea-tures to domain-dependent features (case frameand background image).We are planning to perform object recognitionusing the automatically-constructed object modeland utilize the object recognition results as a fea-ture for HMM-based topic identification.ReferencesNoboru Babaguchi and Naoko Nitta.
2003.
Intermodalcollaboration: A strategy for semantic content anal-ysis for broadcasted sports video.
In Proceedings ofIEEE International Conference on Image Process-ing(ICIP2003), pages 13?16.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In Proceedings ofthe NAACL/HLT, pages 113?120.Peng Chang, Mei Han, and Yihong Gong.
2002.Extract highlights from baseball game video withhidden markov models.
In Proceedings of theInternational Conference on Image Processing2002(ICIP2002), pages 609?612.Michel Galley, Kathleen McKeown, Eric Fosler-Lussier, and Hongyan Jing.
2003.
Discourse seg-mentation of multi-party conversation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 562?569, 7.Barbara J. Grosz and Candace L. Sidner.
1986.
Atten-tion, intentions, and the structure of discourse.
Com-putational Linguistic, 12:175?204.Reiko Hamada, Ichiro Ide, Shuichi Sakai, and Hide-hiko Tanaka.
2000.
Associating cooking video withrelated textbook.
In Proceedings of ACM Multime-dia 2000 workshops, pages 237?241.Hearst.M.
1997.
TextTiling: Segmenting text intomulti-paragraph subtopic passages.
ComputationalLinguistics, 23(1):33?64, March.Chih-Wei Huang, Winston Hsu, and Shin-Fu Chang.2003.
Automatic closed caption alignment basedon speech recognition transcripts.
Technical report,Columbia ADVENT.Radu Jasinschi, Nevenka Dimitrova, Thomas McGee,Lalitha Agnihotri, John Zimmerman, and Dongge.2001.
Integrated multimedia processing for topicsegmentation and classification.
In Proceedings ofIEEE International Conference on Image Process-ing(ICIP2003), pages 366?369.Daisuke Kawahara and Sadao Kurohashi.
2002.
Fertil-ization of case frame dictionary for robust japanesecase analysis.
In Proceedings of 19th COLING(COLING02), pages 425?431.Daisuke Kawahara and Sadao Kurohashi.
2004.
Zeropronoun resolution based on automatically con-structed case frames and structural preference of an-tecedents.
In Proceedings of The 1st InternationalJoint Conference on Natural Language Processing,pages 334?341.Sadao Kurohashi and Makoto Nagao.
1994.
A syntac-tic analysis method of long japanese sentences basedon the detection of conjunctive structures.
Compu-tational Linguistics, 20(4).Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-sumoto, and Makoto Nagao.
1994.
Improve-ments of Japanese morphological analyzer JUMAN.In Proceedings of the International Workshop onSharable Natural Language, pages 22?28.Daniel Marcu.
2000.
The rhetorical parsing of unre-stricted texts: A surface-based approach.
Computa-tional Linguistics, 26(3):395?448.Huu Bach Nguyen, Koichi Shinoda, and Sadaoki Fu-rui.
2005.
Robust highlight extraction using multi-stream hidden markov models for baseball video.
InProceedings of the International Conference on Im-age Processing 2005(ICIP2005), pages 173?176.Dinh Q.Phung, Thi V.T Duong, Hung H.Bui, andS.Venkatesh.
2005.
Topic transition detection usinghierarchical hidden markov and semi-markov mod-els.
In Proceedings of ACM International Confer-ence on Multimedia(ACM-MM05), pages 6?11.Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-hashi.
2004.
Automatic construction of nominalcase frames and its application to indirect anaphoraresolution.
In Proceedings of the 20th InternationalConference on Computational Linguistics, number1201?1207, 8.762
