INTERACTIVE NATURAL LANGUAGE PROBLEM SOLVING:A PRAGMATIC APPROACH* ** 8allard*, ** A. Biermann , R. Rodman , B. T. Betancourt ,**  ?
Fineman t G. Bi lbro , H. Deas , L.* * * Heidlage* P. Fink , K. Gi lbert , D. Gregory , F.* Department of Computer ScienceDuke Univers i tyDurham, North Carol ina** Department of Computer ScienceNorth Carol ina State Univers i tyRaleigh, North Carol inaABSTRACTI NTRODUCT IONA class of natural language proces-sors is descr ibed which al low a user todisplay objects of interest on a computerterminal and manipulate them via typed orspoken Engl ish sentences.This paper concerns itself with theimplementat ion of the voice input faci l i tyusing an automatic speech recognizer, andthe touch input faci l i ty using a touchsensit ive screen.
To overcome the higherror rates of the speech recognizer undercondit ions of actual problem solving innatural language, error correct ionsoftware has been designed and isdescr ibed here.
Also described are prob-lems involving the resolution of voiceinput with touch input, and the identif i -cation of the intended referents of touchinput.To measur~ system performance we haveconsidered two classes of factors: thevarious condit ions of testing, and thelevel and quality of training of the sys-tem user.
In the paper a sequence of fivedifferent testing situations is observed,each one result ing in a lowering of systemperformance by several percentage pointsbelow the previous one.
A training pro-cedure for potential  users is described.and an experiment is discussed which util-izes the training procedure to enableusers to solve actual non-trivial problemsusing natural language voice communica-tion.A class of natural language proces-sors is under development which al low auser to display objects of interest on acomputer terminal and manipulate them viatyped or spoken Engl ish imperative sen-tences.
Such a processor is designed torespond within one to four seconds by exe-cuting the input command and updating thedisplayed world for user veri f icat ion.
Ifan undesired action is observed, a"backup" command makes it possible to undoany action and return the system to a pre-vious state.
The domains of interestinclude matrix computation, where one candisplay tables of data and manipulatethem: off ice automation, where one canwork with texts, files, calendars, or mes-sages: and machine control, where onemight wish to command a robot or otherequipment via natural language input.The first such system (Biermann andBallard \[6\]), cal led NLC, provides amatr ix computat ion faci l ity and al lowsusers to display matrices, enter data, andmanipulate the entries, rows, and columns.It became operat ive in Ig79 and includes avariety of special purpose features1 ~his Work was supported by NationalScience Foundat ion Grants MCS 7904120 andMCS 8113491, by the IBM Corporat ion underGSD agreement no.
260880, and by theUnivers i te de Paris-Sud, Laboratoire deRecherche en Informatique during the sum-mer of Ig82.1~0including arbitrari ly deep nesting of noungroups, extensive conjunction processing,user defined imperative verbs, and loopingand branching features.
More recently, adomain independent abstraction of the NLCsystem has been constructed and now isbeing specialized to handle a text pro-cesslng task.
In this system, text can bedisplayed and modified or formatted withnatural language commands.Current work emphasizes the additionof voice input, voice output, and a touchsensitive display screen.
Speech recogni-tion is being done on an experimentalbasis with the Nippon Electric DP-200 Con-nected Speech Recognizer in both discreteand connected speech modes, and with theVotan Corporation V-SO00 Development Sys-tem.
The touch sensitive screen beingused is a Carroll touch panel mounted on a19-inch color monitor.
Voice response isalso provided by the Votan V-5000 whichassembles and vocalizes digital ly recordedhuman voice messages.
The work has pro-gressed to the point where OUr naturallanguage matrix computer NLC is operativeunder voice control using the DP-200 andthe text processing system is beginning tofunction using the V-5000 speech recog-nizer.
The touch panel interface andvoice response systems are still in thedesign phase.The goal of the project is to makepossible voice and touch interactions ofthe following kind:Retrieve file Budget83.Find the largest number in thiscolumn and zero it.
(with touchinput)Add this column putting the resulthere.
(with two touch inputs)Send this file to Jones and file itas Budget83.
(touch input)~at  is, imperative sentences are to beprocessed that operate on domain objectsto produce modifications to the existingobjects or their relationship to eachother.
The objects are, for example,rows, columns, numbers, entries, labels,etc.
in the matrix domain or sections,paragraphs, sentences, margins, pages,etc.
in the text processing domain.
Theexecution of each command is accompaniedby an update of the displayed data withhighlighting to indicate changes.
Promptsand error messages will be given by voiceresponse, gystem design is aimed atallowing fast interactive control of theobjects on the screen while the user main-tains uninterrupted eye contact with th~events as they happen.A continuous program of human factorstesting has been maintained by the pro jec tin order to build a realistic view ofpotential users and to measure Progress inachieving usability.
For example, in atest of the matrix computation system withtyped input, twenty-three subjects solvedproblems similar to those that might beassigned in a first course in programming(Biermann, Ballard, and Sigmon \[7\]).
Inthis test, the NLC system correctly pro-cessed 81 percent of the sentences andusers were quite satisfied with its gen-eral performance.
Other tests of the sys-tem are described in Fink \[14\] and Geistet el.
\[IS\].
In another test (Fineman\[13\]), a simulator for a voice drivenoffice automation system was used toobtain data on user behaviors when problemsolving is with discrete and slow con-netted speech.
It was found that usersquickly adapted their speech to therequired discipl ine of slow, methodical,and simple sentences which can be recog-nized by machine.
Since the data obtainedin any system test is heavily dependent onthe amount and kind of training given tosubjects, it is necessary to have a stand-ardlzed training procedure.
In thecurrent work, a voice tutorial has beendeveloped for training users to use avoice interactive system (Deas \[Ii\]).This paper reports on the currentstatus of these projects with emphasis onsystem design, speech input facilities andtheir performance, the touch input systemand human factors considerations.SYSTEM OVERVZEW181The basic system design includesmodules to do the following tasks:(i) token acquisit ion(2) parsing(3) noun group resolution(4) imperative verb execution(5) flow-of-control semantics(6) system outputThe token acquisit ion phase receivestyped inputs, word guesses ~com the voicerecognizer, and screen coordinates fromthe touch panel.
These inputs are prepro-cessed and passed tO the parser which usesan augmented transition network to ~is-cover the structure of the command and theroles of the individual tokens.
~oungroup resolution attempts to discover whatdomain objects are being referred to, andthe verb execution module transforms thoseobjects as requested by the imperativeverb.
The f low-of-control semanticsmodule manages the execution of meta-imperative verbs such as ~ ,  and han-dles user-defined imperatives.
Finally,system output displays the state of theworld on the screen.
Any module may issueprompts and error messages via text orspoken output.
Backup from any givenmodule to an earlier stage may occur inunusual situations.
More detai ls appearin Ballard \[i\], Biermann \[5\], Biermann andBallard \[6\], and Eallard and 8iermann \[3\].SPEECH INPUTAn automatic speech recognizer suchas the DP-200 or V-5000 recognizes speechby means of pattern matching algorithms.A subject is introduced to the device fora training session, and asked to repeatthe various words of the vocabulary into amicrophone.
The device extracts andstores bit patterns corresponding to eachvocabulary word uttered by that part icularspeaker.
After training, when a speakerwishes to use the device, the appropriatebit patterns are loaded.
Each utteranceof the speaker is compared with the pre-stored bit patterns and the best matchabove a threshold limit is presented asthe recognized word.
Depending on thedevice being used, the speaker may berequired to talk with discrete or con-nected speech.
The results descr ibedbelow were obtained primari ly in thediscrete mode with a pause of at least 200mil l iseconds after each word.Error Handlin~The major di f f iculty facing users ofautomatic speech recognit ion equipment isthe high error rate.
Even the best dev-ices in the best of circumstances are notentirely free of error, and when cir-cumstances are less than optimal, and morelike the real world, the error rate rises.Thus, a good part of the project efforthas gone into coping with errors in recog-nition.
In our view the speech recogni-tion device is a component of the largernatural language computing system, and ourgoal is to reduce the system error rate asmuch as possible.
We have thereforedesigned error correction software thatcorrects for certain kinds of errors, anderror messages that elicit repetit ion fromthe human subject in less tractable cases.Error correction essential ly func-tions by start ing with a sequence of wordguesses from the input system and filter-ing out the meaningless alternatives atthe appropriate stages of processing.Beginning in the token acquisit ion phase,certain unacceptable word sequences can bedisal lowed.
For example, a noun such as"matrix" or "row" would be disal lowed asthe first word in the sentence since thisis i l legal in the system grammar.
In theparsing phase, a grammatical sequence ofwords is selected from the incoming setsof word guesses.
Thus all ungrammaticalword sequences are el iminated.
The parseralso disal lows phrases containing certainsemantical ly  unacceptable relat ionshipssuch asthe second row in 6.or phrases containing disal lowed opera-tions such asAdd the matr ix to 6.In the noun group processor and Laterstages, various other semantic errors canbe el iminated such as references to nonex-istent objects or impossible operations.For discrete mode operations, errorsare c lassi f ied into four types:a. Substitutions.The device reports word B whenword A was actual ly spoken.b.
Re~ections.The device sends a reject ioncode when a vocabulary word wasspoken.c.
Insertions.The device reports a vocabularyword when a non-vocabulary word,or noise, was uttered:d. Fusions.
Two (or more) words arespoken but only one word isreported.Subst i tut ion ErrorsSubst i tut ion errors are the easiestto correct since the subst ituted wordoften resembles the actual word phoneti-cally.
Some of the substitut ions arefairly predictable, e.g.
"by" for"five", "and" for "add", or "up" for"of".
We have coined the term synophoneto describe such sets.
Many synophonepairs are symmetr ical ly  interchangable:however, some are not.
For example, withsome speakers, the word "a" is fre-quently reported as "eight" although theconverse seldom occurs.Synophones of a part icular wordutterance come from two sources: alter-nate guesses offered by the recognit iondevice based on its pattern matching com-putation, and a set of words stored in thesystem that are known to be confused withthe selected word.
Whenever a token iscol lected by the scanner, its synophone182llst is compiled.
Passing the completeset of synophones for each word to theparser would result in excessive parsetime so it is desirable to  eliminatebeforehand any synophones whose occurrencecan be determined to be impossible basedon grammatical or contextual considera-tions.
For example the syntax of English(and of NLC) prevents certai~ words fromoccurring next to each other, or beginningor ending sentences.
This information isrecorded i n  a table of adJacencies.
Ifthere is a synophone in a word slot thatcannot be preceded by any of the syno-phones in the previous word slot thatsynophone is deleted.
This process isrepeated until no more deletlons are pos-sible.
On average, roughly one-half ofthe candidate synophones are deleted.Since parsing time may increase exponen-tlally with the number of candidate syno-phones, and this table driven el iminationprocess is very quick, considerable sav-ings result.For reasons of indivldual speechvariation some vocabulary words will havesynophones peculiar to an individualspeaker.
The set of synophones of eachvocabulary word is therefore augmented toaccommodate this situation so that eachspeaker has personalized synophone sets.Early training includes a tutorial intro-ductlon, part of which  requires the sub-Ject to repeat sentences word for word.In this mode, the software has a prioriknowledge of the correct token--for eachword slot.
If a given word slot does notcontain the correct token, the substitutedword can be added to the appropriate syno-phone set for that subject.
Thereafter,if the same substitution error recurs dur-ing a session with that subject, thecorrect word will be included in the syno-phone list for that word slot.Re~ection ErrorsThe occurrence of one or more rejec-tions in a sentence almost always resultsin a request for repetition.
However, weare designing a number of facilities tohandle rejections.
I n  some cases, therejected word can be determined from con-text, and processing can continue uninter-rupted.
Otherwise, the current plan is tohandle a single rejection by returning anaudio response that repeats all of thesentence with the word "what" in place ofthe rejected element.
The speaker willthen .be able to choose to repeat therejected word or, in case other errors areapparent, to repeat the entire utterance.I n  cases of multiple rejectionerrors, the speaker is requested to repeatthe entire utterance.
In all cases previ-ous utterances will not be.
discarded.
Thescanner will merge them, complete withsynophones, in an attempt to el iminatereJectione and provide the broadest amountof information from which to extract whatthe speaker actual ly said.
For example,if the actual utterance wereABC D E FGand the recognizer returnedAm * Z E*  Gwhere * s tands  for rejection, the speakerwill be asked to repeat.
IfABC*  EFHis then recognized, it will be combinedwith the first utterance so that thescanner considers the seven word slots tocontain=s(A) sis) sic) s(z) sis) s(F) s(G)sin)where siX) is the union of X with itssynophones.
(Hopefully D is in s(Z).)
Ifsubsequent utterances are so differentfrom previous ones that they are unl ikelyto be word-for-word repetitions (for exam-ple, by containing a different number ofwords), previous utterances will be dis-carded and processing will be startedover .It may also be possible to predict arejected word with some degree of cer -tainty based on semantic or pragmaticinformation.
(We consider pragmatics toinvolve discourse dependent contextualfactors.)
For example suppose the scannerreceives from the recognizer:Double * nine and add column four to it.The most l ikely possibi l it ies for therejection are entry, row and column.Entry can be ellmz~'~ate~--on semanticgrounds since it is meaningless to a<\]d acolumn to an entry.
Row is semantical lypossible, but pragma-'~cally less likelythan column since adding columns tocolumns is much more common than addingcolumns to rows.
Thus column may bechosen.
Furthermore if t-h-e matrix infocus is six by seven, then the nine is asubstitution error, and the sentence willbe rejected on pragmatic grounds ini-tially.
However, since five is a syno-phone of nine the sentence ~ be triedwith flve 'in the place of nine.
Ulti-mate!y t't~'~e~e user will see displayS, on thescreen the result From:Double column five and add columnfour to it.183The act iv ity descr ibed above is tran-sparent to the user.
If the results areunsat is factory to the user, the command"backup" will undo them.An addit ional source of pragmaticerror correct ion comes from utterances inh is tor ica l ly  similar dialogs.
We aredeveloping a method for ut i l iz ing thistype of information.
Consider ing the lastexample, if the user had been addingcolumns to rows quite freque~: "~-" in thecurrent and/or recent sessions, but rarelyif ever adding columns to columns, thesystem would choose row as the rejectedword.Insertion Errors and Fusion ErrorsMost speech recognizers al low thethreshold value to be adjusted that deter-mines whether the best match is "recog-nized" or is rejected.
Since rejectionsare harder to correct for than substitu-tions there is reason to lower this value.Too low a value, however, aggravates theinsert ion problem.
When the speakerutters a non-vocabulary word, or emits agrunt or uncouth sound, the correctresponse is a rejection.
A non-reject ionin this s i tuat ion may be dif f icult  to dealwith.In our experience users have l ittletrouble in conf ining themselves to thetrained vocabulary.
Most insert ion errorsoccur between sentences, rather thanbetween words within a sentence.
Thisresults in extraneous "words" in the firstone or two word slots.
These can often beel iminated because neither they nor theirsynophones can begin a sentence in the NLCgrammar.
Timing considerations, too,could be used to eliminate, or at leastcast suspicion on, inter-sentence inser-tions, though we have not found the needfor such measures.Raw Error RateAlthough a good deal of our interesti s  in correct ing or compensat ing for thevarious kinds of errors in recognit ion, weare also working on ways to reduce theactual number of errors made by the recog-nit ion devices (the raw error rate).Careful vocabulary choice and proper tun-ing of the hardware such as thresholdlevel select ions are crucial factors.It is important to choose vocabularywords as widely separated phonet ic~l ly  asc ircumstances allow.
Addit ional ly,  wehave found that words containing non-str ident fr icatives (e.g.
the th infifth), af fr icates (e.g.
the c--h inc u-'~-~r'ch), l iquids (r and I) and nasals-'(m,nan~q)  are mort  di f fTcult  to recognizethan words contain ing other sounds.Monosyl lab ic  words, in general, are notrecognized as readi ly as po lysy l lab icones, though words that are long and dif-ficult to pronounce (e.g.
anaesthet ist)are also to be avoided.
Often the domainleaves l ittle lat itude for vocabularychoice.
If ordinal numbers are needed itis necessary to have fifth and sixth,which are di f f icult  to - -~ ingu ish .
Butinstead of a word like rate which iseasi ly confused with eig t~-~-, tax rate orrate-of-pay (pronounced as a s ingle-- '~rd)m%--~t~ a better choice.Correct training procedures areinstrumental  in reducing the raw errorrate as are such factors as whether theuser receives immediate feedback from therecognizer, the form and frequency oferror messages request ing repetit ion, andthe degree of comfort fett by the userinsofar as att itude toward computers isconcerned.
Some of these are d iscussedbelow in the section Measurin@ System Per-formance.We have observed fusion errors indiscrete mode.
They arise when thespeaker neglects to pause long enoughbetween words.
In our experience theyoccur so infrequently we have not tried tocompensate for them.
This type of erroris more crucial when operat ing in con-nected mode.
It may be the case that two(or possibly more) words are reported as asingle word different from either of thetwo or ig inal ly  uttered words.
It may alsohappen that two words, A and B, arereported as either A or 8.
In this casethe fusion error takes on the appearanceof an omission.
Our connected speechparser, current ly under construction, willhave the abi l i ty z9 guess an omission andinser t  a correct ion if suff icient contex-tual information is available.Some Miscel laneous QuestionsApart from error correction, a numberof other questions have arisen during ourimplementat ion of the voice driven system.Among these are:a) How is the beginning of a sen-tence detected?b) How is the end of a sentencedetected?c) How can a user make a correct ionin mid-sentence?Current ly  a sentence begins with anyinput after the end of the previous sen-tence.
The instances of inter- or pre-sentence insertions were discussed above.Sentences are terminated by themete-word over.
This word has few syno-18 ~.phones in the current word set and has theadvantage of being widely understood tomean "end of transmission."
However, weplan to experiment with other kinds oftermination such as use of touch input ort iming information.A user may misspeak in instructingthe computer to perform a task and maywish to repeat all or part of the command.Also, if the words from the woice recog-nizer are displayed as they are spoken,the user may desire to correct a misrecog-n i t ion .
'~ne metaword cor rect ion  i scurrently used to implement this facility.There are several levels of correction.Some may be accomplished by the scanner,while others require more information thanis avai lable to the scanner and musttherefore be handled by the parser.
Thesimplest type of correction consists ofchanging one word at the end of the sen-tence:Add row one to row fourcorrection three.Here the scanner merely deletes the wordslot before the metaword.
If severalwords follow "correction" as inAdd row one to row two correctionrow one to column three.the scanner detects this fact and scansbackward in the sentence, attempting tOmatch the largest possible number of wordslots before and immediately after themetaword.
In this example the tokens forrow, one and to match, so the scannercopies--t'~e last ~r t  of the sentence intothe earlier part of the buffer to arriveatAdd row one to column three.In the case of an utterance such asAdd row one to row twocorrection column three.it is impossible to match the tokensbefore and after the metaword.
Thescanner therefore deletes the token\[~Ime,\]iately before the metaword, flags theword slot preceding that token and passesthe result to the parser.
In the example,Add row one to row column three.is passed, with the word slot containingro w flagged.
The parser attempts to make185sense of the set of tokens passed.
If itcannot, the flagged word slot is deleted,the word previous to it is flagged andanother parse is attempted.
The processis repeated until a successful Parse isfound.
If none is found, an error messageis issued.
Thus in the example, afterfail ing tO Parse the tokens as passed, theparser triesAdd row one to column three.which is parsed successful ly.TOUCH INPUTAn important aspect of naturallanguage communication is pointing, whichis often used in connection with wordssuch as this, that, here and there.Pointing may---'f'~ncto~-o'n--as em-'m'~asis, as inPut the dog out.where either the dog, the outside, or pos-sibly both are pointed to.
Pointing alsofunctions to put objects into focus,al lowing subsequent references to use adefinite pronoun: for example,Move that there and cover it.with a point to the object to be moved andcovered.A point ing abi l i ty would fit in verynicely with voice driven NLC and our pro-Ject includes a touch sensit ive screen sothat the user can say "double this", pointto a row, and cause the processor to dou-ble every element in that row.
More com-plex sentences such asAdd this row to that row puttingthe results here.
(with threetouchee)also become possible.Apart from being "natural" in thesense that ordinary language users pointoften, pointing may increase the effi-ciency of communication.There has  been a good deal ofinterest among human factors scientists asto the eff ic iency of various modes of com-munication.
Past experiments, for exam-ple, have compared the eff ic iency of typedversus voice messages (voice messages aremore efficient).
We carried out an exper-iment to verify the hypothesis that voiceinput together with touch input is moreefficient than voice input alone, and weattempted to quantify the results, wesolved eight different types of matrixproblems including Gaussian elimination,div ided dif ferences and matrix inversion,using NLC without touch.
We then wentback and rewrote the solutions using thetouch facility, but without any otherchanges.
On the average 29% fewer wordswere needed to solve the problem, andindividual sentences were shortened by23%.A number of interest ing problemsarise when a touch faci l i ty is imple-mented.
One is how to pair up tacti le andverbal input in the way intended by theuser.
Another problem is identi fying theactual object the user intends to refer toonce the tacti le and verbal input havebeen resolved.An example of the latter problemwould be the commandDouble thisaccompanied by a touch of element <3,2> ofa displayed matrix.
Does the user want todouble element (3,2>, double row 3, doublecolumn 2, or even double the entirematr ix?
The same touch paired withDouble this entry.Double this matrix.Double this column.orDouble this matrix.would be unambiguous.
If the demonstra-tive is not accompanied by a nominal somestrategy is needed to process the sen-tence.
We opt for the smallest possiblenoun group encompassed by the touch (the<3,2> entry in the above case), and relyon our "backup" facil ity in case theuser's intentions are not fulfi l led.
Ifthe utterance "double this" is accompaniedby a touch of the displayed name of a row,column or matrix, then the named objectwill be referenced.Pair ing up touches with spokenphrases is straightforward when a singlenoun group is used with a single touch, asin "double this entry."
In a more compli-cated case we might haveAdd this entry to that rowand put the result here.accompanied by three touches.
The stra-tegy here is to -air touches and utter-ances in the order given by the user.In the last example all touches func-t ioned to establ ish focus or resol~=e no,~ngroup reference.
If the emphasis functionof touch is mixed in, a more dif f icults i tuat ion arises.
If three touches accom-panyAdd this entry to the first rowand put the result here.then the second touch was presumably toemphasize the first row or even to estab-lish a rhythm of touching.
In any casethe faci l i ty to match touches with non-deict ic expressions iS needed.
If onlytwo touches accompany this last sentencethen the focusing funct ion should takeprecedence, and the touches should bematched with "this entry" and "here.
"The s i tuat ion is made even more com-plex by the abi l i ty  to establ ish focusverbally.
In NLC the user can sayConsider row four.Double that row.and the express ion "that row" will referto row four.
~f the same utterance isaccompanied by a touch to a row other thanfour a potential  confl ict results.
Ourstrategy is to give precedence to touch,since it is the more immediate focussingmechanism.
Thus the sequenceConsider row four.Double that row.
(touching row three)will result in the doubl ing of row three.When both verbal and touch focus arepresent, nearly unresolvable ambiguit iesmay result.
The sequenceConsider row four.Add this row to that row.accompanied by one touch, gives rise tothe problem as to which demonstrat ive noungroup to associate with row four, andwhich to associate with the touch.
Onestrategy is to associate with a demonstra-tive noun group the touch that occurredclosest to the time of utterance.
Anotherpossible strategy is to assume that theexpression with that refers to the moredistant element in focus (the one esta-bl ished verbal ly in this case).
Thistakes advantage of the ~act that this andthat can be dist inguished in Engl ish gram-mar by the feature +NEAR.
Unfortunate lyby a simple change iF stress pattern aspeaker can undo this fairly weak regular-ity.
Thus the sequenceConsider row four.~dd th{s row to that row.186plus a single touch, where this bears prl-mary stress and that bears secondarystress, should flnd t-- e~touch  referring to"this row."
If the stress pattern wereAdd th i s  row to  that  row.with primary stress on Add, the touchwould more llkely be assoc--~-ated with thatrow.
It is unfortunate that to date we~w of  no vo ice  equ ipment  sens i t i veenough to  d i s t ingu ish  between two suchs t ress  patterns.Somewhat more complicated cases arepossible=Consider row three.~ld this row to that row andput the result in the first row.accompanied by two touches.
Since weallow a touch to occur with expressionssuch as "the first row," and since it isposs ib le  to d is regard  the element i n  ver-bal focus altogether, such a case producesmult iple ambiguities.
Although we foreseebeing able to resolve these ambiguit ieseffectively, and ca~ always fall back onour "backup" facil ity in case of mistakes,we also believe that such complex caseswill be extremely rare.
No sentence ofsuch complexity was produced in our solu-tions to  the e ight  prob lems ment ionedabove.
With a voice and touch facility,sentences tend to be shorter and simpler.NLC has implemented plurals, but wehave not considered their use in touchinput.
Such sentences asorMultiply these elements bythis element.Add these elements up.with multiple touches, would be useful.In the trial run of eight problems, theintroduction of plural ity resulted in upto fifty percent reduction in number ofwords needed and sentence length.MEASURING SYSTEM PERFORMANCEProgress in any endeavor is greatlyaided if the level of accomplishment canbe measured in some meaningful way.
It isdesirable to give a figure of merit for asystem both so that a project can indicateto the world the degree of the achievementand also so that the project can in ter -nally Judge its own improvements overtime.
In voice language processing, onecan attempt to measure performance by theword and sentence error rates.
However,exper ience  shows that these measures arehighly dependent on two factors and thatalmost any level of performance can bereached if those factors are appropr iatelyadjusted.
Those factors are(a) the environment and type of testwithin which the measurement ismade, and(b} the level of training of thesystem user.Type O~f Testln~ EnvironmentConsider ing (a), we tend to classifythe type of test for a recognizer into oneof the fol lowing five categories and weexpect signif icant differences in deviceresponse in each case.187(1) Lists of words are read in testsperformed by the manufacturer.
(2) Lists of words are read in ourlaboratory.
(3) Sentences are read in our labora-tory.
(discrete or connected)(4) Sentences are uttered in a prob-lem solving situation in ourlaboratory.
(discrete or con-nected)(5) Sentences are uttered in a prob-lem solving situation in the userenvironment.
(discrete or con-nected)In the first situation, a manufac-turer is interested in advertis ing thebest performance achievable.
Tests areperformed in control led conditions withmicrophone placement and all system param-eters set for optimum performance, and anexpert speaker is used.
In our labora-tory, we are not interested in the bestpossible system performance but ratherwhat we can real ist ical ly expect.
Theparameters are set at medium levels, thereis some ambient noise, the microphone ~aymove during the test, and the user wil\] beanyone we happen to bring in regardless oftheir speech characteristics.As soon as the sequential wordsbecome organized as sentences, situation(3), the speaker begins to impose inflec-tions on the utterance that will affectrecognition.
Certain words may bestressed, and intonation may rise an~\] fallas the sequential parts of each sentenceare voiced.
Training samples based onreading lists of vocabulary items tend tobe inaccurate templates for words spokenin context.
When sentences are spoken ina problem solving environment, s ituation(4), these effects increase and otheraspects of word pronunciat ion change.When voice control stops being the centralconcern of the speaker, largeT variationsin speech are bound to occur with accom-panying larger error rates.The most dif f icult  s ituation of alloccurs in s ituation (5) where the usermight not even be a person who could bebrought into a voice laboratory.
In thiscase, the user has on ly  one concern,achieving the desired machine performance.Encouragement to speak careful ly could bemet with impatience, and a few systemerrors could result in even worse speechqual i ty and further degraded performance.Our experience has been that worderror rates increase from about three toseven percent as one moves to each moredi f f icult  s ituation type depending on thevocabulary, t~e equipment, and other fac-tors.
Consequently, we tend to distrustany figures gathered in the easier classesof environments and attempt to do our owntesting in the more diff icult  and moreinteresting situations.
Most of ourrecent data is of type (4) and we hope togain some type (5) experience in the com-ing year.Training the System UserThe second major factor affectingvoice recognit ion performance is the levelof training of the system user.
Humansare extremely adaptive and capable oflearning behaviors to a high degree ofperfection.
Thus the designer of a voicesystem might, over the years, learn tochat with it like an old friend whereasothers might not be able to use the systemat all.
~gain, almost any level of systemperformance can be observed depending onthe quality of training of the user.Our approach to control l ing this fac-tor has been to develop a standardizedtraining procedure and to only reportstatist ics on uninit iated users whoseexperience with the system is l imited tothis procedure.
Ideally this procedurewould be administered by machine to obtainmaximum uniformity in training but thishas not yet been possible.The training procedure has two parts.The first part is an informal session inwhich the user is told how to speak indi-vidual words to the system and examples ofthe complete vocabulary are col lected bythe recognit ion system.
~he second partis administered very mechanical ly  by read-ing a tutorial document to the user andrequest ing the utterance of trial sen-tences.
This port ion of the trainingintroduces the user to the interactivesystem's capabi l i t ies and is speci f ical lydesigned to be administered by themachine.Some Performance DataAn experiment was run during the sum-mer of 1982 to obtain DP-200 performancedata in an environment of type (4) asdescribed above.
Beca~ise no voiceinteractive system was yet available, asystem simulat ion was used.
After thefirst part of the training session inwhich the voice samples were collected,the subject was placed in a room behind adisplay terminal with a head mountedmicrophone.
The voice tutorial was readto the subject through a loudspeaker atthe terminal introducing the capabi l i t iesof the simulated system and the types ofvoice commands that could be executed.The subject's commands were recognized bythe DP-200 and executed by the simulation.Thus each user command resulted in eitherappropr iate act ion visible on the screenor a voice error message.
In the finalport ion of the experiment, the subject wasasked to solve an invoice pro61em thatinvolved computing costs for a series ofindividual items and finding the tax andtotal.
The experiment gave a reasonablyaccurate simulat ion of the expected NLCsystem behavior when it becomes completelyvoice interactive.
The experimentattempted to simulate a syntact ic level ofvoice error correct ion but nothing deeper.It was fo,lnd that the DP-~00 worderror rate rose to about 20 percent inthis test with about 14 of the 20 percentbeing automatical ly  correctable.
Thevocabulary size was 80, with three samplesof most words, and six samples of a few ofthe diff icult words, stn1=ed in the DP-200.This means that roughly every two to foursentences will have a single word errornot correctable at shal low levels.
Thisdata comes from the first two hours o~usage for these subjects and we expectsignif icant improvement as usage experi-ence increases over time.More recently, the ~LC system hasbecome operat ive in a voice driven modeand subject testing has begun using thesame training procedure.
It is too earlyto report results but it appears that theperformance predicted in the s imulat iouwill be approximately achieved.
Thisexperiment will include longer usage bythe subjects and thus indicate how mucherror rates decrease over time.188In conclusion, we have at this timeonly fragmentary information regardingwhat levels of performance can beachieved.
HOwever, we have developed sometools for making measurements and willreport the results as they become avail-able.systems has been refined to the point thatit could actual ly support user interac-tions in real time as we are attempting todo.
Our project uses well developedspeaker dependent voice recognit ion equip-ment with a small enough vocabulary toachieve usable accuracy rates.OTRER kK)RKMuch of the applied work in naturallanguage processing has concerned databasequery (Bronnenberg et al C8\], CoddC9\],Harris\[17,18\], Hendrix\[22\], MylO-poUlOe\[27\], Plath\[29\], Thompson and Thomp-son\[32\], Weltz\[35\], and Woods et el.\[36\]).
At least one such system is beingmarketed (namely INTELLECT \[18\]), whileseveral others have been successful ly usedin pilot studies.
(Damerau\[10\], F.~ly andWescourt\[12\], Hershman et el.
\[24\],Krause\[25\], Tennant\[31\]).As descr ibed  in  th i s  paper ,  our  in i -t i a l  work with N-LC i nvo lved  programming  asan application area, while our more recentinterest has shifted toward officedomains.
However, as Petrick\[2R\]observes, many of the same technical prob-lems arise regardless of application area.For the most part, the imperative sentencestructures we are dealing with are simplerthan the question forms recognized by thedatabase systems cited above, whi le  ournoun phrases tend to exhibit more ela-borate structures.
Furthermore, whereastypical database sy-tems process eachinput separately, or perhaps seek to han-dle ellipsis by consult ing the immedlatelypreceding input, we build up a richersemantic context as a session proceeds tobe used in handling matters such as focusand pronoun resolution.The most distinctive features of ourpresent work are (a) the inclusion ofvoice input and output facilities, and (b)an attempt to deal with relatively "deep"relationships among domain objects.
Amore detailed discussion of the domain-independent mechanisms appears in Bier-mann\[5\], and as described in Ballard \[2\]the related LDC project being conducted inour laboratory is built around many ofthese techniques.
Similar research pro-jects which are moving away from a fixeddatabase setting include work by Haas andHendrix\[16\], Reldorn\[20\], Hendrix andLewis\[231, and Thompson and Thompson \[33\].During the 197O's a number of speechunderstanding systems were developed underARPA support (Lea \[26\], Reddy C30\], Walker\[34\], Woods \[37\]) and currently some sys-tems ace being built in other countries,for example \[19\].
Rowever, none of theset89r.Zl\[2\]\[3\]\[4\]\[s\]\[ 6\]\[71r87\[93\[i0\]REFERENCESB.W.
Ballard, "Semantic and Pro-cedural Processing for a NaturalLanguage Programming System," Ph.D.Dissertation, Report CS-1979-5, Dept.Of Computer Science, Duke University,Durham, NC, 1979.B.W.
Ballard, "A Domain-ClassApproach to Transportable NaturalLanguage Processing," COgnition andBrain Theory, 5, pp.
269-~87, 1982.B.W.
Ballard and A.W.
Biermann, "Pro-gramming in Natural Language: NLC asPrototype," Proceedings of the 197gACM National" Conference, "~to~, ,I-~J79.A.W.
Biermann.
"A Natural LanguageProcessor for Office Automation,"Proceedings of the 1982 Office Auto-marion Conir~re-'~'6e, San Franc{sco,~rai'l-~rnla, April, 1982.A.W.
8iermann, "Natural Language Pro-gramming," to appear in Computer Pro-~ m  Synthesis Methodologies (E~.ann and Guiho), Reide~, 1983.A.W.
Biermann and S.W.
Ballard,"Towards Natural Language Computa-tion," American Journal of Computa-tional L in@uis t l~ ,  vol.
6, No.
2,pp.--67-~T-86, 1980.A.W.
Biermann, ~.W.
Ballard, and A.H.Sigmon, "An Experimental Study ofNatural Language Programming," toappear in International Journal ofMan-Machine Studies, 1983.W.
Bronnenberg, S. Landsbergen, R.Scha, and W. Schoenmaker, "PHLIQA-I,A Question-Answering System forData-Base Consultat ion in NaturalEnglish," Philips Tech.
Rev., 38, DD.229-239 an~- -~8~' -~97,~1979.
""E.F. Codd, "Seven Steps to RENDEVOUSwith the Casua' User," IBM ReportJ1333, 1974.F.J.
Damerau, "Operating Statisticsfor the Transformational QuestionAnswering System," American Journalof Computational Linpuist~cs,N-~.
I, pp.
30-45, 1981.\[II\] H. Deas, M.Sc.
Thesis, Dept.
of Com-puter Science, Duke University, Dur-ham, N.C., November 1982.\[12\] D. Egly and K. Wescourt, "CognitiveStyle, Categorizations, and Voca-tional Effects on Performance of RELDatabase Users," Joint Conference onEasier and More P r~ct ive  Use o-~Comput i~ Systems, Ann Arbor,--~ch~--Nan, May 1981.\ [13 \ ]  L. Fineman, "Prel iminary Results onthe Voice Driven Information SystemSimulation Experiment," Report to IBMCorporation, Dept.
of Computer Sci-ence, Duke University, Durham, N.C.,1981.\[14\] P.K.
Pink, "Conditionals in a NaturalLanguage System" (Master's Thesis ),Report CS-1981-8, Duke University,Durham, N.C., 1981.\[153 R. Geist, D. Kraines, and P. Fink,"Natural Language Computing in aLinear Algebra Course," Proceedingsof the National Educational ComputingC'on e~ence ,  June, i982.\[16\] N. Haas and G. Hendrix, "An Approachto Acquir ing and Applying Knowledge,"First National Conference on Artif i-c 1 - -~ Inte l  !igence, 1980.\[17\] L.R.
Harris, "User Oriented Data BaseQuery with the ROBOT Natural LanguageQuery System," International Journalof Man-Machine Studies, pp.
6~-~,Sept em~e r---'~.\[183 L. Harris, "The ROBOT System:Natural Language Processing Appliedto Database  Query," Proceedings ofthe 1978 ACM National Conference, p~.\[19\] J.P. Haton and J.M.
Pierrel, "Data?
Structures and Organization of theMYRT ILLE II System, " FourthT.I.C.P.R., Kyoto, Japan, 1978.
- -\[20\] G. Heidorn, "Natural Language Dialo-gue for Managing an On-Line Calen-dar, " IBM ~esear ch Report RC7447,1978.\[21\] G.G.
Hendri x, E.D.
Sacerdot i, D.Sagalowicz, and J. Slocum, "Develop-ing a Natural Language Interface toComplex Data, " ACM Transactions onDatabase Systems,-~-~ol.
3, No.
2, pp-'?r0~:rr~, rvrs:--.\[22\] G.G.
Henarix, "Human Engineering forA oplied Natural Language Processing,"Fifth International Conference onAr---{'~icial Intelli~ence, pp.
183-191-~,~9~7.\[23\] G. Hendrix and W. Lewis, "Transport-able Natural Language Interfaces toDatabases," Annual Meeting of theAssoc.
for Compu~io~~uT-6t i c -~,\[24\] R. Hershman, R. Kelly, and H. Miller,"User Performance with a NaturalLanguage Query System for CommandControl," NPRDC TR 79-7, Navy Person-nel Research and Development Center,San Diego, California, January 1979.\[25\] J. Krause, "Results of a User Studywith the "User Specialty Language,"System and Consequences for theArchitecture of Natural LanguageInterfaces," Technical Report79.04.003, IBM Heidelberg Scientif icCenter, May 1979.\[26\] W.A.
Lea (Ed.
), Trends in SpeechRecognition, Prentice---~l,'-\[982.\[27\] J. Mylopoulos, A. Borgida, P. C~hen,N.
Roussopoulos, J. Tsotsos, and H.Wong, "TORUS - A Natural LanguageUnderstanding System for Data Manage-ment," Proceedings of the FourthInternational Conference on Arti f i -cial Intelli~enc"e, 1975.\[28\] S.R.
Petrick, "On Natural LanguageBased Computer Systems," IBM Journalof Research and Development, Vol.
~-~,~.
4, pp.
3~-335,  1976.\[29\] W.J.
Plath, "REQUEST: A NaturalLanguage Quest ion-Answering System,"ISM Journal of Research and Develop-ment, Vol.
20, No.
4, pp.
326-335,19-97~.\[30\] D.R.
Reddy, "Speech Recognit ion byMachine: A Review," Proceedings ofthe IEEE, Vol.
64, No.
4, pp.
50~ "/-\[31\] H. Tennant, "~xperience with theEvaluation of Natural Language Ques-tion Answerers," Working Paper 18,Advanced Automation Group, Coordi-nated Science Lab., U-iv.
of Illi-nois, January 1979.\[32\] F.B.
Thompson and B.H.
~ompson,"Practical Natural Language Process-ing: The REL System as Prototype,"in Advances in Computers, Vol.
13(Eds.
M. Rubino--~f and M.C.
Yovits),Academic Press, New York, 1975.\[33\] F. Thompson and B. Thompson, "Shaft-ing to a Higher Gear in a NaturalLanguage System," AFIPS Proc.
of theNational Computer Conf., Vol.
50, pp.6~7-662, 1981.190\[34\] D.E.
Walker (ed.
), Understandin~ Spo-ken Language, Elsevier North-Holland,Ne"'wYork, 1978.\[35\] D.L.
Waltz?
"An English LanguageOuestion Answering System for a LargeRelational Database," Communicationsof the ACM, Vol.
21, No.
7, pp.
526-\[36\] W.A.
Woods, R.M.
Kaplan, and B.Nash-Webber0 "The Lunar SciencesNatural Language Information System:Final RepOrt," RepOrt 2378, Bolt,Berenek, and Newman?
Cambridge, HA.,1972.\ [37\ ]  W.A.
Woods?
"Mot lvat lon  and Overv iewof  SPEECHLIS: An Exper lmenta l  P ro to -type  fo r  Speech Unders tand in  9Research , "  IEEE Transact ions  onAcoust i cs ,  Spee-e'c~, and Stqna l  Pr~:- ~; .
vo- r :~sp-~,  ~o.-:-~, pp.--'~-t9t
